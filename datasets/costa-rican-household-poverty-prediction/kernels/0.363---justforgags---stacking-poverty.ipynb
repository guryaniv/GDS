{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#[\"rooms\",\"r4h1\",\"r4h2\",\"tamviv\",\"hhsize\",\n\n#for outside wall---\n#brick-25.40--paredblolad\n#zinc 4 ---- paredzocalo\n#cement----4.25-paredpreb\n#waste ----0.50---- pareddes\n#wood----1.50----paredmad\n#zinc-----12---paredzinc\n#natural fiber-1.64--paredfibras\n#paredother--0.75-other\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1777d01808d05961a2695017c8f1b98feb2f449e"},"cell_type":"markdown","source":"**Visual analysis for costa rica poverty dataset**\n\nWe first start with importing libraries.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, ClusterMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"777f9a2e717b5a860c9c3c14f7bce900c9d481d2"},"cell_type":"markdown","source":"**Loading the data set.**\n"},{"metadata":{"trusted":true,"_uuid":"33414024c94bc55df2d56bbd21f97b60fefce423","_kg_hide-output":false},"cell_type":"code","source":"path_train=\"../input/train.csv\"\npath_test=\"../input/test.csv\"\n\ntrain_data=pd.read_csv(path_train)\ntest_data=pd.read_csv(path_test)\ntrain_data.head()\ndata_test_cpy=test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75120e4a57b8bcf4d99b611815ea8701701895e6"},"cell_type":"markdown","source":"We also generate a train test split to get test data for scoring purpose."},{"metadata":{"trusted":true,"_uuid":"2cc1ffa4352c4ce308c8406c85ad0db46503f8bc"},"cell_type":"code","source":"train_set, test_set = train_test_split (train_data, test_size = 0.1, random_state = 42)\ntrain_set_y=train_set.Target\ntrain_set_x=train_set.drop(['Target'],axis=1)\ntest_set_y=test_set.Target\ntest_set_x=test_set.drop(['Target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0ffcc10d854e0b176c1f87407fcd312aef56c16"},"cell_type":"markdown","source":"Now we generate info for the dataset to look at what we are dealing with"},{"metadata":{"trusted":true,"_uuid":"8632330723cbdd26b05867770d230b832a740f42"},"cell_type":"code","source":"train_data.info()\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa2cf5763f033f25dbe31fed7d87e0be329dcad"},"cell_type":"code","source":"train_data.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f4041b7324dae28ab429a7d03acd4a86eb14a13"},"cell_type":"markdown","source":"Cleaning the data"},{"metadata":{"trusted":true,"_uuid":"1721d7de339d4f43fce2dc119ed855fd256eb015"},"cell_type":"code","source":"print(train_data.isnull().sum().sum(),test_data.isnull().sum().sum())#We check number of NAN\ny=['Target']#setting target variable\n#to set X as all variables minustarget variables\nX=[i for i in train_data.columns if i!=\"Target\"]\ntrain_data.isnull().sum().sum()#getting number of NAN data\nfeature_lis=[]#the list of features with less than 4500 NAN\nfeature_lis_no_target=[]#for test data above list - {'target'}\nfor cols in train_data:#getting all the columns with more than 7000NAN deleted\n    #print(train_data[cols].isnull().sum().sum())\n    if train_data[cols].isnull().sum().sum() <4500:\n        feature_lis.append(cols)\n        if cols != 'Target':\n            feature_lis_no_target.append(cols)\ntrain_data=train_data[feature_lis]\n#doing same with test data\ntest_data=test_data[feature_lis_no_target]\nprint(train_data.isnull().sum().sum(),test_data.isnull().sum().sum())# now we see only 10 NAN values left out of soo many\ntrain_data=train_data.fillna(method='ffill')\ntest_data=test_data.fillna(method='ffill')\nprint(\"after forward fill\")\nprint(train_data.isnull().sum().sum(),test_data.isnull().sum().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded244f6f5f9396efd6674a1edac5ccdd0e6c4fa"},"cell_type":"markdown","source":"Finding all rows with which are string"},{"metadata":{"trusted":true,"_uuid":"c4c0649a429dc9065318e101e61e7e303d80db35"},"cell_type":"code","source":"train_data.edjefe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c6d74932c1420d6f38d31e2cc589cbb66b34848"},"cell_type":"code","source":"all_strings=[i for i in train_data.columns if isinstance(train_data[i][0], str)]\nprint(all_strings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1451ee76f6854ae8fd4053146389713cf1f0c2d6"},"cell_type":"markdown","source":"Cleaning edjefe and edjefa from any string data"},{"metadata":{"trusted":true,"_uuid":"5aee0825f8f9792b896c6109586ef149f52a9543"},"cell_type":"code","source":"#for edjefe and edjefa  we substitute yes with 1 and no with 0\nsubstitute={\"yes\":1,\"no\":0}\ntrain_data[\"edjefe\"].replace(substitute,inplace=True)\ntrain_data[\"edjefa\"].replace(substitute,inplace=True)\ntest_data[\"edjefe\"].replace(substitute,inplace=True)\ntest_data[\"edjefa\"].replace(substitute,inplace=True)\ntrain_data[\"dependency\"].replace(substitute,inplace=True)\ntest_data[\"dependency\"].replace(substitute,inplace=True)\ntrain_data=train_data.drop(['Id', 'idhogar'],axis=1).select_dtypes(exclude=['object'])\n\ntest_data=test_data.drop(['Id', 'idhogar'],axis=1).select_dtypes(exclude=['object'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad90cf68b8d9aeede753a71a3491e2e84bfd747d"},"cell_type":"markdown","source":"checking if training and test data cleared "},{"metadata":{"trusted":true,"_uuid":"e39c96acb3f283babbee25bd1198329da6f5daf4"},"cell_type":"code","source":"print(train_data.isnull().sum().sum(),test_data.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c58702f7d9f11f01cb661f9296fa43da89d27b2"},"cell_type":"markdown","source":"Now we start taking features as per our training set and try to find complete cost of house"},{"metadata":{"trusted":true,"_uuid":"ddf6ed8bd7cf987cca83df555d0f9858e18786c9"},"cell_type":"code","source":"\ndic_wall={\"paredblolad\":25.40,\"paredzocalo\":4,\"paredpreb\":4.25,\"pareddes\":0.50,\"paredmad\":1.50,\"paredzinc\":12,\"paredfibras\":1.64,\"paredother\":0.75}\ndic_floor={\"pisomoscer\":9.07,\"pisocemento\":4.25,\"pisonatur\":15,\"pisonotiene\":0,\"pisomadera\":12,\"pisoother\":0.5,}#if pisonotiene=0\ndic_roof={\"techozinc\":4,\"techoentrepiso\":9.07,\"techocane\":15,\"techootro\":0.5,}# if cielorazo=1 then else dont\nfeature_list_denoting_poor=[\"abastaguano\",\"noelec\",\"sanitario1\",\"energcocinar1\",]#abastaguano=1 if no water noelec=1 if no electricity sanitario1=1 if no toilet\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2363096f4f880e40cbda7ccb477ae064f27575f6"},"cell_type":"markdown","source":"We generate two new features froof_cost_approx_on_material and floor_cost_approx_on_material based on floor and roof data"},{"metadata":{"trusted":true,"_uuid":"9201c4e2ef02db921e3e3060d27c1ad8accbf237"},"cell_type":"code","source":"train_data_test=train_data.copy()\nmask=train_data_test[\"pisonotiene\"]==0\nmask2=test_data[\"pisonotiene\"]==0\ntrain_data_test.loc[mask,\"pisonotiene\"]=1\ntest_data.loc[mask2,\"pisonotiene\"]=1\n#print(train_data_test[\"pisonotiene\"])\n#train_data_test[\"pisonotiene\"]=train_data_test[\"pisonotiene\"].apply(lambda x: if x ==0 1 else 0)\ntrain_data_test[\"roof_cost_approx_on_material\"]=train_data_test[\"cielorazo\"]*(train_data_test[\"techozinc\"]*dic_roof[\"techozinc\"]+train_data_test[\"techoentrepiso\"]*dic_roof[\"techoentrepiso\"]+train_data_test[\"techocane\"]*dic_roof[\"techocane\"]+train_data_test[\"techootro\"]*dic_roof[\"techootro\"])\ntrain_data_test[\"floor_cost_approx_on_material\"]=train_data_test[\"pisonotiene\"]*(train_data_test[\"pisomoscer\"]*dic_floor[\"pisomoscer\"]+train_data_test[\"pisocemento\"]*dic_floor[\"pisocemento\"]+train_data_test[\"pisonatur\"]*dic_floor[\"pisonatur\"]+train_data_test[\"pisomadera\"]*dic_floor[\"pisomadera\"]+train_data_test[\"pisoother\"]*dic_floor[\"pisoother\"])\ntest_data[\"roof_cost_approx_on_material\"]=test_data[\"cielorazo\"]*(test_data[\"techozinc\"]*dic_roof[\"techozinc\"]+test_data[\"techoentrepiso\"]*dic_roof[\"techoentrepiso\"]+test_data[\"techocane\"]*dic_roof[\"techocane\"]+test_data[\"techootro\"]*dic_roof[\"techootro\"])\ntest_data[\"floor_cost_approx_on_material\"]=test_data[\"pisonotiene\"]*(test_data[\"pisomoscer\"]*dic_floor[\"pisomoscer\"]+test_data[\"pisocemento\"]*dic_floor[\"pisocemento\"]+test_data[\"pisonatur\"]*dic_floor[\"pisonatur\"]+test_data[\"pisomadera\"]*dic_floor[\"pisomadera\"]+test_data[\"pisoother\"]*dic_floor[\"pisoother\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035490b0a5670e2ad9a0c0ce7d36b071f764ad09"},"cell_type":"code","source":"train_data_test[\"floor_cost_approx_on_material\"]\n#train_data_test[\"number_of_materials_used_in_roof\"]==train_data_test[\"rooms\"]#thus we se","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfbf07df4e8491fc698edda266fad8d6c83d7dcc","scrolled":true},"cell_type":"code","source":"df=train_data_test.groupby(\"Target\").agg({\"roof_cost_approx_on_material\":['sum']}).plot.bar(figsize=(16,11),colormap='summer')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1773d2d8b2f2812bc4d4daddb5b50aeda77d6afe"},"cell_type":"markdown","source":"**As expected non vulnerable households have more spendings on roof and extreme poverty ones spend incredibly less**"},{"metadata":{"trusted":true,"_uuid":"cfb71df504ccc6c8d69ff93ad37553cca905c660","scrolled":false},"cell_type":"code","source":"fig = plt.figure()\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 50\nfig_size[1] = 60\nplt.rcParams[\"figure.figsize\"] = fig_size\nfeature_list=[\"rooms\",\"r4h1\",\"r4h2\",\"r4h3\",\"r4m1\",\"r4m2\",\"r4m3\",\"r4t3\",\"escolari\",\"hhsize\",\"SQBovercrowding\"]\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(223)\nax4 = fig.add_subplot(224)\nsns.barplot(\"Target\",\"rooms\",data=train_data_test,hue=\"area1\",ax=ax1)\nsns.barplot(\"Target\",\"rooms\",data=train_data_test,hue=\"area2\",ax=ax2)\nsns.barplot(\"Target\",\"meaneduc\",data=train_data_test,ax=ax3)\nsns.barplot(\"Target\",\"floor_cost_approx_on_material\",data=train_data_test,ax=ax4)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7217a6d7a78f192194e5fa26e9c5bf1349258924"},"cell_type":"markdown","source":"**We deduce following:**\n1.  Plot 221 shows non vulnerable category has more number of rooms both in rural nad urban area.\n2. We also see from plot 223 that more mean education means more non vulnerable\n3.Plot 224 shows floor costing used more by category 4 non vulnerable."},{"metadata":{"_uuid":"b303018a1184e4f4d31bbf6b952f77d883c46876"},"cell_type":"markdown","source":"Now we form pearson correlation heat map of data frame\n"},{"metadata":{"trusted":true,"_uuid":"e16051fce6b6255244d50851eea454f26be358b3"},"cell_type":"code","source":"colormap = plt.cm.RdBu\nall_strings=[i for i in train_data_test.columns if isinstance(train_data_test[i][0], str)]\nprint(all_strings)\nplt.figure(figsize=(14,12))\n#print([i for i in train_data_test.edjefe if i==\"d6c086aa3\"])\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_data_test.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267ec0c0f5d1586f2a86a54fc2b63f7e6487c337"},"cell_type":"markdown","source":"Making pairplots\n"},{"metadata":{"_uuid":"f80baa153ad18d74107b65d5bfc74b8df70499b2","trusted":true},"cell_type":"code","source":"#print(train_data.Target)\n#g = sns.pairplot(train_data[[u'hacdor', u'hhsize', u'r4h1', u'sanitario1', u'SQBage',u'Target']], hue='Target', palette=\"husl\",height=10 ,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10))\ng = sns.pairplot(train_data, vars=[u'hacdor', u'hhsize', u'r4h1', u'sanitario1', u'SQBage'],hue='Target', palette=\"husl\",height=6,diag_kind = 'kde',plot_kws=dict(s=25))\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de12b2ec354e229e033f422e9f6ac6e00eefed7b"},"cell_type":"markdown","source":"**Now we start defining all classifiers and we will also use f1 score to predict accuracy**"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"7ebf18f1ae1e9b6d3eebf61082793e443e646312"},"cell_type":"code","source":"SEED = 0 # for reproducibility\nNFOLDS = 10 # set folds for out-of-fold prediction\nntrain=train_data_test.shape[0]\nkf = KFold(ntrain, n_folds= 10, random_state=SEED)\n\nclass SklearnHelper(BaseEstimator):\n    def __init__(self, clf, params, seed=0):\n        self.clf = clf(params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a04e0bea0c9634dd801163b699a2de2a2c2bdd1"},"cell_type":"markdown","source":"Defining confusion matrix"},{"metadata":{"trusted":true,"_uuid":"71180e6636003c532132b3ca4be7e9edc7fe5d40"},"cell_type":"code","source":"# confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f5997e732e4fa52d1b9d582acee20fe54e4861"},"cell_type":"markdown","source":"Defining params for base classifier"},{"metadata":{"trusted":true,"_uuid":"3dfb423c8b467dfef4abcf95f6288a46d4b329a8"},"cell_type":"code","source":"# Put in our parameters for said classifiers\n# lgbm Forest parameters n_estimators,max_depth,learning_rate\nlgbm_params = {\n    'n_estimators': 5000,\n    'max_depth': -1,\n    'learning_rate': 0.1,\n    'random_state':0\n    #'objective':'multiclass',\n    #'metric':'None',\n    #'class_weight':'balanced',\n    #'colsample_bytree':0.89,\n    #'min_child_samples':30,\n    #'num_leaves':32,\n    #'subsample':0.96\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Stratified k fold parameters \nskf_params = {\n    \n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a228899b5b5089adcdc188d339208ad28e2452f"},"cell_type":"markdown","source":"splitting data"},{"metadata":{"trusted":true,"_uuid":"e1048031fa44b0bfe867394893bc541997c0e32e"},"cell_type":"code","source":"train_set, test_set = train_test_split (train_data_test, test_size = 0.1, random_state = 42)\ntrain_set_y=train_set.Target\ntrain_set_x=train_set.drop(['Target'],axis=1)\ntest_set_y=test_set.Target\ntest_set_x=test_set.drop(['Target'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ecd14994410c775db2c37ddd7cc8e556404a941"},"cell_type":"code","source":"lgbm=LGBMClassifier(random_state=0)\n\n#lgbm = SklearnHelper(clf=LGBMClassifier,params=lgbm_params)\n#et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n#ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n#gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n#skf = SklearnHelper(clf=StratifiedKFold, seed=SEED, params=skf_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43add1471c4cb228aff000d4e2698b5a729fe937"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77d6b052268deb363300baab2e41f5df64ce8dc3"},"cell_type":"code","source":"ntrain=train_data_test.shape[0]\nntest=test_data.shape[0]\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train.iloc[train_index]\n        y_tr = y_train.iloc[train_index]\n        x_te = x_train.iloc[test_index]# x_te has both test X and test y\n        y_te = y_train.iloc[test_index]\n        #print(i,oof_train[test_index])\n        clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        \n        #print(oof_train[test_index],test_index)\n        print(i)\n        oof_test_skf[i, :] = clf.predict(x_test)\n        print(i,oof_test_skf[i,:])\n        \n    print(oof_test_skf)\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    print(oof_test)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bccb68ef12ced3bff8ef081c5fe1087f98749ce"},"cell_type":"markdown","source":"Hyper parameters tuning for classifiers"},{"metadata":{"trusted":true,"_uuid":"974fdd7b38747df6f1ae867fafc668b5d87bb314","scrolled":true},"cell_type":"code","source":"'''\nprint(\"lgbm classifier\")\nprint(\"current parameters for lgbm\")\nprint(lgbm.get_params().keys())\n\nlgbm_params_gs={\n    'n_estimators': [10,100,1000],\n    'max_depth':[5,10,15],\n    'learning_rate':[0.1,0.2,0.3,0.4,0.5]\n\n\nCV_rnd_cfl = GridSearchCV(estimator = lgbm, param_grid = lgbm_params_gs, scoring= 'f1_macro', verbose = 0, n_jobs = -1)\nCV_rnd_cfl.fit(train_set_x, train_set_y)\n\nbest_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)\n\n#lgbm_oof_train, lgbm_oof_test = get_oof(lgbm,X_train, y_train, test_data)\n#plt.show()\n#et_oof_train, et_oof_test = get_oof(et,X_train, y_train, test_data)\n#ada_oof_train, ada_oof_test = get_oof(ada,X_train, y_train, test_data)\n#gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, test_data)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef33302a5d540a6c4ea3734f729338561941a8c6"},"cell_type":"markdown","source":"We get {'learning_rate': 0.3, 'max_depth': 10, 'n_estimators': 1000}\nas best parameters next we are gonna implement these in our lgb"},{"metadata":{"trusted":true,"_uuid":"d8210ffa113c4a405cfd9469dc9fdddfac5704b6"},"cell_type":"code","source":"lgbm=LGBMClassifier(random_state=0,learning_rate=0.3,max_depth=10,n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8627b0a07f21f17895276fb5e7b41f59b294c3c"},"cell_type":"markdown","source":"Confusion matrix and f1 scoring of lgbm classifier"},{"metadata":{"trusted":true,"_uuid":"0ebb4763c59884981e9e590cb131b6644e7a46bf"},"cell_type":"code","source":"lgbm.fit(train_set_x,train_set_y)\ny_pred=lgbm.predict(test_set_x)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(test_set_y, y_pred)\nclass_names = [1,2,3,4]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes = class_names, \n                      title = 'LGBM Confusion matrix')\nplt.savefig('2.lgbm_confusion_matrix.png')\nplt.show()\nf1_score(test_set_y,y_pred, labels=None, pos_label=1, average= 'macro', sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c79309b23fad58b8809acf94a547921cb83f7802"},"cell_type":"markdown","source":"Doing the same process with extra tree classifier to get best hyper parameters"},{"metadata":{"trusted":true,"_uuid":"bc4f20658bc21d7a33015330614c36a899462f3b"},"cell_type":"code","source":"et=ExtraTreesClassifier(random_state=0,n_jobs=-1,min_samples_leaf= 2)\nprint(et)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b67db9564536f08555eb1d2d5846ed529991fc21"},"cell_type":"code","source":"'''print(\"extra tree classifier\")\nprint(\"current parameters for e-tree\")\nprint(et.get_params().keys())\n\net_params_gs={\n    'n_estimators': [10,100,1000,5000],\n    'max_depth':[5,10],\n    }\n\nCV_rnd_cfl = GridSearchCV(estimator = et, param_grid = et_params_gs, scoring= 'f1_macro', verbose = 0, n_jobs = -1)\nCV_rnd_cfl.fit(train_set_x, train_set_y)\n\nbest_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46b63fa134862c583a52c909ed218dda53358913"},"cell_type":"markdown","source":"we get The best parameters for using this model is {'max_depth': 10, 'n_estimators': 10} applying them"},{"metadata":{"trusted":true,"_uuid":"cdeeff8f8e2f8b0c08c6c3140f598fd157a5d325"},"cell_type":"code","source":"et=ExtraTreesClassifier(random_state=0,n_jobs=-1,min_samples_leaf= 2,max_depth=10,n_estimators=10)\nprint(et)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3607eaa9ae22289db12e5d1baa250552d60f61d5"},"cell_type":"markdown","source":"Confusion matrix and f1 scoring for extra trees"},{"metadata":{"trusted":true,"_uuid":"816d91227e0c7aa2e93ec16a76b5ed152bce4eb3"},"cell_type":"code","source":"et.fit(train_set_x,train_set_y)\ny_pred=et.predict(test_set_x)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(test_set_y, y_pred)\nclass_names = [1,2,3,4]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes = class_names, \n                      title = 'Extra tree Confusion matrix')\nplt.savefig('2.et_confusion_matrix.png')\nplt.show()\nf1_score(test_set_y,y_pred, labels=None, pos_label=1, average= 'macro', sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c954d4e1c5455b951d7e37eabcc4bd3f0214482"},"cell_type":"markdown","source":"Seems like it doesnt do well we wont use it"},{"metadata":{"_uuid":"0fd759651c83ca2e9cded0649cd43590b293e974"},"cell_type":"markdown","source":"Trying with gradient boosting  same steps as above"},{"metadata":{"trusted":true,"_uuid":"e17f2c350a07a4476eea2da04d34b8273b8bcffd"},"cell_type":"code","source":"gbc=GradientBoostingClassifier(random_state=0)\nprint(gbc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0b1e6ab04fc957866f9b8e4cfd40a2a50ff2d62d"},"cell_type":"code","source":"'''\nprint(\"gradient boost classifier\")\n\n\ngb_params_gs={\n    'n_estimators':[500,700,1000,1500],\n    'max_depth': [2,3,5]\n}\nprint(\"start\")\nCV_rnd_cfl = GridSearchCV(estimator = gbc, param_grid = gb_params_gs, scoring= 'f1_macro', verbose = 0, n_jobs = -1)\nprint(\"mid\")\nCV_rnd_cfl.fit(train_set_x, train_set_y)\nprint(\"end\")\nbes\nt_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d24c7f6f17f2bf65b26a2803ac623f7ee5b431f"},"cell_type":"markdown","source":"We get The best parameters for using this model is {'max_depth': 5, 'n_estimators': 1500} applying"},{"metadata":{"trusted":true,"_uuid":"b390a1712b951ce1a1cff7c9b557ab28f82ac6f3"},"cell_type":"code","source":"gbc=GradientBoostingClassifier(random_state=0,max_depth=5,n_estimators=1500)\nprint(gbc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865ecddbcaad1c6be16da0f20df9a7b4a56065c0"},"cell_type":"code","source":"'''\ngbc.fit(train_set_x,train_set_y)\ny_pred=gbc.predict(test_set_x)\n# Confusion maxtrix & metrics\ncm = confusion_matrix(test_set_y, y_pred)\nclass_names = [1,2,3,4]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes = class_names, \n                      title = 'gbc boost Confusion matrix')\nplt.savefig('2.gbc_confusion_matrix.png')\nplt.show()\nf1_score(test_set_y,y_pred, labels=None, pos_label=1, average= 'macro', sample_weight=None)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43beddc071909f79a6b3efbece5ba91081809b8f"},"cell_type":"markdown","source":"That is a 0.93 accuracy score so keeping it."},{"metadata":{"_uuid":"c1453fd589042245525b888398eaba74873c8710"},"cell_type":"markdown","source":"We get The best parameters for using this model is The best parameters for using this model is {'max_depth': 200, 'max_leaf_nodes': 1000}"},{"metadata":{"_uuid":"85b751737278f4ba342f7fb3e980b1f56ea552cb","trusted":true},"cell_type":"code","source":"y_train=train_data_test.Target\nX_train=train_data_test.drop([\"Target\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33faf3de19c19d27bbb4239b6e78897f1e8194ad"},"cell_type":"code","source":"lgbm_oof_train, lgbm_oof_test = get_oof(lgbm,X_train, y_train, test_data)\n#plt.show()\ngbc_oof_train, et_oof_test = get_oof(gbc,X_train, y_train, test_data)\n#ada_oof_train, ada_oof_test = get_oof(ada,X_train, y_train, test_data)\n#gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04a23a9ca1ba0534e04d635c5cfe9164d8045b2d"},"cell_type":"code","source":"base_models=pd.DataFrame({'lgbm':lgbm_oof_train.ravel(),\n                          'Gradient_boost':gbc_oof_train.ravel()})\nbase_models.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"266f6309028d33bd8315119b9be75d928ab2d1d2"},"cell_type":"markdown","source":"generating a heatmap to check models correlation"},{"metadata":{"trusted":true,"_uuid":"07f0cb80348284761894d63b164fe588840dfefb"},"cell_type":"code","source":"plt.title('Pearson Correlation of Classifiers', y=1.05, size=15)\nsns.heatmap(base_models.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True,annot=True, cmap=colormap, linecolor='white')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"297fe4279a9d355f780debd7eda728e265c48d36"},"cell_type":"markdown","source":"Now we concatinate train and test data from the abov three models"},{"metadata":{"trusted":true,"_uuid":"ccb69cbdda18271600befa7e1c40cec1b71deb9e"},"cell_type":"code","source":"x_train = np.concatenate(( lgbm_oof_train, gbc_oof_train), axis=1)\nx_test = np.concatenate(( lgbm_oof_test, et_oof_test), axis=1)\nprint(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"382dfb02b25be50a2260833103e2866157ef3b58"},"cell_type":"markdown","source":"We start with scond level learning model and we use XGBoost Classifier"},{"metadata":{"trusted":true,"_uuid":"fe09b9ce3a5eba508c76a32dc33fb7d0322ad0cb"},"cell_type":"code","source":"gbmn = LGBMClassifier(n_estimators= 2000)\nprint(gbmn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43e2380bd9011ea8306e2a7d3190749b901e01b9"},"cell_type":"code","source":"print(\"gradient boost classifier\")\n\n\ngbmn_params_gs={\n    'n_estimators': [10,100,1000],\n    'max_depth':[5,10,15],\n    'learning_rate':[0.1,0.2,0.3,0.4,0.5]}\nprint(\"start\")\nCV_rnd_cfl = GridSearchCV(estimator = gbmn, param_grid = gbmn_params_gs, scoring= 'f1_macro', verbose = 0, n_jobs = -1)\nprint(\"mid\")\nCV_rnd_cfl.fit(x_train, y_train)\nprint(\"end\")\n\nbest_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87c91b9d17b9b87fb163cb2311cca0c04009235d"},"cell_type":"markdown","source":"The best parameters for using this model is {'max_depth': 2, 'n_estimators': 500}"},{"metadata":{"trusted":true,"_uuid":"7e6df6223f5ff9f9d25a0edd0f51e51fd77c37a6","scrolled":false},"cell_type":"code","source":"gbmn = LGBMClassifier(n_estimators= 100,max_depth=10,learning_rate=0.1)\nprint(gbmn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bab5118c2bec3ea5e887a58fdc51a80e5ecd67b"},"cell_type":"code","source":"gbmn.fit(x_train, y_train)\npredictions = gbmn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc983155c43857dfe0dea2e7db400f57d48fdace"},"cell_type":"code","source":"print(predictions)\nprint(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d9e2cdf8c9ce53374e14659b96e02b5c98ca45"},"cell_type":"markdown","source":"Putting predictions to submission files"},{"metadata":{"trusted":true,"_uuid":"9dded8f44cbbb8618d81b4d397b1aff8b43e4cdb"},"cell_type":"code","source":"df_ = pd.DataFrame(columns=['Id','Target'])\ndf_['Id']=data_test_cpy['Id']\ndf_['Target']=predictions\ndf_.to_csv('submission7.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5b21e21f3523f8151ded1f6aa55d8cda8fc56da"},"cell_type":"code","source":"print(( lgbm_oof_train, gbc_oof_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e9a2b3828d182f72bccbe8a392eac8c06a31af9"},"cell_type":"code","source":"print( lgbm_oof_test, et_oof_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7d05896b0a9d437721bd6f2caf1b71d1c311b33"},"cell_type":"code","source":"print(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d8b75f26b576cf2ac745c4abbc17869ebd2c25c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}