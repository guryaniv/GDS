{"cells": [{"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "0313038422ce5108783b4fd7954a8be3949660ba", "_cell_guid": "63e1ea3c-972f-4d04-88a2-12a2c216c376"}, "source": "This notebook goes through a simple process of extracting usable columns, append decomposition components to the data set, generating a few basic features, building a model and then make predictions.  \n\nI am using the TPOT package to create a pipeline. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.  I thought it is useful to add this to the list of kernels available in this competition."}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "295491e395dfb448a81973c93717dac238ba5983", "_cell_guid": "1028a36e-e1b2-49ab-8adf-6740b7581a49"}, "source": "Objective:\n----------\n\nThis data set contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing.\n\nLet us first import the necessary modules."}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"trusted": false, "_execution_state": "idle", "_uuid": "4646ce99e2ca315ac72c5f0da2f46b77267dc316", "_cell_guid": "38d489d2-a24a-4d67-a946-32b60092c402"}, "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\ncolor = sns.color_palette()\n\n%matplotlib inline"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "ef57b4640393945a29f78ba1540020b99b1b8ef8", "_cell_guid": "e869644c-f670-4ca6-9850-a59caf9a5f3f"}, "source": "# Load the data\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "13d70999eb777f2979f77c479d89cf9c01e2c92c", "_cell_guid": "1d2cc31d-6579-4e72-b835-f4dfafb2fe28"}, "source": "train_df.head()"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "2b50fa0af26c1cc520d2572dddcc2b4a6d6478db", "_cell_guid": "c12df355-559c-465b-bd3b-46b573c7b90e"}, "source": "test_df.head()"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "52bdfa92197d28ff927611c2c0c0ca3bc29b46bf", "_cell_guid": "25a94fa1-1032-4743-8040-08583bd7eb16"}, "source": "# Do label encoding\nfor c in train_df.columns:\n    if train_df[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train_df[c].values) + list(test_df[c].values))\n        train_df[c] = lbl.transform(list(train_df[c].values))\n        test_df[c] = lbl.transform(list(test_df[c].values))\n"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "b9a55c03216aaf9edb4e1b50da5a1ee5b38075c8", "_cell_guid": "014d63c9-2371-4f3e-8738-41abd5d13e3f"}, "source": "#n_comp = 12\nn_comp = 20\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_df.drop([\"y\"], axis=1))\ntsvd_results_test = tsvd.transform(test_df)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train_df.drop([\"y\"], axis=1))\npca2_results_test = pca.transform(test_df)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train_df.drop([\"y\"], axis=1))\nica2_results_test = ica.transform(test_df)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_df.drop([\"y\"], axis=1))\ngrp_results_test = grp.transform(test_df)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_df.drop([\"y\"], axis=1))\nsrp_results_test = srp.transform(test_df)\n"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "a947b3ff516dbf812c4e133e2910286784ee4b30", "_cell_guid": "6dc158c3-fb8b-448a-a6a4-28c55adb5d33"}, "source": "usable_columns = list(set(train_df.columns) - set(['y']))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    train_df['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test_df['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train_df['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test_df['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train_df['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test_df['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train_df['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test_df['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train_df['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test_df['srp_' + str(i)] = srp_results_test[:, i - 1]"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "1292a41ef538ecf9aed50bff22d69f24f0a273b9", "_cell_guid": "608feaea-ddd7-4cd4-8c79-581603308347"}, "source": "y_train = train_df['y'].values\ny_mean = np.mean(y_train)\nid_test = test_df['ID'].values\nfinaltrainset = train_df[usable_columns].values\nfinaltestset = test_df[usable_columns].values"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "c8112eea3bfcdff4dc14d11b1d487cee475fb8d9", "_cell_guid": "04eff464-1233-45a9-91e7-a6f4ce1dab75"}, "source": "Train a simple classifier\n----------\n\nWe use the TPOT package to handle the cross validation and hyperparameters for us"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "61af659060de30aa0790be90e1fb134d64f42ba3", "_cell_guid": "df3cf0ee-db25-49ee-9366-370526875711"}, "source": "from tpot import TPOTRegressor\nauto_classifier = TPOTRegressor(generations=2, population_size=6, verbosity=2)\nfrom sklearn.model_selection import train_test_split"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "f81f13f1e37c82099e4adae40220f64592f3929f", "_cell_guid": "7d0cbadf-3ea2-4b38-ba35-14bcf82d9d49"}, "source": "# Split training data to train and validate\nX_train, X_valid, y_train, y_valid = train_test_split(finaltrainset, y_train,\n                                                    train_size=0.75, test_size=0.25)"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "ff412c0701477fd7e4c4763a81ed39a24f85d016", "_cell_guid": "68fc184b-f172-4936-b260-c56ca1debd08"}, "source": "auto_classifier.fit(X_train, y_train)"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "e4af53268a278eddfac16ea8024a9afe6bc1819d", "_cell_guid": "fa4e1781-98c6-4139-b0d0-8421057ed761"}, "source": "print(\"The cross-validation MSE\")\nprint(auto_classifier.score(X_valid, y_valid))"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "94cac4a4c5ac2072a5cfe0ea78dbe866c7fae616", "_cell_guid": "e8a18fe4-590a-4865-9cef-7a17fa575c7d"}, "source": "# we need access to the pipeline to get the probabilities\ntest_result = auto_classifier.predict(finaltestset)\nsub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = test_result\n\nsub.to_csv('MB_TpotModels.csv', index=False)\n\n\nsub.head()"}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "0fd7e528fb935e1250405f6c708c2d2d484e3f7c", "_cell_guid": "376a0192-e485-4564-91a7-0331a9505be4"}, "source": "auto_classifier.export('tpot_pipeline.py')"}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "9f3304f93538f2779934eed09b0ae57b8ff8c19e", "_cell_guid": "aa9324f9-bbaf-4d3c-bae6-ba766b779e4b"}, "source": "That is it for now. You can run locally with more number of generations, population, etc. to get a better result. Because of Kaggle time limitations I could not choose parameters that take longer to run.\nI hope you like it. If so please up-vote."}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 0}