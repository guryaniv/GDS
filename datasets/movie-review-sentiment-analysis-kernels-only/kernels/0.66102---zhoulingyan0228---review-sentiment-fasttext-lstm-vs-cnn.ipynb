{"cells":[{"metadata":{"_uuid":"ce1c5cf09681da9f78234b7b502de579390501f5"},"cell_type":"markdown","source":"# FastText / LSTM / CNN Comparison\n\n## Contents\n\n1.  FastText\n1.  LSTM\n1.  CNN\n1.  Bagging of the above"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as n\nimport pandas as pd\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport keras\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.models import *\nfrom keras.callbacks import *\nfrom keras.activations import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import WordPunctTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep='\\t')\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep='\\t')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8356f520cdcf20a8a709b212fd2d0cc8c662b569"},"cell_type":"code","source":"def get_preprocessing_func():\n    tokenizer = WordPunctTokenizer()\n    lemmatizer = WordNetLemmatizer()\n    def preprocessing_func(sent):\n        return [lemmatizer.lemmatize(w) for w in tokenizer.tokenize(sent)]\n    return preprocessing_func\n\nX = train['Phrase'].apply(get_preprocessing_func()).values\ny = train['Sentiment'].values\nX_test = test['Phrase'].apply(get_preprocessing_func()).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ba1050071fa34bd34dd3995f111be7de7e1d315"},"cell_type":"code","source":"def prepare_tokenizer_and_weights(X):\n    tokenizer = Tokenizer(filters='')\n    tokenizer.fit_on_texts(X)\n    \n    weights = np.zeros((len(tokenizer.word_index)+1, 300))\n    with open(\"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\") as f:\n        next(f)\n        for l in f:\n            w = l.split(' ')\n            if w[0] in tokenizer.word_index:\n                weights[tokenizer.word_index[w[0]]] = np.array([float(x) for x in w[1:301]])\n    return tokenizer, weights","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"tokenizer, weights = prepare_tokenizer_and_weights(np.append(X, X_test))\nX_seq = tokenizer.texts_to_sequences(X)\nMAX_LEN = max(map(lambda x: len(x), X_seq))\nX_seq = pad_sequences(X_seq, MAX_LEN)\nMAX_ID = len(tokenizer.word_index)\nprint('MAX_LEN=', MAX_LEN)\nprint('MAX_ID=', MAX_ID)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bdcd846a53f6f83d5dcd2d26992c70af7c991ac"},"cell_type":"markdown","source":"## FastText Model\n\nFastText is simple and fast, and sometimes can achieve state-of-art performance."},{"metadata":{"trusted":true,"_uuid":"ae8da0d6b81c090f7ca9442e4175155f6e752e4d"},"cell_type":"code","source":"def make_fast_text():\n    fast_text = Sequential()\n    fast_text.add(InputLayer((MAX_LEN,))) \n    fast_text.add(Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True))\n    fast_text.add(SpatialDropout1D(0.5))\n    fast_text.add(GlobalMaxPooling1D())\n    fast_text.add(Dropout(0.5))\n    fast_text.add(Dense(5,activation='softmax'))\n    return fast_text\n\nfast_texts = [make_fast_text() for i in range(3)]\nfast_texts[0].summary()\n\nfor fast_text in fast_texts:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    fast_text.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    fast_text.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"353d2f044cfb702be3f6bd8b7b933e67de58801e"},"cell_type":"markdown","source":"## LSTM Model with Pretrained Embedding\n\nLSTM is the top performer on this problem, but is slow to train."},{"metadata":{"trusted":true,"_uuid":"5da70dcb0fbd8985f4ba1941b70b760781f25a0c"},"cell_type":"code","source":"def make_model_lstm():\n    model_lstm = Sequential()\n    model_lstm.add(InputLayer((MAX_LEN,))) \n    model_lstm.add(Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True))\n    model_lstm.add(SpatialDropout1D(0.5))\n    model_lstm.add(Bidirectional(CuDNNLSTM(300, return_sequences=True)))\n    model_lstm.add(BatchNormalization())\n    model_lstm.add(SpatialDropout1D(0.5))\n    model_lstm.add(Bidirectional(CuDNNLSTM(300)))\n    model_lstm.add(BatchNormalization())\n    model_lstm.add(Dropout(0.5))\n    model_lstm.add(Dense(5,activation='softmax'))\n    return model_lstm\n\nmodel_lstms = [make_model_lstm() for i in range(2)]\nmodel_lstms[0].summary()\n\nfor model_lstm in model_lstms:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_lstm.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=1, verbose=0)],\n                 epochs=30, \n                 verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8151d6c7273c4d944f6b3dfcbe1c858a35a9b979"},"cell_type":"markdown","source":"## CNN Model with Pretrained Embedding\n\nCNN's are fast to train and performs second to LSTM."},{"metadata":{"trusted":true,"_uuid":"1a38c19c0fcec0db432e848ebfa867821d886156"},"cell_type":"code","source":"def make_model_cnn():\n    inputs = Input((MAX_LEN,))\n    x = Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True)(inputs)\n    x = SpatialDropout1D(0.5)(x)\n    x = Conv1D(300, kernel_size=5,activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=5,activation='relu')(x)\n    x = GlobalMaxPooling1D()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(5,activation='softmax')(x)\n    model_cnn = Model(inputs, outputs)\n    return model_cnn\n\nmodel_cnns = [make_model_cnn() for i in range(3)]\nmodel_cnns[0].summary()\n\nfor model_cnn in model_cnns:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_cnn.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ba4969f950e5450100b6edf0758b80fd6ca6d91"},"cell_type":"markdown","source":"## Attention-Based CNN with Pretrained Embedding\n\nRef: [Keras Attenton Mechanism](https://github.com/philipperemy/keras-attention-mechanism)\n[ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs](http://www.aclweb.org/anthology/Q16-1019)"},{"metadata":{"trusted":true,"_uuid":"e04ed4f371bca0c777a16d25cf780e7ff2d4c1ee"},"cell_type":"code","source":"def make_model_abcnn():\n    def attention_layer(l):\n        x = Permute((2,1))(l)\n        x = Dense(K.int_shape(x)[2], activation='sigmoid')(x)\n        x = Permute((2,1))(x)\n        return multiply([x, l])\n    inputs = Input((MAX_LEN,))\n    x = Embedding(input_dim=MAX_ID+1, output_dim=300, weights=[weights], trainable=True)(inputs)\n    x = SpatialDropout1D(0.5)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = SpatialDropout1D(0.5)(x)\n    x = MaxPooling1D(pool_size=2, strides=2)(x)\n    x = Conv1D(300, kernel_size=3,activation='relu')(x)\n    x = attention_layer(x)\n    x = GlobalMaxPooling1D()(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(5,activation='softmax')(x)\n    model_cnn = Model(inputs, outputs)\n    return model_cnn\n\nmodel_abcnns = [make_model_abcnn() for i in range(3)]\nmodel_abcnns[0].summary()\n\nfor model_abcnn in model_abcnns:\n    X_seq_train, X_seq_valid, y_train, y_valid = train_test_split(X_seq, y, test_size=0.1)\n    model_abcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    model_abcnn.fit(X_seq_train, y_train, validation_data=(X_seq_valid, y_valid),\n                 callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=0)],\n                 epochs=30, \n                 verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99c6fc9f056a80f31fe31b4e4b3a2681237493b1"},"cell_type":"markdown","source":"## Bagging of ABCNN Model\n\nFinally, let's ensemble them."},{"metadata":{"trusted":true,"_uuid":"1ac59501db462408ea5567781a000e98b57dd5e1","collapsed":true},"cell_type":"code","source":"def make_model_bagged(models):\n    inputs = Input((MAX_LEN,))\n    outputs = average([model(inputs) for model in models])\n    return Model(inputs, outputs)\nmodel_bagged = make_model_bagged(model_abcnns)\nmodel_bagged.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\ny_prob = model_bagged.predict(X_seq)\ny_predict = np.argmax(y_prob, axis=1)\nprint(classification_report(y, y_predict))\nsns.heatmap(confusion_matrix(y, y_predict));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee4e6eeee4ae28ad53da0fad383668928852a161","collapsed":true},"cell_type":"code","source":"X_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_seq = pad_sequences(X_test_seq, MAX_LEN)\ny_test_prob = model_bagged.predict(X_test_seq)\ny_test_predict = np.argmax(y_test_prob, axis=1)\nout_df = test[['PhraseId']]\nout_df['Sentiment'] = y_test_predict\nout_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}