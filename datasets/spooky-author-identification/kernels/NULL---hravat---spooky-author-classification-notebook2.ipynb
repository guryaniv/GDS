{"cells": [{"source": ["##import necessary packages\n", "#####\n", "\n", "import numpy as np # linear algebra\n", "import scipy as sp \n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib\n", "import seaborn as sns \n", "import nltk\n", "from nltk.corpus import stopwords\n", "import wordcloud\n", "from wordcloud import WordCloud, STOPWORDS\n", "from matplotlib import pyplot as plt\n", "\n", "##read in the data file an display \n", "df_spooky_author=pd.read_csv('../input/train.csv')\n", "df_spooky_author\n", "\n", "sentence_list=df_spooky_author['text'].values.tolist()\n", "author_list=df_spooky_author['author'].values.tolist()\n", "id_list=df_spooky_author['id'].values.tolist()\n", "sentence_list\n", "author_list\n", "\n", "\n", "combined_list=[list(author_list) for author_list in zip(author_list, sentence_list)]\n", "\n", "tokenized_list=[]\n", "\n", "for authors,sentence in combined_list:\n", "    tokenized_list.append([authors,nltk.word_tokenize(sentence)])\n", "\n", "tokenized_list\n", "\n", "##make individual authorwise list of words after removing stop words.\n", "##This is to prepare the data for wordcloud\n", "stop = set(stopwords.words('english'))    \n", "\n", "tokenized_stop_words_list=[]\n", "\n", "\n", "for author,sentence in tokenized_list:\n", "    tokenized_stop_words_list_temp=[]   \n", "    for word in sentence:\n", "        if not word in stop:\n", "            tokenized_stop_words_list_temp.append(word)\n", "    tokenized_stop_words_list.append([author,tokenized_stop_words_list_temp])        \n", "            \n", "tokenized_stop_words_list\n", "\n", "tokenized_words_EAP=[]\n", "tokenized_words_MWS=[]\n", "tokenized_words_HPL=[]\n", "\n", "for author,sentence in tokenized_stop_words_list:\n", "    if author=='EAP':\n", "        for words in sentence:\n", "            tokenized_words_EAP.append(words)\n", "    if author=='MWS':\n", "        for words in sentence:\n", "            tokenized_words_MWS.append(words)\n", "    if author=='HPL':\n", "        for words in sentence:\n", "            tokenized_words_HPL.append(words)\n", "            \n", "##word cloud for HPLovenCraft            \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_HPL))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for HP Lovencraft\n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "\n", "\n", "##word cloud for Edgar Allen Poe         \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_EAP))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for Edgar Allen Poe\n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "\n", "\n", "##word cloud for Mary Shelley            \n", "plt.figure(figsize=(30,30))\n", "wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=stop, max_font_size= 40)\n", "wc.generate(\" \".join(tokenized_words_HPL))\n", "##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n", "##Uncomment below line and run to see wordcloud for Mary Shelley \n", "##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n", "plt.axis('off')\n", "\n", "df_test=pd.DataFrame(tokenized_words_HPL)\n", "df_test[0].value_counts()\n", "\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import preprocessing  \n", "from sklearn import linear_model\n", "from sklearn import metrics\n", "from sklearn.metrics import accuracy_score\n", "from nltk.stem import WordNetLemmatizer\n", "\n", "## TF-IDF implementation\n", "count_vect = CountVectorizer()\n", "wordnet_lemmatizer = WordNetLemmatizer()\n", "\n", "sentence_new=''\n", "sentence_new_list=[]\n", "sentence_list_lemmatized=[]\n", "\n", "for sentence in sentence_list:\n", "    for words in nltk.word_tokenize(sentence):\n", "        sentence_new=sentence_new+wordnet_lemmatizer.lemmatize(words)\n", "    sentence_list_lemmatized.append(sentence_new)\n", "    sentence_new=''  \n", "    \n", " \n", "sentence_train_counts=count_vect.fit_transform(sentence_list_lemmatized)\n", "sentence_train_counts.shape\n", "\n", "tfidf_transformer = TfidfTransformer()\n", "sentence_train_tfidf = tfidf_transformer.fit_transform(sentence_train_counts)\n", "sentence_train_tfidf.shape\n", "\n", "##Label Encoder to encode values\n", "le = preprocessing.LabelEncoder()\n", "le.fit(author_list)\n", "\n", "list(le.classes_)\n", "\n", "author_list_encoded=le.transform(author_list)\n", "author_list_encoded\n", "\n", "X_scale_train,X_scale_test,Y_scale_train,Y_scale_test = train_test_split(sentence_train_tfidf,author_list_encoded,test_size=0.3,random_state=78)\n", "\n", "C_list=[0.0001,0.0003,0.001,0.003,0.01,0.01,0.1,0.3,1,3]\n", "accuracy=0\n", "c_final=0\n", "accuracy_1=0\n", "\n", "for c in C_list:\n", "    mul_lr = linear_model.LogisticRegression(penalty='l2',multi_class='multinomial', solver='newton-cg',C=c)\n", "    mul_lr.fit(X_scale_train,Y_scale_train)    \n", "    accuracy_1=accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test)) \n", "    accuracy_1\n", "    if  accuracy_1 > accuracy:  \n", "        accuracy=accuracy_1                     \n", "        c_final=c\n", "        ##Uncomment below line and run to see how acuuracy improves with change in value of c\n", "        print ('C is',c,'Accuracy is ',accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test)))\n", "    \n", "\n", "    \n", "mul_lr = linear_model.LogisticRegression(penalty='l2',multi_class='multinomial', solver='newton-cg',C=c)\n", "mul_lr.fit(X_scale_train,Y_scale_train)    \n", "accuracy_1=accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test))     \n", "accuracy_1\n", "\n", "\n", "y_predict=mul_lr.predict(X_scale_test)\n", "y_predict_author=le.inverse_transform(y_predict)\n", "##list of predicted authors \n", "y_predict_author\n", "\n", "###Predict Probabilities\n", "mul_lr.predict_proba(X_scale_test)\n", "\n", "########Preparing the data for training predictions\n", "df_spooky_author_test=pd.read_csv('../input/test.csv')\n", "df_spooky_author_test\n", "\n", "sentence_list_test=df_spooky_author_test['text'].values.tolist()\n", "id_list_test=df_spooky_author_test['id'].values.tolist()\n", "\n", "\n", "#####Very important use transorm here instread of fit transofrm\n", "#####Training data has been fitted above hence here we need to use only transform\n", "##### If fit transform is used we get dimensional mismatch while making predictions from the trained model\n", "sentence_test_count=count_vect.transform(sentence_list_test)\n", "sentence_test_count.shape\n", "\n", "sentence_test_tfidf = tfidf_transformer.transform(sentence_test_count)\n", "sentence_test_tfidf.shape\n", "\n", "X_prob_predict=mul_lr.predict_proba(sentence_test_tfidf)\n", "df_X_prob_predict=pd.DataFrame(X_prob_predict)\n", "df_X_prob_predict.columns=['EAP', 'HPL', 'MWS']\n", "\n", "df_X_prob_predict\n", "df_X_ID=pd.DataFrame(id_list_test)\n", "df_X_ID.columns=['id']\n", "df_predict_final=df_X_ID.join(df_X_prob_predict)\n", "df_predict_final\n", "\n", "###Export Data to csv for final submission\n", "df_predict_final.to_csv('hravat_spooky_autor_predict.csv',sep=',')\n"], "outputs": [], "metadata": {"_uuid": "056ff353273e0845a64b92eb2d2322134aa1852a", "_kg_hide-output": true, "_cell_guid": "22b5ecdf-73f3-45dd-b400-780b58b35d1b", "_kg_hide-input": false}, "cell_type": "code", "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}