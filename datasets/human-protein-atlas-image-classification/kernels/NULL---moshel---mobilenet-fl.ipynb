{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nFOCAL_LOSS=True\n\nimport numpy as np # linear algebra\nnp.random.seed(42)\nimport random\nrandom.seed(42)\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb35b5eb961c5463fe4366072b0575aeebb7e192"},"cell_type":"code","source":"def FocalLoss(gamma=2):\n    gamma = float(gamma)\n\n    def compute_loss(target, logit):\n        max_val = tf.clip_by_value(-logit, 0, 1.e9)\n        loss = logit - logit*target + max_val + tf.log(tf.exp(-max_val) + tf.exp(-logit - max_val))\n        invprobs = tf.log_sigmoid(-logit * (target *2.0 - 1.0))\n        loss = tf.exp(invprobs * gamma) * loss\n        loss =  tf.reduce_sum(loss, axis=1)\n        return tf.reduce_mean(loss)\n    return compute_loss\n\n#original implementation:\n#class FocalLoss(nn.Module):\n#    def __init__(self, gamma=2):\n#        super().__init__()\n#        self.gamma = gamma\n#\n#    def forward(self, logit, target):\n#        target = target.float()\n#        max_val = (-logit).clamp(min=0)\n#        loss = logit - logit * target + max_val + \\\n#               ((-max_val).exp() + (-logit - max_val).exp()).log()\n#\n#        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n#        loss = (invprobs * self.gamma).exp() * loss\n#        if len(loss.size())==2:\n#            loss = loss.sum(dim=1)\n#        return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os, sys\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import MobileNet\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Activation, Dense, Multiply, Input, Flatten, Dropout, Conv2D\n#from keras import metrics\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom itertools import chain\nfrom collections import Counter\nimport cv2\nimport random\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nbatch_size = 64\n\n\npath_to_train = '../input/human-protein-atlas-image-classification/train/'\ndata = pd.read_csv('../input/human-protein-atlas-image-classification/train.csv')\n\ndef conv_df2np(df_):\n    train_dataset_info = []\n    for name, labels in zip(df_['Id'], df_['Target'].str.split(' ')):\n        train_dataset_info.append({\n            'path':\n            os.path.join(path_to_train, name),\n            'labels':\n            np.array([int(label) for label in labels])\n        })\n    train_dataset_info = np.array(train_dataset_info)\n    return train_dataset_info\n\ntrain_df_info = conv_df2np(data)\n\n\nclass DataGenerator:\n    def __init__(self):\n        pass\n    \n    def create_train(self, dataset_info, batch_size, shape, augument=True):\n        assert shape[2] == 3\n        while True:\n            random_indexes = np.random.choice(len(dataset_info), batch_size)\n            batch_images = np.empty((batch_size, shape[0], shape[1], shape[2]))\n            batch_labels = np.zeros((batch_size, 28))\n            for i, idx in enumerate(random_indexes):\n                image = self.load_image(\n                    dataset_info[idx]['path'], shape)\n                batch_images[i] = image\n                batch_labels[i][dataset_info[idx]['labels']] = 1\n            yield batch_images, batch_labels\n\n    def load_image(self, path, shape):\n        image_red_ch = cv2.imread(path + '_red.png', cv2.IMREAD_GRAYSCALE)\n        image_green_ch = cv2.imread(path + '_green.png', cv2.IMREAD_GRAYSCALE)\n        image_blue_ch = cv2.imread(path + '_blue.png', cv2.IMREAD_GRAYSCALE)\n\n        image1 = np.stack((image_red_ch, image_green_ch, image_blue_ch), -1)\n        image1 = cv2.resize(image1, (224, 224), interpolation = cv2.INTER_AREA)\n        return image1.astype(np.float)\n\ntrain_datagen = DataGenerator()\nvalid_datagen = DataGenerator()\ndata['target_list'] = data['Target'].map(\n    lambda x: [int(a) for a in x.split(' ')])\nall_labels = list(chain.from_iterable(data['target_list'].values))\nc_val = Counter(all_labels)\nn_keys = c_val.keys()\nmax_idx = max(n_keys)\ndata['target_vec'] = data['target_list'].map(\n    lambda ck: [i in ck for i in range(max_idx + 1)])\nlab_vec = data['target_list'].map(lambda ck: [int(i in ck) for i in range(28)])\n\ntrain_df = pd.read_csv('../input/mobilenetflsplit/train_part.csv')\nvalid_df = pd.read_csv('../input/mobilenetflsplit/valid_part.csv')\n\n\ntrain_dataset_info = conv_df2np(train_df)\nvalid_dataset_info = conv_df2np(valid_df)\nprint(train_dataset_info.shape, valid_dataset_info.shape)\n\n# create train and valid datagens\ntrain_generator = train_datagen.create_train(train_dataset_info, batch_size,\n                                             (224, 224, 3))\nvalidation_generator = valid_datagen.create_train(\n    valid_dataset_info, batch_size, (224, 224, 3), False)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2adb62299ec975308dc64c278753590e85c8fe23"},"cell_type":"code","source":"def create_model(input_shape, n_out):\n    inp_mask = Input(shape=input_shape)\n    pretrain_model_mask = MobileNet(\n        input_shape=(224, 224, 3),  #SWITCH\n        include_top=False,\n#        weights='imagenet',\n        dropout=0.5,\n        pooling='avg')\n    pretrain_model_mask.name = 'mobilenet'\n\n    x = pretrain_model_mask(inp_mask)\n    if FOCAL_LOSS:\n        out = Dense(n_out, activation='linear')(x)\n    else:\n        out = Dense(n_out, activation='sigmoid')(x)\n    model = Model(inputs=inp_mask, outputs=[out])\n\n    return model\n\nkeras.backend.clear_session()\n\nmodel = create_model(input_shape=(224, 224, 3), n_out=28)\n\nif FOCAL_LOSS:\n    model.compile(\n        loss=FocalLoss(gamma=2), optimizer='adam', metrics=['acc'])\nelse:\n    model.compile(\n        loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.summary()\n\n# train model\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=len(train_df) // batch_size,\n    validation_data=validation_generator,\n    validation_steps=len(valid_df) // batch_size,\n    epochs=1,\n    verbose=1,\n    use_multiprocessing=False,\n    shuffle=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}