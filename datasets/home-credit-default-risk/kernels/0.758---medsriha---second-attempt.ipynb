{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcaea29333cffb643cce0c35ff73957d18970c1c"},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\napp_test = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dbb31f8fb98187a30dd84a261964ca7d4c44e66"},"cell_type":"markdown","source":"1. ## Missing Values Overview (Training Data)"},{"metadata":{"trusted":true,"_uuid":"d0214a4a3edd1f9cb3ed1cb6401ba659c5d27ede"},"cell_type":"code","source":"app_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8fa433cd7120212f7bd5f3813960a119211e988","collapsed":true},"cell_type":"code","source":"def missing_values(df):\n    miss = df.isnull().sum()\n    a = (miss/len(df)) * 100\n    frame = pd.DataFrame({'Features':df.columns, 'Missing_percentage':a,'Total_number': miss}).reset_index()\n    frame = frame.drop('index',axis=1)\n    d =frame[frame['Total_number'] > 0].sort_values(by='Missing_percentage',ascending=False).round(1)\n    print('Total Columns Number: %s' %len(df.columns))\n    print('Total Columns having missing values: %s' %len(d))\n    return d\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24d034485048c0378e3ce8e399fff689450d8d95"},"cell_type":"code","source":"missing_values(app_train).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99784959dc6b5208bf327860a0494d8909d9bb55"},"cell_type":"markdown","source":"## Distribution of the Target"},{"metadata":{"trusted":true,"_uuid":"6ac85f1efa9aa49af36e951393ed5ee5514b396b"},"cell_type":"code","source":"app_train['TARGET'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df40a9b64b26994bd79365976e5bb4e47f62b65d"},"cell_type":"markdown","source":"We can see from the above histogram that the TARGET attribute is unbalanced. There is more 0's than 1's. Let see the difference in count."},{"metadata":{"trusted":true,"_uuid":"cc7b81c25dc1623013c87f9ac2d1f13e027a837b"},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36248d6a2d27b97e134455058257f426d99c83c9"},"cell_type":"markdown","source":"There are **282686** cases in which customers did not or could not pay their loan on time, and **24825** customers did. \nLet see who are they."},{"metadata":{"trusted":true,"_uuid":"e8cb8f382ae9a65711f8143b0c84c21c2a956019"},"cell_type":"code","source":"print(app_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a882147f39ffe9807e0f1d22c056f7bcbe1ce302"},"cell_type":"markdown","source":"## Encoding"},{"metadata":{"trusted":true,"_uuid":"900dcf3b1113f9e4443d46d9b791656cb4811fde","collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b6147e7a0106d0afb8d0bde152a5511f25b8a01"},"cell_type":"markdown","source":"### Label Encoding"},{"metadata":{"trusted":true,"_uuid":"ee44ae14bd719ab325539e14a21c2f1adb6e847e","collapsed":true},"cell_type":"code","source":"def label_encoding(df):\n    enc = preprocessing.LabelEncoder()\n    count = 0\n    for col in df:\n        if df[col].dtype == 'object':\n            if len(list(df[col].unique())) <= 2:\n                enc.fit(df[col])\n                df[col] = enc.transform(df[col])\n                count+=1\n                \n                \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77c38549343ed4bb3ec88ae90f4fa8f9e19af98c","collapsed":true},"cell_type":"code","source":"app_train = label_encoding(app_train)\napp_test = label_encoding(app_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe8520d53d578cf3562fd8ff5be63eb77fed5a5e"},"cell_type":"markdown","source":"### Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"c5c33fd2766e503ff6ab4a296da70ca5e66f8613","collapsed":true},"cell_type":"code","source":"def hot_encoding(df):\n    return pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21b7ab12df7dcd5611fb348aa18b7c462e0614e1","collapsed":true},"cell_type":"code","source":"app_train = hot_encoding(app_train)\napp_test = hot_encoding(app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae552cc368d49fabe21eccca6406d96a414d4e6e"},"cell_type":"code","source":"print(app_train.shape)\nprint(app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20c74a02ce38568d3619bc90d6f2a745ae2dd831"},"cell_type":"markdown","source":"### Aligning Training and Testing Data\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a41cdf6a410ce3f6f2122d6fb9b570f1034e892d"},"cell_type":"code","source":"def align_data(df_train, df_test):\n    labels = df_train['TARGET']\n    a,b = df_train.align(df_test, join='inner', axis=1)\n    a['TARGET'] = labels\n    return a,b\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eddda8d564909434b98d1bfe616f570523e9aa8c","collapsed":true},"cell_type":"code","source":"app_train,app_test = align_data(app_train,app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c87e5ccfa76f2780f5e6b3323d996f4a7a638c64"},"cell_type":"code","source":"print(app_train.shape)\nprint(app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a843a70dba546c7b3b9df3db6292f721921be2d"},"cell_type":"markdown","source":"## EDA: Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"5f20f53e189dd8c1abad68bfd1f42b30f6535cff"},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -360).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aff32bf43ee2fd2b7cc193ac19d5a0b5ab4b279"},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\nprint('There are %d anomalies in the train data out of %d entries' % (app_train[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_train)))\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93352db7921f81185ca429a10a2cab7c9f6a367c"},"cell_type":"markdown","source":"### Correlation\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method."},{"metadata":{"trusted":true,"_uuid":"9e46d708b154df375fce266f38a528263717bda4","collapsed":true},"cell_type":"code","source":"correlations = app_train.corr()['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bc5e2c0bd69075c69624ef4fc05be4a16a714a4"},"cell_type":"code","source":"print('Negatively Correlated Variables',correlations.head(20))\nprint('Positively Correlated Variables',correlations.tail(20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71048fd48095603201302c0b8198b0c1281555cf"},"cell_type":"markdown","source":"Let's take a look at some of more significant correlations: the DAYS_BIRTH is the most positive. Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."},{"metadata":{"trusted":true,"_uuid":"ca836456bc793511ada274e47320d722fbba54ec","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4093f8633ca25832bde5b6da4e30659de7f05203"},"cell_type":"code","source":"plt.hist(app_train[app_train['TARGET'] == 1]['DAYS_BIRTH'] / -365,edgecolor='k', bins=30)\nplt.title('Age of Client (Target = 1)'); \nplt.xlabel('Age'); \nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e97135e33936e688f61b50facd0aaf97e8bcce18"},"cell_type":"code","source":"plt.hist(app_train[app_train['TARGET'] == 0]['DAYS_BIRTH'] / -365,edgecolor='k', bins=30)\nplt.title('Age of Client (Target  = 0)'); \nplt.xlabel('Age'); \nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ad2e51407730bcf866cc100391dd3b0b8742082"},"cell_type":"code","source":"seg = app_train[['TARGET','DAYS_BIRTH']]\nseg['YEAR_BIRTH'] = seg['DAYS_BIRTH'] / -360\n\nseg['YEAR_BIN'] = pd.cut(seg['YEAR_BIRTH'], bins = np.linspace(20, 70, num = 11))\nseg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"702abda077583d80774f9704ce1c2b441b0e0011"},"cell_type":"code","source":"seg = seg.groupby('YEAR_BIN').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85b426aa6cb7365eaf2fa6412c355667a0001c9a"},"cell_type":"code","source":"seg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed32453db42919737688d42f6c930a0f099c0bba"},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(seg.index.astype(str), 100 * seg['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2c34262b203ae290d27c6247e4720da1fd48f3f"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"8d7772608a5a33c5eebba1257aa845085f48213d","collapsed":true},"cell_type":"code","source":"#Impute missing values\nimputer = preprocessing.Imputer(strategy='median')\npoly_features_train = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1208e81c3fd4ec6e753e239bce69c030cbd04487","collapsed":true},"cell_type":"code","source":"poly_target = poly_features_train['TARGET']\n\npoly_features_train = poly_features_train.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features_train = imputer.fit_transform(poly_features_train)\npoly_features_test = imputer.transform(poly_features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c38fd0eb2d44af5f3bb954e370486ae0597e7311"},"cell_type":"code","source":"poly_transformer = preprocessing.PolynomialFeatures(degree=3)\npoly_transformer.fit(poly_features_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"995226aa06da36ad887d06bd02e6dd95f2253127"},"cell_type":"code","source":"poly_features_train = poly_transformer.transform(poly_features_train)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd6921f989a777c250481dc6c270aa6633185b0"},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4668a1a084222c8651dbeff1f42c626b95afd71","collapsed":true},"cell_type":"code","source":"#Merge the new features into the train and test dataframe\npoly_train = pd.DataFrame(poly_features_train, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3', 'DAYS_BIRTH']))\npoly_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3', 'DAYS_BIRTH']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcd898a3a24bd2bc1d28d7d201b3a6ed8b470814"},"cell_type":"code","source":"poly_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fdf702577e67ec16cd6818c474e3c7b5bdceac2","collapsed":true},"cell_type":"code","source":"poly_train = poly_train.drop('1',axis=1)\npoly_test = poly_test.drop('1',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4719fd0b22bc0eb9d7aa1448c2b62687665b37f3","collapsed":true},"cell_type":"code","source":"poly_train['SK_ID_CURR'] = app_train['SK_ID_CURR']\npoly_test['SK_ID_CURR'] = app_test['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49d59cf3528905ea308b6c1a1ca2fbdef41f9ae9"},"cell_type":"code","source":"print('Shape poly train',poly_train.shape)\nprint('Shape poly test',poly_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23000a2e4d33b18165bd5b29ec2147c8404d2e70"},"cell_type":"markdown","source":"## Domain Knowledge"},{"metadata":{"trusted":true,"_uuid":"f6cfa98c375db9ec181dc1026a455814e84f5723","collapsed":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fdeeccc099d29223cdc4aca0d5a081d71c7cdad3"},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6be3f6aa9e0506a207bf5fab2252650a545231eb","collapsed":true},"cell_type":"code","source":"app_train_domain_poly = app_train_domain.merge(poly_train,how='left',on='SK_ID_CURR')\napp_test_domain_poly = app_test_domain.merge(poly_test,how='left',on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02020a56a938e8a374812d19944af366e09a7f0d"},"cell_type":"code","source":"print('Shape poly train',app_train_domain_poly.shape)\nprint('Shape poly test',app_test_domain_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ce7e40ad3a8198d3c278afb8020ffa87f24207","collapsed":true},"cell_type":"code","source":"corr = app_train_domain_poly.corr()['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c5cb8cf56dab5ef7cbd7a0ce3eb2a5876333953"},"cell_type":"markdown","source":"## Logistic Regression "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bfe4c7407146f81ccc9d5657780946a9651b727c"},"cell_type":"code","source":"imputer_ = preprocessing.Imputer(strategy='median')\nscale_ = preprocessing.MinMaxScaler(feature_range=(0,1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff925082612d696a4e301ed7661841995d36e361"},"cell_type":"code","source":"#align the two dataset\ntrain, test = align_data(app_train_domain_poly, app_test_domain_poly)\ntarget = train['TARGET']\ntrain = train.drop('TARGET',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ced4df3a145f000789473caa13793b67542e2326"},"cell_type":"code","source":"print('Shape poly train',train.shape)\nprint('Shape poly test',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5834068c14e9771aaea73df8313dea347b85e95"},"cell_type":"code","source":"imputer_.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0344e014fb40c6a8dfd8022a24878d9d30c8dc9e","collapsed":true},"cell_type":"code","source":"train = imputer_.transform(train)\ntest = imputer_.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f040ea45d9146aa11b2188c7de05efd27b9675"},"cell_type":"code","source":"print('Shape poly train',train.shape)\nprint('Shape poly test',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e445cee7b3f8a8aa19786d04d349be67661a91f2"},"cell_type":"code","source":"scale_.fit(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8090e784094629687ee46c360f4e9c70871de81b"},"cell_type":"code","source":"train = scale_.transform(train)\ntest = scale_.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75559d503c4bba844477b16ecd0222420a69feef"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=0.0001)\nlr.fit(train,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1db2a6654db1acb7b106fbab330632c660a7ecf"},"cell_type":"code","source":"prediction_two = lr.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d583be801661bc2cd2efa7913e3c00bfa9199a9c"},"cell_type":"code","source":"submission_two = pd.DataFrame({'SK_ID_CURR':app_test_domain_poly['SK_ID_CURR'],'TARGET':prediction_two})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d63d3d7227227f213aa89b87ddf45184233cd9f"},"cell_type":"markdown","source":"Logistic Regression AUCROC: 0.714"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2378773638e85d9c0658d476bb89b969debdf1e"},"cell_type":"code","source":"def plot_feature_importances(data):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    df = data.copy()\n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a4dde3aaa487af30e1f18aaa359a736fa6f4487","collapsed":true},"cell_type":"markdown","source":"## Light GBoost"},{"metadata":{"trusted":true,"_uuid":"7c733c1eab0b7beabfaad8366110dd02cdb0517f","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model_lgb(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9de5f1c282b8da0bf32a91499797cd419d8d6a97"},"cell_type":"code","source":"# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ebdec5b3de957a4c046b5fe6628c1d9b06688592"},"cell_type":"code","source":"# Preprocess bureau.csv and bureau_balance.csv\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    bureau = pd.read_csv('../input/bureau.csv')\n    bb = pd.read_csv('../input/bureau_balance.csv')\n    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n    for col in bb_cat:\n        bb_aggregations[col] = ['mean']\n    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del bb, bb_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {\n        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n        'DAYS_CREDIT_UPDATE': ['mean'],\n        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean'],\n        'CNT_CREDIT_PROLONG': ['sum'],\n        'MONTHS_BALANCE_MIN': ['min'],\n        'MONTHS_BALANCE_MAX': ['max'],\n        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n    }\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left')\n    del active, active_agg\n    gc.collect()\n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left')\n    del closed, closed_agg, bureau\n    gc.collect()\n    return bureau_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ab721249701e724462c192e42ec17e49a5cade","collapsed":true},"cell_type":"code","source":"bureau = bureau_and_balance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bcea460d54009e63d5f4ea83804cffff83b16a02"},"cell_type":"code","source":"# Preprocess previous_applications.csv\ndef previous_applications(nan_as_category = True):\n    prev = pd.read_csv('../input/previous_application.csv')\n    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n    # Days 365.243 values -> nan\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    # Add feature: value ask / value received percentage\n    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n    # Previous applications numeric features\n    num_aggregations = {\n        'AMT_ANNUITY': ['min', 'max', 'mean'],\n        'AMT_APPLICATION': ['min', 'max', 'mean'],\n        'AMT_CREDIT': ['min', 'max', 'mean'],\n        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n        'DAYS_DECISION': ['min', 'max', 'mean'],\n        'CNT_PAYMENT': ['mean', 'sum'],\n    }\n    # Previous applications categorical features\n    cat_aggregations = {}\n    for cat in cat_cols:\n        cat_aggregations[cat] = ['mean']\n    \n    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left')\n    del refused, refused_agg, approved, approved_agg, prev\n    gc.collect()\n    return prev_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d0f04088ac079452670e8a81a078c9c87bc6e08","collapsed":true},"cell_type":"code","source":"previous_app = previous_applications()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6062e60b1c2a36d8155d962fcf9a8dd34f26ab49"},"cell_type":"code","source":"# Preprocess POS_CASH_balance.csv\ndef pos_cash(nan_as_category = True):\n    pos = pd.read_csv('../input/POS_CASH_balance.csv')\n    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n    # Features\n    aggregations = {\n        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n        'SK_DPD': ['max', 'mean'],\n        'SK_DPD_DEF': ['max', 'mean']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    \n    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n    del pos\n    gc.collect()\n    return pos_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a378bcc4cc5a847b2d910eaf9dff95782d8ef12a"},"cell_type":"code","source":"cash = pos_cash()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5eaf8a79387d20561c46275037389074c05bbe1"},"cell_type":"code","source":"# Preprocess installments_payments.csv\ndef installments_payments(nan_as_category = True):\n    ins = pd.read_csv('../input/installments_payments.csv')\n    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n    # Percentage and difference paid in each installment (amount paid and installment value)\n    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n    # Days past due and days before due (no negative values)\n    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n    # Features: Perform aggregations\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n    del ins\n    gc.collect()\n    return ins_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"71925b5a2bddbd1d095bf971bdf62c9851e4d349"},"cell_type":"code","source":"ins_payments = installments_payments()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8c1e62eaaa335fb15b6f04428a1689bb2f837271"},"cell_type":"code","source":"# Preprocess credit_card_balance.csv\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    cc = pd.read_csv('../input/credit_card_balance.csv', nrows = num_rows)\n    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n    # General aggregations\n    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n    del cc\n    gc.collect()\n    return cc_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0da363b4699e331216b66cc087c5b7b060e3a7a4"},"cell_type":"code","source":"cred_car_bal = credit_card_balance()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8644ae79225f00f7d66032713120a4559e037f2"},"cell_type":"code","source":"bureau = bureau.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a89ab55e793616084a967a109eb87695a885b73e"},"cell_type":"code","source":"previous_app = previous_app.reset_index()\ncred_car_bal = cred_car_bal.reset_index()\ncash = cash.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ddef63c34742063c0c3a9220c86fdaa823d494c"},"cell_type":"code","source":"ins_payments = ins_payments.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6c3153e144198b20252cdd8cbeb6059851fb5ac","collapsed":true},"cell_type":"code","source":"app_train_domain_poly = app_train_domain_poly.merge(bureau, on='SK_ID_CURR',how='left')\napp_test_domain_poly = app_test_domain_poly.merge(bureau, on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78b16da7c40136e8cce7c515b03954b479d3f97a"},"cell_type":"code","source":"app_train_domain_poly = app_train_domain_poly.merge(previous_app, on='SK_ID_CURR',how='left')\napp_test_domain_poly = app_test_domain_poly.merge(previous_app, on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc5dcab9a53748bcd2b7fe6bdfded497d247c5d1"},"cell_type":"code","source":"app_train_domain_poly = app_train_domain_poly.merge(cred_car_bal, on='SK_ID_CURR',how='left')\napp_test_domain_poly = app_test_domain_poly.merge(cred_car_bal, on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd6d93453d44a4e1acf676b3509a07fe8c3c8b32"},"cell_type":"code","source":"app_train_domain_poly = app_train_domain_poly.merge(cash, on='SK_ID_CURR',how='left')\napp_test_domain_poly = app_test_domain_poly.merge(cash, on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39db806af12ddbd173cdf7b02e1521c8dc2db650","collapsed":true},"cell_type":"code","source":"app_train_domain_poly = app_train_domain_poly.merge(ins_payments, on='SK_ID_CURR',how='left')\napp_test_domain_poly = app_test_domain_poly.merge(ins_payments, on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbc34c4df842faced79b4d00bec01f793b9b9eed"},"cell_type":"code","source":"print('Shape poly train',app_train_domain_poly.shape)\nprint('Shape poly test',app_test_domain_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba54bdd97ff8e65646af58738fd93f00fed9ea70"},"cell_type":"code","source":"train, _ = one_hot_encoder(app_train_domain_poly)\ntest,_ = one_hot_encoder(app_test_domain_poly)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0cdda8ae70d6a66911a3694592d37ba5e6bc60e"},"cell_type":"code","source":"print('Shape poly train',train.shape)\nprint('Shape poly test',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3afcc932203ae3911acbb4f3c10879c98e8c094c"},"cell_type":"code","source":"submission, feature_importances, metrics = model_lgb(train,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b73733d95783f1e8a06f5596e0d8fa1f0378d7d"},"cell_type":"code","source":"submission.to_csv('submissionFive', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}