{"cells":[{"metadata":{"_uuid":"7f5b4e27b63fe52670bc1993da6cf18ddfa29e8d"},"cell_type":"markdown","source":"Here, I'm going to use fastText to attempt to classify this dataset. Some experimentation I did with this can be found [here](Here, I'm going to use fastText to attempt to classify this dataset. Some experimentation I did with this can be found [here](https://frankkloster.github.io/2018/11/20/comparison-of-deep-learning-techniques-for-text-classification.html).\n\n# Preliminaries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\n# Numpy libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Keras libraries\nimport keras\nfrom keras.layers import Activation, Conv1D, Dense, Dropout, GlobalAveragePooling1D, Embedding\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n\n# Scikit-learn, just to split everything into training/validation/testing sets.\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"479f1ea9e9c3d46f5791121fce445a0fc285ffa2"},"cell_type":"markdown","source":"Our training data should be loaded into memory! Lets have a look."},{"metadata":{"trusted":true,"_uuid":"ef8b0c10573b42a7fcfde13dfeb755a972db54cd"},"cell_type":"code","source":"print(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c485e3bfd1ca5a992a459e805a4dda0a9921bc5"},"cell_type":"markdown","source":"Our target prediction is our author. Let's translate everything a strandard one-hot encoding"},{"metadata":{"trusted":true,"_uuid":"7fb2a70741f0cfc089bf1cbc8de42de8663913ae"},"cell_type":"code","source":"author_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\ny = np.array([author_dict[x] for x in df.author])\ny = to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92aa636c3e31d94b6a087f406d6bcd48c0b2ade0"},"cell_type":"markdown","source":"Let's translate our text into something a bit more interpretable by Keras. Specifically, we are going to vectorize our text, using 1-grams and 2-grams."},{"metadata":{"trusted":true,"_uuid":"8d4594505502de223529004c85505e69bdb8b8fb"},"cell_type":"code","source":"def preprocess(text):\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text\n\ndef add_ngram(q, n_gram_max):\n    '''\n    Creates a list of n-grams, up to n_gram_max.\n    \n    q -> (1-grams of q) + (2-grams of q) + ... + (n_gram_max-grams of q)\n    '''\n    ngrams = []\n    for n in range(2, n_gram_max+1):\n        for w_index in range(len(q)-n+1):\n            ngrams.append('--'.join(q[w_index:w_index+n]))\n    return q + ngrams\n\ndef create_docs(df, n_gram_max=2):\n    '''\n    Preprocesses text located into dataframe, creating all n-grams up to n_gram_max.\n    '''\n    docs = []\n    for doc in df.text:\n        doc = preprocess(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs\n\ndef create_vector(df, n_gram_max=2, min_count=2, maxlen=256):\n    '''\n    Creates a tokenized vector to train.\n    '''\n    X = create_docs(df)\n    tokenizer = Tokenizer(lower=False, filters='')\n    tokenizer.fit_on_texts(X)\n    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\n    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n    tokenizer.fit_on_texts(X)\n    X = tokenizer.texts_to_sequences(X)\n    \n    X = pad_sequences(sequences=X, maxlen=maxlen)\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f60b54e8f0766356adea25ee6b9cc82926ddbac"},"cell_type":"code","source":"maxlen = 256\n\nX = create_vector(df, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15c6e885a99e4e9c40c28784fe6208e29f82985"},"cell_type":"markdown","source":"Now we have everything dataset in proper form. All that is left is to split everything into training and testing sets."},{"metadata":{"trusted":true,"_uuid":"c4d9d14c789ee681634edb93c4d937b5b7f7e5ad"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc5c0a7f3bbeb721795cb31884bc1549aae7c1b"},"cell_type":"markdown","source":"Here, we create an auxilary function to visualize how well our model is learning, or potentially overtraining."},{"metadata":{"trusted":true,"_uuid":"116fdfdc71ee974335d2df67bb6c126c0f081be2"},"cell_type":"code","source":"def plot_scores(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.figure(figsize=(20, 10))\n\n    plt.subplot(121)\n    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    plt.title(\"Training and validation accuracy\")\n    plt.legend()\n\n    plt.subplot(122)\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title(\"Training and validation loss\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"666a82e29615a61f11440ff6377d02ebb6d816cf"},"cell_type":"markdown","source":"# FastText Model\nLet's try and test out FastText! Note that doing some experimentation with the few hyperparameters doesn't yield much different results. Thus I won't be doing a standard grid search through hyperparameters."},{"metadata":{"trusted":true,"_uuid":"67b24c6c96ef27e1b4009ac910c0caefba7e01a0"},"cell_type":"code","source":"model = Sequential()\n\nmax_features = 20000\nbatch_size = 32\nembedding_dims = 15\nepochs = 30\n\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ea98edb9441f7f9466681be538d344bbdd2f644"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cad05706c1500fbecc93cfbfec6962cf8efe8f4f"},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=(x_test, y_test),\n                    callbacks=[EarlyStopping(patience=2, monitor='val_loss')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47d0f70b25e28bb59da29853e01eee261a415247"},"cell_type":"code","source":"plot_scores(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"024c4eafc520c0597b4eccc00440383f9a7f6095"},"cell_type":"markdown","source":"We were starting to get around 85% validation classification rate. Not bad!"},{"metadata":{"trusted":true,"_uuid":"a68e00dbfc01ff57df17b8aa009bde4fbe478231"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}