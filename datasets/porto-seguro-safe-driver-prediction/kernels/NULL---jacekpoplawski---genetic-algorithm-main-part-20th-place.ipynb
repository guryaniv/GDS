{"nbformat": 4, "cells": [{"source": ["detailed description: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44659"], "metadata": {"_uuid": "a310165c81cca4523da4c8a5d2640ba1aeb8c3e3", "_cell_guid": "ae427796-ee9f-4308-a317-6bfe17a8cb7c"}, "cell_type": "markdown"}, {"source": ["*(I am trying to publish this kernel since saturday morning, there was some issue with kaggle, hopefuly this time it will work)*"], "metadata": {"_uuid": "e76ff09703c045b24f8c13eea15496e8ccc0dd49", "_cell_guid": "fb64a022-cf69-4398-bba7-d3bba97462c2"}, "cell_type": "markdown"}, {"source": ["Let's import standard tools and LightGBM. \n", "I use lgb version 2.0.10 on my system, there is 2.0.11 installed on Kaggle. Looks like it makes huge difference, because results are different. I have read on forum that other people noticed lower scores on newer lgb version too.\n", "\n", "Please use LightGBM 2.0.10 to reproduce this specific result"], "metadata": {"_uuid": "12fa3192c731950e667df9d4fb58f5b1c531d6a8", "_cell_guid": "91310960-94fc-43f4-84b9-ddbec35bb45e"}, "cell_type": "markdown"}, {"source": ["from sklearn.model_selection import StratifiedKFold\n", "import pandas as pd\n", "import lightgbm as lgb\n", "import numpy as np\n", "import pickle\n", "\n", "train_data = pd.read_csv(\"../input/train.csv\")\n", "test_data = pd.read_csv(\"../input/test.csv\")"], "execution_count": null, "metadata": {"_uuid": "1a6e582006a2a3672ed20db5aa37def59ad28655", "_cell_guid": "13e38dba-4a44-4a49-a922-389eccbc3fe4", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["lgb.__version__"], "execution_count": null, "metadata": {"_uuid": "83ddcca913a3f2657841dd24590326931555d591", "_cell_guid": "21a61208-af49-4ac3-89c3-691a2ae1b281", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["gini helpers, we will use gini_lgb during the training and gini_normalizedc to score predictions"], "metadata": {"_uuid": "177f63a71f2436580110e50e9e94fc870a6a99eb", "_cell_guid": "aec09d84-d363-448c-aa42-6ee379edc04c"}, "cell_type": "markdown"}, {"source": ["def ginic(actual, pred):\n", "    n = len(actual)\n", "    a_s = actual[np.argsort(pred)]\n", "    a_c = a_s.cumsum()\n", "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n", "    return giniSum / n\n", " \n", "def gini_normalizedc(a, p):\n", "    if p.ndim == 2:\n", "        p = p[:,1] \n", "    return ginic(a, p) / ginic(a, a)\n", "\n", "def gini_lgb(preds, dtrain):  \n", "    actuals = np.array(dtrain.get_label())   \n", "    return 'gini', gini_normalizedc(actuals, preds), True\n"], "execution_count": null, "metadata": {"_uuid": "62b74d78cd5f6c299387d8f08c938e38724fd19e", "_cell_guid": "c4db07d9-3ee8-4a02-a644-2629bab6eff2", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["This is the main training function.\n", "\n", "Please note that divide by zero gives you NaN which is perfectly fine for tree algorithms.\n", "However, if you want to use Neural Networks you should instead do something like:\n", "value = a / (b + epsilon)\n", "or just add some ifs\n", "\n", "Target encoding is my own implementation but, min_samples_leaf and smoothing was taken from Olivier's kernel, type and level of noise was found after some experiments.\n", "\n", "copy() was used because I needed to perform feature engineering here, in the inner loop - you can do math feature engineering outside, but for target encoding you can do it only here - validation set is not used in target mean/count!, additional memory optimization can be done by using smaller types for columns (like many kernels do)\n", "\n"], "metadata": {"_uuid": "90cc5ed2d08880fa3bd21cd3dd0795ab7fa81bdc", "_cell_guid": "4103508c-03d8-4ca6-8ab2-a1d557be2752"}, "cell_type": "markdown"}, {"source": ["def perform_single_train(data, hyper):\n", "\n", "    X_train = data[\"X_train\"]\n", "    y_train = data[\"y_train\"]\n", "    X_valid = data[\"X_valid\"]\n", "    y_valid = data[\"y_valid\"]\n", "    X_test = data[\"X_test\"]\n", "    \n", "    lgb_pars = hyper[\"lgb_pars\"]\n", "    features = hyper[\"features\"]\n", "    \n", "    rounds = hyper[\"rounds\"]\n", "    early = hyper[\"early\"]\n", "    noise_level = hyper[\"noise_level\"]\n", "    smoothing = hyper[\"smoothing\"]\n", "    min_samples_leaf= hyper[\"min_samples_leaf\"]\n", "\n", "    X_data = X_train.copy()\n", "    X_data[\"target\"] = y_train\n", "\n", "    X_train_c=X_train.copy()\n", "    X_valid_c=X_valid.copy()\n", "    X_test_c=X_test.copy()\n", "\n", "    for f in features:\n", "        s = f.split(\"_add_\")\n", "        if (len(s) == 2):\n", "            c1 = s[0]\n", "            c2 = s[1]\n", "            X_train[f] = X_train_c[c1] + X_train_c[c2]\n", "            X_valid[f] = X_valid_c[c1] + X_valid_c[c2]\n", "            X_test[f] = X_test_c[c1] + X_test_c[c2]\n", "\n", "        s = f.split(\"_sub_\")\n", "        if (len(s) == 2):\n", "            c1 = s[0]\n", "            c2 = s[1]\n", "            X_train[f] = X_train_c[c1] - X_train_c[c2]\n", "            X_valid[f] = X_valid_c[c1] - X_valid_c[c2]\n", "            X_test[f] = X_test_c[c1] - X_test_c[c2]\n", "\n", "        s = f.split(\"_mul_\")\n", "        if (len(s) == 2):\n", "            c1 = s[0]\n", "            c2 = s[1]\n", "            X_train[f] = X_train_c[c1] * X_train_c[c2]\n", "            X_valid[f] = X_valid_c[c1] * X_valid_c[c2]\n", "            X_test[f] = X_test_c[c1] * X_test_c[c2]\n", "\n", "        s = f.split(\"_div_\")\n", "        if (len(s) == 2):\n", "            c1 = s[0]\n", "            c2 = s[1]\n", "            X_train[f] = X_train_c[c1] / X_train_c[c2]\n", "            X_valid[f] = X_valid_c[c1] / X_valid_c[c2]\n", "            X_test[f] = X_test_c[c1] / X_test_c[c2]\n", "\n", "        s = f.split(\"_mean_\")                    \n", "        if (len(s) > 1):\n", "            if (s[0] == '0'):\n", "                s.remove('0')\n", "\n", "            averages = X_data.groupby(s)[\"target\"].agg([\"mean\", \"count\"])\n", "            smoothing_v = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n", "            averages[f] = X_data[\"target\"].mean() * (1 - smoothing_v) + averages[\"mean\"] * smoothing_v\n", "            averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n", "\n", "            np.random.seed(42)\n", "            noise = np.random.randn(len(averages[f])) * noise_level\n", "            averages[f] = averages[f] + noise\n", "\n", "            X_train = pd.merge(X_train, averages, how='left', left_on=s, right_index=True)\n", "            X_valid = pd.merge(X_valid, averages, how='left', left_on=s, right_index=True)\n", "            X_test = pd.merge(X_test, averages, how='left', left_on=s, right_index=True)                       \n", "            \n", "    X_train_subset=X_train[features]\n", "    X_valid_subset=X_valid[features]\n", "    X_test_subset=X_test[features]\n", "    \n", "    lgb_train = lgb.Dataset(X_train_subset, y_train)\n", "    lgb_eval = lgb.Dataset(X_valid_subset, y_valid, reference=lgb_train)\n", "\n", "    model = lgb.train(lgb_pars,\n", "            lgb_train,\n", "            num_boost_round=rounds,\n", "            valid_sets=lgb_eval,\n", "            early_stopping_rounds=early,\n", "            feval=gini_lgb,\n", "            verbose_eval=100)\n", "\n", "    p_train = model.predict(X_train_subset, num_iteration=model.best_iteration)            \n", "    p_valid = model.predict(X_valid_subset, num_iteration=model.best_iteration)            \n", "    p_test = model.predict(X_test_subset, num_iteration=model.best_iteration)   \n", "\n", "    train_score = gini_normalizedc(y_train, p_train) \n", "    valid_score = gini_normalizedc(y_valid, p_valid)     \n", "\n", "    return [train_score, valid_score, p_test]\n"], "execution_count": null, "metadata": {"_uuid": "b6973f34c310b0a5d6d0a2e993aa8419bea413a1", "_cell_guid": "2782e68c-d17a-43f5-aee7-f096f8f1ff4d", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["Here we do kfold split and write results to the file. \n", "I tried saving train/validation results too for ensembling, however, average was always better than ensembling so here only test results are stored."], "metadata": {"_uuid": "706503c4b68c48a56856f497ff2d6f79e0771891", "_cell_guid": "a4ea0119-af64-43d6-9c06-958b5b3194c3"}, "cell_type": "markdown"}, {"source": ["def perform_full(X, y, X_test, hyper, prefix):    \n", "    scores = []   \n", "    kfold = hyper[\"kfold\"]\n", "    \n", "    skf = StratifiedKFold(n_splits=kfold, random_state=42)\n", "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n", "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n", "        y_train, y_valid = y[train_index], y[test_index]\n", "        \n", "        X_test_c = X_test.copy()\n", "\n", "        data = {\"X_train\": X_train,\n", "                \"y_train\": y_train,\n", "                \"X_valid\": X_valid,\n", "                \"y_valid\": y_valid,\n", "                \"X_test\": X_test_c\n", "                }\n", "        \n", "        [train_score, valid_score, p_test] = perform_single_train(data, hyper)\n", "\n", "        with open(\"test_\"+prefix+str(i)+\".pkl\", 'wb') as f:\n", "            pickle.dump(p_test,f)\n", "\n", "        scores.append([train_score, valid_score])\n", "        \n", "    return scores     \n"], "execution_count": null, "metadata": {"_uuid": "e7c47dfb38b740b7fa5dbe6f4d95ba0d7b215cbc", "_cell_guid": "91aa0c51-bce2-4b42-a6da-1993f346455a", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["This is one feature set generated with my genetic algorithm.\n", "It was selected here because on Public LB it scored 0.28330.\n", "Of course now we all know that Public LB is not very good measure, so on private it scores only 0.29045.\n", "\n", "If you want to play - it is very likely that you can increase score simply by removing some of these features. My top solutions (according to local CVs) were 0.289, then I started removing single feature from each good solutions and in the morning I had collection of 0.290 solutions."], "metadata": {"_uuid": "5aad95280e02ea44018116618e47c6fd9390a9ac", "_cell_guid": "833c6e52-ed6c-48e1-b5e8-6efc91a11c9b"}, "cell_type": "markdown"}, {"source": ["\n", "best_features = ['0_mean_ps_car_05_cat',\n", " '0_mean_ps_car_10_cat',\n", " '0_mean_ps_car_12',\n", " '0_mean_ps_ind_04_cat',\n", " '0_mean_ps_ind_17_bin',\n", " '0_mean_ps_ind_18_bin',\n", " 'ps_car_01_cat',\n", " 'ps_car_01_cat_add_ps_car_02_cat',\n", " 'ps_car_01_cat_mean_ps_car_07_cat',\n", " 'ps_car_01_cat_mean_ps_ind_18_bin',\n", " 'ps_car_02_cat',\n", " 'ps_car_02_cat_add_ps_ind_12_bin',\n", " 'ps_car_02_cat_mul_ps_ind_09_bin',\n", " 'ps_car_02_cat_sub_ps_car_08_cat',\n", " 'ps_car_03_cat',\n", " 'ps_car_03_cat_add_ps_ind_05_cat',\n", " 'ps_car_03_cat_div_ps_ind_11_bin',\n", " 'ps_car_03_cat_mean_ps_ind_12_bin_mean_ps_ind_16_bin',\n", " 'ps_car_04_cat',\n", " 'ps_car_04_cat_mean_ps_ind_04_cat',\n", " 'ps_car_04_cat_mean_ps_ind_14',\n", " 'ps_car_05_cat',\n", " 'ps_car_05_cat_add_ps_ind_10_bin',\n", " 'ps_car_05_cat_mean_ps_ind_13_bin',\n", " 'ps_car_06_cat',\n", " 'ps_car_06_cat_add_ps_reg_02',\n", " 'ps_car_07_cat',\n", " 'ps_car_07_cat_mean_ps_car_10_cat_mean_ps_ind_10_bin',\n", " 'ps_car_07_cat_mean_ps_ind_02_cat_mean_ps_ind_10_bin',\n", " 'ps_car_07_cat_mean_ps_ind_14',\n", " 'ps_car_07_cat_sub_ps_ind_09_bin',\n", " 'ps_car_08_cat',\n", " 'ps_car_08_cat_mul_ps_ind_17_bin',\n", " 'ps_car_09_cat',\n", " 'ps_car_09_cat_mean_ps_car_10_cat_mean_ps_ind_13_bin',\n", " 'ps_car_09_cat_mean_ps_ind_05_cat',\n", " 'ps_car_09_cat_mul_ps_car_10_cat',\n", " 'ps_car_09_cat_sub_ps_ind_03',\n", " 'ps_car_10_cat',\n", " 'ps_car_10_cat_mul_ps_ind_07_bin',\n", " 'ps_car_10_cat_sub_ps_ind_12_bin',\n", " 'ps_car_11',\n", " 'ps_car_11_cat',\n", " 'ps_car_11_div_ps_ind_02_cat',\n", " 'ps_car_11_mean_ps_ind_02_cat',\n", " 'ps_car_11_mean_ps_ind_10_bin',\n", " 'ps_car_11_mean_ps_ind_12_bin',\n", " 'ps_car_11_mean_ps_ind_12_bin_mean_ps_ind_16_bin',\n", " 'ps_car_12',\n", " 'ps_car_13',\n", " 'ps_car_14',\n", " 'ps_car_15',\n", " 'ps_car_15_add_ps_ind_01',\n", " 'ps_car_15_add_ps_ind_11_bin',\n", " 'ps_car_15_div_ps_ind_14',\n", " 'ps_car_15_mul_ps_ind_01',\n", " 'ps_ind_01',\n", " 'ps_ind_01_div_ps_ind_16_bin',\n", " 'ps_ind_01_mean_ps_ind_09_bin',\n", " 'ps_ind_01_mean_ps_ind_11_bin',\n", " 'ps_ind_01_sub_ps_ind_03',\n", " 'ps_ind_02_cat',\n", " 'ps_ind_02_cat_add_ps_reg_01',\n", " 'ps_ind_02_cat_mean_ps_ind_11_bin',\n", " 'ps_ind_02_cat_mean_ps_ind_12_bin',\n", " 'ps_ind_02_cat_mean_ps_ind_14',\n", " 'ps_ind_02_cat_mul_ps_ind_08_bin',\n", " 'ps_ind_02_cat_mul_ps_ind_11_bin',\n", " 'ps_ind_03',\n", " 'ps_ind_03_mean_ps_ind_15',\n", " 'ps_ind_03_mul_ps_ind_11_bin',\n", " 'ps_ind_04_cat',\n", " 'ps_ind_04_cat_mul_ps_ind_12_bin',\n", " 'ps_ind_04_cat_mul_ps_ind_17_bin',\n", " 'ps_ind_04_cat_sub_ps_ind_11_bin',\n", " 'ps_ind_05_cat',\n", " 'ps_ind_05_cat_add_ps_ind_14',\n", " 'ps_ind_05_cat_add_ps_ind_17_bin',\n", " 'ps_ind_05_cat_mean_ps_ind_13_bin',\n", " 'ps_ind_05_cat_mean_ps_reg_01',\n", " 'ps_ind_06_bin',\n", " 'ps_ind_06_bin_div_ps_reg_02',\n", " 'ps_ind_06_bin_mean_ps_ind_13_bin',\n", " 'ps_ind_06_bin_sub_ps_ind_17_bin',\n", " 'ps_ind_07_bin',\n", " 'ps_ind_07_bin_mean_ps_ind_12_bin',\n", " 'ps_ind_08_bin',\n", " 'ps_ind_08_bin_add_ps_ind_17_bin',\n", " 'ps_ind_08_bin_mean_ps_ind_13_bin_mean_ps_ind_14',\n", " 'ps_ind_08_bin_mean_ps_reg_01',\n", " 'ps_ind_09_bin',\n", " 'ps_ind_09_bin_add_ps_ind_15',\n", " 'ps_ind_09_bin_mul_ps_ind_17_bin',\n", " 'ps_ind_10_bin',\n", " 'ps_ind_11_bin',\n", " 'ps_ind_11_bin_div_ps_ind_12_bin',\n", " 'ps_ind_12_bin',\n", " 'ps_ind_12_bin_mul_ps_reg_03',\n", " 'ps_ind_13_bin',\n", " 'ps_ind_13_bin_div_ps_reg_03',\n", " 'ps_ind_13_bin_sub_ps_reg_03',\n", " 'ps_ind_14',\n", " 'ps_ind_14_div_ps_reg_01',\n", " 'ps_ind_15',\n", " 'ps_ind_16_bin',\n", " 'ps_ind_17_bin',\n", " 'ps_ind_18_bin',\n", " 'ps_reg_01',\n", " 'ps_reg_01_mean_ps_ind_18_bin',\n", " 'ps_reg_02',\n", " 'ps_reg_03']\n", "\n"], "execution_count": null, "metadata": {"_uuid": "0e2933e493022d90c560a1acd757db253c8b2158", "_cell_guid": "86379fe9-82c7-4f8e-b048-4378e12d09b8", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["Now let's perform actual training with best_features and store results to test_kaggle_kernel files.\n", "\n", "*(commented out execution - this code runs about 5 minutes on my home computer, but kernel died after 2 hours at least in the saturday morning)*"], "metadata": {"_uuid": "c8315f3fe603b4f3a3651313f9f3ce99d5b2274f", "_cell_guid": "48aecf55-1c7a-4d12-a928-452c5f537019"}, "cell_type": "markdown"}, {"source": ["X = train_data.drop([\"id\",\"target\"],axis=1)\n", "y = train_data[\"target\"].values \n", "X_test = test_data.drop([\"id\"],axis=1)\n", "\n", "lgb_pars = {\n", "    'max_depth': 4,\n", "    'min_data_in_leaf': 20,\n", "    'min_sum_hessian_in_leaf': 1e-3,\n", "    'feature_fraction': 0.47,\n", "    'bagging_fraction': 0.87,\n", "    'bagging_freq': 10,\n", "    'lambda_l1': 8.0,    \n", "    'lambda_l2': 13.0,    \n", "    'min_split_gain': 0,\n", "    'max_bin': 255,\n", "    'min_data_in_bin': 3,\n", "    'learning_rate': 0.08,\n", "    'metric': {'gini_lgb'},\n", "    'objective': \"binary\"\n", "}\n", "\n", "hyper = {\"rounds\": 1000,\n", "         \"early\": 100,\n", "         \"lgb_pars\": lgb_pars,\n", "         \"features\": best_features,\n", "         \"noise_level\": 0.1,\n", "         \"kfold\" : 5,\n", "         \"smoothing\": 30.0,\n", "         \"min_samples_leaf\": 300}\n", "\n", "#result = perform_full(X, y, X_test, hyper, \"kaggle_kernel_\")\n", "\n"], "execution_count": null, "metadata": {"_uuid": "04fe60dd7e161da46614eeeb23bc43edbc86db86", "_cell_guid": "97faac0b-ac4e-4d55-8bfe-8be77977c202", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["This is how I average results from stored files. \n", "I tried other ways - like harmonic mean or rank mean. \n", "Results (on public LB) were similar.\n", "When averaging 100 or 500 kernels rank mean is very tricky, because you need to have them all in the memory, so I wrote algorithm to split each file into pieces and perform rank mean on each piece.\n", "Simple average is easy and works for any number of files.\n"], "metadata": {"_uuid": "e506a37217da229eba6740a8c53ad4b4047fc4e4", "_cell_guid": "3b671e72-278a-4100-9879-870ef5dcd6e7"}, "cell_type": "markdown"}, {"source": ["predictions = []\n", "\n", "kfold = 5\n", "\n", "#for i in range(kfold):    \n", "#    with open(\"test_kaggle_kernel_\"+str(i)+\".pkl\", 'rb') as f:\n", "#        pred = pickle.load(f)        \n", "#    predictions.append(pred)\n", "\n", "#final_prediction = np.zeros(predictions[0].shape[0])\n", "#for i in range(kfold):\n", "#    pred = predictions[i]\n", "#    final_prediction += pred / kfold\n", "\n", "submission=pd.DataFrame()\n", "submission[\"id\"] = test_data[\"id\"]\n", "#submission[\"target\"] = final_prediction\n", "submission.set_index(\"id\", inplace=True)\n", "submission.to_csv(\"kaggle_kernel_1.csv\")  \n", "#submission[\"target\"].describe()\n", "\n", "# Scores on LB:\n", "# 0.29045\n", "# 0.28330"], "execution_count": null, "metadata": {"_uuid": "d4beb8aefbe26af1290240c0ef0d0ae9f8069d97", "_cell_guid": "410aa7ea-9bb6-4cd3-8bbf-da9db89fae85", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["So now let's check is there any benefit in this genetic algorithm at all. \n", "Let's remove all engineered features and perform training on base features set."], "metadata": {"_uuid": "46ab331f35fc58d412f5681373b55221af6996e2", "_cell_guid": "e1d9cc4c-6473-45e3-b560-f034417a90c9"}, "cell_type": "markdown"}, {"source": ["base_features = [\n", " 'ps_car_01_cat',\n", " 'ps_car_02_cat',\n", " 'ps_car_03_cat',\n", " 'ps_car_04_cat',\n", " 'ps_car_05_cat',\n", " 'ps_car_06_cat',\n", " 'ps_car_07_cat',\n", " 'ps_car_08_cat',\n", " 'ps_car_09_cat',\n", " 'ps_car_10_cat',\n", " 'ps_car_11',\n", " 'ps_car_11_cat',\n", " 'ps_car_12',\n", " 'ps_car_13',\n", " 'ps_car_14',\n", " 'ps_car_15',\n", " 'ps_ind_01',\n", " 'ps_ind_02_cat',\n", " 'ps_ind_03',\n", " 'ps_ind_04_cat',\n", " 'ps_ind_05_cat',\n", " 'ps_ind_06_bin',\n", " 'ps_ind_07_bin',\n", " 'ps_ind_08_bin',\n", " 'ps_ind_09_bin',\n", " 'ps_ind_10_bin',\n", " 'ps_ind_11_bin',\n", " 'ps_ind_12_bin',\n", " 'ps_ind_13_bin',\n", " 'ps_ind_14',\n", " 'ps_ind_15',\n", " 'ps_ind_16_bin',\n", " 'ps_ind_17_bin',\n", " 'ps_ind_18_bin',\n", " 'ps_reg_01',\n", " 'ps_reg_02',\n", " 'ps_reg_03']\n", "\n", "X = train_data.drop([\"id\",\"target\"],axis=1)\n", "y = train_data[\"target\"].values \n", "X_test = test_data.drop([\"id\"],axis=1)\n", "\n", "hyper = {\"rounds\": 1000,\n", "         \"early\": 100,\n", "         \"lgb_pars\": lgb_pars,\n", "         \"features\": base_features,\n", "         \"noise_level\": 0.1,\n", "         \"kfold\" : 5,\n", "         \"smoothing\": 30.0,\n", "         \"min_samples_leaf\": 300}\n", "\n", "#result = perform_full(X, y, X_test, hyper, \"kaggle_kernel_base_\")\n", "\n"], "execution_count": null, "metadata": {"_uuid": "adb69fbb4ba3020adf8bf5ee5bc24bf48176ffac", "_cell_guid": "99ad91f5-d2f3-47dd-bb20-eee59eff19b2", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["predictions = []\n", "\n", "kfold = 5\n", "\n", "#for i in range(kfold):    \n", "#    with open(\"test_kaggle_kernel_base_\"+str(i)+\".pkl\", 'rb') as f:\n", "#        pred = pickle.load(f)        \n", "#    predictions.append(pred)\n", "\n", "#final_prediction = np.zeros(predictions[0].shape[0])\n", "#for i in range(kfold):\n", "#    pred = predictions[i]\n", "#    final_prediction += pred / kfold\n", "\n", "submission=pd.DataFrame()\n", "submission[\"id\"] = test_data[\"id\"]\n", "#submission[\"target\"] = final_prediction\n", "submission.set_index(\"id\", inplace=True)\n", "submission.to_csv(\"kaggle_kernel_2.csv\")  \n", "#submission[\"target\"].describe()\n", "\n", "#Scores on LB:\n", "# 0.28885\n", "# 0.28251\n", "    "], "execution_count": null, "metadata": {"_uuid": "3a992558a4b2b213d2a21ec8fe4c416c9898291a", "_cell_guid": "c7d1a902-8259-4768-b592-e25e538a859f", "collapsed": true}, "cell_type": "code", "outputs": []}, {"source": ["As you can see larger set of features scored 0.29045 on private LB (0.28330 on public) while base set of features scored 0.28885 on private LB (0.28251 on public).\n", "\n", "Competition requires lots of luck, but I think one of the reasons I scored high was stability of this approach. My feature sets were different and I was able to create many of them. My final solution was average of 180 sets. Later I realized average of 3 feature sets scored higher. But - before the grand finale I didn't know which feature sets are best. So averaging lots _different_ models was quite good idea."], "metadata": {"_uuid": "6140156b6d1ef64d0e0671bd1020cb581bbf910b", "_cell_guid": "29df6dc6-9b18-4a41-8aff-ec71f10dbb87"}, "cell_type": "markdown"}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}