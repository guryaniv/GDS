{"cells":[{"metadata":{"_uuid":"bab09f609bd1300ad70abb2fcbf8542382fc7dee"},"cell_type":"markdown","source":"Looking at the graph at the end, you see that the blew dots = train, red dots = test\nNow train is a sample of 15.000 points, test is 150.000 points\nYou see that the train sample is much broader then the test sample ?\nIs this right you would expect the test data to be much larger and cluster wider then the training data ??"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsamp = pd.read_csv('../input/sample_submission.csv')\n\nytrain=train['Cover_Type']  #first cop\n#train=train/10*10\ntrain=train.drop('Cover_Type',axis=1)\n\ntotaal=train.append(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b522aff53696971ffad27c23aec9e43e0d81f3f","collapsed":true},"cell_type":"code","source":"totaal['HF1'] = totaal['Horizontal_Distance_To_Hydrology']+totaal['Horizontal_Distance_To_Fire_Points']\ntotaal['HF2'] = abs(totaal['Horizontal_Distance_To_Hydrology']-totaal['Horizontal_Distance_To_Fire_Points'])\ntotaal['HR1'] = abs(totaal['Horizontal_Distance_To_Hydrology']+totaal['Horizontal_Distance_To_Roadways'])\ntotaal['HR2'] = abs(totaal['Horizontal_Distance_To_Hydrology']-totaal['Horizontal_Distance_To_Roadways'])\ntotaal['FR1'] = abs(totaal['Horizontal_Distance_To_Fire_Points']+totaal['Horizontal_Distance_To_Roadways'])\ntotaal['FR2'] = abs(totaal['Horizontal_Distance_To_Fire_Points']-totaal['Horizontal_Distance_To_Roadways'])\ntotaal['ele_vert'] = totaal.Elevation-totaal.Vertical_Distance_To_Hydrology\n\ntotaal['slope_hyd'] = (totaal['Horizontal_Distance_To_Hydrology']**2+totaal['Vertical_Distance_To_Hydrology']**2)**0.5\n#Mean distance to Amenities \ntotaal['Mean_Amenities']=(totaal.Horizontal_Distance_To_Fire_Points + totaal.Horizontal_Distance_To_Hydrology + totaal.Horizontal_Distance_To_Roadways) / 3 \n#Mean Distance to Fire and Water \ntotaal['Mean_Fire_Hyd']=(totaal.Horizontal_Distance_To_Fire_Points + totaal.Horizontal_Distance_To_Hydrology) / 2 \ntotaal['W3S3839']=(totaal['Soil_Type38']+totaal['Soil_Type39'])*totaal['Wilderness_Area3']\ntotaal['W1S2922']=(totaal['Soil_Type29']+totaal['Soil_Type22'])*totaal['Wilderness_Area1']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"def cohen_effect_size(X, y):\n    \"\"\"Calculates the Cohen effect size of each feature.\n    \n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n        Returns\n        -------\n        cohen_effect_size : array, shape = [n_features,]\n            The set of Cohen effect values.\n        Notes\n        -----\n        Based on https://github.com/AllenDowney/CompStats/blob/master/effect_size.ipynb\n    \"\"\"\n    print(X.shape,y.shape)\n    group1, group2 = X[y<y.median()], X[y>y.median()]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff / np.sqrt(pooled_var)\n    return d\n\nexcluded_feats = [] #['SK_ID_CURR']\n\nfeatures = [f_ for f_ in totaal.columns if f_ not in excluded_feats]\nprint('Number of features %d' % len(features),totaal.shape,ytrain.shape)\neffect_sizes = cohen_effect_size(totaal[:len(train)], ytrain.fillna(0))\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.02]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aaad778eb40ba52e68fca157748c12a7539e001","collapsed":true},"cell_type":"code","source":"def normalized(a, axis=-1, order=2):\n    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n    a=a.values  #if panda dataframe\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2==0] = 1\n    return a / np.expand_dims(l2, axis)\n\ndef mapping6(data1,data2,data3,y1,k,sigfeat):\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier,VotingClassifier, RandomForestClassifier\n    from sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,SGDClassifier\n    from sklearn.svm import SVC\n    from sklearn.neighbors import KNeighborsClassifier\n    from xgboost import XGBClassifier\n    #data1 the unknown start data\n    #data2 the database you want to link with and classify with\n\n    datatot=(data3)[significant_features]\n    print(data1.shape,data2.shape,k,datatot.shape)    \n    \n\n    #svd\n    if k>1:\n        svd = TruncatedSVD(n_components=k, n_iter=7, random_state=42)\n        U123=svd.fit_transform(normalized(datatot))\n        kleur=['b']*len(data1)+['r']*len(data2)\n        pd.DataFrame(U123[:,:2]).plot.scatter(x=0,y=1,c=kleur)        \n        Xr=svd.inverse_transform(U123)\n        #U123,s123,V123=svds(vect.fit_transform(data1[veld1].append(data2[veld2])),k=k) #.append(data3[veld3])\n        print(\"datasvd\",U123.shape)\n        #temp=np.concatenate( (U123[len(data1):len(data1)+len(data2)]*s123[:k]   , dwm[len(data1):len(data1)+len(data2)]), axis=1 )\n        temp=Xr[len(data1):len(data1)+len(data2)] #*s123[:k]\n        U2=pd.DataFrame( temp  , index= data2.index)\n    else:\n        U2=pd.DataFrame(datatot.values[len(data1):len(data1)+len(data2)], index= data2.index)\n        \n    \n    if k>1:\n        temp=Xr[:len(data1)] #*s123[:k]\n        U1=pd.DataFrame( temp , index=data1.index )\n    else:\n        U1=pd.DataFrame( datatot.values[:len(data1)], index=data1.index )\n    \n    et = ExtraTreesClassifier(n_estimators=25, max_depth=300, min_samples_split=5, min_samples_leaf=1, random_state=None, min_impurity_decrease=1e-7)\n    \n    clf1 = SVC()\n    clf2 = KNeighborsClassifier()\n    clf3 = GradientBoostingClassifier()\n    clf4 = XGBClassifier()\n    #et = SVC()\n    clf5 = RandomForestClassifier()\n    model = VotingClassifier(estimators=[('svc', clf1), ('knn', clf2), ('gbc', clf3), ('xgbc', clf4), ('rf', clf5)], voting='hard')\n\n    #y_pred = eclf1.predict(x)\n    \n    # TRAINING\n    #et = SGDClassifier(n_jobs=4,max_iter=100)\n    model = OneVsRestClassifier(et)  \n    #temp=pd.DataFrame( dwm[ len(data1):len(data1)+len(data2) ] )\n    temp=U2 #.T.append(temp.T)   #U2  append word tfidf vector\n    print('U2',temp.shape)\n    model.fit(U1,y1)\n    \n    print( (model.predict(U1)==y1).mean()*100 ) \n    \n    #PREDICTING\n    #temp=pd.DataFrame(dwm[ :len(data1)] )\n    temp=U2 #.T.append(temp.T)\n    data2['pre']=model.predict(U2)\n    \n    return data2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79428425da842a9994a4c7371238f763d79bf45f","collapsed":true},"cell_type":"code","source":"test=mapping6(train,test,totaal,ytrain,2,significant_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7ec97380d2b30c2c359185eaa6b417ab926b058","collapsed":true},"cell_type":"code","source":"sub_eclf = pd.DataFrame()\nsub_eclf['Id'] = test['Id']\nsub_eclf['Cover_Type'] = test['pre']\n\nsub_eclf.to_csv('submission_eclf.csv', index=False)\ntest","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}