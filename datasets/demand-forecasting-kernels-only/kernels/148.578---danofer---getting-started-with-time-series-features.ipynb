{"cells":[{"metadata":{"_uuid":"db3eb07d026abb3e5996d7ad09540697ae9b6895"},"cell_type":"markdown","source":"### A very basic kernel with some naive features for predicting seasonal or hierachical autoregressive problems (such as store sales)\n* Kernel will be updated\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport datetime\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"997814be0ebc19a9c17b58cc11998ac89efa9758"},"cell_type":"markdown","source":"### merge train and test for easier feature engineering. \n* Beware leaks and the offset needed in predicting the future!!\n*  Can also be useful for creating a baseline; see: https://machinelearningmastery.com/model-residual-errors-correct-time-series-forecasts-python/\n*  Note that here, train has no ID column\n"},{"metadata":{"trusted":true,"_uuid":"889296250d9bba0923919fed888d1f9411e5a386"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e19527ad88eabb1d9a35cde9c44e0960efa56e0"},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"decd45459d52f78aec26627e4ceb613cb04a958b"},"cell_type":"code","source":"print(\"train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\ndf = pd.concat([train,test])\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6bf70f3afdbebdc946ca8b843a0cd429a94846b"},"cell_type":"markdown","source":"### naive datetime features:\n* Could also add holidays, weekends, work-hours if relevant and known"},{"metadata":{"trusted":true,"_uuid":"68617404bfcff04c542538e1d629e638cc9c27e1"},"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'],infer_datetime_format=True)\n\n\ndf['month'] = df['date'].dt.month\ndf['weekday'] = df['date'].dt.dayofweek\ndf['year'] = df['date'].dt.year\n# df['date'].dt.\ndf['week_of_year']  = df.date.dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"523f01561258cfed028380c387dc39131afa25c9"},"cell_type":"code","source":"df.set_index(\"date\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7119960d7be8ba50730eb61e393a03f4d32550ab"},"cell_type":"markdown","source":"## Add historical / seasonal features\n* Additional features could includes slopes, trends, item-basket level features, but require more work to avoid leakage. Will add in future\n* For now -  naive features: we expect sales to be what they were at the same time of year, in the past, for each store+item combo\n    * Could be done more elegantly with pandas's agg_func"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d205600f7bd4d68fc62f8597ab4534a80e1ece5"},"cell_type":"code","source":"df[\"median-store_item-month\"] = df.groupby(['month',\"item\",\"store\"])[\"sales\"].transform(\"median\")\ndf[\"mean-store_item-week\"] = df.groupby(['week_of_year',\"item\",\"store\"])[\"sales\"].transform(\"mean\")\ndf[\"item-month-sum\"] = df.groupby(['month',\"item\"])[\"sales\"].transform(\"sum\") # total sales of that item  for all stores\ndf[\"store-month-sum\"] = df.groupby(['month',\"store\"])[\"sales\"].transform(\"sum\") # total sales of that store  for all items\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49ee3602ee13ef5e7e0ad403130dcb450b442430"},"cell_type":"code","source":"# get shifted features for grouped data. Note need to sort first! \ndf['store_item_shifted-90'] = df.groupby([\"item\",\"store\"])['sales'].transform(lambda x:x.shift(90)) # sales for that item 90 days = 3 months ago\ndf['store_item_shifted-180'] = df.groupby([\"item\",\"store\"])['sales'].transform(lambda x:x.shift(180)) # sales for that item 180 days = 3 months ago\ndf['store_item_shifted-365'] = df.groupby([\"item\",\"store\"])['sales'].transform(lambda x:x.shift(365)) # sales for that 1 year  ago\n\ndf[\"item-week_shifted-90\"] = df.groupby(['week_of_year',\"item\"])[\"sales\"].transform(lambda x:x.shift(12).sum()) # shifted total sales for that item 12 weeks (3 months) ago\ndf[\"store-week_shifted-90\"] = df.groupby(['week_of_year',\"store\"])[\"sales\"].transform(lambda x:x.shift(12).sum()) # shifted total sales for that store 12 weeks (3 months) ago\ndf[\"item-week_shifted-90\"] = df.groupby(['week_of_year',\"item\"])[\"sales\"].transform(lambda x:x.shift(12).mean()) # shifted mean sales for that item 12 weeks (3 months) ago\ndf[\"store-week_shifted-90\"] = df.groupby(['week_of_year',\"store\"])[\"sales\"].transform(lambda x:x.shift(12).mean()) # shifted mean sales for that store 12 weeks (3 months) ago","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ce4b998e3fe6fca6ac6d5c02d920db71c324f18"},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0e30a545d849af5a62453769f1fa5448aa62650"},"cell_type":"markdown","source":"## We should do one hot encoding at this point on the store and ite mIDs to avoid silly range based features. \n* We'll do that later, as it's also possible the numbers/order has meaning. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8e136176983da48c1a870764c7d3bb541c8df6b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71cb3f23135c1579888b3abc231e6a6586e2bf1a"},"cell_type":"markdown","source":"## split our data for modelling"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"504770e10670931bdd967b7fddb1dff3a69c0a3d"},"cell_type":"code","source":"col = [i for i in df.columns if i not in ['date','id']]\ny = 'sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98f75766361d9d3266192376cddd741ee84a9158"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"267065c36dfab945123117452474be38be6a0b32"},"cell_type":"code","source":"train = df.loc[~df.sales.isna()]\nprint(\"new train\",train.shape)\ntest = df.loc[df.sales.isna()]\nprint(\"new test\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58dda59a9ac062b22c9c43ab7ae8fa9931276308"},"cell_type":"markdown","source":"# Evaluation should use **temporal train test** split or temporal CV\n\n*  we can define it manually or use sklearn's functions. these aren't trivial to plug and play with xgboost, so i'll skip for this version of the kernel, but without it, our local score is meaningless!"},{"metadata":{"trusted":true,"_uuid":"06450306227c447b32712d81168cad0a3abc6d8a"},"cell_type":"code","source":"train_x, train_cv, y, y_cv = train_test_split(train[col],train[y], test_size=0.15, random_state=42)\n# train_x, train_cv, y, y_cv = TimeSeriesSplit(train[col],train[y], test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a26ba3bad272bc19ffbf06ba916c4b85acaeed5"},"cell_type":"code","source":"def XGB_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=2017, num_rounds=500):\n    param = {}\n    param['objective'] = 'reg:linear'\n    param['eta'] = 0.1\n    param['max_depth'] = 6\n    param['silent'] = 1\n    param['eval_metric'] = 'mae'\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = 0.8\n    param['seed'] = seed_val\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n        \n    return model    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d555b82f45622d79d1547c91f13a9035ab697898"},"cell_type":"code","source":"model = XGB_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\ny_test = model.predict(xgb.DMatrix(test[col]), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d5fe26a6c543a2635e9cd6312fa7bbbe674c7115"},"cell_type":"code","source":"sample['sales'] = y_test\nsample.to_csv('simple_starter.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}