{"cells":[{"metadata":{"_uuid":"5484dedb6db529f4dc58d0cb433186b3d823f429","_cell_guid":"73969602-c633-450e-9364-9481a21a879c"},"cell_type":"markdown","source":"# **This notebook's best result: val_acc is 0.8779, val_loss is 0.36393**"},{"metadata":{"_uuid":"7ccc3b4516a4fcde346a162ae4f9461016bbfbbf","_cell_guid":"d5736ce6-a0b0-4cf0-beaf-a73837398da9"},"cell_type":"markdown","source":"# **1. Few Preprocessings**\n# **2. Model: FastText by Keras**\n## **2.1** Change Preprocessings:\n- Do lower case "},{"metadata":{"_kg_hide-input":false,"_uuid":"b05ef71268db76a4e2565177bf6a5668a5fc428e","_kg_hide-output":true,"_cell_guid":"93e00783-a024-4e87-a5e1-6709cb8cc981","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d700f739101e37903112e1de293323dcfbb577be","_cell_guid":"a5cc2c3e-7960-482e-b548-c447b89925ec","trusted":true},"cell_type":"code","source":"df = pd.read_csv('./../input/train.csv')\ndf_test = pd.read_csv('./../input/test.csv')\ndf_full = df.append(df_test, sort=False)\n\na2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\ny = np.array([a2c[a] for a in df.author])\ny = to_categorical(y)\ny","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a01bab31ed7b8a55820612063576963488d99eb6","_cell_guid":"a45cb3ba-d1bc-48e0-956c-27d0f49a9943"},"cell_type":"markdown","source":"# 1. **Few Preprocessings**\n\nIn traditional NLP tasks, preprocessings play an important role, but...\n\n## **Low-frequency words**\nIn my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n\n**NOTE**:\nSome keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\nBut these are useful for this task.\n\n## **Removing Stopwords**\n\nNothing.\nTo identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n\n## **Stemming and Lowercase**\n\nNothing.\nThis reason is the same for stopwords removing.\nAnd I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n\n## **Cutting long sentence**\n\nToo long documents are cut.\n\n## **Punctuation**\n\nBecause I guess each author has unique punctuations's usage in the novel, I separate them from words.\n\ne.g. `Don't worry` -> `Don ' t worry`\n\n## **Is it slow?**\n\nDon't worry! FastText is a very fast algorithm if it runs on CPU. "},{"metadata":{"_uuid":"0023cd1542d866d931deb8472f8a0d6fb0262d9a","_cell_guid":"8182b25a-f490-4b41-9865-ee1c04afecee"},"cell_type":"markdown","source":"# **Let's check character distribution per author**"},{"metadata":{"_uuid":"246a428ca3a063294c15c8c08d234ecf01e4ddbb","_kg_hide-output":true,"_cell_guid":"c1d00b0d-90e0-4f19-842c-51a82de42a10","trusted":true},"cell_type":"code","source":"counter = {name : defaultdict(int) for name in set(df.author)}\nfor (text, author) in zip(df.text, df.author):\n    text = text.replace(' ', '')\n    for c in text:\n        counter[author][c] += 1\n\nchars = set()\nfor v in counter.values():\n    chars |= v.keys()\n    \nnames = [author for author in counter.keys()]\n\nprint('c ', end='')\nfor n in names:\n    print(n, end='   ')\nprint()\nfor c in chars:    \n    print(c, end=' ')\n    for n in names:\n        print(counter[n][c], end=' ')\n    print()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e72d6f22587780364ed24cae13ece4a403479dd","_cell_guid":"7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8"},"cell_type":"markdown","source":"# **Summary of character distribution**\n\n- HPL and EAP used non ascii characters like a `ä`.\n- The number of punctuations seems to be good feature\n"},{"metadata":{"_uuid":"fee49fd9139b78ae03603d7d37eafa38f3cb29dc","_cell_guid":"ce97fc0a-b85c-4f34-92c5-ae66a0730ace"},"cell_type":"markdown","source":"# **Preprocessing**\n\nMy preproceeings are \n\n- Separate punctuation from words\n- Remove lower frequency words ( <= 2)\n- Cut a longer document which contains `256` words"},{"metadata":{"_uuid":"999012010cd8b9b20d3c5b16c11a2374a5ce44c0","_cell_guid":"72ff2ff5-0945-4f39-8b02-39e4d5df16c5","trusted":true},"cell_type":"code","source":"def preprocess(text):\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53f325a090a44f7109f0537022398797704cdc80","_cell_guid":"f123742f-540f-438d-aba3-ebbca69235be","trusted":true},"cell_type":"code","source":"def create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n        \n    docs = []\n    for doc in df.text:\n        doc = preprocess(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150f9f6643e6753386b2021ac812ecc0cac66202","_cell_guid":"888047de-806e-4ad2-9fff-18b4d6583d30","trusted":true},"cell_type":"code","source":"min_count = 2\n\ndocs = create_docs(df)\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\nnum_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cb38307cd93ad2cfdfe2d140f9c0c8338d80d2f"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=num_words, lower=False,  filters='')\ntokenizer.fit_on_texts(docs_full)\ndocs = tokenizer.texts_to_sequences(docs)\nprint(\"Samples Number:\", len(docs))\nprint(\"Sample 1:\\n{}\\nSample 2:\\n{}\".format(docs[0], docs[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a62982b99504707cc40c4acf8ba2d37fe7b7b17"},"cell_type":"code","source":"max_size = 0 \nfor ind, text in enumerate(docs):\n    max_size = len(text) if len(text) > max_size else max_size\n    \nprint(\"Max number of words in a sample for full dataset:\", max_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"092c3da5a76f0075d4f4c03d44c99e96ed23d799"},"cell_type":"code","source":"maxlen = 256\ndocs = pad_sequences(sequences=docs, maxlen=max_size)\nprint(\"Samples Number:\", len(docs))\nprint(\"Sample 1:\\n{}\\nSample 2:\\n{}\".format(docs[0][-40:], docs[1][-40:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba40980de666b8249bd307506f409ed91cc26b32"},"cell_type":"code","source":"num_words\ndocs.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9e353b548b0dfbd4b42a40d8a2643efeb359a20","_cell_guid":"f9ebc033-2a26-4656-9472-8990c1a27c79"},"cell_type":"markdown","source":"# **2. Model: FastText by Keras**\n\nFastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n\nFastText contains only three layers:\n\n1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n3. Softmax layer\n\nThere are some implementations of FastText:\n\n- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n\nOriginal Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"},{"metadata":{"_uuid":"8b56b2ef90e519b939b7bf9ec5a146f749807b02","_cell_guid":"636eb75e-6fba-413e-996d-1395609b422c"},"cell_type":"markdown","source":"# My FastText parameters are:\n\n- The dimension of word vector is 20\n- Optimizer is `Adam`\n- Inputs are words and word bi-grams\n  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"},{"metadata":{"_uuid":"bba1d1a6416876e74ed688f56e4d5bc4990ec12a","_cell_guid":"393d1ddb-0a87-42a3-8575-53ff7abff1da","trusted":true},"cell_type":"code","source":"input_dim =  np.max(docs) + 1\nembedding_dims = 20\ninput_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f347f52ccbe3a9342f2fceb0d6b0182b7e1527c4"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.20)# random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6c16572e6b32923af39dfd29467e32b52561bb1","_cell_guid":"2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f","trusted":true},"cell_type":"code","source":"def create_model(embedding_dims=20, optimizer='adam'):\n    \n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    #model.add(Dense(20, activation='relu')) #tanh \n    #model.add(Dropout(0.5)) #relu \n    #model.add(Dense(15, activation='relu')) #tanh \n    #model.add(Dropout(0.5)) #relu \n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f187415d51e540d341eb017c07cc1c6cc1098c7"},"cell_type":"code","source":"model = create_model(embedding_dims=20)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_kg_hide-output":true,"_cell_guid":"0db889db-0b3e-4025-8847-e3eb5f853f37","_kg_hide-input":false,"_uuid":"22e57e010206a3044adf7b82160c7c3ca78030f8","trusted":true},"cell_type":"code","source":"epochs = 500\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc9fc905d087fcfcc12fae5d435eaa9fc2d91bc3"},"cell_type":"markdown","source":"### **Result**\n\n- Best val_loss is 0.3467\n- Best val_acc is 0.8613\n\n"},{"metadata":{"_uuid":"124728b6e2ed0dacf812fef3443741b6de0541af"},"cell_type":"markdown","source":"# **2.1 Change Preprocessings**\n\nNext, I change some parameters and preprocessings to improve fastText model.\n## **2.1.1 Do lower case**"},{"metadata":{"_uuid":"682548b62e0d5583fdcd08ac9f9383bd65da9681","trusted":true},"cell_type":"markdown","source":"\ndocs = create_docs(df)\ntokenizer = Tokenizer(lower=True, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\n\ninput_dim = np.max(docs) + 1\n"},{"metadata":{"trusted":true,"_uuid":"c3e7f12d80560a425ecac044335d44690d51c150"},"cell_type":"code","source":"#model = create_model()\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb25b2dfa4cd2b756a35295c2c6acf2ec6888eb7","trusted":true},"cell_type":"markdown","source":"epochs = 25\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.20)\n\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"},{"metadata":{"_uuid":"05b1da349532551795c718b5992e46edbcde272c"},"cell_type":"markdown","source":"**Result**\n\n- Best val_loss is 0.3129\n- Best val_acc is 0.8787"},{"metadata":{"_uuid":"6fe736115e53146ddf281eeacfb7274b3c532ff7","trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ndocs = create_docs(test_df)\ndocs = tokenizer.texts_to_sequences(docs)\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\ny = model.predict_proba(docs)\n\nresult = pd.read_csv('../input/sample_submission.csv')\nfor a, i in a2c.items():\n    result[a] = y[:, i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e979eae461f81a3bd670f3b5e8ad3d57786e6ae9","trusted":true},"cell_type":"code","source":"result.to_csv('fastText_result_01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"753a773d5b68e17058fc1b8b9a7793b033ab2a26","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}