{"cells":[{"metadata":{"_uuid":"a9b819d8e56ca332a05db3827a968b5c77cb0907","_cell_guid":"ee954090-cc55-4ee9-9fe1-98e487f0bd4c"},"cell_type":"markdown","source":"## Idéias\n* Criar novas features -> serão usadas como input após as camadas LSTM\n\n      - quantas vezes a palavra aparece na sentença -> int (? cada palavra uma nova feature? matriz (m, n_total_palavras))\n      - quantas vezes a palavra aparece em todo o dataset (? cada palavra uma nova feature? matriz (m, n_total_palavras))\n      - checar se é uma palavra que estava no embedding pretreinado -> [0,1]\n\n            TODAS ESSAS FEATURES ACIMA TEM O MESMO PROBLEMA.\n            POSSÍVEL SOLUÇÃO seria uma matriz (m, n_total_palavras, n_features), sendo  n_features = número de features com o formato (m, n_total_palavras)\n            \n      -1 coluna - quantas palavras existem na sentenca?\n      -1 coluna - quantas palavras diferentes?\n      -1 coluna - quantas vezes a palavra que mais repete aparece?\n      -1 coluna - tem palavra inteira maiúscla?  \n      -1 coluna - quantas palavras inteiras maiúscula?\n      -1 coluna - taxa de repetição\n      (m, 6)\n\nPara predições é necessário uma função predict que recebe os dados e faz um pré-processamento, criando as features descritas acima.\n\n\n* Os inputs de cacacteres de uma sentença tem de ter sua extensão limitada à extenção correspondente de palavras de uma sentença. Por exemplo, na frase:  \"`Paglia is somewhat of intellectual enigma, a conservative academic feminist, who revels in`\" a sequencia de caracteres para essa sentença terminaria no último `\"n\"`, enquanto a sequencia de palavras terminaria em `\"in\"`.\n\n* Testar tokenização por meio da bilbioteca NLTK que parece criar divisões de palavras melhores.\nEx: \" **queen's** \" seria dividido em \" **queen** \" e \" **'s** \", algo que não acontece no Tokenizer da biblioteca Keras que junta essas partículas em uma nova palavra \"**queen's**\"\n\n* **English Stemmer NLTK**\n\n* Testar **Stanford Tokenizer**\n\n* Adicionar embedding para caracteres e criar uma LSTM que irá processar um input de sequências de caracteres, isso será concatenado com as outras formas de inputs numa camada mais interna do modelo.\n\n* Treinar **Character Embeddings** separadamente, e carrega-los no notebook ou usar **Word Embeddings** para [gerar os novos.](http://minimaxir.com/2017/04/char-embeddings)"},{"metadata":{"_uuid":"bb41ad86b25fecf332927b0c8f55dd710101e33f","_cell_guid":"9d2dbdb3-6c74-4f96-9865-2951dfd653ce"},"cell_type":"markdown","source":"# Improved LSTM baseline\n\nThis kernel is a somewhat improved version of [Keras - Bidirectional LSTM baseline](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051) along with some additional documentation of the steps. (NB: this notebook has been re-run on the new test set.)"},{"metadata":{"_uuid":"598f965bc881cfe6605d92903b758778d400fa8b","_cell_guid":"2f9b7a76-8625-443d-811f-8f49781aef81","collapsed":true,"trusted":false},"cell_type":"code","source":"import sys, os, re, csv, codecs, operator, numpy as np, pandas as pd\nfrom collections import defaultdict, OrderedDict\n\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import Callback\n\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d961885dfde18796893922f72ade1bf64456404e","_cell_guid":"c297fa80-beea-464b-ac90-f380ebdb02fe"},"cell_type":"markdown","source":"We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."},{"metadata":{"_uuid":"729b0f0c2a02c678631b8c072d62ff46146a82ef","_cell_guid":"66a6b5fd-93f0-4f95-ad62-3253815059ba","collapsed":true,"trusted":false},"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n#EMBEDDING_FILE=\"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\"\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b62d39216c8d00b3e6b78b825212fd190757dff9","_cell_guid":"98f2b724-7d97-4da8-8b22-52164463a942"},"cell_type":"markdown","source":"\nSet some basic config parameters:"},{"metadata":{"_uuid":"d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3","_cell_guid":"2807a0a5-2220-4af6-92d6-4a7100307de2","collapsed":true,"trusted":false},"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"623e8b1669ee3dc593cc7ad7a3df3b5541841ee1","_cell_guid":"81d40752-4001-4496-af66-04a53a9f1dfd","collapsed":true,"trusted":false},"cell_type":"code","source":"window_sizes = [40,80,maxlen]\nstep_size = 20\n\ndef augment_toxic(list_sequences, y, window_sizes=[100],step_size=20, leakage=0, \n                  maxlen=None, truncated_only=False, non_repeat=False):\n    \"\"\"\n    This function is intended to augment the amount of toxic comments on the training data\n    using the provided dataset.\n    Given a list ´window_sizes´ of integer values, the window will slide through the sentence with \n    a ´step_size´ until the end. Each window will be a new sentence to be appended to the training \n    data list.\n    This function solves the problem of losing data when the truncate from Tokenizer is applied for\n    values in ´window_sizes´ lower than `maxlen` (if local ´´maxlen=None´´, consider the global `maxlen`.\n    \n    arguments: \n    ´list_sequences´ -> 2D-array like: sequence of sentences\n    ´window_sizes´ -> integer\n    ´step_size´ -> integer\n    ´leakage´ -> float: percentage of words in the new sentence to be randomly removed\n    \n    ||Maybe better to be another function\n    ||'''Augment only truncated'''\n    ||´truncated_only´ -> boolean: if True, ´step_size´ will be ignored and only the words \n    ||    after ´maxlen´ index will be augmented.\n    ||´maxlen´ -> max lenght of the Tokenized comment. Should be equal the global `maxlen`.\n    \n    \n    return: \n    ´new_comments´ -> a list with new comments of sizes ´window_sizes´\n    ´new_ys´ -> a list with indices for the new comments\n    \"\"\"\n    new_comments = []\n    new_ys = []\n    if isinstance(list_sequences,(list,tuple)):\n        list_sequences = np.array(list_sequences)\n    # If ´non-repeat´ is True, create a list to store the already processed comments\n    if non_repeat:\n        done = []\n    \n    # True if any of 6 clases == 1, False otherwise.\n    toxic_filter = y.sum(axis=1) > 0\n    # Indices of toxic comments.\n    toxic_indices = [i for i in range(len(toxic_filter)) if toxic_filter[i] == True] \n    toxic_comments = list(list_sequences[toxic_indices])\n\n    window_sizes = sorted(window_sizes)\n    \n    for window in window_sizes:\n        if truncated_only:\n            step_size = window\n        for index, comment in zip(toxic_indices,toxic_comments):\n            # Check if this comment has already been augmented\n            if non_repeat and index in done:\n                continue\n            # mode\n            if truncated_only:\n                splitted = comment[maxlen:]                \n            else:\n                splitted = comment\n            \n            # Check if it is possible to take spall pieces of size ´window´ from the sentence.\n            # Go to the next comment if it is not.\n            if len(splitted) <= window:\n                continue\n                \n            # Get how many times it is possible to apply the window with the given ´step_size´.\n            n_steps = 0\n            window_end = window        \n            while window_end <= len(splitted):\n                window_end += step_size\n                n_steps += 1\n\n            # Perform the possible steps\n            for step in range(n_steps):\n                # Get the window.                \n                new_sequence = np.array(splitted[step*step_size:window+step*step_size])\n                \n                # Leakage control\n                if leakage < 0 or leakage > 1:\n                    raise ValueError(f'leakage must be between 0 and 1. It was passed leakage={leakage}')\n                elif leakage > 0:\n                    ## total number of words that can be removed\n                    possible_leaks = int(len(new_sequence)*leakage) \n                    ## random number between 0 and ´possible_leaks´\n                    n_leaks = np.random.randint(possible_leaks) \n                    ## indices of words to be removed\n                    leaky_indices = set(np.random.randint(0,len(new_sequence),n_leaks)) \n                    leak_filter = [False if i in leaky_indices else True for i in range(len(new_sequence))] \n                    new_sequence = new_sequence[leak_filter]\n                \n                # Append new sentence to a list        \n                new_comments.append(new_sequence)\n                # Append labels from the original sentece to the new sentece to a list\n                new_ys.append(y[index,:])\n                # Append index to ´done´ list if ´non_repeat´ is True\n                if non_repeat:\n                    done.append(index)\n    # pad or truncate sequences to have length of `maxlen`\n    list_sequences = pad_sequences(list_sequences, maxlen=maxlen, truncating='post')\n    new_comments = pad_sequences(new_comments, maxlen=maxlen, truncating='post')\n    \n    # concatenate new comments with the old ones\n    list_sequences = np.concatenate((list_sequences, new_comments), axis=0)\n    y = np.concatenate((y, new_ys),axis=0)\n    # shuffle data\n    shuffle_index = np.random.permutation(len(y))\n    list_sequences, y = list_sequences[shuffle_index], y[shuffle_index]\n                \n    return list_sequences, y\n\ndef get_helper(list_sentences):\n    \"\"\" \n    Generates new features:\n    \n    Lists of length `len(list_sentences)`\n    `sentence_length` -> number of words in each sentence after sequencing.\n    `n_distinct_words` -> number of distinct words for each sentence.\n    `n_uppercase` -> number of uppercase words in each sentence.\n    `any_uppercase` -> True if there is any uppercase word in the sentence, False otherwise.\n    `n_most_occurring_word` -> number of times the most frequent word of each sentence appears.\n    `repetitive` -> Percentage of repeating words ´´1 - (n_distinct_words / sentence_lenght)´´\n    \n    Return:\n        `helper` -> An array of shape (len(list_sentences), 6), where 6 is the number of\n            features and each column is a feature.\n    \"\"\"\n    sentence_length = []        #(int) how mane words in each sentence?\n    n_distinct_words = []       #(int) how many distinct words in each sentence?\n    n_uppercase = []            #(int) how many words are uppercase in each sentence?\n    any_uppercase = []          #(bool) is the any uppercase word in each sentence?\n    n_most_occurring_word = []  #(int) how many times the most frequent word appears in each sentence?\n    \n    repetitive = []             #|(float) Percentage of repeating words\n                                #| ´´1 - (n_distinct_words / sentence_lenght)´´\n                                #| small values means a higly repetitive sentence\n    \n    for sentence in list_sentences:\n        word_sequence = text_to_word_sequence(sentence)\n        \n        sentence_length.append(len(word_sequence))\n        n_distinct_words.append(len(set(word_sequence)))\n        try:\n            repetitive.append(1 - (n_distinct_words[-1]/sentence_length[-1]))\n        except ZeroDivisionError:\n            repetitive.append(0)\n        try:\n            n_most_occurring_word.append(word_sequence.count(max(word_sequence)))\n        except ValueError:\n            n_most_occurring_word.append(0)\n        n_upper = 0\n        for word in sentence:\n            n_upper += word.isupper()\n                        \n        n_uppercase.append(n_upper)\n        any_uppercase.append(n_upper > 0)\n            \n    features = [sentence_length, n_distinct_words, n_uppercase, \n                any_uppercase, n_most_occurring_word, repetitive]\n    helper = np.zeros((len(list_sentences), len(features)))    \n    for i, l in enumerate(features):\n        helper[:,i] = l        \n    return helper\n\ndef get_sequence_maxlen(list_sentences):\n    \"\"\" Return the maxlen value. This function looks for the sentence with more characters \"\"\"    \n    maxlen = 0\n    for sentence in list_sentences:\n        sequence = text_to_word_sequence(sentence)\n        length = len(sequence)\n        maxlen = length if length > maxlen else maxlen        \n    return maxlen\n\ndef get_num_words(list_sentences):\n    \"\"\" Returns the number of distinct words in the sentences \"\"\"\n    words = set()\n    for sentence in list_sentences:\n        sequence = text_to_word_sequence(sentence)\n        for word in sequence:\n            words.add(word)            \n    return len(words)\n\ndef get_maxlen_char(list_sentences):\n    \"\"\" Return the maxlen value. This function looks for the sentence with more words \"\"\"    \n    maxlen = 0\n    for sentence in list_sentences:\n        length = len(sentence)\n        maxlen = length if length > maxlen else maxlen        \n    return maxlen\n\ndef get_num_char(list_sentences):\n    \"\"\" Returns the number of distinct characters in the sentences \"\"\"\n    chars = set()\n    for sentence in list_sentences:\n        for char in sentence:\n            chars.add(char)            \n    return len(chars)\n\ndef get_word_length(list_sentences, mode='max'):\n    \"\"\" \n    Returns the biggest word length for `mode='max'`or the average length \n    between all words for `mode='avg'.\n    \"\"\"\n    length = 0\n    if mode is 'avg':\n        n_words = 0\n        for sentence in list_sentences:\n            sequence = text_to_word_sequence(sentence)\n            for word in sequence:\n                length += len(word)\n                n_words += 1\n        length = length//n_words\n            \n    elif mode is 'max':\n        for sentence in list_sentences:\n            sequence = text_to_word_sequence(sentence)\n            for word in sequence:\n                t_len = len(word)\n                if length < t_len and t_len < 50: \n                    length = t_len\n    \n    return length","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba7f69bb45bff1ae07a61a3a3608027c11be6a27","_cell_guid":"6da183a3-f7e3-4e9f-a9a5-8b5f0345f8e7"},"cell_type":"markdown","source":"### Read Glove\nRead the glove word vectors (space delimited strings) into a dictionary from word->vector."},{"metadata":{"_uuid":"cee7a350751d64a5da4eefc76d85b84c08bf0df5","_cell_guid":"cdd00a63-a5fe-4d34-ba52-18be449960ab","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1d1a7dfa67d1ca1279f2dc5642797e03cab2c7d","_cell_guid":"bbcd4cfe-9c41-4238-b7f0-aaeb703e6e05","collapsed":true,"trusted":false},"cell_type":"code","source":"def text_to_sequence(list_sentences, lower=True, char_level=False):\n    if lower:\n        if char_level:\n            sequences = [list(sentence.lower()) for sentence in list_sentences]\n        else:\n            sequences = [word_tokenize(sentence.lower()) for sentence in list_sentences]\n    else:\n        if char_level:\n            sequences = [list(sentence) for sentence in list_sentences]\n        else:\n            sequences = [word_tokenize(sentence) for sentence in list_sentences]   \n    return sequences\n\ndef get_word_index(sequences, return_counts=False):\n    \n    word_counts = defaultdict(int)\n    word_index = dict()\n    # Count words frequency\n    for sequence in sequences:\n        for word in sequence:\n            word_counts[word] += 1\n            \n    # Sort from most to less frequent\n    word_counts = OrderedDict(sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True))\n    \n    # Associate numeric indices to each word starting at 1\n    for i, word in enumerate(word_counts.keys(),1):\n        word_index[word] = i\n        \n    if return_counts:\n        return word_index, word_counts\n    return word_index\n\ndef apply_indices(sequences, num_words, word_index):\n    if not isinstance(word_index, dict):\n        raise TypeError('word_index must be a dict')\n    id_sequences = []\n    # Apply indices\n    for sequence in sequences:\n        id_sequence = []\n        for word in sequence:\n            # ignore words with index higher than the max number or words (`num_words`)\n            if word not in word_index.keys() or word_index[word] >= num_words:\n                continue\n            else:\n                id_sequence.append(word_index[word])\n        id_sequences.append(id_sequence)\n    return id_sequences","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"471034a35dc9a9fe3fb72c45edcf05c1970c3ad7","_cell_guid":"ffe91720-f398-4726-b676-daf84c40b653"},"cell_type":"markdown","source":"### RocAucEvaluation with scores for the Training and Validation sets"},{"metadata":{"_uuid":"dc6163ffe79c73deddf3cf6cfd8863278362d5d5","_cell_guid":"80fb7a50-cca1-491e-a89a-0ce3c1c8ffa7","collapsed":true,"trusted":false},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    \n    def __init__(self, validation_data=(), train_data=(), interval=1):\n        \n        super(Callback, self).__init__()        \n        assert isinstance(train_data,(tuple,list)) or len(train_data) == 2, \\\n            \"train_data must be a list or tuple with 2 elements: ´´(X_train, y_train)´´\"        \n        \n        self.interval = interval        \n        self.X_val, self.y_val = validation_data\n        \n        if train_data:\n            self.X_train, self.y_train = train_data\n            self.train_score = True\n            \n    def on_epoch_end(self, epoch, logs={}):\n        \n        if epoch % self.interval == 0:            \n            y_val_pred = self.model.predict(self.X_val, verbose=0)\n            score_val = roc_auc_score(self.y_val, y_val_pred)\n\n            if self.train_score:\n                y_train_pred = self.model.predict(self.X_train, verbose=0)\n                score_train = roc_auc_score(self.y_train, y_train_pred)\n                print(f\"\\n ROC-AUC - epoch: {epoch} - score_train: \"\n                      f\"{score_train:.6f} | score_val: {score_val:.6f}\")\n            else:\n                print(f\"\\n ROC-AUC - epoch: {epoch} - score: {score_val:.6f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecf93116cb8f8e008e338f767bd2a10ab0419845","_cell_guid":"92a6f8ea-1fc0-4afd-b900-4d193a31825e"},"cell_type":"markdown","source":"### Read in our data and replace missing values:"},{"metadata":{"_uuid":"8ab6dad952c65e9afcf16e43c4043179ef288780","scrolled":true,"_cell_guid":"ac2e165b-1f6e-4e69-8acf-5ad7674fafc3","collapsed":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values\n\ny.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1ef893da1874ca8834e39c7237dd976b77891c0","_cell_guid":"1aa900ec-36e0-457f-ace5-a69cea176939","collapsed":true,"trusted":false},"cell_type":"code","source":"sequences_train = text_to_sequence(list_sentences_train)\nword_index = get_word_index(sequences_train)\nsequences_train = apply_indices(sequences_train, max_features, word_index)\nX_w_t, y = augment_toxic(sequences_train, y, window_sizes=window_sizes, maxlen=maxlen, truncated_only=True, non_repeat=True)\n\nsequences_test = text_to_sequence(list_sentences_test)\nsequences_test = apply_indices(sequences_test, max_features, word_index)\nX_w_te = pad_sequences(sequences_test, maxlen=maxlen, truncating='post')\n\n# In this version, `sequences_train` is already padded in `augment_toxic()`\n#X_w_t = pad_sequences(sequences_train, maxlen=maxlen, truncating='post')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00cf574a698fdd49e9098a1d78e864349bdda4d3","_cell_guid":"365635cf-e916-4aba-9195-3b34ae38b350"},"cell_type":"markdown","source":"### Augment or get part the Tokenizer would truncate"},{"metadata":{"_uuid":"690663293ac5db814a41a57ea085f0be60bbb1da","_cell_guid":"b1b362f2-d965-433b-8c79-3f3851be751d","collapsed":true,"trusted":false},"cell_type":"code","source":"## augment data\n#new_comments, new_ys = augment_toxic(list_sentences_train, window_sizes, step_size, \n#                                 maxlen=maxlen, truncated_only=True, non_repeat=True)\n#list_sentences_train = np.concatenate((list_sentences_train, new_comments),axis=0)\n#y = np.concatenate((y, new_ys),axis=0)\n## shuffle data\n#shuffle_index = np.random.permutation(len(y))\n#list_sentences_train, y = list_sentences_train[shuffle_index], y[shuffle_index]\n\n#y.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"467dc9033e37d47f04be9e139c9451442d0dbded","_cell_guid":"04f6db4d-95e0-4a2f-abc6-34e559b32dd6"},"cell_type":"markdown","source":"### Relation between classes"},{"metadata":{"_uuid":"60cd7b52009a36fda8a9c384c897ae1f54bbd00f","_cell_guid":"a28d0971-1f8b-4e9a-b463-b41f2a460443","collapsed":true,"trusted":false},"cell_type":"code","source":"[y[y[:,i]==1].sum(axis=0) for i in range(6)]\npd.DataFrame(data=[y[y[:,i]==1].sum(axis=0) for i in range(6)], columns=list_classes,index=list_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8810c303980f41dbe0543e1c15d35acbdd8428f","_cell_guid":"54a7a34e-6549-45f7-ada2-2173ff2ce5ea"},"cell_type":"markdown","source":"## Word embedding matrix\n### Clean and get word sequences from sentences \nStandard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."},{"metadata":{"_uuid":"20cea54904ac1eece20874e9346905a59a604985","_cell_guid":"7370416a-094a-4dc7-84fa-bdbf469f6579"},"cell_type":"markdown","source":"### Create words embedding matrix\nUse these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."},{"metadata":{"_uuid":"96fc33012e7f07a2169a150c61574858d49a561b","_cell_guid":"4d29d827-377d-4d2f-8582-4a92f9569719","collapsed":true,"trusted":false},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n#emb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"574c91e270add444a7bc8175440274bdd83b7173","_cell_guid":"62acac54-0495-4a26-ab63-2520d05b3e19","collapsed":true,"trusted":false},"cell_type":"code","source":"nb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96fe294a2d3933c3bc660c1b8aac066f93aeeecb","_cell_guid":"4e061675-c698-462a-bee1-4c92e260282c"},"cell_type":"markdown","source":"The reason for us to use `max_features = 20000` and `maxlen = 100` or something close is because the training set has a lot of features and some large comments"},{"metadata":{"_uuid":"addc573fef69408374b9107504d5d8a27b7f765a","_cell_guid":"b1f68d74-a1b8-4776-ab35-5db28e641fcd","collapsed":true,"trusted":false},"cell_type":"code","source":"#original_maxlen = get_sequence_maxlen(list_sentences_train)\n#original_max_features = get_num_words(list_sentences_train)\n#print(f'The original number of features and maximum length in the sequences '\n#      f'are: {original_max_features} features and {original_maxlen} words, respectively')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4795b990dc04791b3ad35617c439fb089259c2","_cell_guid":"47291b07-f828-4396-b1fa-652da9f58348"},"cell_type":"markdown","source":"## Char embedding matrix"},{"metadata":{"_uuid":"71e5dfa68d251b729aaa2e25ec2340c14137a849","scrolled":true,"_cell_guid":"ad5fa06f-6fe0-42de-b3d4-dc12f6e2fe73","collapsed":true,"trusted":false},"cell_type":"code","source":"#embed_char_size = 50\n#original_num_char = get_num_char(list_sentences_train) # how many unique chars to use\n#original_maxlen_char = get_maxlen_char(list_sentences_train) # maximum \n#num_char = 500\n#maxlen_char = 800\n#print(f'original_num_char = {original_num_char}, original_maxlen_char = {original_maxlen_char}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4210f179edf5a2855de86e960589829b832d57bf","scrolled":false,"_cell_guid":"9f0ef8de-9636-46fd-a0e9-c7ea61dd149f","collapsed":true,"trusted":false},"cell_type":"code","source":"#tokenizer_char = Tokenizer(num_words=num_char, char_level=True, lower=False) # Initialize the tokenizer\n#tokenizer_char.fit_on_texts(list_sentences_train) # associate each \"word\" or \"character\" to an indice\n#list_char_tokenized_train = tokenizer_char.texts_to_sequences(list_sentences_train) # get sequence of indices for the training data\n#list_char_tokenized_test = tokenizer_char.texts_to_sequences(list_sentences_test) # get sequences of indices for the test data\n#X_c_t = pad_sequences(list_char_tokenized_train, maxlen=maxlen_char, truncating='post') # pad with zeros\n#X_c_te = pad_sequences(list_char_tokenized_test, maxlen=maxlen_char, truncating='post') # pad with zeros\n\n##########\n#sequences_char_train = text_to_sequence(list_sentences_train, lower=False, char_level=True)\n#char_index = get_word_index(sequences_char_train)\n#sequences_char_train = apply_indices(sequences_char_train, max_features, char_index)\n#X_c_t, _ = augment_toxic(sequences_char_train, y, window_sizes=window_sizes, maxlen=maxlen, truncated_only=True, non_repeat=True)\n\n#sequences_char_test = text_to_sequence(list_sentences_test, lower=False, char_level=True)\n#sequences_char_test = apply_indices(sequences_char_test, max_features, word_index)\n#X_c_te = pad_sequences(sequences_test, maxlen=maxlen, truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e5abd5d1d6d9f622c534484e3ffa7a6a3e8aa12","_cell_guid":"90a5f484-8a6f-4df9-8d71-c705511c7427","collapsed":true,"trusted":false},"cell_type":"code","source":"#indx = 9999\n#for i, j in tokenizer.word_index.items():\n#    if j == indx:\n#        print(i, tokenizer.word_index[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ea37bcca965ff7c67e91b6ff7a320fca78009d1","_cell_guid":"cd301d81-c8b5-41c8-8ac3-052ce261198f","collapsed":true,"trusted":false},"cell_type":"code","source":"#embedding_char_matrix = np.random.normal(size=(num_char, embed_char_size))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7d44407b93bea04eba4ed5f50c9d6da002953ab","_cell_guid":"15966d0a-95a9-40ba-8257-c1726639f254"},"cell_type":"markdown","source":"### Train/Val split"},{"metadata":{"_uuid":"03601d645a7d662cb1642a09e2d5fbfd6a954826","_cell_guid":"1a614e9a-7f26-406a-8771-88b31bda434d","collapsed":true,"trusted":false},"cell_type":"code","source":"#y = y_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93f6dfcbcdd63a80afc24b8bce05beff075a3bb7","_cell_guid":"6955beee-1aeb-4930-bfad-510557f0a46c","collapsed":true,"trusted":false},"cell_type":"code","source":"[X_word_train, X_word_val, y_train, y_val] = train_test_split(X_w_t, y, train_size=0.95)\n#[X_char_train, X_char_val, _, _] = train_test_split(X_c_t, y, train_size=0.95)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eca109f92fc1a9eea82100652a234ca63bc0a53","_cell_guid":"06c2b147-7049-4be4-8269-defaa0fb77e3"},"cell_type":"markdown","source":"### Creating new features"},{"metadata":{"_uuid":"783467d32446374d00188775bf8b4633101e6a1b","_cell_guid":"b33aaeb4-544a-4f37-b61b-8cc7851dc461","collapsed":true,"trusted":false},"cell_type":"code","source":"m_train = X_word_train.shape[0]\n\nX_train_helper = get_helper(list_sentences_train[:m_train])\nX_val_helper = get_helper(list_sentences_train[m_train:])\n\nX_h_te = get_helper(list_sentences_test)\n\nn_helper = X_train_helper.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f39e92539f00073793baa88123d4644870a0b768","_cell_guid":"92fa675e-526c-4c78-a4fb-71ee87bf26d8"},"cell_type":"markdown","source":"### Joining input data in lists"},{"metadata":{"_uuid":"6c7b3104708b326a5cf80a87a23a77bb5d53bc96","_cell_guid":"146c3b28-f44d-45d2-94e1-2d54b069ceb7","collapsed":true,"trusted":false},"cell_type":"code","source":"#input_train = [X_word_train, X_char_train, X_train_helper]\n#input_val = [X_word_val, X_char_val, X_val_helper]\n#input_test = [X_w_te, X_c_te, X_h_te]\n\ninput_train = [X_word_train, X_train_helper]\ninput_val = [X_word_val, X_val_helper]\ninput_test = [X_w_te, X_h_te]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8609620f808e29e2ca3703bf54514e1d30a3085","_cell_guid":"2b687890-eb32-43f7-9e35-f38c4b8a9d7c","collapsed":true,"trusted":false},"cell_type":"code","source":"#input_test[0].shape, input_test[1].shape, input_test[2].shape, input_val[0].shape, input_val[1].shape, input_val[2].shape, input_train[0].shape, input_train[1].shape, input_train[2].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c88443a18dc5c0c276692d6cb4fd6c47c887e1","_cell_guid":"8c481b42-7474-4fbc-b410-92782c6edb45"},"cell_type":"markdown","source":"### Prepare RocAuc callback"},{"metadata":{"_uuid":"21f93569b7dcfedf6bd6571bfc512dd6412e0c80","_cell_guid":"92d4a124-e5f3-49d1-b5e4-e7a0dbc02010","collapsed":true,"trusted":false},"cell_type":"code","source":"ra_val = RocAucEvaluation(\n    validation_data=(input_val, y_val),\n    train_data=(input_train, y_train), \n    interval=1\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b24ad7fc6007b4e4e5b2bbb2d1180ff4ffb731","_cell_guid":"6bf54d9f-beaf-4de5-9ef5-0c9d1355b6f7"},"cell_type":"markdown","source":"### Sample Weights\nCreating weights of the samples that are in some of the toxic classes to higher values and the clean comment samples to a lower value to be applied on the `model.fit` method.\nThis will penalize the loss function in order for it to pay more attention in the minority classes (samples in any of the toxic classes)"},{"metadata":{"_uuid":"e4e2c3efe006334718c2904284d41a974567ffb3","scrolled":true,"_cell_guid":"26e04952-3dad-4a61-aa31-b200b04ae37e","collapsed":true,"trusted":false},"cell_type":"code","source":"#cw = compute_class_weight('balanced', , y)\nsw = compute_sample_weight('balanced', y_train)\nsw = 1/(1+np.exp(-sw))\n#sw[sw == sw.min()] *=.2\n#sw2 = np.array([sw[i]*sw[i] if (sw[i] < 0.8) else sw[i] for i in range(sw.shape[0])])\nsw = sw**3\n#np.unique(sw),np.unique(sw2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237345510bd2e664b5c6983a698d80bac2732bc4","_cell_guid":"f1aeec65-356e-4430-b99d-bb516ec90b09"},"cell_type":"markdown","source":"Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."},{"metadata":{"_uuid":"d91a94757b90798bb4c9b66157884bb8eb3089a3","_cell_guid":"330a2a50-3f73-4e69-ba4b-2c325e31aa28","collapsed":true,"trusted":false},"cell_type":"code","source":"# LSTM for the Word Sequences\ninput_word = Input(shape=(maxlen,), name=\"input_word_sequences\")\nX_word = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_word)\n\nX_word = Bidirectional(LSTM(50, return_sequences=True, recurrent_dropout=0.1))(X_word)\nX_word = Dropout(0.3)(X_word)\n#X_word = BatchNormalization()(X_word)\n\n###X_word = LSTM(10, return_sequences=True, recurrent_dropout=0.1, unroll=False)(X_word)\nX_word = Bidirectional(LSTM(50, return_sequences=True, recurrent_dropout=0.1))(X_word)\nX_word = Dropout(0.3)(X_word)\nX_word = GlobalMaxPool1D()(X_word)\nX_word = BatchNormalization()(X_word)\n\n####{\n## LSTM for the Character Sequences\n#input_char = Input(shape=(maxlen_char,), name=\"input_char_sequences\")\n#X_char = Embedding(num_char, embed_char_size, weights=[embedding_char_matrix])(input_char)\n\n##X_char = Bidirectional(LSTM(20, return_sequences=True, recurrent_dropout=0.1, unroll=True))(X_char)\n##X_char = Dropout(0.3)(X_char)\n##X_char = BatchNormalization()(X_char)\n\n####X_char = LSTM(10, return_sequences=True, recurrent_dropout=0.1, unroll=True)(X_char)\n#X_char = Bidirectional(LSTM(20, return_sequences=True, recurrent_dropout=0.1, unroll=True))(X_char)\n#X_char = Dropout(0.3)(X_char)\n#X_char = GlobalMaxPool1D()(X_char)\n#X_char = BatchNormalization()(X_char)\n\n# Helper Hand Engineered Features\ninput_helper = Input(shape=(n_helper,), name=\"input_helper\")\nX_helper = BatchNormalization()(input_helper)\n\nX_helper = Dense(40, activation=\"relu\")(X_helper)\nX_helper = Dropout(0.3)(X_helper)\n#X_helper = BatchNormalization()(X_helper)\n\nX_helper = Dense(40, activation=\"relu\")(X_helper)\nX_helper = Dropout(0.3)(X_helper)\nX_helper = BatchNormalization()(X_helper)\n\n## Merge all networks\nconcat = Concatenate()([X_word, X_helper])\n#concat = Concatenate()([X_word, X_char, X_helper])\n####}\n#X = Dense(40, activation=\"relu\")(X_word)\nX = Dense(40, activation=\"relu\")(concat)\nX = Dropout(0.2)(X)\nX = BatchNormalization()(X)\nX = Dense(30, activation=\"relu\")(X)\nX = Dropout(0.2)(X)\nX = BatchNormalization()(X)\n\nX = Dense(6, activation=\"sigmoid\")(X)\n\n#model = Model(inputs=[input_word, input_char, input_helper], outputs=X)\nmodel = Model(inputs=[input_word, input_helper], outputs=X)\nopt = optimizers.Adam(\n    lr=0.01,\n    decay=0.0000009\n)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448a7bb845f958319384a1f0524032ad5cd3c027","_cell_guid":"4c8f2789-8eb0-49e6-90e2-381163cadc13","collapsed":true,"trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a0e9ce12e1ff5ea102665e79de23df5caf5802","_cell_guid":"4a624b55-3720-42bc-ad5a-7cefc76d83f6"},"cell_type":"markdown","source":"Now we're ready to fit out model! Use `validation_split` when not submitting."},{"metadata":{"_uuid":"4c9886d588c4dad69b492e56a410b5de0454e04b","_cell_guid":"99de400a-07a0-472c-ae02-afaf70590482","collapsed":true,"trusted":false},"cell_type":"code","source":"# Find the decay you want for the batch size you're using\nmtrain = X_word_train.shape[0]\nbatches = [16,32,64,128,256,512]\nbatch_steps_decay = []\ndecay = 0.0000002\nfor i,b_size in enumerate(batches):\n    steps = mtrain//b_size\n    total_decay = steps * decay\n    batch_steps_decay.append([b_size, steps, total_decay])\nprint(batch_steps_decay)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1558c6b2802fc632edc4510c074555a590efbd8","scrolled":true,"_cell_guid":"333626f1-a838-4fea-af99-0c78f1ef5f5c","collapsed":true,"trusted":false},"cell_type":"code","source":"model.fit(\n    input_train,\n    y_train,\n    batch_size=64,\n    epochs=2,\n    validation_data=(input_val, y_val),\n    sample_weight=sw,\n    callbacks=[ra_val])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf","_cell_guid":"d6fa2ace-aa92-40cf-913f-a8f5d5a4b130"},"cell_type":"markdown","source":"And finally, get predictions for the test set and prepare a submission CSV:"},{"metadata":{"_uuid":"e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0","_cell_guid":"28ce30e3-0f21-48e5-af3c-7e5512c9fbdc","collapsed":true,"trusted":false},"cell_type":"code","source":"y_test = model.predict(input_test, batch_size=1024, verbose=1)\nsample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\nsample_submission[list_classes] = y_test\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b969bab77ab952ecd5abf2abe2596a0e23df251","_cell_guid":"617e974a-57ee-436e-8484-0fb362306db2","collapsed":true,"trusted":false},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7718c6bdb2fa39bf8adf1dc3d13f00bde2bb865d","scrolled":true,"_cell_guid":"983e9fbd-ba03-4132-b198-4e46c064d24c","collapsed":true,"trusted":false},"cell_type":"code","source":"list_sentences_test[0]","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}