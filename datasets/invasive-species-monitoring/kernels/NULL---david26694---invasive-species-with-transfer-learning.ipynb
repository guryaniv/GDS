{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport tensorflow as tf\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\n\nnp.random.seed(42)\n\n# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"e8f43ded580ce21f03bc2369cc9d59c8ce3be2a3","_cell_guid":"5e0d39bb-12d5-4bcb-b280-74fdd136acb0"},"cell_type":"markdown","source":"These are variables to load the v3 inception model that we will use for transfer learning"},{"metadata":{"_uuid":"acf8aef8fc399534875c62688e8bff1d586d2e2b","collapsed":true,"_cell_guid":"2c6cd3b0-d99b-4ae9-b9ed-06f7196b04ec","trusted":true},"cell_type":"code","source":"INCEPTION_PATH = os.path.join(\"../input\", \"v3-inception\")\nINCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, \"inception_v3.ckpt\")","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"d673f90ad4c1837d090cd6544e1df288c9153ffb","_cell_guid":"4cca14c4-206c-4a94-8cdc-aa75f79abcf5"},"cell_type":"markdown","source":"Here we split the labels into the invasive and autoctonous, and in the next cell we create a dictionary that has all the paths of the autoctonous and invasive images, respectively"},{"metadata":{"_uuid":"cdf205e13cc3bdfb27ff892efe63aa5444437987","collapsed":true,"_cell_guid":"edee105c-a559-4398-9eaf-860694ed61fc","trusted":true},"cell_type":"code","source":"labels = pd.read_csv('../input/invasive-species-monitoring/train_labels.csv')\nlabels_invasive = labels[labels['invasive'] == 1]['name']\nlabels_autoctonous = labels[labels['invasive'] == 0]['name']","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"fb549fe79dd5f151ccf4f9bb1490b30559e690c8","collapsed":true,"_cell_guid":"bf7c205e-9864-4c21-a969-e20ae4f54208","trusted":true},"cell_type":"code","source":"image_paths = {}\nimage_paths['autoctonous'] = ['../input//invasive-species-monitoring/train/' + str(label) + '.jpg' for label in (labels_autoctonous.values)] \nimage_paths['invasive'] = ['../input//invasive-species-monitoring/train/' + str(label) + '.jpg' for label in (labels_invasive.values)] ","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"f3fdb3e3989c75fd8d6fd008192e75904407574e","_cell_guid":"90f49d58-a4af-4e8c-858d-c7277ba52eea"},"cell_type":"markdown","source":"We can plot some of the images and its dimensions"},{"metadata":{"_uuid":"4317f1fb81fe65f0f6a4cffff8f1f631cce548f3","_cell_guid":"52b297be-60d3-4fa5-a01f-f47ca21be13d","trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\n\nn_examples_per_class = 6\nchannels = 3\n\nspecies_classes = ['autoctonous', 'invasive']\n\nfor species in species_classes:\n    print(\"Class:\", species)\n    plt.figure(figsize=(10,5))\n    for index, example_image_path in enumerate(image_paths[species][:n_examples_per_class]):\n        example_image = mpimg.imread(example_image_path)[:, :, :channels]\n        plt.subplot(100 + n_examples_per_class * 10 + index + 1)\n        plt.title(\"{}x{}\".format(example_image.shape[1], example_image.shape[0]))\n        plt.imshow(example_image)\n        plt.axis(\"off\")\n    plt.show()","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"d181b8b157300de9bf48eafa01eaf2794a6d77ed","_cell_guid":"a142a2ce-c929-4cba-8455-85bbf24e1581"},"cell_type":"markdown","source":"This is a function to preprare an image in a standard format that the neural net will recieve as an input, and in addition it performs data augmentation"},{"metadata":{"_uuid":"d1e92dfe4031acac2510a86165aa962a9c7a9784","collapsed":true,"_cell_guid":"57904a41-f70c-4691-9fdd-5f38a30be5db","trusted":true},"cell_type":"code","source":"from scipy.misc import imresize\nfrom skimage.transform import resize\n\ndef prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n\n    # First, let's find the largest bounding box with the target size ratio that fits within the image\n    height = image.shape[0]\n    width = image.shape[1]\n    image_ratio = width / height\n    target_image_ratio = target_width / target_height\n    crop_vertically = image_ratio < target_image_ratio\n    crop_width = width if crop_vertically else int(height * target_image_ratio)\n    crop_height = int(width / target_image_ratio) if crop_vertically else height\n        \n    # Now let's shrink this bounding box by a random factor (dividing the dimensions by a random number\n    # between 1.0 and 1.0 + `max_zoom`.\n    resize_factor = np.random.rand() * max_zoom + 1.0\n    crop_width = int(crop_width / resize_factor)\n    crop_height = int(crop_height / resize_factor)\n    \n    # Next, we can select a random location on the image for this bounding box.\n    x0 = np.random.randint(0, width - crop_width)\n    y0 = np.random.randint(0, height - crop_height)\n    x1 = x0 + crop_width\n    y1 = y0 + crop_height\n    \n    # Let's crop the image using the random bounding box we built.\n    image = image[y0:y1, x0:x1]\n\n    # Let's also flip the image horizontally with 50% probability:\n    if np.random.rand() < 0.5:\n        image = np.fliplr(image)\n\n    # Now, let's resize the image to the target dimensions.\n    image = resize(image, (target_width, target_height), mode = 'constant')\n    # Finally, let's ensure that the colors are represented as\n    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n    return image.astype(np.float32) #/ 255","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"99037e8e29992efbf5b8644eacb43df238db7668","_cell_guid":"ade668c6-b93e-45e9-b903-7e8521ba75f1"},"cell_type":"markdown","source":"Global variables regarding shape of the tensors:"},{"metadata":{"_uuid":"56d8c94e0a1cd03f27813f4cc15bcbc38281bdff","collapsed":true,"_cell_guid":"3886b521-5b30-45dc-baf7-35c0f47c8f32","trusted":true},"cell_type":"code","source":"channels = 3\nheight = 299\nwidth = 299","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"9cc71c7922e4a16e1794be1150d4950bf10586a6","_cell_guid":"55a884d5-cecc-489f-86f1-df3136fd3a2b"},"cell_type":"markdown","source":"We can plot the different augmentations performed on an image"},{"metadata":{"_uuid":"4eeda87b0c44b2b3c8a611ec923c34002b192459","_cell_guid":"949ad710-045a-46c9-8c18-e2ccace7888a","trusted":true},"cell_type":"code","source":"example_image_path = image_paths['invasive'][0]\nexample_image = mpimg.imread(example_image_path)[:, :, :channels]\nreset_graph()\n\nrows = 2\ncols = 3\n\nplt.figure(figsize=(14, 8))\nfor row in range(rows):\n    for col in range(cols):\n        prepared_image = prepare_image(example_image)\n        plt.subplot(rows, cols, row * cols + col + 1)\n        plt.title(\"{}x{}\".format(prepared_image.shape[1], prepared_image.shape[0]))\n        plt.imshow(prepared_image)\n        plt.axis(\"off\")\nplt.show()","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"3bc5b43fe56fffa6b7d4978cb10e536c01fba592","_cell_guid":"0ec4c28e-537b-4175-bfa4-bf4eed284c99"},"cell_type":"markdown","source":"We create a dictionary that maps the names of the outputs into binary format"},{"metadata":{"_uuid":"5eebc4f968e166964fa801cf170fdce3f3d45d12","_cell_guid":"778ac8d6-5a68-43e9-adf6-54b7a1895da7","trusted":true},"cell_type":"code","source":"species_class_ids = {species_class: index for index, species_class in enumerate(species_classes)}\nspecies_class_ids","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"d250d70146c69b1731fffcf6c073d148e70b46a3","_cell_guid":"a9378fc9-9721-4d27-b513-f0e0513f9c6f"},"cell_type":"markdown","source":"And a list that has as elements pairs of paths, labels (in binary)."},{"metadata":{"_uuid":"5779a570e11b35adbff9891fe9109b02d389100f","collapsed":true,"_cell_guid":"d589424d-7768-49dd-8616-9c4263ca173f","trusted":true},"cell_type":"code","source":"species_paths_and_classes = []\nfor species, paths in image_paths.items():\n    for path in paths:\n        species_paths_and_classes.append((path, species_class_ids[species]))","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"e5c7da30b6e020df318c8543a91a259aa794327e","collapsed":true,"_cell_guid":"6206896c-08ee-4cd6-91cd-78ab0e488a08","trusted":true},"cell_type":"code","source":"val_ratio = 0.1\ntrain_size = int(len(species_paths_and_classes) * (1 - val_ratio))\n\nnp.random.shuffle(species_paths_and_classes)\n\nspecies_paths_and_classes_train = species_paths_and_classes[:train_size]\nspecies_paths_and_classes_val = species_paths_and_classes[train_size:]","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"81fbcdb6ed0d6d96cf5832fe22f3aa844267f6bf","collapsed":true,"_cell_guid":"5dafbde8-9c2c-40ba-8abd-c26a9bbb206d","trusted":true},"cell_type":"code","source":"from random import sample\n\ndef prepare_batch(species_paths_and_classes, batch_size):\n    batch_paths_and_classes = sample(species_paths_and_classes, batch_size)\n    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n    prepared_images = [prepare_image(image) for image in images]\n    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n    return X_batch, y_batch\n\ndef prepare_batch_indices(species_paths_and_classes, indices):\n    batch_paths_and_classes = [species_paths_and_classes[i] for i in indices]\n    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n    prepared_images = [prepare_image(image) for image in images]\n    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n    return X_batch, y_batch","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"a143e77d9ebcfb6c5cbe2e39d00a66817ec96a7d","collapsed":true,"_cell_guid":"bc2064c6-ab1d-419a-8f2f-31185adf77f4","trusted":true},"cell_type":"code","source":"def prepare_batch_total(species_paths_and_classes):\n    batch_paths_and_classes = species_paths_and_classes\n    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n    prepared_images = [prepare_image(image) for image in images]\n    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n    return X_batch, y_batch\n\nX_val, y_val = prepare_batch_total(species_paths_and_classes_val)","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"fd22113fce6f4902b4d94a5b2806b288d9bbc7fb","collapsed":true,"_cell_guid":"4c824970-f9b7-44ce-842e-d342739d1de5","trusted":true},"cell_type":"code","source":"#This function creates an augmented validation set. It does so by creating a list.\n#Every element of the list is a different version of the validation data.\n\ndef create_test_data(species_paths_and_classes_val, tta_len):\n    val_data_tta = []\n    for i in range(tta_len):\n        val_data_tta.append(prepare_batch_total(species_paths_and_classes_val))\n    return val_data_tta","execution_count":67,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3529688a92ff789e99fc0ddd34957f3576db1051"},"cell_type":"code","source":"val_data = create_test_data(species_paths_and_classes_val, 5)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"06a754117903c707f7efdc1b0febd5f2fab840f5"},"cell_type":"code","source":"def computeAugmentedProbabilities(Y_proba, val_data):\n    probabilities = np.zeros((len(val_data[0][1]), len(val_data)))\n    i = 0\n    for X_val_tta, y_val_tta in val_data:\n        probabilities[:, i] = Y_proba.eval(feed_dict={X: X_val_tta, y: y_val_tta})\n        i += 1\n    return np.mean(probabilities, axis = 1)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"87827eb1302c4fac7f6b2bb59be122f2e2f70c97"},"cell_type":"code","source":"def computeAugmentedAccuracy(Y_proba, val_data):\n    probs = computeAugmentedProbabilities(Y_proba, val_data)\n    labels = val_data[0][1]\n    predictions = probs > .5\n    return ((predictions == labels).sum())/len(labels)","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"e80d8c439b39327ae0fdbd5e2903a2e7752d7962","_cell_guid":"51c81e0f-1bf6-4fd6-83cf-563b01b78a0f"},"cell_type":"markdown","source":"We start with our tf code. We load the inception v3 network."},{"metadata":{"_uuid":"9c5832345a0a9e1c90ca3c051f1d0b061a72fe6f","_cell_guid":"9c4a02bf-a5fa-4ad2-8c73-007cc8a6c485","trusted":true},"cell_type":"code","source":"from tensorflow.contrib.slim.nets import inception\nimport tensorflow.contrib.slim as slim\n\nreset_graph()\n\nX = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\ntraining = tf.placeholder_with_default(False, shape=[])\nwith slim.arg_scope(inception.inception_v3_arg_scope()):\n    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n\ninception_saver = tf.train.Saver()","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"1cdb975f4f3316cf625fece0d75d631f25eccd4f","_cell_guid":"6fdd332b-be68-440a-88ed-e0a03c0f4d9d"},"cell_type":"markdown","source":"Here we create a node containing the layer right before the logits."},{"metadata":{"_uuid":"fcecb32067d6c781691a0b02d15f2f9a36e4f4d7","collapsed":true,"_cell_guid":"0472e14d-09b5-4b86-92cb-30cd18042aad","trusted":true},"cell_type":"code","source":"prelogits = tf.squeeze(end_points[\"PreLogits\"], axis=[1, 2])","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"6bb60c09ce6af39a137cc8fc6d964050ef28fbf9","_cell_guid":"63784194-a056-4b45-8a42-caa8698dc570"},"cell_type":"markdown","source":"We create a dnn structure with two hidden layers before the output layer."},{"metadata":{"_uuid":"5b21123afba0191a2a36ecd98ba0ca7eabd8a1b7","collapsed":true,"_cell_guid":"e16830a7-9d15-4c93-9ea6-1b554fdfecb2","trusted":true},"cell_type":"code","source":"n_outputs_1 = 100\n#n_outputs_2 = 10\nn_outputs_3 = 1\nwith tf.name_scope(\"new_output_layer\"):\n    species_fc1 = tf.layers.dense(prelogits, n_outputs_1, name=\"species_fc1\", activation=tf.nn.relu)\n    #species_fc2 = tf.layers.dense(species_fc1, n_outputs_2, name=\"species_fc2\", activation=tf.nn.relu)\n    species_logits = tf.layers.dense(species_fc1, n_outputs_3, name=\"species_logits\")\n    reshaped_logits = tf.reshape(species_logits, shape = [-1])\n    Y_proba = tf.nn.sigmoid(reshaped_logits, name=\"Y_proba\")","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"ddcac6cd8ea073ab97456a93c224bc8c73cd906a","collapsed":true,"_cell_guid":"96e2e73b-45af-4658-88d2-43d31ced376d","trusted":true},"cell_type":"code","source":"y = tf.placeholder(tf.float32, shape=[None])\n\nwith tf.name_scope(\"train\"):\n    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=reshaped_logits, labels=y)\n    loss = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer()\n    species_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"species_logits\")\n    training_op = optimizer.minimize(loss, var_list=species_vars)\n\nwith tf.name_scope(\"eval\"):\n    correct_prediction = tf.equal(tf.to_int32(Y_proba > 0.5),\n                                  tf.cast(y, tf.int32))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    \n\nwith tf.name_scope(\"init_and_save\"):\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd1f51bab25e113ee12e7aa1cdfccadbe2b5a9d8","_cell_guid":"0fdcc2fa-17fa-4c2b-aeb0-411f521d2ae4","scrolled":true,"trusted":true},"cell_type":"code","source":"n_epochs = 10\nbatch_size = 64\nn_iterations_per_epoch = len(species_paths_and_classes_train) // batch_size\n\nwith tf.Session() as sess:\n    init.run()\n    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n\n    for epoch in range(n_epochs):\n        print(\"Epoch\", epoch, end=\"\")\n        for iteration in tqdm(range(n_iterations_per_epoch)):\n            #print(\".\", end=\"\")\n            X_batch, y_batch = prepare_batch(species_paths_and_classes_train, batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n            if iteration % 10 == 0:\n                acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n                print(\"Train accuracy: \", acc_train)\n        acc_val = computeAugmentedAccuracy(Y_proba, val_data)\n        print('Epoch:', epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n        save_path = saver.save(sess, \"./my_species_model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c918d7839127e628e0130e0715b79297f1c4003","collapsed":true,"_cell_guid":"492578a9-a88b-4095-af42-c95c360b9a1d","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}