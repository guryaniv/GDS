{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guided Data Science\n",
    "================\n",
    "#### Next-Line Recommendation chatbot for Data-Scientists\n",
    "#### Recommendation Engine\n",
    "\n",
    "<p>In this notebook you can test the model for yourself and see how it works... </p>\n",
    "    <p>For a user input (code cell), we transform it to a masked representation, get its workflow stage (see @LINK), and call the relavant trained model accordingly to get a recommendation for the next line of code. </p>\n",
    "<p>To see how we built the model and trained it, check out @LINK </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../data_gathering')\n",
    "from masking import init_trans_dict\n",
    "from masking import parse_imports_to_trans_dict\n",
    "from masking import mask_source\n",
    "from masking import unmask_source\n",
    "from masking import consts\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "save_dir = os.path.join(\"save\")\n",
    "corpus_name = \"cells\"\n",
    "corpus = os.path.join(\"data\", corpus_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we'll create the vocabulary for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pairs = \"../Data/Import.tsv\"\n",
    "lo_pairs = \"../Data/Load.tsv\"\n",
    "ex_pairs = \"../Data/Explore.tsv\"\n",
    "pr_pairs = \"../Data/Prep.tsv\"\n",
    "tr_pairs = \"../Data/Train.tsv\"\n",
    "ev_pairs = \"../Data/Eval.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 10652 sentence pairs\n",
      "Trimmed to 10652 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 2921\n",
      "Load Data Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 37624 sentence pairs\n",
      "Trimmed to 37624 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 6506\n",
      "Data Exploration Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 133369 sentence pairs\n",
      "Trimmed to 133369 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 15783\n",
      "Data Preparation Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 91557 sentence pairs\n",
      "Trimmed to 91557 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 20163\n",
      "Model Training and parameter Tuning Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 33162 sentence pairs\n",
      "Trimmed to 33162 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 11258\n",
      "Model Evaluation Cells:\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 73514 sentence pairs\n",
      "Trimmed to 73514 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 16891\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100 #30  # Maximum code line length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, strip, and remove special chars\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([!?])\", r\" \\1\", s) # .\n",
    "    s = re.sub(r\"\\\\[rn]+\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9.!=?_]+\", r\" \", s) # 0-9\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    file = open(datafile, encoding='utf-8')\n",
    "    next(file) #skip header line\n",
    "    lines = file.read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    try:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs for each model\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "print(\"Import Cells:\")\n",
    "Import_voc, Import_pairs = loadPrepareData(corpus, corpus_name, im_pairs, save_dir)\n",
    "print(\"Load Data Cells:\")\n",
    "Load_voc, Load_pairs = loadPrepareData(corpus, corpus_name, lo_pairs, save_dir)\n",
    "print(\"Data Exploration Cells:\")\n",
    "Explore_voc, Explore_pairs = loadPrepareData(corpus, corpus_name, ex_pairs, save_dir)\n",
    "print(\"Data Preparation Cells:\")\n",
    "Prep_voc, Prep_pairs = loadPrepareData(corpus, corpus_name, pr_pairs, save_dir)\n",
    "print(\"Model Training and parameter Tuning Cells:\")\n",
    "Train_voc, Train_pairs = loadPrepareData(corpus, corpus_name, tr_pairs, save_dir)\n",
    "print(\"Model Evaluation Cells:\")\n",
    "Eval_voc, Eval_pairs = loadPrepareData(corpus, corpus_name, ev_pairs, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define functions to load and use our trained models:\n",
    "\n",
    "<i> note: For more information see our model training process</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the relavant model according to the workflow stage\n",
    "def load_model(workflow_stage, workflow_stage_voc):\n",
    "    # Configure models\n",
    "    model_name = 'cb_model'\n",
    "    attn_model = 'dot'\n",
    "    hidden_size = 500\n",
    "    encoder_n_layers = 2\n",
    "    decoder_n_layers = 2\n",
    "    dropout = 0.1\n",
    "    batch_size = 64\n",
    "\n",
    "    # Set checkpoint to load from - choose model according to workflow stage\n",
    "    loadFilename = \"./Models/\"+workflow_stage\n",
    "    \n",
    "    #set voacbulary according to workflow stage\n",
    "    voc = workflow_stage_voc\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "    print('Building encoder and decoder ...')\n",
    "    # Initialize word embeddings\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    # load embeddings\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "    # Initialize encoder & decoder models\n",
    "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "    decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "    # Use appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    print('Models built and ready to go!')\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each trained model size is more than 100MB so we split them to 14 rar files...\n",
    "Run this cell to extract the trained models (<b>if not already extracted</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consts.unrar_trained_models();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Load Data Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Data Exploration Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Data Preparation Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Model Training and parameter Tuning Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Model Evaluation Model:\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(12164, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=12164, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set encoder decoder for each stage model\n",
    "print(\"Import Model:\")\n",
    "encoder_Import, decoder_Import = load_model(\"Import.tar\", Import_voc)\n",
    "print(\"Load Data Model:\")\n",
    "encoder_Load, decoder_Load = load_model(\"Load.tar\", Load_voc)\n",
    "print(\"Data Exploration Model:\")\n",
    "encoder_Explore, decoder_Explore = load_model(\"Explore.tar\", Explore_voc)\n",
    "print(\"Data Preparation Model:\")\n",
    "encoder_Prep, decoder_Prep = load_model(\"Prep.tar\", Prep_voc)\n",
    "print(\"Model Training and parameter Tuning Model:\")\n",
    "encoder_Train, decoder_Train = load_model(\"Train.tar\", Train_voc)\n",
    "print(\"Model Evaluation Model:\")\n",
    "encoder_Eval, decoder_Eval = load_model(\"Eval.tar\", Eval_voc)\n",
    "\n",
    "# Set dropout layers to eval mode (for training see previous notebook)\n",
    "encoder_Import.eval()\n",
    "decoder_Import.eval()\n",
    "encoder_Load.eval()\n",
    "decoder_Load.eval()\n",
    "encoder_Explore.eval()\n",
    "decoder_Explore.eval()\n",
    "encoder_Prep.eval()\n",
    "decoder_Prep.eval()\n",
    "encoder_Train.eval()\n",
    "decoder_Train.eval()\n",
    "encoder_Eval.eval()\n",
    "decoder_Eval.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers] # added self. to avoid 'undefined'\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_model():\n",
    "    # load the trained model (not needed if you train again)\n",
    "    with open('../Classification/tokenizer.pickle', 'rb') as handle:\n",
    "        load_tokenizer = pickle.load(handle)\n",
    "\n",
    "    json_file = open('../Classification/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    load_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    load_model.load_weights(\"../Classification/model.h5\")\n",
    "\n",
    "    return load_model, load_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = init_trans_dict()\n",
    "\n",
    "class_model, class_tokenizer = load_classifier_model()\n",
    "labels_arr = ['Load', 'Prep', 'Train', 'Eval', 'Explore', 'Import']\n",
    "\n",
    "def get_workflow_stage(cell, model, tokenizer, labels):\n",
    "    seq = tokenizer.texts_to_sequences([cell])\n",
    "    padded = pad_sequences(seq, maxlen=120)\n",
    "    pred = model.predict(padded)\n",
    "    return labels[np.argmax(pred)]\n",
    "\n",
    "# use masking and get the summed representation of a cell, also fills the variables dictionary\n",
    "def get_summed_rep(cell, var_dict):\n",
    "    parse_imports_to_trans_dict(cell, var_dict)\n",
    "    summed_rep = mask_source(cell, var_dict)[1]\n",
    "    return summed_rep\n",
    "\n",
    "# get Model output (generalized) for input sentence (summed representation)\n",
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "# gets the output tokens and turns them to a next-line reccomendation, using the var_dict\n",
    "def specificalization(masked, var_dict):\n",
    "    #turn vars to their mapping in the dict\n",
    "    recommendation = unmask_source(masked, var_dict)\n",
    "    return recommendation\n",
    "\n",
    "# get an ready-to-execute next line recommendation for input_sentence\n",
    "def get_recommendation(input_sentence, prints=False):\n",
    "    if prints:\n",
    "        print(\"Getting workflow stage for input cell...\")\n",
    "    stage = get_workflow_stage(input_sentence, class_model, class_tokenizer, labels_arr) # get worklow stage\n",
    "    if prints:\n",
    "        print(\"Worklfow stage is:\" + stage +\"\\nSetting model accordingly...\")\n",
    "    \n",
    "    # set encoder,decoder,voc according to workflow stage (choose relevant model)\n",
    "    if stage == \"Import\":\n",
    "        encoder, decoder = encoder_Import, decoder_Import\n",
    "        voc = Import_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Import.\")\n",
    "    elif stage == \"Load\":\n",
    "        encoder, decoder = encoder_Load, decoder_Load\n",
    "        voc = Load_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Load.\")\n",
    "    elif stage == \"Explore\":\n",
    "        encoder, decoder = encoder_Explore, decoder_Explore\n",
    "        voc = Explore_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Explore.\")\n",
    "    elif stage == \"Prep\":\n",
    "        encoder, decoder = encoder_Prep, decoder_Prep\n",
    "        voc = Prep_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Prep.\")\n",
    "    elif stage == \"Train\":\n",
    "        encoder, decoder = encoder_Train, decoder_Train\n",
    "        voc = Train_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Train.\")\n",
    "    elif stage == \"Eval\":\n",
    "        encoder, decoder = encoder_Eval, decoder_Eval\n",
    "        voc = Eval_voc\n",
    "        if prints:\n",
    "            print(\"Encoder, Decoder, Voc set to Eval.\")\n",
    "    else:\n",
    "        print(\"Error getting workflow stage\")\n",
    "        \n",
    "    searcher = GreedySearchDecoder(encoder, decoder) # set searcher\n",
    "    \n",
    "    if prints:\n",
    "        print(\"Getting Masked summed representation...\")\n",
    "    input_sentence = get_summed_rep(input_sentence, var_dict) # Normalize sentence\n",
    "    if prints:\n",
    "        print(\"Masked cell: \"+input_sentence+\"\\nGetting next-line recommendation from model...\")\n",
    "    input_sentence = normalizeString(input_sentence) ######################################\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) # Evaluate sentence\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    output_words = ''.join(output_words)\n",
    "    if prints:\n",
    "        print(\"Generalized Recommendation: \"+output_words+\"\\nSpecificalizing...\")\n",
    "    recommendation = specificalization(output_words, var_dict) # adjust recommendation to user\n",
    "    return recommendation\n",
    "    \n",
    "# chat with the chatbot (input from user)\n",
    "def chat(prints=False):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "                \n",
    "            recommendation = get_recommendation(input_sentence, prints)\n",
    "            print('Bot: ', recommendation)\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word or masking failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEMO\n",
    "let's see some example recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DEMO - TODO: add good examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can <b>try it out</b> yourself, \"chat\" with the chatbot. Insert your cell of code and get a next-line recommendation:\n",
    "\n",
    "<i>Note: call chat(True) to see the entire process or chat() to just get a recommendation</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> df = pd.read_csv('path.csv')\n",
      "Getting workflow stage for input cell...\n",
      "Worklfow stage is:Load\n",
      "Setting model accordingly...\n",
      "Encoder, Decoder, Voc set to Load.\n",
      "Getting Masked summed representation...\n",
      "Masked cell: var0=pandas.read_csv \n",
      "Getting next-line recommendation from model...\n",
      "Generalized Recommendation: var1=pandas.read_csv\n",
      "Specificalizing...\n",
      "['var1=pandas.read_csv']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ca157a3cde7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9da8879f0c10>\u001b[0m in \u001b[0;36mchat\u001b[1;34m(prints)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'q'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'quit'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mrecommendation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_recommendation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bot: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecommendation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-9da8879f0c10>\u001b[0m in \u001b[0;36mget_recommendation\u001b[1;34m(input_sentence, prints)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprints\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Generalized Recommendation: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\nSpecificalizing...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mrecommendation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecificalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# adjust recommendation to user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrecommendation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-9da8879f0c10>\u001b[0m in \u001b[0;36mspecificalization\u001b[1;34m(masked, var_dict)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mspecificalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m#turn vars to their mapping in the dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mrecommendation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munmask_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrecommendation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\GDS\\guided-ds\\data_gathering\\masking.py\u001b[0m in \u001b[0;36munmask_source\u001b[1;34m(mask, trans_dict)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mmod_as\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_search_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0munmask\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvar_u\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod_as\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# expression line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mvar_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "chat(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "tp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
