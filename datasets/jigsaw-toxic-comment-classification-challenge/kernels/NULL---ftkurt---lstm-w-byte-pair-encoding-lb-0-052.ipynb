{"cells": [{"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import time"], "metadata": {"collapsed": true, "_uuid": "e09d75288ecb7f8cb435555bd7dfd2f86b1a8283", "_cell_guid": "7f863db1-de97-4196-bcff-9c837ad9d3aa"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "metadata": {"collapsed": true, "_uuid": "c61628f644424235fe988cb673f5080421f5cfbd", "_cell_guid": "6fc0e4d9-12ad-4ed8-9687-ee334afe1813"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from keras.models import Model\n", "from keras.layers import Dense, Embedding, Input\n", "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D\n", "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n", "from keras.layers.merge import Concatenate\n", "from keras.preprocessing import text, sequence\n", "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint"], "metadata": {"_kg_hide-input": false, "collapsed": true, "_uuid": "25610729d9558038fde5cd4d4fa80b56681eb9fa", "_cell_guid": "21f248f1-e559-49b7-a1dd-95e66f8ebb6c"}}, {"cell_type": "markdown", "source": ["# Byte-Pair Encoding Implementation"], "metadata": {"_uuid": "3a18059265ab65b5d4ff4b428e7d564123d657fe", "_cell_guid": "59c4ab24-5cfa-42fa-bff6-968ba166993c"}}, {"cell_type": "markdown", "source": ["Since there is no library for BPE in this Kaggle Docker, and it's also not possible to install new libraries, I manually imported a minimized version of implementation on [this repository](https://github.com/soaxelbrooke/python-bpe). And yes, I know this is a dirty way to do it. Btw, for those interested, you can install it like below.\n", "\n", "    $ pip install git+https://github.com/soaxelbrooke/python-bpe.git@master"], "metadata": {"_uuid": "36a5e48269fe56316e24f8e9fa001ab492cbeab3", "_cell_guid": "d8b127c1-0950-44ae-945b-f93f155179ed"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Source: https://raw.githubusercontent.com/soaxelbrooke/python-bpe/master/bpe/encoder.py\n", "# coding=utf-8\n", "\"\"\" An encoder which learns byte pair encodings for white-space separated text.  Can tokenize, encode, and decode. \"\"\"\n", "from collections import Counter\n", "\n", "try:\n", "    from typing import Dict, Iterable, Callable, List, Any, Iterator\n", "except ImportError:\n", "    pass\n", "\n", "from nltk.tokenize import wordpunct_tokenize\n", "from tqdm import tqdm\n", "import toolz\n", "import json\n", "\n", "DEFAULT_EOW = '__eow'\n", "DEFAULT_SOW = '__sow'\n", "DEFAULT_UNK = '__unk'\n", "DEFAULT_PAD = '__pad'\n", "\n", "\n", "class Encoder:\n", "    \"\"\" Encodes white-space separated text using byte-pair encoding.  See https://arxiv.org/abs/1508.07909 for details.\n", "    \"\"\"\n", "\n", "    def __init__(self, vocab_size=8192, pct_bpe=0.2, word_tokenizer=None,\n", "                 silent=True, ngram_min=2, ngram_max=2, required_tokens=None, strict=False, \n", "                 EOW=DEFAULT_EOW, SOW=DEFAULT_SOW, UNK=DEFAULT_UNK, PAD=DEFAULT_PAD):\n", "        if vocab_size < 1:\n", "            raise ValueError('vocab size must be greater than 0.')\n", "\n", "        self.EOW = EOW\n", "        self.SOW = SOW\n", "        self.eow_len = len(EOW)\n", "        self.sow_len = len(SOW)\n", "        self.UNK = UNK\n", "        self.PAD = PAD\n", "        self.required_tokens = list(set(required_tokens or []).union({self.UNK, self.PAD}))\n", "        self.vocab_size = vocab_size\n", "        self.pct_bpe = pct_bpe\n", "        self.word_vocab_size = max([int(vocab_size * (1 - pct_bpe)), len(self.required_tokens or [])])\n", "        self.bpe_vocab_size = vocab_size - self.word_vocab_size\n", "        self.word_tokenizer = word_tokenizer if word_tokenizer is not None else wordpunct_tokenize\n", "        self.custom_tokenizer = word_tokenizer is not None\n", "        self.word_vocab = {}  # type: Dict[str, int]\n", "        self.bpe_vocab = {}  # type: Dict[str, int]\n", "        self.inverse_word_vocab = {}  # type: Dict[int, str]\n", "        self.inverse_bpe_vocab = {}  # type: Dict[int, str]\n", "        self._progress_bar = iter if silent else tqdm\n", "        self.ngram_min = ngram_min\n", "        self.ngram_max = ngram_max\n", "        self.strict = strict\n", "\n", "    def mute(self):\n", "        \"\"\" Turn on silent mode \"\"\"\n", "        self._progress_bar = iter\n", "\n", "    def unmute(self):\n", "        \"\"\" Turn off silent mode \"\"\"\n", "        self._progress_bar = tqdm\n", "\n", "    def byte_pair_counts(self, words):\n", "        # type: (Encoder, Iterable[str]) -> Iterable[Counter]\n", "        \"\"\" Counts space separated token character pairs:\n", "            [('T h i s </w>', 4}] -> {'Th': 4, 'hi': 4, 'is': 4}\n", "        \"\"\"\n", "        for token, count in self._progress_bar(self.count_tokens(words).items()):\n", "            bp_counts = Counter()  # type: Counter\n", "            for ngram in token.split(' '):\n", "                bp_counts[ngram] += count\n", "            for ngram_size in range(self.ngram_min, min([self.ngram_max, len(token)]) + 1):\n", "                ngrams = [''.join(ngram) for ngram in toolz.sliding_window(ngram_size, token.split(' '))]\n", "\n", "                for ngram in ngrams:\n", "                    bp_counts[''.join(ngram)] += count\n", "\n", "            yield bp_counts\n", "\n", "    def count_tokens(self, words):\n", "        # type: (Encoder, Iterable[str]) -> Dict[str, int]\n", "        \"\"\" Count tokens into a BPE vocab \"\"\"\n", "        token_counts = Counter(self._progress_bar(words))\n", "        return {' '.join(token): count for token, count in token_counts.items()}\n", "\n", "    def learn_word_vocab(self, sentences):\n", "        # type: (Encoder, Iterable[str]) -> Dict[str, int]\n", "        \"\"\" Build vocab from self.word_vocab_size most common tokens in provided sentences \"\"\"\n", "        word_counts = Counter(word for word in toolz.concat(map(self.word_tokenizer, sentences)))\n", "        for token in set(self.required_tokens or []):\n", "            word_counts[token] = int(2**63)\n", "        sorted_word_counts = sorted(word_counts.items(), key=lambda p: -p[1])\n", "        return {word: idx for idx, (word, count) in enumerate(sorted_word_counts[:self.word_vocab_size])}\n", "\n", "    def learn_bpe_vocab(self, words):\n", "        # type: (Encoder, Iterable[str]) -> Dict[str, int]\n", "        \"\"\" Learns a vocab of byte pair encodings \"\"\"\n", "        vocab = Counter()  # type: Counter\n", "        for token in {self.SOW, self.EOW}:\n", "            vocab[token] = int(2**63)\n", "        for idx, byte_pair_count in enumerate(self.byte_pair_counts(words)):\n", "            for byte_pair, count in byte_pair_count.items():\n", "                vocab[byte_pair] += count\n", "\n", "            if (idx + 1) % 10000 == 0:\n", "                self.trim_vocab(10 * self.bpe_vocab_size, vocab)\n", "\n", "        sorted_bpe_counts = sorted(vocab.items(), key=lambda p: -p[1])[:self.bpe_vocab_size]\n", "        return {bp: idx + self.word_vocab_size for idx, (bp, count) in enumerate(sorted_bpe_counts)}\n", "\n", "    def fit(self, text):\n", "        # type: (Encoder, Iterable[str]) -> None\n", "        \"\"\" Learn vocab from text. \"\"\"\n", "        _text = [l.lower().strip() for l in text]\n", "\n", "        # First, learn word vocab\n", "        self.word_vocab = self.learn_word_vocab(_text)\n", "\n", "        remaining_words = [word for word in toolz.concat(map(self.word_tokenizer, _text))\n", "                           if word not in self.word_vocab]\n", "        self.bpe_vocab = self.learn_bpe_vocab(remaining_words)\n", "\n", "        self.inverse_word_vocab = {idx: token for token, idx in self.word_vocab.items()}\n", "        self.inverse_bpe_vocab = {idx: token for token, idx in self.bpe_vocab.items()}\n", "\n", "    @staticmethod\n", "    def trim_vocab(n, vocab):\n", "        # type: (int, Dict[str, int]) -> None\n", "        \"\"\"  Deletes all pairs below 10 * vocab size to prevent memory problems \"\"\"\n", "        pair_counts = sorted(vocab.items(), key=lambda p: -p[1])\n", "        pairs_to_trim = [pair for pair, count in pair_counts[n:]]\n", "        for pair in pairs_to_trim:\n", "            del vocab[pair]\n", "\n", "    def subword_tokenize(self, word):\n", "        # type: (Encoder, str) -> List[str]\n", "        \"\"\" Tokenizes inside an unknown token using BPE \"\"\"\n", "        end_idx = min([len(word), self.ngram_max])\n", "        sw_tokens = [self.SOW]\n", "        start_idx = 0\n", "\n", "        while start_idx < len(word):\n", "            subword = word[start_idx:end_idx]\n", "            if subword in self.bpe_vocab:\n", "                sw_tokens.append(subword)\n", "                start_idx = end_idx\n", "                end_idx = min([len(word), start_idx + self.ngram_max])\n", "            elif len(subword) == 1:\n", "                sw_tokens.append(self.UNK)\n", "                start_idx = end_idx\n", "                end_idx = min([len(word), start_idx + self.ngram_max])\n", "            else:\n", "                end_idx -= 1\n", "\n", "        sw_tokens.append(self.EOW)\n", "        return sw_tokens\n", "\n", "    def tokenize(self, sentence):\n", "        # type: (Encoder, str) -> List[str]\n", "        \"\"\" Split a sentence into word and subword tokens \"\"\"\n", "        word_tokens = self.word_tokenizer(sentence.lower().strip())\n", "\n", "        tokens = []\n", "        for word_token in word_tokens:\n", "            if word_token in self.word_vocab:\n", "                tokens.append(word_token)\n", "            else:\n", "                tokens.extend(self.subword_tokenize(word_token))\n", "\n", "        return tokens\n", "    def transform(self, sentences, reverse=False, fixed_length=None):\n", "        # type: (Encoder, Iterable[str], bool, int) -> Iterable[List[int]]\n", "        \"\"\" Turns space separated tokens into vocab idxs \"\"\"\n", "        direction = -1 if reverse else 1\n", "        for sentence in self._progress_bar(sentences):\n", "            encoded = []\n", "            tokens = list(self.tokenize(sentence.lower().strip()))\n", "            for token in tokens:\n", "                if token in self.word_vocab:\n", "                    encoded.append(self.word_vocab[token])\n", "                elif token in self.bpe_vocab:\n", "                    encoded.append(self.bpe_vocab[token])\n", "                else:\n", "                    encoded.append(self.word_vocab[self.UNK])\n", "\n", "            if fixed_length is not None:\n", "                encoded = encoded[:fixed_length]\n", "                while len(encoded) < fixed_length:\n", "                    encoded.append(self.word_vocab[self.PAD])\n", "\n", "            yield encoded[::direction]"], "metadata": {"collapsed": true, "_uuid": "3761d5343a6fdce0a6af044f95209855ff062493", "_cell_guid": "6b0af2b3-3190-4a04-94ae-02032102aaee"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "from IPython.display import clear_output"], "metadata": {"collapsed": true, "_uuid": "d9e8586cc67c548fee32d5827e04d728d446cdb7", "_cell_guid": "5a8347c1-5c56-4e6e-8c7e-14ec5ddd0c4f"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["%matplotlib inline\n", "pd.set_option('display.max_colwidth', -1)\n", "plt.rcParams['figure.figsize'] = [15,9]"], "metadata": {"collapsed": true, "_uuid": "fffa3f9939d2b70751f0b6861efc2cfe64584cbd", "_cell_guid": "dadbf7f9-aee1-4f3f-83cc-a5e364127171"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "print(\"Lengths Train/Test: {} / {}\".format(len(train),len(test)))"], "metadata": {"collapsed": true, "_uuid": "44434f3f6375a9b28413d9574b721f7061385388", "_cell_guid": "ebace3a5-d2af-4c7a-9545-95b4b4fc93d3"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train = train.sample(frac=1)"], "metadata": {"collapsed": true, "_uuid": "1efe01fb85c0e2de18add603f220ff292bf30e0d", "_cell_guid": "8f349486-2bca-42fc-bdc9-19debb50b3a1"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train.head(2)"], "metadata": {"collapsed": true, "_uuid": "f4bc3454f9dacc66e5f20f65044b304c73e48cdb", "_cell_guid": "d840851c-4639-4b98-9cdd-fcaa6875275d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def fit_encoder(encoder,corpus):\n", "    start = time.time()\n", "    encoder.fit(corpus)\n", "    print(\"Encoder trained: \"+str(int(time.time() - start))+\"s\")"], "metadata": {"collapsed": true, "_uuid": "f7094709c691872fcb169f5a6d253dcea89a71c2", "_cell_guid": "2340ce2e-e550-4285-a048-4b2c90bd89ec"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["list_sentences_train = train[\"comment_text\"].fillna(\"__empty__\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "y = train[list_classes].values\n", "list_sentences_test = test[\"comment_text\"].fillna(\"__empty__\").values"], "metadata": {"collapsed": true, "_uuid": "d8fe10551f570da900230f55a8251dd8f901ba72", "_cell_guid": "155b54f2-c4d1-4f14-8da3-e2bf7fbfef6a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["vocabulary = 50000\n", "encoder = Encoder(vocabulary, ngram_max=10)\n", "corpus = list_sentences_train\n", "fit_encoder(encoder,corpus)"], "metadata": {"collapsed": true, "_uuid": "69615ac553c562f07e1b1fbae55c3e30ab821902", "_cell_guid": "46b3f33e-bdab-4feb-844a-ead0cb11a9a7"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["list_tokenized_train = list(encoder.transform(list_sentences_train))\n", "list_tokenized_test = list(encoder.transform(list_sentences_test))"], "metadata": {"collapsed": true, "_uuid": "f342d95880ff0d9f52a2c48a4632bbb0a6fd6267", "_cell_guid": "f22301ae-79f9-4a36-a083-198f1162b10d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["cutout_percentile = 99.0"], "metadata": {"collapsed": true, "_uuid": "5f9488b3e9647d71a61cd61da948d0f2e5b8dac8", "_cell_guid": "bd76f456-1e36-45c4-a607-88b06aa8cd74"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["sizes = [len(x) for x in list_tokenized_train+list_tokenized_test]\n", "max_size = int(np.percentile(sizes,cutout_percentile))\n", "max_features = max([max(x) for x in list_tokenized_train+list_tokenized_test])+1"], "metadata": {"collapsed": true, "_uuid": "d4891273d531ee8a9ca2df86403970a96c43c5b5", "_cell_guid": "835dacc0-ab94-4808-8b48-b00dfb5472e8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(\"Vocabulary: {}, Max Entry Size: {}\".format(max_features,max_size))"], "metadata": {"collapsed": true, "_uuid": "e382fef7fe5c8c2f31f469c6c1169a5963c11988", "_cell_guid": "e78e8ccf-9918-4bdc-8ac6-e5b8b702b459"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["X_t = sequence.pad_sequences(list_tokenized_train, maxlen=max_size)\n", "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=max_size)"], "metadata": {"collapsed": true, "_uuid": "d7eafa032bdaf1c0b85df9778a4154c5d35fa499", "_cell_guid": "5791e6de-117a-4ec6-be98-6bf3a18b91df"}}, {"cell_type": "markdown", "source": ["My custom Callback that captures scores and plot them. "], "metadata": {"_uuid": "4407525d2b31eb493a11b38b3c10504c73bf5179", "_cell_guid": "b2fd61d0-f416-4d6a-aac9-0b4286b19b06"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["class PlotModel(Callback):\n", "    def set_params_(self, model_type, checkpoint_path, dataset, \n", "                   batch_size, plot_per_batch,\n", "                   dictionary_size, max_review_length, \n", "                   best_model_monitor, early_stop_monitor):\n", "        self.path = checkpoint_path\n", "        self.model_type = model_type\n", "        self.dataset = dataset\n", "        self.batch_size = batch_size\n", "        self.dictionary_size = dictionary_size\n", "        self.max_review_length = max_review_length\n", "        self.best_model_monitor = best_model_monitor\n", "        self.early_stop_monitor = early_stop_monitor\n", "        self.plot_per_batch = plot_per_batch\n", "    \n", "    def get_monitor_ticks(self):\n", "        if self.best_model_monitor == \"val_acc\":\n", "            bm_ind = np.argmax(self.val_acc)\n", "        else:\n", "            bm_ind = np.argmin(self.val_losses)\n", "        \n", "        if self.early_stop_monitor == \"val_acc\":\n", "            es_ind = np.argmax(self.val_acc)\n", "        else:\n", "            es_ind = np.argmin(self.val_losses)\n", "        return [bm_ind, es_ind]\n", "    \n", "    def plot(self, save):\n", "        clear_output(wait=True)\n", "        fig = plt.figure()\n", "        plt.grid(True)\n", "        plt.plot(self.x, self.acc, '--', label=\"acc\")\n", "        plt.plot(self.x, self.val_acc, label=\"val_acc\")\n", "        plt.plot(self.x, self.losses, '--', label=\"loss\")\n", "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n", "        bm_ind, es_ind = self.get_monitor_ticks()\n", "        plt.axvline(x=self.x[bm_ind],alpha=0.6,color=\"green\", linestyle='--')\n", "        plt.axvline(x=self.x[es_ind],alpha=0.6,color=\"red\", linestyle='--')\n", "        title = \"{}: accuracy & loss for {} (length:{}, val acc/loss: {:.4f}/{:.4f})\"\n", "        title = title.format(self.model_type,self.dataset,self.max_review_length,max(self.val_acc),min(self.val_losses))\n", "        plt.title(title)\n", "        plt.legend(['acc: train', 'acc: validation','loss: train', 'loss: validation'], loc='upper left')\n", "        plt.show()\n", "        if save:\n", "            fig.savefig(self.path+\"-figure.png\")\n", "            \n", "    def on_train_begin(self, logs={}):\n", "        self.i = 1\n", "        self.x = [0]\n", "        self.acc = [0]\n", "        self.val_acc = [0]\n", "        self.losses = [1]\n", "        self.val_losses = [1]\n", "        \n", "        self.fig = plt.figure()\n", "        \n", "        self.logs = []\n", "\n", "    def on_epoch_end(self, epoch, logs={}):\n", "        if 'acc' in logs  and 'loss' in logs:\n", "            self.logs.append(logs)\n", "            self.x.append(self.i)\n", "            self.acc.append(logs.get('acc'))\n", "            self.losses.append(logs.get('loss'))\n", "            if \"val_acc\" in logs and \"val_loss\" in logs:\n", "                self.val_acc.append(logs.get('val_acc'))\n", "                self.val_losses.append(logs.get('val_loss'))\n", "            else:\n", "                self.val_acc.append(self.val_acc[len(self.val_acc)-1])\n", "                self.val_losses.append(self.val_losses[len(self.val_losses)-1])\n", "            self.i += 1\n", "            self.plot(False)\n", "        else:\n", "            keys = []\n", "            for k in ['acc', 'val_acc', 'loss', 'val_loss']:\n", "                if k not in logs:\n", "                    keys.append(k)\n", "            print((\"Missing parameters\",keys,logs))\n", "    \n", "    def on_batch_end(self, batch, logs={}):\n", "        if batch%self.plot_per_batch != 0:\n", "            return\n", "        else:\n", "            self.on_epoch_end(batch, logs)\n", "    \n", "    def on_train_end(self, epoch, logs={}):\n", "        self.plot(True)"], "metadata": {"collapsed": true, "_uuid": "ea707d8e822094db0df088b3ea3005eb804daf97", "_cell_guid": "a382dcad-49cb-496b-89ae-da61baddac0c"}}, {"cell_type": "markdown", "source": ["LSTM network provided in [Keras - Bidirectional LSTM baseline ( lb 0.051)](https://www.kaggle.com/CVxTz/keras-bidirectional-lstm-baseline-lb-0-051)"], "metadata": {"_uuid": "e26e2e45f58d569bf6d6f5f1633a9a88c2c639ef", "_cell_guid": "335ae052-d579-4b8f-a6ab-61a800daf07a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def get_lstm():\n", "    embed_size = 128\n", "    inp = Input(shape=(max_size, ))\n", "    x = Embedding(max_features, embed_size)(inp)\n", "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n", "    x = GlobalMaxPool1D()(x)\n", "    x = Dropout(0.1)(x)\n", "    x = Dense(50, activation=\"relu\")(x)\n", "    x = Dropout(0.1)(x)\n", "    x = Dense(6, activation=\"sigmoid\")(x)\n", "    model = Model(inputs=inp, outputs=x)\n", "    model.compile(loss='binary_crossentropy',\n", "                  optimizer='adam',\n", "                  metrics=['accuracy'])\n", "    return model"], "metadata": {"collapsed": true, "_uuid": "0f9411e37ad014e20f5a34d49d83fedf7d43981b", "_cell_guid": "f205f65d-e455-4d3d-ba87-8345cc5d10db"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model_type = \"lstm\""], "metadata": {"collapsed": true, "_uuid": "b7cd93d4c1fe16dcc3cc12c532cc914ad9fd6552", "_cell_guid": "0500130b-3b3c-456e-b2d5-cdee4b88e7b2"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model = get_lstm()\n", "batch_size = 256\n", "epochs = 1\n", "es_patience = 1\n", "file_path=model_type+\".weights_base.best.hdf5\""], "metadata": {"collapsed": true, "_uuid": "c363dcd9a43f8336b433754b4cb4bc86838fac92", "_cell_guid": "6066dcde-e4cd-4ec5-bf88-fe5bf60313e9"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model.summary()"], "metadata": {"collapsed": true, "_uuid": "b15509d3a01b36b376c51ca9578631528384721d", "_cell_guid": "89718b20-2c12-49fc-bfee-aa84ab2ad4b7"}}, {"cell_type": "markdown", "source": ["I set epoch number to 1 so that this kaggle docker does not interrupt the training. Best scores are achived around 3 epochs."], "metadata": {"_uuid": "8d9142a364b82a8288c009fc721c32a697ac684f", "_cell_guid": "74e82fe2-71c8-44b7-98e7-f56a9258e2a2"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["number_of_plots = 200\n", "number_of_batches_per_epoch = len(X_t)/batch_size\n", "plot_per_batch = int(number_of_batches_per_epoch/(number_of_plots/epochs))\n", "if plot_per_batch == 0:\n", "    plot_per_batch = 1\n", "plot_per_batch"], "metadata": {"collapsed": true, "_uuid": "3732b141331e651902a3a3c3abc5eb05ab87c2dd", "_cell_guid": "a5a25d00-4335-4268-b26e-c7b4ee41c3b4"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["plotter = PlotModel()\n", "plotter.set_params_(model_type, model_type+\"-checkpoint-path\", \"bpe-{}k\".format(int(vocabulary/1000)), \n", "                   batch_size, plot_per_batch,\n", "                   vocabulary, max_size, \n", "                   'val_loss', 'val_loss')\n", "\n", "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n", "earlyStopping = EarlyStopping(monitor=\"val_loss\", mode=\"max\", patience=es_patience)\n", "callbacks_list = [plotter, checkpoint, earlyStopping]"], "metadata": {"collapsed": true, "_uuid": "d26117fe42b3488e14823ec84678601eb3c02281", "_cell_guid": "516c33b6-c903-4ab3-88c4-d7dd84934b1b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)"], "metadata": {"collapsed": true, "_uuid": "39c42702c8e3ba8bc10ebe45b43338bae42d96da", "scrolled": false, "_cell_guid": "4c6a38c6-e3d2-4408-b451-4591ba639a14"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["model = get_lstm()\n", "model.load_weights(file_path)"], "metadata": {"collapsed": true, "_uuid": "16ebad79ba3116fbfb71dd8d1b95c9e3909b47da", "_cell_guid": "fcac9257-20ab-4be1-b139-32216ebc6d19"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["sample_per_prediction = 1000\n", "def get_predictions(model,data):\n", "    preds = []\n", "    cnt = 0\n", "    total = len(data)\n", "    while cnt*sample_per_prediction<len(data):\n", "        start = time.time()\n", "        sample = data[cnt*sample_per_prediction:(cnt+1)*sample_per_prediction]\n", "        preds += model.predict(sample).tolist()\n", "        clear_output(wait=True)\n", "        cnt += 1\n", "        percent = cnt*sample_per_prediction/total\n", "        print(\"Progress: {:.2f}%, (Remaining: {}s)\".format(100.0*percent,int(total*(1-percent)*int(time.time() - start)/sample_per_prediction)))\n", "    return np.array(preds)"], "metadata": {"collapsed": true, "_uuid": "e08d09930a1f3a117ae0db25d344c0b77dfc8cda", "_cell_guid": "83beed0a-347a-4252-8d36-04e538c96636"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["y_test = get_predictions(model,X_te)"], "metadata": {"collapsed": true, "_uuid": "ca86da2185b8b764bc490319d99bfba9b303ea7a", "_cell_guid": "7350d7c0-ea06-4280-9395-27ed5ddb031b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "sample_submission[list_classes] = y_test"], "metadata": {"collapsed": true, "_uuid": "b78d32e4a25bedc591a4d750d728917585b616c0", "_cell_guid": "a243e2cc-4c0e-409a-a123-c6231b2079d3"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def plot_predictions_grid(df):\n", "    fig = plt.figure()\n", "    for i,c in enumerate(list_classes):\n", "        sub1 = fig.add_subplot(321+i)\n", "        sub1.hist(df[c].values, label=c, bins=100, color=\"red\")\n", "        sub1.set_title(c+\" Prediction Distribution\")\n", "        sub1.set_yscale('log', nonposy='clip')\n", "        sub1.legend(loc='upper right')\n", "    plt.show()"], "metadata": {"collapsed": true, "_uuid": "2434594836af7c7e13b0cc3490538ecaff14ad8a", "_cell_guid": "9c5907d9-8612-42cd-a552-4f6028e23a5f"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["plot_predictions_grid(sample_submission)"], "metadata": {"collapsed": true, "_uuid": "429223507f15989e08ef8dfec1bd2b5a741bfa06", "_cell_guid": "89dff73e-b139-4773-acc8-7b5219037233"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["sample_submission.to_csv(\"baseline.csv\", index=False)"], "metadata": {"collapsed": true, "_uuid": "f6efbc61b0bea16d938690454ca42c75d13c2928", "_cell_guid": "76fad935-221b-4210-bcf8-a8153d617c45"}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}