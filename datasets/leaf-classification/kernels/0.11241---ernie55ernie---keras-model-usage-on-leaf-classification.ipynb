{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "727469b3-29cd-c14f-73db-c8ce9d570e45"
      },
      "source": [
        "# Introduction\n",
        "This notebook contains the basic usage of keras library on leaf classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7761a48-c526-f60f-00e4-694a3ac03e41"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers.convolutional import ZeroPadding2D, Convolution2D, MaxPooling2D\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Activation, Merge\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras.layers.advanced_activations import ELU, LeakyReLU, ThresholdedReLU, SReLU\n",
        "\n",
        "from keras.callbacks import ProgbarLogger, ModelCheckpoint\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "target_size = (256, 256)\n",
        "grayscale = True\n",
        "\n",
        "# Relative path for the train, test, and submission file\n",
        "train_path = '../input/train.csv'\n",
        "test_path = '../input/test.csv'\n",
        "submission_path = '../input/sample_submission.csv'\n",
        "submission_output = './submission.csv'\n",
        "\n",
        "def load_image(id):\n",
        "    img_path = '../input/images/%d.jpg' % (id, )\n",
        "    img = image.load_img(img_path,\n",
        "                         grayscale=grayscale)\n",
        "    img.thumbnail(target_size)\n",
        "    bg = Image.new('L', target_size, (0,))\n",
        "    bg.paste(\n",
        "        img, (int((target_size[0] - img.size[0]) / 2), int((target_size[1] - img.size[1]) / 2))\n",
        "    )\n",
        "    img_arr = image.img_to_array(bg)\n",
        "    \n",
        "    return img_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "869fd19f-c427-b5a5-c1a1-1c3fe8358600"
      },
      "source": [
        "# Data preprocessing\n",
        "Load data from csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38836a22-1f97-605f-92ff-2cedf3501952"
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "train_data = pd.read_csv(train_path)\n",
        "# load the ids in the training data set\n",
        "x_ids = train_data.iloc[:, 0]\n",
        "x_images = list()\n",
        "for i in x_ids:\n",
        "    x_images.append(load_image(i))\n",
        "x_images = np.array(x_images)\n",
        "plt.imshow(x_images[0].squeeze())\n",
        "plt.show()\n",
        "print('Shape of images', x_images[0].shape)\n",
        "# Ignore the first column (id) and the second column (species)\n",
        "x_features = train_data.iloc[:, 2:].values\n",
        "print('Number of features', x_features.shape[1])\n",
        "\n",
        "# Convert the species to category type\n",
        "y = train_data['species']\n",
        "# Get the corresponding categories list for species\n",
        "le = LabelEncoder()\n",
        "le.fit(y)\n",
        "y = le.transform(y)\n",
        "\n",
        "nb_classes = len(le.classes_)\n",
        "print('Number of classes', nb_classes)\n",
        "print('Number of instances', len(y))\n",
        "\n",
        "plt.hist(y, bins=nb_classes)\n",
        "plt.title('Number of instances in each class')\n",
        "plt.xlabel('Class id')\n",
        "plt.ylabel('Number of instances')\n",
        "plt.show()\n",
        "\n",
        "# convert a class vectors (integers) to a binary class matrix\n",
        "y = np_utils.to_categorical(y)\n",
        "\n",
        "# Load testing data\n",
        "test_data = pd.read_csv(test_path)\n",
        "test_ids = test_data.iloc[:, 0]\n",
        "test_images = list()\n",
        "for i in test_ids:\n",
        "    test_images.append(load_image(i))\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Load submission file\n",
        "submission_data = pd.read_csv(submission_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3ca41ce6-88d0-92a7-cb5d-5f8a8b23deb9"
      },
      "source": [
        "# Train and test split\n",
        "Split the dataset into training and testing part in order to evaluate the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0decf5e-ddc6-4397-0913-34ac1a4fa090"
      },
      "outputs": [],
      "source": [
        "# The folds are made by preserving the percentage of samples for each class\n",
        "sss = StratifiedShuffleSplit(10, 0.2, random_state=15)\n",
        "for train_index, test_index in sss.split(x_images, y):\n",
        "\tx_train_images, x_test_images, x_train_features, x_test_features = x_images[train_index], x_images[test_index], x_features[train_index], x_features[test_index]\n",
        "\ty_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "print('Shape of x train images', x_train_images.shape)\n",
        "print('Shape of x train features', x_train_features.shape)\n",
        "print('Shape of y train', y_train.shape)\n",
        "print('Shape of x test images', x_test_images.shape)\n",
        "print('Shape of x test features', x_test_features.shape)\n",
        "print('Shape of y test', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5554c2dd-dd08-a083-5374-aec619a5798b"
      },
      "outputs": [],
      "source": [
        "def construct_feature_model():\n",
        "    print('Contructing the model')\n",
        "    \n",
        "    model1 = Sequential([\n",
        "        Dense(nb_classes * 2, input_shape=x_train_features.shape[1:]),\n",
        "        BatchNormalization(),\n",
        "        Activation('tanh'),\n",
        "        Dropout(0.25)\n",
        "    ])\n",
        "    \n",
        "    model2 = Sequential([\n",
        "        Convolution2D(32, 3, 3, border_mode='same',\n",
        "                            input_shape=x_images.shape[1:]),\n",
        "        Activation('tanh'),\n",
        "        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(nb_classes),\n",
        "        Activation('tanh')\n",
        "    ])\n",
        "    \n",
        "    model = Sequential([\n",
        "        Merge([model1, model2], mode='concat', concat_axis=1),\n",
        "        Dense(nb_classes * 2),\n",
        "        Activation('tanh'),\n",
        "        Dropout(0.5),\n",
        "        Dense(nb_classes),\n",
        "        Activation('softmax')\n",
        "    ])\n",
        "    \n",
        "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    \n",
        "    model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    print('Finish construction of the model')\n",
        "    return model\n",
        "\n",
        "model = construct_feature_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e86fbc8-7235-6196-79c5-b8c45501cb25"
      },
      "outputs": [],
      "source": [
        "print('Start to fit')\n",
        "s = time.time()\n",
        "# Save the parameter for the best model\n",
        "best_model_file = 'leaf.h5'\n",
        "best_model_cb = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=0, save_best_only=True)\n",
        "\n",
        "# Fitting parameters\n",
        "batch_size = 32\n",
        "nb_epoch = 8\n",
        "verbose = 2\n",
        "callbacks = [best_model_cb]\n",
        "validation_split = 0.0\n",
        "validation_data = ([x_test_features, x_test_images], y_test)\n",
        "#validation_data = (x_test_features, y_test)\n",
        "shuffle = True\n",
        "class_weight = None\n",
        "sample_weight = None\n",
        "data_augmentation = False\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation')\n",
        "    history = model.fit([x_features, x_images], y,\n",
        "    #history = model.fit(x_features, y,\n",
        "              batch_size=batch_size,\n",
        "              nb_epoch=nb_epoch, \n",
        "              verbose=verbose,\n",
        "              callbacks=callbacks,\n",
        "              validation_split=validation_split,\n",
        "              validation_data=validation_data,\n",
        "              shuffle=shuffle,\n",
        "              class_weight=class_weight,\n",
        "              sample_weight=sample_weight)\n",
        "print('Finish fitting\\nFitting time', time.time() - s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "76f51060-c39f-81f0-03e6-ab005a237b42"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['val_acc'])\n",
        "plt.xlabel('Number of epoch')\n",
        "plt.ylabel('Validation accrucy')\n",
        "plt.title('Validation accuracy vs number of epoch')\n",
        "plt.show()\n",
        "print('Maximum accuracy', max(history.history['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e6bd64f-7623-03dc-7d18-a2361b5e0e81"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['val_loss'], color='r')\n",
        "plt.xlabel('Number of epoch')\n",
        "plt.ylabel('Categorical cross entropy loss')\n",
        "plt.title('Categorical cross entropy loss vs number of epoch')\n",
        "plt.show()\n",
        "print('Minimum loss', min(history.history['val_loss']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b06e625c-4958-3837-8a78-4fb9aecb07d2"
      },
      "outputs": [],
      "source": [
        "model = load_model(best_model_file)\n",
        "\n",
        "y_prob = model.predict([test_data.iloc[:, 1:].values, test_images]) # Remove id column\n",
        "#y_prob = model.predict(test_data.iloc[:, 1:].values) # Remove id column\n",
        "\n",
        "submission_data.iloc[:, 1:] = y_prob\n",
        "submission_data.tail()\n",
        "\n",
        "f = open(submission_output, 'w')\n",
        "f.write(pd.DataFrame(submission_data).to_csv(index = False))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "66d98131-7a6f-d158-2ea8-207c5235bc7e"
      },
      "source": [
        "# Storing all data into a h5df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92420460-5c7f-ed93-2c51-1eeae7b36683"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "file_name = 'data.h5'\n",
        "if os.path.isfile(file_name):\n",
        "    os.remove(file_name)\n",
        "\n",
        "h5f = h5py.File(file_name, 'w')\n",
        "h5f.create_dataset('x_train_images', data=x_train_images)\n",
        "h5f.create_dataset('x_train_features', data=x_train_features)\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.create_dataset('x_test_images', data=x_test_images)\n",
        "h5f.create_dataset('x_test_features', data=x_test_features)\n",
        "h5f.create_dataset('y_test', data=y_test)\n",
        "\n",
        "h5f.create_dataset('test_images', data=test_images)\n",
        "h5f.create_dataset('test_faetures', data=test_data.iloc[:, 1:].values)\n",
        "\n",
        "h5f.close()\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(nb_classes * 2, input_dim=x_train_features.shape[1]),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(nb_classes * 2),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(nb_classes),\n",
        "        Activation('softmax'),\n",
        "    ])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "98a57e7a-595c-d519-e4a9-656ee09ffaf9"
      },
      "source": [
        "# Reference\n",
        "1. https://github.com/fchollet/keras/issues/68\n",
        "2. http://stackoverflow.com/questions/31997366/python-keras-shape-mismatch-error\n",
        "3. https://www.kaggle.com/abhmul/leaf-classification/keras-convnet-lb-0-0052-w-visualization\n",
        "4. https://keras.io/models/sequential/\n",
        "5. http://stackoverflow.com/questions/1386352/pil-thumbnail-and-end-up-with-a-square-image\n",
        "6. http://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7df50e7-a70a-05de-dd2f-7fc44d9b8097"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}