{"cells":[{"metadata":{"_uuid":"8389bc04262b7e586b2800f3d23b5411e7a28a92"},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction\n---\n## Abstract\nThis project participated a _kaggle_ competition to predict the poverty level from a limited high dimensional dataset. As the purpose of gaining a high rank from the competition, the project is divided into three parts: exploratory data analysis(EDA), clustering analysis and machine learning. \n\nIn the EDA section, project extract the aggregation of features among the individuals within same household to help the prediction. It also drop the features with high correlations to avoid misleading the models. Some important features(The importance is gained from boosting algorithm) are performed visualising operation.\n\nIn clustering analysis, the similarity between cluster is demonstrated. PCA is involved for both visualisation and preprocessing.\n\nIn machine learning section, a variety of algorithms are explored. The pipeline of machine learning process is :preprocessing &rarr; model &rarr; predict. The process of finding optimised hyperparameters are built from scratch only for `KNN`, the other algorithms use `sklearn` model standard [cross validate grid parameter search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) procedure. Boosting algorithm is the main force in this competition. The highest ranking came from [LightGBM](https://github.com/Microsoft/LightGBM) algorithm."},{"metadata":{"_uuid":"039e520ce9f38b1431ed399942381a5925b25542","trusted":true},"cell_type":"code","source":"prefix =  \"../input/costa-rican-household-poverty-prediction/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d395ee37672c78843a126cef110367999bc99f4a","scrolled":true,"trusted":true},"cell_type":"code","source":"from __future__ import division\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a90bb7bfb64a378edaefba3a96450ed0148fb25","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\n# link pandas and plotly\nimport cufflinks as cf\n\ninit_notebook_mode(connected=True)\ncf.set_config_file(offline=True, world_readable=True, theme='ggplot')\n%matplotlib inline\n\nfrom matplotlib import rc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9f87a050630bd030b058c5189c897ce336fb046"},"cell_type":"markdown","source":"### Predefine Function\n---\nHere are the plotting functions that used in the project. Put them here to increase the readability of the notebookv"},{"metadata":{"_uuid":"47e22a97ca282e4c0f7c865b58ad95539226b602","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_hist(x, col_name, title, x_label=None, histnorm=\"percent\"):\n    if x_label == None:\n        x_label = col_name\n    data = []\n    for i in range(4):\n        trace = go.Histogram(\n            x=x[x.Target == i + 1][col_name],\n            opacity=0.75,\n            histnorm=histnorm,\n            name=poverty_map[i+1]\n        )\n        \n        data.append(trace)\n        \n    layout = go.Layout(\n        barmode='stack', \n        title=title,\n        xaxis=dict(\n            title=x_label\n        ),\n        yaxis=dict(\n            title=\"Percentage(%)\"\n        )\n    )\n    fig = go.Figure(data=data, layout=layout)\n\n    iplot(fig, filename='overlaid histogram')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f95237fbc3e115a0c53dab538fdc0915aebda0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def pca_3d_plot(pca_train_df):\n    data = []\n    colors = ['rgb(228,26,28)','rgb(55,126,184)','rgb(77,175,74)', 'rgb(230, 230, 20)']\n\n    for i in range(4):\n        color = colors[i]\n        x = pca_train_df[pca_train_df.Target == i + 1][0]\n        y = pca_train_df[pca_train_df.Target == i + 1][1]\n        z = pca_train_df[pca_train_df.Target == i + 1][2]\n\n        trace = dict(\n            name = poverty_map[i + 1],\n            x = x, y = y, z = z,\n            type = \"scatter3d\",    \n            mode = 'markers',\n            marker = dict( size=3, color=color, line=dict(width=0) ) )\n        data.append( trace )\n\n    layout = dict(\n        width=800,\n        height=550,\n        autosize=False,\n        title='Costa Rican Poverty Prediction PCA',\n        scene=dict(\n            xaxis=dict(\n                gridcolor='rgb(255, 255, 255)',\n                zerolinecolor='rgb(255, 255, 255)',\n                showbackground=True,\n                backgroundcolor='rgb(230, 230,230)'\n            ),\n            yaxis=dict(\n                gridcolor='rgb(255, 255, 255)',\n                zerolinecolor='rgb(255, 255, 255)',\n                showbackground=True,\n                backgroundcolor='rgb(230, 230,230)'\n            ),\n            zaxis=dict(\n                gridcolor='rgb(255, 255, 255)',\n                zerolinecolor='rgb(255, 255, 255)',\n                showbackground=True,\n                backgroundcolor='rgb(230, 230,230)'\n            ),\n            aspectratio = dict( x=1, y=1, z=0.7 ),\n            aspectmode = 'manual'        \n        ),\n    )\n\n    fig = dict(data=data, layout=layout)\n\n    # IPython notebook\n    iplot(fig, filename='costa_rican', validate=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6aa519883bc07bf0b1a2630a0ea4ab01d74a11d","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_score, make_scorer\ndef search_optimised_k(n_clusters_list, random_state, X):\n    silhouette_score_list = []\n    result_dict  = {}\n    for n_clusters in n_clusters_list:\n        km_predict = KMeans(random_state=random_state, n_jobs=-1, n_clusters=n_clusters).fit_predict(X)\n        silhouette_score_list.append(silhouette_score(X=X, labels=km_predict, random_state=16446054))\n    result_dict[\"silhouette_score\"] = silhouette_score_list\n    result_dict[\"n_clusters\"] = n_clusters_list\n    \n    return result_dict\n        ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"f45885902994f5b69635896ba1de9f5470811d0d","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n\nclass knn_agent:\n    def __init__(self):\n        self.uniform_neighbours_num = None\n        self.distance_neighbours_num = None\n        # === models\n        self.uniform_knn = None\n        self.distance_knn = None\n        \n        self.f1_df = None\n        \n\n    def optimise_params(self, X_train, y_train, X_dev, y_dev, max_k_num):\n        uniform_f1_test_list = []\n        distance_f1_test_list = []\n        \n        for i in range(1, max_k_num):\n            uniform_knn = KNeighborsClassifier(n_neighbors=i, weights=\"uniform\")\n            distance_knn = KNeighborsClassifier(n_neighbors=i, weights=\"distance\")\n\n            uniform_prediction = uniform_knn.fit(X_train, y_train).predict(X_dev)\n            distance_prediction = distance_knn.fit(X_train, y_train).predict(X_dev)\n\n            uniform_f1_test_list.append(f1_score(y_true=y_dev, y_pred=uniform_prediction, average='macro'))\n            distance_f1_test_list.append(f1_score(y_true=y_dev, y_pred=distance_prediction, average='macro'))\n\n        self.f1_df = pd.DataFrame(data={\"F1(uniform)\": uniform_f1_test_list,\n                                       \"F1(distance)\": distance_f1_test_list},\n                                 index=range(1, max_k_num))\n\n        # Observe from the plot we can tell that the global minimum is the uniform neighbours with total market values\n        self.uniform_neighbours_num = self.f1_df.sort_values(\"F1(uniform)\", ascending=False).head(1).index.values.item(0)\n\n        self.distance_neighbours_num = self.f1_df.sort_values(\"F1(distance)\", ascending=False).head(1).index.values.item(0)\n\n        # Initialise the model with optimised k number\n        self.uniform_knn = KNeighborsClassifier(n_neighbors=self.uniform_neighbours_num, weights=\"uniform\")\n        self.distance_knn = KNeighborsClassifier(n_neighbors=self.distance_neighbours_num, weights=\"distance\")\n        \n        self.plot_validation_result()\n\n    def plot_validation_result(self):\n        \n        self.f1_df.iplot(kind='scatter', yTitle=\"F1 Score\", xTitle=\"K number\", title=\"Optimised K Number Searching\")\n        \n        \n    # Noted that predict data need to clean before feeding\n    def predict(self, X_train, y_train, X_predict):\n        return (self.uniform_knn.fit(X_train, y_train).predict(X_predict), self.distance_knn.fit(X_train, y_train).predict(X_predict))\n\n    def evaluate(self, X_train, y_train, X_test, y_test):\n        uniform_prediction = self.uniform_knn.fit(X_train, y_train).predict(X_test)\n\n        distance_prediction = self.distance_knn.fit(X_train, y_train).predict(X_test)\n\n        return (f1_score(y_true=y_test, y_pred=uniform_prediction, average=\"macro\"),\n                f1_score(y_true=y_test, y_pred=distance_prediction, average=\"macro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c24f2c7b31773335b38374597aaf8abf61be8c","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def generate_final_predict_df(input_prediction):\n    input_prediction = input_prediction.astype(int)\n    X_predict[\"Target\"] = input_prediction\n\n    tmp_household_predict_df = X_predict.set_index(\"idhogar\")\n    tmp_X_predict = all_predict_df.loc[all_predict_df[\"parentesco1\"] == 1, :].set_index(\"idhogar\")\n    tmp_X_predict.loc[tmp_household_predict_df.index, \"Target\"] = tmp_household_predict_df.Target\n    tmp_X_predict = tmp_X_predict.set_index(\"Id\")\n    tmp_all_predict_df = all_predict_df.set_index(\"Id\")\n    tmp_all_predict_df.loc[tmp_X_predict.index, \"Target\"] = tmp_X_predict.Target\n    tmp_all_predict_df[\"Target\"] = tmp_all_predict_df[\"Target\"].fillna(1).astype(int)\n    tmp_all_predict_df\n\n    tmp_all_predict_df[\"Target\"] = tmp_all_predict_df[\"Target\"].fillna(1).astype(int)\n    final_predict_df = tmp_all_predict_df.reset_index()[[\"Id\", \"Target\"]]\n    return final_predict_df\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f970a97c16d484f4fa796e5698dbfeba6e48ef37","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a20ebcc362752b2ce0a5e1c3a6ab2b471a16d2ec","scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_train_test(x, y, data, hue, title=None, x_label=None, y_label=None):\n    \n    # First check if y is a list\n    y_is_list = True if isinstance(y, list) else False\n    \n    x_data_list = []\n    y_data_list = []\n    \n    # Check if the hue is specified\n    hue_list = data.loc[:, hue].unique()\n    for cat in hue_list:\n        x_data = data[data[hue] == cat][x]\n        x_data_list.append(x_data)\n        y_data = data[data[hue] == cat][y]\n        y_data_list.append(y_data)\n\n    data = []\n    style_list = [None, 'dash', 'dot']\n    colour_list = ['rgb(22, 96, 167)', 'rgb(205, 12, 24)']\n    for x_data, y_data, cat, colour in zip(x_data_list, y_data_list, hue_list, colour_list):\n        if(isinstance(y_data, pd.Series)):\n            trace = go.Scatter(x=x_data, y=y_data, mode='lines+markers', name=cat )\n            data.append(trace)\n        elif(isinstance(y_data, pd.DataFrame)):\n            # Means that y is a list of column names\n            for col, style in zip(y, style_list):\n                y_series = y_data[col]\n                trace = go.Scatter(x=x_data, y=y_series, name= '+'.join([str(cat), col]),\n                                   line=dict(\n                                       dash = style,\n                                       color=colour,\n                                   )\n                                  )\n                data.append(trace)\n    \n    if x_label == None:\n        x_label = x\n    if y_label == None:\n        if y_is_list:\n            y_label = '+'.join(y)\n        else:\n            y_label = y\n            \n    layout = dict(\n        title=title,\n        xaxis=dict(title=x_label),\n        yaxis=dict(title=y_label)\n    )\n    \n    fig = dict(data=data, layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c8519fa24cba30c2e4f2ec0afcec701780e2774","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from math import ceil\ndef subplot_test_train(x, y, z, data,hue=None, title=None, x_label=None, y_label=None):\n    \n    y_is_list = True if isinstance(y, list) else False\n    \n    if x_label == None:\n        x_label = x\n    if y_label == None:\n        if y_is_list:\n            y_label = 'f1_macro score'\n        else:\n            y_label = y\n    \n    subplot_val_list = data.loc[:, z].unique()\n    subplot_num = len(subplot_val_list)\n    \n    subplot_titles=[]\n    for val in subplot_val_list:\n        subplot_titles.append(': '.join([z, str(val)]))\n    \n    fig = tools.make_subplots(cols=2, rows=ceil(subplot_num/2), subplot_titles=subplot_titles, print_grid=False)\n    \n    original_data = data\n    for i, val in enumerate(subplot_val_list):\n        \n        data = original_data[original_data[z] == val]\n        xaxis_index = 'xaxis' + str(i + 1)\n        yaxis_index = 'yaxis' + str(i + 1)\n        \n        x_data_list = []\n        y_data_list = []\n        \n        if hue != None:\n            hue_list = data.loc[:, hue].unique()\n            for cat in hue_list:\n                x_data = data[data[hue] == cat][x]\n                x_data_list.append(x_data)\n                y_data = data[data[hue] == cat][y]\n                y_data_list.append(y_data)     \n        else:\n            hue_list = []\n    \n        style_list = [None, 'dash', 'dot']\n        colour_list = ['rgb(22, 96, 167)', 'rgb(205, 12, 24)', 'rgb(12, 205, 24)' ]\n\n        for x_data, y_data, cat, colour in zip(x_data_list, y_data_list, hue_list, colour_list):\n            if(isinstance(y_data, pd.Series)):\n                trace = go.Scatter(x=x_data, y=y_data, mode='lines+markers' )\n                fig.append_trace(trace, i//2 + 1, (i%2 + 1))\n            elif(isinstance(y_data, pd.DataFrame)):\n                # Means that y is a list of column names\n                for col, style in zip(y, style_list):\n                    y_series = y_data[col]\n                    trace = go.Scatter(x=x_data, y=y_series, name= '+'.join([cat, col]),\n                                       line=dict(\n                                           dash=style,\n                                           color=colour,\n                                       )\n                                      )\n                    fig.append_trace(trace, i//2 + 1, (i%2 + 1))\n                    if i == subplot_num - 1 or i == subplot_num - 2:\n                        fig['layout'][xaxis_index].update(title = x_label)\n                    if i%2 == 0:\n                        fig['layout'][yaxis_index].update(title = y_label)\n\n    fig['layout'].update(title='SVC Optimised Parameters Searching')\n    iplot(fig)\n#     for val in subplot_val_list:\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b43370e12a3b2e81e2344486fcd5f416ca0ec4a3","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def ada_report(ada_models, X_train, y_train, X_test, y_test):\n    \n    ada_tree_real, ada_tree_discrete = ada_models\n    \n    real_test_f1 = []\n    real_train_f1 = []\n    discrete_test_f1 = []\n    discrete_train_f1 = []\n    \n    trees_discrete_num = len(ada_tree_discrete)\n    trees_real_num = len(ada_tree_real)\n    \n    for real_test_predict, real_train_predict, discrete_test_predict, discrete_train_predict in zip(\n        ada_tree_real.staged_predict(X_test),\n        ada_tree_real.staged_predict(X_train),\n        ada_tree_discrete.staged_predict(X_test),\n        ada_tree_discrete.staged_predict(X_train)\n    ):\n        real_test_f1.append(\n            f1_score(y_pred=real_test_predict, y_true=y_test, average='macro')\n        )\n        \n        real_train_f1.append(\n            f1_score(y_pred=real_train_predict, y_true=y_train, average='macro')\n        )\n        \n        discrete_test_f1.append(\n            f1_score(y_pred=discrete_test_predict, y_true=y_test, average='macro')\n        )\n        \n        discrete_train_f1.append(\n             f1_score(y_pred=discrete_train_predict, y_true=y_train, average='macro')\n        )\n        \n    f1s = [\n        real_test_f1,\n        real_train_f1,\n        discrete_test_f1,\n        discrete_train_f1, \n    ]\n    labels = [\"SAMME.R: Test\", \"SAMME.R: Train\", \"SAMME: Test\", \"SAMME: Train\"]\n    colours = ['rgb(22, 96, 167)', 'rgb(22, 96, 167)', 'rgb(205, 12, 24)', 'rgb(205, 12, 24)']\n    styles = [None, \"dash\", None, \"dash\"]\n        \n    fig = tools.make_subplots(cols=1, rows=3, print_grid=False, shared_xaxes=True)\n    \n    for f1, label, colour, style in zip(f1s, labels, colours, styles):\n        trace = go.Scatter(\n            x=np.arange(1, trees_real_num, 1),\n            y=f1,\n            name= label,\n            line=dict(\n                color=colour,\n                dash=style\n            )\n        )\n        \n        fig.append_trace(trace, 1, 1)\n    \n    fig[\"layout\"][\"yaxis1\"].update(title=\"F1 Macro Score\")\n    \n    trace_real_error = go.Scatter(\n        x=np.arange(1, trees_real_num, 1),\n        y=ada_tree_real.estimator_errors_,\n        name=\"SAMME.R: Error\",\n#         line=dict(\n#             color='rgb(22, 96, 167)',\n#             width=3\n#         )\n    )\n    \n    trace_discrete_error = go.Scatter(\n        x=np.arange(1, trees_real_num, 1),\n        y=ada_tree_discrete.estimator_errors_,\n        name=\"SAMME: Error\",\n#         line=dict(\n#             color='rgb(205, 12, 24)',\n#             width=3\n#         )\n    )\n    \n    fig.append_trace(trace_real_error, 2, 1)\n    fig.append_trace(trace_discrete_error, 2, 1)\n    fig[\"layout\"][\"yaxis2\"].update(title=\"Error\")\n    \n    trace_real_weights = go.Scatter(\n        x=np.arange(1, trees_real_num, 1),\n        y=ada_tree_real.estimator_weights_,\n        name=\"SAMME.R: Weights\",\n    )\n    \n    trace_discrete_weights = go.Scatter(\n        x=np.arange(1, trees_real_num, 1),\n        y=ada_tree_discrete.estimator_weights_,\n        name=\"SAMME: Weights\",\n    )\n    \n    fig.append_trace(trace_real_weights, 3, 1)\n    fig.append_trace(trace_discrete_weights, 3, 1)\n    fig[\"layout\"][\"yaxis3\"].update(title=\"Weights\")\n    \n    \n    fig[\"layout\"].update(title=\"Adaboost Discrete and Real Algorithm Comparison\")\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_uuid":"b7cb17450ef4d236fe6a034a4cc48292f106565c"},"cell_type":"code","source":"def plot_features(feature_importances, index, most_important=True):\n    indices = np.argsort(feature_importances)[::-1]\n    indices = indices[:index]\n\n    # Visualise these with a barplot\n    plt.subplots(figsize=(20, 15))\n    g = sns.barplot(y=X.iloc[:, 3:].columns[indices], x = lgb_clf.feature_importances_[indices], orient='h')\n    g.set_xlabel(\"Relative importance\",fontsize=20)\n    g.set_ylabel(\"Features\",fontsize=20)\n    g.tick_params(labelsize=15)\n    g.set_title(\"LightGBM feature importance\", fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_uuid":"409f2193b6da9efab20ad8240665d433235a84af"},"cell_type":"code","source":"def plot_submission(df):\n    trace = go.Scatter(x=df.index, y=df[\"public score\"], mode='lines+markers', text=df[\"Note\"])\n    \n    annotations_list = []\n    for index, row in df.iterrows():\n        annotations_list.append(\n            dict(\n                x=index,\n                y=row[2],\n                xref='x',\n                yref='y',\n                text=row[0],\n                showarrow=True,\n                arrowhead=7\n            )\n        )\n        \n    layout = go.Layout(annotations=annotations_list, title=\"Hover Over The Points To See My Note\",\n                       xaxis=dict(title=\"Submission Sequence\"),\n                       yaxis=dict(title=\"Finial F1 Score\")\n                      )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fd503adb04f7563354438d6d58b9ff90539e313","trusted":true},"cell_type":"code","source":"poverty_map = {\n    1:\"extreme poverty\", \n2 : \"moderate poverty\" ,\n3 : \"vulnerable households\", \n4 : \"non vulnerable households\"\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c503c3ed524500d2e8e897235d9bccf35b159c46","trusted":true},"cell_type":"code","source":"raw_train_df = pd.read_csv(prefix + \"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f1ac6a8a8906b9444dbf153256ff5d3a9f9a3d7","trusted":true},"cell_type":"code","source":"raw_predict_df = pd.read_csv(prefix + \"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88630abdd02cfc734c2c575ce9b3cc96e42cf54a","trusted":true},"cell_type":"code","source":"example_predict_df = pd.read_csv(prefix + \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fea0e4ab6ccca067a5842a51c372a46198dd98b"},"cell_type":"markdown","source":"Concatenate two data sets to avoid redundant preprocessing. Split it before the machine learning procedure starts."},{"metadata":{"_uuid":"5c3cfeffef24cc3d84117d58e7693f62a7535d45","trusted":true},"cell_type":"code","source":"raw_predict_df = pd.concat([raw_predict_df, pd.Series(np.nan, name='Target')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ce5cef38f95fc58b0474841ab6148ac6ab31717","trusted":true},"cell_type":"code","source":"raw_all_df = pd.concat([raw_train_df, raw_predict_df], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e76de52a9b833a15fd36e8af518b06e3c5383666"},"cell_type":"markdown","source":"## Data Preprocessing\n---\nFor the real-life data set, it is complex and incomplete. A preprocessing procedure will be crucial before any machine learning process takes place."},{"metadata":{"_uuid":"e08210bce24194a308492c994cae36ee285af3f1"},"cell_type":"markdown","source":"### Data Integrity Checking"},{"metadata":{"_uuid":"ddd3707b19435d38e809355b0d3258e2cd07c941","trusted":true},"cell_type":"code","source":"raw_all_df[\"edjefe\"] = raw_all_df[\"edjefe\"].apply(lambda x: 1 if x == \"yes\" else x).apply(lambda x: 0 if x == \"no\" else x)\n\nraw_all_df[\"edjefa\"] = raw_all_df[\"edjefa\"].apply(lambda x: 1 if x == \"yes\" else x).apply(lambda x: 0 if x == \"no\" else x)\n\nraw_all_df[\"dependency\"] = raw_all_df[\"dependency\"].apply(lambda x: 1 if x == \"yes\" else x).apply(lambda x: 0 if x == \"no\" else x)\n\nraw_all_df[\"edjefe\"] = pd.to_numeric(raw_all_df[\"edjefe\"])\n\nraw_all_df[\"edjefa\"] = pd.to_numeric(raw_all_df[\"edjefa\"])\n\nraw_all_df[\"dependency\"] = pd.to_numeric(raw_all_df[\"dependency\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a20255a63a0ee3477a20b2274d8fc157e6d3251"},"cell_type":"markdown","source":"> ### Fill the NaN Value"},{"metadata":{"_uuid":"67db074c56bd266f4a239bc2c6896aa92e5a37bf","trusted":true},"cell_type":"code","source":"raw_all_df.isnull().sum()[raw_all_df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b49c8d70f8f78d10a7d35421380b575e92b87648"},"cell_type":"markdown","source":"As we can see, `v2a1` which is monthly rent payment and `rez_esc` which is the years behind in school have a significant number of null values. To drop those columns will be an option.\n\n`v18q1` is the number of ipads a household owns, after checking, we found that `NaN` in that value actually represent 0. Then we can drop the `v18q` due to `v18q1` is encoded the information of `v18q`"},{"metadata":{"_uuid":"3b2201413cab6a412937c22257dd572959ec720b","trusted":true},"cell_type":"code","source":"clean_all_df = raw_all_df.drop([\"v2a1\", \"rez_esc\"], axis=1)\nclean_all_df[\"v18q1\"].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdb35d316bfcef966fe9cf0fdc041adc5c7d2a6","scrolled":true,"trusted":true},"cell_type":"code","source":"clean_all_df.isnull().sum()[clean_all_df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6108a414e0906b3b172237e96141196dd12fa535"},"cell_type":"markdown","source":"### Remove the Redundant Features\n---\nIn the data set, there are features which are the calculated from the other features by some math formula.\nThanks to [Will](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)'s exellent kernel that finds out the category of features."},{"metadata":{"_uuid":"5b6667c89a532d19d13eda1e54b58ecdf346c458","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"id_col = ['Id', 'idhogar', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"927f829e906816f294249f21f54a48a7f718fead","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ind_bool_col = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone',]\n\nind_ordered_col = ['escolari', 'age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4725137d3feb58fb6e42919adcaaea83c6c733f8","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hh_bool_col = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area2' , 'area1'] # redundant\n\nhh_ordered_col = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n               'v18q1', 'hogar_nin', 'tamhog','tamviv','hhsize', 'r4t3', 'hogar_total',# redundant\n              'hogar_adul','hogar_mayor',  'bedrooms', 'qmobilephone']\n\nhh_cont_col = ['dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0707079fdd1a31d63b262bb501cbd24e189f616b","trusted":true},"cell_type":"code","source":"sqr_col = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09d1450104f7cacb603f2fc73557bb50ece14279"},"cell_type":"markdown","source":"Sqaure of the features we have can be easily calculated afterwards. There is no need to keep them as a separated column."},{"metadata":{"_uuid":"07d096ef48d0e1ec8aada3f1fd381852b6945f0e","trusted":true},"cell_type":"code","source":"clean_all_df.drop(sqr_col, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45d6532ef2d58be44498a22d9779479583499352"},"cell_type":"markdown","source":"Next step is to reoder the columns to make all the ID columns at the begining."},{"metadata":{"_uuid":"3bc81e30f0ee44e9fd52ab5ef142a3d9c3da38c4","trusted":true},"cell_type":"code","source":"cols_list = id_col + ind_bool_col + ind_ordered_col + hh_bool_col + hh_ordered_col + hh_cont_col","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a3b242f4c48a4336a9a3140e54aa6feb34ed9f8","trusted":true},"cell_type":"code","source":"clean_all_df = clean_all_df[cols_list]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdbc638315ef8fad332f1d7b41e5a54a91ff4d6d","trusted":true},"cell_type":"code","source":"clean_all_df.isnull().sum()[clean_all_df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8bcbd8b7eef72e104e6fb9904ba9c1a666672b9"},"cell_type":"markdown","source":"`meaneduc` is the average years of education for adults (18+). The `NaN` value is unclear in this feature. As the portion of `NaN` value in this feature, we will calculate the median `meaneduc` value within same target and assign it to the `NaN`."},{"metadata":{"_uuid":"0785dab624cc67281c84bb5aa593ac394c16ce36","trusted":true},"cell_type":"code","source":"temp_all_df = pd.concat([clean_all_df, pd.Series(np.argmax(clean_all_df.loc[:, \"instlevel1\" : \"instlevel9\"].values, axis=1), name=\"eduindex\")], axis=1)\nedu_median_dict = temp_all_df.groupby(\"eduindex\")[\"meaneduc\"].median().to_dict()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be4fe84a9532d7280337f890d8e34513e9ca28d","scrolled":true,"trusted":true},"cell_type":"code","source":"def fill_na(x):\n    if pd.isnull(x[\"meaneduc\"]):\n        x.loc[\"meaneduc\"] = edu_median_dict[x.loc[\"eduindex\"]]\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ede7d13f4a06715753eefd0ad067ff7526b471","trusted":true},"cell_type":"code","source":"temp_all_df = temp_all_df.apply(fill_na, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876caec984da4d40fdf660968e43b9d7629c65a8","trusted":true},"cell_type":"code","source":"# edu_list = clean_all_df.loc[:, \"instlevel1\" : \"instlevel9\"].columns\nclean_all_df = temp_all_df.drop(\"eduindex\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acbed4109512977fffce2705f0d5c819dda1d941"},"cell_type":"markdown","source":"Due to only the heads of household will be used in scoring, we should make a seperate dataset for heads only. The rest data set can be used as a support features for heads."},{"metadata":{"_uuid":"1ead6652a18754ed916f45eae97b3128c329243f","trusted":true},"cell_type":"code","source":"heads_all_df = clean_all_df[clean_all_df.parentesco1 == 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"607a1bea9e12d1951878bcd7e3a4700562ceebe6"},"cell_type":"markdown","source":"### Remove the High Correlated Features"},{"metadata":{"_uuid":"5802baa18a11edbd2a3461886f942804339644dd","trusted":true},"cell_type":"code","source":"corr_matrix =  clean_all_df[clean_all_df.Target.notnull()].corr().abs()\nnp.fill_diagonal(corr_matrix.values, np.NaN)\nsorted_corr_series = corr_matrix.unstack().sort_values(kind=\"quicksort\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18317fdef35d4926271bb70692f072a7cb36c52e","trusted":true},"cell_type":"code","source":"sorted_corr_series[sorted_corr_series > 0.90].head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5734088bccc0530bd37c792eeffe93cf6fd0d322"},"cell_type":"markdown","source":"| Feature      | Describe           | \n| ------------- |-------------| \n| tamhog      | size of the household | \n| hhsize      | household size      | \n| hogar_total | # of total individuals in the household      |\n|r4t3| Total persons in the household |\n|tamviv|number of persons living in the household|\n\nWe will keep `tamviv` and drop all other redundant columns."},{"metadata":{"_uuid":"68bcdaa2169352d2a137e97026f68a335652c096","trusted":true},"cell_type":"code","source":"rm_cols = [\"tamhog\", \"r4t3\", \"hhsize\", \"hogar_total\", \"male\", \"area1\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec65e4c332101bab4fbae9289665297f27fc9a5d","trusted":true},"cell_type":"code","source":"clean_all_df.drop(rm_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcade8d1226b6e4138f4a345145090fe2585e360","trusted":true},"cell_type":"code","source":"for col in rm_cols:\n    for _list in [ind_bool_col, ind_ordered_col, hh_bool_col, hh_cont_col, hh_ordered_col]:\n        if col in _list:\n            _list.remove(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d09bfcffe63ce7e183bf25b7e15738da7866a30"},"cell_type":"markdown","source":"In the `cielorazo` column, the values is not consistent. There are mistake in it. We redefine it as the mark of people who don't have a roof."},{"metadata":{"_uuid":"f8bf1a908aa107533155881298524abf1f10128c","trusted":true},"cell_type":"code","source":"roof_list = [\"techozinc\", \"techoentrepiso\", \"techocane\", \"techootro\", \"cielorazo\"]\n\nclean_all_df[\"cielorazo\"] = clean_all_df[roof_list[:-1]].sum(axis=1).map({1:0, 0:1})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6eb07a85206f400384d7073e7dcc7ea270f7ab5"},"cell_type":"markdown","source":"## Household Aggregation\n---\nHeads of household approach reach its limit. Try the approach of group member of a household and train the models."},{"metadata":{"_uuid":"464edb67ab09ae418062ef3ea97b5834c06c25ab"},"cell_type":"markdown","source":"This tell us that there might be the member in a same household might get different targets, this is not fit for the specification of the competition. Since the targets of  the members within a same household should be labeled as same as the head of household."},{"metadata":{"_uuid":"0c90aa795f5472ce8563fa84952e9648a1e1622a","trusted":true},"cell_type":"code","source":"no_head_household_ids = clean_all_df.groupby(\"idhogar\").sum()[clean_all_df.groupby(\"idhogar\").sum()[\"parentesco1\"] == 0].index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f46bd972bb35e68e8b9ec6908e15318cad6de57"},"cell_type":"markdown","source":"We found out that the member inside those household dosen't have a head, which is considered not valid data. Drop them all"},{"metadata":{"_uuid":"44099782c8e6e2cfa7f8fa682c2aada1f248beca","scrolled":false,"trusted":true},"cell_type":"code","source":"clean_drop_no_head_df = clean_all_df.copy()\nfor x in no_head_household_ids:\n    clean_drop_no_head_df.drop(clean_drop_no_head_df.loc[clean_drop_no_head_df.idhogar == x, :].index, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fed273c3a3d1a87b3ef16b6c63fd9e5d96c9346","trusted":true},"cell_type":"code","source":"clean_heads_df = clean_drop_no_head_df.loc[clean_drop_no_head_df.parentesco1 == 1]\n\nheads_dict = clean_heads_df[[\"idhogar\", \"Target\"]].set_index(\"idhogar\").to_dict()[\"Target\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25c4db83a22b28a942acbb1702e077334aaec8c1","trusted":true},"cell_type":"code","source":"def synchronise_diff_target(x):\n    x.Target = heads_dict[x.idhogar]\n    return x\n\nclean_drop_no_head_df = clean_drop_no_head_df.apply(synchronise_diff_target, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66f76df0018e8690ebb7cd35f2ad9a785c8593cb","trusted":true},"cell_type":"code","source":"aggregation_list = [\"max\", \"sum\", \"min\", \"mean\", \"std\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f0fa8647b37dc9c257c2b2cfb4471897535f0fd","trusted":true},"cell_type":"code","source":"hh_agg_features = clean_drop_no_head_df[[\"idhogar\"] + ind_bool_col + ind_ordered_col].groupby(\"idhogar\").agg(aggregation_list)\nhh_agg_features.columns = [\"_\".join(x) for x in hh_agg_features.columns.ravel()]\n\nhh_agg_features = hh_agg_features.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b4dc5552b33c87491f02e065caf985a405b5d58","trusted":true},"cell_type":"code","source":"clean_heads_df = clean_drop_no_head_df[clean_drop_no_head_df.parentesco1 == 1].loc[:, id_col + hh_bool_col + hh_cont_col + hh_ordered_col].set_index(\"idhogar\")\n\nclean_heads_df = pd.concat([clean_heads_df, hh_agg_features], axis=1, sort=False).reset_index().rename(columns={\"index\":'idhogar'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd79999256c0f6cf85e9bf44ee56619113380baf"},"cell_type":"markdown","source":"If you are trained with tree algorithm, one hot encoding features need to be decoded."},{"metadata":{"_uuid":"333447802fdfb50ce5fbb95cba479651acc8ea21","trusted":true},"cell_type":"code","source":"X = clean_heads_df[clean_heads_df.Target.notna()].reset_index(drop=True)\ny = X.Target\nX_predict = clean_heads_df[clean_heads_df.Target.isna()].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6537408ef3d6cbc7f1b17d4f39eddd09367eb044","scrolled":true,"trusted":true},"cell_type":"code","source":"all_predict_df = clean_all_df[clean_all_df.Target.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99cb53f5731d9c330ad2e78914a37b3c416b20b"},"cell_type":"markdown","source":"## EDA\n---\nThere are quite a lot of features in the dataset. Let's take a look at the features that are import(judged by boosting algorithms) and features that are not that crucial."},{"metadata":{"_uuid":"aebc7048ad6b5776fb6211402ddfe50ca9a85d16","trusted":true},"cell_type":"code","source":"plot_hist(X, \"age_mean\", title=\"Average Age In Same Household\" ,x_label=\"Average age\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02915561c81b3364c75a5f640cd339d200ed7094","trusted":true},"cell_type":"code","source":"plot_hist(X, \"escolari_mean\", x_label=\"Average years of schooling\", title=\"Average Years of Schooling In Same Household\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"453892cb90fb226ec411c4b18dce7ab69a4930df","trusted":true},"cell_type":"markdown","source":"Above two features are relatively good features. Especially in average years of schooling feature, we can clear see a rough pattern that _non vulnerable household_ will normally have a longer average years of schooling while the majority of poverty's education time are less than 10 years."},{"metadata":{"_uuid":"5666a36b560495cc0693cd3271bba1e2d6ad7cd3","trusted":true},"cell_type":"code","source":"plot_hist(X, \"hacdor\", x_label=\"Overcrowding by bedrooms\", title=\"If the Bedrooms are Overcrowded\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cae2875ad7236d62b92799ed094fa394f5d98a4","trusted":true},"cell_type":"markdown","source":"This is an unimportant feature example. From the plot above we can see that the lion's share of all four targets are not overcrowding in their bedrooms. Thus, this feature will contribute little to the final prediction."},{"metadata":{"_uuid":"c474b891fb01e194d5f9cf8a23a14b33d6c041a5"},"cell_type":"markdown","source":"## Data Normalisation"},{"metadata":{"_uuid":"a5d9274ec7d3197a549848cc0b05c3071d4195e6","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bda34836eedabc4910d8fa0ca17c2faccc241814","trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nX.iloc[:, 3:], X_predict.iloc[:, 3:] = scaler.fit_transform(X.iloc[:, 3:]), scaler.fit_transform(X_predict.iloc[:, 3:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdd5782bccaa0ced19239772c98706f9d4e02fdc"},"cell_type":"markdown","source":"## PCA\nBefore the machine learning process taking place, a visualisation of the training set can be intuitive. We use the **PCA** technique to visualise the data set."},{"metadata":{"_uuid":"dead5c69a0e933c526343994ab0ef43723d056e6","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be8f2162a1e3fb4f2705ae37e34dd658a6465be6","trusted":true},"cell_type":"code","source":"numeric_train_df = X.iloc[:, 3:]\nlabel_series =  X.Target.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1b7a72e822ef6053bb185c5423add17be1cf98","trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3, whiten=True, svd_solver='auto', random_state=166446054) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f56c750772d0b094624fcb524c21a0b5b6e88a8","trusted":true},"cell_type":"code","source":"pca_train_array = pca.fit_transform(numeric_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b71af896f9e224898ff028302f029889584ab52c","trusted":true},"cell_type":"code","source":"pca_train_df = pd.DataFrame(pca_train_array)\n\npca_train_df = pd.concat([pca_train_df, label_series], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ea7c11c53dd86f3cd609fa9bf67d9868f436589","trusted":true},"cell_type":"code","source":"pca_3d_plot(pca_train_df=pca_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1958c64f46dd9027afa2e9378d6a176d31c18563"},"cell_type":"markdown","source":"Even though the 3D plot is not preferred in the data analysis, due to it is hard for a human to visualise the information that plots want to present, but the interactive 3D plots can make up this shortage and add more intuitions in the plot. Another reason is that the original dataset has more than 200 dimensions, 3D plots can preserve more information from the original dataset than 2D plots.\n\nAbove **PCA** data visualisation can tell use that the _non vulnerable households_ are separated from the rest targets."},{"metadata":{"_uuid":"7b124864e9d48727818928e009acf9e7637a013d"},"cell_type":"markdown","source":"## Clustering Analysis"},{"metadata":{"_uuid":"49a7cf3ac6c3bb07b37429f05083ba4f5d6d5dfa","trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom tempfile import mkdtemp\nfrom shutil import rmtree\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac587ef450c9926be99579425ccee6c211d9a663","trusted":true},"cell_type":"code","source":"result_dict = search_optimised_k([4, 8, 16, 32, 64, 128, 259], random_state=16446054, X=X.iloc[:, 3:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5ad28aab925f6dd913f20d41b0fb5274bd34e90","trusted":true},"cell_type":"code","source":"km_param_results = pd.DataFrame(result_dict)\nax = km_param_results.set_index(\"n_clusters\").plot(figsize=(12,8), fontsize=15, title=\"The Silhouette Score Relationship With Number of Centroid\")\nax.set_ylabel(\"Silhouette Score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8afa2fb24a9fa0da0ed1b1808ebe23ddc0c04b82"},"cell_type":"code","source":"km_predict = KMeans(n_jobs=-1, n_clusters=4, random_state=16446054).fit_predict(X.iloc[:, 3:])\nkm_X = pd.concat([pd.Series(km_predict, name=\"k_means_predict\").map({0:\"centroid1\",\n                                                                     1:\"centroid2\",\n                                                                     2: \"centroid3\",\n                                                                     3: \"centroid4\"\n                                                                    }), X.iloc[:, 2:]], axis=1)\n\nplot_hist(km_X, \"k_means_predict\", x_label=\"KMeans Prediction\", title=\"K Means Clustering Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"983afc9a9c23b7c9314dcf22f19a0ed802c16769"},"cell_type":"markdown","source":"The result is not ideal. For each centroid, the distribution of each target is similar. Let's try using **PCA** to transform the inputs first"},{"metadata":{"_uuid":"0e1238a31061096fc5b85e8b039a36b58c86c73a","trusted":true},"cell_type":"code","source":"result_dict = search_optimised_k([4, 8, 16, 32, 64, 128, 259], random_state=16446054, X=pca_train_df)\nkm_param_results = pd.DataFrame(result_dict)\nax = km_param_results.set_index(\"n_clusters\").plot(figsize=(12,8), fontsize=15, title=\"The Silhouette Score Relationship With Number of Centroid\")\nax.set_ylabel(\"Silhouette Score\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"385ca250ab0707b82c562f1cd4da052b8f4fdd81","trusted":true},"cell_type":"code","source":"km_predict = KMeans(n_jobs=-1, n_clusters=4, random_state=16446054).fit_predict(pca_train_df)\nkm_X = pd.concat([pd.Series(km_predict, name=\"k_means_predict\").map({0:\"centroid1\",\n                                                                     1:\"centroid2\",\n                                                                     2: \"centroid3\",\n                                                                     3: \"centroid4\",\n                                                                     4: \"centroid5\",\n                                                                     5: \"centroid6\",\n                                                                     6: \"centroid7\",\n                                                                     7: \"centroid8\"\n                                                                    }), X.iloc[:, 2:]], axis=1)\n\nplot_hist(km_X, \"k_means_predict\", x_label=\"KMeans Prediction\", title=\"K Means Clustering Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53a627f7b3c73b8fca3ba9f27a9e8a5fe3bf67ed"},"cell_type":"markdown","source":"The result of the above clustering is impressive, the centroid 3 has clustered more than 90% extreme poverty and 80% moderate poverty. Comparing to the raw input without **PCA**, the improvement is huge. A future exploration can be designed to vary the number of components as well as the number of centroids to find an optimised result of clustering. However, the further explore is beyond the domain of this project."},{"metadata":{"_uuid":"f5cea53194ae4a78428c3edbce1b20735b8eaf55"},"cell_type":"markdown","source":"# Machine Learning\n---\nIn this section, a variety of machine learning algorithm will be explored. This project will examine the _K nearest neighbour, support vector machine, ada tree boosting and light gradient boosting algorithm_ in order. The optimised hyperparameter searching process of `KNN` algorithm is built from scratch. The reset use the [cross validation grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class provided by `sklearn` model to find the optimised parameters. The performance of each algorithm is evaluated."},{"metadata":{"_uuid":"c97df8a7c093d8c22f4a64353ffba754f54cf9eb","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"088c9cbeda4a468405b7bd2820d387326c7a8758","collapsed":true},"cell_type":"markdown","source":"## KNN"},{"metadata":{"_uuid":"23727bb62f3547417347fe0c6140c349cb75ff8c","trusted":true},"cell_type":"code","source":"my_knn_agent = knn_agent()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c3f6d68e16f265abd21a28fa6b112cd7cd6302","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X.iloc[:, 3:], y, test_size=0.2, random_state=16446054, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d952f5d89c76607d510ae747f5cc9f6f8fa4e92","trusted":true},"cell_type":"code","source":"my_knn_agent.optimise_params(X_train=X_train, y_train=y_train, X_dev=X_test, y_dev=y_test, max_k_num=50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31bb15134e17743069b052c7b71b119c3f63656c"},"cell_type":"markdown","source":"Put the cursor on the plot we will easily locate the optimised test f1 score. The k number is 5 and with the uniform algorithm, it can achieve the 0.4 f1 score."},{"metadata":{"_uuid":"3e085911070a1e5bc3b5c9b4e4c638ba6ef3fdb7","trusted":true},"cell_type":"code","source":"uniform_f1, distance_f1 = my_knn_agent.evaluate(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\nprint(\"Unifrom test set f1 score: \", uniform_f1, \" Distance test set f1 score: \", distance_f1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee473584230e2a880a9cef93835cdc61213758ad","trusted":true},"cell_type":"code","source":"uniform_prediction, distance_prediction = my_knn_agent.predict(X_train=X_train, y_train=y_train, X_predict=X_predict.iloc[:, 3:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21c9c12b4e2fe3ed493706c7fc6ad457d1f7cc15"},"cell_type":"code","source":"# final_predict_df = generate_final_predict_df(uniform_prediction)\n\n# final_predict_df.to_csv(\"knn_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eddfdf31a4998ce1d714c7f1a630f7179b6fa0c4"},"cell_type":"markdown","source":"It has been prove that the submission has nothing to do with the rows which is not the head of household."},{"metadata":{"_uuid":"4b7a12908bc73fadd97a1add1a32413d134e59ce"},"cell_type":"markdown","source":"---\nFollow will use a `sklearn` approach to search for optimising hyperparameters. We will use the `GridSearchCV` approach to find the optimised hyperparameters. This is a extremely time consuming procedure. As we have found the optimised **K** number in the previous section, here we will stick with the number as benchmark to search other hyperparameters. However, if one wants to achieve the ideal optimised parameters, the search parameters should be combined, because, with different dimension of **X**, the opimised **K** might be different."},{"metadata":{"_uuid":"fe2efae86f12f464a131885eaad1cbb488a3160b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1fe885282ac9edc412d0fc1e8d164e2a6c2947","trusted":true},"cell_type":"code","source":"cachedir = mkdtemp()\n\nknn_pipe = Pipeline([('reduce_dim', PCA(random_state=16446054)), ('classify', my_knn_agent.uniform_knn)], memory=cachedir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b7bee342f32d355c67652499a439708637fde3","trusted":true},"cell_type":"code","source":"num_features_list = np.arange(10, 200, 20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a850f043aa6317ba895fcb8a94df3727dd7311d0","trusted":true},"cell_type":"code","source":"knn_param_grid = [\n    {\n        'reduce_dim': [PCA(random_state=16446054)],\n        'reduce_dim__n_components': num_features_list,\n    }, \n    {\n        'reduce_dim': [SelectKBest(chi2)],\n        'reduce_dim__k': num_features_list,\n    }\n]\n\nreducer_labels = ['PCA', 'KBest(chi2)']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bad4c7263c149699bae34166d80326b3b65c405","trusted":true},"cell_type":"code","source":"knn_grid = GridSearchCV(estimator=knn_pipe, cv=5, n_jobs=4, param_grid=knn_param_grid, scoring='f1_macro')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0579f70f7392fab983dc792ebbcfd7ad995c86cf","trusted":true},"cell_type":"code","source":"_ = knn_grid.fit(X.iloc[:, 3:], y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7e03ed29dc57ea99362fa4e34979d6b55e61e3b","trusted":true},"cell_type":"code","source":"report(knn_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"558fb06058fa58ac9bfe2aef4f5f4a9a30f160e5","trusted":true},"cell_type":"code","source":"knn_params_results = pd.DataFrame(knn_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb0cc0d4a11cc6f1cfe2338dc7d1559e7859adaf","scrolled":true,"trusted":true},"cell_type":"code","source":"knn_params_results = pd.concat([knn_params_results.param_reduce_dim__n_components.fillna(knn_params_results.param_reduce_dim__k).rename(\"num_features\"),\n                                knn_params_results.drop([\"param_reduce_dim__k\", \"param_reduce_dim__n_components\"], axis=1)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a9bb5f0a796aca6bc9180ec69bcb7368b6068b0","trusted":true},"cell_type":"code","source":"knn_params_results.loc[:, \"param_reduce_dim\"] = knn_params_results.param_reduce_dim.apply(lambda x: \"PCA\" if str(x).startswith(\"PCA\") else \"SelectKBest\").rename(\"algorithm\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02464ee37ae5be109438707ebe21c010b4d33edf","trusted":true},"cell_type":"code","source":"plot_train_test(\"num_features\", [\"mean_test_score\", \"mean_train_score\"], \n                hue=\"param_reduce_dim\", data=knn_params_results, y_label=\"F1 Score\",\n                title=\"Average Train/Test F1 Score Relationship with the Number of Features\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01800d34513b0febb6865004ea7bb6d2bec02411"},"cell_type":"markdown","source":"PCA will remain the information of original data while select k best techniques will discard the unimportance. The result is not a surprise that with the low dimensionality that PCA still have a relatively high accuracy. "},{"metadata":{"_uuid":"005b0ea738aac38120a7a1c5db04b6d68d34966d","trusted":true},"cell_type":"code","source":"# Clear the cache directory when you don't need it anymore\nrmtree(cachedir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fd380400fda0c0e225af8c7fa6ea34a8db77fe8"},"cell_type":"markdown","source":"## SVM\n---"},{"metadata":{"_uuid":"c21945ff76e60efdecaf466638fc2b46773af253","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.utils.class_weight import compute_class_weight","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e24ea68b095169b6f8d4cdbf581347efae6a11","trusted":true},"cell_type":"code","source":"cachedir = mkdtemp()\n\nsvc_pipe = Pipeline([('reduce_dim', PCA(random_state=166446054)), ('classify', SVC(random_state=16446054, class_weight=\"balanced\"))], memory=cachedir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28e5ac4f683a5c5712cdddc13b7094bd6e37c170","trusted":true},"cell_type":"code","source":"svc_param_grid = [\n    {\n        'reduce_dim__n_components': [10, 70, 140, 200],\n        'classify__kernel': ['rbf', 'poly'],\n        'classify__C': [0.1, 1, 10, 50],\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"455748219e24c229041437094505ead295c28078","trusted":true},"cell_type":"code","source":"svc_grid = GridSearchCV(svc_pipe, cv=5, n_jobs=4, param_grid=svc_param_grid, scoring='f1_macro', refit=True)\n\n_ = svc_grid.fit(X.iloc[:, 3:], y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4767ecb2158e57273051997f3bb786934f1dcbd","trusted":true},"cell_type":"code","source":"report(svc_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1c88198b46c259f5ccad7267609c64cb6ff3f5f","trusted":true},"cell_type":"code","source":"svc_params_results = pd.DataFrame(svc_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee6fda1ea5df13753d4f21721dd2ae04cd9e64ad","trusted":true},"cell_type":"code","source":"subplot_test_train(x=\"param_reduce_dim__n_components\", y=[\"mean_test_score\", \"mean_train_score\"], z=\"param_classify__C\", data=svc_params_results, hue=\"param_classify__kernel\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53dfc56161271dc0e2fbcf0f4cd53ffb76805e40"},"cell_type":"markdown","source":"From the subplots, we can extract follow intuitions:\n* With high dimensionality, polynomial kernel `SVC` is underfitting\n* `rbf` kernel will be more stable for a varied range of dimensions of the dataset\n* The highest test set score is gained with polynomial kernel and big number C(50) as well as a high dimensional dataset."},{"metadata":{"_uuid":"fbc8790a67041115d8eefd011aed78f73af70be2","trusted":true},"cell_type":"code","source":"rmtree(cachedir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33e04317978a683416c6db015ca3617ea938dd90","trusted":true},"cell_type":"code","source":"# clf.fit(X.iloc[:, 3:], X.Target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2c75d29537fa2d2a63143b90e58ea420451d49d","trusted":true},"cell_type":"code","source":"# svc_test_predict = clf.predict(heads_test_df.iloc[:, 3:])\n# print(\"SVC test prediction f1 score: \",f1_score(y_pred=svc_test_predict, y_true=heads_test_df.Target, average=\"macro\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b244671051b8dc208334c15e9e8368174af41a34","trusted":true},"cell_type":"code","source":"# svc_predict = clf.predict(X_predict.iloc[:, 3:])\n\n# final_predict = generate_final_predict_df(svc_predict)\n\n# final_predict.to_csv(\"svc_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8abda93c8dd91051ad2248ade6dcdf7bbfba4001"},"cell_type":"markdown","source":"## AdaBoost\n---\nBoosting algorithm is efficient on dataset with a lot of features. As a result it achieved a great success on the facial recognition process with haar-like features which are huge in this task. Intuit\n"},{"metadata":{"_uuid":"7a4633fb1ce86fc8d774505b0363f17bbadd3274","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ccdbf6ded0bd4926e1f67a493ea7dad96c139a","trusted":true},"cell_type":"code","source":"ada_tree_real = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1.5\n)\n\nada_tree_discrete = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1.5,\n    algorithm=\"SAMME\"\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4163db763a25e7545378397db1555f6d6534dcb","trusted":true},"cell_type":"code","source":"ada_tree_real.fit(X_train, y_train)\nada_tree_discrete.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b97d8a5e7a517f7cf3b851cc3d491379ecf95b7e","trusted":true},"cell_type":"code","source":"ada_report(ada_models=(ada_tree_real, ada_tree_discrete), X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a29c4af56be34157e7245e1c4931ef1b747af82e"},"cell_type":"markdown","source":"The real boosting algorithm can achieve a good accuracy on the test set within a small iteration. However, the real boosting algorithm seems prone to overfitting due to the big gap between the train set and test set scores. But after 400 estimators , the difference between the two algorithms become small."},{"metadata":{"_uuid":"0f6277a038ff9a4828c944d4e9ffd926f9694dee"},"cell_type":"markdown","source":"Afterwards, let's find out, for the boosting algorithm which can select good features out of bad features, if the more features the better."},{"metadata":{"_uuid":"356940ae37f59b2791db934f61cc30c68ed343c8","trusted":true},"cell_type":"code","source":"cachedir = mkdtemp()\n\nada_pipe = Pipeline([('reduce_dim', PCA(random_state=166446054)),\n                     ('classify', AdaBoostClassifier(random_state=16446054))], memory=cachedir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ef2d00eb7ad5b6fd43f615b6a57696721038d9"},"cell_type":"markdown","source":"Try to evaluate the effect of dimension "},{"metadata":{"_uuid":"1146fb551b366150a82bfb56c764802f656369a0","trusted":true},"cell_type":"code","source":"ada_param_grid = [\n    {\n        'reduce_dim__n_components': [10, 70, 140, 259],\n        'classify__algorithm': ['SAMME.R', 'SAMME'],\n        'classify__learning_rate': [1],\n        'classify__n_estimators': [300],\n        'classify__base_estimator': [DecisionTreeClassifier(max_depth=2)]\n    }\n]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"28478d21225275d3491434e1c4920e9e992ccb74","scrolled":true,"trusted":true},"cell_type":"code","source":"ada_grid = GridSearchCV(ada_pipe, cv=5,\n                        n_jobs=4, param_grid=ada_param_grid,\n                        scoring='f1_macro', refit=True, )#verbose=20) # This can show the progress of searching\n\n_ = ada_grid.fit(X.iloc[:, 3:], y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d670c28399cc3a9b537e5f2156c5ba370725b50b","trusted":true},"cell_type":"code","source":"report(ada_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fdaec28ffa8a0f869efd7ec39ca1af82dc402e4","trusted":true},"cell_type":"code","source":"ada_param_results = pd.DataFrame(ada_grid.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea72c403c6ce8483c2a4e0d4d3b9d7a3d8200559","trusted":true},"cell_type":"code","source":"plot_train_test(x=\"param_reduce_dim__n_components\", y=[\"mean_test_score\", \"mean_train_score\"], data=ada_param_results, hue=\"param_classify__algorithm\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58fe5d444aa49505cead53414e5e288abcedc389","trusted":true},"cell_type":"markdown","source":"The result above demonstrates the fact that with more dimensionalities of the data set, the adaboost algorithm actually becomes prone to overfitting problem. The gap between the training set and testing set becomes larger as the dimension growing bigger. \n\nIt is to my surprise that adaboost is still prone to high dimensional data, an appropriate  dataset preprocessing can still help to increase the performance "},{"metadata":{"_uuid":"728a45740cc49b2c4461f34a4340b1b7e046e7c4","trusted":true},"cell_type":"code","source":"# real_test_predict = np.array(ada_real_test_result).mean(axis=0).round().astype(int)\n# discrete_test_predict = np.array(ada_discrete_test_result).mean(axis=0).round().astype(int)\n# print(\"Real test prediction f1 score: \",f1_score(y_pred=real_test_predict, y_true=heads_dev_df.Target, average=\"macro\"))\n# print(\"Discrete test prediction f1 score: \",f1_score(y_pred=discrete_test_predict, y_true=heads_dev_df.Target, average=\"macro\"))\n\n# real_test_predict = ada_tree_real.predict(heads_test_df.iloc[:, 3:])\n# discrete_test_predict = ada_tree_discrete.predict(heads_test_df.iloc[:, 3:])\n# print(\"Real test prediction f1 score: \",f1_score(y_pred=real_test_predict, y_true=heads_test_df.Target, average=\"macro\"))\n# print(\"Discrete test prediction f1 score: \",f1_score(y_pred=discrete_test_predict, y_true=heads_test_df.Target, average=\"macro\"))\n\n# real_predict = ada_tree_real.predict(X_predict.iloc[:, 3:]).astype(int)\n# discrete_predict = ada_tree_discrete.predict(X_predict.iloc[:, 3:]).astype(int)\n\n# real_final_predict = generate_final_predict_df(real_predict)\n# discrete_finial_predict = generate_final_predict_df(discrete_predict)\n\n# real_final_predict.to_csv(\"real_ada_submission.csv\", index=False)\n\n# discrete_finial_predict.to_csv(\"discrete_ada_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"186ec5f61718fa359ddcdbb7ba0d85d5de8b003f"},"cell_type":"markdown","source":"## LightGBM\n---"},{"metadata":{"_uuid":"8537649a738bf3da37b2a406ecd187833fc12490","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55de0578173235f6e5dafc59c550fb3183062cdc","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f377291d72cbf95252cdf1351e560e90decf15b0","trusted":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24102d040f261b09070eafac45c216c14a4c6ccc","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9704f688834594d25a07f952d8ff2577bc75597"},"cell_type":"markdown","source":"Noted the parameters here are not optimised ones that help me to get my best score, the optimised one will take ages to train so I change it to a fast training mode with a acceptable accuracy"},{"metadata":{"_uuid":"a7d58d6d7a5893567c002ebd115a8b9e8bad0400","trusted":true},"cell_type":"code","source":"lgb_clf = lgb.LGBMClassifier(learning_rate=0.01, n_estimators=15000, \n                             objective='multiclass', matric='logloss', boosting_type='dart',\n                             num_leaves=700, max_depth=10,\n                             class_weight='balanced',  silent=True, n_jobs=-1,\n                             colsample_bytree =  0.93, min_child_samples = 95,  subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021c4656dc2243337707e003120dcf4bfff38aa5","scrolled":true,"trusted":true},"cell_type":"code","source":"kfold = 10\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\npredicts_result = []\ntest_result = []\nfor train_index, test_index in kf.split(X.iloc[:, 3:], X.Target):\n    print(\"#\"*10)\n    X_train, X_val = X.iloc[:, 3:].iloc[train_index], X.iloc[:, 3:].iloc[test_index]\n    y_train, y_val = X.Target.iloc[train_index], X.Target.iloc[test_index]\n    lgb_clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=1000, verbose=10000) #eval_metric=f1_macro_evaluation)\n    test_result.append(f1_score(y_pred=lgb_clf.predict(X_val), y_true=y_val, average=\"macro\"))\n    predicts_result.append(lgb_clf.predict(X_predict.iloc[:, 3:]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb0aa43714ee3f422fca62605dd1440b1841b73","trusted":true},"cell_type":"code","source":"for result in test_result:\n    print(result)\nprint(\"Mean test f1: \", np.mean(test_result))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee5c062da14bfef9460f43dc9e0e57ee8b50f16","trusted":true},"cell_type":"code","source":"lgb_train_predict = lgb_clf.predict(X.iloc[:, 3:])\nlgb_test_f1 = f1_score(y_pred=lgb_train_predict, y_true=X.Target, average=\"macro\")\nprint(\"LGBM train f1: \", lgb_test_f1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d88572b929051021729fcf5a7377a822d26626bc","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# lgb_predict = np.array(predicts_result).mean(axis=0).round().astype(int)\n# lgb_predict = lgb_clf.predict(X_predict.iloc[:, 3:])\n\n# final_predict = generate_final_predict_df(lgb_predict)\n\n# final_predict.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"106127973459abce8c8003dbff39e4d55b5fda74"},"cell_type":"code","source":"plot_features(lgb_clf.feature_importances_, index=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5196adb930272d17562262558445bd49d382bff9"},"cell_type":"markdown","source":"An advantage for boosting algorithm is that it can find the importance of each feature. The actually developing workflow can be circular. We can reduce the unimportant features from the dataset and run boosting algorithm again to see the difference. Another thought could be use **PCA** to merge all the unimportant features into one feature, to sum up the information from those features."},{"metadata":{"trusted":true,"_uuid":"5ee7137c641a91f2249272bfe16c6af9ed7b953b"},"cell_type":"markdown","source":"# Conclusion\n---\nAs the real-life machine learning problem, the data wrangling part is crucial for models training phase. In this project, `NaN` value is filled thoughtfully and the highly correlated features are dropped. To merge the members within the same household can help increase the accuracy of models.\n\nA clustering analysis combined with **PCA** gives an interesting result that one of the centroids can separate 90% extreme poverty and 80% moderate poverty. Another finding is that **PCA** can help improve the clustering result.\n\nIn the machine learning section, a variety of algorithms are evaluated to find the optimised parameters. The main trend is that as the dimensionality of data becomes larger the models become prone to overfitting problem. This rule even applies to ada boost algorithm. **PCA** is a good choice to reduce the dimension than `selectKBest` algorithm overall, especially when the reduction ratio is large. It should be included in the standard process of machine learning workflow especially for the algorithm like K nearest neighbours and support vector machine.\n\nThe boosting algorithm has the best performance among all the algorithms that evaluated in this project for this specific dataset. The **LightGBM** algorithm is especially good for this task."},{"metadata":{"_uuid":"e2ec4a404392d34cc0b990b7bf7f4aaba9dc8e81"},"cell_type":"markdown","source":"# Append\n---\nThis section record the plot of my submission and the improvement I have done in that submission"},{"metadata":{"trusted":true,"_uuid":"422badb28297cd810614b24a802e98c02f0c61ed"},"cell_type":"code","source":"submission_history = pd.read_excel(\"../input/competitionjournal/costa_rican_competition_result.xlsx\")\n\nsubmission_history.set_index(\"sequence\", inplace=True)\n\nplot_submission(submission_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94a0e4bccfc253c47097a7ef32b4d9b284aaadcb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}