{"cells": [{"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1328926e-4cda-45cb-95d3-ebe1658df8c2", "_uuid": "1b100b7e5361d81b846d8ef32eb7fca73e4c699e"}, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import re # to separate pages based on language (regular expression)\n", "import matplotlib.pyplot as plt # to visualize data\n", "from pandas.tools.plotting import autocorrelation_plot # to visualize and configure the parameters of ARIMA model\n", "from statsmodels.tsa.arima_model import ARIMA # to make an ARIMA model that fits the data"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b874b339-ccb9-4204-b00e-fd457d8e7f87", "_uuid": "23c16cdccec1a822a5572e7941c52a37477849fc"}, "source": ["train_df = pd.read_csv('../input/train_1.csv').fillna(0)\n", "train_df.head()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8c76bb20-1525-4f67-8838-fbe84cd63988", "_uuid": "3151f66b33ce9ffd356bccbf7c48f77c64d65c95"}, "source": ["train_df.info()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f80d5f97-0e7c-487a-b4dc-535404a572f2", "_uuid": "c3a9f0a88b6f9202cfa6e84079beb08091d43907"}, "source": ["### Simple code to get the language of any given page"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "73123a60-bf2a-4aa2-a826-a5384df34627", "collapsed": true, "_uuid": "932101ffc8467879fbb00be730bc4dc13bd110b3"}, "source": ["def find_language(url):\n", "    res = re.search('[a-z][a-z].wikipedia.org',url)\n", "    if res:\n", "        return res[0][0:2]\n", "    return 'na'\n", "\n", "train_df['lang'] = train_df.Page.map(find_language)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "93a78312-479e-43e6-b5e8-cfc562e04c55", "_uuid": "4e5baf613e8b435507a36d96a11c7e038de5e3f2"}, "source": ["### Here we separate all the pages based on their language and average them up to find views per page per language"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e78bfee2-98b3-4fd2-8d70-be26928f01e2", "collapsed": true, "_uuid": "2049fb6b9a5a82d6f22186e0f4a76ef84238ac31"}, "source": ["lang_sets = {}\n", "lang_sets['en'] = train_df[train_df.lang=='en'].iloc[:,0:-1]\n", "lang_sets['ja'] = train_df[train_df.lang=='ja'].iloc[:,0:-1]\n", "lang_sets['de'] = train_df[train_df.lang=='de'].iloc[:,0:-1]\n", "lang_sets['na'] = train_df[train_df.lang=='na'].iloc[:,0:-1]\n", "lang_sets['fr'] = train_df[train_df.lang=='fr'].iloc[:,0:-1]\n", "lang_sets['zh'] = train_df[train_df.lang=='zh'].iloc[:,0:-1]\n", "lang_sets['ru'] = train_df[train_df.lang=='ru'].iloc[:,0:-1]\n", "lang_sets['es'] = train_df[train_df.lang=='es'].iloc[:,0:-1]\n", "\n", "sums = {}\n", "for key in lang_sets:\n", "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "189f03e0-27c8-40b5-b85b-1755346ff276", "_uuid": "72f310df144bb407f9a2707593d279733f419351"}, "source": ["### Plots of average number of views for all different languages per day "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "dda148fb-ab53-4935-a962-b10441e04dee", "scrolled": true, "_uuid": "9872d9366c5a581a3d37500eeb81267850987de7"}, "source": ["days = [r for r in range(sums['en'].shape[0])]\n", "\n", "fig = plt.figure(1,figsize=[10,10])\n", "plt.ylabel('Views per Page')\n", "plt.xlabel('Day')\n", "plt.title('Pages in Different Languages')\n", "labels={'en':'English','ja':'Japanese','de':'German',\n", "        'na':'Media','fr':'French','zh':'Chinese',\n", "        'ru':'Russian','es':'Spanish'\n", "       }\n", "\n", "for key in sums:\n", "    plt.plot(days,sums[key],label = labels[key] )\n", "    \n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6b63e83a-55c3-4ee7-8c33-e87ba30ddc94", "_uuid": "80535b2fca75d2c25412b789af824dd0119f5486"}, "source": ["### Now we can plot Autocorrelation and Partial Autocorrelation graphs for all these languages, to estimate the hyperparameters used in training the ARIMA model."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "553f30cd-2d9f-4d41-ab0b-74fd49e755e2", "_uuid": "4964b1bf73a89aa929d3b82b25511be686c030c2"}, "source": ["from statsmodels.tsa.stattools import pacf\n", "from statsmodels.tsa.stattools import acf\n", "\n", "for key in sums:\n", "    fig = plt.figure(1,figsize=[10,5])\n", "    ax1 = fig.add_subplot(121)\n", "    ax2 = fig.add_subplot(122)\n", "    data = np.array(sums[key])\n", "    autocorr = acf(data)\n", "    pac = pacf(data)\n", "\n", "    x = [x for x in range(len(pac))]\n", "    ax1.plot(x[1:],autocorr[1:])\n", "\n", "    ax2.plot(x[1:],pac[1:])\n", "    ax1.set_xlabel('Lag')\n", "    ax1.set_ylabel('Autocorrelation')\n", "\n", "    ax2.set_xlabel('Lag')\n", "    ax2.set_ylabel('Partial Autocorrelation')\n", "    print(key)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f9982671-4b9f-460a-a945-e11b0db92d13", "_uuid": "8a19984748d482717c1d2b3d4dc3195c58d3d5fd"}, "source": ["### Looking at all these graphs we conclude\n", " 1. We won't be needing any differencing for en, ru, fr, na (d=0). For the others we need to subtract them once from their predecessor (d=1).\n", " 2. For ja, de, zh, es there is a trend of peaks after 7 days. Thus a lag of 7 might be used and for the rest a lag of 4 should work okay."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "663b32a7-e0ec-44d3-92d2-5d9eb48a536a", "_uuid": "c19181bd2a7d4ad0c5808a89a0170b43cbb69f9f"}, "source": ["## Now we will be training ARIMA models for different languages"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3c212765-862c-4b11-9df1-e7f1ea55fc07", "_uuid": "840ad13d6803387b2f2618871885eb017a43d7da"}, "source": ["We tune in the parameters discussed above and train our ARIMA models as follows:"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "40529eb1-79af-4eb5-8c49-d4527d1c7a6a", "_uuid": "ecc8041bcb019ecda1fd7c176664fe000fe4bb12"}, "source": ["params = {'en': [4,1,0], 'ja': [7,1,1], 'de': [7,1,1], 'na': [4,1,0], 'fr': [4,1,0], 'zh': [7,1,1], 'ru': [4,1,0], 'es': [7,1,1]}\n", "\n", "for key in sums:\n", "    data = np.array(sums[key][0:300])\n", "    data1 = np.array(sums[key])\n", "    result = None\n", "    arima = ARIMA(data,params[key])\n", "    result = arima.fit(disp=False)\n", "    #print(result.params)\n", "    pred = result.predict(301,608,typ='levels')\n", "    x = [i for i in range(644)]\n", "    i=0\n", "    \n", "    print(key)\n", "    plt.plot(x[2:len(data1)],data1[2:] ,label='Data')\n", "    plt.plot(x[len(data)+1:len(data)+310],pred,label='ARIMA Model')\n", "    \n", "    plt.xlabel('Days')\n", "    plt.ylabel('Views')\n", "    plt.legend()\n", "    plt.show()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "158e4b78-c852-45ad-b0f2-1ab3388610e8", "_uuid": "1b971a2fcacb0bf3b724010256bac5aae1f4d4bf"}, "source": ["params = {'en': [4,1,0], 'ja': [7,1,1], 'de': [7,1,1], 'na': [4,1,0], 'fr': [4,1,0], 'zh': [7,1,1], 'ru': [4,1,0], 'es': [7,1,1]}\n", "\n", "for key in sums:\n", "    data = np.array(sums[key])\n", "    result = None\n", "    arima = ARIMA(data,params[key])\n", "    result = arima.fit(disp=False)\n", "    print(key)\n", "    print(sums[key])\n", "    pred = result.predict(550,608,typ='levels')\n", "    print(pd.Series(pred))\n", "    \n", "    #sums[key].append(pred,ignore_index=True)\n", "    break;\n", "    "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "5664c5e6-5905-49a3-8ff6-b53e15719cb4", "collapsed": true, "_uuid": "57944f6ece133c079ace6b5056419cdf0a88a7f1"}, "source": ["### Now let's use this ARIMA model and submit the output ?"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "24dbd2e0-fe66-496c-b551-86cfbc3e5323", "_uuid": "891746a8792a25b6a14254fe0074c051f3d08ab1"}, "source": ["train_df.head()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "899bf21b-f155-4cba-b5db-3a825b70f334", "_uuid": "d051db6dcb9531e07d489432ee433b79a6de162e"}, "source": ["train_df = train_df.drop('Page',axis = 1)\n", "train_df.shape"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "687c4c41-0023-465f-b895-48765673c6df", "_uuid": "2e6cbdaf51a192f7dd0523dbec04d3297a57c02e"}, "source": ["print(sums['en'])"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ed76d361-8362-4aaa-abf8-1de17594e9a4", "collapsed": true, "_uuid": "cc04998912631b6b518ccc051830d8c42cd9a9de"}, "source": ["#Packages for pre processing\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", " # Importing the Keras libraries and packages for LSTM\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import LSTM"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "71cbc03a-06e7-4bcf-a369-03ccb8672068", "scrolled": true, "collapsed": true, "_uuid": "4a65bd5adf056ca55768340e014966d36a13cda8"}, "source": [" for key in sums:\n", "    row = [0]*sums[key].shape[0]\n", "    for i in range(sums[key].shape[0]):\n", "        row[i] = sums[key][i]\n", "\n", "\n", "    #Using Data From Random Row for Training and Testing\n", "\n", "    X = row[0:549]\n", "    y = row[1:550]\n", "\n", "    # Splitting the dataset into the Training set and Test set\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n", "\n", "    # Feature Scaling\n", "    sc = MinMaxScaler()\n", "    X_train = np.reshape(X_train,(-1,1))\n", "    y_train = np.reshape(y_train,(-1,1))\n", "    X_train = sc.fit_transform(X_train)\n", "    y_train = sc.fit_transform(y_train)\n", "\n", "\n", "    #Training LSTM\n", "\n", "    #Reshaping Array\n", "    X_train = np.reshape(X_train, (384,1,1))\n", "\n", "    # Initialising the RNN\n", "    regressor = Sequential()\n", "\n", "    # Adding the input layerand the LSTM layer\n", "    regressor.add(LSTM(units = 8, activation = 'relu', input_shape = (None, 1)))\n", "\n", "\n", "    # Adding the output layer\n", "    regressor.add(Dense(units = 1))\n", "\n", "    # Compiling the RNN\n", "    regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n", "\n", "    # Fitting the RNN to the Training set\n", "    regressor.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n", "\n", "    # Getting the predicted Web View\n", "    inputs = X\n", "    inputs = np.reshape(inputs,(-1,1))\n", "    inputs = sc.transform(inputs)\n", "    inputs = np.reshape(inputs, (549,1,1))\n", "    y_pred = regressor.predict(inputs)\n", "    y_pred = sc.inverse_transform(y_pred)\n", "\n", "    print(key)\n", "    #Visualising Result\n", "    plt.figure\n", "    plt.plot(y, color = 'red', label = 'Real Web View')\n", "    plt.plot(y_pred, color = 'blue', label = 'Predicted Web View')\n", "    plt.title('Web View Forecasting')\n", "    plt.xlabel('Number of Days from Start')\n", "    plt.ylabel('Web View')\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a48475c0-6d98-42e8-8e92-d294b6307f0d", "_uuid": "b0672f3adb1af10e22561b4e6a113169d582308f"}, "source": ["### Now we will combine the 2 models and make an ensemble out of it in the next step"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}