{"cells":[{"metadata":{"_uuid":"ada861a85e9549dca27667692da408c5fdccbaa5"},"cell_type":"markdown","source":"### About\nSince I am new to learning from image segmentation and kaggle in general I want to share my noteook.\nI saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n\n- As said, the field is new to me so I am open to suggestions.\n- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n- Added stratification by the amount of salt contained in the image.\n- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n- Added dropout to the model which seems to improve performance."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom keras import optimizers\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom tqdm import tqdm_notebook\nfrom imgaug import augmenters as iaa","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e2e936b085526037c9437385102f26e42c758c68"},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"this_split=5\ndebug=False\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\nif debug:\n    train_df=train_df.iloc[:1000,:]\n    depths_df=depths_df.iloc[:1000,:]\n    test_df=test_df.iloc[:2000,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d13744b72d651610e0c46fe273cfae3ae8ac3af3","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"trusted":true,"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d","collapsed":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86620c6a070571895f4f36ec050a25803915ed74","collapsed":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1137f0a009f10b5f69e4dade5f689e744e9ce1d6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b13d1ecc7004832e8e042d034922796263054b7"},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5e66ff4809ea2f9a679b7ddbda5028dc324137a","collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670","collapsed":true},"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2391c568019151b098a002937516bb77a506f403"},"cell_type":"markdown","source":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data."},{"metadata":{"trusted":true,"_uuid":"6ae7b7011b7de3caed58f9ca3939df15ffa319ad","collapsed":true},"cell_type":"code","source":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"},"cell_type":"markdown","source":"# Show some example images"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1a6bc85ee458f72c0917edf77895d5abc5eaf3ee","collapsed":true},"cell_type":"code","source":"# max_images = 60\n# grid_width = 15\n# grid_height = int(max_images / grid_width)\n# fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n# for i, idx in enumerate(train_df.index[:max_images]):\n#     img = train_df.loc[idx].images\n#     mask = train_df.loc[idx].masks\n#     ax = axs[int(i / grid_width), i % grid_width]\n#     ax.imshow(img, cmap=\"Greys\")\n#     ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n#     ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n#     ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n#     ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n#     ax.set_yticklabels([])\n#     ax.set_xticklabels([])\n# plt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"trusted":true,"_uuid":"a517622135321d17e4aaad749def999205da358c","collapsed":true},"cell_type":"code","source":"def conv_block(m, dim, acti, bn, res, do=0):\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n\tn = BatchNormalization()(n) if bn else n\n\tn = Dropout(do)(n) if do else n\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n\tn = BatchNormalization()(n) if bn else n\n\treturn Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n\tif depth > 0:\n\t\tn = conv_block(m, dim, acti, bn, res)\n\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n\t\tif up:\n\t\t\tm = UpSampling2D()(m)\n\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n\t\telse:\n\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n\t\tn = Concatenate()([n, m])\n\t\tm = conv_block(n, dim, acti, bn, res)\n\telse:\n\t\tm = conv_block(m, dim, acti, bn, res, do)\n\treturn m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n\ti = Input(shape=img_shape)\n\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n\treturn Model(inputs=i, outputs=o)\ndef step_decay(epoch):\n    if epoch<=60:\n        return 0.01\n    elif epoch<=90:\n        return 0.001\n    else:\n        return 0.0001\n\ndef build():\n#     sgd=optimizers.SGD(lr=0.01,decay=1e-4,momentum=0.9)\n    model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\n    model.compile(loss=bce_dice_loss, optimizer='adam', metrics=[\"accuracy\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"41009d15bdace219ca11851a2ac3d90e0a72cf32"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"53fb1f01cedf8564f148a5c0b54132e54ad9bc73"},"cell_type":"code","source":"\n# for i,(trdex,valdex) in enumerate(skf.split(X=train_df.index.values,y=train_df.coverage_class.values)):\n#     if i!=this_split:\n#         continue\ndef valid_on_best_iou(x_valid,y_valid,time):\n    threshes=[]\n    model = load_model(str(this_split)+\"_\"+str(time)+'_keras.model',{'bce_dice_loss': bce_dice_loss})\n    preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\n    preds_valid = np.array([downsample(x) for x in preds_valid])\n    y_valid = np.array([downsample(x) for x in y_valid])\n    thresholds = np.linspace(0, 1, 50)\n    ious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])\n    threshold_best_index = np.argmax(ious[9:-10]) + 9\n    iou_best = ious[threshold_best_index]\n    threshold_best = thresholds[threshold_best_index]\n    threshes.append(threshold_best)\n#     print(thresholds,ious)\n#     print(threshold_best,iou_best)\n    return threshold_best,iou_best,thresholds,ious\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e0a4758a99039a60da9d897a700f350a4734977","collapsed":true},"cell_type":"code","source":"seq = iaa.Sequential([\n    iaa.Fliplr(0.5), # horizontally flip\n    iaa.OneOf([\n        iaa.Noop(),\n        iaa.Noop(),\n        iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n    ])\n])\ndef gen_flow_for_two_inputs(X, y):\n    genX1 = gen.flow(X,y,  batch_size=batch_size)\n    while True:\n        X=genX1.next()\n        img_mask=np.concatenate([X[0],X[1]],axis=3)\n        img_mask_aug=seq.augment_images(img_mask)\n        yield np.expand_dims(img_mask_aug[:,:,:,0],axis=3),np.expand_dims(img_mask_aug[:,:,:,1],axis=3)\n#         imgs=[]\n#         masks=[]\n#         for index in range(len(X[0])):\n#             img=X[0][index]\n#             mask=X[0][index]\n#             img,mask,_=train_augment(img,mask,0)\n#             imgs.append(img)\n#             masks.append(mask)\n#         imgs=np.array(imgs)\n#         masks=np.array(masks)\n#         yield np.expand_dims(imgs,axis=3),np.expand_dims(masks,axis=3)\n#         yield X[0], X[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8621e4289d8e696ccc8a5bf3ae472f0bdb897fa2","collapsed":true},"cell_type":"code","source":"print(type(depths_df.z.values[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7762357887d8046852eaef0a8b07313b0176a1b6"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n\nn_split=10\n\nmodels=[]\nhistorys=[]\nious_list=[]\nepochs = 90\nbatch_size=32\nval_loss_limit=0.29\nval_iou_limit=0.796\nif debug:\n    epochs=2\n    n_split=4\n    val_loss_limit=1\n    val_acc_limit=0.1\n#print(train_df.index.values)\nskf=StratifiedKFold(n_splits=n_split)\nfor i,(trdex,valdex) in enumerate(skf.split(X=train_df.index.values,y=train_df.coverage_class.values)):\n    f=open('train_3600_'+str(i),'w')\n    for ids in train_df.index.values[trdex]:\n        f.write('train/'+ids+'\\n')\n    f.close()\n    f=open('valid_400_'+str(i),'w')\n    for ids in train_df.index.values[valdex]:\n        f.write('train/'+ids+'\\n')\n    f.close()\n#     print(trdex)\n#     f1=open('train_3200_sallow_'+str(i),'w')\n#     f2=open('train_3200_mid_'+str(i),'w')\n#     f3=open('train_3200_deep_'+str(i),'w')\n#     for ids in train_df.index.values[trdex]:\n#         if depths_df.loc[ids,'z']<=400:\n#             f1.write('train/'+ids+'\\n')\n#         elif depths_df.loc[ids,'z']<=700:\n#             f2.write('train/'+ids+'\\n')\n#         else:\n#             f3.write('train/'+ids+'\\n')\n#     f1.close()\n#     f2.close()\n#     f3.close()\n#     f1=open('valid_800_sallow_'+str(i),'w')\n#     f2=open('valid_800_mid_'+str(i),'w')\n#     f3=open('valid_800_deep_'+str(i),'w')\n#     for ids in train_df.index.values[valdex]:\n#         if depths_df.loc[ids,'z']<=400:\n#             f1.write('train/'+ids+'\\n')\n#         elif depths_df.loc[ids,'z']<=700:\n#             f2.write('train/'+ids+'\\n')\n#         else:\n#             f3.write('train/'+ids+'\\n')\n#     f1.close()\n#     f2.close()\n#     f3.close()\n#     if i!=this_split:\n#         print(i,this_split)\n#         continue\n#     ids_train=train_df.index.values[trdex]\n#     x_train=np.array(train_df.loc[ids_train].images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n#     y_train=np.array(train_df.loc[ids_train].masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n#     #Augmengtation\n# #     x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n# #     y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n#     #Valid Set\n#     ids_valid=train_df.index.values[valdex]\n#     y_valid=np.array(train_df.loc[ids_valid].masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n#     x_valid=np.array(train_df.loc[ids_valid].images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1) \n#     #Visualization\n# #     fig, axs = plt.subplots(2, 10, figsize=(15,3))\n# #     for j in range(10):\n# #         axs[0][j].imshow(x_train[j].squeeze(), cmap=\"Greys\")\n# #         axs[0][j].imshow(y_train[j].squeeze(), cmap=\"Greens\", alpha=0.3)\n# #         axs[1][j].imshow(x_train[int(len(x_train)/2 + j)].squeeze(), cmap=\"Greys\")\n# #         axs[1][j].imshow(y_train[int(len(y_train)/2 + j)].squeeze(), cmap=\"Greens\", alpha=0.3)\n# #     fig.suptitle(\"Top row: original images, bottom row: augmented images\")\n#     #Train\n#     early_stopping = EarlyStopping(patience=10, verbose=1)\n#     reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n#     #reduce_lr=LearningRateScheduler(step_decay)\n#     best_model=None\n#     best_history=None\n#     best_iou_max=0\n#     best_thres_max=0\n#     val_loss_min=99\n#     for time in range(6):\n#         model_checkpoint = ModelCheckpoint(\"./\"+str(i)+\"_\"+str(time)+\"_keras.model\", save_best_only=True, verbose=1)\n#         model=build()\n#         gen = ImageDataGenerator()\n#         gen_flow = gen_flow_for_two_inputs(x_train, y_train)\n#         history = model.fit_generator(gen_flow,\n#                     validation_data=[x_valid, y_valid], \n#                     epochs=epochs,\n#                     steps_per_epoch=len(x_train) / batch_size,\n#                     callbacks=[early_stopping, model_checkpoint, reduce_lr])\n# #         history = model.fit(x_train, y_train,\n# #                         validation_data=[x_valid, y_valid], \n# #                         epochs=epochs,\n# #                         batch_size=batch_size,\n# #                         callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True,verbose=0)\n#         t_b,i_b,ts,bs=valid_on_best_iou(x_valid,y_valid,time)\n#         ious_list.append([min(history.history['val_loss']),i_b])\n#         print('Splits: '+str(i+1)+\n#               ', \\nTime: '+ str(time)+\n#               ', \\nbest epoch valid loss: '+\n#               str(min(history.history['val_loss']))+\n#               ', \\nvalid accuracy: '+\n#               str(max(history.history['val_acc']))+\n#               ', \\nvalid iou: '+\n#               str(i_b))\n#         if min(history.history['val_loss'])<val_loss_min and i_b>best_iou_max:\n#             best_model=load_model(\"./\"+str(i)+\"_\"+str(time)+\"_keras.model\",{'bce_dice_loss': bce_dice_loss})\n#             best_history=history\n#             val_loss_min=min(history.history['val_loss'])\n#             best_iou_max=i_b\n#             best_thres_max=t_b\n#         if min(history.history['val_loss'])<val_loss_limit and i_b>val_iou_limit:\n#             best_model=load_model(\"./\"+str(i)+\"_\"+str(time)+\"_keras.model\",{'bce_dice_loss': bce_dice_loss})\n#             best_history=history\n#             val_loss_min=min(history.history['val_loss'])\n#             best_iou_max=i_b\n#             best_thres_max=t_b\n#             break\n#     print(min(best_history.history['val_loss']))\n#     print(best_iou_max,i_b)\n#     print(best_thres_max,t_b)\n#     best_model.save(\"./\"+str(i)+\"_keras_\"+str(best_iou_max)+\".model\")\n#     print(ious_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"513ccd395e0d920f3bb3f783d7bc05f7fc96d60f","collapsed":true},"cell_type":"code","source":"ls\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5567f1a37e3d8b2a00c0b305e4355ac1ef5169b6","collapsed":true},"cell_type":"code","source":"# avg=0\n# for i,h in enumerate(historys):\n#     print(min(h.history['val_loss']))\n#     avg+=min(h.history['val_loss'])\n# avg=avg/len(historys)\n# print('avg val loss: '+str(avg))\n# avg_acc=0\n# for i,h in enumerate(historys):\n#     print(max(h.history['val_acc']))\n#     avg_acc+=max(h.history['val_acc'])\n# avg_acc=avg_acc/len(historys)\n# print('avg val acc: '+str(avg_acc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Validation"},{"metadata":{"_uuid":"332a614c0ae837c115ec6563f355753ffbb8cd83"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"trusted":true,"_uuid":"72128add82c6853441671fde67e7e66601a01787","collapsed":true},"cell_type":"code","source":"# # Source https://www.kaggle.com/bguberfain/unet-with-depth\n# def RLenc(img, order='F', format=True):\n#     \"\"\"\n#     img is binary mask image, shape (r,c)\n#     order is down-then-right, i.e. Fortran\n#     format determines if the order needs to be preformatted (according to submission rules) or not\n\n#     returns run length as an array or string (if format is True)\n#     \"\"\"\n#     bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n#     runs = []  ## list of run lengths\n#     r = 0  ## the current run length\n#     pos = 1  ## count starts from 1 per WK\n#     for c in bytes:\n#         if (c == 0):\n#             if r != 0:\n#                 runs.append((pos, r))\n#                 pos += r\n#                 r = 0\n#             pos += 1\n#         else:\n#             r += 1\n\n#     # if last run is unsaved (i.e. data ends with 1)\n#     if r != 0:\n#         runs.append((pos, r))\n#         pos += r\n#         r = 0\n\n#     if format:\n#         z = ''\n\n#         for rr in runs:\n#             z += '{} {} '.format(rr[0], rr[1])\n#         return z[:-1]\n#     else:\n#         return runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3ecb152b492c7126d12c5ef2c701eec8ea3d86f1","collapsed":true},"cell_type":"code","source":"# x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87941b52e0e517bcc352f01bfecbc6b24620339c","collapsed":true},"cell_type":"code","source":"# preds_test=np.zeros([len(x_test),img_size_target, img_size_target],dtype=np.float32)\n# avg_thres=0\n# test_batch_length=1000\n# for i in range(n_split):\n#     model = load_model(\"./\"+str(i)+\"_keras.model\")\n#     avg_thres+=threshes[i]*(min(historys[i].history['val_loss']))\n#     for b in range(int(len(x_test)/test_batch_length)):\n#         print(str(i)+' split: '+str(b)+' batch')\n#         x_test_batch=x_test[b*test_batch_length:(b+1)*test_batch_length]\n#         preds_test_batch = model.predict(x_test_batch)\n#         preds_test[b*test_batch_length:(b+1)*test_batch_length]+=(preds_test_batch).astype(np.float32).squeeze()*(min(historys[i].history['val_loss']))\n# preds_test/=(avg*n_split)\n# avg_thres/=(avg*n_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f278d0b87320c117b4ed7c116a991782b82ba5a7","collapsed":true},"cell_type":"code","source":"# avg_thres=0\n# for i in range(n_split):\n#     model = load_model(\"./\"+str(i)+\"_keras.model\")\n#     avg_thres+=threshes[i]\n#     if i==0:\n#         preds_test = model.predict(x_test)\n#     else:\n#         preds_test+=model.predict(x_test)\n# preds_test/=n_split\n# avg_thres/=n_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"113f816f9db8b87ca7f6845fe6e61328ab606f41","collapsed":true},"cell_type":"code","source":"# pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > avg_thres)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4243166f91c4bcb4da00208f4f53dd912dbb429f"},"cell_type":"code","source":"# sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n# sub.index.names = ['id']\n# sub.columns = ['rle_mask']\n# sub.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}