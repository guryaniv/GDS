{"cells":[{"metadata":{"_uuid":"0105d2736582b3d6410dd99950b59404a6ba55cc","_cell_guid":"223865e4-b2bb-45f7-bb6a-6a378b46eec1"},"cell_type":"markdown","source":"# Introduction\n\nThis Kernel is my first serious attempt at practicing a classic machine learning workflow. I am a beginner in the field and I use this competition to experiment and practice, thus I hope to receive as many feedback as possible.\n\nThe steps of this workflow will be:\n* Exploratory Analysis: what is missing, how the variables distribute, how are they correlated.\n* Data Cleaning: because it is a dirty world\n* Feature engineering: given what I learned from the exploration, how do I create the best features?\n* Algorithm selection: given what I have, what are the best algorithms to go forward?\n* Model training: time to tune the model and train it\n\nWith this workflow, I reached a public score of 0.80382 (top 12% when) which makes me happy but reveals some crucial gaps in my understanding of the topic.\n\nThings I have learned by doing what follows:\n* how to better explore the data to get insights\n* how to create new features from those insights\n* feature selection and cross-validation\n* how to test models\n\nThings I want to learn next:\n* how to improve the model once I reach a certain result\n* error analysis and what to do next\n* stacking\n\nOnce again, thank you for reading and please share any disagreement you have my code.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"195d512603a34a0e2fc02ae9f91b24b1cb1c9a5d","collapsed":true,"_cell_guid":"0156e3a7-a660-4cbc-8926-6292796a3c87","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"933869b8ae1df7fd6ddf325f073af4d1a23cb24b","collapsed":true,"_cell_guid":"86e4f318-8fe7-4db5-9bac-d2211120389b","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98d4ab9776f2bc70122733029d711f2d1d2b8f26","_cell_guid":"a63f827d-c4ad-4e64-9542-f8216a8efc1d"},"cell_type":"markdown","source":"# Exploratory Analysis\n\nJust to have a taste of what is in there\n\n## Missing Data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e2ca17979f46f7f06623a5be5bcd068da2acc31f","_cell_guid":"a1676023-898c-4354-94d1-37e54442deb4","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e67a2c66e63f2280ad3b81290045ca68f32478d8","_cell_guid":"8237c126-a7b8-4b3c-a98d-1af6be179454","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ac31bebea5a339e3f9d1e42625bc79eee561474","_cell_guid":"4813d83f-8298-407a-b6d7-cca31ebec34a","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.info()\nprint('_'*40)\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d5da1bb2a196cb68ec203791f34e96851281705","_cell_guid":"22deeb86-02f7-4d73-bb19-a6f50ae79f5f","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ecaa76f6f6d1122afca7f118571b7c1bb8561f6","_cell_guid":"1e646295-256f-4796-98b0-e8708d7b22ab","trusted":false,"collapsed":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fd437732d444dc981883beb5f0497a7d1fa8834","_cell_guid":"6365d4b9-ca23-4e03-9432-8120c97e508b"},"cell_type":"markdown","source":"A few things are missing:\n* Age is missing 20% of the times in the train and 35% of the time in the test\n* Embarked feature missing twice in the train\n* Cabin missing 75% of the times in train and 78% of the times in test\n* Fare is missing once in the test\n\n## Numeric Features Distribution\n\nAge, Fare, Parch (number of parents/children), and SibSp (number of siblings/ spouses)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"fd216e89bafbc022437335f97fa53a74a7a28872","_cell_guid":"e6b1e41e-b070-4df0-b88b-713e90805bc0","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\n\ntrain_df.Age.hist(bins = 30, ax=axes[0])\ntest_df.Age.hist(bins = 30, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4631db5a04c08ed37c3641b88c31f4983466fa61","_cell_guid":"87a9d016-4e06-42a9-ba8c-03984dd300e8","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\n\ntrain_df.Fare.hist(bins = 30, ax=axes[0])\ntest_df.Fare.hist(bins = 30, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adc7d3aa2dac7bcdf20d03710becdc4345864d4f","_cell_guid":"bca73d42-d031-4e2a-bcf9-5f5a2441cf79","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\n\ntrain_df.Parch.hist(bins = 20, ax=axes[0])\ntest_df.Parch.hist(bins = 20, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d362810e57a0cfd8da611673941770f674df0c3b","_cell_guid":"670206b3-6af4-436f-bddf-36d2edfe6a60","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\n\ntrain_df.SibSp.hist(bins = 20, ax=axes[0])\ntest_df.SibSp.hist(bins = 20, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59a1a514871836712a302c0c710d2ecd325f20ea","_cell_guid":"b1f5496f-8c67-4fcb-9fff-b1cc2afb5514"},"cell_type":"markdown","source":"Some outliers, nothing crazy tho: the distributions are similar for test and train\n\n## Categorical Variables\n\nSurvived, Sex, Pclass, Embarked","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4ca5c3e1c9a854da67b96864cd7b67a1284547ac","_cell_guid":"7085ab8b-bbb8-4a4a-892f-a49a9612fdb7","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Percentage of survival in the train set: {}%\".format(round(sum(train_df.Survived)/train_df.Survived.count(), 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"344e1c9005a720532cd762761d356746761d51bc","_cell_guid":"df71f161-4a39-4884-98d7-293a10769d6c","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"In train: \")\nprint(train_df.Sex.value_counts())\nprint('_'*40)\nprint(\"In test: \")\nprint(test_df.Sex.value_counts())\n\nfig, axes = plt.subplots(1, 2)\n\ntrain_df.Sex.value_counts().plot(kind = \"bar\", ax=axes[0])\ntest_df.Sex.value_counts().plot(kind = \"bar\", ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee44ec11349d15ce13cda124283539435af790e","_cell_guid":"41bf3281-d9b1-4599-a111-da49c5f71b8a","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"In train: \")\nprint(train_df.Pclass.value_counts())\nprint('_'*40)\nprint(\"In test: \")\nprint(test_df.Pclass.value_counts())\n\nfig, axes = plt.subplots(1, 2)\n\ntrain_df.Pclass.value_counts().plot(kind = \"bar\", ax=axes[0])\ntest_df.Pclass.value_counts().plot(kind = \"bar\", ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26199776c47c9c4476b8000e5e19a8588471de79","_cell_guid":"3829ad1f-c2f8-4a45-80e8-1d6748956512","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"In train: \")\nprint(train_df.Embarked.value_counts())\nprint('_'*40)\nprint(\"In test: \")\nprint(test_df.Embarked.value_counts())\n\nfig, axes = plt.subplots(1, 2)\n\ntrain_df.Embarked.value_counts().plot(kind = \"bar\", ax=axes[0])\ntest_df.Embarked.value_counts().plot(kind = \"bar\", ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c2926503f1c3f5c7b5e0352487c856c5e553189","_cell_guid":"3dbd87da-ddbf-41e6-9ac3-3c046d7c20e9"},"cell_type":"markdown","source":"A few issues:\n* The two classes are imbalanced, I have to be careful with that\n* Sex and Pclass are equally distributed\n* Small differences in the distribution of Embarked category between train and test\n\n## Correlations","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e29cf3073e25e0e6b97839035596c4b588d5e84d","_cell_guid":"7483bcd0-8fab-4b1d-bc76-34bb8ca28fce","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00eda97b11e2fc9926aeda90b2a2c72183d13fe5","_cell_guid":"b1af5984-eec3-4144-91d7-4745d6883d14","trusted":false,"collapsed":true},"cell_type":"code","source":"test_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7f66a6e8c8be5b9b2a16becc24962609bfa2571","collapsed":true,"_cell_guid":"ef47b9ea-1aa9-4d05-8643-0313d04b2016"},"cell_type":"markdown","source":"* Similar correlations in train and test.\n* In the training set it seems that class and fare are the most important features.\n* Probably one of the two is redundant since they are also very much correlated.\n\nLet's have a look at some segmentations with the target variable","outputs":[],"execution_count":null},{"metadata":{"_uuid":"69e7b1ce7c754b2c9f0cbad2ca54af94995a89fc","_cell_guid":"982a0444-1a29-411a-b2ac-8b01a959bde9","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"299227d6d753f84416ba28f73d1f63c7269416eb","_cell_guid":"31738e41-26a4-4775-afb4-5234d106f31a","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], \n                                      as_index=True).mean().sort_values(by='Survived', \n                                                                         ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"896a84cb1e0e5aee41d35e37c5701525409607cd","_cell_guid":"e8852a12-6ab5-4d27-aba3-bf728ff0207f","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], \n                                        as_index=True).mean().sort_values(by='Survived', \n                                                                           ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81f585d36a9dc52a03bf72479d80ee6c568019db","_cell_guid":"345f4403-9d4b-4c15-9226-a0e0c4a7989b","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], \n                                        as_index=True).mean().sort_values(by='Survived', \n                                                                           ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8949ea555e3b0465641e131979191095fcd8e0a5","_cell_guid":"6d8a777f-16fc-46f5-b2ed-5d3f5f09aa01","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], \n                                        as_index=True).mean().sort_values(by='Survived', \n                                                                           ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"684ab1ee7bcd429658f5db82ba88a5ba9479abf2","_cell_guid":"0410f3fc-cbc8-4f55-a2e3-542fd35023ba","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae04b7fcf5905d00924d566d597d669e8256a65","_cell_guid":"1a0b8fdb-ce54-4343-8bd8-1bc6b5a1e575"},"cell_type":"markdown","source":"* Clearly the class and the gender play a role in determining the survival\n* It looks like a certain number of people traveling with you will help you surviving, but too many will kill you\n* I will not pretend to not know that the feature IsAlone is very famous and useful in this competition\n* Kids seem to survive\n\nLet's see how these features relate to one another","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f2c33e137d8b8d9c56b64f1d6f2d1472ec6fab0f","_cell_guid":"16fb0fab-6b76-43a0-946e-d094776dd3c4","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'Pclass']).size().unstack(0)\n    df['fem_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15e57a6e9676e08b44d417ed6667d8a91a76e914","_cell_guid":"1e61db79-9eaf-43e5-98c3-027cf92154b5","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"Pclass\", \"Survived\"]].groupby(['Sex', 'Pclass'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee1f159ea006428d44e194abd93a97013c130ba","_cell_guid":"7beed70b-d0b6-44c1-a79c-02978520ca17"},"cell_type":"markdown","source":"The question may now arise: did they die because they were males or because they were in the third class? This calls for a new feature that I will create later on. \n\nThere is a small difference in how the genders are distributed among the classes between train and test set but I am confident that the model will be robust enough to not care about that. (Maybe it is more appropriate to say that I hope that the model will be robust enough...)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7d7efa253dd4624cc3ba8fd904a691c0cdd28675","_cell_guid":"2d4d3002-15be-405f-a3a7-847ed58b73b6","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'Parch']).size().unstack(0).fillna(0)\n    df['male_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['fem_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0665fb83968a6dff8a8a731f4ddd483fd22620e8","_cell_guid":"7a94e781-8129-4001-ad60-1f2be3ad48c3","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"Parch\", \"Survived\"]].groupby(['Sex', 'Parch'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"890ff8a8f105107d153975f20a9c1273c1d4f954","_cell_guid":"fe4997d8-054f-498e-ae4b-f9cb7cfa4c3e"},"cell_type":"markdown","source":"We have seen that Parch= 0 is most likely to not survive and here we observe that most of them are men. Again: which one is the relevant one?","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4fa558c576d82483cb92a7ae4a49602c6d2e1a51","_cell_guid":"2e9715bd-992d-4942-8891-a06cf2b1b479","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'SibSp']).size().unstack(0).fillna(0)\n    df['male_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['fem_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42716f976244555f12da3683832f5de0b36f61ac","_cell_guid":"7d1d41f3-2012-4ebb-bb3c-5b329490446d","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"SibSp\", \"Survived\"]].groupby(['Sex', 'SibSp'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"652b9754ad17946cd0753d1dce3f5e8d83d702e4","_cell_guid":"a4441aeb-72bd-4a51-b993-1a1295bd9242"},"cell_type":"markdown","source":"As before, is it being male, being alone, or both that determines your chances of survival? Getting good ideas for the feature engineering phase.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"40a0d985bdf06f92f33bfada44ad26bc0d197349","_cell_guid":"518d0650-8c5c-4550-b4eb-13fc9bc0895c","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=30)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb12d454250bd1d228e914e3e1741d942c3878d","_cell_guid":"2940bcb1-0749-4d7f-9443-b615505dffc5","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_df, col='Pclass', row='Sex', hue='Survived')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=30)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e895c59880f46ff0d7bd2113dec8f4d26b4717f5","_cell_guid":"b9d6526e-f04a-46b1-a877-cd9f6997d0ea"},"cell_type":"markdown","source":"* Being female seems to guarantee your survival only if you are in the first two classes\n* Being a baby seems to not be important for your survival if you are poor\n* Being a man sucks (only in this case probably, privilege all over the place otherwise) except if you are rich\n\nLet's see the embarked feature.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3a512b775dc1715f07466a1163c086da746c451c","_cell_guid":"b37de475-5a20-4dab-aaa7-2c817d8b6ed6","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'Embarked']).size().unstack(0)\n    df['male_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['fem_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c060f939840d874e9b31519ef717194b3a9b169","_cell_guid":"7ad50eec-8a39-4b67-93a2-e0e53a6265d0","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Embarked\", \"Sex\", \"Survived\"]].groupby(['Sex', 'Embarked'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27c63b97c2bb0e5ba8126e3bdb88279f81b86a66","_cell_guid":"e400c44e-ee75-4c13-a339-b2bbb98e816c","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Pclass', 'Embarked']).size().unstack(0)\n    df['first_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['second_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['third_perc'] = (df[df.columns[2]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c36ea4c97b9f268e12f8e6f6df11b3462b71a2f3","_cell_guid":"94001270-1a3f-493d-8166-a8f7313d22ff","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Embarked\", \"Pclass\", \"Survived\"]].groupby(['Embarked', 'Pclass'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baa1d06ef135a9516f18150eed4d6c8f7b29c285","_cell_guid":"ac2d4b05-b71e-4f74-839e-7172bbd4578e"},"cell_type":"markdown","source":"# Data Cleaning\n\nCabin is missing too many times to be useful, I will just change it with 1/0 values because maybe the fact that is not missing matter","outputs":[],"execution_count":null},{"metadata":{"_uuid":"cf533668379f9310bcfa746506ddf9ca219f8c15","_cell_guid":"f27b1899-a16f-4ef6-a038-31d3f6491940","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    fil1 = (dataset.Cabin.isnull())\n    fil2 = (dataset.Cabin.notnull())\n    dataset.loc[fil1, 'Cabin'] = 0\n    dataset.loc[fil2, 'Cabin'] = 1\n    dataset.Cabin = pd.to_numeric(dataset['Cabin'])\n\nprint(train_df.Cabin.value_counts())\nprint(\"_\"*40)\nprint(test_df.Cabin.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2d7e2f15e2f097037e656f115eb6cf9f2b57686","_cell_guid":"fe60de6c-27b0-4486-a9e8-f9b740e0dd90"},"cell_type":"markdown","source":"Now I want to check if it relates with other things","outputs":[],"execution_count":null},{"metadata":{"_uuid":"53240864b56781897e47b46d952e1b720def86fe","_cell_guid":"3ebf1141-fab9-4091-aef0-7a8f9a6f9fc5","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[['Cabin', 'Survived']].groupby(['Cabin'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b461a091282ef9ea6ed2d10b7208373f1aa9130","_cell_guid":"b3aa8696-ffb5-476d-9961-99618a46ea40","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'Cabin']).size().unstack(0)\n    df['fem_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['male_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04beb163fe20a2cb0d5683f35c4552ff2a84a034","_cell_guid":"c602aaa0-ab8c-41a8-a869-df498c9cde35","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Cabin\", \"Sex\", \"Survived\"]].groupby(['Sex', 'Cabin'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"802df4877b156ec1b0d06cfb43839f4cd719ef14","_cell_guid":"85f25953-9ddc-40c8-a4ed-8d26cb68e605","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Cabin', 'Pclass']).size().unstack(0)\n    df['miss_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04eed7611de6f748222e0e1ce7f71ebc25872b8","_cell_guid":"3875df1c-ca1a-4524-9363-a7f60e851a1a","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Cabin\", \"Pclass\", \"Survived\"]].groupby(['Pclass', 'Cabin'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88915c5998aca92551456ce566d05776af7d0775","_cell_guid":"fd5a79b6-e288-4c6a-bf1e-98da74daa539"},"cell_type":"markdown","source":"It seems to matter a lot in terms of survival and is very related to the class.\n\nThe Embarked feature is missing only twice in the train and the Fare feature is missing once in the test. Putting them as 'missing' or creating a flag feature would introduce a sparse class in our data and this can lead to overfitting.\n\nFor the Embarked feature, I will just segment the data and impute with the mode of that segment\n\nFor the missing Fare, the same strategy but with a mean value.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3d22cc656bed0ab29bc92b3488c017a5b3acaa29","_cell_guid":"fb6561ca-3429-438b-af3c-54019664db21","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[train_df.Embarked.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf84539e64c8d66c0db3ea3e2573552e358fb57","_cell_guid":"4c56fc9a-5834-433f-97c1-8f9d2b8b27f2","trusted":false,"collapsed":true},"cell_type":"code","source":"test_df[test_df.Fare.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71e58cf1d44695d0997d9f7b1ac0ed6b080b4209","_cell_guid":"a2a8ee1c-abae-4fec-a667-1c2610e1c314"},"cell_type":"markdown","source":"I don't want to mix train and test in any way and I don't want to use the target variable to segment. So I will focus on female passengers of first class and traveling alone (also, they have the same ticket number, weird) for the Embarked feature in the train and on male passengers traveling alone of third class embarked in S with missing cabin.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"815160d194838fa96514aab33bda674a10fd9794","_cell_guid":"12299136-586f-43e6-99e5-0aef54f41006","trusted":false,"collapsed":true},"cell_type":"code","source":"fil = ((train_df.Pclass == 1) & (train_df.SibSp == 0) & (train_df.Parch == 0) \n       & (train_df.Sex == 'female'))\nmis = train_df[fil].Embarked.mode()\nprint(mis)\nfil = train_df.Embarked.isnull()\ntrain_df.loc[fil, 'Embarked'] = 'C' #I new it was C from a previous run\nprint(\"_\"*40)\nprint(train_df.Embarked.value_counts(dropna = False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75beda5a562776206c750afeb07a5b947bc1aa49","_cell_guid":"6afa0d2d-f377-4851-86a6-67d4c394c1d4","trusted":false,"collapsed":true},"cell_type":"code","source":"fil = ((test_df.Pclass == 3) & (test_df.SibSp == 0) & (test_df.Parch == 0) \n       & (test_df.Cabin == 0) & (test_df.Sex == 'male') & (test_df.Embarked == 'S'))\nmis = round(test_df[fil].Fare.median(), 4)\nprint(mis)\nfil = test_df.Fare.isnull()\ntest_df.loc[fil, 'Fare'] = mis\nprint(\"_\"*40)\nprint(test_df.Fare.isnull().value_counts(dropna = False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b10d50d6ad66ae52a136ebaa37f49417aca50e2","_cell_guid":"bc2525ca-2374-4fc0-8db7-8088599e73f4"},"cell_type":"markdown","source":"Next, the Age feature is missing quite a bit and not enough to drop it (also, I don't want to). \n\nMy strategy is going to be the following:\n\n* segment the data according to some criteria\n* calculate median of the age of each segment\n* impute random values in the mean \\pm a small variation of each segment\n* create a feature indicating that the age was missing\n* hope for the best\n\nLet' start with the easy part: flagging the missing ages.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"32bfbc5345259616d12c056a66534b6262114de0","_cell_guid":"6b37af81-ed13-41a7-82db-054a680eb47f","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['MisAge'] = 0\n    fil = (dataset.Age.isnull())\n    dataset.loc[fil, 'MisAge'] = 1\n\nprint(train_df.MisAge.value_counts())\nprint(\"_\"*40)\nprint(test_df.MisAge.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\ntrain_df[['MisAge', 'Survived']].groupby(['MisAge'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea0e5c6521d518d7fad6278819b295fd9f57c874","_cell_guid":"0654d693-60eb-40b3-a92b-15d7f48d5256"},"cell_type":"markdown","source":"The missing values seem to be missing for a reason: they didn't survive and nobody asked them their age.\n\nLet me check the usual correlations with sex and class.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"80700ddee2de90637a710c9423826733899f882e","_cell_guid":"e8b05c1b-cdff-4afa-b02a-9a5e70d1dbb5","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['MisAge', 'Sex']).size().unstack(0)\n    df['miss_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['nomiss_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b414ab68c4c5ea0535b2ab15f10fb3ac9f12cc51","_cell_guid":"5e0722b9-7209-4f93-b30a-bbcda911a16d","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"MisAge\", \"Survived\"]].groupby(['Sex', 'MisAge'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9973dd6cea1f274a20c2f441aad3bdc091f05a81","_cell_guid":"5dce324d-6624-442d-8c85-6e3b87644cf8","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['MisAge', 'Pclass']).size().unstack(0)\n    df['miss_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['nomiss_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"410a357072188370c49e9bbbc912f5de14829634","_cell_guid":"51cdf8e8-03bf-422d-8afe-056d766a3549","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Pclass\", \"MisAge\", \"Survived\"]].groupby(['Pclass', 'MisAge'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac3f6c2554fe4a77431c29af8b2a88825a2fbb0","_cell_guid":"30f5cfd8-7f04-4653-a774-b75730216ddb","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Cabin\", \"MisAge\", \"Survived\"]].groupby(['Cabin', 'MisAge'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a2b41c52339a790c14a4db3786c4fe3fc7fc967","_cell_guid":"0527342e-811e-47a7-9e4b-9cf3830d9bb3"},"cell_type":"markdown","source":"Now I want to find who are the people with missing age.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"fa987954e4e7f9ed8b966025655bf7a3b63ec33e","_cell_guid":"6b1114d3-7473-42ff-b579-73cce92f53a9","trusted":false,"collapsed":true},"cell_type":"code","source":"fil = (train_df.Age.isnull())\nprint(\"By class:\")\nprint(train_df[fil].Pclass.value_counts())\nprint(\"_\"*40)\nprint(train_df[train_df.MisAge == 0].Pclass.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By sex:\")\nprint(train_df[fil].Sex.value_counts())\nprint(\"_\"*40)\nprint(train_df[train_df.MisAge == 0].Sex.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By parents and children:\")\nprint(train_df[fil].Parch.value_counts())\nprint(\"_\"*40)\nprint(train_df[train_df.MisAge == 0].Parch.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By spouse and siblings:\")\nprint(train_df[fil].SibSp.value_counts())\nprint(\"_\"*40)\nprint(train_df[train_df.MisAge == 0].SibSp.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11babe804bb1f1f66771602a9e019984e2c2a3de","_cell_guid":"fd390b61-7122-4802-843e-226128683567"},"cell_type":"markdown","source":"It looks that a good segment would be focusing on the Class and Sex categories because the others will be too few to give a meaningful estimate. Let's see in the test.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d8d800b683c6ec5f9110bac9a44df4d2f4290b2b","_cell_guid":"9c9dee1e-beaa-45dc-8cea-30d3e5c6fabb","trusted":false,"collapsed":true},"cell_type":"code","source":"fil = (test_df.Age.isnull())\nprint(\"By class:\")\nprint(test_df[fil].Pclass.value_counts())\nprint(\"_\"*40)\nprint(test_df[test_df.MisAge == 0].Pclass.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By sex:\")\nprint(test_df[fil].Sex.value_counts())\nprint(\"_\"*40)\nprint(test_df[test_df.MisAge == 0].Sex.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By parents and children:\")\nprint(test_df[fil].Parch.value_counts())\nprint(\"_\"*40)\nprint(test_df[test_df.MisAge == 0].Parch.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nprint(\"By spouse and siblings:\")\nprint(test_df[fil].SibSp.value_counts())\nprint(\"_\"*40)\nprint(test_df[test_df.MisAge == 0].SibSp.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63258a0f69f2f07e344199b1e2980566796d6c96","_cell_guid":"d908dd05-624d-4fb5-86f3-fb1cde16375e"},"cell_type":"markdown","source":"One more thing that could help is to use the title feature, which I have to create. Even if the feature engineering step will happen later, I think it will help to create the right segments for imputing the missing age.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7c21942824bf27240ae95cad913de3d400cafbd1","_cell_guid":"0722e200-c219-4e0b-bc91-0c2e3c14e38a","trusted":false,"collapsed":true},"cell_type":"code","source":"# Extract the title from the name feature\nfor df in combine:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \npd.crosstab(train_df['Title'], train_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fab535e2af2ee9e709929a5411b1094127242b7","_cell_guid":"61d84559-580a-4535-8464-8c3940d210d8","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.crosstab(test_df['Title'], test_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55fc40c35edf9b66f52606f4efe5f5dba05d3b68","_cell_guid":"0c22a785-dec1-4ee1-93fd-b6f308ea5edc","trusted":false,"collapsed":true},"cell_type":"code","source":"# handling the rare classes with class\nfor df in combine:\n    df['Title'] = df['Title'].replace(['Mme', 'Countess','Dona'], 'Mrs')\n    df['Title'] = df['Title'].replace(['Capt', 'Col','Don', 'Jonkheer', 'Rev', \n                                                 'Major', 'Sir'], 'Mr')\n    df['Title'] = df['Title'].replace(['Mlle', 'Lady','Ms'], 'Miss')\n    df.loc[(df.Sex == 'male') & (df.Title == 'Dr') , 'Title'] = 'Mr'\n    df.loc[(df.Sex == 'female') & (df.Title == 'Dr') , 'Title'] =  'Mrs' \n    \npd.crosstab(train_df['Title'], train_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43f8f522a1f22cae38e3b2923818ca9459a5580a","_cell_guid":"a3d87776-7a98-4562-b373-a601799013ad","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.crosstab(test_df['Title'], test_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91f7cb44a5144a0ccc643e2d4db19bf8056d5b81","_cell_guid":"c5238a72-1802-47c3-8ac3-c3059c6da58a","trusted":false,"collapsed":true},"cell_type":"code","source":"fil = (test_df.Age.isnull())\nprint(\"By title:\")\nprint(test_df[fil].Title.value_counts())\nprint(\"_\"*40)\nprint(test_df[test_df.MisAge == 0].Title.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\nfil = (train_df.Age.isnull())\nprint(\"By class:\")\nprint(train_df[fil].Title.value_counts())\nprint(\"_\"*40)\nprint(train_df[train_df.MisAge == 0].Title.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec26b98257169a173026015a31b7ca5ffe439886","collapsed":true,"_cell_guid":"fcc2cc24-236d-4fd7-bd8b-ec5463ff8670"},"cell_type":"markdown","source":"I think Title is the right feature to use to segment the age feature because it catches gender and age section. Moreover, I will also segment per class","outputs":[],"execution_count":null},{"metadata":{"_uuid":"93f2f3510e8dc82ca4c26eee816151b8db159036","collapsed":true,"_cell_guid":"dbf7de35-574a-4798-bcd5-1d36cd293dd7","trusted":false},"cell_type":"code","source":"np.random.seed(452) #reproducibility","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ac797e922be03c82e9a2d0548a52ee649c92db2","_cell_guid":"f93fb980-f453-48a6-960b-49210a44c74b","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    titles = list(set(df.Title))\n    classes = list(set(df.Pclass))\n    for title in titles:\n        for cl in classes:\n            fil = (df.Title == title) & (df.Pclass == cl)\n            med_age = df[fil].Age.dropna().median()\n            var_age = med_age / 5\n            mis_age = df[fil].MisAge.sum()\n            df.loc[fil & (df.Age.isnull()), 'Age'] = np.random.randint(int(med_age - var_age - 1), \n                                                                       int(med_age + var_age), mis_age)\n        \ntrain_df.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25da0b0916533dba6e599e72fcedc08b7d5c369e","_cell_guid":"bc6363df-ba15-40be-b602-a83ff17c56c6","trusted":false,"collapsed":true},"cell_type":"code","source":"test_df.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9749b6160cfc07b55e6fbef8e408529ea502b8e7","_cell_guid":"5ef57f39-f74c-4e47-a26e-04d4946fe58d","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.info()\nprint('_'*40)\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94909e07779404ef0beceea7f2aa5533c39235af","collapsed":true,"_cell_guid":"def43498-66fc-4c89-a3e9-6be632958df4"},"cell_type":"markdown","source":"I am happy with the current state of my data now, I could focus on the outliers in Parch and SpSib but we all know I am going to use IsAlone as a feature, so I don't spend any more time in cleaning the data and move to a funnier step\n\n# Feature Engineering\n\nI got some ideas thanks to the data exploration, I will try to produce something useful.\n\n### Not really feature engineering\n\nThis is the moment where I just convert category into numbers because it helps.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ec3cbc0cff8aee228bbcd3371ba0909be061bc7d","_cell_guid":"5e38148f-b0b7-4cc7-9982-da689c5525aa","trusted":false,"collapsed":true},"cell_type":"code","source":"# Convert Sex to numerical\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map({'male':1 , 'female':2}).astype(int)\n\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e4e8d260684088df09a3931973c9a8aa153df4","_cell_guid":"6ab92560-dad5-4551-9497-2a0f71e2f341","trusted":false,"collapsed":true},"cell_type":"code","source":"# Convert Embarked to numerical\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map({'S':1 , 'C':2, 'Q':3}).astype(int)\n\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"980a1bc0d3765672b8696f1ec0ffdabcf15a0232","_cell_guid":"e1f92b2a-be4f-48f0-9798-1183189de016"},"cell_type":"markdown","source":"This was easy and it usually helps to deal with numbers rather than strings.\n\n### Creating new features\n\nI have already created the title feature, let's see how it relates to the other variables","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dbb4df95373d9cf0f6f1c0325614ee257d72a4ff","_cell_guid":"8770eca8-f19d-40db-8832-f5f40b0b6932","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[['Title', 'Survived']].groupby(['Title'], as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1429332131b4808b9907e94187cc867a923347d2","_cell_guid":"892849b5-42ac-42a9-b99a-54e1345e8497","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.Title.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e77a5b83dfdb64188e128cbaae7bee0efdedaa8a","_cell_guid":"de741772-8744-4c2a-afba-034f4fbb58c8","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.Title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8140a5833f7ac00dcd6ff6ca7449177e35bb6ab4","_cell_guid":"25d121b9-6cf7-40d4-80fc-b12967842e5e","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Title'] = df['Title'].map({'Mr':1 , 'Mrs':2, 'Miss':3, 'Master':4}).astype(int)\n\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8431083e5ff2d8ad19ad6aff578bd60aa9c96cc7","_cell_guid":"db3aa491-562c-4ee4-9d2c-6e70fb6ca986"},"cell_type":"markdown","source":"I don't like this feature very much because it doesn't really say anything more than what I know already (adult males are going to die).\n\nHowever, it was useful for imputing the missing ages.\n\nAnother feature I can create is the popular IsAlone","outputs":[],"execution_count":null},{"metadata":{"_uuid":"03b3235f85805a723bcaf6892bdd9d5d479252bc","_cell_guid":"def52f02-5280-4686-83da-178d69469e75","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['IsAlone'] = 0\n    fil = (df.SibSp == 0) & (df.Parch == 0)\n    df.loc[fil, 'IsAlone'] = 1\n    \nprint(train_df.IsAlone.value_counts())\nprint(\"_\"*40)\nprint(test_df.IsAlone.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dfe8275b2e3fd574fd9bafbb28584aac7d201dd","_cell_guid":"6596b0a7-70ae-41b2-bc9e-4f10ca0e6ddb","trusted":false,"collapsed":true},"cell_type":"code","source":"#checking correlation with the target\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96acc820a01dc4717047dbf7efe6426599a12196","_cell_guid":"614c04e1-2fa5-47b7-8924-f4899856f6f2","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Sex', 'IsAlone']).size().unstack(0)\n    df['fem_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]]))\n    df['male_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83a4e15f7e6a3da497aee2d2951569164c3a4124","_cell_guid":"a76579cf-0cef-4209-a6fe-a21461959caa","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"IsAlone\", \"Survived\"]].groupby(['IsAlone','Sex'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0af88d1c6e0c4d6453bc624993c0fda584e85a5","_cell_guid":"710ed5fb-20bd-467c-bdc0-29fbdd899056","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Pclass', 'IsAlone']).size().unstack(0)\n    df['first_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['second_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['third_perc'] = (df[df.columns[2]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a5ee85f1dc93080eb07a815b3df648f4aacb8cc","_cell_guid":"aefd46d4-2a1b-4aea-80c0-521e50fdc699","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Pclass\", \"IsAlone\", \"Survived\"]].groupby(['IsAlone', 'Pclass'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c440f7fa6e6d25318c303faf7fe88743484bf82c","_cell_guid":"ae4c900e-e2e0-4201-95f4-55a07955f042"},"cell_type":"markdown","source":"Not as strong as other correlations, but still clear enough to keep this feature for our models.\n\nBefore I have seen that being a kid helps for your survival, so I create a feature for it.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"6a1d88b7280b5eefd85ef4cbc0113a32e65574c9","_cell_guid":"3091902c-be9e-40ad-b315-919c01be082c","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['IsKid'] = 0\n    fil = (df.Age < 16)\n    df.loc[fil, 'IsKid'] = 1\n    \nprint(train_df.IsKid.value_counts())\nprint(\"_\"*40)\nprint(test_df.IsKid.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9bca295d93ac74255329a62cb1537e884dd7e53","_cell_guid":"2ddf2a5c-a2d6-47c9-91d9-d215550d1d29","trusted":false,"collapsed":true},"cell_type":"code","source":"#checking correlation with the target\ntrain_df[['IsKid', 'Survived']].groupby(['IsKid'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6d055446c4d5ef0c0402987e05928e4aa347c8c","_cell_guid":"e0656652-3466-4642-9fe8-ff3038e64cf1"},"cell_type":"markdown","source":"Again, we see a correlation that might help the models.\n\n### Making categories out of continuous variables\n\nOne thing that can help is to have an Age category and a Fare category. \n\nLet's start with Age by looking again at the distribution.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c6585088e05dda83a59e95df82445236154de6a0","_cell_guid":"d80fec52-a1cd-4772-a5c2-5c7df0d7fbf9","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.FacetGrid(train_df[train_df.Age > -1], hue='Survived')\ng.map(plt.hist, 'Age', bins=30, alpha = 0.6)\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9072f9a1a73c2044a69f644a0c17ee30c2feb2c0","_cell_guid":"595b9903-8815-488f-a01b-45a2ee435333"},"cell_type":"markdown","source":"I thus see that the cuts can be the following","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f8f5b45dac1953f47cd812e4135a8265dc763be6","_cell_guid":"f18962a3-9857-4708-b139-9954c28b5160","trusted":false,"collapsed":true},"cell_type":"code","source":"bins = [0, 16, 32, 48, 81] #I just want to avoid the sparse class at 64-80\n\nfor df in combine:\n    df['AgeBin'] = pd.cut(df['Age'], bins)\n    #df['AgeBin'] = pd.to_numeric(df['AgeBin'])\n    \nprint(train_df.AgeBin.value_counts())\nprint(\"_\"*40)\nprint(test_df.AgeBin.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6383ea3684a559ab26504d6f8e655fb2241e6624","_cell_guid":"fdb3a1f0-c0ec-459d-bb73-36fb8e135447","trusted":false,"collapsed":true},"cell_type":"code","source":"#checking correlation with the target\ntrain_df[['AgeBin', 'Survived']].groupby(['AgeBin'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be57e28fa60e783f4076b072d51d37ea0e99084a","_cell_guid":"02ada919-eccd-4662-a7c7-8753856fe26f","trusted":false,"collapsed":true},"cell_type":"code","source":"#same thing but with labels\nbins = [0, 16, 32, 48, 81]\nnames = [0, 1, 2, 3]\n\nfor df in combine:\n    df['AgeBin'] = pd.cut(df['Age'], bins, labels = names)\n    df['AgeBin'] = pd.to_numeric(df['AgeBin'])\n    \nprint(train_df.AgeBin.value_counts())\nprint(\"_\"*40)\nprint(test_df.AgeBin.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40986860ac4c317f80bec67ac3f4792ff01d8400","_cell_guid":"26affe46-1856-4d9d-839b-6b8e0024ddbf"},"cell_type":"markdown","source":"The Fare feature has some outliers and it does not take into account that a passenger might not be alone, I am tempted to categorize the fare per person.\n\n(This excludes people traveling with non-relatives and we know that Di Caprio was not alone) \n\n(but even paid for all that matter)\n\n(and he died)\n\nFirst, I create a feature for the number of family members on board. Then I calculate the fare per person.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"724dd1f54859a3bd2909ebecf38c0fd03d2cfb2c","_cell_guid":"c2c0fc7f-e4a6-464a-95b4-93e7dc9940bd","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['NumFam'] = df['SibSp'] + df['Parch'] + 1\n    df['FarePP'] = df['Fare'] / df['NumFam']\n    \nfig, axes = plt.subplots(1, 2)\n\ntrain_df.NumFam.hist(ax=axes[0])\ntest_df.NumFam.hist(ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a47e47a2a23a3db01084318736cf3506d07f3ac9","_cell_guid":"d679456e-56bf-4692-ab14-bf5a57dea1a0","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\n\ntrain_df.FarePP.hist(ax=axes[0], bins=20)\ntest_df.FarePP.hist(ax=axes[1], bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6eed8f12a5c25cd1eb7f8003e50642db6ddf5d","_cell_guid":"31ea4fa9-43cc-4d79-854d-62ff8a421053"},"cell_type":"markdown","source":"* The NumFam has too many sparse values and I think the IsAlone feature already captures that kind of information\n* The FarePP feature can be put in a category using qcut","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2d3edc7e46896f909ac8a3b4260d8ec03090e761","_cell_guid":"f5dc15d7-448f-47d2-98cf-f466f7ed36c5","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['FareCat'] = pd.qcut(df.FarePP, 4)\n    \nprint(train_df.FareCat.value_counts())\nprint(\"_\"*40)\nprint(test_df.FareCat.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75e8b2f48e3094624328d1fbb27ce66798b812f9","_cell_guid":"170f8aba-038a-4782-ba77-ed459bc43bbb","trusted":false,"collapsed":true},"cell_type":"code","source":"#same thing with labels\nlabels = [0, 1, 2, 3]\n\nfor df in combine:\n    df['FareCat'] = pd.qcut(df.FarePP, 4, labels=labels)\n    df['FareCat'] = pd.to_numeric(df['FareCat'])\n    \nprint(train_df.FareCat.value_counts())\nprint(\"_\"*40)\nprint(test_df.FareCat.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b14f131a36739c0feaa40561f3e14e76770edea","_cell_guid":"44040aec-aeac-4faf-93cf-a1952844a6a1","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[['FareCat', 'Survived']].groupby(['FareCat'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7348dfbe19b6e1c6cc024941478a8e6396ee45da","_cell_guid":"93876ead-3156-4c11-a07c-3212c361a4f4","trusted":false,"collapsed":true},"cell_type":"code","source":"for dataset in combine:\n    df = dataset.groupby(['Pclass', 'FareCat']).size().unstack(0).fillna(0)\n    df['first_perc'] = (df[df.columns[0]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['second_perc'] = (df[df.columns[1]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    df['third_perc'] = (df[df.columns[2]]/(df[df.columns[0]] + df[df.columns[1]] + df[df.columns[2]]))\n    print(df)\n    print(\"_\"*40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8acca8a0fd307a7d37f9e9f19742c1a5a31dae9d","_cell_guid":"4b4a756b-ff6a-460a-abbb-2af947d941d5"},"cell_type":"markdown","source":"It seems to me that the FareCat is redundant with the class.\n\nNext, I want something to distinguish a large family (which before looked like it was going to not survive), from a small one.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2cca8dc31e230ce2dc5060b06be071753cdb121b","_cell_guid":"1e1e1445-3074-4454-a146-246d1ef32e97","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['FamSize'] = 0 #alone people\n    df.loc[(df.NumFam > 1), 'FamSize'] = 1 #small families\n    df.loc[(df.NumFam > 3), 'FamSize'] = 2 #medium families\n    df.loc[(df.NumFam > 5), 'FamSize'] = 3 #big families\n\nprint(train_df.FamSize.value_counts())\nprint(\"_\"*40)\nprint(test_df.FamSize.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"593cf6d9199aec3c6fb2568433996aac59b774a6","_cell_guid":"384cf400-ee8b-4225-a292-9acbd35907f9","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[['FamSize', 'Survived']].groupby(['FamSize'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d130764a67a9dde2b0bad5d25f583a36c2a2b297","_cell_guid":"b2ee4f1c-ce31-49c4-ada5-e090e12569d8","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Pclass\", \"FamSize\", \"Survived\"]].groupby(['FamSize', 'Pclass'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36b66c90ade30b4468e4cb277e83da8a1b0980b6","_cell_guid":"255d1c22-2109-44a1-a675-6da9b31ba511","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"FamSize\", \"Survived\"]].groupby(['FamSize', 'Sex'], \n                                        as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e9b1a3ff91c6d1f1a184a392b8223bed816f66c","_cell_guid":"42b24114-9176-4a87-a761-604dbc7bf3e5"},"cell_type":"markdown","source":"It seems it gives some more indication. It also include the IsAlone feature, which I will just use in combination with the class.\n\n### Indicator features\n\nI saw before some correlations with the survival rate and gender or class. I also observed that there was no clear answer on which variable was more important. So I now create the following features to express the combinations of different categories:\n\n* Sex and Pclass\n* PClass and IsAlone\n* Cabin and PClass\n* MisAge and PClass\n* IsKid and PClass\n* Embarked and PClass\n* MisAge and Cabin\n* Sex and IsAlone\n* Cabin and Sex","outputs":[],"execution_count":null},{"metadata":{"_uuid":"739f59d5535a10e9b5aaed2024150fabce5c29d6","_cell_guid":"89fb77a6-7ba8-475e-ba35-ca978782b34e","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Se_Cl'] = 0\n    df.loc[((df.Sex == 1) & (df.Pclass == 1)) , 'Se_Cl'] = 1 #rich male\n    df.loc[((df.Sex == 1) & (df.Pclass == 2)) , 'Se_Cl'] = 2 #avg male\n    df.loc[((df.Sex == 1) & (df.Pclass == 3)) , 'Se_Cl'] = 3 #poor male\n    df.loc[((df.Sex == 2) & (df.Pclass == 1)) , 'Se_Cl'] = 4 #rich female\n    df.loc[((df.Sex == 2) & (df.Pclass == 2)) , 'Se_Cl'] = 5 #avg female\n    df.loc[((df.Sex == 2) & (df.Pclass == 3)) , 'Se_Cl'] = 6 #poor female \n    \nprint(train_df.Se_Cl.value_counts())\nprint(\"_\"*40)\nprint(test_df.Se_Cl.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Se_Cl', 'Survived']].groupby(['Se_Cl'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e740966047f33e6b90ec8804347054afd605077f","_cell_guid":"6cbfb54e-e204-4435-a479-17fd19762c0e","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Cl_IA'] = 0\n    df.loc[((df.IsAlone == 1) & (df.Pclass == 1)) , 'Cl_IA'] = 1 #rich alone\n    df.loc[((df.IsAlone == 1) & (df.Pclass == 2)) , 'Cl_IA'] = 2 #avg alone\n    df.loc[((df.IsAlone == 1) & (df.Pclass == 3)) , 'Cl_IA'] = 3 #poor alone\n    df.loc[((df.IsAlone == 0) & (df.Pclass == 1)) , 'Cl_IA'] = 4 #rich with family\n    df.loc[((df.IsAlone == 0) & (df.Pclass == 2)) , 'Cl_IA'] = 5 #avg with family\n    df.loc[((df.IsAlone == 0) & (df.Pclass == 3)) , 'Cl_IA'] = 6 #poor with family \n    \n    \nprint(train_df.Cl_IA.value_counts())\nprint(\"_\"*40)\nprint(test_df.Cl_IA.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Cl_IA', 'Survived']].groupby(['Cl_IA'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b997d4e333bb00d0871653ed9d097aa28bce99b9","_cell_guid":"3022df18-23c0-44b7-9620-8f6da3369603","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Ca_Cl'] = 0\n    df.loc[((df.Cabin == 0) & (df.Pclass == 1)) , 'Ca_Cl'] = 1 #rich no cabin\n    df.loc[((df.Cabin == 0) & (df.Pclass == 2)) , 'Ca_Cl'] = 2 #avg no cabin\n    df.loc[((df.Cabin == 0) & (df.Pclass == 3)) , 'Ca_Cl'] = 3 #poor no cabin\n    df.loc[((df.Cabin == 1) & (df.Pclass == 1)) , 'Ca_Cl'] = 4 #rich with cabin\n    df.loc[((df.Cabin == 1) & (df.Pclass == 2)) , 'Ca_Cl'] = 5 #avg with cabin\n    df.loc[((df.Cabin == 1) & (df.Pclass == 3)) , 'Ca_Cl'] = 6 #poor with cabin\n    \nprint(train_df.Ca_Cl.value_counts())\nprint(\"_\"*40)\nprint(test_df.Ca_Cl.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Ca_Cl', 'Survived']].groupby(['Ca_Cl'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1901d58f727e89472d1581f152fe61cfd4f3e14f","_cell_guid":"208cedf3-62db-4fce-86e6-3dd8c30a9f25","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['MA_Cl'] = 0\n    df.loc[((df.MisAge == 0) & (df.Pclass == 1)) , 'MA_Cl'] = 1 #rich with age\n    df.loc[((df.MisAge == 0) & (df.Pclass == 2)) , 'MA_Cl'] = 2 #avg with age\n    df.loc[((df.MisAge == 0) & (df.Pclass == 3)) , 'MA_Cl'] = 3 #poor with age\n    df.loc[((df.MisAge == 1) & (df.Pclass == 1)) , 'MA_Cl'] = 4 #rich without age\n    df.loc[((df.MisAge == 1) & (df.Pclass == 2)) , 'MA_Cl'] = 5 #avg without age\n    df.loc[((df.MisAge == 1) & (df.Pclass == 3)) , 'MA_Cl'] = 6 #poor without age\n    \nprint(train_df.MA_Cl.value_counts())\nprint(\"_\"*40)\nprint(test_df.MA_Cl.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['MA_Cl', 'Survived']].groupby(['MA_Cl'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e356824e265efe944eb9bd2c68ba23c823e849fe","_cell_guid":"132843ee-8966-47bf-a526-9d4869f2e0e1","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['IK_Cl'] = 0\n    df.loc[((df.IsKid == 0) & (df.Pclass == 1)) , 'IK_Cl'] = 1 #rich adult\n    df.loc[((df.IsKid == 0) & (df.Pclass == 2)) , 'IK_Cl'] = 2 #avg adult\n    df.loc[((df.IsKid == 0) & (df.Pclass == 3)) , 'IK_Cl'] = 3 #poor adult\n    df.loc[((df.IsKid == 1) & (df.Pclass == 1)) , 'IK_Cl'] = 4 #rich kid\n    df.loc[((df.IsKid == 1) & (df.Pclass == 2)) , 'IK_Cl'] = 5 #avg kid\n    df.loc[((df.IsKid == 1) & (df.Pclass == 3)) , 'IK_Cl'] = 6 #poor kid\n    \nprint(train_df.IK_Cl.value_counts())\nprint(\"_\"*40)\nprint(test_df.IK_Cl.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['IK_Cl', 'Survived']].groupby(['IK_Cl'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8098770531d0dd1818a28f343b3634e09e519da","_cell_guid":"e02a0afc-cad7-48bc-9a1c-b629c09b3b34","trusted":false,"collapsed":true},"cell_type":"code","source":"#for embarked and class I will just multiply them\nfor df in combine:\n    df[\"Em_Cl\"] = df[\"Embarked\"] * df[\"Pclass\"]\n\nprint(train_df.Em_Cl.value_counts())\nprint(\"_\"*40)\nprint(test_df.Em_Cl.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Em_Cl', 'Survived']].groupby(['Em_Cl'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91f1b88edba91172c3d0e8c3b99d155f3a7502b6","_cell_guid":"24ccdec7-8316-41b4-8052-aa9048252b80","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Se_Ca'] = 0\n    df.loc[((df.Sex == 1) & (df.Cabin == 0)) , 'Se_Ca'] = 1 #male without cabin\n    df.loc[((df.Sex == 1) & (df.Cabin == 1)) , 'Se_Ca'] = 2 #male with cabin\n    df.loc[((df.Sex == 2) & (df.Cabin == 0)) , 'Se_Ca'] = 3 #female without cabin\n    df.loc[((df.Sex == 2) & (df.Cabin == 1)) , 'Se_Ca'] = 4 #female with cabin\n    \nprint(train_df.Se_Ca.value_counts())\nprint(\"_\"*40)\nprint(test_df.Se_Ca.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Se_Ca', 'Survived']].groupby(['Se_Ca'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60de3d83629d66c22553af2cb2575224d8caba6f","_cell_guid":"2e5be42c-1611-47b5-95f7-562de0a861b5","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['MA_Ca'] = 0\n    df.loc[((df.MisAge == 0) & (df.Cabin == 0)) , 'MA_Ca'] = 1 #Age no Cabin\n    df.loc[((df.MisAge == 0) & (df.Cabin == 1)) , 'MA_Ca'] = 2 #Age and Cabin\n    df.loc[((df.MisAge == 1) & (df.Cabin == 0)) , 'MA_Ca'] = 3 #No Age no Cabin\n    df.loc[((df.MisAge == 1) & (df.Cabin == 1)) , 'MA_Ca'] = 4 #No Age but Cabin\n    \nprint(train_df.MA_Ca.value_counts())\nprint(\"_\"*40)\nprint(test_df.MA_Ca.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['MA_Ca', 'Survived']].groupby(['MA_Ca'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c8afbb151edeab02eea31bd14f6ab96bab7d0b","_cell_guid":"2b881e1a-a9d6-463e-bd70-7144c1e7a275","trusted":false,"collapsed":true},"cell_type":"code","source":"for df in combine:\n    df['Se_IA'] = 0\n    df.loc[((df.Sex == 1) & (df.IsAlone == 0)) , 'Se_IA'] = 1 #Male with family\n    df.loc[((df.Sex == 1) & (df.IsAlone == 1)) , 'Se_IA'] = 2 #Male without family\n    df.loc[((df.Sex == 2) & (df.IsAlone == 0)) , 'Se_IA'] = 3 #Female with family\n    df.loc[((df.Sex == 2) & (df.IsAlone == 1)) , 'Se_IA'] = 4 #Female without family\n    \nprint(train_df.Se_IA.value_counts())\nprint(\"_\"*40)\nprint(test_df.Se_IA.value_counts())\nprint(\"_\"*40)\nprint(\"_\"*40)\n\n#see correlation with target\ntrain_df[['Se_IA', 'Survived']].groupby(['Se_IA'], \n                                         as_index=True).mean().sort_values(by='Survived', \n                                                                            ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7994ddd7cb34453ec5b3e2dcf7c7a59e55675895","collapsed":true,"_cell_guid":"44e56f79-43ac-4ad9-b1ea-c33035c7379e"},"cell_type":"markdown","source":"I am keeping them excepet for Ca_Cl, MA_Cl, IK_Cl, and Em_Cl either because they don't give new insights as hoped or because of sparse classes.\n\n\n# Algorithm Selection\n\nIt is a classification problem and I feel fairly confident in using the following algorithms:\n\n* Logistic regression\n* Decision tree\n* Support vector machine\n* Naive bayes\n* Ensembles of the previous one\n\nI will thus import the modules I need and select the features to feed them\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"72473df6cfa496096c3590b1bc90b98700d30562","_cell_guid":"1a6e3b68-1cd3-492f-9204-70633525ae2f","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd37e7955d5e0feaf85a64411928e04cd551dedd","collapsed":true,"_cell_guid":"5d69f760-ce98-4d32-b7d0-801f3f8226e7","trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.model_selection import GridSearchCV, KFold\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afa279a6018338d6145d4d1b50f0d33d8db32198","_cell_guid":"abd9815e-0e75-4204-a0ce-df4db2355d83","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22457a71724587fcc0b9e70668cd2efaed778f2","_cell_guid":"061332a5-da0c-44cf-8f06-fb57d69af2f0","trusted":false,"collapsed":true},"cell_type":"code","source":"features = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'Title', 'AgeBin', 'MisAge', 'IsKid', \n            'FamSize', 'Se_Cl', 'Cl_IA', 'Se_Ca', 'MA_Ca', 'Se_IA']\n\ny = train_df['Survived'].copy()\nX = train_df[features].copy()\ntest = test_df[features].copy()\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90af895be811a6658ef496c203e5bf3accb2814f","_cell_guid":"7abecbac-3fff-42e8-8a97-e36d64840495","trusted":false,"collapsed":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95d4add6cfb1d2bdb86fd419623331acf40897dd","_cell_guid":"06df4dae-aed2-4606-942e-af4ff1775411"},"cell_type":"markdown","source":"## Consistency and Robustness\n\nI have imported a lot of Algorithms and realistically I will not be patient enough to tune all of them. \n\nThus, as a first selection, I want to keep only those that are more consistent. In other words, I want to keep only the algorithms that are more stable when I cross-validate.\n\nMoreover, I want to find also the more robust: those that perform well across different metrics.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f50dc73d96e46193202ace1dbb319c36b993b1b2","collapsed":true,"_cell_guid":"f7f9201c-656f-4b59-80d6-dc1d896965fb","trusted":false},"cell_type":"code","source":"# classifier list\nclf_list = [DecisionTreeClassifier(), \n            RandomForestClassifier(), \n            AdaBoostClassifier(), \n            GradientBoostingClassifier(), \n            XGBClassifier(),\n            Perceptron(),\n            LogisticRegression(), \n            SVC(), \n            LinearSVC(), \n            KNeighborsClassifier(), \n            GaussianNB(),\n            SGDClassifier()\n           ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba7ca601dc5b98dd172bd2ab029474e46d285084","_cell_guid":"f9b46271-623e-4f11-a011-08ffd9dadf3b","trusted":false,"collapsed":true},"cell_type":"code","source":"mdl = []\nbias_acc = []\nvar_acc = []\nbias_f1 = []\nvar_f1 = []\nbias_auc = []\nvar_auc = []\n\nacc_scorer = make_scorer(f1_score)\n\nfor clf in clf_list:\n    model = clf.__class__.__name__\n    res = cross_val_score(clf, X, y, scoring='accuracy', cv = 5)\n    score = round(res.mean() * 100, 3)\n    var = round(res.std(), 3)\n    bias_acc.append(score)\n    var_acc.append(var)\n    res = cross_val_score(clf, X, y, scoring=acc_scorer, cv = 5)\n    score = round(res.mean() * 100, 3)\n    var = round(res.std(), 3)\n    bias_f1.append(score)\n    var_f1.append(var)\n    res = cross_val_score(clf, X, y, scoring='roc_auc', cv = 5)\n    score = round(res.mean() * 100, 3)\n    var = round(res.std(), 3)\n    bias_auc.append(score)\n    var_auc.append(var)\n    mdl.append(model)\n    print(model)\n    \n#create a small df with the scores\nrobcon = pd.DataFrame({'Model': mdl, 'Bias_acc':bias_acc,'Variance_acc':var_acc, \n                       'Bias_f1':bias_f1,'Variance_f1':var_f1, 'Bias_auc':bias_auc,'Variance_auc':var_auc,\n                      })\nrobcon = robcon[['Model','Bias_acc','Variance_acc', 'Bias_f1','Variance_f1','Bias_auc','Variance_auc' ]]\nrobcon","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13b90ace4a3eb739813d31f35a8820a398df40e9","_cell_guid":"42ef9717-6620-4aa6-adea-9da78efff104","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Best for accuracy\")\nprint(robcon[['Model','Bias_acc']].sort_values(by= 'Bias_acc', ascending=False).head(6))\nprint(\"_\"*40)\nprint(\"Best for f1\")\nprint(robcon[['Model','Bias_f1']].sort_values(by= 'Bias_f1', ascending=False).head(6))\nprint(\"_\"*40)\nprint(\"Best for roc_auc\")\nprint(robcon[['Model','Bias_auc']].sort_values(by= 'Bias_auc', ascending=False).head(6))\nprint(\"_\"*40)\nprint(\"Least variance for accuracy\")\nprint(robcon[['Model','Variance_acc']].sort_values(by= 'Variance_acc').head(6))\nprint(\"_\"*40)\nprint(\"Least variance for f1\")\nprint(robcon[['Model','Variance_f1']].sort_values(by= 'Variance_f1').head(6))\nprint(\"_\"*40)\nprint(\"Least variance for roc_auc\")\nprint(robcon[['Model','Variance_auc']].sort_values(by= 'Variance_auc').head(6))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faffed40ae1fc675e148f91c85df3a8d8fb84dc6","_cell_guid":"cff4aea7-e91c-4f67-b444-f8efb633815e"},"cell_type":"markdown","source":"* I will move forward with SVC, RandomForest, XGB, AdaBoost, and LogisticRegression.\n\n* LogisticRegression might benefit from the creation of dummy variables.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"205bdc63bbfbd326ec200d1d3b4fc33451383d5d","_cell_guid":"c6885aa1-25d9-455f-afef-6feb7bcd0865"},"cell_type":"markdown","source":"## Hyperparameters tuning with feature selection\n\nI want to find the best set of parameters for each algorithm. I will use GridSearchCV so that cross-validation is included. This is fairly time-consuming, so I have to be mindful of that.\n\nMoreover, I am not sure that every algorithm likes having so many features. So will also perform some feature selection.\n\nI want to test my results on previously unseen data, so I will start by splitting","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b9e997fd868ae1d3eeaf2768be5de015490f7466","collapsed":true,"_cell_guid":"96f76465-dd0c-4b1e-80d5-0049de151de7","trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=895)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2820da6d0f6bcab6777017051a8e3d0e700563d1","collapsed":true,"_cell_guid":"79e876f0-8048-4137-900c-72e204e0356c","trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import RFECV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7b0835ce4359ff5e5e1155a5b294b86419dab7b","_cell_guid":"8f4e010c-e480-45c7-8d7f-a9d6999ffc52","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe2f822cd98c127fbdc7503f2a0ccf9df839cb35","_cell_guid":"285a7ee1-d666-4f3e-a38e-9878688d8a09"},"cell_type":"markdown","source":"### Logistic Regression\n\nI will tune the C parameter, which is the inverse of the regularization strength, and the tolerance for stopping.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"316ea2e6a5af40ab2f97a1f28b2cfcae8de46ad4","_cell_guid":"01571dda-ecac-4acd-bc10-df715d4ca369","trusted":false,"collapsed":true},"cell_type":"code","source":"# feature selection\nFeatSel_log = RFECV(LogisticRegression(), step = 1, scoring = 'roc_auc', cv = 10)\nFeatSel_log.fit(X_train, y_train)\n\nBestFeat_log = X_train.columns.values[FeatSel_log.get_support()]\nBestFeat_log","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d06fe1614786d902fa1046400ccf40fa4da3a349","_cell_guid":"b3204931-3841-48ed-a7de-78670592ee8f","trusted":false,"collapsed":true},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n             'tol': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n             'random_state' : [42]}\n\n# create the grid\ngrid_log = GridSearchCV(LogisticRegression(), param_grid, cv = 10, scoring= 'roc_auc')\n\n#training\n%time grid_log.fit(X_train[BestFeat_log], y_train)\n\n#let's see the best estimator\nbest_log = grid_log.best_estimator_\nprint(best_log)\nprint(\"_\"*40)\n#with its score\nprint(np.abs(grid_log.best_score_))\nprint(\"_\"*40)\n#accuracy on test\npredictions = best_log.predict(X_test[BestFeat_log])\naccuracy_score(y_true = y_test, y_pred = predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8434820fb867dd219ccfebcc52ec03533a4090c7","_cell_guid":"84da8b32-8508-4270-8fff-6caaa605e057"},"cell_type":"markdown","source":"### SVC\n\nI will tune the type of kernel, the penalty parameter, and the tolerance for stopping criteria.\n\nSVC do not have a coef or feature_importance, so I will just use them all.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"fb50f154f97b6f16a6006fa1e09098ae41062ff7","_cell_guid":"94c344fa-39a0-4d42-9750-7084c34d530b","trusted":false,"collapsed":true},"cell_type":"code","source":"# define the parameters grid with NORMAL\nparam_grid = {'C': np.arange(1,10),\n             'tol': [0.0001, 0.001, 0.01, 0.1, 1],\n             'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n             'random_state': [42]}\n\n# create the grid\ngrid_SVC = GridSearchCV(SVC(), param_grid, cv = 10, scoring= 'roc_auc')\n\n#training\n%time grid_SVC.fit(X_train, y_train)\n\n#let's see the best estimator\nbest_SVC = grid_SVC.best_estimator_\nprint(best_SVC)\nprint(\"_\"*40)\n#with its score\nprint(np.abs(grid_SVC.best_score_))\n#accuracy on test\npredictions = best_SVC.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe8b750a4d6926cea2b1ec7e152a985138950dfd","_cell_guid":"2e75a8ee-0bfe-4942-bc80-60bba0e71448"},"cell_type":"markdown","source":"### AdaBoost \n\nThis is a boosting algorithm, so it trains constrained learners in sequence and each learner learns from the mistakes of the previous one. Then it combines them into a single unconstrained learner. The base estimator is again a tree.\n\nI will tune the type of algorithm, the number of estimators, and the learning rate","outputs":[],"execution_count":null},{"metadata":{"_uuid":"59d9d8a055baa9b439c267a11c870016e8d638b8","_cell_guid":"3788b0e2-798b-4c65-b702-ac223bd24d3f","trusted":false,"collapsed":true},"cell_type":"code","source":"# feature selection\nFeatSel_ada = RFECV(AdaBoostClassifier(), step = 1, scoring = 'roc_auc', cv = 10)\nFeatSel_ada.fit(X_train, y_train)\n\nBestFeat_ada = X_train.columns.values[FeatSel_ada.get_support()]\nBestFeat_ada","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1c6579c4d865b08a161b52b23431ad03644d796","_cell_guid":"d97e174c-a1ab-4613-9127-2c9c680acdf7","trusted":false,"collapsed":true},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'n_estimators': np.arange(50, 500, 50),\n             'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 2],\n             'algorithm': ['SAMME', 'SAMME.R'],\n             'random_state': [42]}\n\n# create the grid\ngrid_ada = GridSearchCV(AdaBoostClassifier(), param_grid, cv = 10, scoring= 'roc_auc')\n\n#training\n%time grid_ada.fit(X_train[BestFeat_ada], y_train)\n\n#let's see the best estimator\nbest_ada = grid_ada.best_estimator_\nprint(best_ada)\nprint(\"_\"*40)\n#with its score\nprint(np.abs(grid_ada.best_score_))\n#accuracy on test\npredictions = best_ada.predict(X_test[BestFeat_ada])\naccuracy_score(y_true = y_test, y_pred = predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bb6f7be9f82aa94bc0f89c78b571398257afa2c","_cell_guid":"160805bf-5b3d-4f2c-9360-b4914514c7fb"},"cell_type":"markdown","source":"### Random Forest\n\nThis is a bagging algorithm, so it trains a lot of unconstrained learners in parallel and then combines them.\n\nI will tune the number of trees, their depth, and the maximum number of features to determine a split.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0869449b2c814677a48c2e745673afedde35e071","_cell_guid":"aac1ee0a-3863-4b3c-b09c-828b4ec745c3","trusted":false,"collapsed":true},"cell_type":"code","source":"# feature selection\nFeatSel_for = RFECV(RandomForestClassifier(), step = 1, scoring = 'roc_auc', cv = 10)\nFeatSel_for.fit(X_train, y_train)\n\nBestFeat_for = X_train.columns.values[FeatSel_for.get_support()]\nBestFeat_for","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dadfaf7a53d13c1cfaf57ddd8a92065afe2c41dd","_cell_guid":"4442ec9a-dd96-4641-8a4b-17fa2ecbc4c1","trusted":false,"collapsed":true},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'n_estimators': np.arange(10, 100, 10),\n             'max_depth': np.arange(2,20),\n             'max_features' : ['auto', 'log2', None],\n              'criterion' : ['gini', 'entropy'],\n             'random_state' : [42]}\n\n# create the grid\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid, cv = 10, scoring= 'roc_auc')\n\n#training\n%time grid_forest.fit(X_train[BestFeat_for], y_train)\n\n#let's see the best estimator\nbest_forest = grid_forest.best_estimator_\nprint(best_forest)\nprint(\"_\"*40)\n#with its score\nprint(np.abs(grid_forest.best_score_))\n#accuracy on test\npredictions = best_forest.predict(X_test[BestFeat_for])\naccuracy_score(y_true = y_test, y_pred = predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43ca17a797cc988f095bf0f211530ba2a922a245","_cell_guid":"c1b81cac-cfee-410a-b981-a765968f700e"},"cell_type":"markdown","source":"### XGBoost\n\nIt is a regularized boosting technique and it is very popular on Kaggle. \n\nI admit I don't understand it as much as the other algorithms but I will try to tune the number of trees, the learning rate and the max depth of the trees.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"88e07d14ce3091c8b7e9604479e1dda56825fc14","_cell_guid":"39d30e13-1562-44ca-8ade-7e6a8d7bd46b","trusted":false,"collapsed":true},"cell_type":"code","source":"# feature selection\nFeatSel_XGB = RFECV(XGBClassifier(), step = 1, scoring = 'roc_auc', cv = 10)\nFeatSel_XGB.fit(X_train, y_train)\n\nBestFeat_XGB = X_train.columns.values[FeatSel_XGB.get_support()]\nBestFeat_XGB","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04e8142c8d8d87065f865994045a60d3fcace1ff","_cell_guid":"45f1b4f3-84dc-44c3-93af-07c35275b524","trusted":false,"collapsed":true},"cell_type":"code","source":"# define the parameters grid\nparam_grid = {'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1, 2],\n             'max_depth': np.arange(2,10),\n             'n_estimators': np.arange(50, 500, 50),\n             'random_state': [42]}\n\n# create the grid\ngrid_XGB = GridSearchCV(XGBClassifier(), param_grid, cv = 10, scoring= 'roc_auc')\n\n#training\n%time grid_XGB.fit(X_train[BestFeat_XGB], y_train)\n\n#let's see the best estimator\nbest_XGB = grid_XGB.best_estimator_\nprint(best_XGB)\nprint(\"_\"*40)\n#with its score\nprint(np.abs(grid_XGB.best_score_))\n#accuracy on test\npredictions = best_XGB.predict(X_test[BestFeat_XGB])\naccuracy_score(y_true = y_test, y_pred = predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69ba94711d6190187bcdc4af81879bf66ee3fdef","collapsed":true,"_cell_guid":"67f45a73-78e2-41ee-bfa1-47bff7521a82"},"cell_type":"markdown","source":"In conclusion, the order of scoring is the following:\n\n* SVC\n* XGBoost\n* RandomForest\n* AdaBoost\n* LogisticRegression\n\nI can also see that RandomForest and XGB are using fewer features than the others to get their results. I am surprised by the absence of Sex in the XGB features, but it is included in the Title.\n\nSubmitting the results now would give me the following scores:\n\n* LogisticRegressor: 0.78468 (with dummy variables it goes up to 0.79904)\n* SVC: 0.78468\n* RandomForest: 0.77511\n* AdaBoost: 0.78947\n* XGBoost: 0.80382\n\nWhich is nice (top 12% at the time of submission), but I believe that it can't be too difficult to improve it.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"70b4310c684ee4bfff4f472de40593d0078c4e8b","_cell_guid":"79345fd6-8011-45c5-89a3-2f2e0d9efca4"},"cell_type":"markdown","source":"# Conclusions\n\nThis notebook clarified a few methodologies and finally allowed me to crack my personal goal of 80% in accuracy. A few more considerations before concluding.\n\nInterestingly, if instead of imputing the missing age I just make categories out of them (included a \"missing\" category), the result actually improves for some algorithms (RandomForest above all). This makes sense to me because imputing missing values is just reinforcing an existing pattern.\n\nOne quick way of getting a better result is to use both datasets to impute the missing values, which I consider unrealistic since the test dataset is normally not available in a real situation and it gives me the feeling that I am using the solution to find it (if it makes any sense).\n\nAs said in the introduction, I would like to start from this result to better understand how to analyze the misclassified entries (which is not difficult) and, most importantly, what can I do once I get insights from that analysis.\n\nMoreover, I have seen in other Kernels here on Kaggle that stacking algorithms are very promising and, since I don't know much about it, it is my plan to eventually focus on improving my result with this technique.\n\nThank you for reading and for any kind of feedback you might have for me. I hope this can be of any help for beginners like myself out there.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c9b835d6a238b0c5620eb0ac9b2ff8e2f6b913fe","collapsed":true,"_cell_guid":"82e223fe-72e3-4d4b-893c-d8c7750fd9cb","trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": best_XGB.predict(test[BestFeat_XGB])\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}