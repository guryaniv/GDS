{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"Given the number of upsets this year, there has been some discussions ([[1]](https://www.kaggle.com/c/mens-machine-learning-competition-2018/discussion/52184), [[2]](https://www.kaggle.com/c/mens-machine-learning-competition-2018/discussion/52208))  of *trimming* predictions in order to minimize our log loss in case of an upset. Here is a simple notebook that explores this idea a bit further. \n\nTo be precise, for a trim amount $0\\leq\\alpha\\leq 1$ we specify an interval $I=[\\alpha, 1-\\alpha]$ and we will reshape our predictions to be in the interval $I$. The idea is to be a conservative gambler and keep our predictions away from zero and one because the log loss will punish us if we are wrong. We want to find the best $\\alpha$ which will minimize the log loss of the truth and our trimmed predictions. \n\nFirst we load the data. I am using my submitted predictions for the competition which are from a logistic regression model. I'm also using the actual results of the first 60 games of the 2018 tournament; this data was obtained from [Sports-Reference](https://www.sports-reference.com/cbb/play-index/tourney.cgi). \n\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import log_loss\n\ndef trim_pred(x, alpha):\n    upper = 1-alpha\n    lower = alpha\n    if x > upper:\n        return upper\n    if x < lower:\n        return lower\n    else: return x\n    \ndef trimmed_loss(alpha):\n    trimmed_preds = [trim_pred(x,alpha) for x in data.Pred]\n    return log_loss(data.Result , trimmed_preds)\n\ndef annot_min(x,y, ax=None):\n    minIxVal = np.argmin(y);\n    zeroBasedIx = y[minIxVal];\n    xmin = x[minIxVal];\n    ymin = y[minIxVal]\n    text = \"Minimum: Trim Interval = [{}, {}], Log Loss = {}\".format(round(xmin,2), round(1-xmin,2), round(ymin, 3))\n    if not ax:\n        ax=plt.gca()\n    bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3,rad=0.1\")\n    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n    ax.annotate(text, xy=(xmin, ymin), xytext=(0.94,0.90), **kw)\n    \n\npath = \"../input/ncaa-2018-preds-and-truth/\"\n\npreds = pd.read_csv(os.path.join(path,\"2018_predictions_logistic.csv\")) \ntruth = pd.read_csv(os.path.join(path,\"truth.csv\")) \n\ndata = truth.merge(preds, left_on='ID', right_on='ID', how='inner')\ndata.head()","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"926787a7529ca47cf707e705516220aad3dbed92","_cell_guid":"12328014-6f69-4d89-b149-dc7804f3e2a0"},"cell_type":"markdown","source":"Now we plot the log loss of the trimmed predictions against the truth in order to see how effective a trim would be.\n\nWe search for $\\alpha$ in the interval $[0, 1/4]$ to find the ideal $\\alpha$.  The function ```trimmed_loss(alpha)``` computes the log loss of the trimmed predictions against the truth. The minimum value is labeled in the plot and we see that some amount of trim would have benefited me in this competition, but too much trim is detrimental. The amount we shave off is actually minimal, but this can still lead to big movements on the leaderboard. \n\nThis observation does not help us much now that the competition has started, but it is something to keep in mind in the future. In particular, would it have been helpful to find the trim quantity $\\alpha$ in a data-driven manner? (For example by treating it as a parameter and tuning it with the previous tournaments.)\n\nTry this out with your own predictions and let me know what you think!"},{"metadata":{"_uuid":"2fcceb5573b6a44f1186ede49f5a8c84e1a4bb40","_cell_guid":"f66a7bae-a143-48ac-9098-2eb70128401d","trusted":true},"cell_type":"code","source":"xvals = np.arange(0, .30, 0.001)\nyvals = [trimmed_loss(alpha) for alpha in xvals]\n\nplt.figure(figsize=(10,10))\nplt.plot(xvals, yvals)\nplt.xlabel('Alpha (Trim Amount)', fontsize=14)\nplt.ylabel('Log Loss', fontsize=14)\nannot_min(list(xvals),yvals, ax=None)\nplt.show()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"3a12b4b35676caaee14d7056e26590a74bd5f3b9","_cell_guid":"f0c7c077-7b6d-4b00-b22b-d8cf3111b10d","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}