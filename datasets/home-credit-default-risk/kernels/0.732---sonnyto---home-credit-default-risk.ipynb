{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"**Introduction**\n\nHome Credit is a non-banking financial institution, founded in 1997 in the Czech Republic. The company operates in 14 countries (including United States, Russia, Kazahstan, Belarus, China, India) and focuses on lending primarily to people with little or no credit history which will either not obtain loans or became victims of untrustworthy lenders. The company uses a variety of alternative data, such as telco and transactional information, to predict their clients' repayment abilities. They made available their data to the Kaggle community and are challenging Kagglers to help them unlock the full potential of their data.\n\nThe objective is to use historical loan application data to predict whether or not an applicant will be able to repay a loan."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8402a48239618bab61da0e234a555988f2c5296b"},"cell_type":"markdown","source":"**Import Packages**"},{"metadata":{"trusted":true,"_uuid":"9cedb24d4c0cc778f85a1f0d314231d7598aae3d","scrolled":true},"cell_type":"code","source":"# numpy and pandas for data manipulation\n#import numpy as np\n#import pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\n#import os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\n# from plotly import tools\n# import plotly.tools as tls\n# import squarify\n# from mpl_toolkits.basemap import Basemap\n# from numpy import array\n# from matplotlib import cm\n\n# import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\n\n# from sklearn import preprocessing\n# # Supress unnecessary warnings so that presentation looks clean\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Print all rows and columns\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"630bf2345712a475e66e15ed4c3b999b8863a8d9"},"cell_type":"markdown","source":"**Read Data**"},{"metadata":{"trusted":true,"_uuid":"4346b128503031e2325bf22518e1c6c9e0735ae5","collapsed":true},"cell_type":"code","source":"#print os.getcwd(); # Prints the working directory\n\nimport os\n#print(os.listdir(\"../input\"))\n\n#os.chdir('c:\\\\Users\\Sonny\\desktop\\capstone') # Provide the path here","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9da45206fee0db25c5cd405b134720bfa526d03"},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a7348f980eacfe2a2900d670494d14228d4c7c"},"cell_type":"markdown","source":"The application training data has 307,511 records with 122 columns. The TARGET column will be the variable that will be predicted.  "},{"metadata":{"trusted":true,"_uuid":"e0455dd5385326b5bf5b29902b0e5cd3fc535127"},"cell_type":"code","source":"app_train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4169201ade30b10f24d0753c159af70bfe092563","scrolled":true},"cell_type":"code","source":"# Testing data \napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d68272b7ebbcab620ef2a93c83948ff5efdd01fa"},"cell_type":"markdown","source":"The application testing data has 48,744 records with 121 columns. "},{"metadata":{"_uuid":"31e114dbb3d2af281caee4f465a36f1d8f9a2380"},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"_uuid":"0bb906576329f75cb4e61384cd234e8f42a64191"},"cell_type":"markdown","source":"Target column distribution:"},{"metadata":{"trusted":true,"_uuid":"2a22ddeb911779f77820ff51302015eaf100d9fe"},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80e0c52627450a598025325f953892a00a6dda54"},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ec7fe548bfefe79342d80f0f52dbac53fc95020"},"cell_type":"markdown","source":"Out of 307,511 training records, 92% (282,686/307,511) paid loans on time and 8% (24,825/307,511) was not paid on time. "},{"metadata":{"_uuid":"1769d15af2330fa2639ac46441b298804a0b324d"},"cell_type":"markdown","source":"**Determine Column types**"},{"metadata":{"trusted":true,"_uuid":"465c7df49674824541644ba59047afee3272ba9e"},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77943402bdde8daad5f48b9068687dd372307e99"},"cell_type":"markdown","source":"Data types\n--  int64 and float64 are numeric variables, either discrete or continuous \n-- object columns are categorical"},{"metadata":{"_uuid":"53eff2d46c21b8f570a4627dd16c713e54f96447"},"cell_type":"markdown","source":"**Check Missing Values******"},{"metadata":{"trusted":true,"_uuid":"e990651cefc8b79aed3592ce2950df7e0c8e4f35"},"cell_type":"code","source":"app_train.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8501067c88a7ddfe685fa25f0879d69b74226253"},"cell_type":"markdown","source":"Use code below to check percentage of missing values per column"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1b31a9ec0a95547acb8d22de64a95c7cc2647b8"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" rows.\\n\"\n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5b081c19dd3b2aab19b066202ba6ae3550d49bd"},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(41)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"209878375adc0ca70db95a4aa1ff3f5a4052bbd6"},"cell_type":"markdown","source":"Fill these missing values when it's time to build the models. Another option-- drop columns with high % of missing values.  For now, keep all columns as the impact of columns is unknown i.e. which column will help the models."},{"metadata":{"_uuid":"de32d524d70c39f638bcde8df98652f94de01b7a"},"cell_type":"markdown","source":"Check unique entries in each object (categorical variable)"},{"metadata":{"trusted":true,"_uuid":"e15c676793cd6598de98390012a9237128098de1","collapsed":true},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"198e5a40b6d1fe698d982ca8e7d058b7750fa24b"},"cell_type":"markdown","source":"Label Encoding and One-Hot Encoding\n\nFor any categorical variable (dtype == object) with 2 unique categories, use label encoding, and for any categorical variable with more than 2 unique categories, use one-hot encoding.\n\nFor label encoding, use the Scikit-Learn LabelEncoder and for one-hot encoding, the pandas get_dummies(df) function."},{"metadata":{"trusted":true,"_uuid":"8eeb18d10f792316301e3a3dd8f9a4c0c5bcb9a2","collapsed":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42bd488aa1d09affc8c0550b0c798bae0e54c4a6","collapsed":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b1d988191263077baf300448a4a7308bb4700d3"},"cell_type":"markdown","source":"Aligning Training and Testing Data\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!"},{"metadata":{"trusted":true,"_uuid":"1b36d7fa3e05e0c217f99da1db761c11c98d3e49","collapsed":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f22e14382518e9ef19faee5be55cf0a83a6a25c"},"cell_type":"markdown","source":"**Check DAYS_BIRTH\n**\nThis column is negative because it's recorded as it relates to current loan application.\nConvert to years by * -1 and / by 365\n"},{"metadata":{"trusted":true,"_uuid":"56b80953a566f2a44ba9290730bd574a82599979","collapsed":true},"cell_type":"code","source":"(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d552d8b7b11d8f7c219c269146b6dcc2189ca9a1"},"cell_type":"markdown","source":"The age based on DAYS_BIRTH looks reasonable."},{"metadata":{"_uuid":"0dfc4f5d25509616c0623eeb658df1a16581da8f"},"cell_type":"markdown","source":"**Check DAYS_EMPLOYED**"},{"metadata":{"trusted":true,"_uuid":"a183e64b1e6016a513a7588a3b342a254b67e452","collapsed":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c3afb5701a394ac4fd94e785545a07a1a13b505"},"cell_type":"markdown","source":"**Check Correlations**"},{"metadata":{"trusted":true,"_uuid":"23559ed55b4c9ecae3d73b42a19f9563cb6f3c67","collapsed":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ebe587cf01c29dd21a5f5570bc288bf6d75775"},"cell_type":"markdown","source":"DAYS_BIRTH is the most positive correlation."},{"metadata":{"_uuid":"34fc408be78f144df8b2f4079ad09c9ede62fca7"},"cell_type":"markdown","source":"**Determine impact of Age on repayment**"},{"metadata":{"trusted":true,"_uuid":"7b4dfb7d3437179c9abca8bdc925085d7ddd0891","collapsed":true},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4e7646740329fda240dd249c2876b2a60e86276"},"cell_type":"markdown","source":"There's a negative linear relationship between age and target. It means that clients are good in repaying loans as they grow older."},{"metadata":{"_uuid":"6e13b06666a55a2f02cb9eac845708f3831ae950"},"cell_type":"markdown","source":"**Histogram of the Age**"},{"metadata":{"trusted":true,"_uuid":"adad3006e701bc9d99f1455fe2c5fe4d386af909","collapsed":true},"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5f59c2072e93b4173f9cbbd780d8d956b72ac9d"},"cell_type":"markdown","source":"**Determine average failure to repay loans by age**"},{"metadata":{"_uuid":"08b2893401d88b0dcea7504684bb9ed6b45d10e0"},"cell_type":"markdown","source":"Steps:\n1. Divide age categoty into bins of 5 years each\n2. Calculate average target (ratio of loans not repaid per category)"},{"metadata":{"trusted":true,"_uuid":"10c9335462b401c775ed8aebeb15cbcdc78e55b9","collapsed":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1de03fa9be115efb899e6543b6010c004acd9484","collapsed":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"186a39932b2d760061ee55bb22786e4a3cc89ced","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c28b52466666444cb1cea057562a720051de6fa"},"cell_type":"markdown","source":"There is a trend that younger clients are less likely to repay the loans.\nPotential recommendations to the bank:\n1. Provide more guidance and financial planning tips to younger clients.\n2. Take precautionary measures to help younger clients pay on time."},{"metadata":{"_uuid":"ddb401cecd59fb76e0f58111b721a7be34771771"},"cell_type":"markdown","source":"**Most negative correlations**"},{"metadata":{"_uuid":"079f9366b28cefaf0134d991e16c1245cd677fa4"},"cell_type":"markdown","source":"The 3 variables with the strongest negative correlations with the target are EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3. According to the documentation, these features represent a \"normalized score from external data source\". May be a cumulative sort of credit rating made using numerous sources of data.\n\nCheck correlations of the EXT_SOURCE features with the target and with each other."},{"metadata":{"trusted":true,"_uuid":"6558e4b1b8c5b412868d78227b8093916fdfc96b","collapsed":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78ce7b2cb57347d60e3dc2e5b878db098a8b5283","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc1efbe4934a7347a8efec78a85cf3086623a7ba"},"cell_type":"markdown","source":"All three EXT_SOURCE features have negative correlations with the target, indicating that as the value of the EXT_SOURCE increases, the client is more likely to repay the loan. We can also see that DAYS_BIRTH is positively correlated with EXT_SOURCE_1 indicating that maybe one of the factors in this score is the client age."},{"metadata":{"_uuid":"ef1fa9bedcd628b2662b811ba182451c5732ceec"},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{"_uuid":"d3946c0f1ab27aa87d7baae06b85a8735e58321c"},"cell_type":"markdown","source":"Add features:\nCREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\nDAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"},{"metadata":{"trusted":true,"_uuid":"b5324f831c6d5e8a25dbc8dffd73591238154fab","collapsed":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5f7c4bf8d569846b2234ce5240db438fe7f92f3","collapsed":true},"cell_type":"code","source":"# Extract the new features and show correlations\nnew_features = app_train_domain[['TARGET', 'CREDIT_INCOME_PERCENT', 'DAYS_EMPLOYED_PERCENT']]\nnew_features_corrs = new_features.corr()\nnew_features_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d885b5271949a074041ff2558c61a19018313f54","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(new_features_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"450b5b8b45a9b1ac3275fd4e42d0207751c14375"},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"_uuid":"fd89886ade9bc43e8e5453d5281ddca3cd2d912c"},"cell_type":"markdown","source":"Preprocessing: \n1.  filling in the missing values (imputation) \n2. normalizing the range of the features (feature scaling)."},{"metadata":{"trusted":true,"_uuid":"4a4d9f7a811ff1ffb6562de7c82db320e449ed03","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7471a00dee2757e53e434c88f402f6d09bfa15b"},"cell_type":"markdown","source":"Use LogisticRegressionfrom Scikit-Learn for first model. The only change to default model settings is lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). \n\nUse Scikit-Learn modeling syntax:\n1. create the model \n2. train the model using .fit and \n3. make predictions on the testing data using .predict_proba (need probabilities and not a 0 or 1)."},{"metadata":{"trusted":true,"_uuid":"c44af6ce2113ba3b5781bbcdcdebef2c9e2fa36d","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f0b77ac392d01aed27e792eb98d7bea333a8a58e"},"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"231672eda4d24b3e9511070b9010a051de6b3995","collapsed":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97bfbf6d46dfa96248412d82cbc128ab9ac43aeb"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"002743658a3312814ca8928a6ae2fd7d9f2c3a43"},"cell_type":"markdown","source":"The logistic regression baseline score around 0.671."},{"metadata":{"_uuid":"2d4a34c3f43ed2f05a5612c63417cb86789ab6cb"},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b9e8be63786108ac8fd70c2050de2921d99504a"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e0c2153d7c2a61f868384a1feb9945f054bc36","collapsed":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"36e4f7d3ba63ee8091733dff3c2c7c71dd6f9c05"},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7abb79839afecaca0dbce1099d1a56e703c3817"},"cell_type":"markdown","source":"The Random Forest model score around 0.678."},{"metadata":{"_uuid":"6bb9adf46e051fa55a8a9958a8aeb2b2e1656c0f"},"cell_type":"markdown","source":"**Testing added features**"},{"metadata":{"trusted":true,"_uuid":"37f5ccf41162056afb31eb975bbd5a41d7a8134e","collapsed":true},"cell_type":"code","source":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b05624852bf1945f09074e4424ff38f1c4503c1"},"cell_type":"markdown","source":"This scores 0.679. No significant improvement with the use of added features in this model. "},{"metadata":{"_uuid":"e6bd082fea0aedd977cc92434ef0058ca19f9482"},"cell_type":"markdown","source":"**Feature importance**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a383901b105405fb0d655157fd2a716b4f545368"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb125138fbb7f908ee92967319f397c028b24c7d","collapsed":true},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10364f5a14808b773077fc66a9af445a37fcf0a6","collapsed":true},"cell_type":"code","source":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5fb1fe593f42a30d75085ae80cc2b84a28154ba"},"cell_type":"markdown","source":"**Light Gradient Boosting Machine**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"66fb89d0c4cbfb3375968e8a624c352557a7599b"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f424b6b24ac2089a9e7b9eadee27c1bfbd70a5","collapsed":true},"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"329a3bbf062cd0900f220475e323fdb26c45d7e7"},"cell_type":"code","source":"#Alternative LGBM default\nfrom lightgbm import LGBMClassifier\nLGB_clf = LGBMClassifier(n_estimators=100, \n                         boosting_type='gbdt', \n                         objective='binary', \n                         metric='binary_logloss')\nLGB_clf.fit(train, train_labels)\nLGB_clf_pred = LGB_clf.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b11256200097cea5d62fb1c9d9c80a46f6cf506","collapsed":true},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = LGB_clf_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8961eefb09a520726f2c5baf6f4db90482fa557d"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('LGB.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdcf34d45ba1c8be6cd239f30df6e99fd5055588"},"cell_type":"markdown","source":"The Light GBM model score about 0.732."},{"metadata":{"_uuid":"b7aa4d9e4f4253cc456453b6ea62e064d08de25b"},"cell_type":"markdown","source":"**Other things to consider**"},{"metadata":{"_uuid":"b51e7bab65b093563f43e04c3b7eaa269e33151d"},"cell_type":"markdown","source":"1. Suggestion from Dr. Ceni-- check on other features that's time-based. \n2. Suggestion from Dr. Tamer-- consider ensemble. I'll focus more on feature engineering first to improve the model then check on ensemble. \n\nAlthough age has the most correlation, I'll look on other time-based features in feature engineering and see if it will improve the score."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}