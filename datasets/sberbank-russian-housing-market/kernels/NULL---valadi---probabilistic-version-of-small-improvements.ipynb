{"cells": [{"execution_count": null, "cell_type": "markdown", "source": "Update 2017-06-20:  New version will apply a threshold frequency*probability and just round the original price if probability for recoded price is too low.", "metadata": {"_execution_state": "idle", "_uuid": "723b9744f153f8395b4774295f86cc6f753d11b8", "collapsed": false}, "outputs": []}, {"outputs": [], "cell_type": "markdown", "source": ["This kernel attempts to implement a probabilistic version of [Jiwon's \"Small Improvements\"][1].  Jiwon's idea is that predicted prices for the test set should be selected from the set of actual prices that appeared in the training set.  For example, if your model predicts a price 9,999,973 rubles, the actual price was more likely 10,000,000 rubles, and you would realize this if you looked at the training set and saw that many units sold for 10,000,000 rubles but none sold for 9,999,973 rubles.  Also my analysis shows that this is [particularly an issue][2] with investment properties, which have an overwhelming tendency to sell at round numbered prices.  In this version of this kernel I adjust only investment prices, not prices for owner-occupied units, which will need to be adjusted separately if at all.\n", "\n", "How do we know *which* actual training set price would correspond to a given model prediction on the test set?  One possibility is to choose the closest value, or the closest among values above a given frequency threshold.  Here I take a different approach.  I first posit a log-normal probability distribution for the difference between actual and model-predicted price (with a variance parameter that is currently user-specified, but eventually presumably arrived at by cross-validation).  I then posit that the distribution of prices is equal to the frequency distribution of prices on the training set (with adjustments for the upward trend over time).  For each prediction, I multiply the two implied probability densities and choose the modal price from the product of the two.  (Actually I use adjusted frequencies, not a probability density per se, for the overall price distribution, because the scaling of the density is irrelevant.)\n", "\n", "\n", "  [1]: https://www.kaggle.com/rezimitpo/small-improvements\n", "  [2]: https://www.kaggle.com/aharless/an-interesting-fact-about-investment-properties"], "metadata": {"_cell_guid": "19e99671-c8f6-b744-d022-16684406628f", "_uuid": "9c842eef19c32a574073632de3f482c1d2df45a8"}, "execution_count": null}, {"source": "# Parameters\nprediction_stderr = 0.03  #  assumed standard error of predictions\n                          #  (smaller values make output closer to input)\ntrain_test_logmean_diff = 0.1  # assumed shift used to adjust frequencies for time trend\nprobthresh = 30  # minimum probability*frequency to use new price instead of just rounding\nrounder = 2  # number of places left of decimal point to zero", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "2bedb774-1513-b85c-cc56-a9d41acc6b94", "_uuid": "2d2e949a039168f037162e9bfabf704e97f081fc", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Load the required libraries and data. "], "metadata": {"_cell_guid": "ac06cd79-9189-6d2b-9a83-012d19327beb", "_uuid": "cf9a714f9ac8472936ece17e77da0c0246616335"}, "execution_count": null}, {"source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "from sklearn import model_selection, preprocessing\n", "import xgboost as xgb\n", "import datetime\n", "from scipy.stats import norm\n", "\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "macro = pd.read_csv('../input/macro.csv')\n", "id_test = test.id"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "555a1731-6a24-51ed-f194-a47541b955db", "_uuid": "147ebcc7d92c2a2e00f6bb331679d1d0af30c285", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Run a quick naive XGB to generate some predictions"], "metadata": {"_cell_guid": "fdf242d1-32d0-46df-ba07-184dc2cf62ce", "_uuid": "58f346cb20a03e03f2ab03a85af77ae2e2675936"}, "execution_count": null}, {"source": ["y_train = train[\"price_doc\"]\n", "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n", "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n", "\n", "for c in x_train.columns:\n", "    if x_train[c].dtype == 'object':\n", "        lbl = preprocessing.LabelEncoder()\n", "        lbl.fit(list(x_train[c].values)) \n", "        x_train[c] = lbl.transform(list(x_train[c].values))\n", "        \n", "for c in x_test.columns:\n", "    if x_test[c].dtype == 'object':\n", "        lbl = preprocessing.LabelEncoder()\n", "        lbl.fit(list(x_test[c].values)) \n", "        x_test[c] = lbl.transform(list(x_test[c].values))\n", "        \n", "xgb_params = {\n", "    'eta': 0.05,\n", "    'max_depth': 5,\n", "    'subsample': 0.7,\n", "    'colsample_bytree': 0.7,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'rmse',\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dtest = xgb.DMatrix(x_test)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "99278d98-cd21-7952-6391-44a01d2d121b", "_uuid": "c5ee626ad26cb7c4e85927e4bf0b88c098c9d1d4", "_execution_state": "idle"}}, {"source": "num_boost_rounds = 380\nmodel = xgb.train(xgb_params, dtrain, num_boost_round= num_boost_rounds)\n\ny_predict = model.predict(dtest)\noutput = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\noutput.head()", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8a74802e-fde7-bab8-3dc5-0d44fe5a1af9", "_uuid": "0e7a0b66d3b3d4ea7dc7ad5fbca51338b5a3785f", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Save predictions before small improvements"], "metadata": {"_cell_guid": "13b4d962-7569-782b-6923-215a8f036b03", "_uuid": "abf71f4213507ee4f08b6226fbf08495e6a39594"}, "execution_count": null}, {"source": ["output.to_csv('before.csv', index=False)\n", "preds = output"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0a5dbdee-3e96-aded-9c71-2e2c72af3e5a", "_uuid": "49a0887999758d829e35eb6be68dd94437db8048", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Select investment sales from training set and generate frequency distribution"], "metadata": {"_cell_guid": "8dbd31de-47ab-7a95-d6a3-89299b978fa2", "_uuid": "94173c8a45cc1deb67c75a9f56d425fa3af9cc05"}, "execution_count": null}, {"source": ["invest = train[train.product_type==\"Investment\"]\n", "freqs = invest.price_doc.value_counts().sort_index()\n", "print(freqs.head(20))\n", "freqs.sample(10)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "514f9f33-b52a-de56-841a-3241883d2e18", "_uuid": "1728bedd5b1ec85973b16b0264d05c1fdd9c6fe8", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Select investment sales from test set predictions"], "metadata": {"_cell_guid": "e3242fd3-46f4-99a4-4783-09170532f11a", "_uuid": "e2cf2525bcfc5a091b536da4f237aea93ad92d72"}, "execution_count": null}, {"source": ["test_invest_ids = test[test.product_type==\"Investment\"][\"id\"]\n", "invest_preds = pd.DataFrame(test_invest_ids).merge(preds, on=\"id\")\n", "invest_preds.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a7948b66-0c03-0b10-7143-5c69c8175a3e", "_uuid": "5dff35d3b879541524959236f9241f036b33ca76", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Express X-axis of training set frequency distribution as logarithms, and save standard deviation to help adjust frequencies for time trend."], "metadata": {"_cell_guid": "94a5d722-40ef-3879-1b27-aa0d2e044c6c", "_uuid": "d52c6c6a0a7a289e019d54e896559ddf253f766f"}, "execution_count": null}, {"source": ["lnp = np.log(invest.price_doc)\n", "stderr = lnp.std()\n", "lfreqs = lnp.value_counts().sort_index()\n", "lfreqs.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8fc3654d-6159-83c5-70a0-903993e0b115", "_uuid": "80398a9c5551bcfe0a1ce31647a747a424bac927", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Adjust frequencies for time trend"], "metadata": {"_cell_guid": "acda0c6d-94c5-491c-f322-d37ff65f59fd", "_uuid": "7650c46385b9c349a63eb0028d2fbc88c92a0107"}, "execution_count": null}, {"source": ["lnp_diff = train_test_logmean_diff\n", "lnp_mean = lnp.mean()\n", "lnp_newmean = lnp_mean + lnp_diff"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "748e301c-4ab5-4300-e7b2-1c37930ac246", "_uuid": "be222d39032537ae74bd68c69068708ab2961d0e", "_execution_state": "idle"}}, {"source": ["def norm_diff(value):\n", "    return norm.pdf((value-lnp_diff)/stderr) / norm.pdf(value/stderr)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "3b42f03c-7a23-58f4-d532-c35068b4ff2b", "_uuid": "9f583261d8c2a86bc35ef76b6b16c8a3919642bf", "_execution_state": "idle"}}, {"source": ["newfreqs = lfreqs * (pd.Series(lfreqs.index.values-lnp_newmean).apply(norm_diff).values)\n", "\n", "print( \"What the middle of the adjusted and unadjusted freqs look like:\")\n", "print( lfreqs.values[880:900] )\n", "print( newfreqs.values[880:900] )\n", "\n", "print( \"\\nHeads\")\n", "print( lfreqs.head() )\n", "print( newfreqs.head() )\n", "\n", "print( \"\\nTails\")\n", "print( lfreqs.tail() )\n", "print( newfreqs.tail() )\n", "\n", "print( \"\\nSums\")\n", "print( lfreqs.sum() )\n", "print( newfreqs.sum() )\n", "\n", "print( \"\\nFirst prices that have nonzero frequencies:\")\n", "print( np.exp(newfreqs.index.values[0:20]) )\n", "\n", "newfreqs.shape"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "f7d4f5db-c391-04d7-7183-cd4c5d26f774", "_uuid": "2724cfe953bcb089addfb79ce20d239e37a902a4", "_execution_state": "idle"}}, {"source": ["stderr = prediction_stderr"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5e5aef89-3544-8c7d-d348-d746231feb25", "_uuid": "2cf4d29b927b25f91c5a75ea928a2d02c13321b0", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Logs of model-predicted prices"], "metadata": {"_cell_guid": "893bc418-5071-9d5e-e2ad-9ac33da2767f", "_uuid": "fce81f3f28201e5c2fd3e48d0786e19f1020e9f5"}, "execution_count": null}, {"source": ["lnpred = np.log(invest_preds.price_doc)\n", "lnpred.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6970f967-ad79-c485-2cd8-fd195b3c55b8", "_uuid": "920fcb94dffccc02d5357228d9138990eee8f210", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["`lnpred` has one entry for each test case (m=4998).  `newfreqs.index.values` has one entry for each nonzero-frequency price (n=1750).  For each test case we are going create a corresponding probability distribution, based on the assumed distribution of the actual-predicted difference.  We will evaluate the distribution at the prices that correspond to the nonzero-frequency prices, so the result will be a 4998 x 1750 matrix, showing a distribution for each test case."], "metadata": {"_cell_guid": "abbac611-1437-690c-49f6-4adb054957d5", "_uuid": "7f6c8f1e8a65a6ade90b211bf269d39aea7762b8"}, "execution_count": null}, {"source": ["print(lnpred.shape)\n", "print(newfreqs.index.values.shape)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "c722ce61-3b62-9acc-fe87-4426628124ef", "_uuid": "e843db4695c1539327bb4440b1fa62b99439bcaa", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Create assumed probability distributions."], "metadata": {"_cell_guid": "29c05c77-67cc-3b77-1057-3b900d29b264", "_uuid": "daebb1541fead055b1261776d8f6430fd25b1963"}, "execution_count": null}, {"source": ["mat =(np.array(newfreqs.index.values)[:,np.newaxis] - np.array(lnpred)[np.newaxis,:])/stderr\n", "modelprobs = norm.pdf(mat)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "696b2919-1f60-5a7b-eab8-73f1e66e3152", "_uuid": "83989e9281e41491fe51505570cfa3be72f15b70", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Multiply by frequency distribution."], "metadata": {"_cell_guid": "966411e0-65ae-7e3b-c48f-8df01df2301d", "_uuid": "72ed5fdd75a15fed929d5ba5473c26f1684fc634"}, "execution_count": null}, {"source": ["freqprobs = pd.DataFrame( np.multiply( np.transpose(modelprobs), newfreqs.values ) )\n", "freqprobs.index = invest_preds.price_doc.values\n", "freqprobs.columns = freqs.index.values.tolist()\n", "freqprobs.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "43a20684-f6f0-188f-ad43-81219cea011c", "_uuid": "f791c13dc8657bef85751dc25779febd54ae7585", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Find mode for each case."], "metadata": {"_cell_guid": "db226571-be8c-c879-697a-8b9e345bed20", "_uuid": "f385848169c46621af4a72c7d9944dda6c85c327"}, "execution_count": null}, {"source": "prices = freqprobs.idxmax(axis=1)\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ac059822-c76f-2923-739e-50a6a4121ac9", "_uuid": "8ebb08f6413c073c598cb58f7b41716da682d7ac", "_execution_state": "idle"}}, {"execution_count": null, "cell_type": "markdown", "source": "Apply probability*frequency threshold and reset below-threshold points to old values, rounded.  (The point of this is that we don't want to exclude entirely the possibility of having test predictions that were not represented among the the training prices.  So points where we have low confidence are set back to the old predictions and just rounded.)", "metadata": {"_execution_state": "idle", "_uuid": "8605923b0257271c454a2718d713a523647c6f48", "collapsed": false}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "priceprobs = freqprobs.max(axis=1)\nmask = priceprobs<probthresh\nprices[mask] = np.round(prices[mask].index,-rounder)", "metadata": {"_execution_state": "idle", "_uuid": "95e301659db2b986bb8e71a16c83d450c926799a", "collapsed": false}, "outputs": []}, {"execution_count": null, "cell_type": "markdown", "source": "Data frames with new predictions", "metadata": {"_execution_state": "idle", "_uuid": "6b9ed422f4e908a287436dcad44e6eff9f9f4bcc", "collapsed": false}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "pr = invest_preds.price_doc\npd.DataFrame( {\"id\":test_invest_ids.values, \"original\":pr, \"revised\":prices.values}).head()", "metadata": {"_execution_state": "idle", "_uuid": "92b520054762307ba6b069f3d70cbbe823569057", "collapsed": false}, "outputs": []}, {"source": ["newpricedf = pd.DataFrame( {\"id\":test_invest_ids.values, \"price_doc\":prices} )\n", "newpricedf.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1b221fc6-d9f6-f981-f73c-dfde1f57f351", "_uuid": "593219236f1ee344610ac399d9dfa01b839fb7c5", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Merge these new predictions (for just investment properties) back into the full prediction set."], "metadata": {"_cell_guid": "50f37dff-1e5b-86f3-4fcb-933e8979eba8", "_uuid": "5ea6a9d4f79399562695ed135f0fa29d8e8d006f"}, "execution_count": null}, {"source": ["preds.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "58051828-706f-1d02-8f73-8d8412efa4c9", "_uuid": "67565071211b2646d8c7611a8027f2f1c9027625", "_execution_state": "idle"}}, {"source": "newpreds = preds.merge(newpricedf, on=\"id\", how=\"left\", suffixes=(\"_old\",\"\"))\nnewpreds.loc[newpreds.price_doc.isnull(),\"price_doc\"] = newpreds.price_doc_old\nnewpreds.drop(\"price_doc_old\",axis=1,inplace=True)\nnewpreds.head()", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e2911270-1f4d-c8f6-0331-71c93cf6414d", "_uuid": "df674cb226e6e8b8ec274e1382dc45cf4269839d", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": ["Save."], "metadata": {"_cell_guid": "81d900b7-5bff-5a36-b846-3b09ae6f8d1e", "_uuid": "f360d05ecfeef561ee58bd0288d359d48f1714c3"}, "execution_count": null}, {"source": ["newpreds.to_csv('after.csv', index=False)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "83fcce15-4ef7-b500-5849-7d28e242f149", "_uuid": "1c6c9ad94942ee475f586d6351482428096f569c", "_execution_state": "idle"}}, {"source": [""], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "c062bc50-5209-f1ac-94fa-551050b88477", "_uuid": "89acfdd4d7c6a68128515c1f89a8e25c368a5139", "_execution_state": "idle"}}], "nbformat_minor": 0, "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "_change_revision": 0, "language_info": {"file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.1", "nbconvert_exporter": "python", "mimetype": "text/x-python"}, "_is_fork": false}}