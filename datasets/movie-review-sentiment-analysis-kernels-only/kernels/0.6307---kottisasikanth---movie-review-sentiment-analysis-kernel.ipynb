{"cells":[{"metadata":{"_uuid":"b3a87d5b9bba7153e1f92a6d1a8902d9d568f1b9"},"cell_type":"markdown","source":"# Kernel for Movie Review Sentiment Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import svm\nfrom scipy import sparse\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,make_scorer\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e63b106d8ed665c09a00902e9e6f0e5aa10e94bb"},"cell_type":"markdown","source":"### Load train and test datasets in pandas dataframe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.tsv\", delimiter = '\\t')\ntest = pd.read_csv(\"../input/test.tsv\", delimiter = '\\t')\nsubmission = pd.read_csv(\"../input/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad793a57707f23479827372c43fb373531d0e90e"},"cell_type":"markdown","source":"### View sample records in train and test datasets"},{"metadata":{"trusted":true,"_uuid":"8abdac4411155df130e447d2522b745aa42f3b24","collapsed":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b17424a8fe324967f31f6879250ab85f1cd0f224","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44e41a6c8fa72e0b21443b708a64c3116870b308","collapsed":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8ed40c114a696f27b3e5baf6c602a2800fe12f"},"cell_type":"markdown","source":"### Target variable from train dataset"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7cf14c47bc983f04dca28129fc462818ea3dd4ad"},"cell_type":"code","source":"y_train = train['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c46c5ac59eb1222037b7fd9280c5f75d38286f5"},"cell_type":"markdown","source":"### Plot the Target variable from the train dataset"},{"metadata":{"trusted":true,"_uuid":"d44773b4daaf4757be33a505731eee45d706c13e","collapsed":true},"cell_type":"code","source":"sns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3c80e21413be94dda3e4a117a0adbdace57acc1"},"cell_type":"markdown","source":"### Obtain tf-idf representation for train and test dataset"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9d190b8e3d6d088548a66424c4c53b4942e826b5"},"cell_type":"code","source":"class LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [WordNetLemmatizer().lemmatize(w) for w in word_tokenize(doc)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80c4877d038ad64cab9c60e9b53cab53c301f6c","collapsed":true},"cell_type":"code","source":"vectorizer_w = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (1,3), analyzer = 'word', encoding = 'utf-8', tokenizer = LemmaTokenizer())\nvectorizer_c = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (2,6), analyzer = 'char', encoding = 'utf-8', tokenizer = LemmaTokenizer())\nX_train_w = vectorizer_w.fit_transform(train['Phrase'])\nX_train_c = vectorizer_c.fit_transform(train['Phrase'])\nX_test_w = vectorizer_w.transform(test['Phrase'])\nX_test_c = vectorizer_c.transform(test['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48af9b781a77cb088f5a3799c6982b243e7b68d7","collapsed":true},"cell_type":"code","source":"X_train = sparse.hstack([X_train_w, X_train_c])\nX_test = sparse.hstack([X_test_w, X_test_c])\n\n#Tried Oversampling methods using imbalanced-learn API(http://contrib.scikit-learn.org/imbalanced-learn/stable/api.html)\n#However Oversampling did not help\nros = RandomOverSampler(random_state=42)\nada = ADASYN(random_state=152)\n#X_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\n#X_train_ada, y_train_ada = ada.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f95fccea4567bb67fab7955e51b08df38e011f3d","collapsed":true},"cell_type":"code","source":"print(\"Number of samples in Train dataset i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train.shape)\nprint(\"Number of samples in Test dataset i.e. n_samples: %d, Number of features in Test dataset i.e. n_features: %d\" % X_test.shape)\nprint(\"\\n\")\n#print(\"Number of samples in Resample Train dataset(Ramdom Sampler) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ros.shape)\n#print(\"Number of samples in Resample Train dataset(ADASYN) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ada.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee99f0bd0dfb0dae8434e8830e031c178c3b0e8c","collapsed":true},"cell_type":"code","source":"#sns.countplot(y_train_ada)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f7179db7ab2ed0020a218fabcdd55148bd52d7"},"cell_type":"markdown","source":"### Naive Bayes classifier from sklearn . MultinomialNB classifier used below."},{"metadata":{"trusted":true,"_uuid":"e3803082d7676adb472547a9f2fb672a5afe389e","collapsed":true},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train,y_train)\ny_pred_nb = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e901b4e83a89a1675d0f31d3843f681d80bd9801"},"cell_type":"markdown","source":"### Predictions and submissions"},{"metadata":{"trusted":true,"_uuid":"d64810873e2c16f2279ab9d553d519e4e1f6f8cb","collapsed":true},"cell_type":"code","source":"submission['Sentiment'] = y_pred_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87eedf8caac42a39cdd5c6acb7fc06d05a318bf1","collapsed":true},"cell_type":"code","source":"submission.to_csv(\"submission_NB.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0cf29647ed1ff48057c942c9657e80a5f6bc62d5"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"359074fd04670f203a7eeca61966fb51f98e5456","collapsed":true},"cell_type":"code","source":"lclf = LogisticRegression(solver = 'saga',multi_class = 'multinomial', max_iter = 4000, C = 4, random_state = 42, verbose = 10, class_weight = 'balanced')\n\n#parameters = {'C':[2 , 4] }\n#scorer = make_scorer(accuracy_score)\n#cv = StratifiedShuffleSplit(2, random_state = 62)\n#grid_obj = GridSearchCV(lclf, param_grid=parameters, cv = cv, scoring=scorer, n_jobs=-1, verbose=10)\n#grid_fit = grid_obj.fit(X_train, y_train)\n#best_clf = grid_fit.best_estimator_\n\npredictions = (lclf.fit(X_train, y_train)).predict(X_test)\n#best_predictions = best_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e7822fb92c7a988660c35f45ec73e371a271c98","collapsed":true},"cell_type":"code","source":"#submission['Sentiment'] = best_predictions\nsubmission['Sentiment'] = predictions\nsubmission.to_csv(\"submission_LogisticRegression.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ad9a364b1d5f3065f23916e35c045c5be473292"},"cell_type":"markdown","source":"## Standard NLP Pre-Processing"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c24c84e833b7b267d679a65e9f2fe06861cc2cee"},"cell_type":"code","source":"X_train = train['Phrase']\nX_test = test['Phrase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c85f1c7fed8a0054eace9aa2daa4e592aa89a53","collapsed":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c53e107eb9d201f85f22a73d1ea4e5a4d2c53d58"},"cell_type":"markdown","source":"### NORMALIZATION - Converting to lower case"},{"metadata":{"trusted":true,"_uuid":"0db1fa3e011e175664f88a5448a0fbc83e32ee13","collapsed":true},"cell_type":"code","source":"X_train_l = X_train.str.lower()\nprint(X_train_l[0])\nX_train_l.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdaacedf2c5ea0374db409f9fe5a765b6ebb5d36"},"cell_type":"markdown","source":"### NORMALIZATION - Removing Punctuation marks"},{"metadata":{"trusted":true,"_uuid":"aae822d9c99e05bb628e62d39b7dcb3a313c87e8","collapsed":true},"cell_type":"code","source":"import re\ndef punc_rem(y):\n    return re.sub(r\"[^a-zA-Z0-9]\", \" \", y)\nX_train_p = X_train_l.apply(lambda x: punc_rem(x))\nprint(X_train_p[0])\nX_train_p.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68fcc51964fbe61cb0698c5aa6f6f457a6a42471"},"cell_type":"markdown","source":"### TOKENIZATION - Word & Setence tokenizers"},{"metadata":{"trusted":true,"_uuid":"dc852dd9c2efbae468bcdad4eabff529f1650f86","collapsed":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\n\nX_train_wt = X_train_p.apply(lambda x : word_tokenize(x))\nX_train_st = X_train_p.apply(lambda x : sent_tokenize(x))\n\nprint(X_train_wt[0])\nprint(X_train_wt.head())\nprint(X_train_st[0])\nprint(X_train_st.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a375ef05288dab888d88fea31374c4b4f82499a4"},"cell_type":"markdown","source":"### STOPWORDS Removal"},{"metadata":{"trusted":true,"_uuid":"acc45e9d849ade20596f44be85faeeb8a6c1af65","collapsed":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nprint(stopwords.words('english'))\ndef stop_words(x):\n    return [i for i in x if i not in stopwords.words('english')]\nX_train_sw = X_train_wt.apply(lambda x : stop_words(x))\nprint(X_train_sw[0])\nprint(X_train_sw[3])\nprint(X_train_sw.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37efd45264253d7eb5f720fa581d4f47505e5bda"},"cell_type":"markdown","source":"### Although stopwords removal is a standard practice in NLP pipeline, it appears this is not applicable here(Refer row with id 3)"},{"metadata":{"_uuid":"3627a5127a53740c38464ef06bc3afb5965ce135"},"cell_type":"markdown","source":"### POS (Parts Of Speech Tagging) & NER (Named Entity Recognition)"},{"metadata":{"trusted":true,"_uuid":"124d3b6cde4440ecaeec8917af36ade1519a50c6","collapsed":true},"cell_type":"code","source":"import nltk\ndef postag(x):\n    return nltk.pos_tag(x)\nX_train_pos = X_train_wt.apply(lambda x: postag(x))\nprint(X_train_pos[0])\nprint(X_train_pos[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94d0c3e2cb0b84d0278b58f11f7c74225426fc45","collapsed":true},"cell_type":"code","source":"#nltk.help.upenn_tagset('CC')\nfor i in X_train_pos[0]:\n    print(\"{}: \".format(i))\n    (nltk.help.upenn_tagset(i[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3a9ec8acb943eaaef5aacf95b5ba26e44e8c58e","collapsed":true},"cell_type":"code","source":"nltk.corpus.stopwords.readme() #https://www.nltk.org/book/ch05.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"694e266cfe5dec36c5892d06d0af30501beb5b89","collapsed":true},"cell_type":"code","source":"def ner(x):\n    return nltk.ne_chunk(x)\n\nX_train_ner = X_train_pos.apply(lambda x: ner(x))\n\nprint(X_train_ner[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"500a7213212cfa29a6573d74f5cc74286f0f4508","collapsed":true},"cell_type":"code","source":"print(nltk.ne_chunk(nltk.pos_tag(word_tokenize(\"India is a great country\"))))\n#https://www.nltk.org/book/ch07.html","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c44a761455d0cff8adfb12e001fac493fc2ec974"},"cell_type":"markdown","source":"### CFG - Context Free Grammer"},{"metadata":{"trusted":true,"_uuid":"d05e13cebc44cd29c8fa9dcd3fba2d3d6dd4dbf7","collapsed":true},"cell_type":"code","source":"print(X_train[1])\nprint(X_train_wt[1])\nprint(X_train_pos[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83a6ff2787bf7a7aded5dbeb7f2eebea331becd0","collapsed":true},"cell_type":"code","source":"nltk.help.upenn_tagset('JJ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bc6d4d24a0aafe4382ba9c2d1b6c23eebc954fd","collapsed":true},"cell_type":"code","source":"custom_grammer = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP \nVP -> V NP | VP PP | JJ\nDet -> 'the'|'a'\nN -> 'series'|'escapades'|'adage'|'goose'\nV -> 'demonstrating'|'is'\nJJ -> 'good'\nP -> 'that'|'for'|'of'|'what'\n\"\"\")\n\ncustom_parser = nltk.ChartParser(custom_grammer)\nprint(custom_parser.parse(X_train_wt[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c3c8a48815e2ef0706a9362a0a6fafdc29626d","collapsed":true},"cell_type":"code","source":"help(nltk.ChartParser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"159fe0fa473ea21d4c9af8574330d4183e2f7585","collapsed":true},"cell_type":"code","source":"for custom_tree in custom_parser.parse(X_train_wt[1]):\n    print(\"Sasikanth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ece305f41cbe8327b7d0daa49400ec4b119615e"},"cell_type":"code","source":"# Define a custom grammar\nmy_grammar = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP | 'I'\nVP -> V NP | VP PP\nDet -> 'an' | 'my'\nN -> 'elephant' | 'pajamas'\nV -> 'shot'\nP -> 'in'\n\"\"\")\nparser = nltk.ChartParser(my_grammar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a25aee2a770a123a4c636136ce8d7ec0c69a149","collapsed":true},"cell_type":"code","source":"nltk.pos_tag(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ea6cae6af00cc8f8335421f1f4cf3d71622a579","collapsed":true},"cell_type":"code","source":"# Parse a sentence\nsentence = word_tokenize(\"I shot an elephant in my pajamas\")\nprint(type(sentence))\nnltk.pos_tag(sentence)\nprint(parser.parse(sentence))\nfor tree in parser.parse(sentence):\n    print(type(tree))\n    print(tree)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88d90b3896ce3bee6bbd3f4aafdfaec8e9032a9e"},"cell_type":"markdown","source":"### COREFERENCE - Doesn't seem to exist in NLTK"},{"metadata":{"_uuid":"1453d7e092ab751d76243812cb77ae7f0c44c752"},"cell_type":"markdown","source":"### STEMMING and LEMMATIZATION"},{"metadata":{"trusted":true,"_uuid":"ad0645a0e2895dc4f223bdb309f7a12dea181df5","collapsed":true},"cell_type":"code","source":"from nltk.stem import porter\nstemmer = porter.PorterStemmer()\ndef stmr(x):\n    return [stemmer.stem(i) for i in x]\nX_train_stm = X_train_wt.apply(lambda x: stmr(x))\nprint(X_train_wt[0])\nprint(X_train_stm[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb80437cce250b06aad4331c0d068bba4a68960a","collapsed":true},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\ndef lmtr(x):\n    return [WordNetLemmatizer().lemmatize(i) for i in x]\n\ndef lmtrv(x):\n    return [WordNetLemmatizer().lemmatize(i, pos = 'v') for i in x]\n\nX_train_lm = X_train_wt.apply(lambda x: lmtr(x))\nX_train_lmv = X_train_wt.apply(lambda x: lmtrv(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfbcd9b1136af48ba20d289106a3c12d13952f4f","collapsed":true},"cell_type":"code","source":"print(X_train_wt[1])\nprint(X_train_lm[1])\nprint(X_train_lmv[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fe8e7d98926e5e2f50436e22c050529db071bbe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5b54a6a9cffedcd8e2795b18d830a8b5ce2b0485"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}