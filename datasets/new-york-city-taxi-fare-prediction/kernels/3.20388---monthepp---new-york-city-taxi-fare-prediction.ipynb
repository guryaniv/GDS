{"cells":[{"metadata":{"_uuid":"62b090b7913da8a4019515a6540f982f4a7767fa"},"cell_type":"markdown","source":"> **Problem overview**\n\nIn this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!\n\nTo learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org/NEXTextended to claim this specialization for free for the first month!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import library\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# import data preprocessing from sklearn\nfrom sklearn.preprocessing import RobustScaler\n\n# import model function from sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n\n# import model selection from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# import model evaluation regression metrics from sklearn\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63bac28c47a40969bd0349adf87b90f669b4330f"},"cell_type":"markdown","source":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# acquiring training and testing data\ntrain_df = pd.read_csv('../input/train.csv', nrows = 2000000, parse_dates=['pickup_datetime'])\ntest_df = pd.read_csv('../input/test.csv', parse_dates=['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"773d8c286e14a1e9006e8b6a3807870cf23d4413"},"cell_type":"code","source":"# visualize head of the training data\ntrain_df.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2866ad35c88e0fa0f72e0480a141ff4aa51af4de"},"cell_type":"code","source":"# visualize tail of the testing data\ntest_df.tail(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e60bda1ee47bac42e466ba5c9b95d112559d88a4"},"cell_type":"code","source":"# convert training dataframe fare amount to log fare amount\ntrain_df['fare_amount'] = train_df['fare_amount'].apply(lambda x: np.log1p(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"915b1fc0b03d9e125ded941701d386f92e6e707d"},"cell_type":"code","source":"# drop na\ntrain_df = train_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76f1ac478aa137a8885c0362ad9ce423de5a080a"},"cell_type":"code","source":"# combine training and testing dataframe\ntrain_df['datatype'], test_df['datatype'] = 'training', 'testing'\ntest_df.insert(1, 'fare_amount', 0)\ndata_df = pd.concat([train_df, test_df])\ndata_df.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f832542c3bb2d027cd91eaace7bce30f3a2af300"},"cell_type":"markdown","source":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution together with exploring some data."},{"metadata":{"trusted":true,"_uuid":"702692507ca048457608d42c2907d4a7d7bd768a"},"cell_type":"code","source":"# describe training and testing data\ndata_df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a687aba7c686747f098689876c0ada4625a4ce90"},"cell_type":"code","source":"# find distance bewteen 2 latitude and longitude\ndef distance(lat1, lon1, lat2, lon2):\n    angle = 0.017453292519943295 #math.pi / 180\n    x = 0.5 - np.cos((lat2 - lat1) * angle) / 2 + np.cos(lat1 * angle) * np.cos(lat2 * angle) * (1 - np.cos((lon2 - lon1) * angle)) / 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef31347da9fad6e4b1c28944365f80ee38d1670b"},"cell_type":"code","source":"# feature extraction: combination of keyword date\ndata_df['year'] = data_df['pickup_datetime'].dt.year\ndata_df['quarter'] = data_df['pickup_datetime'].dt.quarter\ndata_df['month'] = data_df['pickup_datetime'].dt.month\ndata_df['weekofyear'] = data_df['pickup_datetime'].dt.weekofyear\ndata_df['weekday'] = data_df['pickup_datetime'].dt.weekday\ndata_df['dayofweek'] = data_df['pickup_datetime'].dt.dayofweek\ndata_df['hour'] = data_df['pickup_datetime'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e57546028d74a727ebeb2b8bec159281dee4878e"},"cell_type":"code","source":"# feature extraction: distance\ndata_df['distance_euclidean'] = distance(data_df['pickup_latitude'], data_df['pickup_longitude'], \\\n                                         data_df['dropoff_latitude'], data_df['dropoff_longitude'])\ndata_df['distance_latitude'] = data_df['dropoff_latitude'] - data_df['pickup_latitude']\ndata_df['distance_longitude'] = data_df['dropoff_longitude'] - data_df['pickup_longitude']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbddd8318216b9d3d4735db9fbac5316031cad4a"},"cell_type":"code","source":"# feature extraction: distance to specific location\nnyc = (40.7128, -74.0060)\njfk = (40.6413, -73.7781)\newr = (40.6895, -74.1745)\ndata_df['distance_pickup_to_nyc'] = distance(data_df['pickup_latitude'], data_df['pickup_longitude'], nyc[0], nyc[1])\ndata_df['distance_pickup_to_jfk'] = distance(data_df['pickup_latitude'], data_df['pickup_longitude'], jfk[0], jfk[1])\ndata_df['distance_pickup_to_ewr'] = distance(data_df['pickup_latitude'], data_df['pickup_longitude'], ewr[0], ewr[1])\ndata_df['distance_dropoff_to_nyc'] = distance(data_df['dropoff_latitude'], data_df['dropoff_longitude'], nyc[0], nyc[1])\ndata_df['distance_dropoff_to_jfk'] = distance(data_df['dropoff_latitude'], data_df['dropoff_longitude'], jfk[0], jfk[1])\ndata_df['distance_dropoff_to_ewr'] = distance(data_df['dropoff_latitude'], data_df['dropoff_longitude'], ewr[0], ewr[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5da2c7c712a1091c9b0e3e6f28418a9a6e370ab6"},"cell_type":"code","source":"# feature extraction: fare amount per mile\ndata_df['fare_per_mile'] = data_df['fare_amount'] / data_df['distance_euclidean']\ndata_df['fare_per_mile'] = data_df['fare_per_mile'].apply(lambda x: 0 if x == float('inf') else x)\ndata_df['fare_per_mile'] = data_df['fare_per_mile'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc60c83c14a37f86f00f1a1173263a22f90c76c0"},"cell_type":"code","source":"# scatter plot between distance and fare amount\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.scatterplot(data=data_df[data_df['datatype'] == 'training'], x='distance_euclidean', y='fare_amount')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed991992b0661ec01531dc50411045badb171701"},"cell_type":"code","source":"# scatter plot between distance and fare amount\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.scatterplot(data=data_df[(data_df['datatype'] == 'training') & (data_df['distance_euclidean'] < 50)], x='distance_euclidean', y='fare_amount')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c69c1f6ca3b113ecb195868c35406ae05a463eb4"},"cell_type":"code","source":"# scatter plot between distance and fare per mile\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.scatterplot(data=data_df[(data_df['datatype'] == 'training') & (data_df['distance_euclidean'] > 1)], x='distance_euclidean', y='fare_per_mile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea280b27864e7a5742a03946e741a70583a03fda"},"cell_type":"code","source":"# scatter plot between distance and fare per mile\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.scatterplot(data=data_df[(data_df['datatype'] == 'training') & (data_df['distance_euclidean'] > 1) & (data_df['distance_euclidean'] < 50)], x='distance_euclidean', y='fare_per_mile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1f5b4f7478a568362a8f5ab84251bbdb1368ccc"},"cell_type":"code","source":"# feature extraction: year-month\ngroupby = data_df[data_df['datatype'] == 'training'].groupby(['year', 'month'])\ngroupby = groupby.mean()['fare_amount'].reset_index()\ngroupby.columns = ['year', 'month', 'fare_amount_average']\n\nfig, ax = plt.subplots(figsize=(20, 5))\npointplot = sns.pointplot(data=groupby, join=True, hue='year', x='month', y='fare_amount_average')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f911b0179cc7d9db38c8179508df01eb9be6a09"},"cell_type":"code","source":"# feature extraction: year-hour\ngroupby = data_df[data_df['datatype'] == 'training'].groupby(['year', 'hour'])\ngroupby = groupby.mean()['fare_amount'].reset_index()\ngroupby.columns = ['year', 'hour', 'fare_amount_average']\n\nfig, ax = plt.subplots(figsize=(20, 5))\npointplot = sns.pointplot(data=groupby, join=True, hue='year', x='hour', y='fare_amount_average')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"279e722a93f8733c5ce75c20d1c89b5f9ae93082"},"cell_type":"code","source":"# feature extraction: datatype\ndata_df['datatype'] = data_df['datatype'].map({'testing': 0, 'training': 1, 'excluded': '2'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf64b2d8cbccc45c41e8a21d4230540bb3c3b73"},"cell_type":"code","source":"data_df.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fe322218aa7f731f098e0482b1906889251a7f2"},"cell_type":"markdown","source":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models."},{"metadata":{"trusted":true,"_uuid":"4e12e08c58bc85345d595c3c7010db14c79a9b8e"},"cell_type":"code","source":"# verify dtypes object\ndata_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"867ca1b5e56075b35770683317515ba1a8d7525d"},"cell_type":"code","source":"# convert dtypes object to category\ncol_obj = data_df.select_dtypes(['object']).columns\ndata_df[col_obj] = data_df[col_obj].astype('category')\ndata_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3c09367eece31a7f565748845891c71416defa5"},"cell_type":"code","source":"# convert dtypes category to category codes\ncol_cat = data_df.select_dtypes(['category']).columns\ndata_df[col_cat] = data_df[col_cat].apply(lambda x: x.cat.codes)\ndata_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"224c3bfada9680bf283bf8f7f9762efcbba78a54"},"cell_type":"code","source":"data_df.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89507ef7f98ffafeef409a70c09fb83403ac44ef"},"cell_type":"markdown","source":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value."},{"metadata":{"trusted":true,"_uuid":"e4feaef459f7f7e1080c5fa5a870dceafa5bfc4e"},"cell_type":"code","source":"# compute pairwise correlation of columns, excluding NA/null values and present through heat map\ncorr = data_df[data_df['datatype'] == 1].corr()\nfig, ax = plt.subplots(figsize=(20, 15))\nheatmap = sns.heatmap(corr, annot=True, cmap=plt.cm.RdBu, fmt='.1f', square=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f77ace76d333a0b2864de8050e222446fc2efa82"},"cell_type":"markdown","source":"The pairplots is also useful to observe the distribution of the training data from one feature to the other."},{"metadata":{"trusted":true,"_uuid":"1bced5246381f56f12754d9857c8dde9626642b3"},"cell_type":"code","source":"# plot pairwise relationships in a dataset\n#pairplot = sns.pairplot(data_df[data_df['datatype'] == 1], diag_kind='kde', diag_kws=dict(shade=True), hue='fare_amount')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05ec3b6c33d177b49a2a0de81bfd5e694ae13e20"},"cell_type":"markdown","source":"The pivot table and other visulized plots are also another useful methods to observe the impact between features."},{"metadata":{"trusted":true,"_uuid":"b7880bf6ba4b2a2e2712d7ec5f30e1584485bd85"},"cell_type":"code","source":"# pivot table\npivottable = pd.pivot_table(data_df[data_df['datatype'] == 1], aggfunc=np.mean,\n                            columns=['year'], index=['hour'], values='fare_per_mile')\npivottable.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0300de3ea2f28837077f0e0f5cfa63d9fd35a9dc"},"cell_type":"markdown","source":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models."},{"metadata":{"trusted":true,"_uuid":"583fbd3f2dc8f463da827aaa9d8adc96605d55ec"},"cell_type":"code","source":"# select all features\nx = data_df[data_df['datatype'] == 1].drop(['key', 'pickup_datetime', 'fare_amount', 'datatype', 'fare_per_mile'], axis=1)\ny = data_df[data_df['datatype'] == 1]['fare_amount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9506e27338c3e4979adac8b990f5525f1497c6b8"},"cell_type":"code","source":"x.head(n=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6df3f4f0a5d74af0f01103c45f36c12bc600bf43"},"cell_type":"code","source":"# create scaler to the features\nscaler = RobustScaler()\nx = scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"752b56e9aae1b300f14ea42b7658460d68c67de9"},"cell_type":"code","source":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, random_state=0, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f84ce656a16fa151cca3a44b0e69124ec4acd939"},"cell_type":"code","source":"# random forest model prediction\nforestreg = RandomForestRegressor(max_depth=20, min_samples_split=5, n_estimators=10, random_state=0).fit(x_train, y_train)\nforestreg_ypredict = forestreg.predict(x_validate)\nforestreg_mse = mean_squared_error(y_validate, forestreg_ypredict) ** 0.5\nforestreg_cvscores = np.sqrt(np.abs(cross_val_score(forestreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('random forest regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+/- %0.4f)' %(forestreg_mse, forestreg_cvscores.mean(), 2 * forestreg_cvscores.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2bc8e7bd6741eb6ea5b98d1fc28d4006595e156"},"cell_type":"markdown","source":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome."},{"metadata":{"trusted":true,"_uuid":"8300d72f6cf2e78f7ad5f47d92aad01ce249768d"},"cell_type":"code","source":"# model selection\nmodel = forestreg\n\n# prepare testing data and compute the observed value\nx_test = data_df[data_df['datatype'] == 0].drop(['key', 'pickup_datetime', 'fare_amount', 'datatype', 'fare_per_mile'], axis=1)\nx_test = scaler.transform(x_test)\ny_test = pd.DataFrame(np.expm1(model.predict(x_test)), columns=['fare_amount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d094747e8b8528ee5bbc5ab242fedb2f3d0c890"},"cell_type":"code","source":"# summit the results\nout = pd.DataFrame({'key': test_df['key'], 'fare_amount': y_test['fare_amount']})\nout.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a305bf88068b7b0924ba236dc99bcca1f644bdbc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}