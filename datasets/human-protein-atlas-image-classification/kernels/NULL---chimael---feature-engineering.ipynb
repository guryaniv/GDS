{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport skimage.io\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Feature engineering\nAs a baseline model, I would like to perform some basic feature engeneering to predict the localization of the protein of interest. The features will be the overlaping pixels of the green channel (the protein of interest) vs the other channels (red,blue, yellow)."},{"metadata":{"trusted":true,"_uuid":"ab1a855038f942ad336c3677c8945fdd2064056a"},"cell_type":"code","source":"#import training data\ntrain = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18c7c24beaa21d6c1390f80ced1ffbb6a9d77dd2"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2f7dc53521eb8a3e61786edf4296d12617eaa8c"},"cell_type":"markdown","source":"For each files:\n* import the 4 channels.\n* threshold of the channels\n* convert to black and white pictures\n* mask the green picture with other channels\n* compute the ratio of unmasked green vs total green."},{"metadata":{"_uuid":"9d0850509923fd97facdef4de1c34dab2f5bead1"},"cell_type":"markdown","source":"### Import 4 channels into numpy array"},{"metadata":{"trusted":true,"_uuid":"575082048999f071506b2981cf339b3bd25e4e60"},"cell_type":"code","source":"path_to_train = '../input/train/'\ndef load_image(file):\n    image_red_ch = skimage.io.imread(path_to_train+file+'_red.png')\n    image_yellow_ch = skimage.io.imread(path_to_train+file+'_yellow.png')\n    image_green_ch = skimage.io.imread(path_to_train+file+'_green.png')\n    image_blue_ch = skimage.io.imread(path_to_train+file+'_blue.png')\n    image = np.stack((image_green_ch, image_red_ch, image_blue_ch, image_yellow_ch))\n    return image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff782c5bbbdcaf9cc9993e99af9b77ffa3afcd38"},"cell_type":"markdown","source":"### Threshold"},{"metadata":{"trusted":true,"_uuid":"ca5f3aa3871a2c5893b9ab405d65465d751c1191"},"cell_type":"code","source":"from skimage.filters import threshold_otsu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee1f98d55eadea14cd4f55e6d7dd432708a5b4a5"},"cell_type":"code","source":"def threshod_image(img):\n    bw_img = np.zeros_like(img, dtype=bool)\n    for i, arr in enumerate(img):\n        bw_img[i] = arr > threshold_otsu(arr)\n    return bw_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"939001cf3a4e1728f763031dc09db32af27623e7"},"cell_type":"markdown","source":"### Mask the green channel"},{"metadata":{"trusted":true,"_uuid":"71162da2700e73c2ad40bf50e3a75833ed5a752a"},"cell_type":"code","source":"def mask_green(bw_img):\n    mask_img_red = bw_img[0] & bw_img[1]\n    mask_img_blue = bw_img[0] & bw_img[2]\n    mask_img_yellow = bw_img[0] & bw_img[3]\n    return np.stack((bw_img[0], mask_img_red, mask_img_blue, mask_img_yellow))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f51991a029f5086739aba9b32f7981d8a300e8f"},"cell_type":"markdown","source":"### Ratios"},{"metadata":{"trusted":true,"_uuid":"5ee1f6de62dc7535effef7730e064df19c2e15f4"},"cell_type":"code","source":"def compute_ratios(mask_img):\n    ratios = []\n    for i in range(1,mask_img.shape[0]):\n        ratios.append(mask_img[i].sum()/mask_img[0].sum())\n    return ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b73ed7f4e4cb237ae4117900a2cd80da73a8eb"},"cell_type":"code","source":"def transform(file):\n    a = load_image(file)\n    bw_img = threshod_image(a)\n    mask_img = mask_green(bw_img)\n    return compute_ratios(mask_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8506374e38ac9b07791a3b188cdc38e64b292e73"},"cell_type":"markdown","source":"### Define Target"},{"metadata":{"trusted":true,"_uuid":"5cf7df40b3cceebc3a6488478845321b1e289853"},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04f8b83bbeead3d0ffa4fa18d3ef2b5791c0d517"},"cell_type":"code","source":"def dataset(size=100):\n    targets = []\n    features = []\n    c = 0\n    for i, row in train.sample(size, random_state=1).iterrows():\n        c+=1\n        targets.append([int(x) for x in row[1].split(' ')])\n        features.append(transform(row[0]))\n        if c % 10 == 0:\n            print(\"Processing %.2f\" % ((c*100)/size), end='\\r')\n    return np.array(features), MultiLabelBinarizer().fit_transform(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c17c84b84d172359d0fc999ff9ed80fff5bd674"},"cell_type":"code","source":"features, targets = dataset(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96f285096bf963b9831104b63a6a1c85bb4959b6"},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fce7873563f13865c5a7fbd28103eb8d6896d311"},"cell_type":"code","source":"targets.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a4380dc729a038ebc0f27516569f070c3a3edb1"},"cell_type":"markdown","source":"### Multilabel classifier\n"},{"metadata":{"trusted":true,"_uuid":"017cfb60bdaa9b8f2cc017586b45e11807fccb0e"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c724d6bc4de7fac576f66b22f0527e3cb5dd1009"},"cell_type":"code","source":"neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(features[:700], targets[:700]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8a30db3c77586cb545dafda4e204537b8a4e2c1"},"cell_type":"code","source":"preds = neigh.predict(features[700:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bb08918c00b08f2bbb3cdec0be3bed66bf463c8"},"cell_type":"code","source":"f1_score(targets[700:], preds, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c9386f878bb48c60de686be29ec38b58912a302"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}