{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"! pip3 install pretrainedmodels torch==0.4.1\nimport glob\nimport cv2\nimport numpy as np\nimport pretrainedmodels\nimport torch\nimport time\nfrom tqdm import tqdm\n\ndef read_image(path):\n    return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n\ndef get_image_generator(image_paths, batch_size):\n    batch = []\n    for path in image_paths:\n        batch.append(read_image(path))\n        if len(batch) >= batch_size:\n            yield batch\n            batch = []\n    if len(batch) > 0: yield batch\n\ndef get_images_in(path):\n    return np.sort(glob.glob(f'{path}/*.jpg'))\n\nclass Model(torch.nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.resnet = pretrainedmodels.se_resnet50(pretrained='imagenet')\n        self.resnet.avg_pool = torch.nn.AdaptiveAvgPool2d(1)\n        self.resnet.last_linear = torch.nn.Linear(self.resnet.last_linear.in_features, num_classes)\n\n    def forward(self, x):\n        return self.resnet(x)\n\nmodel = Model(1).cuda()\nmodel.eval()\n\nbatch = torch.randn(16, 3, 224, 224).cuda()\n\n# PyTorch is asynchronous,\n# but when there's no I/O bottleneck, it's forced to wait for previous batch to compute.\nresults = []\ntotal_time = 0\nfor i in tqdm(range(125)):\n    start_time = time.time()\n    results.extend((model(batch) > 0)[:, 0])\n    total_time += (time.time() - start_time)\nprint('Inference Time, automatic synchronization: %0.2f Minutes' % ((total_time)/60))\n\n# When there is an I/O bottleneck, PyTorch can compute batches asynchronously.\n# Since this computation intersects with image reading, it's partially ignored by the timing code.\nresults = []\ntotal_time = 0\nfor _ in tqdm(get_image_generator(get_images_in('/kaggle/input/test_v2')[:2000], 16), total=125):\n    start_time = time.time()\n    results.extend((model(batch) > 0)[:, 0])\n    total_time += (time.time() - start_time)\n\nprint('Inference Time, no synchronization output: %0.2f Minutes' % ((total_time)/60))\n\n# Also, sending the tensor to .extend() doesn't force synchronization\n# But converting it to the numpy array does\nresults = []\ntotal_time = 0\nfor _ in tqdm(get_image_generator(get_images_in('/kaggle/input/test_v2')[:2000], 16), total=125):\n    start_time = time.time()\n    results.extend(np.array((model(batch) > 0)[:, 0]))\n    total_time += (time.time() - start_time)\n\nprint('Inference Time, np.array output: %0.2f Minutes' % ((total_time)/60))\n\n# It's also possible to force synchronization with torch.cuda.synchronize()\nresults = []\ntotal_time = 0\nfor _ in tqdm(get_image_generator(get_images_in('/kaggle/input/test_v2')[:2000], 16), total=125):\n    start_time = time.time()\n    results.extend((model(batch) > 0)[:, 0])\n    torch.cuda.synchronize()\n    total_time += (time.time() - start_time)\n\nprint('Inference Time, torch.cuda.synchronize output: %0.2f Minutes' % ((total_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"538ac3882445bbb3fbe41773126baee4187c3bc5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}