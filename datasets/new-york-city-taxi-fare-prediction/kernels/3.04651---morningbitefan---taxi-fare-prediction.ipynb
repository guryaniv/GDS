{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nTRAIN_PATH = '../input/train.csv'\nTEST_PATH = '../input/test.csv'\n\n# Assume we only know that the csv file is somehow large, but not the exact size\n# we want to know the exact number of rows\n\n# Method 1, using file.readlines. Takes about 20 seconds.\n#with open(TRAIN_PATH) as file:\n    #n_rows = len(file.readlines())\n\n#print (f'Exact number of rows: {n_rows}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b34489f6d787ceda112b8556cd065b068f89cb1d"},"cell_type":"code","source":"# Peep at the training file header\n#df_tmp = pd.read_csv(TRAIN_PATH, nrows=5)\n#df_tmp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ae4f06322a29228f9414be0834f3d97661803274"},"cell_type":"code","source":"#df_tmp.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82012a3b06fd9398aca9bf804ce9ccd70ab33cbe"},"cell_type":"code","source":"# Set columns to most suitable type to optimize for memory usage\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe405b9407b13be7d020a4e8d080bb67ced00a7"},"cell_type":"code","source":"NROWS = 12000000\ntest_df = pd.read_csv(TEST_PATH)\ntrain_df = pd.read_csv(TRAIN_PATH, usecols=cols,nrows = NROWS, dtype=traintypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0e62e0017b70e51dec6ca2a21512df2f1ef445f"},"cell_type":"code","source":"#NROWS = 6000000\n#chunksize = 5_000_000 # 5 million rows at one go. Or try 10 million\n#total_chunk = NROWS // chunksize + 1\n#print(f'Chunk size: {chunksize:,}\\nTotal chunks required: {total_chunk}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"b8256f46489c8e801f2e2baa4010dab584b2d57c"},"cell_type":"code","source":"'''\ndf_list = [] # list to hold the batch dataframe\ni=0\n\nfor df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols,nrows = NROWS, dtype=traintypes, chunksize=chunksize):\n    \n    i = i+1\n    # Each chunk is a corresponding dataframe\n    print(f'DataFrame Chunk {i:02d}/{total_chunk}')\n    \n    # Neat trick from https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboost\n    # Using parse_dates would be much slower!\n    df_chunk['pickup_datetime'] = df_chunk['pickup_datetime'].str.slice(0, 16)\n    df_chunk['pickup_datetime'] = pd.to_datetime(df_chunk['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    # Can process each chunk of dataframe here\n    # clean_data(), feature_engineer(),fit()\n    \n    # Alternatively, append the chunk to list and merge all\n    df_list.append(df_chunk) \n    \n    # Merge all dataframes into one dataframe\ntrain_df = pd.concat(df_list)\n\n# Delete the dataframe list to release memory\ndel df_list\n\n# See what we have loaded\ntrain_df.info()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"027f09bcee36cae8c753be52b8aae6f005a80752"},"cell_type":"code","source":"#display(train_df.head())\n#display(train_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01791408d9b7146dd9f045656d1b72e04ebbca73"},"cell_type":"code","source":"#Identify null values\n#print(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be54d49b9ef20241a310f36452958276731b2d76"},"cell_type":"code","source":"#Drop rows with null values\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e829834c8ee03900d6d4d444b5cd2c669a930c91"},"cell_type":"code","source":"#Plot variables using only 1000 rows for efficiency\n#train_df.iloc[:1000].plot.scatter('pickup_longitude', 'pickup_latitude')\n#train_df.iloc[:1000].plot.scatter('dropoff_longitude', 'dropoff_latitude')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e2b571ec49d61979d1bd599fb5697b2c99c0782"},"cell_type":"code","source":"#Clean dataset\ndef clean_df(df):\n    return df[(df.fare_amount > 0) & \n            (df.pickup_longitude > -80) & (df.pickup_longitude < -70) &\n            (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n            (df.dropoff_longitude > -80) & (df.dropoff_longitude < -70) &\n            (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n            (df.passenger_count > 0) & (df.passenger_count < 10)]\n\n#train_df = clean_df(train_df)\n#print(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98696f9b7e8ab5059dcb1d8263463d1ca340e38c"},"cell_type":"code","source":"def sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef add_airport_dist(dataset):\n    \"\"\"\n    Return minumum distance from pickup or dropoff coordinates to each airport.\n    JFK: John F. Kennedy International Airport\n    EWR: Newark Liberty International Airport\n    LGA: LaGuardia Airport\n    \"\"\"\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pickup_lat = dataset['pickup_latitude']\n    dropoff_lat = dataset['dropoff_latitude']\n    pickup_lon = dataset['pickup_longitude']\n    dropoff_lon = dataset['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon) \n    \n    dataset['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    dataset['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    dataset['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return dataset\n    \ndef add_datetime_info(dataset):\n    #Convert to datetime format\n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['weekday'] = dataset.pickup_datetime.dt.weekday\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    \n    return dataset\n\n#train_df = add_datetime_info(train_df)\n#train_df = add_airport_dist(train_df)\n#train_df['distance'] = sphere_dist(train_df['pickup_latitude'], train_df['pickup_longitude'], \n                                   #train_df['dropoff_latitude'] , train_df['dropoff_longitude'])\n\n#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a598d183d4a1d732ba00a3c8d568f34419e4e1eb"},"cell_type":"code","source":"#train_df.drop(columns=['key', 'pickup_datetime'], inplace=True)\n#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bafba03d9cdef910d9c0523e0c99d71e3c61830"},"cell_type":"code","source":"def transform_features(df):\n    df = add_datetime_info(df)\n    df = add_airport_dist(df)\n    df['distance'] = sphere_dist(df['pickup_latitude'], df['pickup_longitude'], \n                                   df['dropoff_latitude'] , df['dropoff_longitude'])\n    df.drop(columns=['pickup_datetime'], inplace=True)\n    return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"78abaa6b7232aa6f9887eca2d91ea4a1857ca330"},"cell_type":"code","source":"train_df = transform_features(train_df)\ntest_df = transform_features(test_df)\ntest_df.drop(columns=['key'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60a4f789bdbce49904101e3fc64476fdea904146"},"cell_type":"code","source":"#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc28082c4b16f5c0feeff486cfe5b53831e294c5"},"cell_type":"code","source":"#test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a7d9e73af8d7b89a2d4c61786e7a977fb43bd5d6"},"cell_type":"code","source":"train_df_y = train_df.fare_amount.copy()\ntrain_df_X = train_df[test_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbb486d391593e0818d710ec9fe3c1978386064d"},"cell_type":"code","source":"#train_df_X.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4a9989ed76950a829dcc12288f25da44b374a664"},"cell_type":"code","source":"#test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3f0f3adf0390a28a68a3ae3d92d4f471d9be1a9"},"cell_type":"code","source":"#print(\"Does Train feature equal test feature?: \", all(train_df_X.columns == test_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5505b939d23b50c4a73b8595531b9330643f83a6"},"cell_type":"code","source":"trainshape = train_df_X.shape\ntestshape = test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a7683ff59549cdee218e3af5848d5dccffac5c4"},"cell_type":"code","source":"# LGBM Dataset Formating\ndtrain = lgb.Dataset(train_df_X, label = train_df_y, free_raw_data = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2adcc39d678b7b915d9833314f14dbfac2872f1d"},"cell_type":"code","source":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate' :'0.03',\n    'num_leaves':'31',\n    'max_depth' : '-1',\n    'subsample' :'.8',\n     'colsample_bytree' : '0.6',\n        'min_split_gain' : '0.5',\n        'min_child_weight' : '1',\n        'min_child_samples' :'10',\n        'scale_pos_weight' : '1',\n        'num_threads' : '4',\n        'seed' : '0',\n        'eval_freq' : '50'\n                }\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\nfold_preds = np.zeros(testshape[0])\noof_preds = np.zeros(trainshape[0])\ndtrain.construct()\n\n# Fit 5 Folds\nmodelstart = time.time()\nfor trn_idx, val_idx in folds.split(train_df_X):\n    clf = lgb.train(\n        params=lgbm_params,\n        train_set=dtrain.subset(trn_idx),\n        valid_sets=dtrain.subset(val_idx),\n        num_boost_round=10000, \n        early_stopping_rounds=125,\n        verbose_eval=500     \n    )\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n    fold_preds += clf.predict(test_df) / folds.n_splits\n    print(mean_squared_error(train_df_y.iloc[val_idx], oof_preds[val_idx]) ** .5)\nprint(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d80cbfa693cccb88c81b3dbe6e3c26f56207e71"},"cell_type":"code","source":"test_df = pd.read_csv(TEST_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5a2935caed7d91bb40caf8a73680cce85cd4945"},"cell_type":"code","source":"#result = pd.DataFrame(fold_preds,columns=[\"fare_amount\"],index=testdex)\nresult = pd.DataFrame({'key':test_df['key'], 'fare_amount':fold_preds})\nresult.head()\nresult.to_csv('taxi-fare-prediction.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}