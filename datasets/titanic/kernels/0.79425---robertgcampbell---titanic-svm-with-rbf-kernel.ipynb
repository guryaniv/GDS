{"cells":[{"metadata":{"_uuid":"99a7fda968b8ff98e0187a7098ad7d2c6d20b562"},"cell_type":"markdown","source":"Hey everyone! Welcome to my submission for the Titanic kernel on Kaggle. This is my first Kaggle submission so any feedback would be welcome. \n\n**Overview**\n\nThe data has been split into two groups:\n\n    training set (train.csv)\n    test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n\n**Data Dictionary**\n\nVariable\tDefinition\tKey\nsurvival \tSurvival \t0 = No, 1 = Yes\npclass \tTicket class \t1 = 1st, 2 = 2nd, 3 = 3rd\nsex \tSex \t\nAge \tAge in years \t\nsibsp \t# of siblings / spouses aboard the Titanic \t\nparch \t# of parents / children aboard the Titanic \t\nticket \tTicket number \t\nfare \tPassenger fare \t\ncabin \tCabin number \t\nembarked \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton\n\n**Variable Notes**\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport warnings; warnings.simplefilter('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a1527d04b3b8e3b8df869976c71e898bea2d34","collapsed":true},"cell_type":"code","source":"# Extracting features from data\ndef extract_features(X):\n    \"\"\"\n    Format data and prepare features for use in model. One hot encodes categorical data and remove columns for features \n    are not used. \n\n    Parameters\n    ----------\n    \n    X : pandas dataframe that contains either test set or train set data. Shape is (m, n) where\n    m is the number of training examples and n is the number of features. \n    \n    \"\"\"\n    X = X.drop(\"PassengerId\", axis=1)\n    X = X.drop(\"Ticket\", axis=1)\n    X = X.drop(\"Cabin\", axis=1)\n    \n    # Adding polynomial features\n    X[\"Age2\"] = X[\"Age\"] ** 2\n    #X[\"Fare2\"] = X[\"Fare\"] ** 2\n    #X[\"Pclass2\"] = X[\"Pclass\"] ** 2\n\n    \n    male_titles = set([\"Mr\", \"Don\", \"Sir\"])\n    female_titles = set([\"Miss\", \"Ms\", \"Mrs\", \"Mme\", \"Mdm\", \"Lady\"])\n    professionals = set([\"Dr\", \"Rev\", \"Master\"])\n    military = set([\"Col\", \"Major\", \"Capt\"])\n    royalty = set([\"the Countess\", \"Jonkheer\"])\n    \n    names = X[\"Name\"]\n    for i in range(len(names)): \n        name_tokens = names[i].split(\", \") \n        passenger_title = name_tokens[1].split(\".\")[0]\n        if passenger_title in male_titles:\n            names[i] = 1\n        elif passenger_title in female_titles:\n            names[i] = 2\n        elif passenger_title in professionals:\n            names[i] = 3\n        #elif passenger_title in royalty:\n        #    names[i] = 4\n        elif passenger_title in military:\n            names[i] = 5\n        else:\n            names[i] = 6\n    \n    X[\"Name\"].update(names)\n    \n    # One hot encoding of categorical data\n    X = pd.get_dummies(X)    \n    \n    X.fillna(0, inplace=True)\n    X['Fam'] = X['SibSp'] + X['Parch']  # assigned to a column\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e3db574e90f745cc4b4e754f798b46f5a34fdc","collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\n\nX_train = extract_features(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ca0eb37f75caa02b7264f84186a9c32c21276d0","collapsed":true},"cell_type":"code","source":"# Extract y vector from train_set\ny = X_train[\"Survived\"]\nX_train = X_train.drop(\"Survived\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1939397d7ace98ce35749b7c701c2569af03ce72","collapsed":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94f12da9e517986d00d4dc36ca4a0391d2104967","collapsed":true},"cell_type":"code","source":"scaler = StandardScaler()  \n# Don't cheat - fit only on training data\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfcb7100dbc16d456bf427b05e7d1b94bc497170","collapsed":true},"cell_type":"code","source":"# Train model\nmodel = SVC()\n\nmodel.fit(X_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0508f74d0f8e56b71d31e8f09cbd1cb20052bfb3","collapsed":true},"cell_type":"code","source":"# Format test data\n\ntest_data = pd.read_csv('../input/test.csv')\n\nX_test = extract_features(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"987339096182d64f8d58ed913aca7d1ec9d9085d","collapsed":true},"cell_type":"code","source":"# Use model to make predictions on test set\nX_test_scaled = scaler.transform(X_test)\n\npredictions = model.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3923d75b77beef6fe2af81a2cad5063f6cd12ba5","_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"# Write predictions to a CSV file\ncsvfile = open('output.csv', 'w', newline='')\ncsvwriter = csv.writer(csvfile, delimiter = ',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n\ncsvwriter.writerow([\"PassengerId\", \"Survived\"])\nfor x in range(len(predictions)):\n    csvwriter.writerow([test_data[\"PassengerId\"][x], predictions[x]])\n\n    \ncsvfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8d36c8d5643e10fd4f4de4d2e3bfb5407061951","collapsed":true},"cell_type":"code","source":"# Calculate train set error\ntrain_predictions = model.predict(X_train)\ntrain_error = sum(abs(train_predictions - y.values)) / len(y.values)\n\nprint(\"Train set error: \", train_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ea5a81d71849665631af8fe4848d8740e6e6b00","scrolled":true,"collapsed":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = \"Learning Curve for SVM with RBF Kernel\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\nestimator = SVC()\nplot_learning_curve(estimator, title, X_train, y, ylim=(0.7, 1.01), cv=cv)\n\ntitle = \"Learning Curve for LinearSVC\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = LinearSVC()\nplot_learning_curve(estimator, title, X_train, y, ylim=(0.7, 1.01), cv=cv)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}