{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f6bf0c9c-ff7c-7d82-79bc-26a0d98d4263"
      },
      "source": [
        "The author is still studying in Beijing.\n",
        "\n",
        "In the code below, I used cov matrix to determine the most significant contribution factors to 'price_doc', then use them to train a 3-layer fully connected neural network.\n",
        "\n",
        "Trained by Adam optimizer and a batch size of 100 samples, running for 4000 iterations.\n",
        "\n",
        "The test loss for the network is 0.6 using root mean squared log error (which is the standard loss function in evaluating models here). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "682a3318-9adc-1b93-be37-740278cd9120"
      },
      "outputs": [],
      "source": [
        "from pandas import *\n",
        "f = read_csv(r'../input/train.csv')\n",
        "#read the the first few lines of the data\n",
        "print(f.head())\n",
        "#check if there are any duplicates\n",
        "f.duplicated().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc49b18b-8fc6-141b-82b4-e8226fd07bdb"
      },
      "outputs": [],
      "source": [
        "#check how many rows contain NaN\n",
        "print('Rows that contains NaN')\n",
        "print(f.isnull().any(axis=1).value_counts())\n",
        "#check how many columns contain NaN\n",
        "print('Columns that contains Nan')\n",
        "print(f.isnull().any(axis=0).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0723a441-58bf-1f55-bd42-82bfb54e20c6"
      },
      "outputs": [],
      "source": [
        "f['timestamp'] = f['timestamp'].map(to_datetime)#convert to datetime objects\n",
        "f['timestamp'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f4814c8-6a92-47ad-fcfc-e8269c6fba6b"
      },
      "outputs": [],
      "source": [
        "#get a rough view of relevence between timestamp and price_doc\n",
        "from matplotlib.pyplot import *\n",
        "pcm_bytime = f[['timestamp', 'price_doc']].groupby('timestamp').mean()\n",
        "pcm_bytime.plot()\n",
        "show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bf3caa0-5f78-aef4-bcfa-10d245c84e4a"
      },
      "outputs": [],
      "source": [
        "#calculate the covariance matrix\n",
        "pc_cov = f.cov()['price_doc']\n",
        "pc_cov = pc_cov.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6d73c57-a1c1-3151-5af5-e063eb06c618"
      },
      "outputs": [],
      "source": [
        "#sorting the covariance matrix, and take a look\n",
        "pc_cov.sort_values(ascending=False, inplace=True)\n",
        "del pc_cov['price_doc']\n",
        "pc_cov.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95bcb9a6-83c2-90b1-e9e4-1c858e63775d"
      },
      "outputs": [],
      "source": [
        "#we choose first 8 of the covariance matrix \n",
        "index_sel = ['price_doc'] + pc_cov[0:8].index.values.tolist()\n",
        "fnew = f[index_sel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7471b90-3b26-d96c-9fd1-dae0b5bd5c4d"
      },
      "outputs": [],
      "source": [
        "fnew.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2221d1af-5387-8b82-90bd-3f36cc07e80d"
      },
      "outputs": [],
      "source": [
        "fnew['price_doc'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0debcf03-e558-494a-8742-bf28114e62cb"
      },
      "outputs": [],
      "source": [
        "#writing the transformed data to csv file\n",
        "fnew.to_csv(r'./data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2ecfddd9-f9e1-8435-04b3-d7aa820c60c0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "sess = tf.Session()\n",
        "data = pd.read_csv(r'./data.csv')\n",
        "y_vals = data['price_doc']\n",
        "del data['price_doc']\n",
        "del data[data.columns[0]] #discard first column\n",
        "\n",
        "print(data.head())\n",
        "print(y_vals.head())\n",
        "\n",
        "x_vals = data.as_matrix() #convert to numpy ndarray\n",
        "y_vals = y_vals.values #conver to numpy ndarray\n",
        "\n",
        "seed = 3\n",
        "tf.set_random_seed(3)\n",
        "np.random.seed(seed=seed)\n",
        "batch_size = 100\n",
        "\n",
        "#split into train and test sets\n",
        "train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.9), replace=False)\n",
        "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
        "x_vals_train = x_vals[train_indices]\n",
        "y_vals_train = y_vals[train_indices]\n",
        "x_vals_test = x_vals[test_indices]\n",
        "y_vals_test = y_vals[test_indices]\n",
        "\n",
        "def normalize_cols(mat):\n",
        "    col_max = mat.max(axis=0)\n",
        "    col_min = mat.min(axis=0)\n",
        "    return (mat - col_min) / (col_max - col_min)\n",
        "\n",
        "x_vals_train = normalize_cols(x_vals_train)\n",
        "x_vals_test = normalize_cols(x_vals_test)\n",
        "\n",
        "def init_weight_uniform(shape, low, high):\n",
        "    return tf.Variable(tf.random_uniform(shape=shape, minval=low, maxval=high))\n",
        "\n",
        "def init_weight_normal(shape, st_dev):\n",
        "    return tf.Variable(tf.random_normal(shape=shape, stddev=st_dev))\n",
        "\n",
        "def init_bias(shape, st_dev, mean=0.0):\n",
        "    return tf.Variable(tf.random_normal(shape=shape, stddev=st_dev))\n",
        "\n",
        "x_data = tf.placeholder(shape=[None, 8], dtype=tf.float32)\n",
        "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "\n",
        "def fully_connected_layer(input_layer, weights, bias):\n",
        "    layer = tf.add(tf.matmul(input_layer, weights), bias)\n",
        "    return tf.nn.relu(layer)\n",
        "\n",
        "#building network graph\n",
        "#weight_1 = init_weight_uniform(shape=[8, 40], low=-1/np.sqrt(8), high=1/np.sqrt(8))\n",
        "weight_1 = init_weight_normal(shape=[8, 40], st_dev=10.0)\n",
        "bias_1 = init_bias(shape=[40], st_dev=10.0)\n",
        "layer_1 = fully_connected_layer(x_data, weight_1, bias_1)\n",
        "\n",
        "#weight_2 = init_weight_uniform(shape=[40, 15], low=-1/np.sqrt(40), high=1/np.sqrt(40))\n",
        "weight_2 = init_weight_normal(shape=[40, 15], st_dev=10.0)\n",
        "bias_2 = init_bias(shape=[15], st_dev=10.0)\n",
        "layer_2 = fully_connected_layer(layer_1, weight_2, bias_2)\n",
        "\n",
        "#weight_3 = init_weight_uniform(shape=[15, 1], low=-1/np.sqrt(20), high=1/np.sqrt(20))\n",
        "weight_3 = init_weight_normal(shape=[15, 1], st_dev=10.0)\n",
        "bias_3 = init_bias(shape=[15], st_dev=10.0)\n",
        "final_output = fully_connected_layer(layer_2, weight_3, bias_3)\n",
        "\n",
        "#loss function, root mean squared log error\n",
        "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.log(final_output + 1) - tf.log(y_target + 1))))\n",
        "my_opt = tf.train.AdamOptimizer(0.05)\n",
        "train_step = my_opt.minimize(loss)\n",
        "\n",
        "#start training\n",
        "saver = tf.train.Saver()\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)\n",
        "\n",
        "train_loss_vec = []\n",
        "test_loss_vec = []\n",
        "for i in range(4000):\n",
        "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
        "    rand_x = x_vals_train[rand_index]\n",
        "    rand_y = np.transpose([y_vals_train[rand_index]])\n",
        "    sess.run(train_step, feed_dict={x_data:rand_x, y_target:rand_y})\n",
        "    tmp_loss = sess.run(loss, feed_dict={x_data:rand_x, y_target:rand_y})\n",
        "    train_loss_vec.append(tmp_loss)\n",
        "    tmp_loss = sess.run(loss, feed_dict={x_data:x_vals_test, y_target:np.transpose([y_vals_test])})\n",
        "    test_loss_vec.append(tmp_loss)\n",
        "    if i%100 == 0:\n",
        "        '''\n",
        "        #save model\n",
        "        if i%400 == 0:\n",
        "            saver.save(sess, r'../output/model', global_step=i)\n",
        "        '''\n",
        "        print('Generation: ' + str(i) + '. Loss = ' + str(tmp_loss))\n",
        "\n",
        "\n",
        "#plotting model\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(train_loss_vec, 'k--', label='Train Loss')\n",
        "plt.plot(test_loss_vec, 'r--', label='Test Loss')\n",
        "plt.title('Loss per Generation')\n",
        "plt.xlabel('Generation')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}