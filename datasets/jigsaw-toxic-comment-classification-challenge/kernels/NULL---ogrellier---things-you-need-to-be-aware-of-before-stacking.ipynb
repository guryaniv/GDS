{"cells":[{"metadata":{"_cell_guid":"4e0a02d0-ce61-46ef-a4c4-077837d768b1","_uuid":"dc39161c7f6447a42d370656201ac8c1d1bb8aa3"},"cell_type":"markdown","source":"After publishing a [LGBM kernel](https://www.kaggle.com/ogrellier/lgbm-with-words-and-chars-n-gram), \n[@Sergei Fironov](https://www.kaggle.com/sergeifironov]) pointed out substantial differences between AUC scores averaged by fold and full OOF AUC, which is mainly due to the fact AUC is not linear.\n\nSo I decided to publish a kernel showing significant distribution differences between each fold predictions.\n\nI believe we have to tackle this issue before successfully stacking models."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bokeh.io import output_file, show, output_notebook\nfrom bokeh.layouts import column, gridplot\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\noutput_notebook()\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"30a23c25-ab1d-4768-9c1e-624540392f37","_uuid":"b34bb734ae0607a095cb86812cf5b8b5e2bcae0c"},"cell_type":"markdown","source":"Read Out-Of-Fold predictions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"oof_dir = '../input/lgbm-with-words-and-chars-n-gram/'\noof = pd.read_csv(oof_dir +\"lvl0_lgbm_clean_oof.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"bff05315-6a67-4bd7-9d7e-dbb0305fd88d","_uuid":"f038bc440dde45ee7df913192cb2b8e94a73a886","collapsed":true,"trusted":true},"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nclass_preds = [c_ + \"_oof\" for c_ in class_names]\nfolds = KFold(n_splits=4, shuffle=True, random_state=1)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"52075397-2134-4fd4-bf1b-b4b7b10b0314","_uuid":"7668324418162cc6aa0d68b5ff6d8df06ec65728"},"cell_type":"markdown","source":"To show what's happening I often like to display F1 scores against probability thresholds. This shows how different each fold behaves for the same threshold. \n\nWhen folds do not behave properly for the same threshold the overall AUC will usually degrade and optimal weights found on OOF data may not yield good results when applied to test predictions."},{"metadata":{"_cell_guid":"d7f5610b-2815-4591-b13f-6c872d69dff2","_kg_hide-input":true,"_uuid":"44acd2fce015b46b407b651cfd743e94308b8005","trusted":true},"cell_type":"code","source":"figures = []\nfor i_class, class_name in enumerate(class_names):\n    # create a new plot for current class\n    # Compute full score :\n    full = roc_auc_score(oof[class_names[i_class]], oof[class_preds[i_class]])\n    # Compute average score\n    avg = 0.0\n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        avg += roc_auc_score(oof[class_names[i_class]].iloc[val_idx], oof[class_preds[i_class]].iloc[val_idx]) / folds.n_splits\n    \n    s = figure(plot_width=750, plot_height=300, \n               title=\"F1 score vs threshold for %s full oof %.6f / avg fold %.6f\" % (class_name, full, avg))\n    \n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        # Get False positives, true positives and the list of thresholds used to compute them\n        fpr, tpr, thresholds = roc_curve(oof[class_names[i_class]].iloc[val_idx], \n                                         oof[class_preds[i_class]].iloc[val_idx])\n        # Compute recall, precision and f1_score\n        recall = tpr\n        precision = tpr / (fpr + tpr + 1e-5)\n        f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n        # Finally plot the f1_scores against thresholds\n        s.line(thresholds, f1_scores, name=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][4][n_fold])\n    figures.append(s)\n\n# put the results in a column and show\nshow(column(figures))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"b7ebee6f-4e7a-4291-8b36-24e80669c9bd","_uuid":"f632ebd7cfa4b97f7ede6a8755e5a94357ddc440"},"cell_type":"markdown","source":"We can now clearly see problems on severe_toxic, threat and identity_hate.\n\nAnother way to look at this uses the AUC curve directly."},{"metadata":{"_cell_guid":"94b5f81f-68e7-4a3a-bafa-19b49b99ad72","_kg_hide-input":true,"_uuid":"6755d2c0a71d5f2fba865ae9dca3bec120d18e39","trusted":true},"cell_type":"code","source":"figures = []\nfor i_class, class_name in enumerate(class_names):\n    # create a new plot for current class\n    # Compute full score :\n    full = roc_auc_score(oof[class_names[i_class]], oof[class_preds[i_class]])\n    # Compute average score\n    avg = 0.0\n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        avg += roc_auc_score(oof[class_names[i_class]].iloc[val_idx], oof[class_preds[i_class]].iloc[val_idx]) / folds.n_splits\n    \n    s = figure(plot_width=400, plot_height=400, \n               title=\"%s ROC curves OOF %.6f / Mean %.6f\" % (class_name, full, avg))\n    \n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        # Get False positives, true positives and the list of thresholds used to compute them\n        fpr, tpr, thresholds = roc_curve(oof[class_names[i_class]].iloc[val_idx], \n                                         oof[class_preds[i_class]].iloc[val_idx])\n        s.line(fpr, tpr, name=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][4][n_fold])\n        s.line([0, 1], [0, 1], color='navy', line_width=1, line_dash=\"dashed\")\n\n    figures.append(s)\n\n# put the results in a column and show\nshow(gridplot(np.array_split(figures, 3)))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"532fae89-f7ba-4f80-bbba-1b4d8d2fb85e","_uuid":"1874b5b9167d51b1ce544945d2f285c5ba909175"},"cell_type":"markdown","source":"ROC curves are even clearer on that matter as we clearly see curves have the same shape for toxic, obscene and insult while there are significant differences for the last 3 classes.\n\nThe problem now is we don't know if there are even further differences between OOF probabilities and Test predictions. If this were the case this would undermine any stacking attempt. \n\nAs a matter of fact we can't use ROC curves or F1 scores since Kaggle teams do not let us access test ground truth, and this really is a shame ;-)  \n\nCan we see anything interesting in the probability distributions themselves?"},{"metadata":{"_cell_guid":"9f990899-7f99-4a42-890e-e3089b20272f","_kg_hide-input":true,"_uuid":"5c8d7b4ab826df632f7964b2d6a51c1e09bb0ac3","trusted":true},"cell_type":"code","source":"# Read submission data \nsub = pd.read_csv(oof_dir +\"lvl0_lgbm_clean_sub.csv\")\nfigures = []\nfor i_class, class_name in enumerate(class_names):\n    s = figure(plot_width=600, plot_height=300, \n               title=\"Probability logits for %s\" % class_name)\n\n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        probas = oof[class_preds[i_class]].values[val_idx]\n        p_log = np.log((probas + 1e-5) / (1 - probas + 1e-5))\n        hist, edges = np.histogram(p_log, density=True, bins=50)\n        s.line(edges[:50], hist, legend=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][4][n_fold])\n    \n    oof_probas = oof[class_preds[i_class]].values\n    oof_logit = np.log((oof_probas + 1e-5) / (1 - oof_probas + 1e-5))\n    hist, edges = np.histogram(oof_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend=\"Full OOF\", color=brewer[\"Paired\"][6][1], line_width=3)\n    \n    sub_probas = sub[class_name].values\n    sub_logit = np.log((sub_probas + 1e-5) / (1 - sub_probas + 1e-5))\n    hist, edges = np.histogram(sub_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend=\"Test\", color=brewer[\"Paired\"][6][5], line_width=3)\n    figures.append(s)\n\n# put the results in a column and show\nshow(column(figures))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"9e183f05-2ee7-431f-9f4d-8e102945d5a9","_uuid":"139740102a1f956776c121f32b3e6997323f7d12"},"cell_type":"markdown","source":"I can hear you shout :  he should have started here... Agreed !\n\nThings are cristal clear now and we have a way to make sure what we do in OOF will translate to test probabilities... or not ! \n\n\nAs a conclusion I would say that using these OOF outputs for stacking may not be the best idea, especially for severe_toxic and threat.\n\nProbabilities need to be aligned before any stacking and Im' planning on using a simple LogisticRegression for this purpose.\n\nMore on this later...\n"},{"metadata":{"_cell_guid":"6db92c0f-c403-489e-9d82-7dd87d4264d8","_uuid":"820201bfc8fa96406f4ab1cdfdfd793253567dcc","collapsed":true},"cell_type":"markdown","source":"UPDATE: \n\nI'm currently running a fork of LightGBM kernel trying to align probabilities.  LogisticRegression gives even worse results but I may be using wrong parameters. I decided to try pd.Series().rank(), which is appropriate for AUC metric. Things are looking better at least on the OOF side but I still need to check the submission predictions. Once the kernel looks right I'll use the output to add a few graphs here."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e5eeefea7236130ebdd75002cd8a6ef1310ce552"},"cell_type":"markdown","source":"In fact I don't need to wait for the kernell to complete since I can simply use the rank() method directly on the OOF data. So let's have a try!"},{"metadata":{"trusted":true,"_uuid":"a12da269a32e806251149f6f1c6020aff45b9ea7"},"cell_type":"code","source":"figures = []\nfor i_class, class_name in enumerate(class_names):\n    s = figure(plot_width=600, plot_height=300, \n               title=\"Probability logits for %s using rank()\" % class_name)\n\n    for n_fold, (_, val_idx) in enumerate(folds.split(oof)):\n        probas = (1 + oof[class_preds[i_class]].iloc[val_idx].rank().values) / (len(val_idx) + 1)\n        p_log = np.log((probas + 1e-5) / (1 - probas + 1e-5))\n        hist, edges = np.histogram(p_log, density=True, bins=50)\n        s.line(edges[:50], hist, legend=\"Fold %d\" % n_fold, color=brewer[\"Set1\"][4][n_fold])\n    \n    oof_probas = (1 + oof[class_preds[i_class]].rank().values) / (oof.shape[0] + 1)\n    oof_logit = np.log((oof_probas + 1e-5) / (1 - oof_probas + 1e-5))\n    hist, edges = np.histogram(oof_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend=\"Full OOF\", color=brewer[\"Paired\"][6][1], line_width=3)\n    \n    sub_probas = (1 + sub[class_name].rank().values) / (sub.shape[0] + 1)\n    sub_logit = np.log((sub_probas + 1e-5) / (1 - sub_probas + 1e-5))\n    hist, edges = np.histogram(sub_logit, density=True, bins=50)\n    s.line(edges[:50], hist, legend=\"Test\", color=brewer[\"Paired\"][6][5], line_width=3)\n    figures.append(s)\n\n# put the results in a column and show\nshow(column(figures))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"2922bfe4cfdeaff501212491dda65adb9ca8c33f"},"cell_type":"markdown","source":"Ok I think the figures speak for themselves. However I ould still urge you to check your OOF and test predictions since I have a few models that have weird behaviors on the far left or right side of the graph even with rank() !\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc3dcb800f48820f167480559d07f2a9bb8fae01"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}