{"cells":[{"metadata":{"_cell_guid":"1d2c2064-dfeb-4c10-9b37-897201ed3b82","_uuid":"1a796743fea4d95814a5bd5e0c997ddb71027830"},"cell_type":"markdown","source":"*March 2018*\n\n**Introduction**\n\nPreviously I built a simple fully-connected neural network without any convolution [here.](https://www.kaggle.com/mauddib/digit-recogniser-with-r-and-h2o)\nI played around with the H2O framework a bit, but it was a bit slow. So much so that I could not run a tuning grid(Kaggle limits you to 21000 seconds on R notebook) to test a range of values for my hyper parameters, which is what machine learning is all about!\n\nI ended up testing a few activation functions and adding a layer of extra nodes in an attempt to do some 'manual' empirical testing. Bit of a schlepp. At any rate I still managed to achieve a 0.97785 under those circumstances, not too shabby at all!\n\nBut now we get to test drive the more contemporary approach to machine learning, by building a Convolutional Neural Network or CNN.\n\nTensorflow allows you to build a flow for your model, which is called an architecture.\n\n**Here is a typical CNN architecture:**\n\n![CNN](https://cdn-images-1.medium.com/max/1600/1*uUYc126RU4mnTWwckEbctw@2x.png)\n\n\nThe convolution layers are the main powerhouse of a CNN model. Automatically detecting meaningful features given only an image and a label is not an easy task. The convolution layers learn such complex features by building on top of each other. The first layers detect edges, the next layers combine them to detect shapes, to following layers merge this information to infer that this is a car for example, or the number 9.\n\nWithout further ado, let us begin by loading the necessary libraries and also loading the training and test data:\n(The labels were given as integers between 0 and 9. We will convert these to one-hot encoding labels, ie, for the label 5, you will have an array that looks like this : [0,0,0,0,0,1,0,0,0,0])"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plot library\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n#! /usr/bin/env python\n# -*- coding: utf-8 -*- \nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom keras.utils.np_utils import to_categorical   \n\n\ntrain_data0 = pd.read_csv(\"../input/train.csv\")\ntrain_data = np.array(train_data0.iloc[:, 1:785])\ntrain_label = np.array(train_data0.iloc[:,0])\n\n\n# One-hot encoding\ntrain_label = to_categorical(train_label, num_classes=10)\n\ntest_data0 = pd.read_csv(\"../input/test.csv\")\ntest_data = np.array(test_data0)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"181640ef-84ff-459c-9a35-e2fe073b9885","_uuid":"8589c8891b05b63c2057a17a43467c195a94b1f2"},"cell_type":"markdown","source":"OK so lets see the first 10 labels and their one hot encoding equivalent:"},{"metadata":{"_cell_guid":"9feb22ce-2724-460b-ab39-68a210a3dfb7","_uuid":"92096017782f5698557ab775b96284065e21a2d1","collapsed":true,"trusted":false},"cell_type":"code","source":"print(np.array(train_data0.iloc[:,0])[:10])\nprint('is equivalent to:')\nprint(train_label[:10])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb9ec575-5254-4a06-99c3-868a33be9b61","_uuid":"cb5414b959f232da7a9f2881e341824d180bc500"},"cell_type":"markdown","source":"Now let us print some examples of digits from the training data to get a better feel for the data we will be training on:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"\npixels = train_data[9].reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()\n\npixels = train_data[7].reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()\n\npixels = train_data[6].reshape((28, 28))\nplt.imshow(pixels, cmap='gray')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2c29df9-e37d-4815-ae3d-4a895a1dad8b","_uuid":"f89189d4d0f8c06adb85da79e7be290ad6a8e09d"},"cell_type":"markdown","source":"At this point we will need to think about some seeds for our hyper parameters as well as start defining the convolution and pooling functions.\n\nOne of our hyperparameters is the number of epochs. Remember an epoch is, in Machine Learning, the processing by the learning algorithm of the entire training set once. \n\nBecause we only have an hour on the kernel at Kaggle, so we are limited as to how many epochs we can  run. Normally you would run a few hundred, so if you are keen, fork this notebook and run it with epochs set to a few hundred overnight.\n\nLet's see how it performs on 10 Epochs for now(play around with it and see what happens:"},{"metadata":{"_cell_guid":"d3aeee46-daa6-4a3a-b414-4fac36f00912","_uuid":"e770b380afcd8eb7542061c3bea24dc6b6f62a36","collapsed":true,"trusted":false},"cell_type":"code","source":"# Define hyperparameters\nlearning_rate = 0.0001\nepoch = 7\nbatch_size = 20\n\n# Define network parameters\nn_input = 784\nn_classes = 10\n\n# Placeholder\nX = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n# Convolution\ndef conv2d(name, x, W, b, strides=1):\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    #return tf.nn.relu(x, name=name)\n    return tf.nn.elu(x, name=name)\n\n# Pooling\ndef maxpool2d(name, x, k=2):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n\nweights = {\n    'W1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)),\n    'W2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1)),\n    'W4': tf.Variable(tf.truncated_normal([64 * 7 * 7, 784], stddev=0.1)),\n    'Wo': tf.Variable(tf.truncated_normal([784, n_classes], stddev=0.1))\n}\n\nbiases = {\n    'b1': tf.Variable(tf.random_normal([32], stddev=0.1)),\n    'b2': tf.Variable(tf.random_normal([64], stddev=0.1)),\n    'b4': tf.Variable(tf.random_normal([784], stddev=0.1)),\n    'bo': tf.Variable(tf.random_normal([n_classes], stddev=0.1))\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8828463f-190a-4f8a-bd90-6a9bdf648e5e","_uuid":"f2e9db702b7a6414504d3cc61b38c51c4801956f"},"cell_type":"markdown","source":"Next we need to define our model architecture. I am opting for one that looks as follows:\n\nInput -> Conv+Maxpool -> Conv+Maxpool -> FC -> FC -> Output\n\nSo now we are ready to go through the architecture as described above, and then train the model and save the output.\n\nWe will apply standard initialization on the data.  The original data has pixel values between 0 and 255, but data between 0 and 1 should make the net converge faster.\n\nRegarding cost optimization, we are going to use tf.nn.softmax_cross_entropy_with_logits, which computes the cross entropy of the result after applying the softmax function.\n\nAs for the optimizer, I have opted for AdamOptimizer which is faster than Stochastic Gradient Descent (SGD).\n"},{"metadata":{"_cell_guid":"a34c6bf7-d245-4b80-8be4-178d62aff81f","_uuid":"81dc2521dc2c8fa686cc2fe68066c50065f8be1c","collapsed":true,"trusted":false},"cell_type":"code","source":"def model(X, weights, biases):\n    # Conv1\n    x = tf.reshape(X, [-1, 28, 28, 1])\n    \n    # confine range to between 0 and 1\n    x = x/255.\n    \n    #conv1 = tf.nn.relu(conv2d('conv1', x, weights['W1'], biases['b1']))\n    conv1 = tf.nn.elu(conv2d('conv1', x, weights['W1'], biases['b1']))\n    # Pool1\n    pool1 = maxpool2d('pool1', conv1, k=2)\n\n    # Conv2\n    #conv2 = tf.nn.relu(conv2d('conv2', pool1, weights['W2'], biases['b2']))\n    conv2 = tf.nn.relu(conv2d('conv2', pool1, weights['W2'], biases['b2']))\n\n    # Pool2\n    pool2 = maxpool2d('pool2', conv2, k=2)\n    \n    # Full connect layer\n    fc = tf.reshape(pool2, [-1, weights['W4'].get_shape().as_list()[0]])\n    fc = tf.add(tf.matmul(fc, weights['W4']), biases['b4'])\n    #fc = tf.nn.relu(fc)\n    fc = tf.nn.elu(fc)\n\n    # output\n    a = tf.add(tf.matmul(fc, weights['Wo']), biases['bo'])\n\n    return a\n\n\n# prediction\npred = model(X, weights, biases)\n\n# cost\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\nlabel = tf.argmax(pred, 1)\n# evaluation\ncorrect_pred = tf.equal(label, tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit = tf.global_variables_initializer()\nconfig = tf.ConfigProto()  \nconfig.gpu_options.allow_growth=True  \nwith tf.Session(config=config) as sess:\n    sess.run(init)  \n    for e in range(epoch):\n        step = 1\n        while step*batch_size <= train_data.shape[0]:\n            xs, ys = train_data[(step-1)*batch_size:step*batch_size, :], train_label[(step-1)*batch_size:step*batch_size, :]\n            sess.run(optimizer, feed_dict={X:xs, y:ys})\n\n            if step % 100 == 0:\n                loss, acc = sess.run([cost, accuracy], feed_dict={X:xs, y:ys})\n\n                print(\"Iter {0}, Minibatch Loss = {1}, Training accuracy = {2}\".format(str(step),\n                                                                                    loss, acc))\n            step += 1\n    print(\"Optimization Completed\")\n    test_labels = []\n    for i in range(1000):\n        xs, ys = test_data[i*28:(i+1)*28, :], test_data[i*28:(i+1)*28, 0:10]\n        pred_ = sess.run(label, feed_dict={X:xs, y:ys})\n        test_labels.extend(list(pred_))\n\nf1 = open('label', 'wb')\npickle.dump(test_labels, f1)\nf1.close()\n\n\n# now we save the predictions to a csv ready for submission\ndf = pd.DataFrame({'Label': test_labels})\n# Add 'ImageId' column\ndf1 = pd.concat([pd.Series(range(1,28001), name='ImageId'), \n                              df[['Label']]], axis=1)\n\n\ndf1.to_csv('ConvPool_X2.csv', index=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6900378-1b7d-4c25-8ff5-05b42062068e","_uuid":"30f60a225525c42cf078799773356eb338d05a54"},"cell_type":"markdown","source":"**Outcome and Conclusion**\n\nYour Best Entry \nYour submission scored 0.98385, which is an improvement of your previous score of 0.97785. Great job! \n\nOK so with a 2 layer Conv+Maxpool CNN and only a few epochs we have managed to surpass our H2O model results!\n\nI hope you had fun building a CNN architecture with Tensorflow. Try adding another convolution layer or two to your architecture, and let me know what you get!\n\nGood luck!\n"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}