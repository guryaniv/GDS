{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\ntrain.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eea21c99e43d1dd399b89a89ba4aefd4f2a552b"},"cell_type":"code","source":"sub = pd.read_csv('../input/test/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"612000ae8312b3f78698a41add1787c528617132"},"cell_type":"code","source":"y = train.AdoptionSpeed.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"243e779fefd97b1370a55fe5d973837c3187da2b"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nvectorizer = CountVectorizer() #min_df=0.01)\n#vectorizer =TfidfVectorizer(min_df=0.001)\nX = vectorizer.fit_transform(train.Description.append(test.Description).fillna(' '))\n#X=pd.DataFrame(X.todense(),columns=vectorizer.get_feature_names())\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\nu=svd.fit_transform(X)  \nvt=svd.fit_transform(X.T)  \nXi=np.linalg.pinv(X*vt)\n#X=np.dot(X,vt)\nX.shape,vt.shape,Xi.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18d3603c5776d484b9e7244341ca925baab1ac10"},"cell_type":"code","source":"def cohen_effect_size(X, y):\n    \"\"\"Calculates the Cohen effect size of each feature.\n    \n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n        Returns\n        -------\n        cohen_effect_size : array, shape = [n_features,]\n            The set of Cohen effect values.\n        Notes\n        -----\n        Based on https://github.com/AllenDowney/CompStats/blob/master/effect_size.ipynb\n    \"\"\"\n    print(X.shape,y.shape,y.mean())\n    medi=y.mean()\n    group1, group2 = X[y<medi], X[y>=medi]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff / np.sqrt(pooled_var)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"121cceaed48318ae9ae28425cb24fb310b552f78"},"cell_type":"code","source":"excluded_feats = ['Name', 'RescuerID', 'Description', 'PetID','AdoptionSpeed'] #['SK_ID_CURR']\nfeatures = [f_ for f_ in train.columns if f_ not in excluded_feats]\n\nprint('Number of features %d  ' % (len(features)),train.shape,train.AdoptionSpeed.shape) \n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(train[features],train.AdoptionSpeed.values)\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7b532e80cee2e41ffec7c7bec2655999d1f1cdc"},"cell_type":"code","source":"excluded_feats = ['Name', 'RescuerID', 'Description', 'PetID','AdoptionSpeed'] #['SK_ID_CURR']\nfeatures = [f_ for f_ in train.columns if f_ not in excluded_feats]\nfeatures = [f_ for f_ in pd.DataFrame(u).columns if f_ not in excluded_feats]\nprint('Number of features %d  ' % (len(features)),u[:len(train)].shape,train.AdoptionSpeed.shape) \n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(pd.DataFrame(u[:len(train)]),train.AdoptionSpeed.values)\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1c07536e9ea4ad22ba3486dc4f9c53aaf5fa53"},"cell_type":"code","source":"excluded_feats = ['Name', 'RescuerID', 'Description', 'PetID','AdoptionSpeed'] #['SK_ID_CURR']\nfeatures = [f_ for f_ in train.columns if f_ not in excluded_feats]\nfeatures = [f_ for f_ in pd.DataFrame(Xi.T).columns if f_ not in excluded_feats]\nprint('Number of features %d  ' % (len(features)),u[:len(train)].shape,train.AdoptionSpeed.shape) \n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(pd.DataFrame(Xi.T[:len(train)]),train.AdoptionSpeed.values)\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98b89732b8f41dbf5cd4cda5c865c5d55b3bfad7"},"cell_type":"code","source":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,ExtraTreesClassifier,GradientBoostingRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,LogisticRegression, RidgeClassifier,SGDClassifier,ElasticNetCV, LassoLarsCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier,MLPRegressor,BernoulliRBM\nfrom sklearn.svm import SVC,LinearSVC,SVR\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.utils import check_array\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed\n    \nClassifiers = [\n               Perceptron(n_jobs=-1),\n               RidgeClassifier(tol=1e-2, solver=\"lsqr\"),\n               #SVR(kernel='rbf',C=1.0, epsilon=0.2),\n               CalibratedClassifierCV(LinearDiscriminantAnalysis(), cv=4, method='sigmoid'),    \n               #OneVsRestClassifier( SVC(    C=50,kernel='rbf',gamma=1.4, coef0=1,cache_size=3000,)),\n               KNeighborsClassifier(10),\n               DecisionTreeClassifier(),\n               #RandomForestClassifier(n_estimators=200),\n               ExtraTreesClassifier(n_estimators=250,random_state=0), \n               OneVsRestClassifier(ExtraTreesClassifier(n_estimators=10)) , \n               MLPClassifier(alpha=0.510,activation='logistic'),\n               LinearDiscriminantAnalysis(),\n               OneVsRestClassifier(GaussianNB()),\n               AdaBoostClassifier(),\n               GaussianNB(),\n               QuadraticDiscriminantAnalysis(),\n               SGDClassifier(average=True,max_iter=100),\n               LogisticRegression(C=1.0,multi_class='multinomial',penalty='l2', solver='saga',n_jobs=-1),\n               XGBClassifier(max_depth=5, base_score=0.005),\n               #LabelPropagation(n_jobs=-1),\n               #LinearSVC(),\n               #MultinomialNB(alpha=.01),    \n\n              ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7645111b9ae54ed9099c7369a21ca6ba9707f8db"},"cell_type":"code","source":"def klasseer(e_,mtrain,label,mtest,veld,idvld,thres,probtrigger):\n    # e_ total matrix without veld, \n    # veld the training field\n    #thres  threshold to select features\n    velden=[v for v in e_.columns if v not in [veld,idvld]]\n    #label = mtrain[veld]\n    #print(e_.shape,velden)\n    e_=e_.loc[:,velden]\n    print(e_.shape)\n    # select features find most relevant ifo threshold\n    #clf = ExtraTreesClassifier(n_estimators=100)\n    ncomp=e_.shape[1]-18\n    #model = SelectFromModel(clf, prefit=True,threshold =(thres)/100)\n       # SVD\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.preprocessing import Normalizer\n    svd = TruncatedSVD(n_components=ncomp, n_iter=7, random_state=42)\n    e_=svd.fit_transform(Normalizer().fit_transform(e_))\n    print('SVD expl var',svd.explained_variance_[:10])\n       #tsne not used\n    from sklearn.manifold import TSNE\n    e_=TSNE(n_components=3,perplexity=5).fit_transform(e_)\n    #from sklearn.metrics.pairwise import cosine_similarity\n    \n       #robustSVD not used\n    #A_,e1_,e_,s_=robustSVD(e_,140)\n    #clf = clf.fit( e_[:len(mtrain)], label)\n    #New_features = model.transform( e_[:len(mtrain)])\n    #Test_features= model.transform(e_[-len(mtest):])\n    New_features=e_[:len(mtrain)]\n    Test_features=e_[len(mtrain):]\n    pd.DataFrame(New_features).plot.scatter(x=0,y=1,c=label,colormap='Spectral')\n    pd.DataFrame(np.concatenate((New_features,Test_features))).plot.scatter(x=0,y=1,c=['r' for x in range(len(mtrain))]+['g' for x in range(len(mtest))])    \n\n    print('Model with threshold',thres/100,New_features.shape,Test_features.shape,e_.shape)\n    print('____________________________________________________')\n    \n    Model = []\n    Accuracy = []\n    for clf in Classifiers:\n        #train\n        fit=clf.fit(New_features,label)\n        pred=fit.predict(New_features)\n        Model.append(clf.__class__.__name__)\n        Accuracy.append(accuracy_score(label,pred))\n        #predict\n        sub = pd.read_csv('../input/test/sample_submission.csv')\n        #sub = pd.DataFrame({'PetID': mtest[idvld],veld: fit.predict(Test_features)})\n        sub[veld]=fit.predict(Test_features)\n        #sub.plot(x=idvld,kind='kde',title=clf.__class__.__name__ +str(( mtrain[veld]==pred).mean()) +'prcnt') \n        sub2=pd.DataFrame(pred,columns=[veld])\n        #estimate sample if  accuracy\n        if veld in mtest.columns:\n            print( clf.__class__.__name__ +str(round( accuracy_score(label,pred),2)*100 )+'prcnt accuracy versus unknown',(sub[veld]==mtest[veld]).mean() )\n        else:\n            print(clf.__class__.__name__ +str(round( accuracy_score(label,pred),2)*100 ))\n        #write results\n        klassnaam=clf.__class__.__name__+\".csv\"\n        #sub.to_csv(klassnaam, index=False)\n        sub.to_csv('submission.csv', index=False)\n        if probtrigger:\n            pred_prob=fit.predict_proba(Test_features)\n            sub=pd.DataFrame(pred_prob)\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7f2d4cc32b26f50da67b9267fc6276e117ebef4"},"cell_type":"code","source":"excluded_feats = ['Name', 'RescuerID', 'Description', 'PetID','AdoptionSpeed'] #['SK_ID_CURR']\nfeatures = [f_ for f_ in train.columns if f_ not in excluded_feats]\nfeatures0=features+['PetID','AdoptionSpeed']\nfeatures1=features+['PetID']\nprint(features0)\ntotaal=(train.append(test)).fillna(0)\ncotrain=pd.DataFrame( np.hstack((u[:len(train)],Xi.T[:len(train)],train[features1])) )\ncotest=pd.DataFrame( np.hstack((u[len(train):],Xi.T[len(train):],test[features1])) )\ntotaal=pd.DataFrame( np.hstack((u,Xi.T,train[features].append(test[features]).fillna(0) )))\nsubx=klasseer(totaal,cotrain,train.AdoptionSpeed.values,cotest,'AdoptionSpeed',36,3,False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e491dc3ab20126f87c85fffd036732a70e6d57"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f01cfee25b40d08a1bebb306aa280c8c0d975b"},"cell_type":"code","source":"train = train.drop(['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'], axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685e85061928904eef513830391601fcaa7fbc51"},"cell_type":"code","source":"test = test.drop(['Name', 'RescuerID', 'Description', 'PetID'], axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c63c99a4f66179cf71ac0ca1c484881796f2bd5f"},"cell_type":"code","source":"clf = xgb.XGBClassifier(n_estimators=100, nthread=-1, max_depth=8, learning_rate=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"321baf3c8dfaf1e2e30dcd03b2fc16e62443307d"},"cell_type":"code","source":"clf.fit(np.hstack((u[:len(train)],Xi[:len(train)],train)), y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13e0e8cde2f271c6783b87b0ff44b63a284169c6"},"cell_type":"code","source":"preds = clf.predict(np.hstack((u[len(train):],Xi[len(train):],test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5525cd4810ed1c6ac8a380234ea82f0245403d85"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4604b0219c467ee0bfe18a4491463fba7e04779e"},"cell_type":"code","source":"#sample = pd.read_csv('../input/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da9b6b80b21ddeabfcef556a6cb65f38ae675b2b"},"cell_type":"code","source":"#sample.AdoptionSpeed = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfc98b6c249a187cfe25cdb4448382325166f3f4"},"cell_type":"code","source":"#sample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87392765ce9a0b7dc1426323bf7be8d8aad230c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}