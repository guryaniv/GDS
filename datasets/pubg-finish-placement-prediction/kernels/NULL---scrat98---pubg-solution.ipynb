{"cells":[{"metadata":{"_uuid":"ff91f37d0ce76406c866d551dfb6a1d6e1e74bdc"},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport itertools\n\ntrain_file = \"../input/train_V2.csv\"\ntest_file = \"../input/test_V2.csv\"\nsample_submission_file = \"./data/sample_submission_V2.csv\"\n\n# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndef read_data_with_reduce(file_name):\n    print('Reading ' + file_name)\n    data = pd.read_csv(file_name)\n    print('Reducing ' + file_name)\n    data = reduce_mem_usage(data)\n    return data\n\ntrain = read_data_with_reduce(train_file)\ntest = read_data_with_reduce(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"97c234e908cf9a102cebeace1a1e30a9e046fbd4"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e61075ccf939e40252239d16c49f30672179e91b"},"cell_type":"code","source":"null_cnt = train.isnull().sum().sort_values()\nprint(null_cnt[null_cnt > 0])\ndisplay(train[train.isnull().any(1)])\ndisplay(test[test.isnull().any(1)])\n\ntrain.dropna(inplace=True)\n# train.drop(2744604, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.describe(include=np.number).drop('count').T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b532cf130446fb904c9e0f71cfe9e30e40fb68"},"cell_type":"markdown","source":"# Initial data processing"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c8bd84293113c4fbcca08e615f80f105d26c5e3d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\nimport sklearn.metrics\nfrom sklearn.model_selection import GridSearchCV\n\ndef initial_data_processing(df):\n    # remove complexity fields without data analysis\n    df.drop(columns=['killPoints','rankPoints','winPoints','matchType','maxPlace','Id'],inplace=True)\n    X = df.groupby(['matchId','groupId']).agg(np.mean)\n    y = X['winPlacePerc']\n    X.drop(columns=['winPlacePerc'],inplace=True)\n    X_ranked = X.groupby('matchId').rank(pct=True)\n    X = X.reset_index()[['matchId','groupId']].merge(X_ranked, how='left', on=['matchId', 'groupId'])\n    X.drop(['matchId','groupId'],axis=1, inplace=True)\n    return X, y\n\nX_train, y = initial_data_processing(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0a9d3c5703f14bf9fe0a9275742b0519f253aed"},"cell_type":"code","source":"X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2)\nlgtrain = lgb.Dataset(X_train, label=y_train.reset_index(drop=True))\nres = lgb.cv({'metric': 'mae'}, lgtrain, nfold=5)\nprint(\"Mean score:\",res['l1-mean'][-1])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a35f70a442ef40ddfd570e39976bea696e3b45e"},"cell_type":"markdown","source":"# Add features fields"},{"metadata":{"trusted":true,"_uuid":"4b775a0d39f852012e252e76e0c5e6ee848a7d28"},"cell_type":"code","source":"# TODO","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f07b738e3f4e71862f89dd2c1742a88281def117"},"cell_type":"markdown","source":"# Create model"},{"metadata":{"trusted":true,"_uuid":"67c600652ca0b5b06466dcdac5b381a1f2249b6b"},"cell_type":"code","source":"gridParams = {\n    'num_leaves': [30,50,100],\n    'max_depth': [-1,8,15], \n    'min_data_in_leaf': [100,300,500],\n    'max_bin': [250,500], \n    'lambda_l1': [0.01],\n    'num_iterations': [5], \n    'nthread': [4],\n    'learning_rate': [0.05],\n    'metric': ['mae'],\n    \"bagging_fraction\" : [0.7],\n    \"bagging_seed\" : [0],\n    \"colsample_bytree\" : [0.7]\n    }\nmodel = lgb.LGBMRegressor()\ngrid = GridSearchCV(model,\n                    gridParams,\n                    verbose=1,\n                    cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d021176ee41091835c97d7bbf140fbf4530e8fa"},"cell_type":"code","source":"grid.fit(X_train.iloc[:500000,:], y_train.iloc[:500000])\nprint(\"Best params:\", grid.best_params_, '\\n')\nprint(\"Best score:\", grid.best_score_)\nparams = grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6a95d489b1a2d69f15fcd27613bb58476b2a765"},"cell_type":"markdown","source":"# Write result"},{"metadata":{"trusted":true,"_uuid":"aef9a558f7a97dbead78441583e23aeb4f59b176"},"cell_type":"code","source":"lgtrain = lgb.Dataset(X_train, label=y_train)\nlgval = lgb.Dataset(X_holdout, label=y_holdout)\nmodel = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc4fc60ccb465610049097fee7ece195ee738751"},"cell_type":"code","source":"pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n\n# ids_after['winPlacePerc'] = pred_test\n# predict = ids_init.merge(ids_after, how='left', on=['groupId',\"matchId\"])['winPlacePerc']\ndf_sub = pd.read_csv(sample_submission_file)\ndf_sub['winPlacePerc'] = pred_test\ndf_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}