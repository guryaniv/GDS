{"cells": [{"metadata": {"_cell_guid": "292ca314-214c-4835-9eb1-d458ad7ecd6d", "_uuid": "1a2c325e1f72e13096b46570cfa1ee19146490b2"}, "source": ["This is about reducing the number of feature values (binning) rather than number of features. Best candidates are continuous features with lots of different values.\n", "\n", "Although the required modules are missing on Kaggle, I did it inelegantly by copying the files from **[here](https://github.com/navicto/Discretization-MDLPC)**. All the credit goes to Victor Ruiz (see below) who is the original author."], "cell_type": "markdown"}, {"source": ["from __future__ import print_function\n", "import numpy as np\n", "import pandas as pd\n", "from datetime import datetime\n", "from sklearn.cross_validation import StratifiedShuffleSplit\n", "import xgboost as xgb\n", "\n", "def timer(start_time=None):\n", "    if not start_time:\n", "        start_time = datetime.now()\n", "        return start_time\n", "    elif start_time:\n", "        thour, temp_sec = divmod(\n", "            (datetime.now() - start_time).total_seconds(), 3600)\n", "        tmin, tsec = divmod(temp_sec, 60)\n", "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n", "              (thour, tmin, round(tsec, 2)))\n"], "execution_count": null, "metadata": {"_cell_guid": "70c49c00-bc02-494a-9d5b-23dee9209ee7", "collapsed": true, "_uuid": "43ae189fe405643f330c3b34c3c029f062c25197"}, "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "5a8934eb-b927-4b77-8519-5a6efd018d7e", "_uuid": "c20dfbbfa7cf0cf5cbea8066d47161d91e32ba20"}, "source": ["This part is identical to the two files linked above."], "cell_type": "markdown"}, {"source": ["######\n", "# __author__ = 'Victor Ruiz, vmr11@pitt.edu'\n", "######\n", "from math import log\n", "import random\n", "\n", "\n", "def entropy(data_classes, base=2):\n", "    '''\n", "    Computes the entropy of a set of labels (class instantiations)\n", "    :param base: logarithm base for computation\n", "    :param data_classes: Series with labels of examples in a dataset\n", "    :return: value of entropy\n", "    '''\n", "    if not isinstance(data_classes, pd.core.series.Series):\n", "        raise AttributeError('input array should be a pandas series')\n", "    classes = data_classes.unique()\n", "    N = len(data_classes)\n", "    ent = 0  # initialize entropy\n", "\n", "    # iterate over classes\n", "    for c in classes:\n", "        partition = data_classes[data_classes == c]  # data with class = c\n", "        proportion = len(partition) / N\n", "        #update entropy\n", "        ent -= proportion * log(proportion, base)\n", "\n", "    return ent\n", "\n", "def cut_point_information_gain(dataset, cut_point, feature_label, class_label):\n", "    '''\n", "    Return de information gain obtained by splitting a numeric attribute in two according to cut_point\n", "    :param dataset: pandas dataframe with a column for attribute values and a column for class\n", "    :param cut_point: threshold at which to partition the numeric attribute\n", "    :param feature_label: column label of the numeric attribute values in data\n", "    :param class_label: column label of the array of instance classes\n", "    :return: information gain of partition obtained by threshold cut_point\n", "    '''\n", "    if not isinstance(dataset, pd.core.frame.DataFrame):\n", "        raise AttributeError('input dataset should be a pandas data frame')\n", "\n", "    entropy_full = entropy(dataset[class_label])  # compute entropy of full dataset (w/o split)\n", "\n", "    #split data at cut_point\n", "    data_left = dataset[dataset[feature_label] <= cut_point]\n", "    data_right = dataset[dataset[feature_label] > cut_point]\n", "    (N, N_left, N_right) = (len(dataset), len(data_left), len(data_right))\n", "\n", "    gain = entropy_full - (N_left / N) * entropy(data_left[class_label]) - \\\n", "        (N_right / N) * entropy(data_right[class_label])\n", "\n", "    return gain\n", "\n", "######\n", "# __author__ = 'Victor Ruiz, vmr11@pitt.edu'\n", "######\n", "import sys\n", "import getopt\n", "import re\n", "\n", "class MDLP_Discretizer(object):\n", "    def __init__(self, dataset, testset, class_label, out_path_data, out_test_path_data, out_path_bins, features=None):\n", "        '''\n", "        initializes discretizer object:\n", "            saves raw copy of data and creates self._data with only features to discretize and class\n", "            computes initial entropy (before any splitting)\n", "            self._features = features to be discretized\n", "            self._classes = unique classes in raw_data\n", "            self._class_name = label of class in pandas dataframe\n", "            self._data = partition of data with only features of interest and class\n", "            self._cuts = dictionary with cut points for each feature\n", "        :param dataset: pandas dataframe with data to discretize\n", "        :param class_label: name of the column containing class in input dataframe\n", "        :param features: if !None, features that the user wants to discretize specifically\n", "        :return:\n", "        '''\n", "\n", "        if not isinstance(dataset, pd.core.frame.DataFrame):  # class needs a pandas dataframe\n", "            raise AttributeError('Input dataset should be a pandas data frame')\n", "\n", "        if not isinstance(testset, pd.core.frame.DataFrame):  # class needs a pandas dataframe\n", "            raise AttributeError('Test dataset should be a pandas data frame')\n", "\n", "        self._data_raw = dataset #copy or original input data\n", "        self._test_raw = testset #copy or original test data\n", "\n", "        self._class_name = class_label\n", "\n", "        self._classes = self._data_raw[self._class_name].unique()\n", "\n", "        #if user specifies which attributes to discretize\n", "        if features:\n", "            self._features = [f for f in features if f in self._data_raw.columns]  # check if features in dataframe\n", "            missing = set(features) - set(self._features)  # specified columns not in dataframe\n", "            if missing:\n", "                print ('WARNING: user-specified features %s not in input dataframe' % str(missing))\n", "        else:  # then we need to recognize which features are numeric\n", "            numeric_cols = self._data_raw._data.get_numeric_data().items\n", "            self._features = [f for f in numeric_cols if f != class_label]\n", "        #other features that won't be discretized\n", "        self._ignored_features = set(self._data_raw.columns) - set(self._features)\n", "        self._ignored_features_t = set(self._test_raw.columns) - set(self._features)\n", "\n", "        #create copy of data only including features to discretize and class\n", "        self._data = self._data_raw.loc[:, self._features + [class_label]]\n", "        self._test = self._test_raw.loc[:, self._features]\n", "        #pre-compute all boundary points in dataset\n", "        self._boundaries = self.compute_boundary_points_all_features()\n", "        #initialize feature bins with empty arrays\n", "        self._cuts = {f: [] for f in self._features}\n", "        #get cuts for all features\n", "        self.all_features_accepted_cutpoints()\n", "        #discretize self._data\n", "        self.apply_cutpoints(out_data_path=out_path_data, out_test_path=out_test_path_data, out_bins_path=out_path_bins)\n", "\n", "    def MDLPC_criterion(self, data, feature, cut_point):\n", "        '''\n", "        Determines whether a partition is accepted according to the MDLPC criterion\n", "        :param feature: feature of interest\n", "        :param cut_point: proposed cut_point\n", "        :param partition_index: index of the sample (dataframe partition) in the interval of interest\n", "        :return: True/False, whether to accept the partition\n", "        '''\n", "        #get dataframe only with desired attribute and class columns, and split by cut_point\n", "        data_partition = data.copy(deep=True)\n", "        data_left = data_partition[data_partition[feature] <= cut_point]\n", "        data_right = data_partition[data_partition[feature] > cut_point]\n", "\n", "        #compute information gain obtained when splitting data at cut_point\n", "        cut_point_gain = cut_point_information_gain(dataset=data_partition, cut_point=cut_point,\n", "                                                    feature_label=feature, class_label=self._class_name)\n", "        #compute delta term in MDLPC criterion\n", "        N = len(data_partition) # number of examples in current partition\n", "        partition_entropy = entropy(data_partition[self._class_name])\n", "        k = len(data_partition[self._class_name].unique())\n", "        k_left = len(data_left[self._class_name].unique())\n", "        k_right = len(data_right[self._class_name].unique())\n", "        entropy_left = entropy(data_left[self._class_name])  # entropy of partition\n", "        entropy_right = entropy(data_right[self._class_name])\n", "        delta = log(3 ** k, 2) - (k * partition_entropy) + (k_left * entropy_left) + (k_right * entropy_right)\n", "\n", "        #to split or not to split\n", "        gain_threshold = (log(N - 1, 2) + delta) / N\n", "\n", "        if cut_point_gain > gain_threshold:\n", "            return True\n", "        else:\n", "            return False\n", "\n", "    def feature_boundary_points(self, data, feature):\n", "        '''\n", "        Given an attribute, find all potential cut_points (boundary points)\n", "        :param feature: feature of interest\n", "        :param partition_index: indices of rows for which feature value falls whithin interval of interest\n", "        :return: array with potential cut_points\n", "        '''\n", "        #get dataframe with only rows of interest, and feature and class columns\n", "        data_partition = data.copy(deep=True)\n", "        data_partition.sort_values(feature, ascending=True, inplace=True)\n", "\n", "        boundary_points = []\n", "\n", "        #add temporary columns\n", "        data_partition['class_offset'] = data_partition[self._class_name].shift(1)  # column where first value is now second, and so forth\n", "        data_partition['feature_offset'] = data_partition[feature].shift(1)  # column where first value is now second, and so forth\n", "        data_partition['feature_change'] = (data_partition[feature] != data_partition['feature_offset'])\n", "        data_partition['mid_points'] = data_partition.loc[:, [feature, 'feature_offset']].mean(axis=1)\n", "\n", "        potential_cuts = data_partition[data_partition['feature_change'] == True].index[1:]\n", "        sorted_index = data_partition.index.tolist()\n", "\n", "        for row in potential_cuts:\n", "            old_value = data_partition.loc[sorted_index[sorted_index.index(row) - 1]][feature]\n", "            new_value = data_partition.loc[row][feature]\n", "            old_classes = data_partition[data_partition[feature] == old_value][self._class_name].unique()\n", "            new_classes = data_partition[data_partition[feature] == new_value][self._class_name].unique()\n", "            if len(set.union(set(old_classes), set(new_classes))) > 1:\n", "                boundary_points += [data_partition.loc[row]['mid_points']]\n", "\n", "        return set(boundary_points)\n", "\n", "    def compute_boundary_points_all_features(self):\n", "        '''\n", "        Computes all possible boundary points for each attribute in self._features (features to discretize)\n", "        :return:\n", "        '''\n", "        boundaries = {}\n", "        for attr in self._features:\n", "            data_partition = self._data.loc[:, [attr, self._class_name]]\n", "            boundaries[attr] = self.feature_boundary_points(data=data_partition, feature=attr)\n", "        return boundaries\n", "\n", "    def boundaries_in_partition(self, data, feature):\n", "        '''\n", "        From the collection of all cut points for all features, find cut points that fall within a feature-partition's\n", "        attribute-values' range\n", "        :param data: data partition (pandas dataframe)\n", "        :param feature: attribute of interest\n", "        :return: points within feature's range\n", "        '''\n", "        range_min, range_max = (data[feature].min(), data[feature].max())\n", "        return set([x for x in self._boundaries[feature] if (x > range_min) and (x < range_max)])\n", "\n", "    def best_cut_point(self, data, feature):\n", "        '''\n", "        Selects the best cut point for a feature in a data partition based on information gain\n", "        :param data: data partition (pandas dataframe)\n", "        :param feature: target attribute\n", "        :return: value of cut point with highest information gain (if many, picks first). None if no candidates\n", "        '''\n", "        candidates = self.boundaries_in_partition(data=data, feature=feature)\n", "        # candidates = self.feature_boundary_points(data=data, feature=feature)\n", "        if not candidates:\n", "            return None\n", "        gains = [(cut, cut_point_information_gain(dataset=data, cut_point=cut, feature_label=feature,\n", "                                                  class_label=self._class_name)) for cut in candidates]\n", "        gains = sorted(gains, key=lambda x: x[1], reverse=True)\n", "\n", "        return gains[0][0] #return cut point\n", "\n", "    def single_feature_accepted_cutpoints(self, feature, partition_index=pd.DataFrame().index):\n", "        '''\n", "        Computes the cuts for binning a feature according to the MDLP criterion\n", "        :param feature: attribute of interest\n", "        :param partition_index: index of examples in data partition for which cuts are required\n", "        :return: list of cuts for binning feature in partition covered by partition_index\n", "        '''\n", "        if partition_index.size == 0:\n", "            partition_index = self._data.index  # if not specified, full sample to be considered for partition\n", "\n", "        data_partition = self._data.loc[partition_index, [feature, self._class_name]]\n", "\n", "        #exclude missing data:\n", "        if data_partition[feature].isnull().values.any:\n", "            data_partition = data_partition[~data_partition[feature].isnull()]\n", "\n", "        #stop if constant or null feature values\n", "        if len(data_partition[feature].unique()) < 2:\n", "            return\n", "        #determine whether to cut and where\n", "        cut_candidate = self.best_cut_point(data=data_partition, feature=feature)\n", "        if cut_candidate == None:\n", "            return\n", "        decision = self.MDLPC_criterion(data=data_partition, feature=feature, cut_point=cut_candidate)\n", "\n", "        #apply decision\n", "        if not decision:\n", "            return  # if partition wasn't accepted, there's nothing else to do\n", "        if decision:\n", "            # try:\n", "            #now we have two new partitions that need to be examined\n", "            left_partition = data_partition[data_partition[feature] <= cut_candidate]\n", "            right_partition = data_partition[data_partition[feature] > cut_candidate]\n", "            if left_partition.empty or right_partition.empty:\n", "                return #extreme point selected, don't partition\n", "            self._cuts[feature] += [cut_candidate]  # accept partition\n", "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=left_partition.index)\n", "            self.single_feature_accepted_cutpoints(feature=feature, partition_index=right_partition.index)\n", "            #order cutpoints in ascending order\n", "            self._cuts[feature] = sorted(self._cuts[feature])\n", "            return\n", "\n", "    def all_features_accepted_cutpoints(self):\n", "        '''\n", "        Computes cut points for all numeric features (the ones in self._features)\n", "        :return:\n", "        '''\n", "        for attr in self._features:\n", "            self.single_feature_accepted_cutpoints(feature=attr)\n", "        return\n", "\n", "    def apply_cutpoints(self, out_data_path=None, out_test_path=None, out_bins_path=None):\n", "        '''\n", "        Discretizes data by applying bins according to self._cuts. Saves a new, discretized file, and a description of\n", "        the bins\n", "        :param out_data_path: path to save discretized data\n", "        :param out_test_path: path to save discretized test data\n", "        :param out_bins_path: path to save bins description\n", "        :return:\n", "        '''\n", "        pbin_label_collection = {}\n", "        bin_label_collection = {}\n", "        for attr in self._features:\n", "            if len(self._cuts[attr]) == 0:\n", "#                self._data[attr] = 'All'\n", "                self._data[attr] = self._data[attr].values\n", "                self._test[attr] = self._test[attr].values\n", "                pbin_label_collection[attr] = ['No binning']\n", "                bin_label_collection[attr] = ['All']\n", "            else:\n", "                cuts = [-np.inf] + self._cuts[attr] + [np.inf]\n", "                print(attr, cuts)\n", "                start_bin_indices = range(0, len(cuts) - 1)\n", "                pbin_labels = ['%s_to_%s' % (str(cuts[i]), str(cuts[i+1])) for i in start_bin_indices]\n", "                bin_labels = ['%d' % (i+1) for i in start_bin_indices]\n", "                pbin_label_collection[attr] = pbin_labels\n", "                bin_label_collection[attr] = bin_labels\n", "                self._data[attr] = pd.cut(x=self._data[attr].values, bins=cuts, right=False, labels=bin_labels,\n", "                                          precision=6, include_lowest=True)\n", "                self._test[attr] = pd.cut(x=self._test[attr].values, bins=cuts, right=False, labels=bin_labels,\n", "                                          precision=6, include_lowest=True)\n", "\n", "        #reconstitute full data, now discretized\n", "        if self._ignored_features:\n", "        #the line below may help in removing double class column ; looks like it works\n", "            self._data = self._data.loc[:, self._features]\n", "            to_return_train = pd.concat([self._data, self._data_raw[list(self._ignored_features)]], axis=1)\n", "            to_return_train = to_return_train[self._data_raw.columns] #sort columns so they have the original order\n", "        else:\n", "        #the line below may help in removing double class column ; looks like it works\n", "            self._data = self._data.loc[:, self._features]\n", "            to_return_train = self._data\n", "\n", "        #save data as csv\n", "        if out_data_path:\n", "            to_return_train.to_csv(out_data_path, index=False)\n", "\n", "        #reconstitute test data, now discretized\n", "        if self._ignored_features:\n", "        #the line below may help in removing double class column ; looks like it works\n", "        #    self._test = self._test.loc[:, self._features]\n", "            to_return_test = pd.concat([self._test, self._test_raw[list(self._ignored_features_t)]], axis=1)\n", "            to_return_test = to_return_test[self._test_raw.columns] #sort columns so they have the original order\n", "        else:\n", "        #the line below may help in removing double class column ; looks like it works\n", "        #    self._data = self._data.loc[:, self._features]\n", "            to_return_test = self._test\n", "\n", "        #save data as csv\n", "        if out_test_path:\n", "            to_return_test.to_csv(out_test_path, index=False)\n", "\n", "        #save bins description\n", "        if out_bins_path:\n", "            with open(out_bins_path, 'w') as bins_file:\n", "                print>>bins_file, 'Description of bins in file: %s' % out_data_path\n", "                for attr in self._features:\n", "                    print>>bins_file, 'attr: %s\\n\\t%s' % (attr, ', '.join([pbin_label for pbin_label in pbin_label_collection[attr]]))\n"], "execution_count": null, "metadata": {"_cell_guid": "9bc86e03-81c5-4081-ac7c-3ff83bd170c0", "collapsed": true, "_uuid": "c5b27fe4e402a72a32061353c4f7c1b7e6b3061d"}, "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "44ed36dd-4c96-498b-ac49-b4f125ec1c09", "_uuid": "3139442b398cf3debb4ffa54e1f0831ffdaea49c"}, "source": ["We will convert \"-1\" values to np.NaN so they do not contribute to calculated cutoffs. Thanks to @David Arlund who [__reminded me__](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43886#246381) that this needed to be done in order to get corect cuts. After features are discretized, we will reverse this operation in our final files.\n", "\n", "Here we also define features to be discretized. I am using only three features because the notbook times out otherwise. Make sure to uncomment the line below with 4 features, and maybe add a few of your own."], "cell_type": "markdown"}, {"source": ["df_train = pd.read_csv('../input/train.csv', dtype={'id': np.int32, 'target': np.int8})\n", "print(' Train dataset:', df_train.shape)\n", "train = df_train.drop(['id', 'target'], axis=1)\n", "target = df_train['target']\n", "df_train = df_train.replace(-1, np.nan)\n", "df_test = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\n", "print(' Test dataset:', df_test.shape)\n", "test = df_test.drop(['id'], axis=1)\n", "df_test = df_test.replace(-1, np.nan)\n", "\n", "class_label='target' \n", "features=['ps_reg_03', 'ps_car_12', 'ps_car_14']\n", "#features=['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14']\n", "\n", "start_time=timer(None)\n", "discretizer = MDLP_Discretizer(dataset=df_train, testset=df_test, class_label=class_label, features=features, out_path_data='./train-mdlp.csv', out_test_path_data='./test-mdlp.csv', out_path_bins=None)\n", "timer(start_time)"], "execution_count": null, "metadata": {"_cell_guid": "7add889b-a9b9-42e1-a0ff-494ba141adfd", "collapsed": true, "_uuid": "eea5ce0332548622618300151dc2f1e103bf281d"}, "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "f5e2075f-f490-4f7f-ac7a-331cb0f6e14c", "_uuid": "43320cbe37a1710d0dd9ebabaab39549bad161e6"}, "source": ["After reloading the new data and converting np.NaN values back to -1, we are back to the original format without missing values.\n", "\n", "A quick comparison of continuous variables before and after modification shows a major difference.\n", "\n", "***EDIT: The part below does not seem to work on Kaggle. Even though the files are saved in current directory, it appears that they go into an \"output\" directory dictated by system. So the files can be downloaded from the \"Output\" tab, but I don't know how to read them directly from the notebook. Anyway, the screen output for the rest of this notebook is shown at the end.***"], "cell_type": "markdown"}, {"source": ["mdlp_train = pd.read_csv('./train-mdlp.csv', dtype={'id': np.int32, 'target': np.int8})\n", "mdlp_train = mdlp_train.replace(np.nan, -1)\n", "train_mdlp = mdlp_train.drop(['id', 'target'], axis=1)\n", "mdlp_test = pd.read_csv('./test-mdlp.csv', dtype={'id': np.int32})\n", "mdlp_test = mdlp_test.replace(np.nan, -1)\n", "test_mdlp = mdlp_test.drop(['id'], axis=1)\n", "\n", "cols = ['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14']\n", "print('\\n Original features:')\n", "for col in cols:\n", "    print('\\n', col)\n", "    print(' Unique values: %d' % (len(np.unique(train[col]))))\n", "    if (len(np.unique(train[col]))) <= 50:\n", "        print('', np.unique(train[col]))\n", "    else:\n", "        print('', np.unique(train[col])[:50])\n", "\n", "print('\\n Discretized features:')\n", "for col in cols:\n", "    print('\\n', col)\n", "    print(' Unique values: %d' % (len(np.unique(mdlp_train[col]))))\n", "    if (len(np.unique(mdlp_train[col]))) <= 50:\n", "        print('', np.unique(mdlp_train[col]))\n", "    else:\n", "        print('', np.unique(mdlp_train[col])[:50])\n"], "execution_count": null, "metadata": {"_cell_guid": "83ad058e-9e14-49d2-8715-9c2e035b5bf5", "collapsed": true, "_uuid": "606f71faef094b54524b6e00713feb656b95ac0f"}, "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "e4dbd053-c645-447b-b30c-01f3136bc066", "_uuid": "1aa7371fac9571f057bfc8b4cba2bd8d35fc73b3"}, "source": ["Let's do two quick XGBoost runs with generic parameters to compare the original data versus the discretized version."], "cell_type": "markdown"}, {"source": ["sss = StratifiedShuffleSplit(target, test_size=0.2, random_state=1001)\n", "for train_index, test_index in sss:\n", "    break\n", "train_x, train_y = train.loc[train_index], target.loc[train_index]\n", "val_x, val_y = train.loc[test_index], target.loc[test_index]\n", "train_mdlp_x, train_y = train_mdlp.loc[train_index], target.loc[train_index]\n", "val_mdlp_x, val_y = train_mdlp.loc[test_index], target.loc[test_index]\n", "\n", "xgb_params = {\n", "              'booster': 'gbtree',\n", "              'seed': 1001,\n", "              'gamma': 9.0,\n", "              'colsample_bytree': 0.8,\n", "              'silent': True,\n", "              'nthread': 4,\n", "              'subsample': 0.8,\n", "              'learning_rate': 0.1,\n", "              'eval_metric': 'auc',\n", "              'objective': 'binary:logistic',\n", "              'max_delta_step': 1,\n", "              'max_depth': 5,\n", "              'min_child_weight': 2,\n", "             }\n", "\n", "print('\\n XGBoost on original data:\\n')\n", "d_train = xgb.DMatrix(train_x, label=train_y)\n", "d_valid = xgb.DMatrix(val_x, label=val_y)\n", "watchlist = [(d_train, 'train'), (d_valid, 'val')]\n", "clf = xgb.train(xgb_params, dtrain=d_train, num_boost_round=100000, evals=watchlist, early_stopping_rounds=100, verbose_eval=50)\n", "\n", "print('\\n XGBoost on modified data:\\n')\n", "d_train = xgb.DMatrix(train_mdlp_x, label=train_y)\n", "d_valid = xgb.DMatrix(val_mdlp_x, label=val_y)\n", "watchlist = [(d_train, 'train'), (d_valid, 'val')]\n", "clf = xgb.train(xgb_params, dtrain=d_train, num_boost_round=100000, evals=watchlist, early_stopping_rounds=100, verbose_eval=50)\n"], "execution_count": null, "metadata": {"_cell_guid": "c29a4ac5-6b5b-48bf-af43-400f27ad0e84", "collapsed": true, "_uuid": "c9bc6a6889939401b9285c7b7c26e796fa27e394"}, "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "79a4daf4-be84-4b1c-ba7a-967333a989c9", "_uuid": "cc596d1cbb895c84ea69101de950d5294993f0fb"}, "source": ["Looks good to me.\n", "\n", "**Last two cells of this notebook output the following on my computer:**\n", "\n", "     Original features:\n", "    \n", "     ps_reg_03\n", "     Unique values: 5013\n", "     [-1.          0.06123724  0.075       0.13228757  0.13693064  0.15\n", "      0.15411035  0.1968502   0.21065374  0.21505813  0.22638463  0.22776084\n", "      0.23048861  0.23717082  0.24622145  0.25124689  0.25248762  0.25372229\n", "      0.25495098  0.25617377  0.25860201  0.26339134  0.26457513  0.26575365\n", "      0.27386128  0.27613403  0.27726341  0.27838822  0.2806243   0.28173569\n", "      0.28284271  0.28394542  0.28504386  0.28722813  0.29047375  0.29154759\n", "      0.2936835   0.29580399  0.29685855  0.29790938  0.3         0.30103986\n", "      0.30207615  0.30310889  0.30413813  0.30516389  0.30618622  0.30720514\n", "      0.30923292  0.31024184]\n", "    \n", "     ps_car_12\n", "     Unique values: 184\n", "     [-1.          0.1         0.14142136  0.14832397  0.17320508  0.28284271\n", "      0.3088689   0.31527766  0.31559468  0.31575307  0.31606961  0.31622777\n", "      0.33166248  0.34641016  0.35199432  0.3524202   0.36        0.36041643\n", "      0.36055513  0.36878178  0.37040518  0.37255872  0.3726929   0.37416574\n", "      0.38639358  0.38691084  0.38729833  0.39433488  0.39471509  0.39522146\n", "      0.39560081  0.39724048  0.39749214  0.39761791  0.39799497  0.39812058\n", "      0.39837169  0.39862263  0.39874804  0.39899875  0.39937451  0.39949969\n", "      0.39962482  0.39974992  0.39987498  0.4         0.40841156  0.41231056\n", "      0.41797129  0.41964271]\n", "    \n", "     ps_car_13\n", "     Unique values: 70482\n", "     [ 0.25061907  0.28733601  0.29041151  0.29119304  0.30872304  0.30925798\n", "      0.31065779  0.31324086  0.31333978  0.31373516  0.31468862  0.31606438\n", "      0.32017557  0.3221185   0.32250312  0.32463425  0.32524625  0.32527801\n", "      0.32550025  0.3256589   0.32619773  0.32638769  0.32667243  0.32727273\n", "      0.32736741  0.32840712  0.32850148  0.32872154  0.32922398  0.32934947\n", "      0.33153786  0.33172477  0.33184931  0.33188044  0.33201115  0.33256454\n", "      0.3330922   0.33358806  0.33380476  0.33405226  0.33408318  0.33429956\n", "      0.33439226  0.33448493  0.33488619  0.33513288  0.33617931  0.33676266\n", "      0.33685468  0.33700798]\n", "    \n", "     ps_car_14\n", "     Unique values: 850\n", "     [-1.          0.10954451  0.1183216   0.13601471  0.2         0.2078461\n", "      0.20976177  0.21095023  0.21307276  0.21794495  0.22135944  0.22315914\n", "      0.2236068   0.22427661  0.23021729  0.23323808  0.23345235  0.23452079\n", "      0.24899799  0.26172505  0.26457513  0.27386128  0.27748874  0.2792848\n", "      0.28195744  0.28284271  0.28372522  0.28548205  0.28600699  0.28635642\n", "      0.28722813  0.28809721  0.28827071  0.28879058  0.28982753  0.29257478\n", "      0.29325757  0.29410882  0.29444864  0.29495762  0.29512709  0.29529646\n", "      0.29614186  0.29664794  0.29732137  0.29832868  0.29916551  0.3\n", "      0.30033315  0.30066593]\n", "    \n", "     Discretized features:\n", "    \n", "     ps_reg_03\n", "     Unique values: 5\n", "     [-1.  1.  2.  3.  4.]\n", "    \n", "     ps_car_12\n", "     Unique values: 6\n", "     [-1.  1.  2.  3.  4.  5.]\n", "    \n", "     ps_car_13\n", "     Unique values: 7\n", "     [1 2 3 4 5 6 7]\n", "    \n", "     ps_car_14\n", "     Unique values: 8\n", "     [-1.  1.  2.  3.  4.  5.  6.  7.]\n", "    \n", "     XGBoost on original data:\n", "    \n", "    [0]     train-auc:0.5   val-auc:0.5\n", "    Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n", "    \n", "    Will train until val-auc hasn't improved in 100 rounds.\n", "    [50]    train-auc:0.667636      val-auc:0.62964\n", "    [100]   train-auc:0.70354       val-auc:0.636332\n", "    [150]   train-auc:0.724789      val-auc:0.634334\n", "    Stopping. Best iteration:\n", "    [87]    train-auc:0.696711      val-auc:0.636762\n", "    \n", "    \n", "     XGBoost on modified data:\n", "    \n", "    [0]     train-auc:0.5   val-auc:0.5\n", "    Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n", "    \n", "    Will train until val-auc hasn't improved in 100 rounds.\n", "    [50]    train-auc:0.66269       val-auc:0.632063\n", "    [100]   train-auc:0.697454      val-auc:0.637846\n", "    [150]   train-auc:0.719444      val-auc:0.636979\n", "    [200]   train-auc:0.740215      val-auc:0.634624\n", "    Stopping. Best iteration:\n", "    [115]   train-auc:0.705178      val-auc:0.638287\n", "    "], "cell_type": "markdown"}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1}