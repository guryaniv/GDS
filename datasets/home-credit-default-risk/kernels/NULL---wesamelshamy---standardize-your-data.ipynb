{"cells":[{"metadata":{"_uuid":"ab5152cccefe0aadefe3f926b40ba30c837ff249"},"cell_type":"markdown","source":"# Introduction\nIn our training data we have more than 200 features with different charachteristics like type, dirstribution, range, outliers, level of sparsity among others.  It's hard and time consuming to deep anlyze them one by one, and then proccess them differently before we use them in our model (Apples $\\neq$ Oranges).  Let's do part of this work here.\n\nLet's use some features from the `application_train.csv` data to maximize the common good since most of us use it extensively in modeling.  The same analysis applies to other numerical features.\n\nThe customer's total income (`AMT_INCOME_TOTAL`) and credit amount (`AMT_CREDIT`) are among the factors used by credit bureaus in credit scoring.  A credit score is considered a good predictor of the borrower's ability to pay back their dept on time.\n\nThe income amount, however, have a few marginal outlier values that could slow down or even prevent convergence of some gradient based algorithms.  Moreover, the distribution of the values is heavily skewed for both features, and they have different range values compared to the other features.  The negative effect of having different feature ranges is less pronounced in tree based algorithms, though.\n\nIn the two figures below, on the left we have the customer income plotted against an arbitrary customer number to show marginal outlier values.  On the right plot we see big skewness in the distribution of the credit amount of the loan.\n\n[1]: http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/lecture_notes/boosting/boosting.pdf"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e133ba292efd7bd5da47bbdde576f4f6fd678d16","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport os\n\napps = pd.read_csv('../input/application_train.csv', index_col='SK_ID_CURR')\ntarget = apps.pop('TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de333f75b968ed6df86bba92569ea5634f8d624f","_kg_hide-input":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nX = apps[['AMT_CREDIT', 'AMT_INCOME_TOTAL']]\n\nax = axes[0]\nincome = X['AMT_INCOME_TOTAL']\nax.scatter(np.arange(income.shape[0]), income)\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.set_title('Marginal outliers in customer income feature')\nax.set_xlabel('Customer number (arbitrary)')\nax.set_ylabel('Customer income')\n\nax = axes[1]\ncredit = X['AMT_CREDIT']\nsns.distplot(credit, ax=ax, kde=False)\nplt.xticks(rotation=30)\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nax.set_title('Highly skewed distribution for credit amount of loan')\nax.set_xlabel('Credit amount')\nax.set_ylabel('Number of customers');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"25f580d146e615832a6b61953ca1121d66fca9fe","collapsed":true},"cell_type":"code","source":"# Author:  Raghav RV <rvraghav93@gmail.com>\n#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n#          Thomas Unterthiner\n# License: BSD 3 clause\n\nimport numpy as np\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib import patches\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing.data import QuantileTransformer\n\n\ny_full = target.values\n\n# Take only 2 features to make visualization easier\n# AMT_CREDIT has a long tail distribution.\n# AMT_INCOME_TOTAL has a few but very large outliers.\n\nX = apps[['AMT_CREDIT', 'AMT_INCOME_TOTAL']].values\n\ndistributions = [\n    ('Unscaled data', X),\n    ('Data after standard scaling',\n        StandardScaler().fit_transform(X)),\n    ('Data after min-max scaling',\n        MinMaxScaler().fit_transform(X)),\n    ('Data after max-abs scaling',\n        MaxAbsScaler().fit_transform(X)),\n    ('Data after robust scaling',\n        RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n    ('Data after quantile transformation (uniform pdf)',\n        QuantileTransformer(output_distribution='uniform')\n        .fit_transform(X)),\n    ('Data after quantile transformation (gaussian pdf)',\n        QuantileTransformer(output_distribution='normal')\n        .fit_transform(X)),\n    ('Data after sample-wise L2 normalizing',\n        Normalizer().fit_transform(X))\n]\n\ny = y_full\n\n\ndef create_axes(title, figsize=(16, 6)):\n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title)\n\n    # define the axis for the first plot\n    left, width = 0.1, 0.22\n    bottom, height = 0.1, 0.7\n    bottom_h = height + 0.15\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter = plt.axes(rect_scatter)\n    ax_histx = plt.axes(rect_histx)\n    ax_histy = plt.axes(rect_histy)\n\n    # define the axis for the zoomed-in plot\n    left = width + left + 0.2\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter_zoom = plt.axes(rect_scatter)\n    ax_histx_zoom = plt.axes(rect_histx)\n    ax_histy_zoom = plt.axes(rect_histy)\n\n    return ((ax_scatter, ax_histy, ax_histx),\n            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom))\n\n\ndef plot_distribution(axes, X, y, hist_nbins=50, title=\"\",\n                      x0_label=\"\", x1_label=\"\"):\n    ax, hist_X1, hist_X0 = axes\n\n    ax.set_title(title)\n    ax.set_xlabel(x0_label)\n    ax.set_ylabel(x1_label)\n\n    # The scatter plot    \n    ax.scatter(X[y < 0.5, 0], X[y < 0.5, 1], alpha=0.2, marker='o', s=2, lw=0, c='yellow', label='Target = 0')\n    ax.scatter(X[y > 0.5, 0], X[y > 0.5, 1], alpha=0.2, marker='o', s=2, lw=0, c='black', label='Target = 1')\n    ax.legend()\n\n    # Removing the top and the right spine for aesthetics\n    # make nice axis layout\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n\n    # Histogram for axis X1\n    hist_X1.set_ylim(ax.get_ylim())\n    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal',\n                 color='grey', ec='grey')\n    hist_X1.axis('off')\n    \n\n    # Histogram for axis X0\n    hist_X0.set_xlim(ax.get_xlim())\n    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical',\n                 color='grey', ec='grey')\n    hist_X0.axis('off')\n\n\ndef make_plot(item_idx):\n    title, X = distributions[item_idx]\n    ax_zoom_out, ax_zoom_in = create_axes(title)\n    axarr = (ax_zoom_out, ax_zoom_in)\n    plot_distribution(axarr[0], X, y, hist_nbins=200,\n                      x0_label=\"Credit amount\",\n                      x1_label=\"Total income\",\n                      title=\"Full data\")\n\n    # zoom-in\n    zoom_in_percentile_range = (0, 99)\n    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n\n    non_outliers_mask = (\n        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n                      hist_nbins=50,\n                      x0_label=\"Credit amount\",\n                      x1_label=\"Total income\",\n                      title=\"Zoom-in\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82e987dff71aa8170380cbdfc9fb275ba8681a37"},"cell_type":"markdown","source":"## Standardization: scale *vs* transform\nTo metigate the effect of the heavily skewed distribution, the present of outliers, and having different range values, we need to standardize the data.  There are two main ways of doing this.  We can **scale** the data by applying linear tranformations, or we can non-linearly **transform** it.\n\nIn the figures below, we see the joint distribution of the total income and credit amount values colored by default (black dot) and non-default (yellow dot).  The charts are adapted from [this sklearn's tutorial][1].  On the left plot we have a full view of the data where the outlier points compress the rest of the data in a tiny sliver.  The right plot zooms in on that tiny sliver for better illustration.\n\n[1]: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b54451686a4eb9ebc144ea3357bc549f4ee2730d","collapsed":true},"cell_type":"code","source":"make_plot(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"339c724467e0dff583d84baa4f144228cbf6c369"},"cell_type":"markdown","source":"## Scale to unit variance\nBy scaling we remove the mean value and then divide by the standard deviation.  We can achieve this using [sklearn's `StandardScaler`][1].  The data ends up having unit variance, but the effect of the outliers can be seen in the left figure below where most of the data are compressed in s small range.  Furthermore, because the features have different outlier values, they end up having different range values.\n\n### Do not use if\nRemoving the mean value from all data points destroys the sparseness structure of sparse data.  This type of scaling is also susceptible to the presense of marginal outliers.  There are other types of feature standardization you may consider using if your data is sparse or has a few large marginal outilers.\n\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"ba10e18f127340096a3826b053d4a4e9eb0294aa","collapsed":true},"cell_type":"code","source":"make_plot(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e857460f124db9d8752e4f6f34f5d1a34a49561"},"cell_type":"markdown","source":"## Scale to range\nTo metigate the effect of ending up with transformed features having different ranges, we can dictate the range to the [sklearn's `MinMaxScaler`][1].  Our features will be scalled to the given range, but **will not be centered around zero**.\n\n[`MaxAbsScaler`][2] is another alternative that scales the max absolute value to 1.  However, the lower range of the scaled feature is not guaranteed to be zero.\n\n### Do not use if\nAs we can see from the plots below, this scaler is affected by a few marginal outliers.  Notice how in the plot on the right below the range for most of the points for the credit amount is [0, 0.4], while it's [-0.002, 0.006] for the total income.\n\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n[2]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0e3c50b480bfceb3c695dd1d87eedd77c57b2813","collapsed":true},"cell_type":"code","source":"make_plot(2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1e5df91c939efa3d9a3c5acc33a3976ef29c73fd","collapsed":true},"cell_type":"code","source":"make_plot(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9becd885c637ff16c0e308c960c42aaf482d098c"},"cell_type":"markdown","source":"## Scale with outliers\nTo partially metigate the problem of marginal outliers in the data, we can use the [sklearn's `RobustScaler`][1].  It scales the data to its middle 50% ([IQR][2]) and is thus robust to a few outliers.  We can see in the right chart below how both features have most of their values lie in similar ranges [-1, 3.5] and [-1, 2.5].\n\nNote that we still have outlier points (left plot below) in both features.  Their negative effect may be felt depending on the machine learning algorithm we later use.\n\n### Do not use if\nThis scaler removes the mean from the data destroying its sparseness structure.  Avoid using it if your data is sparse.\n\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\n[2]: https://en.wikipedia.org/wiki/Interquartile_range"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7d16bfed3239d0f4e7756fa9aa5066cd6d6005b5","collapsed":true},"cell_type":"code","source":"make_plot(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7846daee278db1fb4aef9a4ed04034cd05d22390"},"cell_type":"markdown","source":"## Non-linear transform\nLinear transormations preserve the big gap between the marginal outliers and the rest of the data.  However, we can apply a non-linear transformation to change the distribution of the data and bring the outliers to order.  [Sklearn's `QuantileTransformer`][1] molds the values into uniform distribution in the [0, 1] range by default.  It can also be parametrized to transform the values into a Gaussian distribution.\n\n### Do not use if\nThis non-linear transformation may destroy the feature's linear correlation with other features.  It also desroys the sparseness structure of the data. Consider using other feature scaling or transformers if feature linear correlation or sparseness structure needs to be preserved.\n\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8224643cc2319ea8d690ec1a803cdefb736f563d","collapsed":true},"cell_type":"code","source":"make_plot(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d570d29d8252e61937e62c2df9353b9dcc3e4252","collapsed":true},"cell_type":"code","source":"make_plot(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"842eaca2e2ea33f32d5e08efb13ab15359f26201"},"cell_type":"markdown","source":"## Normalization of rows\nAll previously discussed scalers and transformers standardize the features independently.  The total income feature was scaled and sometimes centered independent of the credit amount feature.  If the features are related or are used later on to derive a variable then we may want to normalize them row-wise to unit norm.  This transformation resuls in having our data points with unit distance from the origin.\n\n### Do not use if\nIf the features are not related or have different types, then you should probably use a different standardization measure."},{"metadata":{"trusted":true,"_uuid":"e4f7901cbec840fb005c90b5c057f79bd605edfe","collapsed":true},"cell_type":"code","source":"make_plot(7)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}