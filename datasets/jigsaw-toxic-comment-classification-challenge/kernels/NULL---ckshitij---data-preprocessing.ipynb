{"cells":[{"metadata":{"_cell_guid":"c5ae927f-2cf1-43bd-af23-f00fce18ef4f","_uuid":"8b4da544904d11dcf730785899e0c579f49dafae"},"cell_type":"markdown","source":"## Comment Toxicity Classificaion\n\n+ This folder Contains 3 files :\n    - train.csv\n    - test.csv\n    - sample_submission.csv","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"adc7ffae-e295-497c-a930-bd2a12cbb9cd","_uuid":"e24418dcfb9cb9bcaf00f84bb2317af4533f7439","trusted":true},"cell_type":"code","source":"%ls -l","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98104849-bf6f-4527-ad80-89901dc41955","_uuid":"43314e031a9cd7837839aa42e19fd710b358e4d8"},"cell_type":"markdown","source":"## Import required packages\n\n+ Basics\n+ Vizualization\n+ Natural language Processing tool\n+ Feture Engineering\n+ Setting","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"016172f201bf02d96f74ccd3e7aa3f57f2c4a6ab","_cell_guid":"7b6a8dc2-07e7-425f-8eb8-280622665eee","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7d925158cdc1a18c172f71a3898e487f314cf531","_cell_guid":"619a49f2-934d-4433-b7a8-7b749df0ca28","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99a31c72-8146-4c34-b15a-4517933528ff","_uuid":"baf6a96d6cbc1f0ca9e7cd62e18f74947629c5ff"},"cell_type":"markdown","source":"### About NLP Libararies\n\n+ Spacy\n    - [Spacy Tutorial by Analytics Vidiya](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)\n+ NLTK \n    - [NLTK book](http://www.nltk.org/book/)\n+ RE (Regular Expression libraries)\n    - [RE tutorial](https://docs.python.org/2/howto/regex.html)","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"7d55b4032b6e6b4362069f157fcb3c03674f16ce","_cell_guid":"76cc36e1-e9f8-4fec-b58d-17ff69d13c5e","trusted":true},"cell_type":"code","source":"import string\nimport re    \nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"59cf163d1415d334c0546e3250dae708d259b284","_cell_guid":"f0b387c0-6d49-4f82-914e-9d433987346c","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nstoplist = set(stopwords.words(\"english\"))\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ecced14e-8433-4e4c-9c3a-0435a9f57f8c","_uuid":"72caf672ba4717a6040773fdc7c361419fe84cbb"},"cell_type":"markdown","source":"### Starting  feature Engg","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2115f72a-7814-4c50-8136-7eb8d92a78cd","_uuid":"2a48e88059e5410c44f44489bcbc445f7453725e","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e53c4ed-286b-4fea-a7c0-d22a40258d22","_uuid":"7474714747b064db676cb08c027a423b374bf54c","trusted":true},"cell_type":"code","source":"train.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39e9c0aa-8ece-4399-893c-84016410f603","_uuid":"70ad3162445421427ca38b629cecfdaabea90472","trusted":true},"cell_type":"code","source":"x = train.iloc[:,2:].sum()\nprint(x.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0559aed6-ffa0-4604-a3fe-df122247b6b5","_uuid":"069165b13f95997afab99b00f3172362e269a26d","trusted":true},"cell_type":"code","source":"rowsums = train.iloc[:,2:].sum(axis=1)\ntrain['non-toxic'] = (rowsums==0)\ntrain['non-toxic'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a917fa76-505d-4503-91d8-0e06c526a2d5","_uuid":"d14877eb78a1a34a11a6fa9267cbd1cc9b855c21","trusted":true},"cell_type":"code","source":"print(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['non-toxic'].sum())\nprint(\"Total tags =\",x.sum())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"5461493c4b2ad500b247c514b30805932dab3cde","_cell_guid":"2a0f14c4-971d-4b9a-8ada-25fd8d6fa71d","trusted":true},"cell_type":"code","source":"x = train.iloc[:,2:].sum()\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values)\nplt.title(\"Class Distribution\")\nplt.ylabel('Class frequency', fontsize=15)\nplt.xlabel('Class Types', fontsize=15)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"3985966c0f2d57e6ed996623ec188b529d20b4e2","_cell_guid":"4b2877f9-ac1b-4d46-973e-c868c0a844ef","trusted":true},"cell_type":"code","source":"train.iloc[2,:][1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4531acf3-9364-4749-a4c7-d94ca36095a8","_uuid":"35d467ced9469e47511d612ee1e764a32dbb8661"},"cell_type":"markdown","source":"## Replacement Algorithm or Modules","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"467ce821ebc6d68b6375dd5e9a118ae8efe3109a","_cell_guid":"145b9809-25bb-417f-8faf-c1a71e0e4db2","trusted":true},"cell_type":"code","source":"replacement_patterns = [  \n    (r'won\\'t', 'will not'),  \n    (r'can\\'t', 'cannot'),  \n    (r'i\\'m', 'i am'),  \n    (r'ain\\'t', 'is not'),  \n    (r'(\\w+)\\'ll', '\\g<1> will'),  \n    (r'(\\w+)n\\'t', '\\g<1> not'),  \n    (r'(\\w+)\\'ve', '\\g<1> have'),  \n    (r'(\\w+)\\'s', '\\g<1> is'),  \n    (r'(\\w+)\\'re', '\\g<1> are'),  \n    (r'(\\w+)\\'d', '\\g<1> would')\n]\n\nclass RegexpReplacer(object):  \n    def __init__(self, patterns=replacement_patterns):    \n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]  \n        \n    def replace(self, text):    \n        s = text    \n        for (pattern, repl) in self.patterns:      \n            s = re.sub(pattern, repl, s)    \n        return s\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8da36eba-2344-4e17-a89a-d314d73e820a","_uuid":"e2eec679d17369e2c6d54e5d91220781917e8786"},"cell_type":"markdown","source":"## Replacing negations with antonyms","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"ec1e4898dac08203b3514b37a1ba07bc039b473c","_cell_guid":"5bd3e09b-4d52-4078-b8d9-2181a9556a38","trusted":true},"cell_type":"code","source":"from nltk.corpus import wordnet\n\nclass AntonymReplacer(object):\n    \n    def replace(self, word, pos=None):\n        antonyms = set()\n        for syn in wordnet.synsets(word, pos=pos):\n            for lemma in syn.lemmas():\n                for antonym in lemma.antonyms():\n                    antonyms.add(antonym.name())\n        if len(antonyms) == 1:\n            return antonyms.pop()\n        else:\n            return None\n        \n    def replace_negations(self, sent):\n        i, l = 0, len(sent)\n        words = []\n        while i < l:\n            word = sent[i]\n            if word == 'not' and i+1 < l:\n                ant = self.replace(sent[i+1])\n                if ant:\n                    words.append(ant)\n                    i += 2\n                    continue\n            words.append(word)\n            i += 1\n        return words\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a5b6233d-43a4-466d-ad71-fce5dccde320","_uuid":"36e93d263c861511ea37778fbf092de17b21e301"},"cell_type":"markdown","source":"# Noise Removal","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"8824a779a2f4b6bb16079e18c4aed43c3fdbe7a3","_cell_guid":"9547ae23-d3bf-4756-84b4-a3c3c0d82486","trusted":true},"cell_type":"code","source":"from nltk import wordpunct_tokenize\nfrom nltk import WordNetLemmatizer\nfrom nltk import sent_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet as wn\n\nstoplist = set(stopwords.words(\"english\"))\n\nclass Remove_Noise(object):\n    \n    def __init__(self,stop_word = stoplist):\n        self.stop_word = stoplist\n    \n    def noise_rm(self,doc):\n        doc = re.sub('[#$%^&\\',:()*+/<=>@[\\\\]^_``{|}~]',' ',doc)\n        doc = re.sub('[0-9]+',' ',doc)\n        doc = re.sub('\\n','',doc)\n        doc = re.sub(' +',' ',doc)\n        doc = doc.lower()\n        return doc\n    \n    def lemmatize(self,token, tag):\n        tag = {\n            'N': wn.NOUN,\n            'V': wn.VERB,\n            'R': wn.ADV,\n            'J': wn.ADJ\n        }.get(tag[0], wn.NOUN)\n        lemmatizer = WordNetLemmatizer()\n        return lemmatizer.lemmatize(token, tag)\n    \n    def tokenize(self,document): \n        #document = unicode(document,'utf-8')\n        lemmy = []\n        for sent in sent_tokenize(document):\n            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n                if token in self.stop_word:\n                    continue\n                lemma = self.lemmatize(token, tag)\n                lemmy.append(lemma)\n        return lemmy","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"09520579e81dcac0df88732d73a4317eb2dc1c75","_cell_guid":"9315f78c-0857-47d8-9b6e-0229209d347f","trusted":true},"cell_type":"code","source":"replacer = RegexpReplacer()\nremover = Remove_Noise()\nAntoRep = AntonymReplacer()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0b66d500064a60e83fb9de5befc871300d0c8c6b","_cell_guid":"82d4b8cf-98ec-465b-b0aa-04c78d2511d8","trusted":true},"cell_type":"code","source":"train['comment_text'].fillna(' ', inplace=True)\ntest['comment_text'].fillna(' ', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"95ab2f7aa21bbfc42499e552c8cfe678a76eea88","_cell_guid":"3ce03cf3-fb00-42c7-9c53-5d5459fcc319","trusted":true},"cell_type":"code","source":"train['comment_full'] = train['comment_text'].apply(replacer.replace)\ntest['comment_full'] = test['comment_text'].apply(replacer.replace)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a4c105a7902ba56f15efc98f2b9e7b2689267ccd","_cell_guid":"f6e59a62-9ad2-473a-b490-27eabccc3bb3","trusted":true},"cell_type":"code","source":"train['Remove_noise'] = train['comment_full'].apply(remover.noise_rm)\ntest['Remove_noise'] = test['comment_full'].apply(remover.noise_rm)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"455e82f31c8428f94bb470e21ee0ba7f92803a76","_cell_guid":"b9e19a70-9dd7-447e-a3e1-19969a4ada7d","trusted":true},"cell_type":"code","source":"train['TokenandLemma'] = train['Remove_noise'].apply(remover.tokenize)\ntest['TokenandLemma'] = test['Remove_noise'].apply(remover.tokenize)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ecff1565300988320bce5d1439b0dc69993abc99","_cell_guid":"7ac5d109-6d66-4465-8dac-8ecbb65d182f","trusted":true},"cell_type":"code","source":"train[\"Processed\"] = train['TokenandLemma'].apply(AntoRep.replace_negations)\ntest[\"Processed\"] = test['TokenandLemma'].apply(AntoRep.replace_negations)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"98d04c3d2523c10bd09a9e443815b83289fc1593","_cell_guid":"125d0d4b-92e7-4d76-abd9-f3b8b4bb967e","trusted":true},"cell_type":"code","source":"train.to_pickle('train_processed.pkl')\ntest.to_pickle('test_processed.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e43ea57aa698dd66607cc9fbf621515e8cdee6f8","_cell_guid":"4d3519f5-f9fb-4cff-9760-a47c84c0c44f","collapsed":true,"trusted":false,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}