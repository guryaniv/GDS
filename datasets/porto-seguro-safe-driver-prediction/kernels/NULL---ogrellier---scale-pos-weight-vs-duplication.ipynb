{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "adf773f9b1c57bd6f2a7d483206a4cd5c1c1758d", "_cell_guid": "da6aa26a-7972-488f-ac14-646ffdb70c62"}, "source": ["The purpose of this notebook is to check the effect of both scale_pos_weight and duplication on a LightGBM classifier."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "037573c3943c5c71f471ee7a2cc9faa2583ea142", "_cell_guid": "a924553d-bcf2-429a-8f9d-1f61fecf0da9", "collapsed": true}, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from lightgbm import LGBMClassifier\n", "from sklearn.model_selection import StratifiedKFold\n", "import gc\n", "from numba import jit\n", "from sklearn.preprocessing import LabelEncoder\n", "import time \n", "from sklearn.metrics import confusion_matrix, f1_score\n", "import matplotlib.pyplot as plt\n", "import matplotlib.gridspec as gridspec\n", "%matplotlib inline\n", "import matplotlib.gridspec as gridspec\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import roc_curve\n", "import itertools\n", "import math\n", "np.set_printoptions(precision=3)\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9a4d445da3a2b2e5e479ab8ead5e5c8ca00ef7d8", "_cell_guid": "76893dbf-a616-44d4-b988-adab98425eff", "collapsed": true}, "outputs": [], "source": ["@jit  # for more info please visit https://numba.pydata.org/\n", "def eval_gini(y_true, y_prob):\n", "    \"\"\"\n", "    Original author CMPM \n", "    https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n", "    \"\"\"\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini\n", "\n", "# This is taken from sklearn examples and is available at\n", "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n", "def plot_confusion_matrix(cm, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "03c05e2f6ae6aaef9fe0adfbb2033b7d56493001", "_cell_guid": "51a49415-dcff-41d5-bd89-2da96e433f1b"}, "source": ["### Get the data and reduce the features \n", "Feature selection is done according to kernel \n", "https://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features \n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "401c253e8901e414057f1da1db5ff43ea55bd5b5", "_cell_guid": "db8286e3-f398-4e39-9155-17e539d00153", "collapsed": true}, "outputs": [], "source": ["trn_df = pd.read_csv(\"../input/train.csv\", index_col=0)\n", "target = trn_df[\"target\"]\n", "del trn_df[\"target\"]\n", "\n", "train_features = [\n", "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n", "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n", "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n", "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n", "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n", "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n", "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n", "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n", "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n", "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n", "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n", "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n", "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n", "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n", "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n", "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n", "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n", "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n", "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n", "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n", "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n", "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n", "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n", "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n", "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n", "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n", "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n", "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n", "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n", "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n", "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n", "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n", "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n", "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n", "]\n", "    \n", "trn_df = trn_df[train_features]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d5ff6720cee65bd1eb5e4189433a1c29099086ee", "_cell_guid": "5c407b57-95eb-4010-bddc-e83547fcfa8a"}, "source": ["###\u00a0Let's have a look at the evolution of predictions with scale_pose_weight"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ac4b44873f00fe25c295c75947fd41bd8b019309", "_cell_guid": "2c0faf80-6ac5-49a6-af7c-7f6d213d0846", "collapsed": true}, "outputs": [], "source": ["scale_pos_weights = range(1, 20, 2)\n", "oof_proba = np.empty((len(trn_df), len(scale_pos_weights)))\n", "oof_label = np.empty((len(trn_df), len(scale_pos_weights)))\n", "for i_w, scale_pos_weight in enumerate(scale_pos_weights):\n", "    n_splits = 2 # That's enough to get a feel on what's going on\n", "    n_estimators = 100\n", "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=14) \n", "\n", "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n", "        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n", "        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n", "\n", "        clf = LGBMClassifier(n_estimators=n_estimators,\n", "                             max_depth=-1,\n", "                             num_leaves=25,\n", "                             learning_rate=.1, \n", "                             subsample=.8, \n", "                             colsample_bytree=.8,\n", "                             min_split_gain=1,\n", "                             reg_alpha=0,\n", "                             reg_lambda=0,\n", "                             scale_pos_weight=scale_pos_weight, # <= We do not overweight positive samples\n", "                             n_jobs=2)\n", "\n", "        clf.fit(trn_dat, trn_tgt, \n", "                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n", "                eval_metric=\"auc\",\n", "                early_stopping_rounds=None,\n", "                verbose=False)\n", "\n", "        oof_proba[val_idx, i_w] = clf.predict_proba(val_dat)[:, 1]\n", "        oof_label[val_idx, i_w] = clf.predict(val_dat)\n", "        \n", "    print(\"Full OOF score : %.6f for scale_pos_weight = %2d\" \n", "          % (eval_gini(target, oof_proba[:, i_w]), scale_pos_weight))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f11adf537b8683dd446ca169dce96751e956eb19", "_cell_guid": "3b75729c-2d32-48ac-8e3a-cb23d0b27f6b"}, "source": ["### Now let's look at the evolution of f1_scores against scale_pos_weight\n", "We can see that probabilities are more spread into [0, 1] space when scale_pos_weight increase"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "34fed3b8a3e213ac7302287387b5b3dd5762e11c", "_cell_guid": "72e09f2a-8294-4712-8d21-472717bc1bfe", "collapsed": true}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(15, 10))\n", "plt.rc('legend', fontsize=18) \n", "plt.rc('axes', labelsize=18)\n", "plt.rc('axes', titlesize=24)\n", "for i_w, scale_pos_weight in enumerate(scale_pos_weights):\n", "    # Get False positives, true positives and the list of thresholds used to compute them\n", "    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n", "    # Compute recall, precision and f1_score\n", "    recall = tpr\n", "    precision = tpr / (fpr + tpr + 1e-5)\n", "    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n", "    # Finally plot the f1_scores against thresholds\n", "    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n", "             label=\"scale_pos_weight=%2d\" % scale_pos_weight)\n", "plt.title(\"F1 scores against threshold for different scale_pos_weight\")\n", "plt.ylabel(\"F1 score\")\n", "plt.xlabel(\"Probability thresholds\")\n", "plt.legend(loc=\"lower left\")\n", "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b4396821e4bb6d39f612ee8311f691ffa4cb8410", "_cell_guid": "4fa40a79-86f3-4bc6-9d14-c7bd7b4ecad7"}, "source": ["###\u00a0Confusion matrices\n", "To see the way probabilities spread more when scale_pos_weight increase let's display the evolution of the confusion matrices "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "185f6258ef88724f4a40063cdcc19277c97c7b29", "_cell_guid": "4be31532-81c7-4300-aaf6-1d35eff032bc", "collapsed": true}, "outputs": [], "source": ["fig = plt.figure(figsize=(20, 15))\n", "plt.rc('legend', fontsize=10) \n", "plt.rc('axes', labelsize=10)\n", "plt.rc('axes', titlesize=10)\n", "gs = gridspec.GridSpec(int(len(scale_pos_weights) / 2), 2)\n", "for i_w, weight in enumerate(scale_pos_weights):\n", "    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n", "    class_names = [\"safe\", \"unsafe\"]\n", "    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n", "    plot_confusion_matrix(cnf_matrix, classes=class_names, \n", "                          normalize=True,\n", "                          title='Matrix for scale_pos_weight = %2d, Gini %.6f' \n", "                          % (weight, eval_gini(target, oof_proba[:, i_w])))\n", "plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "06087f1d313bda68c0b5fe2c13366a1e0a2e479f", "_cell_guid": "305cdf44-34a6-438a-aab6-10630d37b268"}, "source": ["As you can see we have more and more true positives while false negative increase as well. \n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f4c0d2b17519058e01903f7299707bb52dcb4bfc", "_kg_hide-input": false, "_cell_guid": "fdf00c9f-69ae-4c8c-955d-6420ce57c6e2", "collapsed": true}, "source": ["### What about duplication ?"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "263807436fd025bd40760b694a5b5bd53b0e41f1", "_cell_guid": "dbc2ad3e-95f4-4e92-bd6f-5da826bb3d81"}, "outputs": [], "source": ["dupes = np.arange(0.5, 3.1, .5)\n", "oof_proba = np.empty((len(trn_df), len(dupes)))\n", "oof_label = np.empty((len(trn_df), len(dupes)))\n", "for i_w, dupe in enumerate(dupes):\n", "    n_splits = 2 # That's enough to get a feel on what's going on\n", "    n_estimators = 100\n", "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=14) \n", "\n", "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n", "        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n", "        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n", "\n", "        clf = LGBMClassifier(n_estimators=n_estimators,\n", "                             max_depth=-1,\n", "                             num_leaves=25,\n", "                             learning_rate=.1, \n", "                             subsample=.8, \n", "                             colsample_bytree=.8,\n", "                             min_split_gain=1,\n", "                             reg_alpha=0,\n", "                             reg_lambda=0,\n", "                             scale_pos_weight=1,\n", "                             min_child_weight=1,\n", "                             n_jobs=2)\n", "        # Duplicate positives on the training part\n", "        pos = pd.Series(trn_tgt == 1)\n", "        pos_dat = trn_dat.loc[pos]\n", "        pos_tgt = trn_tgt.loc[pos]\n", "        pos_idx = np.arange(len(pos_tgt))\n", "        if dupe <= 1.0: \n", "            # Add positive examples\n", "            np.random.shuffle(pos_idx)\n", "            trn_dat = pd.concat([trn_dat, pos_dat.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n", "            trn_tgt = pd.concat([trn_tgt, pos_tgt.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n", "        else:\n", "            dupint = math.floor(dupe)\n", "            remain = dupe - dupint\n", "            for i in range(dupint):\n", "                trn_dat = pd.concat([trn_dat, pos_dat], axis=0)\n", "                trn_tgt = pd.concat([trn_tgt, pos_tgt], axis=0)\n", "            np.random.shuffle(pos_idx)\n", "            trn_dat = pd.concat([trn_dat, pos_dat.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n", "            trn_tgt = pd.concat([trn_tgt, pos_tgt.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n", "        print(len(trn_dat), len(pos_dat))\n", "        # Shuffle data\n", "        idx = np.arange(len(trn_dat))\n", "        np.random.shuffle(idx)\n", "        trn_dat = trn_dat.iloc[idx]\n", "        trn_tgt = trn_tgt.iloc[idx]\n", "        \n", "        clf.fit(trn_dat, trn_tgt, \n", "                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n", "                eval_metric=\"auc\",\n", "                early_stopping_rounds=None,\n", "                verbose=False)\n", "\n", "        oof_proba[val_idx, i_w] = clf.predict_proba(val_dat)[:, 1]\n", "        oof_label[val_idx, i_w] = clf.predict(val_dat)\n", "\n", "    print(\"Full OOF score : %.6f for duplication %.1f\" \n", "          % (eval_gini(target, oof_proba[:, i_w]), dupe))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's check how confusion matrices are affected\n", "Please remember labels are computed using a .5 threshold"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "87e9b481b0fac1f7909265de52c11ecc2ed0c008", "_kg_hide-input": false, "_cell_guid": "a72e47ef-2a65-4f60-a4de-de665606a212"}, "outputs": [], "source": ["fig = plt.figure(figsize=(20, 15))\n", "plt.rc('legend', fontsize=10) \n", "plt.rc('axes', labelsize=10)\n", "plt.rc('axes', titlesize=10)\n", "gs = gridspec.GridSpec(math.ceil(len(dupes) / 2), 2)\n", "for i_w, weight in enumerate(dupes):\n", "    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n", "    class_names = [\"safe\", \"unsafe\"]\n", "    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n", "    plot_confusion_matrix(cnf_matrix, classes=class_names, \n", "                          normalize=False,\n", "                          title='Matrix for duplication = %.1f, Gini %.6f' \n", "                          % (weight, eval_gini(target, oof_proba[:, i_w])))\n", "plt.tight_layout()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d9b7a45cceb1e9dbaf1f2b85e9ddd02aabb4a4c2", "_cell_guid": "4bcfc850-caeb-4dac-ba0a-1f4bd0fdfcce"}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(15, 10))\n", "plt.rc('legend', fontsize=18) \n", "plt.rc('axes', labelsize=18)\n", "plt.rc('axes', titlesize=24)\n", "for i_w, dupe in enumerate(dupes):\n", "    # Get False positives, true positives and the list of thresholds used to compute them\n", "    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n", "    # Compute recall, precision and f1_score\n", "    recall = tpr\n", "    precision = tpr / (fpr + tpr + 1e-5)\n", "    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n", "    # Finally plot the f1_scores against thresholds\n", "    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n", "             label=\"duplication rate x %.1f\" % dupe)\n", "plt.title(\"F1 scores against threshold for different duplication rate\")\n", "plt.ylabel(\"F1 score\")\n", "plt.xlabel(\"Probability thresholds\")\n", "plt.legend(loc=\"lower left\")\n", "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["I must say this is a strange plot with lots of overlap. I'm really puzzled by this...\n", "\n", "At least we can see that you really need a strong duplication rate to spread probabilities the way scale_pos_weight does. As far as I'm concerned I will probably go the scale_pos_weight way. If you plan to merge predictions you will have to make sure they span approximately the same way in the [0, 1] space, all the more if you use geometric or harmonic mean...  "]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}