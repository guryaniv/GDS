{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "c52c10c6655a3343e151d3908cb484604d32e7b1", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "So, I decided to determine chance for row to be outlier ( i.e. label greater than some high percentile ). In order to do that, I've built classifier based on naive Bayes principle, stacked on feature selector - logistic regression equipped with L1 penalty"}, {"metadata": {"_uuid": "bda1d984e915d3754b928711337c2b8e6d77b1bc", "_execution_state": "idle"}, "outputs": [], "cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ny = train.pop('y')\nID = train.pop('ID')", "execution_count": 1}, {"metadata": {"_uuid": "4b4fce846894163f8e4a19578a22fd65768f2bbb", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.linear_model import RandomizedLogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline, make_union"}, {"metadata": {"_uuid": "911432118cea926d44fda4f1a84a9866d91ff351", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')"}, {"metadata": {"_uuid": "0a97ba55fadc3d4cc2d9ce75157743a1878ee68a", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix"}, {"metadata": {"_uuid": "118e3cc8c0dfc4231e8666567d2ec6e3341a3af0", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "ints = train.select_dtypes(['int']).columns.tolist()\nobjs = train.select_dtypes(['object']).columns.tolist()\n\nfor col in ints:\n    if np.var(train[col])==0:\n        train.pop(col)\n        ints.remove(col)\n"}, {"metadata": {"_uuid": "599604d0076bd172e5dfc737e23463f910f3245a", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "outs = (y>120).as_matrix().astype(int)\n\n#let's define outliers as labels greater than 120"}, {"metadata": {"_uuid": "bd19e216b2aed79c4456b97d3f9697c3d96e4cb9", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "def evaluate(y_true, pred, thresh=.5):\n    print('precision', precision_score(y_true, pred[:, 1]>thresh))\n    print('recall', recall_score(y_true, pred[:, 1]>thresh))\n    print('roc', roc_auc_score(y_true, pred[:, 1]))\n    print('f1', f1_score(y_true, pred[:, 1]>thresh))"}, {"metadata": {"_uuid": "069de2e33ce5c7fdbed09f6515f3b0ab3394520f", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "cv_preds = cross_val_predict(BernoulliNB(), train[ints], outs, cv=10, method='predict_proba')"}, {"metadata": {"_uuid": "c58abec24b1187597330a0b5426cf4e38917bcce", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "evaluate(outs, cv_preds)"}, {"metadata": {"_uuid": "f0d96767479fe17c4d53642df834bec40adad6e6", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Okay, that's quite bad. Let's include some feature selection pipeline"}, {"metadata": {"_uuid": "678fd5579829a22bbda3233f884d19d6feae35fe", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "pip = make_pipeline(RandomizedLogisticRegression(C=5), BernoulliNB())\n\nselection_preds = cross_val_predict(pip, train[ints], outs, cv=10, method='predict_proba')\nevaluate(outs, cv_preds)"}, {"metadata": {"_uuid": "8a0abcf140b93e94e36106a8e0f042e956cbe370", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "This takes quite long, so I've settled on C=5 ( intuition, possibly flawed ) and did not test any other hyperparameteres. We see that feature selection improves ROC, but hits f1. "}, {"metadata": {"_uuid": "3237a4dec588bc8db9f5f1434a791184bc220a0e", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Nonetheless, it's now time to perform some analysis of non-binary features. I will build transform that will decide whether to decode feature as promising or not, based on proportion of outliers associated with level of feature. "}, {"metadata": {"_uuid": "3304144d720286f3faf450ebb5b486e3d4ac7dda", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "class OutlierThresholder(TransformerMixin):\n    \n    def __init__(self, thresh=1.5):\n        self.th = thresh\n    \n    def fit(self, X, y):\n        \n        X = np.asarray(X)\n        maps = []\n        for col in range(X.shape[1]):\n            \n            val = X[:, col].copy()\n            useful = []\n            not_useful = []\n            for u in np.unique(X[:, col]):\n                \n                o, no = y[val==u].mean(), y[val!=u].mean()\n                q = o/no if no else 0\n                \n                if q > self.th:\n                    useful.append(u)\n                else:\n                    not_useful.append(u)\n                    \n            col_map = dict(zip(useful+not_useful, [0]*len(useful)+[1]*len(not_useful)))\n            maps.append(col_map)\n            \n        self.maps = maps\n        return self\n        \n    def transform(self, X, y=None):\n        \n        X = X.copy()\n        X = np.asarray(X)\n        for col in range(X.shape[1]):\n            \n            X[:, col] = [self.maps[col][x] if x in self.maps[col] else 1 for x in X[:, col]]\n            \n        return X"}, {"metadata": {"_uuid": "0ecc45f96425d044a9d446e850c18f4437aa91c7", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "def sel_obj(X):\n    return X[:, :8]\n\ndef sel_ints(X):\n    return X[:, 8:]"}, {"metadata": {"_uuid": "0bae03907bcf670d562468e7d45fe4f1a262e768", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "pip = make_pipeline(OutlierThresholder(), BernoulliNB())\n\noutlier_obj_preds = cross_val_predict(pip, train[objs], outs, method='predict_proba', cv=10)\nevaluate(outs, outlier_obj_preds)"}, {"metadata": {"_uuid": "6e1c26c5f33242b2df75266c10072af8a73bf6c7", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Very, very bad. Let's include binary features"}, {"metadata": {"_uuid": "b46cab8ec229e70c09dead9141e9a0f81b690bfe", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "un = make_union(make_pipeline(FunctionTransformer(sel_obj), OutlierThresholder()), FunctionTransformer(sel_ints))\n\nfor col in objs:\n    train[col] = pd.factorize(train[col])[0]\n\nbinary_with_obj = make_pipeline(un, BernoulliNB())"}, {"metadata": {"_uuid": "ebaffb4b2521a168607c32710101ec929432c644", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "full_preds = cross_val_predict(binary_with_obj, train, outs, method='predict_proba', cv=10)"}, {"metadata": {"_uuid": "57277db7ecffce8ef7d95b551e23a7725f00290d", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "evaluate(outs, full_preds)"}, {"metadata": {"_uuid": "fd4b0b789360adf7e14807a75d6667a8b973235b", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "comparable with model based solely on binary features. not worth the hassle"}, {"metadata": {"_uuid": "b3236493f9138af84b1489a8a50cabd3358e9d2f", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "upd_binary_with_obj = make_pipeline(un, RandomizedLogisticRegression(C=5), BernoulliNB())\n\nfull_upd_preds = cross_val_predict(upd_binary_with_obj, train, outs, method='predict_proba', cv=10)\nevaluate(outs, full_upd_preds)"}, {"metadata": {"_uuid": "c2d8ff4e4eb176e722b9634c99ad688fcef6c327", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Another bad score. Before I include these in final model, I would like to plot probability curves. Intuitively, we would like for our model to be n% right for every sample it assigns n% of confidence. Such curve will be called \"properly calibrated probability\". In case of such an event, we should see straight line of equation y=x on our plots"}, {"metadata": {"_uuid": "bf38e418bc87ca5c5737f260f31bceff7996d2db", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "import matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve"}, {"metadata": {"_uuid": "57a1b37769c0e81f921a02f84e68fdee449aea07", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "plt.plot(*calibration_curve(outs, full_upd_preds[:, 1], n_bins=5)[::-1])\nplt.xlabel('mean predicted probability')\nplt.ylabel('percent of correctly assigned labels')\nplt.show()"}, {"metadata": {"_uuid": "8004e1a689db9dcb06255b9903759e20ef788fbf", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "But we see that assumption does not hold. Let's check earlier models"}, {"metadata": {"_uuid": "d8aafeccb60819bb1e68ebd2b0891e4b805999ec", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "plt.plot(*calibration_curve(outs, outlier_obj_preds[:, 1], n_bins=5)[::-1])\nplt.xlabel('mean predicted probability')\nplt.ylabel('percent of correctly assigned labels')\nplt.show()"}, {"metadata": {"_uuid": "09cb618820870a960f4ee14c6e40658ed6c860a2", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "plt.plot(*calibration_curve(outs, cv_preds[:, 1], n_bins=5)[::-1])\nplt.xlabel('mean predicted probability')\nplt.ylabel('percent of correctly assigned labels')\nplt.show()"}, {"metadata": {"_uuid": "bc7fe579f68438cddaeaebe9340732a8e8e8be02", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "plt.plot(*calibration_curve(outs, selection_preds[:, 1], n_bins=5)[::-1])\nplt.xlabel('mean predicted probability')\nplt.ylabel('percent of correctly assigned labels')\nplt.show()"}, {"metadata": {"_uuid": "130ba312bf24d4e86a0086771c0fc4b751fbddba", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Let's build some xgboost models"}, {"metadata": {"_uuid": "9722619196fe45414a3368a36d078cfe39be1d73", "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "from xgboost import XGBRegressor\nfrom functools import partial\n\nxgb_params = dict(max_depth=3, learning_rate=0.05, n_estimators=100, subsample=.7, colsample_bytree=.7)\nxgbr = XGBRegressor(**xgb_params)\nmy_cv = partial(cross_val_score, scoring='r2', cv=10)\ncv_ordinary = my_cv(xgbr, train, y)\ncv_add = my_cv(xgbr, np.hstack([train, cv_preds[:, 1].reshape(-1, 1)]), y)"}, {"metadata": {"_uuid": "640f0626b53d1c926bc3ad7e0453b0d0d6a8591b", "_execution_state": "busy", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "cv_ordinary.mean(), cv_add.mean()"}, {"metadata": {"_uuid": "1244b5bd1abd255e00dfc8a48e368251a9a72610", "_execution_state": "idle", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Doesn't look very helpful. On the other hand, I didn't put a lot of effort into choosing hyperparameters.\n\n( for some reason I two cells above won't run. On my computer results are ~ 0.57 with second one being slightly worse )"}]}