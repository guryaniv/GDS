{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"source": ["# Introduction:\n", "\n", "Although we typically represent text as strings -- sequences of words which are, in turn, sequences of characters-- real sentences often possess complicated non-sequential structures. A sentence may be built up of clauses, which are in turn composed of phrases, which are themselves constructed from words. Thus, it often makes more sense to think of sentences as *trees* rather than chains.\n", "\n", "The structure of these trees is dictated by the *grammar* of English -- a collection of rules about how smaller structures can be combined to form larger ones. For example, English's grammar allows an adjective to modify a noun to form a noun phrase. A noun phrase may be combined with a prepositional phrase to form a yet larger noun phrase, which might itself act as the object of a verb in a verb phrase, which might act as the predicate of a clause, and so on.\n", "\n", "An author's 'style' can be viewed as the manner in which he or she uses the grammar of a language to convey a message: how many, and what kinds of phrases does he use? What kinds of sub-phrases are those phrases composed of? What words does she use to build those phrases? And, of course, sometimes authors will violate the \"standard\"  grammatical rules, combining words and phrases in atypical ways. This is also part of an author's style.\n", "\n", "In this notebook, we will construct a model designed to account for such choices.\n", "\n", "# Model: \n", "\n", "Below is a syntax tree generated from the first sentence in test.csv using the Stanford natural language parser. \n", "\n", "![https://i.imgur.com/UPqK9mR.png](https://i.imgur.com/UPqK9mR.png)\n", "\n", "Any such structure can be transformed into a binary tree -- one in which each node has at most two branches. In natural language processing, this is known as *Chomsky Normal Form*. \n", "\n", "![https://i.imgur.com/Sc9qQkD.png](https://i.imgur.com/Sc9qQkD.png)\n", "\n", "We will use trees in this form to construct our classifier.\n", "\n", "The assumptions of our model are as follows\n", "\n", "1. An author constructs a sentence starting from an S (sentence) node and works his or her way down. \n", "\n", "2. With two exceptions, described in 4), each node branches into exactly two child nodes, i.e. each structure within a sentence is composed of two sub-structures.\n", "\n", "3. The probability that a particular child will be chosen on a given branch for a given parent depends on three things: \n", "\n", "    a) The content of the parent node (e.g. prepositional phrases often occur within noun phrases).\n", "    \n", "    b) The direction of the branch (left or right -- e.g. maybe prepositional phrases are more likely to be found on the right branch of noun phrases).\n", "    \n", "    c) The author of the sentence (e.g. when deciding how to fill the right branch of a noun phrase, perhaps Shelley likes to use prepositional phrases, while Poe prefers adverbs)\n", "\n", "4. There are two ways in which a branch of the tree may terminate:\n", "    \n", "    a) There is a class of nodes that corresponds to parts of speech (nouns, verbs, particular punctuation marks, e.g. commas, etc.) Such nodes always have only a single child, corresponding to an individual word or character. Individual words or characters are always 'leaves' of the tree, and have no children themselves.\n", "\n", "    b) There is a special leaf node called 'END', which represents an author's decision to simply not place any child node on a branch. For example, if an author writes a noun phrase consisting of only a single noun, we will represent this as a noun phrase branching into a noun node and an END node.\n", "    \n", "5. The branching process described above continues until every branch of the tree ends in a leaf node.\n", "\n", "In essence, we will be modeling each sentence in our train and test sets as a tree-shaped [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network). A Bayesian network is a model that represents dependencies between random variables as a graph. Each edge is directed (it points *from* a parent node *to* a child node) and represents the probability distribution of its child node conditioned on its parent node.\n", "\n", "In this case, our random variables are components of a sentence's syntax tree -- words, phrases, etc. We will assume that each such object is chosen randomly from a distribution that depends on the three factors listed in 3. Attached to each branch of the syntax tree is a conditional probability -- the probability P(c|d,p,a) that the branch's child would have been 'c', given that the branch's direction was 'd', its parent was 'p', and the author of the sentence was 'a'. These conditional probabilities, along with the prior probabilities P(a),will form the parameters of our model \n", "\n", "When given a tree to identify, the probability associated with each author will simply be the product of P(c|d,p,a) over all nodes in the tree, multiplied by the prior probability of a, divided by a normalization constant (more on this in the \"Generating Predictions\" section).\n", "\n", "Obviously, the process above isn't actually how sentences are written. However, this model automatically takes into account three very important parts of an author's style: sentence structure, sentence length (modeled by an author's propensity for choosing END nodes or part-of-speech nodes versus phrase or clause nodes), and word choice (modeled by the probability distribution of leaf nodes for each part of speech).\n", "\n", "# Code:\n", "\n", "## Tree generation:\n", "\n", "To start, we will need to generate a tree for every sentence in the train and test datasets. We can do this using Stanford's Natural Language Parser. Although NLTK has methods for interfacing with the parser, I do not believe it can be run on Kaggle, as the methods require access to the parser's jar files, which must be downloaded from [the Stanford Natural Language Processing Groups's page](https://nlp.stanford.edu/software/lex-parser.shtml).\n", "\n", "As such, I have uploaded two pickle files, one for the training data (train_trees.p) and one for the test data (test_trees.p). Each pickle file contains a dictionary with a tree for (almost) every sentence its respective data set. There were a few sentences that were too long to process--the parser's memory requirements scale quadratically with sentence length--so some entries in the dictionaries have values of \"None\". This amounts to about eight sentences in the training set and about three in the test set.\n", "\n", "The code used to generate the trees is below (commented out). If you wish to run it on your computer, you will have to download the Stanford parser and change the path names at the top of the code accordingly. Be aware that it will take at least several hours to run to completion."], "metadata": {"_cell_guid": "34916613-f40e-4595-8ab1-36ae5aaa4a47", "_uuid": "935de377e3d40bb0cbded16dfb1cf3cc71c2296a"}, "cell_type": "markdown"}, {"source": ["'''\n", "import time\n", "import os\n", "import pickle\n", "import pandas as pd\n", "from nltk.internals import find_jars_within_path\n", "\n", "pathstring = \"C:/Users/Nathaniel/Desktop/stanford-parser-full-2017-06-09\"\n", "os.environ[\"STANFORD_MODELS\"] = pathstring + \"/stanford-parser-3.8.0-models.jar\"\n", "os.environ[\"STANFORD_PARSER\"] = pathstring + \"/stanford-parser.jar\"\n", "os.environ[\"JAVAHOME\"] = \"C:/Program Files/Java/jdk-9.0.1\"\n", "os.environ[\"CLASSPATH\"] = pathstring\n", "\n", "input_file = 'test.csv'\n", "output_file = \"test_trees.p\"\n", "\n", "from nltk.parse import stanford\n", "import nltk as nl\n", "parser = nl.parse.stanford.StanfordParser(model_path=(pathstring + \"/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"),java_options='-mx16000m')\n", "parser._classpath = tuple(find_jars_within_path(pathstring))\n", "data = pd.read_csv(input_file)\n", "data['tokens'] = data['text'].map(nl.word_tokenize)\n", "t = time.clock()\n", "treelist = dict()\n", "nones = 0\n", "for i, sent in enumerate(data['text']):\n", "    l = len(data['tokens'][i])\n", "    if l < 200:\n", "        treelist[i] = next(next(parser.raw_parse_sents([sent])))    \n", "    else:\n", "        treelist[i] = None\n", "        nones = nones + 1\n", "        print(l, end=' ')\n", "    print(i, \": \", time.clock() - t)\n", "print(nones)\n", "\n", "with open(output_file,\"wb\") as filehandle:\n", "    pickle.dump(treelist,filehandle)\n", "    \n", "'''"], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b965456a-a28e-44a2-92bb-c0fe8d2ef42e", "_uuid": "5d540f7c437b089580bc056b1fc83f45209ad913", "_kg_hide-input": true, "_kg_hide-output": true}, "outputs": [], "cell_type": "code"}, {"source": ["## Model Training:\n", "\n", "Now that we have our trees, we'll convert them into Chomsky Normal Form and generate 'count' matrices in the form of pandas DataFrames. The entries in each sentence's matrix count the number of times each parent-child-direction combination occurs within that sentence. So, for example, if there are four occasions in a sentence where a noun phrase contains an adjective on its left branch, there will be a number '4' in the row labeled (NODE, JJ) and column labeled (NP, L). Here, JJ stands for 'Adjective' and 'NP' stands for 'Noun Phrase'. [This page contains links to documentation for all tags used by the Stanford parser.](https://nlp.stanford.edu/software/parser-faq.shtml#c)\n", "\n", "A precise description of the matrix is as follows:\n", "\n", "The columns of the DataFrames are a multi-index, with the top index corresponding to parent nodes, and the bottom index corresponding to directions (Left, Right, and Center for nodes that only have a single child).\n", "\n", "The rows of this dataframes are also a multi-index. The inner index corresponds to child nodes, while the outer index, which exists mostly for convenience, denotes whether or not the inner index corresponds to a leaf.\n", "\n", "The value in the field with row index C, top column index P, and lower column index D denotes the number of times a parent node of type P connects to a child node of type C along direction D. \n", "\n", "Below is one such matrix, generated from the first sentence in the test dataset -- the same sentence modeled by the two trees above."], "metadata": {"_cell_guid": "32e0c881-d382-4a3f-aa82-a6480ddd8864", "_uuid": "bd6433a2a85b8e1441ecff216b52d13aadc88249"}, "cell_type": "markdown"}, {"source": ["import pickle\n", "int_outputs = \"../input/intermediate-outputs\"\n", "\n", "with open(int_outputs+\"/test_matrices_all.p\", \"rb\") as filehandle:\n", "    test_matrices = pickle.load(filehandle)"], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9a5978b2-0c83-4cdd-856a-2acad785ce2a", "_uuid": "b6368cf7f11854a0afddc7692a65cce7d99d819b", "_kg_hide-input": true}, "outputs": [], "cell_type": "code"}, {"source": ["test_matrices[0]"], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7eedfb98-2482-40aa-9a48-9618b7262082", "_uuid": "30af3fdba61f6f5eee8d6cf8b2fd098176a24613", "_kg_hide-input": true}, "outputs": [], "cell_type": "code"}, {"source": ["A small amount of processing is applied to the tree before generating such a matrix. First, words in the sentence are 'stemmed', i.e. converted to a root form. For example, the word \"leaving\" is replaced with \"leav.\" This destroys some information, such as tense. However, by only stemming after our sentences have been parsed into trees, we preserve information about a word's part of speech. Stemming helps to keep the size of the matrix managable.\n", "\n", "Second, conversion to Chomsky Normal Form produces many 'composite' tags. An example in the tree above would be VP<\\NP-PP>, which represents a portion of a verb phrase consisting of a noun phrase and a prepositional phrase. In generating the matrix above, VP<\\NN-PP> is simply replaced with VP++. Again, some information is lost but this helps to keep the size of the matrix managable.\n", "\n", "Such a matrix is generated for every tree in the training and test data. The matrices for the training data are summed together into 3+1 \"master\" matrices, one for each author plus an even larger matrix that is the sum of the other three, while the matrices for testing data are kept separate. Below is code for generating these matrices."], "metadata": {"_cell_guid": "2b0d5f12-b2df-43d6-a9e1-8b4ee8fd0512", "_uuid": "7d000ce4c58516bdc34f7f7e37c2a3939d673018"}, "cell_type": "markdown"}, {"source": ["def get_rels_ch3(tree, stemmer=None):\n", "    '''takes a single tree and an optional stemmer, returns a matrix as a pandas DataFrame'''\n", "    tree.chomsky_normal_form()\n", "    tree = nl.ParentedTree.convert(tree)#ParentedTree allows us to access a node's parents\n", "    start_rows = pd.MultiIndex.from_tuples([(\"LEAF\",\"_END\")])\n", "    start_cols = pd.MultiIndex.from_tuples([(\"ROOT\", 'R')])\n", "    child_par = pd.DataFrame([0],index=start_rows, columns=start_cols)#Dataframe to return\n", "    \n", "    for sub in tree.subtrees():\n", "        par = sub.parent()\n", "        if par:\n", "            p_ind = sub.parent_index()\n", "            par_label = par.label()\n", "            child_label = sub.label()\n", "            \n", "            par_label = re.sub(r\"(\\|[^-]*)|(-[^-]*)\",\"+\",par_label)#simplify composite labels\n", "            child_label = re.sub(r\"(\\|[^-]*)|(-[^-]*)\",\"+\",child_label)#like VP<NN-PP> to VP++\n", "            \n", "            new_cols = pd.MultiIndex.from_tuples([(par_label, 'L'), (par_label, 'R')])\n", "            new_row = pd.MultiIndex.from_tuples([(\"NODE\",child_label)])\n", "            new_entry = pd.DataFrame([[1-p_ind, p_ind]],index=new_row, columns=new_cols)\n", "            \n", "            child_par = child_par.add(new_entry, fill_value=0)\n", "            \n", "            if not sub.left_sibling() and not sub.right_sibling():\n", "                child_par.loc[(\"LEAF\",\"_END\"),(par_label,'R')] +=1\n", "        \n", "            if sub.height() == 2:#nltk does not consider leaves to be subtrees\n", "                for leaf in sub.leaves():#so we must handle them separately\n", "                    if stemmer:\n", "                        leaf = stemmer.stem(leaf)\n", "                    leaf = pd.MultiIndex.from_tuples([(\"LEAF\",leaf.lower())])\n", "                    leaf_par = pd.MultiIndex.from_tuples([(child_label, \"C\")])\n", "                    \n", "    \n", "                    new_leaf = pd.DataFrame([1],index=leaf, columns=leaf_par)\n", "                    child_par = child_par.add(new_leaf, fill_value=0)\n", "            return child_par.to_sparse()"], "execution_count": null, "metadata": {"_cell_guid": "4df99d42-06b8-497a-8d78-33231b8a3109", "_uuid": "150986fbf6f58e48b56c64ffefcbecbe89753d44", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["def get_counts(column, trees,stem = True):\n", "    '''takes a pandas series, like the author column of the training data, and a list or dictionary of trees.\n", "    the column is used for identifying who each sentence belongs to, and must line up with the list/dict indices\n", "    returns the sum of all matrices generated from the list of trees\n", "    '''\n", "    t1 = time.clock()\n", "    \n", "    stemmer = None\n", "    if stem: stemmer = nl.stem.snowball.SnowballStemmer(\"english\")\n", "        \n", "    start_rows = pd.MultiIndex.from_tuples([(\"LEAF\",\"_END\"), (\"LEAF\",\"_OTHER\"),(\"NODE\",\"_OTHER\")])\n", "    start_cols = pd.MultiIndex.from_tuples([(\"_OTHER\", 'L'), (\"_OTHER\", 'R'),(\"_OTHER\", 'C')])\n", "        \n", "    categories = column.unique()\n", "    counts = dict()\n", "    \n", "    for cat in categories: #build an empty matric for each category (author)\n", "        counts[cat] = pd.SparseDataFrame(index=start_rows, columns=start_cols)\n", "    counts['TOTAL'] = pd.SparseDataFrame(index=start_rows, columns=start_cols)\n", "    for i in column.index:\n", "        tree = trees[i]\n", "        if tree:        \n", "            counts[column[i]] = counts[column[i]].add(get_rels_ch3(tree,stemmer), fill_value=0)\n", "        t2 = time.clock()\n", "        print(i,\":\",t2 - t1)#for gauging the speed of this function\n", "    for i in categories:\n", "        counts['TOTAL'] = counts['TOTAL'].add(counts[i], fill_value=0)\n", "    return counts"], "execution_count": null, "metadata": {"_cell_guid": "1a9ca266-1742-4c9d-a990-592811412213", "_uuid": "7742f0e3566f1507fe5829d1a2a6740e135bff1b", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["def get_counts2(column, trees,stem = True): #for getting matrices separately\n", "    t1 = time.clock()\n", "    stemmer = None\n", "    if stem: stemmer = nl.stem.snowball.SnowballStemmer(\"english\")  \n", "    counts = []    \n", "    for i in column.index:\n", "        tree = trees[i]\n", "        if tree:\n", "            counts_i = get_rels_ch3(tree,stemmer)\n", "            \n", "            counts.append(counts_i)\n", "        else:\n", "            counts.append(None)\n", "        t2 = time.clock()\n", "        print(i,\":\",t2 - t1)\n", "    return counts"], "execution_count": null, "metadata": {"_cell_guid": "de409085-b723-43f6-91e9-0e0af4d53730", "_uuid": "9346838ea98305b79be320cd9ae9acdf20e97ddb", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["I've uploaded four pickle files with matrices generated directly from sentence trees: train_matrices_master.p contains summed matrices generated from the first 16,000 sentences in the train csv, to be used for training our model. A list of separate matrices from the next 2,000 sentences is stored in train_matrices_calib.p. The remaining sentences from the training csv are stored in train_matrices_holdout.p, to be used for testing. Finally, all matrices from the testing csv are stored in test_matrices_all.p.\n", "\n", "\n", "\n"], "metadata": {"_cell_guid": "3d6ea0bf-388d-46e9-a6d6-9c611570affb", "_uuid": "c0395b4de96b433a5ccd4f4e7779fe21a372a39b"}, "cell_type": "markdown"}, {"source": ["## Generating Predictions:\n", "\n", "Now that we have all of these matrices, we can now begin to make predictions. From the giant summed matrices for each author, we can directly obtain the conditional probabilities P(c|d,p,a) mentioned earlier: P(c|d,p,a) is simply the entry in row c, column (d,p) of matrix a, divided by the sum of values in that column (subject to some Laplace smoothing).\n", "\n", "The probability associated with the entire tree T, given a, is\n", "\n", "$$P(\\textrm{T|a}) = \\prod_{c \\in T}^{} P(c|d,p,a)$$\n", "\n", "([This is a property of bayesian networks](https://en.wikipedia.org/wiki/Bayesian_network#Definitions_and_concepts). Our network was defined so as to satisffy the local Markov property conditioned on each author, so it also satisfies the above factorization property.)\n", "\n", "The probability that a particular tree was written by a is thus given by:\n", "\n", "$$P(\\textrm{a|T}) =  \\frac{P(T|a) P(a)}{P(T)} =   \\frac{P(a)}{P(T)}  \\prod_{c \\in T}^{} P(c|d,p,a)$$\n", "$$ \\propto  P(a) \\prod_{c \\in T}^{} P(c|d,p,a)$$\n", "\n", "For the purposes of predicting an author, P(T) is merely a constant normalization factor.\n", "\n", "So, to figure out the odds that a particular author wrote a given sentence in the test set, we'll do the following: \n", "\n", "1. Generate a matrix for the sentence. We'll call this matrix S.\n", "2. Pull up the master matrix A for the author in question.\n", "3. For each nonzero value $S_{c,p,d}$ in $S$, look up the corresponding value in A (i.e. the value in the same row and column) and calculate P(c|d,p,a)\n", "4. The contribution to the overall probability from this matrix entry is simply P(c|d,p,a) raised to the power $S_{c,p,d}$\n", "5. The overall probability that this author wrote this sentence is the product of such contributions over all nonzero $S_{c,p,d}$ (subject to a normalization factor)\n", "\n", "Below is a function that carries out the computations described above."], "metadata": {"_cell_guid": "9d8f95d5-0c76-4b68-9d9a-4d2725546543", "_uuid": "a623e92b95d5808a5eb889030848024619f1c427"}, "cell_type": "markdown"}, {"source": ["def get_probs(train_dict,test_list,ids,weights,smoothing=0.5):\n", "    t0=time.clock()\n", "    train_cats = {i:v.to_dense().fillna(0) for (i, v) in train_dict.items() if i != \"TOTAL\"}#training 'master' matrix associated with each category\n", "    train_total = train_dict[\"TOTAL\"].to_dense().fillna(0)\n", "    \n", "    all_rows = train_total.index\n", "    all_columns = train_total.columns\n", "    \n", "    results = pd.DataFrame(index=ids.index, columns=['id']+[i for i in train_cats],dtype=float)\n", "    \n", "    for (n,test_mat) in enumerate(test_list):#for each matrix to be tested in test list\n", "        cat_probs = pd.Series({i:1.0 for i in train_cats})\n", "        if test_mat is not None:\n", "            test_matrix = test_mat.to_dense().fillna(0)\n", "            nonzeros = test_matrix[test_matrix>0].stack().stack()#convert to a single multi-indexed column listing all nonzero entries\n", "            for (j, count) in nonzeros.iteritems():#for each nonzero value in matrix\n", "\n", "                row=(j[0],j[1] if (j[0],j[1]) in all_rows else \"_OTHER\")\n", "\n", "                column = (j[3] if (j[3],j[2]) in all_columns else \"_OTHER\", j[2])\n", "                column_total_nonzeros = train_total[column][train_total[column] >0].size\n", "                if column_total_nonzeros == 0: #if column is empty for all authors, shift to 'other' column\n", "                    column = (\"_OTHER\", j[2])\n", "                    column_total_nonzeros = train_total[column][train_total[column] >0].size\n", "                \n", "                for (cat,train_matrix) in train_cats.items():\n", "                    numerator = smoothing\n", "\n", "                    denominator = (column_total_nonzeros+1)*smoothing\n", "\n", "                    if (column in train_matrix.columns and column_total_nonzeros >=2):\n", "                        denominator = denominator + train_matrix.loc[:,column].sum()\n", "                        if (row in train_matrix.index):\n", "                            numerator = numerator +train_matrix.loc[row,column]\n", "\n", "                    cat_probs[cat] = cat_probs[cat]*pow(numerator / denominator,count)\n", "                    \n", "                cat_probs = cat_probs/cat_probs.sum()\n", "\n", "        cat_probs=cat_probs * weights       \n", "        results.loc[ids.index[n]] = cat_probs/cat_probs.sum()\n", "        print(n,\":\",time.clock() - t0)\n", "    \n", "    results['id'] = ids\n", "    \n", "    return results"], "execution_count": null, "metadata": {"_cell_guid": "1abdbd29-4f18-4d05-8f29-354ce1ffb9a8", "_uuid": "a17d3a89c47035cdd38ad27f1716cffaa6c2a506", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["The parameters of the function above are \n", "\n", "1: A dictionary containing matrices as generated by the get_counts function. The indices of this matrix should correspond to the classification categories, with a last matrix labeled 'TOTAL'. So in this case, the indices would be \"EAP\", \"HPL\", \"MWS\", and \"TOTAL\"\n", "\n", "2: A list of matrices corresponding to sentences to be classified, as generated by get_counts 2\n", "\n", "3: A column (pandas Series) of labels with the same length as the list from 2. This is used to create the index of and forms the first column of the output dataframe. For calibrating and holdout testing, we use the corresponding rows of the 'author' column in train.csv. For classifying new data, we use the 'id' column of test.csv\n", "\n", "4: A series containing counts of the number of sentences from each author in the training data, for calculating the prior probabilities P(a)\n", "\n", "5: An optional Laplace smoothing parameter. From testing, I find that a value of 0.5 works fairly well.\n", "\n", "At this point, we almost have our full classifier. However, in my tests I found that this function, on its own, tends to be 'overconfident' in its predictions. That is, it tends to produce probabilities that are close to 0 or 1 much more often than probabilities that are close to the author priors. \n", "\n", "[Naive Bayes classifiers also share this problem of overcondfidence](http://scikit-learn.org/stable/modules/calibration.html). This happens because Naive Bayes classifiers make very strong independence assumptions, which are not always borne out in reality. The result is that information shared between different features is essentially double-counted.\n", "\n", "Likewise, it may be that the indpendence assumptions in our model are the reason it is overconfident. In particular, we assumed that the label of a node depends *only* on its parent, direction, and author.\n", "\n", "Although overconfidence doesn't affect a model's accuracy, it does harm the model's log-loss metric. One solution, which is also used with naive Bayes classifiers, is to run the outputs of the first classifier through a regression model.\n"], "metadata": {"_cell_guid": "854ada39-4705-4e48-a371-e56ee99942aa", "_uuid": "d3252b285fb4199c4f6e565df97f142e92c7dd88"}, "cell_type": "markdown"}, {"source": ["def probs_regress(train_dict,test_list,ids,weights,smoothing=0.5, regressors={}):\n", "    probs = get_probs(train_dict,test_list,ids,weights,smoothing)\n", "    num_columns = probs.columns.drop('id')\n", "    for i, v in regressors.items():\n", "        probs[i] = regressors[i].predict(probs[i])\n", "    results_sum = probs[num_columns].sum(axis=1)\n", "    probs[num_columns] = probs[num_columns].divide(results_sum, axis=0)\n", "    return probs"], "execution_count": null, "metadata": {"_cell_guid": "de888fb9-837b-406f-8b8b-8ba6c3f82976", "_uuid": "fe9050787c231d4e8bf0c4afe57e487f8dc04609", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["This function has the same parameters as get_probs, with an added parameter called 'regressors'. It is meant to take the form of a dictionary containing one scikit-learn isotonic regressor per classification category (one per author in this case).\n", "\n", "Below is a function for training those regressors. It returns a dictionary as described above."], "metadata": {"_cell_guid": "bfa99646-a3bc-45cf-9a09-3c151a4fcf28", "_uuid": "665b7d90100663367d41ee0106657cec8c3525f7"}, "cell_type": "markdown"}, {"source": ["def calibrate_reg(train_dict, calib_set,id_calib, weights, smoothing=0.5):\n", "    \n", "    raw_results = probs_regress(train_dict,calib_set,id_calib,weights,smoothing)\n", "    raw_results_n = pd.DataFrame(index = raw_results.index, columns = ['EAP','HPL','MWS'])\n", "\n", "    for i,v in raw_results['id'].iteritems():#generates a dataframe of 'correct' results\n", "        conversion = {'EAP':[1,0,0], 'HPL':[0,1,0], 'MWS':[0,0,1]}\n", "        raw_results_n.loc[i,:] =conversion[v]\n", "    \n", "    regressors = {}\n", "    for i,v in raw_results_n.iteritems():\n", "        ir = sk.isotonic.IsotonicRegression(out_of_bounds = 'clip')\n", "        regressors[i] = ir.fit(raw_results[i], raw_results_n[i])\n", "         \n", "    return regressors"], "execution_count": null, "metadata": {"_cell_guid": "442fa5d2-7d0a-4364-8df4-311918b50121", "_uuid": "f37c7decd51337188f5cef2fef6ba7f8c04b483b", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["Finally, putting everything together, we're ready to start classifying. Below, the matrices and regressors used have been extracted from the pickle files uploaded with this kernel. However, example commands are included in comments showing how to generate these objects from the functions above."], "metadata": {"_cell_guid": "edd5f5d2-821e-46cc-a6b3-2ccdb2d1d99f", "_uuid": "9ddea9ef9da829c05e847d3ee98ed8d1ceae20ef"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import nltk as nl\n", "import sklearn as sk\n", "from sklearn import isotonic\n", "import pickle\n", "import time\n", "import re\n", "train_data = pd.read_csv('../input/spooky-author-identification/train.csv')\n", "test_data = pd.read_csv('../input/spooky-author-identification/test.csv')\n", "int_outputs = \"../input/intermediate-outputs\"\n", "\n", "#with open(int_outputs+\"/train_trees.p\", \"rb\") as filehandle:\n", "#    train_trees = pickle.load(filehandle)\n", "#with open(int_outputs+\"/test_trees.p\", \"rb\") as filehandle:\n", "#    test_trees = pickle.load(filehandle)\n", "\n", "#train_matrices_all = get_counts(test_data['id'], test_trees)    \n", "with open(int_outputs+\"/test_matrices_all.p\", \"rb\") as filehandle:\n", "    test_matrices_all = pickle.load(filehandle)\n", "\n", "#train_matrices_master = get_counts(train_data['author'][:16000], train_trees)    \n", "with open(int_outputs+\"/train_matrices_master.p\", \"rb\") as filehandle:\n", "    train_matrices_master = pickle.load(filehandle)\n", "\n", "#train_matrices_calib = get_counts2(train_data['author'][16000:18000], train_trees)    \n", "with open(int_outputs+\"/train_matrices_calib.p\", \"rb\") as filehandle:\n", "    train_matrices_calib  = pickle.load(filehandle)\n", "\n", "#train_matrices_holdout = get_counts2(train_data['author'][18000:], train_trees)\n", "with open(int_outputs+\"/train_matrices_holdout.p\", \"rb\") as filehandle:\n", "    train_matrices_holdout = pickle.load(filehandle)\n", "\n", "#isotonic_regs = calibrate_reg(train_matrices_master, train_matrices_calib + train_matrices_holdout,train_data['author'][16000:], train_data['author'][16000:].value_counts(), smoothing=0.5)\n", "with open(int_outputs+\"/isotonic_regs.p\", \"rb\") as filehandle:\n", "    isotonic_regs = pickle.load(filehandle)\n", "    \n", "\n", "    \n", "probs = probs_regress(train_dict=train_matrices_master,\n", "                      test_list = test_matrices_all,\n", "                      ids = test_data['id'],\n", "                      weights = train_data['author'][:16000].value_counts(),\n", "                      smoothing=0.5,\n", "                      regressors=isotonic_regs\n", "                     )    \n", "probs.to_csv('submission.csv',index=False)"], "execution_count": null, "metadata": {"_cell_guid": "010974ab-036c-4569-95d4-a8371b8598bb", "_uuid": "dcb8b02ebdf17df3696286ef8758356ade762313", "collapsed": true, "_kg_hide-output": true}, "outputs": [], "cell_type": "code"}, {"source": ["probs"], "execution_count": null, "metadata": {"_cell_guid": "eec51d11-fabc-4cc1-a8a1-f3e3fe0d4010", "_uuid": "ec86e5ff35f045ed452c47a6cb23b5f12d1b5000", "collapsed": true}, "outputs": [], "cell_type": "code"}, {"source": ["# Final Notes:\n", "\n", "In my tests, this model's predictions tend to be about 80-85% accurate, with log loss around 0.4. A laplace smoothing parameter of 0.1 instead of 0.5 actually seems to yield a slightly better log loss, at the cost of a small amount of accuracy. \n", "\n", "Initially, I was worried about this model overfitting the data, due to the very large number of features involved. As such, I created a function for merging 'rare' words or constructs into a label called 'other'. More specifically, it would take in a matrix, and search for rows whose sum fell below a certain threshold. The contents of these rows would be summed together to form an 'other' row, and the original rows would be dropped, and likewise for columns. \n", "\n", "The intent was that when classifying a sentence containing words or constructs that had never been seen before, the model would consult the 'other' row or column. In essence, the 'other' tag was meant to represent an author's propensity for using extremely uncommon words or structures.\n", "\n", "What I found was that using this function actually decreased the model's performance, with higher 'rarity' thresholds producing worse results. This suggests to me that the model is not overfitting, and that it could possibly handle an even larger feature space.\n", "\n", "Some possibilities might include leaving words unstemmed, being less aggressive when merging composite constructs like VP<\\NN-PP>, or possibly even modeling dependencies between left and right children of a node\n"], "metadata": {"_cell_guid": "9fb7a0bf-272a-40eb-b2cf-13cc13d46c9e", "_uuid": "e827c401d4da545df8205bf7bcf6cce2931ba3a6"}, "cell_type": "markdown"}, {"source": ["def prep2(train_dict, threshold):\n", "    train_cats = {i:v.to_dense().fillna(0) for (i, v) in train_dict.items() if i != \"TOTAL\"}\n", "    train_total = train_dict[\"TOTAL\"].to_dense().fillna(0)\n", "    \n", "    row_sums=train_total.sum(axis=1)#sum of rows\n", "\n", "    all_oth_rows = train_total.loc[row_sums < threshold,:].drop([(\"LEAF\", \"_OTHER\"),(\"NODE\", \"_OTHER\")],axis=0)\n", "    \n", "    all_oth_leaf = all_oth_rows.loc['LEAF']\n", "    all_oth_node = all_oth_rows.loc['NODE']\n", "    \n", "    all_oth_leaf_ind = pd.MultiIndex.from_product([[\"LEAF\"],all_oth_leaf.index])\n", "    all_oth_node_ind = pd.MultiIndex.from_product([[\"NODE\"],all_oth_node.index])\n", "      \n", "    train_total.loc[('LEAF', '_OTHER'),:] = all_oth_leaf.sum(axis=0)\n", "    train_total.loc[('NODE', '_OTHER'),:] = all_oth_node.sum(axis=0)\n", "    \n", "    train_total = train_total.drop(all_oth_leaf_ind, axis=0)\n", "    train_total = train_total.drop(all_oth_node_ind, axis=0)\n", "    \n", "    \n", "    all_oth_l_ind = train_total.columns.intersection(pd.MultiIndex.from_product([all_oth_node.index,[\"L\"]]))\n", "    all_oth_r_ind = train_total.columns.intersection(pd.MultiIndex.from_product([all_oth_node.index,[\"R\"]]))\n", "    all_oth_c_ind = train_total.columns.intersection(pd.MultiIndex.from_product([all_oth_node.index,[\"C\"]]))\n", "    \n", "    all_oth_l = train_total.loc[:,all_oth_l_ind]\n", "    all_oth_r = train_total.loc[:,all_oth_r_ind]\n", "    all_oth_c = train_total.loc[:,all_oth_c_ind]\n", "    \n", "    \n", "    train_total.loc[:,('_OTHER', 'L')] = all_oth_l.sum(axis=1)\n", "    train_total.loc[:,('_OTHER', 'R')] = all_oth_r.sum(axis=1)\n", "    train_total.loc[:,('_OTHER', 'C')] = all_oth_c.sum(axis=1)\n", "    \n", "    train_total = train_total.drop(all_oth_l_ind, axis=1)\n", "    train_total = train_total.drop(all_oth_r_ind, axis=1)\n", "    train_total = train_total.drop(all_oth_c_ind, axis=1)\n", "    \n", "    for (i, matrix) in train_cats.items():\n", "        oth_leaf_ind = matrix.index.intersection(all_oth_leaf_ind)\n", "        oth_node_ind = matrix.index.intersection(all_oth_node_ind)\n", "        \n", "        oth_leaf = matrix.loc[oth_leaf_ind,:]\n", "        oth_node = matrix.loc[oth_node_ind,:]\n", "        \n", "        \n", "        train_cats[i].loc[('LEAF', '_OTHER'),:] = oth_leaf.sum(axis=0)\n", "        train_cats[i].loc[('NODE', '_OTHER'),:] = oth_node.sum(axis=0)\n", "        \n", "        train_cats[i] = train_cats[i].drop(oth_leaf_ind, axis=0)\n", "        train_cats[i] = train_cats[i].drop(oth_node_ind, axis=0)\n", "        \n", "        oth_l_ind = matrix.columns.intersection(all_oth_l_ind)\n", "        oth_r_ind = matrix.columns.intersection(all_oth_r_ind)\n", "        oth_c_ind = matrix.columns.intersection(all_oth_c_ind)\n", "\n", "        oth_l = matrix.loc[:, oth_l_ind]\n", "        oth_r = matrix.loc[:, oth_r_ind]\n", "        oth_c = matrix.loc[:, oth_c_ind]\n", "\n", "        train_cats[i].loc[:,('_OTHER', 'L')] = oth_l.sum(axis=1)\n", "        train_cats[i].loc[:,('_OTHER', 'R')] = oth_r.sum(axis=1)\n", "        train_cats[i].loc[:,('_OTHER', 'C')] = oth_c.sum(axis=1)\n", "        \n", "        \n", "        train_cats[i] = train_cats[i].drop(columns=oth_l_ind)\n", "        train_cats[i] = train_cats[i].drop(columns=oth_r_ind)\n", "        train_cats[i] = train_cats[i].drop(columns=oth_c_ind)\n", "        \n", "        \n", "    return {'TOTAL':train_total, **train_cats}"], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "e86622a7-784d-4c64-ad53-61dc00f3d424", "_uuid": "573a49150784c6041b182f3554f8c63761c9712f", "_kg_hide-input": true}, "outputs": [], "cell_type": "code"}]}