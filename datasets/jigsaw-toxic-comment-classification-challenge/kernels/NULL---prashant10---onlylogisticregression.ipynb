{"nbformat": 4, "cells": [{"outputs": [], "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics import log_loss\n", "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n", "from sklearn import svm\n", "import xgboost as xgb\n", "from sklearn.decomposition import TruncatedSVD\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "e5825421-3048-4e87-a859-a2248bb93a9a", "_uuid": "fbc088d4976520309d25ad1cb12357202841373f"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "sampleSubmission = pd.read_csv(\"../input/sample_submission.csv\")"], "metadata": {"_cell_guid": "6b3055c3-8a40-42fa-9666-462528bb5486", "collapsed": true, "_uuid": "bd012f975667e886ba1d61eb483fda2e4e691d9d"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "trainTxt = train['comment_text']\n", "testTxt = test['comment_text']\n", "trainTxt = trainTxt.fillna(\"unknown\")\n", "testTxt = testTxt.fillna(\"unknown\")\n", "combinedTxt = pd.concat([trainTxt,testTxt],axis=0)"], "metadata": {"_cell_guid": "179cc113-bfc6-4311-ac49-0dd1f61f6a04", "collapsed": true, "_uuid": "9c2ca37258371081c246c45267a5c2e8d260f17f"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["vect = TfidfVectorizer(decode_error='ignore',use_idf=True,smooth_idf=True,min_df=10,ngram_range=(1,3),lowercase=True,\n", "                      stop_words='english')"], "metadata": {"_cell_guid": "9ab12926-8938-47cb-8c70-710d86f04174", "collapsed": true, "_uuid": "d075dccbc903590cb08439c4179849f2e2e595a0"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["combinedDtm = vect.fit_transform(combinedTxt) #fit on combine\n", "trainDtm = combinedDtm[:train.shape[0]]\n", "testDtm = vect.transform(testTxt) #transform only test"], "metadata": {"_cell_guid": "21434a80-de63-44b1-8304-5ea9ff3a658c", "collapsed": true, "_uuid": "0864159d1607202dbd4d04ab8b56d10191530fdb"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["lrpreds = np.zeros((test.shape[0],len(col)))\n", "nbpreds = np.zeros((test.shape[0],len(col)))\n", "svmpreds = np.zeros((test.shape[0],len(col)))\n", "xgbpreds = np.zeros((test.shape[0],len(col)))"], "metadata": {"_cell_guid": "be226ccd-289c-47c0-bbcb-a5f220504b25", "collapsed": true, "_uuid": "c9f9491cbd5dedf6a3160e444417791883e6e783"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["svd = TruncatedSVD(n_components=100, n_iter=10, random_state=42)\n", "trainDtmSvd = svd.fit_transform(trainDtm)\n", "testDtmSvd = svd.transform(testDtm)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#call fit on every single col value \n", "#normal lr\n", "loss = []\n", "for i,j in enumerate(col):\n", "    lr = LogisticRegression(C=4)\n", "    lr.fit(trainDtm,train[j]) #train[j] is each type of comment\n", "    lrpreds[:,i] = lr.predict_proba(testDtm)[:,1]\n", "    train_preds = lr.predict_proba(trainDtm)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {"_cell_guid": "53f0a61a-94a3-4b93-a893-aef2aa3f1a6a", "_uuid": "50ec5df1aaf7ec596cfa0301f7e561d23f1ce821"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#lr with Svd\n", "loss = []\n", "for i,j in enumerate(col):\n", "    lr = LogisticRegression(C=4)\n", "    lr.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n", "    lrpreds[:,i] = lr.predict_proba(testDtmSvd)[:,1]\n", "    train_preds = lr.predict_proba(trainDtmSvd)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#normal nb\n", "loss = []\n", "for i,j in enumerate(col):\n", "    nb = MultinomialNB()\n", "    nb.fit(trainDtm,train[j]) #train[j] is each type of comment\n", "    nbpreds[:,i] = nb.predict_proba(testDtm)[:,1]\n", "    train_preds = nb.predict_proba(trainDtm)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {"_cell_guid": "7e0b86c7-79dd-43c3-8e7f-25eeaf2e208d", "_uuid": "b1ea6603361e23e0761141bd571c6b3417faf896"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#GaussianNB  with svd\n", "loss = []\n", "for i,j in enumerate(col):\n", "    nb = GaussianNB()\n", "    nb.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n", "    nbpreds[:,i] = nb.predict_proba(testDtmSvd)[:,1]\n", "    train_preds = nb.predict_proba(trainDtmSvd)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#normal xgb\n", "loss = []\n", "for i,j in enumerate(col):\n", "    xg = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n", "                        subsample=0.8, nthread=10, learning_rate=0.1)\n", "    xg.fit(trainDtm,train[j]) #train[j] is each type of comment\n", "    xgbpreds[:,i] = xg.predict_proba(testDtm)[:,1]\n", "    train_preds = xg.predict_proba(trainDtm)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {"_cell_guid": "991b3f0c-95fe-495b-a174-b6473df903db", "_uuid": "fa682526efb6be6b587ac4d237e41d48adf18f3a"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#xgb with svd\n", "loss = []\n", "for i,j in enumerate(col):\n", "    xg = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n", "                        subsample=0.8, nthread=10, learning_rate=0.1)\n", "    xg.fit(trainDtmSvd,train[j]) #train[j] is each type of comment\n", "    xgbpreds[:,i] = xg.predict_proba(testDtmSvd)[:,1]\n", "    train_preds = xg.predict_proba(trainDtmSvd)[:,1]\n", "    loss.append(log_loss(train[j],train_preds))\n", "np.mean(loss)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# predsMix = 0.6*lrpreds+0.3*xgbpreds+0.1*nbpreds\n", "predsMix = xgbpreds\n", "predsDf = pd.DataFrame(predsMix,columns = col)\n", "subid = pd.DataFrame({'id':sampleSubmission['id']})\n", "finalPreds = pd.concat([subid,predsDf],axis=1)\n", "finalPreds.to_csv(\"mix.csv\",index=False)"], "metadata": {"_cell_guid": "7223d4a0-2bfa-4316-85d6-8f3a6162901e", "_uuid": "221cc9df76de73fa95f797dae07a5604c89ad6c9"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": [], "metadata": {"_cell_guid": "a78c0695-480c-475e-8bc8-daef670ff9c5", "collapsed": true, "_uuid": "3cb33203358ac9723a1520030e6a3405d2458f47"}, "cell_type": "code"}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3"}}}