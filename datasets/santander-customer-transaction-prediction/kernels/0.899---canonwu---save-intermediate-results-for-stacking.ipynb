{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom bayes_opt import BayesianOptimization\n\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7521ee1f4f5def5f333489cc6b140309615a8e2e"},"cell_type":"code","source":"raws_features = train_data.columns[2:]\ntrain_X, train_y = train_data[raws_features], train_data['target']\ntest_X = test_data[raws_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd96067b0b25aeb070524d114765a2d454dcaf4"},"cell_type":"code","source":"n_splits = 10\nnum_round = 77777\nseed = 7777","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da5dafe8e35583a85be651511639beeae6ae1e98"},"cell_type":"markdown","source":"Creating classes fo CV to automate the process"},{"metadata":{"trusted":true,"_uuid":"02eb8127268b7fed534a81b2ca1c8462d53621e7"},"cell_type":"code","source":"class CVClassifier():\n    def __init__(self, estimator, n_splits=5, stratified=True, num_round=77777, **params):\n        self.n_splits_ = n_splits\n        self.scores_ = []\n        self.clf_list_ = []\n        self.estimator_ = estimator\n        self.stratified_ = stratified\n        self.num_round_ = num_round\n        if params:\n            self.params_ = params\n        \n    def cv(self, train_X, train_y):\n        if self.stratified_:\n            folds = StratifiedKFold(self.n_splits_, shuffle=True, random_state=seed)\n        else:\n            folds = KFold(self.n_splits_, shuffle=True, random_state=seed)\n        oof = np.zeros(len(train_y))\n        for fold, (train_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n            print('fold %d' % fold)\n            trn_data, trn_y = train_X.iloc[train_idx], train_y[train_idx]\n            val_data, val_y = train_X.iloc[val_idx], train_y[val_idx]\n            if self.estimator_ == 'lgbm':\n                train_set = lgb.Dataset(data=trn_data, label=trn_y)\n                val_set = lgb.Dataset(data=val_data, label=val_y)\n                clf = lgb.train(params=params, train_set=train_set, num_boost_round=num_round, \n                                valid_sets=[train_set, val_set], verbose_eval=100, early_stopping_rounds=200)\n                oof[val_idx] = clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)\n                \n            elif self.estimator_ == 'xgb':\n                train_set = xgb.DMatrix(data=trn_data, label=trn_y)\n                val_set = xgb.DMatrix(data=val_data, label=val_y)\n                watchlist = [(train_set, 'train'), (val_set, 'valid')]\n                clf = xgb.train(self.params_, train_set, self.num_round_, watchlist, \n                               early_stopping_rounds=200, verbose_eval=100)\n                oof[val_idx] = clf.predict(val_set, ntree_limit=clf.best_ntree_limit)\n            \n            elif self.estimator_ == 'cat':\n                clf = CatBoostClassifier(self.num_round_, task_type='GPU', early_stopping_rounds=500, **self.params_)\n                clf.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=500)\n                oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n\n            # sk-learn model\n            else:\n                clf = self.estimator_.fit(trn_data, trn_y)\n                try:\n                    oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n                except AttributeError:\n                    oof[val_idx] = clf.decision_function(val_data)\n            \n            self.clf_list_.append(clf)\n            fold_score = roc_auc_score(train_y[val_idx], oof[val_idx])\n            self.scores_.append(fold_score)\n            print('Fold score: {:<8.5f}'.format(fold_score))\n        self.oof_ = oof\n        self.score_ = roc_auc_score(train_y, oof)\n        print(\"CV score: {:<8.5f}\".format(self.score_))\n        \n    def predict(self, test_X):\n        self.predictions_ = np.zeros(len(test_X))\n        \n        if self.estimator_ == 'lgbm':\n            self.feature_importance_df_ = pd.DataFrame()\n            for fold, clf in enumerate(self.clf_list_):\n                fold_importance_df = pd.DataFrame()\n                fold_importance_df[\"feature\"] = features\n                fold_importance_df[\"importance\"] = clf.feature_importance()\n                fold_importance_df[\"fold\"] = fold + 1\n                self.feature_importance_df_ = pd.concat([self.feature_importance_df_, fold_importance_df], axis=0)\n                \n                self.predictions_ += clf.predict(test_X, num_iteration=clf.best_iteration) * (self.scores_[fold] / sum(self.scores_))\n        elif self.estimator_ == 'xgb':\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict(xgb.DMatrix(test_X), ntree_limit=clf.best_ntree_limit) \\\n                * (self.scores_[fold] / sum(self.scores_))\n        elif self.estimator_ == 'cat':\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] / sum(self.scores_))\n        else:\n            for fold, clf in enumerate(self.clf_list_):\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] / sum(self.scores_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d276b1d563ed161216cfbf817f1c7e38c6d2293","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Class for Bayesian Optimisation\nclass CVForBO():\n    def __init__(self, model, train_X, train_y, test_X, base_params, int_params=[], n_splits=5, num_round=77777):\n        self.oofs_ = []\n        self.params_ = []\n        self.predictions_ = []\n        self.cv_scores_ = []\n        self.model_ = model\n        self.train_X_ = train_X\n        self.train_y_ = train_y\n        self.test_X_ = test_X\n        self.base_params_ = base_params\n        self.int_params_ = int_params\n        self.n_splits_ = n_splits\n        self.num_round_ = num_round\n        \n    def cv(self, **opt_params):\n        for p in self.int_params_:\n            if p in opt_params:\n                opt_params[p] = int(np.round(opt_params[p]))\n        self.base_params_.update(opt_params)\n        \n        cv_model = CVClassifier(self.model_, n_splits=self.n_splits_, num_round=self.num_round_, **self.base_params_)\n        cv_model.cv(self.train_X_, self.train_y_)\n        cv_model.predict(self.test_X_)\n        \n        self.oofs_.append(cv_model.oof_)\n        self.predictions_.append(cv_model.predictions_)\n        self.params_.append(self.base_params_)\n        self.cv_scores_.append(cv_model.score_)\n\n        return cv_model.score_\n    \n    def post_process(self, model_type=None, oof_path='inter_oofs.csv', pred_path='inter_preds.csv', params_path='inter_params.csv'):\n        if not model_type:\n            model_type=self.model_\n        cols = ['{}_{}_{}'.format(model_type, str(self.cv_scores_[k]).split('.')[-1][:5], k) for k in range(len(self.cv_scores_))]\n        self.oof_df = pd.DataFrame(np.array(self.oofs_).T, columns=cols)\n        self.pred_df = pd.DataFrame(np.array(self.predictions_).T, columns=cols)\n        self.params_df = pd.DataFrame(self.params_).T.rename(columns={c_old: c_new for c_old, c_new in enumerate(cols)})\n        \n        self.oof_df.to_csv(oof_path)\n        self.pred_df.to_csv(pred_path)\n        self.params_df.to_csv(params_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111ce6f5afbea91f95937f413c9f6aeea19eb8d7","_kg_hide-output":true},"cell_type":"code","source":"cat_params = {\n    'eval_metric': 'AUC',\n    'bootstrap_type': 'Bernoulli',\n    'objective': 'Logloss',\n    'od_type': 'Iter',\n    'random_seed': seed,\n    'allow_writing_files': False}\n\ncv_cat_for_BO = CVForBO('cat', train_X, train_y, test_X, cat_params, ['depth'])\ncat_BO = BayesianOptimization(cv_cat_for_BO.cv, {\n    'depth': (2, 4), \n    'l2_leaf_reg': (37, 97), \n    'random_strength': (5, 17), \n    'eta': (0.01, 0.1)\n    }, random_state=seed)\n\ncat_BO.maximize(init_points=2, n_iter=15, acq='ei')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f6e55a1aa51839b60129f13429c194787bd9a6"},"cell_type":"code","source":"print(cat_BO.max)\ncv_cat_for_BO.post_process()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b37b70dff5c84f5dddf644d7ddf36b796da9b36"},"cell_type":"code","source":"max_idx = cv_cat_for_BO.cv_scores_.index(cat_BO.max['target'])\n\nsub_df = pd.DataFrame({'ID_code': test_data['ID_code'], \n                      'target': cv_cat_for_BO.predictions_[max_idx]})\nsub_df.to_csv('submissions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a38a818e26e2a5a7d4aa623d59a1a81f57eec77"},"cell_type":"markdown","source":"Many thanks to [u1234x1234](https://www.kaggle.com/u1234x1234) who shared me the idea of how to save intermediate results in this [discussion](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82621)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}