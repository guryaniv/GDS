{"cells":[{"metadata":{"_uuid":"3b6c7289176418b96ecbd28fba263f13f5675c83"},"cell_type":"markdown","source":"**Inspired from SRKs great kernel:**\n\nhttps://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc\n\n**References to other kernels given below.**"},{"metadata":{"_uuid":"f6b3b5dfd94a34b37eaacdab8e5853c1c3ead8ac"},"cell_type":"markdown","source":"**Importing libraries,train and test**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.plots import plot_convergence\nimport json\nimport string\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes,linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nrand_seed = 13579\nnp.random.seed(rand_seed)\n\nsns.set(style=\"darkgrid\", context=\"notebook\")\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/train\"))\nprint(os.listdir(\"../input/test\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train/train.csv\")\ntest_df = pd.read_csv(\"../input/test/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef5d56c772b4442e32b2637e94141cb09ac70751"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b2c5891daf2c14f58e247c84e2422834fe138c"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a0028636f61fd70e4669b7c941bc82eed636e5b"},"cell_type":"markdown","source":"**Target Distribution**"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c419b10165e53132d8a14a407e7341bdddc63640"},"cell_type":"code","source":"cnt_srs = train_df['AdoptionSpeed'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs / cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a34af1eca1e92879ad035919b077027e0a820457"},"cell_type":"markdown","source":"**Wordcloud for Train**\n\n***Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes*** \n"},{"metadata":{"trusted":true,"_uuid":"dd7b37ec53f4fb067de4b7f2de203d2024e242d7"},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"Description\"], title=\"Word Cloud of Train\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6fc4748fa91b0a0d823ac1d08d3f4c058e2a00b"},"cell_type":"markdown","source":"**Word Cloud for Test**"},{"metadata":{"trusted":true,"_uuid":"a6cf58d62baacd07871954f7d8de5cd62b09f4cb"},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(test_df[\"Description\"], title=\"Word Cloud of Test\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3edfd9883c0bf9c6c5221ddbf6110cfcf14d5af5"},"cell_type":"markdown","source":"**Filling up nan values**"},{"metadata":{"trusted":true,"_uuid":"2ccd6533b9f4165c038b2d31df72e7fa2be62c32"},"cell_type":"code","source":"train_df[\"Description\"].fillna(\"#\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"905c71b5c054fa7573d7fa97c9326a5b84e42475"},"cell_type":"markdown","source":"**Word Frequency Plot**"},{"metadata":{"trusted":true,"_uuid":"5627f862399f299fca4615a4139af6ad7bde64cb"},"cell_type":"code","source":"from collections import defaultdict\ntrain1_df = train_df[train_df[\"AdoptionSpeed\"]==0]\ntrain2_df = train_df[train_df[\"AdoptionSpeed\"]==1]\ntrain3_df = train_df[train_df[\"AdoptionSpeed\"]==2]\ntrain4_df = train_df[train_df[\"AdoptionSpeed\"]==3]\ntrain5_df = train_df[train_df[\"AdoptionSpeed\"]==4]\n\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train4_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train5_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Adoption Speed==0\", \n                                          \"Frequent words of Adoption Speed==1\",\n                                         \"Frequent words of Adoption Speed==2\",\n                                         \"Frequent words of Adoption Speed==3\",\"Frequent words of Adoption Speed==4\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\n\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89f24c504603a0486f39bb7a26fe30ce00f089d4"},"cell_type":"markdown","source":"**Bigram Frequency Plot**"},{"metadata":{"trusted":true,"_uuid":"d724c7632fc36dbed8b64e2b820bdbbffc7eb53c"},"cell_type":"code","source":"freq_dict = defaultdict(int)\nfor sent in train1_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train4_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train5_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent bigrams of Adoption Speed==0\", \n                                          \"Frequent bigrams of Adoption Speed==1\",\n                                         \"Frequent bigrams of Adoption Speed==2\",\n                                         \"Frequent bigrams of Adoption Speed==3\",\"Frequent bigrams of Adoption Speed==4\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\n\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b9bfa7cb3bd96274a6edcbbf1d3cf4ba123d9b"},"cell_type":"markdown","source":"**Exploration of some features**"},{"metadata":{"trusted":true,"_uuid":"fce21ed700a2d59beca709b08d59b4cb88a975f7"},"cell_type":"code","source":"## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"Description\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"Description\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"Description\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"Description\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"Description\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"Description\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe0b11ecd71a3cfbd15f0d9a59a40393a8bd459d"},"cell_type":"code","source":"f, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='AdoptionSpeed', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='AdoptionSpeed', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='AdoptionSpeed', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2866f594f143aade7504cbdaf2615ba7d46dbf6d"},"cell_type":"markdown","source":"**Tfidf Vectorization**"},{"metadata":{"trusted":true,"_uuid":"a171ca199f6425a27f522fe9937ce22bcbcbcb4f"},"cell_type":"code","source":"# Get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(stop_words='english',ngram_range=(1,3),encoding='utf-8')\ntfidf_vec.fit_transform(train_df['Description'].values.astype(\"U\").tolist() + test_df['Description'].values.astype(\"U\").tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['Description'].astype(\"U\").values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['Description'].astype(\"U\").values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b19e5674ffa2fee84dbfda769ec48498ab6b5b9"},"cell_type":"markdown","source":"Baseline Model"},{"metadata":{"trusted":true,"_uuid":"ec3f8a5129c255d9c5d1a7f6da528792bd5f2890"},"cell_type":"code","source":"train_y = train_df[\"AdoptionSpeed\"].values\nmodel = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=2019)\nmodel.fit(train_tfidf, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0785743ba3874ed964d77088e728aeb4c6d16af"},"cell_type":"markdown","source":"**Now we will look at the important words . We will use eli5 library for the same. Thanks to this excellent kernel by   @lopuhin. **"},{"metadata":{"trusted":true,"_uuid":"253e65605d45da4db28f84363edeb98fb54431fb"},"cell_type":"code","source":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}