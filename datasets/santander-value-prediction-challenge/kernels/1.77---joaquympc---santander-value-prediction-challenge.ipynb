{"cells":[{"metadata":{"_uuid":"5aa6b2bde73c9f612629df5aa1911fafbd86c121"},"cell_type":"markdown","source":"## Importing Files and Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.feature_selection as feat_sel\nimport sklearn.ensemble as ensemble\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11a72ff340f662f5446aa0be3b0107ca1671db7b"},"cell_type":"markdown","source":"### Analysing columns and distribuitions"},{"metadata":{"trusted":true,"_uuid":"62f070e44187604fcb715514929f2e37633f3aed"},"cell_type":"code","source":"# Nº of rows and colums\nprint('Train: Rows - '+str(len(df_train)) + ' Columns - ' + str(len(df_train.columns)))\nprint('Test: Rows - '+str(len(df_test)) + ' Columns - ' + str(len(df_test.columns)))\n\n# Type of columns\ntrain_col_types = df_train.dtypes\ntest_col_types = df_train.dtypes\nprint('-'*60)\nprint('Train: Type of columns')\nprint('-'*60)\nprint(train_col_types.groupby(train_col_types).count())\nprint('-'*60)\nprint('Test: Type of columns')\nprint('-'*60)\nprint(test_col_types.groupby(test_col_types).count())\n\n# Missing values?\nprint('-'*60)\nlist = []\ncounts = []\nfor i in df_train.columns:\n    list.append(i)\n    counts.append(sum(df_train[i].isnull()))\nprint('Train: Nº of columns with missing values')\nprint('-'*60)\nprint(sum(counts))\nprint('-'*60)\nlist = []\ncounts = []\nfor i in df_test.columns:\n    list.append(i)\n    counts.append(sum(df_test[i].isnull()))\nprint('Test: Nº of columns with missing values')\nprint('-'*60)\nprint(sum(counts))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5281dbb871794559fe4b1fafe05c3ba49623c474"},"cell_type":"markdown","source":"1. The train set has 4.459 rows and 4.993 columns\n1. The test set has 49.342 rows and 4.992 columns\n1. All the columns are numbers and looks like they area anonized"},{"metadata":{"trusted":true,"_uuid":"90c530789be9b24ac09bdc2c8fac69a8f254bec0"},"cell_type":"code","source":"# Columns with all rows zero\ncolumns_train_sum = pd.DataFrame(df_train.sum(),columns=['Sum of Row'])\nprint('Train: Nº of columns with all rows zero train: ' + str(columns_train_sum[columns_train_sum==0].count()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e7332612df79d94825c5bd246027cc641b3bf8d"},"cell_type":"markdown","source":"1. The train dataset has 256 columns with all values zero "},{"metadata":{"trusted":true,"_uuid":"d3102cb63c2a2a0900914f24e1580d3402680fa4"},"cell_type":"code","source":"# Is any ID on the test dataset?\nfor i in df_train.ID.values:\n    c = 0\n    if i in df_test.ID.values:\n        c = c + 1\nprint('Nº of ID''s on the test dataset: ' + str(c))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fb97710e8a298408ce926a17d32864b84876946"},"cell_type":"markdown","source":"#### As expeted theres no ID form the train set ont the test set"},{"metadata":{"trusted":true,"_uuid":"f168d42601bc1bf68ac82e4f75e6aba2148e6363"},"cell_type":"code","source":"# There's any visibla outlier on the target?\n#plt.scatter(range(df_train.shape[0]), np.sort(df_train['target'].values))\nplt.plot(df_train.ID, np.sort(df_train.target.values))\nplt.xlabel('ID')\nplt.ylabel('Target')\nplt.title('ID vs Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c10acb2372c6d0a95ea0385d42c666745a5c0de"},"cell_type":"markdown","source":"#### There isn't visible outliers on the train set"},{"metadata":{"trusted":true,"_uuid":"33bb007a3e3ddf7ba7f2d37ddd2ab2481b41a481"},"cell_type":"code","source":"# How is the target distribuition\ndf_train.target.hist()\nplt.xlabel('Target')\nplt.ylabel('Nº of ID''s')\nplt.title('Histogram of target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"579e1c0473e76e9126f045d558cdc2e076642cee"},"cell_type":"markdown","source":"#### Looks like the distribuition is not normal so lets try the log of the target"},{"metadata":{"trusted":true,"_uuid":"a58afe98d35ae9cdccf722a80dd47ecb4d6fbbf7"},"cell_type":"code","source":"# How is the log target distribuition\ndf_train['target_log'] = np.log(df_train.target)\ndf_train.target_log.hist()\nplt.xlabel('Target')\nplt.ylabel('Nº of ID''s')\nplt.title('Histogram of log target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9813f91f8bce09f78fa8ff925ecf93b32b906908"},"cell_type":"markdown","source":"#### With the log of the target the distribuiton looks more distribuite, more like a normal distribuition"},{"metadata":{"trusted":true,"_uuid":"df5cd28729c0b9266e0e63a00b7440ebb531c3d6"},"cell_type":"code","source":"## Drop columns will all zero values\nlist_columns_train_drop=[]\nfor i in columns_train_sum[columns_train_sum['Sum of Row']==0].index:\n    list_columns_train_drop.append(i)\ndf_train = df_train.drop(columns=list_columns_train_drop)\nlen(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a727a0e14fc748f134920d995a285a70dadeb08d"},"cell_type":"code","source":"## Verify the correlatin between the target and the variables\ncorr_train_target_values = []\ncorr_train_target_column = []\nfor i in df_train.columns:\n    if i in ['ID','target']:\n        None\n    else:\n        corr = df_train[['target',i]].corr(method='spearman')\n        corr_train_target_values.append(corr.target[1])\n        corr_train_target_column.append(i)\n\ncorr_train_target = pd.DataFrame(corr_train_target_values,index=corr_train_target_column)\ncorr_train_target.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02097b43b982aec49e83f74e6d9d8005cee00c98"},"cell_type":"markdown","source":"#### There is a low correlation of the variables with the target. Let's see how's the distribuiton off eah variable"},{"metadata":{"trusted":true,"_uuid":"54dcac0e07c5ed792dc0ab0c0534b287c571f933"},"cell_type":"code","source":"X = df_train.drop(columns=['target','target_log','ID'])\nvariable_mean = X.mean()\nvariable_std = X.std()\nvariable_name = X.columns\nhigh_indices = np.argsort(variable_mean)[::-1][:50]\nlow_indices = np.argsort(variable_mean)[:50]\nplt.bar(range(len(variable_mean[high_indices])),variable_mean[high_indices],yerr=variable_std[high_indices])\n#plt.xticks(range(len(variable_mean[high_indices])),variable_name[high_indices],rotation='vertical')\nplt.show()\nplt.bar(range(len(variable_mean[low_indices])),variable_mean[low_indices],yerr=variable_std[low_indices])\n#plt.xticks(range(len(variable_mean[low_indices])),variable_name[low_indices],rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e69897b5a625109b7f59bf8f457a7067c972ca"},"cell_type":"markdown","source":"#### There a big difference between each variable in terms off the mean and std. Best to standardize the data and reduce the number of variables for predictions"},{"metadata":{"trusted":true,"_uuid":"b111bae3bbe1e821c351c3caf41a21f6d5f8d402","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Implementing a PCA to reduze the amount of variables and standardize data\n\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\nX = df_train.drop(columns=['target','ID','target_log'])\nX = preprocessing.scale(X)\nlist_n_comp=[]\nlist_var_ratio=[]\nn_comp = 100\nmax_list_var_ratio = 0.0\nwhile max_list_var_ratio<0.8: #n_comp <= 1000:\n    print(n_comp)\n    pca = PCA(n_components=n_comp)\n    pca.fit(X)\n    list_n_comp.append(n_comp)\n    list_var_ratio.append(sum(pca.explained_variance_ratio_))\n    max_list_var_ratio = max(list_var_ratio)\n    print(max_list_var_ratio)\n    n_comp = n_comp + 100\n#list_n_comp,list_var_ratio\n\nplt.plot(list_n_comp, list_var_ratio)\nplt.xlabel('Number of components')\nplt.ylabel('Variance Ratio')\nplt.title('PCA')\nplt.ylim([0,1])\nplt.axhline(0.8,color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e56527e91f18045de2530a90bfd187fed2e2aaf"},"cell_type":"markdown","source":"#### PCA with 900 componente explains more than 80% of the variability"},{"metadata":{"trusted":true,"_uuid":"cdf9b61eca3dc848bc666e0dcbd085fa976b7e63","collapsed":true},"cell_type":"code","source":"# PCA with 80% explained ratio is with 900 components\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\nX = df_train.drop(columns=['target','ID','target_log'])\nX = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\npca = PCA(n_components=900)\nX_pca = pd.DataFrame(pca.fit_transform(X))\n\ndf_train_pca = df_train[['ID','target','target_log']]\ndf_train_pca[X_pca.columns.values]= X_pca\n\nX = df_test.drop(columns=['ID'])\nX = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\npca = PCA(n_components=900)\nX_pca = pd.DataFrame(pca.fit_transform(X))\n\ndf_test_pca = pd.DataFrame(df_test['ID'],columns=['ID'])\ndf_test_pca[X_pca.columns.values]= X_pca","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aa28497cd8b76b963490e3938eee27c8c1d3358"},"cell_type":"markdown","source":"#### After scale and reduze the variables let's see what's the best model using cross validation"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"507082556706e9d3698ebaf0fe4bd528073c814e"},"cell_type":"code","source":"# Split train data into test and train\nX = df_train_pca.drop(columns=['target','ID','target_log'])\ny = df_train_pca.target_log\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Finding the best model out of the box\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import make_scorer\n\ndef score(y_true,y_pred):\n    score = np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n    return score\nmy_score = make_scorer(score,greater_is_better=False)\n\nlist_model = [Ridge(),SVR(),GradientBoostingRegressor(),RandomForestRegressor(),AdaBoostRegressor(),MLPRegressor()]\nfor i in list_model:\n    print(i)\n    print(cross_val_score(i, X_train, y_train, cv=5, scoring = my_score))\n    print('-'*60)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b7ee87233ad3d5986217a5d9aa3a17684c07f65"},"cell_type":"markdown","source":"#### GradientBoost looks the best performer. Lets see what's the best hyperparameters"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"645c5b9c511ccc4131481e5f1fbe79ad7f651964"},"cell_type":"code","source":"# Finding best hyperparameters\n\nparameters = {'n_estimators':[50,100,150],'max_depth':[3,5]}\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(model, parameters,scoring=my_score)\ngrid_result = grid.fit(X_train,y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"aa047065e62e7acf34f7f5720e43fa5e47d5930a"},"cell_type":"code","source":"# Using best hyperparameters and evaluate test set\n\nmodel = GradientBoostingRegressor(n_estimators=100,max_depth=3)\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\nresults = {'rmse train': np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_pred_train))),\n           'rmse test': np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_test))),\n           'std y_train': np.exp(y_train).std(),\n           'mean y_train': np.exp(y_train).mean(),\n           'std y_test': np.exp(y_test).std(),\n           'mean y_test': np.exp(y_test).mean(),\n           'std y_pred_train': np.exp(y_pred_train).std(),\n           'mean y_pred_train': np.exp(y_pred_train).mean(),\n           'std y_pred_test': np.exp(y_pred_test).std(),\n          }\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"742a1f54f79e8589c57b6a022829594832cf2a44","collapsed":true},"cell_type":"code","source":"# make prediction with gradient boost\nX = df_test_pca.drop(columns=['ID'])\ndf_test_pca['target'] = np.exp(model.predict(X))\ndf_test_pca[['ID','target']].to_csv('subsmission_gb.csv',index=False,sep=',')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"532e2b8dac86d78b4ff0efcc97adcdcaf1d723d6"},"cell_type":"markdown","source":"> #### Aflter Gradient Boost lets see how a neural network on Keras/Tensorflow performes"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"635e0b35dec7bb9d03d83559a11fb230e9df82c5"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Split train data into test and train\nX = df_train_pca.drop(columns=['target','ID','target_log'])\ny = df_train_pca.target_log\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Finding best hyperparameters\n\nfrom sklearn.metrics import make_scorer\ndef score(y_true,y_pred):\n    score = np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n    return score\nmy_score = make_scorer(score,greater_is_better=False)\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(2,input_dim=900,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1,activation='relu'))\n    model.compile(loss=root_mean_squared_error,optimizer='SGD',metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model,epochs=10,batch_size=3)\n\nepochs = [5,10,20]\nbatch_size = [3,5,7]\nverbose = [0]\nparameters = dict(epochs=epochs,batch_size=batch_size,verbose=verbose)\n\ngrid = GridSearchCV(model, parameters,scoring=my_score)\ngrid_result = grid.fit(X_train,y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b35606e92b33a455ed3c0e8d93f685bba417d0ab","scrolled":true},"cell_type":"code","source":"# Using best hyperparameters and evaluate test set\n\nmodel = Sequential()\nmodel.add(Dense(2,input_dim=900,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='relu'))\nmodel.compile(loss=root_mean_squared_error,optimizer='SGD')\n\nmodel.fit(X_train, y_train, epochs=10, batch_size=7,verbose=1)\ny_pred_train = model.predict(X_train, batch_size=32)\ny_pred_test = model.predict(X_test, batch_size=32)\n\nresults = {'rmse train': np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_pred_train))),\n           'rmse test': np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_test))),\n           'std y_train': np.exp(y_train).std(),\n           'mean y_train': np.exp(y_train).mean(),\n           'std y_test': np.exp(y_test).std(),\n           'mean y_test': np.exp(y_test).mean(),\n           'std y_pred_train': np.exp(y_pred_train).std(),\n           'mean y_pred_train': np.exp(y_pred_train).mean(),\n           'std y_pred_test': np.exp(y_pred_test).std(),\n          }\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e0b43a75dc62c03190a4186f4b9c6267d64dad6"},"cell_type":"code","source":"# make prediction with nueral network\nX = df_test_pca.drop(columns=['ID','target'])\ndf_test_pca['target'] = np.exp(model.predict(X))\ndf_test_pca[['ID','target']].to_csv('subsmission_nn.csv',index=False,sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"601f9560c89320b43d83a7d9bac7bc7881c09e7c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}