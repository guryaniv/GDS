{"cells":[{"metadata":{"_uuid":"9749cc64711569bb9728c5656f0e4000efb79c47"},"cell_type":"markdown","source":"##Leaf Classification : Selecting Best Estimate and then predicting probabilities of outcome"},{"metadata":{"_uuid":"b14c15ec6ef3c0912a1bd8b0a1ac762feac3d78c"},"cell_type":"markdown","source":"These are the steps we would be following :\n\n1. Read the train and test data\n2.Check the distribution of classes in training data. If distributions are skewed f1 score would be advisable, else we can use the accuracy score for model estimation\n3. Encode the target classes if required\n4. Run the dataset as is against set of predefined models with varying parameters\n5. Use gridsearch for parameter tuning\n6. Display the accuracy and f1 score\n7. Plot the training vs testing accuracy scores to check for variances and bias\n8. Select the best model and apply it on the test set.\n9. Output the prediction probabilities\n"},{"metadata":{"trusted":true,"_uuid":"41223dc47de928e527ef67dc7be9accf5b83897c"},"cell_type":"code","source":"import pandas as panda\n\nfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, classification_report, confusion_matrix, f1_score, roc_curve, roc_auc_score\nfrom sklearn.linear_model import Perceptron, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom scipy import stats\n\n\nfrom matplotlib import pyplot as plot\nimport matplotlib.patches as mpatches\n\nimport seaborn as sns\n\n\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\n\nimport itertools, time, datetime\nfrom collections import Counter\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3480715fbca42d7543119a232c1130e7db6bd0ba"},"cell_type":"code","source":"remote_location_test_data = \"../input/test.csv\"\nremote_location_training_data = \"../input/train.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0f072ee8e738c42c3c1c4d7ad040edac5fdb86f"},"cell_type":"code","source":"test_data = panda.read_csv(remote_location_test_data)\ntrain_data = panda.read_csv(remote_location_training_data)\n\nprint('Testing Data Shape: ', test_data.shape, ' , Training Data Shape: ' , train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d84b513f09ae2569f3abcebceb1ef4a044a0f4f3"},"cell_type":"markdown","source":"\n\nAs expected, testing data has one lesser column. This is because the testing data set does not have the target column.\n\nLets proceed with our data analysis on the training data"},{"metadata":{"trusted":true,"_uuid":"938018b5a382cf4cbc7a31c8acbcf93a1b6071c3"},"cell_type":"code","source":"target_attribute = 'species'\ntarget_spread = train_data[target_attribute].value_counts()\n\nprint('Spread of unique leaf attributes in training data: \\n', target_spread)\ntrain_data[target_attribute].describe(include = 'all')\n\n\ntarget_spread.plot(kind = 'barh', figsize = (10,20))\nplot.title(\"Distribution of Target Values\")\nplot.xlabel(\" COunt\")\nplot.ylabel(\"Leaf\")\nplot.show()\n\nencoder = LabelEncoder()\nencoder.fit(train_data[target_attribute])\n_y_train = encoder.transform(train_data[target_attribute])\nY = _y_train\n_y_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4992aff8d66ba927d7fc32614f342d593c4244f1"},"cell_type":"markdown","source":"\n\nFor our target attribute, we see a pretty even distribution of leaf species, each unique leaf is marked ten times. \n\nLets check the data types of our remaining columns. And also check for empty values. \n\n\nFindings:\n\n1. All our columns are numeric\n\n\n2. All our columns contain values, none are empty of numpy.nan\n\n\n3. There are some columns where values are very miniscule, ie most values are 0. eg margin8, margin 23, etc. Since it is difficult to visualize all such columns, lets divide it up by names (shapes, margins, textures) and see if we can figure out such columns where most appearances are of 0 and mean value is very less .\n\n\nWe may chose to check the correlation coefficients of these features, in case our model performs badly\n"},{"metadata":{"trusted":true,"_uuid":"72e2cb3c2c8a40790205004ca536d6e079099094"},"cell_type":"code","source":"all_columns = train_data.columns.tolist()\ncolumns_unwanted = ['id', 'species']\ncolumns_wanted = [i for i in all_columns if i not in columns_unwanted]\nprint('Number of unique data types in the data \\n', train_data[columns_wanted ].dtypes.value_counts())\n\n\nempty_values = train_data[columns_wanted ].isnull().sum().to_frame()\nempty_values= empty_values.assign(column_type = train_data.dtypes)\nprint('Number of empty columns: ', set(empty_values[0].values))\n\n\ntrain_data_description = train_data[columns_wanted ].describe(include = \"all\")\ntrain_data_description = train_data_description.transpose()[['25%', '50%', '75%', 'max', 'mean']]\ntrain_data_description\n\n# train_data_description.iloc[0 : 64, :]\n# train_data_description.loc[['margin1', 'margin2'], ['max']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4de567d58d6673f213e919921805b12aa0a891e"},"cell_type":"code","source":"\nexclude_numbers = list(map(str, range(0,10)))\ncolumn_identifiers = map(lambda x:''.join([i for i in x if i not in exclude_numbers]), columns_wanted)\ncolumn_identifiers = set(column_identifiers)\n\n## lets start plotting 25%, 50%, 75% and mean of the indivual column identifiers. we know from our data description that there are 64 of each\n\nfor col in column_identifiers:\n    train_data_description.loc[[col + str(i) for i in range(1,65)]].plot(kind='bar', figsize = (30,40))\n    plot.title(\"Distribution of data for columns starting with identifier \" + col)\n    plot.xlabel(\"Count\")\n    plot.ylabel(col)\n    plot.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de2b5a3c35123aec30493de4dedd1e2f8ade27fb"},"cell_type":"markdown","source":"\n\nBased off our bar charts drawn above we can draw the following conclusions:\n    \n    1. Shape has good distribution of non zero data. Hence we are not going to touch columns starting with shape( for now )    \n    \n    2. Margin columns has a few \"non conformers\", where max appearances of 0 has been observed. These columns are margin8, margin16, margin23.    \n    \n    3. Texture columns which require further analysis are 56,61,36 and 23\n    "},{"metadata":{"trusted":true,"_uuid":"414680461ee657c2cf136e3c93e8f9d960226a32"},"cell_type":"code","source":"print('value counts for column margin8 \\n', train_data['margin8'].value_counts()\\\n          , '\\n for column margin16 \\n', train_data['margin16'].value_counts()\\\n          , '\\n for column margin23 \\n', train_data['margin23'].value_counts())\n\nprint('value counts for column texture23 \\n', train_data['texture23'].value_counts()\\\n          , '\\n for column texture36 \\n', train_data['texture36'].value_counts()\\\n          , '\\n for column texture56 \\n', train_data['texture56'].value_counts() \\\n          , '\\n for column texture61 \\n', train_data['texture61'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2bc11c9c0ee0e3ef78b18cbe1f8520dc05d3e6b"},"cell_type":"code","source":"def calculateCorrelationCoefficientsAndpValues(x_data, y_data, xlabel):\n    \n    pearson_coef, p_value = stats.pearsonr(x_data, y_data)\n    print(\"The Pearson Correlation Coefficient for %s is %s with a P-value of P = %s\" %(xlabel,pearson_coef, p_value))\n    \n    return (pearson_coef,p_value)\n\n\ndef plotRegressionBetweenTwoVariables(x_label,y_label, x_y_data, pearson_coef, p_value):\n    \n    plot.figure(figsize=(15,15))\n    \n    sns.regplot(x = x_label , y = y_label , data = x_y_data)\n\n\n    # plot.text(x = 1, y = 40000 , s =\"Pearson Correlation Coefficient = %s\"%pearson_coef, fontsize = 12 )\n    # plot.text(x = 1, y = 38000 , s =\"P value = %s\"%p_value, fontsize = 12 )\n\n    blue_patch = mpatches.Patch(color='blue', label='Pearson Correlation Coefficient = %s, p value is %s '%(pearson_coef, p_value))\n    plot.legend(handles=[blue_patch], loc ='best')\n    plot.title(\"Regression Plot %s vs %s\"%(x_label, y_label))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ec0d4b09564adc003707bbe255f0786c894264e"},"cell_type":"markdown","source":"\n\nNow we would simply start our modeling efforts using all other columns and estimate accuracy scores across model to select the best one\n\n"},{"metadata":{"trusted":true,"_uuid":"4bbe6b50e48b1b8b45b9838599b45422d7f181bc"},"cell_type":"code","source":"X = train_data[columns_wanted]\n# Y = _y_train\n\n_x_train, _x_test, _y_train, _y_test = train_test_split(X, Y, test_size =0.30, stratify = Y, random_state = 1)\n\n##using COunter object we check to see if test and training has been properly distributed and we find it is.\nprint(Counter(Y))\nprint(Counter(_y_train))\nprint(Counter(_y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89823403371fea9d4ec2dc1f2350a036cfcdb6c5"},"cell_type":"markdown","source":"\n\nNow that our data has been separated out equally (70:30) between train and test data. Lets start our estimation algorithms across the following models:\n\n1. LogisticRegression\n\n2. SVM\n\n3. DecisionTreeClassifier\n\n4. RandomForestClassifier\n\n5. KNearestNeighborClassifier\n\n6. LinearDiscriminantAnalysis\n"},{"metadata":{"trusted":true,"_uuid":"6d34a1d60ba81900a58a0c5632c5205c95533ba3"},"cell_type":"code","source":"class CodeTimer:\n    \n    \"\"\"\n        Utility custom contextual class for calculating the time \n        taken for a certain code block to execute\n    \n    \"\"\"\n    def __init__(self, name=None):\n        self.name = \" '\"  + name + \"'\" if name else ''\n\n    def __enter__(self):\n        self.start = time.clock()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (time.clock() - self.start) * 1000.0\n        time_taken = datetime.timedelta(milliseconds = self.took)\n        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d42c6abfb348f2b2ecedf8bc897e1af2fddd80"},"cell_type":"code","source":"## cv is essentially value of K in k fold cross validation\n    \n## n_jobs = 1 is  non parallel execution    , -1 is all parallel , any other number say 2 means execute in 2 cpu cores\n\ndef plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n    \n    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n                                                                X = _x_train, \\\n                                                                y = _y_train, \\\n                                                                train_sizes = training_sample_sizes, \\\n                                                                cv = k_fold, \\\n                                                                n_jobs = jobsInParallel) \n\n\n    training_mean = mean(training_score, axis = 1)\n    training_std_deviation = std(training_score, axis = 1)\n    testing_std_deviation = std(testing_score, axis = 1)\n    testing_mean = mean(testing_score, axis = 1 )\n\n    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n    ## to see the accuracy results and check for bias vs variance\n\n    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n\n    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n\n    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n\n    plot.title(\"Scoring of our training and testing data vs sample sizes\")\n    plot.xlabel(\"Number of Samples\")\n    plot.ylabel(\"Accuracy\")\n    plot.legend(loc= 'best')\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16a83bf66f4f0d745b59948671a2aa12f54e5ba3"},"cell_type":"code","source":"def runGridSearchAndPredict(pipeline, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n    \n    response = {}\n    training_timer       = CodeTimer('training')\n    testing_timer        = CodeTimer('testing')\n    learning_curve_timer = CodeTimer('learning_curve')\n    predict_proba_timer  = CodeTimer('predict_proba')\n    \n    with training_timer:\n        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n\n        search = gridsearch.fit(x_train,y_train)\n\n        print(\"Grid Search Best parameters \", search.best_params_)\n        print(\"Grid Search Best score \", search.best_score_)\n            \n    with testing_timer:\n        y_prediction = gridsearch.predict(x_test)\n            \n    print(\"Accuracy score %s\" %accuracy_score(y_test,y_prediction))\n    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average = 'macro'))\n    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n    \n    with learning_curve_timer:\n        plotLearningCurve(_x_train, _y_train, search.best_estimator_, k_fold = 7)\n                   \n    response['learning_curve_time'] = learning_curve_timer.took\n    response['testing_time'] = testing_timer.took\n    response['_y_prediction'] = y_prediction\n    response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n    response['training_time'] = training_timer.took\n    response['f1_score']  = f1_score(y_test, y_prediction, average= 'macro')\n    \n    \n    return response\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa88866ebb4b5d881773f07df9d2e4e31db738c8"},"cell_type":"code","source":"classifiers = [\n    \n    LogisticRegression(random_state = 1),\n    LogisticRegression(random_state = 1, solver = 'lbfgs', multi_class = 'multinomial'),\n    DecisionTreeClassifier(random_state = 1, criterion = 'gini'),\n    RandomForestClassifier(random_state = 1, criterion = 'gini'),\n    KNeighborsClassifier(metric = 'minkowski'),\n    SVC(random_state = 1, kernel = 'rbf'), \n    LinearDiscriminantAnalysis()\n     \n]\n\n\nclassifier_names = [\n            'logisticregression',\n            'multinomiallogisticregression',\n            'decisiontreeclassifier',\n            'randomforestclassifier',\n            'kneighborsclassifier',\n            'svc', \n            'lda',\n    \n]\n\nclassifier_param_grid = [\n            \n            {'logisticregression__C':[100,200,300,50,20,600]},\n            {'multinomiallogisticregression__C':[100,200,300,50,20,600], 'multinomiallogisticregression__penalty':['l2'], 'multinomiallogisticregression__max_iter':[100,200,300,400]},\n            {'decisiontreeclassifier__max_depth':[1,2,4,6,7,8,9,10,11]},\n            {'randomforestclassifier__n_estimators':[1,2,3,5,6]} ,\n            {'kneighborsclassifier__n_neighbors':[4,6,7,8]},\n            {'svc__C':[1, 10, 100, 200], 'svc__gamma':[0.01 , 0.1, 0.05]},\n            {'lda__n_components':[4,5,6]},\n    \n]\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30f0fcd19c8deaf1709433004681edbd31d3a906"},"cell_type":"code","source":"\ntimer = CodeTimer(name='overalltime')\nmodel_metrics = {}\n\nwith timer:\n    for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n\n        pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                (model_name, model)\n        ])\n\n        result = runGridSearchAndPredict(pipeline, _x_train, _y_train, _x_test, _y_test, model_param_grid , cv = 7,score = 'accuracy')\n\n        _y_prediction = result['_y_prediction']\n\n        _matrix =  confusion_matrix(y_true = _y_test ,y_pred = _y_prediction)\n\n        model_metrics[model_name] = {}\n        model_metrics[model_name]['confusion_matrix'] = _matrix\n        model_metrics[model_name]['training_time'] = result['training_time']\n        model_metrics[model_name]['testing_time'] = result['testing_time']\n        model_metrics[model_name]['learning_curve_time'] = result['learning_curve_time']\n        model_metrics[model_name]['accuracy_score'] = result['accuracy_score']\n        model_metrics[model_name]['f1_score'] = result['f1_score']\n        model_metrics[model_name]['classes'] = encoder.inverse_transform(_y_prediction)\n        \n        \n        \n        \nprint(timer.took)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b48ca2ff86329f116ff4817276100d1e5687298"},"cell_type":"code","source":"\nmodel_estimates = panda.DataFrame(model_metrics).transpose()\n\n\n## convert model_metrics into panda data frame\n## print out across model estimations and accuracy score bar chart\n\n\nmodel_estimates['learning_curve_time'] = model_estimates['learning_curve_time'].astype('float64')\nmodel_estimates['testing_time'] = model_estimates['testing_time'].astype('float64')\nmodel_estimates['training_time'] = model_estimates['training_time'].astype('float64')\nmodel_estimates['f1_score'] = model_estimates['f1_score'].astype('float64')\n\n#scaling time parameters between 0 and 1\nmodel_estimates['learning_curve_time'] = (model_estimates['learning_curve_time']- model_estimates['learning_curve_time'].min())/(model_estimates['learning_curve_time'].max()- model_estimates['learning_curve_time'].min())\nmodel_estimates['testing_time'] = (model_estimates['testing_time']- model_estimates['testing_time'].min())/(model_estimates['testing_time'].max()- model_estimates['testing_time'].min())\nmodel_estimates['training_time'] = (model_estimates['training_time']- model_estimates['training_time'].min())/(model_estimates['training_time'].max()- model_estimates['training_time'].min())\n\nprint(model_estimates)\nmodel_estimates.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7095670b16121f1eb78db20730d64d3f6fdfb08"},"cell_type":"markdown","source":"We can see that our highest accuracy scores are from LogisticRegression using the best parameters:\n\nsolver = 'lbfgs'\n\nmulti_class = 'multinomial', \n\nC = 100, \n\npenalty = 'l2', \n\nmax_iter = 100 \n\n\nwith an accuracy of ~99%. It also displays low variance at sample sizes greater than 600. And best of all, among all the models, when sample size is close to 100, it shows the best training vs testing score. This is important since our testing data has around only 90 samples\n"},{"metadata":{"trusted":true,"_uuid":"9768ac452475e79b59bf008db91d4808ca3cba61"},"cell_type":"code","source":"\nX_test = test_data[columns_wanted]\nids = test_data['id'].values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1254d86159d5a7f04e0b0da1ba1e7136d305850"},"cell_type":"code","source":"pipeline = Pipeline([('scaler', StandardScaler()),('lineardiscriminantanalysis', LogisticRegression(random_state = 1, solver = 'lbfgs', multi_class = 'multinomial', C = 100, penalty = 'l2', max_iter = 100))])\n\nparam_grid = [{}]\n\nsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = 'accuracy', cv = 10)\n\nsearch.fit(train_data[columns_wanted], Y) ## fit on the entire training data\n\nprobabilities = search.predict_proba(X_test)\n\n\nclasses = train_data['species'].unique()\n\n# print(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03572ff487f4630b3477a50c1cc839e834158ef5"},"cell_type":"code","source":"classes = classes.tolist()\n## it is important to sort the classes, since predict_proba returns array-like, shape = [n_samples, n_classes]\n# Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_.\nclasses.sort() \n\nsubmission = panda.DataFrame(probabilities, columns=classes)\nsubmission.insert(0, 'id', ids)\nsubmission.reset_index()\n\n# Export Submission\n# submission.to_csv('C:/Users/somak/Documents/somak_python/real-world-use-cases/supervised/classification/kaggle/leaf_classification/submission.csv', index = False)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd559be77802ea0a71c422330e62177b7783e6d6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}