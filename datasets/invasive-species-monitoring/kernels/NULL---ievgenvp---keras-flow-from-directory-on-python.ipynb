{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.1", "name": "python"}}, "cells": [{"metadata": {"_uuid": "4913a0e86a6d52a5a7ec2e6e2c6e3e7ffbb52808", "_cell_guid": "af736dc3-2e73-486b-9b64-a1f01bd72deb"}, "outputs": [], "source": "From the beginning, I decided to use Keras for the task. As for the start I googled \"keras images classification\" and found great blog post by Francois Chollet.\n\n(blog https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n\ngithub code https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975)\n\n\nThe code in the blog has simple and understandable structure. Also, there are few posts in the competition Kernels where the similar approach was applied. Thus some addition insight is available.\n\n\n(Specifically, these two kernels helped me a lot:\n\nhttps://www.kaggle.com/ogurtsov/0-99-with-r-and-keras-inception-v3-fine-tune\n\nhttps://www.kaggle.com/fujisan/use-keras-pre-trained-vgg16-acc-98)", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "240fb2d3d01968ee51c51b0d59ee71dc28ee7e61", "_cell_guid": "d9785f5f-7281-48ff-9238-b517bffcb217"}, "outputs": [], "source": "This script was designed to work on local machine, thus its sections are surrounded by ''' marks to avoid running it in Jupiter Notebook. I use Python 2.7 and Keras with Theano backend. The code is tested and runs well.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "a178fb8fb100c3fb5053c7273d4eb94eca17cce1", "_cell_guid": "249c3e59-db0e-4daa-8da7-b1d883f61a75"}, "outputs": [], "source": "As the first step pictures were uploaded to my local hard drive and placed into the project folder. Following script was applied to get txt files with list of training and validation data for each class.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"collapsed": true, "_uuid": "ffbe0f3415e49af9e9595c8d7efc3cc257d3461a", "_cell_guid": "cbbdca6a-053c-4257-be8d-6934d3a6433e", "trusted": false}, "source": "import pandas as pd\nimport numpy as np\n\n'''\nlabels_df = pd.read_csv('train_labels.csv')\nprint (labels_df['invasive'].value_counts())\nclasses = [0,1]\n\nfor i in classes:\n    df_class = labels_df.loc[labels_df['invasive'] == i]\n    df_class = df_class['name'].astype(str)+'.jpg'\n    n = lambda x: 100 if i == 1 else 37\n    validation_pack = np.random.choice(df_class.values, n(i), replace = False)\n    np.savetxt('class_{0}_val.txt'.format(i), validation_pack, fmt = '%s')\n\n    training_pack = np.setdiff1d(df_class.values, validation_pack)\n    np.savetxt('class_{0}_tr.txt'.format(i), training_pack, fmt = '%s')\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "16ecbe2ef2dcb5e67ec94aba13a979cde2052c13", "_cell_guid": "0e5f5390-e4f1-4933-b1cb-8431702963b6"}, "outputs": [], "source": "Next step was to create folder \"data\" and move into it folders with train and test data. Inside of train subfolder it was necessary to make train and validation class for each folder. \n\nAt this step my structure of the project folder was following:\n    \n    [project folder] \"invasive_species\"/..\n        [subfolder] data/..\n            [subfolder] train/..\n                [subfolder] class_0_tr/..\n                [subfolder] class_0_val/..\n                [subfolder] class_1_tr/..\n                [subfolder] class_1_val/..\n            [subfolder] test/..\n \n Text files \"class_0_tr\", \"class_0_val\", \"class_1_tr\", \"class_1_val\" were placed into \"train\" subfolder. When all these were done I could use following code in command line (I'm using Windows).\n \n     cd ../../invasive_species/data/train (here you should put your path)\n     for /f \"delims=\" %i in (class_0_tr.txt) do copy \"%i\" class_0_tr\n     for /f \"delims=\" %i in (class_0_val.txt) do copy \"%i\" class_0_val\n     for /f \"delims=\" %i in (class_1_tr.txt) do copy \"%i\" class_1_tr\n     for /f \"delims=\" %i in (class_1_val.txt) do copy \"%i\" class_1_val\n\nThis allocated all pictures by respective folders. Then I changed the structure of the project folder to the following:\n    \n    [project folder] \"invasive_species\"/..\n        [subfolder] data/..\n            [subfolder] train_tr/..\n                [subfolder] class_0_tr/..\n                [subfolder] class_1_tr/..\n             [subfolder] train_val/..\n                [subfolder] class_0_val/..\n                [subfolder] class_1_val/..\n            [subfolder] test/test/..\n\nPlease notice that folder \"test\" was putted inside of another folder \"test\". Without this step function \"flow_from_directory\" retrieved no test images.\n\nNow it was time to implement approach described in Keras blog.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "ad450cd6031fb56667f4075abd69c4f737bc5510", "collapsed": true, "_cell_guid": "6930e8ba-9a27-4a88-9c0c-45c1a0d75138", "trusted": false}, "source": "import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import roc_auc_score\n\n# place to put some parameters\nbatch_size = 20\nnum_val_samples = 137\nsteps = 2295*1.4/batch_size", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "d2f457cacaba256cf285571a9c7a39eafd865a2f", "_cell_guid": "842484e0-9ee1-438b-bd7c-aa45a77b689f"}, "outputs": [], "source": "Data generator for train set included augmentation and processing (shear, zoom, horizontal_flip, rescale). For the test set only rescale should be made (we don't want to mess with new data - just predict its class).", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "894d9fc197b04da754bb1147fa711ca42f9fea5c", "collapsed": true, "_cell_guid": "0d506053-fbaa-46c3-8269-2340de245bfa", "trusted": false}, "source": "'''\n# data generator for training set\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255,\n    shear_range = 0.2, # random application of shearing\n    zoom_range = 0.2,\n    horizontal_flip = True) # randomly flipping half of the images horizontally\n\n# data generator for test set\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e9c1a7eb758fff5524227b7be724699aa256892b", "_cell_guid": "313ec4f9-55d9-4de9-aff3-56dbb39c1e90"}, "outputs": [], "source": "Data generators take data from specified folders. Also it is time to put some additional parameters, like class_mode, target_size and color mode. \n\nBatch_size for train and validation data was judgmentally selected as 20. For test data batch size is 1 as we apply trained model to all test pictures without separating them by batches.\n\nIt is important to put \"shuffle\" equal to False in test generator. Otherwise order of test pictures will be distorted by the function and predictions won't fit order expected by Kaggle checking system.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "9f837314891e6595d76c67b8b75a5dcf8e7fe173", "collapsed": true, "_cell_guid": "a89f58c9-b093-4429-b94b-c6ab78e52238", "trusted": false}, "source": "'''\n# generator for reading train data from folder\ntrain_generator = train_datagen.flow_from_directory(\n    'data/train_tr',\n    target_size = (256, 256),\n    color_mode = 'rgb',\n    batch_size = batch_size,\n    class_mode = 'binary')\n\n# generator for reading validation data from folder\nvalidation_generator = test_datagen.flow_from_directory(\n    'data/train_val',\n    target_size = (256, 256),\n    color_mode = 'rgb',\n    batch_size = batch_size,\n    class_mode = 'binary')\n\n# generator for reading test data from folder\ntest_generator = test_datagen.flow_from_directory(\n    'data/test',\n    target_size = (256, 256),\n    color_mode = 'rgb',\n    batch_size = 1,\n    class_mode = 'binary',\n    shuffle = False)\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "753e349e44b5b17b2ede35afc97d85b8c8589476", "_cell_guid": "da02789f-61dd-437c-b33b-45129ba672bc"}, "outputs": [], "source": "The model architecture was taken from the blog. Rmsprop optimizer showed the best LB result with 10 epochs (more epoch provided inferior results). I expect that SGD with small learning rate and more epochs should make it better.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "bcc29dc9e35af3c6e8fcbab7be6aa445b0eed6aa", "collapsed": true, "_cell_guid": "9f58143c-c8bc-4579-8b2b-eb4271da45b2", "trusted": false}, "source": "'''\n# neural network model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), input_shape = (256, 256, 3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(32, (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(64, (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'rmsprop',\n              metrics = ['accuracy'])\n\nmodel.fit_generator(train_generator,\n                    steps_per_epoch = steps,\n                    epochs = 10,\n                    validation_data = validation_generator,\n                    validation_steps = num_val_samples/batch_size)\n\n#model.save_weights('nn_weights.h5')\n#model.load_weights('nn_weights.h5')\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8d4a3553ba315c3186da0b254d1bd5b4d2bb3620", "_cell_guid": "c164c079-400c-4339-b53e-2cd9f2193506"}, "outputs": [], "source": "Check against validation set  provided some degree of confidence that the code was made in a right way. But after few tries it was clear that validation results are higher than LB score, and there is not always positive correlation between them. In case of powerful machine cross-validation may be applied.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "a7c1af12bb0016d58cc7af77d116fc568a40a033", "collapsed": true, "_cell_guid": "fa435480-9f91-4648-8fbf-9cd2abf4b853", "trusted": false}, "source": "'''\n# AUC for prediction on validation sample\nX_val_sample, val_labels = next(validation_generator)\nval_pred = model.predict_proba(X_val_sample)\nval_pred = np.reshape(val_pred, val_labels.shape)\nval_score_auc = roc_auc_score(val_labels, val_pred)\nprint (\"AUC validation score\")\nprint (val_score_auc)\nprint ('\\n')\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6fe6b80aef65b313826562c0ab7e7a2141a5da9c", "_cell_guid": "bde7902e-1893-48df-9115-833d3ab8affe"}, "outputs": [], "source": "After draft code had been done I had a good validation score (0.975) and depressingly low LB score (around 0.5). Eventually it became clear that the problem is related to how Keras tool flow_from_directory handle download from folder with test images.\n\n\"flow_from_directory\" sort names in the target folder. Thus while Kaggle checking system expects predictions for labels  \"1, 2, 3, ..., 1531\" the function make predictions for labels \"1, 10, 100, 1000, 1001, ..., 2, 20, 200, ..., 1531\". \n\nThe code below \"sort back\" results of the predictions.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "7fbd56acc5932cb1c281f92e85a0f086f2b144fb", "collapsed": true, "_cell_guid": "58c67dbd-5b34-442d-99c4-519e0061af3e", "trusted": false}, "source": "'''\n# test predictions with generator\ntest_files_names = test_generator.filenames\npredictions = model.predict_generator(test_generator,\n                                      steps = 1531)\npredictions_df = pd.DataFrame(predictions, columns = ['invasive'])\npredictions_df.insert(0, \"name\", test_files_names)\npredictions_df['name'] = predictions_df['name'].map(lambda x: x.lstrip('test\\\\').rstrip('.jpg'))\npredictions_df['name'] = pd.to_numeric(predictions_df['name'], errors = 'coerce')\npredictions_df.sort_values('name', inplace = True)\npredictions_df.to_csv('predictions_df.csv', index = False)\n'''", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "a3e12f032b0ceb6fa7923cc7683cc394ae811120", "_cell_guid": "e17072f4-f2c8-4527-b816-b452890e7929"}, "outputs": [], "source": "The script gets 0.96749 of LB score and definitely has lots of possible improvements like use of pre-trained model and parameters tuning. Also it is reasonable to select the best model/parameters using validation and then train on full set.", "cell_type": "markdown", "execution_count": null}], "nbformat": 4, "nbformat_minor": 1}