{"cells":[{"metadata":{"_uuid":"aa0e60c3a2c8079b14b6a7ced2c45c6ff2834176","_cell_guid":"eb6f45ca-ecd2-4696-9fe5-1fc7a0b017ba"},"cell_type":"markdown","source":"**Problem:**\nThe goal is to predict if a passenger of Titanic wil survivie or not. This a typical Binary classification problem and we will try to solve it using maching learning tools such as Decision Tree. \n\nThe first few sections of the code is entirely based on https://www.datacamp.com/community/tutorials/kaggle-tutorial-machine-learning\n\nHowever, I tried to consoldate the three step approach into a single flow\n\nIn the later sections, I implement other classification models such as Logistic Regression, Random Forest, KNN, etc. "},{"metadata":{"_uuid":"3df11ade0418c7b015f9450aa5859f2c1289151e","_cell_guid":"9c62f641-23c5-4241-ac8b-090e549101cf"},"cell_type":"markdown","source":"**Import all necessary libraries**"},{"metadata":{"_uuid":"f34ab8f99f63b19cdc3e72e4207ac976a355b405","_cell_guid":"b85a5b78-2a68-4f28-8ad3-947d0b9ec5c8","collapsed":true,"trusted":false},"cell_type":"code","source":"# Import modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"963a94c78f22c2c9f7af44606826f9b9bfe72873","_cell_guid":"d80afd1e-d6a6-4d4a-b651-f63740ae4101"},"cell_type":"markdown","source":"**Import the data and have a look at what it has**"},{"metadata":{"_uuid":"e2db570baf3f3215216d873b86faf86027ddb946","_cell_guid":"7a369bd2-89de-4d56-80f6-7bb02a9bba6e","collapsed":true,"trusted":false},"cell_type":"code","source":"# Import data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\n# View first few lines of training data\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1adf1c40a9d0b3c16fc6bbf09085a2f3b3530fb","_cell_guid":"33b90987-72a1-43e8-ae76-d58805c54b6c","collapsed":true,"trusted":false},"cell_type":"code","source":"# You can also view the data types and missing data\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d7ca4cb72070eb46fff70cbf45397dc2844f22c","_cell_guid":"2d6b9c4e-5c61-47a6-b384-5a524784eac5"},"cell_type":"markdown","source":"As shown above the training data consists of 12 columns. The target variable that we want to predict is Survived. The remaining variables are called as predictor variables. In this case, you see that there are only 714 non-null values for the 'Age' column in a DataFrame with 891 rows. This means that are are 177 null or missing values. We will see later how to deal with missing values."},{"metadata":{"_uuid":"12f66ce8d9a079f0f4501046397e01b7965e57cb","_cell_guid":"2cc85006-ed6f-4d15-871a-d37a3ff871c5","collapsed":true,"trusted":false},"cell_type":"code","source":"# you can also see the statistical summary of the training data\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dae1679657d48bed135591df1c8e596c697f44a9","_cell_guid":"86bf2ae8-632a-4dbb-b029-721f7110a05f"},"cell_type":"markdown","source":"**Visual Exploratory Data Analysis (EDA)**"},{"metadata":{"_uuid":"ce95eff84d51a31395f0b17abdb723729aa482f8","_cell_guid":"79a93acb-2ee5-48b8-9eee-f5de36cbb1e9","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot(x='Survived', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df70ecfc58c3d55866c7f4621a2b4efc4fe42931","_cell_guid":"1939251d-a834-4dbe-bcdf-2b6cb7640fd7","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot(x='Sex', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf9cc2c1d7e6cf1195e80d39e97d1f622b42887","_cell_guid":"a3f1cfce-a7ae-4723-bb70-236c2ff520ac","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.factorplot(x='Survived', col='Sex', kind='count', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4dab721c6a4b5c012741a6249e8d7bc92132582","_cell_guid":"39f0f313-6d2b-4d90-9ce7-576f8ac96568"},"cell_type":"markdown","source":"It looks like females are more most likely to survive than male. With this we can use Pandas to calculate how many male and female survived. "},{"metadata":{"_uuid":"fe74a0b1ae38e69720432f4666c9a5e3f12fed6d","_cell_guid":"98e61d36-e955-485a-801e-4c1c97f96231","collapsed":true,"trusted":false},"cell_type":"code","source":"df_train.groupby(['Sex']).Survived.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2823b3c2d6d96111816f33fae7da5ef7b346e2c1","_cell_guid":"d6b0c776-f214-42d7-bbeb-a188b2b0fb23","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use pandas to figure out the proportion of women that survived, along with the proportion of men\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7793109c06f0bf97a19e1d6b2c80757c96323866","_cell_guid":"5b70d469-328a-4414-8bfd-88aa093b3511"},"cell_type":"markdown","source":"We see that 74% of female and 18% of male survived the Titanic disaster. "},{"metadata":{"_uuid":"f684f939b23f6b1a946cc9540365b72c7d036fde","_cell_guid":"a27d09c9-78ba-45d5-86ac-ceb5babdbaf8","collapsed":true,"scrolled":false,"trusted":false},"cell_type":"code","source":"# Use seaborn to build bar plots of the Titanic dataset feature 'Survived' split (faceted) over the feature 'Pclass'\nsns.factorplot(x='Survived', col='Pclass', kind='count', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"847a3780a9b2cae8d420e70633ba8987cf2d5e70","_cell_guid":"9be29f84-3213-4d1d-ac3c-abb366c26b36"},"cell_type":"markdown","source":"It looks like passengers that travelled in first class were more likely to survive. On the other hand, passengers travelling in third class were more unlikely to survive. "},{"metadata":{"_uuid":"bd45ccec450e361e1357c37ac5d4c443d27db0e5","_cell_guid":"ae2658bf-9762-40ad-a36e-dc5c5ad7f86f","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use seaborn to build bar plots of the Titanic dataset feature 'Survived' split (faceted) over the feature 'Embarked'\nsns.factorplot(x='Survived', col='Embarked', kind='count', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cc525b92a6f880b231b7544d4ce2de52f91bed0","_cell_guid":"044ec576-3895-40b7-98f8-b76cef13b745"},"cell_type":"markdown","source":"It looks like passengers that embarked in Southampton were less likely to survive. "},{"metadata":{"_uuid":"3a204688842ed1e689fde2f0ebc90bc0e1b77a94","_cell_guid":"c56fddad-d9a2-4101-b2a1-6782a4398115","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use seaborn to plot a histogram of the 'Fare' column of df_train\nsns.distplot(df_train.Fare, kde=False);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cda6f770e942f9e7d4adb739897ec075bef6d60d","_cell_guid":"09634b2c-44c9-4def-bcf1-8080dd850213"},"cell_type":"markdown","source":"It looks like most passengers paid less than 100 for travelling with the Titanic."},{"metadata":{"_uuid":"4818f93037ed12290c56ac7234c679ee3042ce23","_cell_guid":"2d8b4c68-d9c8-4ff8-9624-491177f8e9f8","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use a pandas plotting method to plot the column 'Fare' for each value of 'Survived' on the same plot.\ndf_train.groupby('Survived').Fare.hist(alpha=0.6);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c32015198cfd7b87b286924b56f1627f3c869c48","_cell_guid":"d41555a8-4865-4117-9c3a-532a253db61b"},"cell_type":"markdown","source":"It looks as though those that paid more had a higher chance of surviving."},{"metadata":{"_uuid":"88c4935622b8457765f9e8f447b10743540eeee7","_cell_guid":"de6a8378-4118-4c22-9291-aa439275bda2","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use seaborn to plot a histogram of the 'Age' column of df_train. You'll need to drop null values before doing so\ndf_train_drop = df_train.dropna()\nsns.distplot(df_train_drop.Age, kde=False);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02144882a954a2892e58db299cdbb0fcdc1784b6","_cell_guid":"b1b88280-b239-47cd-a1b8-a22562b0bd9d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Plot a strip plot & a swarm plot of 'Fare' with 'Survived' on the x-axis\nsns.stripplot(x='Survived', y='Fare', data=df_train, alpha=0.3, jitter=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"096113e9a7ffdaee21464c32a7a5c4ceb8cf8db1","_cell_guid":"d5518223-e67e-4962-80f7-2bea36faa793","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.swarmplot(x='Survived', y='Fare', data=df_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff6deae4eefad58478ed40806a4b60987e43c5c5","_cell_guid":"d6073dba-d8f7-48dc-9581-5922efef6636"},"cell_type":"markdown","source":"It looks like fare is correlated with survival aboard the Titanic."},{"metadata":{"_uuid":"945d6b1f5605cd6c87956eb5b6a9ebd7da7c581a","_cell_guid":"0c0ba714-bcaf-4444-8aaf-b3e8ff767e09","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use the DataFrame method .describe() to check out summary statistics of 'Fare' as a function of survival\ndf_train.groupby('Survived').Fare.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3491b0791e6f78397f7082d6a98bae63da5b2e8","_cell_guid":"d1d65450-4c43-44e9-80d8-dce9e6cf51bc","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use seaborn to plot a scatter plot of 'Age' against 'Fare', colored by 'Survived'\nsns.lmplot(x='Age', y='Fare', hue='Survived', data=df_train, fit_reg=False, scatter_kws={'alpha':0.5});","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30e206c3510a2bb0382c4242401779fd8f55e103","_cell_guid":"683e05c0-8a95-4a31-98d2-d30880f86849"},"cell_type":"markdown","source":"It looks like those who survived either paid quite a bit for their ticket or they were young."},{"metadata":{"_uuid":"e57f28b79a927d897ba710f38bba8e53ad9fbd6c","_cell_guid":"9f37236b-4931-45de-b0e7-1286decb6e8d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Use seaborn to create a pairplot of df_train, colored by 'Survived'\nsns.pairplot(df_train_drop, hue='Survived');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5036adb032f81e74aefbde37bd555d45539d3083","_cell_guid":"837f6ff5-53c3-4dae-9954-d47e48875ed1"},"cell_type":"markdown","source":"**Your first machine Learning Model**"},{"metadata":{"_uuid":"85395cf9e35fd444fe115c0f1ceab81ff7f7670c","_cell_guid":"f9b2e46f-ba59-4cbf-af04-e796ce1d8d3c","collapsed":true,"trusted":false},"cell_type":"code","source":"# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n# Check out your new DataFrame data using the info() method\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f222cb79c6da9e4f000fa2a4938099bfaec20d5","_cell_guid":"f033e996-2749-4e18-bf43-dd3a74483fb6"},"cell_type":"markdown","source":"There are two numerical variables that have missing values namely 'Age' and 'Fare' columns. In 'Age' column there are only 1046 non-null values for the total of 1309 entries of the dataframe which says that there are 263 missing values. In 'Fare' column there is only 1 missing value. \nThe missing values of 'Age' and 'Fare' column can be imputed using the median values of the variable. Median is a suitable value for imputing as it is less likely to be affected by outliers in the data. Usually it is a good practice to fill the missing numerical values by median. "},{"metadata":{"_uuid":"f6cd09bf5f6a5a84de12dd78c6cde182545e1c2f","_cell_guid":"df7689ed-4dcc-46e0-8d41-42cc2860a412","collapsed":true,"trusted":false},"cell_type":"code","source":"# Impute missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77baea35fea5ba9407027aaa1816c83466742c4d","_cell_guid":"2359e9db-de7b-4b16-91b2-496e84f73692","collapsed":true,"trusted":false},"cell_type":"code","source":"# Encode the data with numbers because most machine learning models might require numerical inputs\n# yo can do this using Pandas function get_dummies() which converts the categorical variable into numerical\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab9d14f5403b7536e6a2c096ad627ac10d5524e","_cell_guid":"33ab11ab-b490-43f1-b528-4ddb60bda169"},"cell_type":"markdown","source":"get_dummies() creates a new columns for each of the options in 'Sex' so that it creates a new columns for female called 'Sex_female' and new columns for male called 'Sex_male' which encodes if that row was male or female. As drop_first argument in get_dummies() was set as true 'Sex_female' columns was dropped. \n\nNow you will select the 'Sex_male', 'Fare' 'Age'. 'Pclass', 'SibSp' columns from your dataframe to build your first machine learning model. "},{"metadata":{"_uuid":"ff2844ec72d3e8addf4b70c893fa0ba2f28c9e73","_cell_guid":"9f51c2c7-0403-4b7a-b195-618db0fa2dd3","collapsed":true,"trusted":false},"cell_type":"code","source":"# Select columns and view head\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5f6549fa5e485aa49858379614d9a8719895a26","_cell_guid":"069821a3-c727-4922-a200-a298c16c92b3","collapsed":true,"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e3f8312233c4070fa343edc370c93f68352a30","_cell_guid":"3aa9aedd-5c43-4a0b-93e9-60618fdf0f69"},"cell_type":"markdown","source":"**Build a Decision Tree Classifier**\nWhat is a decision tree classifier? It is a tree that allows you to classify data points, which are also known as target variables, based on feature variables. \nFor example, this tree below has a root node that forces you to make a first decision, based on the following question: \"Was 'Sex_male'\" less than 0.5? In other words, was the data point a female. If the answer to this question is True, you can go down to the left and you get 'Survived'. If False, you go down the right and you get 'Dead'."},{"metadata":{"_uuid":"0483193e8522ce1f013f506300980b4092c3d88d","_cell_guid":"68a0c0af-8ad6-4e19-9f97-315b5bec14c7","collapsed":true,"trusted":false},"cell_type":"code","source":"# Before fitting a model to your data, split it back into training and test sets\ndata_train = data.iloc[:891]\ndata_test = data.iloc[891:]\n# A Scikit requirement transform the dataframes to arrays\nX = data_train.values\ntest = data_test.values\ny = survived_train.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13000e961bb9175e803326c058f1256b15303ac4","_cell_guid":"28329584-5aed-4d51-8ea9-17f4a8131a7b","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1de117a528c44455ceb3c2c30e1db8456acfe7ee","_cell_guid":"af9cf69a-9b65-4ad1-8787-7e32fbbfeda1","collapsed":true,"trusted":false},"cell_type":"code","source":"# build your decision tree classifier with max_depth=3 and then fit it your data\nclf = tree.DecisionTreeClassifier(max_depth=3)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101b9d3c2ccadb3dab175704525c3c923ceb8256","_cell_guid":"c390a82c-7ee4-48e2-a5a3-58f9558051e2"},"cell_type":"markdown","source":"Now, you'll make predictions on your test set, create a new column 'Survived' and store your predictions in it. Save 'PassengerId' and 'Survived' columns of df_test to a .csv and submit to Kaggle."},{"metadata":{"_uuid":"130233af986aa46e113c81b43193bcc1e9ccb754","_cell_guid":"727dc3b0-8b78-418d-905a-44b3fda10929","collapsed":true,"trusted":false},"cell_type":"code","source":"# Make predictions and store in 'Survived' column of df_test\nY_pred = clf.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('1st_dec_tree.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28c0334237e8f48a518eab296e44274565433882","_cell_guid":"70838df3-a36a-4399-904b-536564375249"},"cell_type":"markdown","source":"The accuracy of this model as reported by Kaggle is 78%. Congratulations on the first machine learning model!"},{"metadata":{"_uuid":"4e6256dbc34e5eddfeb523f97956be1e98852253","_cell_guid":"3adc12d0-20c0-42fb-9455-011e04bf0714"},"cell_type":"markdown","source":"**Feature Engineering**\n\nYou perform feature engineering to extract more information from your data, so that you can up your game when building models.\n\n**Titanic's Passenger Titles**\nThis name column contains strings or text that contain titles, such as 'Mr', 'Master' and 'Dona'. \nThese titles of course give you information on social status, profession, etc., which in the end could tell you something more about survival. \nAt first sight, it might seem like a difficult task to separate the names from the titles, but don't panic! Remember, you can easily use regular expressions to extract the title and store it in a new column 'Title' which will be our new feature of the dataset. "},{"metadata":{"_uuid":"6287b5a6632ac3f9cb87ae70cd33f6f4cc9cb45d","_cell_guid":"b7f725ea-5344-4011-968b-1373459bbacf","collapsed":true,"trusted":false},"cell_type":"code","source":"# Import data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n# Extract Title from Name, store in column and plot barplot\ndata['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4528ef3ce80554f41bd93aa64e3cf3a8c38a6f45","_cell_guid":"a8ef44b7-cc28-49d8-9dd3-f65448ede991"},"cell_type":"markdown","source":"As you can see that there are several titles in the above plot and there are many that don't occur so often. So, it makes sense to put them in fewer buckets. For example, you probably want to replace 'Mlle' and 'Ms' with 'Miss' and 'Mme' by 'Mrs', as these are French titles and ideally, you want all your data to be in one language. Next, you also take a bunch of titles that you can't immediately categorize and put them in a bucket called 'Special'. Next, you view a barplot of the result with the help of the .countplot() method"},{"metadata":{"_uuid":"7fb801423859843fd092bb9cad3d55aaa35135a5","_cell_guid":"72b812b2-bc68-4c9f-81fc-310958b62757","collapsed":true,"trusted":false},"cell_type":"code","source":"data['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ndata['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f28d64a0fac4ee3bc8d13d6f22a0286df08c39f","_cell_guid":"aba7dc6a-a7ac-4d09-afee-4eb5b7fe3881"},"cell_type":"markdown","source":"**Passenger's Cabins** When you loaded in the data and inspected it, you saw that there are several NaNs or missing values in the 'Cabin' column. It is reasonable to presume that those NaNs didn't have a cabin, which could tell you something about 'Survival'. So, let's now create a new column 'Has_Cabin' that encodes this information and tells you whether passengers had a cabin or not."},{"metadata":{"_uuid":"63fc37ce9a9fcae239a18cd7717d2a5048d32830","_cell_guid":"eb82fe96-a10c-4681-82f7-053f89f8bcf8","collapsed":true,"trusted":false},"cell_type":"code","source":"# Did they have a Cabin?\ndata['Has_Cabin'] = ~data.Cabin.isnull()\n\n# Drop columns and view head\ndata.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eed7b3aec32ba7e8feaa8f820f4122418238f70","_cell_guid":"b66daacd-1c6f-4ac5-9828-068adb3152d0","collapsed":true,"trusted":false},"cell_type":"code","source":"# Impute missing values for Age, Fare, Embarked\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\ndata['Embarked'] = data['Embarked'].fillna('S')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ab1cb7068ce35aae58e1bc9fd5ef2feb01adc8","_cell_guid":"1763639a-dc52-484b-bdab-441553d737bb","collapsed":true},"cell_type":"markdown","source":"**Bin numerical data**  Next, you want to bin the numerical data, because you have a range of ages and fares. However, there might be fluctuations in those numbers that don't reflect patterns in the data, which might be noise. That's why you'll put people that are within a certain range of age or fare in the same bin. You can do this by using the pandas function qcut() to bin your numerical data"},{"metadata":{"_uuid":"34d54c5ff76fbaede71ef5fa3ef899b98bd21db1","_cell_guid":"0480ce49-0d3c-4a39-9491-1cf11a961371","collapsed":true,"trusted":false},"cell_type":"code","source":"# Binning numerical columns\ndata['CatAge'] = pd.qcut(data.Age, q=4, labels=False )\ndata['CatFare']= pd.qcut(data.Fare, q=4, labels=False)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1e00a2f99b7141aa684cf8e7397e8c9664d408f","_cell_guid":"4360e3ce-2f1e-484c-a8f6-fea5fbe2dfa4","collapsed":true,"trusted":false},"cell_type":"code","source":"data = data.drop(['Age', 'Fare','SibSp','Parch'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06136e95fe12c5c02f8ec6dac101b16af6f0b435","_cell_guid":"63ae448a-868e-45f4-8110-477d37605d37","collapsed":true,"trusted":false},"cell_type":"code","source":"# Transform into binary variables\ndata_dum = pd.get_dummies(data, drop_first=True)\ndata_dum.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4293500b49e50b696d282b384f47342a5b63068e","_cell_guid":"9fa8d0c7-f2a8-4aee-bfda-808ce2fcb805"},"cell_type":"markdown","source":"**Building models with Your New Data Set** \nWe will use the same Decision Tree classifer wit this new data set. This time, we will use GridSearch with CrossValidation to determine the hyperparameter max_depth of decision tree classifier. "},{"metadata":{"_uuid":"914614a06626f2119857ceb96381542e1c1b3d81","_cell_guid":"539c9688-9e92-4090-bcac-ee7df8c7e97d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Split into test.train\ndata_train = data_dum.iloc[:891]\ndata_test = data_dum.iloc[891:]\n\n# Transform into arrays for scikit-learn\nX = data_train.values\ntest = data_test.values\ny = survived_train.values\n\n# Setup the hyperparameter grid\ndep = np.arange(1,9)\nparam_grid = {'max_depth' : dep}\n\n# Instantiate a decision tree classifier: clf\nclf = tree.DecisionTreeClassifier()\n\n# Instantiate the GridSearchCV object: clf_cv\nclf_cv = GridSearchCV(clf, param_grid=param_grid, cv=5)\n\n# Fit it to the data\nclf_cv.fit(X, y)\n\n# Print the tuned parameter and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(clf_cv.best_params_))\nprint(\"Best score is {}\".format(clf_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c99b7a0910a52b0737eb1c72dbc26bbb5f37964","_cell_guid":"fd3ed659-0340-4ccd-9d81-5af5b24ea388","collapsed":true,"trusted":false},"cell_type":"code","source":"# Now, you can make predictions on your test set, create a new column 'Survived' and store your predictions in it\nY_pred = clf_cv.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('dec_tree_feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cc8bb570a29130227ec8c5d241f15656bc51646","_cell_guid":"fc23e89e-7adf-4e8e-9e0c-0d1ea878bf1b"},"cell_type":"markdown","source":"The accuracy of this model as reported by Kaggle is 78.9%. "},{"metadata":{"_uuid":"d1be2c2f1a5254edb8092f571e7f82cc703779f0","_cell_guid":"3e64c0b6-c45d-4102-a379-5c298a6759ec"},"cell_type":"markdown","source":"**Other classification models implemented by me. **\n\nIn the following sections, I implement other classification models. The models will be trained on the feature engineered new dataset. We will use the models straight away with default parameters and also do hyperparameter tuning using GridSearchCV. "},{"metadata":{"_uuid":"2b95d288e37c26aad3428bec67da2f429a601d50","_cell_guid":"8519313b-3929-45e8-a0f7-83eec0e1998f"},"cell_type":"markdown","source":"**Logistic Regression**\nThe first model to try is Logistic Regression. \nFor more detials about Logistic Regression http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"},{"metadata":{"_uuid":"e10abd0dc38001b7c4480fd434ca8aa7642c2bbd","_cell_guid":"890575b1-8506-4752-8993-bd1b3d3d3549","collapsed":true,"trusted":false},"cell_type":"code","source":"# import logisitc regression from sklearn\nfrom sklearn.linear_model import LogisticRegression\n\n#instantiate the classifier without any parameters\nlogreg = LogisticRegression()\n\n#fit the data to the classifier\nlogreg.fit(X,y)\n\n#predict the survivors and submit the results\nY_pred = logreg.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('log_reg_feat_eng.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b17034de78875be3bbe1bf46e9a200f698633604","_cell_guid":"e0cb5d10-8af0-43e1-8d60-788915c65a75"},"cell_type":"markdown","source":"The accuracy of logistic regression model as reported by Kaggle is 77.5% which is less than Decision Tree classifier. \n\nWe will fine tuner the hyperparameters of Logistic Regression using GridSearchCV. "},{"metadata":{"_uuid":"0b05e077bff45ee768aefd686991113178ead86e","_cell_guid":"3c0000d7-6317-47a7-89fa-f417c503bb25","collapsed":true,"trusted":false},"cell_type":"code","source":"# create parameter grid for hyperparameter tuning\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X,y)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n\n#predict the survivors and submit the results\nY_pred = logreg_cv.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('log_reg__feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5cd35ffc1bd8b8cd7896b6a4fc0c08c90aa6bca","_cell_guid":"836b9683-deac-42a3-aa37-c3cf9946c403"},"cell_type":"markdown","source":"The accuracy of logisitic regression with tuned parameters of C and penalty using GridSearch as reported by Kaggle is 79.425% which is better then Decision Tree. \n\n**Random Forest Classifier**\n\nWe will try RandomForest classifier. For more details, http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"},{"metadata":{"_uuid":"0b26f2669b8424310a5212d4f0cb4065e6079e7a","_cell_guid":"f11a7a57-d7f2-4ced-852c-b2c2d6aa0f19","collapsed":true,"trusted":false},"cell_type":"code","source":"#import random forest classifer\nfrom sklearn.ensemble import RandomForestClassifier\n\n#instantiate RandomForest\nrf_clf = RandomForestClassifier()\n\n#fit the data\nrf_clf.fit(X,y)\n\n#predict the survivors and submit the results\nY_pred = rf_clf.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('random_forest_feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c06163420ece9f6ede2b2556fcd4da9e82afba1","_cell_guid":"2384deae-9f0f-4ec5-aad2-94b2d41ba363"},"cell_type":"markdown","source":"The accuracy of Random Forest classifier as reported by Kaggle is 76.5% and not an improvement from the best results so far. We will use hyperparameter tuning of RandomForest using GridSearch. The n_estimators in the hyperparameter for RandomForestClassifier."},{"metadata":{"_uuid":"3c761208bdae6c2c9b351ecaf4378a9cbce81b22","_cell_guid":"d5366a33-add6-4e4f-86d9-02d15a7e988f","collapsed":true,"trusted":false},"cell_type":"code","source":"# create parameter grid for hyperparameter tuning\nn_estimators = np.arange(10,50)\nparams_grid = {'n_estimators':n_estimators}\n\n#instantiate RandomForest\nrf_clf = RandomForestClassifier()\n\n# Instantiate the GridSearchCV object\nrf_clf_cv = GridSearchCV(rf_clf,params_grid,cv=5)\n\n# Fit it to the training data\nrf_clf_cv.fit(X,y)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Random Forest Classifier Parameter: {}\".format(rf_clf_cv.best_params_))\nprint(\"Tuned Random Forest Classifier Accuracy: {}\".format(rf_clf_cv.best_score_))\n\n#predict the survivors and submit the results\nY_pred = rf_clf_cv.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('random_forest_cv_feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08f029183f3d3249a12e3ca6627066d93f19d1d","_cell_guid":"e0a22eb8-654d-4b55-af92-a89ac81e2e8d","collapsed":true},"cell_type":"markdown","source":"The accuracy of Random Forest Classifier with n_estiamtors tunded as reported by Kaggle is 77.511% which is not an improvement."},{"metadata":{"_uuid":"448a3f4d4e568a4b903b94d5bfeee8b39f9491d9","_cell_guid":"a71a5fae-3c56-4993-8911-d85af04b4786"},"cell_type":"markdown","source":"**KNN Classifier**\nWe will use KNN - K Nearest Neigbor classifier to predict Titanic survivors. For more details, http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier"},{"metadata":{"_uuid":"7e14e7f8a95923765450eb1ebc23078fa1ef3ee6","_cell_guid":"14da9f9c-8708-4610-8807-eacb02e42ec2","collapsed":true,"trusted":false},"cell_type":"code","source":"#import KNN classifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Instantiate KNN classifier\nknn = KNeighborsClassifier()\n\n#Fit training data to knn\nknn.fit(X,y)\n\n#predict the survivors and submit the results\nY_pred = knn.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('knn_feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82ead151a8115c9c0857e9b41d24cae43b70c375","_cell_guid":"ebc8f97c-db15-4785-be36-c1effc9cefb4","collapsed":true},"cell_type":"markdown","source":"The accuracy of KNN classifier as reported by Kaggle is 77.033% which is not an improvement. \n\nWe will tune the hyperparameter n_neighbors using GridSearchCV."},{"metadata":{"_uuid":"bb0f54f2bffa90355ba3e4be2d080f44340fb7d0","_cell_guid":"4b79f307-7448-48cb-8ca2-1715b6a66503","collapsed":true,"trusted":false},"cell_type":"code","source":"# create parameter grid for hyperparameter tuning\nn_neighbors = np.arange(1,20)\nparams_grid = {'n_neighbors':n_neighbors}\n\n#instantiate KNN\nknn = KNeighborsClassifier()\n\n# Instantiate the GridSearchCV object\nknn_cv = GridSearchCV(knn,params_grid,cv=5)\n\n# Fit it to the training data\nknn_cv.fit(X,y)\n\n# Print the optimal parameters and best score\nprint(\"Tuned KNN Classifier Parameter: {}\".format(knn_cv.best_params_))\nprint(\"Tuned KNN Classifier Accuracy: {}\".format(knn_cv.best_score_))\n\n#predict the survivors and submit the results\nY_pred = knn_cv.predict(test)\ndf_test['Survived'] = Y_pred\ndf_test[['PassengerId', 'Survived']].to_csv('knn_cv_feat_eng.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b39173fc6fa6006444a5110f462641e5f1b49918","_cell_guid":"ae96d5ec-73a5-4d64-bd0d-539193751485"},"cell_type":"markdown","source":"The accuracy of KNN classifier after hyperparameter tuning is 75.69% which is not an improvement. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.4","name":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}