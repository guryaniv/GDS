{"cells":[{"metadata":{"_uuid":"c52aa2c857b9c96caacc0d21ca77952a6d366789","_cell_guid":"e4b26614-a2be-47b7-a352-3e6bce512fcc"},"cell_type":"markdown","source":"## Loading Data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b8c1bee4f3050a18c0829d48ea8274bc2b80ee28","collapsed":true,"_cell_guid":"ea0dd1d2-6c51-48c4-a9d7-8e343dedcba9","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d3d90053ad35edadb68825707dcc95630a20452","collapsed":true,"_cell_guid":"676ac7d8-22ad-497f-94a9-a8997e082b85","trusted":false},"cell_type":"code","source":"train=pd.read_table('../input/train.tsv')\ntest=pd.read_table('../input/test.tsv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c394dc2ab915590e519ec866d49f175252f5ec73","_cell_guid":"77225b71-3763-4f27-bfc0-bde7e0bbf540"},"cell_type":"markdown","source":"## Preprocessing","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a06869cf8bb6e321a9a560d543b730d924e6248b","collapsed":true,"_cell_guid":"b6c6450d-c20f-416e-a347-5fc76c9c2ab5","trusted":false},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c735be17a6c698e655dd8e00fa93199e6c12b5d","_cell_guid":"559121d7-a991-4689-88d8-fc48bfb67b04"},"cell_type":"markdown","source":"Brand name, category name, item condition and shipping are all categorical variables. We are going to encode those into proper formats for later processing, but right now we should take care of more pressing matters: dealing with missing values.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1f0cb58eeed8dd80e11bf91f366c826728e0109c","collapsed":true,"_cell_guid":"808b2664-5cb2-4907-a498-c58b6a525b44","trusted":false},"cell_type":"code","source":"train=train.fillna('missing')\ntest=test.fillna('missing')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab6711739903a17cbbeda75ef016b9af64b4648","_cell_guid":"d402e714-c0ae-49fd-af1e-ac9dbe0ddc8c"},"cell_type":"markdown","source":"For a first approximation, we just fill it in with the word 'missing', but this can certainly be improved in a few cases. We could try imputing values where possible, especially in the category section, by training a secondary classifier. This will probably remain for a further inspection, though.\n\nAt this point, a closer look at the category_name feature is worthwhile.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d265e2b76b60a737c76c037f2bf5e5e2e9160aca","_cell_guid":"8ef9666d-56e8-40bc-8b1d-8a029eee477f"},"cell_type":"markdown","source":"category_name seems to be split in 3 increasingly specific levels. It might be of interest to treat these separately.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a87d0eff4837b15aa4bb0f93c885f03ea456b0c2","collapsed":true,"_cell_guid":"7bd8ce84-a8b6-4eb7-b41e-02fbdfe5fb5a","trusted":false},"cell_type":"code","source":"allData=train.append(test).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45ff6741cc77e4c1caafd286897206c99ec0b5b3","collapsed":true,"_cell_guid":"0439dff8-47f0-4676-88f2-95d14265785c","trusted":false},"cell_type":"code","source":"del train\ndel test\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86d8ecf1a9d3fa9bd57e88bd21beb12f10a04670","collapsed":true,"_cell_guid":"feec38ee-7a41-438f-a867-bb2961406588","trusted":false},"cell_type":"code","source":"le=LabelEncoder()\nallData['c_lv3']=le.fit_transform(allData.category_name.ravel())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd37050a374c863f605baa6a3effe62e7bd96a0e","_cell_guid":"9f591f64-5f88-4425-9795-68960f33ed94"},"cell_type":"markdown","source":"The other 3 categorical features will be handled directly: Just encode them and be done with it.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b7bd1769a36f4dea5231ee75878d7d99751a37e0","collapsed":true,"_cell_guid":"6673bde6-a80a-4eef-9c72-c84f39a07648","trusted":false},"cell_type":"code","source":"allData['c_brand']=le.fit_transform(allData.brand_name.str.lower())\nallData['c_condition']=le.fit_transform(allData.item_condition_id)\nallData['c_shipping']=le.fit_transform(allData.shipping)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dacda4aa7c1991b853e30e20626ce3d964f94526","_cell_guid":"1dcdb9b8-128c-4be3-801a-b9db16fc4d39"},"cell_type":"markdown","source":"Now for the two most difficult features to handle: Description and Name. We start by first tokenizing (splitting the text into proper words) both of them.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1d90be6946932194b69eedb7366d86b1d0866930","collapsed":true,"_cell_guid":"7fe0e00f-87db-4d5f-a572-29702aa084b4","trusted":false},"cell_type":"code","source":"tokenizer=Tokenizer()\ntokenizer.fit_on_texts(allData.name.str.lower())\nallData['c_names']=pd.Series(tokenizer.texts_to_sequences(allData.name.str.lower()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dc8c96ea046269fc93196344632e80b6f0fba52","collapsed":true,"_cell_guid":"12a09f7b-16a0-4948-91f0-cc7260f49f8e","trusted":false},"cell_type":"code","source":"tokenizer.fit_on_texts(allData.item_description.str.lower())\nallData['c_descriptions']=pd.Series(tokenizer.texts_to_sequences(allData.item_description.str.lower()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d57c3dc01c163f55b0cac508bdea296dfbe6a3af","_cell_guid":"5ddfebd4-ce9d-4c86-8493-ad4e72ab6312"},"cell_type":"markdown","source":"In order to properly feed all this to a Neural Network, we need to standardize the length of all sequences. To accomplish this, we will pad all texts with dummy words until all of them are the same size of the biggest one. We start by gathering the maximum name and description length in words.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"90e700cf2007ba41d78e6144aabcee4467517333","collapsed":true,"_cell_guid":"f7b039d3-db1c-4635-8d2f-0788d468854b","trusted":false},"cell_type":"code","source":"max_desc=allData['c_descriptions'].apply(lambda x: len(x)).max()\nmax_name=allData['c_names'].apply(lambda x: len(x)).max()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"703f0c251832eece14e90f001bbc41d0efe02330","collapsed":true,"_cell_guid":"fbcc9874-0f5b-449c-979f-e6aeac6aaf21","trusted":false},"cell_type":"code","source":"dummy_name=allData['c_names'].apply(lambda x: max(x) if len(x)>=1 else 0).max()+1\ndummy_desc=allData['c_descriptions'].apply(lambda x: max(x) if len(x)>=1 else 0).max()+1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0fd507e62852118bfff272293a721b50c2d415d","_cell_guid":"59de402c-ea08-43cf-b08b-23e085535881"},"cell_type":"markdown","source":"Now we build a 'pad' function to apply to the sequences.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c7847ea505bf2c289e18b4101437c698b3b01193","collapsed":true,"_cell_guid":"dc705e6f-fe70-42c1-8113-9628db2ef6c5","trusted":false},"cell_type":"code","source":"def pad(sequence,maxlen,dummy_word):\n    lsequence=list(sequence)\n    if len(lsequence)>maxlen:\n        return sequence[:maxlen]\n    while len(lsequence)<maxlen:\n        lsequence.append(dummy_word)\n    return np.array(lsequence)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac004656bf598acb50c3708b9e33d0a1b16a03f9","_cell_guid":"5dbc8880-13ab-47f2-9516-2b0bda55d7f6"},"cell_type":"markdown","source":"Finally, we apply the function.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4d45c787b99dd83d6c40e9b90005f2429dc983a0","collapsed":true,"_cell_guid":"b6284b99-9870-4a4c-9967-7fd3cc11856f","trusted":false},"cell_type":"code","source":"allData['c_names']=allData['c_names'].apply(lambda x: pad(x,max_name,dummy_name))\nallData['c_descriptions']=allData['c_descriptions'].apply(lambda x: pad(x,60,dummy_desc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc7bc4de8d77e8c4aa4bf5bd54c8c7155c56654d","_cell_guid":"5d1aab6d-3cab-48b4-be5a-354ecf7a33e2"},"cell_type":"markdown","source":"Now all that is left is to convert all these DataFrames into numpy arrays so that Keras can deal with them. We will be dropping all data points that are sold for free before training those points will only serve to confuse the neural network. The increase in error for doing this should be smaller than what we would see if we tried to fit them.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"616bc4971b1257581abc4688492901ece1536d47","collapsed":true,"_cell_guid":"01d8d636-4932-481c-aacf-1ae473957c03","trusted":false},"cell_type":"code","source":"tr_data=allData[(allData.price.as_matrix()>=1.0) & (np.isnan(allData.test_id.as_matrix()))]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"858f91e53270793f5a80c8de0e13b3e5e0f12a53","collapsed":true,"_cell_guid":"a415e5f9-b094-4add-a678-ff86adca8582","trusted":false},"cell_type":"code","source":"names=np.array(list(tr_data.c_names))\ndescs=np.array(list(tr_data.c_descriptions))\ncategory3=tr_data.c_lv3.ravel()\nbrand=tr_data.c_brand.ravel()\ncondition=tr_data.c_condition.ravel()\nshipping=tr_data.c_shipping.ravel()\nlabels=tr_data.price.ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13ebbf886129592fe4ae6a1d126bbad67a4f71e0","_cell_guid":"a37514f0-aa7b-4217-85ff-14715fe7bf65"},"cell_type":"markdown","source":"Setting up vocab size for later embeddings...","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c94d151264eaae710457ea4e11346d813d3ed584","collapsed":true,"_cell_guid":"6edf86c5-7c9a-47bd-ab6b-4899ad139c85","trusted":false},"cell_type":"code","source":"max_c_lv3=allData.c_lv3.max()\nmax_brand=allData.c_brand.max()\nmax_condition=allData.c_condition.max()\nmax_shipping=allData.c_shipping.max()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f7a757605924423ddefa3792f602deaa4169fe7","_cell_guid":"4612f0cf-f64b-4e02-b0cc-7c638ee7e15e"},"cell_type":"markdown","source":"We also need to prepare our test set:","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ac9cfb2625a508dc11ab949eb000f345119f43c1","collapsed":true,"_cell_guid":"d3bf6b5d-781a-47a1-9d58-34a9b8f15fb1","trusted":false},"cell_type":"code","source":"te_data=allData[np.isnan(allData.train_id.as_matrix())]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cddc1ed22737f659b796b13f0463fc7d2d1573f6","collapsed":true,"_cell_guid":"96e58606-3fa2-4cbe-bedd-06394da56744","trusted":false},"cell_type":"code","source":"names_te=np.array(list(te_data.c_names))\ndescs_te=np.array(list(te_data.c_descriptions))\ncategory3_te=te_data.c_lv3.ravel()\nbrand_te=te_data.c_brand.ravel()\ncondition_te=te_data.c_condition.ravel()\nshipping_te=te_data.c_shipping.ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3a5040d661ee11d04def9e8cf52da8a2d1442ee","_cell_guid":"7149aba4-f2dd-4f29-8e50-f7ed478f2ba3"},"cell_type":"markdown","source":"Scaling the labels so that they are in a sensible range...","outputs":[],"execution_count":null},{"metadata":{"_uuid":"cbb5ed35c2198d49c6e01511fa0d45adb64e1821","collapsed":true,"_cell_guid":"a183a586-3609-45b8-b53d-de5a72446bb1","trusted":false},"cell_type":"code","source":"#s_labels=scaler.fit_transform(labels.reshape(-1,1))\ns_labels=np.log(labels+1.).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2887808a1a39cdcea26402bc0fc5f538ff7aeb","_cell_guid":"cdcec093-1d37-4a37-9705-b91e7037678a"},"cell_type":"markdown","source":"Getting the input variables in a dict...","outputs":[],"execution_count":null},{"metadata":{"_uuid":"aaae86b25d3253f7bba6ce068931692709cef300","collapsed":true,"_cell_guid":"37b3ac4b-455d-4f58-81d4-4df961c62691","trusted":false},"cell_type":"code","source":"X={'name':names,\n   'descriptions':descs,\n   #'category_level_1':category1,\n   #'category_level_2':category2,\n   'category_level_3':category3,\n   'brand':brand,\n   'condition':condition,\n   'shipping':shipping}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9936df059e6c687f6e7639c332f637a62a25fbaa","_cell_guid":"821585e1-85fe-4200-ad99-e9492857f3cb"},"cell_type":"markdown","source":"Including the test set...","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dbfaeaf773b5c2864ca37e1264c7831bdf96c3ca","collapsed":true,"_cell_guid":"e0e25f17-2fe2-4f68-9cfa-1e846784addb","trusted":false},"cell_type":"code","source":"X_te={'name':names_te,\n      'descriptions':descs_te,\n      #'category_level_1':category1,\n      #'category_level_2':category2,\n      'category_level_3':category3_te,\n      'brand':brand_te,\n      'condition':condition_te,\n      'shipping':shipping_te}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d22ab986873cd38b05db2ec09048445d9969bcde","collapsed":true,"_cell_guid":"66a48835-bc65-47f2-b3ad-c256e602168b","trusted":false},"cell_type":"code","source":"del names\ndel descs\ndel category3\ndel brand\ndel condition\ndel shipping\ndel labels\n\ndel names_te\ndel descs_te\ndel category3_te\ndel brand_te\ndel condition_te\ndel shipping_te\n\ndel allData\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d63bf3b8e0789f8e0245b0006d64fa15f1f5c81a","_cell_guid":"bc95c7f5-2569-42fc-b6ec-9b0b51e55d6e"},"cell_type":"markdown","source":"## Building a Keras Model","outputs":[],"execution_count":null},{"metadata":{"_uuid":"305d8079e60b4a86445bc225844ccd913d5a8590","_cell_guid":"937ad451-eaab-4710-9416-491987839ea9"},"cell_type":"markdown","source":"Now we must build our Neural Network model. We have two distinct types of features in this problem: Categorical variables and text variables. The treatment that we should give each of these should be, obviously, very distinct. We will be passing all our variables through an embedding layer first, to better build our feature space before we go into recursive + dense layers. The output is a single neuron with linear activation, as the target variable is a scalar value.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3961e287e14a700f94a7871684ce10c7677bf796","collapsed":true,"_cell_guid":"a0e56cd3-c6b0-45af-93ae-aec0cd1548a3","trusted":false},"cell_type":"code","source":"import keras.layers as kl\nimport keras.models as km\nimport keras.backend as K\nimport keras","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"866766eedc3b76814923cd281fe56a1d80f28438","collapsed":true,"_cell_guid":"eb37fba1-8eed-48f1-8704-a639a3cf3302","trusted":false},"cell_type":"code","source":"def schedule(e):\n    if e<2:\n        return 0.0013\n    elif e==2:\n        return 0.0012\n    else:\n        return 0.0011","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f80eac9bf26e46b6073cf4c3ec5a454346c5aa5b","_cell_guid":"f78e65da-4754-448d-bab2-fa67960501a7"},"cell_type":"markdown","source":"We now set up our layers one by one, starting with the inputs. Variable names that start with NN are Neural Network layers.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a33b516a83f21b01b48178b5801d7ca126688df5","collapsed":true,"_cell_guid":"3e969bbb-8917-441e-b1c1-56529ab01d98","trusted":false},"cell_type":"code","source":"#Inputs\nNN_names=kl.Input(shape=[X['name'].shape[1]],name='name')\nNN_descs=kl.Input(shape=[X['descriptions'].shape[1]],name='descriptions')\n#NN_cat1=kl.Input(shape=[1],name='category_level_1')\n#NN_cat2=kl.Input(shape=[1],name='category_level_2')\nNN_cat3=kl.Input(shape=[1],name='category_level_3')\nNN_brand=kl.Input(shape=[1],name='brand')\nNN_condition=kl.Input(shape=[1],name='condition')\nNN_shipping=kl.Input(shape=[1],name='shipping')\n\n#Embeddings\nNN_emb_name=kl.Embedding(dummy_name+1, 20)(NN_names)\nNN_emb_desc=kl.Embedding(dummy_desc+1, 30)(NN_descs)\n#NN_emb_cat1=kl.Embedding(max_c_lv1+1, 3)(NN_cat1)\n#NN_emb_cat2=kl.Embedding(max_c_lv2+1, 5)(NN_cat2)\nNN_emb_cat3=kl.Embedding(max_c_lv3+1, 8)(NN_cat3)\nNN_emb_brand=kl.Embedding(max_brand+1, 5)(NN_brand)\n\n#LSTM Layer\nNN_lstm_name=kl.LSTM(8)(NN_emb_name)\nNN_lstm_desc=kl.LSTM(20)(NN_emb_desc)\n\n#Main layer, joins all data\nNN_main=kl.concatenate([#kl.Flatten() (NN_emb_cat1),\n#                        kl.Flatten() (NN_emb_cat2),\n                        kl.Flatten() (NN_emb_cat3),\n                        kl.Flatten() (NN_emb_brand),\n                        NN_condition,\n                        NN_shipping,\n                        NN_lstm_name,\n                        NN_lstm_desc])\n\n#Add a dropout layer before two dense layers to process the whole picture\nNN_main=kl.Dropout(.1) (kl.Dense(128,activation='relu') (NN_main))\nNN_main=kl.Dropout(.1) (kl.Dense(64,activation='relu') (NN_main))\n\n#output\nNN_output=kl.Dense(1,activation='linear') (NN_main)\n\nmodel=km.Model([NN_names,NN_descs,NN_cat3,NN_brand,NN_condition,NN_shipping],NN_output)\nmodel.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(lr=0.0013,decay=0.0), metrics=[\"mae\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8e0395d13817c813cc757b76bd83b8fe53e7a18","_cell_guid":"6a01b996-d097-4f4d-8499-fbedc671699b"},"cell_type":"markdown","source":"And now we are ready to train!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1d052429b75cdd1f7943bdc067f1810dca1233de","collapsed":true,"_cell_guid":"37800259-ad02-4471-9bda-322341ed26cb","trusted":false},"cell_type":"code","source":"history=model.fit(X,s_labels,epochs=5,batch_size=15000,validation_split=0.0,callbacks=[keras.callbacks.LearningRateScheduler(schedule)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33bd1962262304e3463aef89a9515a77820562b9","_cell_guid":"b94ae744-9d4c-4ae4-aa3e-f6409b69de40"},"cell_type":"markdown","source":"## Outputs and Results","outputs":[],"execution_count":null},{"metadata":{"_uuid":"88d15d89e00a223461270efbaec9f00aa029b6a6","_cell_guid":"4157b89f-723c-4599-b73d-595b053e382b"},"cell_type":"markdown","source":"Preparing predictions into submission format and saving the csv file:","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d6c2f4a5c9b7c9f5946d95b2bd23f413747ebe2f","collapsed":true,"_cell_guid":"4d3eaaef-7476-4570-a2c8-94a0449b7c59","trusted":false},"cell_type":"code","source":"pd.DataFrame({'test_id':te_data.test_id.as_matrix().astype(int),\n              'price':(np.exp(model.predict(X_te))-1.).reshape(-1)}).to_csv('submissions.csv',\n                                                                             index=False,\n                                                                             header=True,columns=['test_id','price'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efaf032ae7108479acdc6dab0094262e5452dd9b","_cell_guid":"e55bb5aa-2526-45ee-9a15-d493ec513c64"},"cell_type":"markdown","source":"And we're done. Just as a curiosity, let's check the distribution plots for training and test prices to see how we're doing:","outputs":[],"execution_count":null},{"metadata":{"_uuid":"df0000dd2938e5ebee8b10689f50524710023b86","collapsed":true,"_cell_guid":"17a3558e-9df6-4b6e-bf65-58d46a50f359","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nsns.set_palette('nipy_spectral')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"111667405d89e9f305fef5aff610df179ab759d8","collapsed":true,"_cell_guid":"5aaab167-7683-47a5-bf3d-d4b01fcf280e","trusted":false},"cell_type":"code","source":"sns.distplot(s_labels)\nsns.distplot(np.log(pd.read_csv('submissions.csv').price.as_matrix()+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f9d754e212779588e3cb07537b732c693febae3","_cell_guid":"a21d610e-6165-4742-9f66-f73c15f80efb"},"cell_type":"markdown","source":"The shapes are pretty close, as a general rule. We can see the Neural Network fails to capture a lot of the noise inherent in the training set and kind of overpredicts values in the peak area of the distribution, having a shorter tail. If we had time, we could run a couple more epochs and see how that would improve things.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8b0b53d7e22061e06bf787ddee0aef2c95bf8536","collapsed":true,"_cell_guid":"865e5588-c3c1-4d5e-8575-0eee5b853374","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","mimetype":"text/x-python","name":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}