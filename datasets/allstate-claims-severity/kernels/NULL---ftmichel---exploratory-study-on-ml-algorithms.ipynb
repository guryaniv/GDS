{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "326306ca-a5d0-4828-f587-d6e4e3f84d09"
      },
      "source": [
        "Thank you for opening this script!\n",
        "\n",
        "I have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n",
        "\n",
        "Please **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n",
        "\n",
        "My other exploratory studies can be accessed here :\n",
        "https://www.kaggle.com/sharmasanthosh/kernels\n",
        "***\n",
        "## Data statistics\n",
        "* Shape\n",
        "* Peek\n",
        "* Description\n",
        "* Skew\n",
        "\n",
        "## Transformation\n",
        "* Correction of skew\n",
        "\n",
        "## Data Interaction\n",
        "* Correlation\n",
        "* Scatter plot\n",
        "\n",
        "## Data Visualization\n",
        "* Box and density plots\n",
        "* Grouping of one hot encoded attributes\n",
        "\n",
        "## Data Preparation\n",
        "* One hot encoding of categorical data\n",
        "* Test-train split\n",
        "\n",
        "## Evaluation, prediction, and analysis\n",
        "* Linear Regression (Linear algo)\n",
        "* Ridge Regression (Linear algo)\n",
        "* LASSO Linear Regression (Linear algo)\n",
        "* Elastic Net Regression (Linear algo)\n",
        "* KNN (non-linear algo)\n",
        "* CART (non-linear algo)\n",
        "* SVM (Non-linear algo)\n",
        "* Bagged Decision Trees (Bagging)\n",
        "* Random Forest (Bagging)\n",
        "* Extra Trees (Bagging)\n",
        "* AdaBoost (Boosting)\n",
        "* Stochastic Gradient Boosting (Boosting)\n",
        "* MLP (Deep Learning)\n",
        "* XGBoost\n",
        "\n",
        "## Make Predictions\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d9dd03a6-5daa-e559-b1c3-23b330f330b7"
      },
      "source": [
        "## Load raw data:\n",
        "\n",
        "Information about all the attributes can be found here:\n",
        "\n",
        "https://www.kaggle.com/c/allstate-claims-severity/data\n",
        "\n",
        "Learning: \n",
        "We need to predict the 'loss' based on the other attributes. Hence, this is a regression problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c463d4f-7dcc-42b3-500e-fe0aecd73c4c"
      },
      "outputs": [],
      "source": [
        "# Supress unnecessary warnings so that presentation looks clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Read raw data from the file\n",
        "\n",
        "import pandas #provides data structures to quickly analyze data\n",
        "#Since this code runs on Kaggle server, data can be accessed directly in the 'input' folder\n",
        "#Read the train dataset\n",
        "dataset = pandas.read_csv(\"../input/train.csv\") \n",
        "\n",
        "#Read test dataset\n",
        "dataset_test = pandas.read_csv(\"../input/test.csv\")\n",
        "#Save the id's for submission file\n",
        "ID = dataset_test['id']\n",
        "#Drop unnecessary columns\n",
        "dataset_test.drop('id',axis=1,inplace=True)\n",
        "\n",
        "#Print all rows and columns. Dont hide any\n",
        "pandas.set_option('display.max_rows', None)\n",
        "pandas.set_option('display.max_columns', None)\n",
        "\n",
        "#Display the first five rows to get a feel of the data\n",
        "print(dataset.head(5))\n",
        "\n",
        "#Learning : cat1 to cat116 contain alphabets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e9d7e580-ca0d-1939-e037-ce6e53077be0"
      },
      "source": [
        "## Data statistics\n",
        "* Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63dfe415-ba1f-0b31-f75a-a8be177ca134"
      },
      "outputs": [],
      "source": [
        "# Size of the dataframe\n",
        "\n",
        "print(dataset.shape)\n",
        "\n",
        "# We can see that there are 188318 instances having 132 attributes\n",
        "\n",
        "#Drop the first column 'id' since it just has serial numbers. Not useful in the prediction process.\n",
        "dataset = dataset.iloc[:,1:]\n",
        "\n",
        "#Learning : Data is loaded successfully as dimensions match the data description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9dc622e7-2f8b-90a6-1f0d-da7d442c4994"
      },
      "source": [
        "## Data statistics\n",
        "* Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "390f4a98-b5c0-0f08-b483-61a8ea998022"
      },
      "outputs": [],
      "source": [
        "# Statistical description\n",
        "\n",
        "print(dataset.describe())\n",
        "\n",
        "# Learning :\n",
        "# No attribute in continuous columns is missing as count is 188318 for all, all rows can be used\n",
        "# No negative values are present. Tests such as chi2 can be used\n",
        "# Statistics not displayed for categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "256cafeb-2554-1eca-2c85-d194cc734593"
      },
      "source": [
        "## Data statistics\n",
        "* Skew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa552a5f-3792-4d0f-155b-b590a7a15a98"
      },
      "outputs": [],
      "source": [
        "# Skewness of the distribution\n",
        "\n",
        "print(dataset.skew())\n",
        "\n",
        "# Values close to 0 show less ske\n",
        "# loss shows the highest skew. Let us visualize it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5de92921-6b8f-83ca-65f3-3813ca366bcd"
      },
      "source": [
        "## Data Visualization\n",
        "* Box and density plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a171815-f5ba-f451-33e5-456e8ded239c"
      },
      "outputs": [],
      "source": [
        "# We will visualize all the continuous attributes using Violin Plot - a combination of box and density plots\n",
        "\n",
        "import numpy\n",
        "\n",
        "#import plotting libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#range of features considered\n",
        "split = 116 \n",
        "\n",
        "#number of features considered\n",
        "size = 15\n",
        "\n",
        "#create a dataframe with only continuous features\n",
        "data=dataset.iloc[:,split:] \n",
        "\n",
        "#get the names of all the columns\n",
        "cols=data.columns \n",
        "\n",
        "#Plot violin for all attributes in a 7x2 grid\n",
        "n_cols = 2\n",
        "n_rows = 7\n",
        "\n",
        "for i in range(n_rows):\n",
        "    fg,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12, 8))\n",
        "    for j in range(n_cols):\n",
        "        sns.violinplot(y=cols[i*n_cols+j], data=dataset, ax=ax[j])\n",
        "\n",
        "\n",
        "#cont1 has many values close to 0.5\n",
        "#cont2 has a pattern where there a several spikes at specific points\n",
        "#cont5 has many values near 0.3\n",
        "#cont14 has a distinct pattern. 0.22 and 0.82 have a lot of concentration\n",
        "#loss distribution must be converted to normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3b52e6c3-6d33-bfcd-417a-e42993019e57"
      },
      "source": [
        "## Data Transformation\n",
        "* Skew correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "991f616d-056f-c31d-51b7-a5495f52b1a5"
      },
      "outputs": [],
      "source": [
        "#log1p function applies log(1+x) to all elements of the column\n",
        "dataset[\"loss\"] = numpy.log1p(dataset[\"loss\"])\n",
        "#visualize the transformed column\n",
        "sns.violinplot(data=dataset,y=\"loss\")  \n",
        "plt.show()\n",
        "\n",
        "#Plot shows that skew is corrected to a large extent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e01c75f0-d1e7-cb29-a9ea-6a1b169fd3f3"
      },
      "source": [
        "## Data Interaction\n",
        "* Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "497cc626-14f9-d813-bf2a-e99dcc704492"
      },
      "outputs": [],
      "source": [
        "# Correlation tells relation between two attributes.\n",
        "# Correlation requires continous data. Hence, ignore categorical data\n",
        "\n",
        "# Calculates pearson co-efficient for all combinations\n",
        "data_corr = data.corr()\n",
        "\n",
        "# Set the threshold to select only highly correlated attributes\n",
        "threshold = 0.5\n",
        "\n",
        "# List of pairs along with correlation above threshold\n",
        "corr_list = []\n",
        "\n",
        "#Search for the highly correlated pairs\n",
        "for i in range(0,size): #for 'size' features\n",
        "    for j in range(i+1,size): #avoid repetition\n",
        "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
        "            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n",
        "\n",
        "#Sort to show higher ones first            \n",
        "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n",
        "\n",
        "#Print correlations and column names\n",
        "for v,i,j in s_corr_list:\n",
        "    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n",
        "\n",
        "# Strong correlation is observed between the following pairs\n",
        "# This represents an opportunity to reduce the feature set through transformations such as PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3cb5b864-ec2b-07c4-659f-09760f78620a"
      },
      "source": [
        "## Data Interaction\n",
        "* Scatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e7fc7f5-f9a6-4b5c-b176-b555174c8772"
      },
      "outputs": [],
      "source": [
        "# Scatter plot of only the highly correlated pairs\n",
        "for v,i,j in s_corr_list:\n",
        "    sns.pairplot(dataset, size=6, x_vars=cols[i],y_vars=cols[j] )\n",
        "    plt.show()\n",
        "\n",
        "#cont11 and cont12 give an almost linear pattern...one must be removed\n",
        "#cont1 and cont9 are highly correlated ...either of them could be safely removed \n",
        "#cont6 and cont10 show very good correlation too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bcd4e9e4-da38-a95b-62ea-4dedb5a267c5"
      },
      "source": [
        "## Data Visualization\n",
        "* Categorical attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6fdc916f-2eb8-6fc8-67b2-02bf997a55d9"
      },
      "outputs": [],
      "source": [
        "# Count of each label in each category\n",
        "\n",
        "#names of all the columns\n",
        "cols = dataset.columns\n",
        "\n",
        "#Plot count plot for all attributes in a 29x4 grid\n",
        "n_cols = 4\n",
        "n_rows = 29\n",
        "for i in range(n_rows):\n",
        "    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n",
        "    for j in range(n_cols):\n",
        "        sns.countplot(x=cols[i*n_cols+j], data=dataset, ax=ax[j])\n",
        "\n",
        "#cat1 to cat72 have only two labels A and B. In most of the cases, B has very few entries\n",
        "#cat73 to cat 108 have more than two labels\n",
        "#cat109 to cat116 have many labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "816254c9-6ceb-3777-099a-d6c3fa2de438"
      },
      "source": [
        "##Data Preparation\n",
        "* One Hot Encoding of categorical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90e603ad-40f9-aa94-db4f-aa87679e30c8"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "\n",
        "#cat1 to cat116 have strings. The ML algorithms we are going to study require numberical data\n",
        "#One-hot encoding converts an attribute to a binary vector\n",
        "\n",
        "#Variable to hold the list of variables for an attribute in the train and test data\n",
        "labels = []\n",
        "\n",
        "for i in range(0,split):\n",
        "    train = dataset[cols[i]].unique()\n",
        "    test = dataset_test[cols[i]].unique()\n",
        "    labels.append(list(set(train) | set(test)))    \n",
        "\n",
        "del dataset_test\n",
        "\n",
        "#Import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#One hot encode all categorical attributes\n",
        "cats = []\n",
        "for i in range(0, split):\n",
        "    #Label encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(labels[i])\n",
        "    feature = label_encoder.transform(dataset.iloc[:,i])\n",
        "    feature = feature.reshape(dataset.shape[0], 1)\n",
        "    #One hot encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
        "    feature = onehot_encoder.fit_transform(feature)\n",
        "    cats.append(feature)\n",
        "\n",
        "# Make a 2D array from a list of 1D arrays\n",
        "encoded_cats = numpy.column_stack(cats)\n",
        "\n",
        "# Print the shape of the encoded data\n",
        "print(encoded_cats.shape)\n",
        "\n",
        "#Concatenate encoded attributes with continuous attributes\n",
        "dataset_encoded = numpy.concatenate((encoded_cats,dataset.iloc[:,split:].values),axis=1)\n",
        "del cats\n",
        "del feature\n",
        "del dataset\n",
        "del encoded_cats\n",
        "print(dataset_encoded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e679f431-acf3-d7ed-4307-f92424338379"
      },
      "source": [
        "##Data Preparation\n",
        "* Split into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "584b6d52-b4e3-b50c-e758-cb56a993f9cc"
      },
      "outputs": [],
      "source": [
        "#get the number of rows and columns\n",
        "r, c = dataset_encoded.shape\n",
        "\n",
        "#create an array which has indexes of columns\n",
        "i_cols = []\n",
        "for i in range(0,c-1):\n",
        "    i_cols.append(i)\n",
        "\n",
        "#Y is the target column, X has the rest\n",
        "X = dataset_encoded[:,0:(c-1)]\n",
        "Y = dataset_encoded[:,(c-1)]\n",
        "del dataset_encoded\n",
        "\n",
        "#Validation chunk size\n",
        "val_size = 0.1\n",
        "\n",
        "#Use a common seed in all experiments so that same chunk is used for validation\n",
        "seed = 0\n",
        "\n",
        "#Split the data into chunks\n",
        "from sklearn import cross_validation\n",
        "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\n",
        "del X\n",
        "del Y\n",
        "\n",
        "#All features\n",
        "X_all = []\n",
        "\n",
        "#List of combinations\n",
        "comb = []\n",
        "\n",
        "#Dictionary to store the MAE for all algorithms \n",
        "mae = []\n",
        "\n",
        "#Scoring parameter\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#Add this version of X to the list \n",
        "n = \"All\"\n",
        "#X_all.append([n, X_train,X_val,i_cols])\n",
        "X_all.append([n, i_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ab93796-e594-f865-67ad-144398f774de"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Linear Regression (Linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ca65813-470a-d333-4d79-2071fb5f6ec4"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of LinearRegression\n",
        "\n",
        "#Import the library\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#uncomment the below lines if you want to run the algo\n",
        "##Set the base model\n",
        "#model = LinearRegression(n_jobs=-1)\n",
        "#algo = \"LR\"\n",
        "#\n",
        "##Accuracy of the model using all features\n",
        "#for name,i_cols_list in X_all:\n",
        "#    model.fit(X_train[:,i_cols_list],Y_train)\n",
        "#    result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "#    mae.append(result)\n",
        "#    print(name + \" %s\" % result)\n",
        "#comb.append(algo)\n",
        "\n",
        "#Result obtained after running the algo. Comment the below two lines if you want to run the algo\n",
        "mae.append(1278)\n",
        "comb.append(\"LR\" )    \n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#MAE achieved is 1278"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a69eb393-e11d-7f80-e0e8-61a345e8dc9c"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Ridge Regression (Linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d04ac8d0-0623-269b-1d71-e7f53bd8f452"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of Ridge LinearRegression\n",
        "\n",
        "#Import the library\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Add the alpha value to the below list if you want to run the algo\n",
        "a_list = numpy.array([])\n",
        "\n",
        "for alpha in a_list:\n",
        "    #Set the base model\n",
        "    model = Ridge(alpha=alpha,random_state=seed)\n",
        "    \n",
        "    algo = \"Ridge\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % alpha )\n",
        "\n",
        "#Result obtained by running the algo for alpha=1.0    \n",
        "if (len(a_list)==0):\n",
        "    mae.append(1267.5)\n",
        "    comb.append(\"Ridge\" + \" %s\" % 1.0 )    \n",
        "    \n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is 1267 with alpha=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9419425-a8f4-315e-31e9-d65648224f07"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* LASSO Linear Regression (Linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b034d80-92fa-ba98-7bc0-b41165b5c244"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of Lasso LinearRegression\n",
        "\n",
        "#Import the library\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "#Add the alpha value to the below list if you want to run the algo\n",
        "a_list = numpy.array([])\n",
        "\n",
        "for alpha in a_list:\n",
        "    #Set the base model\n",
        "    model = Lasso(alpha=alpha,random_state=seed)\n",
        "    \n",
        "    algo = \"Lasso\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % alpha )\n",
        "\n",
        "#Result obtained by running the algo for alpha=0.001    \n",
        "if (len(a_list)==0):\n",
        "    mae.append(1262.5)\n",
        "    comb.append(\"Lasso\" + \" %s\" % 0.001 )\n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#High computation time\n",
        "#Best estimated performance is 1262.5 for alpha = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f4f695a5-49b2-709f-d74d-e1dfa17c6b95"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Elastic Net Regression (Linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f30aa83-61a3-8720-2960-bc08f0cc4032"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of ElasticNet LinearRegression\n",
        "\n",
        "#Import the library\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "#Add the alpha value to the below list if you want to run the algo\n",
        "a_list = numpy.array([])\n",
        "\n",
        "for alpha in a_list:\n",
        "    #Set the base model\n",
        "    model = ElasticNet(alpha=alpha,random_state=seed)\n",
        "    \n",
        "    algo = \"Elastic\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % alpha )\n",
        "\n",
        "if (len(a_list)==0):\n",
        "    mae.append(1260)\n",
        "    comb.append(\"Elastic\" + \" %s\" % 0.001 )\n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#High computation time\n",
        "#Best estimated performance is 1260 for alpha = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5eec3a22-fb38-2876-4069-4171710a199a"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* KNN (non-linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "201e9309-153a-65f4-2616-43b2a6c51163"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of KNN\n",
        "\n",
        "#Import the library\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "#Add the N value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_neighbors in n_list:\n",
        "    #Set the base model\n",
        "    model = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\n",
        "    \n",
        "    algo = \"KNN\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_neighbors )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1745)\n",
        "    comb.append(\"KNN\" + \" %s\" % 1 )\n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Very high computation time\n",
        "#Best estimated performance is 1745 for n=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e04fbf8f-e13a-5bea-fb83-2a066fbea25f"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* CART (non-linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2fe4450-2d9e-cea1-c4ac-4577fe29ace5"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of CART\n",
        "\n",
        "#Import the library\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#Add the max_depth value to the below list if you want to run the algo\n",
        "d_list = numpy.array([])\n",
        "\n",
        "for max_depth in d_list:\n",
        "    #Set the base model\n",
        "    model = DecisionTreeRegressor(max_depth=max_depth,random_state=seed)\n",
        "    \n",
        "    algo = \"CART\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % max_depth )\n",
        "\n",
        "if (len(a_list)==0):\n",
        "    mae.append(1741)\n",
        "    comb.append(\"CART\" + \" %s\" % 5 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#High computation time\n",
        "#Best estimated performance is 1741 for depth=5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fbfb9819-cb69-88c6-13de-ebe529306e1b"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* SVM (Non-linear algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02470cad-9931-1644-eea7-85fcc31d2c40"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of SVM\n",
        "\n",
        "#Import the library\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#Add the C value to the below list if you want to run the algo\n",
        "c_list = numpy.array([])\n",
        "\n",
        "for C in c_list:\n",
        "    #Set the base model\n",
        "    model = SVR(C=C)\n",
        "    \n",
        "    algo = \"SVM\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % C )\n",
        "\n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#very very high computation time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e0e9b735-5757-d3da-d0fa-1443193ce83d"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Bagged Decision Trees (Bagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0bb62d33-2b7c-fb3f-ed6e-e4d6fe6d34ed"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of Bagged Decision Trees\n",
        "\n",
        "#Import the library\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\n",
        "    \n",
        "    algo = \"Bag\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#very high computation time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "47762d5b-f1df-3c26-4fc1-4eb29cf0275c"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Random Forest (Bagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a247d11-e71b-a2a4-69ad-e5d3fdb57b13"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of RandomForest\n",
        "\n",
        "#Import the library\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = RandomForestRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n",
        "    \n",
        "    algo = \"RF\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1213)\n",
        "    comb.append(\"RF\" + \" %s\" % 50 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is 1213 when the number of estimators is 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cb5b0c4f-1cac-01d3-1e86-d3502c05b369"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Extra Trees (Bagging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68d51895-60c4-81a9-c7f1-92f9c9637b95"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of ExtraTrees\n",
        "\n",
        "#Import the library\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = ExtraTreesRegressor(n_jobs=-1,n_estimators=n_estimators,random_state=seed)\n",
        "    \n",
        "    algo = \"ET\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1254)\n",
        "    comb.append(\"ET\" + \" %s\" % 100 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is 1254 for 100 estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f0cc6bf4-4448-ca0f-6155-a011a92828dd"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* AdaBoost (Boosting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6a61f56-fd32-ab56-b645-73d0c511ea61"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of AdaBoost\n",
        "\n",
        "#Import the library\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = AdaBoostRegressor(n_estimators=n_estimators,random_state=seed)\n",
        "    \n",
        "    algo = \"Ada\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1678)\n",
        "    comb.append(\"Ada\" + \" %s\" % 100 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is 1678 with n=100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "618b74ca-39cb-041f-7a47-fa15757b40af"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* Stochastic Gradient Boosting (Boosting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb8c5701-5223-d7c9-9c5a-9470b389cc15"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of SGB\n",
        "\n",
        "#Import the library\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = GradientBoostingRegressor(n_estimators=n_estimators,random_state=seed)\n",
        "    \n",
        "    algo = \"SGB\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1278)\n",
        "    comb.append(\"SGB\" + \" %s\" % 50 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2526bf7f-5c5a-a067-facd-dadecfbb8019"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2fad9ca-8d6b-bdb9-b4b8-b24a9fa7ac46"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of XGB\n",
        "\n",
        "#Import the library\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#Add the n_estimators value to the below list if you want to run the algo\n",
        "n_list = numpy.array([])\n",
        "\n",
        "for n_estimators in n_list:\n",
        "    #Set the base model\n",
        "    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n",
        "    \n",
        "    algo = \"XGB\"\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for name,i_cols_list in X_all:\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo + \" %s\" % n_estimators )\n",
        "\n",
        "if (len(n_list)==0):\n",
        "    mae.append(1169)\n",
        "    comb.append(\"XGB\" + \" %s\" % 1000 )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "##Plot the MAE of all combinations\n",
        "#fig, ax = plt.subplots()\n",
        "#plt.plot(mae)\n",
        "##Set the tick names to names of combinations\n",
        "#ax.set_xticks(range(len(comb)))\n",
        "#ax.set_xticklabels(comb,rotation='vertical')\n",
        "##Plot the accuracy for all combinations\n",
        "#plt.show()    \n",
        "\n",
        "#Best estimated performance is 1169 with n=1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1f0e24c8-693b-3b56-f152-b97f06dc5e75"
      },
      "source": [
        "## Evaluation, prediction, and analysis\n",
        "* MLP (Deep Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6100543b-7713-162c-852b-86dd7c96cb2c"
      },
      "outputs": [],
      "source": [
        "#Evaluation of various combinations of multi-layer perceptrons\n",
        "\n",
        "#Import libraries for deep learning\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# define baseline model\n",
        "def baseline(v):\n",
        "     # create model\n",
        "     model = Sequential()\n",
        "     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
        "     model.add(Dense(1, init='normal'))\n",
        "     # Compile model\n",
        "     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "     return model\n",
        "\n",
        "# define smaller model\n",
        "def smaller(v):\n",
        "     # create model\n",
        "     model = Sequential()\n",
        "     model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n",
        "     model.add(Dense(1, init='normal', activation='relu'))\n",
        "     # Compile model\n",
        "     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "     return model\n",
        "\n",
        "# define deeper model\n",
        "def deeper(v):\n",
        " # create model\n",
        " model = Sequential()\n",
        " model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
        " model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n",
        " model.add(Dense(1, init='normal', activation='relu'))\n",
        " # Compile model\n",
        " model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        " return model\n",
        "\n",
        "# Optimize using dropout and decay\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dropout\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "def dropout(v):\n",
        "    #create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, init='normal', activation='relu'))\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n",
        "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
        "    return model\n",
        "\n",
        "# define decay model\n",
        "def decay(v):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n",
        "    model.add(Dense(1, init='normal', activation='relu'))\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n",
        "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
        "    return model\n",
        "\n",
        "est_list = []\n",
        "#uncomment the below if you want to run the algo\n",
        "#est_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n",
        "\n",
        "for name, est in est_list:\n",
        " \n",
        "    algo = name\n",
        "\n",
        "    #Accuracy of the model using all features\n",
        "    for m,i_cols_list in X_all:\n",
        "        model = KerasRegressor(build_fn=est, v=1, nb_epoch=10, verbose=0)\n",
        "        model.fit(X_train[:,i_cols_list],Y_train)\n",
        "        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n",
        "        mae.append(result)\n",
        "        print(name + \" %s\" % result)\n",
        "        \n",
        "    comb.append(algo )\n",
        "\n",
        "if (len(est_list)==0):\n",
        "    mae.append(1168)\n",
        "    comb.append(\"MLP\" + \" baseline\" )    \n",
        "    \n",
        "##Set figure size\n",
        "#plt.rc(\"figure\", figsize=(25, 10))\n",
        "\n",
        "#Plot the MAE of all combinations\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(mae)\n",
        "#Set the tick names to names of combinations\n",
        "ax.set_xticks(range(len(comb)))\n",
        "ax.set_xticklabels(comb,rotation='vertical')\n",
        "#Plot the accuracy for all combinations\n",
        "plt.show()    \n",
        "\n",
        "#Best estimated performance is MLP=1168"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c5ef128-af5a-7c4e-87be-85676139ef5b"
      },
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a2f64707-4996-e4b2-11d7-41cefa618da1"
      },
      "outputs": [],
      "source": [
        "# Make predictions using XGB as it gave the best estimated performance        \n",
        "\n",
        "X = numpy.concatenate((X_train,X_val),axis=0)\n",
        "del X_train\n",
        "del X_val\n",
        "Y = numpy.concatenate((Y_train,Y_val),axis=0)\n",
        "del Y_train\n",
        "del Y_val\n",
        "\n",
        "n_estimators = 1000\n",
        "\n",
        "#Best model definition\n",
        "best_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n",
        "best_model.fit(X,Y)\n",
        "del X\n",
        "del Y\n",
        "#Read test dataset\n",
        "dataset_test = pandas.read_csv(\"../input/test.csv\")\n",
        "#Drop unnecessary columns\n",
        "ID = dataset_test['id']\n",
        "dataset_test.drop('id',axis=1,inplace=True)\n",
        "\n",
        "#One hot encode all categorical attributes\n",
        "cats = []\n",
        "for i in range(0, split):\n",
        "    #Label encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(labels[i])\n",
        "    feature = label_encoder.transform(dataset_test.iloc[:,i])\n",
        "    feature = feature.reshape(dataset_test.shape[0], 1)\n",
        "    #One hot encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
        "    feature = onehot_encoder.fit_transform(feature)\n",
        "    cats.append(feature)\n",
        "\n",
        "# Make a 2D array from a list of 1D arrays\n",
        "encoded_cats = numpy.column_stack(cats)\n",
        "\n",
        "del cats\n",
        "\n",
        "#Concatenate encoded attributes with continuous attributes\n",
        "X_test = numpy.concatenate((encoded_cats,dataset_test.iloc[:,split:].values),axis=1)\n",
        "\n",
        "del encoded_cats\n",
        "del dataset_test\n",
        "\n",
        "#Make predictions using the best model\n",
        "predictions = numpy.expm1(best_model.predict(X_test))\n",
        "del X_test\n",
        "# Write submissions to output file in the correct format\n",
        "with open(\"submission.csv\", \"w\") as subfile:\n",
        "    subfile.write(\"id,loss\\n\")\n",
        "    for i, pred in enumerate(list(predictions)):\n",
        "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}