{"cells":[{"metadata":{"_cell_guid":"9e9dca22-b608-431e-8b77-082bccbf26c3","_uuid":"771498df676a38f17459b7e4d8387f0f8e67e51f"},"cell_type":"markdown","source":"Mens Tourney Prediction Analysis\n\nI the believe the following are important in determing a teams success in the tourney\n\n1) Seeding\n2) Strength of Conference\n3) Individual team statistics\n4) Experience\n\n\n"},{"metadata":{"_cell_guid":"43bad430-365d-4080-a8b4-95b6d38ac5d0","_uuid":"1502eb8556f1ccd5ace2724e4360e3efb3aef320","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom math import pi\nimport seaborn as sns\nimport time\nimport math\nfrom scipy.stats import skew, skewtest, norm\nimport os\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing, metrics, ensemble, model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.preprocessing import normalize\n\npd.set_option('display.max_columns', 999)\npd.options.display.float_format = '{:.6f}'.format\n\nstart_time = time.time()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"b0c80706-5fe1-451d-9f36-f38fa71b4e51","_uuid":"692155783153697cfb3d11eed8ba794703dccb54","collapsed":true,"trusted":true},"cell_type":"code","source":"#standard files\n\n#df_tourney = pd.read_csv('NCAATourneyCompactResults.csv')\n#df_season = pd.read_csv('RegularSeasonDetailedResults.csv')\n#df_teams = pd.read_csv('Teams.csv')\n#df_seeds = pd.read_csv('NCAATourneySeeds.csv')\n#df_conferences = pd.read_csv('Conferences.csv')\n#df_rankings = pd.read_csv('MasseyOrdinals.csv')\n#df_rankings2 = pd.read_csv('MasseyOrdinals_2018_133_only_43Systems.csv')\n\n#df_sample_sub1 = pd.read_csv('SampleSubmissionStage1.csv')\n#df_sample_sub = pd.read_csv('SampleSubmissionStage2.csv')\n#df_team_conferences = pd.read_csv('Teamconferences.csv')\n#df_ConferenceTourneyGames = pd.read_csv('ConferenceTourneyGames.csv')\n\n#my custom file\n\n#df_tourney_experience = pd.read_csv('tourney_experience_senior_class.csv')\n\n# Kaggle locations\n\ndf_tourney = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneyCompactResults.csv')\ndf_season = pd.read_csv('../input/mens-machine-learning-competition-2018/RegularSeasonDetailedResults.csv')\ndf_teams = pd.read_csv('../input/mens-machine-learning-competition-2018/Teams.csv')\ndf_seeds = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneySeeds.csv')\ndf_conferences = pd.read_csv('../input/mens-machine-learning-competition-2018/Conferences.csv')\ndf_rankings = pd.read_csv('../input/mens-machine-learning-competition-2018/MasseyOrdinals.csv')\ndf_sample_sub1 = pd.read_csv('../input/mens-machine-learning-competition-2018/SampleSubmissionStage1.csv')\ndf_sample_sub = pd.read_csv('../input/mens-machine-learning-competition-2018/SampleSubmissionStage2.csv')\ndf_team_conferences = pd.read_csv('../input/mens-machine-learning-competition-2018/TeamConferences.csv')\ndf_ConferenceTourneyGames = pd.read_csv('../input/mens-machine-learning-competition-2018/ConferenceTourneyGames.csv')\n\n\n#private data file\n\ndf_tourney_experience = pd.read_csv('../input/ncaa-tourney-experience/Tourney_Experience_Senior_Class.csv')\ndf_rankings2 = pd.read_csv('../input/massey-ordinals-2018-update/MasseyOrdinals_2018_133_only_43Systems.csv')\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"a383936c-fa08-44e4-aeb7-aa7314d94d18","_uuid":"7458efd353d2c14ae131c474780829d7a90e6cee","collapsed":true,"trusted":true},"cell_type":"code","source":"# What is logloss (This is what the competition is based on)?\n\n#The graph below shows the range of possible log loss values given a true observation (Prediction = 1). \n\n#As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, \n#however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications\n#that are confident and wrong!\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec4612f5-07d2-4282-b5bb-c73a543ef8fb","_uuid":"a20cec5b2038537a13320f2b2f597b62b9aa3cb6","collapsed":true,"trusted":true},"cell_type":"code","source":"def logloss(true_label, predicted, eps=1e-15):\n  p = np.clip(predicted, eps, 1 - eps)\n  if true_label == 1:\n    return -(math.log(p))\n  else:\n    return -(math.log(1 - p))\n\ny = []\nvalue = []\nx = []\n\nfor i in range(1,100):\n    \n    value = logloss(1,i/100, eps=1e-15)\n    y.append(value)\n    x.append(i/100)\n\nGraph = pd.DataFrame(np.array(x))    \nGraph['Log_Loss'] =  np.array(y)  \n\nf, ax = plt.subplots(figsize=(11, 7))\nsns.regplot(x = Graph.iloc[:, 0:1], y = Graph['Log_Loss'], data=Graph)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eff73f73-9577-4e2c-937f-5c66ba349659","_uuid":"8c655edc823a8f6271efe98509f6171229d341e6","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c41506c-8e13-48ac-93c2-1faa9b444c1b","_uuid":"e2f5b1de5c11add168bd3d90420fc168d7c7cc37","collapsed":true,"trusted":true},"cell_type":"code","source":"#Calculate Winning/losing Team Possesion Feature\n\n#https://www.nbastuffer.com/analytics101/possession/\n\n\nwPos = df_season.apply(lambda row: 0.96*(row.WFGA + row.WTO + 0.44*row.WFTA - row.WOR), axis=1)\nlPos = df_season.apply(lambda row: 0.96*(row.LFGA + row.LTO + 0.44*row.LFTA - row.LOR), axis=1)\n\n#two teams use almost the same number of possessions in a game\n#(plus/minus one or two - depending on how quarters end)\n#so let's just take the average\n\ndf_season['Possesions'] = (wPos+lPos)/2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"564937b6-a34f-4a78-be84-20bf26c2cf89","_uuid":"928bddecbefede6247456008a3e61d42fa390b1f","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c35c48ca-c97c-4d9a-8ea8-e1ccf2d6805f","_uuid":"6563467a57abe854b1581ceef8c2486f2efbb073","collapsed":true,"trusted":true},"cell_type":"code","source":"# Name Player Impact Estimate Definition PIE measures a player's overall statistical contribution\n# against the total statistics in games they play in. PIE yields results which are\n# comparable to other advanced statistics (e.g. PER) using a simple formula.\n# Formula (PTS + FGM + FTM - FGA - FTA + DREB + (.5 * OREB) + AST + STL + (.5 * BLK) - PF - TO)\n# / (GmPTS + GmFGM + GmFTM - GmFGA - GmFTA + GmDREB + (.5 * GmOREB) + GmAST + GmSTL + (.5 * GmBLK) - GmPF - GmTO)\n\n# We will use this to measure Team Skill\n\nwtmp = df_season.apply(lambda row: row.WScore + row.WFGM + row.WFTM - row.WFGA - row.WFTA + row.WDR + 0.5*row.WOR + row.WAst +row.WStl + 0.5*row.WBlk - row.WPF - row.WTO, axis=1)\nltmp = df_season.apply(lambda row: row.LScore + row.LFGM + row.LFTM - row.LFGA - row.LFTA + row.LDR + 0.5*row.LOR + row.LAst +row.LStl + 0.5*row.LBlk - row.LPF - row.LTO, axis=1) \n\ndf_season['WPIE'] = wtmp/(wtmp + ltmp)\ndf_season['LPIE'] = ltmp/(wtmp + ltmp)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90c0b4b8-a6aa-4f2e-a63b-599a80b9da33","_uuid":"7507449b3e29df4bd4b282ff5ea47f033cf4c1c5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Four factors statistic from the NBA\n\n# https://www.nbastuffer.com/analytics101/four-factors/\n\n\n# Effective Field Goal Percentage=(Field Goals Made) + 0.5*3P Field Goals Made))/(Field Goal Attempts)\n# you have to put the ball in the bucket eventually\n\ndf_season['WeFGP'] = df_season.apply(lambda row:(row.WFGM + 0.5 * row.WFGM3) / row.WFGA, axis=1)      \ndf_season['LeFGP'] = df_season.apply(lambda row:(row.LFGM + 0.5 * row.LFGM3) / row.LFGA, axis=1) \n\n# Turnover Rate= Turnovers/(Field Goal Attempts + 0.44*Free Throw Attempts + Turnovers)\n# he who doesnt turn the ball over wins games\n\ndf_season['WTOR'] = df_season.apply(lambda row: row.WTO / (row.WFGA + 0.44*row.WFTA + row.WTO), axis=1)\ndf_season['LTOR'] = df_season.apply(lambda row: row.LTO / (row.LFGA + 0.44*row.LFTA + row.LTO), axis=1)\n\n\n# Offensive Rebounding Percentage = (Offensive Rebounds)/[(Offensive Rebounds)+(Opponent’s Defensive Rebounds)]\n# You can win games controlling the offensive glass\n\ndf_season['WORP'] = df_season.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\ndf_season['LORP'] = df_season.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)\n\n# Free Throw Rate=(Free Throws Made)/(Field Goals Attempted) or Free Throws Attempted/Field Goals Attempted\n# You got to get to the line to win close games\n\ndf_season['WFTAR'] = df_season.apply(lambda row: row.WFTA / row.WFGA, axis=1)\ndf_season['LFTAR'] = df_season.apply(lambda row: row.LFTA / row.LFGA, axis=1)\n\n# 4 Factors is weighted as follows\n# 1. Shooting (40%)\n# 2. Turnovers (25%)\n# 3. Rebounding (20%)\n# 4. Free Throws (15%)\n\ndf_season['W4Factor'] = df_season.apply(lambda row: .40*row.WeFGP + .25*row.WTOR + .20*row.WORP + .15*row.WFTAR, axis=1)\ndf_season['L4Factor'] = df_season.apply(lambda row: .40*row.LeFGP + .25*row.LTOR + .20*row.LORP + .15*row.LFTAR, axis=1)                                      \n                                       \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5139add1-fb9c-4bf6-b896-1784f67f005d","_uuid":"932ff10ca6d76165962e1d0cfe01bd2c617723f0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Offensive efficiency (OffRtg) =  (Points / Possessions)\n# Every possession counts\n\ndf_season['WOffRtg'] = df_season.apply(lambda row: (row.WScore / row.Possesions), axis=1)\ndf_season['LOffRtg'] = df_season.apply(lambda row: (row.LScore / row.Possesions), axis=1)\n\n# Defensive efficiency (DefRtg) = (Opponent points / Opponent possessions)\n# defense wins championships\n\ndf_season['WDefRtg'] = df_season.LOffRtg\ndf_season['LDefRtg'] = df_season.WOffRtg\n\n                        \n# Assist Ratio : Percentage of team possessions that end in assists\n# distribute the rock - dont go isolation all the time\n\ndf_season['WAstR'] = df_season.apply(lambda row: row.WAst / (row.WFGA + 0.44*row.WFTA + row.WAst + row.WTO), axis=1)\ndf_season['LAstR'] = df_season.apply(lambda row: row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)\n\n\n# DREB% : Percentage of team defensive rebounds\n# control your own glass\n\ndf_season['WDRP'] = df_season.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\ndf_season['LDRP'] = df_season.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1) \n\n# Free Throw Percentage\n# Make your damn free throws\n\ndf_season['WFTPCT'] = df_season.apply(lambda row : 0 if row.WFTA < 1 else row.WFTM / row.WFTA, axis=1)\ndf_season['LFTPCT'] = df_season.apply(lambda row : 0 if row.LFTA < 1 else row.LFTM / row.LFTA, axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf3431c2-1f53-468a-ad80-e984a47604d6","_uuid":"f9745e3db7ee84e5bed2261df68f2c868f027740","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season.drop(['WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF'], axis=1, inplace=True)\ndf_season.drop(['LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18e35640-d23d-46d1-978b-979382c3a98e","_uuid":"0c1b574fe9b4d499c93670703f4c647cbda5552e","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d2d2973-0e0b-4135-b5b1-18508078c338","_uuid":"7a34fe23cb83e34c1549cc2f95abcef095c476f3","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season_composite = pd.DataFrame()\n\n#This will aggregate individual games into season totals for a team\n\n#calculates wins and losses to get winning percentage\n\ndf_season_composite['WINS'] = df_season['WTeamID'].groupby([df_season['Season'], df_season['WTeamID']]).count()\ndf_season_composite['LOSSES'] = df_season['LTeamID'].groupby([df_season['Season'], df_season['LTeamID']]).count()\ndf_season_composite['WINPCT'] = df_season_composite['WINS'] / (df_season_composite['WINS'] + df_season_composite['LOSSES'])\n\n# calculates averages for games team won\n\ndf_season_composite['WPIE'] = df_season['WPIE'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WeFGP'] = df_season['WeFGP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WTOR'] = df_season['WTOR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WORP'] = df_season['WORP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WFTAR'] = df_season['WFTAR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['W4Factor'] = df_season['W4Factor'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WOffRtg'] = df_season['WOffRtg'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WDefRtg'] = df_season['WDefRtg'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WAstR'] = df_season['WAstR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WDRP'] = df_season['WDRP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\ndf_season_composite['WFTPCT'] = df_season['WFTPCT'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n\n# calculates averages for games team lost\n\ndf_season_composite['LPIE'] = df_season['LPIE'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LeFGP'] = df_season['LeFGP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LTOR'] = df_season['LTOR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LORP'] = df_season['LORP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LFTAR'] = df_season['LFTAR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['L4Factor'] = df_season['L4Factor'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LOffRtg'] = df_season['LOffRtg'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LDefRtg'] = df_season['LDefRtg'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LAstR'] = df_season['LAstR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LDRP'] = df_season['LDRP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\ndf_season_composite['LFTPCT'] = df_season['LFTPCT'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n\n# calculates weighted average using winning percent to weight the statistic\n\n\ndf_season_composite['PIE'] = df_season_composite['WPIE'] * df_season_composite['WINPCT'] + df_season_composite['LPIE'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['FG_PCT'] = df_season_composite['WeFGP'] * df_season_composite['WINPCT'] + df_season_composite['LeFGP'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['TURNOVER_RATE'] = df_season_composite['WTOR'] * df_season_composite['WINPCT'] + df_season_composite['LTOR'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['OFF_REB_PCT'] = df_season_composite['WORP'] * df_season_composite['WINPCT'] + df_season_composite['LORP'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['FT_RATE'] = df_season_composite['WFTAR'] * df_season_composite['WINPCT'] + df_season_composite['LFTAR'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['4FACTOR'] = df_season_composite['W4Factor'] * df_season_composite['WINPCT'] + df_season_composite['L4Factor'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['OFF_EFF'] = df_season_composite['WOffRtg'] * df_season_composite['WINPCT'] + df_season_composite['LOffRtg'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['DEF_EFF'] = df_season_composite['WDefRtg'] * df_season_composite['WINPCT'] + df_season_composite['LDefRtg'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['ASSIST_RATIO'] = df_season_composite['WAstR'] * df_season_composite['WINPCT'] + df_season_composite['LAstR'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['DEF_REB_PCT'] = df_season_composite['WDRP'] * df_season_composite['WINPCT'] + df_season_composite['LDRP'] * (1 - df_season_composite['WINPCT'])\ndf_season_composite['FT_PCT'] = df_season_composite['WFTPCT'] * df_season_composite['WINPCT'] + df_season_composite['LFTPCT'] * (1 - df_season_composite['WINPCT'])\n\ndf_season_composite.reset_index(inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"918590f7-f347-44a3-9000-c85088448ad3","_uuid":"3f630088b68468bcf345a80a64397e34352d1b5a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Kentucy and Witchita State went undefeated causing problems with the data since cant calculate average stats without WINPCT\n\ndf_season_composite[df_season_composite['LOSSES'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e16c6bb6-31ac-4bcc-91e5-d86bcd20f875","_uuid":"4c372c4f5a2a3d4c45343ee6e47e4c476d64a0db","collapsed":true,"trusted":true},"cell_type":"code","source":"# Complete hack to fix the data\n\ndf_season_composite.loc[4064,'WINPCT'] = 1\ndf_season_composite.loc[4064,'LOSSES'] = 0\ndf_season_composite.loc[4064,'PIE'] = df_season_composite.loc[4064,'WPIE']\ndf_season_composite.loc[4064,'FG_PCT'] = df_season_composite.loc[4064,'WeFGP']\ndf_season_composite.loc[4064,'TURNOVER_RATE'] = df_season_composite.loc[4064,'WTOR']\ndf_season_composite.loc[4064,'OFF_REB_PCT'] = df_season_composite.loc[4064,'WORP']\ndf_season_composite.loc[4064,'FT_RATE'] = df_season_composite.loc[4064,'WFTAR']\ndf_season_composite.loc[4064,'4FACTOR'] = df_season_composite.loc[4064,'W4Factor']\ndf_season_composite.loc[4064,'OFF_EFF'] = df_season_composite.loc[4064,'WOffRtg']\ndf_season_composite.loc[4064,'DEF_EFF'] = df_season_composite.loc[4064,'WDefRtg']\ndf_season_composite.loc[4064,'ASSIST_RATIO'] = df_season_composite.loc[4064,'WAstR']\ndf_season_composite.loc[4064,'DEF_REB_PCT'] = df_season_composite.loc[4064,'WDRP']\ndf_season_composite.loc[4064,'FT_PCT'] = df_season_composite.loc[4064,'WFTPCT']\n\ndf_season_composite.loc[4211,'WINPCT'] = 1\ndf_season_composite.loc[4211,'LOSSES'] = 0\ndf_season_composite.loc[4211,'PIE'] = df_season_composite.loc[4211,'WPIE']\ndf_season_composite.loc[4211,'FG_PCT'] = df_season_composite.loc[4211,'WeFGP']\ndf_season_composite.loc[4211,'TURNOVER_RATE'] = df_season_composite.loc[4211,'WTOR']\ndf_season_composite.loc[4211,'OFF_REB_PCT'] = df_season_composite.loc[4211,'WORP']\ndf_season_composite.loc[4211,'FT_RATE'] = df_season_composite.loc[4211,'WFTAR']\ndf_season_composite.loc[4211,'4FACTOR'] = df_season_composite.loc[4211,'W4Factor']\ndf_season_composite.loc[4211,'OFF_EFF'] = df_season_composite.loc[4211,'WOffRtg']\ndf_season_composite.loc[4211,'DEF_EFF'] = df_season_composite.loc[4211,'WDefRtg']\ndf_season_composite.loc[4211,'ASSIST_RATIO'] = df_season_composite.loc[4211,'WAstR']\ndf_season_composite.loc[4211,'DEF_REB_PCT'] = df_season_composite.loc[4211,'WDRP']\ndf_season_composite.loc[4211,'FT_PCT'] = df_season_composite.loc[4211,'WFTPCT']\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ce27e31-9fd1-469c-a3ec-f0b29bd36030","_uuid":"4b9e7f0aa9033f65e176a538724bb22483391bed","collapsed":true,"trusted":true},"cell_type":"code","source":"# we only need the final summary stats\n\ndf_season_composite.drop(['WINS','WPIE','WeFGP','WTOR','WORP','WFTAR','W4Factor','WOffRtg','WDefRtg','WAstR','WDRP','WFTPCT'], axis=1, inplace=True)\ndf_season_composite.drop(['LOSSES','LPIE','LeFGP','LTOR','LORP','LFTAR','L4Factor','LOffRtg','LDefRtg','LAstR','LDRP','LFTPCT'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42a17c65-115c-4b54-b8e2-64e403743812","_uuid":"d4d80f48ced633dee87beeb0fd7340727e69d145","collapsed":true,"trusted":true},"cell_type":"code","source":"df_season_composite.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33ccb0fe-5b3e-49b0-8c0b-611eda892139","_uuid":"0acf5459ee7de145cc2dbf970577b5b64942d7f5","collapsed":true,"trusted":true},"cell_type":"code","source":"# a little housekeeping to make easier to graph correlation matrix\n\ncolumns = list(df_season_composite.columns.values) \ncolumns.pop(columns.index('WINPCT')) \ncolumns.append('WINPCT')\ndf_season_composite = df_season_composite[columns]\ndf_season_composite.rename(columns={'WTeamID':'TeamID'}, inplace=True)\ndf_season_composite.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0b4d1f4f-7996-4fe0-aae8-26d28b2243df","_uuid":"31bbd32f525429357d2baf4e5208fdd03fda3e24","collapsed":true,"trusted":true},"cell_type":"code","source":"# This shows we have some good predictors of winning percentage\n\n# the PIE variable is very powerfully correlated with winning percentage\n# also we can see turnovers will kill you as well as having a bad defense\n\n\ncorrmatrix = df_season_composite.iloc[:, 2:].corr()\n\nf, ax = plt.subplots(figsize=(11, 7))\nsns.heatmap(corrmatrix, vmax=.8, cbar=True, annot=True, square=True);\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8407e8ce-e567-41fd-8195-d20ce92ffab7","_uuid":"fe50b561fe0c336e898c1f3ffc1ddc54abfbcab3","collapsed":true,"trusted":true},"cell_type":"code","source":"# Strength of Schedule\n\n# We will use the RPI ranking of the teams before entering the tourney to get a measure of strength of schedule.\n\n# Rating Percentage Index (RPI) Formula=.25*(Team’s Winning Percentage)+\n# .50*(Opponents’  Average Winning Percentage)+0.25*(Opponents’ Opponents’  Average Winning Percentage)\n\n# The rating percentage index, commonly known as the RPI, is a quantity used to rank sports teams based upon\n# a team's wins and losses and its strength of schedule. It is one of the sports rating systems by which NCAA basketball,\n# baseball, softball, hockey, soccer, lacrosse, and volleyball teams are ranked.\n\n# The final pre-tournament rankings each year have a RankingDayNum of 133.\n# and can thus be used to make predictions of the games from the NCAA® tournament","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75c419c4-ae7b-4808-b0e8-1ffb50148168","_uuid":"bb005da7c0895a297b415c6ab16a053490893e58","collapsed":true,"trusted":true},"cell_type":"code","source":"df_RPI2 = df_rankings2[df_rankings2['SystemName'] == 'RPI']\ndf_RPI2_final = df_RPI2[df_RPI2['RankingDayNum'] == 133]\ndf_RPI2_final.drop(labels=['RankingDayNum', 'SystemName'], inplace=True, axis=1)\ndf_RPI2_final.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a41c282-8d4e-4807-86d5-8f4ec27f799e","_uuid":"b9f19db9c04d1d781a025d4d1a3fe87d43418995","collapsed":true,"trusted":true},"cell_type":"code","source":"df_RPI = df_rankings[df_rankings['SystemName'] == 'RPI']\ndf_RPI_final = df_RPI[df_RPI['RankingDayNum'] == 133]\ndf_RPI_final.drop(labels=['RankingDayNum', 'SystemName'], inplace=True, axis=1)\ndf_RPI_final.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de7c53b2-d146-4348-80b9-9333e187f489","_uuid":"24e3fa8f02ba7c1022e465b3d1607f2dbf8e4c98","collapsed":true,"trusted":true},"cell_type":"code","source":"df_RPI_final = pd.concat((df_RPI_final, df_RPI2_final ))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc33b3e1-1848-413c-ac9e-127b51236963","_uuid":"eb173a087193980f0b5ae2016d49fc7bc07a3348","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets look at the conference tourney games to see if their is a correlation between a team doing well\n# in the conference tourney and having that carry over to winning games in the big dance \n\n# Lets calculate the number of games a team played in the conference tournament\n\ndf_ConferenceTourneyGames.head()\n\nConferenceTourney_SummaryW = pd.DataFrame()\nConferenceTourney_SummaryL = pd.DataFrame()\nConferenceTourney_SummaryW['WINS'] = df_ConferenceTourneyGames['WTeamID'].groupby([df_ConferenceTourneyGames['Season'], df_ConferenceTourneyGames['WTeamID']]).count()\nConferenceTourney_SummaryL['LOSSES'] = df_ConferenceTourneyGames['LTeamID'].groupby([df_ConferenceTourneyGames['Season'], df_ConferenceTourneyGames['LTeamID']]).count()\n##ConferenceTourney_SummaryW['LOSSES'].fillna(0, inplace=True)\n#ConferenceTourney_SummaryW['GAMES'] = ConferenceTourney_SummaryW['WINS'] + ConferenceTourney_SummaryW['LOSSES']\nConferenceTourney_SummaryW.reset_index(inplace = True)\nConferenceTourney_SummaryL.reset_index(inplace = True)\nConferenceTourney_SummaryW.rename(columns={'WTeamID':'TeamID'}, inplace=True)\nConferenceTourney_SummaryL.rename(columns={'LTeamID':'TeamID'}, inplace=True)\n\nConferenceTourney_Summary = pd.merge(left=ConferenceTourney_SummaryW, right=ConferenceTourney_SummaryL, how='outer', on=['Season', 'TeamID'])\nConferenceTourney_Summary['LOSSES'].fillna(0, inplace=True) \nConferenceTourney_Summary['GAMES'] = ConferenceTourney_Summary['LOSSES'] + ConferenceTourney_Summary['WINS']\n\nConferenceTourney_Summary.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3dc75821-917a-4a6d-956f-1c79dce25fb3","_uuid":"49f44f15f4c038763d0b76b2760ebbb97eaf0c30","collapsed":true,"trusted":true},"cell_type":"code","source":"# Now lets do the same thing for the actual tourney\n\nNCAATourney_SummaryW = pd.DataFrame()\nNCAATourney_SummaryL = pd.DataFrame()\nNCAATourney_SummaryW['WINS'] = df_tourney['WTeamID'].groupby([df_tourney['Season'], df_tourney['WTeamID']]).count()\nNCAATourney_SummaryL['LOSSES'] = df_tourney['LTeamID'].groupby([df_tourney['Season'], df_tourney['LTeamID']]).count()\n##NCAATourney_SummaryW['LOSSES'].fillna(0, inplace=True)\n#NCAATourney_SummaryW['GAMES'] = NCAATourney_SummaryW['WINS'] + NCAATourney_SummaryW['LOSSES']\nNCAATourney_SummaryW.reset_index(inplace = True)\nNCAATourney_SummaryL.reset_index(inplace = True)\nNCAATourney_SummaryW.rename(columns={'WTeamID':'TeamID'}, inplace=True)\nNCAATourney_SummaryL.rename(columns={'LTeamID':'TeamID'}, inplace=True)\n\nNCAATourney_Summary = pd.merge(left=NCAATourney_SummaryW, right=NCAATourney_SummaryL, how='outer', on=['Season', 'TeamID'])\nNCAATourney_Summary['LOSSES'].fillna(0, inplace=True) \nNCAATourney_Summary['GAMES'] = NCAATourney_Summary['LOSSES'] + NCAATourney_Summary['WINS']\n\nNCAATourney_Summary.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f4c0e87-79c9-4117-bda1-60c4518509dd","_uuid":"b3630bfa0629f520fceb176d41c3eeb7218ad3ec","collapsed":true,"trusted":true},"cell_type":"code","source":"Hot_team = pd.merge(left=NCAATourney_Summary, right=ConferenceTourney_Summary, how='left', on=['Season', 'TeamID'])\n\nHot_team['GAMES_y'].fillna(0, inplace=True) \nHot_team.rename(columns={'GAMES_x':'Conf_Games'}, inplace=True)\nHot_team.rename(columns={'GAMES_y':'Tourney_Games'}, inplace=True)\nHot_team.drop(labels=['WINS_x', 'LOSSES_x', 'WINS_y', 'LOSSES_y'], inplace=True, axis=1)\n\nHot_team = Hot_team[Hot_team['Tourney_Games'] > 0]\nHot_team.iloc[:,2:4].corr()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a88b9551-98bf-471e-acba-31d9f53c08e9","_uuid":"4708a7585faa2a335ed392687889281d38ace149","collapsed":true,"trusted":true},"cell_type":"code","source":"# Not much correlation here so we will not use this feature in the model. Maybe next\n# year we can look at last ten games played in season/conference tourney as an indicator of success.\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"630c4976-63bb-47ef-80d2-02d10bb37def","_uuid":"3acd66857847b70bebf31a20d4e1d2843404da90","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets look at the team conferences file\n\ndf_team_conferences = df_team_conferences[df_team_conferences['Season'] > 2002]\n\ndf_team_conferences.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"abb84684-8c85-45e1-a5f1-18f02b57b9a4","_uuid":"66b9bae370880f1ced41e705fcae6c25d831abab","collapsed":true,"trusted":true},"cell_type":"code","source":"# For Strength of schedule measure we will take the conference average of RPI and assign it to each team in the conference\n# All things being equal a team with a higher Conference RPI should win the game\n\ndf_team_conferences_stage = pd.merge(left=df_team_conferences, right=df_RPI_final, how='left', on=['Season', 'TeamID'])\ndf_team_conferences_stage.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d9b9d48-f1dd-40e9-bc68-b3233be04bab","_uuid":"2094680ef513b8dfb8c385ab13fcbf29b5f24001","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculates the Conference RPI\n\ndf_team_conference_strength = pd.DataFrame()\n\ndf_team_conference_strength['Conf_Strength'] = df_team_conferences_stage['OrdinalRank'].groupby([df_team_conferences_stage['Season'], df_team_conferences_stage['ConfAbbrev']]).mean()\ndf_team_conference_strength.reset_index(inplace=True)\ndf_team_conference_strength.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bd08736-72ea-43fc-a573-395bdd0cf1c3","_uuid":"3a357983ecd2b421a2f6ba9135d806d90f8012d4","collapsed":true,"trusted":true},"cell_type":"code","source":"# Now we assign the Conference Strength back to each team\n\ndf_team_conferences_stage= pd.merge(left=df_team_conferences_stage, right=df_team_conference_strength, how='left', on=['Season', 'ConfAbbrev'])\ndf_team_conferences_stage.drop(labels=['OrdinalRank', 'ConfAbbrev'], inplace=True, axis=1)\n\ndf_team_conferences_stage.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6283608-7b3a-4e14-a3cd-fd6e6e6939eb","_uuid":"13281648b91ea15241843204d2481dd27fdf5770","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get seeds of teams for all tourney games\n\ndf_seeds.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dcd8642e-4bf7-45c6-bb4a-410c73d3373a","_uuid":"efd6abef7638976bdf1f6e313b4cdad8cfb01300","collapsed":true,"trusted":true},"cell_type":"code","source":"# Convert string to an integer\n\ndf_seeds['seed_int'] = df_seeds['Seed'].apply( lambda x : int(x[1:3]) )\ndf_seeds.drop(labels=['Seed'], inplace=True, axis=1) \ndf_seeds.rename(columns={'seed_int':'Seed'},inplace=True)\n\n# Merge in the Conference Strength variable here\n\ndf_seeds= pd.merge(left=df_seeds, right=df_team_conferences_stage, how='left', on=['Season', 'TeamID'])\n\ndf_seeds.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b48916d6-4233-4012-bf27-69d82b5ddff2","_uuid":"61ebf62d71646738c005fe7cf824e0f5257f9d22","collapsed":true,"trusted":true},"cell_type":"code","source":"# Create team features for all seasons\n\n# ranks only start since 2003\n\ndf_seeds_final = df_seeds[df_seeds['Season'] > 2002]\n\n#2 step merge\n\ndf_tourney_stage = pd.merge(left=df_seeds_final, right=df_RPI_final, how='left', on=['Season', 'TeamID'])\ndf_tourney_final = pd.merge(left=df_tourney_stage, right=df_season_composite, how='left', on=['Season', 'TeamID'])\n\ndf_tourney_final.tail()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"204466b7-138d-43a0-9d3c-1429c1a0282f","_uuid":"db942fb0946715b7a6fb7db3691a0033818762c7","collapsed":true,"trusted":true},"cell_type":"code","source":"# I couldnt figure out how to manipulate/calculate the way I wanted so I exported to Excel and am reimporting it back in here.\n\n# This indicates the number of tourney games that the senior class would have played in going in to this\n# years tourney (basically games played in the prior 3 tourneys) Using it as a gage of tourney experience of the team. \n# All things being equal between two #teams the team with more experience in the tourney I feel would win the game.\n\ndf_tourney_experience.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1cb3f88-3d38-4e51-b47b-a0a6f4b1ad51","_uuid":"24bd20eab617fb5729b21362b7177140065f01f4","collapsed":true,"trusted":true},"cell_type":"code","source":"# this function looks up the number of games for a year/team combination\n\ndef get_wins(year, teamid):\n    \n    row_id = df_tourney_experience[df_tourney_experience['TeamID'] == teamid].index[0]\n    column_id = df_tourney_experience.columns.get_loc(str(year))\n    games = df_tourney_experience.iloc[row_id,column_id]\n      \n    return games\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ab2dc69-afc1-4269-baf6-fb7b6592346e","_uuid":"671705082037452c00db703f297961ed273425d1","collapsed":true,"trusted":true},"cell_type":"code","source":"# iterates thru the dataframe to build another single column dataframe by calling the function\n\nresult = []\n             \n\nfor row in df_tourney_final.iterrows():\n    \n    years = (df_tourney_final['Season'])\n    teams = (df_tourney_final['TeamID'])\n    \nfor i in range(len(df_tourney_final)):\n    \n    matrix = ((years[i], teams[i]))\n    result.append(get_wins(*matrix))\n    \n\nteam_experience = pd.DataFrame(result, columns=['experience']) \n\nteam_experience.head()\n  \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cd31a13-b499-42fe-8e2b-9e46b389ca98","_uuid":"19f0e26112327e5143eb077336b95ac1fde1826e","collapsed":true,"trusted":true},"cell_type":"code","source":"# merges them together \n\ndf_tourney_final = pd.concat((df_tourney_final, team_experience), axis=1)\n\ndf_tourney_final.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35361060-0a72-48d9-98b0-4e744ac0c248","_uuid":"163f035c083f21a7cf878464021f237a58706d41","collapsed":true,"trusted":true},"cell_type":"code","source":"# generate teams in the tourney\n\ndf_tourney.drop(labels=['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], inplace=True, axis=1)\ndf_tourney = pd.merge(left=df_tourney, right=df_seeds, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf_tourney = pd.merge(left=df_tourney, right=df_seeds, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\ndf_tourney.drop(labels=['TeamID_x', 'TeamID_y', 'Conf_Strength_x', 'Conf_Strength_y' ], inplace=True, axis=1)\ndf_tourney.rename(columns={'Seed_x':'WSeed', 'Seed_y':'LSeed'},inplace=True)\ndf_tourney.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36361c05-b75a-4265-8b01-f6f0c57dc850","_uuid":"e3fac62b6000c35193b4023263396baad65313da","collapsed":true,"trusted":true},"cell_type":"code","source":"# Great graph showing how seeding has extreme effect in early rounds\n\n# No 16 seed has ever beaten a number 1 seed (absence of +15 values)\n# Very rarely does a #15 seed beat a #2 seed (low value of +13 values)\n\n# this needs to be in our model\n\ndf_tourney['SeedDiff'] = df_tourney['WSeed'] - df_tourney['LSeed']\nsns.countplot(df_tourney['SeedDiff'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1fd086ce-8d09-447f-a8d7-af367093b125","_uuid":"e94aca46a24064d658f7713e0fe6354ae1101ab6","collapsed":true,"trusted":true},"cell_type":"code","source":"# quick and dirty to see how good a predictor Seed difference is\n\ndf_wins = pd.DataFrame()\ndf_wins['SeedDiff'] = df_tourney['SeedDiff']\ndf_wins['Result'] = 1\n\ndf_losses = pd.DataFrame()\ndf_losses['SeedDiff'] = -df_tourney['SeedDiff']\ndf_losses['Result'] = 0\n\ndf_predictions = pd.concat((df_wins, df_losses))\ndf_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cce4ac54-aa97-4b0e-82d6-6d16085f66ae","_uuid":"2c5367bddff5235c044c21adc5284a80559e8965","collapsed":true,"trusted":true},"cell_type":"code","source":"# setup the data\n\nX_train = df_predictions.SeedDiff.values.reshape(-1,1)\ny_train = df_predictions.Result.values\nX_train, y_train = shuffle(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf3579b5-10c1-46d7-bb71-6bd6e55a1042","_uuid":"deefdb7f3aaf25c1b57ee8de8945c6577f844586","collapsed":true,"trusted":true},"cell_type":"code","source":"# use Logistic regression with Gridsearch for parameter tuning\n\nlogreg = LogisticRegression(random_state=0)\nparams = {'C': np.logspace(start=-5, stop=3, num=9)}\nclf = GridSearchCV(logreg, params, scoring='neg_log_loss', refit=True, cv=10, )\nclf.fit(X_train, y_train)\nprint('Best log_loss: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_['C']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd9c0154-4f1e-453e-af1d-97e7686522a3","_uuid":"ad8de6be973fda02ab9b9415a900664641ef35a0","collapsed":true,"trusted":true},"cell_type":"code","source":"# model is accurately reflecting the low probability of major upsets based on seeds differentials\n\nX = np.arange(-15, 15).reshape(-1, 1)  # this creates the range of seed differentials\npreds = clf.predict_proba(X)[:,1]  # the 1 signifies winning\n\nplt.plot(X, preds)\nplt.xlabel('Team1 seed - Team2 seed')\nplt.ylabel('P(Team1 will win)')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e213773d-8ef0-47c2-bac7-6018d58607ff","_uuid":"d09d39cfd9d2096c001b934b5e1880cf67abf6b7","collapsed":true,"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba4047c3-25f0-4ff5-8077-c738304ed209","_uuid":"3f09cc7be35306fc7f228192ba046af7c63aa3c1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Seeding alone seems to predict 70% accurately\n\ntrain_acc = accuracy_score(y_true=y_train, y_pred=clf.predict(X_train))\n        \nprint('Training Accuracy: %.2f%%' % (100 * train_acc))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a5e95301-f10e-49dc-b4c0-61479bafda41","_uuid":"532bc2694e1bc65eccec300e5e4b196c0fd6c2c7","collapsed":true,"trusted":true},"cell_type":"code","source":"# This is the building block which we build the matchups from\n\ndf_tourney_final.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9fcaad5-5318-4ab7-a724-7e17dbb07cc0","_uuid":"dc7434942f4c5eea25123a1c442b0343b82f3981","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate a list of all matchups in the tourney since 2003\n\n#df_tourney_list = pd.read_csv('NCAATourneyCompactResults.csv')\ndf_tourney_list = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneyCompactResults.csv')\ndf_tourney_list.drop(labels=['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], inplace=True, axis=1)\ndf_tourney_list = df_tourney_list[df_tourney_list['Season'] > 2002]\ndf_tourney_list.reset_index(inplace = True, drop=True)\ndf_tourney_list.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d74bfbec-d600-4bb9-8aec-0888ba08c6ca","_uuid":"3bace30bf6af43e2ba0381b5ae8b0024b031b0cb","collapsed":true,"trusted":true},"cell_type":"code","source":"# gets the features for the winning team\n\ndf_model_winners = pd.merge(left=df_tourney_list, right=df_tourney_final ,how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\ndf_model_winners.drop(labels=['TeamID'], inplace=True, axis=1)\ndf_model_winners.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e31ca11-db64-443a-b820-825639305a0d","_uuid":"a92e860d9c2711168a53f642b4f182421e0556df","collapsed":true,"trusted":true},"cell_type":"code","source":"# gets the features for the losing team\n\ndf_model_losers = pd.merge(left=df_tourney_list, right=df_tourney_final ,how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\ndf_model_losers.drop(labels=['TeamID'], inplace=True, axis=1)\ndf_model_losers.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0eed29e5-9a86-4c40-aabb-8e019b3bf334","_uuid":"1396ba397efe2d71121beb51df4c8009d7a4ce51","collapsed":true,"trusted":true},"cell_type":"code","source":"# This generates the differences between the features between winning and losing team and assigns 1 as the classifier for winning\n\ndf_model_winner_diff = (df_model_winners.iloc[:, 3:] - df_model_losers.iloc[:, 3:])\ndf_model_winner_diff['result'] = 1\ndf_model_winner_diff = pd.merge(left=df_model_winner_diff, right=df_tourney_list, left_index=True, right_index=True, how='inner')\n\n#This generates the differences between the features between losing and winning team and assigns 0 as the classifier for losing\n\ndf_model_loser_diff = (df_model_losers.iloc[:, 3:] - df_model_winners.iloc[:, 3:])\ndf_model_loser_diff['result'] = 0\ndf_model_loser_diff = pd.merge(left=df_model_loser_diff, right=df_tourney_list, left_index=True, right_index=True, how='inner')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d58e59a0-25c9-40fe-b628-5cfcadd23d05","_uuid":"dbdd93138ef4cc3107bb82182aafc40ef073e99f","collapsed":true,"trusted":true},"cell_type":"code","source":"df_model_winner_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8420421e-ad8a-431c-9b48-bb00b961dcf6","_uuid":"f8c714eb8f0aed1ffb01b858281060f480ac2be2","collapsed":true,"trusted":true},"cell_type":"code","source":"df_model_loser_diff.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae874f05-5ae5-4e8a-b92b-1707819770f2","_uuid":"a086862448d44d5cda67e1c41ecdd5ad5e40416c","collapsed":true,"trusted":true},"cell_type":"code","source":"\ndf_predictions_tourney = pd.concat((df_model_winner_diff, df_model_loser_diff), axis=0)\n\ndf_predictions_tourney.sort_values('Season', inplace=True)\n\ndf_predictions_tourney.reset_index(inplace = True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ecb537b6-a3f8-4469-8acf-bf0be6bb001b","_uuid":"ea52fb2db6a2a4dfa4b60ffa0a6966e140c08398","collapsed":true,"trusted":true},"cell_type":"code","source":"df_predictions_tourney.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"11b5e01f-58f5-4dcf-b4c2-76b5733a8ee1","_uuid":"77e9ce31b12caab02f389b18f96e1360df168f21","collapsed":true,"trusted":true},"cell_type":"code","source":"# reorder column to make it easier to group features together\n\ntemp= df_predictions_tourney['Conf_Strength']\ndf_predictions_tourney.drop(labels=['Conf_Strength'], axis=1,inplace = True)\ndf_predictions_tourney.insert(0, 'Conf_Strength', temp)\n\ndf_predictions_tourney.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f43da5be-94df-4ce7-a2e3-a8da520dbf0c","_uuid":"2172a6817038d85273960aaa4d595b025be6c6ab","collapsed":true,"trusted":true},"cell_type":"code","source":"# The plan is to test out 7 different models using Grid Search Cross Validation\n\n#1 -  Ranks  -   This will be RPI plus Seeding\n\n#2 -  Experience  - This will be the experience feature only\n\n#3 -  Stats -  This will be the seasons teams statistics features\n\n#4 -  Conference Strength -  This will be the Conference Strength feature only\n\n#5 -  Full -  encompassing all features of models 1, 2, and 3\n\n#6 - An ensemble model with the features being the actual probabilties of models 1, 2, 3 and 4\n\n#7 - An ensemble model which calculates a weighted average probability of the 4 models with the weights\n#  based on the predictive power of the 4 learner models\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32be52f4-563c-4459-a0da-da0c75c917d5","_uuid":"a90f71b3d8357860ac3891c88243dd7f64aeabdd","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of the team statistic features on the training data before we begin full modeling\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nX_features_stats = df_predictions_tourney.iloc[:1426, 3:14]\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2014]\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(X_features_stats, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\": X_features_stats.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9696554b-a49b-49e6-85e0-32cd370bbedf","_uuid":"1ffa08ef339d127f009c09cf75b57df112036284","collapsed":true,"trusted":true},"cell_type":"code","source":"# Based on the above we will drop DEF_REB_PCT and FT_PCT as their p values are > 0.05 and thus we cant rule\n# out the null hypothesis.  We will also drop FT_RATE as they doesn't seem to have much predictive power with\n# the lower F_score as well.\n\n# drop from predictions file used for training analysis\n\ndf_predictions_tourney.drop(labels=['DEF_REB_PCT', 'FT_PCT', 'FT_RATE' ], inplace=True, axis=1)\n\n# drop from team statistics file used for building tourney testing analysis\n\ndf_tourney_final.drop(labels=['DEF_REB_PCT', 'FT_PCT', 'FT_RATE' ], inplace=True, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54a0cfd8-934f-4ba1-9ed0-8259bdb260fd","_uuid":"f77a69842739849e021b43c01e9b78a2415beef0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets split the entire dataset into training/test sets and into feature categories for modeling\n\nlabels = df_predictions_tourney['result']\nIDs = df_predictions_tourney.iloc[:, 15:]\nfeatures = df_predictions_tourney.iloc[:, 0:13]                  # model 5\nfeatures_rank = df_predictions_tourney.iloc[:, 1:3]              # model 1\nfeatures_experience = df_predictions_tourney.iloc[:, 12:13]      # model 2\nfeatures_stats = df_predictions_tourney.iloc[:, 3:12]            # model 3\nfeatures_conference = df_predictions_tourney.iloc[:, 0:1]        # model 4\n\n# Test data set split (2014-1017 onward which corresponds from row 1426 to the end)\n\nlabels_submission = df_predictions_tourney['result'][df_predictions_tourney['Season'] > 2013]\nIDs_submission = df_predictions_tourney.iloc[1426:, 15:]\nfeatures_submission = df_predictions_tourney.iloc[1426:,  0:13]\n\n# Training data set split (2003 thru 2013 which is from the beginning thru row 1425)\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2014]\nIDs_training = df_predictions_tourney.iloc[:1426, 15:]\nX_features = df_predictions_tourney.iloc[:1426, 0:13]\nX_features_rank = df_predictions_tourney.iloc[:1426, 1:3]\nX_features_experience = df_predictions_tourney.iloc[:1426, 12:13]\nX_features_stats = df_predictions_tourney.iloc[:1426, 3:12]\nX_features_conference = df_predictions_tourney.iloc[:1426, 0:1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bdfc8a0-8779-4655-9ee3-c29673e77bf5","_uuid":"5a75313840ab2116a5c81f2f33d139c8242236da","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of the Seeding and RPI features on the training data before we begin full modeling\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(X_features_rank, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\": X_features_rank.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"345ab177-b8a1-41da-9f02-24bfbf6aa6f4","_uuid":"b91764f11f6b903f76bf352febb73560a0659c69","collapsed":true,"trusted":true},"cell_type":"code","source":"# Both ranking stats are powerful predictors so we will keep them!\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c54d039-21da-4d58-8e03-f380344036a5","_uuid":"d3372178817fcbbf70b80283a6c0feb50264ed2e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of the experience feature on the training data before we begin full modeling\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(X_features_experience, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\":X_features_experience.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf2f3617-1f14-4446-943d-789bdc1692f1","_uuid":"ebdcd107e2cf49e5f1cf909f0b387c2af2c8f2d6","collapsed":true,"trusted":true},"cell_type":"code","source":"# Experience is a factor as well. Keep it!\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3f1e49a-2ebb-482e-87f2-08191da777e5","_uuid":"96b9a102ea35a2d8e764313c6a4752447d2c0e74","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of the conference strength feature on the training data before we begin full modeling\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(X_features_conference, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\":X_features_conference.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef126b18-e8d1-4ba0-8adc-97cfbbc203dd","_uuid":"33a5c19004ef216f0fb4030b0229001d0ca7a123","collapsed":true,"trusted":true},"cell_type":"code","source":"# Conference Strength is a factor. Keep it!","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"827ee503-06e6-413e-baf9-8de912282f0b","_uuid":"e08180845b52ca877a44a90e94c252b1e63b60b5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of team ID's on the training data before we begin full modeling\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(IDs_training, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\":IDs_training.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef27446a-8422-4165-ae84-78c059cfb94a","_uuid":"8384a8f0c80756e00b182f9a6740987bb1ce8cc4","collapsed":true,"trusted":true},"cell_type":"code","source":"# As we would expect and hoped for it adds no value.  This was just a sanity check.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7df776f5-f6ce-42b3-874d-25eb2343f33c","_uuid":"5c72d745210c23e1c080d715493d5c0a56c1e8ad","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #1 (Experience)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(X_features_experience, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15d1d3b9-890c-4c07-b2f1-7a194953291a","_uuid":"1c33667ef40647c5737842a6833ffa438a1a4e23","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model 1 analysis (Experience)\n\n# Logistic - Best score on Grid Search Cross Validation is -0.64378%\n# XGBoost - Best score on Grid Search Cross Validation is -0.65592%\n# DTree -   Best score on Grid Search Cross Validation is -0.64823%\n# RForest - Best score on Grid Search Cross Validation is -0.64469%\n\n#We will chose Logistic here as it is good at picking out winners (True Positive Rate) vs RForest\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3dd603f-b36c-4bcf-bad5-8db7ef2c3868","_uuid":"63a4f984016cee992db80116303c166075cdc883","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #2 (Ranks)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(X_features_rank, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c20b18b-646b-400a-b958-4e950cc20362","_uuid":"25e16141ea95c9117974e1d8e93450480a94624a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model 2 Analysis (Ranks)\n\n# Logistic - Best score on Grid Search Cross Validation is -0.54360%\n# XGBoost - Best score on Grid Search Cross Validation is -0.55679%\n# DTree -   Best score on Grid Search Cross Validation is -0.57824%\n# RForest - Best score on Grid Search Cross Validation is -0.54537%\n\n# Logistic and RForest are very close again but RForest has a little higher accuracy\n\n# We will choose Logistic","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6569462b-45e1-40a9-9d53-6744ba283a57","_uuid":"8fdaec58b92ea76c5e3e6d295284a5712f34f443","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #3 (Stats)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(X_features_stats, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier(n_estimators=30)\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9afeb38-dab4-4a87-ad89-902e5ddf4b7d","_uuid":"601d64051df9d2ce7be6a115e24bb51de1389bca","collapsed":true,"trusted":true},"cell_type":"code","source":"#Model 3 Analysis (Stats)\n\n# Logistic -Best score on Grid Search Cross Validation is -0.57541%\n# XGBoost - Best score on Grid Search Cross Validation is -0.60065%\n# DTree -   Best score on Grid Search Cross Validation is -0.63439%\n# RForest - Best score on Grid Search Cross Validation is -0.60063%\n\n# Logistic is the winner here.  XGBoost has high accuracy in training but way lower in test due to overfitting.\n\n# We will choose Logistic","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e5a6277-950b-4029-a8d4-931a0b4c04b3","_uuid":"0f09082a01d3b08120d2c8e2f1fe2df9ec82699e","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #4 (Conference Strength)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(X_features_conference, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce40ad04-adc8-4c8d-93c1-5072e80280d1","_uuid":"93bbfe37f528820e580215d87e0325803914e763","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model 4 analysis (Conference Strength)\n\n# Logistic -Best score on Grid Search Cross Validation is -0.60571%\n# XGBoost - Best score on Grid Search Cross Validation is -0.61737%\n# DTree -   Best score on Grid Search Cross Validation is -0.61198%\n# RForest - Best score on Grid Search Cross Validation is -0.60562%\n\n# # We will choose Random Forest for this model - good at picking out losers (High Specificity)\n#this can counter balance Model which 1 which had high True Positive (High Recall)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1798f02b-1a2f-42f0-b501-ecaf731d3ad1","_uuid":"8d9f30bcf72e2cb64a37832892a1c487f5249307","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #5 (Full)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95f9fa62-5df1-4643-afdf-8f74421fbd29","_uuid":"63342f60cec7aa0524c6681a31ba95a3128f7d9f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model 5 analysis (Full Model)\n\n# Logistic -Best score on Grid Search Cross Validation is -0.53239%\n# XGBoost - Best score on Grid Search Cross Validation is -0.56486%\n# DTree -   Best score on Grid Search Cross Validation is -0.56361%\n# RForest - Best score on Grid Search Cross Validation is -0.55288%\n\n\n# Logistic is the winner here.  XGBoost has high accuracy in training but way lower in test due to overfitting.\n\n# We will choose Logistic for this model\n\n# This model has the best performance of the 5 models so we will submit this as one of our models\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1da4db5c-be3f-42d7-8c2d-0547cc8fcaa5","_uuid":"46aed8f77239d9b50f6201110f02a275158e665c","collapsed":true,"trusted":true},"cell_type":"code","source":"# This sets up the data so we can make predictions year by year.  \n\npredictions_2014 = df_sample_sub1[:2278]\npredictions_2015 = df_sample_sub1[2278:4556]\npredictions_2016 = df_sample_sub1[4556:6833]\npredictions_2017 = df_sample_sub1[6833:]\n\nLength2014 = len(df_predictions_tourney[df_predictions_tourney['Season'] < 2015])\nLength2015 = len(df_predictions_tourney[df_predictions_tourney['Season'] < 2016])\nLength2016 = len(df_predictions_tourney[df_predictions_tourney['Season'] < 2017])\nLength2017 = len(df_predictions_tourney[df_predictions_tourney['Season'] < 2018])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb6633ad-f8c0-43aa-acb7-67230d0fe278","_uuid":"3e6386c839f72da26c76e7df3c8a55001b930045","collapsed":true,"trusted":true},"cell_type":"code","source":"#this gets the training data (all historical data up to the year 2014)\n\nn_test_games= len(predictions_2014)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2014.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2014 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2014.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2014['Conf_Strength']\nPredictions_2014.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2014.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nclf = LogisticRegression(C=1)\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(features.iloc[:(Length2014), 0:13])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2015]\n\nclf.fit(X_scaled, y) \n\nPredictions_2014_scaled = scaler.transform(Predictions_2014)\npreds = clf.predict_proba(Predictions_2014_scaled)[:,1]\n\npredictions_2014['Pred'] = preds\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ad216f5-0be6-458b-babe-f248baf7f0a7","_uuid":"ff951234bc3c98ab9d1dae748c598ba779082371","collapsed":true,"trusted":true},"cell_type":"code","source":"#this gets the training data (all historical data up to the year 2015)\n\nn_test_games= len(predictions_2015)\n\ncolumns = df_tourney_final.columns.get_values()\n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2015.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2015 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2015.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2015['Conf_Strength']\nPredictions_2015.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2015.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nclf = LogisticRegression(C=1)\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(features.iloc[:(Length2015), 0:13])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2016]\n\nclf.fit(X_scaled, y) \n\nPredictions_2015_scaled = scaler.transform(Predictions_2015)\npreds = clf.predict_proba(Predictions_2015_scaled)[:,1]\n\npredictions_2015['Pred'] = preds\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fc2d6f6-6f4a-4877-afa4-df8fa4617baf","_uuid":"a374a6b268f3fc57aa2ce278a22a10006ac5a490","collapsed":true,"trusted":true},"cell_type":"code","source":"#this gets the training data (all historical data up to the year 2016)\n\nn_test_games= len(predictions_2016)\n\ncolumns = df_tourney_final.columns.get_values()\n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2016.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2016 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2016.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2016['Conf_Strength']\nPredictions_2016.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2016.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nclf = LogisticRegression(C=1)\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(features.iloc[:(Length2016), 0:13])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2017]\n\nclf.fit(X_scaled, y) \n\nPredictions_2016_scaled = scaler.transform(Predictions_2016)\npreds = clf.predict_proba(Predictions_2016_scaled)[:,1]\n\npredictions_2016['Pred'] = preds\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2688fb2-56b2-4b30-91f5-c5627002e667","_uuid":"a165641ea4b09dacca8ef6abd3226cd7a7dfd6db","collapsed":true,"trusted":true},"cell_type":"code","source":"#this gets the training data (all historical data up to the year 2017)\n\nn_test_games= len(predictions_2017)\n\ncolumns = df_tourney_final.columns.get_values()\n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2017.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2017 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2017.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2017['Conf_Strength']\nPredictions_2017.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2017.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nclf = LogisticRegression(C=1)\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(features.iloc[:(Length2017), 0:13])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2018]\n\nclf.fit(X_scaled, y) \n\nPredictions_2017_scaled = scaler.transform(Predictions_2017)\npreds = clf.predict_proba(Predictions_2017_scaled)[:,1]\n\npredictions_2017['Pred'] = preds\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"911aa73d-81a5-4c94-b2e8-7735b906898a","_uuid":"36576972dfd51148c60ef6f694556f3e37e59a60","collapsed":true,"trusted":true},"cell_type":"code","source":"yearlypredictions = predictions_2014.append(predictions_2015)\nyearlypredictions = yearlypredictions.append(predictions_2016)\nyearlypredictions = yearlypredictions.append(predictions_2017)\n\n# This is the Full Models predictions for stage 1\n\n#yearlypredictions.to_csv('stage1_predictions_fullmodel.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8af427b1-cd08-43ef-9ffc-db6c6ff1dc8e","_uuid":"a4924056349c9f1687f40a3056dcc7cf18011c02","collapsed":true,"trusted":true},"cell_type":"code","source":"yearlypredictions.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0acfefe1-f546-40a7-99da-03fc69bf91e2","_uuid":"1ca438853528bf33ec107b9963011c95ee1c0a9d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Ensemble modeling\n\n# We will try some ensemble modeling by combining the 4 learner models individually since they are modelling different\n# features that shouldnt be too correlated with each other and try combining them to see if we can beat the Full\n# models performance\n\n#from our results we will go with these 4 classifiers on the models with their respective best tuning paramaters from training\n\n#1 -  Ranks  -   Logistic\n\nClf_ranks = LogisticRegression(C=.10)\n\n#2 -  Experience  -  Logistic\n\nClf_experience = LogisticRegression(C=.10)\n\n#3 -  Stats -  Logistic\n\nClf_stats = LogisticRegression(C=1)\n\n#4 -  Conference Strength -  Random Forest\n\nClf_Conference = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=2, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=3,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cee2984-116e-4551-b390-98e7ceaa721d","_uuid":"0fb6b27c66d1215ef5a52ee1c61bcb90e3a21b99","collapsed":true,"trusted":true},"cell_type":"code","source":"\nscaler_ranks = StandardScaler()\nscaler_experience = StandardScaler()\nscaler_stats = StandardScaler()\nscaler_conference = StandardScaler()\n\nX_scaled_ranks = scaler_ranks.fit_transform(df_predictions_tourney.iloc[:1426, 1:3])\nX_scaled_experience = scaler_experience.fit_transform(df_predictions_tourney.iloc[:1426, 12:13])\nX_scaled_stats = scaler_stats.fit_transform(df_predictions_tourney.iloc[:1426, 3:12])\nX_scaled_conference = scaler_conference.fit_transform(df_predictions_tourney.iloc[:1426, 0:1])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2014]\n\nClf_ranks.fit(X_scaled_ranks, y)\nClf_experience.fit(X_scaled_experience, y)\nClf_stats.fit(X_scaled_stats, y)\nClf_Conference.fit(X_scaled_conference, y)\n\npred_ranks = Clf_ranks.predict_proba(X_scaled_ranks)[:, 1]\npred_experience = Clf_experience.predict_proba(X_scaled_experience)[:, 1]\npred_stats = Clf_stats.predict_proba(X_scaled_stats)[:, 1]\npred_conference = Clf_Conference.predict_proba(X_scaled_conference)[:, 1]\n\n#combine the 4 models predictions together \n\npred_ranks.reshape(len(X_scaled_ranks),1)\npred_experience.reshape(len(X_scaled_experience),1)\npred_stats.reshape(len(X_scaled_stats),1)\npred_conference.reshape(len(X_scaled_conference),1)\n\n\nmodel_predictions = pd.DataFrame()\n\nmodel_predictions = pd.DataFrame(pred_ranks, columns=['pred_ranks'])\nmodel_predictions['pred_experience'] = pd.DataFrame(pred_experience, columns=['pred_experience'] )\nmodel_predictions['pred_stats'] = pd.DataFrame(pred_stats, columns=['pred_stats'] )\nmodel_predictions['pred_conference'] = pd.DataFrame(pred_conference, columns=['pred_conference'] )\n\nmodel_predictions.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29d37416-20c6-43c8-af2c-2265fd9b157b","_uuid":"f8dc0052903d360417822182aea1139317756783","collapsed":true,"trusted":true},"cell_type":"code","source":"# Lets test out the predictive power of the individual models themselves\n\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\nselector = SelectPercentile(f_classif, percentile=100)\nselector.fit(model_predictions, y)\np_scores = (selector.pvalues_) \nF_scores = (selector.scores_)\n\ndf_significance = pd.DataFrame({\"Feature\": model_predictions.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n\ndf_significance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cfbe2031-f37c-4ac0-9732-74395c960108","_uuid":"c5a748de9ebed547f70eb73c2cf27b9449a7becb","collapsed":true,"trusted":true},"cell_type":"code","source":"# All individual models have predictive power \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"063301d9-d187-4b11-a20f-490438b0a5e8","_uuid":"c8eff7cd474479469552b9e71f1f66ade2b90848","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #6 (Ensemble model of weaker 4 models probability predictions only)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(model_predictions, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6c50173f-8785-4945-b1db-dac7257afa53","_uuid":"f6c30c5d0f4f56fd7af043d3a0ecc49e7657f218","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model #6 Analysis (Ensemble model of weaker 4 models probability predictions only)\n\n# Logistic - Best score on Grid Search Cross Validation is -0.53611%\n# XGBoost - Best score on Grid Search Cross Validation is -0.54701%\n# DTree -  Best score on Grid Search Cross Validation is -0.60620%\n# RForest - Best score on Grid Search Cross Validation is -0.53149%\n\n# No improvement versus the full model - we will submit in test but probably not in stage 2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5944197-fd58-4c21-b386-cc8040ed49a9","_uuid":"a483576f1c08783866e8b1036c41b81d0ba0dab1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model #7 (Ensemble model with weighted average probability)\n\n\n#The log loss scores were as follows for our 4 models on the test data\n\n# Experience = -.64378\n# Ranks =      -.54360\n# Stats =      -.57540\n# Conference = -.60571\n\n# We take the difference from these scores versus a perfect -.50 score and then normalize those differences against each\n# other to come up with a weight of how good the individual model is versus the others\n\n# Experience =    20%\n# Ranks =         29%\n# Stats =         27%\n# Conference =    24%\n\nload= {'type': ['PredRank' , 'PredExperience', 'Predstats' , 'Predconferernce'],\n         'weights': [.20, .29, .27, .24]}\n                \ndf_weights = pd.DataFrame.from_dict(load)\n\ndf_weights.set_index(('type'), inplace = True)\n\ndf_weights.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9a0ff71-41ac-45b7-a457-e73cef7d1f99","_uuid":"191f176d8946e51846146d6490fecf84e7438c0d","collapsed":true,"trusted":true},"cell_type":"code","source":"# These are individual models probabilities calculated up above\n\nmodel_predictions.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b471be3-b341-45b5-871c-0775d08d3061","_uuid":"540f7ff0030896be696bfda28bdf28a660717625","collapsed":true,"trusted":true},"cell_type":"code","source":"# now lets multiple the predictions for each feature by the weight of that feature and then sum up all those\n# weights to get the final probability\n\nweighted_predictions_train = pd.DataFrame()\n\nweighted_predictions_train['pred_ranks'] = model_predictions['pred_ranks'].apply(lambda x: x * df_weights.loc['PredRank']['weights'])\nweighted_predictions_train['pred_experience'] = model_predictions['pred_experience'].apply(lambda x: x * df_weights.loc['PredExperience']['weights'])\nweighted_predictions_train['pred_stats'] = model_predictions['pred_stats'].apply(lambda x: x * df_weights.loc['Predstats']['weights'])\nweighted_predictions_train['pred_conference'] = model_predictions['pred_conference'].apply(lambda x: x * df_weights.loc['Predconferernce']['weights'])\nweighted_predictions_train['probability'] = weighted_predictions_train['pred_ranks'] + weighted_predictions_train['pred_experience'] + weighted_predictions_train['pred_stats'] + weighted_predictions_train['pred_conference']\n\nweighted_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77e45930-5a3e-404b-b1b0-195ce97d6324","_uuid":"cc5fcaf47b4be4a7da97acae1c1518625a7b83fe","collapsed":true,"trusted":true},"cell_type":"code","source":"# Setup the data for training\n\nweighted_predictions_train.drop(['pred_ranks', 'pred_experience', 'pred_stats', 'pred_conference'], axis =1, inplace = True)\n\nweighted_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e04746ef-ac5e-423b-8432-8e2e6660830e","_uuid":"3e07697b6ba8031c949a27ab213f3ae3b607514b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Training for Model #7 (Ensemble model with weighted average probability)\n\n#split the training data further for cross validation\n\nX_train, X_test, y_train, y_test = train_test_split(weighted_predictions_train, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n\n#Intiating Classifiers\n\nclf1 = LogisticRegression()\n\nclf3 = XGBClassifier()\n\nclf4 = DecisionTreeClassifier() \n\nclf5 = RandomForestClassifier()\n\n# Setting up the parameter grids\n\nparam_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n\nparam_grid3 = [{'learning_rate' : [0.1, 0.3],\n                'max_depth': [3, 6],\n                'min_child_weight': list(range(1, 3))}]\n\nparam_grid4 = [{'max_depth': list(range(3, 6)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_leaf': [20, 50]}]\n\nparam_grid5 = [{'max_depth': list(range(1, 5)),\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split' : [2, 3]}]\n\n# Building the pipelines\n\npipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n\npipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n\npipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n\npipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n\n\n# Setting up multiple GridSearchCV objects, 1 for each algorithm\n\ngridcvs = {}\n\ninner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\nouter_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n\nfor pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n                            (pipe1, clf3, clf4, clf5,),\n                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n    \n    #First loop runs GridSearch and does Cross validation to find the best parameters\n\n    gcv = GridSearchCV(estimator=est,\n                       param_grid=pgrid,\n                       scoring='neg_log_loss',\n                       cv=outer_cv,\n                       verbose=0,\n                       refit=True,\n                       return_train_score=False)\n    \n    gcv.fit(X_train, y_train)\n    \n    gridcvs[name] = gcv\n    \n    print(name)\n    print()\n    print(gcv.best_estimator_)\n    print()\n    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n    print()\n    results = pd.DataFrame(gcv.cv_results_)\n      \n\n#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n\n    # for name, gs_est in sorted(gridcvs.items()):\n    \n    nested_score = 0\n    nested_score = cross_val_score(gcv, \n                                  X=X_train, \n                                  y=y_train, \n                                  cv=inner_cv,\n                                  scoring='neg_log_loss')\n                                \n    \n    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n    print()\n    \n    \n    #Generate predictions and probabilities\n    \n    best_algo = gcv    \n\n    best_algo.fit(X_train, y_train)\n    \n    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n\n    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n    print()\n    \n    # prints classification report and confusion matrix\n    \n    predictions = best_algo.predict(X_test)\n    probability = best_algo.predict_proba(X_test)\n    print(classification_report(y_test,predictions))\n    print()\n    print(confusion_matrix(y_test,predictions))\n    print()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c0009a1-6b0f-4920-8fdb-09995ef76b7f","_uuid":"4aa7ea3064678bd4418ea52a3bab6f1abdabec42","collapsed":true,"trusted":true},"cell_type":"code","source":"# Model #7 Analysis (Ensemble model with weighted average probability)\n\n# Logistic - Best score on Grid Search Cross Validation is -0.54411%\n# XGBoost - Best score on Grid Search Cross Validation is -0.55594%\n# DTree -  Best score on Grid Search Cross Validation is -0.55523%\n# RForest - Best score on Grid Search Cross Validation is -0.54821%\n\n# No improvement versus the full model - we will submit in test but not in stage 2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a0fb1b2-b59e-4c29-a088-e97fffc190bc","_uuid":"e3e0214c7aba4ee10858b7e3b858ce1b28095cda","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate predictions on the test set for the 4 learner models a year at a time - 2014\n\nn_test_games= len(predictions_2014)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2014.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2014 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2014.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2014['Conf_Strength']\nPredictions_2014.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2014.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nscaler_ranks = StandardScaler()\nscaler_experience = StandardScaler()\nscaler_stats = StandardScaler()\nscaler_conference = StandardScaler()\n\nX_scaled_ranks = scaler_ranks.fit_transform(features.iloc[:(Length2014), 1:3])\nX_scaled_experience = scaler_experience.fit_transform(features.iloc[:(Length2014), 12:13])\nX_scaled_stats = scaler_stats.fit_transform(features.iloc[:(Length2014), 3:12])\nX_scaled_conference = scaler_conference.fit_transform(features.iloc[:(Length2014), 0:1])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2015]\n\nClf_ranks.fit(X_scaled_ranks, y)\nClf_experience.fit(X_scaled_experience, y)\nClf_stats.fit(X_scaled_stats, y)\nClf_Conference.fit(X_scaled_conference, y)\n\nPredictions_ranks_2014_scaled = scaler_ranks.transform(Predictions_2014.iloc[:, 1:3])\nPredictions_experience_2014_scaled = scaler_experience.transform(Predictions_2014.iloc[:, 12:13])\nPredictions_stats_2014_scaled = scaler_stats .transform(Predictions_2014.iloc[:, 3:12])\nPredictions_conference_2014_scaled = scaler_conference.transform(Predictions_2014.iloc[:, 0:1])\n\npred_ranks_2014 = Clf_ranks.predict_proba(Predictions_ranks_2014_scaled )[:,1]\npred_experience_2014 = Clf_experience.predict_proba(Predictions_experience_2014_scaled)[:,1]\npred_stats_2014 = Clf_stats.predict_proba(Predictions_stats_2014_scaled )[:,1]\npred_conference_2014 = Clf_Conference.predict_proba(Predictions_conference_2014_scaled)[:,1]\n\n\n#combine the 4 models predictions together \n\npred_ranks_2014.reshape((n_test_games),1)\npred_experience_2014.reshape((n_test_games),1)\npred_stats_2014.reshape((n_test_games),1)\npred_conference_2014.reshape((n_test_games),1)\n\npredictions_2014['PredRank'] = pred_ranks_2014\npredictions_2014['PredExperience'] = pred_experience_2014\npredictions_2014['Predstats'] = pred_stats_2014\npredictions_2014['Predconferernce'] =pred_conference_2014\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98d28aa1-e4f6-4faf-82a8-b4285743e406","_uuid":"13db0375faf0c2574d560f34a8a0392cdcc03ea0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate predictions on the test set the 4 learner models a year at a time - 2015\n\nn_test_games= len(predictions_2015)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2015.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2015 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2015.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2015['Conf_Strength']\nPredictions_2015.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2015.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nscaler_ranks = StandardScaler()\nscaler_experience = StandardScaler()\nscaler_stats = StandardScaler()\nscaler_conference = StandardScaler()\n\nX_scaled_ranks = scaler_ranks.fit_transform(features.iloc[:(Length2015), 1:3])\nX_scaled_experience = scaler_experience.fit_transform(features.iloc[:(Length2015), 12:13])\nX_scaled_stats = scaler_stats.fit_transform(features.iloc[:(Length2015), 3:12])\nX_scaled_conference = scaler_conference.fit_transform(features.iloc[:(Length2015), 0:1])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2016]\n\nClf_ranks.fit(X_scaled_ranks, y)\nClf_experience.fit(X_scaled_experience, y)\nClf_stats.fit(X_scaled_stats, y)\nClf_Conference.fit(X_scaled_conference, y)\n\nPredictions_ranks_2015_scaled = scaler_ranks.transform(Predictions_2015.iloc[:, 1:3])\nPredictions_experience_2015_scaled = scaler_experience.transform(Predictions_2015.iloc[:, 12:13])\nPredictions_stats_2015_scaled = scaler_stats .transform(Predictions_2015.iloc[:, 3:12])\nPredictions_conference_2015_scaled = scaler_conference.transform(Predictions_2015.iloc[:, 0:1])\n\npred_ranks_2015 = Clf_ranks.predict_proba(Predictions_ranks_2015_scaled )[:,1]\npred_experience_2015 = Clf_experience.predict_proba(Predictions_experience_2015_scaled)[:,1]\npred_stats_2015 = Clf_stats.predict_proba(Predictions_stats_2015_scaled )[:,1]\npred_conference_2015 = Clf_Conference.predict_proba(Predictions_conference_2015_scaled)[:,1]\n\n\n#combine the 4 models predictions together \n\npred_ranks_2015.reshape((n_test_games),1)\npred_experience_2015.reshape((n_test_games),1)\npred_stats_2015.reshape((n_test_games),1)\npred_conference_2015.reshape((n_test_games),1)\n\npredictions_2015['PredRank'] = pred_ranks_2015\npredictions_2015['PredExperience'] = pred_experience_2015\npredictions_2015['Predstats'] = pred_stats_2015\npredictions_2015['Predconferernce'] =pred_conference_2015\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f28db5a-cd8b-4d9f-9dbd-979fb97e41b8","_uuid":"e052b37eb2805ee99ee0b72f9686639b9abd5d26","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate predictions on the test set the 4 learner models a year at a time - 2016\n\nn_test_games= len(predictions_2016)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2016.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2016 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2016.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2016['Conf_Strength']\nPredictions_2016.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2016.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nscaler_ranks = StandardScaler()\nscaler_experience = StandardScaler()\nscaler_stats = StandardScaler()\nscaler_conference = StandardScaler()\n\nX_scaled_ranks = scaler_ranks.fit_transform(features.iloc[:(Length2016), 1:3])\nX_scaled_experience = scaler_experience.fit_transform(features.iloc[:(Length2016), 12:13])\nX_scaled_stats = scaler_stats.fit_transform(features.iloc[:(Length2016), 3:12])\nX_scaled_conference = scaler_conference.fit_transform(features.iloc[:(Length2016), 0:1])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2017]\n\nClf_ranks.fit(X_scaled_ranks, y)\nClf_experience.fit(X_scaled_experience, y)\nClf_stats.fit(X_scaled_stats, y)\nClf_Conference.fit(X_scaled_conference, y)\n\nPredictions_ranks_2016_scaled = scaler_ranks.transform(Predictions_2016.iloc[:, 1:3])\nPredictions_experience_2016_scaled = scaler_experience.transform(Predictions_2016.iloc[:, 12:13])\nPredictions_stats_2016_scaled = scaler_stats .transform(Predictions_2016.iloc[:, 3:12])\nPredictions_conference_2016_scaled = scaler_conference.transform(Predictions_2016.iloc[:, 0:1])\n\npred_ranks_2016 = Clf_ranks.predict_proba(Predictions_ranks_2016_scaled )[:,1]\npred_experience_2016 = Clf_experience.predict_proba(Predictions_experience_2016_scaled)[:,1]\npred_stats_2016 = Clf_stats.predict_proba(Predictions_stats_2016_scaled )[:,1]\npred_conference_2016 = Clf_Conference.predict_proba(Predictions_conference_2016_scaled)[:,1]\n\n\n#combine the 4 models predictions together \n\npred_ranks_2016.reshape((n_test_games),1)\npred_experience_2016.reshape((n_test_games),1)\npred_stats_2016.reshape((n_test_games),1)\npred_conference_2016.reshape((n_test_games),1)\n\npredictions_2016['PredRank'] = pred_ranks_2016\npredictions_2016['PredExperience'] = pred_experience_2016\npredictions_2016['Predstats'] = pred_stats_2016\npredictions_2016['Predconferernce'] =pred_conference_2016\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13160b2c-b8be-4315-aed6-0242359964b8","_uuid":"e219332f05629e73781cd8be34961af8e5ede52c","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate predictions on the test set the 4 learner models a year at a time - 2017\n\nn_test_games= len(predictions_2017)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in predictions_2017.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2017 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2017.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2017['Conf_Strength']\nPredictions_2017.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2017.insert(0, 'Conf_Strength', temp)\n\n\n# Main model\n\nscaler_ranks = StandardScaler()\nscaler_experience = StandardScaler()\nscaler_stats = StandardScaler()\nscaler_conference = StandardScaler()\n\nX_scaled_ranks = scaler_ranks.fit_transform(features.iloc[:(Length2017), 1:3])\nX_scaled_experience = scaler_experience.fit_transform(features.iloc[:(Length2017), 12:13])\nX_scaled_stats = scaler_stats.fit_transform(features.iloc[:(Length2017), 3:12])\nX_scaled_conference = scaler_conference.fit_transform(features.iloc[:(Length2017), 0:1])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2018]\n\nClf_ranks.fit(X_scaled_ranks, y)\nClf_experience.fit(X_scaled_experience, y)\nClf_stats.fit(X_scaled_stats, y)\nClf_Conference.fit(X_scaled_conference, y)\n\nPredictions_ranks_2017_scaled = scaler_ranks.transform(Predictions_2017.iloc[:, 1:3])\nPredictions_experience_2017_scaled = scaler_experience.transform(Predictions_2017.iloc[:, 12:13])\nPredictions_stats_2017_scaled = scaler_stats .transform(Predictions_2017.iloc[:, 3:12])\nPredictions_conference_2017_scaled = scaler_conference.transform(Predictions_2017.iloc[:, 0:1])\n\npred_ranks_2017 = Clf_ranks.predict_proba(Predictions_ranks_2017_scaled )[:,1]\npred_experience_2017 = Clf_experience.predict_proba(Predictions_experience_2017_scaled)[:,1]\npred_stats_2017 = Clf_stats.predict_proba(Predictions_stats_2017_scaled )[:,1]\npred_conference_2017 = Clf_Conference.predict_proba(Predictions_conference_2017_scaled)[:,1]\n\n\n#combine the 4 models predictions together \n\npred_ranks_2017.reshape((n_test_games),1)\npred_experience_2017.reshape((n_test_games),1)\npred_stats_2017.reshape((n_test_games),1)\npred_conference_2017.reshape((n_test_games),1)\n\npredictions_2017['PredRank'] = pred_ranks_2017\npredictions_2017['PredExperience'] = pred_experience_2017\npredictions_2017['Predstats'] = pred_stats_2017\npredictions_2017['Predconferernce'] =pred_conference_2017\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50644549-3391-4b2b-8478-400e8c0615c4","_uuid":"520ba997f395c2871c9ff513481e100dbe7839b2","collapsed":true,"trusted":true},"cell_type":"code","source":"yearlypredictions_combined = predictions_2014.append(predictions_2015)\nyearlypredictions_combined = yearlypredictions_combined.append(predictions_2016)\nyearlypredictions_combined = yearlypredictions_combined.append(predictions_2017)\nyearlypredictions_combined.drop(labels=['Pred'], axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83e26500-1bdd-482c-b77d-d2b00bcf7ff1","_uuid":"338511c77e8b6524d0b881e14d1fdd6be09fb63b","collapsed":true,"trusted":true},"cell_type":"code","source":"yearlypredictions_combined.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a0c1849-38c0-49ae-a173-c8fb7d45ee64","_uuid":"e17e4c59f520003082b40e18dd3b483ba383e7a7","collapsed":true,"trusted":true},"cell_type":"code","source":"# now lets multiple the predictions for each feature by the weight of that feature and then sum up all those\n# weights to get the final probability\n\nweighted_predictions = pd.DataFrame()\n\nweighted_predictions['pred_ranks'] = yearlypredictions_combined['PredRank'].apply(lambda x: x * df_weights.loc['PredRank']['weights'])\nweighted_predictions['pred_experience'] = yearlypredictions_combined['PredExperience'].apply(lambda x: x * df_weights.loc['PredExperience']['weights'])\nweighted_predictions['pred_stats'] = yearlypredictions_combined['Predstats'].apply(lambda x: x * df_weights.loc['Predstats']['weights'])\nweighted_predictions['pred_conference'] = yearlypredictions_combined['Predconferernce'].apply(lambda x: x * df_weights.loc['Predconferernce']['weights'])\nweighted_predictions['probability'] = weighted_predictions['pred_ranks'] + weighted_predictions['pred_experience'] + weighted_predictions['pred_stats'] + weighted_predictions['pred_conference']\n\nweighted_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b254179-4df0-44b0-9b4c-4a2395277733","_uuid":"c2b2ea3f4e854f19518ccf41c2c779f3a88b18d7","collapsed":true,"trusted":true},"cell_type":"code","source":"# The probability column is our weighted average of the other 4 models probabilities weighted its ability to predict\n\n# This is for the Test submission\n\ndf_sample_sub1['Pred'] = weighted_predictions['probability']\n\n# df_sample_sub1.to_csv('Stage1_Ensemble_weighted_predictions.csv', index=False)\n\ndf_sample_sub1.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b2a85e1-4f0d-4461-b1e0-2a391df710e6","_uuid":"8063fbad1d441041eb5a4520171d577892a9c2c3","collapsed":true,"trusted":true},"cell_type":"code","source":"# This makes the predictions for the 2018 tourney using the Full Model\n\nn_test_games= len(df_sample_sub)\n\ncolumns = df_tourney_final.columns.get_values()\n \n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))\n\nmodel = []\ndata = []\n\n\n#This part is the key as it creates the input for the model which is the differences between the 2 teams playing\n\nfor ii, row in df_sample_sub.iterrows():\n    year, s1, s2 = get_year_t1_t2(row.ID)\n    \n    team1 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s1))].values\n    team2 = df_tourney_final[((df_tourney_final['Season'] == year) & (df_tourney_final['TeamID'] == s2))].values\n    \n    model = team1 - team2\n    \n    data.append(model)\n\n\nPredictions_2018 = pd.DataFrame(np.array(data).reshape(n_test_games,15), columns = (columns))\n\nPredictions_2018.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)    \n\ntemp= Predictions_2018['Conf_Strength']\nPredictions_2018.drop(labels=['Conf_Strength'], axis=1,inplace = True)\nPredictions_2018.insert(0, 'Conf_Strength', temp)\n\nLength2018 = len(df_predictions_tourney[df_predictions_tourney['Season'] < 2019])\n\n# Main model\n\nclf = LogisticRegression(C=1)\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(features.iloc[:(Length2018), 0:13])\n\ny = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2018]\n\nclf.fit(X_scaled, y) \n\nPredictions_2018_scaled = scaler.transform(Predictions_2018)\npreds = clf.predict_proba(Predictions_2018_scaled)[:,1]\n\ndf_sample_sub['Pred'] = preds\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbc172b2-42ac-4ee7-9b40-7e053db4d226","_uuid":"5f373088eacffa1126c9ce3cadace562a2fa9c4b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Generate the Full Model 2018 Submission file\n\n# This is for the Test submission\n\ndf_sample_sub.to_csv('Full Model 2018 Tourney Submission.csv', index=False)\n\ndf_sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"264b1305-3b98-4dc5-bdfe-9723f0bcd197","_uuid":"30bf1c919679c0cbedd97d5ef6b2dbfbb1209230","collapsed":true,"trusted":true},"cell_type":"code","source":"\nend_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bde0338e-4da1-48ef-afa7-2a9e381d5d1b","_uuid":"54820916c683b447e393af1f9cc89d252846bb87","collapsed":true,"trusted":true},"cell_type":"code","source":"(end_time - start_time) / 60","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}