{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "772a19dd-4ff7-6dbf-083f-c2665ec8be28"
      },
      "source": [
        "Rent Interest Classifier \n",
        "===\n",
        "---\n",
        "\n",
        " - This classification model predicts the degree of popularity for a rental listing judged by its profiles such as the number of rooms, location, price, etc.  \n",
        " - It predicts whether a given listing would receive \"low,\" \"medium,\" or\n",
        "   \"high\" interest with its corresponding probability to a particular listing.\n",
        "\n",
        "---\n",
        "**Multiclass Classifier with Probability Estimates**\n",
        "---\n",
        "The problem of classification is considered as learning a model that maps instances to class labels. While useful for many purposes, there are numerous applications in which the estimation of the probabilities of the different classes is more desirable than just selecting one of them, in that probabilities are useful as a measure of the reliability of a classification.\n",
        "\n",
        "**Datasets**\n",
        "---\n",
        "NYC rent listing data from the rental website RentHop which is used to find the desired home.\n",
        "Datasets include \n",
        "\n",
        " 1. ***train*** and ***test*** databases, both provided in a JavaScript Object Notation format,\n",
        " 2. ***sample submission*** listing_id with interest level probabilities for each class i.e., high, medium, and low, \n",
        " 3. ***image sample*** of selective 100 listings, and\n",
        " 4. ***kaggle-renthop*** zipfile that contains all listing images where the file size is 78.5GB. \n",
        "\n",
        "The JSON dataset is a structured database that contains the listing information as the number of bathrooms and bedrooms, building_id, created, description, display_address, features, latitude, listing_id, longitude, manager_id, photos links, price, street_address,  and interest_level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "317d7a86-7851-cb29-843d-96899eeecdcd"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "sns.set(font_scale=1)\n",
        "\n",
        "import plotly.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "from plotly import tools\n",
        "\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "train = pd.read_json(\"../input/train.json\")\n",
        "test = pd.read_json(\"../input/test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed645430-1c9d-36db-24cc-b09f8e02720a"
      },
      "outputs": [],
      "source": [
        "print ('There are {0} rows and {1} attributes.'.format(train.shape[0], train.shape[1]))\n",
        "print (len(train['listing_id'].unique()))\n",
        "train = train.set_index('listing_id')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23a4f5bd-769d-79f9-2caf-b4958e74cc37"
      },
      "outputs": [],
      "source": [
        "print ('There are {0} rows and {1} attributes.'.format(test.shape[0], test.shape[1]))\n",
        "test.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e11ca9bb-eff7-d4d7-4681-5c139dee0f29"
      },
      "source": [
        "**Pre-processing and feature extraction**\n",
        "---\n",
        "**Feature Selection in Python with Scikit-Learn**\n",
        "\n",
        "Feature selection is a process where you automatically select affective features in your data that contribute most to the prediction variable or target output. In order to maximize the performance of machine learning techniques,  important attributes are selected before creating a machine learning model using the Scikit-learn library having the feature_importances_ member variable of the trained model. \n",
        "\n",
        "Given an importance score for each attribute where the larger score the more important the attribute. The scores show price, the number of features/photos/words, and date as the importance attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a078a250-42eb-29a0-59a0-43731526b86c"
      },
      "outputs": [],
      "source": [
        "train.info()\n",
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9e93e922-6758-022f-2978-4b9ad54be7c8"
      },
      "source": [
        "----------\n",
        "**Interest Level Distribution**\n",
        "----------\n",
        "Distributions: \n",
        " - **Low (69.5%)**\n",
        " - Medium (22.8%)\n",
        " - Hight (7.8%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a4a42a0-c29d-a0e4-98ee-0dd4477df371"
      },
      "outputs": [],
      "source": [
        "plt.subplots(figsize=(10, 8))\n",
        "sizes = train['interest_level'].value_counts().values\n",
        "patches, texts, autotexts= plt.pie(sizes, labels=['Low', 'Medium', 'High'],\n",
        "                                  colors=['mediumaquamarine','lightcoral', 'steelblue'],\n",
        "                                  explode=[0.1, 0, 0], autopct=\"%1.1f%%\", \n",
        "                                  startangle=90)\n",
        "\n",
        "texts[0].set_fontsize(13)\n",
        "texts[1].set_fontsize(13)\n",
        "texts[2].set_fontsize(13)\n",
        "plt.title('Interest level', fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "019fa8eb-29d6-c27a-08e1-495f1ae445b8"
      },
      "source": [
        "----------\n",
        "**Feature Importance**\n",
        "----------\n",
        "Ensemble methods are a promising solution to highly imbalanced nonlinear classification tasks with mixed variable types and noisy patterns with high variance. Methods compute the relative importance of each attribute. These importance values can be used to inform a feature selection process. This shows the construction of an Extra Trees ensemble of the dataset and the display of the relative feature importance.\n",
        "\n",
        "As can be seen in the *train.info()* table, data types are mixed.\n",
        "\n",
        " 1. **Categorical**: description, display_address, features, manager_id, building_id, street_address\n",
        " 2. **Numeric**: bathrooms, bedrooms, latitude, longitude, price\n",
        " 3. Other: created, photos \n",
        "\n",
        "In order to generate the feature importance matrix, non-numeric data types attributes should be converted to numerical values. Following assumptions are considered.\n",
        "\n",
        " - **description**: The more words and well-described listings might be spotted. \n",
        " - **features**: Some features are more preferred over others.\n",
        " - **photos**: The more images might get more views with having interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "adfb517a-4cf2-cfc6-be24-b72e1b8b67ec"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "def room_price(x, y):\n",
        "    if y == 0:\n",
        "        return 0\n",
        "    return x/y\n",
        "\n",
        "train['nb_images'] = train['photos'].apply(len)\n",
        "train['nb_features'] = train['features'].apply(len)\n",
        "train['nb_description'] = train['description'].apply(lambda x: len(x.split(' ')))\n",
        "train['description_len'] = train['description'].apply(len)\n",
        "train = train.join(\n",
        "                   train['description'].apply(\n",
        "                       lambda x: TextBlob(x).sentiment.polarity).rename('sentiment'))\n",
        "\n",
        "train['price_room'] = train.apply(lambda row: room_price(row['price'], \n",
        "                                                         row['bedrooms']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e0925482-2cd4-d923-0a87-f07f0e11894b"
      },
      "source": [
        "----------\n",
        "Attribute: Building ID\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0cdfd37a-6611-de5d-b536-8a27cc099c55"
      },
      "outputs": [],
      "source": [
        "# Number of listings based on building ID\n",
        "top_buildings = train['building_id'].value_counts().nlargest(10)\n",
        "print (top_buildings)\n",
        "print (len(train['building_id'].unique()))\n",
        "\n",
        "grouped_building = train.groupby(\n",
        "                           ['building_id', 'interest_level']\n",
        "                          )['building_id'].count().unstack('interest_level').fillna(0)\n",
        "\n",
        "grouped_building['sum'] = grouped_building.sum(axis=1)\n",
        "x = grouped_building[(grouped_building['sum'] > 50) & (grouped_building['high'] > 10)]\n",
        "\n",
        "# x = x[x.index != '0'] # Ignore N/A value\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.title('Hight-interest buildings', fontsize=13)\n",
        "plt.xlabel('High interest level', fontsize=13)\n",
        "plt.ylabel('Building ID', fontsize=13)\n",
        "x['high'].plot.barh(color=\"palevioletred\");\n",
        "\n",
        "build_counts = pd.DataFrame(train.building_id.value_counts())\n",
        "build_counts['b_counts'] = build_counts['building_id']\n",
        "build_counts['building_id'] = build_counts.index\n",
        "build_counts['b_count_log'] = np.log2(build_counts['b_counts'])\n",
        "train = pd.merge(train, build_counts, on=\"building_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f3dc69b2-ca36-8cd2-cae3-def867a23abd"
      },
      "source": [
        "----------\n",
        "Attribute: Manager ID\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da7409d0-2f03-bee5-3e85-a3b37a8936cf"
      },
      "outputs": [],
      "source": [
        "# Hight-interest managers\n",
        "top_managers = train['manager_id'].value_counts().nlargest(10)\n",
        "print (top_managers)\n",
        "print (len(train['manager_id'].unique()))\n",
        "\n",
        "grouped_manager = train.groupby(\n",
        "    ['manager_id', 'interest_level'])['manager_id'].count().unstack('interest_level').fillna(0)\n",
        "\n",
        "grouped_manager['sum'] = grouped_manager.sum(axis=1)\n",
        "print (grouped_manager.head())\n",
        "\n",
        "x = grouped_manager.loc[(grouped_manager['high'] > 20 ) & (grouped_manager['sum'] > 50)]\n",
        "\n",
        "plt.title('High-interest managers', fontsize=13)\n",
        "plt.xlabel('High interest level', fontsize=13)\n",
        "plt.ylabel('Manager ID', fontsize=13)\n",
        "x['high'].plot.barh(figsize=(10, 9), color=\"teal\");\n",
        "\n",
        "man_counts = pd.DataFrame(train.manager_id.value_counts())\n",
        "man_counts['m_counts'] = man_counts['manager_id']\n",
        "man_counts['manager_id'] = man_counts.index\n",
        "man_counts['m_count_log'] = np.log10(man_counts['m_counts'])\n",
        "train = pd.merge(train, man_counts, on=\"manager_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3874ab2b-009c-4c8c-ec90-2dc536a4a88f"
      },
      "source": [
        "----------\n",
        "Feature Importance Ranking\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "08b1c66e-634f-985d-3d63-3014b07eec72"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "\n",
        "numerical_features = train[['bathrooms', 'bedrooms', 'price', 'price_room',\n",
        "                            'latitude','longitude', 'nb_images','nb_features', \n",
        "                            'nb_description', 'description_len','sentiment',\n",
        "                            'b_counts', 'm_counts',\n",
        "                            'b_count_log', 'm_count_log']]\n",
        "\n",
        "# Fit an Extra Trees model to the data\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(numerical_features, train['interest_level'])\n",
        "\n",
        "# Display the relative importance of each attribute\n",
        "print (model.feature_importances_)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.subplots(figsize=(13, 6))\n",
        "plt.title('Feature ranking', fontsize = 18)\n",
        "plt.ylabel('Importance degree', fontsize = 13)\n",
        "# plt.xlabel(\"Features\", fontsize = 14)\n",
        "\n",
        "feature_names = numerical_features.columns\n",
        "plt.xticks(range(numerical_features.shape[1]), feature_names, fontsize = 9)\n",
        "pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad85389b-b112-5d54-4cc8-a030d7586741"
      },
      "outputs": [],
      "source": [
        "# Use feature importance for feature selection\n",
        "from numpy import sort\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Converting categorical values for Interest Level to numeric values\n",
        "# Low: 1, Medium: 2, High: 3\n",
        "train['interest'] = np.where(train['interest_level']=='low', 1,\n",
        "                             np.where(train['interest_level']=='medium', 2, 3))\n",
        "\n",
        "X = numerical_features\n",
        "Y = train['interest']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, \n",
        "                                                    random_state=7)\n",
        "\n",
        "# Fit model on all training data\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions for test data and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "# Fit model using each importance as a threshold\n",
        "thresholds = sort(model.feature_importances_)\n",
        "for thresh in thresholds:\n",
        "\t# Select features using threshold\n",
        "\tselection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
        "\tselect_X_train = selection.transform(X_train)\n",
        "    \n",
        "\t# Train model\n",
        "\tselection_model = XGBClassifier()\n",
        "\tselection_model.fit(select_X_train, y_train)\n",
        "    \n",
        "\t# Evalation model\n",
        "\tselect_X_test = selection.transform(X_test)\n",
        "\ty_pred = selection_model.predict(select_X_test)\n",
        "\tpredictions = [round(value) for value in y_pred]\n",
        "\taccuracy = accuracy_score(y_test, predictions)\n",
        "\tprint (\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], \n",
        "                                                    accuracy*100.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6c7270be-6287-b0fc-605f-1be3fb8925d8"
      },
      "source": [
        "----------\n",
        "Correlation Graph\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d414cac-0013-4718-9727-bd67a95aa831"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(13, 13))\n",
        "corr = numerical_features.corr()\n",
        "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), \n",
        "            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "            square=True, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e1f19a3-c66c-9a15-ec00-33cbaab008cd"
      },
      "source": [
        "----------\n",
        "Correlation Matrix\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4abeb08-a81e-86c3-baf5-b1cd2095d536"
      },
      "outputs": [],
      "source": [
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"10pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"11pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '11pt')])]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '8pt'})\\\n",
        "    .set_caption('Correlation Matrix')\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ce58f92-b815-8b3a-b220-0e984c3d8f7b"
      },
      "outputs": [],
      "source": [
        "numerical_features[['bathrooms', 'bedrooms', 'price', 'price_room',\n",
        "                    'latitude','longitude', 'nb_images','nb_features', \n",
        "                    'nb_description', 'description_len','sentiment',\n",
        "                    'b_counts', 'm_counts',\n",
        "                    'b_count_log', 'm_count_log']].hist(figsize=(12, 12))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f5ce1f3e-3369-83cd-f58f-29ff0da9a6bd"
      },
      "source": [
        "----------\n",
        "**Attribute:  Bathrooms, Bedrooms**\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1392bd00-d289-95f8-af9d-bbee69c02e0a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "subplot grid parameters encoded as a single integer.\n",
        "ijk means i x j grid, k-th subplot\n",
        "subplot(221) #top left\n",
        "subplot(222) #top right\n",
        "subplot(223) #bottom left\n",
        "subplot(224) #bottom right \n",
        "'''\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Number of listings\n",
        "sns.countplot(train['bathrooms'], ax = plt.subplot(121));\n",
        "plt.xlabel('NB of bathrooms', fontsize=13);\n",
        "plt.ylabel('NB of listings', fontsize=13);\n",
        "\n",
        "sns.countplot(train['bedrooms'], ax = plt.subplot(122));\n",
        "plt.xlabel('NB of bedrooms', fontsize=13);\n",
        "plt.ylabel('NB of listings', fontsize=13);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54289df2-b1ed-43e5-6664-c9420bcb2062"
      },
      "outputs": [],
      "source": [
        "# Number of rooms based on Interest level\n",
        "grouped_bathroom = train.groupby(\n",
        "    ['bathrooms', 'interest_level'])['bathrooms'].count().unstack('interest_level').fillna(0)\n",
        "grouped_bathroom[['low', 'medium', 'high']].plot.barh(stacked=True, figsize=(12, 4));\n",
        "\n",
        "grouped_bedroom = train.groupby(\n",
        "    ['bedrooms', 'interest_level'])['bedrooms'].count().unstack('interest_level').fillna(0)\n",
        "grouped_bedroom[['low', 'medium', 'high']].plot.barh(stacked=True, figsize=(12.25, 4));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c1ef193d-6051-7860-9474-56c3e8492ec3"
      },
      "source": [
        "----------\n",
        "**Attribute:  Geographical information - latitude, longitude**\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d02479c-8c0b-0886-6f6f-eca5e6451784"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "seaborn.lmplot(x, y, data, hue=None, col=None, row=None, palette=None, \n",
        "col_wrap=None, size=5, aspect=1, markers='o', sharex=True, sharey=True, \n",
        "hue_order=None, col_order=None, row_order=None, legend=True, legend_out=True, \n",
        "x_estimator=None, x_bins=None, x_ci='ci', scatter=True, fit_reg=True, ci=95, \n",
        "n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, \n",
        "logx=False, x_partial=None, y_partial=None, truncate=False, x_jitter=None, \n",
        "y_jitter=None, scatter_kws=None, line_kws=None)\n",
        "'''\n",
        "\n",
        "# Rent interest based on geographical information\n",
        "sns.lmplot(x='longitude', y='latitude', fit_reg=False, hue='interest_level',\n",
        "           hue_order=['low', 'medium', 'high'], size=9, aspect=1, scatter_kws={'alpha':0.4,'s':30},\n",
        "           data=train[(train['longitude']>train['longitude'].quantile(0.1))\n",
        "                      &(train['longitude']<train['longitude'].quantile(0.9))\n",
        "                      &(train['latitude']>train['latitude'].quantile(0.1))                           \n",
        "                      &(train['latitude']<train['latitude'].quantile(0.9))]);\n",
        "plt.xlabel('Longitude', fontsize=13);\n",
        "plt.ylabel('Latitude', fontsize=13);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9fc81a32-e48a-7320-285e-749411af821b"
      },
      "source": [
        "**Methods**\n",
        "---\n",
        "**Building the Classification Model**\n",
        "\n",
        "Two main techniques are considered to build the classification model: Decision Tree and Ensemble Method. Let us start with the definitions. \n",
        "\n",
        " - A decision tree is a tree structure, where the classification process starts from a root node and is split on every subsequent step based on the features and their values. The exact structure of a given decision tree is determined by a tree induction algorithm; there are a number of different induction algorithms which are based on different splitting criteria such as information gain.\n",
        " - Ensemble learning method constructs a collection of individual classifiers that are diverse yet accurate. \n",
        "    1. Bagging\n",
        "   - One of the most popular techniques for constructing ensembles is boostrap aggregation called\n",
        "   \u2018bagging\u2019. In bagging, each training set is constructed by forming a bootstrap replicate of the original training set. So this bagging algorithm is promising ensemble learner that improves the results of any decision tree based learning algorithm.\n",
        "    2. Boosting\n",
        "   - Gradient boosting is also powerful techniques for building predictive models. While bagging considers candidate models equally, boosting technique is based on whether a weak learner can be modified to become better. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost stands for eXtreme Gradient Boosting.\n",
        "\n",
        "I generated a set of new features derived from the datasets as a preprocessing. A table with a new set of 15 features is generated in a CSV format instead of original mixed data types instances and it is mapped into inputs in XGBoost classification model. Other classification models - Support Vector Machine, Rnadom Forest, and Gradient Random Boosting were used to compare its performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5b37d9f-7d46-9c10-172b-1f73cf418817"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "def pre_processing(data):\n",
        "    \n",
        "    global important_features\n",
        "    important_features = ['bathrooms', 'bedrooms', 'price', 'price_room',\n",
        "                          'latitude','longitude', 'nb_images','nb_features',\n",
        "                          'sentiment', 'nb_description', 'description_len',\n",
        "                          'b_counts', 'm_counts','b_count_log', 'm_count_log']\n",
        "    \n",
        "    data['nb_images'] = data['photos'].apply(len)\n",
        "    data['nb_features'] = data['features'].apply(len)\n",
        "    data['nb_description'] = data['description'].apply(lambda x: len(x.split(' ')))\n",
        "    data['description_len'] = data['description'].apply(len)\n",
        "    \n",
        "    def room_price(x, y):\n",
        "        if y == 0:\n",
        "            return 0\n",
        "        return x/y\n",
        "    \n",
        "    def sentiment_analysis(x):\n",
        "        if len(x) == 0:\n",
        "            return 0\n",
        "        return TextBlob(x[0]).sentiment.polarity\n",
        "    \n",
        "    data = data.join(data['description'].apply(\n",
        "                         lambda x: TextBlob(x).sentiment.polarity).rename('sentiment'))\n",
        "    data['price_room'] = data.apply(lambda row: \n",
        "                                    room_price(row['price'],row['bedrooms']), axis=1)\n",
        "    \n",
        "    build_counts = pd.DataFrame(data.building_id.value_counts())\n",
        "    build_counts['b_counts'] = build_counts['building_id']\n",
        "    build_counts['building_id'] = build_counts.index\n",
        "    build_counts['b_count_log'] = np.log2(build_counts['b_counts'])\n",
        "    data = pd.merge(data, build_counts, on='building_id')\n",
        "    \n",
        "    man_counts = pd.DataFrame(data.manager_id.value_counts())\n",
        "    man_counts['m_counts'] = man_counts['manager_id']\n",
        "    man_counts['manager_id'] = man_counts.index\n",
        "    man_counts['m_count_log'] = np.log10(man_counts['m_counts'])\n",
        "    data = pd.merge(data, man_counts, on='manager_id')\n",
        "    \n",
        "    return data[important_features]\n",
        "\n",
        "def print_scores(test_name, train, test):\n",
        "    print ('{0} train score: {1}\\n{0} test score: {2}\\n'.format(test_name,\n",
        "                                                               train,\n",
        "                                                               test))\n",
        "\n",
        "def classification(train_data, test_data, target, test_size=0.2, random_state=42):    \n",
        "    # Split data into X and y\n",
        "    X = numerical_features\n",
        "    Y = train['interest_level']\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size,\n",
        "                                                        random_state=random_state)\n",
        "    \n",
        "    # Support vector machine\n",
        "    svm_model = svm.SVC(decision_function_shape='ovo', tol=0.00000001)\n",
        "    svm_model = svm_model.fit(X_train, y_train)\n",
        "    print_scores(\"Support Vector Machine\",\n",
        "                 svm_model.score(X_train, y_train),\n",
        "                 accuracy_score(y_test, svm_model.predict(X_test)))\n",
        "\n",
        "    # Random Forest\n",
        "    random_forest = RandomForestClassifier(n_estimators=10)\n",
        "    random_forest = random_forest.fit(X_train, y_train)\n",
        "    print_scores(\"Random Forest\",\n",
        "                 random_forest.score(X_train, y_train),\n",
        "                 accuracy_score(y_test, random_forest.predict(X_test)))\n",
        "\n",
        "    # GradientBoostingClassifier\n",
        "    gradientB_model = GradientBoostingClassifier(n_estimators=20,\n",
        "                                      learning_rate=1.0,\n",
        "                                      max_depth=1,\n",
        "                                      random_state=0).fit(X_train, y_train)\n",
        "    gradientB_model = gradientB_model.fit(X_train, y_train)\n",
        "    print_scores(\"Gradient Boosting Classifier\",\n",
        "                 gradientB_model.score(X_train, y_train),\n",
        "                 accuracy_score(y_test, gradientB_model.predict(X_test)))\n",
        "\n",
        "processed_test_data = pre_processing(test)\n",
        "print ('A set of 15 derived features:{0}'.format(important_features))\n",
        "'''\n",
        "start_time = time.time()\n",
        "classification(numerical_features, processed_test_data, train['interest_level'])\n",
        "print ('--- %s seconds ---' % (time.time() - start_time))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f6682a9-8ccf-d877-a1af-23a9d73319c2"
      },
      "source": [
        " 1. - XGBoost Classifier train score: 0.727565157924\n",
        "    - XGBoost Classifier test score: 0.708742781886\n",
        " 2. - Support Vector Machine train score: 0.976672323396 \n",
        "   - Support Vector Machine test score: 0.694053287408\n",
        " 3. - Random Forest train score: 0.96651553912 \n",
        "   - Random Forest test score: 0.702360449802\n",
        " 4. - Gradient Boosting Classifier train score: 0.717155087257\n",
        "   - Gradient Boosting Classifier test score: 0.700638233208\n",
        "\n",
        "--- 787.343513966 seconds\n",
        "\n",
        "\n",
        "Reference\n",
        "----------\n",
        "\n",
        " - Classification models:\n",
        "\n",
        "1. https://blog.nycdatascience.com/student-works/renthop-kaggle-competition-team-null/\n",
        "2. http://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
        "\n",
        " - EDA:\n",
        "   https://www.kaggle.com/poonaml/two-sigma-connect-rental-listing-inquiries/two-sigma-renthop-eda"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}