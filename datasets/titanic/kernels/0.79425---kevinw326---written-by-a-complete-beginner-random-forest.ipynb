{"cells":[{"metadata":{"_uuid":"5b3309dcd97b62c9b952c5b88d5146974f3f0b54"},"cell_type":"markdown","source":"If someone does actually read this, please let me know in the comments if there's anything that I can improve on code (like more effcient ways to write the helpers etc.) or writing-wise, or with anything in regards to ML."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**Setup for the File**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport math\nimport re\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55e8a8ec2d55ee862aa102340d6f5895fb81e618"},"cell_type":"markdown","source":"**Getting the Training and Testing Data**"},{"metadata":{"trusted":true,"_uuid":"237196ea5dc5c0ba4017dcee31e3f91279e05dce"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d38d14a897f880d8f8a2d4ec81aff774da301e8"},"cell_type":"markdown","source":"**Functions for Encoding Categorical Data such as Male/Female, etc.**\n\nBecause some of the categories that are important are categorical, it is necessary to first encode them before running any functions on them. Furthermore, to further categorize the data, we can also split features such as age, which has a large span, into classes of their own, such as 0 to 10, 10 to 20, etc. We will just use these numbers, and following increments of 10, to encode age.  We can encode sex using 0 for male and 1 for female, and by encoding these values using simple functions, we can pass them through sklearn functions. However, we know that there are missing values in the training data, such as in age. Perhaps we can see whether we can just plug them into a most common group."},{"metadata":{"trusted":true,"_uuid":"ace2b66014ac888b68eb23433c39561df5e6cdd1"},"cell_type":"code","source":"fig, ax = pyplot.subplots(1,figsize = (12,9))\nsns.distplot(train['Age'].dropna(), bins = 16, ax = ax)\nprint(\"Missing Ages: \", len(train[train['Age'].isnull()]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbe0540f7cf2c08131cda61babf2ca3252cb31b9"},"cell_type":"markdown","source":"We see that most of the passengers are around 15-30 years old. However, we see that there are 177 passengers with an unknown age. We can't just say that all of them are 15-30. So, even though it will affect the training set, we will just drop them for now. Later, maybe we can put them in to keep the data."},{"metadata":{"trusted":true,"_uuid":"25ed1059d14c5e8836c44922899c4f51e0357153"},"cell_type":"code","source":"#drop the null Age values\n#remove the null data for now\ntrain_drop = train.copy()\ntrain_drop = train_drop.dropna(subset = ['Age'])\n\n#encoding the sex into 0s and 1s\ndef encodeSex(sex):\n    if sex == \"male\": return 0\n    #if female\n    return 1\n\n#encoding the ages of passengers into groups\ndef encodeAge(age):\n    return int(age/5)\n\n#encode the data in the DataFrame\ntrain_drop.Sex = train_drop.Sex.apply(encodeSex)\ntrain_drop.Age = train_drop.Age.apply(encodeAge)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"460ce499dbf116a4bf53c7f2a33736fda9cb0818"},"cell_type":"markdown","source":"**Picking Features**\n\nThere are several factors that could affect the survival of a particular passenger. For example, features such as Pclass and Fare are representative of the passenger's wealth, and wealthier passengers could have been deemed important and were priorities to save. Sex and Age were important as women and children were prioritized as well. "},{"metadata":{"trusted":true,"_uuid":"67a7f6b3896ae3957b7ffa96709dec3800a72246"},"cell_type":"code","source":"#The features that seem important to survival\nfeatures = ['Pclass','Sex', 'Age', 'Fare']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33a1381e9551af7c1cce558b6eb0f8966b5a22ce"},"cell_type":"markdown","source":"Now that we have everything, we can put a Random Forest Classifier on our data. "},{"metadata":{"trusted":true,"_uuid":"d38ec9e423a9c8878e658bebd994cfc5c3199a3e"},"cell_type":"code","source":"X = train_drop[features]\ny = train_drop.Survived\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n\n#create a RandomForestClassifier\nforest = RandomForestClassifier(max_leaf_nodes = 55)\nforest.fit(train_X, train_y)\nprint(\"Feature importance: \", forest.feature_importances_)\nprint(\"Accuracy: \", forest.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"398279fc7f7336526c10f90069fce5106cdfa880"},"cell_type":"markdown","source":"In testing, I found that ~50 for max_leaf_nodes results in, on average, the best accuracy, which seems to hover around 85%. Furthermore, using forest.feature_importances_, we see that age and fare seem to be the best indicators of survival. However, we did drop quite a few values. Perhaps, by keeping those values in and adding them to the most common group, something will change. But we can't just drop them all into the most common age group, as that would skew the data. Instead, we can try to predict the age of the passengers.\n\nA pattern that one may see is in the titles given to the passengers, which could be a solid indicator of their average age. To test this theory, we can create a new feature for each passenger called Title, which will indicate what title the passenger has. We can see that titles such as Mrs. and  Mr. indicate that the passenger is married, and therefore may be in an older group, while Miss. and Master. mean that the passenger may be younger. Aside from these there are also other titles, such as Sir and Don. With these, we can see if there is a corrlation between age and title to predict the missing ages. To go along with this, we can use the other existing features to predict age as well. "},{"metadata":{"trusted":true,"_uuid":"861f24bf3862854a8a02a01792ca598c6f3ae223"},"cell_type":"code","source":"#Some of the code here is borrowed\n\ndef extractTitle(name):\n    title = re.search(' ([A-Za-z]+)\\.', name)\n    if title:\n        return title.group(1)\n    return \"\"\n    \n#Create a new column in the data called title\ntrain_with_title = train.copy()\ntrain_with_title = train_with_title.dropna(subset = ['Age'])\ntrain_with_title['Title'] = train_with_title.Name.apply(extractTitle)\n\n#plot the age against the title\nfig, ax = pyplot.subplots(1,figsize = (18,6))\nsns.scatterplot(x = \"Title\", y = \"Age\", data = train_with_title, ax = ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"473814376c90654871e2ee0994b36f342738f17f"},"cell_type":"markdown","source":"Maybe there isn't much of a correlation. While it seems that Master generally indicates a young male, and Miss a young to middle-aged female, the other two significant titles, Mr and Mrs, have a wide span of ages. Nevertheless, we'll run a linear regression on this."},{"metadata":{"trusted":true,"_uuid":"a39c0bc4202ee00bbbd4ee5d7af2933b6b33b9d8"},"cell_type":"code","source":"#encode the titles\ndef encodeTitle(title):\n    if title == \"Master\": return 0\n    if (title == \"Miss\" or title == \"Mme\" or\n       title == \"Ms\" or title == \"Mlle\"): return 1\n    if (title == \"Mr\" or title == \"Mrs\" or\n       title == \"Countess\" or title == \"Jonkheer\"): return 2\n    if (title == \"Rev\" or title == \"Dr\"): return 3\n    if (title == \"Don\" or title == \"Major\" or\n        title == \"Lady\" or title == \"Sir\"): return 4\n    return 5\n\ntrain_with_title.Title = train_with_title.Title.apply(encodeTitle)\n\n#plot the age against the title\nfig, ax = pyplot.subplots(1,figsize = (18,6))\nsns.scatterplot(x = \"Title\", y = \"Age\", data = train_with_title, ax = ax)\n\ntest_features = [\"Title\"]\n\n#We can test a decision tree on train_with_title\nline = LinearRegression()\n\nX = train_with_title[test_features]\ny = train_with_title.Age\n\n#training the data\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\nline.fit(train_X, train_y)\nprint(\"Accuracy: \", line.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789e64622fea24c425404960d762d2163ecd8b10"},"cell_type":"markdown","source":"It did better than I expected, but the prediction is still pretty bad. But, we can see an upward trend in age as the title number increases, so maybe not all is lost. Perhaps, by adding in some other features, we can increase the accuracy of our age predictor. "},{"metadata":{"trusted":true,"_uuid":"c23f4e56f0086dba2367254f1fed0354a3cacf3b"},"cell_type":"code","source":"test_features = [\"Title\", \"SibSp\", \"Parch\", \"Pclass\", \"Fare\"]\n\nX = train_with_title[test_features]\ny = train_with_title.Age\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\nline.fit(train_X, train_y)\nprint(\"Accuracy: \", line.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4997c8c1f6e266e4b34a13d406b488760d4540a"},"cell_type":"markdown","source":"It does a little bit better, but it's still a pretty bad predictor. But, it's definitely better than just putting all the missings values into the same class, so we can add them in and see where it goes from there."},{"metadata":{"trusted":true,"_uuid":"a9e5c36ec3a4396954a247a97d633afd8991bc49"},"cell_type":"code","source":"predicted_ages = train[train['Age'].isnull()]\npredicted_ages['Title'] = predicted_ages.Name.apply(extractTitle)\npredicted_ages['Title'] = predicted_ages.Title.apply(encodeTitle)\npredictions = pd.Series(line.predict(predicted_ages[test_features]))\n\ntrain_with_ages = train\n#fill in the missing values\n#There should be a cleaner and quicker way to write this\n#because doing this is really really slow\ninc = 0\nfor i in range(0, len(train['Age'])):\n    if math.isnan(train['Age'][i]):\n        train_with_ages['Age'][i] = predictions[inc]\n        inc += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d320ac16e05b2a53ebe344e7f454f04e7e4419f"},"cell_type":"markdown","source":"Now that we've filled in the values, let's fit the classifier again. "},{"metadata":{"trusted":true,"_uuid":"a2aaf5fc45b603bf18fe65aa85d38c97f3597e37"},"cell_type":"code","source":"#encode the new df\n#encode the data in the DataFrame\ntrain_with_ages.Sex = train_with_ages.Sex.apply(encodeSex)\ntrain_with_ages.Age = train_with_ages.Age.apply(encodeAge)\n\nfeatures = ['Pclass','Sex', 'Age', 'Fare']\n\nX = train_with_ages[features]\ny = train_with_ages.Survived\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=1)\n\n#fit the RandomForestClassifier\nforest = RandomForestClassifier(max_leaf_nodes = 55)\nforest.fit(train_X, train_y)\nprint(\"Feature importance: \", forest.feature_importances_)\nprint(\"Accuracy: \", forest.score(test_X, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd01fc3e31a27b01c14a1d7fc8341c7048c6923a"},"cell_type":"markdown","source":"Now that we filled in the ages, we see that the accuracy actually decreased, likely due to some error from predicting the age with only a 40% accuracy rate. But, for now, I will accept it as is and submit. If anybody does happen to read this, please leave a comment with what I can improve or other ideas that I should look at. I'll probably come back to this and try to improve my accuracy periodically."},{"metadata":{"trusted":true,"_uuid":"8e48750485c499b5087c4ecc3699f82d54da7f94"},"cell_type":"code","source":"#making, training and submitting test\npredicted_ages = test[test['Age'].isnull()]\npredicted_ages['Title'] = predicted_ages.Name.apply(extractTitle)\npredicted_ages['Title'] = predicted_ages.Title.apply(encodeTitle)\npredictions = pd.Series(line.predict(predicted_ages[test_features]))\n\n#fill in the missing values\n#There should be a cleaner and quicker way to write this\ninc = 0\nfor i in range(0, len(test['Age'])):\n    if math.isnan(test['Age'][i]):\n        test['Age'][i] = predictions[inc]\n        inc += 1\n        \n#encode the new df\n#encode the data in the DataFrame\ntest.Sex = test.Sex.apply(encodeSex)\ntest.Age = test.Age.apply(encodeAge)\ntest['Fare'] = test['Fare'].fillna(value = 13)\n\nfeatures = ['Pclass','Sex', 'Age', 'Fare']\nX = test[features]\n\nfinal_predict = forest.predict(X)\nprediction = pd.DataFrame(test.PassengerId)\nprediction['Survived'] = final_predict.astype('int')\n\nprediction.to_csv('predict.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}