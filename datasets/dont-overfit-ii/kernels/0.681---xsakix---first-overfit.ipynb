{"cells":[{"metadata":{"_uuid":"5c23f3101199c38222b13ba2ae0406c4d43dc97e"},"cell_type":"markdown","source":"Just for fun:)\n\nUsing knowledge gained by competing in quora incencere questions:\n\nhttps://www.kaggle.com/xsakix/pytorch-bilstm-meta-v2\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cb7d4ec05661a3023a790cb659a024c9b6e7c5c"},"cell_type":"code","source":"import random\nimport torch\n\nseed = 6017\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed)\nprint('Seeding done...')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\nprint(df_train.drop(columns=['id']).head())\n# print(df_train.drop(columns=['id'])['target'].values == df_train.target.values)\ntrain_y = df_train.target.values\ntrain_x = df_train.drop(columns=['id','target']).values\nprint(train_x[:5])\nprint('-'*80)\nprint(train_y[:5])\nprint('-'*80)\nprint(train_y.shape)\nprint(train_x.shape)\nprint('-'*80)\n\ndf_test = pd.read_csv(\"../input/test.csv\")\nprint(df_test.shape)\ntest_x = df_test.drop(columns=['id']).values\nprint(test_x.shape)\nprint('-'*80)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7d2183392ccd01fc9ff0842e42834975714bd65"},"cell_type":"code","source":"# oversampling doesnt work\n# from imblearn.over_sampling import RandomOverSampler,ADASYN, SMOTE, SMOTENC\n\n# resampled_train_x1, resampled_train_y1 = RandomOverSampler(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x2, resampled_train_y2 = ADASYN(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x3, resampled_train_y3 = SMOTE(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x4, resampled_train_y4 = SMOTENC(random_state=seed,categorical_features=[0,1]).fit_resample(train_x, train_y)\n# resampled_train_x = np.concatenate([resampled_train_x1,resampled_train_x2,resampled_train_x3,resampled_train_x4])\n# resampled_train_y = np.concatenate([resampled_train_y1,resampled_train_y2,resampled_train_y3,resampled_train_y4])\n# print(resampled_train_x.shape)\n# print(resampled_train_y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b381258dffb0e94f2da21eb428d2a9bf74ebf134"},"cell_type":"code","source":"print(np.min(train_x))\nprint(np.max(train_x))\nprint(train_x.shape[0])\n\n# for i in range(train_x.shape[1]):\n#     print(np.mean(train_x[:,i]),':',np.std(train_x[:,i]))\n    \nmu_x = np.mean(train_x,axis=0)\nstd_x = np.std(train_x,axis=0)\nprint(mu_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b7891e2595d62173eba1c7bd22c50524e53c702"},"cell_type":"code","source":"# takes too long, not feasible\n# import pymc3 as pm\n\n# betas = []\n\n# with pm.Model() as model:\n#     sigma = pm.Uniform(name='sigma', lower=np.min(train_x), upper=np.max(train_y))\n#     for i in range(train_x.shape[1]):\n#         betas.append(pm.Normal(name='b'+str(i), mu=mu_x[i], sd=std_x[i]))\n#     mu = pm.Deterministic('mu', sum([betas[i]*train_x[:,i] for i in range(train_x.shape[1])]))\n#     target = pm.Normal(name='target', mu=mu, sd=sigma, observed=train_y)\n#     trace_model = pm.sample(1000, tune=1000)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4893074ab477530d93da58f79f46b514f95a532"},"cell_type":"code","source":"#src: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss dosen't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf9961402ee4f0ffc1b9969623ce809f65e6321"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n#https://www.kaggle.com/shujian/single-rnn-with-4-folds-v1-9\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            score = roc_auc_score(y_true=y_true, y_score=y_proba > threshold)\n#         print('\\rthreshold = %f | score = %f'%(threshold,score),end='')\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n#     print('\\nbest threshold is % f with score %f'%(best_threshold,best_score))\n    search_result = {'threshold': best_threshold, 'AUCROC': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97e0c212296726882b88aad1361e32f7ff50588f"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nimport torchtext.data\nimport warnings\nfrom sklearn.metrics import accuracy_score\nfrom torch.autograd import Variable\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator,self).__init__()\n        self.c1 = nn.Conv1d(300,256,kernel_size=1)\n        self.generator = nn.Sequential(\n            nn.Linear(256,256),\n            nn.ReLU(),\n            nn.Linear(256,300),\n            nn.Tanh()\n        );\n\n    def forward(self,x):\n        x = x.view(x.shape[0],x.shape[1],1)\n        x = F.relu(self.c1(x)).squeeze(2)\n        return self.generator(x)\n\n    \nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator,self).__init__()\n        self.discriminator = nn.Sequential(\n            nn.Linear(300,256),\n            nn.ReLU(),\n            nn.Linear(256,1)\n        );\n\n    def forward(self,x):\n        return self.discriminator(x)\n\n    \n    \n\ndef train_generator_on_set(data_set):\n    batch_size=data_set.shape[0]\n\n    train_tensor = torch.tensor(data_set, dtype=torch.float32).cuda()\n    print(train_tensor.shape)\n    train_dataset = torch.utils.data.TensorDataset(train_tensor)\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size)\n\n    generator = Generator().cuda()\n    discriminator = Discriminator().cuda()\n\n    criterion = nn.BCEWithLogitsLoss().cuda()        \n    optimizerG = optim.Adam(generator.parameters(),lr=1e-3,weight_decay=1e-5)\n    optimizerD = optim.Adam(discriminator.parameters(),lr=1e-3,weight_decay=1e-5)\n\n    G_losses = []\n    D_losses = []\n\n    num_epochs=100\n\n    real_label = 1\n    fake_label = 0\n\n    for epoch in range(num_epochs):\n        for i,x_batch in enumerate(list(iter(train_loader)),1):\n        ############################\n            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n            ###########################\n            ## Train with all-real batch\n            discriminator.zero_grad()\n            # Format batch\n            label = torch.full((batch_size,), real_label).cuda()\n            # Forward pass real batch through D\n            output = discriminator(x_batch[0]).view(-1)\n            # Calculate loss on all-real batch\n            errD_real = criterion(output, label)\n            # Calculate gradients for D in backward pass\n            errD_real.backward()\n            D_x = output.mean().item()\n\n            ## Train with all-fake batch\n            # Generate batch of latent vectors\n            noise = torch.randn(batch_size, 300).cuda()\n            # Generate fake image batch with G\n            label = torch.full((batch_size,), fake_label).cuda()\n            fake = generator(noise)\n            # Classify all fake batch with D\n            output = discriminator(fake.detach()).view(-1)\n            # Calculate D's loss on the all-fake batch\n            errD_fake = criterion(output, label)\n            # Calculate the gradients for this batch\n            errD_fake.backward()\n            D_G_z1 = output.mean().item()\n            # Add the gradients from the all-real and all-fake batches\n            errD = errD_real + errD_fake\n            # Update D\n            optimizerD.step()\n\n            ############################\n            # (2) Update G network: maximize log(D(G(z)))\n            ###########################\n            generator.zero_grad()\n            # Since we just updated D, perform another forward pass of all-fake batch through D\n            label = torch.full((batch_size,), real_label).cuda()\n            output = discriminator(fake).view(-1)        \n            # Calculate G's loss based on this output\n            errG = criterion(output, label)        \n            # Calculate gradients for G\n            errG.backward()\n            D_G_z2 = output.mean().item()\n            # Update G\n            optimizerG.step()\n\n            # Output training stats\n            if epoch % 10 == 0:\n                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                      % (epoch, num_epochs, i, len(train_loader),\n                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n            # Save Losses for plotting later\n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n\n    plt.plot(G_losses,label='generator')\n    plt.plot(D_losses,label='discriminator')\n    plt.legend()\n    plt.show()\n    return generator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e3012692b80a1951fe4c3e9be002de38f431d36"},"cell_type":"code","source":"train_data_set = df_train.drop(columns=['id'])\nprint(train_data_set.shape)\ntrain_1 = train_data_set[train_data_set.target == 1].drop(columns=['target'])\ntrain_0 = train_data_set[train_data_set.target == 0].drop(columns=['target'])\nprint(train_1.shape)\nprint(train_0.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c096766b4d26dc12c90602618dab9f865235c40"},"cell_type":"code","source":"generator_1 = train_generator_on_set(train_1.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41180d28b913efe907d43bb480848a10cd95ee51"},"cell_type":"code","source":"generator_0 = train_generator_on_set(train_0.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40169905972ae7eb49649a837321a9ec79438815"},"cell_type":"code","source":"# generate new data set\ngenerator_1.eval()\ngenerator_0.eval()\nsample_size=1000000\nnew_data_1 = generator_1(torch.randn(int(sample_size/2), 300).cuda()).detach().cpu().numpy()\ny_1 = np.full(int(sample_size/2),1)\nnew_data_0 = generator_0(torch.randn(int(sample_size/2), 300).cuda()).detach().cpu().numpy()\ny_0 = np.full(int(sample_size/2),0)\n\nnew_x = np.concatenate([new_data_1,new_data_0, train_x])\nnew_y = np.concatenate([y_1,y_0,train_y])\n\nprint(new_x.shape)\nprint(new_y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28c53c0c24f0776362bac369306da09943d5582f"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nimport torchtext.data\nimport warnings\nfrom sklearn.metrics import accuracy_score\nfrom torch.autograd import Variable\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nbatch_size=25\n\nclass OverfitModel(nn.Module):\n    def __init__(self):\n        super(OverfitModel,self).__init__()\n        self.input_dim = 300\n        self.hidden_dim=256\n#         self.conv1 = nn.Conv1d(self.input_dim,self.hidden_dim,kernel_size=1)\n#         self.drop_conv = nn.Dropout(0.1)\n#         self.batch_norm_conv = nn.BatchNorm1d(self.input_dim)\n#         self.conv2 = nn.Conv1d(self.hidden_dim,self.hidden_dim,kernel_size=1)\n#         self.conv3 = nn.Conv1d(self.hidden_dim,self.hidden_dim,kernel_size=1)\n#         self.mem = nn.LSTM(int(self.input_dim/50),self.hidden_dim,bidirectional=True,batch_first=True)\n        self.classifier = nn.Sequential(\n#             nn.BatchNorm1d(self.input_dim), - bad\n            nn.Dropout(0.7),\n            nn.Linear(self.input_dim,512),\n#             nn.LayerNorm(256),\n            nn.ReLU(True),\n            nn.Dropout(0.9),\n            nn.Linear(512,1)\n        );\n\n    def forward(self,x):\n#         h = (torch.zeros(2,x.shape[0],self.hidden_dim).cuda(),torch.zeros(2,x.shape[0],self.hidden_dim).cuda())\n#         x = x.view(x.shape[0],50,int(x.shape[1]/50))\n#         _,h = self.mem(x)\n#         x = torch.cat([h[0][-1,:,:],h[0][-2,:,:]],dim=1).cuda()\n#         x = self.batch_norm_conv(x)\n    \n#         x = x.view(x.shape[0],x.shape[1],1)\n#         x = F.relu(self.conv1(x))\n#         x = F.relu(self.conv2(x))\n#         x = F.relu(self.conv1(x)).squeeze(2)\n#         print(x.shape)\n        return self.classifier(x)\n\n\ndef eval_on_set(model,test_loader,loss_function):\n    pred = []\n    avg_loss = 0.\n    with torch.no_grad():\n        model.eval()\n        for batch,(x_test_batch,y_test_batch) in enumerate(list(test_loader),1):\n            y_pred = model(x_test_batch).squeeze(1)\n            pred += torch.sigmoid(y_pred).cpu().detach().numpy().tolist()\n            loss = loss_function(y_pred,y_test_batch)\n            avg_loss += loss.item()\n            \n    return np.array(pred),avg_loss/batch\n\n\ndef train(model, train_loader,optimizer,loss_function ):    \n    \n    model.train()\n    avg_loss = 0\n    pred = []\n    for batch,(x_batch,y_true) in enumerate(list(iter(train_loader)),1):\n        optimizer.zero_grad()\n\n        y_pred = model(x_batch).squeeze(1)\n        pred += torch.sigmoid(y_pred).cpu().detach().numpy().tolist()\n        loss = loss_function(y_pred,y_true)\n        avg_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n    \n    return np.array(pred),avg_loss/batch\n\ndef eval_sub(model,submission_loader):\n\n    pred = []\n    with torch.no_grad():\n        model.eval()\n        for (x,) in list(submission_loader):       \n            y_pred = torch.sigmoid(model(x).squeeze(1)).detach()\n            pred += y_pred.cpu().numpy().tolist()\n\n    return np.array(pred)\n\n    \nsubmission_dataset = torch.utils.data.TensorDataset(torch.tensor(test_x, dtype=torch.float32).cuda())\nsubmission_loader = torch.utils.data.DataLoader(dataset=submission_dataset,batch_size=batch_size, shuffle=False)\n\n# weights = compute_class_weight('balanced',np.unique(train_y),train_y)\n# print(weights)\nX_train1,X_val,y_train1, y_val = train_test_split(new_x,new_y,random_state=seed,stratify=new_y)\n    \nx_train_tensor = torch.tensor(X_train1, dtype=torch.float32).cuda()\ny_train_tensor = torch.tensor(y_train1, dtype=torch.float32).cuda()\ntrain_dataset = torch.utils.data.TensorDataset(x_train_tensor,y_train_tensor)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n\nx_val_tensor = torch.tensor(X_val, dtype=torch.float32).cuda()\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).cuda()\nval_dataset = torch.utils.data.TensorDataset(x_val_tensor,y_val_tensor)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,batch_size=batch_size,shuffle=False)\n\nmodel = OverfitModel().cuda()\n\nloss_function = nn.BCEWithLogitsLoss().cuda()        \noptimizer = optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-5)\nearly_stop = EarlyStopping(patience=5)\n\naurocs = []\nval_aurocs = []\nlosses = []\nval_losses = []\n\nfor epoch in range(20):\n    y_pred,loss = train(model, train_loader,optimizer,loss_function)\n    losses.append(loss)\n    search = threshold_search(y_train1,y_pred)\n    auroc = search['AUCROC']\n    aurocs.append(auroc)\n    y_pred, val_loss = eval_on_set(model,val_loader, loss_function)\n    val_losses.append(val_loss)\n    search = threshold_search(y_val,y_pred)\n    val_aurocs.append(search['AUCROC'])\n    print('EPOCH: ',epoch,': loss :',loss,': auroc: ',auroc,' : val loss: ',val_loss,': val AUC: ',search['AUCROC'])\n    print('-'*80)\n#     early_stop(np.round(1.-search['AUCROC'],decimals=4),model)\n#     if early_stop.early_stop:\n#         break\n\nprint('FINISHED TRAINING META...')\ntorch.save(model.state_dict(), 'checkpoint.pt')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa83c123bde90d12e5f0a5aa711cbe94a994d932"},"cell_type":"code","source":"f,ax = plt.subplots(1,2)\nf.set_size_inches(18.5, 10.5)\nax[0].plot(aurocs,label='aurocs')\nax[0].plot(val_aurocs,label='val_aurocs')\nax[0].legend()\nax[1].plot(losses,label='losses')\nax[1].plot(val_losses,label='val_losses')\nax[1].legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd5ecb73f89683345f7865c4d8becedd29f94b36"},"cell_type":"code","source":"x_train_tensor = torch.tensor(train_x, dtype=torch.float32).cuda()    \nmodel = OverfitModel().cuda()\nmodel.load_state_dict(torch.load('checkpoint.pt'))\n\n#0.31\nmodel.eval()\ny_pred = torch.sigmoid(model(x_train_tensor)).squeeze(1)\ny_pred = y_pred.detach().cpu().numpy()\nprint(y_pred.shape)\n\nsearch_result = threshold_search(train_y, y_pred)\nprint(search_result)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8630ab81c8f865624c185ce0b71b645a30e46c3a"},"cell_type":"code","source":"y_pred = eval_sub(model,submission_loader)\n\ndf_subm = pd.DataFrame()\ndf_subm['id'] = df_test.id\ndf_subm['target'] = (y_pred > search_result['threshold']).astype(int)\nprint(df_subm.head())\nprint(df_subm.shape)\ndf_subm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}