{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["To find the author of text we need two things: features and classificator. My approach here is to use one classifier for tagged words and one for feature classification. First I take treebank POS tagged words form NLTK packages. With this I train the tagger. POS means Part OF Speach, so we get a word and must define it is verb (VB) for example or adjective (JJ). The idea is to use tagged words ( or sequence of them) as features."], "metadata": {"_uuid": "fc8e9cebbac1662ca841662d7f2e286fd34cec02", "_cell_guid": "10c002ab-1615-431a-9bf6-ef8b5404ecfd"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["from nltk.corpus import treebank\n", "from nltk.tag.sequential import ClassifierBasedPOSTagger"], "metadata": {"_uuid": "f186de777437746380a94131581fedc742718b5c", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "828c71eb-de3e-457f-9095-0bdcdf50ea51"}}, {"cell_type": "markdown", "source": ["Now other imports:"], "metadata": {"_uuid": "c5bab4465f0769900743bb6cfa52bb1f1e60c33b", "_cell_guid": "9d85d4bc-4ccd-49c1-ba49-6fa5bbe422d0"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["import pandas as pd\n", "from sklearn import preprocessing\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction import DictVectorizer\n", "from sklearn.naive_bayes import BernoulliNB\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.pipeline import make_pipeline\n", "import numpy as np\n", "import mglearn\n", "from sklearn.metrics import confusion_matrix\n", "import matplotlib.pyplot as plt\n", "import nltk\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem.snowball import SnowballStemmer\n", "import itertools"], "metadata": {"_uuid": "584a2491a19f92920b7597ee1b52d86abd0746f5", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "663a7668-8477-431d-9ccb-cb4d44319773"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["# -------------- Main code\n", "train = pd.read_csv('train.csv')\n", "train_sents = treebank.tagged_sents()\n", "tagger = ClassifierBasedPOSTagger(train=train_sents)\n", "stemmer = SnowballStemmer('english')"], "metadata": {"_uuid": "c6535147a78221914f0bd3dc905e0194ce319a3e", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "34df873a-6cad-4112-ba77-afb10fe55947"}}, {"cell_type": "markdown", "source": ["NLTK has a good parser RegexpParser, it takes as argument taggs in appropriate format. For more info follow this link:\n", "\n", "[nltk book](http://http://www.nltk.org/api/nltk.chunk.htm)\n", "\n", "Tags can give as information about style of give author for example. I use this and search features as \"tags sequences\", with other words: sequence of words with given tag. One sequence is one unique feature. It can be more or less usefull but I beleave this approach can gvie as many possibilities: see get_sequence_tags()"], "metadata": {"_uuid": "0b26e523e1431b47a4d7ae770e5afe4f078a4b0a", "_cell_guid": "c5cbad5c-a758-4829-841c-6d50a1473d6c"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["# Define tag sequences\n", "SEQ_1 = \"SEQ_1: {<DT|PP>?<JJ>*}\"\n", "SEQ_2 = \"SEQ_2: {<NN><DT|PP\\$>?<JJ>}\"\n", "SEQ_3 = \"SEQ_3: {<NP>?<VERB>?<NP|JJ>}\"\n", "SEQ_4 = \"SEQ_4: {<VB.*><NP|PP|CLAUSE>+$}\"\n", "\n", "cp1 = nltk.RegexpParser(SEQ_1)\n", "cp2 = nltk.RegexpParser(SEQ_2)\n", "cp3 = nltk.RegexpParser(SEQ_3)\n", "cp4 = nltk.RegexpParser(SEQ_4)\n", "\n", "lst_seq = list([cp1, cp2, cp3, cp4])"], "metadata": {"_uuid": "eb3407e471c307006186a98a1dda2333b6e76af2", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "1deec3c1-a076-4a0a-9705-9e947eba2e90"}}, {"cell_type": "markdown", "source": ["In the code above I define foor sequnces which then I send as arguments to nltk.RegexpParser(), then I collect the foor nltk.RegexpParser objects. This list is ised as you can see in function get_sequence_tags. Here is the place to show all functions we need to get features, except function plot_confusion_matrix(), the others are for gewtting features."], "metadata": {"_uuid": "7140d501840206fd935845e9b125d627edc6378e", "_kg_hide-output": true, "_cell_guid": "0f1a36be-7557-4b49-882c-b1ff02003c56"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        print(\"Normalized confusion matrix\")\n", "    else:\n", "        print('Confusion matrix, without normalization')\n", "\n", "    print(cm)\n", "\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.tight_layout()\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')\n", "\n", "\n", "def get_number_of_spaces(sentence):\n", "    return sentence.count(' ')\n", "\n", "\n", "def get_number_of_capitals(sentence):\n", "    n = sum(1 for c in sentence if c.isupper())\n", "    return n\n", "\n", "\n", "def get_number_of_nouns(taged_tokens):\n", "    n = sum(1 for word, tag in taged_tokens if tag == 'NN' or tag == 'NNS' \\\n", "            or tag == 'NNP' or tag == 'NNP')\n", "    return n\n", "\n", "\n", "def get_number_of_adjectives(taged_tokens):\n", "    n = sum(1 for word, tag in taged_tokens if tag == 'JJ' or tag == 'JJR' or tag == 'JJS')\n", "    return n\n", "\n", "\n", "def get_count_of_tagged(taged_tokens, tag_in):\n", "    n = sum(1 for word, tag in taged_tokens if tag == tag_in)\n", "    return n\n", "\n", "\n", "def is_past_tense(taged_tokens):\n", "    n = sum(1 for word, tag in taged_tokens if tag == 'VBD')\n", "    return (n > 0)\n", "\n", "\n", "def is_modal(taged_tokens):\n", "    n = sum(1 for word, tag in taged_tokens if tag == 'MD')\n", "    return (n > 0)\n", "\n", "\n", "def vocab_richness(sentence):\n", "    unique = set(sentence.split())\n", "    count_uniques = len(unique)\n", "    return count_uniques\n", "\n", "\n", "def get_first_words(sentence, count):\n", "    arr_words = sentence.split()\n", "    ret_words = arr_words[:count]\n", "    str_ret = ' '.join(ret_words)\n", "    return str_ret\n", "\n", "\n", "def get_one_word(sentence, position):\n", "    arr_words = sentence.split()\n", "    if len(arr_words) >= (position + 1):\n", "        ret_word = arr_words[position]\n", "        return ret_word\n", "    else:\n", "        return False\n", "\n", "\n", "def exists_she(sentense):\n", "    if 'she' in sentense.lower():\n", "        return True\n", "    else:\n", "        return False\n", "\n", "\n", "def exists_he(sentense):\n", "    if 'he' in sentense.lower():\n", "        return True\n", "    else:\n", "        return False\n", "\n", "\n", "\n", "def first_tag(taged_tokens):\n", "    return str(taged_tokens[0][1])\n", "\n", "\n", "def second_tag(taged_tokens):\n", "    if len(taged_tokens) > 1:\n", "        return str(taged_tokens[1][1])\n", "    else:\n", "        return False\n", "\n", "\n", "def third_tag(taged_tokens):\n", "    if len(taged_tokens) > 2:\n", "        return str(taged_tokens[2][1])\n", "    else:\n", "        return False\n", "\n", "def get_consonant_letters(sentence):\n", "    consonants = 0\n", "    for word in sentence:\n", "        for letter in word:\n", "            if letter in 'bcdfghjklmnpqrstvwxz':\n", "                consonants += 1\n", "\n", "    return consonants\n", "\n", "\n", "def get_sonant_letters(sentence):\n", "    sonants = 0\n", "    for word in sentence:\n", "        for letter in word:\n", "            if letter in 'aieouy':\n", "                sonants += 1\n", "\n", "    return sonants\n", "\n", "\n", "def lexical_diversity(text):\n", "    return len(set(text)) / len(text)\n", "\n", "\n", "def get_sequence_tags(taged_tokens, n_sequence):\n", "    countSequence = 0\n", "    cp = lst_seq[n_sequence-1]\n", "    result = cp.parse(taged_tokens)\n", "\n", "    for tre in result:\n", "        if isinstance(tre, nltk.tree.Tree):\n", "            if tre.label() ==  cp._stages[0]._chunk_label:\n", "                countSequence += 1\n", "\n", "    return (countSequence > 0)"], "metadata": {"_uuid": "d38e5877f0425a371c49638abe5e2feb77bfa63d", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "78deed95-8c01-48ad-b7a7-bbc180dad366"}}, {"cell_type": "markdown", "source": ["The most important function, is get_sentence_features(), lets comment a litle bit the code.\n", "Here is the right place to use the NLTK.SnowballStemmer(), when the input sentence com in the function\n", "first it is transformed with the stemmer.What it does? It simply gets the word and give as output the grammatical stem\n", "of it. then the sentence is tokenized from nltk.wordpunct_tokenize and finaly the token string\n", "commes to the tagger. The variable taged_tokens is used as argument to functions to get features."], "metadata": {"_uuid": "860a133340ac008f878d4e1f085241abab3fb408", "_cell_guid": "42cdd0f6-0979-42b5-b871-bb2434766bca"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["def get_sentence_features(sentens_in):\n", "    stemmed_words = list()\n", "    for w in sentens_in.split():\n", "        stemmed_words.append(stemmer.stem(w))\n", "\n", "    sentence = ' '.join(stemmed_words)\n", "    word_tokens = nltk.wordpunct_tokenize(sentence)\n", "\n", "    taged_tokens = tagger.tag(word_tokens)\n", "\n", "    X_dict = {}\n", "\n", "    X_dict['seq_01'] = get_sequence_tags(taged_tokens, 1)\n", "    X_dict['seq_02'] = get_sequence_tags(taged_tokens, 2)\n", "    X_dict['seq_03'] = get_sequence_tags(taged_tokens, 3)\n", "    X_dict['seq_04'] = get_sequence_tags(taged_tokens, 4)\n", "\n", "    X_dict['lexical_diversity'] = lexical_diversity(sentence.lower())\n", "    X_dict['get_consonant_letters'] = get_consonant_letters(sentence.lower())\n", "    X_dict['get_sonant_letters'] = get_sonant_letters(sentence.lower())\n", "\n", "    X_dict['count_of_spaces'] = get_number_of_spaces(sentence)\n", "    X_dict['count_capitals'] = get_number_of_capitals(sentence)\n", "    X_dict['count_nouns'] = get_number_of_nouns(taged_tokens)\n", "    X_dict['count_adjectives'] = get_number_of_adjectives(taged_tokens)\n", "\n", "    X_dict['count_numbers'] = get_count_of_tagged(taged_tokens, 'CD')\n", "    X_dict['count_NNS'] = get_count_of_tagged(taged_tokens, 'NNS')\n", "    X_dict['count_NNP'] = get_count_of_tagged(taged_tokens, 'NNP')\n", "    X_dict['count_NNPS'] = get_count_of_tagged(taged_tokens, 'NNPS')\n", "    X_dict['count_RBS'] = get_count_of_tagged(taged_tokens, 'RBS')\n", "    X_dict['count_RBR'] = get_count_of_tagged(taged_tokens, 'RBR')\n", "    X_dict['count_WP'] = get_count_of_tagged(taged_tokens, 'WP')\n", "    X_dict['count_WP$'] = get_count_of_tagged(taged_tokens, 'WP$')\n", "    X_dict['count_WRB'] = get_count_of_tagged(taged_tokens, 'WRB')\n", "    X_dict['count_PRP'] = get_count_of_tagged(taged_tokens, 'PRP')\n", "    X_dict['count_POS'] = get_count_of_tagged(taged_tokens, 'POS')\n", "    X_dict['count_FW'] = get_count_of_tagged(taged_tokens, 'FW')\n", "    X_dict['count_VB'] = get_count_of_tagged(taged_tokens, 'VB')\n", "    X_dict['count_VBD'] = get_count_of_tagged(taged_tokens, 'VBD')\n", "    X_dict['count_VBG'] = get_count_of_tagged(taged_tokens, 'VBG')\n", "    X_dict['count_VBN'] = get_count_of_tagged(taged_tokens, 'VBN')\n", "    X_dict['count_CC'] = get_count_of_tagged(taged_tokens, 'CC')\n", "\n", "    X_dict['count_DT']         = get_count_of_tagged(taged_tokens, 'DT')\n", "    X_dict['count_UH']         = get_count_of_tagged(taged_tokens, 'UH')\n", "    X_dict['count_SYM']        = get_count_of_tagged(taged_tokens, 'SYM')\n", "    X_dict['count_PDT']        = get_count_of_tagged(taged_tokens, 'PDT')\n", "    X_dict['count_LS']         = get_count_of_tagged(taged_tokens, 'LS')\n", "\n", "    X_dict['count_3rd person'] = get_count_of_tagged(taged_tokens, 'VBZ')\n", "    X_dict['count_gerund'] = get_count_of_tagged(taged_tokens, 'VBG')\n", "\n", "    X_dict['is_past_tense'] = is_past_tense(taged_tokens)\n", "    X_dict['is_modal'] = is_modal(taged_tokens)\n", "    X_dict['vocab_richness'] = vocab_richness(sentence)\n", "    X_dict['first_tag'] = first_tag(taged_tokens)\n", "    X_dict['second_tag'] = second_tag(taged_tokens)\n", "    X_dict['third_tag'] = third_tag(taged_tokens)\n", "    \n", "    X_dict['first_one_word'] = get_one_word(sentence, 0)\n", "    X_dict['second_one_word'] = get_one_word(sentence, 1)\n", "    X_dict['third_one_word'] = get_one_word(sentence, 2)\n", "    X_dict['forth_one_word'] = get_one_word(sentence, 3)\n", "    X_dict['fifth_one_word'] = get_one_word(sentence, 4)\n", "    X_dict['sixth_one_word'] = get_one_word(sentence, 5)\n", "    X_dict['seventh_one_word'] = get_one_word(sentence, 6)\n", "    X_dict['eith_one_word'] = get_one_word(sentence, 7)\n", "    X_dict['ninth_one_word'] = get_one_word(sentence, 8)\n", "    X_dict['tenth_one_word'] = get_one_word(sentence, 9)\n", "\n", "    X_dict['first_6_word'] = get_first_words(sentence, 6)\n", "    X_dict['first_5_word'] = get_first_words(sentence, 5)\n", "    X_dict['first_4_word'] = get_first_words(sentence, 4)\n", "    X_dict['first_3_word'] = get_first_words(sentence, 3)\n", "    X_dict['first_2_word'] = get_first_words(sentence, 2)\n", "\n", "    X_dict['exists_she'] = exists_she(sentence)\n", "    X_dict['exists_he']  = exists_he(sentence)\n", "\n", "    X_dict['first_word_is_the'] = ('the' == get_first_words(sentence.lower(), 1))\n", "    X_dict['first_word_is_she'] = ('she' == get_first_words(sentence.lower(), 1))\n", "    X_dict['first_word_is_he']  = ('he' == get_first_words(sentence.lower(), 1))\n", "    X_dict['first_word_is_it']  = ('it' == get_first_words(sentence.lower(), 1))\n", "    X_dict['first_word_is_this'] = ('this' == get_first_words(sentence.lower(), 1))\n", "    X_dict['first_word_is_you']  = ('you' == get_first_words(sentence.lower(), 1))\n", "\n", "\n", "    X_dict['Raymond'] = ('raymond' in sentence.lower())\n", "    X_dict['Perdita'] = ('perdita' in sentence.lower())\n", "    X_dict['Idris']   = ('idris' in sentence.lower())\n", "    X_dict['Adrian']  = ('adrian' in sentence.lower())\n", "    X_dict['Chapter'] = ('chapter' in sentence.lower())\n", "    X_dict['sinister'] = ('sinister' in sentence.lower())\n", "    X_dict['weird']    = ('weird' in sentence.lower())\n", "    X_dict['horrible'] = ('horrible' in sentence.lower())\n", "\n", "    return X_dict"], "metadata": {"_uuid": "d108956fd226d1d0c9cc4c84cd8656b5769e898d", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "1f3341a3-295b-423b-8b94-a620bbe6a9e1"}}, {"cell_type": "markdown", "source": [" \n", "This function is expected to return dictionary with features which are used later for classification. Let explain the method I use to extract features from a sentence. I mentioned above the tag sequences. What is a tag? This is grammatical description of a given word. The full name of these tags are POS or Part Of Speech tags. It marks a word as verb or adjective for example. In my opinion every author of text, not only horror authors use different style and structure of the written text. This style can be described as feature.\n", "\n", "The full list of POS tags can be viewed here:\n", "\n", "[penn_treebank_pos](http:///www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n", "\n", "Next variables: SEQ_1, SEQ_2, SEQ_3, SEQ_4 are just strings used as argument for nltk.RegexpParser(). Here you can find detailed information how to used this parser:\n", "\n", "[nltk.org book](http://www.nltk.org/book/ch07.html)\n", "\n", "Take attention to functions I use for feature extraction: get_sonant_letters(): All letters that are sonant in english language: \"aieouy\" Another function to get consonant letters is: get_consonant_letters() theese letters are : \"bcdfghjklmnpqrstvwxz\" Getting the count of these types of lettes are our features.\n", "\n", "Another important feature is vocab_richness: it gives the count of unoque words in sentence. Also lexical_diversity feature that gives the divercity of given sentence. The function get_count_of_tagged(taged_tokens, 'CD') returns the count of \"CD Cardinal number\" numbers, the function get_count_of_tagged(taged_tokens, 'NNP') returns the count of nouns. So in this way we can get count of different POS tags in the sentence. So, we can find the use of \" 3rd person\" or if the sentence is in past tense. Additionaly I take first, second etc... words evey as feature. Last features are just words form train.csv. They are choosen emiricaly , not used any special method, just words with many counts. \n", "\n", "However they do not give as much information about the author, we cannot rely on them, it is not sure that these names are used in any text from the authors."], "metadata": {"_uuid": "5356becdfbd15598b11ff19e9b8fb3044d72b397", "_cell_guid": "8229f255-f870-4c85-911f-2e46f0bc7d1b"}}, {"cell_type": "markdown", "source": ["Now, using LbelEncoder I will transform \"y\" lables as binary with values : [0,1,2]. I realy need this because our classifier will understand only these values."], "metadata": {"_uuid": "48ee37938ff392f93ccd5305a3590a41b3738d08", "_cell_guid": "a4fdf7ce-368c-461e-ae8c-eec2b6f3344c"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["lbl_enc = preprocessing.LabelEncoder()\n", "y = lbl_enc.fit_transform(train.author.values)"], "metadata": {"_uuid": "6e7bd5f8b74c7bbe3dfff9d15bd535bf0d5d781d", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "9f8b8f2d-013d-4577-86d9-330a7d4044d5"}}, {"cell_type": "markdown", "source": ["\n", "The i split the input data to foor variables, two for train data and two for validation data."], "metadata": {"_uuid": "327b250491428968c01d4b18e51502e44014520f", "_cell_guid": "786ad883-2471-41e2-81dc-538099172686"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y,\n", "                                                  stratify=y,\n", "                                                  random_state=32,\n", "                                                  test_size=0.2, shuffle=True)\n", "print(xtrain.shape)\n", "print(xvalid.shape)"], "metadata": {"_uuid": "3dc9ffcb760d2978e0f147281e92db7898b746b9", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "57e66f94-1f38-414d-9bb1-a8c204be58f4"}}, {"cell_type": "markdown", "source": ["\n", "Next I need is to get stopwords. Frquently used for NLP tasks stopwords usualy increase positive percent of classifications. Here i just added an array with additional values '.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']"], "metadata": {"_uuid": "616ec587b8ab666eb7371703f55cbd9fa7d3b0e9", "_cell_guid": "34a2545b-b4ea-4de0-ae22-a7cd635ee1b0"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["from nltk.corpus import stopwords\n", "\n", "stop_words = set(stopwords.words('english'))\n", "stop_words.update( ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])"], "metadata": {"_uuid": "70f499a14df1ba546d5597f21f40f935eb8f35aa", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "849f09d0-93d9-4855-b703-0008f48fbc7c"}}, {"cell_type": "markdown", "source": ["\n", "The main processs for extracting features begins here: For all data in input IN_x we try to extract features. The list Out_x contains feature vectors for all input data (IN_x), Out_y is a list with labels. As final result we have two pairs:\n", "\n", "1. X_Train, Y_train -> for train data;\n", "2. X_valid, Y_valid -> for validation;"], "metadata": {"_uuid": "485261086540ddb2d49f5ab1ed5382119a46e0b9", "_cell_guid": "463400bf-9450-437f-908b-e8f81b82eaf8"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["def get_train_features(IN_x, IN_y):\n", "    Out_x, Out_y = [], []\n", "    index = 0\n", "\n", "    for sentens_edna in IN_x:\n", "        word_tokens1 = [i for i in word_tokenize(sentens_edna) if i not in stop_words]\n", "        sentens_in = ' '.join(word_tokens1)\n", "        X_feat_dict = get_sentence_features(sentens_in)\n", "        Out_x.append(X_feat_dict)\n", "        Out_y.append(IN_y[index])\n", "        index += 1\n", "\n", "    return Out_x, Out_y\n", "\n", "\n", "X_Train, Y_train = get_train_features(xtrain, ytrain)\n", "X_valid, Y_valid = get_train_features(xvalid, yvalid)"], "metadata": {"_uuid": "8247fd8e6f60b87d2de8cc8a43a1ba254fac343d", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "0d21ac57-aad7-41f1-8f8e-84ccc9aa23f6"}}, {"cell_type": "markdown", "source": ["We make pipeline and cross validation to achieve best results. \n", "Next step is to transform Xtrain with DictVectorizer() from sklearn.feature_extraction. Our classifier is Naive Bayes or the variant implemention BernoulliNB(). On cross validation, GridSearchCV sends different alpha parameters to BernoulliNB(), the best result is shown.\n", "\n", "We need Xtrain_ctv to train the classifier and X_valid_ctv for our confusion matrix"], "metadata": {"_uuid": "37cf9bc70274cfe70aa512ab88b6718e264ab447", "_cell_guid": "35a8467b-307a-4b8a-8dcb-53d97df741bb"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["clf = grid.best_estimator_.named_steps['bernoullinb']\n", "coef = grid.best_estimator_.named_steps['bernoullinb'].coef_\n", "best_alpha = grid.best_estimator_.named_steps['bernoullinb'].alpha\n", "print(\"Best cross-validation alpha: {:.2f}\".format(best_alpha))"], "metadata": {"_uuid": "a68c4e2f215ccd0c1a9730e773abcbac040f3d7b", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "a140c033-716d-497a-9f2d-7125d53a1fa7"}}, {"cell_type": "markdown", "source": ["We have to determine which of the features are important or what is the \"coef\" of them. \n", "First we get the feature names, then with help the great tool mglearn we draw 3 grapchics , one for every class."], "metadata": {"_uuid": "508a947b3a384de40e8e02a5df96ad2210d60850", "_cell_guid": "42ecce21-e89d-4caa-8bca-4464db559159"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["feature_names = np.array(dict_vect.get_feature_names())\n", "\n", "mglearn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=25)\n", "mglearn.tools.visualize_coefficients(coef[1], feature_names, n_top_features=25)\n", "mglearn.tools.visualize_coefficients(coef[2], feature_names, n_top_features=25)"], "metadata": {"_uuid": "e20ad8f057410a20fc29234d5b982a70115224cd", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "0e839239-a688-4545-9461-0f18c9ae225c"}}, {"cell_type": "markdown", "source": ["mglearn.tools.visualize_coefficients method takes as argument coef - it is array with 3 dimensions, one for every class, next argument is feature names and n_top_features=25 means we need 25 from all features with best coeficient. Next 3 pictures show the these features.\n", "The coeficients are negative and you can see on the rigth most important features. Some of them are:\n", "\"get_consonant_letters\",\n", "\"lexical_diversity\",\n", "\"vocab_richness\"\n", "and other.\n", " First graphics is most important features for  EAP."], "metadata": {"_uuid": "ed299369609c7d59672bd595ee18c6822a8f8bcd", "_cell_guid": "41d9de32-d770-4c0a-ad35-e61c7b349777"}}, {"cell_type": "markdown", "source": ["![](https://i.imgur.com/gIp6PWo.png)"], "metadata": {"_uuid": "4ebdc30b4e5cb2441f5abdf2f86abcaadbe2cc88", "_cell_guid": "02024062-d42d-47a0-b0e9-b6a4df5bfcd0"}}, {"cell_type": "markdown", "source": ["Next is graphic for second author HPL. we see that the most important features here are almost the same."], "metadata": {"_uuid": "46f611f091d9d33cdff0da87e1e969a8c9366306", "_cell_guid": "b66afa59-09f1-40e3-ba98-fffea711f466"}}, {"cell_type": "markdown", "source": ["![](https://i.imgur.com/D9kPt9i.png)"], "metadata": {"_uuid": "10ab92d2d4be3d9adfb723b178341190df4821e9", "_cell_guid": "4d67bf32-7ba0-4879-be70-90d9059c8715"}}, {"cell_type": "markdown", "source": ["Last graphic is for last author MWS:\n", "The most important features are litle bit different here."], "metadata": {"_uuid": "95a32f6661b86cbfd345fa240ed38565144ab758", "_cell_guid": "d0b03bc6-f8af-4a5f-b070-3616a08cc6d2"}}, {"cell_type": "markdown", "source": ["![](https://i.imgur.com/ufNyje6.png)"], "metadata": {"_uuid": "d2bed94d0313072b01e08f401954bde5882810be", "_cell_guid": "33c5fc34-9d39-4d03-908e-a3b6ed74b344"}}, {"cell_type": "markdown", "source": [], "metadata": {"_uuid": "3d78a12d91df37cee9feda56eb1531052b3d640c", "_cell_guid": "c14082d2-1cb1-4985-b4ec-3cbcab0d96cb"}}, {"cell_type": "markdown", "source": ["I must explain that features are comming for DictVectorizer(), so it generates ( when transform) many, many features.\n", "So the features we can see in function are only basic features. The real features you can see in the pcituires above.\n", "Exactly these features takes the classifier. \n"], "metadata": {"_uuid": "837562f48d953d592d2f01365da4d01e74887ab2", "_cell_guid": "9935833b-b772-490e-9ac2-0bfbec05bd0f"}}, {"cell_type": "markdown", "source": ["Confusion matrix shows the percent we achieve or the score of predicted sentences for every one author:  the higher the diagonal values of the confusion matrix - the better indicating many correct predictions."], "metadata": {"_uuid": "416e1b07b4443e8e793fe7ea6ea8ab8ff811c4c6", "_cell_guid": "02607858-e956-4c59-ba1e-1b2e95b13355"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["predictions = clf.predict_proba(X_valid_ctv)\n", "predicted_lables = clf.predict(X_valid_ctv)\n", "\n", "cnf_matrix = confusion_matrix(Y_valid, predicted_lables)\n", "\n", "# Plot normalized confusion matrix\n", "plt.figure()\n", "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n", "                      title='Normalized confusion matrix')\n", "\n", "plt.show()"], "metadata": {"_uuid": "c67ba47a039b402976d359b5bd008babf2779dd8", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "e599b4c9-3011-4cac-9cc8-c8778b30c5b6"}}, {"cell_type": "markdown", "source": ["![](https://i.imgur.com/GKha9Zv.png)"], "metadata": {"_uuid": "dba6551a56d105c23340b60a0913c826b23c8662", "_cell_guid": "396995d2-c727-43d8-9c28-a32dfcfdfa99"}}, {"cell_type": "markdown", "source": ["Take a look to the diagonal form left upper corner. First is EAP with 75% predicted results score. HPL is 60% and MWS is 66%. Also we can see here that most of the predictions are biased to EAP.\n", "\n", "Next is Receiver Operating Characteristic curve (or ROC curve.)\n", "The ROC curve is created by plotting the true positive rate  against the false positive rate.\n", "The true-positive rate is also known as sensitivity. ROC is bumary classification so we have one curve per class.\n", "On the graphic you see them with different colors. The legend shows AUC ot Area under the Curve.\n", "If we can say with some words AUC as representing the probability that a classifier will rank a randomly chosen positive observation higher than a randomly chosen negative observation.\n", "Here you find good explanation of ROC and AUC:\n"], "metadata": {"_uuid": "b22a93999499011bf216f0e478c0394bb8eb07f8", "_cell_guid": "ddcc5c35-36d0-4846-bb1e-8918dff67182"}}, {"cell_type": "markdown", "source": ["[www.dataschool.io](http://www.dataschool.io/roc-curves-and-auc-explained/)"], "metadata": {"_uuid": "757d7641c87120d3d6f2c4af8d63bee3fdd8ffbd", "_cell_guid": "6c8e8947-45fe-43ba-a829-23c82087970b"}}, {"cell_type": "markdown", "source": ["![](https://i.imgur.com/Te4Iaye.png)"], "metadata": {"_uuid": "8807b1904f6edcb2784910fd64098a60c5a700f3", "_cell_guid": "180ad57d-7cf2-41a5-b4c8-f53a9be9f616"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["from sklearn.metrics import roc_curve, auc\n", "n_classes = len(class_names)\n", "from sklearn.preprocessing import label_binarize\n", "\n", "# Binarize the output\n", "Y_valid = label_binarize(Y_valid, classes=[0, 1, 2])\n", "\n", "# Compute ROC curve and ROC area for each class\n", "fpr     = dict()\n", "tpr     = dict()\n", "roc_auc = dict()\n", "\n", "plt.figure(1)\n", "plt.plot([0, 1], [0, 1], 'k--')\n", "\n", "\n", "for i in range(n_classes):\n", "    fpr[i], tpr[i], _ = roc_curve(  Y_valid[:,i] , predictions[:, i] )\n", "    roc_auc[i] = auc(fpr[i], tpr[i])\n", "    plt.plot(fpr[i], tpr[i], label=class_names[i] + 'ROC curve (area = %0.2f)' % roc_auc[i])\n", "\n", "print('EAP ROC curve (area = %0.2f)' % roc_auc[0])\n", "print('HPL ROC curve (area = %0.2f)' % roc_auc[1])\n", "print('MWS ROC curve (area = %0.2f)' % roc_auc[2])\n", "\n", "plt.xlabel('False positive rate')\n", "plt.ylabel('True positive rate')\n", "plt.title('ROC curve')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()\n"], "metadata": {"_uuid": "946eddca0857ce3ce346f87d2880136e3507b971", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "30ec2ac1-e8cf-4de3-955c-6a6541ce307a"}}], "metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}}