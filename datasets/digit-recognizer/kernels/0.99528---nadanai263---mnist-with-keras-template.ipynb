{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.initializers import glorot_uniform as Xavier\nfrom keras import backend as K\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix\nimport os\nimport itertools\nprint('Tensorflow version', tf.__version__)\nprint('Keras version', keras.__version__)\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad2faef008927d39f153da63c9bc1b54e09f1779"},"cell_type":"markdown","source":"My first kernel on Kaggle. Here I put together some useful Keras functions for training and testing models, using the MNIST dataset."},{"metadata":{"trusted":true,"_uuid":"a96bfe16f25631891b2bbe9b5809bbb3547d9420"},"cell_type":"code","source":"DIR = '../input/digit-recognizer/'\nDIR_IN = '../input/mnist-with-keras-template/'\nDIR_OUT = './'\nBATCH_SIZE = 32\nNUM_CLASSES = 10\nEPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64a17c86b4a936f4c421e2cc606a85ef5f670f63"},"cell_type":"code","source":"# Define useful functions\n\n# Real-time plot updates for Keras (credits: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e)\nclass PlotLearn(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        self.acc = []\n        self.val_acc = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n        \n        self.i += 1\n\n        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4), sharex=True)\n                \n        clear_output(wait=True)\n\n        ax1.plot(self.x, self.losses, label=\"loss\")\n        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n        ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss');\n        ax1.legend()\n        \n        ax2.plot(self.x, self.acc, label=\"accuracy\")\n        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n        ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy');\n        ax2.legend()\n\n        plt.show(); \n        \nplot_learn = PlotLearn()\n\n# Save Keras model\ndef savemodel(model, model_params_dir,name):\n    json_string = model.to_json()\n    # Save model architecture in JSON file\n    open(model_params_dir + name + '_arch.json', 'w').write(json_string)\n    # Save weights as HDF5\n    model.save_weights(model_params_dir + name + '_weights.h5')\n    print(\"Saved model to disk\")\n\n# Load Keras model\ndef loadmodel(model_params_dir,name): \n    # Load model architecture from JSON file\n    model = keras.models.model_from_json(open(model_params_dir + name + '_arch.json').read())\n    # Load model weights from HDF5 file\n    model.load_weights(model_params_dir + name + '_weights.h5')\n    print(\"Loaded model from disk\")\n    return model\n\n# Load weights only\ndef loadmodelweights(model,model_params_dir,name_weightfile): \n    # Load model weights from HDF5 file\n    model.load_weights(model_params_dir + name_weightfile+'.h5')\n    print(\"Loaded model weights from disk\")\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3cee3867702e5ad47df8da06f2297c8e47ae8c"},"cell_type":"markdown","source":"# 1. Data input"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load all data into memory\ndf_train=pd.read_csv(DIR + 'train.csv')\ndf_test=pd.read_csv(DIR + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fe30751f5515e8503cc027a218d2a252622440a"},"cell_type":"code","source":"# Convert images\nimages_train=np.asarray(df_train.drop(['label'],axis=1,inplace=False))\nlabels_train=np.asarray(df_train['label'])\nimages_test=np.asarray(df_test)\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# Pre-process input data\nx_train, y_train, x_test = images_train,labels_train, images_test\n\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32') # Reshape and convert fmt\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32')\nx_train /= 255\nx_test /= 255\n\ninput_shape = (img_rows, img_cols, 1)\n\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('input shape:', input_shape)\nprint(x_train.shape[0], 'Number of training samples')\nprint(x_test.shape[0], 'Number of test samples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77f0bcd60173ca843d83519655737db8aa395690"},"cell_type":"code","source":"# convert class vectors from digit labels to binary class matrices (N x 10)\ny_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a461365cf0f0205cf9c8e6c64345a21e3d7eb58"},"cell_type":"code","source":"# Shuffle data and generate dev set\n\npercent_holdout=0.02 # percentage of data to hold out\n\n# Set seed for reproducible results\nnp.random.seed(1)\n\n# Select random subset by index\ninds=np.arange(y_train.shape[0])\nnp.random.shuffle(inds) # Shuffle indices in-place\n\n# Carve into train/dev/test sets\ndev_inds=inds[int((1-percent_holdout)*len(inds)):] # Dev set\ntrain_inds=inds[:int((1-percent_holdout)*len(inds))] # Training set \n\n# Look at final distributions\nplt.hist(train_inds); \nplt.hist(dev_inds); plt.show()\n\nprint('Number of training examples', len(train_inds))\nprint('Number of dev examples', len(dev_inds))\n\nx_train_subset=x_train[train_inds]\ny_train_subset=y_train[train_inds]\nx_dev=x_train[dev_inds]\ny_dev=y_train[dev_inds]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eedc5f6900f63c8b018606ef9999a54f7e225942"},"cell_type":"markdown","source":"# 2. Build model"},{"metadata":{"trusted":true,"_uuid":"9c7423c1519f32cecc9deb94c350166bb9892260"},"cell_type":"code","source":"# Define model graph\n\nmodel_name = 'CNN1'\n\nx_input = Input(shape=(28,28,1), name='Input1')\nx = x_input\n\n# Convs. BNs after Relus!\n\nx = Conv2D(32,(3, 3), padding='same', name='C1_1')(x)\nx = Activation('relu', name='Relu1_1')(x)\nx = BatchNormalization(axis=-1, name='BN1_1')(x)\n\nx = Conv2D(32,(3, 3), padding='same', name='C1_2')(x)\nx = Activation('relu', name='Relu1_2')(x)\nx = BatchNormalization(axis=-1, name='BN1_2')(x)\n\nx = Conv2D(32,(5, 5), padding='same', name='C1_3')(x)\nx = Activation('relu', name='Relu1_3')(x)\nx = BatchNormalization(axis=-1, name='BN1_3')(x)\n\nx = MaxPooling2D(pool_size=(2, 2), name='MP1')(x)\nx = Dropout(0.4, name='Drop1')(x)\n\n\nx = Conv2D(64,(3, 3), padding='same', name='C2_1')(x)\nx = Activation('relu', name='Relu2_1')(x)\nx = BatchNormalization(axis=-1, name='BN2_1')(x)\n\nx = Conv2D(64,(3, 3), padding='same', name='C2_2')(x)\nx = Activation('relu', name='Relu2_2')(x)\nx = BatchNormalization(axis=-1, name='BN2_2')(x)\n\nx = Conv2D(64,(5, 5), padding='same', name='C2_3')(x)\nx = Activation('relu', name='Relu2_3')(x)\nx = BatchNormalization(axis=-1, name='BN2_3')(x)\n\nx = MaxPooling2D(pool_size=(2, 2), name='MP2')(x)\nx = Dropout(0.4, name='Drop2')(x)\n\n\nx = Conv2D(128,(3, 3), padding='same', name='C3')(x)\nx = Activation('relu', name='Relu3')(x)\nx = BatchNormalization(axis=-1, name='BN3')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='MP3')(x)\nxinter = Flatten(name='Features')(x)\n\nx = Dropout(0.5, name='Drop4')(xinter)\nx = Dense(NUM_CLASSES, name='D4')(x)\nx = Activation('softmax', name='Output')(x)\n\nmodel = Model(inputs=x_input, outputs=x)\nmodel_features_only = Model(inputs=x_input, outputs=xinter)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b535811df06965510bd2812d66f9282b99a8705f"},"cell_type":"code","source":"# Compile\nadam=Adam(lr=0.001)\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=adam,\n              metrics=['accuracy'])\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=2, \n                                            factor=0.5, \n                                            min_lr=0.000001)\n\n# Create checkpoint: save model after every epoch\nmodelcheckpoint = keras.callbacks.ModelCheckpoint(DIR_OUT + model_name +'_bestweights.h5',\n                                              monitor = 'val_loss',\n                                              verbose = True,\n                                              save_best_only = True,\n                                              mode = 'auto')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad114d1d92a7be73eb43abe8107826e46468ad4e"},"cell_type":"markdown","source":"# 3. Train"},{"metadata":{"trusted":true,"_uuid":"1df2b29db7321ad632342f5bf0e54187165e1f08"},"cell_type":"code","source":"# Run!\nhistory=model.fit(x_train_subset, y_train_subset,\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            verbose=1,\n            validation_data=(x_dev, y_dev),\n            callbacks=[modelcheckpoint,learning_rate_reduction])\n\n# Print evaluation\nscore = model.evaluate(x_train_subset,y_train_subset,verbose=0)\nprint('Train loss:', score[0])\nprint('Train Accuracy:', score[1])\n\nscore = model.evaluate(x_dev,y_dev,verbose=0)\nprint('Dev loss:', score[0])\nprint('Dev Accuracy:', score[1])\n\n# save the model \nsavemodel(model, DIR_OUT, model_name)\nprint('Completed ', len(history.epoch), 'epochs.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd24086c338e3a325dc356cba732f6b131fdda3"},"cell_type":"code","source":"# Plot train history\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('accuracy')\nax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")\nax[0].legend()\nax[1].legend()\nplt.savefig(DIR_OUT+model_name+'_trainprofile.pdf',bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b06af77fca91e2053f22e394b5e258ac0a2e7403"},"cell_type":"markdown","source":"# 4. Evaluate"},{"metadata":{"trusted":true,"_uuid":"d1147fcbf3fffca409e7a76c0ca5eb3b48fc1a21"},"cell_type":"code","source":"# Load a model if you want:\n#model=loadmodel(DIR_OUT,'CNN1')\n# or take best weights:\n#model=loadmodelweights(model,DIR_IN,model_name+'_bestweights') # Load weights only\n#model.compile(loss=keras.losses.categorical_crossentropy,optimizer=adam,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c90bce85c213d967fb4a14653887f9fb87de7735"},"cell_type":"code","source":"# Look at confusion matrix \n\nmodel=loadmodelweights(model,DIR_IN,model_name+'_bestweights') # Load best weights only\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(x_dev)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_dev,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54f3478ed7ae25210a105aedd9b60f4a990f8a71"},"cell_type":"markdown","source":"# 5. Submit"},{"metadata":{"trusted":true,"_uuid":"d6516a5e35283d8575f7bf981cf47a59eeb6ab82"},"cell_type":"code","source":"# Create submission file\nY_pred = model.predict(x_test)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nids = np.arange(Y_pred_classes.shape[0])+1\n\ndata = {'ImageId':ids, 'Label':Y_pred_classes}\npred=pd.DataFrame(data)\npred.to_csv(DIR_OUT+'preds1.csv',sep=',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"515f16ba6cd57b314aff112c9431e0c5c823976d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}