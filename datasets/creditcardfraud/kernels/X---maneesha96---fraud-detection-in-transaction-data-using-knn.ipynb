{"cells":[{"metadata":{"_uuid":"4e53daea64ae7fe95c31f576300557f904d31085"},"cell_type":"markdown","source":"In this analysis I will be using K Nearest Neighbors to build the model and as the dataset: Credit Card Fraud Detection dataset provided by Kaggle. For this analysis I focused more on data preprocessing because it helps to gain a better accuracy rather than just sending raw data and telling to build a model. \n\n# Data Preprocessing\nWhat is data preprocessing?  It is a data mining technique that transforms raw data into understandable format. Raw data(real world data) is always incomplete and that data cannot be send through a model. Yhay would cause certain errors. That is why we need to preprocess data before sending through a model.\n\nI have used severel preprocessing techniques in this analysis.\nWithout further a do let's get started!!!"},{"metadata":{"_uuid":"4118f3252b7a55b0c5538cfe387006606696cee8"},"cell_type":"markdown","source":"# Steps in Data Preprocessing\nHere iare the steps I have followed;\n1. Import libraries\n2. Read data\n3. Checking for missing values\n4. Checking for categorical data\n5. Standardize the data\n6. PCA transformation\n7. Data splitting"},{"metadata":{"_uuid":"f268518f535ac9d70988aabb31ea49fb20d89c08"},"cell_type":"markdown","source":"# 1. Import libraries\nAs the main libraries, I am using Pandas, Numpy and time; \n* Pandas : Use for data manipulation and data analysis. \n* Numpy : fundamental package for scientific computing with Python. \n\nAs for the visualization I am using Matplotlib and Seaborn.\n\nFor the data preprocessing techniques and algorithms I used Scikit-learn libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# main libraries\nimport pandas as pd\nimport numpy as np\nimport time\n\n# visual libraries\nfrom matplotlib import pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D \nplt.style.use('ggplot')\n\n# sklearn libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2da9321245350925e7a73ad1803c29269e4c8c0"},"cell_type":"markdown","source":"# 2. Read data\nYou can find more details on dataset here: "},{"metadata":{"trusted":true,"_uuid":"2e70de34465fa3f2b17ad536099bdf52b131115f"},"cell_type":"code","source":"# Read the data in the CSV file using pandas\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c01134faf6085f5e59b1cf5bf69e14e31bef48f"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8175d370b89cdbdddcd268f635bf165a33b87f77"},"cell_type":"markdown","source":"# 3. Checking for missing values\nIn this data set, there are no missing values. So we don't need to handle missing values in the dataset."},{"metadata":{"trusted":true,"_uuid":"2347ebe9ca4141a9fb1366af6608cf6627f40182"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"587d53f059d6fd4ad6dd1cf1754b9348b7f4a60f"},"cell_type":"markdown","source":"## Let's look at the data. \nSo the dataset is labeled as 0s and 1s. 0 = non fraud and 1 = fraud. \n"},{"metadata":{"trusted":true,"_uuid":"1ed4301fde7d58f6320adfba10c4f09e28e942cf"},"cell_type":"code","source":"All = df.shape[0]\nfraud = df[df['Class'] == 1]\nnonFraud = df[df['Class'] == 0]\n\nx = len(fraud)/All\ny = len(nonFraud)/All\n\nprint('frauds :',x*100,'%')\nprint('non frauds :',y*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Let's plot the Transaction class against the Frequency\nlabels = ['non frauds','fraud']\nclasses = pd.value_counts(df['Class'], sort = True)\nclasses.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb5dc3bc8db240c1b7db8131ad3100aebf7fb7bc"},"cell_type":"markdown","source":"# 4. Checking for categorical data\nthe only categorical cariable we have in this data set is the target variable.  Other features are already in numerical format, so no need of converting to categorical data.\n\n## Let's plot the distribution of features \nI used seaborn distplot() to visualize the distribution of features in the dataset.\nWe have 30 features and target variable  in the dataset. "},{"metadata":{"trusted":true,"_uuid":"8b35aa9b4b7c88efe13424fa4f166d2395a4e507"},"cell_type":"code","source":"# distribution of Amount\namount = [df['Amount'].values]\nsns.distplot(amount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea7239021b5cab547fdbe9073576eef203d6a972"},"cell_type":"code","source":"# distribution of Time\ntime = df['Time'].values\nsns.distplot(time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"949e5e60df7835f4b2cbf9d9ef004c1627d23f08"},"cell_type":"code","source":"# distribution of anomalous features\nanomalous_features = df.iloc[:,1:29].columns\n\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[anomalous_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c65f8b64ec734ebe9cc6f885bcc3c8a0717dd373"},"cell_type":"markdown","source":"In this analysis I will not be dropping any features looking at the distribution of features, because I am still in the learning process of working with data preprocessing in numarous ways.So I would like to experiment  step by step on data.\n\nInstead all the features will be tranformed to scaled variables."},{"metadata":{"trusted":true,"_uuid":"97285e878e8d5b0120b11b04b124c232758d3af8"},"cell_type":"code","source":"# heat map of correlation of features\ncorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fa0c3fcb5e1d0dc3547659d129caaa0316b15b7"},"cell_type":"markdown","source":"# 5. Standardize the data\nThe dataset is  contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. So PCA is effected by scale so we need to scale the features in the data before applying PCA. For the scaling I am using Scikit-learn's StandardScaler(). In order to fit to the scaler the data should be reshaped within -1 nad 1. "},{"metadata":{"trusted":true,"_uuid":"bb65c84dd0164ac176d2ade4d2cdb1e1caa97fd2"},"cell_type":"code","source":"# Standardizing the features\ndf['Vamount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Vtime'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))\n\ndf = df.drop(['Time','Amount'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfac3c2829f35f5fb258cc0198108a6a71ffda7c"},"cell_type":"markdown","source":"Now all the features are standardize into unit sclae (mean = 0 and variance = 1)"},{"metadata":{"_uuid":"92a3af1179286f5e5cbd8fddbc0041142f209a82"},"cell_type":"markdown","source":"# 6. PCA transformation\nPCA (Principal Component Analysis) mainly using to reduce the size of the feature space while retaining as much of the information as possible.  In here all the features transformed into 2 features using PCA."},{"metadata":{"trusted":true,"_uuid":"c3b9797903f64576d37b6548346963bd074467d7"},"cell_type":"code","source":"X = df.drop(['Class'], axis = 1)\ny = df['Class']\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X.values)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3debd364193ff4654350a8fe2bbbf5a4fc87801"},"cell_type":"code","source":"finalDf = pd.concat([principalDf, y], axis = 1)\nfinalDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c5e30bd1b94c481a42b19f75b0282bfb3b6dc4f"},"cell_type":"code","source":"# 2D visualization\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0, 1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['Class'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99688bbddeb776e935aecff5adc3fcb3739e2c4c"},"cell_type":"markdown","source":"Since the data is highly imbalanced, I am only taking 492 rows from the non_fraud transactions."},{"metadata":{"trusted":true,"_uuid":"7a2f4853bbf85e67be035cc62870ba137fe56123"},"cell_type":"code","source":"# Lets shuffle the data before creating the subsamples\ndf = df.sample(frac=1)\n\nfrauds = df[df['Class'] == 1]\nnon_frauds = df[df['Class'] == 0][:492]\n\nnew_df = pd.concat([non_frauds, frauds])\n# Shuffle dataframe rows\nnew_df = new_df.sample(frac=1, random_state=42)\n\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158a9693da5d8a1d76dc8a6e9cba2cec978f68ee"},"cell_type":"code","source":"# Let's plot the Transaction class against the Frequency\nlabels = ['non frauds','fraud']\nclasses = pd.value_counts(new_df['Class'], sort = True)\nclasses.plot(kind = 'bar', rot=0)\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d3b20c80a842b244cdd8f3be03bc1724710db9d"},"cell_type":"code","source":"# prepare the data\nfeatures = new_df.drop(['Class'], axis = 1)\nlabels = pd.DataFrame(new_df['Class'])\n\nfeature_array = features.values\nlabel_array = labels.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"336f76cf1d136a4079ca84c94979cebc496d8616"},"cell_type":"markdown","source":"# 7. Data splitting"},{"metadata":{"trusted":true,"_uuid":"3bcf871b5ccfa8b4a21de5c0f53645a71f66f932"},"cell_type":"code","source":"# splitting the faeture array and label array keeping 80% for the trainnig sets\nX_train,X_test,y_train,y_test = train_test_split(feature_array,label_array,test_size=0.20)\n\n# normalize: Scale input vectors individually to unit norm (vector length).\nX_train = normalize(X_train)\nX_test=normalize(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b99020fb4fb2fc87860f53a7940f606bd668090"},"cell_type":"markdown","source":"For the model building I am using K Nearest Neighbors. So we need find an optimal K to get the best out of it. "},{"metadata":{"trusted":true,"_uuid":"6868e4ee23a60eb5f03769a3c41482a49c51a292"},"cell_type":"code","source":"neighbours = np.arange(1,25)\ntrain_accuracy =np.empty(len(neighbours))\ntest_accuracy = np.empty(len(neighbours))\n\nfor i,k in enumerate(neighbours):\n    #Setup a knn classifier with k neighbors\n    knn=KNeighborsClassifier(n_neighbors=k,algorithm=\"kd_tree\",n_jobs=-1)\n    \n    #Fit the model\n    knn.fit(X_train,y_train.ravel())\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train.ravel())\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test.ravel()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21b8b6b3b2d6a9dbaac374b9f103881566f03b4f"},"cell_type":"code","source":"#Generate plot\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbours, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbours, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0522a3b1200b42c45de2c93add385736e36d382"},"cell_type":"code","source":"idx = np.where(test_accuracy == max(test_accuracy))\nx = neighbours[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaebcd1b8a6c22fdcdc5163db45b050db5ef13c5"},"cell_type":"code","source":"#k_nearest_neighbours_classification\nknn=KNeighborsClassifier(n_neighbors=x[0],algorithm=\"kd_tree\",n_jobs=-1)\nknn.fit(X_train,y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edff3eb07c6c7b32e74a88150d714c6edac469ef"},"cell_type":"code","source":"# save the model to disk\nfilename = 'finalized_model.sav'\njoblib.dump(knn, filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e7e041cf252dc24ea2e6333fc3dc4e40c43bade"},"cell_type":"code","source":"# load the model from disk\nknn = joblib.load(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9baf532b35c1f6ec3fa87368ec5479852e296d64"},"cell_type":"code","source":"# predicting labels for testing set\nknn_predicted_test_labels=knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b15a28a1dbf6734c24fd68a0feae5de71ccd2ce"},"cell_type":"code","source":"from pylab import rcParams\n#plt.figure(figsize=(12, 12))\nrcParams['figure.figsize'] = 14, 8\nplt.subplot(222)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=knn_predicted_test_labels)\nplt.title(\" Number of Blobs\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e5b0a80f48c92cb3f354f6e8e00698d5353c6ae"},"cell_type":"code","source":"#scoring knn\nknn_accuracy_score  = accuracy_score(y_test,knn_predicted_test_labels)\nknn_precison_score  = precision_score(y_test,knn_predicted_test_labels)\nknn_recall_score    = recall_score(y_test,knn_predicted_test_labels)\nknn_f1_score        = f1_score(y_test,knn_predicted_test_labels)\nknn_MCC             = matthews_corrcoef(y_test,knn_predicted_test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"812e5dcb69d9a9a4f6a5aee28daa41c461e313f1"},"cell_type":"code","source":"#printing\nprint(\"\")\nprint(\"K-Nearest Neighbours\")\nprint(\"Scores\")\nprint(\"Accuracy -->\",knn_accuracy_score)\nprint(\"Precison -->\",knn_precison_score)\nprint(\"Recall -->\",knn_recall_score)\nprint(\"F1 -->\",knn_f1_score)\nprint(\"MCC -->\",knn_MCC)\nprint(classification_report(y_test,knn_predicted_test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9743363ee130457bf810ce2fa898f2431d952440"},"cell_type":"code","source":"import seaborn as sns\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(y_test, knn_predicted_test_labels)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31c8bfa0e4d77a1e25d00a9f30edb5e4a310adca"},"cell_type":"markdown","source":"## Conclusion\nI tried without standardizing the data to get a better accuracy. But after I learnt this method and applied it, it gave a promising result. I am still doing experiments and still learning about data preprocessing techniques. I only used KNN algorithm for this dataset.  \nFeel free to comment and give an Upvote if you find this kernel helpful.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}