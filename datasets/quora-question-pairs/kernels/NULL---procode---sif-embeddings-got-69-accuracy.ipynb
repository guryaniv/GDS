{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "a015673c7c6bd389f1f0ce298c8d5937bc3712ba", "_cell_guid": "0d43fb36-23f2-404a-9440-b207bffbc782"}, "cell_type": "markdown", "source": ["# Quora Question Pairs 69% Accuracy\n", "The Kernel is based on the reasearch paper \"A simple but tough to beat baseline for sentence embeddings\" by princeton university so we would like to thank for their great research on sentence embeddings.\n", "link : https://openreview.net/forum?id=SyK00v5xx"]}, {"metadata": {"_uuid": "aedf470ffbd958a152a546350e4c691b86cb4298", "_kg_hide-input": false, "collapsed": true, "_kg_hide-output": true, "_cell_guid": "d7915bb7-36ea-4922-9322-2e5a1edbcc05"}, "outputs": [], "cell_type": "code", "source": ["from __future__ import division\n", "import gensim\n", "import numpy as np\n", "from sklearn.decomposition import PCA\n", "import gensim.models.word2vec\n", "from collections import Counter\n", "from sklearn.metrics.pairwise import cosine_similarity\n", "import itertools\n", "from nltk.tokenize import word_tokenize\n", "import re\n", "from nltk.corpus import stopwords\n", "from nltk.stem import SnowballStemmer\n", "from string import punctuation\n", "import csv\n", "print(\"All Libraries Successfully imported\")"], "execution_count": null}, {"metadata": {"_uuid": "2a0c86b01bdd6db54b5505f89a45986e6d643aed", "collapsed": true, "_cell_guid": "7c642534-04d1-468f-98e1-06fb573ceff2"}, "cell_type": "markdown", "source": ["## The function \"text_to_wordlist\" is from\n", "## https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text"]}, {"metadata": {"_uuid": "381cc81031800b8957fe39bfe7999aec3853bd3d", "collapsed": true, "_cell_guid": "2f8ad94b-5cdd-41dc-a1dd-e7e86f190c92"}, "outputs": [], "cell_type": "code", "source": ["def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n", "    # Clean the text, with the option to remove stopwords and to stem words.\n", "    \n", "    # Convert words to lower case and split them\n", "    text = text.lower().split()\n", "\n", "    # Optionally, remove stop words\n", "    if remove_stopwords:\n", "        stops = set(stopwords.words(\"english\"))\n", "        text = [w for w in text if not w in stops]\n", "    \n", "    text = \" \".join(text)\n", "\n", "    # Clean the text\n", "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n", "    text = re.sub(r\"what's\", \"what is \", text)\n", "    text = re.sub(r\"\\'s\", \" \", text)\n", "    text = re.sub(r\"\\'ve\", \" have \", text)\n", "    text = re.sub(r\"can't\", \"cannot \", text)\n", "    text = re.sub(r\"n't\", \" not \", text)\n", "    text = re.sub(r\"i'm\", \"i am \", text)\n", "    text = re.sub(r\"\\'re\", \" are \", text)\n", "    text = re.sub(r\"\\'d\", \" would \", text)\n", "    text = re.sub(r\"\\'ll\", \" will \", text)\n", "    text = re.sub(r\",\", \" \", text)\n", "    text = re.sub(r\"\\.\", \" \", text)\n", "    text = re.sub(r\"!\", \" ! \", text)\n", "    text = re.sub(r\"\\/\", \" \", text)\n", "    text = re.sub(r\"\\^\", \" ^ \", text)\n", "    text = re.sub(r\"\\+\", \" + \", text)\n", "    text = re.sub(r\"\\-\", \" - \", text)\n", "    text = re.sub(r\"\\=\", \" = \", text)\n", "    text = re.sub(r\"'\", \" \", text)\n", "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n", "    text = re.sub(r\":\", \" : \", text)\n", "    text = re.sub(r\" e g \", \" eg \", text)\n", "    text = re.sub(r\" b g \", \" bg \", text)\n", "    text = re.sub(r\" u s \", \" american \", text)\n", "    text = re.sub(r\"\\0s\", \"0\", text)\n", "    text = re.sub(r\" 9 11 \", \"911\", text)\n", "    text = re.sub(r\"e - mail\", \"email\", text)\n", "    text = re.sub(r\"j k\", \"jk\", text)\n", "    text = re.sub(r\"\\s{2,}\", \" \", text)\n", "    \n", "    # Optionally, shorten words to their stems\n", "    if stem_words:\n", "        text = text.split()\n", "        stemmer = SnowballStemmer('english')\n", "        stemmed_words = [stemmer.stem(word) for word in text]\n", "        text = \" \".join(stemmed_words)\n", "    \n", "    # Return a list of words\n", "    return(text)"], "execution_count": null}, {"metadata": {"_uuid": "0bfd4b9618ce8b45ea6cfda030cc857bab2989a3", "_cell_guid": "b7efba69-9b70-4440-9c89-fd69e405a4ed"}, "cell_type": "markdown", "source": ["## Loading word vectors of Google to get Sentence Embeddings "]}, {"metadata": {"_uuid": "e7c92699d32a4b13c1692f3150702f217c2fca36", "collapsed": true, "_cell_guid": "647b9c29-15df-4853-ab5f-882b1e2e1944"}, "outputs": [], "cell_type": "code", "source": ["#Initializing word2vec\n", "\n", "def gensim_load_vec(path=\"GoogleNews-vectors-negative300.bin\"):\n", "    #use gensim_emb.wv.index2word if used this way to load vectors\n", "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n", "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True,limit=500000)\n", "    vocab = gensim_emb.index2word\n", "    vec = gensim_emb.syn0\n", "    shape = gensim_emb.syn0.shape\n", "    return gensim_emb, vec, shape, vocab"], "execution_count": null}, {"metadata": {"_uuid": "84eb4c9854a63e66705d2189cd0cc543e6909305", "collapsed": true, "_cell_guid": "48571027-3e89-4354-bb38-f1ed7597a2ee"}, "outputs": [], "cell_type": "code", "source": ["def map_word_frequency(document):\n", "    return Counter(itertools.chain(*document))"], "execution_count": null}, {"metadata": {"_uuid": "fa8125fd4b48d1c5ec93e26e196ae8e52d7cf801", "_cell_guid": "c28e2c28-e7dc-41c3-a530-76fd7ff037fb"}, "cell_type": "markdown", "source": ["# Sentence Embedding Calculation as Described in the Research Paper"]}, {"metadata": {"_uuid": "a452b99ac8509d7699b0b2ab28cfcc33483266cf", "collapsed": true, "_cell_guid": "82be2e61-798c-452e-b002-efb80210e8a4"}, "outputs": [], "cell_type": "code", "source": ["\n", "def sentence2vec(tokenised_sentence_list, embedding_size, word_emb_model, a = 1e-3):\n", "    sentence_vecs = []\n", "    try:\n", "        word_counts = map_word_frequency(tokenised_sentence_list)\n", "        sentence_set=[]\n", "        for sentence in tokenised_sentence_list:\n", "            vs = np.zeros(embedding_size)\n", "            sentence_length = len(sentence)\n", "            for word in sentence:\n", "                a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n", "            try:\n", "                vs = np.add(vs, np.multiply(a_value, word_emb_model[word])) # vs += sif * word_vector\n", "            except Exception as e:\n", "                pass\n", "            vs = np.divide(vs, sentence_length) # weighted average\n", "            sentence_set.append(vs)\n", "    except Exception as e:\n", "           print(\"Exception in sentence embedding\")\n", "    return sentence_set"], "execution_count": null}, {"metadata": {"_uuid": "fc20d7232cab825d08f335d8fa51e60f4ed4b047", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "dd321fdd-e87e-422b-bd20-6500e271c700"}, "outputs": [], "cell_type": "code", "source": ["#Testing with one pair\n", "\n", "text1 = \"How can I be good geologist\"\n", "text2 = \"should I do be great geologist\"\n", "token1 = text_to_wordlist(text1)\n", "token2 = text_to_wordlist(text2)\n", "token1 = word_tokenize(token1)\n", "token2 = word_tokenize(token2)\n", "sent_list=[]\n", "sent_list.append(token1)\n", "sent_list.append(token2)\n", "gensim_emb, vec, shape, vocab = gensim_load_vec()"], "execution_count": null}, {"metadata": {"_uuid": "781354d2f73284637af809f52201654f57b75f8d", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "072c2556-6d92-4957-af8b-925942a93634"}, "outputs": [], "cell_type": "code", "source": ["sent_emb = sentence2vec(sent_list,300,gensim_emb)"], "execution_count": null}, {"metadata": {"_uuid": "539d5b416769b1de81eea124804406d2f7c2aa80", "collapsed": true, "_cell_guid": "cb6c00a1-9470-4608-83cd-3b85d230e670"}, "outputs": [], "cell_type": "code", "source": ["score=float(cosine_similarity([sent_emb[0]],[sent_emb[1]]))\n", "print(score)"], "execution_count": null}], "nbformat": 4}