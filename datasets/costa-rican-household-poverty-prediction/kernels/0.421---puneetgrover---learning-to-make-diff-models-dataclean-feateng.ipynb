{"cells":[{"metadata":{"_uuid":"c8bb445936cbb4cbeb1b1d5a340ef5923454610e"},"cell_type":"markdown","source":"# Introduction\n\nThis is a step by step kernel for: `Data Cleaning`, `Feature Engineering`, and `Model Making and Prediction`. I will explain as I go what I am doing. \n\n## Index\n\n+ [Imports](#imports)\n+ [Some Useful Functions](#someUsefulFunctions)\n+ [Data Preprocessing](#dataPreprocessing)\n    - [Taking care of Non-Numerical Features](#numFeatures)\n    - [Taking care of Null Values](#nullValues)\n+ [Checking corrupted data](#checkingCorruptedData)\n+ [Data Exploration and Visualization](#explore) **\\***\n+ [Feature Engineering](#featureEng)\n    - [Combine Some Features](#combiningFeatures)\n    - [Remove Highly Correlated Features](#removeHighlyCorrelatedFeatures)\n    - [An Error I got during training (Infinite values)](#anErrorIGot)\n    - [Making new Features](#makingNewFeatures)\n        * [More features using PCA](#pcaFeat)\n+ [Handling - few data points for some categories](#handlingSmallData)\n+ [Random Forest](#randomForest)\n    - [Check Feature's Importance](#checkFeatureImportance)\n    - [Removing Redundant Features](#removingRedundantFeatures)\n+ [Gradient Boosting](#gradBoosting)\n+ [Deep Neural Network](#dnn)\n+ [Comparing Models](#compModels) **\\*\\***\n+ [Making Submission File](#makingSubmission)"},{"metadata":{"_uuid":"b8a9f69f72c223e240f7e35a807b99ea10576fbb"},"cell_type":"markdown","source":"# Imports  <a id=\"imports\"></a>\n---"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.autograd import Variable\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c07ced671aad9249db684fa4f7fe2e22fb8792a"},"cell_type":"markdown","source":"# Some Useful functions <a id=\"someUsefulFunctions\"></a>\n---"},{"metadata":{"_uuid":"c0e46e69eb63aab70da302155f0de5d422da3c70"},"cell_type":"markdown","source":"F1 score (macro) is what we need as a `metrics` to check how good or bad our model is. F1 score is a better quantifier of viability of model than `accuracy`.  "},{"metadata":{"trusted":true,"_uuid":"163be4d838716d94184645992e5652f2eb306f6d","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef F1score(actuals, preds):\n    \"\"\"\n    To get F1 score (macro) for our predictions.\n    -----------------------------------------------------------\n    Parameters:\n        preds: Array of Predicted values\n        actuals: Array of Actual labels\n    Output:\n        Return F1 score (macro)\n    \"\"\"\n    return f1_score(actuals, preds, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b439139d736a7484f07a31c0d2d2be8f39a84ecd"},"cell_type":"markdown","source":"# Data Preprocessing <a id=\"dataPreprocessing\"></a>\n---"},{"metadata":{"trusted":true,"_uuid":"6092013940929afbfb1fee7a02971996263e4559","_kg_hide-input":true},"cell_type":"code","source":"PATH = \"../input/\"\n\ntrain = pd.read_csv(f'{PATH}train.csv')\ntest = pd.read_csv(f'{PATH}test.csv')\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b170ee8bb2def35fba452233a0198f6271968c23"},"cell_type":"markdown","source":"### Taking care of non-numerical features:  <a id=\"numFeatures\"></a>"},{"metadata":{"_uuid":"05c2dcf400a5a72f543d2372fa364d020923b031"},"cell_type":"markdown","source":"Here, we will check for features (columns) that are non numerical. We need to take care of them because we cannot send `objects` into a Neural Network.\nSo, we will convert non-numerical data to numerical data and then it will work for any model we use."},{"metadata":{"trusted":true,"_uuid":"009e6eb890ebbbd95dfac9b226278f9b5041cb09"},"cell_type":"code","source":"obj_cols = train.columns[train.dtypes == \"object\"]; obj_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82ae12d7c9bd5cbf584b4fc50826b8e61beea8ab"},"cell_type":"markdown","source":"Description of these variables:\n1.  **Id : ** a unique identifier for each row.\n1.  **idhogar : ** this is a unique identifier for each household.\n1.  **dependency : ** Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n1.  **edjefe : ** years of education of male head of household\n1. **edjefa : ** years of education of female head of household"},{"metadata":{"trusted":true,"_uuid":"f9e94468575de441fbd3063505356a2020023ddb"},"cell_type":"code","source":"train[obj_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1323c4efb82ab16dd9f7c9904589e7ce0b62a107"},"cell_type":"markdown","source":"First,  we will take care of `Id` and `idhogar`:\n\nWe will use sklearn's `LabelEncoder` to encode these values. We can delete them as they are unique person or household identifier and they are different for every single unit of them, so they can't give any useful information for our output `Target`. For example: every household with`Target` value of `1` will have different `idhogar` and every individual with `Target` value of `1` will have different `Id`.\nBut we will keep them for now, as we will use them in further Data Preprocessing."},{"metadata":{"trusted":true,"_uuid":"112a548e786d11c50fbe863349c5f8ada9cba6b7","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# For saving space and compute time (mostly comparison for these)\nfrom sklearn.preprocessing import LabelEncoder\n# We will have to use two different label encoders. One for 'Id' and other for 'idhogar'.\nlb1 = LabelEncoder()\nlb1.fit(list(train['Id'].values))\nlb2 = LabelEncoder()\nlb2.fit(list(train['idhogar'].values))\n# Now we will replace each unique id's with a unique number.\ntrain['Id'] = lb1.transform(list(train['Id'].values))\ntrain['idhogar'] = lb2.transform(list(train['idhogar'].values))\n\nlb3 = LabelEncoder()\nlb3.fit(list(test['Id'].values))\nlb4 = LabelEncoder()\nlb4.fit(list(test['idhogar'].values))\n# Now we will replace each unique id's with a unique number.\ntest['Id'] = lb3.transform(list(test['Id'].values))\ntest['idhogar'] = lb4.transform(list(test['idhogar'].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12da618ac76625f19fce0890cb71a78870b95a11"},"cell_type":"markdown","source":"Now let's see the others:\n\n1) **Dependency** :"},{"metadata":{"trusted":true,"_uuid":"dbbe7cb236579329810903f852353bd4a217e061"},"cell_type":"code","source":"train['dependency'].unique()  # rate dependency  (yes:1, no:0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab9b6a8ff55e9c5c77c4b9e98b0bfd449c45734","scrolled":true},"cell_type":"code","source":"train['dependency'].replace('yes', '1', inplace=True)\ntrain['dependency'].replace('no', '0', inplace=True)\ntrain['dependency'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb08816262063611910964bf78f60098877a99d9"},"cell_type":"code","source":"test['dependency'].replace('yes', '1', inplace=True)\ntest['dependency'].replace('no', '0', inplace=True)\ntest['dependency'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03de6cf8f0860349a36f4ed6bd24906cf5e13c2a"},"cell_type":"markdown","source":"2) **Edjefe** :"},{"metadata":{"trusted":true,"_uuid":"40cd46716fd37532135965d594bd77551a011407","scrolled":true},"cell_type":"code","source":"train['edjefe'].unique()  # years of education of male head of household  (given, yes:1, no:0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"601299c31bdbf4f2f51e14141fa17c3fffde8363"},"cell_type":"code","source":"train['edjefe'].replace('yes', '1', inplace=True)\ntrain['edjefe'].replace('no', '0', inplace=True)\ntrain['edjefe'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"840095950505a736167170d24c83152415824a99"},"cell_type":"code","source":"test['edjefe'].replace('yes', '1', inplace=True)\ntest['edjefe'].replace('no', '0', inplace=True)\ntest['edjefe'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2490196cb5ffa0087924948861e18dffe0202e5"},"cell_type":"markdown","source":"3) **Edjefa** :"},{"metadata":{"trusted":true,"_uuid":"a31acbf8ccafb4189debf4915c5ec0d227b07a0c"},"cell_type":"code","source":"train['edjefa'].unique()  # years of education of female head of household  (given, yes:1, no:0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788579d3daa1c8268b5fadcbcab185d4f1a25a75"},"cell_type":"code","source":"train['edjefa'].replace('yes', '1', inplace=True)\ntrain['edjefa'].replace('no', '0', inplace=True)\ntrain['edjefa'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c603571bcb8eda6e12cb3611e5ecf60ba78169"},"cell_type":"code","source":"test['edjefa'].replace('yes', '1', inplace=True)\ntest['edjefa'].replace('no', '0', inplace=True)\ntest['edjefa'].astype(np.float64);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"287baa0ec2f855c3ced1e865b0bfe5b3d4170ab5"},"cell_type":"markdown","source":"### Taking care of NULL values: <a id=\"nullValues\"></a>"},{"metadata":{"_uuid":"ac909ac66d2c5069cd6eaeea690fa74f904b53e4"},"cell_type":"markdown","source":"Null values in dataset can arise from many factors:\n1.  Non availability of data as not applicable for that particular row\n1.  Non availability of data as the Org. was not able to get it for some reason\n1. Due to some error or misplacement\n\nHere, we will consider that every feature had some data for every individual and put missing values equal to mean if no other option is available."},{"metadata":{"trusted":true,"_uuid":"fd02920bb940d2ed3614ca502ec610e698c7e0f1","scrolled":true},"cell_type":"code","source":"null_counts = train.isnull().sum()\nnull_counts[null_counts>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ad8907a0f3c77bab31130d2b114e64b50a52efb"},"cell_type":"markdown","source":"Description of columns with missing values:\n\n1. **v2a1 :** Monthly rent payment\n1. **v18q1 :** number of tablets household owns\n1. **ez_esc : ** Years behind in school\n1. **meaneduc :** average years of education for adults (18+)\n1. **SQBmeaned :** square of the mean years of education of adults (>=18) in the household\n---"},{"metadata":{"trusted":true,"_uuid":"1ca59be74bd945e8d7506a212d7bb1477d09f57d"},"cell_type":"code","source":"test_null_counts = test.isnull().sum()\ntest_null_counts[test_null_counts>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf852fb385d205586feaa635a5fa66ed59f4e0c2"},"cell_type":"markdown","source":"Same as train."},{"metadata":{"_uuid":"29630a9f928feefec18fe363eecedeefee0f4104"},"cell_type":"markdown","source":" **1) v2a1 **: Monthly rent payment\n\n        For this lets check these columns:\n        a) v2a1 : Monthly rent payment\n        b) v18q : owns a tablet\n        c) hacapo : Overcrowding by rooms\n        d) rooms : number of all rooms in the house\n        e) r4t3 : Total persons in the household\n        f) hhsize : household size\n        g) escolari : years of schooling\n        h) epared3 : =1 if walls are good\n        i) epared2 : =1 if walls are regular\n        j) tipovivi1 : =1 own and fully paid house\n        k) tipovivi2 : =1 own,  paying in installments\n        l) tipovivi3 : =1 rented\n        m) tipovivi4 : =1 precarious\n        n) tipovivi5 : =1 other(assigned,  borrowed)\n        p) Target : poverty level\n\n    And check if they own their house or not."},{"metadata":{"trusted":true,"_uuid":"0181faa1102fab9d53a001c058d1f69779c18f6a"},"cell_type":"code","source":"cols = ['Id', 'parentesco1', 'v2a1', 'v18q', 'hacapo', 'rooms', 'r4t3', 'hhsize', 'escolari', 'epared2',\n        'epared3', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09fd3ed0b11b56776c833503400b5dc5f6475386"},"cell_type":"code","source":"v2a1_null = train.query('v2a1 == \"NaN\"')[cols]; v2a1_null.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7d076d6e1299d747ee5def5d3e0bafa28708881e"},"cell_type":"code","source":"# Let us get the family heads of each household in this\nv2a1_null_heads = v2a1_null.query('parentesco1 == 1'); v2a1_null_heads.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1dcd9fc76e641f655600ab8d3372d9521b2e8ef9"},"cell_type":"code","source":"v2a1_null_heads.query('hacapo != 1').shape, v2a1_null_heads.query('hacapo == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ce41106f8ed285487339cf80318716d039c91da"},"cell_type":"markdown","source":"So there are *`26`* families who have overcrowding in their home. (where `v2a1` is null)\n\nIt means out of these null values, most of them are not living in poverty or extreme poverty. i.e. having `Target` value of *`1`* (most probably)."},{"metadata":{"trusted":true,"_uuid":"d1b022f8c4ca752d8cd095a1fca996ef6a1375c2"},"cell_type":"code","source":"v2a1_null.query('Target == 1').shape, v2a1_null.query('Target == 2').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fc0c053a435a3309a51fd66dbc933bbde90ebc4"},"cell_type":"markdown","source":"Out of *`6860`* people having null `v2a1`, *`1862`* are from `Target` of *`1`* or *`2`*. Not much. \n\nSo it is a possibility that many of them own their house or data is missing for some other reason."},{"metadata":{"trusted":true,"_uuid":"529904071622fb1d6f4f8e3162f9e8ff98c8a922"},"cell_type":"code","source":"v2a1_null_heads.query('epared2 != 1 & epared3 != 1').shape # Families who don't have regular or good walls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d809e821d6aa29d662ada970e0c5e2e5fcb381c"},"cell_type":"markdown","source":"Not much, again."},{"metadata":{"trusted":true,"_uuid":"401e8b4aebcc3602325fcb73ac5a88eede4ab5ae"},"cell_type":"code","source":"v2a1_null_heads.query('tipovivi1 != 1').shape # Families who don't own thier own home.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3bdca93a18125fe850a4cb3df90987301e34422"},"cell_type":"markdown","source":"So out of *`2156`* families, *`300`* don't own their home."},{"metadata":{"trusted":true,"_uuid":"3248cf1fdd42a1bca1361c846cdd48f474b62c5f"},"cell_type":"code","source":"v2a1_null_heads.query('tipovivi2 == 1').shape, v2a1_null_heads.query('tipovivi3 == 1').shape, v2a1_null_heads.query('tipovivi4 == 1').shape, v2a1_null_heads.query('tipovivi5 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5859828e1a773fd81ca43bbcc009be343e59a204"},"cell_type":"markdown","source":"Out of people who don't own their home have either `precarious`, or `other (assigned or borrowed)` homes."},{"metadata":{"trusted":true,"_uuid":"c7aea6a371535bf24dd152dd6111fe9fbd6ed3ed"},"cell_type":"code","source":"v2a1_null_heads.query('tipovivi1 != 1 & tipovivi2 != 1 & tipovivi3 != 1 & tipovivi4 != 1 & tipovivi5 != 1') \n# Checking for any wrong data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4c5e7baccad65e36086d915dc09551480fdf3d4"},"cell_type":"code","source":"v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 1').shape, v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 2').shape, v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 3').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce5fe881ffd916e349bdce8cf16bf10051dced24"},"cell_type":"markdown","source":"Out of families with `precarious` or `other` homes, *`114`* have `Target` value <=*`2`*.\n\nAs we see, out of these *`300`* families who don't own their homes, we have a mix of families, with all `Target` values.\n\nSo, we will put `v2a1` value equal to mean of `v2a1` values in set of that `Target` value."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cde38089d69e6718fa0282f07dc71f1cc2e39c29"},"cell_type":"markdown","source":"But firstly lets put `v2a1` values for people who own their homes equal to zero."},{"metadata":{"trusted":true,"_uuid":"8e95c8ba0de4c431f0e071d27f2126e53eb10d91","scrolled":true},"cell_type":"code","source":"train.loc[train['v2a1'].isnull() & train['tipovivi1'] == 1, 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed66c03e6e92a333038d5d168dd80df45bb19d4"},"cell_type":"markdown","source":"Doing the same with test set:"},{"metadata":{"trusted":true,"_uuid":"505f01e29e0a007efb9895e18c009ae3b76aaca5"},"cell_type":"code","source":"test.loc[test['v2a1'].isnull() & test['tipovivi1'] == 1, 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8831c1d6ae5a92ec96c0ef80ebe5259f55507ca"},"cell_type":"code","source":"train.query('v2a1 == \"NaN\"').shape, test.query('v2a1 == \"NaN\"').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d93b325524c83250886d4386a8169e1744eebd03"},"cell_type":"markdown","source":"And now we will make others equal to their means, taking data only from their category."},{"metadata":{"trusted":true,"_uuid":"e946aa8c81b9a5c0091eda4196658aef622e7cf7"},"cell_type":"code","source":"train.query('v2a1 != \"NaN\"')['v2a1'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"507c2d76cd0f48b7a2827990e797292e63e75106"},"cell_type":"code","source":"a, b = train.query('Target == 1 & v2a1 != \"NaN\"')['v2a1'].mean(), train.query('Target == 2 & v2a1 != \"NaN\"')['v2a1'].mean(); a, b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37599e76eb0a831bcf3a84720e8543b6eae55d73"},"cell_type":"code","source":"c, d = train.query('Target == 3 & v2a1 != \"NaN\"')['v2a1'].mean(), train.query('Target == 4 & v2a1 != \"NaN\"')['v2a1'].mean(); c, d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cfa88280eeb72a0df62ab7c46d2cf21fa9cc5392"},"cell_type":"code","source":"train.loc[train['v2a1'].isnull() & (train['Target']== 1), 'v2a1'] = a\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 2), 'v2a1'] = b\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 3), 'v2a1'] = c\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 4), 'v2a1'] = d\ntrain.loc[train['v2a1'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"26813c3abde1e6eef6ddef1b2cbfe0239e2cefd4"},"cell_type":"code","source":"test.loc[test['v2a1'].isnull(), 'v2a1'] = (a+b+c+d)/4  # We cannot check Target value here\ntest.loc[test['v2a1'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ac75fc1cdd8d2e16e284d16647fe52fa1c8d723"},"cell_type":"markdown","source":"** 2) v18q1** : number of tablets household owns\n\n    For this lets check these columns:\n    a) v18q : owns a tablet  # And no value is null here, we will use this.\n"},{"metadata":{"trusted":true,"_uuid":"e817ab3c708dfb7ad8666d4a75b347e8f6755e08"},"cell_type":"code","source":"v18q1_null = train.query('v18q1 == \"NaN\"'); v18q1_null.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d9d558e0052aa9f2cb9b6b81f7b77c46b78985c","scrolled":true},"cell_type":"code","source":"h_ids = v18q1_null['idhogar'].unique(); h_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91fc8dd2066a1c735bc67df5beeb28eb0f43cec5"},"cell_type":"code","source":"# For every household we will calulate how many of them owns a tablet and put 'v18q1' equal to that sum\nfor idn in h_ids:\n    train.loc[(train['idhogar'] == idn), 'v18q1'] = train.query(f'idhogar == {idn}')['v18q'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e966ba7aa40157dbaca785899625a10f8fc9bce2"},"cell_type":"markdown","source":"Doing the same for test set:"},{"metadata":{"trusted":true,"_uuid":"9a78cd25e4410969a3e3d35016fd95ca0a57ca28"},"cell_type":"code","source":"test_v18q1_null = test.query('v18q1 == \"NaN\"')\nh_ids = test_v18q1_null['idhogar'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4bc4371c59f0b8fd94e3ee353c4a147042f1e9c7"},"cell_type":"code","source":"for idn in h_ids:\n    test.loc[(test['idhogar'] == idn), 'v18q1'] = test.query(f'idhogar == {idn}')['v18q'].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e5bea0014c68aa4d6faac2ffb7024c628ad2b3e"},"cell_type":"markdown","source":"**3) rez_esc** : Years behind in school\n\n        For this we will check columns:\n        a) escolari: years of schooling\n        b) estadocivil1: =1 if less than 10 years old\n        c) estadocivil2: =1 if free or coupled union        # We are checking these ones, because they may be old\n        d) estadocivil3: =1 if married                      # and it might be the case that IADB does not have \n        e) estadocivil4: =1 if divorced                     # this data about them.\n        f) estadocivil5: =1 if separated\n        g) estadocivil6: =1 if widower\n        h) estadocivil7: =1 if single\n        i) instlevel1: =1 no level of education\n        j) age: Age in years"},{"metadata":{"trusted":true,"_uuid":"e37673c2e551efbf4f9c8b6fbd920188822b1011"},"cell_type":"code","source":"cols = ['Id', 'idhogar', 'escolari', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',\n        'estadocivil5', 'estadocivil6', 'estadocivil7', 'instlevel1', 'age', 'Target']\ncols2 = cols[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94d8a1085870d56c42d1eac1ec59d1ccc724648"},"cell_type":"code","source":"rez_esc_null = train.query('rez_esc == \"NaN\"')[cols]\ntest_rez_esc_null = test.query('rez_esc == \"NaN\"')[cols2]; rez_esc_null.shape, test_rez_esc_null.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a14e9872ac3ab2fe1637eacb66ec4fcf6eb27b39"},"cell_type":"code","source":"rez_esc_null.query('instlevel1 == 1').shape, test_rez_esc_null.query('instlevel1 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac37eeb00874a5bf030c35e2dd75de46343ce5d4"},"cell_type":"markdown","source":"Good, out of these *`1183`* don't have any level of education (for training data). We will put `rez_esc` for them equal to *`0`*."},{"metadata":{"trusted":true,"_uuid":"7252bb7bd55ea5e405f6ee74ce2e9af54e3091b7"},"cell_type":"code","source":"train.loc[(train['rez_esc'].isnull()) & (train['instlevel1'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['instlevel1'] == 1), 'rez_esc'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e203c4639f93b3847562f46abd21b6e69c8f651c"},"cell_type":"code","source":"rez_esc_null = train.query('rez_esc == \"NaN\"')[cols]\ntest_rez_esc_null = test.query('rez_esc == \"NaN\"')[cols2]; rez_esc_null.shape, test_rez_esc_null.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e472506289727e4237abcc43c9d694a5dffcb566"},"cell_type":"code","source":"# estadocivil1: =1 if less than 10 years old\nrez_esc_null.query('estadocivil1 == 1').shape, test_rez_esc_null.query('estadocivil1 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65aa6990d900cba9ec00a06a2d97d82afd6b2aed"},"cell_type":"markdown","source":"So, we don't have any child for whom this value is missing. Maybe we were right about thinking that they don't have this value for adults."},{"metadata":{"trusted":true,"_uuid":"e7fff5ca656503d4f83ffc816255b5eeb7ba5d06"},"cell_type":"code","source":"rez_esc_null.query('estadocivil2 == 1').shape, rez_esc_null.query('estadocivil3 == 1').shape, rez_esc_null.query('estadocivil4 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0d855ea481d99d6136f35808e8a56171a7785fd"},"cell_type":"markdown","source":"So, we have *`1111`* `free or coupled union`, *`2486`* `married` and *`300`* `divorced`."},{"metadata":{"trusted":true,"_uuid":"0b07f308714237fd184094b2c26ee7d43dad5297"},"cell_type":"code","source":"test_rez_esc_null.query('estadocivil2 == 1').shape, test_rez_esc_null.query('estadocivil3 == 1').shape, test_rez_esc_null.query('estadocivil4 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eebedea840aa7b77a1ce881214d8025af55ad0e3"},"cell_type":"code","source":"rez_esc_null.query('estadocivil5 == 1').shape, rez_esc_null.query('estadocivil6 == 1').shape, rez_esc_null.query('estadocivil7 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73e592e41dae228c206b3aaba3133b636db3d7e1"},"cell_type":"markdown","source":"*`564`* `separated`, *`279`* `widower` and *`2005`* `single`."},{"metadata":{"trusted":true,"_uuid":"774e249a77c162715b6650e869d1dd29c5219360"},"cell_type":"code","source":"test_rez_esc_null.query('estadocivil5 == 1').shape, test_rez_esc_null.query('estadocivil6 == 1').shape, test_rez_esc_null.query('estadocivil7 == 1').shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"363cdf5f11f22170d0f19d97728dc5be9d1d725e"},"cell_type":"markdown","source":"We will put `rez_esc` equal to average value in their category:"},{"metadata":{"trusted":true,"_uuid":"d5d948181b460550d4d150f3418c27531badc395","_kg_hide-input":true},"cell_type":"code","source":"a = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil2'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nb = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil3'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nc = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil4'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nd = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil5'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\ne = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil6'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nf = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil7'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\na, b, c, d, e, f\n\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil2'] == 1), 'rez_esc'] = 3\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil3'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil4'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil5'] == 1), 'rez_esc'] = 2\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil6'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil7'] == 1), 'rez_esc'] = 1\n\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil2'] == 1), 'rez_esc'] = 3\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil3'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil4'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil5'] == 1), 'rez_esc'] = 2\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil6'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil7'] == 1), 'rez_esc'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bde542cd05696db16b6226134205a7391ea70c8"},"cell_type":"code","source":"train['rez_esc'].isnull().sum(), test['rez_esc'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"625a9b45bde8fde3eb4b01e50a322a9d7aa56275"},"cell_type":"markdown","source":" 4) **meaneduc** : average years of education for adults (18+) and \n \n 5) **SQBmeaned**: square of the mean years of education of adults (>=18) in the household\n\n       For this we will check: \n       a) instlevel1 : =1 no level of education\n       b) instlevel2 : =1 incomplete primary\n       c) instlevel3 : =1 complete primary\n       d) instlevel4 : =1 incomplete academic secondary level\n       e) instlevel5 : =1 complete academic secondary level\n       f) instlevel6 : =1 incomplete technical secondary level\n       g) instlevel7 : =1 complete technical secondary level\n       h) instlevel8 : =1 undergraduate and higher education\n       i) instlevel9 : =1 postgraduate higher education"},{"metadata":{"trusted":true,"_uuid":"ac3d27747474fa595a46ef67a30b48d746937f79"},"cell_type":"code","source":"cols = ['Id', 'idhogar', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5',\n       'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27a6e678b8df4ee40890f7d130318eb97d1a1a87"},"cell_type":"code","source":"# We will just put this value equal to avg year of education of adults with the help of selected cols\nmeaneduc_null = train.query('meaneduc == \"NaN\"')[cols]\ntest_meaneduc_null = test.query('meaneduc == \"NaN\"')[cols]\nh_ids = meaneduc_null['idhogar'].unique(); h_ids","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbb06c64d4d0ce2a8257f84b3e1ffd28c3f400e4"},"cell_type":"markdown","source":"*`3`* families whose `meaneduc` is not available."},{"metadata":{"trusted":true,"_uuid":"600a13c101962b489c8781b11fdc3eb4ff900db5"},"cell_type":"code","source":"print(train.loc[(train['idhogar'] ==  326), 'meaneduc'].values)\nprint(train.loc[(train['idhogar'] == 1959), 'meaneduc'].values) \nprint(train.loc[(train['idhogar'] == 2908), 'meaneduc'].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ee385ad402db92da999f3185b8c37a0f790c10"},"cell_type":"markdown","source":"So, all of them are actually here. Otherwise we could have put `meaneduc` equal to someone in their family who had that value present."},{"metadata":{"trusted":true,"_uuid":"19f8536315d424de860d0f31da650db459275fcd","_kg_hide-input":true},"cell_type":"code","source":"def meaneduc_correction(null_view, df, hids):\n    \"\"\"\n    Function to correct null_values in \"meaneduc\" feature. Will put them equal to mean, after calculating it\n    using \"instlevel\"'s.\n    ---------------------------------------------------------------------------------------------------------\n    Parameters:\n        null_view: View of origianl dataframe with null values of \"meaneduc\"\n        df: Original DataFrame\n        hids: Unique Household ids of households with null \"meaneduc\"\n    \"\"\"\n    for idn in hids:\n        # Number of people with no education and so on\n        a = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel1'] == 1)].shape[0] # No ed\n        b = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel2'] == 1)].shape[0] # Inc. prim\n        c = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel3'] == 1)].shape[0] # Com. prim\n        d = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel4'] == 1)].shape[0] # Inc Acad Sec L.\n        e = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel5'] == 1)].shape[0] # Com Acad Sec L.\n        f = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel6'] == 1)].shape[0] # Inc Tech Sec L.\n        g = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel7'] == 1)].shape[0] # Com Tech Sec L.\n        h = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel8'] == 1)].shape[0] # UndGrad n HigEd\n        i = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel9'] == 1)].shape[0] # Postgrad\n\n        mean_educ = (a*0 + b*4 + c*8 + d*2 + e*4 + f + g*2 + h*4 + i) / (a+b+c+d+e+f+g+h+i)\n\n        df.loc[(df['meaneduc'].isnull()) & (df['idhogar'] == idn), 'meaneduc'] =  mean_educ\n        df.loc[(df['SQBmeaned'].isnull()) & (df['idhogar'] == idn), 'SQBmeaned'] =  mean_educ**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbdae0afb91d0632a8285d49e6d2427be9e0d236"},"cell_type":"code","source":"meaneduc_correction(meaneduc_null, train, h_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8843f56faa9e40f874316e0e8c8466c43132f7"},"cell_type":"code","source":"null_counts = train.isnull().sum()\nnull_counts[null_counts>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b6848150e95711194242cbffca272947ed9860d"},"cell_type":"markdown","source":"Now same for test:"},{"metadata":{"trusted":true,"_uuid":"8c4a1267b90ddfc2403322152a1da36c7ac15ece"},"cell_type":"code","source":"h_ids = test_meaneduc_null['idhogar'].unique(); h_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c6e681809b1dced87a20aa00a9bad79f380dd3e"},"cell_type":"code","source":"meaneduc_correction(test_meaneduc_null, test, h_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb0e3a5da95c9a0b241852a3dec4fc6462140c7"},"cell_type":"code","source":"test_null_counts = test.isnull().sum()\ntest_null_counts[test_null_counts>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0092f6df0481ae84de7aadb8f3e45b8999494cbd"},"cell_type":"markdown","source":"# Checking for corrupted data <a id=\"checkingCorruptedData\"></a>\n---"},{"metadata":{"_uuid":"ba2a49df97bd3e03424b587a28198fe2cf057a7e"},"cell_type":"markdown","source":"The only things we can check here are:\n     \n    1. Check if all Id's are unique. (Should have checked first. But all are unique)\n    2. Check if same household has same Target value, meaneduc value, zone value ( urban or rural), region value, house properties (wall type, ceiling type etc), number of persons in houshold, number of adults, number of childern, number of tablets household owns.\n\nWe cannot check the others, because there is no way to check their validity. If for some reson they are wrong, they are wrong. But such cases happen rarely.  So, we don't need to worry about that."},{"metadata":{"trusted":true,"_uuid":"5ffb78a8acd22834613d1185002d611eaefd5f15"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6ab5802284954cf6295ffb7715af9eb6fe4bb1b"},"cell_type":"code","source":"train['Id'].unique().size, test['Id'].unique().size  # So, Ids are unique","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b224ca9e2eb78ae0c0cee008c33d7f235001c83","_kg_hide-input":true},"cell_type":"code","source":"# Now for the second part\ncols = ['v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'v18q1',\n       'r4h3', 'r4m3', 'r4t3', 'tamhog', 'tamviv', 'hhsize', 'paredblolad',\n       'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc',\n       'paredfibras', 'paredother', 'pisomoscer', 'pisocemento', \n       'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera', 'techozinc',\n       'techoentrepiso', 'techocane', 'techootro', 'cielorazo',\n       'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', \n       'planpri', 'noelec', 'coopele', 'sanitario1', 'sanitario2', \n       'sanitario3', 'sanitario5', 'sanitario6', \n       'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',\n       'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5',\n       'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', \n       'etecho3', 'eviv1', 'eviv2', 'eviv3', 'hogar_nin', 'hogar_adul',\n       'hogar_mayor', 'hogar_total', 'dependency', 'meaneduc', 'bedrooms', \n       'overcrowding', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4',\n       'tipovivi5', 'computer', 'television', 'qmobilephone', 'lugar1',\n       'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c559522f3a7891874c787e8714e6a15bbc55bad6","_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ndef check_for_wrong_data(data, columns, labelE, gpby='idhogar'):\n    \"\"\"\n    Checks for data mismatches in rows of every group, which we get by gpby (groupby)\n    feature, on columns in \"columns\". If mismatch is there, put it equal to value in\n    columns of head of the household and print a message for this.\n    ----------------------------------------------------------------------------------\n    Input:\n        data : Train or Test set or their sliced views\n        columns : columns to check for corrupted data\n        labelE : Label encoder of \"data\"'s  \"idhogar\" column used in inverse_transform \n        gpby : feature to group by to check for diff \"cols\" in that group\n    Output:\n        Return four arrays:\n        1) Array with ids of households with no head\n        2) Array with ids of households with wrong data\n        3) Array of arrays with column name with wrong data for each household in (2)nd array\n        4) Array of arrays with ids of members with wrong data for each household in (2)nd array\n    \"\"\"\n    id_head_zero = [] # Will contain house ids with no head\n    idhogarId_f = []\n    cols_f = []\n    mem_f = []\n    grouped = data.groupby(gpby, sort=True)\n    for gid in range(len(grouped)):\n        members = grouped.get_group(gid)\n        h_Head = members.loc[(members['parentesco1'] == 1)]\n        if h_Head.shape[0] == 0:\n            id_head_zero.append(members['idhogar'].values[0])\n            continue\n        idhogarId_w = []\n        cols_t = []\n        mem_t = []\n        if members.shape[0] > 1:\n            for col in columns:\n                for m in members.iterrows():\n                    if h_Head[col].values[0] != m[1][col]:\n                        if h_Head['idhogar'].values[0] not in idhogarId_w : idhogarId_w.append(h_Head['idhogar'].values[0])\n                        if col not in cols_t : cols_t.append(col)\n                        if m[1]['Id'] not in mem_t : mem_t.append(m[1]['Id'])\n                        # Correct this column\n                        data.loc[(train['Id'] == m[1]['Id']), col] = h_Head[col].values[0]\n        idhogarId_f.append(idhogarId_w); cols_f.append(cols_t); mem_f.append(mem_t)\n        if len(idhogarId_w) > 0:\n            for i in range(len(idhogarId_w)):\n                print(\"Household with Id: \"\n                +str(labelE.inverse_transform([idhogarId_w[i]])[0])\n                +\" has \" + str(len(mem_t)) + \" member(s) with diff. value(s) of \" + str(len(cols_t)) + \" column(s).\"\n                + \" \" + str(cols_t) )\n    return id_head_zero, idhogarId_f, cols_f, mem_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba0023479c286f085bc3118f5aa140d0bf2d7fd4","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"id_head_zero, *_ = check_for_wrong_data(train, cols, lb2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00253a0666be90fdc9823d4ffdaa4b949d5e8029"},"cell_type":"markdown","source":"Now all of them are fixed."},{"metadata":{"trusted":true,"_uuid":"d6a2b219eac0de8e5ce7a0c289a7ecccb2e358e6"},"cell_type":"code","source":"train.loc[(train['idhogar'] == id_head_zero[11])]   # 4, 6, 7, 8, 11 have more than 1 persons in home but no head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e2c2e4ecba5e2620a94f339c649bd5b7f6cca3a1"},"cell_type":"markdown","source":"We will leave it as it is, as we don't know what to change it to."},{"metadata":{"trusted":true,"_uuid":"ebbbd1406d37b036965ba6fcb513c56b0b548715","collapsed":true},"cell_type":"markdown","source":"Same for `test` dataset:"},{"metadata":{"trusted":true,"_uuid":"4968825e38f00976d57195ebaef390765bb1e069"},"cell_type":"code","source":"cols = cols[:-1] # Remove \"Target\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5877dcb0f8bd3ba5fe4d5b41c807fccaf2ae584"},"cell_type":"code","source":"id_head_zero, *_ = check_for_wrong_data(test, cols, lb4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e2a3125207351c64a06df0680a777f105d9d566"},"cell_type":"markdown","source":"# Data Exploration and Visualization: <a id=\"explore\"></a>\n---"},{"metadata":{"trusted":true,"_uuid":"eea34e52447d4a2c3d9efa00f374d1b6cb9d486b","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"import seaborn as sns\ncolumns = train.select_dtypes('number').drop(['Id', 'idhogar', 'Target'], axis=1).columns\n\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 15))\nfig.subplots_adjust(top=1.3)\n#train.loc[:,columns[1:22]].boxplot(ax=axes[0])\na = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[1:22]]), ax=axes[0])\nb = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[22:70]]), ax=axes[1])\nb.set_xticklabels(rotation=30, labels = columns[22:70]);\nc = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[70:98]]), ax=axes[2])\nc.set_xticklabels(rotation=30, labels = columns[70:120]);\nd = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[99:120]]), ax=axes[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8979d072afe8e5f67f47abe3c6fc6560b4179471"},"cell_type":"code","source":"possible_outliers = [columns[0]] + [columns[98]]; columns[0], columns[98]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fb454187faf30d5c5471c251ba08d66fc21be31","scrolled":true},"cell_type":"code","source":"sns.boxplot(data = train[possible_outliers[0]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83c92666a5d579b857ccca240215e8f64788fd9a"},"cell_type":"markdown","source":"Someone or some families have `v2a1` i.e. `Monthly rent Payment` of more than **`2,000,000`**!!"},{"metadata":{"trusted":true,"_uuid":"579426aef489bfd30e791d566db76d3eb08c1e40"},"cell_type":"code","source":"train.loc[(train['v2a1'] > 300000), ['idhogar', 'v2a1', 'Target']].query(\"Target != 4\")  # Actually, all above 300,000 are from \"Target\" of 4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3fa3e6db5bf8fc56ebf5e7c4bdc79fa8096a528"},"cell_type":"markdown","source":"But this one particular family pays about **`2,000,000`** (i.e. `$3456.20` at current rates) and I checked at a [site](https://www.propertiesincostarica.com) and some bunglows have similar rates..."},{"metadata":{"trusted":true,"_uuid":"e9b0a1f9475439651ead8c4f829323ecf84edcb0"},"cell_type":"code","source":"train.loc[(train['v2a1'] > 2000000), ['idhogar', 'v2a1', 'Target']]   # So leave it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db3f2bbb6861964fdeb7aeb24db8edc271cffcca"},"cell_type":"code","source":"sns.boxplot(data = train[possible_outliers[1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea583418baecc545bf37bb9ca80fef05a5705c01"},"cell_type":"code","source":"train.loc[(train['meaneduc'] > 25), ['idhogar', 'Target', 'meaneduc']].query(\"Target != 4\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26cfad3797f220b4c8582ef3f0baeebec9f7d84c"},"cell_type":"markdown","source":"They are also from `Target` value of **`4`**, so leave them too...\n\nNow let's check correlation between some of the columns I selected:\n1. `v2a1` :  Monthly rent payment\n1. `rooms`: number of all rooms in the house\n1. `tamhog`: size of the household\n1. `overcrowding`: # persons per room\n1. `v18q1`: number of tablets household owns\n1. `r4t3`: Total persons in the household\n1. `meaneduc`: average years of education for adults (18+)\n1. `qmobilephone`: # of mobile phones\n1. `Target`: the target is an ordinal variable indicating groups of income levels"},{"metadata":{"trusted":true,"_uuid":"0f202c56cf1e778a7dffa56b9cac947fc2cde6d3"},"cell_type":"code","source":"columns = ['v2a1', 'rooms', 'tamhog', 'overcrowding', 'v18q1', 'r4t3', 'meaneduc', 'qmobilephone', 'Target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5eb59576eec64c42dd742544e1845c1bada15c8","scrolled":false},"cell_type":"code","source":"sns.pairplot(train[columns])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65dda48cfe36a223a83c5bd2cc0d4a3cd2261168"},"cell_type":"markdown","source":"Some insights from this pair plot:\n1. `meaneduc` with `v2a1` has a kind of Gaussian Distribution with people with about 20yrs of education paying greater house rent than others,\n1. `v2a1` decreases as `overcrowing` increases. It may be possible that most of the overcrowded are from `Target` **`4`**,\n1. `v2a1` also decreases as `r4t3` increases. It may be because of similar reason above, more # of people increase may be indication of a lower `Target` value,\n1. `r4t3` and `tamhog` showing strong positive linear behaviour. Size of house hold doesn't guarantee quality."},{"metadata":{"_uuid":"57ab2261bc9de37954166f7c73b78c1aba359887"},"cell_type":"markdown","source":"# Feature Engineering <a id=\"featureEng\"></a>\n---"},{"metadata":{"_uuid":"6e9f216776229d251f0cb5be9cfc99f6e4433a95"},"cell_type":"markdown","source":"Firstly we can remove all the Square values columns : \n\n    SQBescolari     : escolari squared\n    SQBage          : age squared\n    SQBhogar_total  : hogar_total squared\n    SQBedjefe       : edjefe squared\n    SQBhogar_nin    : hogar_nin squared\n    SQBovercrowding : overcrowding squared\n    SQBdependency   : dependency squared\n    SQBmeaned       : square of the mean years of education of adults (>=18) in the household\n    agesq           : Age squared\n\nbecause they will be highly correlated with their unit degree counterparts, and we will be using a Neural Network and in Neural Network you don't need higher order terms of your features to check if that degree of feature explains better or not.  We do that in Linear Regression to capture non linear relationship of some features with the output. Here, Neural Network will learn these relations by itself by adjusting the weights.\n\nIn RandomForest and GradientBoosting also, we wont use these highly correlated featues."},{"metadata":{"trusted":true,"_uuid":"c73cdd48bca9e2f53773b9579d3d6fe68e852d0a","scrolled":false},"cell_type":"code","source":"train.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding',\n            'SQBdependency', 'SQBmeaned', 'agesq'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aba532f5b43a785da963d7cf2f1eb9c8ecfbb6f"},"cell_type":"code","source":"test.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding',\n            'SQBdependency', 'SQBmeaned', 'agesq'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2562fdf68339ba583e76b5daa450ee8d6fdc08e","scrolled":false},"cell_type":"code","source":"# Plotting a heat map\nimport seaborn as sns\nplt.subplots(figsize=(20,15))\nsns.heatmap(train.corr().abs(), cmap=\"BuPu\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57933bcd452481ead0fe96a5313a5a272cbd8f9"},"cell_type":"markdown","source":"### Combining some of the features: <a id=\"combiningFeatures\"></a>"},{"metadata":{"_uuid":"b3337b5c49df1ca6607b7d471df8d3429f43f055"},"cell_type":"markdown","source":"We also can combine some ordinal groups, by making one feature from a group of features which give information about the same thing and have a ordinal relationship between them. Combining features will save us some space and compute time.\n\nOrdinal feature Groups:\n*  Material outside wall:  `[ 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother' ]`\n*  Material Floor : `[ 'pisomoscer', 'pisocemento', 'pisoother' , 'pisonatur', 'pisonotiene', 'pisomadera' ]`\n*  Material Roof : `[ 'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo' ]`\n*  Water Provisoin : `[ 'abastaguadentro', 'abastaguafuera', 'abastaguano' ]`\n*  Electricity Provision : `[ 'public', 'planpri', 'noelec', 'coopele' ]`\n*  Sanitary Provision : `[ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6' ]`\n*  Cooking Provision : `[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]`\n*  Disposal Type : `[ 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6' ]`\n*  Walls Type : `[ 'epared1', 'epared2', 'epared3' ]`\n*  Roof Type : `[ 'etecho1', 'etecho2', 'etecho3' ]`\n*  Floor Type : `[ 'eviv1', 'eviv2', 'eviv3' ]`\n*  Education Level : `[ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]`\n*  House Type : `[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]`\n\nWe won't combine Material used features because we don't know the relative price of things there. \n\n**For water provision** : `no` < `outside` < `inside` **OR** `no` < `inside` < `outside`, we don't really know. So we will leave this group too.\n\n**For elec Povision** : `no` < `JASEC/ESPH` < `Cooperative` < `private plant` **OR** `no` < `Coop` < `ESPH` < `Private Plant`; Leave can leave this too\n\n**For Sanit Provision** : `no` < `blackhole` < `septic` < `sever`, `other` (?) **OR**  `no` < `blackhole` < `sever` < `septic`, `other`(?)\n\n**For Cook'n Prov** : `no` < `wood` < `gas` < `elec`\n\n**For Disposal type** : `river` >< `burning` >< `throwUnoccu` < `botan` < `Truck`, `other`(?); Leave it.\n\n**For Wall Type** : `bad` < `reg` < `good`\n\n**For Roof Type** : `bad` < `reg` < `good`\n\n**For Floor Type** : `bad` < `reg` < `good`\n\n**Ed Level** :  This group can be used\n\n**House Type** : `Precarious` < `other` < `rented` < `installment` < `fullyOwned`\n"},{"metadata":{"trusted":true,"_uuid":"62587f210f4a831d55261ae4071200fdf9f26116","_kg_hide-input":true},"cell_type":"code","source":"DropCols = ['energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2', 'epared1', 'epared2', 'epared3',\n        'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', #'instlevel1', 'instlevel2', 'instlevel3', \n        #'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n        'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\n\ntrain['CookingType'] = np.argmax(np.array(train[[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]]), axis=1)\ntrain['WallType'] = np.argmax(np.array(train[[ 'epared1', 'epared2', 'epared3' ]]), axis=1)\ntrain['RoofType'] = np.argmax(np.array(train[[ 'etecho1', 'etecho2', 'etecho3' ]]), axis=1)\ntrain['FloorType'] = np.argmax(np.array(train[[ 'eviv1', 'eviv2', 'eviv3' ]]), axis=1)\n# EdLevel is being removed during deletion of highly correlated features\n# train['EdLevel'] = np.argmax(np.array(train[ [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]]), axis=1)\ntrain['HouseType'] = np.argmax(np.array(train[[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]]), axis=1)\n\ntest['CookingType'] = np.argmax(np.array(test[[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]]), axis=1)\ntest['WallType'] = np.argmax(np.array(test[[ 'epared1', 'epared2', 'epared3' ]]), axis=1)\ntest['RoofType'] = np.argmax(np.array(test[[ 'etecho1', 'etecho2', 'etecho3' ]]), axis=1)\ntest['FloorType'] = np.argmax(np.array(test[[ 'eviv1', 'eviv2', 'eviv3' ]]), axis=1)\n# EdLevel is being removed during deletion of highly correlated features\n# test['EdLevel'] = np.argmax(np.array(test[[ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]]), axis=1)\ntest['HouseType'] = np.argmax(np.array(test[[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]]), axis=1)\n\ntrain.drop(DropCols, axis=1, inplace=True)\ntest.drop(DropCols, axis=1, inplace=True)\ntest.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3fb64a3bb408641671c3988959034a555a2988f"},"cell_type":"markdown","source":"More Domain Knowledge Features: (from [here](https://www.kaggle.com/willkoehrsen/featuretools-for-good))"},{"metadata":{"trusted":true,"_uuid":"0f74df95aefb700a10e6a9a3cb37295a4d3a7b1b","_kg_hide-input":true},"cell_type":"code","source":"# Per member features\ntrain['phones-per-mem'] = train['qmobilephone'] / train['tamviv']\ntrain['tablets-per-mem'] = train['v18q1'] / train['tamviv']\ntrain['rooms-per-mem'] = train['rooms'] / train['tamviv']\ntrain['rent-per-adult'] = train['v2a1'] / train['hogar_adul']\n\ntest['phones-per-mem'] = test['qmobilephone'] / test['tamviv']\ntest['tablets-per-mem'] = test['v18q1'] / test['tamviv']\ntest['rooms-per-mem'] = test['rooms'] / test['tamviv']\ntest['rent-per-adult'] = test['v2a1'] / test['hogar_adul']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e77cfb541a4aeee8d59584b9e79fdfc6c0ef6b62"},"cell_type":"markdown","source":"### Remove Highly Correlated features:  <a id=\"removeHighlyCorrelatedFeatures\"></a>\n\nAll Highly Correlated features are not necessary to kept in the dataset. We can take only one of them, which will be sufficient for getting what they were all telling together. Keeping all of them will be redundant, as they have same trend in dataset, and even one of them can capture that trend."},{"metadata":{"trusted":true,"_uuid":"3c94bb16e593b18f905222d6d0ab1a79086484da","_kg_hide-input":true},"cell_type":"code","source":"def chk_n_remove_corr(df):\n    \"\"\"\n    Checks for highly correlated features and removes them.\n    ---------------------------------------------------------------------\n    Parameters:\n        df: Dataframe to check for correlation\n    Output:\n        Return list of removed features/columns.\n    \"\"\"\n    corr_matrix = train.corr()\n    \n    # Taking only the upper triangular part of correlation matrix: (We want to remove only one of corr features)\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find index of feature columns with correlation greater than 0.975\n    to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n    \n    train.drop(to_drop, axis=1, inplace=True)\n    return to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b50300c7be062d99b52e61d418c6eef5c616b98","scrolled":true},"cell_type":"code","source":"to_drop = chk_n_remove_corr(train)\nto_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e6274c4501c59cf6d785e7c4d11d8d38e057eb6"},"cell_type":"code","source":"test.drop(to_drop, axis=1, inplace=True)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b3fb2c7598cc1507f812532b6d24b91489759f3"},"cell_type":"markdown","source":"#### Changing the type of columns:\n\nWe will set type of all columns according to their possible values. "},{"metadata":{"trusted":true,"_uuid":"61f775546edcc542b521f132dea75f4fabc33239","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6',\n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1','tamviv','hogar_nin',# 'hhsize', 'tamhog',\n              'CookingType', 'WallType', 'RoofType', 'HouseType' , 'FloorType', \n              'hogar_adul','hogar_mayor',  'bedrooms', 'qmobilephone'] # ,'hogar_total']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n          'phones-per-mem', 'tablets-per-mem', 'rooms-per-mem', 'rent-per-adult']\n\nind_bool = ['v18q', 'dis', 'male', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5',\n            'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'mobilephone']\n\nind_ordered = ['age', 'escolari', 'rez_esc']#, 'EdLevel']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6bde01e468d2c7f986f3ab7785b9aed0a38dc48"},"cell_type":"code","source":"train[hh_bool + ind_bool] = train[hh_bool + ind_bool].astype(bool)\ntest[hh_bool + ind_bool] = test[hh_bool + ind_bool].astype(bool)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d10303e89c14e60720253bf8106485601d8098"},"cell_type":"code","source":"train[hh_cont] = train[hh_cont].astype('float64')\ntest[hh_cont] = test[hh_cont].astype('float64');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6016e7d6a97412b05564c890bb4b8d7a4becbb48"},"cell_type":"code","source":"train[hh_ordered + ind_ordered] = train[hh_ordered + ind_ordered].astype(int)\ntest[hh_ordered + ind_ordered] = test[hh_ordered + ind_ordered].astype(int);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ae875c33daaeccd4e563091c9e0d639b0c2de57"},"cell_type":"code","source":"train['Target'] = train['Target'].astype(int);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad1f90dded448e0796358d1d951b75dece694aa0"},"cell_type":"markdown","source":"### During prediction, I got an error that some values in validation set are null or infinity. So lets check: <a id=\"anErrorIGot\"></a>"},{"metadata":{"trusted":true,"_uuid":"2867a2c2451d9afa0b7cec7bcc5ce0c95334f84c","scrolled":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() > 0], test.isnull().sum()[test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ce4b9f86a131940c938aac5525bfde640c999ee"},"cell_type":"code","source":"# Only one value in train and 10 in test\ntrain.loc[train['rent-per-adult'].isnull(), 'rent-per-adult'] = train.loc[train['rent-per-adult'].isnull(), 'v2a1'].values[0]\ntest.loc[test['rent-per-adult'].isnull(), 'rent-per-adult'] = test.loc[test['rent-per-adult'].isnull(), 'v2a1'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dace93cd34102c0ee27863092ee2dfe46fb8e51"},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() > 0], test.isnull().sum()[test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd6cfac0230b7d4acdfff28aabd111f4973de7f"},"cell_type":"code","source":"for c in train.columns:\n    if train[c].dtype != 'float64': continue\n    s = np.where(train[c].values >= np.finfo(np.float32).max)\n    if len(s[0])>0:\n        print(c)\n        print(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5874f7ef3874c6a8c0047d197bdb9142b946f8e"},"cell_type":"code","source":"train[train['rent-per-adult'] > np.finfo(np.float32).max][['rent-per-adult']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad3e978a8557443e32af4d3b298b55375a8d44fe"},"cell_type":"markdown","source":"Woah! how did this happen?"},{"metadata":{"trusted":true,"_uuid":"e295ae4f92a1081118abc55e53046b53a1b019f8"},"cell_type":"code","source":"train[train['rent-per-adult'] > np.finfo(np.float32).max][['Id', 'idhogar', 'v2a1', 'hogar_adul', 'age']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e1502a3408e14419892362a605bf7fb0c86e8bf"},"cell_type":"markdown","source":"Thats why, because for these `hogar_adul` is zero."},{"metadata":{"trusted":true,"_uuid":"6aed8b7a1c23faebe93b5e4e46e9cf99dbd8b67e"},"cell_type":"code","source":"train[train['idhogar']==1959][['idhogar', 'v2a1', 'age']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a4d105d0be9a849f113cb024f85b934157b5d86"},"cell_type":"code","source":"train[train['idhogar']==2908][['idhogar', 'v2a1', 'age']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fd7fafb44685323901342f13002223aa405c1d1"},"cell_type":"markdown","source":"So, they are the only ones in their household."},{"metadata":{"trusted":true,"_uuid":"5fee92036ef92ca866fbef5ec843ed25035a2d73"},"cell_type":"code","source":"h_ids = train[train['rent-per-adult'] > np.finfo(np.float32).max]['idhogar'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70250e7ebd83e886da5cc3975e74daf9993bbcb8"},"cell_type":"code","source":"for h_id in h_ids:\n    rent_per_adul = train.loc[(train['idhogar']==h_id), 'v2a1'].values[0] / train.loc[(train['idhogar']==h_id)].shape[0]\n    # Assuming the rent is being divided among them equally\n    train.loc[train['idhogar']==h_id, 'rent-per-adult'] = rent_per_adul\n    train.loc[train['idhogar']==h_id, 'rent-per-adult_sum'] = train.loc[(train['idhogar']==h_id), 'v2a1'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed540f6e843238485dd4304bbc0dd91b6d11eb2"},"cell_type":"markdown","source":"#### Now for test set:"},{"metadata":{"trusted":true,"_uuid":"a9175690f30d74644bd31c40cf735c6a0c4ac36b"},"cell_type":"code","source":"for c in test.columns:\n    if test[c].dtype != 'float64': continue\n    s = np.where(test[c].values >= np.finfo(np.float32).max)\n    if len(s[0])>0:\n        print(c)\n        print(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7f4af58f2d0641903bfc6ea42187d4c220ae02c"},"cell_type":"code","source":"test[test['rent-per-adult'] > np.finfo(np.float32).max][['rent-per-adult']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847f99067c157ce112a0fc8c0e72b555f3905511"},"cell_type":"code","source":"test[test['rent-per-adult'] > np.finfo(np.float32).max][['Id', 'idhogar', 'v2a1', 'hogar_adul', 'age']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1537925a4e0f86c50dabe73075e14e91f3b04e12"},"cell_type":"markdown","source":"Assuming the rent is being divided among them equally."},{"metadata":{"trusted":true,"_uuid":"c5f64f0331e94809dbf82d8286ef9734234b24cb"},"cell_type":"code","source":"h_ids = test[test['rent-per-adult'] > np.finfo(np.float32).max]['idhogar'].unique()\n\nfor h_id in h_ids:\n    rent_per_adul = test.loc[(test['idhogar']==h_id), 'v2a1'].values[0] / test.loc[(test['idhogar']==h_id)].shape[0]\n    # Assuming the rent is being divided among them equally\n    test.loc[test['idhogar']==h_id, 'rent-per-adult'] = rent_per_adul\n    test.loc[test['idhogar']==h_id, 'rent-per-adult_sum'] = test.loc[(test['idhogar']==h_id), 'v2a1'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca7d74f407d35d186b570176794f4932d409b82e"},"cell_type":"markdown","source":"## Making new features: <a id=\"makingNewFeatures\"></a>\n\nMaking new features from the existing features can help our model to learn new trends in data which were not given in dataset before. We take groups of data points from the dataset, and calculate a feature which is true for that group, and we do this for all groups.\n\nHere, we group by `idhogar` (household id) and calculate `count`, `mean`, `max`, `min`, and `sum` of all numeric type features and make new features for all groups, in hopes that now these features will explain more about `Target` value."},{"metadata":{"trusted":true,"_uuid":"f29611e1e99b245c9c26784a923339b3ff2de21d","_kg_hide-input":true},"cell_type":"code","source":"def make_new_features_grouping(df, dtypes, gpby, customAggFunc=None):\n    \"\"\"\n    Make new features aggregating on groups found by \"gbpy\".\n    -----------------------------------------------------------------\n    Parameters:\n        df: Dataset for which new features are to be made\n        dtypes: Data Types of features which will be used to create new features (string, type or array)\n                eg: bool, 'number', 'float' etc\n        gbpy: Feature on which grouping will be done\n        customAggFunc: A custom Aggregation function or a list of such functions\n    Output: \n        Returns Original DataFrame with new features\n    \"\"\"\n    # Grouping\n    if 'Target' in df.columns: numeric_type = df.select_dtypes(dtypes).drop(['Target', 'Id'], axis=1).copy()\n    else: numeric_type = df.select_dtypes(dtypes).drop(['Id'], axis=1).copy()\n    \n    funcs = ['count', 'mean', 'max', 'min', 'sum', 'std', 'var', 'quantile']\n    \n    if customAggFunc is None: new = numeric_type.groupby(gpby).agg(funcs)\n    elif isinstance(customAggFunc, list): new = numeric_type.groupby(gpby).agg(funcs + customAggFunc)\n    else: new = numeric_type.groupby(gpby).agg(funcs + [customAggFunc])\n    \n    # Rename all columns and remove levels\n    columns = []\n    for old_col in new.columns.levels[0]:\n        if old_col != 'idhogar':\n            for new_col in new.columns.levels[1]:\n                columns.append(old_col + '_' + new_col)\n    new.columns = columns\n    \n    return df.merge(new.reset_index(), on=\"idhogar\", how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ce8b3f7f34316d01ceadec514edc274fb213fd9"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25190135d155c3a72789bce53653d7a848194a2d"},"cell_type":"code","source":"%time train = make_new_features_grouping(train, ['number', bool], \"idhogar\")\n%time test = make_new_features_grouping(test, [\"number\", bool], \"idhogar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b0b5413c71ce12ee4b5141ef254d683ba27ea6"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe5487b9768757a8bcd1f3621282b28b6610057c"},"cell_type":"code","source":"train.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b05f1788a6d257a31f846fc66efa158c17020470"},"cell_type":"markdown","source":"**Checking the correlation of all features again: **\n\nMost probably we have created many correlated features in previous step. Infact some features even have 100% correlation. That is they are exactly same."},{"metadata":{"trusted":true,"_uuid":"27bc00d4e783140e063c2b0d60daa2e2e2c72b57"},"cell_type":"code","source":"to_drop = chk_n_remove_corr(train)\nlen(to_drop), 'Target' in to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0161b71ae412f27ff7703ef4a0b25334018b068"},"cell_type":"code","source":"test.drop(to_drop, axis=1, inplace=True)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d819e6376d220f46d2c20cbb707105dd907f93b"},"cell_type":"markdown","source":"## Adding more features using PCA:  <a id=\"pcaFeat\"></a>\n\n\nNow we will add more features by **`PCA`** (Principal Component Analysis) method. It uses `SVD` (Singular Value Decomposition) method to reduce dimentionality from `N` to `n`, where `n < N`. This actually gives us direction of vectors in which data has the most variance with top component having the most variance. All `n` components are orthogonal to each other because the next highest variance direction is always orthogonal to the previous ones.\n*And because they are orthogonal they are not linearly correlated*.\n\n![Source](http://www.nlpca.org/fig_pca_principal_component_analysis.png)"},{"metadata":{"_uuid":"cbe79d0a1644491d8cc281de59ddddaf6f7011bd"},"cell_type":"markdown","source":"And PCA works [better](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) if we standardize our data first. So:"},{"metadata":{"trusted":true,"_uuid":"24995ac548066347df35de262c10258a8750f0d5","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler1 = RobustScaler()\nscaler2 = RobustScaler()\n\nscaled1 = scaler1.fit_transform(train.drop(['Target', 'Id', 'idhogar'], axis=1))\nscaled2 = scaler2.fit_transform(test.drop(['Id', 'idhogar'], axis=1))\n\ncols1 = train.drop(['Target', 'Id', 'idhogar'], axis=1).columns\ncols2 = test.drop(['Id', 'idhogar'], axis=1).columns\n\ntrPCA = pd.DataFrame(scaled1, index=np.arange(train.shape[0]), columns=cols1)\ntsPCA = pd.DataFrame(scaled2, index=np.arange(test.shape[0]), columns=cols2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62362dda24bd7ac3fa2c1c25e5c174ac1078f48c"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=5, svd_solver='full')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09d8b2ad6e11a8cc8726ac734dc936bcacdb16be","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"transformed1 = pca.fit_transform(trPCA)\ntransformed2 = pca.transform(tsPCA)\nfor i in range(5):\n    train[f'PCA{i+1}'] = transformed1[:,i]\n    test[f'PCA{i+1}'] = transformed2[:,i]\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c29c345c135aa8cca05a81eae83e5ec0ea98ab46"},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc70e6f13ab7360a02693ab12071a3865da88af6"},"cell_type":"markdown","source":"## Handling small number of data points for some categories: <a id=\"handlingSmallData\"></a>"},{"metadata":{"_uuid":"7eae30267c0401ab3b3c2c2a67d475379c0b106a"},"cell_type":"markdown","source":"If we have only few data points for some category/categories, our model might not learn about that category that much or anything at all.\n\nHere to bridge the gap we I have taken two approaches:\n1.  Cutting down the category with many data points to make it comparable to others.  [Down Sampling]\n1.  Copying category with small datapoints again and again to make them comparable to others. [Up Sampling] (For more info on such methods see [SMOTE](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html))\n1. Using `sample_weights` or `class_weight` hyperparameter. (We will see these during [RandomForest](#randomForest) and GradientBoosting)"},{"metadata":{"trusted":true,"_uuid":"4db8fca508d3fc42d31c9c1176588161b0666bcd","scrolled":true},"cell_type":"code","source":"train['Target'].value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90eb9190cc56a77601c17d981b710ad5b75a614a"},"cell_type":"markdown","source":"#### 1) Downsampling:\n---"},{"metadata":{"trusted":true,"_uuid":"c0987a7e9d86c5572bbbd78e2026c0b7e30490e4"},"cell_type":"code","source":"train['Target'].value_counts(), 774+1221+1558+1500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2abeb82d540990dbedfc13e0932e56549edbd934","_kg_hide-input":true},"cell_type":"code","source":"rows1 = (train['Target'] == 1)\nrows2 = (train['Target'] == 2)\nrows3 = (train['Target'] == 3)\nrows123 = (rows1 | rows2 | rows3)\nrows4 = (train['Target'] == 4)\nrows123.sum(), rows4.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b02ee16d89e4b4aa979558d74f9532e021ce055","_kg_hide-input":true},"cell_type":"code","source":"# We will take only first count1+count2 rows. Where count1 will go to train and count2 will go to validation set.\ndef train_val_split(rows, tvlen=None, vper=None):\n    \"\"\"\n    Takes in \"row\" array which is location matrix for specific category(say) and \n    divides it into \"train row\" and \"validation row\" of locations. If you only want\n    limited rows from \"rows\" then specify tvlen, which is a tuple of number of rows\n    you want in train and validaion set.\n    -----------------------------------------------------------------------------------\n    Parameters:\n        rows = An array of specific selected rows. (Where ith row is true if selected)\n        tvlen = An array or a tuple of number of elements in train and val. set\n        vper = perecent of elements you want in Validation set (Use it if you want all rows \n                to be divided into test and val sets from the \"rows\" Array or pd.Series)\n    Output:\n        Returns two Arrays or pd.Series of selected rows for train and validation set\n        where ith element is True if that row is selected.\n    \"\"\"\n    if tvlen is not None and vper is None:\n        count1 = tvlen[0]\n        count2 = tvlen[1]\n    elif tvlen is None and vper is not None:\n        c = rows.sum()\n        count1 = int((1-vper)*c)\n        count2 = int(vper*c)\n    else:\n        raise Exception('One of \"tvlen\" or \"vper\" should be given.')\n    \n    rowst, rowsv = rows.copy(), rows.copy()\n    \n    for i in range(len(rows)):\n        \n        # If we have taken count1 rows in training set, put all values equal to False. (after, count1 == 0)\n        if not count1:\n            rowst[i] = False\n            # If we have got count2 rows in validation set, set all others equal to False.\n            if not count2:\n                rowsv[i] = False\n            # Don't do anything to fisrt count2 rows after first count1 rows of training set,\n            # where Target = selected Target and dec. count2\n            count2 -= rowsv[i] # As True = 1 and False = 0\n            continue\n        # Equal to False because they will be in Training set\n        rowsv[i] = False\n        # Don't do anything to fisrt count2 rows, where Target = selected Target, and dec. count1\n        count1 -= rowst[i]\n    \n    return rowst, rowsv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9718e239a0432f7ef951678ddfbcf5cb297de6d"},"cell_type":"markdown","source":"We will take only 1500 rows from `Target` of 4 to make a balance between all categories."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"68dee375a8dc986688100598084e055ef075a436"},"cell_type":"code","source":"rows123t, rows123v = train_val_split(rows123, vper=0.1)\nrows4t, rows4v = train_val_split(rows4, tvlen=(1300, 200))\nrows123t.sum(), rows123v.sum(), rows4t.sum(), rows4v.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c29c4e399b7bb9870baea7c3709c8d307e29df10"},"cell_type":"code","source":"train.drop(['Id', 'idhogar'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd04b12694ddeb01401bdd066cd41be6ae9963b3"},"cell_type":"code","source":"xtrain, xvalid = train.loc[rows123t|rows4t].drop('Target', axis=1).copy(), train.loc[rows123v|rows4v].drop('Target', axis=1).copy()\nytrain, yvalid = train['Target'].loc[rows123t|rows4t].copy(), train['Target'].loc[rows123v|rows4v].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc11a8c14800d21c4d9504c3c39adfa95ea944c"},"cell_type":"code","source":"xtrain.shape, ytrain.shape, xvalid.shape, yvalid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"571c2681f5926a9664898e31b27237fb8bc830fc","scrolled":true},"cell_type":"code","source":"xtrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed2ef8c573935517e020cc68bd74b53a07ffa4e7","scrolled":true},"cell_type":"code","source":"ytrain.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3b4d92700b221bb09ed289291830e6414389b5d"},"cell_type":"code","source":"yvalid.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f8882b2f53da76b198e6d8fd18713fca02d73c0"},"cell_type":"code","source":"ytrain.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840adf28ca28d02dc27188de7b984401f6f9bebd"},"cell_type":"markdown","source":"#### 2) Upsampling:\n---"},{"metadata":{"trusted":true,"_uuid":"8433e408ba2f8cc7c1e1d16e0fc501b9f761e8c5"},"cell_type":"code","source":"train['Target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f25bdefaf7a56dd5617a7711c2c3425df89e8e44"},"cell_type":"code","source":"target1 = train.loc[train['Target']==1].copy()\ntarget2 = train.loc[train['Target']==2].copy()\ntarget3 = train.loc[train['Target']==3].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a59c948b96abb8f987171772cd9e5d6ce138aa5f"},"cell_type":"code","source":"target1 = pd.concat([target1]*8, ignore_index=True).copy(); target1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73624c5a99cb6fc7066a2a1b6b7b986f9be943f5"},"cell_type":"code","source":"target2 = pd.concat([target2]*4, ignore_index=True).copy(); target2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df35b7e598900cd52ad76dab1d87c1942c6fa8e"},"cell_type":"code","source":"target3 = pd.concat([target3]*5, ignore_index=True).copy(); target3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccdbcd61c60fd91ceb67a06ab09cb116c3f85a45"},"cell_type":"code","source":"train2 = train.copy()\ntrain2 = pd.concat([train2, target1, target2, target3], ignore_index=True); train2.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"481a70ae4ab025479b8f395e1e05735c20754aa2"},"cell_type":"markdown","source":"We have any rows with same `Target` value in the end. If we don't want same `Target` value in our validation set. And to do that I found a way [here](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows)."},{"metadata":{"trusted":true,"_uuid":"988833dc70d0673cd44b1f8c567bfe20657251f5"},"cell_type":"code","source":"train2 = train2.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbaf2113bb7749a92efee84f5d0b7e2a0e34cc72"},"cell_type":"code","source":"xtrain2, xvalid2 = train2.iloc[:15000].drop(['Target'], axis=1).copy(), train2.iloc[15000:].drop(['Target'], axis=1).copy()\nytrain2, yvalid2 = train2.iloc[:15000]['Target'].copy(), train2.iloc[15000:]['Target'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c24d3059dfae801717b2cfaf7e53c779ab410670"},"cell_type":"code","source":"xtrain2.shape, ytrain2.shape, xvalid2.shape, yvalid2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc856b3350b1b96c7cb9dfd416027050683cd6a3"},"cell_type":"code","source":"train2['Target'].value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"143743492cfcee3c018ad94e9a2e0a4ac20b83ea"},"cell_type":"markdown","source":" # Random Forest Ensemble: <a id=\"randomForest\"></a>\n ---"},{"metadata":{"trusted":true,"_uuid":"a2db1d2a8d47f13750c9c1767f46430ebdbacd28"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"973c1c0847ae2190574b7b6521956a9429d36a97","_kg_hide-input":true},"cell_type":"code","source":"def print_score(m, trn, val):\n    \"\"\"\n    Print F1 score for training set and validation set, where m is a\n    RandomForestClassifier.\n    ----------------------------------------------------------------------\n    Parameters:\n        m: RandomForestClassifier model\n        trn: tuple or array of Input and Output training data points\n        val: tuple or array of Input and Output validation data points\n    \"\"\"\n    print(\"Train F1score: \", str(F1score(trn[1], m.predict(trn[0]))),\n    \",  Valid. F1score: \", str(F1score(val[1], m.predict(val[0]))))\n    print(\"Train Acc.: \", str(m.score(trn[0], trn[1])),\n    \", Valid. Acc.: \", str(m.score(val[0], val[1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4436a7baa50a11dd9e5fdf530b3a056132157fc0","scrolled":false},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.7, n_jobs=-1)\nm.fit(xtrain, ytrain)\nprint_score(m, (xtrain, ytrain), (xvalid, yvalid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1fd4691e8f82b6933c5c0719aa4312fd3029f63"},"cell_type":"code","source":"# Here I increased min_sample_leaf hyperparameter\nm2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=150, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2, ytrain2)\nprint_score(m2, (xtrain2, ytrain2), (xvalid2, yvalid2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50eafa8a62b0d50757215e85a4d04e639e6158e2"},"cell_type":"markdown","source":"We are getting this much validation score here, because we have many repetitions of rows and it has learned many of them. (i.e. it is overfitted, but we can control that by `max_depth`, `max_leaf_nodes` etc. hyperparameters.)\n\nNow, we will also use the hyperparameter `class_weights` = `'balanced'` which we discussed in [Handling small datapoints](#handlingSmallData) section."},{"metadata":{"trusted":true,"_uuid":"2d5df237127c7d74dd4bb285204cf91836a553e8"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\na, b, c, d = train_test_split(train.drop('Target', axis=1), train['Target'], test_size=0.20,\n                                                    stratify=train['Target'])\nxtrain3, xvalid3, ytrain3, yvalid3 = a.copy(), b.copy(), c.copy(), d.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88da9f6d92b8a6bc397c0e0d399d2a3d4358fecc"},"cell_type":"code","source":"m3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight='balanced')\nm3.fit(xtrain3, ytrain3)\nprint_score(m3, (xtrain3, ytrain3), (xvalid3, yvalid3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e1a7aca6ff897b8e3f2b3042388daec7dc8ebe6"},"cell_type":"markdown","source":"Function to plot stacked bar plot:"},{"metadata":{"trusted":true,"_uuid":"ed94f0ba13317127ee0370e0e6f45708a9552b2b","_kg_hide-input":true},"cell_type":"code","source":"def plot_bar_stacked(y, preds):\n    \"\"\"\n    For plotting predictions, right and wrong. For wrong predictions it will\n    plot stacked bars in diff. colors denoting the class to which it was \n    misplaced.\n    -------------------------------------------------------------------------\n    Parameters:\n        y : actual ouput values\n        preds : predicted output values\n    Output:\n        Plot a stacked graph of count of right and wrong\n    \"\"\"\n    # Output Categories \n    categories = np.array([1, 2, 3, 4])\n    # This will keep count of right predictions and count of wrong prediction in each category\n    counts = [[0, 0, 0, 0], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n    # Calculating wrong and right predictions for all categories\n    for cat in categories:\n        index = (y == cat)\n        right = (preds[index] == y[index]).sum()\n        # For wrong preds:\n        p = preds[index]\n        w1 = (p[(p != y[index])] == 1).sum()\n        w2 = (p[(p != y[index])] == 2).sum()\n        w3 = (p[(p != y[index])] == 3).sum()\n        w4 = (p[(p != y[index])] == 4).sum()\n\n        counts[1][cat-1] = [w1, w2, w3, w4]\n        counts[0][cat-1] = right\n        \n    # Plotting\n    ind = np.arange(4)\n    width = 0.15\n\n    fig, ax = plt.subplots(figsize=(15,10), sharey=True)\n    \n    # Quite a simple way to plot stacked bar plot\n    df = pd.DataFrame(counts[1], index=np.arange(1, 5), columns=['W Pred=1', 'W Pred=2', 'W Pred=3', 'W Pred=4'])\n    df.plot.bar(ax=ax, width=width, stacked=True, colormap='RdYlBu')\n\n    ax.bar(ind+width, counts[0], width=-width, color='green', label='Right')\n\n    ax.set(xticks=ind + width, xticklabels=categories, xlim=[2*width - 1, 4])\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e5981806e8ef627efcd1c5a3efb8a5f3a857e369"},"cell_type":"code","source":"preds = m.predict(xvalid)\nplot_bar_stacked(yvalid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98a1139c18b7a80fbea16628ff1640ccd7c7b230"},"cell_type":"code","source":"# It is not necessary that it will generalize well for test set too. (Though this is \n# giving me better results on public leaderboard.)\npreds = m2.predict(xvalid2)\nplot_bar_stacked(yvalid2, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"39e6a2a76e75e3e3b1c9c6e0f651b1a4ae47c842"},"cell_type":"code","source":"preds = m3.predict(xvalid3)\nplot_bar_stacked(yvalid3, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b10889a8ccbc91bf15d8fa7faaf10dc44e495c75"},"cell_type":"markdown","source":"### Checking the feature inportances: <a id=\"checkFeatureImportance\"></a>"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2f848a4d2e7a0c43cee3fd3f8da91473b4617ea4"},"cell_type":"code","source":"fi = pd.DataFrame({'cols':xtrain.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a3931297f33c1c57942eb7eed7d62159e5ef7b"},"cell_type":"markdown","source":"So, the most important feature is `escolari_mean`, which is average of years of schooling of members per household. Makes sense."},{"metadata":{"trusted":true,"_uuid":"434ebec6f648d5094f1acd214f0896e8f953d698","scrolled":true},"cell_type":"code","source":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dae61a6c6a792488079ca221cecb3f717d3abe0"},"cell_type":"markdown","source":"Wow, only a few features can predict quite accurately. (every point shows its contribution to the model prediction.)"},{"metadata":{"trusted":true,"_uuid":"3f000a30a3255046a30ea13c6d570214e9e3fe67"},"cell_type":"code","source":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6ab8595d75755b0222d0bf27fe012f4b8e9b9e"},"cell_type":"code","source":"xtrain_new, xvalid_new = xtrain[to_keep].copy(), xvalid[to_keep].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfa90a07a9c7d8806eb56f45a91e3d103851b245"},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nm.fit(xtrain_new, ytrain)\nprint_score(m, (xtrain_new, ytrain), (xvalid_new, yvalid))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1680f8ac77f3c9154eb580566194e2c46980ec6d"},"cell_type":"markdown","source":"---\nNow according to the second RandomForestClassifier `m2`:"},{"metadata":{"trusted":true,"_uuid":"958a28378ae0899c7407c4d96aa01309ab613f83"},"cell_type":"code","source":"fi = pd.DataFrame({'cols':xtrain2.columns, 'imp':m2.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2542f8485bb6219a17f317d96632ec83f9f72a4d"},"cell_type":"markdown","source":"\nHere we have somewhat different features' importances, but on the top is still `escolari_mean`."},{"metadata":{"trusted":true,"_uuid":"d2979704db66868c38a2768c90ed88c92b615d5f"},"cell_type":"code","source":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fdcfeb05d6d791d2a3ce002d1b8aaf5fce5782a"},"cell_type":"code","source":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"160de168d4237d7fea628b95d33da4d78a53750c"},"cell_type":"code","source":"xtrain2_new, xvalid2_new = xtrain2[to_keep].copy(), xvalid2[to_keep].copy()\nm2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=150, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2_new, ytrain2)\nprint_score(m2, (xtrain2_new, ytrain2), (xvalid2_new, yvalid2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a689126ed95ac391594e9050b376c25c1d3d197"},"cell_type":"markdown","source":"---\nNow according to the third RandomForestClassifier `m3`:"},{"metadata":{"trusted":true,"_uuid":"5f796d4f99b738fc4f9eb56a0fff27701bb39492"},"cell_type":"code","source":"fi = pd.DataFrame({'cols':xtrain3.columns, 'imp':m3.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73dd17d6195462d47190773d6d6836c02e8bb8fd","scrolled":true},"cell_type":"code","source":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f62ca5f3643f4617ad8a3b835070cd187dfb5587"},"cell_type":"code","source":"xtrain3_new, xvalid3_new = xtrain3[to_keep].copy(), xvalid3[to_keep].copy()\nm3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight=\"balanced\")\nm3.fit(xtrain3_new, ytrain3)\nprint_score(m3, (xtrain3_new, ytrain3), (xvalid3_new, yvalid3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a521ce161bc3c59aec2d83fd9f8d2b99a429e929"},"cell_type":"markdown","source":"### Removing redundant features: <a id=\"removingRedundantFeatures\"></a>"},{"metadata":{"_uuid":"14d6d102cead6a30ee5365d2f2b6d147f949f04b"},"cell_type":"markdown","source":"For this we will use Dendrogram plot to see closely related features."},{"metadata":{"trusted":true,"_uuid":"a12b3c503b936f87d7aff69912aee588aee65000"},"cell_type":"code","source":"import scipy\nfrom scipy.cluster import hierarchy as hc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"233fcc171d6b52a9cee3b2b4c1509d5d4c2571a4","scrolled":false},"cell_type":"code","source":"corr = np.round(scipy.stats.spearmanr(xtrain_new).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,15))\ndendrogram = hc.dendrogram(z, labels=xtrain_new.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ffd69b786d9042653f208c90dafe7ff42b28316"},"cell_type":"markdown","source":"There are 3 groups which are quite closer to each other than others. Lets remove some features from them from model one by one and lets see what happens to our `F1score`."},{"metadata":{"trusted":true,"_uuid":"e86df99f83f815f600358f94874c3a662f33d3fb","scrolled":true},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=50, min_samples_leaf=25, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(xtrain_new, ytrain)\nprint(m.oob_score_)\ncheck_with = m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4a55da4deb760386fbd8b584b73821b3ebcba27"},"cell_type":"code","source":"#scores = []\n#for col in ['v2a1_sum', 'v2a1']:\n#    m = RandomForestClassifier(n_estimators=50, min_samples_leaf=25, max_features=0.6, n_jobs=-1, oob_score=True)\n#    m.fit(xtrain_new.drop(col, axis=1), ytrain)\n#    scores.append(m.oob_score_)\n#    print(m.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d370dd661f1fca3463ede78909b0da26b9c1e665"},"cell_type":"code","source":"#to_drop = []\n#for i, col in enumerate(['v2a1_sum', 'v2a1']):\n#    if scores[i] > check_with: to_drop.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0cdfac57b6b924d30e38e4e25abf4a6c3f4adc8"},"cell_type":"code","source":"#xtrain_new, xvalid_new = xtrain_new.drop(to_drop, axis=1), xvalid_new.drop(to_drop, axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87a927b090dc378ea11ff1a8bd83b63851ce3643"},"cell_type":"markdown","source":"### RandomForest with top features:"},{"metadata":{"trusted":true,"_uuid":"0c00dbc1e5ef13f6f0eba5d09d4eb9762b795573"},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm.fit(xtrain_new, ytrain)\nprint_score(m, (xtrain_new, ytrain), (xvalid_new, yvalid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a9c1b8a40c4b976a4e7dd4e907925510bc7c676","scrolled":false},"cell_type":"code","source":"preds = m.predict(xvalid_new)\nplot_bar_stacked(yvalid, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3915bae3b4804732af4c3b6cf70f898c1aeecff"},"cell_type":"markdown","source":"### Random Forest with top features from Upsampled Dataset:"},{"metadata":{"trusted":true,"_uuid":"97bdbe8b800b4abff117d28ea99bfd80ae544e21"},"cell_type":"code","source":"pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b76541d621d9ecbedc8d3ccd9fa7760d8afcc6d"},"cell_type":"markdown","source":"# Gradient Boosting: <a id=\"gradBoosting\"></a>"},{"metadata":{"trusted":true,"_uuid":"df73dfa4f92c71cf3e04fdbe24469afb7ae858d2","scrolled":true},"cell_type":"code","source":"# from xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1037872554f0e9b3f3a0ab4ee2739cefb56dbc1c"},"cell_type":"code","source":"# It needs labels from 0 to n-1, where n is number of classes\n#ytrain3 = ytrain3-1\n#yvalid3 = yvalid3-1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ce184d4360df90d431fa1cb5a88948d21423d3b"},"cell_type":"markdown","source":"Best hyperparameter search method and LR reduction callback are taken from [here](https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro)."},{"metadata":{"trusted":true,"_uuid":"bfb0fd6f9c6a0c9372400818ffceab4dd5ec35d1"},"cell_type":"code","source":"# For decreasing learning rate of model with time\ndef learningRateAnnl(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0699a36aee97a35857a893ccd3c09a09e5afbf0a"},"cell_type":"code","source":"fit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(xvalid3, yvalid3.copy()-1)],\n            'eval_names': ['valid'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learningRateAnnl)],\n            'verbose': False,\n            'categorical_feature': 'auto'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9b9f8f4a8da3b2677c0214e2d0b40c324a81766"},"cell_type":"code","source":"from scipy.stats import randint\nfrom scipy.stats import uniform\nparam_test ={'num_leaves': randint(12, 20), \n             'min_child_samples': randint(40, 120), \n             #'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': uniform(loc=0.75, scale=0.20), \n             'colsample_bytree': uniform(loc=0.8, scale=0.15),\n             #'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n             #'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n             #'boosting': ['dart', 'goss', 'gbdt']\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf6e12a2dfb1e310adb60537a56a01a2cd92bbd3"},"cell_type":"code","source":"maxHPs = 400\nclassifier = lgb.LGBMClassifier(learning_rate=0.05, n_jobs=-1, n_estimators=500, objective='multiclass')\n\n#rs = RandomizedSearchCV(estimator= classifier, param_distributions=param_test, n_iter=maxHPs,\n#                        scoring='f1_macro', cv=5, refit=True, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"07cbe8b5f1221c80514bc5658e290817f367eea5"},"cell_type":"code","source":"#_ = rs.fit(xtrain3, (ytrain3).copy()-1, **fit_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035c6bb7cbf8d4da173e05b4e09a59b0caa1aab0"},"cell_type":"code","source":"#opt_parameters = rs.best_params_; opt_parameters\n# op_parameters found by above method (Random Search)\nopt_parameters = {'colsample_bytree': 0.8755593602517565,\n 'min_child_samples': 51,\n 'num_leaves': 19,\n 'subsample': 0.9437154452377117}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c0292d1362f69419669db7ba9deb6089d3cafe","scrolled":true},"cell_type":"code","source":"classifier = lgb.LGBMClassifier(**classifier.get_params())\nclassifier.set_params(**opt_parameters)\n\nfit_params['verbose'] = 200\n_ = classifier.fit(xtrain3, (ytrain3).copy() -1, **fit_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f16d9c79709f83f7b97b034ef9977da985edb55a"},"cell_type":"markdown","source":"### K-Fold Fitting:"},{"metadata":{"trusted":true,"_uuid":"ec249c1827e57511549128961779d467c77b080d","scrolled":true},"cell_type":"code","source":"kfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\nfor trn_idx, tst_idx in kf.split(train.drop(['Target'], axis=1), train['Target']):\n    xtr, xval = train.drop(['Target'], axis=1).iloc[trn_idx], train.drop(['Target'], axis=1).iloc[tst_idx]\n    ytr, yval = train['Target'].iloc[trn_idx].copy() -1, train['Target'].iloc[tst_idx].copy() -1\n    \n    classifier.fit(xtr, ytr, eval_set=[(xval, yval)], \n            early_stopping_rounds=300, verbose=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de14d53fb7af05304fd967c22666d0f5ad2f9776","scrolled":true},"cell_type":"code","source":"preds = classifier.predict(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cc3a5bbead447fb2151fc37a71e3a2ec6ccb0ff"},"cell_type":"code","source":"((preds+1) == yvalid).sum()/len(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb4daeb56850b14acc6a4cd8e84a06192c49412d","scrolled":false},"cell_type":"code","source":"plot_bar_stacked(yvalid, preds+1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d55f8c391ef3d302f73d7ab5bf7d2fd494795f1"},"cell_type":"markdown","source":"### Gradient boosting with copied rows in training set:\n\nWe will use the same hyper-parameters that we discovered above:"},{"metadata":{"trusted":true,"_uuid":"ca942830a1c347cc5bb0695b67f6c72b077fee25","scrolled":false},"cell_type":"code","source":"classifier2 = lgb.LGBMClassifier(**classifier.get_params())\n\nkfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\nfor trn_idx, tst_idx in kf.split(train2.drop(['Target'], axis=1), train2['Target']):\n    xtr, xval = train2.drop(['Target'], axis=1).iloc[trn_idx].copy(), train2.drop(['Target'], axis=1).iloc[tst_idx].copy()\n    ytr, yval = train2['Target'].iloc[trn_idx].copy()-1, train2['Target'].iloc[tst_idx].copy() -1\n    \n    classifier2.fit(xtr, ytr, eval_set=[(xval, yval)], \n            early_stopping_rounds=300, verbose=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"63ce888d4cf14bbefa983336121e6dae53fc2152"},"cell_type":"code","source":"# Won't necessarily generalize well.\npreds = classifier2.predict(xvalid2)\nplot_bar_stacked(yvalid2, preds+1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"321c540eb4da09d9a915904d1246a3a02f4f355b"},"cell_type":"markdown","source":"### Gradient Boosting with top features:"},{"metadata":{"trusted":true,"_uuid":"aa578cff312840fd939bb4a2bc6d71eb37065cad"},"cell_type":"code","source":"pass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5219269c4ad2279e285b67b84ff2a197b8a378e"},"cell_type":"markdown","source":"# Deep Neural Network: <a id=\"dnn\"></a>\n\nFor introduction on how to make custom Neural Network with PyTorch look at my work: [Training your own CNN using PyTorch](https://www.kaggle.com/puneetgrover/training-your-own-cnn-using-pytorch)"},{"metadata":{"trusted":true,"_uuid":"ec04a554102152806bd155426db7d553b2be52ef"},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.autograd.variable import Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26cc6f715459a85e80dfdd8c8c61e605dacb7fee"},"cell_type":"code","source":"Ttrain = TensorDataset(torch.DoubleTensor(np.array(xtrain.values, dtype=\"float32\")), #.cuda\n                       torch.LongTensor(np.array(ytrain.values, dtype=\"float32\")-1)) #.cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8b7da14b76080871e3ee82fc273daf1192a76a3"},"cell_type":"code","source":"trainLoader = DataLoader(Ttrain, batch_size = 20, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd3eef3c5b4dac34179dc6e464217fd57166434","_kg_hide-input":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, n_cols):\n        super(Net, self).__init__()\n        \n        self.first = nn.Sequential(\n            nn.BatchNorm1d(n_cols),\n            nn.Linear(n_cols, 10),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            #nn.Linear(80, 80),\n            #nn.BatchNorm1d(80),\n            #nn.ReLU(),\n            #nn.Linear(80, 80),\n            #nn.BatchNorm1d(80),\n            #nn.ReLU(),\n            #nn.Dropout(p=0.25),\n            #nn.Linear(80, 20),\n            #nn.BatchNorm1d(20),\n            #nn.ReLU(),\n            #nn.Linear(50, 20),\n            #nn.ReLU(),\n            nn.Linear(10, 4),\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x):\n        return self.first(x)\nnet = Net(len(xtrain.columns)).double() #.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"441f8fd1f532dc299993888223bd809b2ea3fd1c"},"cell_type":"code","source":"loss = nn.CrossEntropyLoss()\nmetrics = [F1score]\n#opt = optim.SGD(net.parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)\nopt = optim.Adam(net.parameters(), weight_decay=1e-3)\n#opt = optim.RMSprop(net.parameters(), momentum=0.9, weight_decay=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a573116e79ede0e72f5b432c8ca62bc371057bb","_kg_hide-input":true},"cell_type":"code","source":"def fit(model, lr, xtr, ytr, xvl, yvl, train_dl, n_epochs, loss, opt, metrics, annln=False, mult_dec=True):\n    \"\"\"\n    Function to fit the model to training set and print F1 scores for both training set\n    and validation set.\n    -------------------------------------------------------------------------------------\n    Parameters:\n        model: Model (Neural Network) to which Training set will fit\n        lr: Learning rate (initil learning rate if annln=True)\n        xtr: Input train array (for getting F1score on whole array)\n        ytr: Output train array (for getting F1score on whole array)\n        xvl: Input validation array (for val. F1score)\n        yvl: Output validation array (for val. F1score)\n        train_dl: Train DataLoader which loads training data in batches (should give Tensors as output)\n        n_epochs: number of epochs\n        loss: Loss function to calculate and backpropagate loss (eg: CrossEntropy)\n        opt: Optimizer, to update weights (eg: RMSprop)\n        metrics: Function to calculate score of model (eg: accuracy, F1 score)\n        annln: (default=False) If to use LRAnnealing or not\n        mult_dec: (default=True) If to dec. max Learning rate on every cosine cycle\n    \"\"\"\n    if(annln): annl = lrAnnealing(lr, 40, 449, mult_dec)  # itr_per_epoch = len(xtrain) // batch_size\n    for epoch in range(n_epochs):\n        tl = iter(train_dl)\n        length = len(train_dl)\n        \n        for t in range(length):\n            xt, yt = next(tl)\n\n            #y_pred = model(Variable(xt).cuda())\n            #l = loss(y_pred, Variable(yt).cuda())\n            y_pred = model(Variable(xt))\n            l = loss(y_pred, Variable(yt))\n            if(annln): annl(opt)\n            opt.zero_grad()\n            l.backward()\n            opt.step()\n        \n        val_score = get_f1score(model, \n                                torch.DoubleTensor(np.array(xvl, dtype = \"float32\")), #.cuda\n                                torch.LongTensor(np.array(yvl, dtype = \"float32\")-1)) #.cuda\n        trn_score = get_f1score(model, \n                                torch.DoubleTensor(np.array(xtr, dtype = \"float32\")), #.cuda\n                                torch.LongTensor(np.array(ytr, dtype = \"float32\")-1)) #.cuda\n        \n        if (epoch+1)%5 == 0:\n            print(\"Epoch \" + str(epoch) + \"::\"\n                + \"  trnF1score: \" + str(trn_score)\n                +\", valF1score: \" + str(val_score))\n            \ndef get_f1score(model, x, y):\n    \"\"\"\n    To get F1score of predictions from Neural Network.\n    -----------------------------------------------------------------\n    Parameters:\n        model: Neural Network Model\n        x: Input Values to be sent to model() function to get predictions\n        y: Output Values to be checked with predictions\n    Output:\n        Return F1 score of predictions \n    \"\"\"\n    pred = model(Variable(x).contiguous())\n    ypreds = np.argmax(pred.contiguous().data.numpy(), axis=1) #.cpu()\n    yactuals = y.contiguous().numpy() #.cpu()\n    return F1score(yactuals, ypreds)\n\ndef set_lr(opt, lr):\n    \"\"\"\n    Function to set lr for optimizer in every layer.\n    ------------------------------------------------------------------\n    Parameters:\n        opt: optimizer used in neural network\n        lr: New Learning rate to be set in each layer\n    \"\"\"\n    for pg in opt.param_groups: pg['lr'] = lr\n\nclass lrAnnealing():\n    def __init__(self, ini_lr, epochs, itr_per_epoch, mult_dec):\n        \"\"\"\n        Class to Anneal learning rate with warm restarts with time. It decreases \n        learning rate as multiple cosine waves with dec. amplitudes.1e-10 is taken \n        as zero. (The lower point for cosine)\n        ---------------------------------------------------------------------------\n        Parameters:\n            ini_lr: Initial learning rate\n            epochs: Number of epochs\n            itr_per_epoch: iterations per epoch\n            mult_dec: T/F, If to use Annealing with warm restarts or hard\n        \"\"\"\n        self.epochs = epochs\n        self.ipe = itr_per_epoch\n        self.m_dec = mult_dec\n        self.ppw = (self.ipe * self.epochs) // 4    # Points per wave of cosine (For 4 waves per fit method)\n        self.count = 0\n        self.lr = ini_lr\n        self.values = np.cos(np.linspace(np.arccos(self.lr), np.arccos(1e-10), self.ppw))\n        self.mult = 1\n    def __call__(self, opt):\n        \"\"\"\n            opt: optimizer of which lr is to set\n        \"\"\"\n        self.count += 1\n        set_lr(opt, self.values[self.count-1]*self.mult)\n        if self.count == len(self.values):\n            self.count = 0\n            if(self.m_dec): self.mult /= 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dba02f9497df3a267294def579c4c50ccf5eb9f9"},"cell_type":"code","source":"from sklearn.exceptions import UndefinedMetricWarning\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0c5ec4f9d8dca851653b73418a248245700f9af3"},"cell_type":"code","source":"%time fit(net, 1e-2, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8ade4d8b6bdf85f92da355312fc50b06a752aed","scrolled":false},"cell_type":"code","source":"%time fit(net, 1e-3, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4c6dc61df29b4b0a84f0e723b4ec9c7218353085"},"cell_type":"code","source":"# Getting max F1 score of .85 in training and .85 for test\n# But not giving good results on public leaderboard (0.303)\n%time fit(net, 1e-4, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"242b0588e86a1e54f0332e95155e2951fe175708"},"cell_type":"code","source":"%time fit(net, 1e-5, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5997445c24f41decfee6c0f4c736a473e67c456"},"cell_type":"code","source":"%time fit(net, 1e-6, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e997e7f25ccf28610dc06d17fe5540e63d68d2b2"},"cell_type":"code","source":"# Plotting the result:\nypreds = net(Variable(torch.DoubleTensor(np.array(xvalid, dtype=\"float32\"))).contiguous()).data.numpy().argmax(1)+1 #.cuda, .cpu()\nplot_bar_stacked(yvalid, ypreds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5073cee6530a63a640086c508f96c3028d941f1e"},"cell_type":"markdown","source":"Though it giving me acceptable score here, but on Public Leaderboard this giving me poor results.\nSo, neural network is not giving good results for the optimizers and hyperparameters I tried.\n\nI am still trying to tweak it a bit. I have made it less deeper now."},{"metadata":{"_uuid":"e0d839aed74ebb7fd70e4c19200125636e7a2d02"},"cell_type":"markdown","source":"### Now fitting to Upsampled dataset:"},{"metadata":{"trusted":true,"_uuid":"be4ea03b3d853a92dceac46931c80f52e80673c6"},"cell_type":"code","source":"#Ttrain = TensorDataset(torch.DoubleTensor(np.array(xtrain2.values, dtype=\"float32\")), #.cuda\n#                       torch.LongTensor(np.array(ytrain2.values, dtype=\"float32\")-1)) #.cuda\n#trainLoader = DataLoader(Ttrain, batch_size = 20, shuffle=True)\n#net2 = Net(len(xtrain2.columns)).double() #.cuda()\n#loss = nn.CrossEntropyLoss()\n#metrics = [F1score]\n#opt = optim.Adam(net.parameters(), weight_decay=1e-3)\n#opt = optim.SGD(net.parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)\n#opt = optim.RMSprop(net.parameters(), momentum=0.9, weight_decay=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a055a3cec6ad27f2a356125cf1d597a3ec25a0"},"cell_type":"code","source":"#ytrain2.unique(), yvalid2.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"542cb214ca90e06da484cb9faf8d72fcbcec977f","scrolled":true},"cell_type":"code","source":"#%time fit(net2, 1e-2, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d14781c1abd7edd3d5c0d9911bdbd840ba24671","scrolled":true},"cell_type":"code","source":"#%time fit(net2, 1e-3, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e20acff4bc65328463de07f12e761b0bf5c1c1a"},"cell_type":"code","source":"#%time fit(net, 1e-4, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"287e0117e778bf512378bd749f983a6c866ca9dd"},"cell_type":"code","source":"#%time fit(net, 1e-5, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c488ec0487f1187757d04de7b3cebfd9a5c969e7","scrolled":false},"cell_type":"code","source":"# Plotting the result:\n#ypreds = net2(Variable(torch.DoubleTensor(np.array(xvalid2, dtype=\"float32\"))).contiguous()).data.numpy().argmax(1)+1 #.cuda, .cuda()\n#plot_bar_stacked(yvalid2, ypreds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33b770f4f572333b0c06a7187b399253f0aa8c45"},"cell_type":"markdown","source":"Our Upsampled dataset, is giving worse results than our first neural network. (Opposite of what we saw in case of RandomForeset and GradientBoosting)"},{"metadata":{"_uuid":"652792f016eee204b899ec8d22e8cb2e2a29de4f"},"cell_type":"markdown","source":"# Comparison of Different models: <a id=\"compModels\"></a>\n\n---\n\n*We had very small dataset here. We had total 9557 rows and 4 categories to predict from. Out of the total 9557 rows about 6000 belonged to one category only. Thats a huge mismatch in quantity. And that was the main challenge. But still F1score of about 0.40 was achievable.* \n\n\\*\\* = Kaggle Takes only 5 submissions per day.\n\n-- = Not Implemented Yet\n\n| Models \\ Data Type | Downsampled Data | Upsampled Data | FeatEng Data | Original Data | If class_weight = 'balanced' for original data |\n|-|:-:|:-:|:-:|:-:|:-:|\n| Random Forest | 0.99, 0.60, 0.346,  | 0.94, 0.92, **0.420** | 0.99, 0.59, \\*\\*  |  0.80, 0.74, **0.414**  |  Yes  |\n| LightGBM 5-Fold| --  |  --, 0.99, **0421**  |  --  |  --, 0.96, 0.387 (0.406)  |\n| Neural Network (4 Hidden Layers) |  0.88, 0.40, 0.342 |  0.90, 0.50, 0.295   |  --   |   --   |\n| Neural Network (2 hidden Layers) | 0.90, 0.87, 0.303 | -- | -- | -- |\n\n.\n\nFormat : TrainF1Score, ValF1Score, PublicF1Score\n\n---"},{"metadata":{"_uuid":"cc467764af633a0332830588329793b3a975de60"},"cell_type":"markdown","source":"# Make Submission File: <a id=\"makingSubmission\"></a>"},{"metadata":{"_uuid":"885dd7e5c8daf3ceb5c21a3a3b6d0b20aaca0f40"},"cell_type":"markdown","source":"* **Random Forest** with Downsampled data:"},{"metadata":{"trusted":true,"_uuid":"701c02fee9660516539396a117db4a3ba3d2b31e","_kg_hide-input":true},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm.fit(xtrain, ytrain)\nprint_score(m, (xtrain, ytrain), (xvalid, yvalid))\n\nto_pred = test[xtrain.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m.predict(to_pred)], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b61580938738d3a3fe9885a69979d9121d0734d"},"cell_type":"markdown","source":"* **Random Forest** with Upsampled data:"},{"metadata":{"trusted":true,"_uuid":"44c01cb479ad09bfc49fbb9e67bea952003330e1","_kg_hide-input":true},"cell_type":"code","source":"m2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2, ytrain2)\nto_pred = test[xtrain2.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m2.predict(to_pred)], axis=-1)\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4660800d023799ce0af4dd16eda565675230e1f9"},"cell_type":"markdown","source":"* **Random Forest** with original Dataset:"},{"metadata":{"trusted":true,"_uuid":"37776c0b540ae65dcf3b7d31c7d0d92aa1eb390c","_kg_hide-input":true},"cell_type":"code","source":"# Uncomment it to get output for 3rd RandomForest Classifier (Ready for submission)\nm3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight='balanced')\nm3.fit(xtrain3, ytrain3)\nto_pred = test[xtrain3.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m3.predict(to_pred)], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76711b4807d965a7b3070edcc3d91c4b95794a69"},"cell_type":"markdown","source":"* **Neural Network** with Downsampled data:"},{"metadata":{"trusted":true,"_uuid":"e82bae193c3fdc2c874a3bd47c51e82d50004ce6","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# For output of net (DNN-1 with all features)\npred = net(Variable(torch.DoubleTensor(np.array(test.drop(['Id', 'idhogar'], axis=1).values, dtype=\"float32\"))).contiguous()) # .cuda\nypreds = pred.contiguous().data.numpy().argmax(1) + 1 # .cpu()\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1); npArray[0]\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission4.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2452f5d5aea2fc92e9f6e93e39dc51625808532a"},"cell_type":"markdown","source":"* **XGBoost** with Downsampled data:"},{"metadata":{"trusted":true,"_uuid":"2c1690b07557723a5afeb5fcaca235a94d355681","_kg_hide-input":true},"cell_type":"code","source":"ypreds = classifier.predict(test[xtrain.columns]) + 1\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission5.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c90a111d5fc7e59e5135b5de30ce179d147d9d"},"cell_type":"markdown","source":"* **XGBoost** with Upsampled data:"},{"metadata":{"trusted":true,"_uuid":"20c71a11530e0e223ad761319573e685a4cfc4c1","_kg_hide-input":true},"cell_type":"code","source":"ypreds = classifier2.predict(test[xtrain2.columns]) + 1\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1)\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission6.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}