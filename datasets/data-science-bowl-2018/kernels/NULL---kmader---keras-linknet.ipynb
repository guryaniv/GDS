{"cells":[{"metadata":{"_cell_guid":"e61ef2d8-f315-4f7f-b07e-1de0f4e8441a","_uuid":"1677fddbb95f7545b6540e9201f3339a0fdbfc5d"},"cell_type":"markdown","source":"# Intro\nBasically a fork of the U-Net starter where U-Net is replaced with LinkNet. The success of [LinkNet for Carvana](http://slides.com/vladimiriglovikov/kaggle-deep-learning-to-create-a-model-for-binary-segmentation-of-car-images#/) and presentation by Vladimir Iglovikov made me think it might be worth a try for segmenting nuclei. \n\nThe architecture used is the so-called [LinkNet](https://arxiv.org/abs/1707.03718), which is very common for image segmentation problems such as this. They also have a tendency to work quite well even on small datasets.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5c38504af3a84bee68c66d3cde74443c58df422f","_cell_guid":"c332549b-8d23-4bb5-8497-e7a8eb8b21d2","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf\n\n# Set some parameters\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nTRAIN_PATH = '../input/stage1_train/'\nTEST_PATH = '../input/stage1_test/'\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3fb9d6530fbbd0e22e41fc4fd9fd9fc0bff027ac","_cell_guid":"ffa0caf0-2d1b-40f2-865b-8e6db88526b6","trusted":true},"cell_type":"code","source":"# Get train and test IDs\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\ntest_ids = next(os.walk(TEST_PATH))[1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59c4a25d-645f-4b74-9c53-145ac78cc481","_uuid":"875af74f980236825de3a650825b46e25632422c"},"cell_type":"markdown","source":"# Get the data\nLet's first import all the images and associated masks. I downsample both the training and test images to keep things light and manageable, but we need to keep a record of the original sizes of the test images to upsample our predicted masks and create correct run-length encodings later on. There are definitely better ways to handle this, but it works fine for now!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ca0cc34b-c26f-41ee-88d7-975aebdb634e","_uuid":"9e389ba8bdb5b6fc03b231b6a6c84a8bde634053","trusted":true},"cell_type":"code","source":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '/masks/'))[2]:\n        mask_ = imread(path + '/masks/' + mask_file)\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n                                      preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)\n    Y_train[n] = mask\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Getting and resizing test images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0523b03-1fc5-4505-a1b8-eb35ee617c8a","_uuid":"d4f8327802a1ec6139ce0585953986272ba62ce1"},"cell_type":"markdown","source":"Let's see if things look all right by drawing some random images and their associated masks.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"88829b53-50ce-45d9-9540-77dd7384ad4c","_uuid":"283af26f0860b7069bdfd133c746e5d20971542c","trusted":true},"cell_type":"code","source":"# Check if training data looks all right\nix = random.randint(0, len(train_ids))\nimshow(X_train[ix])\nplt.show()\nimshow(np.squeeze(Y_train[ix]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2574ffe9-b911-4bfd-a00f-9ba5c25f45de","_uuid":"938648da705689a0f940ff462477c801db3f0737"},"cell_type":"markdown","source":"Seems good!\n\n# Create our Keras metric\n\nNow we try to define the *mean average precision at different intersection over union (IoU) thresholds* metric in Keras. TensorFlow has a mean IoU metric, but it doesn't have any native support for the mean over multiple thresholds, so I tried to implement this. **I'm by no means certain that this implementation is correct, though!** Any assistance in verifying this would be most welcome! \n\n*Update: This implementation is most definitely not correct due to the very large discrepancy between the results reported here and the LB results. It also seems to just increase over time no matter what when you train ... *","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"5abd38950ae99b60f8afec7656eb654a48d449fe","_cell_guid":"c1df6f3a-d58f-434b-9216-ef7be38637d4","trusted":true},"cell_type":"code","source":"# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3b9f148-1dba-4b6a-981b-6cdbf394fc3c","_uuid":"986488a4c5223576be370e224426a30431911eb2"},"cell_type":"markdown","source":"# Build and train our neural network\nNext we build our model, loosely based on [LinkNet](https://arxiv.org/abs/1707.03718)","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"c7443a2bbdc7bb0588dacd9db665e2162578f6d3","_cell_guid":"366f302a-a591-4524-a84e-64b1b6a6a9db","trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, Deconv2D, MaxPool2D, concatenate, AvgPool2D\nnetwork_mode = 'bn'\ns_c2 = lambda fc, k, s = 1, activation='elu', **kwargs: Conv2D(fc, kernel_size = (k,k), strides= (s,s),\n                                       padding = 'same', activation = activation,\n                                       **kwargs)\ns_d2 = lambda fc, k, s = 1, activation='elu', **kwargs: Deconv2D(fc, kernel_size=(k,k), strides=(s,s), \n                                                       padding = 'same', activation=activation,\n                                                       **kwargs)\nif network_mode == 'bn':\n    from keras.layers import BatchNormalization, Activation\n    c2 = lambda fc, k, s = 1, **kwargs: lambda x: Activation('elu')(BatchNormalization()(\n        Conv2D(fc, kernel_size = (k,k), strides= (s,s),\n               padding = 'same', activation = 'linear', **kwargs)(x)))\n\n    d2 = lambda fc, k, s = 1, **kwargs: lambda x: Activation('elu')(BatchNormalization()(\n        Deconv2D(fc, kernel_size=(k,k), strides=(s,s), \n                 padding = 'same', activation='linear', **kwargs)(x)))\nelse:\n    c2 = s_c2\n    d2 = s_d2","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0aa97d66c29f45dfac9b0f45fcf74ba0e778ba5d","_cell_guid":"c1dbc57c-b497-4ccb-b077-2053203ab7ed","trusted":true},"cell_type":"code","source":"# Build U-Net model\nstart_in = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), name = 'Input')\nstart_scale = Lambda(lambda x: x / 255) (start_in)\n# pre-processing\nin_filt = c2(64, 7, 2)(start_scale)\nin_mp = MaxPool2D((3,3), strides = (2,2), padding = 'same')(in_filt)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9b7b7f3bfd750ea77c0392b9c3ec187a3f862244","_cell_guid":"9bc51273-4490-477b-9b2e-00891b3fc6da","trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.regularizers import l2\nfrom keras.layers import add\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[1] / residual_shape[1]))\n    stride_height = int(round(input_shape[2] / residual_shape[2]))\n    equal_channels = input_shape[3] == residual_shape[3]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[3],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\ndef enc_block(m, n):\n    def block_func(x):\n        cx = c2(n, 3)(c2(n, 3, 2)(x))\n        cs1 = concatenate([AvgPool2D((2,2))(x), \n                           cx])\n        cs2 = c2(n, 3)(c2(n, 3)(cs1))\n        return concatenate([cs2, cs1])\n    return block_func\ndef dec_block(m, n):\n    def block_func(x):\n        cx1 = c2(m//4, 1)(x)\n        cx2 = d2(m//4, 3, 2)(cx1)\n        return Dropout(0.1)(c2(n, 1)(cx2))\n    return block_func","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"621f2d7c7793583aa5d0c53855a4ceec6f8713bb","_cell_guid":"bcb08f5d-c0cd-4c13-846c-9a43447c8627","trusted":true},"cell_type":"code","source":"enc1 = enc_block(64, 64)(in_mp)\nenc2 = enc_block(64, 128)(enc1)\n\ndec2 = dec_block(64, 128)(enc2)\ndec2_cat = _shortcut(enc1, dec2)\ndec1 = dec_block(64, 64)(dec2_cat)\n\nlast_out = _shortcut(dec1, in_mp)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b08ce9de6228ab9d59a833182cd1890a9d3696a9","_cell_guid":"25b071dc-800c-4e56-a097-1903cd532ac9","trusted":true},"cell_type":"code","source":"# post-processing\nout_upconv = d2(32, 3, 2)(last_out)\nout_conv = c2(32, 3)(out_upconv)\nout = s_d2(1, 2, 2, activation = 'sigmoid')(out_conv)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7dd87bf02f2c37f6a50d6353d532d0327d125653","_cell_guid":"7baf55fd-c4b5-4a5e-9b99-6e61379670c6","trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.metrics import binary_crossentropy\nsmooth = 1.\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\ndef bce_dice(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred)-K.log(dice_coef(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3aed6f9286c32b5739a47960de1f35476d147cda","_cell_guid":"3122fbd2-9f8e-4bfc-ba40-0b163e4862e4","trusted":true},"cell_type":"code","source":"model = Model(inputs = [start_in], outputs = [out])    \nmodel.compile(optimizer = 'adam', \n              loss = bce_dice, \n              metrics = ['binary_crossentropy', dice_coef, mean_iou])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72330944-6ce7-4070-b276-c3c4b20c4fe5","_uuid":"92350b6e18cc50f3fa7b6e9a02d39fcbff8238f7"},"cell_type":"markdown","source":"*Update: Changed to ELU units, added dropout.*\n\nNext we fit the model on the training data, using a validation split of 0.1. We use a small batch size because we have so little data. I recommend using checkpointing and early stopping when training your model. I won't do it here to make things a bit more reproducible (although it's very likely that your results will be different anyway). I'll just train for 10 epochs, which takes around 10 minutes in the Kaggle kernel with the current parameters. \n\n*Update: Added early stopping and checkpointing and increased to 30 epochs.*","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9415b1c4-aa69-41b9-a1e3-d6053dbd4f64","_uuid":"c060db22daa2abf12b28240cd81bbcbf1ce1bf87","trusted":true},"cell_type":"code","source":"# Fit model\nearlystopper = EarlyStopping(patience=5, verbose=1)\ncheckpointer = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)\nresults = model.fit(X_train, Y_train, \n                    validation_split=0.25, \n                    batch_size=16, epochs=10, \n                    callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f381f5b-1b71-4daa-a417-e02f4894540b","_uuid":"bb15226ea617cf91ed8f43179fccb5a15809e5a0"},"cell_type":"markdown","source":"All right, looks good! Loss seems to be a bit erratic, though. I'll leave it to you to improve the model architecture and parameters! \n\n# Make predictions\n\nLet's make predictions both on the test set, the val set and the train set (as a sanity check). Remember to load the best saved model if you've used early stopping and checkpointing.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2daa48d5-ac98-4e18-af3f-a582baaa44f0","_uuid":"f841760b4abca1a25cb750822f88268bd79bf2ce","trusted":true,"collapsed":true},"cell_type":"code","source":"# Predict on train, val and test\nmodel = load_model('model-dsbowl2018-1.h5', custom_objects={'mean_iou': mean_iou}, compile = False)\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)\n\n# Create list of upsampled test masks\npreds_test_upsampled = []\nfor i in range(len(preds_test)):\n    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"649248cd-a1fb-4da6-ade2-4bebad44bcab","_uuid":"7e06242a50870e07a080064a4912b761775990fa","trusted":true,"collapsed":true},"cell_type":"code","source":"# Perform a sanity check on some random training samples\nix = random.randint(0, len(preds_train_t))\nimshow(X_train[ix])\nplt.show()\nimshow(np.squeeze(Y_train[ix]))\nplt.show()\nimshow(np.squeeze(preds_train_t[ix]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af602aea-5e56-42a8-9331-54b4b2650593","_uuid":"5fcee2b9aee2fba5c60d43ad48a14139e9c1318c"},"cell_type":"markdown","source":"The model is at least able to fit to the training data! Certainly a lot of room for improvement even here, but a decent start. How about the validation data?","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4f66b75c-c694-41a1-8c91-34bb6595837b","_uuid":"d4ccbb559375bc2777ffb692a20adc313159f2cc","trusted":true,"collapsed":true},"cell_type":"code","source":"# Perform a sanity check on some random validation samples\nix = random.randint(0, len(preds_val_t))\nimshow(X_train[int(X_train.shape[0]*0.9):][ix])\nplt.show()\nimshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]))\nplt.show()\nimshow(np.squeeze(preds_val_t[ix]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6690535-b2e4-49ac-98d9-7191bfabfb6f","_uuid":"6a34c98de7c6ae473f676a34fe7e099b46764eca"},"cell_type":"markdown","source":"Not too shabby! Definitely needs some more training and tweaking.\n\n# Encode and submit our results\n\nNow it's time to submit our results. I've stolen [this](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python) excellent implementation of run-length encoding.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4f99c1bf852e82b60bd4f982ca0df293f712cdf0","_cell_guid":"59a0af60-a7d7-41ef-a6fe-9e3c72defa07","trusted":true},"cell_type":"code","source":"# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31133f8c-3f40-4dff-8e1d-898d56672332","_uuid":"2e07f6afc4787b068ba714428145dcb3951d718f"},"cell_type":"markdown","source":"Let's iterate over the test IDs and generate run-length encodings for each seperate mask identified by skimage ...","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"089587843ed6a3955fdcb9b23a6ec3bf5d703688","_cell_guid":"22fe24a1-7659-4cc9-9d23-211f38e5b99f","trusted":true},"cell_type":"code","source":"new_test_ids = []\nrles = []\nfor n, id_ in enumerate(test_ids):\n    rle = list(prob_to_rles(preds_test_upsampled[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20b6b627-0fd6-425d-888f-da7f39efb124","_uuid":"849184a40a2c9c21506d8b8eb10ad9155fa229e8"},"cell_type":"markdown","source":"... and then finally create our submission!","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"ba589f56f5be1e6886bc88f5bf9e7d0a408e4048","_cell_guid":"1ba0ee3a-cca0-4349-83f6-09a1ac6fcb44","trusted":true},"cell_type":"code","source":"# Create submission DataFrame\nsub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fb5e6f8cca872f1bd7036f6d9ac2ed2cab615536","_cell_guid":"222475b9-3171-461a-90f0-a820a6bd2634"},"cell_type":"markdown","source":"This scored 0.233 on the LB for me. That was with version 2 of this notebook; be aware that the results from the neural network are extremely erratic and vary greatly from run to run (version 3 is significantly worse, for example). Version 7 scores 0.277!\n\nYou should easily be able to stabilize and improve the results just by changing a few parameters, tweaking the architecture a little bit and training longer with early stopping.\n\n**Have fun!**\n\nLB score history:\n- Version 7: 0.277 LB","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"2a83eab66bf55194f300953bea5534b6a043130f","_cell_guid":"3f5e5a47-6133-4870-976a-a8e4fa7bf46c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}