{"nbformat": 4, "nbformat_minor": 1, "cells": [{"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b3d5bbb2-18e6-4c8f-bd87-43dcdecc9d3d", "collapsed": true, "_uuid": "7dd7d54e32957387a59a1c80a19283bb5f8b4f2e"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "%matplotlib inline\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from datetime import timedelta\n", "import datetime as dt\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = [16, 10]\n", "import seaborn as sns\n", "import xgboost as xgb\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n"]}, {"source": ["Loading Data"], "cell_type": "markdown", "metadata": {"_cell_guid": "d17f54ec-3a9e-4ea2-af83-f07d4df8278f", "_uuid": "29ed5c1bd54a37d9a58c067fde68dd0166ba9f77"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "d8ed3278-c4ce-411e-823e-8771922f3bf0", "collapsed": true, "_uuid": "869d7d0bfbc389dcdefb545ef7bb3fb2c3ba5a8f"}, "source": ["t0 = dt.datetime.now()\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "sample_submission = pd.read_csv('../input/sample_submission.csv')"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "036f4ca1-7f7c-43d4-b281-b702c3a0ca5c", "_uuid": "63c77417decc521826881b5bab28ed9c8ea05dfb"}, "source": ["train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n", "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n", "train.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\n", "test.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\n", "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n", "train['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\n", "test['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\n", "train['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\n", "duration_difference = train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values) > 1]\n", "print('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "af9c0559-e4fa-4fe4-9867-a3f0f07cb77d", "collapsed": true, "_uuid": "579dc3c06d4ba4f08b1bdc5a248aa4679d536cf4"}, "source": ["# Let's compute pickup hour for each ride\n", "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n", "train['pickup_hour'] = train.pickup_datetime.dt.hour\n", "train['day_week'] = train.pickup_datetime.dt.weekday\n", "# Get pick up hour for test data as well\n", "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\n", "test['pickup_hour'] = test.pickup_datetime.dt.hour\n", "test['day_week'] = test.pickup_datetime.dt.weekday"]}, {"source": ["We could logtransform our target label and use RMSE during training."], "cell_type": "markdown", "metadata": {"_cell_guid": "f0a6e613-e014-471d-b018-6378c852a2b9", "_uuid": "23197e2a09d584cca0b208f4dcc65374b73795d6"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "465e9af2-9a25-4715-8ed8-24e10573dc52", "_uuid": "8582ddbeed9b5863895cd3e2683ed6e81f8309e0"}, "source": ["train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\n", "sns.distplot(train['log_trip_duration'].values, bins=100)\n", "plt.xlabel('log(trip_duration)')\n", "plt.ylabel('number of train records')\n", "plt.show()"]}, {"source": ["First let's check the train test split. It helps to decide our validation strategy and gives ideas about feature engineering."], "cell_type": "markdown", "metadata": {"_cell_guid": "c818a114-164d-41db-8cbb-713be6084897", "_uuid": "0391f5ce0f2421c9b57995d78914635e3c0bf27e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a6fc0163-5bf7-46d1-a872-7e6df80bb2a4", "_uuid": "abc41ea3891d7e5282ef9b8bbd9c20c625d1dc1f"}, "source": ["#from pandas.plotting import parallel_coordinates\n", "#dct = {'training': train.groupby('pickup_date').count()[['id']], \n", "#       'testing': train.groupby('pickup_date').count()[['id']] }\n", "\n", "#df = pd.DataFrame.from_dict(dct)\n", "\n", "#parallel_coordinates(df, 'training')\n", "\n", "#df = pd.DataFrame( {train.groupby('pickup_date').count()[['id']] columns=['a', 'b', 'c', 'd']})\n", "\n", "#df.plot.area();\n", "#fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n", "\n", "plt.plot((train.groupby('pickup_date').count()[['id']]), label='train', color = 'g')\n", "plt.plot((test.groupby('pickup_date').count()[['id']]), label='train', color = 'r')\n", "\n", "plt.title('Train and test period complete overlap.')\n", "plt.legend(loc=0)\n", "plt.ylabel('number of records')\n", "plt.show()"]}, {"source": ["\n", "remove obvious outliers and convert everything to sensible units"], "cell_type": "markdown", "metadata": {"_cell_guid": "83bc3684-ba66-4d56-90aa-d0a44b13b28e", "_uuid": "2270617390ce78847a5bba3a2783216ebf679570"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f43e8a52-6204-4c59-9ff5-fdab84b90fd5", "collapsed": true, "_uuid": "26b8727bc64f1505ef5b2ed5504e1ef80bd0a3db"}, "source": ["def bearing_array(lat1, lng1, lat2, lng2):\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lng_delta_rad = np.radians(lng2 - lng1)\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n", "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n", "    return np.degrees(np.arctan2(y, x))\n", "\n", "train.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n", "                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "\n", "test.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n", "                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "546deafa-bdfe-4d12-86f0-1e2b3c0e716a", "collapsed": true, "_uuid": "3fd8c179a334556d5096b5e86dc15727b9fd9dbf"}, "source": ["allLat  = np.array(list(train['pickup_latitude'])  + list(train['dropoff_latitude']))\n", "allLong = np.array(list(train['pickup_longitude']) + list(train['dropoff_longitude']))\n", "\n", "longLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\n", "latLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\n", "durLimits  = [np.percentile(train['trip_duration'], 0.4), np.percentile(train['trip_duration'], 99.7)]\n", "\n", "train = train[(train['pickup_latitude']   >= latLimits[0] ) & (train['pickup_latitude']   <= latLimits[1]) ]\n", "train = train[(train['dropoff_latitude']  >= latLimits[0] ) & (train['dropoff_latitude']  <= latLimits[1]) ]\n", "train = train[(train['pickup_longitude']  >= longLimits[0]) & (train['pickup_longitude']  <= longLimits[1])]\n", "train = train[(train['dropoff_longitude'] >= longLimits[0]) & (train['dropoff_longitude'] <= longLimits[1])]\n", "train = train[(train['trip_duration']     >= durLimits[0] ) & (train['trip_duration']     <= durLimits[1]) ]\n", "train = train.reset_index(drop=True)\n", "\n", "allLat  = np.array(list(train['pickup_latitude'])  + list(train['dropoff_latitude']))\n", "allLong = np.array(list(train['pickup_longitude']) + list(train['dropoff_longitude']))\n", "\n", "# convert fields to sensible units\n", "medianLat  = np.percentile(allLat,50)\n", "medianLong = np.percentile(allLong,50)\n", "\n", "latMultiplier  = 111.32\n", "longMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n", "\n", "train['duration [min]'] = train['trip_duration']/60.0\n", "train['src lat [km]']   = latMultiplier  * (train['pickup_latitude']   - medianLat)\n", "train['src long [km]']  = longMultiplier * (train['pickup_longitude']  - medianLong)\n", "train['dst lat [km]']   = latMultiplier  * (train['dropoff_latitude']  - medianLat)\n", "train['dst long [km]']  = longMultiplier * (train['dropoff_longitude'] - medianLong)\n", "\n", "allLat  = np.array(list(train['src lat [km]'])  + list(train['dst lat [km]']))\n", "allLong = np.array(list(train['src long [km]']) + list(train['dst long [km]']))"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b9bc4e2f-6232-43be-aea4-d55991512b79", "scrolled": true, "_uuid": "43001d6fd01a3695ad78e3d83b60516e4f17d38e"}, "source": ["# show the log density of pickup and dropoff locations\n", "imageSize = (700,700)\n", "longRange = [-5,19]\n", "latRange = [-13,11]\n", "\n", "allLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n", "allLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n", "\n", "locationDensityImage = np.zeros(imageSize)\n", "for latInd, longInd in zip(allLatInds,allLongInds):\n", "    locationDensityImage[latInd,longInd] += 1\n", "\n", "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\n", "ax.imshow(np.log(locationDensityImage+1),cmap='magma')\n", "ax.set_axis_off()"]}, {"source": ["Cluster The Trips and Look at their distribution\n", "every trip is essentially made up of five major attributes: pickup and dropoff locations and the trip duration. We have also included direction as an additional measure to see its effect. \n", "\n", "let's cluster all 1.4 million trips to 80 stereotypical template trips and then look at the distribution of this \"bag of trips\" and how it changes over time"], "cell_type": "markdown", "metadata": {"_cell_guid": "a9ef0182-ac7c-4f25-90ec-1d0253d8476f", "_uuid": "fbcda387dfd9d501ec36b88ce2a4b30586101a69"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "ab00b3fa-36e3-49c2-9398-b66ce775b89a", "_uuid": "482bb3c35b089b31186f1cb0ba47274cfa3252b8"}, "source": ["from sklearn import decomposition\n", "from scipy import stats\n", "from sklearn import cluster\n", "\n", "\n", "tripAttributes = np.array(train.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]', 'direction']])\n", "meanTripAttr = tripAttributes.mean(axis=0)\n", "stdTripAttr  = tripAttributes.std(axis=0)\n", "tripAttributes = stats.zscore(tripAttributes, axis=0)\n", "\n", "# choose number of clusters\n", "\n", "\n", "numClusters = 60\n", "TripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100)\n", "clusterInds = TripKmeansModel.fit_predict(tripAttributes)\n", "\n", "clusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\n", "sortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n", "\n", "plt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\n", "plt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\n", "plt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\n", "plt.xlim(0,numClusters+1)"]}, {"source": ["To visualize where the pick up and drop off locations are located, two circles are plotted. The yellow ones indicate the source coordinates an the green ones are the detinations. Credit goes to Selfish Gene 's Kernel for this visualization"], "cell_type": "markdown", "metadata": {"_cell_guid": "05a7a544-0d0f-4810-9e65-37f1b195348e", "_uuid": "0e7f35ca7d2eacab1717333f05caf676e78245b6"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "686805b0-9998-4b47-9f20-103764badcae", "_uuid": "69ef22d0b2f78285b71f5c41859752c01fed4223"}, "source": ["#%% show the templeate trips on the map \n", "\n", "def ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n", "    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n", "    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n", "\n", "    return latInds, longInds\n", "\n", "templateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n", "\n", "srcCoords = templateTrips[:,:2]\n", "dstCoords = templateTrips[:,2:4]\n", "\n", "srcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\n", "dstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n", "\n", "plt.figure(figsize=(12,12))\n", "plt.imshow(np.log(locationDensityImage+1),cmap='magma'); plt.grid('off')\n", "plt.scatter(srcImCoords[1],srcImCoords[0],c='y',s=200,alpha=0.9)\n", "plt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.9)\n", "\n", "for i in range(len(srcImCoords[0])):\n", "    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], \n", "              edgecolor='c', facecolor='c', width=2.4,alpha=0.6,head_width=10.0,head_length=10.0,length_includes_head=True)"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "0666d7cc-ad3e-4983-b9a0-ab9b7b480301", "_uuid": "88a9d30934154d4faf3157d2b4cba727404d271a"}, "source": ["#color = sns.color_palette()\n", "\n", "grouped_df = train.groupby('pickup_hour')['trip_duration'].aggregate(np.mean).reset_index()\n", "plt.figure(figsize=(12,8))\n", "sns.pointplot(grouped_df.pickup_hour.values, grouped_df.trip_duration.values, alpha=0.8, \n", "              color='k' )\n", "plt.ylabel('Average trip duration', fontsize=12)\n", "plt.xlabel('Pickup Hour', fontsize=12)\n", "plt.xticks(rotation='vertical')\n", "plt.show()"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "357ab1d6-8c60-40ab-8a02-a9badd46dcc9", "_uuid": "27401e2d930e309ed2fcb33cb4e2db86ddd4b252"}, "source": ["sns.set(style=\"ticks\")\n", "sns.set_context(\"poster\")\n", "sns.boxplot(x=\"day_week\", y=\"trip_duration\", hue=\"vendor_id\", data=train\n", "             )\n", "plt.ylim(0, 6000)\n", "sns.despine(offset=10, trim=True)\n", "train.trip_duration.max()"]}, {"source": ["### Categorical Features Encoding"], "cell_type": "markdown", "metadata": {"_cell_guid": "aeeb4252-8ec5-4bf8-8161-732aa4b379a2", "_uuid": "7e7aa03c80de1d4f1887f69eff0452a74ecbccb1"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "e43407e9-b927-4bed-8d84-882bb610990f", "collapsed": true, "_uuid": "77d4ea548157e02c3756ef6797641ed48e87cd34"}, "source": ["from sklearn import model_selection, preprocessing\n", "import xgboost as xgb\n", "\n", "for f in train.columns:\n", "    if train[f].dtype=='object':\n", "        lbl = preprocessing.LabelEncoder()\n", "        lbl.fit(list(train[f].values)) \n", "        train[f] = lbl.transform(list(train[f].values))\n"]}, {"source": ["### Feature Importance Calculation"], "cell_type": "markdown", "metadata": {"_cell_guid": "a34aea8b-74f5-4a65-84df-8691c084b1b9", "_uuid": "aebf30f0b671e281472cd02398724de269fceffb"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "273339e3-5e9a-4ffc-b1b2-ce31a27d3190", "collapsed": true, "_uuid": "f7c07291ee1270fa2a2ef5db3ac7c797a0de0aff"}, "source": ["train_y = train.trip_duration.values\n", "train_X = train.drop([\"id\", \"dropoff_datetime\", \"pickup_datetime\", \"trip_duration\"], axis=1)\n"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "20ab5d60-8178-420d-be95-3de4c36ffb09", "_uuid": "d3974e19d52652e7714f9b7348e7993adc704920"}, "source": ["from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.grid_search import GridSearchCV\n", "import matplotlib.pyplot as plt\n", "\n", "rf_clf = RandomForestRegressor(max_depth=15,n_estimators=100, min_samples_leaf=75,\n", "                                  min_samples_split=100, random_state=10)\n", "\n", "# Train the model\n", "rf_clf.fit(train_X, train_y)"]}, {"source": ["Please keep checking back, It's a work in progress :)"], "cell_type": "markdown", "metadata": {"_cell_guid": "4fcda865-db36-4419-b2f3-053ff73e37f0", "_uuid": "26eb957a9e6dad715ddafad43edcb6e8702bdebf"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "382bad18-d531-44a1-8228-b2f3699753a4", "collapsed": true, "_uuid": "be96f6d54eccd83c7f52ba2af34707f0d194f9b9"}, "source": []}], "metadata": {"language_info": {"mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}}