{"cells": [{"source": ["# Intro\n", "\n", "With some experimentation I found that VGG16 and VGG19 did not perform as well as Inception and XceptionV3 on the data. Therefore this kernel is about how to get the best out of the Xception and InceptionV3 pretrained weights using different ensembling methods.\n"], "metadata": {"_cell_guid": "e4791b71-6c29-4cb7-8c83-096c38cb5d0a", "_kg_hide-output": true, "_uuid": "60f80d3d7a95ea743039eb897a404cd90e3904e5"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["import numpy as np \n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "from keras.preprocessing.image import ImageDataGenerator\n", "from keras.applications.xception import Xception\n", "from keras.applications.inception_v3 import InceptionV3\n", "\n", "from keras.applications.xception import preprocess_input as xception_preprocessor\n", "from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor"], "metadata": {"_cell_guid": "9a117fce-e7d6-41a2-8a8c-e73d27b8f5f3", "scrolled": true, "_kg_hide-output": false, "_kg_hide-input": false, "_uuid": "b5f0e7f31915743033c945839dd64770bca282ca"}, "cell_type": "code"}, {"source": ["# Data Exploration\n", "\n", "If we look at the spread of frequencies of each class, the most common class has a frequency of almost double that of the least common class.\n", "\n", "NB. So that the kernel does not timeout we will limit to just the top 16 classes, but for submission we will predict on all classes."], "metadata": {"_cell_guid": "6a3d460a-0f16-4545-9fbf-a82deabec46d", "_uuid": "c00bfad8f8e8e10b5778449cd40700f2f075d0c7"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["LABELS = \"../input/dog-breed-identification/labels.csv\"\n", "\n", "train_df = pd.read_csv(LABELS)\n", "#return top 16 value counts and convert into list\n", "plt.figure(figsize=(13, 6))\n", "train_df['breed'].value_counts().plot(kind='bar')\n", "plt.show()\n", "\n", "top_breeds = sorted(list(train_df['breed'].value_counts().head(16).index))\n", "train_df = train_df[train_df['breed'].isin(top_breeds)]\n", "\n", "print(top_breeds)"], "metadata": {"_cell_guid": "02d7074e-e3db-428f-80e9-f4a92fe27e03", "_uuid": "842d24fbe652fd120cff1b774c20eaf9445d2eec"}, "cell_type": "code"}, {"source": ["# Train and Validation Split\n", "\n", "We will load the images into an numpy array and split the data into train and  cross validation sets.\n", "\n", "I've chosen to take a 80:20 split of the data for cross validation. Strictly speaking we don't need to stratify split the dataset but it will ensure that the training and cross validation set are balanced."], "metadata": {"_cell_guid": "5a86b64a-99ca-478b-9dea-e2df60055be5", "_uuid": "2962e972c1fb7effecf2d332a8aca7b586d08d02"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["from keras.preprocessing.image import img_to_array\n", "from keras.preprocessing.image import load_img\n", "from sklearn.model_selection import train_test_split\n", "\n", "SEED = 1234\n", "\n", "TRAIN_FOLDER = \"../input/dog-breed-identification/train/\"\n", "TEST_FOLDER = \"../input/dog-breed-identification/test/\"\n", "\n", "DIM = 299\n", "\n", "train_df['image_path'] = train_df.apply( lambda x: ( TRAIN_FOLDER + x[\"id\"] + \".jpg\" ), axis=1 )\n", "\n", "train_data = np.array([ img_to_array(load_img(img, target_size=(DIM, DIM))) for img in train_df['image_path'].values.tolist()]).astype('float32')\n", "train_labels = train_df['breed']\n", "\n", "\n", "x_train, x_validation, y_train, y_validation = train_test_split(train_data, train_labels, test_size=0.2, stratify=np.array(train_labels), random_state=SEED)\n", "\n", "#calculate the value counts for train and validation data\n", "data = y_train.value_counts().sort_index().to_frame()\n", "data.columns = ['train']\n", "data['validation'] = y_validation.value_counts().sort_index().to_frame()\n", "\n", "new_plot = data[['train','validation']].sort_values(['train']+['validation'], ascending=False)\n", "new_plot.plot(kind='bar', stacked=True)\n", "plt.show()"], "metadata": {"_cell_guid": "e87b2d23-cc29-4361-b960-143966571a6d", "_uuid": "24c4289b8ec666db47335e607f997372a947167b"}, "cell_type": "code"}, {"source": ["## One-hot Encoding\n", "\n", "Since the output of our predictor for each input is a vector of probabilities for each class we must convert out label dataset to be the same format. That is for each input a row vector of length num_classes with a 1 at the index of the label and 0's everywhere else."], "metadata": {"_cell_guid": "bb77e77b-e7d4-4a36-9c1c-1f6a402089b5", "_uuid": "dd688e94664a5061f7af657c26e9b1228a45fbfb"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["# Let's convert our labels into one hot encoded format\n", "\n", "y_train = pd.get_dummies(y_train.reset_index(drop=True), columns=top_breeds).as_matrix()\n", "y_validation = pd.get_dummies(y_validation.reset_index(drop=True), columns=top_breeds).as_matrix()\n", "\n", "print(y_train[0])"], "metadata": {"_cell_guid": "06269a9f-ca34-4344-ae84-25adf97816b0", "_uuid": "42997e86530b345078d3498260b897bfa3734c36"}, "cell_type": "code"}, {"source": ["Let's double check that our inputs and labels match."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["plt.subplot(1, 2, 1)\n", "plt.title(top_breeds[np.where(y_train[5]==1)[0][0]])\n", "plt.axis('off')\n", "plt.imshow(x_train[5].astype(np.uint8))\n", "\n", "plt.subplot(1, 2, 2)\n", "plt.title(top_breeds[np.where(y_train[7]==1)[0][0]])\n", "plt.axis('off')\n", "plt.imshow(x_train[7].astype(np.uint8))\n", "plt.show()"], "metadata": {}, "cell_type": "code"}, {"source": ["# Generate bottleneck features\n", "\n", "Since kaggle kernels have no access to the internet we must use a pre-downloaded dataset and copy the files to the cache and models directory."], "metadata": {"_cell_guid": "fdfdbd10-afbc-4028-9634-a9dfb17a76ad", "_uuid": "eef1db5472d421abcdf149f9cd0d3ae7a1b5d778"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["from os import makedirs\n", "from os.path import expanduser, exists, join\n", "\n", "!ls ../input/keras-pretrained-models/\n", "\n", "cache_dir = expanduser(join('~', '.keras'))\n", "if not exists(cache_dir):\n", "    makedirs(cache_dir)\n", "models_dir = join(cache_dir, 'models')\n", "if not exists(models_dir):\n", "    makedirs(models_dir)\n", "    \n", "!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n", "!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n", "!cp ../input/keras-pretrained-models/resnet50* ~/.keras/models/"], "metadata": {"_cell_guid": "da5212d7-0a8d-46df-9ea1-4c5c133760fe", "_uuid": "5df2a9768533b4381be4c34a2b6e54076caec4be"}, "cell_type": "code"}, {"source": ["Let's define a function that will output bottleneck features from a given model. \n", "\n", "We will use 'imagenet' weights and remove the final layers of the neural network so that we can use our own classifier."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import log_loss, accuracy_score\n", "\n", "batch_size = 32\n", "epochs = 30\n", "num_classes = len(top_breeds)\n", "\n", "def generate_features(model_info, data, labels, datagen):\n", "    print(\"generating features...\")\n", "    datagen.preprocessing_function = model_info[\"preprocessor\"]\n", "    generator = datagen.flow(data, labels, shuffle=False, batch_size=batch_size, seed=model_info[\"seed\"])\n", "    bottleneck_model = model_info[\"model\"](weights='imagenet', include_top=False, input_shape=model_info[\"input_shape\"], pooling=model_info[\"pooling\"])\n", "    return bottleneck_model.predict_generator(generator)\n"], "metadata": {"_cell_guid": "e8cf9356-5ca4-4082-8840-84b3fa176622", "_uuid": "c3f767103e7b131dad138cf038caea029258dc49"}, "cell_type": "code"}, {"source": ["## Define our models and run\n", "\n", "First we define the settings for our models such as the input shape and the preprocessor which we will feed into generate_features.\n", "\n", "Then let's generate our train features and validation features and save them to file so that we don't need to compute them again."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["import time\n", "\n", "models = {\n", "    \"InceptionV3\": {\n", "        \"model\": InceptionV3,\n", "        \"preprocessor\": inception_v3_preprocessor,\n", "        \"input_shape\": (299,299,3),\n", "        \"seed\": 1234,\n", "        \"pooling\": \"avg\"\n", "    },\n", "    \"Xception\": {\n", "        \"model\": Xception,\n", "        \"preprocessor\": xception_preprocessor,\n", "        \"input_shape\": (299,299,3),\n", "        \"seed\": 5512,\n", "        \"pooling\": \"avg\"\n", "    }\n", "}\n", "\n", "for model_name, model in models.items():\n", "    print(\"Predicting : {}\".format(model_name))\n", "    filename = model_name + '_features.npy'\n", "    validfilename = model_name + '_validfeatures.npy'\n", "    if exists(filename):\n", "        features = np.load(filename)\n", "        validation_features = np.load(validfilename)\n", "    else:\n", "        train_datagen = ImageDataGenerator(\n", "                zoom_range = 0.3,\n", "                width_shift_range=0.1,\n", "                height_shift_range=0.1)\n", "        validation_datagen = ImageDataGenerator()\n", "        features = generate_features( model, x_train, y_train, train_datagen)\n", "        validation_features = generate_features(model, x_validation, y_validation, validation_datagen)\n", "        np.save(filename, features)\n", "        np.save(validfilename, validation_features)\n", "    \n", "    # Now that we have created or loaded the features  we need to do some predictions.\n", "    start_time = time.time()\n", "    \n", "    logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n", "    logreg.fit(features, (y_train * range(num_classes)).sum(axis=1))\n", "\n", "    model[\"predict_proba\"] = logreg.predict_proba(validation_features)\n", "    end_time = time.time()\n", "    print('Training time : {} {}'.format(np.round((end_time-start_time)/60, 2),' minutes'))"], "metadata": {"_cell_guid": "c3db3b42-6ee5-492c-bf0e-e3b81260c3b9", "_uuid": "49fe04a1adc1dc51bd109c51bb45eba154a49217"}, "cell_type": "code"}, {"source": ["# Ensemble by average\n", "\n", "Using a logistic regression classifier seems to yield good results so one method of esembling is to take the average probability from each prediction made from the logistric regression.\n", "\n", "We have saved the predictions in \"predict_proba\" so it should be fairly easy to retrieve and ensemble."], "metadata": {"_cell_guid": "926a2425-e8ac-4149-98a7-ea75e9ac2f08", "_uuid": "7f44936476cb6b624303784ade7ff5334bf52eca"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["probas = [ model[\"predict_proba\"] for model_name, model in models.items() ]\n", "\n", "avgprobas = np.average(probas, axis=0, weights=[1,1])\n", "\n", "print('ensemble validation logLoss : {}'.format(log_loss(y_validation, avgprobas)))"], "metadata": {"_cell_guid": "af86d51c-fc2c-402f-b04b-8a7df5878aad", "_uuid": "e3fe4d5b0c6c6a23b4582ce868f9fc155b93264e"}, "cell_type": "code"}, {"source": ["# Ensemble input features\n", "\n", "Another way of ensembling is to merge the input features of the classifier together so we have more data to learn from. "], "metadata": {"_cell_guid": "29373d33-3a25-4ef0-bf94-e8d23de8c7d0", "_uuid": "9e421d895515f6b58bb7ac8cbbcc04617af1503c"}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["features  = np.hstack( [ np.load(model_name + '_features.npy') for model_name, model in models.items() ])\n", "validation = np.hstack( [ np.load(model_name + '_validfeatures.npy') for model_name, model in models.items() ])\n", "\n", "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\n", "logreg.fit(features, (y_train * range(num_classes)).sum(axis=1))\n", "\n", "predict_probs = logreg.predict_proba(validation)\n", "\n", "print('ensemble of features va logLoss : {}'.format(log_loss(y_validation, predict_probs)))"], "metadata": {"_cell_guid": "1e11d450-6e7d-4fb2-b78d-a69bdd11c2ef", "_uuid": "d0cd2db54fb5c58cf160a8e697554ede182e37b0"}, "cell_type": "code"}, {"source": ["# Conclusion\n", "\n", "I was able to achieve a leaderboard score of 0.28 using the average probabilities ensemble method.\n", "\n", "I think the next stage will be to look at other models and different classifiers."], "metadata": {}, "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3"}}}