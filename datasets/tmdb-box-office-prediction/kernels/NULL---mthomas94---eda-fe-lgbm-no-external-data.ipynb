{"cells":[{"metadata":{"trusted":false,"_uuid":"6b8cce39aee6743eb93d63e963ecc18d6fc15903"},"cell_type":"code","source":"import re\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6904383d0a0a7bc3d69d03d8b22a37bed45c5b19"},"cell_type":"code","source":"# Function to reduce memory usage.  From: https://www.kaggle.com/fabiendaniel/detecting-malwares-with-lgbm\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"6de69c92dbf116b365d28048ab8824e3928effee"},"cell_type":"code","source":"train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv'))\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"197e99a51e7109deaab71a9fb5bf203b161d0bc7"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec9e6c70c444e4a505ce7b037af7692370f67610"},"cell_type":"markdown","source":"## EDA\nIt's likely features like budget, popularity, and release date will correlate strongly with revenue.  By contrast, features like poster path might not be helpful without extensive analysis."},{"metadata":{"trusted":false,"_uuid":"770085e071dc933e879eb285cb130777af0f81c5"},"cell_type":"code","source":"# Revenues are not uniformally distributed\ntrain['revenue'].hist(bins=25)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"7100dd2f6f3c73198f38ca0beee6fbf42a1e8c39"},"cell_type":"code","source":"# When comparing the listed revenues with their actual values found online, \n# it's clear the values given here are not accurate.\ntrain.sort_values('revenue').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"442e08539567aa5de8cf0f21be3e3bcfb2a7f0d7"},"cell_type":"code","source":"# Budget is also skewed.\ntrain['budget'].hist(bins=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d343a152e9617b6ee25f658470c54270f5c7e43e"},"cell_type":"code","source":"# $0 budget for some movies?\ntrain['budget'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"e9641720ef8f8fec82414869fd8f2c88e2f9f6c7"},"cell_type":"code","source":"print('Movies with 0$ Budget:', len(train[train['budget'] == 0]))\ntrain[train['budget'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb480b65b069c47f20639408b2ad3a82246556d5"},"cell_type":"markdown","source":"I'll come back to budget and update the values using a linear regression approach.  But it will be helpful to have as much information as possible for other features like runtime as this might affect the total budget."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"7db2b6c3f24f7a6fc0dc4a96d7ca4d0946952785"},"cell_type":"code","source":"# Create columns for year, month, and day of week\ntrain['release_date'] = pd.to_datetime(train['release_date'], infer_datetime_format=True)\ntrain['release_day'] = train['release_date'].apply(lambda t: t.day)\ntrain['release_weekday'] = train['release_date'].apply(lambda t: t.weekday())\ntrain['release_month'] = train['release_date'].apply(lambda t: t.month)\n# Year was being interpreted as future dates in some cases so I had to adjust some values\ntrain['release_year'] = train['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)\n\n#train.drop('release_date', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cb910b6aff05da2dae828bece6aa86954d454441"},"cell_type":"code","source":"train['runtime'].hist(bins=25)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"d65c57f8d62e0a66ff1bbdd3bc3555edec5a68e4"},"cell_type":"code","source":"len(train[train['runtime'] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6daa5639c1195d8804db9677dfaa509fb4aaf657"},"cell_type":"code","source":"# I'll write a function that will map the average runtime for each year to movies with 0 runtie.\nfrom collections import defaultdict\ndef map_runtime(df):\n    df['runtime'].fillna(0)\n    \n    run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n    year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n    d = dict(year_mean)\n    \n    for i in df[df['runtime'] == 0]:\n        df['runtime'] = df.loc[:, 'release_year'].map(d)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"21d608012502bbeb957e37ca6b4b3de13f870714"},"cell_type":"code","source":"train = map_runtime(train)\ntrain.runtime.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd0042dc0e463572cc7adca3d39a95b90a35f16a"},"cell_type":"code","source":"train['homepage'].head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"6dca16f4b597ebdc9e598dcbd5bce845179cef88"},"cell_type":"code","source":"# For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\ntrain['homepage'].fillna(0, inplace=True)\ntrain.loc[train['homepage'] != 0, 'homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16d349742106e4ea95893ba80d685a32aedac2b7"},"cell_type":"code","source":"train['poster_path'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fd496605c50c8f756161c5c384237067e09b3999"},"cell_type":"code","source":"# For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\ntrain['poster_path'].fillna(0, inplace=True)\ntrain.loc[train['poster_path'] != 0, 'poster_path'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"960fbc5ce75197e564327bb973b414530feb65e9"},"cell_type":"code","source":"train['genres'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"141ff5a5670a5b91c75c5054902692091807aa96"},"cell_type":"code","source":"# For genres, I'll fill Na values with drama (most common).  Likely a better approach available.\ntrain.genres = train.genres.fillna('18')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c9a8674e5400321a131994196683ed52eb1d29b1"},"cell_type":"code","source":"# To fill in zero budget data points, I'll try to use correlated values as predictors\nX = train[train['budget'] != 0]\nfor i in X.select_dtypes(include='number', exclude='datetime'):\n    print(i, stats.pearsonr(X.budget, X[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"389f42a81c167326488dbfc71be4056f102fcac7"},"cell_type":"code","source":"# release_year and popularity correlate most strongly with budget\ndef map_budget(df):\n    d = defaultdict()\n    #df['budget'] = df['budget'].fillna(0)\n    X = df[df['budget'] != 0]\n    \n    year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n    d = dict(year_mean)\n    \n    for i in df[df['budget'] == 0]:\n        df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n    # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n    df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"053035862b9bc0f1a56ae0f5641783165bbfe361"},"cell_type":"code","source":"train = map_budget(train)\ntrain.budget.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"95500fdd3e19c9efbcc2a640dea17d7eebe2428b"},"cell_type":"code","source":"train['belongs_to_collection'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"875d500b0b9c22dd76988f7ef9da9c3b80afc593"},"cell_type":"code","source":"# belongs_to_collection NaN values can be replaced with 'none'\ntrain['belongs_to_collection'] = train['belongs_to_collection'].fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1cc7369e5ac2e9f438617fa55c1de28afc89a6fe"},"cell_type":"code","source":"train['spoken_languages'].head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"66f72ae1c385c1549999694950746bdb8d56ec1c"},"cell_type":"code","source":"train.spoken_languages.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2f235f41ab23fe4de5df83168bd2e27b80d550d6"},"cell_type":"code","source":"# For spoken_languages I'll fill Na values with [{'iso_639_1': 'en', 'name': 'English'}]\ntrain.spoken_languages = train.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"67d763c9c70fa285e81154d77e3d0baa221ec416"},"cell_type":"code","source":"train['overview'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92d3811b9b9e1d159415b0fae31e973ec9baf82a"},"cell_type":"code","source":"# For overview, I'll fill Na values with 'none'\ntrain.overview = train.overview.fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6799c32eac9b16026fbed6fe121e605227d2e91a"},"cell_type":"code","source":"train['Keywords'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b6ab5e2722cbbd1016cf12c4fd8d971b7d58bd0"},"cell_type":"code","source":"# For Keywords, I'll fill Na values with 'none'\ntrain.Keywords = train.Keywords.fillna('none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"026ab88b9e1fc23b79a26c51295334706752f96d"},"cell_type":"code","source":"train.production_countries.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b7cb3aebccaf7ac1b72947ef3a07ba188d9733ff"},"cell_type":"code","source":"# For production_countries, I'll fill Na with the most common value\ntrain.production_countries = train.production_countries.fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1f41a082468793e7509a7f384273b19943eee98d"},"cell_type":"code","source":"train.production_companies.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8945fc639c58a5b3cb88b89945cc05abe319363e"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":false,"_uuid":"184028799a2fb8f791d724fbbb82a72fee7e53c3"},"cell_type":"code","source":"# Create a columns for title length\ntitle_len = []\nfor i in train['title']:\n    title_len.append(len(i.split()))\ntitle_len = pd.Series(title_len, name='title_length')\ntrain = pd.concat([train,title_len], axis=1)\n\ntrain['title_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2a38b5282b4bbb3a51a0bc16133605a77945174"},"cell_type":"code","source":"# For genres, I'll make a new column counting the number of listed genre types\n# This will strip out all characters except for numbers, and return this as an array\ngenre_ids = []\nfor i in train['genres']:\n    i = re.findall('\\d+', i)\n    genre_ids.append(i)\ngenre_ids = pd.Series(genre_ids, name='genre_ids').astype(str)\n\n# This will count the number of genres listed for each film\nnum_genre_types = []\nfor i in genre_ids:\n    num_genre_types.append(len(i.split()))\nnum_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\ntrain = pd.concat([train, genre_ids, num_genre_types], axis=1)\n\ntrain['num_genre_types'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"911cae2c5df29ee5b7bd217d2ea5bf0b2a8b98b9"},"cell_type":"code","source":"# Create column for sequels\nis_sequel = []\nfor i in train['Keywords']:\n    if 'sequel' in str(i):\n        is_sequel.append(1)\n    else:\n        is_sequel.append(0)\nis_sequel = pd.Series(is_sequel, name='is_sequel')\ntrain = pd.concat([train, is_sequel], axis=1)\n\ntrain['is_sequel'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"c2704cd1ecc880d01613b82badf99b5ec1597876"},"cell_type":"code","source":"keyword_words = []\nfor i in train['Keywords']:\n    i = re.findall('[a-zA-Z \\t]+', i)\n    stopwords = ['id', 'name', ' ']\n    i = [word for word in i if word not in stopwords]\n    keyword_words.append(i)\nkeyword_words = pd.Series(keyword_words, name='keyword_words').astype(str)\ntrain = pd.concat([train, keyword_words], axis=1)\n\n# This will count the number of Keywords listed for each film\nnum_keywords = []\nfor i in keyword_words:\n    num_keywords.append(len(i.split(',')))\nnum_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\ntrain = pd.concat([train, num_keywords], axis=1)\n\ntrain['num_keywords'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"e0d1ce12d85f59ef0839dc34192e9bae62592163"},"cell_type":"code","source":"# could use the numbers from the categories, sum them up, and then convert them to a category to target incode\nkeyword_ids = []\nfor i in train['Keywords']:\n    i = re.findall('[0-9]+', i)\n    keyword_ids.append(i)\nkeyword_ids = pd.Series(keyword_ids, name='keyword_ids')\ntrain = pd.concat([keyword_ids, train], axis=1)\ntrain.keyword_ids.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"250166d19c02dcbedc4601d4b9de39d76d53a08d"},"cell_type":"code","source":"train.belongs_to_collection.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"b6fb4af218c223ce58388a1932f8ce24bb337e61"},"cell_type":"code","source":"# Extract number from belongs to collection\ncollection_id = []\nfor i in train['belongs_to_collection']:\n    i = re.findall('[0-9]+', i)\n    collection_id.append(i[:1])\ncollection_id = pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n# Fill in blank values with 'No Collection'\nfor i in collection_id[collection_id == ''].index:\n    collection_id.loc[i] = 'No Collection'\n\ntrain = pd.concat([train, collection_id], axis=1)\n\ntrain['collection_id'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa7982f9a4c8b5eab8fc23948c371a297b99b721"},"cell_type":"code","source":"# Add column with 1 for movies in a collection and 0 if not\nis_in_collection = []\nfor i in train['collection_id']:\n    if i != 'No Collection':\n        is_in_collection.append(1)\n    else:\n        is_in_collection.append(0)\n\nis_in_collection = pd.Series(is_in_collection, name='is_in_collection')\ntrain = pd.concat([train, is_in_collection], axis=1)\n\ntrain['is_in_collection'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea4b573e73b313783cd8c51fe0e8310de941aabf"},"cell_type":"code","source":"train['production_countries'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f6492c0c0e1d3b91ca0a0d190ecab4b7d4cd3413"},"cell_type":"code","source":"# Create a column for production country (1 for US, 0 for rest of world)\n# It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\nUS_prod_country = []\nfor i in train['production_countries']:\n    if 'US' in str(i):\n        US_prod_country.append(1)\n    else:\n        US_prod_country.append(0)\nUS_prod_country = pd.Series(US_prod_country, name='US_prod_country')\ntrain = pd.concat([train, US_prod_country], axis=1)\n\ntrain['US_prod_country'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"e17587d37b708b0dbd1119ce1d6102e00fd54240"},"cell_type":"code","source":"# Create column for number of production countries\nnum_production_countries = []\nfor i in train['production_countries']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    num_production_countries.append(str(i).count('name'))\nnum_production_countries = pd.Series(num_production_countries, name='num_production_countries')\ntrain = pd.concat([train, num_production_countries], axis=1)\n\ntrain['num_production_countries'].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"068bc8a4104b432ab187981807badbb69696f1fb"},"cell_type":"code","source":"# Create a column for each production company name and a column for the number of companies\nproduction_company_names = []\nnum_production_companies = []\nfor i in train['production_companies']:\n    i = re.findall('[a-zA-Z \\t]+', str(i))\n    stopwords = ['id', 'name', ' ']\n    production_company_names.append([word for word in i if word not in stopwords])\n    num_production_companies.append(str(i).count('name'))\n\nproduction_company_1 = []\nproduction_company_2 = []\nproduction_company_3 = []\nproduction_company_4 = []\nproduction_company_5 = []\nproduction_company_6 = []\nproduction_company_7 = []\nproduction_company_8 = []\n\nfor i in production_company_names:\n    try:\n        production_company_1.append(i[:][0:1])\n        production_company_2.append(i[:][1:2])\n        production_company_3.append(i[:][2:3])\n        production_company_4.append(i[:][3:4])\n        production_company_5.append(i[:][4:5])\n        production_company_6.append(i[:][5:6])\n        production_company_7.append(i[:][6:7])\n        production_company_8.append(i[:][7:8])\n    except:\n        production_company_1.append('none')\n        production_company_2.append('none')\n        production_company_3.append('none')\n        production_company_4.append('none')\n        production_company_5.append('none')\n        production_company_6.append('none')\n        production_company_7.append('none')\n        production_company_8.append('none')\n\nnum_production_companies = pd.Series(num_production_companies, name='num_production_companies')\nproduction_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\nfor i in production_company_1[production_company_1 == ''].index:\n    production_company_1.iloc[i] = False\nproduction_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\nfor i in production_company_2[production_company_2 == ''].index:\n    production_company_2.iloc[i] = False\nproduction_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\nfor i in production_company_3[production_company_3 == ''].index:\n    production_company_3.iloc[i] = False\nproduction_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\nfor i in production_company_4[production_company_4 == ''].index:\n    production_company_4.iloc[i] = False\nproduction_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\nfor i in production_company_5[production_company_5 == ''].index:\n    production_company_5.iloc[i] = False\nproduction_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\nfor i in production_company_6[production_company_6 == ''].index:\n    production_company_6.iloc[i] = False\nproduction_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\nfor i in production_company_7[production_company_7 == ''].index:\n    production_company_7.iloc[i] = False\nproduction_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\nfor i in production_company_8[production_company_8 == ''].index:\n    production_company_8.iloc[i] = False\ntrain = pd.concat([train, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n\ntrain.production_company_8.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3448c92570c2366f93836665d8846fd435cf154"},"cell_type":"code","source":"# Create a column for number of spoken languages\nnum_spoken_languages = []\nfor i in train['spoken_languages']:\n    a = str(i).split()\n    num_spoken_languages.append(a.count(\"'name':\"))\nnum_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\ntrain = pd.concat([train, num_spoken_languages], axis=1)\n\ntrain['num_spoken_languages'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"af7b6e56c294007155ddd6e3230e0d5c7e8ed65b"},"cell_type":"code","source":"# Create column for release status\nstatus_is_released = []\nfor i in train['status']:\n    if i == 'Released':\n        status_is_released.append(1)\n    else:\n        status_is_released.append(0)\nstatus_is_released = pd.Series(status_is_released, name = 'status_is_released')\ntrain = pd.concat([train, status_is_released], axis=1)\ntrain['status_is_released'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"231c88e6da765ed75ef26c04282bfecdb0a7c6bd"},"cell_type":"code","source":"def data_processing(df):\n    # Create columns for year, month, and day of week\n    df['release_date'] = df['release_date'].fillna(method='ffill')\n    df['release_date'] = pd.to_datetime(df['release_date'], infer_datetime_format=True)\n    df['release_day'] = df['release_date'].apply(lambda t: t.day)\n    df['release_weekday'] = df['release_date'].apply(lambda t: t.weekday())\n    df['release_month'] = df['release_date'].apply(lambda t: t.month)\n    # Year was being interpreted as future dates in some cases so I had to adjust some values\n    df['release_year'] = df['release_date'].apply(lambda t: t.year if t.year < 2018 else t.year -100)\n    \n    # Function that will map the average runtime for each year to movies with 0 runtie.\n    def map_runtime(df):\n        df['runtime'].fillna(0)\n    \n        run = df[(df['runtime'].notnull()) & (df['runtime'] != 0)]\n        year_mean = run.groupby(['release_year'])['runtime'].agg('mean')\n        d = dict(year_mean)\n    \n        for i in df[df['runtime'] == 0]:\n            df['runtime'] = df.loc[:, 'release_year'].map(d)\n        return df\n    df = map_runtime(df)\n    \n    # For homepage, I'll change it to 0 for NaN and 1 if a page is listed.\n    df['homepage'].fillna(0, inplace=True)\n    df.loc[df['homepage'] != 0, 'homepage'] = 1\n    \n    # For poster_path, I'll change it to 0 for NaN and 1 if a path is listed.\n    df['poster_path'].fillna(0, inplace=True)\n    df.loc[df['poster_path'] != 0, 'poster_path'] = 1\n    \n    # release_year correlates strongly with budget, so I'll use that to estimate the null values\n    def map_budget(df):\n        d = defaultdict()\n        X = df[df['budget'] != 0]\n        year_mean = pd.Series(X.groupby(['release_year'])['budget'].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df[df['budget'] == 0]:\n            df['budget'] = df.loc[:, 'release_year'].map(d)\n    \n        # In a few cases, there are only 1 or 2 movies provided from a given year and are filled with Na values\n        df.budget = df.sort_values(by='release_year').budget.fillna(method='ffill')\n        return df\n    df = map_budget(df)\n    \n    # Fill remaining Na values\n    df['belongs_to_collection'] = df['belongs_to_collection'].fillna('none')\n    df.spoken_languages = df.spoken_languages.fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")\n    df.overview = df.overview.fillna('none')\n    df.Keywords = df.Keywords.fillna('none')\n    df.production_countries = df.production_countries.fillna(\n        \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")\n    df.genres = df.genres.fillna('18')\n    \n    ############ Feature Engineering ############\n    \n    # Create a columns for title length\n    title_len = []\n    for i in df['title']:\n        title_len.append(len(str(i).split()))\n    title_len = pd.Series(title_len, name='title_length')\n    df = pd.concat([df, title_len], axis=1)\n    \n    # Create columns for genres id's and for number of genres listed\n    genre_id = []\n    num_genre_types = []\n    for i in df['genres']:\n        i = re.findall('\\d+', str(i))\n        genre_id.append(i)\n    genre_id = pd.Series(genre_id, name='genre_id') #.apply(lambda x: ''.join([str(i) for i in x]))\n    \n    genre_id_1 = []\n    genre_id_2 = []\n    genre_id_3 = []\n    genre_id_4 = []\n    genre_id_5 = []\n    genre_id_6 = []\n    genre_id_7 = []\n\n    for i in genre_id:\n        try:\n            genre_id_1.append(i[:][0:1])\n            genre_id_2.append(i[:][1:2])\n            genre_id_3.append(i[:][2:3])\n            genre_id_4.append(i[:][3:4])\n            genre_id_5.append(i[:][4:5])\n            genre_id_6.append(i[:][5:6])\n            genre_id_7.append(i[:][6:7])\n        except:\n            genre_id_1.append('none')\n            genre_id_2.append('none')\n            genre_id_3.append('none')\n            genre_id_4.append('none')\n            genre_id_5.append('none')\n            genre_id_6.append('none')\n            genre_id_7.append('none')\n            \n    genre_id_1 = pd.Series(genre_id_1, name='genre_id_1').apply(''.join)\n    for i in genre_id_1[genre_id_1 == ''].index:\n        genre_id_1.iloc[i] = 'none'\n    genre_id_2 = pd.Series(genre_id_2, name='genre_id_2').apply(''.join)\n    for i in genre_id_2[genre_id_2 == ''].index:\n        genre_id_2.iloc[i] = 'none'\n    genre_id_3 = pd.Series(genre_id_3, name='genre_id_3').apply(''.join)\n    for i in genre_id_3[genre_id_3 == ''].index:\n        genre_id_3.iloc[i] = 'none'\n    genre_id_4 = pd.Series(genre_id_4, name='genre_id_4').apply(''.join)\n    for i in genre_id_4[genre_id_4 == ''].index:\n        genre_id_4.iloc[i] = 'none'\n    genre_id_5 = pd.Series(genre_id_5, name='genre_id_5').apply(''.join)\n    for i in genre_id_5[genre_id_5 == ''].index:\n        genre_id_5.iloc[i] = 'none'\n    genre_id_6 = pd.Series(genre_id_6, name='genre_id_6').apply(''.join)\n    for i in genre_id_6[genre_id_6 == ''].index:\n        genre_id_6.iloc[i] = 'none'\n    genre_id_7 = pd.Series(genre_id_7, name='genre_id_7').apply(''.join)\n    for i in genre_id_7[genre_id_7 == ''].index:\n        genre_id_7.iloc[i] = 'none'\n    \n    for i in genre_id.astype(str):\n        num_genre_types.append(len(i.split(',')))\n    num_genre_types = pd.Series(num_genre_types, name='num_genre_types').astype(int)\n    df = pd.concat([df, genre_id_1, genre_id_2, genre_id_3, genre_id_4, genre_id_5, \n                    genre_id_6, genre_id_7, num_genre_types], axis=1)\n    \n    # Create column for sequels\n    is_sequel = []\n    for i in df['Keywords']:\n        if 'sequel' in str(i):\n            is_sequel.append(1)\n        else:\n            is_sequel.append(0)\n    is_sequel = pd.Series(is_sequel, name='is_sequel')\n    df = pd.concat([df, is_sequel], axis=1)\n    \n    keyword_words = []\n    for i in df['Keywords']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        i = [word for word in i if word not in stopwords]\n        keyword_words.append(i)\n    keyword_words = pd.Series(keyword_words, name='keyword_words')\n    df = pd.concat([df, keyword_words], axis=1)\n\n    # This will count the number of Keywords listed for each film\n    num_keywords = []\n    for i in keyword_words:\n        num_keywords.append(len(str(i).split(',')))\n    num_keywords = pd.Series(num_keywords, name='num_keywords').astype(int)\n    df = pd.concat([df, num_keywords], axis=1)\n    \n    # Create column for Keyword Id numbers\n    keyword_ids = []\n    for i in df['Keywords']:\n        i = re.findall('[0-9]+', str(i))\n        keyword_ids.append(i)\n    keyword_ids = pd.Series(keyword_ids, name='keyword_ids')\n    #df = pd.concat([keyword_ids, df], axis=1)\n    \n    # Extract number from belongs to collection\n    collection_id = []\n    for i in df['belongs_to_collection']:\n        i = re.findall('[0-9]+', str(i))\n        collection_id.append(i[:1])\n    collection_id= pd.Series(collection_id, name='collection_id').apply(lambda x: ''.join([str(i) for i in x]))\n\n    # Fill in blank values with 'No Collection'\n    for i in collection_id[collection_id == ''].index:\n        collection_id.loc[i] = 'no collection'\n    collection_id = collection_id\n    df = pd.concat([df, collection_id], axis=1)\n    \n    # Add column with 1 for movies in a collection and 0 if not\n    is_in_collection = []\n    for i in df['collection_id']:\n        if i != 'no collection':\n            is_in_collection.append(1)\n        else:\n            is_in_collection.append(0)\n    is_in_collection = pd.Series(is_in_collection, name='is_in_collection').astype(int)\n    df = pd.concat([is_in_collection, df], axis=1)\n    \n    # Create a column for production country (1 for US, 0 for rest of world)\n    # It would be helpful if countries had different codes, but they all appear to be the same so it's difficult to work with\n    US_prod_country = []\n    for i in df['production_countries']:\n        if 'US' in str(i):\n            US_prod_country.append(1)\n        else:\n            US_prod_country.append(0)\n    US_prod_country = pd.Series(US_prod_country, name='US_prod_country')\n    df = pd.concat([df, US_prod_country], axis=1)\n    \n    # Create column for number of production countries\n    num_prod_countries = []\n    for i in df['production_countries']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        num_prod_countries.append(str(i).count('name'))\n    num_prod_countries = pd.Series(num_prod_countries, name='num_production_countries')\n    df = pd.concat([df, num_prod_countries], axis=1)\n    \n    # Create a column for each production company name and a column for the number of companies\n    production_company_names = []\n    num_production_companies = []\n    for i in df['production_companies']:\n        i = re.findall('[a-zA-Z \\t]+', str(i))\n        stopwords = ['id', 'name', ' ']\n        production_company_names.append([word for word in i if word not in stopwords])\n        num_production_companies.append(str(i).count('name'))\n\n    production_company_1 = []\n    production_company_2 = []\n    production_company_3 = []\n    production_company_4 = []\n    production_company_5 = []\n    production_company_6 = []\n    production_company_7 = []\n    production_company_8 = []\n\n    for i in production_company_names:\n        try:\n            production_company_1.append(i[:][0:1])\n            production_company_2.append(i[:][1:2])\n            production_company_3.append(i[:][2:3])\n            production_company_4.append(i[:][3:4])\n            production_company_5.append(i[:][4:5])\n            production_company_6.append(i[:][5:6])\n            production_company_7.append(i[:][6:7])\n            production_company_8.append(i[:][7:8])\n        except:\n            production_company_1.append('none')\n            production_company_2.append('none')\n            production_company_3.append('none')\n            production_company_4.append('none')\n            production_company_5.append('none')\n            production_company_6.append('none')\n            production_company_7.append('none')\n            production_company_8.append('none')\n\n    num_production_companies = pd.Series(num_production_companies, name='num_production_companies')\n    production_company_1 = pd.Series(production_company_1, name='production_company_1').apply(''.join)\n    for i in production_company_1[production_company_1 == ''].index:\n        production_company_1.iloc[i] = 'none'\n    production_company_2 = pd.Series(production_company_2, name='production_company_2').apply(''.join)\n    for i in production_company_2[production_company_2 == ''].index:\n        production_company_2.iloc[i] = 'none'\n    production_company_3 = pd.Series(production_company_3, name='production_company_3').apply(''.join)\n    for i in production_company_3[production_company_3 == ''].index:\n        production_company_3.iloc[i] = 'none'\n    production_company_4 = pd.Series(production_company_4, name='production_company_4').apply(''.join)\n    for i in production_company_4[production_company_4 == ''].index:\n        production_company_4.iloc[i] = 'none'\n    production_company_5 = pd.Series(production_company_5, name='production_company_5').apply(''.join)\n    for i in production_company_5[production_company_5 == ''].index:\n        production_company_5.iloc[i] = 'none'\n    production_company_6 = pd.Series(production_company_6, name='production_company_6').apply(''.join)\n    for i in production_company_6[production_company_6 == ''].index:\n        production_company_6.iloc[i] = 'none'\n    production_company_7 = pd.Series(production_company_7, name='production_company_7').apply(''.join)\n    for i in production_company_7[production_company_7 == ''].index:\n        production_company_7.iloc[i] = 'none'\n    production_company_8 = pd.Series(production_company_8, name='production_company_8').apply(''.join)\n    for i in production_company_8[production_company_8 == ''].index:\n        production_company_8.iloc[i] = 'none'\n    df = pd.concat([df, num_production_companies, production_company_1, production_company_2,\n              production_company_3, production_company_4, production_company_5, production_company_6,\n              production_company_7, production_company_8], axis=1)\n    \n    # Create a column for number of spoken languages\n    num_spoken_languages=[]\n    for i in df['spoken_languages']:\n        a = str(i).split()\n        num_spoken_languages.append(a.count(\"'name':\"))\n    num_spoken_languages = pd.Series(num_spoken_languages, name = 'num_spoken_languages')\n    df = pd.concat([df, num_spoken_languages], axis=1)\n        \n    # Create column for release status\n    status_is_released = []\n    for i in df['status']:\n        if i == 'Released':\n            status_is_released.append(1)\n        else:\n            status_is_released.append(0)\n    status_is_released = pd.Series(status_is_released, name = 'status_is_released')\n    df = pd.concat([df, status_is_released], axis=1)\n    \n    # Drop columns that have been engineered\n    df = df.drop(['belongs_to_collection', 'genres', 'Keywords', 'belongs_to_collection', 'homepage', 'imdb_id', \n                 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries',\n                 'release_date', 'spoken_languages', 'status', 'tagline', 'title', 'cast', 'crew'], axis=1)\n    # Drop 'keyword_words' column for now.  Can work with it later.\n    df = df.drop(['keyword_words'], axis=1)\n    return reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"d1923a95b113d10332179524225386b4d8f592be"},"cell_type":"code","source":"# Reload the data fresh and apply the processing function\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain = data_processing(train)\ntest = data_processing(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6c9c8061682a7dcad27797e6088175366f853dc"},"cell_type":"markdown","source":"#### Dealing with categorical columns"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"f36d3d4978274e43cc489a981aac5420b65044a2"},"cell_type":"code","source":"# There are 13 object columns that will need to be converted to numeric\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e4ad2bba6a604914441d6fd53b44dbc44bd40a4"},"cell_type":"markdown","source":"I'll label encode the category columns using sklearn."},{"metadata":{"trusted":false,"_uuid":"eafe16383b8e988fb18d8fff12d70dab56607328"},"cell_type":"code","source":"def train_target_encoded_year(df, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the average revenue for each unique value from a given year.\"\"\"\n    for i in cols:\n        d = df.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df = df.set_index(['release_year', i], drop=False)\n        df[i] = d.revenue\n        df = df.reset_index(drop=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8e6e373b1839dbf689625319598da7ffb458d309"},"cell_type":"code","source":"def test_target_encoded_year(df_train, df_test, cols):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    for i in cols:\n        d = df_train.groupby(['release_year', i]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', i], drop=False)\n        df_test[i] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n    return df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b404f5ee05a33212b4052dce9fb9edb8059c2be"},"cell_type":"code","source":"def target_encode(df, target_feature, m = 300): \n    d = defaultdict()\n    target_mean = df[target_feature].mean()\n    \n    # Map values and create dictionary   \n    for cat_feature in df.select_dtypes(include='category'):\n        group_target_mean = df.groupby([cat_feature])[target_feature].agg('mean')\n        group_target_count = df.groupby([cat_feature])[target_feature].agg('count')\n        smooth = (group_target_count * group_target_mean + m * target_mean) / (group_target_count + m)\n        k = pd.Series(df[cat_feature])\n        v = df[cat_feature].map(smooth)\n        d[cat_feature] = dict(zip(k, v))\n        df[cat_feature] = df[cat_feature].map(smooth)\n        \n    return df, d\n\nfor i in df_test[df_test.isnull()]: #df_test[df_test.isnull()].index\n    if i['release_year'] in d: # df_test.iloc[i]['release_year'] in d:\n        X[i] = X[i].map(d[i]) # df_test[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d87f88456a795ba5d7936e3b23dda12d678d18ff"},"cell_type":"code","source":"def test_target_encoded_year(df_train, df_test):\n    \"\"\"Function will take a dataframe and replace any passed categorical columns with the unique average revenue per year generated from the training dataframe.\"\"\"\n    cols = df_test.select_dtypes(include='object').columns\n    for col in cols:\n        d = df_train.groupby(['release_year', col]).agg({'revenue':'mean'})\n        df_test = df_test.set_index(['release_year', col], drop=False)\n        df_test[col] = d.revenue\n        df_test = df_test.reset_index(drop=True)\n        \n    # There are a numerous missing values in the test set after processing so I'll fill them with the yearly avg.\n    for col in cols:\n        #d = defaultdict()\n        X = df_test[df_test[col].notnull()]\n        year_mean = pd.Series(X.groupby(['release_year'])[col].agg('mean'))\n        d = dict(year_mean)\n    \n        for i in df_test[df_test['budget'].isnull()]:\n            df_test[col] = df_test.loc[:, 'release_year'].map(d)\n    \n    return reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"499332be552987657276b3ec493fff8dcbbd44ea"},"cell_type":"code","source":"# The numeric columns look okay, but budget may need normalization as the st. dev is quite large\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f72af2ad1e317d6059c6f87bb799f07b8381d3da"},"cell_type":"code","source":"# Budget normalization - Didn't improve model accuracy for linear regression (remained the same)\n#train.budget = (train.budget - train.budget.mean()) / (train.budget.max() - train.budget.min())\n#train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f54af665a0aeeb49280c9a8dcc1ea65f62c1b022"},"cell_type":"code","source":"from category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"860998886078317c7ce883bf41d8160f7bc36efc"},"cell_type":"code","source":"# Make complete list of genre ids\ngenre_ids = train['genre_id_1']\nfor i in train.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n    genre_ids = pd.concat([genre_ids, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(genre_ids)\ngenre_ids_dict = dict(zip(genre_ids, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n    train[i] = train[i].map(genre_ids_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dca4b56f87a9d0086965f15754ff045e73eed56f"},"cell_type":"code","source":"train.loc[:, 'production_company_2': 'production_company_8'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"917222fede43609ad030aac550d294e47ff4e0e2"},"cell_type":"code","source":"# Make complete list of production companies\nprod_companies = train['production_company_1']\nfor i in train.loc[:, 'production_company_2': 'production_company_8'].columns:\n    prod_companies = pd.concat([prod_companies, train[i]], axis=0)\n\nle = LabelEncoder()\nlab_enc = le.fit_transform(prod_companies)\nprod_companies_dict = dict(zip(prod_companies, lab_enc))\n\n# Map genre_ids_dict to genre_id columns\nfor i in train.loc[:, 'production_company_1': 'production_company_8'].columns:\n    train[i] = train[i].map(prod_companies_dict)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"9d775ced41029f403c50b60feaa1cc29c2520990"},"cell_type":"code","source":"le = LabelEncoder()\ntrain['collection_id'] = le.fit_transform(train['collection_id'])\ntrain['original_language'] = le.fit_transform(train['original_language'])\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"c0c740a9d475214d5edc8853b1de1b56a6fe934a"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef cat_encode(df):\n    le = LabelEncoder()\n    \n    # Make complete list of genre ids\n    genre_ids = df['genre_id_1']\n    for i in df.loc[:, 'genre_id_2': 'genre_id_7'].columns:\n        genre_ids = pd.concat([genre_ids, df[i]], axis=0)\n\n    lab_enc_genres = le.fit_transform(genre_ids)\n    genre_ids_dict = dict(zip(genre_ids, lab_enc_genres))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'genre_id_1': 'genre_id_7'].columns:\n        df[i] = df[i].map(genre_ids_dict)\n\n    # Make complete list of production companies\n    prod_companies = df['production_company_1']\n    for i in df.loc[:, 'production_company_2': 'production_company_8'].columns:\n        prod_companies = pd.concat([prod_companies, df[i]], axis=0)\n\n    lab_enc_comp = le.fit_transform(prod_companies)\n    prod_companies_dict = dict(zip(prod_companies, lab_enc_comp))\n\n    # Map genre_ids_dict to genre_id columns\n    for i in df.loc[:, 'production_company_1': 'production_company_8'].columns:\n        df[i] = df[i].map(prod_companies_dict)\n        \n    df['collection_id'] = le.fit_transform(df['collection_id'])\n    df['original_language'] = le.fit_transform(df['original_language'])\n    \n    return reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b946a3df8e4d12ea85eb2b9f811e3d93c812d14"},"cell_type":"code","source":"train = cat_encode(train)\ntest = cat_encode(test)\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"19871aaab517bc77e99687d13b234cd982f20a34"},"cell_type":"code","source":"# Get an idea of what correlates most strongly with revenue\nfor i in train.columns:\n    print(i, stats.pearsonr(train[i], train['revenue']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3580273b8da34b0f31c78b262116c47b57b53873"},"cell_type":"markdown","source":"#### Model Building and Parameter Tuning"},{"metadata":{"trusted":false,"_uuid":"aacb4faf2b04358e67acefa24fdd0696e650cdde"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9bedf6d10e462308df668d2b724bd714ffb39d67"},"cell_type":"code","source":"X = train.drop(['id', 'revenue'], axis=1)\ny = train['revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c1d69c30dbf03d0ce44a4f3f55c691e572a55d4"},"cell_type":"markdown","source":"I think it's useful to first use a basic linear regression model.  We can make a more complex model later."},{"metadata":{"trusted":false,"_uuid":"7c09b79cc03003453290c636d377679ad79485e6"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0bd483b059966b77bde39614e2e467021e2bf7d1"},"cell_type":"code","source":"sns.distplot((y_test-pred),bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"60659ec34bdfd63ef796f68d0a9c7fafef9a10a3"},"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return 'rmsle', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b3962ef4bf4b68568658ab1197e6a23ac874a46b"},"cell_type":"code","source":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7748ff8dff16424b2bff2c5721f49d58d8814b64"},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nlr = LGBMRegressor(boosting_type='dart', random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3d0a96bd9974db31c0bb3d60ca2371d15aa66e0"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4821b5051ef52fc8b512ceb217cfd24531774230"},"cell_type":"code","source":"# grid_1\nparams_1 = {'num_leaves': [20, 40, 60, 80, 100], #20 is best\n          'max_depth': [-1, 2, 4, 6, 8], # -1 is best\n          'min_data_in_leaf': [20, 50, 100, 200], #20 is best\n          #'learning_rate': [0.05, 0.1, 0.15, 0.2],\n          #'n_estimators': [100, 500, 1000],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6eed776682a4b19c51e9bc4be0400eb344e693fa"},"cell_type":"code","source":"grid_1 = GridSearchCV(lr, param_grid=params_1, scoring='neg_mean_squared_error', cv=5)\ngrid_1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8f23636c86d95b6a77fa9067e066799c0545a67"},"cell_type":"code","source":"print(grid_1.best_params_)\nprint(grid_1.best_score_)\nprint(grid_1.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f77b8d6b1ea3bfbc0747a52acecb6bc20830a1bf"},"cell_type":"code","source":"# grid_2\nparams_2 = {'num_leaves': [10, 15, 20], # 20 is best\n          'max_depth': [-1],\n          'min_data_in_leaf': [10, 15, 20], # 20 is best\n          #'learning_rate': [0.05, 0.1, 0.15, 0.2],\n          #'n_estimators': [100, 500, 1000],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4bfe8e73da249c8e3492fd797dfc615e0daefa2"},"cell_type":"code","source":"grid_2 = GridSearchCV(lr, param_grid=params_2, scoring='neg_mean_squared_error', cv=5)\ngrid_2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"45190811d1c09285f6bb2025cf89657bfff926d4"},"cell_type":"code","source":"print(grid_2.best_params_)\nprint(grid_2.best_score_)\nprint(grid_2.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"46227eeb6707365781cf505c3d622d04a92e6a62"},"cell_type":"code","source":"# grid_3\nparams_3 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.05, 0.1, 0.15, 0.2], # 0.2 is best\n          'n_estimators': [100, 250, 500, 1000], # 500 is best\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          #'subsample': [0.5, 0.75, 1.0],\n          'subsample_freq': [0],\n          #'colsample_bytree': [0.5, 0.75, 1.0],\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8f6d61d3efc892d60348318b5a32033383dbfcbf"},"cell_type":"code","source":"grid_3 = GridSearchCV(lr, param_grid=params_3, scoring='neg_mean_squared_error', cv=5)\ngrid_3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9fe74ef5c6a6e18a0d3a8bb50ab7927da27f0090"},"cell_type":"code","source":"print(grid_3.best_params_)\nprint(grid_3.best_score_)\nprint(grid_3.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"36770d5d3a4d59279344ce60a8b2a4a135d9fff8"},"cell_type":"code","source":"# grid_4\nparams_4 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.2], \n          'n_estimators': [500],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          'subsample': [0.1, 0.25, 0.5, 0.75, 1.0], # 0.1 is best\n          'subsample_freq': [0],\n          'colsample_bytree': [0.1, 0.25, 0.5, 0.75, 1.0], # 0.75 is best\n          #'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1],\n          #'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1],\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d915d07781f7c9c3d434c64723a0f16565df95c0"},"cell_type":"code","source":"grid_4 = GridSearchCV(lr, param_grid=params_4, scoring='neg_mean_squared_error', cv=5)\ngrid_4.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f8e7a661d2d8431bef710bbd2f317479e613b32"},"cell_type":"code","source":"print(grid_4.best_params_)\nprint(grid_4.best_score_)\nprint(grid_4.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99f8888681902d78c2e9c6c6bb0a33322b5018e1"},"cell_type":"code","source":"# grid_5\nparams_5 = {'num_leaves': [20],\n          'max_depth': [-1],\n          'min_data_in_leaf': [20],\n          'learning_rate': [0.2], \n          'n_estimators': [500],\n          'subsample_for_bin': [200000],\n          #'objective': 'regression',\n          #'class_weight': None,\n          'min_split_gain': [0.0],\n          'min_child_weight': [0.001],\n          'subsample': [0.1],\n          'subsample_freq': [0],\n          'colsample_bytree': [0.75],\n          'reg_alpha': [0.0, 0.25, 0.5, 0.75, 1], # 0 is best\n          'reg_lambda': [0.0, 0.25, 0.5, 0.75, 1], # 0 is best\n          'random_state': [101],\n          'n_jobs': [-1]\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15ec406c0d38f6b0df6eabde4d7298ab8410da99"},"cell_type":"code","source":"grid_5 = GridSearchCV(lr, param_grid=params_4, scoring='neg_mean_squared_error', cv=5)\ngrid_5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c08f73775dd60a7ba74f3e559618b7d4338d473c"},"cell_type":"code","source":"print(grid_5.best_params_)\nprint(grid_5.best_score_)\nprint(grid_5.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"271d2519c75395ed7167b4d99d9ef20bfd4c83ac"},"cell_type":"code","source":"lr = LGBMRegressor(boosting_type='dart',\n                   num_leaves=20,\n                   max_depth=-1,\n                   min_data_in_leaf=20, \n                   learning_rate=0.2,\n                   n_estimators=500,\n                   subsample_for_bin=200000,\n                   #objective='regression',\n                   class_weight=None,\n                   min_split_gain=0.0,\n                   min_child_weight=0.001,\n                   subsample=0.1,\n                   subsample_freq=0,\n                   colsample_bytree=0.75,\n                   reg_alpha=0.0,\n                   reg_lambda=0.0,\n                   random_state=101,\n                   n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"398b69323317b0f64b8cfaed645df606ca07e2bb"},"cell_type":"code","source":"lr.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=rmsle,\n        early_stopping_rounds=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7585f5ebb3ea66e8e3779812c5d5f76031585766"},"cell_type":"code","source":"pred = lr.predict(X_test, num_iteration=lr.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f9b36ee33c243160fbcd5f7141c4fdb3a10f19fa"},"cell_type":"code","source":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\nprint('RMSLE:', rmsle(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"871d7c1d8a8b41c1d00c79c8e18d432c0a714087"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['revenue'] = lr.predict(test.drop('id', axis=1), num_iteration=lr.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4f70a664874c9a7a04d4bf23ebfd7cb10f9a1c50"},"cell_type":"code","source":"submission.to_csv('TMDB_test_predictions.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}