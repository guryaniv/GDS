{"cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "879ce38f-d26c-4a80-84d3-d0173841b597", "_execution_state": "busy", "_uuid": "5536dbba67695acf82112e993e4bcf9c66343490"}, "source": ["comparing all clustering, regressing methods: the forecast errors on raw data (withoug logarithm), it's remarkably 50% high... then we refine to find a narrow foracasting error\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "f547012d-8676-4efe-84c9-607244d7d7ba", "_execution_state": "busy", "collapsed": true, "_uuid": "0d89983d2fc791d275c3b77dbf93d1c7576b3448"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn import model_selection as b\n", "from haversine import haversine\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.cluster import KMeans"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0540bea4-a0d1-46ee-a1fc-b9344a93f7b6", "_execution_state": "busy", "collapsed": true, "_uuid": "2e21b645a2e2a910beba92bb34b8ae9bbc93643f"}, "source": ["ds=pd.read_csv('../input/train.csv')[:10000]\n", "test=pd.read_csv('../input/test.csv')[:10000]"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "93983c06-a18d-4154-a5ec-b09ac3aff4b1", "_execution_state": "busy", "collapsed": true, "_uuid": "f7a83e6046b020e5413ba9d62f68f0ea98dfb9aa"}, "source": ["def dddraw(X_reduced,name):\n", "    import matplotlib.pyplot as plt\n", "    from mpl_toolkits.mplot3d import Axes3D\n", "    # To getter a better understanding of interaction of the dimensions\n", "    # plot the first three PCA dimensions\n", "    fig = plt.figure(1, figsize=(8, 6))\n", "    ax = Axes3D(fig, elev=-150, azim=110)\n", "    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n", "    titel=\"First three directions of \"+name \n", "    ax.set_title(titel)\n", "    ax.set_xlabel(\"1st eigenvector\")\n", "    ax.w_xaxis.set_ticklabels([])\n", "    ax.set_ylabel(\"2nd eigenvector\")\n", "    ax.w_yaxis.set_ticklabels([])\n", "    ax.set_zlabel(\"3rd eigenvector\")\n", "    ax.w_zaxis.set_ticklabels([])\n", "\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "64d41bd7-eb49-4673-9942-e77e51de23da", "_uuid": "a352561b27bb8ff9ccd7e642b9af2aef61f5640d"}, "source": ["What do we see ?\n", "---\n", "\n", "* PCA shows 2x6 clusters grouped in two clusters, and forecast 7.9% error\n", "* FastICa shows 2 clusters, didn't converge\n", "* gauss is not so interesting 10% error\n", "* KMeans has also that same 2x6 clusters, 8.9% error\n", "* sparseRP separates in similar 2x6 clustersn, 7.9% error\n", "* Birch has the same separation as Kmeans, 9.7% error\n", "* NMF shows one big cloud, and evenhow gets 8.6%error\n", "\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9dac59b9-bde7-4fa1-8bac-261b1794aa74", "_execution_state": "busy", "scrolled": false, "_uuid": "0d524c652c7d173aa4ead1490f365711a7fc95fc"}, "source": ["from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\n", "from sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\n", "from sklearn.cluster import KMeans,Birch\n", "import statsmodels.formula.api as sm\n", "from scipy import linalg\n", "from sklearn import preprocessing\n", "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n", "def rmsle(y_predicted, y_real):\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\n", "def procenterror(y_predicted, y_real):\n", "     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n", "\n", "n_col=28\n", "X = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','trip_duration'],axis=1) # we only take the first two features.\n", "le = preprocessing.LabelEncoder()\n", "def rmsle(y_predicted, y_real):\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\n", "def procenterror(y_predicted, y_real):\n", "     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n", "\n", "    \n", "#le.fit(np.round(ds['trip_duration']/30))\n", "#print(list(le.classes_))\n", "#Y=le.transform(np.round(ds['trip_duration']/30))\n", "Y=np.round(np.log(ds['trip_duration'])*25)\n", "scaler = MinMaxScaler()\n", "scaler.fit(X)\n", "X=scaler.transform(X)\n", "poly = PolynomialFeatures(2)\n", "X=poly.fit_transform(X)\n", "\n", "\n", "names = [\n", "         'PCA',\n", "         'FastICA',\n", "         'Gauss',\n", "         'KMeans',\n", "         #'SparsePCA',\n", "         'SparseRP',\n", "         'Birch',\n", "         'NMF',    \n", "       #  'LatentDietrich',    \n", "        ]\n", "\n", "classifiers = [\n", "    \n", "    PCA(n_components=n_col),\n", "    FastICA(n_components=n_col),\n", "    GaussianRandomProjection(n_components=3),\n", "    KMeans(n_clusters=10),\n", "    #SparsePCA(n_components=n_col),\n", "    SparseRandomProjection(n_components=n_col, dense_output=True),\n", "    Birch(branching_factor=10, n_clusters=3, threshold=0.5),\n", "    NMF(n_components=n_col),    \n", "  #  LatentDirichletAllocation(n_topics=n_col),\n", "    \n", "]\n", "correction= [1,1,0,0,0,0,0,0,0]\n", "\n", "temp=zip(names,classifiers,correction)\n", "print(temp)\n", "\n", "for name, clf,correct in temp:\n", "    Xr=clf.fit_transform(X,Y)\n", "    dddraw(Xr,name)\n", "    res = sm.OLS(Y,Xr).fit()\n", "    #print(res.summary())  # show OLS regression\n", "    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n", "    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n", "    \n", "    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n", "    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #\n", "    \n", "    \n", "    \n", "    "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a1a4dd94-a0a4-43d5-9bfc-e3e8fce23607", "_execution_state": "busy", "collapsed": true, "_uuid": "40aacba6f7c4f0cec06ef6a7b2afd3aa4ccd23fc"}, "source": ["pl=np.array(ds['pickup_latitude'])\n", "plo=np.array(ds['pickup_longitude'])\n", "dl=np.array(ds['dropoff_latitude'])\n", "dlo=np.array(ds['dropoff_longitude'])\n", "pl=pl.reshape(-1,1)\n", "plo=plo.reshape(-1,1)\n", "dl=dl.reshape(-1,1)\n", "dlo=dlo.reshape(-1,1)\n", "m = pl.size\n", "n = 4\n", "xk = np.empty([m,n], float)\n", "xk[:,0]=pl[:,0]\n", "xk[:,1]=plo[:,0]\n", "xk[:,2]=dl[:,0]\n", "xk[:,3]=dlo[:,0]"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "90f39ac8-d1a8-467e-8574-9bc6e74555b3", "_uuid": "108159c205db8ef4318a03b48056c03ae41bc946"}, "source": ["Lets cluster in 24 groups \n", "---\n", "even though there are 12 groups visible in the clusters it gives some redundancy. And we try to find a statistical tripduration/standardevation, and efficiency clustering\n", "Its simply a grouping of start-stop places and helps to forecast the typical speed for such a distance\n", "\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "547d1405-f144-42cf-aa66-14e45097ce6f", "collapsed": true, "_uuid": "e5ea8085f6ed790f413fd0b8961420258b443a2d"}, "source": ["from geopy.distance import vincenty\n", "ds['geo_distance']=ds.apply(lambda x: vincenty((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude'])).miles,axis=1)\n", "test['geo_distance']=test.apply(lambda x: vincenty((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude'])).miles,axis=1)\n", "ds['time_per_km']=np.log(ds['trip_duration']/(ds['geo_distance']/60+0.0001)+1)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6ce3afdb-1e83-4bcc-b0b0-50eb7fd8a361", "collapsed": true, "_uuid": "59e1463c9d0bd506357dfa32c40c4661f1e79a12"}, "source": ["kmeans = KMeans(n_clusters=48, random_state=0).fit(xk)\n", "ds['pickup_id']=kmeans.labels_"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "cdbe446f-fd80-4d1a-bc6c-724b6025267c", "_execution_state": "busy", "_uuid": "d45542adb07d46d29ad804a99b265d5d5dfa1773"}, "source": ["def clust(x):\n", "    kl=0\n", "    if x<0.3:\n", "        kl=1\n", "    if x>0.29 and x<0.6:\n", "        kl=2\n", "    if x>0.59:\n", "        kl=4\n", "    return kl\n", "ds['log_trip_duration']=np.log(ds['trip_duration'])\n", "new_col= ds[['time_per_km','log_trip_duration','pickup_id']].groupby('pickup_id').describe().fillna(method='bfill')\n", "new_col.columns=['countt','meant','stdt','mint','p25t','p50t','p75t','maxt','countgt','meangt','stdgt','mingt','p25gt','p50gt','p75gt','maxgt']\n", "new_col['efft']=new_col['stdt']/new_col['meant']\n", "new_col['eff2t']=new_col['efft']*new_col['stdt']\n", "new_col['clustt']=new_col['eff2t'].map(clust)\n", "print(new_col.head())\n", "ds=pd.merge(ds,new_col, how='outer', left_on='pickup_id',suffixes=('', '_c'), right_index=True)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "47976693-e717-44eb-bcd3-a3c720795e21", "_uuid": "2072f716ef7fd5461b4680d2ad874b91fbd6c883"}, "source": ["HourofWeek\n", "----\n", "Another grouping statistics. based upon the dayofweek and hour of day combination. There are 168hours per week. And each hour has his average speed and efficiency properties\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9dc99c8e-897a-46d3-a39f-210e5f24ee48", "_execution_state": "busy", "_uuid": "14c939f60686cf787dea1e264881d2c65a29e971"}, "source": ["ds['pickup_datetime']=pd.to_datetime(ds['pickup_datetime'])\n", "test['pickup_datetime']=pd.to_datetime(test['pickup_datetime'])\n", "\n", "ds['pickup_weekday']=ds['pickup_datetime'].dt.weekday\n", "ds['pickup_hour']=ds['pickup_datetime'].dt.hour\n", "ds['pickup_month']=ds['pickup_datetime'].dt.month\n", "ds['pickup_weekhour']=ds['pickup_weekday']*24+ds['pickup_hour']\n", "\n", "new_col= ds[['log_trip_duration','time_per_km','pickup_weekhour']].groupby('pickup_weekhour').describe().fillna(method='bfill')\n", "new_col.columns=['countt','meant','stdt','mint','p25t','p50t','p75t','maxt','countgt','meangt','stdgt','mingt','p25gt','p50gt','p75gt','maxgt']\n", "new_col['efft']=new_col['stdt']/new_col['meant']\n", "new_col['eff2t']=new_col['efft']*new_col['stdt']\n", "new_col['clustt']=new_col['eff2t'].map(clust)\n", "print(new_col.head())\n", "ds=pd.merge(ds,new_col, how='outer', left_on='pickup_weekhour',suffixes=('', '_wh'), right_index=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ccef08e0-072f-4cf7-8ef4-f9058cb912a7", "_execution_state": "busy", "collapsed": true, "_uuid": "217de66e63c8946cbfeafd94f3e2c213cb31413a"}, "source": ["#ds['distance']=ds.apply(lambda x: haversine((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude']),miles=True),axis=1)\n", "#test['distance']=test.apply(lambda x: haversine((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude']),miles=True),axis=1)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0007439b-3fd3-452e-8512-8a2e6d99aeeb", "_uuid": "3d82462f183d3017a4ea2fed377f2b8ab9354ecc"}, "source": ["The distance and the average time per distance\n", "---"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d8a63cbd-c2eb-4119-bc81-4818af861157", "_execution_state": "busy", "collapsed": true, "scrolled": true, "_uuid": "7387974e6332613c42a5252b64b35d1d6cb001d7"}, "source": ["loc_df = pd.DataFrame()\n", "loc_df['longitude'] = list(ds.pickup_longitude) + list(ds.dropoff_longitude)\n", "loc_df['latitude'] = list(ds.pickup_latitude) + list(ds.dropoff_latitude)\n", "kmeans = KMeans(n_clusters=24, random_state=2, n_init = 10).fit(loc_df)\n", "loc_df['label'] = kmeans.labels_\n", "ds['pickup_cluster'] = kmeans.predict(ds[['pickup_longitude','pickup_latitude']])\n", "ds['dropoff_cluster'] = kmeans.predict(ds[['dropoff_longitude','dropoff_latitude']])\n", "test['pickup_cluster'] = kmeans.predict(test[['pickup_longitude','pickup_latitude']])\n", "test['dropoff_cluster'] = kmeans.predict(test[['dropoff_longitude','dropoff_latitude']])"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9f85482f-f2c4-4db7-a019-b3ce47a49cb9", "_execution_state": "busy", "collapsed": true, "_uuid": "22c6e8a720ba4f393a1c76887fe8d1a6cd582789"}, "source": ["test['pickup_weekday']=test['pickup_datetime'].dt.weekday\n", "test['pickup_hour']=test['pickup_datetime'].dt.hour\n", "test['pickup_month']=test['pickup_datetime'].dt.month\n", "test['pickup_weekhour']=test['pickup_weekday']*24+test['pickup_hour']"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4471d33b-59b5-4dda-a59a-238d6288b03c", "_execution_state": "busy", "collapsed": true, "_uuid": "d97ebe304cecb90985b84ecd5f3ce287665ec5b8"}, "source": ["test['isweekend']= test.apply(lambda x : (x['pickup_weekday']==6 | x['pickup_weekday']==5),axis=1)\n", "test['isweekend']=test['isweekend'].map({True: 1, False:0})\n", "test['store_and_fwd_flag']=test['store_and_fwd_flag'].map({'N': 1, 'Y':0})"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "26bdcb77-2a3b-4610-af79-9de7ad840cb1", "_uuid": "0823d58f399c7920af14a383223f67528bbd205a"}, "source": ["Ypl = np.round(np.log(ds['trip_duration']))\n", "plt.figure()\n", "Ypl.plot.hist(alpha=0.5)\n", "\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "487a36ed-d94b-4dc9-9ce1-0bc8e9268518", "_uuid": "e69e8f56c6c43f436f553a6227ec308a5814070c"}, "source": ["Clustering again shows smaller error\n", "----\n", "* PCA and FastIca improves to 6.6% error\n", "* Gauss is worse\n", "* Kmeans, improves to 7%, SparseRP improves minimally\n", "\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1ce99c62-70c9-49a2-9708-e99b41f04fea", "_execution_state": "busy", "_uuid": "681882b9372afceace8e098f5ba3ac436652f810"}, "source": ["from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\n", "from sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\n", "from sklearn.cluster import KMeans,Birch\n", "import statsmodels.formula.api as sm\n", "from scipy import linalg\n", "from sklearn import preprocessing\n", "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n", "import matplotlib.pyplot as plt\n", "\n", "def rmsle(y_predicted, y_real):\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\n", "def procenterror(y_predicted, y_real):\n", "     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n", "\n", "n_col=28\n", "X = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','trip_duration','log_trip_duration','time_per_km'],axis=1) # we only take the first two features.\n", "le = preprocessing.LabelEncoder()\n", "def rmsle(y_predicted, y_real):\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\n", "def procenterror(y_predicted, y_real):\n", "     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n", "\n", "    \n", "le.fit(np.round(np.log(ds['trip_duration'])*10))\n", "\n", "Y=le.transform(np.round(np.log(ds['trip_duration'])*10))\n", "\n", "Y=np.round(np.log(ds['trip_duration'])*10)\n", "               \n", "scaler = MinMaxScaler()\n", "scaler.fit(X)\n", "X=scaler.transform(X)\n", "poly = PolynomialFeatures(2)\n", "X=poly.fit_transform(X)\n", "\n", "\n", "names = [\n", "         'PCA',\n", "         #'FastICA',\n", "         'Gauss',\n", "         'KMeans',\n", "          #'SparsePCA',\n", "         'SparseRP',\n", "         #'Birch',\n", "      #   'NMF',    \n", "      #   'LatentDietrich',    \n", "        ]\n", "\n", "classifiers = [\n", "    \n", "    PCA(n_components=n_col),\n", "   # FastICA(n_components=n_col),\n", "    GaussianRandomProjection(n_components=3),\n", "    KMeans(n_clusters=24),\n", " #   SparsePCA(n_components=n_col),\n", "    SparseRandomProjection(n_components=n_col, dense_output=True),\n", "  #  Birch(branching_factor=10, n_clusters=12, threshold=0.5),\n", "  #  NMF(n_components=n_col),    \n", " #   LatentDirichletAllocation(n_topics=n_col),\n", "    \n", "]\n", "correction= [1,1,0,0,0,0,0,0,0]\n", "\n", "temp=zip(names,classifiers,correction)\n", "print(temp)\n", "\n", "for name, clf,correct in temp:\n", "    Xr=clf.fit_transform(X,Y)\n", "    dddraw(Xr,name)\n", "    res = sm.OLS(Y,Xr).fit()\n", "    #print(res.summary())  # show OLS regression\n", "    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n", "    print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n", "\n", "    print('Ypredict *log_sec',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n", "    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y))"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d1ee34fe-7b1f-4386-a480-d1389fa2f809", "_uuid": "491b357b7420963bbc4973f2b262645bd4656969"}, "source": ["\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "96b33f20-c0f9-4dad-a90b-6d1eeae6613a", "_uuid": "7815531676aa15416e3abb96493dd9ecb93bba11"}, "source": ["linear models forecast with lowest error for time being\n", "----\n", "\n", "* ElasticNet %error 6.24 rmsle 0.104493207131\n", "* HuberRegressor %error 6.06 rmsle 0.103178076065\n", "* Ridge %error 5.91 rmsle 0.0995130160778\n", "* Lasso %error 6.43 rmsle 0.107492482631\n", "* LassoCV %error 6.01 rmsle 0.100907758631\n", "* Lars %error 6.45 rmsle 0.107083059727\n", "* BayesianRidge %error 5.92 rmsle 0.0996487967579\n", "* SGDClassifier %error 10.1 rmsle 0.174949522616\n", "* RidgeClassifier %error 7.13 rmsle 0.123412139794\n", "* LogisticRegression %error 7.03 rmsle 0.116931520745\n", "* OrthogonalMatchingPursuit %error 6.31 rmsle 0.104854612413"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1b79a30c-a4e6-4458-8c27-78621350c40a", "_execution_state": "busy", "_uuid": "6193cc69a7cdbcf59ddd483dd0075c663fb4ebba"}, "source": ["from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "# import some data to play with\n", "n_col=50\n", "X = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','time_per_km','log_trip_duration','trip_duration'],axis=1) # we only take the first two features.\n", "Y=np.round(ds['trip_duration']/30)\n", "Y=np.round(np.log(ds['trip_duration'])*10)\n", "\n", "scaler = MinMaxScaler()\n", "scaler.fit(X)\n", "X=scaler.transform(X)\n", "\n", "\n", "def rmsle(y_predicted, y_real):\n", "    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\n", "def procenterror(y_predicted, y_real):\n", "     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,2)\n", "\n", "names = [\n", "         'ElasticNet',\n", "         'HuberRegressor',\n", "         'Ridge',\n", "         'Lasso',\n", "         'LassoCV',\n", "         'Lars',\n", "         'BayesianRidge',\n", "         'SGDClassifier',\n", "         'RidgeClassifier',\n", "         'LogisticRegression',\n", "         'OrthogonalMatchingPursuit',\n", "         #'RANSACRegressor',\n", "         ]\n", "\n", "classifiers = [\n", "    ElasticNetCV(cv=10, random_state=0),\n", "    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n", "    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n", "    Lasso(alpha=0.05),\n", "    LassoCV(),\n", "    Lars(n_nonzero_coefs=10),\n", "    BayesianRidge(),\n", "    SGDClassifier(),\n", "    RidgeClassifier(),\n", "    LogisticRegression(),\n", "    OrthogonalMatchingPursuit(),\n", "    #RANSACRegressor(),\n", "]\n", "correction= [0,0,0,0,0,0,0,0,0,0,0,0]\n", "\n", "temp=zip(names,classifiers,correction)\n", "print(temp)\n", "\n", "for name, clf,correct in temp:\n", "    regr=clf.fit(X,Y)\n", "    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n", "    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "23bbb3a6-8d0a-4f60-af86-a1a257dc2641", "_uuid": "662d6c9d2a599dd0ab897ded1ba36b3b69a52841"}, "source": ["XGB superiority \n", "----\n", "error 3.6%"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d6f8d180-0757-4a73-b504-33653a2eef97", "_uuid": "811b7dc2a9e1c36f3b43aa36b409c7c4fa97823e"}, "source": ["import xgboost as xgb\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "\n", "#train1=total[total['split']==0]\n", "#test1=total[total['split']==1]\n", "X = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','time_per_km','log_trip_duration','trip_duration'],axis=1) # we only take the first two features.\n", "Y=np.round(np.log(ds['trip_duration'])*10)\n", "poly = PolynomialFeatures(2)\n", "X=poly.fit_transform(X)\n", "\n", "Xtr, Xv, ytr, yv = train_test_split(X, Y, test_size=0.2, random_state=2017)\n", "dtrain = xgb.DMatrix(Xtr, label=ytr)\n", "dvalid = xgb.DMatrix(Xv, label=yv)\n", "#dtest = xgb.DMatrix(test1[feature_names].values)\n", "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "# Try different parameters! My favorite is random search :)\n", "xgb_pars = {'min_child_weight': 100, 'eta': 0.03, 'colsample_bytree': 0.3, 'max_depth': 10,\n", "            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n", "            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n", "\n", "# You could try to train with more epoch\n", "model = xgb.train(xgb_pars, dtrain, 2000, watchlist, early_stopping_rounds=25,\n", "                  maximize=False, verbose_eval=50)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "68eefde2-1050-4b38-af85-f0a167b2e564", "_uuid": "a67562584a4346623ae6a84995c06af8c69f922c"}, "source": ["print('XGB %error',procenterror(model.predict(dtrain),ytr),'rmsle',rmsle(model.predict(dtrain),ytr))\n"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}