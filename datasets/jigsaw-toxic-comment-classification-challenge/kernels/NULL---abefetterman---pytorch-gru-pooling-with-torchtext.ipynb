{"cells":[{"metadata":{"_cell_guid":"812f52c2-62a4-492e-b237-a69db99341e4","_uuid":"71ca7f6fb533b6cc67b52b824d8cbeac739a7032"},"cell_type":"markdown","source":"Pytorch hasn't been working for me on kaggle, but I've been developing with it locally. I use the torchtext library to wrangle the data, and while it's not quite ready for prime time, it does streamline a lot of that. I like that in pytorch we can see more of the internals of what's happening vs keras, but it is still a lot easier to read than tensorflow. My local output got 0.9799 LB score, which is the same as I got from this keras version: https://www.kaggle.com/antmarakis/gru-pooling-lightweight"},{"metadata":{"_cell_guid":"53335c2d-a596-4595-85b7-e9bb0b294b8a","_uuid":"8dcde073cda54e2386d085989dd06e49ccccc2d1","collapsed":true,"trusted":true},"cell_type":"code","source":"from torchtext import data\nid_label = 'id'\ntext_label = 'comment_text'\nlabel_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\npath='../input/'\ntrain_file = 'train.csv'\ntest_file = 'test.csv'\n\nembedding_file = '../input/glove6b300dtxt/glove.6B.300d.txt'\n\n# some iterators produce StopIteration, which is no longer a warning, we don't need to hear about it\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e5ecb38-59e2-4347-a3ce-b7343f85fde8","_uuid":"dd69b33eda34673ba8fc57b3b856d7d7c10ea5ac"},"cell_type":"markdown","source":"The default dataset reader doesn't like the encoding of the toxic data, so we need our own dataset definition, just to set the CSV encoding."},{"metadata":{"_cell_guid":"1c7dc352-83f7-4a20-8e75-d6d17a4bbcd7","_uuid":"2d0e5a2c73060d0935acd60a9854b47919f4b378","collapsed":true,"trusted":true},"cell_type":"code","source":"import io,os,csv\n\nclass ToxicDataset(data.Dataset):\n    \"\"\"Defines a Dataset of columns stored in CSV format.\"\"\"\n\n    def __init__(self, path, fields, skip_header=True, **kwargs):\n        with io.open(os.path.expanduser(path), encoding=\"utf8\") as f:\n            reader = csv.reader(f)\n                \n            if skip_header:\n                next(reader)\n\n            examples = [data.Example.fromlist(line, fields) for line in reader]\n\n        super(ToxicDataset, self).__init__(examples, fields, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc417672-0914-4aa0-8874-a1350180716c","_uuid":"8a072beca08bc8246eb09fd4db310ba2e3dc5a11"},"cell_type":"markdown","source":"Define the fields:\n- lower: converts text to lowercase\n- include_lengths: includes a separate array with the length\n- tokenize: set to 'spacy' to use spacy tokenizer, requires spacy to be installed. Otherwise will just split on whitespace.\n- fix_length: Will buffer or trim fields to this length. Not required but speeds up processing significantly from trimming super long comments\n- sequential: if False, won't do any tokenization (whole field is the token)\n- use_vocab: if False, data must be numeric"},{"metadata":{"_cell_guid":"16030964-d267-4575-9af2-720f564757d8","_uuid":"f79fd26fc49fd142959e01f9b0e0896406aa69bf","collapsed":true,"trusted":true},"cell_type":"code","source":"# Define all the types of fields\n# pip install spacy for the tokenizer to work (or remove to use default)\nTEXT = data.Field(lower=True, include_lengths=True, fix_length=150, tokenize='spacy')\nLABEL = data.Field(sequential=False, use_vocab=False)\n\n# we use the index field to re-sort test data after processing\nINDEX = data.Field(sequential=False)\n\ntrain_fields=[\n    (id_label, INDEX),\n    (text_label, TEXT)\n]\nfor label in label_cols:\n    train_fields.append((label,LABEL))\n\ntrain_data = ToxicDataset(\n            path=f'{path}{train_file}',\n            fields=train_fields\n        )\n\ntest_fields=[\n    (id_label, INDEX),\n    (text_label, TEXT)\n]\ntest_data = ToxicDataset(\n            path=f'{path}{test_file}',\n            fields=test_fields\n        )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ccc36821-be8f-4be4-b7a1-8ad1997adf94","_uuid":"5a3589b20286a19d08c8100f2c8030c358155611"},"cell_type":"markdown","source":"Build the vocab, for the index it will numericalize. For the text, locally, we can choose from predefined vectors:\n- charngram.100d\n- fasttext.en.300d\n- fasttext.simple.300d\n- glove.42B.300d\n- glove.840B.300d\n- glove.twitter.27B.25d\n- glove.twitter.27B.50d\n- glove.twitter.27B.100d\n- glove.twitter.27B.200d\n- glove.6B.50d\n- glove.6B.100d\n- glove.6B.200d\n- glove.6B.300d\n\nThe proper files will be downloaded to .vector_cache/ if necessary. For kaggle we use the path to \n\nWorth noticing that a few extra tokens are tacked on: `<unk>` and `<pad>`"},{"metadata":{"_cell_guid":"8d0d74f7-5b48-40c2-a310-fe1b75f44fdf","_uuid":"cac3347db9b90accd1a8c7d88a7560d7ab9e7860","collapsed":true,"trusted":true},"cell_type":"code","source":"from torchtext.vocab import Vectors\n# This will download the glove vectors, see torchtext source for other options\nmax_size = 30000\nTEXT.build_vocab(train_data, test_data, vectors=Vectors(embedding_file), max_size=max_size)\nINDEX.build_vocab(test_data)\n\n# print vocab information\nntokens = len(TEXT.vocab)\nprint('ntokens', ntokens)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47d803f5-8266-486c-8a15-bc3bea678589","_uuid":"588e5690224222d4629ef89ef86646c4e83ddba2"},"cell_type":"markdown","source":"The BucketIterator will shuffle the data and produce batches with sequences of roughly the same length. If we didn't want to split into epochs, we could set repeat=True and run for a set number of batches (rather than epochs). Must have `sort_within_batch=True` to use the lengths we picked up earlier.\n\nWe also define convenience methods to access the comment text and labels from the batch"},{"metadata":{"_cell_guid":"c5883bfa-56d3-4712-a7ed-40bb7337e591","_uuid":"0506dbd6efdc71ec7c9f565df527c7b08e19346a","collapsed":true,"trusted":true},"cell_type":"code","source":"train = data.BucketIterator(train_data, batch_size=32,\n                                sort_key=lambda x: len(x.comment_text),\n                                sort_within_batch=True, repeat=False)\ntest = data.BucketIterator(test_data, batch_size=128,\n                                sort_key=lambda x: len(x.comment_text),\n                                sort_within_batch=True, train=False, repeat=False)\n\ndef get_text(batch):\n    return getattr(batch, text_label)\ndef get_labels(batch):\n    # Get the labels as one tensor from the batch object\n    return torch.cat([getattr(batch, label).unsqueeze(1) for label in label_cols], dim=1).float()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"024a4a53-1c35-4083-b7d3-f44e95734ca7","_uuid":"4a9fa6aa4c78dd968bb9424681eb4515d2dcf409"},"cell_type":"markdown","source":"Here is the meat of the model. A few points to notice in `__init__`:\n- `Dropout2d` is a spatial dropout function, which will drop entire layers (rather than just individial connections). It doesn't necessarily require 2d data\n- We define `self.rnns` as a ModuleList so that all of the sub-components will be discovered properly\n- The pools require an argument that is number of output segments, but we just want a global one for each avg/max\n\nand in `forward`:\n- We move to/from a packed sequence for the rnn section if we have the lengths\n- We need to rearrange the output of the rnn to have sequence last for pooling layers\n- We don't have a sigmoid output because we will later use a special loss function that takes the logit output directly"},{"metadata":{"_cell_guid":"39fefe7f-ea6f-4d3f-a3f0-555af1198e38","_uuid":"ae665aee5cdf7089962cd1e789c0935e01c3af2e","collapsed":true,"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNNModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nout, nlayers, dropemb=0.2, droprnn=0.0, bidirectional=True):\n        super(RNNModel, self).__init__()\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.drop = nn.Dropout2d(dropemb)\n        self.ndir = 2 if bidirectional else 1\n        assert rnn_type in ['LSTM', 'GRU'], 'RNN type is not supported'\n        if rnn_type == 'LSTM':\n            self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n        if rnn_type == 'GRU':\n            self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid*self.ndir, nhid, 1, dropout=droprnn, bidirectional=bidirectional) for l in range(nlayers)]\n        \n        self.rnns = torch.nn.ModuleList(self.rnns)\n        self.avg_pool = torch.nn.AdaptiveAvgPool1d(1)\n        self.max_pool = torch.nn.AdaptiveMaxPool1d(1)\n        self.decoder = nn.Linear(nhid*self.ndir*2, nout)\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def forward(self, input, lengths=None):\n        emb = self.encoder(input)\n        \n        raw_output = self.drop(emb)\n        \n        if lengths is not None:\n            lengths = lengths.view(-1).tolist()\n            raw_output = nn.utils.rnn.pack_padded_sequence(raw_output, lengths)\n            \n        for rnn in self.rnns:\n            raw_output,_ = rnn(raw_output)\n        \n        if lengths is not None:\n            raw_output, lengths = nn.utils.rnn.pad_packed_sequence(raw_output)\n            \n        bsz = raw_output.size(1)\n        rnn_avg = self.avg_pool(raw_output.permute(1,2,0))\n        rnn_max = self.max_pool(raw_output.permute(1,2,0))\n        rnn_out = torch.cat([rnn_avg.view(bsz,-1),rnn_max.view(bsz,-1)], dim=1)\n            \n        result = self.decoder(rnn_out)\n        return self.decoder(rnn_out)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b1e905f-b0cf-4bab-aab6-f444e0d29067","_uuid":"6f9631169b0d0516eae5e0d4673f6e2a45889200"},"cell_type":"markdown","source":"These are parameters from other example kernels--not necessarily optimized yet."},{"metadata":{"_cell_guid":"30811013-c4e4-481a-a1b1-87764509fb73","_uuid":"67190b16f17c5aa907cd21ba00921f4a51eae051","collapsed":true,"trusted":true},"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nnhidden=100\nemsize=300\nnlayers = 1\ndropemb = 0.2\ndroprnn = 0.0\nmodel = RNNModel('GRU', ntokens, emsize, nhidden, 6, nlayers, dropemb=dropemb, droprnn=droprnn, bidirectional=True)\nmodel.encoder.weight.data.copy_(TEXT.vocab.vectors)\n\nimport torch.optim as optim\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\nif use_cuda:\n    model=model.cuda()\n    criterion=criterion.cuda()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b176a50-148b-4f73-98bf-ec2fda9fb7fb","_uuid":"4a1a16fb89c686f23567d1abada6c1a1c1b4c06e"},"cell_type":"markdown","source":"This is the main pytorch training loop!"},{"metadata":{"_cell_guid":"1ce55df5-a9ee-48db-bee6-6c73794f22d5","_uuid":"6ca3ef7b73e995d12a9036050c18da45d984b14e","collapsed":true,"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook as tqdm\n\nepochs = 2\n\nfor epoch in range(1, epochs + 1):\n    running_loss = 0.0\n    running_count = 0\n    model.train() \n    t = tqdm(train)\n    for batch in t:\n        (x,xl) = get_text(batch)\n        y = get_labels(batch)\n        \n        optimizer.zero_grad()\n\n        preds = model(x, lengths=xl)\n        loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.data[0]*len(x)\n        running_count += len(x)\n        t.set_postfix(loss=(running_loss/running_count))\n\n    epoch_loss = running_loss / running_count\n\n    print('Epoch: {}, Loss: {:.5f}'.format(epoch, epoch_loss))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f00f9d07-01a1-4b27-949d-402a10ae7cf7","_uuid":"0632317bd65372aeebd55a0a0b2389bec94e1a8f"},"cell_type":"markdown","source":"Great! Now we define a quick convenience function to access the ids from the test data"},{"metadata":{"_cell_guid":"75d47686-0aef-485a-be0a-d6e4285b5e3f","_uuid":"6190716bcae4d54f9537fea9b37ddc27f46f53ac","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_ids(batch):\n    return getattr(batch, id_label).data.cpu().numpy().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"181ffb91-91b3-4d32-8992-24130dc4d90f","_uuid":"bae8cd41e98af2f4123f6731c8ad242d2799b43d"},"cell_type":"markdown","source":"And go ahead and store the data in a matrix. Because we get the comments out of order, the ids help us reorder them later"},{"metadata":{"_cell_guid":"0ef466b2-db9e-4459-af71-8f685ed538fa","_uuid":"22569a00e0833bef6aa0c26506a020b24cd9f3a1","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\ntest_preds = np.zeros((len(INDEX.vocab), 6))\nmodel.eval()\nfor batch in test:\n    (x,xl) = get_text(batch)\n    ids = get_ids(batch)\n    preds=model(x,lengths=xl)\n    preds = preds.data.cpu().numpy()\n    preds = 1/(1+np.exp(-np.clip(preds,-10,10)))\n    test_preds[ids]=preds","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f53a163a-32b3-4e06-b269-d1d174faa28e","_uuid":"083cdde8f8ea93ddecb42857e5a4780bbebb3d75"},"cell_type":"markdown","source":"Great, now reread the test file with pandas and write the output!"},{"metadata":{"_cell_guid":"04343d49-714c-47f8-86b5-f39cb8166342","_uuid":"ef7dae1cf2f0ad35e7010c9384b7bee0332ad527","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(f'{path}{test_file}')\nfor i, col in enumerate(label_cols):   \n    df[col] = test_preds[1:, i]\ndf.drop(text_label,axis=1).to_csv(\"submission.csv\",index=False)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2df59c28-4b92-4873-96f2-8d19a845c165","_uuid":"5460f1a058b284e40e1b60a724f277f3858322f2","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}