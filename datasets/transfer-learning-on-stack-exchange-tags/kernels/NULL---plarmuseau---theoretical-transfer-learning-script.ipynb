{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28351307-298d-7875-7e8c-08bd5d8232ae",
        "_active": false
      },
      "outputs": [],
      "source": "import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nimport re\nimport string",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f47f516-4fca-f152-6ef9-da53a2d1d90a",
        "_active": false
      },
      "source": "Datasets loading\n---------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d1903d81-0470-d285-b670-839ed023d457",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "RES_DIR = \"../input/\"\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title','content' ,'tags'])\n        data['category'] = cat\n        train_data.append(data)\n    return pd.concat(train_data)\ntrain_data = load_train_data()\n#import the test data\n\nprint(train_data.head())\n\n#import test data in same format\ntest = pd.read_csv(\"../input/test.csv\")\ntest['tags'] = ''\ntest['category'] = 'physics'\nprint(test.head())\n\n#looks fine for me we assume that the corpus is a physics corpus.\n",
      "execution_state": "idle"
    },
    {
      "metadata": {
        "_cell_guid": "7c535a4b-9650-ab3a-c1fd-f04d5c3ae571",
        "_active": false,
        "collapsed": false
      },
      "source": "**How to find the Physic stopwords** \n---------\n\n\nmake use of the scifit functions: vectorizing > normalizing > multinomila selection",
      "execution_count": null,
      "cell_type": "markdown",
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "563b18b4-2a04-339e-db7c-b567eb85bb63",
        "_active": false
      },
      "outputs": [],
      "source": "#define scikit pipeline\n# vectorizing document > tfidf normalizing > multinomial selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf', MultinomialNB()),\n ])",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2e100b33-bc15-1f4f-bc3f-d7685e15622e",
        "_active": true,
        "collapsed": false
      },
      "outputs": [],
      "source": "#learn words from training set\ntext_clf = text_clf.fit(train_data['title'],train_data['category'])\n#use trained words as stop words\nstop_words = stopwords.words('english')\n#print(stop_words)\npredicted=text_clf.predict(test['title'])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(train_data['title'])\nX_train_counts.shape\n\n# sparse matrix = X_train_counts\n#word index...\nprint(count_vect.vocabulary_)\n      \n#Find list of physics words...and using the trainingkit as stopword vocabulary\nvectorizerPHYSICS = TfidfVectorizer(sublinear_tf=True,max_df=0.5,stop_words='englisch')\nPHYS=vectorizerPHYSICS.fit_transform(test['title'])\nprint(vectorizerPHYSICS.vocabulary_)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6345c551-c161-822b-4189-300ae8f3b4bf",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf', MultinomialNB()),\n ])\n\n#learn words\ntext_clf = text_clf.fit(train_data['title'],train_data['category'])\n#use trained words as stop words\npredicted_tit=text_clf.predict(test['title'])\npredicted_con=text_clf.predict(test['title']+test['content'])",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "543b5bd8-6f39-ae59-1b85-faecee01f242",
        "_active": false
      },
      "outputs": [],
      "source": "#create export file for tags\nprint(predicted_tit)\nprint(predicted_con)\nright=pd.DataFrame(predicted_con)\nright.columns = ['tags']\nprint(zoek.head())\nleft=pd.DataFrame(test['id'])\n\nresult = pd.concat([left,right], axis=1)\n# little stat\ngroep=result.groupby(by='tags',axis=0).count()\nprint(groep)",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "04073ecb-8a3f-84d8-46e4-c959a7cdc2d8",
        "_active": false
      },
      "source": "Removing html tags and uris from contents\n-----------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be09b92d-23f0-4047-0c18-17c0449da393",
        "_active": false
      },
      "outputs": [],
      "source": "uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\"",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3e14aedd-2356-7f75-9b5f-05fc79d2865b",
        "_active": false
      },
      "outputs": [],
      "source": "# This could take a while\nprint(train_data)\n\nfor df in train_data.values():\n    df[\"content\"] = df[\"content\"].map(stripTagsAndUris)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9be37a9d-d579-3cc9-c766-ecc5fcfb3d7f",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\ncategories = ['alt.atheism','talk.religion.misc','comp.graphics','sci.space',]\nremove = ('headers', 'footers', 'quotes')\ndata_train = fetch_20newsgroups(subset='train', categories=categories,\n                                shuffle=True, random_state=42,\n                                remove=remove)",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "67ec7c55-e370-f827-a93a-5bdc43da74bb",
        "_active": false
      },
      "source": "Removing punctuation from titles and contents\n-----------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad63580e-4c46-46a0-ebf5-129e5f413707",
        "_active": false
      },
      "outputs": [],
      "source": "def removePunctuation(x):\n    # Lowercasing all words\n    x = x.lower()\n    # Removing non ASCII chars\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    # Removing (replacing with empty spaces actually) all the punctuations\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b036a06-1080-5794-d2ce-b792cc3b9cb2",
        "_active": false
      },
      "outputs": [],
      "source": "for df in dataframes.values():\n    df[\"title\"] = df[\"title\"].map(removePunctuation)\n    df[\"content\"] = df[\"content\"].map(removePunctuation)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "760d828f-1d11-7f61-d50b-6ee99f8e6690",
        "_active": false
      },
      "outputs": [],
      "source": "print(dataframes[\"robotics\"].iloc[1])",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ac476e3e-5fcf-2609-6443-43b2396a345b",
        "_active": false
      },
      "source": "Removing stopwords from titles and contents\n-----------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2b940f9-4f2e-b66f-bf5f-4e08b191db64",
        "_active": false
      },
      "outputs": [],
      "source": "stops = set(stopwords.words(\"english\"))\ndef removeStopwords(x):\n    # Removing all the stopwords\n    filtered_words = [word for word in x.split() if word not in stops]\n    return \" \".join(filtered_words)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02ba768c-3023-a252-717a-a601c76fa44f",
        "_active": false
      },
      "outputs": [],
      "source": "for df in dataframes.values():\n    df[\"title\"] = df[\"title\"].map(removeStopwords)\n    df[\"content\"] = df[\"content\"].map(removeStopwords)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "860aeb9a-d724-9b46-b895-fe2dc258cc74",
        "_active": false
      },
      "outputs": [],
      "source": "print(dataframes[\"robotics\"].iloc[1])",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9948787-3750-7d66-5494-8a313ebbf8a9",
        "_active": false
      },
      "source": "Splitting tags string in a list of tags\n-----------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d90c04f-74fb-75bb-ad12-600bbf751f5c",
        "_active": false
      },
      "outputs": [],
      "source": "print(dataframes[\"robotics\"].iloc[1])",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9012c948-4b04-c75e-1172-6e2084539e7a",
        "_active": false
      },
      "source": "Saving preprocessed dataframes to csv\n-----------",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a1643423-4665-0b96-1a36-7b53fe8b060b",
        "_active": false
      },
      "outputs": [],
      "source": "categories = ['robotics', 'comp.graphics',\n              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n              'comp.windows.x', 'misc.forsale', 'rec.autos',\n              'rec.motorcycles', 'rec.sport.baseball',\n              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n              'sci.med', 'sci.space', 'soc.religion.christian',\n              'talk.politics.guns', 'talk.politics.mideast',\n              'talk.politics.misc', 'talk.religion.misc']",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38e0fec7-141f-17e1-17eb-b6e129179650",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom operator import itemgetter\nfrom heapq import nlargest\n# each phrase here could be document in your list \n# of documents\nmy_phrases = df['title']\n#print(my_phrases)\n#  and you want to find the most similar document\n#  to this document             \nphrase = [\"What spin relates subatomic particles  often hear about subatomic particles having a property called spin  actually relate spinning axis likewould think. Which particles spin spin mean actual spinning motion?\"]\n\n# You could do it like this:\nvectorizer = TfidfVectorizer(min_df=1.0, stop_words='english')\nall_phrases = phrase+my_phrases\nmy_features = vectorizer.fit_transform(all_phrases)\nscores = (my_features[0, :] * my_features[1:, :].T).A[0]\nbest_score = np.argmax(scores)\nanswer = my_phrases[best_score]\nscoress=scores.sort()\nprint(vectorizer.stop_words_)\nprint(scoress[-5])\n\nprint(my_features[0, :])",
      "execution_state": "idle"
    }
  ]
}