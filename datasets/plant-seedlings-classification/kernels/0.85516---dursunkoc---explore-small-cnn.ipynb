{"cells":[{"metadata":{"_cell_guid":"d47abb13-d284-40ec-af43-2c70610a0747","_uuid":"0d2acfafac41e41c969ad1aeafaa4e1e9a25c1d9"},"cell_type":"markdown","source":"# Artificial Intelligence Nanodegree"},{"metadata":{"_cell_guid":"51b898b1-fad6-445b-9191-63ce732c15a9","_uuid":"b65070ef8914a8be0a43b7f2cf31cb6278180e9c"},"cell_type":"markdown","source":"## Project: Seedling Identification App"},{"metadata":{"_cell_guid":"408cd4b8-1466-46bf-9f4c-7712380f5f94","_uuid":"d0cd9fb1505e723399d47dfde9b5daae4ea2a0ed"},"cell_type":"markdown","source":"### The Road Ahead"},{"metadata":{"_cell_guid":"b64aa8aa-2ecf-40ce-bc41-9b6b30dd24ac","_uuid":"274fd2fe1e1338b57d5ad23516a147ff19ddfef2"},"cell_type":"markdown","source":"0. Library Imports.\n1. Explore Data.\n2. Establish a baseline with the most probable guess.\n3. Preprocess data by removing sharpening and removing the background\n4. Visualize the resulting preprocessed image.\n5. Split data with 0.2 test ratio.\n6. Establish another baseline with the help of a small cnn model.\n7. Preprocess data for transfer learning, and etablish the final model."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nfrom keras.utils import to_categorical\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dense, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import GlobalAveragePooling2D, Input, Conv2D, multiply, LocallyConnected2D\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"f47b866b-0b98-4e27-a6c2-ddbc673992c6","_uuid":"3f9d6963087cd615b662a115aef1f3fd8e3451e2","collapsed":true,"trusted":true},"cell_type":"code","source":"classes = os.listdir(\"../input/train\")\nall_images_class = [os.listdir(\"../input/train/\"+c) for c in classes]\nint_to_classes = {i:classes[i] for i in range(len(classes))}\nclasses_to_int = {classes[i]:i for i in range(len(classes))}","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"9ba1e92c-5b08-428b-b93c-c76c1a1f6949","_uuid":"714d29296cdeda146995fec0ac01b7613f54f3c1","collapsed":true,"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({\"n_images\": [len(x) for x in all_images_class]}, index=classes)\ndf.index.name = \"Specie\"","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"0db59974-8b9e-4cb5-8fe2-67eef5e40568","_uuid":"773ea25fd3628e630496a9a52f891fb948efef42","scrolled":false,"trusted":true},"cell_type":"code","source":"df.plot(kind=\"bar\", figsize=(15,10))\ndf.sort_values(\"n_images\", ascending=False)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"92ce9d80-9d3c-42b2-9779-9a93b51608a2","_uuid":"9d2a94c5670872d9bd0844818a1b28551966a75c"},"cell_type":"markdown","source":"## Preprocess Data"},{"metadata":{"_cell_guid":"9e56fac3-4462-42a3-b3d9-b179ec750c51","_uuid":"dc5bf3fe77d766f5d1fe2fb8d907bdeafb2cae4a","collapsed":true,"trusted":true},"cell_type":"code","source":"def create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n    \n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"15956206-0916-480d-9366-86c0c1e8b578","_uuid":"fb6b36073d584c0e8bf189a75c556d2b3d48a4c3","collapsed":true,"trusted":true},"cell_type":"code","source":"input_shape = (200,200,3)\nnp.random.seed(42)\n\ndef preprocessImagesToArray(img_name, specie):\n    img = cv2.imread(\"../input/train/\"+specie+\"/\"+img_name, cv2.IMREAD_COLOR)\n    img = segment_plant(img)\n    return classes_to_int[specie],cv2.resize(img_to_array(img), input_shape[:-1])\n\nall_data = [preprocessImagesToArray(all_images_class[i][j], classes[i]) for i in range(len(classes)) for j in range(len(all_images_class[i]))]\nlabels = to_categorical(np.array(list(map(lambda tup: tup[0], all_data))),num_classes=len(classes))\nfeatures = np.array(list(map(lambda tup: tup[1], all_data)), dtype=\"float\") / 255.0","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"630f3753-c614-4ee2-b87f-0309bc2ca509","_uuid":"28fb9b3817a4f8c7dfbf230739804a0c5bb70d7c","scrolled":false,"trusted":true},"cell_type":"code","source":"columns = 4\nrows = 5\n\nfig, axs = plt.subplots(rows,columns, figsize=(20,20))\n\nfor i in range(rows*columns):\n    axs[int(i/columns), i%columns].imshow(features[i])","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"31f2ed4a-5c3b-4f99-9bcb-7e52b1ad2a8d","_uuid":"ef8fdfdbd8c0d6b1454bcb9794cffe90d2274f85"},"cell_type":"markdown","source":"### Test and Train Split with shuffle"},{"metadata":{"_cell_guid":"2dbe91a9-2032-4d12-a91b-274e62109144","_uuid":"560b8bddc4d4149603667e1c9752748cc03517f2","collapsed":true,"trusted":true},"cell_type":"code","source":"(train_features, test_features, train_labels, test_labels) = train_test_split(features,labels,test_size=0.20, random_state=42, shuffle=True)","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"3f018c21-fef5-41b8-aefd-7280326e48b2","_uuid":"2a4bcaea1544610e43dae848f09ce01cb5525d15"},"cell_type":"markdown","source":"### Establish a baseline with a random choice of most probable specie guess\nBefore going further I will generate a baseline with a random choice guess with the most probable specie. The Most probable specie is the `Loose Silky-bent`, and I will get its value as `to_categorical(classes_to_int['Loose Silky-bent'])`"},{"metadata":{"_cell_guid":"01e77d8f-6490-4ac7-8f35-825710852b63","_uuid":"0702cf67fd1d06afa5c7a0f4eb13ec29d6fd93cc","collapsed":true,"trusted":true},"cell_type":"code","source":"the_most_probable_guess = np.matlib.repmat(to_categorical(classes_to_int['Loose Silky-bent']), len(test_labels), 1)\nbaseline_pred = np.argmax(the_most_probable_guess, axis=1)\nbaseline_true = np.argmax(test_labels, axis=1)\n\nf1_score(baseline_true, baseline_pred, average=\"micro\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6985f12-f18f-46a6-bf4a-cd27e5eed61d","_uuid":"3b2e4141c5f3b0adf64f971d08e8b95310395e8a"},"cell_type":"markdown","source":"### so, a naive predictor's F1 score is 0.1473"},{"metadata":{"_cell_guid":"52da9164-f6ea-4b46-a8b1-31ee3653cdea","_uuid":"72048336f3a7a1e42a33bc654bb9061379cffb82"},"cell_type":"markdown","source":"### Establish a baseline with a small CNN"},{"metadata":{"_cell_guid":"099bcfd1-db74-4420-8723-0ea424ef7197","_uuid":"638cb9d214ef37e11655480d1f34caaa0d891ac5","collapsed":true,"trusted":true},"cell_type":"code","source":"def f1_score(y_true, y_pred):\n\n    # Count positive samples.\n    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n\n    # If there are no true samples, fix the F1 score at 0.\n    if c3 == 0:\n        return 0\n\n    # How many selected items are relevant?\n    precision = c1 / c2\n\n    # How many relevant items are selected?\n    recall = c1 / c3\n\n    # Calculate f1_score\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return f1_score","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"7e71893e-843a-4ca3-87cf-96aba9a8304d","_uuid":"f71e091d1288ef1a2bf09b93512eb579db896963","trusted":true},"cell_type":"code","source":"EPOCHS = 20\nINIT_LR = 1e-3\nBS = 64\n\nsmall_conv_nn = Sequential()\n\nsmall_conv_nn.add(Conv2D(10, (5, 5), padding=\"same\", input_shape=input_shape))\nsmall_conv_nn.add(Activation(\"relu\"))\nsmall_conv_nn.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4)))\nsmall_conv_nn.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=input_shape))\nsmall_conv_nn.add(Activation(\"relu\"))\nsmall_conv_nn.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4)))\nsmall_conv_nn.add(Conv2D(30, (5, 5), padding=\"same\", input_shape=input_shape))\nsmall_conv_nn.add(Activation(\"relu\"))\nsmall_conv_nn.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4)))\nsmall_conv_nn.add(Flatten())\n\nsmall_conv_nn.add(Dense(2048))\nsmall_conv_nn.add(Activation(\"relu\"))\nsmall_conv_nn.add(Dense(12))\nsmall_conv_nn.add(Activation(\"softmax\"))\n\nsmall_conv_nn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS), metrics=[f1_score])\nsmall_conv_nn.summary()","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"4cd1355b-ccdb-4dd3-b532-6899ac67862b","_uuid":"3a46b1d9c6dc5f4df533f4b9c03046a542eeabac","scrolled":true,"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, vertical_flip=True, fill_mode=\"nearest\")\ncheck_pt = ModelCheckpoint(\n    'small_conv_nn_{epoch:02d}_{val_f1_score:.4f}.hdf5', \n    monitor='val_f1_score', \n    verbose=1, \n    save_best_only=False, \n    save_weights_only=False, \n    period=1\n)\n\n\nh_small_conv_nn = small_conv_nn.fit_generator(generator = aug.flow(train_features, train_labels, batch_size=BS), \n                            validation_data = (test_features, test_labels), \n                            steps_per_epoch = len(train_features) // BS, \n                            epochs = EPOCHS,\n                            callbacks=[check_pt],\n                            verbose = 1)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"7ca98725-f6d0-488e-bf69-61c9557b87f0","_uuid":"95a8708a005fa44813b67540f4b7daefc34c7fa7","trusted":true},"cell_type":"code","source":"plt.plot(np.arange(0, EPOCHS), h_small_conv_nn.history[\"f1_score\"], label=\"train_f1_score\")\nplt.plot(np.arange(0, EPOCHS), h_small_conv_nn.history[\"val_f1_score\"], label=\"val_f1_score\")\nplt.title(\"Training Loss and Accuracy on  crop classification\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/F1-Score\")\nplt.legend(loc=\"lower right\")","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"0c5d423b-1801-4a0e-9914-f23cc6592129","_uuid":"58cede6af9f74de13f081da1d8f33e85ce024006"},"cell_type":"markdown","source":"### so, a small CNN's F1 score is 0.7084"},{"metadata":{"_cell_guid":"b5af4bd8-1039-4290-8820-c608d2bc3ee6","_uuid":"a572bc3178a19cdfe468a0d4873ab536816d42ae"},"cell_type":"markdown","source":"### Preprocessing for Prediction"},{"metadata":{"_cell_guid":"08a4388f-c299-4547-b846-1f4b42995a31","_uuid":"6771539a28cfa3ecdcdfaf7133d64f04f54f74b8","collapsed":true,"trusted":true},"cell_type":"code","source":"input_shape = (200,200,3)\nnp.random.seed(42)\n\ndef preprocessImagesToArray_pred(img_name):\n    img = cv2.imread(\"../input/test/\"+img_name, cv2.IMREAD_COLOR)\n    img = segment_plant(img)\n    return cv2.resize(img_to_array(img), input_shape[:-1])\n\nall_test_images = os.listdir(\"../input/test/\")\nall_test_data = [preprocessImagesToArray_pred(img_name) for img_name in all_test_images]\nfeatures_test = np.array(all_test_data, dtype=\"float\") / 255.0","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"fc329ca5-2882-4a7e-b010-076a7aa1390f","_uuid":"27fd2d56fb9d9d1525765bb6ed921c30c8cb8a15","collapsed":true,"trusted":true},"cell_type":"code","source":"predictions_test = small_conv_nn.predict_classes(features_test)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"59cdc29f-ead8-404d-9dab-caf68e949504","_uuid":"cecba9d7b3502e0596eac6331416d90bd52238a8","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\npreds = np.column_stack((all_test_images,np.array(list(map(lambda i: int_to_classes[i], predictions_test)))))\nsubmission = pd.DataFrame(data=preds, columns=[\"file\",\"species\"])","execution_count":50,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e2fd31b1d500b8f206b082e92103f7280129b7e9"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":52,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}