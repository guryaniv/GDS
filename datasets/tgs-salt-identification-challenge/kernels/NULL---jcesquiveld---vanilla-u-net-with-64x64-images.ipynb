{"cells":[{"metadata":{"_uuid":"2ce78599f3e78b5a40c4d2e44ad29fa29c4d978b"},"cell_type":"markdown","source":"<h1>U-Net with (64,64,1) images</h1>\n\nIn this  kernel I'll try to assess the effectiveness of switching to a U-Net that has (64,64,1) images instead of (128,128,1). 64 is still a power of two, so a U-Net with feature maps in the encoder part of 64,32,16,8 and 4 can still be designed, but removes the need for resizing the original (101,101,1) images, as four crops of (64,64,1) of the image can be taken, predicted and then composed to make the original image's prediction. In addition, I'll have many more images in the training phase, alleviating the data augmentation problem.\n\n"},{"metadata":{"trusted":true,"_uuid":"89ed0ca48b40aa1af74f38625236c00b56ef9a76","collapsed":true},"cell_type":"code","source":"# Version of the notebook (used for output file names)\nversion = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c3e3f268d54f9ce376889a47aa6a0de564283d"},"cell_type":"markdown","source":"<h3>Needed imports</h3>"},{"metadata":{"_uuid":"c902dc643c15e1dba0b3245aa2fd19ffd5a0b0ce","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Lambda, Conv2D, SpatialDropout2D, BatchNormalization,Activation\nfrom keras.layers import MaxPooling2D, Conv2DTranspose, concatenate\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.models import Model, load_model, model_from_json\nfrom keras.optimizers import Adam, SGD\nimport keras.backend as K\nfrom keras import losses\nimport tensorflow as tf\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b022bfb3b9aada9f16da2f0b2db7b9cc823a488","trusted":true,"collapsed":true},"cell_type":"markdown","source":"<h2>Helper functions<h2>"},{"metadata":{"_uuid":"d86ea6215926571e5f763f352f6d8b898d02ba76"},"cell_type":"markdown","source":"<h3>Metrics used<h3>\n\n<code>competition_metric</code> is based on kernel <a href=\"https://www.kaggle.com/pestipeti/explanation-of-scoring-metric\">https://www.kaggle.com/pestipeti/explanation-of-scoring-metric</a>"},{"metadata":{"trusted":true,"_uuid":"dda2ff0a0cd55b0ef22b51708c8bd2115ff59faf","collapsed":true},"cell_type":"code","source":"def castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n    iou = (intersection + smooth) / ( union + smooth)\n    return iou\n\n\ndef iou_loss(y_true, y_pred):\n    return 1 - iou_loss_core(y_true, y_pred)\n\ndef iou_bce_loss(y_true, y_pred):\n    return losses.binary_crossentropy(y_true, y_pred) + 5 * iou_loss(y_true, y_pred)\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef focal_loss(y_true, y_pred):\n    gamma=0.5\n    alpha=0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n\ndef competition_metric(true, pred): #any shape can go\n\n    tresholds = [0.5 + (i*.05)  for i in range(10)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n\n# For threshold determination\ndef faster_iou_metric_batch(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n            metric.append(0)\n            continue\n        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n            metric.append(1)\n            continue\n\n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = np.sum(intersection > 0) / np.sum(union > 0)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d0d9ef8b6f82b3e5667bf4f0fbadad2ac46f55d"},"cell_type":"markdown","source":"<h3>Network architecture</h3>"},{"metadata":{"trusted":true,"_uuid":"c3c1c11b7b27ae872ea9e0d29fe4ded27498d37d","collapsed":true},"cell_type":"code","source":"def conv_block(neurons, block_input, bn=False, dropout=None):\n    conv1 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='he_normal')(block_input)\n    if bn:\n        conv1 = BatchNormalization()(conv1)\n    conv1 = Activation('relu')(conv1)\n    if dropout is not None:\n        conv1 = SpatialDropout2D(dropout)(conv1)\n    conv2 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='he_normal')(conv1)\n    if bn:\n        conv2 = BatchNormalization()(conv2)\n    conv2 = Activation('relu')(conv2)\n    if dropout is not None:\n        conv2 = SpatialDropout2D(dropout)(conv2)\n    pool = MaxPooling2D((2,2))(conv2)\n    return pool, conv2  # returns the block output and the shortcut to use in the uppooling blocks\n\ndef middle_block(neurons, block_input, bn=False, dropout=None):\n    conv1 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='he_normal')(block_input)\n    if bn:\n        conv1 = BatchNormalization()(conv1)\n    conv1 = Activation('relu')(conv1)\n    if dropout is not None:\n        conv1 = SpatialDropout2D(dropout)(conv1)\n    conv2 = Conv2D(neurons, (3,3), padding='same', kernel_initializer='he_normal')(conv1)\n    if bn:\n        conv2 = BatchNormalization()(conv2)\n    conv2 = Activation('relu')(conv2)\n    if dropout is not None:\n        conv2 = SpatialDropout2D(dropout)(conv2)\n    \n    return conv2\n\ndef deconv_block(neurons, block_input, shortcut, bn=False, dropout=None):\n    deconv = Conv2DTranspose(neurons, (3, 3), strides=(2, 2), padding=\"same\")(block_input)\n    uconv = concatenate([deconv, shortcut])\n    uconv = Conv2D(neurons, (3, 3), padding=\"same\", kernel_initializer='he_normal')(uconv)\n    if bn:\n        uconv = BatchNormalization()(uconv)\n    uconv = Activation('relu')(uconv)\n    if dropout is not None:\n        uconv = SpatialDropout2D(dropout)(uconv)\n    uconv = Conv2D(neurons, (3, 3), padding=\"same\", kernel_initializer='he_normal')(uconv)\n    if bn:\n        uconv = BatchNormalization()(uconv)\n    uconv = Activation('relu')(uconv)\n    if dropout is not None:\n        uconv = SpatialDropout2D(dropout)(uconv)\n        \n    return uconv\n    \ndef build_model(start_neurons, bn=False, dropout=None):\n    \n    input_layer = Input((64, 64, 1))\n    \n    # 64 -> 32\n    conv1, shortcut1 = conv_block(start_neurons, input_layer, bn, dropout)\n\n    # 32 -> 16\n    conv2, shortcut2 = conv_block(start_neurons * 2, conv1, bn, dropout)\n    \n    # 16 -> 8\n    conv3, shortcut3 = conv_block(start_neurons * 4, conv2, bn, dropout)\n    \n    # 8 -> 4\n    conv4, shortcut4 = conv_block(start_neurons * 8, conv3, bn, dropout)\n    \n    # Middle\n    convm = middle_block(start_neurons * 16, conv4, bn, dropout)\n    \n    # 4 -> 8\n    deconv4 = deconv_block(start_neurons * 8, convm, shortcut4, bn, dropout)\n    \n    # 8 -> 16\n    deconv3 = deconv_block(start_neurons * 4, deconv4, shortcut3, bn, dropout)\n    \n    # 16 -> 32\n    deconv2 = deconv_block(start_neurons * 2, deconv3, shortcut2, bn, dropout)\n    \n    # 32 -> 64\n    deconv1 = deconv_block(start_neurons, deconv2, shortcut1, bn, dropout=False)\n    \n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(deconv1)\n    \n    model = Model(input_layer, output_layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bdfe0f4aa08acb4632954915876e4fbcfe7c895"},"cell_type":"markdown","source":"<h2>Data loading and preprocessing<h2>\n\nI set as data directory the output directory of another kernel of mine in which I store for convenience all data including images as HF5 files. Loading this files just takes a few seconds (5 s on average), instead of several minutes to load the CSV and images every time. <a href=\"https://www.kaggle.com/jcesquiveld/tgs-reading-data-and-storing-in-hf5\">https://www.kaggle.com/jcesquiveld/tgs-reading-data-and-storing-in-hf5</a>.\n\nTo see how to access data generated as output in another kernel see: <a href=\"https://www.kaggle.com/product-feedback/45472\">https://www.kaggle.com/product-feedback/45472</a>."},{"metadata":{"trusted":true,"_uuid":"07c6e3c94c1d1191eab7a895ec9157b8b986ea32"},"cell_type":"code","source":"%%time\nDATA_DIR = '../input/tgs-eda/'\ntrain = pd.read_hdf(DATA_DIR + 'tgs_data.h5', key='filtered_train')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"563890e367bb9bfd7c856dcb97eeac9e299485ef"},"cell_type":"markdown","source":"<h2>Data augmentation</h2>\n\nTaking a random (64,64) crop (also random horizontal flip) from the image."},{"metadata":{"trusted":true,"_uuid":"cfecc7bcf4f8923a4a56ebf302630072fa3d1050"},"cell_type":"code","source":"np.random.seed(42)\n\nNUM_CROPS = 25    # Number of crops for every image\nWIDTH = 64\nHEIGHT = 64\nIMAGE_PIXELS = WIDTH * HEIGHT\nMAX_X = 37\nMAX_Y = 37\n\ndef coverage(pixels):\n    if pixels == 0:\n        return 0\n    else:\n        percentage = pixels / IMAGE_PIXELS\n        return np.ceil(percentage * 10).astype(np.uint)\n\ndef random_crop_params():\n    x = np.random.randint(0, MAX_X)\n    y = np.random.randint(0, MAX_Y)\n    flip = np.random.choice(a=[False, True])\n    intensity = np.random.normal(1,0.2)\n    if intensity == 0:\n        intensity = 0\n    return x, y, flip, intensity\n\ndef crop(img, x, y, flip, intensity=1):\n    random_img = img[x:x+WIDTH,y:y+WIDTH]\n    if flip:\n        random_img = np.fliplr(random_img)\n    random_img = random_img * intensity\n    return random_img.reshape(WIDTH, HEIGHT, 1)\n\nimgs_aug = []\nmasks_aug = []\nfor idx in train.index:\n    img = train.loc[idx]['images']\n    mask = train.loc[idx]['masks']\n    for i in range(NUM_CROPS):\n        r = i // 5\n        c = i % 5\n        x, y, flip, intensity = random_crop_params()\n        random_img = crop(img, x, y, flip, intensity)\n        imgs_aug.append(random_img)\n        random_mask = crop(mask, x, y, flip)\n        masks_aug.append(random_mask)\n\ndata = pd.DataFrame({'images':imgs_aug, 'masks':masks_aug})\nprint(data.shape)\n\n# Calculate coverage\ndata['pixels'] = data['masks'].map(lambda x: np.sum(x/255)).astype(np.int16)\ndata['coverage'] = data['pixels'].map(coverage).astype(np.float16)\n\ndata.describe()\n\ndel imgs_aug\ndel masks_aug\ndel train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbad23bc93873e32406f94da4fdd7faa7a1654c2"},"cell_type":"code","source":"# Plot coverage distribution\nlabels, counts = np.unique(data['coverage'], return_counts=True)\nplt.bar(labels, counts, align='center')\nplt.gca().set_xticks(labels)\nplt.grid(axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c39a9120719c5dca6036ce2a033985b6230035ab"},"cell_type":"markdown","source":"<h3>Train/val splitting</h3>\n\nSimple holdout validation set. Stratification on coverage class."},{"metadata":{"trusted":true,"_uuid":"418ebc55b0f2b651f24b6b7833c679fb9c86cb0d"},"cell_type":"code","source":"# Split the train data into actual train data and validation data\n# train_test_split already shuffles data by default, so no need to do it\n\nX = np.stack(data['images']) / 255\ny = np.stack(data['masks']) / 255\n\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=data.coverage, random_state=42)\n\ndel data\ndel X\ndel y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aee18221e83eef20a4d1f78f6da514ad7c38c87e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89435685f6d52abd0c1426965053c04a9309c10a"},"cell_type":"markdown","source":"<h2>Build and save the model</h2>"},{"metadata":{"trusted":true,"_uuid":"8f5236510f4333a334c9afc85db594e3d20ded84"},"cell_type":"code","source":"# Build and save model in JSON format\n\njson_filename = 'unet_salt_{}.json'.format(version)\n\nmodel = build_model(start_neurons=8, bn=True, dropout=False)\nmodel_json = model.to_json()\nwith open(json_filename, 'w') as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91b774b12d5d7460e30b2b56a9412997561db8b9"},"cell_type":"markdown","source":"<h2>Training</h2>\n\nDuring training, the best model will be saved with the callback <code>ModelCheckpoint</code>.  To pass what metric to the checker, pass its name to the <code>monitor</code> attribute (you can see the metrics used by the model with <code>model.metrics__names</code>. If it is from the validation set, prefix the name with <code>val_</code>. If you want to maximize or minimize this metric, use <code>mode='min'</code> or <code>mode='max'</code>."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"291e7e8c99f2404c807b63fc26dec159e5297b64","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', verbose=1)\nweights_filename = 'unet_salt_weights_{}.h5'.format(version)\ncheckpoint = ModelCheckpoint(weights_filename, monitor='val_competition_metric', verbose=0, save_best_only=True, save_weights_only=True, mode='max')\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=10, verbose=1)\noptimizer = SGD(lr=0.1, momentum=0.8, nesterov=False)\n#optimizer = Adam(lr=0.1)\n\nmodel.compile(optimizer=optimizer, loss=iou_bce_loss, metrics=['accuracy', competition_metric])\nhistory = model.fit(X_train, y_train, batch_size=64, validation_data = [X_val, y_val], \n                    epochs=50, callbacks=[checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d4828ca2634418aea09cd948220ab88a06f4854"},"cell_type":"code","source":"# Restore the best model's weight\nweights_filename = 'unet_salt_weights_{}.h5'.format(version)\nmodel.load_weights(weights_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d01afe96ab7609b584d59e0612d143c68f95080","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Let's see how the model performs (last model after training, not the saved best one)\n\n# On the train set\nprint('*** Best model on train set ***')\nmodel.evaluate(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c64c6b57ec55f87f0333e2a4ec9d04f5c0779994","collapsed":true},"cell_type":"code","source":"# On the validation set\nprint('*** Best model on val set ***')  \nmodel.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc9fd2a522c538dbe2a4bfebc4719c446710be9d","trusted":true,"collapsed":true},"cell_type":"code","source":"print(model.metrics_names)\n\nhist = history.history\n\nfigure, ax = plt.subplots(1,3, figsize=(18,6))\nprint(ax.shape)\n\ndef plot_history(history, metric, title, ax):\n    ax.plot(history[metric])\n    ax.plot(history['val_' + metric])\n    ax.grid(True)\n    ax.set_title(title)\n    ax.set_ylabel(metric)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'validation'], loc='upper left')                \n    \nplot_history(hist, 'loss', 'LOSS', ax[0])\nplot_history(hist, 'acc', 'ACCURACY', ax[1])\nplot_history(hist, 'competition_metric', 'COMPETITION METRIC', ax[2])\n\n\n# Save image for reports\nhistory_df = pd.DataFrame(hist)\nplt.savefig('history_unet_salt_{}.png'.format(version))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad5acd95ce0674165f71e128863cf2151a8a07b7"},"cell_type":"markdown","source":"<h3>Finding the optimal threshold for predictions</h3>\n\nThis is based on this kernel by Ding Hang: <a href=\"https://www.kaggle.com/dingdiego/u-net-batchnorm-augmentation-stratification\">https://www.kaggle.com/dingdiego/u-net-batchnorm-augmentation-stratification</a>.\n"},{"metadata":{"trusted":true,"_uuid":"db39b6d889a4726452713ccf731d58c62ee17949","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\ny_val_pred = model.predict(X_val)\nious = np.array([faster_iou_metric_batch(y_val, np.uint8(y_val_pred > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3f9c5c15a2930f30afa16adf4d4c80b9cdc1bca","collapsed":true},"cell_type":"code","source":"best_th_index = ious.argmax()\nbest_th = thresholds[best_th_index]\nbest_iou = ious[best_th_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8435ed6c6674f5b978da3fc5a6de7cb656ada4c2","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(best_th, best_iou, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(best_th, best_iou))\nplt.legend()\n# Save image for reports\nplt.savefig('threshold_selection_{}.png'.format(version))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}