{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "ada14f875d2dc510a14e51e7e639a4c7f1cebe62", "_cell_guid": "473a7e6a-ad12-4615-9096-ae9bf90ac2a2"}, "source": ["# Import"]}, {"cell_type": "code", "metadata": {"_uuid": "226a0e08386e9cfcfca3db10ca7d6e78b40c4c43", "_cell_guid": "616375a8-409f-4290-a4e3-2ce75e9778ae", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["import os\n", "import gc\n", "import time\n", "start_time = time.time()\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "import nltk\n", "from nltk.corpus import stopwords\n", "stops = set(stopwords.words('english'))\n", "import string\n", "puns = string.punctuation\n", "\n", "import gensim"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ce75b9aae8dad082d7f1df36782b2fdb74208279", "_cell_guid": "4b3bb412-ce27-472b-b1df-92924d57bfac"}, "source": ["# Loading Data"]}, {"cell_type": "code", "metadata": {"_uuid": "2939e421506f02064089c3f1a799c03c54face34", "_cell_guid": "a99d34bc-ef3b-40c9-8d08-d8fdc694fb86", "scrolled": true, "collapsed": true}, "outputs": [], "execution_count": null, "source": ["df_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv')\n", "df_test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv')\n", "# for testing(concerning time)\n", "# df_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv').loc[:10000]\n", "# df_test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv').loc[:10000]\n", "df_train['target'] = np.log1p(df_train['price']) ## (np.log(array + 1))"]}, {"cell_type": "code", "metadata": {"_uuid": "0a10b062122a85b57728657275aa4885e94afcd0", "_cell_guid": "2e7d1077-5bf1-4537-b5bf-5c6c5321dc7a", "scrolled": true, "collapsed": true}, "outputs": [], "execution_count": null, "source": ["print(df_train.shape)\n", "print('5 folds scaling the test_df')\n", "print(df_test.shape)\n", "test_len = df_test.shape[0]\n", "def simulate_test(df_test):\n", "    if df_test.shape[0] < 800000:\n", "        indices = np.random.choice(df_test.index.values, 2800000)\n", "        df_test_ = pd.concat([df_test, df_test.iloc[indices]], axis=0)\n", "        return df_test_.copy()\n", "    else:\n", "        return df_test\n", "df_test = simulate_test(df_test)\n", "\n", "print('new shape ', df_test.shape)\n", "print('[{}] Finished scaling test set...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "be2bdedbbda7ebca4f93e06c23c9ea95b55137f8", "_cell_guid": "649f34c5-104b-41ed-82ae-a3cf27341e3c"}, "source": ["# Handling missing values"]}, {"cell_type": "code", "metadata": {"_uuid": "35f1f35b5fa5445deaffa6c7abd3d50e4363b136", "_cell_guid": "c273e333-0a13-4fb2-bb5a-198f9351717b", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["print(\"Handling missing values...\")\n", "def handle_missing(dataset):\n", "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n", "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n", "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n", "    return (dataset)\n", "\n", "df_train = handle_missing(df_train)\n", "df_test = handle_missing(df_test)\n", "print(df_train.shape)\n", "print(df_test.shape)\n", "\n", "print('[{}] Finished handling missing data...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "39ef0394554141d2f511cf4965ddc64346a39efa", "_cell_guid": "c5383da1-3fc4-4b8e-af7b-601eba6f9dfb"}, "source": ["# Preprocess - Word2Vec\n", "considering time, this part is abandomed"]}, {"cell_type": "code", "metadata": {"_uuid": "ebf910850fdd4b9b08530187aa5c3f24a41390f5", "_cell_guid": "231c806d-2948-4e34-8108-e82db80ab6a9", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["# def tokenize(Doc):\n", "#     if pd.notnull(Doc):\n", "#         tokens = nltk.wordpunct_tokenize(Doc)\n", "#         text = nltk.Text(tokens)\n", "#         words = [w.lower() for w in text if w not in stops and w not in puns]\n", "#         return words\n", "#     else:\n", "#         return None"]}, {"cell_type": "code", "metadata": {"_uuid": "e04b6c4ccedb0a9f938f9724ef00899e61c14703", "_cell_guid": "9d23cf80-d7c2-4f55-bab0-38fd01d9239f", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["# tokens = []\n", "# for sent in np.hstack([df_train['item_description'], df_test['item_description'], \n", "#            df_train['name'], df_test['name']]):\n", "#     if pd.notnull(sent):\n", "#         tokens.extend(tokenize(sent))\n", "\n", "# tokens = list(set(tokens))"]}, {"cell_type": "code", "metadata": {"_uuid": "26f1e3602fcd79af55cf53547ff486bfd30f52bc", "_cell_guid": "3dbcef36-6e4d-4457-b6b8-c8c7bc37d887", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["# word_vec_mapping = {}\n", "# path = \"../input/wrod2vec-twitter-50d/glove.twitter.27B.50d.txt\"\n", "\n", "# with open(path, 'r', encoding='utf8') as f:\n", "#     for line in f:\n", "#         tokens = line.split()\n", "#         token = tokens[0]\n", "#         vec = tokens[1:]\n", "#         if token in tokens:\n", "#             word_vec_mapping[token] = np.array(vec, dtype=np.float32)\n", "# vec_dimensions = len(word_vec_mapping.get('a'))"]}, {"cell_type": "code", "metadata": {"_uuid": "e5428890b72a9f930176a965759f96783b9456da", "_cell_guid": "7b82624a-a12b-4c2d-9e8a-37fe5bc1f5f0", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["# def doc2vec(doc, word2vec=word_vec_mapping):\n", "#     docvec=np.zeros(vec_dimensions, )\n", "#     vec_count = 1\n", "    \n", "#     if pd.notnull(doc):\n", "#         terms = tokenize(doc)\n", "#         for term in terms:\n", "#             termvec = word_vec_mapping.get(term, None)\n", "#             if termvec is not None:\n", "#                 docvec += np.array(termvec, dtype=np.float32)\n", "#                 vec_count += 1              \n", "#     return (docvec/vec_count)\n", "\n", "# def doc2vec_desc(doc, word2vec=word_vec_mapping):\n", "#     docvec=np.zeros(vec_dimensions, )\n", "#     vec_count = 1\n", "    \n", "#     if pd.notnull(doc):\n", "#         terms = tokenize(doc)\n", "#         for term in terms:\n", "#             termvec = word_vec_mapping.get(term, None)\n", "#             if termvec is not None:\n", "#                 docvec += np.array(termvec, dtype=np.float32)\n", "#                 vec_count += 1              \n", "#     return (docvec/vec_count)\n", "\n", "# def embedding_name_cat_word2vec(df):\n", "#     name_vecs = []\n", "#     desc_vecs = []\n", "#     for idx, row in df.iterrows():\n", "#         name_vecs.append(doc2vec(row['name']))\n", "#         desc_vecs.append(doc2vec(row['item_description']))\n", "#         if idx % 200000 == 0:\n", "#             print(idx)\n", "#     df['name_word2vec'] = name_vecs\n", "#     df['item_description_word2vec'] = desc_vecs\n", "#     return df\n", "\n", "# df_train = embedding_name_cat_word2vec(df_train)\n", "# df_test = embedding_name_cat_word2vec(df_test)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "04f7c69a2f9b1858be33b055b332b85edb9d5799", "_cell_guid": "a64c437b-937f-4fce-8357-50de5a8c5bf3"}, "source": ["# Peprocess - category and brand"]}, {"cell_type": "code", "metadata": {"_uuid": "d5d443c3a4c9b6422056ae3d68b6bc7b79d8a36e", "_cell_guid": "7cbc8d9c-e05c-40b3-bab4-627de7112677", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#PROCESS CATEGORICAL DATA\n", "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, MultiLabelBinarizer\n", "print(\"Handling categorical variables...\")\n", "le_cat = LabelEncoder()\n", "le_cat.fit(np.hstack([df_train.category_name, df_test.category_name]))\n", "df_train['category'] = le_cat.transform(df_train.category_name)\n", "df_test['category'] = le_cat.transform(df_test.category_name)\n", "\n", "le_brand = LabelEncoder()\n", "le_brand.fit(np.hstack([df_train.brand_name, df_test.brand_name]))\n", "df_train['brand'] = le_brand.transform(df_train.brand_name)\n", "df_test['brand'] = le_brand.transform(df_test.brand_name)\n", "# del le, df_train['brand_name'], df_test['brand_name']\n", "\n", "print('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\n", "df_train.head(3)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "547505344f191f3c842c138a639e421086a4681b", "_cell_guid": "dfdfb00f-be7e-48b7-8350-894dd1a5d37b"}, "source": ["# Peprocess - category_split"]}, {"cell_type": "code", "metadata": {"_uuid": "ef4d81c4ad955106a20a734eda8f95fabd4077b1", "_cell_guid": "c3f1cc61-df31-4489-8c96-25d217472da8", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#Build Category LabelEncoder\n", "def serialize_category(cats):\n", "    return cats.split('/')\n", "df_train['category_split'] = df_train['category_name'].apply(serialize_category)\n", "df_test['category_split'] = df_test['category_name'].apply(serialize_category)\n", "mlb = MultiLabelBinarizer()    \n", "mlb.fit(np.hstack([df_train['category_split'], df_test['category_split']]))\n", "\n", "idx_cat_mapping = {}\n", "for idx, cat_name in enumerate(mlb.classes_):\n", "    idx_cat_mapping[cat_name] = idx\n", "\n", "def getidxs(cats):\n", "    idxs = []\n", "    for cat in cats:\n", "        idx = idx_cat_mapping.get(cat)\n", "        idxs.append(idx)\n", "    return idxs\n", "\n", "df_train['category_split'] = df_train['category_split'].apply(getidxs)\n", "df_test['category_split'] = df_test['category_split'].apply(getidxs)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "547d8bed516bb8e285d35f6e87596ef4c38a4fb6", "_cell_guid": "d8b24477-a82a-4de1-b9b6-fb7bb6856cc0"}, "source": ["# Prprocess - category_name_onehot, item_description_onehot, name_onehot"]}, {"cell_type": "code", "metadata": {"_uuid": "37103e42b613d13342463701a0a67acbffd8640d", "_cell_guid": "a3ef4034-9af8-49f3-85e8-3f65dd087ada", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#PROCESS TEXT: RAW\n", "print(\"Text to seq process...\")\n", "print(\"   Fitting tokenizer...\")\n", "from keras.preprocessing.text import Tokenizer\n", "raw_text = np.hstack([df_train['category_name'].str.lower(), \n", "                      df_train['item_description'].str.lower(), \n", "                      df_train['name'].str.lower(),\n", "                      df_test['category_name'].str.lower(), \n", "                      df_test['item_description'].str.lower(), \n", "                      df_test['name'].str.lower()])\n", "\n", "tok_raw = Tokenizer()\n", "tok_raw.fit_on_texts(raw_text)\n", "print(\"   Transforming text to seq...\")\n", "df_train[\"category_name_onehot\"] = tok_raw.texts_to_sequences(df_train['category_name'].str.lower())\n", "df_test[\"category_name_onehot\"] = tok_raw.texts_to_sequences(df_test['category_name'].str.lower())\n", "df_train[\"item_description_onehot\"] = tok_raw.texts_to_sequences(df_train['item_description'].str.lower())\n", "df_test[\"item_description_onehot\"] = tok_raw.texts_to_sequences(df_test['item_description'].str.lower())\n", "df_train[\"name_onehot\"] = tok_raw.texts_to_sequences(df_train['name'].str.lower())\n", "df_test[\"name_onehot\"] = tok_raw.texts_to_sequences(df_test['name'].str.lower())\n", "df_train.head(3)\n", "\n", "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "67973afcb65ad42cc742013e0b828267f0680cb1", "_cell_guid": "2a794e74-6335-4d06-8f94-1428bd95850e"}, "source": ["# train_test_split"]}, {"cell_type": "code", "metadata": {"_uuid": "65e6cb624dba6243706aa52f44fd4ace68df1558", "_cell_guid": "3ac741fc-3611-421a-a10c-9b1eecc9e2f3", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#EXTRACT DEVELOPTMENT TEST\n", "from sklearn.model_selection import train_test_split\n", "dtrain, dvalid = train_test_split(df_train, random_state=233, train_size=0.99)\n", "print(dtrain.shape)\n", "print(dvalid.shape)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "fef3dc6d9fb192b017979cfee20b664791bca325", "_cell_guid": "cbd6d167-8022-4fe0-b49c-77a039a08efc"}, "source": ["# Preparing Keras Input"]}, {"cell_type": "code", "metadata": {"_uuid": "b3e1d7c2c20beda598c8c71f5d7616cd63658405", "_cell_guid": "375b4caa-ba0e-4563-83c4-2a40c116b52a", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#EMBEDDINGS MAX VALUE\n", "#Base on the histograms, we select the next lengths\n", "MAX_NAME_SEQ = 20 #17\n", "MAX_ITEM_DESC_SEQ = 60 #269\n", "MAX_CATEGORY_NAME_SEQ = 20 #8\n", "MAX_CATEGORY_SPLIT_SEQ = np.max([df_train['category_split'].apply(len).max(), df_test['category_split'].apply(len).max()])\n", "\n", "MAX_TEXT = len(tok_raw.word_index) +1\n", "MAX_CATEGORY = len(le_cat.classes_)\n", "MAX_CATEGORY_SPLIT = len(mlb.classes_)\n", "MAX_BRAND = len(le_brand.classes_)\n", "MAX_CONDITION = np.max([df_train.item_condition_id.max(), \n", "                        df_test.item_condition_id.max()])+1\n", "\n", "print('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))"]}, {"cell_type": "code", "metadata": {"_uuid": "87c10bd797c7e27290c028faa55069707e1e459e", "_cell_guid": "62ce661f-2e42-4537-9006-ba134fda724e", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#KERAS DATA DEFINITION\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "def get_keras_data(dataset):\n", "    X = {\n", "        'name_onehot': pad_sequences(dataset['name_onehot'], maxlen=MAX_NAME_SEQ),\n", "        'item_description_onehot': pad_sequences(dataset['item_description_onehot']\n", "                                    , maxlen=MAX_ITEM_DESC_SEQ),\n", "        'brand': np.array(dataset['brand']),\n", "        'category': np.array(dataset['category']),\n", "        'category_split': pad_sequences(dataset.category_split, maxlen=MAX_CATEGORY_SPLIT_SEQ),\n", "        'category_name_onehot': pad_sequences(dataset['category_name_onehot']\n", "                                        , maxlen=MAX_CATEGORY_NAME_SEQ),\n", "        'item_condition': np.array(dataset.item_condition_id),\n", "        'num_vars': np.array(dataset[[\"shipping\"]])\n", "    }\n", "    return X\n", "\n", "X_train = get_keras_data(dtrain)\n", "X_valid = get_keras_data(dvalid)\n", "X_test = get_keras_data(df_test)\n", "\n", "print('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "bf53de542ab6d73cb6a8ba7f03e294d6697d974c", "_cell_guid": "b594649d-ac37-4ddb-bb81-ede195264b96"}, "source": ["# Building Keras Model"]}, {"cell_type": "code", "metadata": {"_uuid": "c8399d04c61b37c9daed1095438e6db999fcbdb2", "_cell_guid": "9f7d6bc5-f3c3-4fad-ac82-86017410a496", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#KERAS MODEL DEFINITION\n", "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n", "    Activation, concatenate, GRU, Embedding, Flatten\n", "from keras.models import Model\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n", "from keras import backend as K\n", "from keras import optimizers\n", "from keras import initializers\n", "\n", "def rmsle(y, y_pred):\n", "    import math\n", "    assert len(y) == len(y_pred)\n", "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n", "              for i, pred in enumerate(y_pred)]\n", "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n", "\n", "dr = 0.25\n", "\n", "def get_model():\n", "    #params\n", "    dr_r = dr\n", "    \n", "    #Inputs\n", "    name_onehot = Input(shape=[X_train[\"name_onehot\"].shape[1]], name=\"name_onehot\")\n", "    item_description_onehot = Input(shape=[X_train[\"item_description_onehot\"].shape[1]], \n", "                                    name=\"item_description_onehot\")\n", "    brand = Input(shape=[1], name=\"brand\")\n", "    category = Input(shape=[1], name=\"category\")\n", "    category_split = Input(shape=[X_train[\"category_split\"].shape[1]], name=\"category_split\")\n", "    category_name_onehot = Input(shape=[X_train[\"category_name_onehot\"].shape[1]], \n", "                          name=\"category_name_onehot\")\n", "    item_condition = Input(shape=[1], name=\"item_condition\")\n", "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n", "    \n", "    #Embeddings layers\n", "    emb_size = 60\n", "    \n", "    emb_name_onehot = Embedding(MAX_TEXT, emb_size//3)(name_onehot)\n", "    emb_item_description_onehot = Embedding(MAX_TEXT, emb_size)(item_description_onehot)\n", "    emb_category_name_onehot = Embedding(MAX_TEXT, emb_size//3)(category_name_onehot)\n", "    emb_category_split = Embedding(MAX_CATEGORY_SPLIT, emb_size//2)(category_split)\n", "    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n", "    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n", "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n", "    \n", "    rnn_layer1 = GRU(16) (emb_item_description_onehot)\n", "    rnn_layer2 = GRU(8) (emb_category_name_onehot)\n", "    rnn_layer3 = GRU(8) (emb_category_split)\n", "    rnn_layer4 = GRU(8) (emb_name_onehot)\n", "    \n", "    #main layer\n", "    main_l = concatenate([\n", "        Flatten() (emb_brand)\n", "        , Flatten() (emb_category)\n", "        , Flatten() (emb_item_condition)\n", "        , rnn_layer1\n", "        , rnn_layer2\n", "        , rnn_layer3\n", "        , rnn_layer4\n", "        , num_vars\n", "    ])\n", "    main_l = Dropout(dr)(Dense(512,activation='relu') (main_l))\n", "    main_l = Dropout(dr)(Dense(64,activation='relu') (main_l))\n", "    main_l = Dropout(dr)(Dense(32,activation='relu') (main_l))\n", "    \n", "    #output\n", "    output = Dense(1,activation=\"linear\") (main_l)\n", "    \n", "    #model\n", "    model = Model([name_onehot, item_description_onehot, brand\n", "                   , category, category_name_onehot, category_split\n", "                   , item_condition, num_vars], output)\n", "    #optimizer = optimizers.RMSprop()\n", "    optimizer = optimizers.Adam()\n", "    model.compile(loss=\"mse\", \n", "                  optimizer=optimizer)\n", "    return model\n", "\n", "def eval_model(model):\n", "    val_preds = model.predict(X_valid)\n", "    val_preds = np.expm1(val_preds)\n", "    \n", "    y_true = np.array(dvalid.price.values)\n", "    y_pred = val_preds[:, 0]\n", "    v_rmsle = rmsle(y_true, y_pred)\n", "    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n", "    return v_rmsle\n", "#fin_lr=init_lr * (1/(1+decay))**(steps-1)\n", "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n", "\n", "print('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9e2343c16ad8f19360d29e1bdf106062dcea71d8", "_cell_guid": "d7a4a62e-1bf0-425b-8a41-c9b4f3c09f99"}, "source": ["# Train"]}, {"cell_type": "code", "metadata": {"_uuid": "8f2e0230b773ecc5315ffe6e70a3144c18e5920b", "_cell_guid": "eb6cdfd6-2257-4959-8f0a-aba624880ef9", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["gc.collect()\n", "#FITTING THE MODEL\n", "epochs = 2\n", "BATCH_SIZE = 512 * 3\n", "steps = int(len(X_train['name_onehot'])/BATCH_SIZE) * epochs\n", "lr_init, lr_fin = 0.013, 0.009\n", "lr_decay = exp_decay(lr_init, lr_fin, steps)\n", "log_subdir = '_'.join(['ep', str(epochs),\n", "                    'bs', str(BATCH_SIZE),\n", "                    'lrI', str(lr_init),\n", "                    'lrF', str(lr_fin),\n", "                    'dr', str(dr)])\n", "\n", "model = get_model()\n", "K.set_value(model.optimizer.lr, lr_init)\n", "K.set_value(model.optimizer.decay, lr_decay)\n", "\n", "history = model.fit(X_train, dtrain.target\n", "                    , epochs=epochs\n", "                    , batch_size=BATCH_SIZE\n", "                    , validation_split=0.01\n", "                    #, callbacks=[TensorBoard('./logs/'+log_subdir)]\n", "                    , verbose=10\n", "                    )\n", "\n", "print('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n", "#EVLUEATE THE MODEL ON DEV TEST\n", "v_rmsle = eval_model(model)\n", "print('[{}] Finished predicting valid set...'.format(time.time() - start_time))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ce1feb3103060791ae5bd70ea271dca5553c8b5d", "_cell_guid": "2583cac6-fbbe-4531-8147-12faa6b42029"}, "source": ["# Pred"]}, {"cell_type": "code", "metadata": {"_uuid": "a668cdd99d4a2da74638e641843bc1b0a8727324", "_cell_guid": "6131d97e-2fa2-4eb2-b4db-94bd4f0880e7", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["#CREATE PREDICTIONS\n", "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n", "preds = np.expm1(preds)\n", "print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n", "submission = df_test[[\"test_id\"]]\n", "submission[\"price\"] = preds\n", "submission.to_csv(\"./myNN\"+log_subdir+\"_{:.6}.csv\".format(v_rmsle), index=False)\n", "print('[{}] Finished submission...'.format(time.time() - start_time))"]}, {"cell_type": "code", "metadata": {"_uuid": "c9b8bf267bf0d0105ae7fb308433164cf56dd076", "_cell_guid": "40483e4b-72ea-4919-8e5f-55ef54c9cbc6", "collapsed": true}, "outputs": [], "execution_count": null, "source": []}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.4", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat": 4}