{"nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["This is feature elimination based on **[Boruta](https://m2.icm.edu.pl/boruta/)**. Thanks to **[@olivier](https://www.kaggle.com/ogrellier)** for **[discussions](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41595#233852)** and for **[this notebook](https://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features)** that got me going in this direction.\n", "\n", "Note that olivier used LightGBM as his **[base estimator](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41595#234273)** while I am using Random Forest. Because of that, the results are different. I have no way of telling which feature selection is better as I haven't tested either one yet. If you do test them, please leave a note here. I will do the same."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "134f913c46ed9330d667fe844e7015cd8c24059a", "collapsed": true, "_cell_guid": "524b2ce3-1454-43db-b022-3da107320a70"}, "source": ["from __future__ import print_function\n", "\n", "import pandas as pd\n", "import numpy as np\n", "from datetime import datetime\n", "from sklearn.ensemble import RandomForestClassifier\n", "from boruta import BorutaPy\n", "\n", "pd.set_option('display.max_rows', 1000)\n", "pd.set_option('display.max_columns', 1000)\n", "\n", "def timer(start_time=None):\n", "    if not start_time:\n", "        start_time = datetime.now()\n", "        return start_time\n", "    elif start_time:\n", "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n", "        tmin, tsec = divmod(temp_sec, 60)\n", "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "681a453844ecbc0ce36992aa1ed83cce739fc56a", "_cell_guid": "175c8476-e632-40af-a528-dc62908fbf92"}, "source": ["Loading files."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e2f0ab2ca2c63557d0455f3ffc61cb1930995f8e", "collapsed": true, "_cell_guid": "7b35b99d-91b1-4e69-95b7-5a98cf911445"}, "source": ["train = pd.read_csv('../input/train.csv', dtype={'target': np.int8, 'id': np.int32})\n", "X = train.drop(['id','target'], axis=1).values\n", "y = train['target'].values\n", "tr_ids = train['id'].values\n", "n_train = len(X)\n", "test = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\n", "X_test = test.drop(['id'], axis=1).values\n", "te_ids = test['id'].values"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "e1490b1f7abb55259cb7c0c3ad71cebe2ae46f49", "_cell_guid": "8a01bb11-753b-47b7-8b6c-eabe8e87a9f4"}, "source": ["It is worth playing with **[RFC parameters](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**. Initially, I had *n_estimators=100* and *max_depth=10* which was not selecting enough features. Boruta parameters are explained **[here](https://github.com/scikit-learn-contrib/boruta_py)**."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "89347b7e8f8f3ceca68bcbe026a8a2a413d43ba1", "_cell_guid": "79a24d6c-4215-4135-9b11-4d312af6daf9"}, "source": ["rfc = RandomForestClassifier(n_estimators=200, n_jobs=4, class_weight='balanced', max_depth=6)\n", "boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2)\n", "start_time = timer(None)\n", "boruta_selector.fit(X, y)\n", "timer(start_time)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "d9d007291afdaa42f6269f44e84a0d3a8ff554c8", "_cell_guid": "b2f2d7fc-58a3-4526-bab2-bdcc1f93af66"}, "source": ["The summary of the whole run is shown here. Couple of attributes at the end are commented out. Finally, we save train and test datasets with a subset of selected features."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0cde287e95efbd9f516d1b9c44b7af3578373841", "_cell_guid": "8a903aee-cd84-4589-bb21-41fa12870172"}, "source": ["print ('\\n Initial features: ', train.drop(['id','target'], axis=1).columns.tolist() )\n", "\n", "# number of selected features\n", "print ('\\n Number of selected features:')\n", "print (boruta_selector.n_features_)\n", "\n", "feature_df = pd.DataFrame(train.drop(['id','target'], axis=1).columns.tolist(), columns=['features'])\n", "feature_df['rank']=boruta_selector.ranking_\n", "feature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)\n", "print ('\\n Top %d features:' % boruta_selector.n_features_)\n", "print (feature_df.head(boruta_selector.n_features_))\n", "feature_df.to_csv('boruta-feature-ranking.csv', index=False)\n", "\n", "# check ranking of features\n", "print ('\\n Feature ranking:')\n", "print (boruta_selector.ranking_)\n", "\n", "# check selected features\n", "# print ('\\n Selected features:')\n", "# print (boruta_selector.support_)\n", "\n", "# check weak features\n", "# print ('\\n Support for weak features:')\n", "#print (boruta_selector.support_weak_)\n", "\n", "selected = train.drop(['id','target'], axis=1).columns[boruta_selector.support_]\n", "train = train[selected]\n", "train['id'] = tr_ids\n", "train['target'] = y\n", "train = train.set_index('id')\n", "train.to_csv('train_boruta_filtered.csv', index_label='id')\n", "test = test[selected]\n", "test['id'] = te_ids\n", "test = test.set_index('id')\n", "test.to_csv('test_boruta_filtered.csv', index_label='id')"], "outputs": []}], "nbformat": 4}