{"cells":[{"metadata":{"_uuid":"29b4b822e29e7755633133ad4220748f68cc7af6"},"cell_type":"markdown","source":"## Table of Contents\n### Intro\n\n### Goals\n\n### Resources\n\n### Data Exploration\n\n- Distributions\n- Missing Data\n- Outliers\n- Data Correlation\n\n### Data Cleaning\n\n- Transform Target Variable & Independent Variables\n- Impute Missing Values\n- Remove Outliers\n- Encode Categorical Variables\n- Additional Cleaning\n\n### Data Engineering\n\n- Create Binary Features\n- Combine Features\n\n### Modeling\n"},{"metadata":{"_uuid":"6ededa29b2fc76d5f760a9bf3b14cc85c5215904"},"cell_type":"markdown","source":"____________________\n## Intro\nThis is my first Kaggle notebook, where I will be following along a number of other well-established guides for Exploratory Data Analysis and Data Cleaning as applied to the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) dataset.\n\n## Goals\nAs I jumped into machine learning, I quickly realized that understanding and cleaning data, with the ultimate goal of generating high quality inputs for any given machine learning model was something I needed to learn more about. \n\nWith that said, my goal here is to: \n- Learn how to better understand data prior to applying any machine learning models. \n- Learn data cleaning best practices. \n- Practice documenting my process of EDA and data cleaning. \n\n## Resources\n**Note:** I realized that these first two kernels were too far ahead of my current level in foundational understanding of stats, which is why I switched to following along the 3 kernels in this list. \n- [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by [Pedro Marcelino](https://www.kaggle.com/pmarcelino)\n- [House Prices EDA](https://www.kaggle.com/dgawlik/house-prices-eda) by [Dominik Gawlik](https://www.kaggle.com/dgawlik)\n- [Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction) by [Will Koehrsen](https://www.kaggle.com/willkoehrsen/) - while this kernel is not analyzing the same data, it has concepts that I can easily follow as a beginner. \n\n**Update:** [Claire](https://www.kaggle.com/clairevignonkeser) has joined me to help out!"},{"metadata":{"_uuid":"2feaa39a2db3254e414fa55735f60d9d41bcfa21","scrolled":true,"trusted":false},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport time\n\n# import xgboost as xgb\nfrom IPython.display import HTML, display\n\nfrom scipy import stats\nfrom scipy.stats import norm\nimport sklearn.linear_model as linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n# Some personal settings:\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 20\n%config InlineBackend.figure_format='retina'\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a421e5e7fe79083646eedabf57371089e0bf59"},"cell_type":"markdown","source":"**CONVENTION:** When importing libraries, use common names for libraries as seen with `sns`, `pd`, etc..."},{"metadata":{"_uuid":"060a78e01f42fbfb8fb8896e1ae86d15ae52ac12","scrolled":true,"trusted":false},"cell_type":"code","source":"print(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522b252e3700c44f4ddf06168e01b63310903795","scrolled":true,"trusted":false},"cell_type":"code","source":"# Save training and test data as a pandas DataFrames\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"696b3aad630ca13e16c339d327476def1c84efd0"},"cell_type":"markdown","source":"**CONVENTION:** When naming variables, use `_df` to designate a DataFrame?"},{"metadata":{"_uuid":"4605ce80e3cb96b58838c7d2cfe53ed9223f3ee7","scrolled":true,"trusted":false},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30b46771f83b730b566482f4e4345fef293e7734","scrolled":false,"trusted":false},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15808c2f090312e52eff6b91544a25bf0e28e4ad"},"cell_type":"markdown","source":"Optionally: look at column descriptions"},{"metadata":{"_uuid":"fea1220a81e72747f9237a13d150b5837bf1e86d","scrolled":true,"trusted":false},"cell_type":"code","source":"f = open('../input/data_description.txt', \"r\")\n# print(f.read())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e5eff0f0593ad498fc403aea249be0612e47763","scrolled":true,"trusted":false},"cell_type":"code","source":"# An idea seen here https://www.kaggle.com/dgawlik/house-prices-eda to separate data by type\nquant_vars = [f for f in train_df.columns if train_df.dtypes[f] != 'object']\nquant_vars.remove('SalePrice')\nquant_vars.remove('Id')\nqual_vars = [f for f in train_df.columns if train_df.dtypes[f] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"714a410750018d359640d0da37e256a214d3f1f5"},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"_uuid":"75a2f94c668714b7b558a4184c43a6d20a9dc5b9"},"cell_type":"markdown","source":"### 1- Distributions\n\n#### Target Variable (Y Variable)\nWe want to look at the target variable and see if it follows a normal distribution. This is because models love normally distributed data and perform better when the data is normally distributed."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"55287eed730aac4d194008c28218249669a6a387"},"cell_type":"code","source":"# Look at distribution of the target variable (i.e. sales price)\nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Look at the QQ-plot which is another way to assess if data is normally distributed\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ebf4b31f929d802bd087020b70ca9254a4ff14f"},"cell_type":"markdown","source":"The target variable is right skewed as shown by the long right tail on the first graph. The second graph (the QQplot) is another way to visualize if a variable is normally distributed. If the dots deviate from the straight red line, then, it means that the data is not normally distributed. You can [read more about QQ plots here](https://data.library.virginia.edu/understanding-q-q-plots/).\n\nWe'll need to transform the target variable and make it more normally distributed."},{"metadata":{"_uuid":"ccb19cf1b35dc4d9710396c102ecccfb96278121"},"cell_type":"markdown","source":"#### Independent variables\nWe'll visualize the distribution for each variable. Same as for the target variable, we'd want to tranform the variables that are not normally distributed (above all if we want to use a linear regression) and / or remove the outliers (see section 3- Outliers)."},{"metadata":{"_uuid":"f86bf4e45531c3b102f5f639f292ba7da7713761","scrolled":false,"trusted":false},"cell_type":"code","source":"# .melt unpivots the table, making it easier to analyze each variable.\nquant_vars_long = pd.melt(train_df, value_vars=quant_vars)\n\n# Throwing this into a FacetGrid of distplots\nfacet_grid_quant = sns.FacetGrid(quant_vars_long, col=\"variable\",  col_wrap=5, sharex=False, sharey=False)\nfacet_grid_quant = facet_grid_quant.map(sns.distplot, \"value\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"979802333b1b028fcc0e7ea9424fa128cd28d851"},"cell_type":"markdown","source":"### 2- Missing Data\n\nTime to look at missing data.  "},{"metadata":{"_uuid":"5e0dce1313e8a9b2ba03ea7194c1bbac10a67b5e","scrolled":false,"trusted":false},"cell_type":"code","source":"nulls_sum = train_df.isnull().sum().sort_values(ascending=False)\nnulls_percent = (100 * train_df.isnull().sum()/train_df.isnull().count()).round(1).sort_values(ascending=False)\nnulls_overview = pd.concat([nulls_sum, nulls_percent], axis=1, keys=['Total', 'Percent'])\nnulls_overview[nulls_overview['Total'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1373f5c174da2fd9f8019096843be43891dfff2a"},"cell_type":"markdown","source":"Another way to look at null values using a trick from https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/"},{"metadata":{"_uuid":"6d691950bb326b35fd346b5e59cc19ea0d665d60","scrolled":false,"trusted":false},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(12, 9))\nsns.heatmap(train_df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54faca4f6baa972e8b2b54ccf0de7151aee0b222"},"cell_type":"markdown","source":"**BEST PRACTICE:** As [pointed out](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python#257915) by [Wall-E](https://www.kaggle.com/shuhuagao), it's a good idea to read the [data_desription.txt](../input/data_description.txt) file before going about replacing NAs willy-nilly. For example, NA values in the `GarageType: Garage location` variable simply mean that there is no Garage, so we shouldn't be trying to impute some other value. (At least that's what I've understood.)"},{"metadata":{"_uuid":"43e331cfd05ea9642ce579ecbd42a46e153e07db"},"cell_type":"markdown","source":"### 3- Outliers\nWe plan on using a simple linear regression. While some models such as Random Forest are pretty robust to outliers, linear models (e.g. Lasso, Linear Regression) are not. This means that we'd want to consider removing the outliers when cleaning the data. \n\nAnother option is to make the models more robust to outliers. For example, we could use the `RobustScaler()` method for a Lasso or Elastic Net Regression, or choose **Huber** as the `loss` parameter in the Gradient Boosting Regression.\n\nThe latter may be preferred as they may also be outliers in the testing data. However, even if there were, we want our model to be as accurate as possible and outliers can jeopardize the accuracy of our model in the first place.\n\nWe'll look at a few plots to check outliers."},{"metadata":{"trusted":false,"_uuid":"f5231f46d9e5bbe899e672a637407611a3554883"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[0:5]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"79df4cdf1100b7e64c3799cf70d0201e4dff0974"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[6:11]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b4ab113c9b5b2e389ac96b7f3ae0d086805bdb7e"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[12:17]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c0cf72f1a6b29c9a12d11d82854fc00c9ce296fb"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[18:23]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5fd157d37aaa5c3da04b7d3528f7f2ecc9d8139f"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[24:29]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a177e81a5acd7f721aedd37a315c168fb740e91b"},"cell_type":"code","source":"sns.set()\nx_vars = quant_vars[30:36]\ny_vars = ['SalePrice']\nsns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0287ce0e71f011bf053be3769ff7714435a7e386"},"cell_type":"code","source":"# sns.set()\n# x_vars = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n# y_vars = ['SalePrice']\n# sns.pairplot(train_df, size = 2.5, x_vars = x_vars, y_vars = y_vars)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"322ff819aaa448a3be19c05d111a257b6a293f73"},"cell_type":"markdown","source":"We see that the variable `GrLivArea` has outlier values along with `TotalBsmtSF`. \nIn particular, for the variable `GrLivArea`, there are two with extremely large GrLivArea that are at a low price (at the bottom right corner). These values are outliers and we will delete them when cleaning the data."},{"metadata":{"_uuid":"9015b6746ab9ab571b3e6b390cacdc6424e75d2c"},"cell_type":"markdown","source":"### 4- Data Correlation\n\nThis correlation matrix helps quickly find relationships between variables."},{"metadata":{"_uuid":"5fb1669236b130fa3bc4eb4b048f435b2c22e6a9","scrolled":false,"trusted":false},"cell_type":"code","source":"corr_mat = train_df.corr(method='spearman')\nfig, axes = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_mat[(corr_mat >= 0.7) | (corr_mat <= -0.7)], vmax=1, vmin=-1, \n            square=True, cmap=sns.cm.rocket_r);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"629376cea0d8fecb44d56106812f476d52a95bd9"},"cell_type":"markdown","source":"**CONCEPT:** Variables that are strongly correlated, such as with `TotalBsmtSF` and `1stFlrSF`, could be an example of **multicollinearity**, which I believe means we will need to consolidate these as a single attribute rather than keeping separate. \nUsing some common sense, it's to be expected that `GarageCars` and `GarageArea` describe the same thing. "},{"metadata":{"_uuid":"64ed092bf4588d6413813e69a8277831d74af409","scrolled":false,"trusted":false},"cell_type":"code","source":"# Alternatively, to focus on SalePrice...\nfig, axes = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_mat[['SalePrice']], vmax=.8, square=True, cmap=sns.cm.rocket_r)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da82ae900d462cc4aa18629a4edea84783ca0fca","scrolled":true,"trusted":false},"cell_type":"code","source":"# To clearly see these numerically...\n# corr_mat = train_df.corr(method='spearman')\ncorr_mat.nlargest(10, 'SalePrice')[['SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"278640b4003c62c5549bbea0c64420d102344fa1"},"cell_type":"markdown","source":"#### Additional ways to look for relationships between variables:\n1- Looking for linear relationships between variables using a pairplot:\n\nThis works better with quantitative variables. We select the variables that were highly correlated with the price."},{"metadata":{"_uuid":"b17c657d3fba3c7b86ddd7913647093a8e7a978d","scrolled":false,"trusted":false},"cell_type":"code","source":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b88716652b84f47b1e1032cca373499a877c936c"},"cell_type":"markdown","source":"2- Looking at the relationships between qualitative variables and Sale Price:\n\nWe'll create a boxplot for each variable. "},{"metadata":{"_uuid":"48318e9ce6b424f70d0645ddba03ed1b888f4340","scrolled":false,"trusted":false},"cell_type":"code","source":"for column in qual_vars:\n    train_df[column] = train_df[column].astype('category')\n    if train_df[column].isnull().any():\n        train_df[column] = train_df[column].cat.add_categories(['UNDEFINED'])\n        train_df[column] = train_df[column].fillna('UNDEFINED')\n\ndef show_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \nqual_vars_long = pd.melt(train_df, id_vars=['SalePrice'], value_vars=qual_vars)\nfacet_grid_qual = sns.FacetGrid(qual_vars_long, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, size=5)\nfacet_grid_qual = facet_grid_qual.map(show_boxplot, \"value\", \"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d9133f227d95a88700c777fddc24e599ec10d20"},"cell_type":"markdown","source":"## Data Cleaning\nBased on the data exploration above, we'll clean our data by transforming out target variable, replacing missing values, removing outliers and encoding categorical variables.\n\n### 1- Transform Target Variable & Independent Variables\n#### A- Target Variable\nWe'll perform a log tranformation of the target variable so that it follows a normal distribution. There are other ways and maybe more accurate ways to transform the target variable but we'll stick to the simple solution for now and make sure it works."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"5acaccf7cce21941b8239f6601c28735e01a8f27"},"cell_type":"code","source":"# Use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\n# Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Check the new distribution with the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"643b128ffcef1d35fbec518dee35fe054e761e74"},"cell_type":"markdown","source":"We corrected the right skew. The target variable is now normally distributed."},{"metadata":{"_uuid":"52f2b4605bd4de8f6278bb4ea33de466a09586a5"},"cell_type":"markdown","source":"#### B- Independent variables\nWe'll log tranform all the quantitative features. In theory, we should only do that for features that are not normally distributed but to make it easier for now, we'll do it for all."},{"metadata":{"trusted":false,"_uuid":"8ae4d11246eb711f4672ebc9d759546e273edd4c"},"cell_type":"code","source":"for i in quant_vars:\n    train_df[i] = train_df[i].apply(lambda x: np.log1p(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86137bda157b318bb2ee898c37cfb9ef88540a53"},"cell_type":"markdown","source":"Making sure that the transformation was done correctly. For example, we see that the feature `GrLivArea` now follows a normal distribution."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"38ac25d437fd41df05fb10d11839fb7a17668b73"},"cell_type":"code","source":"# .melt unpivots the table, making it easier to analyze each variable.\nquant_vars_long = pd.melt(train_df, value_vars=quant_vars)\n\n# Throwing this into a FacetGrid of distplots\nfacet_grid_quant = sns.FacetGrid(quant_vars_long, col=\"variable\",  col_wrap=4, sharex=False, sharey=False)\nfacet_grid_quant = facet_grid_quant.map(sns.distplot, \"value\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ad673728095a3360c42b7d2823b27ed69bac40b"},"cell_type":"markdown","source":"Now, let's standardize the numeric features."},{"metadata":{"trusted":false,"_uuid":"6a1448a100c280b699e083473f85343753bea443"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(train_df[quant_vars])\nscaled = scaler.transform(train_df[quant_vars])\n\nfor i, col in enumerate(quant_vars):\n       train_df[col] = scaled[:,i]\n\nscaled = scaler.fit_transform(test_df[quant_vars])\n\nfor i, col in enumerate(quant_vars):\n      test_df[col] = scaled[:,i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7a2b90efe493e20a4e7a561b8ab32a08e4c178a"},"cell_type":"markdown","source":"### 2- Impute Missing Values"},{"metadata":{"_uuid":"41ea6f78799546b20a8c92e5ef0081906031e7d0"},"cell_type":"markdown","source":"### 3- Remove Outliers\n"},{"metadata":{"trusted":false,"_uuid":"35fd34b1204b3e978ede53eff40acc8e222d267e"},"cell_type":"code","source":"# Delete outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\ntrain_df = train_df.drop(train_df[(train_df['TotalBsmtSF']>6000)].index)\n\n# Check the graphic again\nsns.scatterplot(data=train_df, x='GrLivArea', y='SalePrice')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c472b85a301aa16926ea545878175569102ddbb"},"cell_type":"markdown","source":"### 4- Encode Categorical Variables"},{"metadata":{"_uuid":"6b05d42f9dd7e45c4d3cc731716cef4011e32f0d"},"cell_type":"markdown","source":"**NEXT:** We'll continue to follow along https://www.kaggle.com/dgawlik/house-prices-eda to encode these categorical variables."},{"metadata":{"_uuid":"b824e36c1efcfb85d51f4eec92ece3d1a2d52b6d"},"cell_type":"markdown","source":"The function below performs a one-way ANOVA test to see if the categories within each feature differs significantly re. the Sale Price. When the p-value is < 0.5%, then it proves that there is a significant difference between the categories re. their Sale Price. Thus, it would be valuable to encode the variables to account for the differences."},{"metadata":{"_uuid":"990048764f33ef7c581c9860b7cf557f0f01e0c2","scrolled":true,"trusted":false},"cell_type":"code","source":"def get_anova(data):\n    \n    # Create new dataframe\n    anova_df = pd.DataFrame()\n    \n    # Create a row per qualitative variable name\n    anova_df['feature'] = qual_vars\n    \n    # Create empty list to hold p values\n    p_values = []\n    \n    # Loop through qualitative variable name \n    # Example: 'Neighborhood'\n    for qual_var in qual_vars:\n\n        # Create empty list to hold 'samples' \n        # This will contain SalePrice values, grouped by unique values for the qualitative variable\n        samples = []\n        \n        # Loop through unique values for the qualitative variable found in training data \n        # Example: (For Neighborhood) 'CollgCr'\n        for unique_qual_val in data[qual_var].unique():\n            \n            # Get the SalePrice values for that categorical value\n            # Example: (For Neighborhood.CollgCr [208500, 223500, 279500, 230000, 179900, ...]\n            sample = data[data[qual_var] == unique_qual_val]['SalePrice'].values\n            \n            # Append this list of SalePrice values to the samples list of lists\n            samples.append(sample)\n        \n        # Run a one-way ANOVA test comparing the SalePrice values per unique, qualitative value\n        # Example: (For Neighborhood) 1.558600282771154e-225\n            # The reason [1] is there is to grab only the p value since this is what is returned by f_oneway():\n            # F_onewayResult(statistic=71.78486512058272, pvalue=1.558600282771154e-225)\n        p_value = stats.f_oneway(*samples)[1]\n        \n        # Append the p value for the various SalePrice lists to the p_values list\n        p_values.append(p_value)\n    \n    # Merge the p value series to the anova DataFrame\n    anova_df['p_value'] = p_values\n    return anova_df.sort_values('p_value')\n        \nanova_df = get_anova(train_df)\nanova_df.sort_values('p_value').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d571a5e25acd52b32072a55dd96d27883a51f9a8"},"cell_type":"markdown","source":"And then this happens..."},{"metadata":{"_uuid":"2956e32904c900731dd08ad49bde69ab0c0a5da7","scrolled":true,"trusted":false},"cell_type":"code","source":"anova_df['disparity'] = np.log(1./anova_df['p_value'].values)\nanova_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4384c2f79de9af0ec869d422973acbc48523e8d1"},"cell_type":"markdown","source":"Dividing `1` by the p value changes the magnitude if the p value so that smaller p value (which indicates significance) show a higher disparity between the categories within each feature."},{"metadata":{"_uuid":"92aa5b692ae1809c9d4d5a1181c95cefebe20ad0","scrolled":true,"trusted":false},"cell_type":"code","source":"np.log(1./anova_df['p_value'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10b3b8a29befd4dc49828bd065c05f224b21640a"},"cell_type":"markdown","source":"Plotting this gets us a chart showing the \"disparity\" in the SalePrice within each qualitative feature. It helps us understand which features are important (as the categories within these feature do have a significantly different price)."},{"metadata":{"_uuid":"4e66167cb0c5ac1cab9dac7329a110d4de885c80","scrolled":false,"trusted":false},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(12, 9))\nsns.barplot(data=anova_df, x='feature', y='disparity')\nx=plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"765155942e5e2d348d7c5daf6a2b0d31725ed7bc"},"cell_type":"markdown","source":"[Dgawlik](https://www.kaggle.com/dgawlik/) then encodes qualitative variables based on the mean of SalePrice for each unique value, ordered starting at 1. We'll use the median instead of the mean to prevent outliers from skewing the results."},{"metadata":{"_uuid":"50230dcb8ad4a5974db48e41d3931dbe390da7a7","scrolled":true,"trusted":false},"cell_type":"code","source":"def encode_qual_vars(data, qual_vars, suffix):\n\n    # Create empty DataFrame for names of encoded qualitative variables and to use to look up later\n    qual_vars_encoded = pd.DataFrame(columns=['Variable_Name','Encoded_Name','Value','Order'])\n    encoded_df = data.copy();\n    for qual_var in qual_vars:\n        \n        # Establish name for encoded version of qualitative variable and add it to list\n        qual_var_encoded_name = qual_var + suffix\n        qual_vars_encoded\n        \n        # Create empty DataFrame \n        order_df = pd.DataFrame()\n\n        # Example: [CollgCr, Veenker, Crawfor, ...]\n        order_df['val'] = data[qual_var].unique() \n        \n        # Set index to unique variable value (as opposed to default 0,1,2,..)\n        order_df.index = order_df.val  \n\n        # Create column for mean of SalePrice grouped by unique qualitative value\n        order_df['saleprice_median'] = data[[qual_var,'SalePrice']].groupby(qual_var)[['SalePrice']].median()\n\n        # Sort by SalePrice\n        order_df = order_df.sort_values('saleprice_median')\n\n        # Create a column for the order of values, starting at 1\n        order_df['order'] = range(1, len(order_df)+1)\n    \n        # Convert to dictionary to make it easy to iterate over\n        order_df = order_df['order'].to_dict()\n        \n        # For each unique variable and its order... \n        for qual_val, order in order_df.items():\n            # ... Get the matching rows in train_df, \n            # and insert an encoded version of the variable column name with suffix ,\n            # and set value as the order of that value as established above\n            encoded_df.loc[encoded_df[qual_var] == qual_val, qual_var_encoded_name] = order\n            qual_vars_encoded = qual_vars_encoded.append({'Variable_Name': qual_var, 'Encoded_Name': qual_var_encoded_name, 'Value': qual_val, 'Order': order}, ignore_index=True)\n    return (encoded_df, qual_vars_encoded)\n\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n# qual_vars = [f for f in train_df.columns if train_df.dtypes[f] == 'object']\ntrain_df_encoded, qual_vars_encoded = encode_qual_vars(train_df, qual_vars, '_QE')\nqual_vars_encoded.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"cbb6bf994555eec8d697ee5eb7e991dcc6578372"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5593a8bfcf5c4204388fa3e5895a8316164b970"},"cell_type":"markdown","source":"Then a Spearman correlation is conducted: \n> Spearman correlation is better to work with in this case because it picks up relationships between variables even when they are nonlinear. - https://www.kaggle.com/dgawlik/house-prices-eda"},{"metadata":{"_uuid":"b401a39739bad0184aabbb40a69cb3f063026d51","scrolled":false,"trusted":false},"cell_type":"code","source":"def get_correlation(data):\n    var_correlations = pd.DataFrame()\n    var_correlations['variables'] = data.drop(columns=['Id','SalePrice']).columns\n    fig, plot = plt.subplots(1, 1,figsize=(5, 0.25*len(var_correlations['variables'])))\n    fig.subplots_adjust(wspace=0.4)\n    var_correlations['spearman'] = [data[var].corr(data['SalePrice'], method='spearman') for var in var_correlations['variables']]\n    var_correlations = var_correlations.sort_values('spearman')\n    sns.barplot(data=var_correlations, y='variables', x='spearman', orient='h', ax=plot)\n    return var_correlations\n    \nall_var_corr = get_correlation(train_df_encoded.drop(columns=qual_vars))\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"7ab3e89de5bd961c358aca088b2c8c41ac7fca19"},"cell_type":"code","source":"high_corr_vars = all_var_corr[(all_var_corr['spearman'] >= 0.4) | (all_var_corr['spearman'] <= -0.4)]['variables'].values\nhigh_corr_vars","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"f21c1547ec5ee8df596f6da4f30de197255cb57f"},"cell_type":"code","source":"low_corr_vars = all_var_corr[(all_var_corr['spearman'] <= 0.1) & (all_var_corr['spearman'] >= -0.1)]['variables'].values\nlow_corr_vars","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"721a9ed9fed30025f77d2d024df301b9bb979b1b"},"cell_type":"markdown","source":"First, let's get a baseline for where we are without doing too much more clean-up work... "},{"metadata":{"_uuid":"a5d40477dcaab026efd7a08025f9ac0bc5f3eb66","scrolled":true,"trusted":false},"cell_type":"code","source":"def encode_qual_vars_test(data, qual_vars_encoded):\n    for encoded_index, encoded_row in qual_vars_encoded.iterrows():\n        variable = encoded_row['Variable_Name']\n        encoded_name = encoded_row['Encoded_Name']\n        value = encoded_row['Value']\n        order = encoded_row['Order'] \n        data.loc[data[variable] == value, encoded_name] = order\n    return data\n\ndef encode_qual_vars_train(data, qual_vars, suffix):\n    qual_vars_encoded = pd.DataFrame(columns=['Variable_Name','Encoded_Name','Value','Order'])\n    encoded_df = data.copy()\n    for qual_var in qual_vars:\n        qual_var_encoded_name = qual_var + suffix\n        qual_vars_encoded\n        order_df = pd.DataFrame()\n        order_df['val'] = data[qual_var].unique() \n        order_df.index = order_df.val  \n        order_df['saleprice_median'] = data[[qual_var,'SalePrice']].groupby(qual_var)[['SalePrice']].median()\n        order_df = order_df.sort_values('saleprice_median')\n        order_df['order'] = range(1, len(order_df)+1)\n        order_df = order_df['order'].to_dict()\n        for qual_val, order in order_df.items():\n            encoded_df.loc[encoded_df[qual_var] == qual_val, qual_var_encoded_name] = order\n            qual_vars_encoded = qual_vars_encoded.append({'Variable_Name': qual_var, 'Encoded_Name': qual_var_encoded_name, 'Value': qual_val, 'Order': order}, ignore_index=True)\n    return (encoded_df, qual_vars_encoded)\n\ndef encode_qual(train, test):\n    qual_vars = [f for f in train.columns if train.dtypes[f] == 'object']\n   \n    # Training Data\n    train, qual_vars_encoded = encode_qual_vars_train(train, qual_vars, '_QE')\n    train = train.drop(columns=qual_vars)\n    \n    # Test Data\n    test = encode_qual_vars_test(test, qual_vars_encoded)\n    test = test.drop(columns=qual_vars)\n    \n    return (train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e13a506221d1eccbd3624feb6f06907999fb64b"},"cell_type":"markdown","source":"### 5- Additional Cleaning\n#### Change Type of some Variables\nThe variable `MoSold` is an integer. This may confuse the model into thinking that is a numerical variable with Dec (12) being 12 times more valuable than Jan (1)."},{"metadata":{"_uuid":"85f5480fb5f392514f5a79ed37f9e60c8b0af36b"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"scrolled":true,"_uuid":"2d77f181d6d8c4e83e521b3319b88efc2581eb36"},"cell_type":"markdown","source":"### Create Binary Variables\n#### Features: Month Sold & Year Sold\n\nThe features `MoSold` and `YrSold` may not be valuable in themselves. \n\n- It may be a good idea to see if prices are higher for some months and create a binary feature `is_high_seasonal_month`.\n- One hypothesis is that the prices in 2008-2009 (during the recession) were lower. If so, adding a column `is_recession_year` would prove to be very valuable.\n\nLet's look at the price of the houses over the year to see if there is some seasonality and maybe a crash in prices in 2008 and 2009.\nWe'll combine the month and year into a datetime feature. To use the `pd.to_datetime` method, we need to provide a year, month but also day. We'll create a column with 1 for the day of sale."},{"metadata":{"trusted":false,"_uuid":"0f5d17686918c97f72e5f54bc4aa7508616777bb"},"cell_type":"code","source":"date_sold_df = train_df.assign(DaySold=1)\ndate_sold_df['DateSold'] = pd.to_datetime(dict(year=date_sold_df['YrSold'], \n                                               month=date_sold_df['MoSold'], \n                                               day=date_sold_df['DaySold']))\n\n\nplt.figure(figsize=(16,4))\nsns.lineplot(x='DateSold', y='SalePrice', data=date_sold_df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e644ae53cd7543c4cc686b7d5255a751ec4aa6e2"},"cell_type":"markdown","source":"Another way to visualize housing prices by month and year (via a heatmap)."},{"metadata":{"trusted":false,"_uuid":"9084d88d9009e1adf84fd1669e88bdba36320153"},"cell_type":"code","source":"date_sold_full_heatmap = date_sold_df[['YrSold', 'MoSold', 'SalePrice']]\\\n        .pivot_table(index = 'YrSold', columns = 'MoSold', values = 'SalePrice', aggfunc='median')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c93b1167f6b21fc0c687cb35398b7e1b689afde4"},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nsns.heatmap(date_sold_full_heatmap,square=True, cmap=sns.cm.rocket_r);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6747aba7ec04769cf290ed69d7c9622488c0e624"},"cell_type":"markdown","source":"There does not seem to be a monthly seasonality or a crash in prices in 2008-2009.\nAnother way to visualize how the price varies by month or year is to use boxplots."},{"metadata":{"trusted":false,"_uuid":"08dc49efd8259e1cfb7635fb77cd1c73e6bd6e36"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.boxplot(x='MoSold', y='SalePrice', data=date_sold_df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6fa3d83e95fa8731c287d127bb41f8a2853f4417"},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.boxplot(x='YrSold', y='SalePrice', data=date_sold_df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a3b4e7f3bba265b12b9ad4bddfc09f15ce48821"},"cell_type":"markdown","source":"**There is no clear difference in prices by month or year. We will remove these features from our model as they don't seem to provide much value.**"},{"metadata":{"_uuid":"ad5da7758dd435b026ade3d1f39bd528ea69a4a2"},"cell_type":"markdown","source":"#### Features: Year Built & Year Remodeled"},{"metadata":{"trusted":false,"_uuid":"aee2837423996a52599f27170c313d8deba70d6f"},"cell_type":"code","source":"def house_remodel_and_age(df):\n    # add flag is house has been remodeled (the year of the remodel is same as construction date \n    # if no remodeling or additions))\n    df['is_remodeled'] = (df['YearRemodAdd'] != df['YearBuilt'])\n\n    # add feature about the age of the house when sold\n    df['age'] = df['YrSold'] - df['YearBuilt']\n\n    # add flag if house was sold 2 years or less after it was built\n    df['is_new_house'] = (df['YrSold'] - df['YearBuilt'] <= 2)\n\n    # add flag is remodel was recent (i.e. within 2 years of the sale)\n    df['is_recent_remodel'] = (df['YrSold'] - df['YearRemodAdd'] <= 2)\n    \n    # drop the original columns\n    df = df.drop(['YearRemodAdd', 'YearBuilt'], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"56cf2602f49ed8ff76db5c07f1be736868b87081"},"cell_type":"code","source":"house_remodel_and_age(train_df).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"951f86524674c29f3ae3eef06e384bb2ad725cf2"},"cell_type":"markdown","source":"#### Features: Porch & Bathroom Square Footage"},{"metadata":{"trusted":false,"_uuid":"46c224af2fe9ad3cdb75ac1cd8c8a23d07efc2d7"},"cell_type":"code","source":"def bath_porch_sf(df):\n    # total SF for bathroom\n    df['TotalBath'] = df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']) + \\\n    df['FullBath'] + (0.5 * df['HalfBath'])\n\n    # Total SF for porch\n    df['AllPorchSF'] = df['OpenPorchSF'] + df['EnclosedPorch'] + \\\n    df['3SsnPorch'] + df['ScreenPorch']\n    \n    # drop the original columns\n    df = df.drop(['BsmtFullBath', 'FullBath', 'HalfBath', \n                 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch'], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a37dbffe10226a1d788ae64f3fccb062db23ab4"},"cell_type":"markdown","source":"### Combine some features"},{"metadata":{"trusted":false,"_uuid":"25e6f9864c9da3585c6ff645cefa9d6396aa4a93"},"cell_type":"code","source":"# Encode qualitative variables first\ntrain_df, test_df = encode_qual(train_df, test_df)\n\n# Total SF for house (incl. basement)\ntrain_df['AllSF'] = train_df['GrLivArea'] + train_df['TotalBsmtSF']\n\n# Total SF for 1st + 2nd floors\ntrain_df['AllFlrsSF'] = train_df['1stFlrSF'] + train_df['2ndFlrSF']\n\n# Overall quality of house\ntrain_df['OverallGrade'] = train_df['OverallQual'] * train_df['OverallCond']\n\n# Overall quality of the garage\ntrain_df['GarageGrade'] = train_df['GarageQual_QE'] * train_df['GarageCond_QE']\n\n# Overall quality of the exterior\ntrain_df['ExterGrade'] = train_df['ExterQual_QE'] * train_df['ExterCond_QE']\n\n# Overall kitchen score\ntrain_df['KitchenScore'] = train_df['KitchenAbvGr'] * train_df['KitchenQual_QE']\n\n# Overall fireplace score\ntrain_df['FireplaceScore'] = train_df['Fireplaces'] * train_df['FireplaceQu_QE']\n\n# Overall garage score\ntrain_df['GarageScore'] = train_df['GarageArea'] * train_df['GarageGrade']\n\n# Overall pool score\ntrain_df['PoolScore'] = train_df['PoolArea'] * train_df['PoolQC_QE']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fa71aaff8231f39f4fd9cebe721e6cb7c9741df"},"cell_type":"markdown","source":"Let's check whether or not these added features are correlated to the SalePrice"},{"metadata":{"trusted":false,"_uuid":"ec4ef39c18046c7f529a8a2383c2b7d86c5b8ce1"},"cell_type":"code","source":"# Find whether combinations of features are more correlated to target than features by themselves\nnew_cols = ['AllSF', 'GrLivArea', 'TotalBsmtSF', \n            'AllFlrsSF', '1stFlrSF', '2ndFlrSF',\n            'OverallGrade', 'OverallQual', 'OverallCond',\n            'GarageGrade', 'GarageQual_QE', 'GarageCond_QE', \n            'ExterGrade', 'ExterQual_QE', 'ExterCond_QE', \n            'KitchenScore', 'KitchenAbvGr', 'KitchenQual_QE', \n            'FireplaceScore', 'Fireplaces', 'FireplaceQu_QE', \n            'GarageScore', 'GarageArea', \n            'PoolScore', 'PoolArea', 'PoolQC_QE', \n            'SalePrice']\n\ncorr = train_df[new_cols].corr()\n\nplt.figure(figsize = (16,6))\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\n# print(corr.SalePrice)\nsns.heatmap(corr[['SalePrice']], vmax=1, square=True, cmap=sns.cm.rocket_r);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81ded2d50d0cabf7438fa24f5935c6db795d93a6"},"cell_type":"markdown","source":" Of the newly created features, we see that `AllSF`, `AllFlrsSF`, `PoolScore`, `ExterGrade`, `GarageScore`, `OverallGrade`, `KitchenScore` are highly correlated with the SalePrice, that is all the newly created features except for `GarageGrade`.\n We will add the highly correlated features to our model."},{"metadata":{"_uuid":"b6ef8e7d31183032a1b797523aa01803baf0efef"},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":false,"_uuid":"5c0cb4149567ff9cb77604b55a89da8b3ed4630f"},"cell_type":"code","source":"def combine_features(dfs_list):\n    modified_dfs = []\n    for df in dfs_list:\n        # Total SF for house (incl. basement)\n        df['AllSF'] = df['GrLivArea'] + df['TotalBsmtSF']\n\n        # Total SF for 1st + 2nd floors\n        df['AllFlrsSF'] = df['1stFlrSF'] + df['2ndFlrSF']\n\n        # Overall quality of house\n        df['OverallGrade'] = df['OverallQual'] * df['OverallCond']\n\n        # Overall quality of the garage\n        df['GarageGrade'] = df['GarageQual_QE'] * df['GarageCond_QE']\n\n        # Overall quality of the exterior\n        df['ExterGrade'] = df['ExterQual_QE'] * df['ExterCond_QE']\n\n        # Overall kitchen score\n        df['KitchenScore'] = df['KitchenAbvGr'] * df['KitchenQual_QE']\n\n        # Overall fireplace score\n        df['FireplaceScore'] = df['Fireplaces'] * df['FireplaceQu_QE']\n\n        # Overall garage score\n        df['GarageScore'] = df['GarageArea'] * df['GarageGrade']\n\n        # Overall pool score\n        df['PoolScore'] = df['PoolArea'] * df['PoolQC_QE']\n        \n        return modified_dfs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07777a6cfbebb859b4389ee6d71d86fb21536c1e","scrolled":true,"trusted":false},"cell_type":"code","source":"def modify_features(dfs_list, drop, make_binary, combine):\n    modified_dfs = []\n    for df in dfs_list:\n        df = df.fillna(0)\n        \n        for vars_to_combine in combine:\n            if len(vars_to_combine) > 2:\n                raise ValueError('Only put 2 vars at a time to combine.') \n            combined_name = '_'.join(vars_to_combine[:])\n            df[combined_name] = df[vars_to_combine[0]] + df[vars_to_combine[1]]\n            drop = drop+vars_to_combine\n        df = df.drop(columns=drop)\n        modified_dfs.append(df)\n    return modified_dfs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f4a28d24a61c3b7966f6bbcfe449e75ca573202","scrolled":true,"trusted":false},"cell_type":"code","source":"def fit_score_predict(train, test):\n    X = train.drop(columns=['SalePrice'])\n    y = train['SalePrice']\n    \n    # Basic Scoring\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    X_predictions = model.predict(X_test)\n    score = math.sqrt(mean_squared_error(y_test, X_predictions))\n    \n    # Fit on full train data\n    model.fit(X, y)\n    predictions = model.predict(test)\n    \n    return (predictions, score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2214062c1bf4dd9a6f5478fc312c32549fc902b","scrolled":true,"trusted":false},"cell_type":"code","source":"def score_parameters(parameters, times):\n    train_df = pd.read_csv('../input/train.csv')\n    test_df = pd.read_csv('../input/test.csv')\n    train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n    \n    # remove outliers\n    train_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n    train_df = train_df.drop(train_df[(train_df['TotalBsmtSF']>6000)].index)\n    \n    # transform x and y variables\n    train_df[quant_vars] = train_df[quant_vars].apply(lambda x: np.log1p(x))\n    test_df[quant_vars] = test_df[quant_vars].apply(lambda x: np.log1p(x))\n    \n    # scale numeric variables\n    scaler = StandardScaler()\n    scaler.fit(train_df[quant_vars])\n    scaled = scaler.transform(train_df[quant_vars])\n\n    for i, col in enumerate(quant_vars):\n        train_df[col] = scaled[:,i]\n\n    scaled = scaler.fit_transform(test_df[quant_vars])\n\n    for i, col in enumerate(quant_vars):\n          test_df[col] = scaled[:,i]\n        \n    # apply some feature engineering\n    train_df = house_remodel_and_age(train_df)\n    test_df = house_remodel_and_age(test_df)\n    \n    train_df = bath_porch_sf(train_df)\n    test_df = bath_porch_sf(test_df)\n    \n    # encode categorical feature\n    train_df_encoded, test_df_encoded = encode_qual(train_df, test_df)\n    \n#     # combine features\n#     train_df_encoded, test_df_encoded = combine_features([train_df_encoded, test_df_encoded])\n    \n    scores_df = pd.DataFrame(columns=['parameters', 'score', 'predictions'])\n    while times > 0:\n        for index, row in parameters.iterrows():\n            mod_train, mod_test = modify_features([train_df_encoded.copy(), test_df_encoded.copy()], row['drop'], row['make_binary'], row['combine'])\n            predictions, score = fit_score_predict(mod_train, mod_test)\n            scores_df = scores_df.append({'parameters': index, 'score': score, 'predictions': predictions}, ignore_index=True)\n        times -= 1\n    return scores_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc48753857dda9feb60819e8063a13076b03b25","scrolled":true,"trusted":false},"cell_type":"code","source":"def save_predictions(predictions):\n    now = str(time.time()).split('.')[0]\n    test_df = pd.read_csv('../input/test.csv')\n    test_df['SalePrice'] = predictions\n    # Transform the log price back to \"real life\" price\n    test_df['SalePrice'] = test_df['SalePrice'].apply(lambda x: np.expm1(x))\n    test_df[['Id', 'SalePrice']].to_csv('submit-'+ now + '.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47ed16afbe7642d81834e29c1297a680366431ee","scrolled":true,"trusted":false},"cell_type":"code","source":"# TODO: `make_binary` is not yet functional\n\nparameters = pd.DataFrame([\n    {\n        'drop': ['Id'], \n        'make_binary': [],\n        'combine': []\n    },\n    {\n        'drop': ['Id', 'BsmtFinSF1', 'BsmtFinSF2', 'LowQualFinSF', '2ndFlrSF'], \n        'make_binary': [],\n        'combine': []\n    },\n    {\n        'drop': ['Id', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'LowQualFinSF', '2ndFlrSF'], \n        'make_binary': [],\n        'combine': []\n    },\n    {\n        'drop': ['Id', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'LowQualFinSF', '2ndFlrSF'], \n        'make_binary': [],\n        'combine': [['GrLivArea', 'TotalBsmtSF']]\n    },\n    {\n        'drop': ['Id', 'BsmtFinSF1', 'BsmtFinSF2', 'LowQualFinSF'], \n        'make_binary': [],\n        'combine': [['GrLivArea', 'TotalBsmtSF']]\n    },\n    {\n        'drop': ['Id'] + low_corr_vars.tolist(),\n        'make_binary': [],\n        'combine': [['GrLivArea', 'TotalBsmtSF']]\n    },\n    {\n        'drop': ['Id'] + low_corr_vars.tolist(),\n        'make_binary': [],\n        'combine': [['GrLivArea', 'TotalBsmtSF'], ['1stFlrSF', '2ndFlrSF']]\n    }\n])\n\nscores = score_parameters(parameters, 20)\nscores.groupby('parameters')['score'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0050a3aee969c3ad8c0eec3cd860e1b3df2dc6cb","scrolled":true,"trusted":false},"cell_type":"code","source":"select_parameters = 5\npredictions = scores.iloc[select_parameters]['predictions']\nsave_predictions(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"df0e875d23b26d42d8f9ecf29d9071edfe3d2de5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}