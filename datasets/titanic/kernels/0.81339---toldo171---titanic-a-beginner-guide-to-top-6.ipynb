{"cells":[{"metadata":{"_uuid":"abe515ae21b52526d96f8108e164d1abf46b0bf2"},"cell_type":"markdown","source":"# **Titanic Data Science Solutions**"},{"metadata":{"_uuid":"00412b66ae0416e7ecf88fb05b3083c3504f3fce"},"cell_type":"markdown","source":"# Table of content\n\n[0.Disclaimer](#0)\n\n[I. Define the problem](#I)\n* [1. Problem description](#I1)\n* [2. Methodology](#I2)\n* [3. Tools importing](#I3)\n\n[II. Gather the data](#II)\n\n[III. Perform Exploratory Analysis and visualize the data](#III)\n* [1. Descriptive analysis of the data](#III1)\n* [2. Pivot analysis](#III2)\n* [3. Analyze by visualizing data](#III3)\n* [4. Analysis Summary](#III4)\n    \n[IV. Wrangle, cleanse and Prepare Data for Consumption](#IV)\n* [0. The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#IV0)\n* [1. Dropping features](#IV1)\n* [2. Sex: converting feature to numerical values](#IV2)\n* [3. Embarked: completing and converting to numerical values](#IV3)\n* [4. Age: completing feature with its mean value](#IV4)\n* [5. Fare: completing feature with its mean value](#IV5)\n* [6. Name: extracting information from this feature, and converting it to numerical values](#IV6)\n* [7. Ticket: extracting information from this feature, and converting it to numerical values](#IV7)\n* [8. Cabin: extracting information from this feature, and converting it to numerical values](#IV8)\n* [9. SibSp Parch: combining features](#IV9)\n\n[V. Model data](#V)\n* [1. Creating and normalizing matrices for our model](#V1)\n* [2. Modeling](#V2)\n* [3. Evaluate Model Performance](#V3)\n* [4. Tune Model with Feature Selection](#V4)\n\n[VI. Optimize and Strategize](#VI)\n\n[VII. Model Submission](#VII)\n\n\n ## **<div id=\"0\">0. Disclaimer</div>**\n\nThis project is my first Data Science Project. I'm seeing it as a sandbox, and as a way to go indepth through the whole data science process. I am getting help from the ressources below :\n* https://www.kaggle.com/startupsci/titanic-data-science-solutions\n* https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n* https://www.kaggle.com/zlatankr/titanic-random-forest-82-78\n\n**Edit**: This kernel is a work in progress. Feel free to contribute or ask questions in the comment section, I will be happy to answer. :-)\n\n## **<div id=\"I\">I. Define the problem</div>**\n\n### **<div id=\"I1\">1. Problem description</div>**\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nHere, we are trying to to complete the analysis of what sorts of people were likely to survive. In particular, we are going to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nThis is a classification problem.\n\n### **<div id=\"I2\">2. Methodology</div>**\n\nThe methodology involved in this machine learning problem go through multiple stages :\n\n* **1. Define the Problem**: If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n* **2. Gather the Data**: John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n* **3. Prepare Data for Consumption**: This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n* **4. Perform Exploratory Analysis**: Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n* **5. Model Data**: Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.\n* **6. Validate and Implement Data Model**: After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our model overfit, generalize, or underfit our dataset.\n* **7. Optimize and Strategize**: This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange\" rate.\n\n### **<div id=\"I3\">3. Tools importing</div>**\n\nHere we are importing every useful tool needed during our research process."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# File handling\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0535ac42f350564b35966f829e5092546bc9de07"},"cell_type":"markdown","source":"## **<div id=\"II\">II. Gather the data</div>**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"training_df = pd.read_csv(\"../input/train.csv\")\ntesting_df = pd.read_csv(\"../input/test.csv\")\ncombine = [training_df, testing_df]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8feb00eaf02b655e000e1093089a7db97d8d18e"},"cell_type":"markdown","source":"## **<div id=\"III\">III. Perform Exploratory Analysis and visualize the data</div>**\n\n### **<div id=\"III1\">1. Descriptive analysis of the data</div>**\n\nHere we want to answer the following questions :\n* Which features are categorical?\n* Which features are numerical?\n* Which features are mixed data types?\n* Which features may contain errors or typos?\n* Which features contain blank, null or empty values?\n* What are the data types for various features?"},{"metadata":{"_uuid":"4376229097a3f657494e8e526844566884dcaef3"},"cell_type":"markdown","source":"We can first display the first rows of the training dataset, in order to have an overview of the parameters :"},{"metadata":{"_uuid":"5f4d698dbc1a5ed84d12127269876e52f77d8f2d","trusted":true},"cell_type":"code","source":"# preview the data\ntraining_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8bdcbab684d797a343642b7c52aee250979fd18"},"cell_type":"markdown","source":"Then we can display the data type of each feature :\n* Seven features are integer or floats. Six in case of test dataset.\n* Five features are strings (object)."},{"metadata":{"_uuid":"bbb164f9b2510486a99c87e05c5f49b99159dc74","trusted":true,"scrolled":true},"cell_type":"code","source":"training_df.info()\nprint('_'*40)\ntesting_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1385a59934b229ae529cec763731119f77977082"},"cell_type":"markdown","source":"Then, we can display a quick descriptive representation of the dataset :\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers within age range 65-80."},{"metadata":{"_uuid":"780baa612315d8fdd28eca839dd84b9d4150ee12","scrolled":true,"trusted":true},"cell_type":"code","source":"training_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18bc61bc447c8f399473423015134e9da243de90"},"cell_type":"markdown","source":"\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681)."},{"metadata":{"_uuid":"833e2fbadc298d748b3376c5110d011042392a82","trusted":true,"scrolled":true},"cell_type":"code","source":"training_df.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b696b2f39bbb977572821edcd821cd6ae8245916"},"cell_type":"markdown","source":"### **First assumptions**\n\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating**\n* We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing**\n* We may want to complete Age feature as it is definitely correlated to survival.\n* We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n**Correcting**\n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating**\n* We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n* We may want to engineer the Name feature to extract Title as a new feature.\n* We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n* We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying**\n* Women (Sex=female) were more likely to have survived.\n* Children (Age<?) were more likely to have survived.\n* The upper-class passengers (Pclass=1) were more likely to have survived.\n"},{"metadata":{"_uuid":"b0c17fec1ba211c42059668273aa8660f999b2a0"},"cell_type":"markdown","source":"### **<div id=\"III2\">2. Pivot analysis</div>**\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n* **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived. We decide to include this feature in our model.\n* **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74%.\n* **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features.\n"},{"metadata":{"_uuid":"551f348e023faf68d882cfb3e877034c32603573","trusted":true},"cell_type":"code","source":"training_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0285bd0e6731e82b029096bbfa129d6d9e0f76","trusted":true},"cell_type":"code","source":"training_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"575063ae414443ec5bc6b045fee19551a6ba171d","trusted":true},"cell_type":"code","source":"training_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8766c80d8697f2f479d45ced6d93351bb5035a7d","trusted":true},"cell_type":"code","source":"training_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2960aded1e74fbde54608b5edf1644c8f09799"},"cell_type":"markdown","source":"### **<div id=\"III3\">3. Analyze by visualizing data</div>**\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n**Observations**\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.\n\n**Decisions**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n* We should consider Age in our model training.\n* Complete the Age feature for null values.\n* We should band age groups.\n"},{"metadata":{"_uuid":"d2c73941c05b36dfcdfeb98522c176f36c5ecae5","trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(training_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"527c17bc8d6e2ef9a59ff0a4bd52eb6e0c953cd1"},"cell_type":"markdown","source":"We can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations**\n* Pclass=3 had most passengers, however most did not survive.\n* Infant passengers in Pclass=2 and Pclass=3 mostly survived.\n* Most passengers in Pclass=1 survived.\n* Pclass varies in terms of Age distribution of passengers.\n\n**Decisions**\n* Consider Pclass for model training.\n"},{"metadata":{"_uuid":"73882e5ae27a1ce15faf21b027c8c68f7c6bc6df","trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(training_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e56b237f8c57b6576c73695be463bfe1e0057e6f"},"cell_type":"markdown","source":"Now we can correlate categorical features with our solution goal.\n\n**Observations**\n* Female passengers had much better survival rate than males.\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports.\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers.\n\n**Decisions**\n* Add Sex feature to model training.\n* Complete and add Embarked feature to model training.\n"},{"metadata":{"_uuid":"ccda5fbf70535659495e329573fe4b344f4f2f3b","trusted":true,"scrolled":true},"cell_type":"code","source":"grid = sns.FacetGrid(training_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7819d7c79c4e6b4b68f0f5f03e838946eadba603"},"cell_type":"markdown","source":"We may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations**\n* Higher fare paying passengers had better survival. Confirms our assumption for creating fare ranges.\n* Port of embarkation correlates with survival rates.\n\n**Decisions**\n* Consider banding Fare feature.\n"},{"metadata":{"_uuid":"d3a20895f932643f2e6251e6d1bad2bf3f75d044","trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(training_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f98638f3736f104ea15ac7c80b2329b3708dcc3"},"cell_type":"markdown","source":"### **<div id=\"III4\">4. Analysis Summary</div>**\n\nAfter this first analysis work, we can summarize the first decisions we took:\n* In our model training, we should consider:\n  * Pclass\n  * Sex\n  * Age\n  * SibSp\n  * Parch\n  * Fare\n  * Embarked\n  * Name\n* In our model training, we shall not consider:\n    * PassengerId\n    * Ticket\n    * Cabin\n* In our model training, we may:\n    * Complete the Age feature for null values.\n    * Complete Embarked feature to model training.\n    * Band age groups. \n    * Band Fare feature.\n    * Create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n    * Engineer the Name feature to extract Title as a new feature.\n* Edit : For my second iteration, I'm trying to get additional information from columns like \"Cabin\" and \"Ticket, and to perform more feature engineering on the other columns, for example getting the length of \"Names\"."},{"metadata":{"_uuid":"2d59fe926a779695b7161179dc97e2f07bb223b2"},"cell_type":"markdown","source":"## **<div id=\"IV\">IV. Wrangle, cleanse and Prepare Data for Consumption</div>**\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n### **<div id=\"IV0\">0. The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting</div>**\n\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n* **Correcting**: Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n* **Completing**: There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model’s accuracy.\n* **Creating**: Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n* **Converting**: Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.\n\n### **<div id=\"IV1\">1. Dropping features</div>**\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin, PassengerId and Ticket features.\n\n**Edit**: We are not dropping *Cabin* and *Ticket* anymore, since we want ot get information about these features."},{"metadata":{"_uuid":"a8a1491c13c372b3efd2b590a90a44a220aace04","trusted":true},"cell_type":"code","source":"column_choice_training = training_df[['Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Name','Survived']]\ncolumn_choice_test = testing_df[['Pclass','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Name']]\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad8d2ac007f7758048e9f4e0fa49e3d8bd10e3ef"},"cell_type":"markdown","source":"### **<div id=\"IV2\">2. Sex: converting feature to numerical values</div>**\n\nNow let's convert the sex feature, which contain strings, to numerical values."},{"metadata":{"trusted":true,"_uuid":"8c7055747208f8e988ba8b5adaf4ca66f73483a4","scrolled":true},"cell_type":"code","source":"column_choice_training.Sex[column_choice_training.Sex == 'female'] = 0\ncolumn_choice_training.Sex[column_choice_training.Sex == 'male'] = 1\ncolumn_choice_test.Sex[column_choice_test.Sex == 'female'] = 0\ncolumn_choice_test.Sex[column_choice_test.Sex == 'male'] = 1\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bddcd4f07e485e383893d9f0f1f935f2cb559254"},"cell_type":"markdown","source":"### **<div id=\"IV3\">3. Embarked: completing and converting to numerical values</div>**\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.\nThen we convert the feature, which contain strings, to numerical values."},{"metadata":{"trusted":true,"_uuid":"1ffc6f6f1e712294e9dc6019e8a9e2622164d9f8","scrolled":true},"cell_type":"code","source":"#Transformations of test set data must always be fit using training data\nfreq_port = column_choice_training.Embarked.dropna().mode()[0]\ncolumn_choice_training.Embarked = column_choice_training.Embarked.fillna(freq_port)\ncolumn_choice_test.Embarked = column_choice_test.Embarked.fillna(freq_port)\ncolumn_choice_training.Embarked[column_choice_training.Embarked == 'S'] = 0\ncolumn_choice_training.Embarked[column_choice_training.Embarked == 'C'] = 1\ncolumn_choice_training.Embarked[column_choice_training.Embarked == 'Q'] = 2\ncolumn_choice_test.Embarked[column_choice_test.Embarked == 'S'] = 0\ncolumn_choice_test.Embarked[column_choice_test.Embarked == 'C'] = 1\ncolumn_choice_test.Embarked[column_choice_test.Embarked == 'Q'] = 2\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc14132f86e0218cfcf547659f0c85782322c532"},"cell_type":"markdown","source":"### **<div id=\"IV4\">4. Age: completing feature with its mean value</div>**\n\nA lot of things come into mind for the Age feature:\n* This feature has a lot of missing values. We can first fill these missing values with the mean of the feature, but we can also try to find out a clever way,\n* We can store the fact that some Age values are missing, in a new column,\n* We can see what happened to babies and old people.\n\nFirst, let's bin the feature to see what happened to young and old people. We can observe that babies (age < 6) had a higher survival rate, and that older people (age > 60) had a lower survival rate."},{"metadata":{"trusted":true,"_uuid":"336114c9c0a713d61f784272331a526e51b20594"},"cell_type":"code","source":"bins = [0, 6, 60, 80]\ncolumn_choice_training['Age_cut'] = pd.cut(column_choice_training['Age'], bins)\ncolumn_choice_test['Age_cut'] = pd.cut(column_choice_test['Age'], bins)\n\nprint(column_choice_training['Age_cut'].value_counts())\ncolumn_choice_training[['Age_cut', 'Survived']].groupby(['Age_cut'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfcfb51488774d33f602494fc27cef40d0cd4e37"},"cell_type":"markdown","source":"Next, let's store in a new binary column the rows where the Age value is missing. "},{"metadata":{"trusted":true,"_uuid":"d46d66a274760fe8eaa97d12a31d89caaebd73c5"},"cell_type":"code","source":"column_choice_training['Age_is_Null'] = column_choice_training['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\ncolumn_choice_test['Age_is_Null'] = column_choice_test['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bba40aa8fc3e0cbf92543241a097ad16801d012"},"cell_type":"code","source":"column_choice_training[['Age', 'Pclass', 'Sex']].groupby(['Pclass', 'Sex'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e16737e56375e3a83d42238f08a72dfffa74dce"},"cell_type":"markdown","source":"We can observe that the age mean differs a lot according to the sex and the Pclass. A clever way of filling the missing values is by taking the mean age, according to the sex and the Pclass."},{"metadata":{"trusted":true,"_uuid":"cfbc6dbf2baf9ad61702ba2600f55ea9d6e7bd16"},"cell_type":"code","source":"age_means = column_choice_training.groupby(['Sex', 'Pclass'])['Age']\ncolumn_choice_training.Age = age_means.transform(lambda x: x.fillna(x.mean()))\n#Transformations of test set data must always be fit using training data\ncolumn_choice_test.Age = age_means.transform(lambda x: x.fillna(x.mean()))\ncolumn_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b4da2e4c06ff487a8f6c879c0245a6e1ea1f38e"},"cell_type":"markdown","source":"Finally, let's store in 2 new columns the information about babies (age < 6) and older people (age > 60);"},{"metadata":{"trusted":true,"_uuid":"c7a1c8a18ec45c8de9850e2bdcbd75db00d33c23","scrolled":true},"cell_type":"code","source":"bins = [0, 6, 60, 80]\ncolumn_choice_training['Age_cut'] = pd.cut(column_choice_training['Age'], bins)\ncolumn_choice_test['Age_cut'] = pd.cut(column_choice_test['Age'], bins)\n\ncolumn_choice_training = pd.concat([column_choice_training.drop(['Age_cut'], axis=1),\n                                    pd.get_dummies(column_choice_training['Age_cut'], prefix = 'Age_cut')], axis=1)\ncolumn_choice_training = column_choice_training.drop(['Age_cut_(6, 60]'], axis=1)\ncolumn_choice_test = pd.concat([column_choice_test.drop(['Age_cut'], axis=1), \n                                pd.get_dummies(column_choice_test['Age_cut'], prefix = 'Age_cut')], axis=1)\ncolumn_choice_test = column_choice_test.drop(['Age_cut_(6, 60]'], axis=1)\ncolumn_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d03a31703bc99fbd4a33f70474761b1d56519d8"},"cell_type":"markdown","source":"### **<div id=\"IV5\">5. Fare: completing feature with its mean value</div>**\n\nThe Fare feature has one missing value .Here, we choose to fill this missing values with the mean of the feature.\nWe can later try finding a better way to fill these missing values, but for now, we can use this method, which gives usually good results."},{"metadata":{"trusted":true,"_uuid":"30a9742d27976cb54ef42296e6c151887abc4683"},"cell_type":"code","source":"column_choice_training.Fare = column_choice_training.Fare.fillna(column_choice_training.Fare.mean())\n#Transformations of test set data must always be fit using training data\ncolumn_choice_test.Fare = column_choice_test.Fare.fillna(column_choice_training.Fare.mean())\ncolumn_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68f40141083cd55a42a925ecf73964ebc1acf3e2"},"cell_type":"markdown","source":"### **<div id=\"IV6\">6. Name: extracting information from this feature, and converting it to numerical values</div>**\n\nWe want to analyze if the Name feature can be engineered to extract titles (Mr, Mrs, Miss...). Then we want to extract this information into a new column called \"Title\", and convert each Title to a numerical value.\n\nFirst of all, let's split the \"Name\" feature in order to extract the Titles, and create a new column \"Title\", filled with them."},{"metadata":{"trusted":true,"_uuid":"23fd073a9243704b54e9eeb61edde00ab6b6aacc"},"cell_type":"code","source":"column_choice_training['Name_Length'] = column_choice_training['Name'].apply(len)\ncolumn_choice_test['Name_Length'] = column_choice_test['Name'].apply(len)\ncolumn_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51a4e6c529ef729fb1d2efd681e93cd15df52e09"},"cell_type":"markdown","source":"Then, let's split the \"Name\" feature in order to extract the Titles, and create a new column \"Title\", filled with them."},{"metadata":{"trusted":true,"_uuid":"58e6b25357f1dd37c0f51315c9eac6b5a6502443"},"cell_type":"code","source":"column_choice_training['Title'] = column_choice_training['Name'].str.split(', ').str[1]\ncolumn_choice_training['Title'] = column_choice_training['Title'].str.split('.').str[0]\ncolumn_choice_training = column_choice_training.drop(['Name'], axis=1)\ncolumn_choice_test['Title'] = column_choice_test['Name'].str.split(', ').str[1]\ncolumn_choice_test['Title'] = column_choice_test['Title'].str.split('.').str[0]\ncolumn_choice_test = column_choice_test.drop(['Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"839563af6d4bc64005642adaade31d5f7a7ec608"},"cell_type":"code","source":"column_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dcec840a5627b5b0fdacae4515f2a80b4dc2201"},"cell_type":"markdown","source":"Then, we want to visualize every possible \"Title\" value, and correlate them with the Sex column. "},{"metadata":{"trusted":true,"_uuid":"c583874e8169d513488775f245c0c06a5458f309"},"cell_type":"code","source":"pd.crosstab(column_choice_training['Title'], column_choice_training['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fadeaea173f7ab16036717eed58d79ae3d279ec"},"cell_type":"code","source":"pd.crosstab(column_choice_test['Title'], column_choice_test['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94d577f05bf255e23f4a20494022e9308e6745b9"},"cell_type":"markdown","source":"Here, we can see that some titles are very rare, and some of them are very common. Besides, sometimes many titles means the same thing (for example, Mrs = Ms = Mlle = Lady).\nWe choose to regroup each rare title into one unique category, and to normalize titles."},{"metadata":{"trusted":true,"_uuid":"e03d06ba3b46bef4bca8a9d18f243a6a7efc026f","scrolled":true},"cell_type":"code","source":"column_choice_training['Title'] = column_choice_training['Title'].replace(['the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer'], 'Rare')\ncolumn_choice_training['Title'] = column_choice_training['Title'].replace(['Lady','Mlle', 'Ms'], 'Mrs')\ncolumn_choice_training['Title'] = column_choice_training['Title'].replace(['Mme'], 'Miss')\ncolumn_choice_test['Title'] = column_choice_test['Title'].replace(['Col', 'Dona', 'Dr', 'Rev'], 'Rare')\ncolumn_choice_test['Title'] = column_choice_test['Title'].replace(['Ms'], 'Mrs')\npd.crosstab(column_choice_training['Title'], column_choice_training['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cde2f96f22f6f9f888ad8d5344af9adfc741645a"},"cell_type":"code","source":"pd.crosstab(column_choice_test['Title'], column_choice_test['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00a60ad960bfb5a7e845ee4a0c425159f83c6473"},"cell_type":"markdown","source":"Finally, let's convert the title feature, which contain strings, to numerical values."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"96173c9d80b9f05d8847209e32f232e42673db81"},"cell_type":"code","source":"column_choice_training.Title[column_choice_training.Title == 'Master'] = 0\ncolumn_choice_training.Title[column_choice_training.Title == 'Miss'] = 1\ncolumn_choice_training.Title[column_choice_training.Title == 'Mr'] = 2\ncolumn_choice_training.Title[column_choice_training.Title == 'Mrs'] = 3\ncolumn_choice_training.Title[column_choice_training.Title == 'Rare'] = 4\ncolumn_choice_test.Title[column_choice_test.Title == 'Master'] = 0\ncolumn_choice_test.Title[column_choice_test.Title == 'Miss'] = 1\ncolumn_choice_test.Title[column_choice_test.Title == 'Mr'] = 2\ncolumn_choice_test.Title[column_choice_test.Title == 'Mrs'] = 3\ncolumn_choice_test.Title[column_choice_test.Title == 'Rare'] = 4\ncolumn_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92200f40bf1fd5e17e1cfc52225be5995b3334be"},"cell_type":"markdown","source":"### **<div id=\"IV7\">7. Ticket: extracting information from this feature, and converting it to numerical values</div>**\n\nOne piece of potentially useful informatin is the number of characters in the Ticket column. This could be a reflection of the 'type' of ticket a given passenger had, which could somehow indicate their chances of survival. One theory is that some characteristic of the ticket could indicate the location of the passenger's room, which might be a crucial factor in their escape route, and consequently their survival."},{"metadata":{"trusted":true,"_uuid":"31a53e4a99278b8ce376ad6fe2df7540ec3c812d"},"cell_type":"code","source":"column_choice_training['Ticket_Len'] = column_choice_training['Ticket'].apply(len)\ncolumn_choice_test['Ticket_Len'] = column_choice_test['Ticket'].apply(len)\nprint(column_choice_test['Ticket_Len'].value_counts())\ncolumn_choice_training[['Ticket_Len', 'Survived']].groupby(['Ticket_Len'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ccb6bed360d98e8a16f9e247e0d87601e1cf3ca"},"cell_type":"markdown","source":"Another piece of information is the first letter of each ticket, which, again, might be indicative of a certain attribute of the ticketholders or their rooms."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"868d0025468eef35a188381c257157e3c9b811f9"},"cell_type":"code","source":"#Getting the first letter of the column\ncolumn_choice_training['Ticket_Letter'] = column_choice_training['Ticket'].str[0]\ncolumn_choice_test['Ticket_Letter'] = column_choice_test['Ticket'].str[0]\n#Displaying values counts and survival rates\nprint(column_choice_training['Ticket_Letter'].value_counts())\nprint(column_choice_test['Ticket_Letter'].value_counts())\ncolumn_choice_training[['Ticket_Letter', 'Survived']].groupby(['Ticket_Letter'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fdeeff6ba61d16eea07499ed9dd137bab4ac6a7"},"cell_type":"markdown","source":"Then we can group the letters with small value counts, based on their survival rate"},{"metadata":{"trusted":true,"_uuid":"ade4f6378752ec2ab200afa125df8ea5f381442d"},"cell_type":"code","source":"column_choice_training['Ticket_Letter'] = column_choice_training['Ticket_Letter'].replace(['W', '4', '7', '6', 'L', '5', '8'], 'Rare_Low_Surv')\ncolumn_choice_training['Ticket_Letter'] = column_choice_training['Ticket_Letter'].replace(['F', '9'], 'Rare_High_Surv')\ncolumn_choice_test['Ticket_Letter'] = column_choice_test['Ticket_Letter'].replace(['W', '4', '7', '6', 'L', '5', '8'], 'Rare_Low_Surv')\ncolumn_choice_test['Ticket_Letter'] = column_choice_test['Ticket_Letter'].replace(['F', '9'], 'Rare_High_Surv')\ncolumn_choice_training.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e9ed523fdcfdcbe20786b4809989d369389ed70"},"cell_type":"markdown","source":"Finally, we are using the \"get_dummies\" function to convert the *Ticket_Letter* column into dummy columns that can be used by our future models.\n\nWe do not longer need the Ticket and Ticket_Letter columns, we can drop them."},{"metadata":{"trusted":true,"_uuid":"64e030172107bf961120540204ec649233e49969"},"cell_type":"code","source":"column_choice_training = pd.concat([column_choice_training.drop(['Ticket', 'Ticket_Letter'], axis=1), \n                                    pd.get_dummies(column_choice_training['Ticket_Letter'], prefix = 'Ticket_Letter')], axis=1)\ncolumn_choice_test = pd.concat([column_choice_test.drop(['Ticket', 'Ticket_Letter'], axis=1), \n                                    pd.get_dummies(column_choice_test['Ticket_Letter'], prefix = 'Ticket_Letter')], axis=1)\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1620d578e9a09b63c26eba8cd44439837b942a41"},"cell_type":"markdown","source":"### **<div id=\"IV8\">8. Cabin: extracting information from this feature, and converting it to numerical values</div>**\n\nOne piece of potentially useful informatin is the first letter of each ticket, which, again, might be indicative of a certain attribute of the ticketholders or their rooms. We need to : \n* Fill in missing values\n* Display values counts and survival rates according to the first letter of the cabin\n* Use the \"get_dummies\" function to convert the *Cabin_Letter* column into dummy columns that can be used by our future models."},{"metadata":{"trusted":true,"_uuid":"4668cbb364dbd9da58a0d46a7ebc2a39a6b0d594","scrolled":false},"cell_type":"code","source":"#Filling missing values\ncolumn_choice_training.Cabin = column_choice_training.Cabin.fillna('N0')\ncolumn_choice_test.Cabin = column_choice_test.Cabin.fillna('N0')\n#Getting the first letter of the column\ncolumn_choice_training['Cabin_Letter'] = column_choice_training['Cabin'].str[0]\ncolumn_choice_test['Cabin_Letter'] = column_choice_test['Cabin'].str[0]\n#Displaying values counts and survival rates\nprint(column_choice_training['Cabin_Letter'].value_counts())\nprint(column_choice_test['Cabin_Letter'].value_counts())\ncolumn_choice_training[['Cabin_Letter', 'Survived']].groupby(['Cabin_Letter'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbc326aa9e66ae36a599f8599929cfa479084da4"},"cell_type":"markdown","source":"Another piece of information is the ticket number. We can band them and use the \"get_dummies\" function to convert the *Cabin_Number* column into dummy columns"},{"metadata":{"trusted":true,"_uuid":"964c406427a568e6aa637f48f0ef8adb5a606ad0"},"cell_type":"code","source":"#Getting cabin number\ncolumn_choice_training['Cabin_Number'] = column_choice_training['Cabin'].str.split(' ').str[-1].str[1:]\ncolumn_choice_test['Cabin_Number'] = column_choice_test['Cabin'].str.split(' ').str[-1].str[1:]\n\n#Replacing 0 values by NaN\ncolumn_choice_training['Cabin_Number'] = column_choice_training['Cabin_Number'].replace(['0'], np.NaN)\ncolumn_choice_test['Cabin_Number'] = column_choice_test['Cabin_Number'].replace(['0'], np.NaN)\n\n#Converting numbers to int values\ncolumn_choice_training['Cabin_Number'] = column_choice_training['Cabin_Number'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\ncolumn_choice_test['Cabin_Number'] = column_choice_test['Cabin_Number'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\n\n#Banding the feature\ncolumn_choice_training['Cabin_Number_cut'] = pd.qcut(column_choice_training['Cabin_Number'],4)\ncolumn_choice_test['Cabin_Number_cut'] = pd.qcut(column_choice_test['Cabin_Number'],4)\n\n#Getting dummies\ncolumn_choice_training = pd.concat([column_choice_training.drop(['Cabin_Number', 'Cabin_Number_cut'], axis=1), \n                                    pd.get_dummies(column_choice_training['Cabin_Number_cut'], prefix = 'Cabin_num')], axis=1)\ncolumn_choice_test = pd.concat([column_choice_test.drop(['Cabin_Number', 'Cabin_Number_cut'], axis=1), \n                                    pd.get_dummies(column_choice_test['Cabin_Number_cut'], prefix = 'Cabin_num')], axis=1)\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ad597a8f34d08fca6a2fad301a7d4938bb2b359"},"cell_type":"code","source":"#There is no \"T\" in the test set. We replace its value to a \"N\", since the guy has not survived and the survival rate is low for people without cabin information\ncolumn_choice_training['Cabin_Letter'] = column_choice_training['Cabin_Letter'].replace(['T'], 'N')\n\ncolumn_choice_training = pd.concat([column_choice_training.drop(['Cabin', 'Cabin_Letter'], axis=1), \n                                    pd.get_dummies(column_choice_training['Cabin_Letter'], prefix = 'Cabin_Letter')], axis=1)\ncolumn_choice_test = pd.concat([column_choice_test.drop(['Cabin', 'Cabin_Letter'], axis=1), \n                                    pd.get_dummies(column_choice_test['Cabin_Letter'], prefix = 'Cabin_Letter')], axis=1)\ncolumn_choice_training.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12ba3fb0d8c4aa87c59238cfce790bbdfefd5f5d"},"cell_type":"markdown","source":"### **<div id=\"IV9\">9. SibSp Parch: combining features</div>**\n\n\nWhen we have two seemingly weak predictors, one thing we can do is combine them to get a stronger predictor. In the case of SibSp and Parch, we can combine the two variables to get a 'family size' metric, which might prove to be a better predictor than the two original variables.\n"},{"metadata":{"trusted":true,"_uuid":"96d75fd21451ee33932c2c4a748457d6bf023457"},"cell_type":"code","source":"#Creating the Family column\ncolumn_choice_training['Family'] = column_choice_training['SibSp'] + column_choice_training['Parch']\ncolumn_choice_test['Family'] = column_choice_test['SibSp'] + column_choice_test['Parch']\n\ncolumn_choice_training['Family'] = column_choice_training['Family'].replace([0], 'Alone')\ncolumn_choice_training['Family'] = column_choice_training['Family'].replace([1, 2, 3], 'Little_Family')\ncolumn_choice_training['Family'] = column_choice_training['Family'].replace([4, 5, 6, 7, 10], 'Big_Family')\ncolumn_choice_test['Family'] = column_choice_test['Family'].replace([0], 'Alone')\ncolumn_choice_test['Family'] = column_choice_test['Family'].replace([1, 2, 3], 'Little_Family')\ncolumn_choice_test['Family'] = column_choice_test['Family'].replace([4, 5, 6, 7, 10], 'Big_Family')\n\nprint(column_choice_training['Family'].value_counts())\nprint(column_choice_test['Family'].value_counts())\ncolumn_choice_training[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\ncolumn_choice_training = pd.concat([column_choice_training.drop(['SibSp', 'Parch', 'Family'], axis=1), \n                                    pd.get_dummies(column_choice_training['Family'], prefix = 'Family')], axis=1)\ncolumn_choice_test = pd.concat([column_choice_test.drop(['SibSp', 'Parch', 'Family'], axis=1), \n                                    pd.get_dummies(column_choice_test['Family'], prefix = 'Family')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"777e6ae46cb9fd6dc0a8f2b232b25c5c17afbc92"},"cell_type":"code","source":"column_choice_training.head(6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e6f6de0bf7fa5af0cea70f84708dbe19ee8e45"},"cell_type":"markdown","source":"## **<div id=\"V\">V. Model data</div>**\n\nNow that we have acquired, analyzed and prepared the data, we are ready to train a model and predict the required solution. \nThere are many predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Here we are performing supervised learning, and our problem is a classification and regression problem. With these two criteria, we can narrow down our choice of models to a few. These include:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine"},{"metadata":{"_uuid":"f66d566cf4ee7a179cdcd40e3bddc39d39a42024"},"cell_type":"markdown","source":"### **<div id=\"V1\">1. Creating and normalizing matrices for our model</div>**\n\nFirst, we need to prepare and normalize our train and test matrices, which we are then going to use for our models.\n\n**Normalizing the data** has two purposes : \n* Making training less sensitive to the scale of features. If we don't normalize the data when we face features with different scales (for example, age and house price), our ML algorithms might take too much care to features with large scales.\n* Accelerating optimization. Most machine learning optimizations are solved using gradient descent, or a variant thereof. And the speed of convergence depends on the scaling of features. Normalization makes the problem better conditioned, improving the convergence rate of gradient descent."},{"metadata":{"_uuid":"13faf014760fbe9b4ba722a6bf85c31f133c7f13","trusted":true,"scrolled":true},"cell_type":"code","source":"X_train = np.asarray(column_choice_training.drop(['Survived'], axis=1))\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)\nX_train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba286a80481067d2b1d4c151aa4d79785934ece7","scrolled":true},"cell_type":"code","source":"X_test = np.asarray(column_choice_test)\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\nX_test[0:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb359423fa62fe414eade203383c443cea4452c4","trusted":true},"cell_type":"code","source":"y_train = np.asarray(column_choice_training['Survived'])\ny_train[0:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d901dc093b6b08cc098c4b8cbe6ce8ccd78c4140","trusted":true,"scrolled":true},"cell_type":"code","source":"print ('Train set:', X_train.shape,y_train.shape)\nprint ('Test set:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b34372fdc207f558dbefedbb93511e9ef11ec5ae"},"cell_type":"markdown","source":"### **<div id=\"V2\">2. Modeling</div>**\n\nLogistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n\n#### **a. Cross validation**\n\nFor a prediction problem, a model is generally provided with a data set of known data, called the training data set, and a set of unknown data against which the model is tested, known as the test data set. The target is to have a data set for testing the model in the training phase and then provide insight on how the specific model adapts to an independent data set. A round of cross-validation comprises the partitioning of data into complementary subsets, then performing analysis on one subset. After this, the analysis is validated on other subsets (testing sets). To reduce variability, many rounds of cross-validation are performed using many different partitions and then an average of the results are taken. **Cross-validation is a powerful technique in the estimation of model performance technique.**\n\nHere, we are using the ShuffleSplit function from Scikit Learn. ShuffleSplit will randomly sample your entire dataset during each iteration to generate a training set and a test set. The test_size and train_size parameters control how large the test and training test set should be for each iteration. Since you are sampling from the entire dataset during each iteration, values selected during one iteration, could be selected again during another iteration.  \n\nThe main difference between ShuffleSplit and K-Fold is that In KFold, during each round you will use one fold as the test set and all the remaining folds as your training set. However, in ShuffleSplit, during each round n you should only use the training and test set from iteration n. **As your data set grows, cross validation time increases, making shufflesplits a more attractive alternate**. If you can train your algorithm, with a certain percentage of your data as opposed to using all k-1 folds, ShuffleSplit is an attractive option."},{"metadata":{"trusted":true,"_uuid":"cd3448938c70e87d9a0bb421e69e4420ac0c7909","scrolled":true},"cell_type":"code","source":"models = [\n    LogisticRegression(solver='liblinear'), \n    RandomForestClassifier(n_estimators=100, oob_score = True, random_state = 1)\n    ]\n\nmodel_results = pd.DataFrame(data = {'test_score_mean': [], 'fit_time_mean': []})\n\n# Spliting the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n# Performing shufflesplit cross validation, with the whole training set (the cross_validate function coupled with ShuffleSplit take care of spliting the training set) \nfor model in models:\n    cross_validation_results = model_selection.cross_validate(model, X_train, y_train, cv= cross_validation_split, return_train_score=True)\n    # Checking the mean of test scores for each iteration of the validation\n    model_results = model_results.append({'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                      'fit_time_mean' : cross_validation_results['fit_time'].mean()}, ignore_index=True) \n \nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e268f4f013046cdcef91c9084f4a6b048cdce21"},"cell_type":"markdown","source":"#### **b. Tune Model with Hyper-Parameters**\n\nIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.\n\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. Model hyperparameters are set manually and are used in processes to help estimate model parameters.\n**Tuning an hyperparameter means trying to get the closest possible to its best value, in order to maximize the accuracy of the model** (for larger dataset, time computing may also be taken into account for parameter tuning).\n\nFirst, we are using **RandomizedSearchCV**. We need to create a parameter grid to sample from during fitting. On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2x6x6x8x8 = 4608 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values."},{"metadata":{"trusted":true,"_uuid":"388d48423f1fc697bb94faef43c7a6544917b89e","scrolled":true},"cell_type":"code","source":"RFC = RandomForestClassifier(oob_score = True, random_state = 1)\nparam_grid = {'min_samples_leaf' : [1, 2, 4, 6, 8, 10], \n              'min_samples_split' : [2, 4, 6, 8, 10, 12, 14, 16], \n              'n_estimators': [200, 500, 800, 1100]}\n#'criterion' : [\"gini\", \"entropy\"],\n#'max_depth': [10, 20, 30, 40, 50, None],\n\n#RS = RandomizedSearchCV(estimator = RFC, \n                        param_distributions = param_grid, \n                        n_iter = 100, \n                        cv = cross_validation_split, verbose = 2, random_state = 0, n_jobs = -1)\n\n#RS = RS.fit(X_train, y_train)\n\nprint(RS.best_score_)\nprint(RS.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5d8e01e32457e357e309e1e9cfc82df56e53f0f"},"cell_type":"markdown","source":"Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with **GridSearchCV**, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"},{"metadata":{"trusted":true,"_uuid":"24d8852aa37619c3cfab6c5116b1c207d3cb6aba","scrolled":true},"cell_type":"code","source":"param_grid = { 'criterion' : ['gini', 'entropy'],\n              #'max_depth': [45, 50, None],\n              'min_samples_leaf' : [1, 2, 3], \n              'min_samples_split' : [3, 4, 5], \n              'n_estimators': [1000, 1100, 1200]}\n\n#GS = GridSearchCV(estimator = RFC, param_grid = param_grid, cv = cross_validation_split, verbose = 1, n_jobs = -1)\n#GS = GS.fit(X_train, y_train)\n\nprint(GS.best_score_)\nprint(GS.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09bf86f1ab27f5e07c47fcdc8d8ef7433f68dd1c"},"cell_type":"markdown","source":"#### **c. Visualizing the result of the model with a confusion matrix**"},{"metadata":{"trusted":true,"_uuid":"e231e513049ca82542236a78cb504c12948a4e06"},"cell_type":"code","source":"best_model = RandomForestClassifier(n_estimators=1000, \n                                    oob_score = True, \n                                    criterion = 'gini',\n                                    min_samples_leaf = 3,\n                                    min_samples_split = 8,\n                                    max_depth = None,\n                                    random_state = 1).fit(X_train,y_train)\n#best_model = LogisticRegression(solver='liblinear').fit(X_train,y_train)\nyhat = best_model.predict(X_train)\nprint(\"%.4f\" % best_model.oob_score_)\nimportance_df = pd.concat((pd.DataFrame(column_choice_training.drop(['Survived'], axis=1).columns, columns = ['variable']), \n           pd.DataFrame(best_model.feature_importances_, columns = ['importance'])), \n           axis = 1).sort_values(by='importance', ascending = False)\nimportance_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fe44b35042051852f888be6cfa3055c2b2d8bb5"},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='variable', data=importance_df.sort_values(by=\"importance\", ascending=False))\nplt.title('Feature Importance')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd6c6095f0b0e56bbf9c38e1d5e102ac435e321","trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_train, yhat, labels=[1,0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f038baf7b0ac8c9fd80247319c964ce653254d8","trusted":true},"cell_type":"code","source":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838e56c605778c66729b710cf850436f77fa23c9"},"cell_type":"markdown","source":"### **<div id=\"V3\">3. Evaluate Model Performance</div>**\n\nLet's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~80% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind when improving your model.\n\n#### **Determine a Baseline Accuracy**\n\nBefore we decide how-to make our model better, let's determine if our model is even worth keeping. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip problem. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing correct. So, let's set 50% as the worst model performance; because anything lower than that, then why do I need you when I can just flip a coin?\n\nOkay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, if we just predict the most frequent occurrence, that 100% of people died, then we would be right 67.5% of the time. So, let's set 68% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.\n\n#### ** How-to Create Your Own Model**\n\nOur accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that segment your target response, placing the survived/1 and dead/0 into homogeneous subgroups.\n\nRemember, the name of the game is to create subgroups using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is less than 10 and/or our model accuracy plateaus or decreases.\n\n**Question 1**: Were you on the Titanic? If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.\n\n**Question 2**: Are you male or female? Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.\n\n**Question 3A (going down the female branch with count = 314)**: Are you in class 1, 2, or 3? Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.\n\n**Question 4A (going down the female class 3 branch with count = 144)**: Did you embark from port C, Q, or S? We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%.\n\n**Question 5A (going down the female class 3 embarked S branch with count = 88)**: So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.\n\n**Question 3B (going down the male branch with count = 577)**: Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.\n\nWith very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model? This leads us to the next section."},{"metadata":{"_uuid":"6ba98434b82063c50f0eab2c391082c6b95a6895"},"cell_type":"markdown","source":"### **<div id=\"V4\">4. Tune Model with Feature Selection</div>**\n\nAs stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection.\n\nFeature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:\n\n* simplification of models to make them easier to interpret by researchers/users,\n* shorter training times,\n* to avoid the curse of dimensionality,\n* enhanced generalization by reducing overfitting (formally, reduction of variance)\n\n**The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information**. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated."},{"metadata":{"_uuid":"71fde9b6e6888bdaa1e112ff91c5418344c5a9d9"},"cell_type":"markdown","source":"## **<div id=\"VI\">VI. Optimize and Strategize</div>**\n\nBasically, get back to step I and perform a new iteration. Rinse and Repeat. \nFor my second iteration, I'm trying to get additional information from columns like \"Cabin\" and \"Ticket, and to perform more feature engineering on the other columns, for example getting the length of \"Names\"."},{"metadata":{"_uuid":"b5d4a2ab21eaa308174bac94aabcc67d65b0b3d9"},"cell_type":"markdown","source":"## **<div id=\"VII\">VII. Model Submission</div>**"},{"metadata":{"_uuid":"c4e0f624f05dfe41c7b21a474f7abd441ad598bb","trusted":true},"cell_type":"code","source":"# Predicting the results of the tessting set with the model\nyhat_test = best_model.predict(X_test)\n# Submitting\nsubmission = testing_df.copy()\nsubmission['Survived'] = yhat_test\nsubmission.to_csv('submission.csv', columns=['PassengerId', 'Survived'], index=False)\n\nsubmission[['PassengerId', 'Survived']].head(15)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}