{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a56a262-8015-68e8-6c7a-6cb74a64b48f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b2baecb-79aa-77fb-7480-62b50051f778"
      },
      "outputs": [],
      "source": [
        "#read data\n",
        "train_df = pd.read_csv('../input/train.csv')\n",
        "test_df  = pd.read_csv('../input/test.csv')\n",
        "\n",
        "#get an idea\n",
        "train_df.info()\n",
        "test_df.info()\n",
        "train_df.head()\n",
        "test_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "704272fe-af92-cd58-6757-8fc03dfaffcb"
      },
      "outputs": [],
      "source": [
        "#use csv file directly to create features for faster operatio\n",
        "# refrences : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n",
        "# https://www.kaggle.com/the1owl/matching-que-for-quora-end-to-end-0-33719-pb\n",
        "\n",
        "import os, sys, re, csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "import math\n",
        "import nltk\n",
        "from datetime import datetime\n",
        "\n",
        "sw = set(stopwords.words('english'))\n",
        "\n",
        "def get_cosine(str1, str2):\n",
        "\n",
        "    vec1 = Counter(str1.split())\n",
        "    vec2 = Counter(str2.split())\n",
        "\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def DistJaccard(str1, str2):\n",
        "    str1 = set(str1.split())\n",
        "    str2 = set(str2.split())\n",
        "    numerator = len(str1 & str2)\n",
        "    denominator = len(str1 | str2)\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def getAlphabetCount(word):\n",
        "    alphabet_dict = {}\n",
        "    txt = re.sub('[^A-Za-z]','',re.sub(' ','',word))\n",
        "    for aa in txt:\n",
        "        if aa in alphabet_dict:\n",
        "            alphabet_dict[aa] += 1\n",
        "        else:\n",
        "            alphabet_dict[aa] = 1\n",
        "    list1 = list(txt)\n",
        "    list2 = list('abcdefghijklmnopqrstuvwxyz')\n",
        "    diff = set(list2).symmetric_difference(list1)\n",
        "    for bb in diff:\n",
        "        alphabet_dict[bb] = 0\n",
        "    return alphabet_dict\n",
        "\n",
        "def decisionMaker(a,b):\n",
        "    if(a==b):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def word_match_share(txt1, txt2):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in txt1:\n",
        "        q1words[word] = 1\n",
        "    for word in txt2:\n",
        "        q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    match_score = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
        "    return match_score\n",
        "\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1.0 / (count + eps)\n",
        "\n",
        "eps = 5000\n",
        "\n",
        "def getAlphabetCount(word):\n",
        "    alphabet_dict = {}\n",
        "    txt = re.sub('[^A-Za-z]','',re.sub(' ','',word))\n",
        "    for aa in txt:\n",
        "        if aa in alphabet_dict:\n",
        "            alphabet_dict[aa] += 1\n",
        "        else:\n",
        "            alphabet_dict[aa] = 1\n",
        "    list1 = list(txt)\n",
        "    list2 = list('abcdefghijklmnopqrstuvwxyz')\n",
        "    diff = set(list2).symmetric_difference(list1)\n",
        "    for bb in diff:\n",
        "        alphabet_dict[bb] = 0\n",
        "    return alphabet_dict\n",
        "\n",
        "def decisionMaker(a,b):\n",
        "    if(a==b):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def word_match_share(txt1, txt2):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in txt1:\n",
        "        q1words[word] = 1\n",
        "    for word in txt2:\n",
        "        q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    match_score = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
        "    return match_score\n",
        "\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1.0 / (count + eps)\n",
        "\n",
        "eps = 5000\n",
        "\n",
        "#This function is messed up - can be written much better\n",
        "def tfidf_word_match_share(txt1, txt2):\n",
        "    tfidf = []\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in txt1:\n",
        "        q1words[word] = 1\n",
        "    for word in txt2:\n",
        "        q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q1words) == 0 :\n",
        "        return 0,0,0,0,0,0,0,0,0,0,0\n",
        "    words = txt1 + txt2\n",
        "    counts = Counter(words)\n",
        "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
        "    q1_tfidf_weights = [weights.get(w, 0) for w in q1words.keys()]\n",
        "    q2_tfidf_weights = [weights.get(w, 0) for w in q2words.keys()]\n",
        "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
        "    total_weights  = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
        "    tfidfRatio = np.sum(shared_weights)*1.0 / np.sum(total_weights)\n",
        "    q1_tfidf_sum     = sum(q1_tfidf_weights)\n",
        "    q2_tfidf_sum     = sum(q2_tfidf_weights)\n",
        "    q1_tfidf_mean    = np.mean(q1_tfidf_weights)\n",
        "    q2_tfidf_mean    = np.mean(q2_tfidf_weights)\n",
        "    q1_tfidf_min     = min(q1_tfidf_weights)\n",
        "    q1_tfidf_max     = min(q1_tfidf_weights)\n",
        "    q1_tfidf_range   = q1_tfidf_max - q1_tfidf_min\n",
        "    q2_tfidf_min     = min(q2_tfidf_weights)\n",
        "    q2_tfidf_max     = min(q2_tfidf_weights)\n",
        "    q2_tfidf_range   = q2_tfidf_max - q2_tfidf_min\n",
        "    return q1_tfidf_sum, q2_tfidf_sum, q1_tfidf_mean, q2_tfidf_mean, q1_tfidf_min, q1_tfidf_max, q1_tfidf_range, q2_tfidf_min, q2_tfidf_max, q2_tfidf_range, q2_tfidf_range\n",
        "    \n",
        "def shared_2gram(q1, q2):\n",
        "    q1_2gram = set([i for i in zip(q1.split(), q1.split()[1:])])\n",
        "    q2_2gram = set([i for i in zip(q2.split(), q2.split()[1:])])\n",
        "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
        "    if len(q1_2gram)==0 or len(q2_2gram) == 0:\n",
        "        return 0\n",
        "    return len(shared_2gram)*1.0/(len(q1_2gram) + len(q2_2gram))\n",
        "\n",
        "def shared_3gram(q1, q2):\n",
        "    q1_3gram = set([i for i in zip(q1.split(), q1.split()[1:], q1.split()[2:])])\n",
        "    q2_3gram = set([i for i in zip(q2.split(), q2.split()[1:], q2.split()[2:])])\n",
        "    shared_3gram = q1_3gram.intersection(q2_3gram)\n",
        "    if len(q1_3gram)==0 or len(q2_3gram) == 0:\n",
        "        return 0\n",
        "    return len(shared_3gram)*1.0/(len(q1_3gram) + len(q2_3gram))\n",
        "\n",
        "def avgWordLenDiff(q1, q2):\n",
        "    len_char_q1 = len(q1.replace(' ',''))\n",
        "    len_char_q2 = len(q2.replace(' ',''))\n",
        "    len_word_q1 = len(q1.split())\n",
        "    len_word_q2 = len(q2.split())\n",
        "    if len_char_q1 == 0 or len_char_q2 == 0:\n",
        "        return 0, 0, 0\n",
        "    avg_world_len_q1 = len_char_q1*1.0/len_word_q1\n",
        "    avg_world_len_q2 = len_char_q2*1.0/len_word_q2\n",
        "    return avg_world_len_q1, avg_world_len_q2, avg_world_len_q1-avg_world_len_q2\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "def avgStopWords(q1, q2):\n",
        "    q1stops_len = len(set(q1.split()).intersection(stops))\n",
        "    q2stops_len = len(set(q2.split()).intersection(stops))\n",
        "    q1word_len  = len(q1.split())\n",
        "    q2word_len  = len(q2.split())\n",
        "    if q1stops_len == 0 or q2stops_len == 0:\n",
        "        return 0,0,0\n",
        "    q1_r = q1stops_len*1.0/q1word_len\n",
        "    q2_r = q2stops_len*1.0/q2word_len\n",
        "    q_diff = q1_r - q2_r\n",
        "    return q1_r, q2_r, q_diff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7d9a914-0f19-f588-ef0a-c147c4558a06"
      },
      "outputs": [],
      "source": [
        "def create_data(infile,outfile):\n",
        "    \n",
        "    if('train' in infile):\n",
        "        with open('../input/' + infile) as file:\n",
        "            reader = csv.reader(file, delimiter = ',')\n",
        "            i = 0\n",
        "            for line in reader:\n",
        "                q1 = [re.sub('[^A-Za-z0-9]','',x).lower() for x in line[3].split() if len(x)>1 and x not in sw]\n",
        "                q2 = [re.sub('[^A-Za-z0-9]','',x).lower() for x in line[4].split() if len(x)>1 and x not in sw]\n",
        "                if len(q1) > 1 :\n",
        "                    q1 = ' '.join([re.sub('[^A-Za-z0-9]','',x).lower() for x in q1 if len(x)>1 and x not in sw])\n",
        "                else:\n",
        "                    q1 = 'blank'\n",
        "                if len(q2):\n",
        "                    q2 = ' '.join([re.sub('[^A-Za-z0-9]','',x).lower() for x in q2 if len(x)>1 and x not in sw])\n",
        "                else:\n",
        "                    q2 = 'blank'\n",
        "                q1_len_with_space, q2_len_with_space  = len(q1), len(q2)\n",
        "                q1_len_without_space, q2_len_without_space  = len(re.sub(' ','',q1)), len(re.sub(' ','',q2))\n",
        "                q1_no_of_words, q2_no_of_words = len(q1.split()), len(q2.split())\n",
        "                q1_no_of_uniq_words, q2_no_of_uniq_words = len(set(q1.split())), len(set(q2.split()))\n",
        "\n",
        "                len_with_space_ind = decisionMaker(q1_len_with_space, q2_len_with_space)\n",
        "                len_without_space_ind = decisionMaker(q1_len_without_space, q2_len_without_space)\n",
        "                no_of_words_ind = decisionMaker(q2_no_of_words, q2_no_of_words)\n",
        "                no_of_uniq_words_ind = decisionMaker(q2_no_of_uniq_words, q2_no_of_uniq_words)\n",
        "\n",
        "                diff_len_with_space = q1_len_with_space - q2_len_with_space\n",
        "                diff_len_without_space = q1_len_without_space - q2_len_without_space\n",
        "                diff_no_of_words = q1_no_of_words - q2_no_of_words\n",
        "                diff_no_of_uniq_words = q1_no_of_uniq_words - q2_no_of_uniq_words\n",
        "\n",
        "                alphabetCountDictQ1 = getAlphabetCount(q1)\n",
        "                alphabetCountDictQ2 = getAlphabetCount(q2)\n",
        "                q1_a, q2_a = alphabetCountDictQ1['a'], alphabetCountDictQ2['a']\n",
        "                q1_b, q2_b = alphabetCountDictQ1['b'], alphabetCountDictQ2['b']\n",
        "                q1_c, q2_c = alphabetCountDictQ1['c'], alphabetCountDictQ2['c']\n",
        "                q1_d, q2_d = alphabetCountDictQ1['d'], alphabetCountDictQ2['d']\n",
        "                q1_e, q2_e = alphabetCountDictQ1['e'], alphabetCountDictQ2['e']\n",
        "                q1_f, q2_f = alphabetCountDictQ1['f'], alphabetCountDictQ2['f']\n",
        "                q1_g, q2_g = alphabetCountDictQ1['g'], alphabetCountDictQ2['g']\n",
        "                q1_h, q2_h = alphabetCountDictQ1['h'], alphabetCountDictQ2['h']\n",
        "                q1_i, q2_i = alphabetCountDictQ1['i'], alphabetCountDictQ2['i']\n",
        "                q1_j, q2_j = alphabetCountDictQ1['j'], alphabetCountDictQ2['j']\n",
        "                q1_k, q2_k = alphabetCountDictQ1['k'], alphabetCountDictQ2['k']\n",
        "                q1_l, q2_l = alphabetCountDictQ1['l'], alphabetCountDictQ2['l']\n",
        "                q1_m, q2_m = alphabetCountDictQ1['m'], alphabetCountDictQ2['m']\n",
        "                q1_n, q2_n = alphabetCountDictQ1['n'], alphabetCountDictQ2['n']\n",
        "                q1_o, q2_o = alphabetCountDictQ1['o'], alphabetCountDictQ2['o']\n",
        "                q1_p, q2_p = alphabetCountDictQ1['p'], alphabetCountDictQ2['p']\n",
        "                q1_q, q2_q = alphabetCountDictQ1['q'], alphabetCountDictQ2['q']\n",
        "                q1_r, q2_r = alphabetCountDictQ1['r'], alphabetCountDictQ2['r']\n",
        "                q1_s, q2_s = alphabetCountDictQ1['s'], alphabetCountDictQ2['s']\n",
        "                q1_t, q2_t = alphabetCountDictQ1['t'], alphabetCountDictQ2['t']\n",
        "                q1_u, q2_u = alphabetCountDictQ1['u'], alphabetCountDictQ2['u']\n",
        "                q1_v, q2_v = alphabetCountDictQ1['v'], alphabetCountDictQ2['v']\n",
        "                q1_w, q2_w = alphabetCountDictQ1['w'], alphabetCountDictQ2['w']\n",
        "                q1_x, q2_x = alphabetCountDictQ1['x'], alphabetCountDictQ2['x']\n",
        "                q1_y, q2_y = alphabetCountDictQ1['y'], alphabetCountDictQ2['y']\n",
        "                q1_z, q2_z = alphabetCountDictQ1['z'], alphabetCountDictQ2['z']\n",
        "\n",
        "                a_count_ind = decisionMaker(q1_a, q2_a)\n",
        "                b_count_ind = decisionMaker(q1_b, q2_b)\n",
        "                c_count_ind = decisionMaker(q1_c, q2_c)\n",
        "                d_count_ind = decisionMaker(q1_d, q2_d)\n",
        "                e_count_ind = decisionMaker(q1_e, q2_e)\n",
        "                f_count_ind = decisionMaker(q1_f, q2_f)\n",
        "                g_count_ind = decisionMaker(q1_g, q2_g)\n",
        "                h_count_ind = decisionMaker(q1_h, q2_h)\n",
        "                i_count_ind = decisionMaker(q1_i, q2_i)\n",
        "                j_count_ind = decisionMaker(q1_j, q2_j)\n",
        "                k_count_ind = decisionMaker(q1_k, q2_k)\n",
        "                l_count_ind = decisionMaker(q1_l, q2_l)\n",
        "                m_count_ind = decisionMaker(q1_m, q2_m)\n",
        "                n_count_ind = decisionMaker(q1_n, q2_n)\n",
        "                o_count_ind = decisionMaker(q1_o, q2_o)\n",
        "                p_count_ind = decisionMaker(q1_p, q2_p)\n",
        "                q_count_ind = decisionMaker(q1_q, q2_q)\n",
        "                r_count_ind = decisionMaker(q1_r, q2_r)\n",
        "                s_count_ind = decisionMaker(q1_s, q2_s)\n",
        "                t_count_ind = decisionMaker(q1_t, q2_t)\n",
        "                u_count_ind = decisionMaker(q1_u, q2_u)\n",
        "                v_count_ind = decisionMaker(q1_v, q2_v)\n",
        "                w_count_ind = decisionMaker(q1_w, q2_w)\n",
        "                x_count_ind = decisionMaker(q1_x, q2_x)\n",
        "                y_count_ind = decisionMaker(q1_y, q2_y)\n",
        "                z_count_ind = decisionMaker(q1_z, q2_z)\n",
        "                \n",
        "                diff_a_count = q1_a - q2_a\n",
        "                diff_b_count = q1_b - q2_b\n",
        "                diff_c_count = q1_c - q2_c\n",
        "                diff_d_count = q1_d - q2_d\n",
        "                diff_e_count = q1_e - q2_e\n",
        "                diff_f_count = q1_f - q2_f\n",
        "                diff_g_count = q1_g - q2_g\n",
        "                diff_h_count = q1_h - q2_h\n",
        "                diff_i_count = q1_i - q2_i\n",
        "                diff_j_count = q1_j - q2_j\n",
        "                diff_k_count = q1_k - q2_k\n",
        "                diff_l_count = q1_l - q2_l\n",
        "                diff_m_count = q1_m - q2_m\n",
        "                diff_n_count = q1_n - q2_n\n",
        "                diff_o_count = q1_o - q2_o\n",
        "                diff_p_count = q1_p - q2_p\n",
        "                diff_q_count = q1_q - q2_q\n",
        "                diff_r_count = q1_r - q2_r\n",
        "                diff_s_count = q1_s - q2_s\n",
        "                diff_t_count = q1_t - q2_t\n",
        "                diff_u_count = q1_u - q2_u\n",
        "                diff_v_count = q1_v - q2_v\n",
        "                diff_w_count = q1_w - q2_w\n",
        "                diff_x_count = q1_x - q2_x\n",
        "                diff_y_count = q1_y - q2_y\n",
        "                diff_z_count = q1_z - q2_z\n",
        "\n",
        "                cos_sim = get_cosine(q1, q2)\n",
        "                jac_sim = DistJaccard(q1, q2)\n",
        "                seq_mat = similar(q1, q2)\n",
        "                #lav_dis = distance.levenshtein(q1, q2)\n",
        "                word_match = word_match_share(q1, q2)\n",
        "                tfidf_match = tfidf_word_match_share(q1, q2)[10]\n",
        "                shared_2grams = shared_2gram(q1, q2)\n",
        "                shared_3grams = shared_3gram(q1, q2)\n",
        "                avg_world_len_q1 = avgWordLenDiff(q1, q2)[0]\n",
        "                avg_world_len_q2 = avgWordLenDiff(q1, q2)[1]\n",
        "                avg_world_diff   = avgWordLenDiff(q1, q2)[2]\n",
        "                q1_stop_ratio    = avgStopWords(q1, q2)[0]\n",
        "                q2_stop_ratio    = avgStopWords(q1, q2)[1]\n",
        "                ratio_diff       = avgStopWords(q1, q2)[2]\n",
        "                caps_count_q1 = sum([1 for j in line[3] if j.isupper()])\n",
        "                caps_count_q2 = sum([1 for j in line[4] if j.isupper()])\n",
        "                caps_count_diff = caps_count_q1 - caps_count_q2\n",
        "                qmarks_q1 = sum([1 for j in line[3] if j=='?'])\n",
        "                qmarks_q2 = sum([1 for j in line[4] if j=='?'])\n",
        "                qmarks_diff = qmarks_q1 - qmarks_q2\n",
        "                fs_q1 = sum([1 for j in line[3] if j=='.'])\n",
        "                fs_q2 = sum([1 for j in line[4] if j=='.'])\n",
        "                fs_diff = qmarks_q1 - qmarks_q2\n",
        "                if(len(line[3])>1):\n",
        "                    first_caps_count_q1 = sum([1 if line[3][0].isupper() else 0])\n",
        "                else:\n",
        "                    first_caps_count_q1 = 0\n",
        "                if(len(line[4])>1):\n",
        "                    first_caps_count_q2 = sum([1 if line[4][0].isupper() else 0])\n",
        "                else:\n",
        "                    first_caps_count_q2 = 0\n",
        "                first_caps_count_diff = first_caps_count_q1 - first_caps_count_q2\n",
        "                numb_count_q1 = sum([1 for j in line[3] if j.isdigit()])\n",
        "                numb_count_q2 = sum([1 for j in line[4] if j.isdigit()])\n",
        "                numb_count_diff = numb_count_q1 - numb_count_q2\n",
        "                nouns_q1 = [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(q1).lower())) if t[:1] in ['N']]\n",
        "                nouns_q2 = [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(q2).lower())) if t[:1] in ['N']]\n",
        "                noun_count_q1 = len(nouns_q1)\n",
        "                noun_count_q2 = len(nouns_q2)\n",
        "                noun_match    = sum([1 for w in nouns_q1 if w in nouns_q2])\n",
        "                tfidf = tfidf_word_match_share(q1, q2)\n",
        "                q1_tfidf_sum = tfidf[0]\n",
        "                q2_tfidf_sum = tfidf_word_match_share(q1, q2)[1]\n",
        "                q1_tfidf_mean = tfidf_word_match_share(q1, q2)[2]\n",
        "                q2_tfidf_mean = tfidf_word_match_share(q1, q2)[3]\n",
        "                q1_tfidf_min = tfidf_word_match_share(q1, q2)[4]\n",
        "                q1_tfidf_max = tfidf_word_match_share(q1, q2)[5]\n",
        "                q1_tfidf_range = tfidf_word_match_share(q1, q2)[6]\n",
        "                q2_tfidf_min = tfidf_word_match_share(q1, q2)[7]\n",
        "                q2_tfidf_max = tfidf_word_match_share(q1, q2)[8]\n",
        "                q2_tfidf_range = tfidf_word_match_share(q1, q2)[9]\n",
        "\n",
        "    #outfile.write(line[0] + '^' + q1 + '^' + q2 + '^' + str(len_with_space_ind) + '^' + str(len_without_space_ind) + '^' + str(no_of_words_ind) + '^' + str(no_of_uniq_words_ind) + '^' + str(diff_len_with_space) + '^' + str(diff_len_without_space) + '^' + str(diff_no_of_words) + '^' +  str(diff_no_of_uniq_words) + '^' + str(a_count_ind) + '^' + str(b_count_ind) + '^' + str(c_count_ind) + '^' + str(d_count_ind) + '^' + str(e_count_ind) + '^' + str(f_count_ind) + '^' + str(g_count_ind) + '^' + str(h_count_ind) + '^' + str(i_count_ind) + '^' + str(j_count_ind) + '^' + str(k_count_ind) + '^' + str(l_count_ind) + '^' + str(m_count_ind) + '^' + str(n_count_ind) + '^' + str(o_count_ind) + '^' + str(p_count_ind) + '^' + str(q_count_ind) + '^' + str(r_count_ind) + '^' + str(s_count_ind) + '^' + str(t_count_ind) + '^' + str(u_count_ind) + '^' + str(v_count_ind) + '^' + str(w_count_ind) + '^' + str(x_count_ind) + '^' + str(y_count_ind) + '^' + str(z_count_ind) + '^' + str(diff_a_count) + '^' + str(diff_b_count) + '^' + str(diff_c_count) + '^' + str(diff_d_count) + '^' + str(diff_e_count) + '^' + str(diff_f_count) + '^' + str(diff_g_count) + '^' + str(diff_h_count) + '^' + str(diff_i_count) + '^' + str(diff_j_count) + '^' + str(diff_k_count) + '^' + str(diff_l_count) + '^' + str(diff_m_count) + '^' + str(diff_n_count) + '^' + str(diff_o_count) + '^' + str(diff_p_count) + '^' + str(diff_q_count) + '^' + str(diff_r_count) + '^' + str(diff_s_count) + '^' + str(diff_t_count) + '^' + str(diff_u_count) + '^' + str(diff_v_count) + '^' + str(diff_w_count) + '^' + str(diff_x_count) + '^' + str(diff_y_count) + '^' + str(diff_z_count) + '^' + str(cos_sim) + '^' + str(jac_sim) + '^' + str(seq_mat) + '^' + str(lav_dis) + '^' + str(word_match) + '^' + str(tfidf_match) + '^' + str(shared_2grams) + '^' + str(shared_3grams) + '^' + str(avg_world_len_q1) + '^' + str(avg_world_len_q2) + '^' + str(avg_world_diff) + '^' + str(q1_stop_ratio) + '^' + str(q1_stop_ratio) + '^' + str(ratio_diff) + '^' + str(caps_count_q1) + '^' + str(caps_count_q2) + '^' + str(caps_count_diff) + '^' + str(qmarks_q1) + '^' + str(qmarks_q2) + '^' + str(qmarks_diff) + '^' + str(fs_q1) + '^' + str(fs_q2) + '^' + str(qmarks_diff) + '^' + str(first_caps_count_q1) + '^' + str(first_caps_count_q2) + '^' + str(first_caps_count_diff) + str(numb_count_q1) + '^' + str(numb_count_q2) + '^' + str(numb_count_diff) + '^' + str(noun_count_q1) + '^' + str(noun_count_q2) + '^' + str(noun_match) + '^' + str(q1_tfidf_sum) + '^' + str(q2_tfidf_sum) + '^' + str(q1_tfidf_mean) + '^' + str(q2_tfidf_mean) + '^' + str(q1_tfidf_min) + '^' + str(q1_tfidf_max) + '^' + str(q1_tfidf_range) + '^' + str(q2_tfidf_min) + '^' + str(q2_tfidf_max) + '^' + str(q2_tfidf_range) + '^' + str(line[5]) + '\\n')              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4c6f89af-551b-6e27-64e9-afdcddaf4093"
      },
      "outputs": [],
      "source": [
        "create_data('train.csv', 'train_ftrs.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "472c2ddd-6552-c860-57da-e3d2ee5e2c36"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}