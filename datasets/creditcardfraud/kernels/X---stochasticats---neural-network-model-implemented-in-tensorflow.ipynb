{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"source": ["# Data Analysis and Prediction of Credit Card Fraud Detection Data\n", "\n", "This Python notebook contains a data analysis of credit card fraud data from Kaggle (https://www.kaggle.com/dalpozz/creditcardfraud), along with a predictive model aiming at detecting a fraudulent transaction. The predictive model to be developed is a neural network implemented in tensorflow.\n", "\n", "First we load in the required libraries and the data set we are going to be working with.\n", "\n", "The data set has been anonymized for confidentiality and the features V1,..., V28 are the principal components of a PCA transformation. There is three other variables: Amount, Class, Time. Amount denotes the amount of money of the transaction; Class denotes a fraudulent transaction, 0, or normal transaction, 1; and Time is an integer denoting time since first transaction in seconds. Also note that the entire data set is two days of credit card transactions.\n", "\n", "## Data Exploration\n", "\n", "So let's take a look at the structure of the data set and do some analysis."], "metadata": {}, "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["import math\n", "import pandas as pd\n", "import numpy as np \n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import scipy\n", "from tensorflow.python.framework import ops\n", "\n", "%matplotlib inline\n", "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n", "plt.rcParams['image.interpolation'] = 'nearest'\n", "plt.rcParams['image.cmap'] = 'gray'"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["dataset = pd.read_csv(\"../input/creditcard.csv\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["dataset.head()"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["dataset.describe()"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(\"Percent of total transactions that are fraudulent\")\n", "print(dataset[\"Class\"].mean()*100)"], "cell_type": "code"}, {"source": ["Fraudulent transactions represent only ~0.17% of total transactions. This means that we are aiming to predict anomalous events."], "metadata": {}, "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(\"Losses due to fraud:\")\n", "print(\"Total amount lost to fraud\")\n", "print(dataset.Amount[dataset.Class == 1].sum())\n", "print(\"Mean amount per fraudulent transaction\")\n", "print(dataset.Amount[dataset.Class == 1].mean())\n", "print(\"Compare to normal transactions:\")\n", "print(\"Total amount from normal transactions\")\n", "print(dataset.Amount[dataset.Class == 0].sum())\n", "print(\"Mean amount per normal transactions\")\n", "print(dataset.Amount[dataset.Class == 0].mean())"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n", "\n", "bins = 40\n", "\n", "ax1.hist(dataset.Amount[dataset.Class == 1], bins = bins, normed = True, alpha = 0.75, color = 'red')\n", "ax1.set_title('Fraud')\n", "\n", "ax2.hist(dataset.Amount[dataset.Class == 0], bins = bins, normed = True, alpha = 0.5, color = 'blue')\n", "ax2.set_title('Not Fraud')\n", "\n", "plt.xlabel('Amount')\n", "plt.ylabel('% of Transactions')\n", "plt.yscale('log')\n", "plt.show()"], "cell_type": "code"}, {"source": ["It is interesting to see that while fraudulent transactions make up a small portion of the data set, they have a higher average amount per transaction. It may be useful to try a model with Amount as a feature."], "metadata": {}, "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["bins = 75\n", "plt.hist(dataset.Time[dataset.Class == 1], bins = bins, normed = True, alpha = 0.75, label = 'Fraud', color = 'red')\n", "plt.hist(dataset.Time[dataset.Class == 0], bins = bins, normed = True, alpha = 0.5, label = 'Not Fraud', color = 'blue')\n", "plt.legend(loc='upper right')\n", "plt.xlabel('Time (seconds)')\n", "plt.ylabel('% of ')\n", "plt.title('Transactions over Time')\n", "plt.show()"], "cell_type": "code"}, {"source": ["This histogram shows the percentage of transactions made over the time period. We see that more fraudulent activity typically happens when there is downtime in overall transactions. If we assume that the data is collected from Day 0 12:01 AM to Day 2 11:59 PM, since it is described as being collected over \"two days\", we see that fraudulent activity is occuring in the very early AM. I am reluctant to use Time as a feature here in our predictive model because there is only two days of data. If there was a month or so of data, this would definitely be useful as a feature if we see a similar pattern over a longer time period.\n", "\n", "Let's take a look at the V1,...,V28 features."], "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["Vfeatures = dataset.iloc[:,1:29].columns\n", "print(Vfeatures)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["import matplotlib.gridspec as gridspec\n", "import seaborn as sns\n", "bins = 50\n", "plt.figure(figsize=(12,28*4))\n", "gs = gridspec.GridSpec(28, 1)\n", "for i, V in enumerate(dataset[Vfeatures]):\n", "    ax = plt.subplot(gs[i])\n", "    sns.distplot(dataset[V][dataset.Class == 1], bins = bins, norm_hist = True, color = 'red')\n", "    sns.distplot(dataset[V][dataset.Class == 0], bins = bins, norm_hist = True, color = 'blue')\n", "    ax.set_xlabel('')\n", "    ax.set_title('distributions (w.r.t fraud vs. non-fraud) of feature: ' + str(V))\n", "plt.show()"], "cell_type": "code"}, {"source": ["This shows the distribution differences of the features when comparing fraudulent transactions to normal transactions. \n", "\n", "Ok, let's develop a neural network in tensorflow with the goal of predicting credit card fraud. We will use the all the V features and Amount as features in our model. First we need to put our input data sets into the correct format."], "metadata": {}, "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["model_features = dataset.iloc[:,1:30].columns\n", "print(model_features)\n", "\n", "# normalize Amount column\n", "dataset[\"Amount\"] = (dataset[\"Amount\"]-dataset[\"Amount\"].mean())/dataset[\"Amount\"].std()\n", "\n", "# shuffle and split our data set\n", "dataset = dataset.sample(frac=1).reset_index(drop=True)\n", "split = np.random.rand(len(dataset)) < 0.95\n", "dataset_train = dataset[split]\n", "dataset_test = dataset[~split]\n", "train_x = dataset_train.as_matrix(columns = model_features)\n", "train_y = dataset_train[\"Class\"]\n", "test_x = dataset_test.as_matrix(columns = model_features)\n", "test_y = dataset_test[\"Class\"]\n", "\n", "# check the distribution of fraud between train and test\n", "# if these are too far off, try shuffling again\n", "print(dataset[\"Amount\"].sum())\n", "print(train_y.mean()*100)\n", "print(test_y.mean()*100)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["''' \n", "modify train and test sets for correct dimensions and check it.\n", "dimensions should be \n", "X - (# of features, # of examples)\n", "Y - (1, # of examples)\n", "'''\n", "train_x = train_x.T\n", "train_y = np.reshape(train_y, (1,len(dataset_train)))\n", "test_x = test_x.T\n", "test_y = np.reshape(test_y, (1,len(dataset_test)))\n", "\n", "print(train_x.shape)\n", "print(train_y.shape)\n", "print(test_x.shape)\n", "print(test_y.shape)"], "cell_type": "code"}, {"source": ["# Neural Network Model Implemented in TensorFlow\n", "\n", "Here we develop the neural network model we will use to try and predict fraudulent transactions."], "metadata": {}, "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def create_placeholders(n_x, n_y):\n", "    # n_x - number of features\n", "    # n_y - number of classes\n", "    X = tf.placeholder(tf.float32, shape = (n_x, None))\n", "    Y = tf.placeholder(tf.float32, shape = (n_y, None))\n", "    return X, Y"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def initialize_parameters():                  \n", "    W1 = tf.get_variable(\"W1\", [14,29], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n", "    b1 = tf.get_variable(\"b1\", [14,1], initializer = tf.zeros_initializer())\n", "    W2 = tf.get_variable(\"W2\", [7,14], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n", "    b2 = tf.get_variable(\"b2\", [7,1], initializer = tf.zeros_initializer())\n", "    W3 = tf.get_variable(\"W3\", [1,7], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n", "    b3 = tf.get_variable(\"b3\", [1,1], initializer = tf.zeros_initializer())\n", "\n", "    parameters = {\"W1\": W1,\n", "                  \"b1\": b1,\n", "                  \"W2\": W2,\n", "                  \"b2\": b2,\n", "                  \"W3\": W3,\n", "                  \"b3\": b3}\n", "    \n", "    return parameters"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def forward_propagation(X, parameters):\n", "    # Retrieve the parameters from the dictionary parameters\n", "    W1 = parameters['W1']\n", "    b1 = parameters['b1']\n", "    W2 = parameters['W2']\n", "    b2 = parameters['b2']\n", "    W3 = parameters['W3']\n", "    b3 = parameters['b3']\n", "\n", "    Z1 = tf.add(tf.matmul(W1, X), b1)                                   \n", "    A1 = tf.nn.elu(Z1)                                              \n", "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                      \n", "    A2 = tf.nn.elu(Z2)                                         \n", "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n", "    \n", "    return Z3"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def compute_cost(Z3, Y):\n", "    logits = tf.transpose(Z3)\n", "    labels = tf.transpose(Y)\n", "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n", "    \n", "    return cost"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n", "    \n", "    m = X.shape[1]                  # number of training examples\n", "    mini_batches = []\n", "    np.random.seed(seed)\n", "    \n", "    permutation = list(np.random.permutation(m))\n", "    shuffled_X = X[:, permutation]\n", "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n", "\n", "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n", "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n", "    for k in range(0, num_complete_minibatches):\n", "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n", "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    # Handling the end case (last mini-batch < mini_batch_size)\n", "    if m % mini_batch_size != 0:\n", "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n", "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    return mini_batches"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001,\n", "          num_epochs = 1500, minibatch_size = 1024, print_cost = True):\n", "    # Implements a three layer layer neural network using tensorflow\n", "    \n", "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n", "    tf.set_random_seed(1)                             # to keep consistent results\n", "    seed = 3                                          # to keep consistent results\n", "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n", "    n_y = Y_train.shape[0]                            # n_y : output size\n", "    costs = []                                        # To keep track of the cost\n", "\n", "    X, Y = create_placeholders(n_x, n_y)\n", "\n", "    parameters = initialize_parameters()\n", "    Z3 = forward_propagation(X, parameters)\n", "    cost = compute_cost(Z3, Y)\n", "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n", "    \n", "    init = tf.global_variables_initializer()\n", "\n", "    with tf.Session() as sess:\n", "        sess.run(init)\n", "        for epoch in range(num_epochs):\n", "\n", "            epoch_cost = 0.                       # Defines a cost related to an epoch\n", "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n", "            seed = seed + 1\n", "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n", "\n", "            for minibatch in minibatches:\n", "                (minibatch_X, minibatch_Y) = minibatch\n", "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n", "                epoch_cost += minibatch_cost / num_minibatches\n", "                \n", "            if print_cost == True and epoch % 100 == 0:\n", "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n", "            if print_cost == True and epoch % 5 == 0:\n", "                costs.append(epoch_cost)\n", "                \n", "        plt.plot(np.squeeze(costs))\n", "        plt.ylabel('cost')\n", "        plt.xlabel('iterations (per tens)')\n", "        plt.title(\"Learning rate =\" + str(learning_rate))\n", "        plt.show()\n", "\n", "        parameters = sess.run(parameters)\n", "        print (\"Parameters have been trained!\")\n", "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n", "\n", "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "\n", "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n", "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n", "        \n", "        return parameters"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["model_params = model(train_x, train_y, test_x, test_y)"], "cell_type": "code"}, {"source": ["Here we see an accuracy of 100% on both the training and test sets! What a model!"], "metadata": {}, "cell_type": "markdown"}]}