{"cells": [{"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0226e3c042150c988cd3aeab2a1c55dd1b9dd2a8", "_cell_guid": "3f694542-f3d9-440c-87c8-ee42db042b08"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import tensorflow as tf\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import math\n", "import cv2\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "cdddf9bb20a3d5a2e444143cecfb73a7f2650a00", "_cell_guid": "ce0099e8-f3be-4abf-ad81-f3d134080a2c"}, "source": ["train = pd.read_json('../input/train.json')\n", "train.keys()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a4359baae8b5b63a0e80bc9b66d92405f8477edd", "_cell_guid": "50c90bcb-e328-40aa-b775-78e586b7d058"}, "source": ["train.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7c44b8bc7d2d1a1f08dbc7a921c4747bdac955c9", "_cell_guid": "ac1346d1-bf6a-4ab7-a7d4-39d7532164de"}, "source": ["test = pd.read_json('../input/test.json')\n", "test.keys()"]}, {"metadata": {"_uuid": "3f7c233b0446b66e13bbb5129e2840156a26d766", "_cell_guid": "05df72bc-a4bd-4f3a-94e3-028eac1bfe0a"}, "cell_type": "markdown", "source": ["**Let's explore some images.**"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4588ed19eee06741f9fc1e308568b36665b679f4", "_cell_guid": "284049d1-af7a-423e-a5b7-88650311f324"}, "source": ["icebergs = train[train.is_iceberg == 1]\n", "print(type(icebergs))\n", "icebergs = icebergs.sample(n=18, random_state = 456)\n", "\n", "ships = train[train.is_iceberg == 0]\n", "ships = ships.sample(n=18, random_state = 456)\n", "\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4977fdb363024652bca459dcd37ad06b3a7322fb", "_cell_guid": "16ba1431-df4d-4469-af6e-a3b50ea3281c"}, "source": ["fig = plt.figure(1, figsize = (15, 15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3, 3, i+1)\n", "    arr = np.reshape(np.array(icebergs.iloc[i, 0]), (75, 75)) + \\\n", "          np.reshape(np.array(icebergs.iloc[i, 1]), (75, 75))\n", "    ax.set_title('Incidence Angle: {}, label: {}'.format(icebergs.iloc[i, 3], icebergs.iloc[i, 4]))\n", "    ax.imshow(arr, cmap = 'winter')\n", "    \n", "plt.show()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1b22acfa23e24601353bbe75d113e0dd0be3bc03", "_cell_guid": "c6178b79-ee63-4271-8a8f-ef973ad31f92"}, "source": ["img = icebergs.iloc[0, 0]\n", "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n", "\n", "\n", "#blurred = cv2.medianBlur(x_band1, 5)\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "303d5b8638c1576a2523e236820407bddbb132ea", "_cell_guid": "d30fe24c-b60a-468d-a58d-9d146c77d110"}, "source": ["blurred = cv2.medianBlur(x_band1[0], 5)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c32f90fa04f8057d8c8842f86a197d047a9e0094", "_cell_guid": "c378842f-a6e4-4a26-a2ca-fbaa8f4bd06a"}, "source": ["fig = plt.figure(1,figsize=(15,15))\n", "for i in range(9):\n", "    ax = fig.add_subplot(3, 3, i+1)\n", "    arr = np.reshape(np.array(ships.iloc[i, 0]), (75, 75)) #+ \\\n", "          #np.reshape(np.array(ships.iloc[i, 1]), (75, 75))\n", "    ax.set_title('Incidence Angle: {}, label: {}'.format(ships.iloc[i, 3], ships.iloc[i, 4]))\n", "    ax.imshow(arr, cmap='winter')\n", "    \n", "plt.show()"]}, {"metadata": {"_uuid": "d1c139a224d8ea537ec6631b406a1a15fbfa99e0", "_cell_guid": "9c025675-0124-4eac-8c9e-8137904dc409"}, "cell_type": "markdown", "source": ["I follow Miha Skalic guide to process data for tensorflow model. "]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "49a4f41a69e5bda25670c31cc1749df4af5dafa5", "_cell_guid": "52a8a273-443b-49ed-a65e-263f37ddab42"}, "source": ["x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n", "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n", "X_train = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\n", "y_train = np.array(train[\"is_iceberg\"], dtype = np.float32).reshape(train.shape[0],1)\n", "print(\"Xtrain:\", X_train.shape)\n", "\n", "# Test data\n", "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n", "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n", "X_test = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\n", "print(\"Xtest:\", X_test.shape)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "e464adc2bf561abfe36ae002505e13a604d62458", "_cell_guid": "00b6aa73-afe4-4200-ac33-7e7940e8e52a"}, "source": ["y_train.shape"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "3a5c93bfe89c2cb92cf56bbcc00f4439c753eae9", "_cell_guid": "be2eec8b-02e5-42e3-81f7-2c704fb1bb89"}, "source": ["nans = lambda df: df[df.isnull().any(axis=1)]\n", "\n", "nans(train).shape"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9dd627358337829e159def9b6f8fd5e22f638d2c", "_cell_guid": "4313f0cd-ad2c-4bb1-abfa-b38485540304"}, "source": ["learning_rate = 0.01\n", "epochs = 100\n", "batch_size = 128"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "cd8d3a701a18a3d06809dceea5a9ee5b7233f2f7", "_cell_guid": "ffacb9f6-2163-4e7e-ba54-8508d0f73ada"}, "source": ["x = tf.placeholder(tf.float32, [None, 75, 75, 2])\n", "y = tf.placeholder(tf.float32, [None, 1])"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5e1cb02f9959fc6f98b31412f300599dd47f64ff", "_cell_guid": "636d00f1-ca93-4ad1-b6f5-dea977438e6b"}, "source": ["def max_pool_2x2(x):\n", "    return tf.nn.max_pool(x, ksize = [1, 3, 3, 1], \n", "                         strides = [1, 2, 2, 1], padding = 'SAME')\n", "def conv_layer(input, shape):\n", "    W = weight_variable(shape)\n", "    b = bias_variable([shape[3]])\n", "    return tf.nn.relu(conv2d(input, W) + b)\n", "def full_layer(input, size):\n", "    in_size = int(input.get_shape()[1])\n", "    W = weight_variable([in_size, size])\n", "    b = bias_variable([size])\n", "    return tf.matmul(input, W) + b\n", "def weight_variable(shape):\n", "    initial = tf.truncated_normal(shape, stddev=0.1)\n", "    return tf.Variable(initial)\n", "def bias_variable(shape):\n", "    initial = tf.constant(0.1, shape=shape)\n", "    return tf.Variable(initial)\n", "def conv2d(x, W):\n", "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "791f98c2571fba838b55abbe9490e1f36f031112", "_cell_guid": "83cafd0e-72e3-4f23-aa4e-c246df1da736"}, "source": ["#x_ = tf.reshape(x, [-1, 75, 75, 2])\n", "\n", "conv1 = conv_layer(x, shape = [5, 5, 2, 32])\n", "conv1_pool = max_pool_2x2(conv1)\n", "\n", "conv2 = conv_layer(conv1_pool, shape = [5, 5, 32, 64])\n", "conv2_pool = max_pool_2x2(conv2)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "57e72795df5e348d829a036e7b884f10c31e4f65", "_cell_guid": "9577a59f-f938-462f-bd31-1d271d524226"}, "source": ["print(conv1.shape)\n", "print(conv1_pool.shape)\n", "print(conv2.shape)\n", "print(conv2_pool.shape)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ebd89dd27d95ada6aea2b23bd4a796334895c550", "_cell_guid": "39677465-7cd7-4c4c-bccb-579bc30d0a72"}, "source": ["conv2_flat = tf.reshape(conv2_pool, shape = [-1, 19*19*64])\n", "full_2 = tf.nn.relu(full_layer(conv2_flat, 1024))\n", "\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "53d71d7894f7195c9948b4d91e531669d16e5c38", "_cell_guid": "87017bf9-dd03-44a5-9a2f-151b67ae234c"}, "source": ["keep_prob = tf.placeholder(tf.float32)\n", "full2_drop = tf.nn.dropout(full_2, keep_prob = keep_prob)\n", "\n", "y_conv = full_layer(full2_drop, 1)\n", "\n", "log_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n", "    logits = y_conv,\n", "    labels =   y))"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c0d43733242bec7f9f9179cc7332e0c4059eeae9", "_cell_guid": "155be489-f10d-4cc0-96de-dcb15012282c"}, "source": ["train_step = tf.train.AdamOptimizer(1e-4).minimize(log_loss)\n", "correct_prediction = tf.equal(tf.argmax(y_conv, axis = 1), \n", "                              tf.argmax(y, axis = 1))"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "003ec8b090071436043fa3b841d28219a35bad6b", "_cell_guid": "de9ccaad-725c-4cf0-ad4b-c1816a9162bd"}, "source": ["accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0127f184db27c2fec0e5955196c467f252f999de", "_cell_guid": "b9e9f7b4-ab0c-4d73-8e7b-053b727abf0f"}, "source": ["y_train[0:10].dtype"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "6e20268ec5019e919aa7003b3a0a02cb2919f6e6", "_cell_guid": "e32600e3-b05f-4bde-91fc-999025f29eda"}, "source": ["with tf.Session() as sess:\n", "    sess.run(tf.global_variables_initializer())\n", "    for i in range(epochs):\n", "        for j in range(math.ceil(train.shape[0]/batch_size)):\n", "            if j == 12:\n", "                training = X_train[(train.shape[0] - 12*batch_size):train.shape[0]]\n", "                y_training = y_train[(train.shape[0] - 12*batch_size):train.shape[0]]\n", "            else:\n", "                training = X_train[(j*batch_size):((j+1)*batch_size)]\n", "                y_training = y_train[(j*batch_size):((j+1)*batch_size)]\n", "            if i % 5 == 0:\n", "                 #print(\"step {}, training accuracy {}\". format(i, \n", "                 #                                        train_accuracy))\n", "                print(sess.run(log_loss, feed_dict = {x: training,\n", "                                         y: y_training,\n", "                                             keep_prob: 1.0}))\n", "            sess.run(train_step, feed_dict = {x: training,\n", "                                         y: y_training,\n", "                                             keep_prob: 0.5})"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b98d71669a92e62403a96a7e67439b9427b4f6ee", "_cell_guid": "a0bba079-e102-46d4-ac1e-08aa4b6dbed9"}, "source": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}