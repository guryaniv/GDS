{"cells":[{"metadata":{"_uuid":"93e67ce62ff89567a107a8c7c43d92fb0c3c5669"},"cell_type":"markdown","source":"<h1><center><font size=\"6\">TalkingData AdTracking EDA</font></center></h1>\n\n\n<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/5340/logos/front_page.png\" width=\"400\"></img>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Load packages</a>  \n- <a href='#2'>Parameters</a>  \n- <a href='#3'>Read the data</a>  \n- <a href='#4'>Check the data</a>  \n    - <a href='#41'>Glimpse the data</a>  \n    - <a href='#42'>Check missing data</a>\n    - <a href='#43'>Check data unbalance</a>\n- <a href='#5'>Data exploration</a>\n- <a href='#6'>Data engineering</a>\n    - <a href='#61'>Extract date and time data</a>\n    - <a href='#62'>Additional data engineering</a>    \n    - <a href='#63'>Data unbalance between train and test data</a>\n- <a href='#7'>Model</a>  \n    - <a href='#71'>Prepare the model</a>  \n    - <a href='#71'>Train the model</a> \n- <a href='#8'>Predict and submission</a>\n"},{"metadata":{"_uuid":"60cceac6fe355d3939732ec7fec216c3fcd60af6"},"cell_type":"markdown","source":"# <a id=\"1\">Introduction</a>\n\nFraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fraudulent traffic.\nFounded in 2011, TalkingData is China’s largest independent big data service platform. \n\nThey handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.\nWhile successful, they wanted to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, they challenge participants to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support the modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!\n"},{"metadata":{"_uuid":"fa4f34bcb4e21902f1127e0857b58e0da38aee76"},"cell_type":"markdown","source":"# <a id=\"2\">Load packages</a>\n\nHere we load the libraries and set few parameters."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"62d21a85ecd4a44064a1dd7eeab07ea7f8c4d73a"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport lightgbm as lgb\nimport gc # memory \nfrom datetime import datetime # train time checking\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4245317acdf63b0a16a29daa412a8ee3d84e5d0a"},"cell_type":"markdown","source":"# <a id=\"3\">Parameters</a>\n\nHere we set most of the parameters used in this Notebook.\n\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4950dc5714b477736a2aa6507715925f185ee415"},"cell_type":"code","source":"#VALIDATION\nVALIDATE = False  #validation using train_test_split\nVALID_SIZE = 0.90 # simple validation using train_test_split\n\n#CROSS-VALIDATION\nVALIDATE_KFOLDS = True #cross-validation using KFolds\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\nSAMPLE = True    #True: use train.sample (100,000 rows) False: use full training set (train)\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50  #lgb early stop \nOPT_ROUNDS = 650  #To be adjusted based on best validation rounds\nskiprows = range(1,109903891) #\nnrows = 75000000\n#USE SAMPLE FROM FULL TRAIN SET\nSAMPLE_SIZE = 1 # use a subsample of the train set\noutput_filename = 'submission.csv'\n\nIS_LOCAL = False\n\n\nif (IS_LOCAL):\n    PATH = '../input/talkingdata-adtracking-fraud-detection/'\nelse:\n    PATH = '../input/'\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d73c3758ae57d728fa40a59f35b8ade6ee8a1abc"},"cell_type":"markdown","source":"# <a id=\"3\">Read the data</a>\n\nWe read the data with the options set in the **Parameters** section.\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"816690c2de4c8b8ca575807725b21796b0dd71ed"},"cell_type":"code","source":"dtypes = {\n        'ip'            : 'uint32',\n        'app'           : 'uint16',\n        'device'        : 'uint16',\n        'os'            : 'uint16',\n        'channel'       : 'uint16',\n        'is_attributed' : 'uint8',\n        'click_id'      : 'uint32'\n        }\n\ntrain_cols = ['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n\nif SAMPLE:\n    trainset = pd.read_csv(PATH+\"train_sample.csv\", dtype=dtypes, usecols=train_cols)    \nelse:\n    trainset = pd.read_csv(PATH+\"train.csv\", skiprows=skiprows, nrows=nrows,dtype=dtypes, usecols=train_cols)\n    trainset = trainset.sample(frac=SAMPLE_SIZE)\n\ntestset = pd.read_csv(PATH+\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce279a29465123be5c2ab40f400ec51295b628be"},"cell_type":"markdown","source":"# <a id=\"4\">Check the data</a>\n\n## <a id=\"41\">Glimpse the data</a>\n\nLet's check the train and test set structure.\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"150cc32a306de629ad748a80ee9aa5dd6837ec72"},"cell_type":"code","source":"print(\"train -  rows:\",trainset.shape[0],\" columns:\", trainset.shape[1])\nprint(\"test -  rows:\",testset.shape[0],\" columns:\", testset.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ab13a4e97327bc4d61d85be9f905b4971ba0037c"},"cell_type":"code","source":"trainset.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c4d6c6a0fd4f93f5446de70b2524610e65a0e322"},"cell_type":"code","source":"testset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3599ee3e5c50612ef854441da5dbdd6c9268da73"},"cell_type":"markdown","source":"The trainset columns are:\n* **ip**: ip address of click (numeric);\n* **app**: app id for marketing (category);\n* **device**: device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.) (category);\n* **os**: os version id of user mobile phone (category);\n* **channel**: channel id of mobile ad publisher (category);\n* **click_time**: timestamp of click (UTC) (time);\n* **attributed_time**: if user download the app for after clicking an ad, this is the time of the app download (time);\n* **is_attributed**: the target that is to be predicted, indicating the app was downloaded (binary);\n\nThe testset differs with three columns:\n* **click_id** reference for making predictions (additional column) (numeric);\n* **attributed_time**: missing column;\n* **is_attributed**: missing column (target column) (binary).\n\nLet's see what is the dimmension of the train and test set.\n\n"},{"metadata":{"_uuid":"b681450d619ba1b4d3d0481fa3b23697558f4990"},"cell_type":"markdown","source":"## <a id=\"42\">Check missing data</a>\n\nLet's check missing data in train and test.\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dc75a6baf9c48f8dd219aa6ada03e08570784834"},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2d02213c79dab569ee5fc6314bee0a9a64e62c"},"cell_type":"markdown","source":"Train data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c1fbdf854b23b039ef75bc870715b710de3c150"},"cell_type":"code","source":"missing_data(trainset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f40bb4896a875ae877cfdb318313d39b1c0a457"},"cell_type":"markdown","source":"Test data:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"08201c8b2840b295e0da2e4716b83052ccece02f"},"cell_type":"code","source":"missing_data(testset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98ac0055568d0aa8550be22d10d12ab50243939d"},"cell_type":"markdown","source":"## <a id=\"43\">Check data unbalance</a>\n\nWe will check the **target** (**is_attributed**) data unbalance. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8e6bc8c1463118a645ebd0201dfb4fcb12e9fc1e"},"cell_type":"code","source":"plt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\nx = trainset['is_attributed'].value_counts().index.values\ny = trainset[\"is_attributed\"].value_counts().values\n# Bar plot\n# Order the bars descending on target mean\nsns.barplot(ax=ax, x=x, y=y)\nplt.ylabel('Number of values', fontsize=12)\nplt.xlabel('is_attributed value', fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2459833313627f619b7ec95c7e849ace5faf0033"},"cell_type":"markdown","source":"Only a very small part of the **is_attributed** data have 1 value (227 out of 100,000 values, 176,627 out of 75 M values). This means that the training dataset is highly imbalanced (0.23% have **is_attributed** = 1). Usually is used either undersample the records with **is_attributed** = 0 or oversample records with **is_attributed** = 1; because is a large dataset, it is a good option to do undersampling of records with **is_attributed** = 0."},{"metadata":{"_uuid":"799a1118677be640c0d02fdece7903605a419df5"},"cell_type":"markdown","source":"# <a id=\"5\">Data exploration</a>"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9b116615b4be1c6ef01b9bd1e605d7e4ed1dc090"},"cell_type":"code","source":"trainset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840dbeb5b7a0db17f8f58d1958626713a2638810"},"cell_type":"markdown","source":"We can observe that, while ip is a number with a wide range of values, **app**, **device**, **os** and **channel** are categorical values with a smaller range of variation. Let's separate between values with **is_attributed** with value `1` and value `0`."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c048ed054738b9e85fb43deb5b6061768426ec4"},"cell_type":"code","source":"t1 = trainset.loc[trainset['is_attributed'] != 0.]\nt0 = trainset.loc[trainset['is_attributed'] == 0.]\n\nt0.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e5c380e95a909e76d0856ac53a6fe4e40ca51a6d"},"cell_type":"code","source":"t1.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"642e327e32a159c56891e456cb7ebc8e5cf8c793"},"cell_type":"markdown","source":"We can observe that in the two sets, **ip** and **os** and **channel** have values in the same ranges for both **is_attributed** values (0 and 1). Maximum values for **app** is almost double and maximum value for **device** is four times larger for **is_attributed** = 0.\n\nLet's visualize the distribution of values of **app**, **device**, **os** and **channel**, grouped on **is_attributed**."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0be9da78d069e6e48efe07c73163933fb2947c7d"},"cell_type":"code","source":"var = ['app','device','os','channel']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(1,4,figsize=(16,4))\n\nfor feature in var:\n    i += 1\n    plt.subplot(1,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c9e69dc7cbc546e1a0cc671b298e9d74681c6fc6"},"cell_type":"markdown","source":"# <a id=\"6\">Data engineering<a/>\n\n## <a id=\"61\">Extract date and time data</a>"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fd51a0b6cf5848c56034cdc5195bc83c4f61e43a"},"cell_type":"code","source":"trainset['year'] = pd.to_datetime(trainset.click_time).dt.year\ntrainset['month'] = pd.to_datetime(trainset.click_time).dt.month\ntrainset['day'] = pd.to_datetime(trainset.click_time).dt.day\ntrainset['hour'] = pd.to_datetime(trainset.click_time).dt.hour\ntrainset['min'] = pd.to_datetime(trainset.click_time).dt.minute\ntrainset['sec'] = pd.to_datetime(trainset.click_time).dt.second\ntrainset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4466f03a080fbeb1376f5755241a96622dabc392"},"cell_type":"code","source":"trainset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"71feefb247059812816b22dc4d1dfe3bfa9bbc60"},"cell_type":"code","source":"var = ['day','hour']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,4))\n\nfor feature in var:\n    i += 1\n    plt.subplot(1,2,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf237ddd3f24b6ab43b4c6b02d8f3f4905f850b6"},"cell_type":"markdown","source":"One can observe that the distribution for **true** (**is_attributed = 1**) clicks is more diverse (hour, min, sec) compared with **false** (**is_attributed = 0**). This might be explained in two ways: one explanation can be that due to the reduced number of **true** cases, the distribution is less uniform. Another explanation might be (to be verified with larger number of cases) that due to programatic nature of *artificial* (**false**) clicks, their distribution is more uniform. One observation, related to the density plot for hours: for both **true** and **false** there is a certain hourly profile, with a plateau between 1 and 16, a saddle between 16 and 20 and a peak between 21 and 22. The plateau for the artificial (**false**) clicks shows an additional pattern, with oscilant profile."},{"metadata":{"_uuid":"e0127085ac2f33c3d94ee10a6a4bcc843f532491"},"cell_type":"markdown","source":"Let's represent the hour distribution with an alternative method, using barplots to show percent from all data of the **true** data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0b397efdbb0e3814683395b57d27458473a33403"},"cell_type":"code","source":"var = ['day','hour']\n\nfor feature in var:\n    fig, ax = plt.subplots(figsize=(16,6))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = trainset[[feature, 'is_attributed']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='is_attributed', ascending=False, inplace=True)\n    # Bar plot\n    #sns.barplot(ax=ax,x=feature, y='is_attributed', data=cat_perc, order=cat_perc[feature]) #for ordered bars\n    sns.barplot(ax=ax,x=feature, y='is_attributed', data=cat_perc)\n    plt.ylabel('Percent of `is_attributed` with value 1 [%]', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abf89285f0993a96f79810989fcb15dafcfc69bd"},"cell_type":"markdown","source":"## <a id=\"62\">Additional feature engineering</a>\n\nDefine several functions for calculation of derived features."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ccdb1cdb92a2d4a7ba1f94b215f7eba45f987267"},"cell_type":"code","source":"#------------------------------------------------------------------------------\ndef show_max_clean(df,gp,agg_name,agg_type,show_max):\n#------------------------------------------------------------------------------    \n    del gp\n    if show_max:\n        print( agg_name + \" max value = \", df[agg_name].max() )\n    df[agg_name] = df[agg_name].astype(agg_type)\n    gc.collect()\n    return( df )\n    \n#------------------------------------------------------------------------------\ndef perform_count( df, group_cols, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n#------------------------------------------------------------------------------\n    if show_agg:\n        print( \"Aggregating by \", group_cols , '...' )\n    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n    df = df.merge(gp, on=group_cols, how='left')\n    return (show_max_clean(df,gp,agg_name,agg_type,show_max))\n\n#------------------------------------------------------------------------------\ndef perform_countuniq( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n#------------------------------------------------------------------------------    \n    if show_agg:\n        print( \"Counting unique \", counted, \" by \", group_cols , '...' )\n    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    return (show_max_clean(df,gp,agg_name,agg_type,show_max))\n\n#------------------------------------------------------------------------------    \ndef perform_cumcount( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n#------------------------------------------------------------------------------    \n    if show_agg:\n        print( \"Cumulative count by \", group_cols , '...' )\n    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n    df[agg_name]=gp.values\n    return (show_max_clean(df,gp,agg_name,agg_type,show_max))\n\n#------------------------------------------------------------------------------\ndef perform_mean( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n#------------------------------------------------------------------------------    \n    if show_agg:\n        print( \"Calculating mean of \", counted, \" by \", group_cols , '...' )\n    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    return (show_max_clean(df,gp,agg_name,agg_type,show_max))\n\n#------------------------------------------------------------------------------\ndef perform_var( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n#------------------------------------------------------------------------------    \n    if show_agg:\n        print( \"Calculating variance of \", counted, \" by \", group_cols , '...' )\n    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n    df = df.merge(gp, on=group_cols, how='left')\n    return (show_max_clean(df,gp,agg_name,agg_type,show_max))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f005fbe4f880f10c9e7be16c6d74dc52fe3b43a4"},"cell_type":"markdown","source":"Now we call the routines for additional features calculation."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b94fbe9b15727684a711903fde6c68c0a39f988a"},"cell_type":"code","source":"trainset = perform_countuniq( trainset, ['ip'], 'channel', 'X0', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_cumcount( trainset, ['ip', 'device', 'os'], 'app', 'X1', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip'], 'app', 'X3', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip'], 'device', 'X5', 'uint16', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['app'], 'channel', 'X6', show_max=True ); gc.collect()\ntrainset = perform_cumcount( trainset, ['ip'], 'os', 'X7', show_max=True ); gc.collect()\ntrainset = perform_countuniq( trainset, ['ip', 'device', 'os'], 'app', 'X8', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'day', 'hour'], 'ip_tcount', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'app'], 'ip_app_count', show_max=True ); gc.collect()\ntrainset = perform_count( trainset, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=True ); gc.collect()\ntrainset = perform_var( trainset, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=True ); gc.collect()\ntrainset = perform_mean( trainset, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=True ); gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5b86a4a1985018312fe9a00f338ed0a2128a97b2"},"cell_type":"code","source":"trainset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"62a9076002d2f175d8ca10f4d529a4044e74a874"},"cell_type":"code","source":"var = ['X0','X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\n\nfor feature in var:\n    i += 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cb9e079fb9d0c7f152ae18652d52992e61d4c11e"},"cell_type":"code","source":"var = ['X8', 'ip_tcount', 'ip_app_count','ip_app_os_count', \n        'ip_tchan_count','ip_app_os_var','ip_app_channel_var_day', 'ip_app_channel_mean_hour']\n\ni = 0\nt1 = trainset.loc[trainset['is_attributed'] != 0]\nt0 = trainset.loc[trainset['is_attributed'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\n\nfor feature in var:\n    i += 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"is_attributed = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"is_attributed = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ae9022b26cada269fa4b1498217ad7526c84463"},"cell_type":"markdown","source":"## <a id=\"63\">Data unbalance between train and test data</a>\n\nLet's compare the distribution of the features in the train and test datasets.\n\n\nLet's start with the data about application, devide, operating system and channel."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3b91daa58d43c032b8378755bde003c4583953ca"},"cell_type":"code","source":"var = ['app','device','os','channel']\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,4,figsize=(16,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a94614a50b878ac815b7c03bd2ce6e05823a04af"},"cell_type":"markdown","source":"Let's continue with the data on time. First we will have to do the date and time extraction operation on testset data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b0525a4413a5be81f15fa91eb101ce5c7fa47310"},"cell_type":"code","source":"testset['year'] = pd.to_datetime(testset.click_time).dt.year\ntestset['month'] = pd.to_datetime(testset.click_time).dt.month\ntestset['day'] = pd.to_datetime(testset.click_time).dt.day\ntestset['hour'] = pd.to_datetime(testset.click_time).dt.hour\ntestset['min'] = pd.to_datetime(testset.click_time).dt.minute\ntestset['sec'] = pd.to_datetime(testset.click_time).dt.second\ntestset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b3128d42b1ee295e6ee2885e724a3ae7a89970f0"},"cell_type":"code","source":"testset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1228442bd394884bc539d3b52d9d95addabd8195"},"cell_type":"code","source":"var = ['day','hour']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,2,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23103040ce0c1b31717f74f05deb4f5a87203d6d"},"cell_type":"markdown","source":"We can see that the day for the train data is for 6-9 November 2017, the test data is only from 10 November 2017. As well, the hours distribution are very different between train and test data.\n"},{"metadata":{"_uuid":"7efc1abca3d257b864962762b0e044bac348c167"},"cell_type":"markdown","source":"Let's calculate the derived features as well for the test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c2c3605fbbf05146989db0a76377bbabec9e0490"},"cell_type":"code","source":"testset = perform_countuniq( testset, ['ip'], 'channel', 'X0', 'uint8', show_max=True ); gc.collect()\ntestset = perform_cumcount( testset, ['ip', 'device', 'os'], 'app', 'X1', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip'], 'app', 'X3', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip'], 'device', 'X5', 'uint16', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['app'], 'channel', 'X6', show_max=True ); gc.collect()\ntestset = perform_cumcount( testset, ['ip'], 'os', 'X7', show_max=True ); gc.collect()\ntestset = perform_countuniq( testset, ['ip', 'device', 'os'], 'app', 'X8', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'day', 'hour'], 'ip_tcount', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'app'], 'ip_app_count', show_max=True ); gc.collect()\ntestset = perform_count( testset, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=True ); gc.collect()\ntestset = perform_var( testset, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=True ); gc.collect()\ntestset = perform_mean( testset, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=True ); gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"45269819206437552cfbe565454e2750dd721086"},"cell_type":"code","source":"var = ['X0','X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7d92894277815e31d3413d7820a85b0f934bd750"},"cell_type":"code","source":"var = ['X8', 'ip_tcount', 'ip_app_count','ip_app_os_count', \n       'ip_tchan_count', 'ip_app_os_var','ip_app_channel_var_day', 'ip_app_channel_mean_hour']\n\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(2,4,figsize=(14,8))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(2,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"103220baab3e782cc87fc20d29b5e16a29ca87e8"},"cell_type":"markdown","source":"# <a id=\"7\">Model</a>"},{"metadata":{"_uuid":"36b519d04b1beeb5107782e619626f10ee279cee"},"cell_type":"markdown","source":"## <a id=\"71\">Prepare the model</a>"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"99b720dc2d2eda59462123b5799376b67c768d04"},"cell_type":"code","source":"start = datetime.now()\n\nlen_train = len(trainset)\ngc.collect()\n\nmost_freq_hours_in_test_data = [4, 5, 9, 10, 13, 14]\nleast_freq_hours_in_test_data = [6, 11, 15]\n\ndef prep_data( df ):\n    \n    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('uint8')\n    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('uint8')\n    df.drop(['click_time'], axis=1, inplace=True)\n    gc.collect()\n    \n    df['in_test_hh'] = (   3 \n                         - 2*df['hour'].isin(  most_freq_hours_in_test_data ) \n                         - 1*df['hour'].isin( least_freq_hours_in_test_data ) ).astype('uint8')\n    gp = df[['ip', 'day', 'in_test_hh', 'channel']].groupby(by=['ip', 'day', 'in_test_hh'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_test_hh'})\n    df = df.merge(gp, on=['ip','day','in_test_hh'], how='left')\n    df.drop(['in_test_hh'], axis=1, inplace=True)\n    df['nip_day_test_hh'] = df['nip_day_test_hh'].astype('uint32')\n    del gp\n    gc.collect()\n\n    gp = df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_hh'})\n    df = df.merge(gp, on=['ip','day','hour'], how='left')\n    df['nip_day_hh'] = df['nip_day_hh'].astype('uint16')\n    del gp\n    gc.collect()\n    \n    gp = df[['ip', 'os', 'hour', 'channel']].groupby(by=['ip', 'os', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_os'})\n    df = df.merge(gp, on=['ip','os','hour'], how='left')\n    df['nip_hh_os'] = df['nip_hh_os'].astype('uint16')\n    del gp\n    gc.collect()\n\n    gp = df[['ip', 'app', 'hour', 'channel']].groupby(by=['ip', 'app',  'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_app'})\n    df = df.merge(gp, on=['ip','app','hour'], how='left')\n    df['nip_hh_app'] = df['nip_hh_app'].astype('uint16')\n    del gp\n    gc.collect()\n\n    gp = df[['ip', 'device', 'hour', 'channel']].groupby(by=['ip', 'device', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_dev'})\n    df = df.merge(gp, on=['ip','device','hour'], how='left')\n    df['nip_hh_dev'] = df['nip_hh_dev'].astype('uint32')\n    del gp\n    gc.collect()\n\n    df.drop( ['ip','day'], axis=1, inplace=True )\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"47e318c59bde27fd165214fb348042c70f1eb125"},"cell_type":"code","source":"trainset = prep_data(trainset)\ngc.collect()\n\nparams = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.1,\n          'num_leaves': 9,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 5,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':99.7, # because training data is extremely unbalanced \n         }\n\ntarget = 'is_attributed'\npredictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh', 'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\ncategorical = ['app', 'device', 'os', 'channel', 'hour']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ac4a1ccae16db39e826057fe5b2837996fa67b4"},"cell_type":"markdown","source":"## <a id=\"72\">Train the model</a>"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"87c106998dad841f2af68a2c63b67d422be0ed67"},"cell_type":"code","source":"if VALIDATE:\n\n    train_df, val_df = train_test_split(trainset, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )\n    \n    dtrain = lgb.Dataset(train_df[predictors].values, \n                         label=train_df[target].values,\n                         feature_name=predictors,\n                         categorical_feature=categorical)\n    del train_df\n    gc.collect()\n\n    dvalid = lgb.Dataset(val_df[predictors].values,\n                         label=val_df[target].values,\n                         feature_name=predictors,\n                         categorical_feature=categorical)\n    del val_df\n    gc.collect()\n\n    evals_results = {}\n\n    model = lgb.train(params, \n                      dtrain, \n                      valid_sets=[dtrain, dvalid], \n                      valid_names=['train','valid'], \n                      evals_result=evals_results, \n                      num_boost_round=MAX_ROUNDS,\n                      early_stopping_rounds=EARLY_STOP,\n                      verbose_eval=50, \n                      feval=None)\n\n    del dvalid\n    \nelif VALIDATE_KFOLDS:\n    kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\n    for train_index, test_index in kf.split(trainset):\n        train_X, valid_X = trainset.iloc[train_index], trainset.iloc[test_index]\n\n        dtrain = lgb.Dataset(train_X[predictors].values, label=train_X[target].values,\n                         feature_name=predictors, categorical_feature=categorical)\n   \n        dvalid = lgb.Dataset(valid_X[predictors].values, label=valid_X[target].values,\n                         feature_name=predictors, categorical_feature=categorical)\n    \n        evals_results = {}\n        model =  lgb.train(params, \n                      dtrain, \n                      valid_sets=[dtrain, dvalid], \n                      valid_names=['train','valid'], \n                      evals_result=evals_results, \n                      num_boost_round=MAX_ROUNDS,\n                      early_stopping_rounds=EARLY_STOP,\n                      verbose_eval=50, \n                      feval=None)\n    \nelse:\n\n    gc.collect()\n    dtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n                          feature_name=predictors,\n                          categorical_feature=categorical\n                          )\n    del train_df\n    gc.collect()\n\n    evals_results = {}\n\n    model = lgb.train(params, \n                      dtrain, \n                      valid_sets=[dtrain], \n                      valid_names=['train'], \n                      evals_result=evals_results, \n                      num_boost_round=OPT_ROUNDS,\n                      verbose_eval=50,\n                      feval=None)\n    \ndel dtrain\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c25d84fe92ed3454b8f350ef7b277bda109d362"},"cell_type":"markdown","source":"# <a id=\"8\">Prediction and submission</a>"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4155c7903e344a0597f376c867920e03e006e7e9"},"cell_type":"code","source":"test_cols = ['ip','app','device','os', 'channel', 'click_time', 'click_id']\n\ntest_df = prep_data(testset)\ngc.collect()\n\nsub = pd.DataFrame()\nsub['click_id'] = test_df['click_id']\nsub['is_attributed'] = model.predict(test_df[predictors])\nsub.to_csv(output_filename, index=False, float_format='%.9f')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}