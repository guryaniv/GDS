{"cells": [{"metadata": {"_uuid": "bbbbeb2e373c375d30c13c46cef786095b9ddc21", "_cell_guid": "abbefc28-c4c1-437f-ae45-192be39bece2"}, "outputs": [], "cell_type": "markdown", "execution_count": null, "source": "## Zillow Prize: Zillow's Home Value Prediction (Zestimate) [Link](https://www.kaggle.com/c/zillow-prize-1)\n\n### 0. Objective\n\n- find out the fittest model to predict the future's house price"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5b918a3a8cc377258002745d8c3abc9dfa33a433", "trusted": false, "_cell_guid": "0133a18c-5872-4d88-b4d4-c5a5fad346c0"}, "source": "#import every library needed for completing task\n\nimport operator\nimport gc\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils import shuffle\nimport xgboost as xgb\nimport lightgbm as lgb"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "69aefd0575fae7e9beffccaa5de9ed3ff27b43d1", "trusted": false, "_cell_guid": "04405bd5-bfe1-4b1b-8842-65d1528ab103"}, "source": "# make the functions which is needed\n\ndef regression_stats(x, y):\n    mask = ~np.isnan(x) & ~np.isnan(y)\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n    pearsonr = stats.pearsonr(x[mask], y[mask])\n    result = {\n        'slope': slope, 'intercept': intercept,\n        'r_value': r_value, 'p_value': p_value,\n        'std_err': std_err, 'r_squared': r_value ** 2,\n        'pearsonr': pearsonr[0]\n    }\n    return result"}, {"metadata": {"_uuid": "11be45ed316be1fadaeafa2f0cd78b89f6a33f39", "_cell_guid": "a1a91df0-f78e-4cb4-8ef3-b42959e58b9d"}, "outputs": [], "cell_type": "markdown", "execution_count": null, "source": "### 1. Acquire and explore the dataset\n\n- look over the dataset's characteristics\n- check out how dirty the dataset is"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "572d3acdd73f5f18a98e31511ea2a7647826980a", "trusted": false, "_cell_guid": "2fcc96f5-df7f-4e5b-8789-900df88e7b36"}, "source": "# Acquire the dataset\n\nsold_result = pd.read_csv('../input/train_2016_v2.csv')\nprop = pd.read_csv('../input/properties_2016.csv', low_memory=False)\n\nsold_result.shape, prop.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "61125055167c8f1e7a8ac5c14259df66f79e7074", "trusted": false, "_cell_guid": "8433483e-af75-43c6-b859-e0f359a5927d"}, "source": "# change the dtypes of dataframes for using less memory\n\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\n        prop[c] = prop[c].astype(np.float32)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "37ab6b8cf8fb0534d51b251375094c51f89bbcd9", "trusted": false, "_cell_guid": "ff277ad7-e19d-4611-be2a-75f229706c75"}, "source": "# train_result parcelid is not unique. I will make the meidan of it and will use the median as my training data\n\nsold_result.parcelid.nunique(), prop.parcelid.nunique()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "7b8be348c8bf3b142ecb4fee2022310ac2c99975", "trusted": false, "_cell_guid": "040d03a6-c4bd-45ca-a4a0-1dc76b0778fb"}, "source": "edited_sold_result = sold_result[['parcelid', 'logerror']].groupby('parcelid').agg(['median'])\nedited_sold_result.columns = ['logerror']\nedited_sold_result = edited_sold_result.reset_index()\nedited_sold_result.head()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "72e1ac56a49f7725ed809b6d5e152eb6f203ef4a", "trusted": false, "_cell_guid": "0ecee14b-c06a-4728-8dea-cce9ed33274c"}, "source": "# make the final dataset to split into train dataset and test dataset\n\nfinal_dataset = pd.merge(prop, edited_sold_result, how=\"left\", on=\"parcelid\")\nfinal_dataset.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f17b975834ee3a1e4728c9e6b99c5fb69880970e", "trusted": false, "_cell_guid": "a112baa6-b8f8-43e5-95ab-b089cf542798"}, "source": "X_train  = final_dataset.loc[~final_dataset.logerror.isnull()]\ny_train = final_dataset.loc[~final_dataset.logerror.isnull()].logerror.values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "_uuid": "37100b96d5e943d9c347a36c402909fc94cb4176", "trusted": false, "_cell_guid": "a6982472-b13a-40d6-9289-0abbc4182f35"}, "source": "X_train.shape, y_train.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "fb2189430f94b19589f33c4e7761323fa3f0958c", "trusted": false, "_cell_guid": "16607d39-a440-47af-9b0d-a32217fc5a7f"}, "source": "X_test = final_dataset.loc[final_dataset.logerror.isnull()]\nX_test.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f8aa17d8143c21649fbb5060a408e0ca4490c35b", "trusted": false, "_cell_guid": "91b8ac94-6170-4089-89a2-c89bdab5cafd"}, "source": "# make list of X_train, X_test to make it easier to do pre-processing the dataset laster\n\ncombine = [X_train, X_test]"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "cd3a47f7651952b0677fa5d9fda54460e83def48", "trusted": false, "_cell_guid": "dd381a09-4105-405e-8d0d-de8cd11cbe0b"}, "source": "pd.set_option('max_columns', 59)\n\nX_train.tail()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "_uuid": "924d5aab95cfaf8a6068c5ef863f4f4215d78b7e", "trusted": false, "_cell_guid": "25090fc6-0a9a-40ef-b900-543d54e4e5d9"}, "source": "# start to explore the dataset. For example, what kind of columns the dataset has, what is the categorical features. the ordinal features. or the numeric values.\npd.set_option('max_columns', 54)\n\nX_train.describe()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "fbbcfcca08e3e6229e287678e1f58135affb8089", "trusted": false, "_cell_guid": "2dc6b3ea-5b28-462b-8794-84006f20459c"}, "source": "# check out how much correlation there is between numeric values and the result (logerror)\n# first of all, find out which columns are numeric values \n\nnumeric_columns = X_train.select_dtypes(include=[np.number]).columns[1:-1]\nnumeric_columns"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "7d80d9ca2d34434b063dc9bb5a4aa7bda3216e7a", "trusted": false, "_cell_guid": "ae604b98-9c59-4d4d-a4a0-aa2057c68402"}, "source": "# keep the p-value of each columns and find out which column has good result\np_value = {}\n\nfor columns in numeric_columns:\n    stats_result = regression_stats(X_train[columns], X_train['logerror']).get('p_value')\n    p_value[columns] = stats_result"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": false, "_uuid": "844a1fc74725b790004f070b16912029669d19f0", "trusted": false, "_cell_guid": "bdfe1e8f-4cee-4374-9398-5b5601c2c321"}, "source": "# sorting the p_value result of each columns\nsorted_p_value = sorted(p_value.items(), key=operator.itemgetter(1))\n\n# plot the graph with columns which has the highest p_value in the columns\nfor item in sorted_p_value[:5]:\n    sns.jointplot(X_train[item[0]], X_train['logerror'], kind='reg')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "86642dc9e70532cd40ad1c83d1c30c81b80c15e4", "trusted": false, "_cell_guid": "60e932e4-8307-4538-8944-43ff24013768"}, "source": "# gather the columns which has no correlation between the column and the logerror and delete the columns later for better modeling\n\ndeleting_cols = [col for col, value in p_value.items() if value > 0.05]\n\ndeleting_cols"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a19e6347c347f26c4e48b979bd04e5829f929585", "trusted": false, "_cell_guid": "0063a2cf-f0a0-4614-a2eb-a07741452748"}, "source": "X_train.shape, X_test.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "c2f4c1c866ff59f6d4c00daadb0a27cf7be11d04", "trusted": false, "_cell_guid": "f20247bb-6217-4108-a303-bbde50e235f2"}, "source": "# fill a few missing value first  to see the correlationship between the categorical columns and the logerror\n\nfor dataset in combine:\n    dataset['hashottuborspa'].fillna('False', inplace=True)\n    dataset['propertycountylandusecode'].fillna('0100', inplace=True)\n    dataset['propertyzoningdesc'].fillna('LAR1', inplace=True)\n    dataset['taxdelinquencyflag'].fillna('N', inplace=True)\n    dataset['fireplaceflag'].fillna('False', inplace=True)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "7a88be7496a11b6d842bdfe6d4d484dc3f834b4b", "trusted": false, "_cell_guid": "f2e72f94-6df4-4f1f-a5ae-5ee2d333cfd3"}, "source": "true_std = X_train[X_train.hashottuborspa==True].logerror.std()\ntrue_mean = X_train[X_train.hashottuborspa==True].logerror.mean()\nfalse_std = X_train[X_train.hashottuborspa=='False'].logerror.std()\nfalse_mean = X_train[X_train.hashottuborspa=='False'].logerror.mean()\n\ngrid = sns.FacetGrid(X_train, col='hashottuborspa')\ngrid_map = grid.map(sns.kdeplot, 'logerror', shade=True)\naxes = grid_map.axes\naxes[0, 0].set_xlim(-0.5, 0.5)\naxes[0, 1].set_xlim(-0.5, 0.5)\nprint(f'true_std: {true_std}, true_mean: {true_mean} \\nfalse_std: {false_std}, false_mean: {false_mean}')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "bbb069b9acb824c6d9a994f2d0b089ec356d4eaf", "trusted": false, "_cell_guid": "e274ca85-fec7-49f3-a1f5-7059b8cee810"}, "source": "true_std = X_train[X_train.taxdelinquencyflag=='Y'].logerror.std()\ntrue_mean = X_train[X_train.taxdelinquencyflag=='Y'].logerror.mean()\nfalse_std = X_train[X_train.taxdelinquencyflag=='N'].logerror.std()\nfalse_mean = X_train[X_train.taxdelinquencyflag=='N'].logerror.mean()\n\n\ngrid = sns.FacetGrid(X_train, col='taxdelinquencyflag')\ngrid_map = grid.map(sns.kdeplot, 'logerror', shade=True)\naxes = grid_map.axes\naxes[0, 0].set_xlim(-0.5, 0.5)\naxes[0, 1].set_xlim(-0.5, 0.5)\nprint(f'true_std: {true_std}, true_mean: {true_mean} \\nfalse_std: {false_std}, false_mean: {false_mean}')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "8cb4de8df1feb0f5542030b152ec23427414a881", "trusted": false, "_cell_guid": "15b78951-1b14-441f-91d7-54f7b9d3d34e"}, "source": "true_std = X_train[X_train.fireplaceflag==True].logerror.std()\ntrue_mean = X_train[X_train.fireplaceflag==True].logerror.mean()\nfalse_std = X_train[X_train.fireplaceflag=='False'].logerror.std()\nfalse_mean = X_train[X_train.fireplaceflag=='False'].logerror.mean()\n\n\ngrid = sns.FacetGrid(X_train, col='fireplaceflag')\ngrid_map = grid.map(sns.kdeplot, 'logerror', shade=True)\naxes = grid_map.axes\naxes[0, 0].set_xlim(-0.5, 0.5)\naxes[0, 1].set_xlim(-0.5, 0.5)\nprint(f'true_std: {true_std}, true_mean: {true_mean} \\nfalse_std: {false_std}, false_mean: {false_mean}')"}, {"metadata": {"_uuid": "4bc815f2d163493817492036eb4a1d6be13b9331", "_cell_guid": "fa984fa3-6287-4292-93d3-c95ad3616220"}, "outputs": [], "cell_type": "markdown", "execution_count": null, "source": "### 2. preprocessing dataset for modeling\n\n- check how many Nan values each columns have and fill the missing value for better prediction\n- change the string features to ordinal features for using the values to model"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0caa343a17b179aa1b58f7e0505e4ed0f8a2bb27", "trusted": false, "_cell_guid": "1806fb02-077b-475e-a6ce-abd550d8132f"}, "source": "deleting_cols"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "cafd07cfb2eed7c65c670acee38ebd5b5e9302b8", "trusted": false, "_cell_guid": "ff34bf45-6fd2-4abb-ad60-160bf25b01b7"}, "source": "# first of all, delete the columns which is proved not to be correlated with the logerror\n\nfor dataset in combine:\n    dataset = dataset.drop(deleting_cols, axis=1)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f046f06898b78f999c3102d537c8d16d4d2f5793", "trusted": false, "_cell_guid": "1ca9a9f9-dbd0-4810-bf78-e27d66e0c25d"}, "source": "# check how many Nan values in X_train, X_test\n\ntrain_columns_values_cnt = X_train.isnull().sum(axis=0).reset_index()\ntrain_columns_values_cnt.columns = ['col_name', 'nan_count']\ntrain_columns_values_cnt['nan_ratio'] = train_columns_values_cnt['nan_count'] / X_train.shape[0]\ntrain_columns_values_cnt = train_columns_values_cnt.sort_values('nan_count', ascending=False)\ntest_columns_values_cnt = X_test.isnull().sum(axis=0).reset_index()\ntest_columns_values_cnt.columns = ['col_name', 'nan_count']\ntest_columns_values_cnt['nan_ratio'] = test_columns_values_cnt['nan_count'] / X_test.shape[0]\ntest_columns_values_cnt = test_columns_values_cnt.sort_values('nan_count', ascending=False)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "7d03d970ef11261c53eeacfdf9fa9cc23a6a16e4", "trusted": false, "_cell_guid": "1ce00bfa-62b4-4991-8221-15d284cda0ce"}, "source": "# delete the columns of which the missing values ratio is more than 0.6\ntrain_deleting_col = train_columns_values_cnt.loc[train_columns_values_cnt.nan_ratio > 0.6]['col_name'].values\ntest_deleting_col = test_columns_values_cnt.loc[test_columns_values_cnt.nan_ratio > 0.6]['col_name'].values\n\nX_train = X_train.drop(train_deleting_col, axis=1)\nX_test = X_test.drop(test_deleting_col, axis=1)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "34e6ad856f54464974c4f2070b315de4842d9269", "trusted": false, "_cell_guid": "9f8c97a0-6217-4dd6-8ad9-c901c1a2a70b"}, "source": "# check how many Nan values in X_train, X_test\n\ntrain_columns_values_cnt = X_train.isnull().sum(axis=0).reset_index()\ntrain_columns_values_cnt.columns = ['col_name', 'nan_count']\ntrain_columns_values_cnt['nan_ratio'] = train_columns_values_cnt['nan_count'] / X_train.shape[0]\ntrain_columns_values_cnt = train_columns_values_cnt.sort_values('nan_count', ascending=False)\ntest_columns_values_cnt = X_test.isnull().sum(axis=0).reset_index()\ntest_columns_values_cnt.columns = ['col_name', 'nan_count']\ntest_columns_values_cnt['nan_ratio'] = test_columns_values_cnt['nan_count'] / X_test.shape[0]\ntest_columns_values_cnt = test_columns_values_cnt.sort_values('nan_count', ascending=False)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "d3086506c839bbee45f3ebf3b8ec69b877ab1d17", "trusted": false, "_cell_guid": "44a2f65f-375b-4aa5-9913-0a102d534dd0"}, "source": "fig, ax = plt.subplots(ncols=2, figsize=(30, 20))\nax[0].set_title(\"train data nan count\")\nax[1].set_title(\"test data nan count\")\ntrain_columns_values_cnt = train_columns_values_cnt.loc[train_columns_values_cnt.nan_ratio > 0]\ntest_columns_values_cnt = test_columns_values_cnt.loc[test_columns_values_cnt.nan_ratio > 0]\nsns.barplot(train_columns_values_cnt.nan_ratio, train_columns_values_cnt.col_name, ax=ax[0])\nsns.barplot(test_columns_values_cnt.nan_ratio, test_columns_values_cnt.col_name, ax=ax[1])"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1e3ac75f318a9c0179b62415b5644b004f3e9770", "trusted": false, "_cell_guid": "838385ba-7cb2-495f-af22-11345a98408a"}, "source": "# the columns which have not that high Nan ratio will filled with using scikit-learn imputer\n\ntrain_imputing_col = train_columns_values_cnt.sort_values('nan_ratio', ascending=False)[3:].col_name.values\ntest_imputing_col = test_columns_values_cnt.sort_values('nan_ratio', ascending=False)[3:].col_name.values\n\ntrain_imputing_col, test_imputing_col\n\nfor col in train_imputing_col:\n    if X_train[col].dtype == np.number:\n        X_train[col].fillna(X_train[col].median(), inplace=True)\n    else:\n        X_train[col].fillna(X_train[col].mode()[0], inplace=True)\n        \nfor col in test_imputing_col:\n    if X_test[col].dtype == np.number:\n        X_test[col].fillna(X_test[col].median(), inplace=True)\n    else:\n        X_test[col].fillna(X_test[col].mode()[0], inplace=True)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "4193345c1195f06e3210f20e6ba12f9f3af4b5a4", "trusted": false, "_cell_guid": "5304fcae-6c56-402e-94d3-4141d463aef4"}, "source": "# check how many Nan values in X_train, X_test\n\ntrain_columns_values_cnt = X_train.isnull().sum(axis=0).reset_index()\ntrain_columns_values_cnt.columns = ['col_name', 'nan_count']\ntrain_columns_values_cnt['nan_ratio'] = train_columns_values_cnt['nan_count'] / X_train.shape[0]\ntrain_columns_values_cnt = train_columns_values_cnt.sort_values('nan_count', ascending=False)\ntest_columns_values_cnt = X_test.isnull().sum(axis=0).reset_index()\ntest_columns_values_cnt.columns = ['col_name', 'nan_count']\ntest_columns_values_cnt['nan_ratio'] = test_columns_values_cnt['nan_count'] / X_test.shape[0]\ntest_columns_values_cnt = test_columns_values_cnt.sort_values('nan_count', ascending=False)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "d6c448313c4b359586c5c7503ae193298afc2421", "trusted": false, "_cell_guid": "9afef595-ac3b-462a-9457-3531356d6456"}, "source": "fig, ax = plt.subplots(ncols=2, figsize=(30, 20))\nax[0].set_title(\"train data nan count\")\nax[1].set_title(\"test data nan count\")\ntrain_columns_values_cnt = train_columns_values_cnt.loc[train_columns_values_cnt.nan_ratio > 0]\ntest_columns_values_cnt = test_columns_values_cnt.loc[test_columns_values_cnt.nan_ratio > 0]\nsns.barplot(train_columns_values_cnt.nan_ratio, train_columns_values_cnt.col_name, estimator=np.sum, ax=ax[0])\nsns.barplot(test_columns_values_cnt.nan_ratio, test_columns_values_cnt.col_name, estimator=np.sum, ax=ax[1])"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "94c80cba54bcf32e4cdc06f87834281eca78d7bb", "trusted": false, "_cell_guid": "d7e3d4e7-bdc9-4a55-8c87-3f7dbc2a2cb2"}, "source": "# change the categorical values to ordinal values (my functions didnt't work properly. I didn't know why. I just did manually....)\n\nX_train['hashottuborspa'] = X_train['hashottuborspa'].astype('category').cat.codes\nX_train['propertycountylandusecode'] = X_train['propertycountylandusecode'].astype('category').cat.codes\nX_train['propertyzoningdesc'] = X_train['propertyzoningdesc'].astype('category').cat.codes\nX_train['fireplaceflag'] = X_train['fireplaceflag'].astype('category').cat.codes\nX_train['taxdelinquencyflag'] = X_train['taxdelinquencyflag'].astype('category').cat.codes\n\nX_test['hashottuborspa'] = X_test['hashottuborspa'].astype('category').cat.codes\nX_test['propertycountylandusecode'] = X_test['propertycountylandusecode'].astype('category').cat.codes\nX_test['propertyzoningdesc'] = X_test['propertyzoningdesc'].astype('category').cat.codes\nX_test['fireplaceflag'] = X_test['fireplaceflag'].astype('category').cat.codes\nX_test['taxdelinquencyflag'] = X_test['taxdelinquencyflag'].astype('category').cat.codes"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "_uuid": "327a56165c4fb6a7f9e12944da6cdc9dff9aa305", "trusted": false, "_cell_guid": "40ee1c81-7d66-4830-8156-77e75a7aa596"}, "source": "# In terms of nan values ratio, top 3 columns will be imputed by predicting Random Forest\n\nimputing_col = train_columns_values_cnt.sort_values('nan_ratio', ascending=False)[:3].col_name.values\nimputing_col"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "f4e6e6365c3f4d674d34cd1119f68153b3bbb184", "trusted": false, "_cell_guid": "727f2b61-6323-4997-8ebe-a89a269eae08"}, "source": "# make dataset for using imputing\n\nimp_dataset = pd.concat([X_train, X_test])\nimp_dataset = imp_dataset.drop('logerror', axis=1)\nimp_dataset.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "64ba461aec42d628f46cf89f3be23f7d7d23430d", "trusted": false, "_cell_guid": "de2700a5-8b64-434d-a50d-1b04a87c6f66"}, "source": "# can't fit all data from the dataset becasue of the performance of my computer. So, just choose 10 percent of the dataset randomly for fitting and predicting\n\nimpute_train_dataset = shuffle(imp_dataset.loc[~imp_dataset.heatingorsystemtypeid.isnull()], random_state=0)[:180640]"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "70a7fccbc37467a365e3ca8aa67adeade88dd776", "trusted": false, "_cell_guid": "166405a4-cccc-492b-8fd2-b82850d86357"}, "source": "# column heatingorsystemtypeid prediction preparation\n\nimpute_X_train = impute_train_dataset.drop(imputing_col, axis=1)\nimpute_y_train = impute_train_dataset['heatingorsystemtypeid'].values\nimpute_X_test = imp_dataset.loc[imp_dataset.heatingorsystemtypeid.isnull()].drop(imputing_col, axis=1)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "34e4cb9af65da9a8fbf1c720f75a03d72297e14c", "trusted": false, "_cell_guid": "166a6c1e-e245-4400-8c73-a57f5859ab3f"}, "source": "# Random Forest accuracy\n\ntuned_parameters = [\n    {'criterion': ['gini', 'entropy']}\n]\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nclf = GridSearchCV(random_forest, tuned_parameters, cv=2)\nclf.fit(impute_X_train, impute_y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nrandom_forest_pred = clf.predict(impute_X_test)\nacc_random_forest = round(clf.score(impute_X_train, impute_y_train) * 100, 2)\nprint()\nprint(acc_random_forest)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "e081e15af9ddfff7e2c980a2dfdeaf0bac80c953", "trusted": false, "_cell_guid": "d038484e-910f-4bd7-a92f-1f4b88121406"}, "source": "# column heatingorsystemtypeid prediction result\n\nheatingorsystemtypeid_prediction = pd.DataFrame(random_forest_pred, index=impute_X_test['parcelid']).reset_index()\nheatingorsystemtypeid_prediction.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b5d85c9986bc39e3d0f3199e85a94646d23d802c", "trusted": false, "_cell_guid": "7d9ac6de-7f4d-482f-ad36-a97fb1a83432"}, "source": "# column buildingqualitytypeid prediction preparation\n\nimpute_train_dataset = shuffle(imp_dataset.loc[~imp_dataset.buildingqualitytypeid.isnull()], random_state=0)[:180640]\nimpute_X_train = impute_train_dataset.drop(imputing_col, axis=1)\nimpute_y_train = impute_train_dataset['buildingqualitytypeid'].values\nimpute_X_test = imp_dataset.loc[imp_dataset.buildingqualitytypeid.isnull()].drop(imputing_col, axis=1)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "7b9be703e1e3dd3d1c84e76c871ff99c0f60a1e7", "trusted": false, "_cell_guid": "50a0eea0-d353-470e-b50f-01e6dd5a2479"}, "source": "# Random Forest accuracy\n\ntuned_parameters = [\n    {'criterion': ['gini', 'entropy']}\n]\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nclf = GridSearchCV(random_forest, tuned_parameters, cv=2)\nclf.fit(impute_X_train, impute_y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nrandom_forest_pred = clf.predict(impute_X_test)\nacc_random_forest = round(clf.score(impute_X_train, impute_y_train) * 100, 2)\nprint()\nprint(acc_random_forest)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "34b32eb11a51cb26f5893c30c845f847a273d777", "trusted": false, "_cell_guid": "0e4fad0f-baa2-43df-8a0b-ff05d58e1b30"}, "source": "# column buildingqualitytypeid prediction result\n\nbuildingqualitytypeid_prediction = pd.DataFrame(random_forest_pred, index=impute_X_test['parcelid']).reset_index()\nbuildingqualitytypeid_prediction.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "00fb4ab96cc200e5a02acf5f4851696301448700", "trusted": false, "_cell_guid": "b1619cff-468f-493a-be54-93e728094da5"}, "source": "# column unitcnt prediction preparation\n\nimpute_train_dataset = shuffle(imp_dataset.loc[~imp_dataset.unitcnt.isnull()], random_state=0)[:180640]\nimpute_X_train = impute_train_dataset.drop(imputing_col, axis=1)\nimpute_y_train = impute_train_dataset['unitcnt'].values\nimpute_X_test = imp_dataset.loc[imp_dataset.unitcnt.isnull()].drop(imputing_col, axis=1)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "15e9e78916f46059d1ec07aedd953cfecc835bd6", "trusted": false, "_cell_guid": "c1f011ee-dcde-4aaf-b5c0-985d253bbbbe"}, "source": "# Random Forest accuracy\n\ntuned_parameters = [\n    {'criterion': ['gini', 'entropy']}\n]\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nclf = GridSearchCV(random_forest, tuned_parameters, cv=2)\nclf.fit(impute_X_train, impute_y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nrandom_forest_pred = clf.predict(impute_X_test)\nacc_random_forest = round(clf.score(impute_X_train, impute_y_train) * 100, 2)\nprint()\nprint(acc_random_forest)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0d5d57906815b8df9dc454a476ede20c4175b6ae", "trusted": false, "_cell_guid": "607d78df-8472-4b24-bb07-b291c3c657ab"}, "source": "# column unitcnt prediction result\n\nunitcnt_prediction = pd.DataFrame(random_forest_pred, index=impute_X_test['parcelid']).reset_index()\nunitcnt_prediction.shape"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0391d65a423818ae175ad2693c5ef32bf6bf273f", "trusted": false, "_cell_guid": "654d92f0-ade8-4de1-a0f5-80bca1065ed0"}, "source": "# change the columns name to make the task easier laster\n\nheatingorsystemtypeid_prediction.columns = ['parcelid', 'heatingorsystemtypeid']\nbuildingqualitytypeid_prediction.columns = ['parcelid', 'buildingqualitytypeid']\nunitcnt_prediction.columns = ['parcelid', 'unitcnt']"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "880fc554e9abd47710ee04b85d4f5225ba50738c", "trusted": false, "_cell_guid": "7307520f-493f-46a9-aba6-bafa4a28c14e"}, "source": "# lastly, fill out all missing values\n\nX_train.loc[X_train.heatingorsystemtypeid.isnull(), 'heatingorsystemtypeid'] = \\\n    heatingorsystemtypeid_prediction.loc[heatingorsystemtypeid_prediction.parcelid.isin(X_train.loc[X_train.heatingorsystemtypeid.isnull()]['parcelid'].values)]['heatingorsystemtypeid'].values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a4874fc4414de3b97fb3a35665891671285f1d2a", "trusted": false, "_cell_guid": "f8e06303-dcc2-46bd-9486-1309d321f2a3"}, "source": "X_test.loc[X_test.heatingorsystemtypeid.isnull(), 'heatingorsystemtypeid'] = \\\n    heatingorsystemtypeid_prediction.loc[heatingorsystemtypeid_prediction.parcelid.isin(X_test.loc[X_test.heatingorsystemtypeid.isnull()]['parcelid'].values)]['heatingorsystemtypeid'].values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9759a3ba25e1068e2ae2363d1b8534a7ac60fefb", "trusted": false, "_cell_guid": "ab8173a7-2c0b-4d36-8d5b-f66049796113"}, "source": "X_train.loc[X_train.buildingqualitytypeid.isnull(), 'buildingqualitytypeid'] = \\\n    buildingqualitytypeid_prediction.loc[buildingqualitytypeid_prediction.parcelid.isin(X_train.loc[X_train.buildingqualitytypeid.isnull()]['parcelid'].values)]['buildingqualitytypeid'].values\nX_test.loc[X_test.buildingqualitytypeid.isnull(), 'buildingqualitytypeid'] = \\\n    buildingqualitytypeid_prediction.loc[buildingqualitytypeid_prediction.parcelid.isin(X_test.loc[X_test.buildingqualitytypeid.isnull()]['parcelid'].values)]['buildingqualitytypeid'].values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "540c57dd59567528c70ac2e2053f4795bb29cd87", "trusted": false, "_cell_guid": "8c31cad3-c06a-4cec-a792-2d7302132cea"}, "source": "X_train.loc[X_train.unitcnt.isnull(), 'unitcnt'] = \\\n    unitcnt_prediction.loc[unitcnt_prediction.parcelid.isin(X_train.loc[X_train.unitcnt.isnull()]['parcelid'].values)]['unitcnt'].values\nX_test.loc[X_test.unitcnt.isnull(), 'unitcnt'] = \\\n    unitcnt_prediction.loc[unitcnt_prediction.parcelid.isin(X_test.loc[X_test.unitcnt.isnull()]['parcelid'].values)]['unitcnt'].values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "03dbc43dc45dd17d11fbcc90f9d70b46ca57b0be", "trusted": false, "_cell_guid": "d79e35ea-f6ae-4d9d-8338-903ff6115417"}, "source": "last_test_dataset = pd.concat([X_train, X_test]).drop('logerror', axis=1)\nlast_test_dataset.shape"}, {"metadata": {"_uuid": "5151e47ad70f060b5a79ebd1ae1b67ef3596219a", "_cell_guid": "9b66d68b-382e-40e8-97b0-45e6174de97a"}, "outputs": [], "cell_type": "markdown", "execution_count": null, "source": "### 3. make model for prediction\n\n- explore the algorithms which is the fittest for this prediction challenge\n- tuning the parameter to be more effective on prediction"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0add8f293d4196796d6c6543a8282ec8f02350cd", "trusted": false, "_cell_guid": "e68e7127-f169-4fc0-b897-e7c24776b26f"}, "source": "# save the final dataset for the unexpected situation\n\ny_train_csv = pd.DataFrame(data=y_train, index=X_train.parcelid.values)\ny_train_csv.columns = ['logerror']\n\nX_train.to_csv('x_train.csv')\ny_train_csv.to_csv('y_train.csv')\nlast_test_dataset.to_csv('x_test.csv')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "54fbcf56521bf0bf44a9ef15c32a794bb75041c2", "trusted": false, "_cell_guid": "718a711d-655a-499d-a9ee-f71a4ab7d0f5"}, "source": "# change the dtypes of dataframes for using less memory\n\nfor c, dtype in zip(X_train.columns, X_train.dtypes):\n    if dtype == np.float64:\t\n        X_train[c] = X_train[c].astype(np.float32)\n        \nfor c, dtype in zip(last_test_dataset.columns, last_test_dataset.dtypes):\n    if dtype == np.float64:\t\n        last_test_dataset[c] = last_test_dataset[c].astype(np.float32)\n\nX_train = X_train.drop('logerror', axis=1)\nlast_test_dataset = last_test_dataset[X_train.columns]"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0652e1bfd15929afd8f57acf9d0ea1f857f933bb", "trusted": false, "_cell_guid": "2420f22b-cfa7-4012-ae7e-af9107cc9253"}, "source": "# using ightgbm algorithm and predict the logerror or each properties\n\ntrain = lgb.Dataset(X_train, label=y_train)\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 \nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          \nparams['sub_feature'] = 0.5      \nparams['bagging_fraction'] = 0.85 \nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512 \nparams['min_data'] = 500\nparams['min_hessian'] = 0.05\nparams['verbose'] = 0\n\nclf = lgb.train(params, train, 430)\n\n# to decrease the memory loss\ndel train; gc.collect()\n\nclf.reset_parameter({\"num_threads\":1})\nlgb_pred = clf.predict(last_test_dataset)\nlgb_pred"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a20de476071ef28406f980557546e7e31b562519", "trusted": false, "_cell_guid": "88e697f7-3f6a-4042-970b-85dd522220ab"}, "source": "# using xgboost algorithm and predict the logerror or each properties\n\ny_mean = np.mean(y_train)\n\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ntrain = xgb.DMatrix(X_train, y_train)\ntest = xgb.DMatrix(last_test_dataset)\n\ncv_result = xgb.cv(\n    xgb_params, \n    dtrain, \n    nfold=5,\n    num_boost_round=350,\n    early_stopping_rounds=50,\n    verbose_eval=10, \n    show_stdv=False,\n)\nnum_boost_rounds = len(cv_result)\n\nmodel = xgb.train(dict(xgb_params, silent=1), train, num_boost_round=num_boost_rounds)\nxgb_pred = model.predict(test)\nxgb_pred"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "fdf62eda12d415ae4d539966d54f9b2152704302", "trusted": false, "_cell_guid": "4c2e43f9-c785-4912-84d7-2d30a52fa679"}, "source": "submission_sample = pd.read_csv('../input/sample_submission.csv')\nsubmission = submission_sample.sort_values(by='ParcelId')\nsubmission.head()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "_uuid": "7f43a64f5f1da838fdab8c75f1547ff5394ba873", "trusted": false, "_cell_guid": "30838e92-7c72-4530-bee6-cba73003c8ba"}, "source": "xgb_submission = pd.DataFrame({'ParcelId': submission.ParcelId.astype(np.int32),\n    '201610': xgb_pred, '201611': xgb_pred, '201612': xgb_pred,\n    '201710': xgb_pred, '201711': xgb_pred, '201712': xgb_pred\n})\nxgb_submission.head()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "625cfbec60914c93840dbf3e1042776bde4a9b8b", "trusted": false, "_cell_guid": "112894ec-70be-4b66-83e3-3e8128c2f7de"}, "source": "lgb_submission = pd.DataFrame({'ParcelId': submission.ParcelId.astype(np.int32),\n    '201610': lgb_pred, '201611': lgb_pred, '201612': lgb_pred,\n    '201710': lgb_pred, '201711': lgb_pred, '201712': lgb_pred\n})\nlgb_submission.head()"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0700a8196e2bacf4f2fea44e949f54d0604eea95", "trusted": false, "_cell_guid": "fe75c13f-3018-41ab-9530-8db55044c75c"}, "source": "xgb_submission = xgb_submission[['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712']]\nlgb_submission = lgb_submission[['ParcelId', '201610', '201611', '201612', '201710', '201711', '201712']]"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "7fcf8eacc645fcf6c3342fa728c9a85c41e310a6", "trusted": false, "_cell_guid": "b1ae0cff-4007-4703-80a4-e4a960c03fee"}, "source": "# export the result for submitting\nxgb_submission.to_csv('output.csv', float_format='%.4g', index=False)\nlgb_submission.to_csv('output_lgb.csv', float_format='%.4g', index=False)"}], "nbformat_minor": 2, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}}}