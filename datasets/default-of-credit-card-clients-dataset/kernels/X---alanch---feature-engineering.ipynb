{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9459ae9a-bfcb-0d65-7911-1716a2ffd0c2"
      },
      "source": [
        "**It seems that Kaggle has some problems with its IPython Notebook, you can refer [here](https://www.kaggle.com/alanch/d/uciml/default-of-credit-card-clients-dataset/feature-engineering/run/668752) for the latest successful run.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93df9b91-a88f-a2d4-f61a-5c035cb1e4f7"
      },
      "source": [
        "This script takes Vipul's python script as a handy boilerplate to kickstart.\n",
        "\n",
        "Since I didn't get the dataset on Kaggle but from my professor, there are some naming incompatibility issues. The following cell resolves the problem.\n",
        "\n",
        "Specifically, the differences are:\n",
        "\n",
        " 1. I didn't have the LIMIT_BAL on my dataset.\n",
        " 2. The default column was placed on the first column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab63b1aa-c5d4-f835-d90f-d3c61401289f"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# Init the script\n",
        "# ERG2050 Group Work\n",
        "# CUHK(SZ) 2016 Term 2\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "import itertools\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "def rename_for_kaggle(default):\n",
        "    default=default.rename(columns = {'default.payment.next.month':'default'})\n",
        "    default=default.rename(columns = {'PAY_0':'PAY_1'})\n",
        "    cols = list(default.columns[2:]) # Dismiss BAL_LIMIT, since it wasn't in my version of dataset originally.\n",
        "    cols = [cols[-1]] + cols[:-1]\n",
        "    return default[cols]\n",
        "\n",
        "default = pd.read_csv(\"../input/UCI_Credit_Card.csv\")\n",
        "default = rename_for_kaggle(default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "042f7293-04bf-0476-dd07-38a8f982bb11"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "colors = cycle(['brown','lightcoral','red','magenta','cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\n",
        "\n",
        "def get_model(algoname,feature,target):\n",
        "    X_train = feature\n",
        "    y_train = target\n",
        "    return algoname.fit(X_train,y_train.values.ravel())\n",
        "\n",
        "def algorithm(algoname,colors,train,test,pos):\n",
        "    mean_tpr,lw,i =0.0, 2,1\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fold_accuracy= []\n",
        "    cnf_mat = 0\n",
        "    skfold = StratifiedKFold(n_splits=10,shuffle = True)\n",
        "    for (trainindex,testindex), color in zip(skfold.split(train, test.values.ravel()), colors):\n",
        "        X_train, X_test = train.loc[trainindex], train.loc[testindex]\n",
        "        y_train, y_test = test.loc[trainindex], test.loc[testindex]\n",
        "        model = algoname.fit(X_train,y_train.values.ravel())\n",
        "        fold_accuracy.append(model.score(X_test,y_test.values.ravel()))\n",
        "        result = model.predict(X_test)\n",
        "        fpr, tpr, thresholds= roc_curve(y_test.values,result,pos_label=pos)\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "        mean_tpr[0] = 0.0\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        cm = confusion_matrix(y_test.values,result)\n",
        "        cnf_mat +=  cm\n",
        "        plt.step(fpr, tpr, lw=lw, color=color,label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
        "        i+=1\n",
        "    mean_tpr /= skfold.get_n_splits(train,test.values.ravel())\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "    plt.step(mean_fpr, mean_tpr, color='g', linestyle='--',\n",
        "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
        "    plt.title(\"Average accuracy: {0:.3f}\".format(np.asarray(fold_accuracy).mean()))\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.legend(loc=\"lower right\") \n",
        "    plt.show()\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_mat, classes=[\"0\",\"1\"],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "    plt.show()\n",
        "    return(\"Average accuracy: {0:.3f} (+/-{1:.3f})\".format(np.asarray(fold_accuracy).mean(),\n",
        "                                                           np.asarray(fold_accuracy).std()),\n",
        "           \"\\n Confustion Matrix:\",cnf_mat)\n",
        "\n",
        "def benchmark(default):\n",
        "    default_train,default_test = default.iloc[:,1:].astype(int), default.iloc[:,0].astype(int)\n",
        "\n",
        "    # In[5]:\n",
        "\n",
        "    print(\"\\n Default of Credit Card Clients Data Set\")\n",
        "    print(\"\\n Random Forest\")\n",
        "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
        "                min_impurity_split=1e-07, min_samples_leaf=50,\n",
        "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                n_estimators=600, n_jobs=-1, oob_score=False,\n",
        "                random_state=None, verbose=0, warm_start=False)\n",
        "    print(algorithm(forest,colors,default_train,default_test,pos = None))\n",
        "    \n",
        "    #print(\"\\n Random Forest (EXP)\")\n",
        "    forest_exp = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
        "                max_depth=200, max_features=None, max_leaf_nodes=None,\n",
        "                min_impurity_split=1e-07, min_samples_leaf=50,\n",
        "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                n_estimators=600, n_jobs=-1, oob_score=False,\n",
        "                random_state=None, verbose=0, warm_start=False)\n",
        "    #print(algorithm(forest_exp,colors,default_train,default_test,pos = None))\n",
        "\n",
        "\n",
        "    # In[6]:\n",
        "    print(\"\\n Logistic\")\n",
        "    logistic = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
        "              penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
        "              verbose=0, warm_start=False)\n",
        "    print(algorithm(logistic,colors,default_train,default_test,pos = None))\n",
        "\n",
        "\n",
        "    # In[7]:\n",
        "    print(\"\\n Naive\")\n",
        "    naive = GaussianNB()\n",
        "    print(algorithm(naive,colors,default_train,default_test,pos = None))\n",
        "\n",
        "\n",
        "    # In[8]:\n",
        "    print(\"\\n KNN\")\n",
        "    knneigh = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "               metric_params=None, n_jobs=-1, n_neighbors=50, p=2,\n",
        "               weights='uniform')\n",
        "    print(algorithm(knneigh,colors,default_train,default_test,pos = None))\n",
        "\n",
        "\n",
        "    # In[9]:\n",
        "    print(\"\\n SVM\")\n",
        "    svm = LinearSVC(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "         intercept_scaling=1, loss='squared_hinge', max_iter=10,\n",
        "         multi_class='ovr', penalty='l1', random_state=1000, tol=0.0001,\n",
        "         verbose=0)\n",
        "    print(algorithm(svm,colors,default_train,default_test,pos = None))\n",
        "    \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "def benchmark_hard(default):\n",
        "    default_train,default_test = default.iloc[:,1:].astype(int), default.iloc[:,0].astype(int)\n",
        "    print(\"\\n Default of Credit Card Clients Data Set\")\n",
        "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                max_depth=1000, max_features='auto', max_leaf_nodes=None,\n",
        "                min_impurity_split=1e-07, min_samples_leaf=10,\n",
        "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                n_estimators=6000, n_jobs=-1, oob_score=False,\n",
        "                random_state=None, verbose=0, warm_start=False)\n",
        "    print(algorithm(forest,colors,default_train,default_test,pos = None))\n",
        "    \n",
        "def export_false_prediction(result, filename, data):\n",
        "    false_index = []\n",
        "    for i in range(len(result)):\n",
        "        if result[i] != data.iloc[i,0]:\n",
        "            false_index.append(i)\n",
        "    false_pred = data.iloc[false_index,:]\n",
        "    false_pred.to_csv(filename, index=False)\n",
        "    print(\"Trainning Errors: \" + str(len(false_pred)))\n",
        "    \n",
        "def export_negative(result, filename, data):\n",
        "    negative_index = []\n",
        "    for i in range(len(result)):\n",
        "        if result[i] == 0:\n",
        "            negative_index.append(i)\n",
        "    data.iloc[negative_index,:].to_csv(filename)\n",
        "    \n",
        "    \n",
        "def stage1(filename):\n",
        "    default = pd.read_csv(filename)\n",
        "    default = rename_for_kaggle(default)\n",
        "    breakpoint = int(1 * len(default))\n",
        "    train_features, train_target = default.iloc[:breakpoint,1:].astype(int), default.iloc[:breakpoint,0].astype(int)\n",
        "    test_features, test_target = default.iloc[breakpoint:,1:].astype(int), default.iloc[breakpoint:,0].astype(int)\n",
        "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                    max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
        "                    min_impurity_split=1e-07, min_samples_leaf=50,\n",
        "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                    n_estimators=600, n_jobs=-1, oob_score=False,\n",
        "                    random_state=None, verbose=0, warm_start=False)\n",
        "    model = get_model(forest, train_features, train_target)\n",
        "    # result = model.predict(test_features)\n",
        "    result = model.predict(train_features)\n",
        "    export_false_prediction(result, \"false_negative.csv\", default)\n",
        "    return model\n",
        "\n",
        "def stage2(filename_false_negative):\n",
        "    default = pd.read_csv(filename_false_negative)\n",
        "    default = rename_for_kaggle(default)\n",
        "    train_features, train_target = default.iloc[:,1:].astype(int), default.iloc[:,0].astype(int)\n",
        "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                    max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
        "                    min_impurity_split=1e-07, min_samples_leaf=50,\n",
        "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                    n_estimators=600, n_jobs=-1, oob_score=False,\n",
        "                    random_state=None, verbose=0, warm_start=False)\n",
        "    logistic = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
        "              penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
        "              verbose=0, warm_start=False)\n",
        "    naive = GaussianNB()\n",
        "    model = get_model(naive, train_features, train_target)\n",
        "    return model\n",
        "\n",
        "def naive(filename):\n",
        "    default = pd.read_csv(filename)\n",
        "    default = rename_for_kaggle(default)\n",
        "    breakpoint = int(1 * len(default))\n",
        "    train_features, train_target = default.iloc[:breakpoint,1:].astype(int), default.iloc[:breakpoint,0].astype(int)\n",
        "    test_features, test_target = default.iloc[breakpoint:,1:].astype(int), default.iloc[breakpoint:,0].astype(int)\n",
        "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "                    max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
        "                    min_impurity_split=1e-07, min_samples_leaf=50,\n",
        "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                    n_estimators=600, n_jobs=-1, oob_score=False,\n",
        "                    random_state=None, verbose=0, warm_start=False)\n",
        "    naive_model = GaussianNB()\n",
        "    model = get_model(naive_model, train_features, train_target)\n",
        "    # result = model.predict(test_features)\n",
        "    result = model.predict(train_features)\n",
        "    export_false_prediction(result, \"false_negative.csv\", default)\n",
        "    return model\n",
        "\n",
        "def magic(train_file, test_file):\n",
        "    model1 = stage1(train_file)\n",
        "    model2 = stage2(\"false_negative.csv\")\n",
        "    # Models trainetest_ext4.csv   \n",
        "    # Load Test file\n",
        "    test = pd.read_csv(test_file)\n",
        "    print(\"Test size: \" + str(len(test)))\n",
        "    test_features = test.iloc[:,1:].astype(int)\n",
        "    test_target = test.iloc[:,0].astype(int)\n",
        "\n",
        "    # Test\n",
        "    # First Stage Fit\n",
        "\n",
        "    print(\"Model 1 Score:\" + str(model1.score(test_features,test_target.values.ravel())))\n",
        "    print(\"Model 2 Score:\" + str(model2.score(test_features,test_target.values.ravel())))\n",
        "    # Using Model 1 to predict test set\n",
        "    result1 = model1.predict(test_features)\n",
        "    error_count = 0\n",
        "    for i in range(len(test)):\n",
        "        if result1[i] == 0 and test_target.iloc[i] == 1:\n",
        "            error_count += 1\n",
        "    print(\"Model 1 Testing False Negative: \" + str(error_count))\n",
        "\n",
        "    # Find negative predictions\n",
        "    export_negative(result1, \"negative.csv\", test)\n",
        "    negative = pd.read_csv(\"negative.csv\")\n",
        "    print(\"Model 2 Input Size: \" + str(len(negative)))\n",
        "    negative_features = negative.iloc[:,2:].astype(int)\n",
        "\n",
        "    # Second Stage\n",
        "    result2 = model2.predict(negative_features)\n",
        "    print(\"Result2 Size: \" + str(len(result2)))\n",
        "    diff = 0\n",
        "    diff_fn = 0\n",
        "    for i in range(len(result2)):\n",
        "        result_1 = result1[negative.iloc[i,1].astype(int)]\n",
        "        if result_1 != result2[i]:\n",
        "            result1[negative.iloc[i,1].astype(int)] = result2[i]\n",
        "            diff += 1\n",
        "            if result_1 != test_target.iloc[negative.iloc[i,1].astype(int)]:\n",
        "                diff_fn += 1\n",
        "    print(\"DIFF: \" + str(diff) + \" Correction: \" + str(diff_fn))\n",
        "\n",
        "\n",
        "    # Get Score\n",
        "    error_count = 0\n",
        "    for i in range(len(test)):\n",
        "        if result1[i] != test_target.iloc[i]:\n",
        "            error_count += 1\n",
        "    print(\"Errors: \" + str(error_count))\n",
        "    print(\"Accuracy: \" + str(1-(error_count/len(test))))\n",
        "    error_count = 0\n",
        "    for i in range(len(test)):\n",
        "        if result1[i] == 0 and test_target.iloc[i] == 1:\n",
        "            error_count += 1\n",
        "    print(\"Model 1+2 Testing False Negative: \" + str(error_count))\n",
        "\n",
        "def get_model(algoname,feature,target):\n",
        "    X_train = feature\n",
        "    y_train = target\n",
        "    return algoname.fit(X_train,y_train.values.ravel())\n",
        "\n",
        "def test(train_file, test_file):\n",
        "    model1 = stage1(train_file)\n",
        "    \n",
        "    # Load Test file\n",
        "    test = pd.read_csv(test_file)\n",
        "    print(\"Test size: \" + str(len(test)))\n",
        "    test_features = test.iloc[:,1:].astype(int)\n",
        "    test_target = test.iloc[:,0].astype(int)\n",
        "    \n",
        "    # Test\n",
        "    # First Stage Fit\n",
        "\n",
        "    print(\"Model Score:\" + str(model1.score(test_features,test_target.values.ravel())))\n",
        "    # Using Model 1 to predict test set\n",
        "    result1 = model1.predict(test_features)\n",
        "    \n",
        "    # Draw Confusion Matrix\n",
        "    cm = confusion_matrix(test_target.values,result1)\n",
        "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "    plt.show()\n",
        "    \n",
        "    # Count\n",
        "    error_count = 0\n",
        "    for i in range(len(test)):\n",
        "        if result1[i] == 0 and test_target.iloc[i] == 1:\n",
        "            error_count += 1\n",
        "    print(\"Model Testing False Negative: \" + str(error_count))\n",
        "    \n",
        "def naive_test(train_file, test_file):\n",
        "    model1 = naive(train_file)\n",
        "    \n",
        "    # Load Test file\n",
        "    test = pd.read_csv(test_file)\n",
        "    print(\"Test size: \" + str(len(test)))\n",
        "    test_features = test.iloc[:,1:].astype(int)\n",
        "    test_target = test.iloc[:,0].astype(int)\n",
        "\n",
        "    # Test\n",
        "    # First Stage Fit\n",
        "\n",
        "    print(\"Model Score:\" + str(model1.score(test_features,test_target.values.ravel())))\n",
        "    # Using Model 1 to predict test set\n",
        "    result1 = model1.predict(test_features)\n",
        "    \n",
        "    # Draw Confusion Matrix\n",
        "    cm = confusion_matrix(test_target.values,result1)\n",
        "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "    plt.show()\n",
        "    \n",
        "    error_count = 0\n",
        "    for i in range(len(test)):\n",
        "        if result1[i] == 0 and test_target.iloc[i] == 1:\n",
        "            error_count += 1\n",
        "    print(\"Model Testing False Negative: \" + str(error_count))\n",
        "    \n",
        "\n",
        "print(\"Ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8a53826f-6a89-7816-688d-9931803ef328"
      },
      "source": [
        "# Benchmarking Several Models Using Original Data\n",
        "\n",
        "The code section below will run a benchmark of several common methods with the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "989f6c2a-2ed7-980a-e79b-6abf6ffd12b4"
      },
      "outputs": [],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "benchmark(default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "534e3a8e-6266-9ac8-128d-e3731be3cb3e"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "It is intuitive to manipulate the data for a bit to let algorithms capture some underlying connection among these dimensions. For example, algorithms cannot understand time series relationships of BILL_AMTX and PAY_X, where X is a time indicator.\n",
        "\n",
        "Here is how we did it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c33a44b-3f0e-0191-241e-9ac177720b2c"
      },
      "outputs": [],
      "source": [
        "def BILL_regression(default):\n",
        "    data = default.copy()\n",
        "    pandas = pd\n",
        "    from scipy import stats\n",
        "    for i in range(len(data)):\n",
        "        temp = pandas.DataFrame.transpose(pandas.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"BILL_AMT\"] = 0\n",
        "        temp[\"BILL_DATE\"] = 0\n",
        "        for j in range(1,7):\n",
        "            temp.at[j, \"BILL_AMT\"] = data.iloc[i][\"BILL_AMT\" + str(j)]\n",
        "            temp.at[j, \"BILL_DATE\"] = j\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(temp[\"BILL_DATE\"],temp[\"BILL_AMT\"])\n",
        "        data.at[i, \"BILL_SLOPE\"] = slope\n",
        "        data.at[i, \"BILL_INCEPT\"] = intercept\n",
        "        data.at[i, \"BILL_STDERR\"] = std_err\n",
        "    return data\n",
        "v1 = BILL_regression(default)\n",
        "v1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc55c5f2-fc57-db12-4d60-c08f1ec373d7"
      },
      "outputs": [],
      "source": [
        "def BILL_poly_regression(default):\n",
        "    data = default.copy()\n",
        "    for i in range(len(data)):\n",
        "        temp = pd.DataFrame.transpose(pd.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"BILL_AMT\"] = 0\n",
        "        temp[\"BILL_DATE\"] = 0\n",
        "        for j in range(1,7):\n",
        "            temp.at[j, \"BILL_AMT\"] = data.iloc[i][\"BILL_AMT\" + str(j)]\n",
        "            temp.at[j, \"BILL_DATE\"] = j\n",
        "        result = np.polynomial.polynomial.polyfit(temp[\"BILL_DATE\"],temp[\"BILL_AMT\"],4)\n",
        "        for j in range(len(result)):\n",
        "            data.at[i, \"BILL_POLY\" + str(j)] = result[j]\n",
        "    return data\n",
        "v2 = BILL_poly_regression(v1)\n",
        "v2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ebfa76b8-5600-3280-8626-17c49c715ceb"
      },
      "outputs": [],
      "source": [
        "def threshold_count(label, default, thresholds):\n",
        "    data = default.copy()\n",
        "    for threshold in thresholds:\n",
        "        for i in range(len(data)):\n",
        "            count = 0\n",
        "            for j in range(1,7):\n",
        "                if data.iloc[i][label + str(j)] <= threshold:\n",
        "                    count += 1\n",
        "            data.at[i, label + \"_COUNT_\" + str(threshold)] = count\n",
        "    return data\n",
        "v3 = threshold_count(\"BILL_AMT\", v2, [0,20000,70000])\n",
        "v3 = threshold_count(\"PAY_AMT\", v3, [0,1000,5000])\n",
        "v3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7059afc5-44b4-16c2-9ae0-36ecca3c1fcf"
      },
      "outputs": [],
      "source": [
        "def pay_count(default):\n",
        "    data = default.copy()\n",
        "    pandas = pd\n",
        "    from scipy import stats\n",
        "    for i in range(len(data)):\n",
        "        temp = pandas.DataFrame.transpose(pandas.DataFrame(data=data.iloc[i]))\n",
        "        for val in [-2,-1,1,2,3,4,5,6,7,8,9]:\n",
        "            count = 0\n",
        "            for j in range(1,7):\n",
        "                if data.iloc[i][\"PAY_\" + str(j)] == val:\n",
        "                    count += 1\n",
        "                data.at[i, \"PAY_COUNT_\" + str(val)] = count\n",
        "    return data\n",
        "v4 = pay_count(v3)\n",
        "v4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "feb4abfb-0818-4475-8fb6-d30181ea60c2"
      },
      "outputs": [],
      "source": [
        "#VAR\n",
        "def var(label, default):\n",
        "    data=default.copy()\n",
        "    for i in range(len(data)):\n",
        "        temp = pd.DataFrame.transpose(pd.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"VAL\"] = 0\n",
        "        for j in range(1,7):\n",
        "            temp.at[j, \"VAL\"] = data.iloc[i][label + str(j)]\n",
        "        data.at[i, label + \"_VAR\"] = temp.VAL.var()\n",
        "    return data\n",
        "v5 = var(\"PAY_AMT\", v4)\n",
        "v5 = var(\"BILL_AMT\", v5)\n",
        "v5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e90736d9-633e-afc3-8fc9-6723835aa27c"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return float(1)/float(1+np.exp(x))\n",
        "\n",
        "def pbr(bill, pay):\n",
        "    if bill > 0:\n",
        "        if pay < bill:\n",
        "            result = pay/bill\n",
        "        else:\n",
        "            result = 1 + f(pay)\n",
        "    elif bill == 0:\n",
        "        if pay != 0:\n",
        "            result = 2 + f(pay)\n",
        "        if pay == 0:\n",
        "            result = 1\n",
        "    else:\n",
        "        if pay == 0:\n",
        "            result = 3 + f(pay)\n",
        "        if pay > 0:\n",
        "            result = 4 + f(pay) \n",
        "    return result\n",
        "\n",
        "#Payback Ratio\n",
        "def calc_pbr(default):\n",
        "    data = default\n",
        "    for i in range(len(data)):\n",
        "        temp = pd.DataFrame.transpose(pd.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"VAL\"] = 0\n",
        "        for j in range(1,7):\n",
        "            bill = data.iloc[i][\"BILL_AMT\" + str(j)]\n",
        "            pay = data.iloc[i][\"PAY_AMT\" + str(j)]\n",
        "            data.at[i, \"PBR_\" + str(j)] = pbr(bill, pay)\n",
        "    return data\n",
        "\n",
        "v6 = calc_pbr(v5)\n",
        "v6.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd8ef1b9-ddfe-0ff7-8930-600060d21182"
      },
      "outputs": [],
      "source": [
        "def PBR_regression(default):\n",
        "    data = default.copy()\n",
        "    pandas = pd\n",
        "    from scipy import stats\n",
        "    for i in range(len(data)):\n",
        "        temp = pandas.DataFrame.transpose(pandas.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"AMT\"] = 0\n",
        "        temp[\"DATE\"] = 0\n",
        "        for j in range(1,7):\n",
        "            temp.at[j, \"AMT\"] = data.iloc[i][\"PBR_\" + str(j)]\n",
        "            temp.at[j, \"DATE\"] = j\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(temp[\"DATE\"],temp[\"AMT\"])\n",
        "        data.at[i, \"PBR_SLOPE\"] = slope\n",
        "        data.at[i, \"PBR_INCEPT\"] = intercept\n",
        "        data.at[i, \"PBR_STDERR\"] = std_err\n",
        "    return data\n",
        "v7 = PBR_regression(v6)\n",
        "v7.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1a29323-c70c-134c-7824-592c4d5e9eec"
      },
      "outputs": [],
      "source": [
        "def PAY_regression(default):\n",
        "    data = default.copy()\n",
        "    pandas = pd\n",
        "    from scipy import stats\n",
        "    for i in range(len(data)):\n",
        "        temp = pandas.DataFrame.transpose(pandas.DataFrame(data=data.iloc[i]))\n",
        "        for j in range(1,7):\n",
        "            temp.loc[j] = temp.iloc[0]\n",
        "        temp[\"AMT\"] = 0\n",
        "        temp[\"DATE\"] = 0\n",
        "        for j in range(1,7):\n",
        "            temp.at[j, \"AMT\"] = data.iloc[i][\"PAY_AMT\" + str(j)]\n",
        "            temp.at[j, \"DATE\"] = j\n",
        "        slope, intercept, r_value, p_value, std_err = stats.linregress(temp[\"DATE\"],temp[\"AMT\"])\n",
        "        data.at[i, \"PAY_SLOPE\"] = slope\n",
        "        data.at[i, \"PAY_INCEPT\"] = intercept\n",
        "        data.at[i, \"PAY_STDERR\"] = std_err\n",
        "    return data\n",
        "v8 = PAY_regression(v7)\n",
        "v8.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "336f6c25-c900-f177-6637-376c1057283e"
      },
      "source": [
        "# Feature Selection\n",
        "Now let's see what we have got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "095bf634-03e7-2237-5f71-bc9869bf7b5f"
      },
      "outputs": [],
      "source": [
        "list(v8.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7b624d17-e777-4971-5f00-63f46abe0594"
      },
      "source": [
        "So now we have a total of 61 features. To select the most useful ones, we tried lasso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59b234fb-f533-e22e-7af5-151a898a30ca"
      },
      "outputs": [],
      "source": [
        "def sub_features(data, alpha):\n",
        "    features = [\"default\"]\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    default_train, default_test = data.iloc[:,1:].astype(int), data.iloc[:,0].astype(int)\n",
        "    lasso.fit(default_train, default_test)\n",
        "    for i in range(len(lasso.coef_)):\n",
        "        if lasso.coef_[i] > 0:\n",
        "            features.append(default_train.columns[i])\n",
        "    return data.copy()[features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bc402f2-0100-32f5-af6d-0c7b5b48c4ed"
      },
      "outputs": [],
      "source": [
        "data_1 = sub_features(v8, 0.1)\n",
        "list(data_1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "875fbef9-92f3-396a-c780-c63b229eef54"
      },
      "outputs": [],
      "source": [
        "benchmark(data_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3ccdb965-1abe-1e9a-ffb0-1b47a6ccb9fa"
      },
      "source": [
        "Feel free to play around the parameters and other subset selction methods. Somehow we got the following subset: (guess how we got it?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc97fd14-7319-6c97-8955-8d06871d0d0c"
      },
      "outputs": [],
      "source": [
        "data_2 = v8.copy()[['default', 'PAY_1',\n",
        " 'PAY_AMT1',\n",
        " 'PAY_AMT_COUNT_1000',\n",
        " 'PAY_AMT_COUNT_5000',\n",
        " 'PAY_COUNT_-2',\n",
        " 'PAY_COUNT_2',\n",
        " 'PAY_COUNT_3',\n",
        " 'PAY_COUNT_7',\n",
        " 'PBR_STDERR',\n",
        " 'BILL_AMT_COUNT_20000']]\n",
        "benchmark(data_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "00ea7f4b-7d9b-c75c-3030-ce6286e482b4"
      },
      "source": [
        "You can see how our feature engineering helps Naive Bayes improve significantly.\n",
        "\n",
        "P.S. There are some experimental methods in the second cell. Feel free to explore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6adc875e-ffb0-8234-07cf-47f69e91ec16",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}