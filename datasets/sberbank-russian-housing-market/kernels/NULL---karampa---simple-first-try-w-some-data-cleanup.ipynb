{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e37bc3c-00da-5c42-69e0-b86c50e7df83"
      },
      "source": [
        "# Trying out..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2e6397e-462b-b02f-7020-9b5b985da1a4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn import model_selection, preprocessing\n",
        "import xgboost as xgb\n",
        "import math\n",
        "#import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0ec5b6b-3e93-6872-3d8b-42e1b525c5d9"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n",
        "test_df = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n",
        "macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9f75e402-b67b-319b-8b7a-615408d11f23"
      },
      "source": [
        "## Some cleanup and correction on most important features... let's see..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cb8028c7-7471-fd0b-95d7-62f492671e0a"
      },
      "source": [
        "### cleaning up floors and max_floors  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab627620-ea16-8615-1e71-de64b88da982"
      },
      "outputs": [],
      "source": [
        "# Few rows here to drop not much impact...\n",
        "\n",
        "train_df.dropna(axis=0, subset=['floor'], how='any', inplace=True)\n",
        "train_df.drop(train_df[train_df['floor'] == 0].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "46dbc6c7-75e4-0ce8-2182-07a4504bfab4"
      },
      "outputs": [],
      "source": [
        "#Assuming Max_floor is equal to floor when it is not consistent.\n",
        "train_df['max_floor'] = train_df['max_floor'].fillna(0)\n",
        "train_df['max_floor'] = np.where(train_df['max_floor'] < train_df['floor'], train_df['floor'], train_df['max_floor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a69f40f9-72cf-20e9-f5c2-40f1a0f2a2f5"
      },
      "source": [
        "## Cleaning up Full_sq and life_Sq "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19c303de-0ab9-4ef7-e5bc-f6f497609114"
      },
      "outputs": [],
      "source": [
        "# wierd values corrected\n",
        "train_df.set_value(train_df[train_df['state'] == 33].index, 'state', 4)\n",
        "train_df.set_value(train_df[train_df['build_year'] == 20052009].index, 'build_year', 2007)\n",
        "# 1 Outlier in full_Sq we will delete for the moment\n",
        "train_df.drop(train_df[train_df['full_sq'] > 2000].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6a05e11-b80a-881e-9629-64e7c06665ee"
      },
      "outputs": [],
      "source": [
        "#droping rows where lif_sq is greater than full_sq (22 records )\n",
        "train_df['bad_life'] = train_df['full_sq'] - train_df['life_sq']\n",
        "train_df.drop(train_df[train_df['bad_life'] < 0].index, inplace=True)\n",
        "\n",
        "#completing NaN values with mean ratio between Full and Life SQ\n",
        "train_df['r_life_ful_sq'] = train_df['bad_life'] / train_df['full_sq']\n",
        "mean_ratio = train_df['r_life_ful_sq'].mean()\n",
        "train_df.life_sq.fillna(train_df.full_sq *(1 - mean_ratio), inplace=True)\n",
        "\n",
        "# droping working columns\n",
        "train_df.drop(['bad_life', 'r_life_ful_sq'], axis=1, inplace=True)\n",
        "\n",
        "# Replacing life_sq < 5sq by mean ration full and life as for NaN\n",
        "train_df['life_sq'] = np.where(train_df['life_sq'] <=5, train_df['full_sq'] * (1 - mean_ratio), train_df['life_sq'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe3dc500-9275-98f0-bd87-031c727bb915"
      },
      "source": [
        "## merging macroeconomics data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2ff69200-e170-8cb9-2c7d-f74d012c3c55"
      },
      "outputs": [],
      "source": [
        "dftrain = pd.merge(train_df, macro, how='left', on='timestamp')\n",
        "dftest = pd.merge(test_df, macro, how='left', on='timestamp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f64c194-1726-ffab-3885-4ff70c51f46b"
      },
      "outputs": [],
      "source": [
        "#y_train = dftrain[\"price_doc\"]\n",
        "x_train = dftrain.drop([\"id\", \"timestamp\"], axis=1)\n",
        "x_test = dftest.drop([\"id\", \"timestamp\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7ad4310-dc6f-7e13-ed67-150d85275537"
      },
      "outputs": [],
      "source": [
        "# these variables are empty in test set or their feat importance is assumed small atm... \n",
        "# we will revisit later it will grow bigger for sure\n",
        "list_empty = ['grp','grp_growth','real_dispos_income_per_cap_growth', 'profitable_enterpr_share',\n",
        "              'unprofitable_enterpr_share','share_own_revenues','overdue_wages_per_cap', 'fin_res_per_cap',\n",
        "              'marriages_per_1000_cap','divorce_rate','construction_value', 'invest_fixed_assets_phys',\n",
        " 'pop_migration','pop_total_inc','housing_fund_sqm','lodging_sqm_per_cap', 'water_pipes_share', 'baths_share',\n",
        " 'sewerage_share','gas_share', 'hot_water_share', 'electric_stove_share', 'heating_share',\n",
        " 'old_house_share', 'infant_mortarity_per_1000_cap', 'perinatal_mort_per_1000_cap', 'incidence_population',\n",
        " 'load_of_teachers_preschool_per_teacher', 'child_on_acc_pre_school', 'provision_doctors',\n",
        " 'power_clinics', 'hospital_beds_available_per_cap', 'hospital_bed_occupancy_per_year',\n",
        " 'provision_retail_space_sqm', 'provision_retail_space_modern_sqm', 'theaters_viewers_per_1000_cap',\n",
        " 'museum_visitis_per_100_cap', 'population_reg_sports_share',\n",
        " 'students_reg_sports_share', 'apartment_build', 'modern_education_share', 'old_education_build_share', \n",
        "              'child_on_acc_pre_school']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a86f2340-595a-7e10-dcb2-cbbe924f61f6"
      },
      "outputs": [],
      "source": [
        "x_train.drop(list_empty, axis=1, inplace=True)\n",
        "x_test.drop(list_empty, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e05ec983-6bcd-57e4-3cf8-81169b446bf0"
      },
      "outputs": [],
      "source": [
        "for c in x_train.columns:\n",
        "    if x_train[c].dtype == 'object':\n",
        "        lbl = preprocessing.LabelEncoder()\n",
        "        lbl.fit(list(x_train[c].values)) \n",
        "        x_train[c] = lbl.transform(list(x_train[c].values))\n",
        "        \n",
        "for c in x_test.columns:\n",
        "    if x_test[c].dtype == 'object':\n",
        "        lbl = preprocessing.LabelEncoder()\n",
        "        lbl.fit(list(x_test[c].values)) \n",
        "        x_test[c] = lbl.transform(list(x_test[c].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6e15636-d089-ba18-cad5-17d345f0258a"
      },
      "outputs": [],
      "source": [
        "def rmsle(preds, dtrain):\n",
        "    labels = dtrain.get_label()\n",
        "    assert len(preds) == len(labels)\n",
        "    labels = labels.tolist()\n",
        "    preds = preds.tolist()\n",
        "    terms_to_sum = [(math.log(labels[i] + 1) - math.log(max(0, preds[i]) + 1)) ** 2.0 for i, pred in enumerate(labels)]\n",
        "    return 'rmsle', (sum(terms_to_sum) * (1.0 / len(preds))) ** 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "475a1470-bd50-4de3-dedf-ec0746e3dac2"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 8,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'silent': 1\n",
        "}\n",
        "\n",
        "y_train = x_train[\"price_doc\"]\n",
        "x_train.drop('price_doc', axis=1, inplace=True)\n",
        "\n",
        "# Train/Valid split\n",
        "split = 27000\n",
        "xx_train, yy_train, xx_valid, yy_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
        "\n",
        "dtrain = xgb.DMatrix(xx_train, yy_train, feature_names=xx_train.columns.values)\n",
        "dvalid = xgb.DMatrix(xx_valid, yy_valid, feature_names=xx_valid.columns.values)\n",
        "\n",
        "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "model = xgb.train(dict(xgb_params), dtrain, 600, watchlist, feval=rmsle, early_stopping_rounds=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "53cbdf83-6817-68de-86fd-9744fc3bdb9b"
      },
      "outputs": [],
      "source": [
        "featureImportance = model.get_fscore()\n",
        "features = pd.DataFrame()\n",
        "features['features'] = featureImportance.keys()\n",
        "features['importance'] = featureImportance.values()\n",
        "features.sort_values(by=['importance'],ascending=False,inplace=True)\n",
        "fig,ax= plt.subplots()\n",
        "fig.set_size_inches(20,25)\n",
        "plt.xticks(rotation=60)\n",
        "sns.set(font_scale=1.5)\n",
        "sns.barplot(data=features.head(50),y=\"features\",x=\"importance\",ax=ax,orient=\"h\")\n",
        "#b.set_ylabel(\"features\",fontsize=20)\n",
        "#sns.plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9659d99-f4ab-24b7-823c-4b3b9d7b124c"
      },
      "outputs": [],
      "source": [
        "p_test = model.predict(xgb.DMatrix(x_test))\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['id'] = dftest['id'].values\n",
        "sub['price_doc'] = p_test\n",
        "sub.to_csv('xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cab97d6c-18bd-4bd4-dc22-fa95db41c615"
      },
      "outputs": [],
      "source": [
        "sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2570daf-cc29-0a75-47a0-97739a745928"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}