{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["MAX_ROUNDS = 5000\n", "OPTIMIZE_ROUNDS = True"], "outputs": [], "metadata": {"_cell_guid": "99393d57-64b8-4e66-a126-037fa9b347fe", "collapsed": true, "_uuid": "be941765ddc97c673bcb21665a55a64510f419df"}, "execution_count": null, "cell_type": "code"}, {"source": ["Based on Keui Shen Nong's [script](https://www.kaggle.com/kueipo/base-on-froza-pascal-single-xgb-lb-0-284).  *The main point of this kernel is to generate out-of-fold data for stacking/validation/etc..*  It also generates a submission file, which I guess might be interesting, mostly in a negative way.  This uses 4-fold CV as compared to the 25% holdout validation in the original.  In principle 4-fold CV is more robust, so if the LB score from this is not as good, that probably means the original is overfit.  But maybe not, because the process of averaging across folds may not be trustworthy.  (You could also try other methods of averaging:  log-odds or rank average or whatever.  Maybe I will in a future version.  If they all produce worse LB results than the original, that would be a pretty strong indication that it's overfit.  But who is going to make all those submissions....?)  Setting a fixed number of rounds (<code>OPTIMIZE_ROUNDS=False</code> if you can find a good choice for <code>MAX_ROUNDS</code>) will probably generate more reliable out-of-fold predictions.  Alternatively, one could add noise to the validation data to compensate for overfitting.  (I might do that in a later version, too.)"], "metadata": {"_cell_guid": "12b97784-1dee-48c4-b831-56c696c88f78", "_uuid": "ef25ae18c01b92601c3222ad9d23a86957e95782"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from multiprocessing import *\n", "import gc\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "import xgboost as xgb\n", "from numba import jit"], "outputs": [], "metadata": {"_cell_guid": "b7258128-55f9-4543-8611-5e0a6661837b", "collapsed": true, "_uuid": "72171ee53e170096d37a18eef84682fa348ae5c4"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Gini\n", "\n", "def ginic(actual, pred):\n", "    actual = np.asarray(actual) \n", "    n = len(actual)\n", "    a_s = actual[np.argsort(pred)]\n", "    a_c = a_s.cumsum()\n", "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\n", "    return giniSum / n\n", " \n", "def gini_normalized(a, p):\n", "    if p.ndim == 2:\n", "        p = p[:,1] \n", "    return ginic(a, p) / ginic(a, a)\n", "    \n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return 'gini', gini_score\n", "\n", "# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n", "@jit\n", "def eval_gini(y_true, y_prob):\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini"], "outputs": [], "metadata": {"_cell_guid": "22381008-e684-428d-a97f-da90eaa44111", "collapsed": true, "_uuid": "b24fae4ad6b332041cc889e33b39e46328dfe7cc"}, "execution_count": null, "cell_type": "code"}, {"source": ["def transform_df(df):\n", "    df = pd.DataFrame(df)\n", "    dcol = [c for c in df.columns if c not in ['id','target']]\n", "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n", "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n", "    for c in dcol:\n", "        if '_bin' not in c: #standard arithmetic\n", "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n", "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n", "\n", "    for c in one_hot:\n", "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n", "            for val in one_hot[c]:\n", "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n", "    return df\n", "\n", "def multi_transform(df):\n", "    print('Init Shape: ', df.shape)\n", "    p = Pool(cpu_count())\n", "    df = p.map(transform_df, np.array_split(df, cpu_count()))\n", "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n", "    p.close(); p.join()\n", "    print('After Shape: ', df.shape)\n", "    return df"], "outputs": [], "metadata": {"_cell_guid": "7f8f84f9-aa46-43d6-8d78-619ef9a7dcce", "collapsed": true, "_uuid": "affb627af275ad7aa0c5c85dd826ee140370eb3e"}, "execution_count": null, "cell_type": "code"}, {"source": ["#### Load Data\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')"], "outputs": [], "metadata": {"_cell_guid": "52b50086-b405-4598-b11c-97887cdcce8e", "collapsed": true, "_uuid": "07a5a5782894611e9006ae1b399b0b8fb8a0f06b"}, "execution_count": null, "cell_type": "code"}, {"source": ["### \n", "y = train['target'].values\n", "testid= test['id'].values\n", "trainid = train['id'].values\n", "\n", "\n", "train.drop(['id','target'],axis=1,inplace=True)\n", "test.drop(['id'],axis=1,inplace=True)\n", "\n", "### Drop calc\n", "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n", "train = train.drop(unwanted, axis=1)  \n", "test = test.drop(unwanted, axis=1)"], "outputs": [], "metadata": {"_cell_guid": "1b36eb15-ee01-43a3-8766-27650f98158d", "collapsed": true, "_uuid": "6255e3c12616b0279cef5c1bdec97751bb72d8b8"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Great Recovery from Pascal's materpiece\n", "\n", "def recon(reg):\n", "    integer = int(np.round((40*reg)**2)) \n", "    for a in range(32):\n", "        if (integer - a) % 31 == 0:\n", "            A = a\n", "    M = (integer - A)//31\n", "    return A, M\n", "train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\n", "train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\n", "train['ps_reg_A'].replace(19,-1, inplace=True)\n", "train['ps_reg_M'].replace(51,-1, inplace=True)\n", "test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\n", "test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\n", "test['ps_reg_A'].replace(19,-1, inplace=True)\n", "test['ps_reg_M'].replace(51,-1, inplace=True)"], "outputs": [], "metadata": {"_cell_guid": "2123222b-efb8-4bda-af06-45a44bb46022", "collapsed": true, "_uuid": "9b5cb463284af71689f14c682b0385cb7a684a30"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Set up folds\n", "K = 4\n", "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n", "y_valid_pred = pd.DataFrame(0*y)\n", "y_test_pred = 0\n", "X = pd.DataFrame(train)\n", "ydf = pd.DataFrame(y)"], "outputs": [], "metadata": {"_cell_guid": "7c6e4823-4e8c-4408-b961-576d469e9241", "collapsed": true, "_uuid": "6aa7ada2193c2e4b8a63eebda925cee5023b45b0"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Set up classifier\n", "params = {\n", "    'eta': 0.025, \n", "    'max_depth': 4, \n", "    'subsample': 0.9, \n", "    'colsample_bytree': 0.7, \n", "    'colsample_bylevel':0.7,\n", "    'min_child_weight':100,\n", "    'alpha':4,\n", "    'objective': 'binary:logistic', \n", "    'eval_metric': 'auc', \n", "    'seed': 99, \n", "    'silent': True\n", "}"], "outputs": [], "metadata": {"_cell_guid": "5d8108f3-e9e8-45d6-93b5-740eb7b4b10b", "collapsed": true, "_uuid": "581c3f15f294378a0e2ac3305e9e3d375f664b21"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Run CV\n", "\n", "for i, (train_index, test_index) in enumerate(kf.split(train)):\n", "    \n", "    # Create data for this fold\n", "    y_train, y_valid = ydf.iloc[train_index].copy(), ydf.iloc[test_index].copy()\n", "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n", "    X_test = test.copy()\n", "    print( \"\\nFold \", i)\n", "\n", "    # Transform data for this fold\n", "    one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n", "    X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing summary stats\n", "    d_median = X_train.median(axis=0)\n", "    d_mean = X_train.mean(axis=0)\n", "    X_train = X_train.fillna(-1)  # Restore -1 for missing values\n", "\n", "    X_train = multi_transform(X_train)\n", "    X_valid = multi_transform(X_valid)\n", "    X_test = multi_transform(X_test)\n", "\n", "    # Run model for this fold\n", "    if OPTIMIZE_ROUNDS:\n", "        watchlist = [(xgb.DMatrix(X_train, y_train), 'train'), \n", "                     (xgb.DMatrix(X_valid, y_valid), 'valid')]\n", "        model = xgb.train( params, xgb.DMatrix(X_train, y_train), MAX_ROUNDS,  \n", "                           watchlist, feval=gini_xgb, maximize=True, \n", "                           verbose_eval=100, early_stopping_rounds=70)\n", "        pred = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n", "        test_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n", "    else:\n", "        model = xgb.train( params, xgb.DMatrix(X_train, y_train), MAX_ROUNDS,  \n", "                           feval=gini_xgb, maximize=True, verbose_eval=100)\n", "        pred = model.predict( xgb.DMatrix(X_valid) )\n", "        test_pred = model.predict( xgb.DMatrix(X_test) )\n", "\n", "    # Save validation predictions for this fold\n", "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n", "    y_valid_pred.iloc[test_index] = pred.reshape( y_valid_pred.iloc[test_index].shape )\n", "    \n", "    # Accumulate test set predictions\n", "    y_test_pred += test_pred\n", "    \n", "y_test_pred /= K  # Average test set predictions\n", "\n", "print( \"\\nGini for full training set:\" )\n", "eval_gini(y, y_valid_pred[0].values)"], "outputs": [], "metadata": {"scrolled": false, "_cell_guid": "c4e48347-920f-4ba7-8b37-cfbaab4c3c00", "collapsed": true, "_uuid": "2b9ed96c98b705d3e4bf2a3d60323dfab4332674"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Save validation predictions for stacking/ensembling\n", "val = pd.DataFrame()\n", "val['id'] = trainid\n", "val['target'] = y_valid_pred[0].values\n", "val.to_csv('forza_pascal_oof.csv', float_format='%.6f', index=False)"], "outputs": [], "metadata": {"_cell_guid": "0e3dfd76-c566-4b8d-a460-b56e964d0772", "collapsed": true, "_uuid": "e61bf4e22c1c29c8358caeecb6e67d6658f2005d"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Create submission file\n", "sub = pd.DataFrame()\n", "sub['id'] = testid\n", "sub['target'] = y_test_pred\n", "sub.to_csv('forza_pascal_test.csv', float_format='%.6f', index=False)"], "outputs": [], "metadata": {"_cell_guid": "f4cbef2c-e52b-4afb-b8ef-904ee9b5f9d5", "collapsed": true, "_uuid": "380fc8053d00cd8bb2796bfd2b59d10cbc4ce7e1"}, "execution_count": null, "cell_type": "code"}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3"}}}