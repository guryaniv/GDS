{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "nbconvert_exporter": "python"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "faa02a48-87ad-45c0-86bd-db7b2c729355", "_uuid": "2aca97c77e100da6bfff313303aaaa1de7411fef"}, "source": ["Based on [my kernel](https://www.kaggle.com/aharless/xgboost-using-4th-quarter-for-validation)"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "f025134c-d894-4be9-9aef-b8ba0fb969a4", "collapsed": true, "_uuid": "049c258d2b526664aafe66ce0d2b9fffe088edaa"}, "source": ["AS_DEMO = True\n", "\n", "if AS_DEMO:\n", "    LR = .05\n", "    NROUNDS = 100\n", "else:\n", "    LR = 0.0008\n", "    NROUNDS = 16000  # Warning: needs to run overnight on multiple cores"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f16ac0de-de1c-4b65-a85e-a3227fbe0c47", "collapsed": true, "_uuid": "47787be314e3cc68e7071ae827aef9710fc8341a"}, "source": ["MAKE_SUBMISSION = True          # Generate output file.\n", "CV_ONLY = False                 # Do validation only; do not generate predicitons.\n", "FIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\n", "FIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\n", "FIT_COMBINED_TRAIN_SET = True  # Fit combined 2016-2017 training set\n", "USE_SEASONAL_FEATURES = True\n", "VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n", "LEARNING_RATE = LR              # shrinkage rate for boosting roudns\n", "ROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\n", "OPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\n", "FUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1c1a4c8e-9379-47b8-aa2e-3df8e5d22918", "collapsed": true, "_uuid": "a6d445499b3664bf165ab992f309a417471707f4", "_execution_state": "idle"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import mean_absolute_error\n", "import datetime as dt\n", "from datetime import datetime\n", "import gc\n", "import patsy\n", "import statsmodels.api as sm\n", "import statsmodels.formula.api as smf\n", "from statsmodels.regression.quantile_regression import QuantReg\n", "from tqdm import tqdm"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a6cc2bdd-9d38-4f66-8903-3ce019f7109a", "collapsed": true, "_uuid": "43ea1cf09b04ca53bb99c95d428b4624572ba75a", "_execution_state": "idle"}, "source": ["properties16 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\n", "properties17 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n", "\n", "# Number of properties in the zip\n", "zip_count = properties16['regionidzip'].value_counts().to_dict()\n", "# Number of properties in the city\n", "city_count = properties16['regionidcity'].value_counts().to_dict()\n", "# Median year of construction by neighborhood\n", "medyear = properties16.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n", "# Mean square feet by neighborhood\n", "meanarea = properties16.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n", "# Neighborhood latitude and longitude\n", "medlat = properties16.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n", "medlong = properties16.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "for c in properties16.columns:\n", "    properties16[c]=properties16[c].fillna(-1)\n", "    if properties16[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties16[c].values))\n", "        properties16[c] = lbl.transform(list(properties16[c].values))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b5752c23-b3d7-4403-ab0b-2813d88e14f5", "collapsed": true, "_uuid": "dfe82baffa246f7ebe77d472de49ee6a6d43a419"}, "source": ["train_df = train.merge(properties16, how='left', on='parcelid')\n", "select_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE\n", "if USE_SEASONAL_FEATURES:\n", "    basedate = pd.to_datetime('2015-11-15').toordinal()\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "101437b6-c54e-4452-9c81-a824818d651a", "_uuid": "95c7a84848e30fa37a75d319b9e7e04deaff9ef9"}, "source": ["del train\n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d9460c6d-bf39-4bfb-a584-f3aafcabc8fa", "collapsed": true, "_uuid": "9722eb834d3cf02e943b8c85bf99e7a7515173e4"}, "source": ["# Inputs to features that depend on target variable\n", "# (Ideally these should be recalculated, and the dependent features recalculated,\n", "#  when fitting to the full training set.  But I haven't implemented that yet.)\n", "\n", "# Standard deviation of target value for properties in the city/zip/neighborhood\n", "citystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "zipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "hoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "058d3299-005e-4458-a8cd-cd3c4d854190", "collapsed": true, "_uuid": "4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77"}, "source": ["def calculate_features(df):\n", "    # Nikunj's features\n", "    # Number of properties in the zip\n", "    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n", "    # Number of properties in the city\n", "    df['N-city_count'] = df['regionidcity'].map(city_count)\n", "    # Does property have a garage, pool or hot tub and AC?\n", "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n", "                         (df['pooltypeid10']>0) & \\\n", "                         (df['airconditioningtypeid']!=5))*1 \n", "\n", "    # More features\n", "    # Mean square feet of neighborhood properties\n", "    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n", "    # Median year of construction of neighborhood properties\n", "    df['med_year'] = df['regionidneighborhood'].map(medyear)\n", "    # Neighborhood latitude and longitude\n", "    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n", "    df['med_long'] = df['regionidneighborhood'].map(medlong)\n", "\n", "    df['zip_std'] = df['regionidzip'].map(zipstd)\n", "    df['city_std'] = df['regionidcity'].map(citystd)\n", "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n", "    \n", "    if USE_SEASONAL_FEATURES:\n", "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n", "                             (2*np.pi/365.25) ).apply(np.cos)\n", "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n", "                             (2*np.pi/365.25) ).apply(np.sin)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e389a360-b1dc-4257-b3c3-fea69026863d", "collapsed": true, "_uuid": "d816890fe0edcab13c415144e76ecd12938ab17e"}, "source": ["dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n", "            'buildingqualitytypeid', 'regionidcity']\n", "droptrain = ['parcelid', 'logerror', 'transactiondate']\n", "droptest = ['ParcelId']"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "720a950e-5392-4571-8fd6-e849a9c6967e", "collapsed": true, "_uuid": "233443f9b4c2af79e91d235fdc4f660ff1d630bc"}, "source": ["calculate_features(train_df)\n", "\n", "x_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\n", "y_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n", "\n", "print('Shape full training set: {}'.format(train_df.shape))\n", "print('Dropped vars: {}'.format(len(dropvars+droptrain)))\n", "print('Shape valid X: {}'.format(x_valid.shape))\n", "print('Shape valid y: {}'.format(y_valid.shape))\n", "\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "print('\\nFull training set after removing outliers, before dropping vars:')     \n", "print('Shape training set: {}\\n'.format(train_df.shape))\n", "\n", "if FIT_FULL_TRAIN_SET:\n", "    full_train = train_df.copy()\n", "\n", "train_df=train_df[~select_qtr4]\n", "x_train=train_df.drop(dropvars+droptrain, axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "n_train = x_train.shape[0]\n", "print('Training subset after removing outliers:')     \n", "print('Shape train X: {}'.format(x_train.shape))\n", "print('Shape train y: {}'.format(y_train.shape))\n", "\n", "if FIT_FULL_TRAIN_SET:\n", "    x_full = full_train.drop(dropvars+droptrain, axis=1)\n", "    y_full = full_train[\"logerror\"].values.astype(np.float32)\n", "    n_full = x_full.shape[0]\n", "    print('\\nFull trainng set:')     \n", "    print('Shape train X: {}'.format(x_train.shape))\n", "    print('Shape train y: {}'.format(y_train.shape))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d0c5819c-0873-4b49-a8a7-5295632ccbb9", "collapsed": true, "_uuid": "f3a9ea8db9161705b9e0d61a82736b44c7dfcb01", "_execution_state": "idle"}, "source": ["if not CV_ONLY:\n", "    # Generate test set data\n", "    \n", "    sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n", "    \n", "    # Process properties for 2016\n", "    test_df = pd.merge( sample_submission[['ParcelId']], \n", "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n", "                        how = 'left', on = 'ParcelId' )\n", "    if USE_SEASONAL_FEATURES:\n", "        test_df['transactiondate'] = '2017-04-16'\n", "            # TO GENERATE DATA FOR WEIGHTING\n", "        droptest += ['transactiondate']\n", "    calculate_features(test_df)\n", "    x_test = test_df.drop(dropvars+droptest, axis=1)\n", "    print('Shape test: {}'.format(x_test.shape))\n", "\n", "    # Process properties for 2017\n", "    for c in properties17.columns:\n", "        properties17[c]=properties17[c].fillna(-1)\n", "        if properties17[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties17[c].values))\n", "            properties17[c] = lbl.transform(list(properties17[c].values))\n", "    zip_count = properties17['regionidzip'].value_counts().to_dict()\n", "    city_count = properties17['regionidcity'].value_counts().to_dict()\n", "    medyear = properties17.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n", "    meanarea = properties17.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n", "    medlat = properties17.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n", "    medlong = properties17.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n", "\n", "    test_df = pd.merge( sample_submission[['ParcelId']], \n", "                        properties17.rename(columns = {'parcelid': 'ParcelId'}), \n", "                        how = 'left', on = 'ParcelId' )\n", "    if USE_SEASONAL_FEATURES:\n", "        test_df['transactiondate'] = '2017-04-16'\n", "    calculate_features(test_df)\n", "    x_test17 = test_df.drop(dropvars+droptest, axis=1)\n", "\n", "    del test_df"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "03fb6b2f-5166-4646-8445-304765a404ad", "_uuid": "c6d653da30197972f12f1a7a0ea4d0a36f28c554", "_execution_state": "idle"}, "source": ["del train_df\n", "del select_qtr4\n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "de59798a-1b8a-4d0e-a92c-846577f4406b", "collapsed": true, "_uuid": "c8c1043d72790dc926ae76f398487e8313caeed1", "_execution_state": "idle"}, "source": ["xgb_params = {  # best as of 2017-09-28 13:20 UTC\n", "    'eta': LEARNING_RATE,\n", "    'max_depth': 7, \n", "    'subsample': 0.6,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 5.0,\n", "    'alpha': 0.65,\n", "    'colsample_bytree': 0.5,\n", "    'base_score': y_mean,'taxdelinquencyyear'\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dvalid_x = xgb.DMatrix(x_valid)\n", "dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n", "if not CV_ONLY:\n", "    dtest = xgb.DMatrix(x_test)\n", "    dtest17 = xgb.DMatrix(x_test17)\n", "    del x_test"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "45e61d05-da7f-4841-811a-3a95c86a3306", "_uuid": "dd0776733ec3d3ac960b9ab9ac6e518a99e012d9", "_execution_state": "idle"}, "source": ["del x_train\n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "de8e97e6-d681-465b-a68a-6178e7593e49", "collapsed": true, "_uuid": "fe6761fd44aaf7a5c063897c9ca3b90b957b7108", "scrolled": true}, "source": ["num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\n", "early_stopping_rounds = round( num_boost_rounds / 20 )\n", "print('Boosting rounds: {}'.format(num_boost_rounds))\n", "print('Early stoping rounds: {}'.format(early_stopping_rounds))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "95d8ac93-f2ff-4ce3-9cb6-8044d73f8600", "collapsed": true, "_uuid": "09c99f75bcfb64225f390eee5e5dbf4f47f4103b", "_execution_state": "idle"}, "source": ["evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n", "model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n", "                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n", "                  verbose_eval=10)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "c241b1e2-d2c6-4936-a6ee-282a4e90ca27", "_uuid": "02a37067bd8ba72b050d6634e9994b234bdf6389", "_execution_state": "idle"}, "source": ["valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n", "print( \"XGBoost validation set predictions:\" )\n", "print( pd.DataFrame(valid_pred).head() )\n", "print(\"\\nMean absolute validation error:\")\n", "mean_absolute_error(y_valid, valid_pred)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "cb819ca9-0791-4d22-8381-db33f98ae2b2", "_uuid": "94e8b6f65061da41ed522be7a651c3b9622b25f5"}, "source": ["0.064258203"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "7dd798fb-9160-4a48-9b52-9bc16f582e0f", "collapsed": true, "_uuid": "dbd7ae96da81ab900d6eee1c186b2b56f17e4fc8"}, "source": ["if OPTIMIZE_FUDGE_FACTOR:\n", "    mod = QuantReg(y_valid, valid_pred)\n", "    res = mod.fit(q=.5)\n", "    print(\"\\nLAD Fit for Fudge Factor:\")\n", "    print(res.summary())\n", "\n", "    fudge = res.params[0]\n", "    print(\"Optimized fudge factor:\", fudge)\n", "    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n", "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n", "\n", "    fudge **= FUDGE_FACTOR_SCALEDOWN\n", "    print(\"Scaled down fudge factor:\", fudge)\n", "    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n", "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n", "else:\n", "    fudge=1.0"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "16612d2b-411a-4372-a7df-3878aa1d3193", "collapsed": true, "_uuid": "ec7549abb9c34aff1d8ed41d3fe52930e76b8070"}, "source": ["if FIT_FULL_TRAIN_SET and not CV_ONLY:\n", "    if FIT_COMBINED_TRAIN_SET:\n", "        # Merge 2016 and 2017 data sets\n", "        train16 = pd.read_csv('../input/train_2016_v2.csv')\n", "        train17 = pd.read_csv('../input/train_2017.csv')\n", "        train16 = pd.merge(train16, properties16, how = 'left', on = 'parcelid')\n", "        train17 = pd.merge(train17, properties17, how = 'left', on = 'parcelid')\n", "        train_df = pd.concat([train16, train17], axis = 0)\n", "        # Generate features\n", "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n", "        calculate_features(train_df)\n", "        # Remove outliers\n", "        train_df=train_df[ train_df.logerror > -0.4 ]\n", "        train_df=train_df[ train_df.logerror < 0.419 ]\n", "        # Create final training data sets\n", "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n", "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n", "        n_full = x_full.shape[0]     \n", "    elif FIT_2017_TRAIN_SET:\n", "        train = pd.read_csv('../input/train_2017.csv')\n", "        train_df = train.merge(properties17, how='left', on='parcelid')\n", "        # Generate features\n", "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n", "        calculate_features(train_df)\n", "        # Remove outliers\n", "        train_df=train_df[ train_df.logerror > -0.4 ]\n", "        train_df=train_df[ train_df.logerror < 0.419 ]\n", "        # Create final training data sets\n", "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n", "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n", "        n_full = x_full.shape[0]     \n", "\n", "    dtrain = xgb.DMatrix(x_full, y_full)\n", "    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "74ad3d77-75eb-447c-9524-21c225507879", "_uuid": "a39b9309cbaa1588b0646a079c4bfea929990cc1"}, "source": ["del properties16\n", "del properties17\n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "04257a56-75cb-4939-a894-9711d6cc8c89", "collapsed": true, "_uuid": "30daaece89b862bed60db26e919df38d6541e43d"}, "source": ["optimal_number_of_trees = NROUNDS\n", "num_ensembles = 10\n", "preds = 0.0\n", "preds17 = 0.0\n", "for i in range(num_ensembles):\n", "    xgb_params['seed'] = i\n", "    full_model = xgb.train(xgb_params, dtrain, num_boost_round=optimal_number_of_trees, \n", "                           evals=[(dtrain,'train')], verbose_eval=False)\n", "    pred = fudge*full_model.predict(dtest)\n", "    pred17 = fudge*full_model.predict(dtest17)\n", "    print( \"\\ni = \", i)\n", "    print( \"XGBoost test set predictions for 2016:\" )\n", "    print( pd.DataFrame(pred).head() )\n", "    print( \"XGBoost test set predictions for 2017:\" )\n", "    print( pd.DataFrame(pred17).head() )\n", "    preds += pred\n", "    preds17 += pred17\n", "preds /= num_ensembles\n", "\n", "# SHOULD ALSO CONTAIN THE LINE \"preds17 /= num_ensembles\"\n", "# BUT I NEGLECTED TO ADD IT, SO RESULTS ARE CRAZY\n", "\n", "print( \"\\n\\nFinal Ensemble\")\n", "print( \"XGBoost test set predictions for 2016:\" )\n", "print( pd.DataFrame(preds).head() )\n", "print( \"XGBoost test set predictions for 2017:\" )\n", "print( pd.DataFrame(preds17).head() )\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "8d793a38-6df2-45e7-a725-cc2161ed3cee", "collapsed": true, "_uuid": "b85616fbde70ef9d6fe220ef12947fb5d2dd3df6", "_execution_state": "idle"}, "source": ["y_pred=[]\n", "y_pred17=[]\n", "\n", "for i,predict in enumerate(preds):\n", "       y_pred.append(str(round(predict,4)))\n", "for i,predict in enumerate(preds17):\n", "       y_pred17.append(str(round(predict,4)))\n", "y_pred=np.array(y_pred)\n", "y_pred17=np.array(y_pred17)\n", "\n", "output = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n", "           '201610': y_pred, '201611': y_pred, '201612': y_pred,\n", "           '201710': y_pred17, '201711': y_pred17, '201712': y_pred17})\n", "   # set col 'ParceID' to first col\n", "cols = output.columns.tolist()\n", "cols = cols[-1:] + cols[:-1]\n", "output = output[cols]\n", "\n", "output.to_csv('finalXgbApril{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), \n", "              float_format='%.6f',\n", "              index=False)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b66c3b07-5941-44ab-b2f2-58fc46071055", "collapsed": true, "_uuid": "b26353143ba91c98ba970f5051bc9656b39c9bb4"}, "source": [], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e5b97a3f-4428-487c-abd4-9507d7850260", "collapsed": true, "_uuid": "9944aa5d56350e51a0b151af9cf38c7b464dcf0b"}, "source": [], "execution_count": null, "cell_type": "code", "outputs": []}]}