{"nbformat": 4, "cells": [{"source": ["# PyTorch GPU based CNN with predictions\n", "\n", "This notebook is heavily based on:\n", "https://www.kaggle.com/nanigans/pytorch-starter\n", "\n", "Modifications:\n", "1. Uses a custom data loader based on torch.utils.data.Dataset\n", "1. Works on a **GPU** as well as on a **CPU** out of the box\n", "1. Runs predictions on the **Test data set**\n", "1. Creates a **submission file**"], "metadata": {"_cell_guid": "9dd723cf-8dee-4234-94fe-eb7996388f62", "_uuid": "292c4605e2c18ebc4a1bc4879c3344dabb20fcee"}, "cell_type": "markdown"}, {"source": ["import torch\n", "import sys\n", "import torch\n", "from torch.utils.data.dataset import Dataset\n", "from torch.utils.data import DataLoader\n", "from torchvision import transforms\n", "from torch import nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.autograd import Variable\n", "\n", "from sklearn import cross_validation\n", "from sklearn import metrics\n", "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n", "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n", "\n", "print('__Python VERSION:', sys.version)\n", "print('__pyTorch VERSION:', torch.__version__)\n", "\n", "import numpy\n", "import numpy as np\n", "\n", "use_cuda = torch.cuda.is_available()\n", "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n", "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n", "Tensor = FloatTensor\n", "\n", "import pandas\n", "import pandas as pd\n", "\n", "import logging\n", "handler=logging.basicConfig(level=logging.INFO)\n", "lgr = logging.getLogger(__name__)\n", "%matplotlib inline\n", "\n", "# !pip install psutil\n", "import psutil\n", "import os\n", "def cpuStats():\n", "        print(sys.version)\n", "        print(psutil.cpu_percent())\n", "        print(psutil.virtual_memory())  # physical memory usage\n", "        pid = os.getpid()\n", "        py = psutil.Process(pid)\n", "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n", "        print('memory GB:', memoryUse)\n", "\n", "cpuStats()"], "metadata": {"_cell_guid": "303a9986-71cd-41d0-b53a-0b26ac4007cb", "_uuid": "c1cf1feee03e3784251c9ae345244d9ae5375af3", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# %%timeit\n", "use_cuda = torch.cuda.is_available()\n", "# use_cuda = False\n", "\n", "lgr.info(\"USE CUDA=\" + str (use_cuda))\n", "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n", "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n", "Tensor = FloatTensor\n"], "metadata": {"_cell_guid": "7358dba4-24e6-41ca-8ed3-8776a1914f3e", "_uuid": "98b66f1ed8ea1a6e776d038d0c445ea20f3e53b0", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# fix seed\n", "seed=17*19\n", "np.random.seed(seed)\n", "torch.manual_seed(seed)\n", "if use_cuda:\n", "    torch.cuda.manual_seed(seed)"], "metadata": {"_cell_guid": "7f63eb8d-84cb-44fc-8622-3fb5fa128a28", "_uuid": "4713772f686ceef6a2ca8f95115467444aba6341", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Load data"], "metadata": {"_cell_guid": "90a5dbd6-75e9-4183-8e09-8e91c63c0b10", "_uuid": "0901ccf48a660dd6a01287cf8d648e7d3a37d715"}, "cell_type": "markdown"}, {"source": ["data = pd.read_json('../input/train.json')\n", "\n", "data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n", "data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n", "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n", "\n", "\n", "band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n", "band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n", "full_img = np.stack([band_1, band_2], axis=1)\n"], "metadata": {"_cell_guid": "5861c386-86d2-4177-900e-7ed30aa76795", "_uuid": "db4d4d6154afcb3a5d444b0f288f04b9b15e35c0", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# From Numpy to PyTorch GPU tensors"], "metadata": {"_cell_guid": "f5c1b74a-631e-4bf6-9188-0eba5905e26c", "_uuid": "b4d6dd93df146fb4026832aa447b4663024dd63c"}, "cell_type": "markdown"}, {"source": ["# Convert the np arrays into the correct dimention and type\n", "# Note that BCEloss requires Float in X as well as in y\n", "def XnumpyToTensor(x_data_np):\n", "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n", "#     print(x_data_np.shape)\n", "#     print(type(x_data_np))\n", "\n", "    if use_cuda:\n", "#         lgr.info (\"Using the GPU\")    \n", "        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n", "    else:\n", "#         lgr.info (\"Using the CPU\")\n", "        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n", "        \n", "#     print((X_tensor.shape)) # torch.Size([108405, 29])\n", "    return X_tensor\n", "\n", "\n", "# Convert the np arrays into the correct dimention and type\n", "# Note that BCEloss requires Float in X as well as in y\n", "def YnumpyToTensor(y_data_np):    \n", "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n", "#     print(y_data_np.shape)\n", "#     print(type(y_data_np))\n", "\n", "    if use_cuda:\n", "#         lgr.info (\"Using the GPU\")            \n", "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n", "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n", "    else:\n", "#         lgr.info (\"Using the CPU\")        \n", "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n", "        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n", "\n", "#     print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n", "#     print(y_data_np.shape)\n", "#     print(type(y_data_np))    \n", "    return Y_tensor"], "metadata": {"_cell_guid": "15bfdd0f-681b-4d76-a19e-1491e83bbb7c", "_uuid": "31d151252657c896e29784513f00cdd555efa1e7", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Train / Validation / Test Split"], "metadata": {"_cell_guid": "d5550f4d-6871-449c-a084-901f7e266ef9", "_uuid": "3b4b633950fe1623fb1a314fc3fb2d6458978e87"}, "cell_type": "markdown"}, {"source": ["class FullTrainningDataset(torch.utils.data.Dataset):\n", "    def __init__(self, full_ds, offset, length):\n", "        self.full_ds = full_ds\n", "        self.offset = offset\n", "        self.length = length\n", "        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n", "        super(FullTrainningDataset, self).__init__()\n", "        \n", "    def __len__(self):        \n", "        return self.length\n", "    \n", "    def __getitem__(self, i):\n", "        return self.full_ds[i+self.offset]\n", "    \n", "validationRatio=0.11    \n", "\n", "def trainTestSplit(dataset, val_share=validationRatio):\n", "    val_offset = int(len(dataset)*(1-val_share))\n", "    print (\"Offest:\" + str(val_offset))\n", "    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, val_offset, len(dataset)-val_offset)"], "metadata": {"_cell_guid": "cc4ffad0-682b-4189-90ff-2ba3f7dd06d1", "_uuid": "03a65e001dd2f489526d69d9050ced2c7f86b278", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Dataset and DataLoader"], "metadata": {"_cell_guid": "f845a93e-faa5-40c6-af89-14e813a8ba34", "_uuid": "e99daa83567a13ab4e86e9451e03995c731e6c2b"}, "cell_type": "markdown"}, {"source": ["from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n", "\n", "batch_size=128\n", "\n", "transformations = transforms.Compose([transforms.ToTensor()])\n", "\n", "# train_imgs = torch.from_numpy(full_img_tr).float()\n", "train_imgs=XnumpyToTensor (full_img)\n", "train_targets = YnumpyToTensor(data['is_iceberg'].values)\n", "dset_train = TensorDataset(train_imgs, train_targets)\n", "\n", "\n", "train_ds, val_ds = trainTestSplit(dset_train)\n", "\n", "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n", "                                            num_workers=1)\n", "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n", "\n", "print (train_loader)\n", "print (val_loader)"], "metadata": {"_cell_guid": "8c2b51ec-1eeb-4519-bb56-49a3dc525386", "_uuid": "dccb943235b8fd0cae479601e70f864526a8df86", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["## Define simple model"], "metadata": {"_cell_guid": "9655dd9c-c792-4d15-bb8b-94a800eb9c0e", "_uuid": "21eacbdcf5f04eecf86da49621b5aff4d3b92d8d"}, "cell_type": "markdown"}, {"source": ["import math \n", "def conv3x3(in_planes, out_planes, stride=1):\n", "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n", "    \n", "\n", "class CifarResNet(nn.Module):\n", "    def __init__(self, block, n_size, num_classes=1):\n", "        super(CifarResNet, self).__init__()\n", "        self.inplane = 16\n", "        self.conv1 = nn.Conv2d(2, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n", "        self.bn1 = nn.BatchNorm2d(self.inplane)\n", "        self.relu = nn.ReLU(inplace=True)\n", "        self.layer1 = self._make_layer(block, 16, blocks=2 * n_size, stride=2)\n", "        self.layer2 = self._make_layer(block, 32, blocks=2 * n_size, stride=2)\n", "        self.layer3 = self._make_layer(block, 64, blocks=2 * n_size, stride=2)\n", "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n", "        self.fc = nn.Linear(64, num_classes)\n", "        self.sig=nn.Sigmoid()   \n", "        \n", "        for m in self.modules():\n", "            if isinstance(m, nn.Conv2d):\n", "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n", "                m.weight.data.normal_(0, math.sqrt(2. / n))\n", "            elif isinstance(m, nn.BatchNorm2d):\n", "                m.weight.data.fill_(1)\n", "                m.bias.data.zero_()\n", "\n", "    def _make_layer(self, block, planes, blocks, stride):\n", "\n", "        layers = []\n", "        for i in range(1, blocks):\n", "            layers.append(block(self.inplane, planes, stride))\n", "            self.inplane = planes\n", "\n", "        return nn.Sequential(*layers)\n", "\n", "    def forward(self, x):\n", "        x = self.conv1(x)\n", "        x = self.bn1(x)\n", "        x = self.relu(x)\n", "\n", "        x = self.layer1(x)\n", "        x = self.layer2(x)\n", "        x = self.layer3(x)\n", "\n", "        x = self.avgpool(x)\n", "        x = x.view(x.size(0), -1)\n", "        x = self.fc(x)\n", "        x = self.sig(x)\n", "\n", "        return x\n", "\n", "model = CifarResNet(CifarSEBasicBlock, 1, 1)        \n", "print (model)"], "metadata": {"_cell_guid": "03178a5e-e8ef-4334-a97d-f1cf7c0fecf8", "_uuid": "4112e0197ba91e6ab2d39c65d67ed112fdc78729", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["loss_func=torch.nn.BCELoss()\n", "# NN params\n", "LR = 0.005\n", "MOMENTUM= 0.9\n", "optimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\n", "if use_cuda:\n", "    lgr.info (\"Using the GPU\")    \n", "    net.cuda()\n", "    loss_func.cuda()\n", "\n", "lgr.info (optimizer)\n", "lgr.info (loss_func)"], "metadata": {"_cell_guid": "7025d0d8-206f-4cb0-af69-7b1a85ebd7f8", "_uuid": "abad58921a6aeb5ea0603056bb0021436b73d039", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["num_epoches = 20\n", "criterion=loss_func\n", "all_losses = []\n", "\n", "\n", "model.train()\n", "for epoch in range(num_epoches):\n", "    print('Epoch {}'.format(epoch + 1))\n", "    print('*' * 5 + ':')\n", "    running_loss = 0.0\n", "    running_acc = 0.0\n", "    for i, data in enumerate(train_loader, 1):\n", "        \n", "        img, label = data        \n", "        img = Variable(img)\n", "        label = Variable(label)\n", "                        \n", "        out = model(img)\n", "        loss = criterion(out, label)\n", "        running_loss += loss.data[0] * label.size(0)        \n", "        \n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "                                \n", "        if i % 10 == 0:\n", "            all_losses.append(running_loss / (batch_size * i))\n", "            print('[{}/{}] Loss: {:.6f}'.format(\n", "                epoch + 1, num_epoches, running_loss / (batch_size * i),\n", "                running_acc / (batch_size * i)))\n", "            \n", "    print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n", "\n", "torch.save(model.state_dict(), './cnn.pth')"], "metadata": {"_cell_guid": "86849fe7-7c0c-4cde-be41-b5809ec7862b", "_uuid": "531c3edfa7dce5409c72f87a19160f9d4150b30b", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["from sklearn.cross_validation import train_test_split\n", "\n", "def kFoldValidation(folds): \n", "    print ('K FOLD VALIDATION ...')\n", "    val_losses = []\n", "    model.eval()\n", "    \n", "    for e in range(folds):\n", "        print ('Fold:' + str(e))        \n", "        data = pd.read_json('../input/train.json')        \n", "        data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n", "        data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n", "        data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n", "        band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n", "        band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n", "        full_img = np.stack([band_1, band_2], axis=1)\n", "        \n", "        X_train,X_val,y_train,y_val=train_test_split(full_img,data['is_iceberg'].values,\n", "                                                   test_size=0.22, \n", "                                                   random_state=(1+e))\n", "        val_imgs=XnumpyToTensor (X_val)\n", "        val_targets = YnumpyToTensor(y_val)\n", "        \n", "        dset_val = TensorDataset(val_imgs, val_targets)        \n", "        val_loader = torch.utils.data.DataLoader(dset_val, batch_size=batch_size, shuffle=True, \n", "                                                    num_workers=1)        \n", "        print (val_loader)\n", "\n", "        eval_loss = 0\n", "        eval_acc = 0\n", "        for data in val_loader:\n", "            img, label = data\n", "\n", "            img = Variable(img, volatile=True)\n", "            label = Variable(label, volatile=True)\n", "\n", "            out = model(img)\n", "            loss = criterion(out, label)\n", "            eval_loss += loss.data[0] * label.size(0)\n", "\n", "        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(dset_val))))\n", "        val_losses.append(eval_loss / (len(dset_val)))\n", "        print()\n", "    \n", "def LeavOneOutValidation(val_loader): \n", "    print ('Leave one out VALIDATION ...')\n", "    val_losses = []\n", "    model.eval()\n", "        \n", "    print (val_loader)\n", "\n", "    eval_loss = 0\n", "    eval_acc = 0\n", "    for data in val_loader:\n", "        img, label = data\n", "\n", "        img = Variable(img, volatile=True)\n", "        label = Variable(label, volatile=True)\n", "\n", "        out = model(img)\n", "        loss = criterion(out, label)\n", "        eval_loss += loss.data[0] * label.size(0)\n", "\n", "    print('Leave on out VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n", "    val_losses.append(eval_loss / (len(val_ds)))\n", "    print()\n", "    print()        \n", "    \n", "LeavOneOutValidation(val_loader)    \n", "# kFoldValidation(10)"], "metadata": {"_cell_guid": "d7a94ac7-9d78-4aef-9401-0cad5840af9a", "_uuid": "4ace55d09023eeac33296d70c037bda67701d45a", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Prediction on test data set"], "metadata": {"_cell_guid": "3f6d0cdd-ae03-4128-9d7b-7797d58bdfb5", "_uuid": "151d206a8aafec78833f400179fa58b6d63bfed4"}, "cell_type": "markdown"}, {"source": ["df_test_set = pd.read_json('../input/test.json')\n", "\n", "df_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n", "df_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n", "df_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n", "\n", "df_test_set.head(3)"], "metadata": {"_cell_guid": "a5316df6-e3fa-4372-a284-30ef1d143262", "_uuid": "2c73c83252ce8d9977df59a40fa000e5b10ae32d", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["print (df_test_set.shape)\n", "columns = ['id', 'is_iceberg']\n", "df_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n", "# df_pred.id.astype(int)\n", "\n", "for index, row in df_test_set.iterrows():\n", "    rwo_no_id=row.drop('id')    \n", "    band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n", "    band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n", "    full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n", "\n", "    x_data_np = np.array(full_img_test, dtype=np.float32)        \n", "    if use_cuda:\n", "        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n", "    else:\n", "        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n", "                    \n", "#     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n", "    predicted_val = (model(X_tensor_test).data).float() # probabilities     \n", "    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n", "    \n", "    df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n", "#     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n", "\n", "df_pred.head(5)"], "metadata": {"_cell_guid": "d80fe5d9-07bc-4254-bef9-d8721b9bd226", "_uuid": "c486408992e2dfcf0d1a5730d3f6bb051c839c91", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Create a CSV with the ID's and the coresponding probabilities."], "metadata": {"_cell_guid": "47afcb96-d2cc-4212-9b15-a33f0777c39b", "_uuid": "fcd82743cc346d03f97ad6c2684cb113ab71c1ac"}, "cell_type": "markdown"}, {"source": ["# df_pred.id=df_pred.id.astype(int)\n", "\n", "def savePred(df_pred):\n", "#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n", "#     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n", "    csv_path='sample_submission.csv'\n", "    df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n", "    print (csv_path)\n", "    \n", "savePred (df_pred)"], "metadata": {"_kg_hide-output": false, "_cell_guid": "24ebbc8b-01e9-4f8f-8682-34b54720a082", "_kg_hide-input": false, "_uuid": "949e671d86d82fe3fea05bbca355aa380a043405", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "6d70809a-43c5-40e0-bcbd-2e57edfca9d7", "_uuid": "a0c5d23ee263765549dcc03e80c8d9c53fbb1944", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "9ad22517-6a3b-4361-bc7a-f099034b356b", "_uuid": "4c32cd8ca4e3611704a172661b9ba759f6b309ce", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "08fd0448-01b3-4a6a-95e5-9ce36ad9bcb9", "_uuid": "86a735b3faa63af3b1420678e1fb812a4bb95f56", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "174e096e-6cbd-40b5-8553-2560bf28582d", "_uuid": "dc443e947603c93d1eeb77bb0dfde5f9f297604b", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "c206f5a4-7b13-4845-8327-9c7f5406fe90", "_uuid": "2b02d612822db2e58f39591c8e329b97d6283765", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "ba365f1e-911b-401e-b9ce-9b763c8e4f10", "_uuid": "6bfa0dbab5b766e9fc38c1dc0bde2f0e1952b5d5", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "c971cf5e-2bb2-4b82-b09a-329e66a2854c", "_uuid": "4ad034f7f7947e36fdaac0e2894d35de27054d2b", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "a555a4c2-ab48-4e7c-8634-b0b65811ff2c", "_uuid": "bf2b39e71d690f3900b916176435974ab10c5918", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "afcb62b2-2b25-4873-a09f-0caacb5a7fb2", "_uuid": "e836872cf64407e8e2ec6d76906fa6ead82893f1", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"_cell_guid": "6b154a7d-69bc-4acc-9287-4b7bb38eff90", "_uuid": "b6581f6927bba3c816a4ef4f71ddf91956c0ef68", "collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}