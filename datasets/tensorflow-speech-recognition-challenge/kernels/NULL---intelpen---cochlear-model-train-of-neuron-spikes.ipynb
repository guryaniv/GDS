{"nbformat_minor": 1, "cells": [{"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["# Any results you write to the current directory are saved as output.\n", "\n", "import cochlea\n", "from scipy.io import wavfile\n", "\n", "import numpy as np\n", "import scipy\n", "import pandas as pd\n", "#import os\n", "#import multiprocessing\n", "\n", "def accumulate(spike_trains, ignore=None, keep=None):\n", "    \"\"\"Concatenate spike trains with the same meta data. Trains will\n", "    be sorted by the metadata.\n", "    \"\"\"\n", "\n", "    assert None in (ignore, keep)\n", "\n", "    keys = spike_trains.columns.tolist()\n", "\n", "    if ignore is not None:\n", "        for k in ignore:\n", "            keys.remove(k)\n", "\n", "    if keep is not None:\n", "        keys = keep\n", "\n", "    if 'duration' not in keys:\n", "        keys.append('duration')\n", "\n", "    if 'spikes' in keys:\n", "        keys.remove('spikes')\n", "\n", "\n", "    groups = spike_trains.groupby(keys, as_index=False)\n", "\n", "    acc = []\n", "    for name,group in groups:\n", "        if not isinstance(name, tuple):\n", "            name = (name,)\n", "        spikes = np.concatenate(tuple(group['spikes']))\n", "        acc.append(name + (spikes,))\n", "\n", "    columns = list(keys)\n", "    columns.append('spikes')\n", "\n", "    acc = pd.DataFrame(acc, columns=columns)\n", "\n", "    return acc\n", "\n", "\n", "\n", "def extractSpikes(filename , channelsNo = 128 , colsNo = 512):\n", "  fs, samples = wavfile.read(filename)\n", "  norm = (500.0)/np.max(np.abs(samples))\n", "  samples = norm*samples  \n", "  samples = cochlea.set_dbspl(samples,50)\n", "  fs100kHz = 100e3\n", "  down = fs*100/fs100kHz \n", "  samples100kHz =scipy.signal.resample_poly(samples,up = 100, down = down)\n", "  samples100kHz = cochlea.set_dbspl(samples100kHz,50)\n", "  anf_trains = cochlea.run_zilany2014(sound= samples100kHz, fs = fs100kHz, anf_num = (0,0,50), cf= (125,10e3,channelsNo) ,seed = 0,\n", "                                    powerlaw=\"approximate\",  species = 'human' )\n", "  anf_trains= accumulate(anf_trains)\n", "  spikes = anf_trains[\"spikes\"].as_matrix()\n", "  spikesImage = np.zeros(shape = [channelsNo, colsNo])\n", "  maxTime=np.concatenate(spikes).max()\n", "  for index in range(spikes.shape[0]):\n", "    spikesNorm = (colsNo-1)*spikes[index]/maxTime\n", "    spikesInt = spikesNorm.astype(dtype = np.int)\n", "    spikesImage[index, spikesInt] = 1 \n", "  #plt.figure()\n", "  #plt.imshow(spikesImage)\n", "  #plt.show()\n", "  return spikesImage\n", "def extractSpikesParallel(arguments):\n", "  try :\n", "    filenameIn, filenameOut = arguments\n", "    print(filenameIn)\n", "    print(filenameOut)\n", "    spikesTrain = extractSpikes(filenameIn, channelsNo = 128 , colsNo = 512)\n", "    np.savetxt(fname =filenameOut, X =spikesTrain,fmt=\"%d\", delimiter = \";\")  \n", "    print(\"finished\"+filenameOut)\n", "  except :\n", "    print(\"There was an error here\")\n", "  return 1"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7c95595c-35ca-45d8-b0ac-a201057d842f", "_uuid": "b9ca9538381a03bdc8e0fd25ce2c7c5f94a78e66"}, "source": ["\n", "\n", "\n", "from scipy import misc\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.python.framework import ops\n", "from tf_utils import load_dataset, random_mini_batchesT, convert_to_one_hot\n", "import sys\n", "#np.random.seed(1)\n", "import time\n", "\n", "labelsList = [\"_silence_\",\"_unknown_\",\"yes\",\"no\",\"up\",\"down\",\"left\",\"right\",\"on\",\"off\",\"stop\",\"go\"]\n", "noClasses = len(labelsList)\n", "#%%\n", "class Params:\n", "  imageWidth = 512\n", "  imageHeight = 128\n", "  def __init__(self, imageWidth = 512, imageHeight = 128):\n", "    self.imageWidth = imageWidth\n", "    self.imageHeight = imageHeight\n", "\n", "\n", "def load_dataset(rootDir, labelsList):\n", "  print(rootDir)\n", "  X = []\n", "  Y = []  \n", "  for label in os.listdir(rootDir):\n", "    \n", "    dirLabel = rootDir + \"//\" + label\n", "    print('Found directory: %s' % dirLabel)\n", "    index = 0\n", "    for filename in os.listdir(dirLabel):\n", "       if index % 1 == 0 :\n", "          #print('\\t%s' % fname)\n", "          filename = dirLabel+\"//\"+ filename\n", "          image= np.zeros(shape = [128,512], dtype = np.uint8)\n", "          img = pd.read_csv(filename, sep=\";\").as_matrix()\n", "          image[0:127,:] = img\n", "          \n", "          X.append(image)\n", "          yL = np.zeros(shape = [noClasses])\n", "          yL[labelsList.index(label)]=1\n", "          Y.append(yL)\n", "          \n", "          #if index > 500:\n", "          #  break\n", "       index = index +1\n", "  X = np.array(X)\n", "  Y= np.array(Y)\n", "  \n", "  print(X.shape)\n", "  print(Y.shape)\n", "  \n", "  permutation = list(np.random.permutation(X.shape[0]))\n", "  X = X[permutation,:]\n", "  Y = Y[permutation,:]\n", "  \n", "  no_examples =X.shape[0] \n", "  X_Train= X[0:int(0.90*no_examples),:]\n", "  Y_Train =Y[0:int(0.90*no_examples),:] \n", "  X_Test = X[int(0.90*no_examples):int(1*no_examples),:]\n", "  Y_Test = Y[int(0.90*no_examples):int(1*no_examples),:]\n", "  classes = labelsList\n", "  return X_Train, Y_Train, X_Test, Y_Test, classes\n", "\n", "def deepnn(x):\n", "  noClasses=12\n", "  \"\"\"deepnn builds the graph for a deep net for classifying digits.  Args:\n", "    x: an input tensor with the dimensions (N_examples, 3*92*46), where 784 is the\n", "    number of pixels in a standard MNIST image.\n", "  Returns:\n", "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, noClasses), with values equal to the logits of classifying the imagesinto one of 2 classes .\n", "    keep_prob is a scalar placeholder for the probability of     dropout.\n", "  \"\"\"\n", "  # Reshape to use within a convolutional neural net.   # Last dimension - it would be 3 for an RGB image, 4 for RGBA, etc.\n", "  XFloat = tf.cast(x, dtype = tf.float32)\n", "  #XNorm = tf.scalar_mul(1.0/255.0, XFloat)\n", "  \n", "  with tf.name_scope('reshape'):\n", "    x_image = tf.reshape(XFloat, [-1, 128, 512, 1]) #was 28 28, 28, 1\n", "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n", "  with tf.name_scope('conv1'):\n", "    W_conv1 = weight_variable([5, 5, 1, 16]) #W_conv1 = weight_variable([5, 5, 1, 32]) try fatter features\n", "    b_conv1 = bias_variable([16])\n", "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n", "\n", "  # Pooling layer - downsamples by 2X.\n", "  with tf.name_scope('pool1'):\n", "    #h_pool1= tf.sqrt(tf.nn.avg_pool(tf.square(h_conv1),ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME'))\n", "    h_pool1 = max_pool_2x2(h_conv1)\n", "\n", "  # Second convolutional layer -- maps 32 feature maps to 64.\n", "  with tf.name_scope('conv2'):\n", "    W_conv2 = weight_variable([8, 8, 16, 32]) #W_conv2 = weight_variable([5, 5, 32, 64])\n", "    b_conv2 = bias_variable([32])\n", "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n", "\n", "  # Second pooling layer.\n", "  with tf.name_scope('pool2'):\n", "    #h_pool2= tf.sqrt(tf.nn.avg_pool(tf.square(h_conv2),ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME'))\n", "    h_pool2 = max_pool_2x2(h_conv2)\n", "\n", "\n", "# third convolutional layer -- maps 32 feature maps to 64.\n", "  with tf.name_scope('conv3'):\n", "    W_conv3 = weight_variable([5, 5, 32, 64]) #W_conv2 = weight_variable([5, 5, 32, 64])\n", "    b_conv3 = bias_variable([64])\n", "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n", "\n", "  # Second pooling layer.\n", "  with tf.name_scope('pool3'):\n", "    #h_pool2= tf.sqrt(tf.nn.avg_pool(tf.square(h_conv2),ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME'))\n", "    h_pool3 = max_pool_2x2(h_conv3)\n", "\n", "\n", "  # Fully connected layer 1 -- after 2 round of downsampling, our 128x512 image\n", "  # is down to 16x64x64 feature maps -- maps this to 1024 features.\n", "  with tf.name_scope('fc1'):\n", "    W_fc1 = weight_variable([16*64*64, 512])\n", "    b_fc1 = bias_variable([512])\n", "\n", "    h_pool2_flat = tf.reshape(h_pool3, [-1,16*64*64])\n", "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n", "\n", "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n", "  # features.\n", "  with tf.name_scope('dropout'):\n", "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n", "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n", "\n", "  # Map the 1024 features to 10 classes, one for each digit\n", "  with tf.name_scope('fc2'):\n", "    W_fc2 = weight_variable([512, noClasses])\n", "    b_fc2 = bias_variable([noClasses])\n", "    y_conv = tf.add(tf.matmul(h_fc1_drop, W_fc2),b_fc2, name = \"Y_Conv\")\n", "  return y_conv, keep_prob\n", "def conv2d(x, W):\n", "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n", "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n", "def max_pool_2x2(x):\n", "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n", "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n", "                        strides=[1, 2, 2, 1], padding='SAME')\n", "def max_pool_4x4(x):\n", "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n", "  return tf.nn.max_pool(x, ksize=[1, 4, 4, 1],\n", "                        strides=[1, 4, 4, 1], padding='SAME')\n", "  \n", "def weight_variable(shape):\n", "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n", "  initial = tf.truncated_normal(shape, stddev=0.1)\n", "  return tf.Variable(initial)\n", "def bias_variable(shape):\n", "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n", "  initial = tf.constant(0.1, shape=shape)\n", "  return tf.Variable(initial)\n", "#%%\n", "  \n", "\n", "def initParameters():\n", "  global PARAMS\n", "  PARAMS = Params()\n", "  PARAMS.imageHeight = 128\n", "  PARAMS.imageWidth = 512\n", "\n", "def one_hot_matrix(labels, C):\n", "  \"\"\" Creates a matrix where the i-th row corresponds to the ith class number and the jth column  corresponds to the jth training example. So if example j had a label i. Then entry (i,j)  will be 1. \n", "  Arguments:\n", "  labels -- vector containing the labels \n", "  C -- number of classes, the depth of the one hot dimension\n", "  Returns: \n", "  one_hot -- one hot matrix\n", "  \"\"\"\n", "  C = tf.constant(C, name = \"C\")\n", "  # Use tf.one_hot, be careful with the axis \n", "  one_hot_matrix = tf.one_hot( labels, depth = C, axis = 0 ) #you can receive variables without a placeholder ... probably if used once\n", "  sess = tf.Session()\n", "  one_hot = sess.run(one_hot_matrix )\n", "  sess.close()\n", "  return one_hot\n", "\n", "def ones(shape):\n", "  \"\"\"\n", "  Creates an array of ones of dimension shape\n", "  Arguments:\n", "  shape -- shape of the array you want to create\n", "  Returns: \n", "  ones -- array containing only ones\n", "  \"\"\"\n", "  ones = tf.ones(shape)\n", "  sess = tf.Session()\n", "  ones = sess.run(ones)\n", "  sess.close()\n", "  return ones\n", "\n", "def create_placeholders( n_y):\n", "  \"\"\"  Creates the placeholders for the tensorflow session.\n", "  Arguments:\n", "  n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n", "  n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n", "  Returns:\n", "  X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n", "  Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n", "  Tips:     - \" None\" let's us be flexible on the number of examples you will for the placeholders.  In fact, the number of examples during test/train is different.\n", "  \"\"\"\n", "  X = tf.placeholder(dtype = tf.uint8, shape=(None,128,512), name = \"X\")\n", "  Y = tf.placeholder(dtype = tf.float32, shape=(None,n_y), name = \"Y\")\n", "  return X, Y\n", "\n", "def compute_cost(Y_conv, Y):\n", "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Y_conv, labels = Y))\n", "    return cost\n", "\n", "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32, print_cost = True, tfModelSavePath =\"\"):\n", "    \"\"\"     Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n", "    Arguments:\n", "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n", "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n", "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n", "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n", "    learning_rate -- learning rate of the optimization\n", "    num_epochs -- number of epochs of the optimization loop\n", "    minibatch_size -- size of a minibatch\n", "    print_cost -- True to print the cost every 100 epochs\n", "    Returns:\n", "    parameters -- parameters learnt by the model. They can then be used to predict.\n", "    \"\"\"\n", "    tf.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n", "    tf.set_random_seed(1)                             # to keep consistent results\n", "    seed = 3                                          # to keep consistent results\n", "    m,n_y = Y_train.shape                            # n_y : output size\n", "    costs = []                                        # To keep track of the cost\n", "    train_accuracys = []                                        # To keep track of the cost\n", "    test_accuracys = []                                        # To keep track of the cost\n", "    # Create Placeholders of shape (n_x, n_y)    \n", "    X, Y = create_placeholders(n_y)\n", "    # Forward propagation: Build the forward propagation in the tensorflow graph\n", "    y_conv, keep_prob = deepnn(X)\n", "    # Cost function: Add cost function to tensorflow graph\n", "    cost = compute_cost(y_conv,Y) \n", "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n", "    #optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n", "    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n", "    # Initialize all the variables\n", "    init = tf.global_variables_initializer()\n", "    # Start the session to compute the tensorflow graph\n", "    correct_prediction = tf.equal(tf.argmax(y_conv, axis =1), tf.argmax(Y, axis = 1))\n", "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "        \n", "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n", "        saver = tf.train.Saver() #SAVE\n", "        # Run the initialization\n", "        sess.run(init)\n", "        curr_time = time.time()\n", "        # Do the training loop\n", "        for epoch in range(num_epochs):\n", "            epoch_cost = 0.                       # Defines a cost related to an epoch\n", "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n", "            seed = seed + 1\n", "            minibatches = random_mini_batchesT(X_train, Y_train, minibatch_size, seed)\n", "            for minibatch in minibatches:\n", "                # Select a minibatch\n", "                (minibatch_X, minibatch_Y) = minibatch\n", "                # IMPORTANT: The line that runs the graph on a minibatch. Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n", "                _ , minibatch_cost = sess.run([optimizer,cost], feed_dict = {X:minibatch_X, Y:minibatch_Y , keep_prob : 0.2 })\n", "                epoch_cost += minibatch_cost / num_minibatches\n", "            # Print the cost every epoch\n", "            if print_cost == True and epoch % 2 == 0:\n", "                new_time = time.time()\n", "                print(\"Time/epoch:\"+str((new_time-curr_time)/2))\n", "                curr_time=new_time\n", "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n", "            if print_cost == True and epoch % 5 == 0:\n", "                costs.append(epoch_cost)\n", "                (minibatch_X, minibatch_Y) = minibatches[0]\n", "                train_accuracys.append(accuracy.eval({X: minibatch_X, Y: minibatch_Y, keep_prob : 1}))\n", "                test_accuracys.append(accuracy.eval({X: X_test[0:128,:], Y: Y_test[0:128  ,:], keep_prob : 1}))\n", "                plotAccuracys(train_accuracys,test_accuracys,learning_rate)\n", "                plotCosts(costs,learning_rate)\n", "        saver.save(sess, tfModelSavePath)\n", "        tf.train.write_graph(sess.graph_def, tfModelSavePath , \"ModelTFPerson.pb\", as_text=True)\n", "        #plot the cost\n", "        plotCosts(costs,learning_rate)\n", "        \n", "        print (\"Parameters have been trained!\")\n", "        #print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, keep_prob : 1}))\n", "        \n", "def predictOnly(X_test ):\n", "  \n", "  tf.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n", "  \n", "  tf.set_random_seed(1)                             # to keep consistent results\n", "  seed = 3                                          # to keep consistent results\n", "  (m,n_x) = X_test.shape                          # (n_x: input size, m : number of examples in the train set)\n", "  costs = []                                        # To keep track of the cost\n", "  # Create Placeholders of shape (n_x, n_y)    \n", "  X = tf.placeholder(dtype = tf.uint8, shape=(None, n_x))\n", "  # Forward propagation: Build the forward propagation in the tensorflow graph\n", "  y_conv, keep_prob = deepnn(X)\n", "  init = tf.global_variables_initializer()\n", "  # Start the session to compute the tensorflow graph\n", "  with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n", "      sess.run(init)\n", "      tfModelSavePath = \"D:\\\\Dev\\\\ML\\\\PersonDetection\\\\NeuralNet3LTensorFlow\\\\TFSavedModels\"\n", "      saver = tf.train.Saver()\n", "      saver = tf.train.import_meta_graph(tfModelSavePath +\"\\\\.meta\")   # Load :  Important\n", "      saver.restore(sess, tfModelSavePath + \"\\\\\" ) # Load :  Important\n", "      start_time = time.time()\n", "      for i in range (100):\n", "        yOut =  sess.run([y_conv], feed_dict = {X:X_test, keep_prob : 1 })\n", "      end_time = time.time()\n", "      print(\"Elapsed\"+str(end_time-start_time))\n", "  return yOut\n", "\n", "        \n", "def plotCosts(costs, learning_rate):\n", "  plt.plot(np.squeeze(costs))\n", "  plt.ylabel('Cost')\n", "  plt.xlabel('iterations (per tens)')\n", "  plt.title(\"Learning rate =\" + str(learning_rate))\n", "  plt.show()\n", "\n", "def plotAccuracys(train_accuracys,test_accuracys,learning_rate):\n", "  plt.plot(np.squeeze(train_accuracys))\n", "  plt.plot(np.squeeze(test_accuracys))\n", "  plt.ylabel('Accuracy')\n", "  plt.xlabel('iterations (per tens)')\n", "  plt.title(\"Learning rate =\" + str(learning_rate))\n", "  plt.show()\n", "\n", "def printOutput(X, Y , Y_True): \n", "  \n", "  for i in range(X.shape[0]):\n", "    if Y[i]!=Y_True[i]:\n", "      plt.imshow(np.reshape(X[i], newshape=[92,46,3]))\n", "      plt.show()\n", "      im=np.reshape(X[i], newshape=[92,46,3])\n", "      print(\"Y_pred=\"+str(Y[i])+ \"Y_true\"+str(Y_True[i]))\n", "      misc.imsave(\"D://data//ML//ImageSegmentation//out//True\"+str(Y_True[i])+\"//Pred\"+str(Y[i])+\"_\" +str(i)+\".jpg\",im)\n", "  \n", "#%%\n", "#%%\n", "# Loading the dataset\n", "\n", "rootDir = \"E://data//kaggle//TFSpeechRC//train//spikes128x512\"  \n", "if not('X_train_orig' in locals()):\n", "  X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset(rootDir, labelsList)\n", "# Change the index below and run the cell to visualize some examples in the dataset.\n", "\n", "#%%\n", "\n", "\n", "# Flatten the training and test images\n", "X_train = X_train_orig #/255.\n", "X_test = X_test_orig #/255.\n", "Y_train = Y_train_orig\n", "Y_test = Y_test_orig\n", "print (\"number of training examples = \" + str(X_train.shape[0]))\n", "print (\"number of test examples = \" + str(X_test.shape[0]))\n", "print (\"X_train shape: \" + str(X_train.shape))\n", "print (\"Y_train shape: \" + str(Y_train.shape))\n", "print (\"X_test shape: \" + str(X_test.shape))\n", "print (\"Y_test shape: \" + str(Y_test.shape))\n", "\n", "#%%\n", "# Example of a picture\n", "index = 300\n", "plt.imshow(X_train_orig[index])\n", "plt.show()\n", "print (\"y = \" + str(Y_train_orig[index])+labelsList[np.argmax(Y_train_orig[index])] )\n", "\n", "index = 100\n", "plt.imshow(X_train_orig[index])\n", "plt.show()\n", "print (\"y = \" + str(Y_train_orig[index])+labelsList[np.argmax(Y_train_orig[index])] )\n", "\n", "\n", "index = 15\n", "plt.imshow(X_train_orig[index])\n", "plt.show()\n", "print (\"y = \" + str(Y_train_orig[index])+labelsList[np.argmax(Y_train_orig[index])] )\n", "\n", "\n", "\n", "index = 4\n", "plt.imshow(X_test[index])\n", "plt.show()\n", "print (\"y = \" + str(Y_test[index])+labelsList[np.argmax(Y_test[index])] )\n", "\n", "#%% def trainAndSave():\n", "tfModelSavePath = \"E://science//info//Competitions//Kaggle//TFSpeechRC//SavedModel//\"\n", "model(X_train, Y_train, X_test, Y_test,learning_rate = 1e-3,num_epochs = 50, minibatch_size = 64, print_cost = True,tfModelSavePath =tfModelSavePath )\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["labelsList = [\"silence\",\"unknown\",\"yes\",\"no\",\"up\",\"down\",\"left\",\"right\",\"on\",\"off\",\"stop\",\"go\"]\n", "#%%\n", "def main():\n", "  \n", "  audioFilesDir = \"E://data//kaggle//TFSpeechRC//test//audio//\"\n", "  spikesFilesDir = \"E://data//kaggle//TFSpeechRC//test//spikes128x512//\"\n", "  #spikesFilesDir = \"E://data//kaggle//TFSpeechRC//train//spikes128x512//off//\"\n", "  index = 0\n", "  \n", "  tf.reset_default_graph()   \n", "  sess= tf.Session() \n", "  #model is restored here\n", "  tfModelSavePath = \"E://science//info//Competitions//Kaggle//TFSpeechRC//SavedModel\"\n", "  saver = tf.train.import_meta_graph(\"E://science//info//Competitions//Kaggle//TFSpeechRC//SavedModel//.meta\")   # Load :  Important\n", "  saver.restore(sess, tfModelSavePath + \"\\\\\" ) # Load :  Important\n", "  graph = tf.get_default_graph() # Load :  Important\n", "  x = graph.get_tensor_by_name(\"X:0\") # Load :  Important\n", "  keep = graph.get_tensor_by_name(\"dropout/keep_prob:0\") # Load :  Important\n", "  Y_conv = graph.get_tensor_by_name(\"fc2/Y_Conv:0\") \n", "  \n", "  \n", "  outL = []\n", "  fileList = os.listdir(spikesFilesDir)\n", "  \n", "  for file in fileList:\n", "    index = index +1 \n", "    if index % 1 == 0 :\n", "      \n", "      if (index%1000==0):\n", "        print(\"Index\" + str(index) +\" : \"+str(file))\n", "      image= np.zeros(shape = [1,128,512], dtype = np.uint8)\n", "      img = pd.read_csv(spikesFilesDir + file, sep=\";\").as_matrix()\n", "      image[0,0:127,:] = img\n", "      \n", "      #plt.imshow(image[0])\n", "      #plt.show()\n", "      \n", "     \n", "      feed_dict ={x:image,keep:1.0 }  \n", "      # Feed the audio data as input to the graph.    #   predictions  will contain a two-dimensional array, where one         #   dimension represents the input image count, and the other has     #   predictions per class\n", "      YConv = sess.run(Y_conv, feed_dict)\n", "      # Sort to show labels in order of confidence\n", "      top_k = np.argmax(YConv)\n", "      #print(YConv)\n", "      human_string = labelsList[top_k]\n", "      outL.append([file[0:-4],human_string])\n", "      #print(file[0:-4] + human_string)\n", "  df = pd.DataFrame(outL,index =None, columns = [\"fname\",\"label\"])\n", "  #print(df)\n", "  df.to_csv(\"my_sample_submission3.csv\",index = False)\n", "\n", "      \n", "        \n", "if __name__ == '__main__':\n", "  main()"]}], "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.4", "nbconvert_exporter": "python"}}}