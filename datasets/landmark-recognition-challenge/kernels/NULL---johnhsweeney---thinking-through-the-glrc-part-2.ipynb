{"cells":[{"metadata":{"_uuid":"c1e0510893d8109dd0a7697f36cdaa8804c50c71"},"cell_type":"markdown","source":"# Part 2 - Broad outline, some data clean-up, and a few decisions"},{"metadata":{"_uuid":"fae6b189a9efba5e1cacb6fcb0cf0a59e32bdb2d"},"cell_type":"markdown","source":"### High level plan...\n\nI'll pick up where we left off yesterday, but first I'd like to give a high level plan of where I think this will ultimately go...\n\nAs I said yesterday, I'm going to take an approach as though I was building a product - not trying to win a competition.\n\nSo - steps I think you should take to build this product\n\n    1) Decide what problem you are trying to solve (see previous discussion)\n    2) Look at the data to get a feel for it.  I started this yesterday and I'll do a bit more today\n    3) Clean up the data as necessary for it to be useful\n    4) Pick an approach and get started\n    5) Implement a quick and dirty model\n    6) Evaluate the model's performance\n    7) Look at the problems and failures to get an idea where the model should be improved\n    8) Update the model\n    9) Repeat 6-8 until you are satisfied with the final results!\n    "},{"metadata":{"_uuid":"c763454a04ed3cd09916b7e3c8cde7efa33d83cb"},"cell_type":"markdown","source":"### Step 2 - Look at the data (cont.)\n\nYesterday I found that a large number of the training images were very small with sizes like 15px x 11px\n\nIn fact, I found >23,000 images with the smaller edge less than 256px.\n\nTo \"solve\" that problem I took advantage of the fact that most of these images come from google user content and exist of a base image plus the ability to resize.\n\nI just modified the url to take the base image.  You saw thoose results previously, so I won't go through it again.\n\nHowever, I this is the function I used to modify the url.  I'm certain there is a more elegant way to do this, but what the heck.  It worked and took 3 minutes to write!"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"40ca12b8004c291f452a531f76adacd93d47a3ae"},"cell_type":"code","source":"def url_stripper(old_url):\n    \n    new_url = old_url\n    #length = len(old_url)\n    position = -1\n    done = False\n    change = False\n    if old_url[position] == '/':\n        while not done:\n            position -=1\n            if old_url[position] == '/':\n                done = True\n                change = True\n    if change:\n        new_url = old_url[:position+1]\n    \n    return new_url","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3f242902cdaefa68e4900d07b93f73c2708bafb"},"cell_type":"code","source":"# A simple demonstration of url_stripper\n\nurl = 'https://lh3.googleusercontent.com/-SXCAgqmUSCY/TKKFZqwVxxI/AAAAAAAADbw/H440k4K4rlY/w11-h15/'\nprint('old url: ', url)\nurl = url_stripper(url)\nprint('new url: ', url)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"4146a2ca9fc2caf5d96fedc746b56a315716e08d"},"cell_type":"markdown","source":"#### Looking some more at the data...\n\nLet's look at the training data to understand how many unique landmarks we have..."},{"metadata":{"trusted":true,"_uuid":"6ad4e7b70f092258f91bef478e9ea5289aa55232"},"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/train.csv') #kaggle version\n#train_data = pd.read_csv('./train.csv') # local version\n\n# find out how many unique landmark ids there are, and how many instances of each one\nlandmark_ids = train_data['landmark_id'].value_counts() \nprint('there are', landmark_ids.shape[0], 'unique landmarks')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a106fedb57a70b8a2f9d09b279260f833fd472c"},"cell_type":"code","source":"print('the top 20 landmarks are:')\nprint('id       count')\nprint(landmark_ids.head(20))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"63b87f6f16d409f6de14311c0bfea72938023428"},"cell_type":"markdown","source":"Notice that there is a 10x reduction in samples from the top landmark to the 20th landmark!"},{"metadata":{"trusted":true,"_uuid":"852cc7b1e2021d77e48979aa79e32b7b49d74e04"},"cell_type":"code","source":"counts = landmark_ids.values\nindex = landmark_ids.index\nsize = 1024\nfor i in [1024,2048,4096, 8192]:\n    percentage = 100*(counts[0:i].sum()/counts.sum())\n    print('the top %d landmarks account for %5.2f percent of the training samples' %(i,percentage))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"e2202d92e4ecdb09933592536e3b34b7e2999684"},"cell_type":"markdown","source":"### Time to make some choices\n\nI want to jump into the modelling, and I want to keep it simple to begin.\n\nI think I'll use a pre-trained ResNet50 network and retrain the output stages for a new softmax layer with 1024 categories (the top 1023 landmarks and a \"don't see a landmark\" categotry).\n\nThat will almost certainly not be very good, since it will only cover ~2/3 of the landmarks inthe training set, but hopefully it will be easier to set up, start training, etc.  I can use the number of landmarks / categories as a hyper-parameter later if I want to see how it improves with 2k or 4k landmarks."},{"metadata":{"_uuid":"55b668ed39be128b209750eb270ae7cba41b2f83"},"cell_type":"markdown","source":"#### Next topic - setting up a ResNEt50 network and using transfer learning to start identifying landmarks"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70488c6ea98f518aa52b391a85fa01cf6fdccd0a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}