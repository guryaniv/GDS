{"cells":[{"metadata":{"_uuid":"1c592418a9ebedbf2d6aadfc819d0c18aa9bc7f2"},"cell_type":"markdown","source":"# Feature Engineering of PUBG data with Model Developement\n\nI previously created a notebook going through some [exploratory anaylsys] of the PUBG data. I went through many of the different features avalailable and displayed an interesting plot describing the data and potential correlation with the target variable.\n\n* I found that there was one missing value for the target variable and decided that this row of data should be removed, as there was only one player for the match identified by the missing value.\n\n* I also made a few decisions about creating new features and one important way of breaking the data up to gain higher correllations with our features for seperate match types.\n\n[exploratory anaylsys]: https://www.kaggle.com/beaubellamy/pubg-eda#"},{"metadata":{"_uuid":"363226b9fe854d7a825a17e6c2eca55ff5c756c7"},"cell_type":"markdown","source":"## Import libraries\nWe import the required libraries and import the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\nimport random\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train_V2.csv')\ntest = pd.read_csv('../input/test_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc90a34196fbffc09ef9a9f14cc99c1abb275b5d"},"cell_type":"markdown","source":"Lets check out the data again."},{"metadata":{"trusted":true,"_uuid":"59fee04c6a6a4285dfc6b54e3c6e18ff71768a93"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb627ae368d79493b831acb7c3b222bf7014d186"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d23e88ceebc824b4d7686fe98dc5c23e6c1bbdf5"},"cell_type":"markdown","source":"## Missing Data\nBased on our EDA, we found a row that had a NULL value for the target variable. We will remove the irrelevant row of data."},{"metadata":{"trusted":true,"_uuid":"90cf3cfbb4f386335af97e836b7e442f940bc9cc"},"cell_type":"code","source":"# Remove the row with the missing target value\ntrain = train[train['winPlacePerc'].isna() != True]\n3train = train.sample(frac=0.3,replace=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"627ead1a30264239d56af16da9b08925fde68501"},"cell_type":"markdown","source":"## Lets Engineer some features\nWe'll process the testing data the same way we do for the training data so the testing data has the same features and scaling as our training data.\n\n### PlayersJoined\nWe can determine the number of players that joined each match by grouping the data by matchID and counting the players."},{"metadata":{"trusted":true,"_uuid":"db2303527c8ecb0a4fca2b3a430c4295ca7c1796"},"cell_type":"code","source":"# Add a feature containing the number of players that joined each match.\ntrain['playersJoined'] = train.groupby('matchId')['matchId'].transform('count')\ntest['playersJoined'] = test.groupby('matchId')['matchId'].transform('count')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1da51b6c6c1454181ffbc6af3c815df289ce77d8"},"cell_type":"code","source":"# Lets look at only those matches with more than 50 players.\ndata = train[train['playersJoined'] > 50]\n\nplt.figure(figsize=(15,15))\nsns.countplot(data['playersJoined'].sort_values())\nplt.title('Number of players joined',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3eae047591d0e6db4f2467bbb3283d47db19fec"},"cell_type":"markdown","source":"You can see that there isn't always 100 players in each match, in fact its more likely to have between 90 and 100 players. It may be benficial to normalise those features that are affected by the number of players.\n\n### Normalised Features\nHere, I am making the assumption that it is easier to find an enemy when there are 100 players, than it is when there are 90 players.\n"},{"metadata":{"trusted":true,"_uuid":"9c25114718a07ecf171f6bbdb3f8c74e966f399b"},"cell_type":"code","source":"def normaliseFeatures(train):\n    train['killsNorm'] = train['kills']*((100-train['playersJoined'])/100 + 1)\n    train['headshotKillsNorm'] = train['headshotKills']*((100-train['playersJoined'])/100 + 1)\n    train['killPlaceNorm'] = train['killPlace']*((100-train['playersJoined'])/100 + 1)\n    train['killPointsNorm'] = train['killPoints']*((100-train['playersJoined'])/100 + 1)\n    train['killStreaksNorm'] = train['killStreaks']*((100-train['playersJoined'])/100 + 1)\n    train['longestKillNorm'] = train['longestKill']*((100-train['playersJoined'])/100 + 1)\n    train['roadKillsNorm'] = train['roadKills']*((100-train['playersJoined'])/100 + 1)\n    train['teamKillsNorm'] = train['teamKills']*((100-train['playersJoined'])/100 + 1)\n    train['damageDealtNorm'] = train['damageDealt']*((100-train['playersJoined'])/100 + 1)\n    train['DBNOsNorm'] = train['DBNOs']*((100-train['playersJoined'])/100 + 1)\n    train['revivesNorm'] = train['revives']*((100-train['playersJoined'])/100 + 1)\n\n    # Remove the original features we normalised\n    train = train.drop(['kills', 'headshotKills', 'killPlace', 'killPoints', 'killStreaks', \n                        'longestKill', 'roadKills', 'teamKills', 'damageDealt', 'DBNOs', 'revives'],axis=1)\n\n    return train\n\ntrain = normaliseFeatures(train)\ntest = normaliseFeatures(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5996305e8c7a615bde60829be74c19c82bbb713"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"565787b793b989b5d51e67326f9df0f99387569e"},"cell_type":"markdown","source":"### TotalDistance\nAn additional feature we can create is the total distance the player travels. This is a combination of all the distance features in the original data set."},{"metadata":{"trusted":true,"_uuid":"7c9cb57049be6172d5c6bd1f7403041824efd7c0"},"cell_type":"code","source":"# Total distance travelled\ntrain['totalDistance'] = train['walkDistance'] + train['rideDistance'] + train['swimDistance']\ntest['totalDistance'] = test['walkDistance'] + test['rideDistance'] + test['swimDistance']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021bd6525fead1d9860f48d7bf79460fc7f42a57"},"cell_type":"markdown","source":"### Features in relation to others\nAfter some additional searching of other kernels, i found some features that i thought were a good approach, so i have blatenlty stolen Hyun Woo Kim's approach to feature engineering.\n\n[Hyun Woo Kim's]: https://www.kaggle.com/chocozzz/lightgbm-baseline"},{"metadata":{"trusted":true,"_uuid":"ac967f5fe4dd2e3a08e00c15a59c4d572aef93dc"},"cell_type":"code","source":"## Blatently stolen from Hyun Woo Kim https://www.kaggle.com/chocozzz/lightgbm-baseline\n\ndef createFeatureRates(data):\n    data['headshotrate'] = data['killsNorm']/data['headshotKillsNorm']\n    data['killStreakrate'] = data['killStreaksNorm']/data['killsNorm']\n    data['healthitems'] = data['heals'] + data['boosts']\n    data['killPlace_over_maxPlace'] = data['killPlaceNorm'] / data['maxPlace']\n    data['headshotKills_over_kills'] = data['headshotKillsNorm'] / data['killsNorm']\n    data['walkDistance_over_weapons'] = data['walkDistance'] / data['weaponsAcquired']\n    data['walkDistance_over_heals'] = data['walkDistance'] / data['heals']\n    data['walkDistance_over_kills'] = data['walkDistance'] / data['killsNorm']\n    data['distance_over_weapons'] = data['totalDistance'] / data['weaponsAcquired']\n    data['distance_over_heals'] = data['totalDistance'] / data['heals']\n    data['distance_over_kills'] = data['totalDistance'] / data['killsNorm']\n    data['killsPerDistance'] = data['killsNorm'] / data['totalDistance']\n    data['skill'] = data['headshotKillsNorm'] + data['roadKillsNorm']\n    \n    data[data == np.Inf] = np.NaN\n    data[data == np.NINF] = np.NaN\n    \n    data.fillna(0, inplace=True)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e29ecd560b9e7bdce0c08bc542cead3757d6fb41"},"cell_type":"code","source":"train = createFeatureRates(train)\ntest = createFeatureRates(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5170492bf383abc176874ba52dbddf2ba4b636a3"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b37809c8ec4d04e57898b3ada407006e63d4181"},"cell_type":"markdown","source":"# Standardize the matchType feature\nHere I decided that many of the existing 16 seperate modes of game play were just different versions of four types of game.\n\n1. Solo: Hunger Games style, last man/women standing.\n2. Duo: Teams of two against all other players.\n3. Squad: Teams of up to 4 players against All other players\n4. Other: These modes consist of custom and special events modes"},{"metadata":{"trusted":true,"_uuid":"94e0117b066266679ee5643b6b4161797e5f3c4c"},"cell_type":"code","source":"# Normalise the matchTypes to standard fromat\ndef standardize_matchType(data):\n    data['matchType'][data['matchType'] == 'normal-solo'] = 'Solo'\n    data['matchType'][data['matchType'] == 'solo-fpp'] = 'Solo'\n    data['matchType'][data['matchType'] == 'normal-solo-fpp'] = 'Solo'\n    data['matchType'][data['matchType'] == 'normal-duo-fpp'] = 'Duo'\n    data['matchType'][data['matchType'] == 'normal-duo'] = 'Duo'\n    data['matchType'][data['matchType'] == 'duo-fpp'] = 'Duo'\n    data['matchType'][data['matchType'] == 'squad-fpp'] = 'Squad'\n    data['matchType'][data['matchType'] == 'normal-squad'] = 'Squad'\n    data['matchType'][data['matchType'] == 'normal-squad-fpp'] = 'Squad'\n    data['matchType'][data['matchType'] == 'flaretpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'flarefpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'crashtpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'crashfpp'] = 'Other'\n\n    return data\n\n\ntrain = standardize_matchType(train)\ntest = standardize_matchType(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a6f6b2eeb8c34b70843a5df7d015d85b02a972"},"cell_type":"code","source":"# We need a copy of the test data with the player id later on\ntest_submission = test.copy()\n\ntrain = train.drop(['Id','groupId','matchId'], axis=1)\ntest = test.drop(['Id','groupId','matchId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24907eae0862b4354706aed72bb7fca29391944f"},"cell_type":"code","source":"# We need to keep a copy of the test data for the test id's later \ntrain_copy = train.copy()\ntest_copy = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"980e01433dbefcb4bd3269669465b3e6c2bccdd5"},"cell_type":"markdown","source":"Now we can transform the matchTypes into dummy values so we can use them in the model."},{"metadata":{"trusted":true,"_uuid":"780310f7f4fc97455589c6a7a915c84ebbaaa08c"},"cell_type":"code","source":"# Transform the matchType into scalar values\nle = LabelEncoder()\ntrain['matchType']=le.fit_transform(train['matchType'])\ntest['matchType']=le.fit_transform(test['matchType'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db727750a32ed96e38a0602b85db869a0700e0b9"},"cell_type":"code","source":"# We can do a sanity check of the data, making sure we have the new \n# features created and the matchType feature is standardised.\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0c82acaa8fecd6b2089ceadf724b4e3c32b7c54"},"cell_type":"markdown","source":"# Scale the features\nSome features have very large values and a vary large variance. For any regresion analysis, it is good practice to scale all features to similar variance. This would not be neccessary if the variance of all features was between 6 and 10."},{"metadata":{"trusted":true,"_uuid":"5d710a638a6cd63130dac34e83c0ebb1102ba0d0"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3202fe27a7d22b7d011068f5b35e3df95645b93f"},"cell_type":"markdown","source":"You can see most features range 0 to 100 or 1000's, but there are two features that doesn't really need scaling, VehicleDestroys and matchType, as they only range between 0 to 5, 6. Its not neccassary to scale these features, but we will any way, because it makes the code easier."},{"metadata":{"trusted":true,"_uuid":"e3478558c107dca6fdc27eb820b5a1b5bdf069e4"},"cell_type":"code","source":"scaler = MinMaxScaler()\n\ntrain_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\ntest_scaled = pd.DataFrame(scaler.fit_transform(test), columns=test.columns)\n\ntrain_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc93d5c7f80a56b1c57ab37d2a94d96c1ba35f69"},"cell_type":"code","source":"train_scaled.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6aa9ab0275e5f079210fbbe362d8d61ce4d8185"},"cell_type":"markdown","source":"As you can see, all the features have the same minimum, maximum and standard deviation. All our features are now normalised and scaled for modelling.\n\n# Model Development\n\nI have previously created a [kernel] to explore the data and develop some new features. From this exploratory analysis, I found that by breaking the data into four seperate matchtypes improved the accuracy of the models. The four main match types will be described by the following four groups;\n\n1. Solo: Hunger Games style, last man/women standing.\n2. Duo: Teams of two against all other players.\n3. Squad: Teams of up to 4 players against All other players\n4. Other: These modes consist of custom and special events modes\n\n\n[kernel]: https://www.kaggle.com/beaubellamy/pubg-eda#"},{"metadata":{"trusted":true,"_uuid":"1bdf496f5756dd8ed0a85149e4959021a1327b0d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db497504ad394726c89ad83df5c7c829148285d9"},"cell_type":"code","source":"# Extract the target variable.\ny = train_scaled['winPlacePerc']\nX = train_scaled.drop(['winPlacePerc'],axis=1)\n\n# Split the data in to training and validation set\nsize = 0.3\nseed = 42\n   \nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=size, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1fd1c9071148f3377ae25565740df0ecac58ee0"},"cell_type":"markdown","source":"# Model Development for seperate matchTypes\nAgain, with the insight we learnt from our [EDA], we found that it is likely that we will improve our overall results by creating four seperate models, one for each matchType. This allows us to potentially use a seperate type of model for each matchType.\n\nHere we'll split the data into the matchTypes and see how the models perform for each one.\n\n[EDA]: https://www.kaggle.com/beaubellamy/pubg-eda#"},{"metadata":{"trusted":true,"_uuid":"da6a24645579f85f2c3f9e5d7546eb03bfb6d6f9"},"cell_type":"code","source":"# Create a data set for each matchType and drop that feature, as there will be no variance, and hence no predictive power.\nsolo = train_copy[train_copy['matchType'] == 'Solo']\nsolo = solo.drop(['matchType'], axis=1)\nduo = train_copy[train_copy['matchType'] == 'Duo']\nduo = duo.drop(['matchType'], axis=1)\nsquad = train_copy[train_copy['matchType'] == 'Squad']\nsquad = squad.drop(['matchType'], axis=1)\nother = train_copy[train_copy['matchType'] == 'Other']\nother = other.drop(['matchType'], axis=1)\n\n# since we used a copy of the trained data that hasn't been scaled, we need to scale the features again.\nscaler = MinMaxScaler()\nsolo_scaled = pd.DataFrame(scaler.fit_transform(solo), columns=solo.columns)\nduo_scaled = pd.DataFrame(scaler.fit_transform(duo), columns=duo.columns)\nsquad_scaled = pd.DataFrame(scaler.fit_transform(squad), columns=squad.columns)\nother_scaled = pd.DataFrame(scaler.fit_transform(other), columns=other.columns)\n\n# Seperate the matchType data\ntest_solo = test_copy[test_copy['matchType'] == 'Solo']\ntest_solo = test_solo.drop(['matchType'], axis=1)\ntest_duo = test_copy[test_copy['matchType'] == 'Duo']\ntest_duo = test_duo.drop(['matchType'], axis=1)\ntest_squad = test_copy[test_copy['matchType'] == 'Squad']\ntest_squad = test_squad.drop(['matchType'], axis=1)\ntest_other = test_copy[test_copy['matchType'] == 'Other']\ntest_other = test_other.drop(['matchType'], axis=1)\n\nsolo_test_scaled = pd.DataFrame(scaler.fit_transform(test_solo), columns=test_solo.columns)\nduo_test_scaled = pd.DataFrame(scaler.fit_transform(test_duo), columns=test_duo.columns)\nsquad_test_scaled = pd.DataFrame(scaler.fit_transform(test_squad), columns=test_squad.columns)\nother_test_scaled = pd.DataFrame(scaler.fit_transform(test_other), columns=test_other.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d28e078b4375e7d67686d009470b339cc6a7dafd"},"cell_type":"markdown","source":"We'll train each of the sets of data separatly."},{"metadata":{"trusted":true,"_uuid":"d8e0f92389caea6f65179c5670bb4e3f4385a851"},"cell_type":"code","source":"solo_y = solo_scaled['winPlacePerc']\nsolo_X = solo_scaled.drop(['winPlacePerc'],axis=1)\n\n# We will maintain the existing size and random state parameters for repeatability.\nX_train, X_validation, Y_train, Y_validation = train_test_split(solo_X, solo_y, test_size=size, random_state=seed)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc13b72e3952cb2b9903543043339691338a4973"},"cell_type":"code","source":"GBR = GradientBoostingRegressor(learning_rate=0.8)\nGBR.fit(X_train,Y_train)\nprint(\"GradientBoost Model traininig: {0:.3f}%\".format(GBR.score(X_validation,Y_validation)*100))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"691f37089b85549ec6ad3ab462b888ae9f38360d"},"cell_type":"code","source":"duo_y = duo_scaled['winPlacePerc']\nduo_X = duo_scaled.drop(['winPlacePerc'],axis=1)\n\n# We will maintain the existing size and random state parameters for repeatability.\nX_train, X_validation, Y_train, Y_validation = train_test_split(duo_X, duo_y, test_size=size, random_state=seed)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e9a36ae412bb7ac610b6af956d7b49109f0a84"},"cell_type":"code","source":"GBR = GradientBoostingRegressor(learning_rate=0.8)\nGBR.fit(X_train,Y_train)\nprint(\"GradientBoost Model traininig: {0:.3f}%\".format(GBR.score(X_validation,Y_validation)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62f2d77ab8cc7be6c4f357009d092397650b2bc1"},"cell_type":"code","source":"squad_y = squad_scaled['winPlacePerc']\nsquad_X = squad_scaled.drop(['winPlacePerc'],axis=1)\n\n# We will maintain the existing size and random state parameters for repeatability.\nX_train, X_validation, Y_train, Y_validation = train_test_split(squad_X, squad_y, test_size=size, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b10f468b50ef91379f498987f45f017a68d073"},"cell_type":"code","source":"GBR = GradientBoostingRegressor(learning_rate=0.8)\nGBR.fit(X_train,Y_train)\nprint(\"GradientBoost Model traininig: {0:.3f}%\".format(GBR.score(X_validation,Y_validation)*100))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e99a695d173a29996f38865f8517ca1dddf9a9c7"},"cell_type":"code","source":"other_y = other_scaled['winPlacePerc']\nother_X = other_scaled.drop(['winPlacePerc'],axis=1)\n\n# We will maintain the existing size and random state parameters for repeatability.\nX_train, X_validation, Y_train, Y_validation = train_test_split(other_X, other_y, test_size=size, random_state=seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fec9ec2fee5aa469017066e498f36220803eaf98"},"cell_type":"code","source":" \nGBR = GradientBoostingRegressor(learning_rate=0.8)\nGBR.fit(X_train,Y_train)\nprint(\"GradientBoost Model traininig: {0:.3f}%\".format(GBR.score(X_validation,Y_validation)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4baa5491f6391630926ba73a5f53fb99c1d92b"},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"587c205f403f76c3fd3209b4d234928f644b6e3b"},"cell_type":"markdown","source":"Fitting 70% of the training data to a Gradient Boosting Regression we can achieve netween 91 - 94% accuracy on the remaining 30% of the data.\n\nWe've validated our model, so now we can fit all the data to each model to maximise the use of the data, then make some predictions on the fully trained model."},{"metadata":{"trusted":true,"_uuid":"8122d1bc913f9fc8a0a8916346547b765679beb0"},"cell_type":"code","source":"GBR_solo = GradientBoostingRegressor(learning_rate=0.8)\nGBR_solo.fit(solo_X,solo_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf73e2c833b7a45c3b2885cab4dcd782934246c2"},"cell_type":"code","source":"GBR_duo = GradientBoostingRegressor(learning_rate=0.8)\nGBR_duo.fit(duo_X,duo_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00215579bb5f9b9380c8d36dbf8e3454be4c0b60"},"cell_type":"code","source":"GBR_squad = GradientBoostingRegressor(learning_rate=0.8)\nGBR_squad.fit(squad_X,squad_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e0298b9b1c6fbd2f1a5660ef7233837bf4df19"},"cell_type":"code","source":"GBR_other = GradientBoostingRegressor(learning_rate=0.8)\nGBR_other.fit(other_X,other_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3801e06f91a61a6f8e3b0ce756ee142a9d8f3d16"},"cell_type":"code","source":"predictions_solo = GBR_solo.predict(solo_test_scaled)\npredictions_duo = GBR_duo.predict(duo_test_scaled)\npredictions_squad = GBR_squad.predict(squad_test_scaled)\npredictions_other = GBR_other.predict(other_test_scaled)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4b6723b4a8d02d17be437002c6944d110f69d5f"},"cell_type":"code","source":"test_submission_solo = test_submission[test_submission['matchType'] == 'Solo']\ntest_submission_duo = test_submission[test_submission['matchType'] == 'Duo']\ntest_submission_squad = test_submission[test_submission['matchType'] == 'Squad']\ntest_submission_other = test_submission[test_submission['matchType'] == 'Other']\n\n# Aggregate the Id's together again\nmatchTypeId = test_submission_solo['Id'].append(test_submission_duo['Id']).append(test_submission_squad['Id']).append(test_submission_other['Id'])\n\npredictions_solo[predictions_solo > 1] = 1\npredictions_solo[predictions_solo < 0] = 0\n\npredictions_duo[predictions_duo > 1] = 1\npredictions_duo[predictions_duo < 0] = 0\n\npredictions_squad[predictions_squad > 1] = 1\npredictions_squad[predictions_squad < 0] = 0\n\npredictions_other[predictions_other > 1] = 1\npredictions_other[predictions_other < 0] = 0\n\n# Aggregate the final predictions from each model. This must be the same order as the aggregation of the Id's\npredictions_matchtype = np.append(np.append(predictions_solo,predictions_duo),np.append(predictions_squad,predictions_other))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62c8fe384744dc48d33aa21441b4a5863d5a34df"},"cell_type":"code","source":"submission = pd.DataFrame({'Id': matchTypeId, 'winPlacePerc': predictions_matchtype})\nsubmission.to_csv('submission_matchType.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b964521d90330ffe2d49627fac98becaf1af3729"},"cell_type":"markdown","source":"If you liked this post, please upvote."},{"metadata":{"trusted":true,"_uuid":"394180972d52420c95d1930ee70405b88803fdb6"},"cell_type":"code","source":"matchTypeId.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b842688726b109fd00e78fcd79c3906b1f39af62"},"cell_type":"code","source":"predictions_matchtype.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a5398fa241c0122da13c5464ef46f1455232bb"},"cell_type":"code","source":"#1934174 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c67d21568e34cec3fb182cd64a04d7a17e67aa9a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db1b6dea6cd5148674a148a9d2051e1b31ca23cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c473eb66a6a3f2fda79ce26b3c8c4b46f16d75bf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43cecfdc277b3b6501abd5a04fd036c0ea00bd0e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72216ded4030c47600f8411ba0e37ccdfac5344d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}