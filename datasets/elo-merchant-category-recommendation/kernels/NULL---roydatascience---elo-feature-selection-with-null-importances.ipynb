{"cells":[{"metadata":{"_uuid":"1dee50e02ba20b8aabe9775e0f8b19c40e1bcff8"},"cell_type":"markdown","source":"Inspired by Olivier's Feature Selection with Null Importances\nhttps://www.kaggle.com/ogrellier/feature-selection-with-null-importances\n\nI am trying to replicate the same process for ELO Category Recommendation to get more useful features"},{"metadata":{"_uuid":"901873665873e05d31210e77482bf5f0d4f33925"},"cell_type":"markdown","source":"The notebook implements the following steps :\n<ul>\n<li>Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.</li>\n<li>Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution</li></ul>\n\nFor each feature test the actual importance:\n<ul><li>Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.</li>\n<li>Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.</li></ul>"},{"metadata":{"_uuid":"a594abe9f66c5a090177dff966d6bedf63718a9c","trusted":true},"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport matplotlib.gridspec as gridspec\nimport warnings\nimport gc\ngc.collect()\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('seaborn')\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfbbf30dcb4c354827aae6fff1aeaa632a9e874e","trusted":true},"cell_type":"code","source":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\nimport lightgbm as lgb\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.model_selection import StratifiedKFold, RepeatedKFold\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"798008f46ddb84ca5ec248990d56b64e5ec84e5a","trusted":true},"cell_type":"code","source":"# to make this notebook's output stable across runs\nnp.random.seed(123)\ngc.collect()\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47d5d544fa7e4bf9305814409c3cabc148b0169a","trusted":true},"cell_type":"code","source":"#Reduce the memory usage - Inspired by Panchajanya Banerjee\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56152464b49828f018e84cf114f520239342270e","trusted":true},"cell_type":"code","source":"def one_hot_encoder(df, nan_as_category=True):\n    original_columns = df.columns.tolist()\n\n    categorical_columns = list(filter(lambda c: c in ['object'], df.dtypes))\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n\n    new_columns = list(filter(lambda c: c not in original_columns, df.columns))\n    return df, new_columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"450f05f1bb8bd470972ec8a8f80a96f4b56402a7","scrolled":true,"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(pd.read_csv('../input/train.csv',parse_dates=[\"first_active_month\"]))\ntest = reduce_mem_usage(pd.read_csv('../input/test.csv', parse_dates=[\"first_active_month\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2352513f1d53487e789e41fbfa050199154f8916","trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78407176c3c40b9d15227165028d6b61f94f3ba3","trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1858899e8d6a7a6808204567ded690e4a10810d7","trusted":true},"cell_type":"code","source":"plt.subplot(1, 2, 2)\nsns.distplot(train.target, kde=True, fit = norm)\nplt.xlabel('Customer Loyality (Skewed)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1dc9399de447ff96d064b085e6152a543ece278"},"cell_type":"code","source":"# Remove the Outliers if any \ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30, 'outliers'] = 1\ntrain['outliers'].value_counts()\n\nfor features in ['feature_1','feature_2','feature_3']:\n    order_label = train.groupby([features])['outliers'].mean()\n    train[features] = train[features].map(order_label)\n    test[features] =  test[features].map(order_label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce244a5a05abaa8c91ff3c73d8c5f86ba592c9a1","trusted":true},"cell_type":"code","source":"# Now extract the days and Qtr\ntrain['days'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\ntrain['quarter'] = train['first_active_month'].dt.quarter\n\nfeature_cols = ['feature_1', 'feature_2', 'feature_3']\nfor f in feature_cols:\n    train['days_' + f] = train['days'] * train[f]\n    train['days_' + f + '_ratio'] = train[f] / train['days']\n\ntest['days'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\ntest['quarter'] = test['first_active_month'].dt.quarter\n\nfeature_cols = ['feature_1', 'feature_2', 'feature_3']\nfor f in feature_cols:\n    test['days_' + f] = test['days'] * test[f]\n    test['days_' + f + '_ratio'] = test[f] / test['days']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b54460353af274062f649afe26f034c4126a6a84"},"cell_type":"markdown","source":"Now we will try to extract more features from Transactions Data"},{"metadata":{"_uuid":"f9835dd6582d3e9c36954b498810929c8438d2d2","trusted":true},"cell_type":"code","source":"def aggregate_transaction_hist(trans, prefix):  \n        \n    agg_func = {\n        'purchase_amount' : ['sum','max','min','mean','var','skew'],\n        'installments' : ['sum','max','mean','var','skew'],\n        'purchase_date' : ['max','min'],\n        'month_lag' : ['max','min','mean','var','skew'],\n        'month_diff' : ['max','min','mean','var','skew'],\n        'weekend' : ['sum', 'mean'],\n        'weekday' : ['sum', 'mean'],\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum','mean', 'max','min'],\n        'card_id' : ['size','count'],\n        'month': ['nunique', 'mean', 'min', 'max'],\n        'hour': ['nunique', 'mean', 'min', 'max'],\n        'weekofyear': ['nunique', 'mean', 'min', 'max'],\n        'day': ['nunique', 'mean', 'min', 'max'],\n        'subsector_id': ['nunique'],\n        'merchant_category_id' : ['nunique'],\n        'price' :['sum','mean','max','min','var'],\n        'duration' : ['mean','min','max','var','skew'],\n        'amount_month_ratio':['mean','min','max','var','skew']\n        \n    }\n    \n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06a04f3393a83cb2687b76deba417012e18ef606","scrolled":true,"trusted":true},"cell_type":"code","source":"transactions = reduce_mem_usage(pd.read_csv('../input/historical_transactions.csv'))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1decf075c9422fe20823691972f055b8a1c84f50","scrolled":true,"trusted":true},"cell_type":"code","source":"#impute missing values - This is now excluded.\ntransactions['category_2'] = transactions['category_2'].fillna(1.0,inplace=True)\ntransactions['category_3'] = transactions['category_3'].fillna('A',inplace=True)\ntransactions['merchant_id'] = transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\ntransactions['installments'].replace(-1, np.nan,inplace=True)\ntransactions['installments'].replace(999, np.nan,inplace=True)\ntransactions['purchase_amount'] = transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n\n#Feature Engineering - Adding new features inspired by Chau's first kernel\ntransactions['authorized_flag'] = transactions['authorized_flag'].map({'Y': 1, 'N': 0})\ntransactions['category_1'] = transactions['category_1'].map({'Y': 1, 'N': 0})\ntransactions['category_3'] = transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n\ntransactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\ntransactions['weekofyear'] = transactions['purchase_date'].dt.weekofyear\ntransactions['month'] = transactions['purchase_date'].dt.month\ntransactions['day'] = transactions['purchase_date'].dt.day\ntransactions['weekday'] = transactions.purchase_date.dt.weekday\ntransactions['weekend'] = (transactions.purchase_date.dt.weekday >=5).astype(int)\ntransactions['hour'] = transactions['purchase_date'].dt.hour \ntransactions['month_diff'] = ((datetime.datetime.today() - transactions['purchase_date']).dt.days)//30\ntransactions['month_diff'] += transactions['month_lag']\n\n# additional features\ntransactions['duration'] = transactions['purchase_amount']*transactions['month_diff']\ntransactions['amount_month_ratio'] = transactions['purchase_amount']/transactions['month_diff']\ntransactions['price'] = transactions['purchase_amount'] / transactions['installments']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93a165f209a8853e4c8e1f624becd0506e6fb0f1","trusted":true},"cell_type":"code","source":"agg_func = {\n        'mean': ['mean'],\n    }\nfor col in ['category_2','category_3']:\n    transactions[col+'_mean'] = transactions['purchase_amount'].groupby(transactions[col]).agg('mean')\n    transactions[col+'_max'] = transactions['purchase_amount'].groupby(transactions[col]).agg('max')\n    transactions[col+'_min'] = transactions['purchase_amount'].groupby(transactions[col]).agg('min')\n    transactions[col+'_sum'] = transactions['purchase_amount'].groupby(transactions[col]).agg('sum')\n    agg_func[col+'_mean'] = ['mean']\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95128e635d5c0ad26b170652971aca4fc9c41ad1","scrolled":false,"trusted":true},"cell_type":"code","source":"merge_trans = aggregate_transaction_hist(transactions, prefix='hist_')\ndel transactions\ngc.collect()\ntrain = pd.merge(train, merge_trans, on='card_id',how='left')\ntest = pd.merge(test, merge_trans, on='card_id',how='left')\ndel merge_trans\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3e6c6e3dec6f2153d02a684fb7ddbbace4c5550","trusted":true},"cell_type":"code","source":"#Feature Engineering - Adding new features inspired by Chau's first kernel\ntrain['hist_purchase_date_max'] = pd.to_datetime(train['hist_purchase_date_max'])\ntrain['hist_purchase_date_min'] = pd.to_datetime(train['hist_purchase_date_min'])\ntrain['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\ntrain['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_card_id_size']\ntrain['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\ntrain['hist_purchase_date_uptomin'] = (datetime.datetime.today() - train['hist_purchase_date_min']).dt.days\ntrain['hist_first_buy'] = (train['hist_purchase_date_min'] - train['first_active_month']).dt.days\ntrain['hist_last_buy'] = (train['hist_purchase_date_max'] - train['first_active_month']).dt.days\n\nfor feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n    train[feature] = train[feature].astype(np.int64) * 1e-9\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"370757bfba37b643f463f385d9e878ff27e320bc","trusted":true},"cell_type":"code","source":"#Feature Engineering - Adding new features inspired by Chau's first kernel\ntest['hist_purchase_date_max'] = pd.to_datetime(test['hist_purchase_date_max'])\ntest['hist_purchase_date_min'] = pd.to_datetime(test['hist_purchase_date_min'])\ntest['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\ntest['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_card_id_size']\ntest['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\ntest['hist_purchase_date_uptomin'] = (datetime.datetime.today() - test['hist_purchase_date_min']).dt.days\n\ntest['hist_first_buy'] = (test['hist_purchase_date_min'] - test['first_active_month']).dt.days\ntest['hist_last_buy'] = (test['hist_purchase_date_max'] - test['first_active_month']).dt.days\n\nfor feature in ['hist_purchase_date_max','hist_purchase_date_min']:\n    test[feature] = test[feature].astype(np.int64) * 1e-9\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c374b5e1327cd726ddfb85f7d1afceb2fd9c5df","scrolled":true,"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88381a0c6e50924860a133e7d61838c86ee1eab1","trusted":true},"cell_type":"code","source":"# Taking Reference from Other Kernels\ndef aggregate_transaction_new(trans, prefix):  \n        \n    agg_func = {\n        'purchase_amount' : ['sum','max','min','mean','var','skew'],\n        'installments' : ['sum','max','mean','var','skew'],\n        'purchase_date' : ['max','min'],\n        'month_lag' : ['max','min','mean','var','skew'],\n        'month_diff' : ['max','min','mean','var','skew'],\n        'weekend' : ['sum', 'mean'],\n        'weekday' : ['sum', 'mean'],\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum','mean', 'max','min'],\n        'card_id' : ['size','count'],\n        'month': ['nunique', 'mean', 'min', 'max'],\n        'hour': ['nunique', 'mean', 'min', 'max'],\n        'weekofyear': ['nunique', 'mean', 'min', 'max'],\n        'day': ['nunique', 'mean', 'min', 'max'],\n        'subsector_id': ['nunique'],\n        'merchant_category_id' : ['nunique'],\n        'price' :['sum','mean','max','min','var'],\n        'duration' : ['mean','min','max','var','skew'],\n        'amount_month_ratio':['mean','min','max','var','skew']\n    }\n    \n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c653335214ef763a6d9e13de46db5800a75c5a3","trusted":true},"cell_type":"code","source":"# Now extract the data from the new transactions\nnew_transactions = reduce_mem_usage(pd.read_csv('../input/new_merchant_transactions.csv'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd7fc65f351e45fdd1e8a43b730b8ed50b2e4456","scrolled":false,"trusted":true},"cell_type":"code","source":"#impute missing values\nnew_transactions['category_2'] = new_transactions['category_2'].fillna(1.0,inplace=True)\nnew_transactions['category_3'] = new_transactions['category_3'].fillna('A',inplace=True)\nnew_transactions['merchant_id'] = new_transactions['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\nnew_transactions['installments'].replace(-1, np.nan,inplace=True)\nnew_transactions['installments'].replace(999, np.nan,inplace=True)\nnew_transactions['purchase_amount'] = new_transactions['purchase_amount'].apply(lambda x: min(x, 0.8))\n\n#Feature Engineering - Adding new features inspired by Chau's first kernel\nnew_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y': 1, 'N': 0})\nnew_transactions['category_1'] = new_transactions['category_1'].map({'Y': 1, 'N': 0})\nnew_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2}) \n\nnew_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\nnew_transactions['month'] = new_transactions['purchase_date'].dt.month\nnew_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\nnew_transactions['day'] = new_transactions['purchase_date'].dt.day\nnew_transactions['weekday'] = new_transactions.purchase_date.dt.weekday\nnew_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\nnew_transactions['hour'] = new_transactions['purchase_date'].dt.hour \nnew_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)//30\nnew_transactions['month_diff'] += new_transactions['month_lag']\n\ngc.collect()\n\n# additional features\nnew_transactions['duration'] = new_transactions['purchase_amount']*new_transactions['month_diff']\nnew_transactions['amount_month_ratio'] = new_transactions['purchase_amount']/new_transactions['month_diff']\nnew_transactions['price'] = new_transactions['purchase_amount'] / new_transactions['installments']\n\naggs = {\n        'mean': ['mean'],\n    }\n\nfor col in ['category_2','category_3']:\n    new_transactions[col+'_mean'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('mean')\n    new_transactions[col+'_max'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('max')\n    new_transactions[col+'_min'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('min')\n    new_transactions[col+'_var'] = new_transactions['purchase_amount'].groupby(new_transactions[col]).agg('var')\n    aggs[col+'_mean'] = ['mean']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ebf2d2ac4064c8e9a008f555c7f902ca0fdf853","scrolled":false,"trusted":true},"cell_type":"code","source":"merge_new = aggregate_transaction_new(new_transactions, prefix='new_')\ndel new_transactions\ngc.collect()\n\ntrain = pd.merge(train, merge_new, on='card_id',how='left')\ntest = pd.merge(test, merge_new, on='card_id',how='left')\ndel merge_new\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4741d24d2c897ebd286807d0d3bd658800e316ca","trusted":true},"cell_type":"code","source":"#Feature Engineering - Adding new features inspired by Chau's first kernel\ntrain['new_purchase_date_max'] = pd.to_datetime(train['new_purchase_date_max'])\ntrain['new_purchase_date_min'] = pd.to_datetime(train['new_purchase_date_min'])\ntrain['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\ntrain['new_purchase_date_average'] = train['new_purchase_date_diff']/train['new_card_id_size']\ntrain['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\ntrain['new_purchase_date_uptomin'] = (datetime.datetime.today() - train['new_purchase_date_min']).dt.days\ntrain['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\ntrain['new_last_buy'] = (train['new_purchase_date_max'] - train['first_active_month']).dt.days\nfor feature in ['new_purchase_date_max','new_purchase_date_min']:\n    train[feature] = train[feature].astype(np.int64) * 1e-9\n\n#Feature Engineering - Adding new features inspired by Chau's first kernel\ntest['new_purchase_date_max'] = pd.to_datetime(test['new_purchase_date_max'])\ntest['new_purchase_date_min'] = pd.to_datetime(test['new_purchase_date_min'])\ntest['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\ntest['new_purchase_date_average'] = test['new_purchase_date_diff']/test['new_card_id_size']\ntest['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\ntest['new_purchase_date_uptomin'] = (datetime.datetime.today() - test['new_purchase_date_min']).dt.days\ntest['new_first_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\ntest['new_last_buy'] = (test['new_purchase_date_max'] - test['first_active_month']).dt.days\n\nfor feature in ['new_purchase_date_max','new_purchase_date_min']:\n    test[feature] = test[feature].astype(np.int64) * 1e-9\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12b0f46661a53ead1c430acc0ec0a83eee4d3b0f","trusted":true},"cell_type":"code","source":"#NEW Features referred from https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\ntrain['card_id_total'] = train['new_card_id_size']+train['hist_card_id_size']\ntrain['card_id_cnt_total'] = train['new_card_id_count']+train['hist_card_id_count']\ntrain['card_id_cnt_ratio'] = train['new_card_id_count']/train['hist_card_id_count']\ntrain['purchase_amount_total'] = train['new_purchase_amount_sum']+train['hist_purchase_amount_sum']\ntrain['purchase_amount_mean'] = train['new_purchase_amount_mean']+train['hist_purchase_amount_mean']\ntrain['purchase_amount_max'] = train['new_purchase_amount_max']+train['hist_purchase_amount_max']\ntrain['purchase_amount_min'] = train['new_purchase_amount_min']+train['hist_purchase_amount_min']\ntrain['purchase_amount_ratio'] = train['new_purchase_amount_sum']/train['hist_purchase_amount_sum']\ntrain['month_diff_mean'] = train['new_month_diff_mean']+train['hist_month_diff_mean']\ntrain['month_diff_ratio'] = train['new_month_diff_mean']/train['hist_month_diff_mean']\ntrain['month_lag_mean'] = train['new_month_lag_mean']+train['hist_month_lag_mean']\ntrain['month_lag_max'] = train['new_month_lag_max']+train['hist_month_lag_max']\ntrain['month_lag_min'] = train['new_month_lag_min']+train['hist_month_lag_min']\ntrain['category_1_mean'] = train['new_category_1_mean']+train['hist_category_1_mean']\ntrain['installments_total'] = train['new_installments_sum']+train['hist_installments_sum']\ntrain['installments_mean'] = train['new_installments_mean']+train['hist_installments_mean']\ntrain['installments_max'] = train['new_installments_max']+train['hist_installments_max']\ntrain['installments_ratio'] = train['new_installments_sum']/train['hist_installments_sum']\ntrain['price_total'] = train['purchase_amount_total'] / train['installments_total']\ntrain['price_mean'] = train['purchase_amount_mean'] / train['installments_mean']\ntrain['price_max'] = train['purchase_amount_max'] / train['installments_max']\ntrain['duration_mean'] = train['new_duration_mean']+train['hist_duration_mean']\ntrain['duration_min'] = train['new_duration_min']+train['hist_duration_min']\ntrain['duration_max'] = train['new_duration_max']+train['hist_duration_max']\ntrain['amount_month_ratio_mean']=train['new_amount_month_ratio_mean']+train['hist_amount_month_ratio_mean']\ntrain['amount_month_ratio_min']=train['new_amount_month_ratio_min']+train['hist_amount_month_ratio_min']\ntrain['amount_month_ratio_max']=train['new_amount_month_ratio_max']+train['hist_amount_month_ratio_max']\ntrain['new_CLV'] = train['new_card_id_count'] * train['new_purchase_amount_sum'] / train['new_month_diff_mean']\ntrain['hist_CLV'] = train['hist_card_id_count'] * train['hist_purchase_amount_sum'] / train['hist_month_diff_mean']\ntrain['CLV_ratio'] = train['new_CLV'] / train['hist_CLV']\n\ntest['card_id_total'] = test['new_card_id_size']+test['hist_card_id_size']\ntest['card_id_cnt_total'] = test['new_card_id_count']+test['hist_card_id_count']\ntest['card_id_cnt_ratio'] = test['new_card_id_count']/test['hist_card_id_count']\ntest['purchase_amount_total'] = test['new_purchase_amount_sum']+test['hist_purchase_amount_sum']\ntest['purchase_amount_mean'] = test['new_purchase_amount_mean']+test['hist_purchase_amount_mean']\ntest['purchase_amount_max'] = test['new_purchase_amount_max']+test['hist_purchase_amount_max']\ntest['purchase_amount_min'] = test['new_purchase_amount_min']+test['hist_purchase_amount_min']\ntest['purchase_amount_ratio'] = test['new_purchase_amount_sum']/test['hist_purchase_amount_sum']\ntest['month_diff_mean'] = test['new_month_diff_mean']+test['hist_month_diff_mean']\ntest['month_diff_ratio'] = test['new_month_diff_mean']/test['hist_month_diff_mean']\ntest['month_lag_mean'] = test['new_month_lag_mean']+test['hist_month_lag_mean']\ntest['month_lag_max'] = test['new_month_lag_max']+test['hist_month_lag_max']\ntest['month_lag_min'] = test['new_month_lag_min']+test['hist_month_lag_min']\ntest['category_1_mean'] = test['new_category_1_mean']+test['hist_category_1_mean']\ntest['installments_total'] = test['new_installments_sum']+test['hist_installments_sum']\ntest['installments_mean'] = test['new_installments_mean']+test['hist_installments_mean']\ntest['installments_max'] = test['new_installments_max']+test['hist_installments_max']\ntest['installments_ratio'] = test['new_installments_sum']/test['hist_installments_sum']\ntest['price_total'] = test['purchase_amount_total'] / test['installments_total']\ntest['price_mean'] = test['purchase_amount_mean'] / test['installments_mean']\ntest['price_max'] = test['purchase_amount_max'] / test['installments_max']\ntest['duration_mean'] = test['new_duration_mean']+test['hist_duration_mean']\ntest['duration_min'] = test['new_duration_min']+test['hist_duration_min']\ntest['duration_max'] = test['new_duration_max']+test['hist_duration_max']\ntest['amount_month_ratio_mean']=test['new_amount_month_ratio_mean']+test['hist_amount_month_ratio_mean']\ntest['amount_month_ratio_min']=test['new_amount_month_ratio_min']+test['hist_amount_month_ratio_min']\ntest['amount_month_ratio_max']=test['new_amount_month_ratio_max']+test['hist_amount_month_ratio_max']\ntest['new_CLV'] = test['new_card_id_count'] * test['new_purchase_amount_sum'] / test['new_month_diff_mean']\ntest['hist_CLV'] = test['hist_card_id_count'] * test['hist_purchase_amount_sum'] / test['hist_month_diff_mean']\ntest['CLV_ratio'] = test['new_CLV'] / test['hist_CLV']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e977529f1ee19cf2b79c1a08fbf65a10bc047f14","trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9aa541a016e5f0c3c66d9eebd30e6c713711d6a","trusted":true},"cell_type":"code","source":"train = train.drop(['card_id', 'first_active_month'], axis = 1)\ntest = test.drop(['card_id', 'first_active_month'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8323aef4eb43a9821d808578fa00514fc1ea063","trusted":true},"cell_type":"code","source":"# Get the X and Y\ndf_train_columns = [c for c in train.columns if c not in ['first_active_month', 'target', 'card_id', 'outliers',\n                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size']] \ntarget = train['target']\ndel train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1067b18790d511ed78b7ad1896bba120dcdf2990"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dac5e3fdfb1731d5b883ab2268f386cf61a869f7"},"cell_type":"markdown","source":"Create a scoring function that uses LightGBM in RandomForest mode fitted on the full dataset"},{"metadata":{"trusted":true,"_uuid":"a5927459a6bba2f5473d4400122444b4471aece3"},"cell_type":"code","source":"def get_feature_importances(data, shuffle, seed=None):\n    # Gather real features\n    train_features = df_train_columns\n    # Go over fold and keep track of CV score (train and valid) and feature importances\n    \n    # Shuffle target if required\n    y = target.copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = target.copy().sample(frac=1.0)\n    \n    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n    lgb_params = {\n        'task': 'train',\n        'boosting': 'goss',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'learning_rate': 0.01,\n        'subsample': 0.9855232997390695,\n        'max_depth': 7,\n        'top_rate': 0.9064148448434349,\n        'num_leaves': 63,\n        'min_child_weight': 41.9612869171337,\n        'other_rate': 0.0721768246018207,\n        'reg_alpha': 9.677537745007898,\n        'colsample_bytree': 0.5665320670155495,\n        'min_split_gain': 9.820197773625843,\n        'reg_lambda': 8.2532317400459,\n        'min_data_in_leaf': 21,\n        'verbose': -1\n    }\n    \n    # Fit the model\n    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n    oof = clf.predict(train, num_iteration=clf.best_iteration)\n    \n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(train_features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n    imp_df['trn_score'] = format(mean_squared_error(oof, target)**0.5)\n    \n    return imp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbdc43403f0a4ddb7ce7de452a75990e4606ab10"},"cell_type":"code","source":"# Seed the unexpected randomness of this world\nnp.random.seed(123)\n# Get the actual importance, i.e. without shuffling\nactual_imp_df = get_feature_importances(data=train, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4312e57515cc1240d3d581fd2214de2d87515d90"},"cell_type":"code","source":"actual_imp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f314e7e2a74384ff32c52b9fd306db28c4e21ccd"},"cell_type":"markdown","source":"Build Null Importances distribution"},{"metadata":{"trusted":true,"_uuid":"87ccfdba2278684883c5cea2e7a438e86a1da56e"},"cell_type":"code","source":"null_imp_df = pd.DataFrame()\nnb_runs = 80\nimport time\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=train, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    # Erase previous message\n    for l in range(len(dsp)):\n        print('\\b', end='', flush=True)\n    # Display current run and time used\n    spent = (time.time() - start) / 60\n    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n    print(dsp, end='', flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2eca5933ca4c0d67298bfbf0d4d6819fe38a99e5"},"cell_type":"code","source":"null_imp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58dd27fca04f32829a0c5d91721207b585d8db07"},"cell_type":"code","source":"def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26fd6e7ce8602589ac4e52bde00476bd6f6b9655"},"cell_type":"markdown","source":"Display distribution examples"},{"metadata":{"trusted":true,"_uuid":"9703dd9b822e4b5cdd9445705e37be498f86686e"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='feature_1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d8d2a194b8675526de4e88e8df896fb55c8a5b"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='feature_2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21348edd92649e1c36a6edc6d51deb3e9c720104"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='days')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39292170c760d704b1019ffe24e7f3719b0fa3a9"},"cell_type":"markdown","source":"Any feature sufficient variance can be used and made sense of by tree models. You can always find splits that help scoring better\nCorrelated features have decaying importances once one of them is used by the model. The chosen feature will have strong importance and its correlated suite will have decaying importances\n\nThe current method allows to :\n* Drop high variance features if they are not really related to the target\n* Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)\n* Score features\n\nThere are several ways to score features :\n* Compute the number of samples in the actual importances that are away from the null importances recorded distribution.\n* Compute ratios like Actual / Null Max, Actual / Null Mean, Actual Mean / Null Max\n* In a first step I will use the log actual feature importance divided by the 75 percentile of null distribution."},{"metadata":{"trusted":true,"_uuid":"e07fd3e705a87366ef9e85f0886afd5304e1eb54"},"cell_type":"code","source":"feature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b72a304799765ac7d968517b0a6f3aa069cc9b3a"},"cell_type":"markdown","source":"Check the impact of removing uncorrelated features"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d5c2171dd6a8b7ec44f2ef9103261a4e849e0c35"},"cell_type":"code","source":"correlation_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n    correlation_scores.append((_f, split_score, gain_score))\n\ncorr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n\nfig = plt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\nfig.subplots_adjust(top=0.93)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c96b3fd6cf98ab52633b7223451a4caabe9f5a58"},"cell_type":"code","source":"null_imp_df.to_csv('null_importances_distribution_rf.csv')\nactual_imp_df.to_csv('actual_importances_ditribution_rf.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}