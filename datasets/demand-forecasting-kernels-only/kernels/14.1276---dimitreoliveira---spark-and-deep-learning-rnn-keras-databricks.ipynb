{"cells":[{"metadata":{"_uuid":"7982bad55cd54d575ce8f215852d463c81a8dd8f"},"cell_type":"markdown","source":"# Store Item Demand Forecasting Challenge - Spark and deep learning\n### link for the github repository (spark part is on ipynb): https://github.com/dimitreOliveira/StoreItemDemand"},{"metadata":{"trusted":true,"_uuid":"a001274bb69489de06b063141c191d87e2a4b02d"},"cell_type":"code","source":"# this code get the resulting dataset that i uploaded from my databricks code and commits.\nimport pandas as pd\nimport os\n\n\nsubmission25 = pd.read_csv('../input/test_data/model25.csv')\nsubmission25.to_csv('submission25.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"62b44e0fbab09d6eea683f585f3f32ec712fe0c6","_kg_hide-output":true},"cell_type":"code","source":"from pyspark.sql import Window\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.types import *\nfrom pyspark.sql import types as T\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import functions as F\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol\n\ndays = lambda i: i * 86400\nget_weekday = udf(lambda x: x.weekday())\nserie_has_null = F.udf(lambda x: reduce((lambda x, y: x and y), x))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"b6b11f1985ee840804bf1ad4a9f1350b93f3e472","_kg_hide-output":true},"cell_type":"code","source":"import json\nimport numpy as np\nfrom keras.models import model_from_json\nunlist = lambda x: [float(i[0]) for i in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8522a12f1c56775731ef39442c1a7a6c39bec23","_kg_hide-output":true},"cell_type":"code","source":"def prepare_data(data):\n    list_result = []\n    for i in range(len(data)):\n        list_result.append(np.asarray(data[i]))\n    return np.asarray(list_result)\n\ndef prepare_collected_data(data):\n    list_features = []\n    list_labels = []\n    for i in range(len(data)):\n        list_features.append(np.asarray(data[i][0]))\n        list_labels.append(data[i][1])\n    return np.asarray(list_features), np.asarray(list_labels)\n\ndef prepare_collected_data_test(data):\n    list_features = []\n    for i in range(len(data)):\n        list_features.append(np.asarray(data[i][0]))\n    return np.asarray(list_features)\n\n\ndef save_model(model_path, weights_path, model):\n    \"\"\"\n    Save model.\n    \"\"\"\n    np.save(weights_path, model.get_weights())\n    with open(model_path, 'w') as f:\n        json.dump(model.to_json(), f)\n    \ndef load_model(model_path, weights_path):\n    \"\"\"\n    Load model.\n    \"\"\"\n    with open(model_path, 'r') as f:\n        data = json.load(f)\n\n    model = model_from_json(data)\n    weights = np.load(weights_path)\n    model.set_weights(weights)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"80a7ba5be683024abc6313599f40946314de7813","_kg_hide-output":true},"cell_type":"code","source":"class DateConverter(Transformer):\n    def __init__(self, inputCol, outputCol):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != TimestampType()):\n        raise Exception('Input type %s did not match input type TimestampType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, df.date.cast(self.inputCol))\n    \n    \nclass DayExtractor(Transformer):\n    def __init__(self, inputCol, outputCol='day'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != DateType()):\n            raise Exception('DayExtractor input type %s did not match input type DateType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.dayofmonth(df[self.inputCol]))\n    \n    \nclass MonthExtractor(Transformer):\n    def __init__(self, inputCol, outputCol='month'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != DateType()):\n            raise Exception('MonthExtractor input type %s did not match input type DateType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.month(df[self.inputCol]))\n    \n    \nclass YearExtractor(Transformer):\n    def __init__(self, inputCol, outputCol='year'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != DateType()):\n            raise Exception('YearExtractor input type %s did not match input type DateType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.year(df[self.inputCol]))\n    \n    \nclass WeekDayExtractor(Transformer):\n    def __init__(self, inputCol, outputCol='weekday'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != DateType()):\n            raise Exception('WeekDayExtractor input type %s did not match input type DateType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, get_weekday(df[self.inputCol]).cast('int'))\n    \n    \nclass WeekendExtractor(Transformer):\n    def __init__(self, inputCol='weekday', outputCol='weekend'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != IntegerType()):\n            raise Exception('WeekendExtractor input type %s did not match input type IntegerType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.when(((df[self.inputCol] == 5) | (df[self.inputCol] == 6)), 1).otherwise(0))\n    \n    \nclass SerieMaker(Transformer):\n    def __init__(self, inputCol='scaledFeatures', outputCol='serie', dateCol='date', idCol=['store', 'item'], serieSize=30):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n        self.dateCol = dateCol\n        self.serieSize = serieSize\n        self.idCol = idCol\n\n    def _transform(self, df):\n        window = Window.partitionBy(self.idCol).orderBy(self.dateCol)\n        series = []   \n        \n    df = df.withColumn('filled_serie', F.lit(0))\n    \n    for index in reversed(range(0, self.serieSize)):\n        window2 = Window.partitionBy(self.idCol).orderBy(self.dateCol).rowsBetween((30 - index), 30)\n        col_name = (self.outputCol + '%s' % index)\n        series.append(col_name)\n        df = df.withColumn(col_name, F.when(F.isnull(F.lag(F.col(self.inputCol), index).over(window)), F.first(F.col(self.inputCol), ignorenulls=True).over(window2)).otherwise(F.lag(F.col(self.inputCol), index).over(window)))\n        df = df.withColumn('filled_serie', F.when(F.isnull(F.lag(F.col(self.inputCol), index).over(window)), (F.col('filled_serie') + 1)).otherwise(F.col('filled_serie')))\n\n    df = df.withColumn('rank', F.rank().over(window))\n    df = df.withColumn(self.outputCol, F.array(*series))\n    \n    return df.drop(*series)\n\n\nclass MonthBeginExtractor(Transformer):\n    def __init__(self, inputCol='day', outputCol='monthbegin'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != IntegerType()):\n            raise Exception('MonthBeginExtractor input type %s did not match input type IntegerType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.when((df[self.inputCol] <= 7), 1).otherwise(0))\n    \n    \nclass MonthEndExtractor(Transformer):\n    def __init__(self, inputCol='day', outputCol='monthend'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != IntegerType()):\n            raise Exception('MonthEndExtractor input type %s did not match input type IntegerType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.when((df[self.inputCol] >= 24), 1).otherwise(0))\n    \n    \nclass YearQuarterExtractor(Transformer):\n    def __init__(self, inputCol='month', outputCol='yearquarter'):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    \n    def check_input_type(self, schema):\n        field = schema[self.inputCol]\n        if (field.dataType != IntegerType()):\n            raise Exception('YearQuarterExtractor input type %s did not match input type IntegerType' % field.dataType)\n\n    def _transform(self, df):\n        self.check_input_type(df.schema)\n        return df.withColumn(self.outputCol, F.when((df[self.inputCol] <= 3), 0)\n                               .otherwise(F.when((df[self.inputCol] <= 6), 1)\n                                .otherwise(F.when((df[self.inputCol] <= 9), 2)\n                                 .otherwise(3))))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"1493d817e5b0b4bacdbb8f43d56a8814c87b5586","_kg_hide-output":true},"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\n\ntrain_data = spark.sql(\"select * from store_item_demand_train_csv\")\n\ntrain, validation = train_data.randomSplit([0.8,0.2], seed=1234)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"5008c5eaca86134f67524f41d029f9c943633a36","_kg_hide-output":true},"cell_type":"code","source":"# Feature extraction\ndc = DateConverter(inputCol='date', outputCol='dateFormated')\ndex = DayExtractor(inputCol='dateFormated')\nmex = MonthExtractor(inputCol='dateFormated')\nyex = YearExtractor(inputCol='dateFormated')\nwdex = WeekDayExtractor(inputCol='dateFormated')\nwex = WeekendExtractor()\nmbex = MonthBeginExtractor()\nmeex = MonthEndExtractor()\nyqex = YearQuarterExtractor()\n\n# Data process\nva = VectorAssembler(inputCols=['store', 'item', 'day', 'month', 'year', 'weekday', 'weekend', 'monthbegin', 'monthend', 'yearquarter'], outputCol=\"features\")\nscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n\n# Serialize data\nsm = SerieMaker(inputCol='scaledFeatures', dateCol='date', idCol=['store', 'item'], serieSize=15)\n\npipeline = Pipeline(stages=[dc, dex, mex, yex, wdex, wex, mbex, meex, yqex, va, scaler, sm])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"a3a36de54795c09ea5906187ad798e52659b807d","_kg_hide-output":true},"cell_type":"code","source":"pipiline_model = pipeline.fit(train)\n\ntrain_transformed = pipiline_model.transform(train)\nvalidation_transformed = pipiline_model.transform(validation)\n\ntrain_transformed.write.saveAsTable('train_transformed_15', mode='overwrite')\nvalidation_transformed.write.saveAsTable('validation_transformed_15', mode='overwrite')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"92094494a7e582a25c8cb93bbbdf582bf626555e","_kg_hide-output":true},"cell_type":"code","source":"test_data = spark.sql(\"select * from store_item_demand_test_csv\")\ntest_transformed = pipiline_model.transform(test_data)\ntest_transformed.write.saveAsTable('test_transformed_15', mode='overwrite')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"3356d0653bb621d4a52456910d71212493792b2c","_kg_hide-output":true},"cell_type":"code","source":"from keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ntrain_transformed = spark.sql(\"select * from train_transformed\")\nvalidation_transformed = spark.sql(\"select * from validation_transformed\")\n\ntrain_x, train_y = prepare_collected_data(train_transformed.select('serie', 'sales').collect())\nvalidation_x, validation_y = prepare_collected_data(validation_transformed.select('serie', 'sales').collect())\n\nn_label = 1\nserie_size = len(train_x[0])\nn_features = len(train_x[0][0])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"c7cada1499116151945e8e24099ecec12ca18b16","_kg_hide-output":true},"cell_type":"code","source":"# hyperparameters\nepochs = 80\nbatch = 512\nlr = 0.001\n\n# design network\nmodel = Sequential()\nmodel.add(GRU(40, input_shape=(serie_size, n_features)))\nmodel.add(Dense(10, kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dense(n_label))\nmodel.summary()\n\nadam = optimizers.Adam(lr)\nmodel.compile(loss='mae', optimizer=adam, metrics=['mse', 'msle'])\n\nhistory = model.fit(train_x, train_y, epochs=epochs, batch_size=batch, validation_data=(validation_x, validation_y), verbose=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"29720d4fab8b02adc26ae8e9776b324b0a2a6c30","_kg_hide-output":true},"cell_type":"code","source":"model_path = '/dbfs/user/model1.json'\nweights_path = '/dbfs/user/weights1.npy'\nsave_model(model_path, weights_path, model)\n\npredictions = model.predict(validation_x)\n\nimport pandas as pd\nids = validation_y\ndf = pd.DataFrame(ids, columns=['label'])\ndf['sales'] = predictions\ndf_predictions = spark.createDataFrame(df)\n\nrmse_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"rmse\")\nmse_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"mse\")\nmae_evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"sales\", metricName=\"mae\")\n\nvalidation_rmse = rmse_evaluator.evaluate(df_predictions)\nvalidation_mse = mse_evaluator.evaluate(df_predictions)\nvalidation_mae = mae_evaluator.evaluate(df_predictions)\nprint(\"RMSE: %f, MSE: %f, MAE: %f\" % (validation_rmse, validation_mse, validation_mae))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"collapsed":true,"_uuid":"e63e77e84fe3fda9c3052d542abd854ef6b59b51","_kg_hide-output":true},"cell_type":"code","source":"model_path = '/dbfs/user/model1.json'\nweights_path = '/dbfs/user/weights1.npy'\nmodel = load_model(model_path, weights_path)\n\ntest_transformed = spark.sql(\"select * from test_transformed_15\")\n\ntest = prepare_collected_data_test(test_transformed.select('serie').collect())\n\nids = test_transformed.select('id').collect()\n\npredictions = model.predict(test)\n\nimport pandas as pd\ndf = pd.DataFrame(ids, columns=['id'])\ndf['sales'] = predictions\ndf_predictions = spark.createDataFrame(df)\n\ndf_predictions = df_predictions.withColumn('sales', df_predictions['sales'].cast('int'))\ndisplay(df_predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}