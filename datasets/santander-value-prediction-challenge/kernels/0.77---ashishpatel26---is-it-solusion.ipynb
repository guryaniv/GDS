{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56f3c3499260fe04c9d0c6535f59ea3320b8c987"},"cell_type":"markdown","source":"Please go through Giba's post and kernel  to underrstand what this leak is all about\nhttps://www.kaggle.com/titericz/the-property-by-giba (kernel)\nhttps://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61329 (post)\n\nAlso, go through this Jiazhen's kernel which finds more columns to exploit leak\nhttps://www.kaggle.com/johnfarrell/giba-s-property-extended-result\n\nI just exploit data property in brute force way and then fill in remaining by row non zero means! This should bring everyone on level-playing field.\n\n**Let the competition begin! :D**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import mode, skew, kurtosis, entropy\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc37a766646b5993cef0bc87ad6882728dd20cb2","collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntransact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\ny = np.log1p(train[\"target\"]).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dcfc4df1340c38bfeac43fd4d19ba2763b3b916"},"cell_type":"markdown","source":"We take time series columns from [here](https://www.kaggle.com/johnfarrell/giba-s-property-extended-result)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import gc\ngc.collect();\ncols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2', '190db8488', \n        'adb64ff71', 'c47340d97', 'c5a231d81'\n       ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d30502c03726f6eb52a3ab05049211e3cb8ce572"},"cell_type":"code","source":"def get_beautiful_test(test):\n    test_rnd = np.round(test.iloc[:, 1:], 2)\n    ugly_indexes = []\n    non_ugly_indexes = []\n    for idx in tqdm(range(len(test))):\n        if not np.all(\n            test_rnd.iloc[idx, :].values==test.iloc[idx, 1:].values\n        ):\n            ugly_indexes.append(idx)\n        else:\n            non_ugly_indexes.append(idx)\n    print(len(ugly_indexes), len(non_ugly_indexes))\n    np.save('test_ugly_indexes', np.array(ugly_indexes))\n    np.save('test_non_ugly_indexes', np.array(non_ugly_indexes))\n    test = test.iloc[non_ugly_indexes].reset_index(drop=True)\n    return test, non_ugly_indexes, ugly_indexes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50e3516743926acab8ae59413ca8ed258547e446"},"cell_type":"code","source":"test, non_ugly_indexes, ugly_indexes = get_beautiful_test(test)\ntest[\"target\"] = train[\"target\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d61c75092518f50a879e9e3d5883ab752f73912b"},"cell_type":"code","source":"from multiprocessing import Pool\nCPU_CORES = 1\ndef _get_leak(df, cols, lag=0, verbose=False):\n    \"\"\" To get leak value, we do following:\n       1. Get string of all values after removing first two time steps\n       2. For all rows we shift the row by two steps and again make a string\n       3. Just find rows where string from 2 matches string from 1\n       4. Get 1st time step of row in 3 (Currently, there is additional condition to only fetch value if we got exactly one match in step 3)\"\"\"\n    series_str = df[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n    series_shifted_str = df[cols].shift(lag+2, axis=1)[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n    if verbose:\n        target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str)[0])\n    else:\n        target_rows = series_shifted_str.apply(lambda x: np.where(x == series_str)[0])\n    target_vals = target_rows.apply(lambda x: df.loc[x[0], cols[lag]] if len(x)==1 else 0)\n    return target_vals\n\ndef get_all_leak(df, cols=None, nlags=15):\n    \"\"\"\n    We just recursively fetch target value for different lags\n    \"\"\"\n    df =  df.copy()    \n    for i in range(nlags):\n        if \"leaked_target_\"+str(i) not in df.columns:\n            print(\"Processing lag {}\".format(i))\n            df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n    return df\n\ndef compiled_leak_result():\n    \n    max_nlags = len(cols) - 2\n    train_leak = train[[\"ID\", \"target\"] + cols]\n    train_leak[\"compiled_leak\"] = 0\n    train_leak[\"nonzero_mean\"] = train[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    \n    scores = []\n    leaky_value_counts = []\n    leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        \n        print('Processing lag', i)\n        train_leak[c] = _get_leak(train_leak, cols, i)\n        \n        leaky_cols.append(c)\n        train_leak = train.join(\n            train_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )\n        zeroleak = train_leak[\"compiled_leak\"]==0\n        train_leak.loc[zeroleak, \"compiled_leak\"] = train_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(train_leak[\"compiled_leak\"] > 0))\n        _correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        leaky_value_corrects.append(_correct_counts/leaky_value_counts[-1])\n        print(\"Leak values found in train\", leaky_value_counts[-1])\n        print(\n            \"% of correct leaks values in train \", \n            leaky_value_corrects[-1]\n        )\n        tmp = train_leak.copy()\n        tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        print(\n            'Score (filled with nonzero mean)', \n            scores[-1]\n        )\n    result = dict(\n        score=scores, \n        leaky_count=leaky_value_counts,\n        leaky_correct=leaky_value_corrects,\n    )\n    return train_leak, result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12641e2cd0b613a5f1536296a0f1105dc10f94ee"},"cell_type":"code","source":"train_leak, result = compiled_leak_result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"756b69322abc1e3a06c179f0bcd603bbf3e030d9"},"cell_type":"code","source":"result = pd.DataFrame.from_dict(result, orient='columns')\nresult\nresult.to_csv('train_leaky_stat.csv', index=False)\ntrain_leak.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb0f96a9aff50eb09b37a696bbc99a430e9e7010"},"cell_type":"code","source":"best_score = np.min(result['score'])\nbest_lag = np.argmin(result['score'])\nprint('best_score', best_score, '\\nbest_lag', best_lag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"596e389db4ee4ae41c1d90a2db206c38ba3892f4"},"cell_type":"code","source":"def rewrite_compiled_leak(leak_df, lag):\n    leak_df[\"compiled_leak\"] = 0\n    for i in range(lag):\n        c = \"leaked_target_\"+str(i)\n        zeroleak = leak_df[\"compiled_leak\"]==0\n        leak_df.loc[zeroleak, \"compiled_leak\"] = leak_df.loc[zeroleak, c]\n    return leak_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad9a7c1db42645c01c9dfe10d600b08c3e6733ba"},"cell_type":"code","source":"leaky_cols = [c for c in train_leak.columns if 'leaked_target_' in c]\ntrain_leak = rewrite_compiled_leak(train_leak, best_lag)\ntrain_leak[['ID']+leaky_cols+['compiled_leak']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bf6adaeb15f194bea4490a632e0ad723862d0b2c"},"cell_type":"code","source":"train_res = train_leak[leaky_cols+['compiled_leak']].replace(0.0, np.nan)\ntrain_res.to_csv('train_leak.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a55b691aad5200cbee855c4eaa47abf024a7ca4c"},"cell_type":"code","source":"def compiled_leak_result_test(max_nlags):\n    test_leak = test[[\"ID\", \"target\"] + cols]\n    test_leak[\"compiled_leak\"] = 0\n    test_leak[\"nonzero_mean\"] = test[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    \n    scores = []\n    leaky_value_counts = []\n    # leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        \n        print('Processing lag', i)\n        test_leak[c] = _get_leak(test_leak, cols, i, verbose=True)\n        \n        leaky_cols.append(c)\n        test_leak = test.join(\n            test_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n        zeroleak = test_leak[\"compiled_leak\"]==0\n        test_leak.loc[zeroleak, \"compiled_leak\"] = test_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(test_leak[\"compiled_leak\"] > 0))\n        #_correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        #leaky_value_corrects.append(_correct_counts/leaky_value_counts[-1])\n        print(\"Leak values found in test\", leaky_value_counts[-1])\n        #print(\n        #    \"% of correct leaks values in train \", \n        #    leaky_value_corrects[-1]\n        #)\n        #tmp = train_leak.copy()\n        #tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        #scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        #print(\n        #    'Score (filled with nonzero mean)', \n        #    scores[-1]\n        #)\n    result = dict(\n        # score=scores, \n        leaky_count=leaky_value_counts,\n        # leaky_correct=leaky_value_corrects,\n    )\n    return test_leak, result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6c17bf082719d9ce6fd28d08820a6d1bdb20e832"},"cell_type":"code","source":"test_leak, test_result = compiled_leak_result_test(max_nlags=38)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"feccff943b947a375c8ad4b95952bd2ea88063a4"},"cell_type":"code","source":"test_result = pd.DataFrame.from_dict(test_result, orient='columns')\ntest_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cee4db12194d4e04defe9c06b86d75fa15ce1ffd"},"cell_type":"code","source":"test_result.to_csv('test_leaky_stat.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb503775965111a0d6a82e9b95f4288a36e0080b"},"cell_type":"code","source":"test_leak = rewrite_compiled_leak(test_leak, best_lag)\ntest_leak[['ID']+leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4bd12e259539a4dd08159c4a2d4ac0c115a4d8b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51d07c04c1af45bd4bc1f297f7416ce7dd88548","collapsed":true},"cell_type":"code","source":"NLAGS = 15 #Increasing this might help push score a bit\nall_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a9bc6b9a8a78fd0898668f899ae46245c2126e3","collapsed":true},"cell_type":"code","source":"leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\ntrain = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\ntest = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24b1b9fbd1626397503ed142c6eeeef04970edf2","collapsed":true},"cell_type":"code","source":"train[[\"target\"]+leaky_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23a5c3edd5556ee8e71d9d2659d9abcb9500ad5d"},"cell_type":"code","source":"train[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\ntest[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9e85f6d8444bdd2ba144502a998558cb926efb8","collapsed":true},"cell_type":"code","source":"#We start with 1st lag target and recusrsively fill zero's\ntrain[\"compiled_leak\"] = 0\ntest[\"compiled_leak\"] = 0\nfor i in range(NLAGS):\n    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n    \nprint(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\nprint(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])/sum(train[\"compiled_leak\"] > 0))\n\ntrain.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nonzero_mean\"]\ntest.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nonzero_mean\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49c9d5e53e52c4307aef6ac402868aaee8566700","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc2e522df90f97456e67f26977fde364acf02876","collapsed":true},"cell_type":"code","source":"#submission\nsub = test[[\"ID\"]]\nsub[\"target\"] = test[\"compiled_leak\"]\nsub.to_csv(\"baseline_submission_with_leaks.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df0d5b05147315d10aa81f5f28c4b2173c103d89"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}