{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"collapsed": false, "_cell_guid": "d1003b5e-ad89-4dbf-bd8f-374873efc3a4", "_execution_state": "idle", "_uuid": "8c2389033b64e0c17af66fdf47a00054f9488076"}, "source": "# Introduction #\nThis Jupyter notebook is an extension of my previous work on agricultural indicators. It continues to use the World Bank's World Development Indicators (WDI) data set in order to determine how certain values correlate and cause major production numbers. For my new project, I wish to use a number of new data analytic techniques and methods that significantly streamlines the analytic process. For this project, I examine how a number of agricultural values, including numbers on land, correlate to Value Added in agriculture, Cereal Production, and a Food Production index. \n\nAs I continue to learn different statistical methods and machine learning types, I plan on adding to this project with those new techniques. In college I studied history and Chinese and for my honors thesis studied land privatization policies in history and how they affected the output of various industries. As I transition over to government and private sector work following my graduation, I am teaching myself statistical analysis in order to further improve my skills. This project is a demonstration of my abilities. I am using [Introduction to Statistical Learning by James, et al]][1] as my current textbook.\n\nUpdates:\n06/27/2017: Section 1 with multiple linear regression completed.\n06/30/2017: Started section 2, clustering.\n\nI start by importing a number of important packages that allow us to conduct the data processing and visualizations that are the aim of this project. We use SQL as our main data import method and connect it to Pandas. \n\nIn this first section I also list a number of regions and overarching \"countries\" that cause the data to double count. Eliminating these tends to decrease our R-Squared scores, but makes the project more accurate by eliminating these extra, double-counted data points. \n\nFinally, I'm taking from [Lj Miranda's fantastic visualization series on Philippines' Energy use seen here][2]. He has a great set of color codes and methods for visualization time series data that are worth repeating here.\n\n\n  [1]: http://www-bcf.usc.edu/~gareth/ISL/\n  [2]: https://www.kaggle.com/ljvmiranda/philippines-energy-use", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"_cell_guid": "d03a8b6e-7a12-430a-b18d-701527e34afb", "_execution_state": "idle", "_uuid": "6d0786d4a7cadf9023a2ed3b4d2ef4ec6b0a3be9", "trusted": false}, "source": "import numpy as np # lienar algebra\nimport pandas as pd # data processing\nimport sqlite3 as sql # sql connector\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt # visualization\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom pylab import fill_between\n\nconn = sql.connect('../input/database.sqlite')\nregions = ['ARB','CSS','EAS','EAP','ECS','ECA','EUU','FCS','HPC','HIC','NOC','OEC','LCN',\n           'LAC','LDC','LMY','LIC','LMC','MEA','MNA','MIC','NAC','MNP','OED','OSS','PSS',\n           'SST','SAS','ZAF','SSF','SSA','UMC','WLD']\n\ntableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]  \n\nfor i in range(len(tableau20)):    \n    r, g, b = tableau20[i]    \n    tableau20[i] = (r / 255., g / 255., b / 255.)", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "fdff0028-0bee-45d3-b338-7a1cdcfbc3e7", "_execution_state": "idle", "_uuid": "623231174711ba0e64d7e8791add8b52c48b51ec", "trusted": false}, "source": "# This section is used purely to look through the wide number of indicators available to us\n# and pick out those that would best correlate for what we want to accomplish\n# series = pd.read_sql('''SELECT * FROM Series''', con = conn)\n# list(zip(series.IndicatorName,series.SeriesCode,series.LongDefinition))", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "0fc6d784-0910-4e9c-b9c0-dd0b45f08b47", "_execution_state": "idle", "_uuid": "16e24e00c24d10c4e71028b94705950f5b33cc8a", "trusted": false}, "source": "ag_indicators = {\n    'ag machinery, tractors':'AG.AGR.TRAC.NO',\n    'ag machinery per 100 sq km':'AG.LND.TRAC.ZS',\n    'ag value added per worker':'EA.PRD.AGRI.KD',\n    'cereal production, tons':'AG.PRD.CREL.MT',\n    'cereal yield, kg per hectare':'AG.YLD.CREL.KG',\n    'fertilizer consumption, % (quantity used per unit of arable land)':'AG.CON.FERT.PT.ZS',\n    'fertilizer consumption, kg per hectare of arable land':'AG.CON.FERT.ZS',\n    'land under cereal production, hectare':'AG.LND.CREL.HA',\n    'ag irrigated land, %':'AG.LND.IRIG.AG.ZS',\n    'ag total land, %':'AG.LND.AGRI.ZS',\n    'ag total land, sq km':'AG.LND.AGRI.K2',\n    'arable land, %':'AG.LND.ARBL.ZS',\n    'arable land, hectares':'AG.LND.ARBL.HA',\n    'average precipitation, mm per year':'AG.LND.PRCP.MM',\n    'land area, sq km':'AG.LND.TOTL.K2',\n    'permanent cropland, %':'AG.LND.CROP.ZS',\n    'food production index':'AG.PRD.FOOD.XD',\n    'value added agriculture':'NV.AGR.TOTL.KD'\n}             ", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "3c902aa8-f33a-4e67-946c-a333bcee00d3", "_execution_state": "idle", "_uuid": "773d03ec1b885b8585b070d5520eb140d58cc97b", "trusted": false}, "source": "# Iterates through the dictionary listed above -- for reference purposes originally -- \n# and reads the designated information from the SQL database into a new dictionary\n# that follows the same style.\nagDF = {}\nfor key, value in ag_indicators.items():\n    statement = '''SELECT CountryCode, Year, Value FROM Indicators WHERE IndicatorCode IS \"{}\"''' \\\n                .format(value)\n    agDF[key] = (pd.read_sql(statement, con = conn))", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "7355f849-c115-4004-95e3-65e77bf8aef6", "_execution_state": "idle", "_uuid": "8e47f0299c067877b746995e87d48ae90ceabaec", "trusted": false}, "source": "# A method for cleaning and merging the dataframes established in the previous section.\n#\n# Using the optional argument *data allows us to incorporate a number of data sets > 1\n# that we can then iterate through with a simple for loop.\n#\n# The method here first changes the value to a separate name dX where X is the number\n# of data frames we're adding. Hence starting at n = 0 for our first data set and then\n# iterating from there in the first if-then statement, and iterating further in the \n# first for loop, which does the exact same thing as the first if-then statement but\n# is done in a new for loop. A better system would likely be to conduct this via \n# recursion, which may be done later on. \ndef clean(df1, *data):\n    n = 0\n    df = df1.copy()\n    if 'Value' in df1.columns.values:\n        df.rename(columns={'Value':'d{}'.format(n)}, inplace=True)\n        n += 1\n    for f in data:\n        frame = f.copy()\n        if 'Value' in frame.columns.values:\n            frame.rename(columns={'Value':'d{}'.format(n)}, inplace=True)\n            n += 1\n        df = pd.merge(df, frame, how='left', on=['CountryCode','Year'])\n    # Necessary to remove the double-counting regions noted in the first section\n    for name in regions:\n        df.drop(df.loc[df.CountryCode == name].index, inplace=True)\n    # Removes any 0 values that raise an error when doing logarithms\n    for col in df.columns.values:\n        df.drop(df.loc[df[col] == 0].index, inplace=True)\n    return df", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "a035e145-feb9-440c-9e0a-e87793f58779", "_execution_state": "idle", "_uuid": "4e0fb8f2627872d5c26e398a2de55078e89633e1"}, "source": "# 1: Cereal Production #\nFor our first set of tests and visualizations we'll work with absolute values only. Doing so instead of working with more-complex values such as cereal yield, fertilizer consumption per hectare, tractors per sq km, etc. we'll be easier to work with off the bat.\n\nSince we're interested in cereal production, we'll start with our values for land as well as tractors since that is in an absolute number.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "d511865b-0ac1-459a-895b-b244a5b7ef44", "_execution_state": "idle", "_uuid": "79808f69d6fde2d4c80bb6898ae53c0723874e01", "trusted": false}, "source": "fertilizer = clean(agDF['fertilizer consumption, kg per hectare of arable land']\n                  ,agDF['arable land, hectares'])\nfertilizer['Value'] = fertilizer.d0 * fertilizer.d1\nfertilizer.drop(['d0','d1'],axis=1,inplace=True)\n\ndf = clean(\n    agDF['cereal production, tons'],\n    agDF['ag machinery, tractors'],\n    fertilizer,\n    agDF['average precipitation, mm per year'],\n    agDF['land under cereal production, hectare'],\n    agDF['ag total land, sq km'],\n    agDF['arable land, hectares'],\n    agDF['land area, sq km']\n)\ndf = df.drop(['CountryCode','Year'], axis=1)\ndf = np.log(df)\ndf.info()", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "e7a6ee35-33a4-4645-8d17-c18e958aa31b", "_execution_state": "idle", "_uuid": "30020893d05673c8ae675ab20cc0d013a5c9bdb4", "trusted": false}, "source": "corr_mean = df.corr()\nplt.figure(figsize=(14, 14))\nsns.heatmap(corr_mean, cbar=True, square=True, annot=True, fmt='.2f', cmap='PiYG')", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "34cf9c88-56ba-47a7-9a9f-a1aef6cee9b0", "_execution_state": "idle", "_uuid": "79a9270647350eddb0dc82bda0423ee8d36ee0e9"}, "source": "This correlation heat map is not a good visualization. Because of the wide range in correlation numbers, it's hard to discern what actually correlates well with other things. This is, primarily, because we included the values for average precipitation per year. If we remove that section it should be easier to discern the correlation numbers.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "171dff02-64d6-4504-a365-747a98258067", "_execution_state": "idle", "_uuid": "41487c6f57e36e0960638797aa76b47c0624bc1f", "trusted": false}, "source": "features = ['d0','d1','d2','d4','d5','d6','d7']\ncorr_mean = df[features].corr()\nplt.figure(figsize=(14, 14))\nsns.heatmap(corr_mean, cbar=True, square=True, annot=True, fmt='.2f', cmap='PiYG')", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "64cd6eeb-991f-42d7-96be-ac5cd14f1967", "_execution_state": "idle", "_uuid": "730eafc8443bca30e60d236196e0f4e0255c15f5"}, "source": "## 1.2 ##\nLand under cereal production in hectares (d4 in our set), has one hell of a correlation coefficient with cereal production. Since we know that the calculations for total cereal production don't contain land elements (which cereal yield would since it's cereal production per hectare) we can use that feature for our linear regression. \n\nAgricultural land in square kilometers also has a strong correlation with cereal production. However, it has a stronger correlation with land under cereal production. We know from the long definitions of the various series that total agriculture land incorporates land under cereal production, we can use either one. Since the latter has a better correlation with cereal production, we'll use that instead. The same method can be applied to arable land: it incorporates not only agriculture land, but also land under cereal production, so we can eliminate it as well. Finally we have total land area: unsurprisingly, this has the worst correlation among all the land values with cereal production since it is the final value of land, whereas all the others are subsets of it. We can eliminate it as well, leaving us with d0, d1, d2, and d4 -- cereal production in tons, total number of tractors in use, fertilizer, and land under cereal production in hectares. First, however, can we visualize this information in a simple manner? We'll need to calculate our land under cereal production for square kilometers, but that isn't difficult.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "7102f3f7-5695-47a1-b015-2a49cc4c419e", "_execution_state": "idle", "_uuid": "42a97d260fadc8384388d8765164a46d0966e5cc", "trusted": false}, "source": "# Here we change the two variables that are in hectares to sq km by dividing by 100 for each\n#\ncereal_land = agDF['land under cereal production, hectare'].copy()\ncereal_land['Value'] = cereal_land['Value'].map(lambda x: x / 100)\narable_land = agDF['arable land, hectares'].copy()\narable_land['Value'] = arable_land['Value'].map(lambda x: x / 100)\nland_df = clean(\n    cereal_land,\n    agDF['ag total land, sq km'],\n    arable_land,\n    agDF['land area, sq km']\n).dropna(axis=0, how='any')\n\nusa_land = land_df.loc[land_df.CountryCode == 'USA']\nfig = plt.figure()\nplt.plot(usa_land.Year,usa_land.d3,label='Total Land',color=tableau20[6])\nplt.plot(usa_land.Year,usa_land.d1,label='Agricultural Land',color=tableau20[0])\nplt.plot(usa_land.Year,usa_land.d2,label='Arable Land',color=tableau20[3])\nplt.plot(usa_land.Year,usa_land.d0,label='Land under Cereal Production',color=tableau20[4])\n\nfill_between(usa_land.Year,usa_land.d3,0,alpha=0.5,color=tableau20[6])\nfill_between(usa_land.Year,usa_land.d1,0,alpha=0.5,color=tableau20[0])\nfill_between(usa_land.Year,usa_land.d2,0,alpha=0.5,color=tableau20[3])\nfill_between(usa_land.Year,usa_land.d0,0,alpha=0.5,color=tableau20[4])\n\nplt.legend(bbox_to_anchor=(1.05,1),loc=2,borderaxespad=0.)\nplt.xlabel('Years', fontsize=14)", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "7f82d61d-ca85-4c68-9de3-afa76de0662a", "_execution_state": "idle", "_uuid": "e01269d53277cc944d3eb4499cdf574e8722d545"}, "source": "We can see from this visualization of the United States' land that each descending set of land values contains the next one: land under cereal production is a subset of arable land, arable land a subset of agricultural land, and agricultural land a subset of total land area. Armed with this knowledge, we can reasonably ignore using any of the other values for calculating cereal production since Land under Cereal Production (d1) contains such a strong correlation to total cereal production in tons.\n\nCereal production in tons is the value we're attempting to determine the value of, so it will be our dependent variable while d1 and d2 will be our independent features. We list those in our features list in order to subset more easily. Then we make a clean data frame that combines only those values and drops any NaN values from the list. We then drop the CountryCode and Year columns -- which are used as indices in our cleaner method, and normalize via logarithm the remaining values.\n\nThen we split the set using the method from sklearn. We then divide our training and test sets into training xs, training ys, test xs and test ys, and use them for a multiple linear regression. We run this 100 times and take the average of our scores, coefficients, and intercepts.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "1a0dcd40-8e64-44a4-9860-ead2d840c070", "_execution_state": "idle", "_uuid": "3c21fbe15b766028144d07c71945b83c94fc9468", "trusted": false}, "source": "features = ['d1','d2','d3']\ndf = clean(\n    agDF['cereal production, tons']\n    ,agDF['ag machinery, tractors']\n    ,fertilizer\n    ,agDF['land under cereal production, hectare']\n).dropna(axis=0, how='any')\ndf.drop(['CountryCode','Year'], axis=1, inplace=True)\ndf = np.log(df)\n\ncoef1, coef2, coef3, intercept, score = [],[],[],[],[]\nfor x in range(1000):\n    train, test = train_test_split(df, test_size=0.2)\n    train_x = train[features]\n    train_y = train.d0\n    test_x = test[features]\n    test_y = test.d0\n\n    regr = LinearRegression()\n    regr.fit(train_x, train_y)\n    pred = regr.predict(test_x)\n    coef1.append(regr.coef_[0])\n    coef2.append(regr.coef_[1])\n    coef3.append(regr.coef_[2])\n    intercept.append(regr.intercept_)\n    score.append(regr.score(test_x, test_y))\n\ncoeff_df = pd.DataFrame({'Features':['Tractors','Fertilizer','Land']})\ncoeff_df['Coefficient'] = pd.Series([np.mean(coef1),np.mean(coef2),np.mean(coef3)])\ncoeff_df['Intercept'] = np.mean(intercept)\ncoeff_df['R-Squared'] = np.mean(score)\ncoeff_df", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "f58028c3-0681-4893-a3fb-35621bcbbdc3", "_execution_state": "idle", "_uuid": "801e5ca1bd8ca4021d5e18345f55b07528959ae9"}, "source": "## 1.3 ##\nAn R-Squared of 0.952 isn't bad at all. This gives us a good grounding in testing values. Having eliminated a number of variables that were merely subsets all containing land under cereal production, we brought the number of features to use down to 3. In addition to land, we used the total number of tractors as well as the total amount of fertilizer in kilograms. \n\nWe can relate this model as:\n\nLN(y) = 0.078 LN(x0) + 0.214 LN (x1) + 0.754 LN (x2) - 0.383\n\nWhere y is cereal production in tons, x0 is the total number of tractors used, and x1 is total amount of fertilizer used, and x2 is the number of hectares of land under cereal production. Now we'll move on to other tests that require more calculations between data frames. \n\nThis of course has some theoretical limitations: not all fertilizer from the data set was likely used purely on cereal production, as it could be used in other types of agricultural production not found here, and the same can be said of the number of tractors used, since it's highly unlikely that was used purely on cereal production given how much more land is used for agricultural production that is not cereal production. \n\nLet's check to see how many values we're working with here, and if simplifying that could increase our data points.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "f6978fba-d8bc-4337-983e-2cf76b5f6b95", "_execution_state": "idle", "_uuid": "56b46fb0469a43c9ab74ab97f3e2efe18b3cd8da", "trusted": false}, "source": "df = clean(\n    agDF['cereal production, tons']\n    ,agDF['ag machinery, tractors']\n    ,fertilizer\n    ,agDF['land under cereal production, hectare']\n).dropna(axis=0, how='any')\ndf.info()", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "987fad7b-0194-4d90-ad6a-5757bb1cf3ec", "_execution_state": "idle", "_uuid": "f21457c0f5d81e2668aedea35cb6d5290c6dc615"}, "source": "302 data points is not a lot, to say the least. Since we saw that at most we could have nearly 8000 data points, what if we simplify the model to account for the theoretical limitations noted earlier.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "0130f3c6-a544-4642-aa82-2498742417a7", "_execution_state": "idle", "_uuid": "62ade5cd2e465ba163fdbd631519a2af40bf8695", "trusted": false}, "source": "df = clean(agDF['cereal production, tons']\n           ,agDF['ag machinery, tractors']\n           ,agDF['land under cereal production, hectare']\n).dropna(axis=0, how='any')\ndf.drop(['CountryCode','Year'],axis=1,inplace=True)\ndf = np.log(df)\ndf.info()", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "2fe0ead1-6ba1-4677-a655-4dee9b233a90", "_execution_state": "idle", "_uuid": "e36cfa7a56fd69c18b42857ebf2bc008d777312b"}, "source": "5000 data points is way better than 302, just from eliminating fertilizer from our inputs. If we run a linear regression off this, let's see what our R-squared is.\n\nAt the same time, let's change that multiple linear regression algorithm used earlier into a method to more easily replicate it throughout this code.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "44c832ca-5ff1-4e38-a19c-6090ca9c496a", "_execution_state": "idle", "_uuid": "c1a55c91936972010a721caad9693ff11a51bf84", "trusted": false}, "source": "features = ['d1','d2']\ndef multiple_regression(data, features):\n    coeff, intercept, score, mse = [],[],[],[]\n    for x in range(len(features)):\n        coeff.append([])\n    for x in range(1000):\n        train, test = train_test_split(df, test_size=0.2)\n        train_x = train[features]\n        train_y = train.d0\n        test_x = test[features]\n        test_y = test.d0\n        \n        regr = LinearRegression()\n        regr.fit(train_x, train_y)\n        pred = regr.predict(test_x)\n        for x in range(len(features)):\n            coeff[x].append(regr.coef_[x])\n        intercept.append(regr.intercept_)\n        score.append(regr.score(test_x,test_y))\n        mse.append(metrics.mean_squared_error(test_y, pred))\n\n    print(\"Coefficients: {0} \\n\"\n          \"Irreducible:  {1} \\n\"\n          \"R-Squared:    {2} \\n\"\n          \"Mean-Squared: {3}\".format(\n              np.array([np.mean(coeff[x]) for x in range(len(features))]),\n              np.mean(intercept),np.mean(score),np.mean(mse)))\n\nmultiple_regression(df, features)", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "026ee505-4cc3-454d-9ea5-d2f4740fc27f", "_execution_state": "idle", "_uuid": "96e8d4bdd32cb2b33340a7aab52af360d3496d55"}, "source": "Looking at that, simplifying our model for theoretical concerns actually improved our R-squared value by almost 2 percentage points, from 0.951 to 0.969. This means that even if we attempted to over-fit the model with extra variables -- not necessarily land -- that it wouldn't even give us a better score. That changes our model to the following:\n\nLN( y ) = 0.179 * LN ( x0 ) + 0.891 * LN ( x1 ) + 0.372,\n\nWhere y is our cereal production in tons, x0 is tractors, and x1 is land in hectares.\n\nLet's see if simplifying it further gives us a better R-Squared.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "0477d1dc-d64f-4f13-9841-79c90082b897", "_execution_state": "idle", "_uuid": "5a705175469d989e71106eecd1697da9f699b294", "trusted": false}, "source": "df = clean(agDF['cereal production, tons'],agDF['land under cereal production, hectare']) \\\n     .dropna(axis=0,how='any')\ndf.drop(['CountryCode','Year'],axis=1,inplace=True)\ndf = np.log(df)\nfeatures = ['d1']\nmultiple_regression(df, features)", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "57f75dda-a84c-4dbb-83f1-a2de7d8acf65", "_execution_state": "idle", "_uuid": "88f3aa4057bef75b870a5daf6a490511aa9f6a41"}, "source": "This final model doesn't give us a better R-squared, not too surprisingly. It may, however, be more accurate given the theoretical problems with the model mentioned earlier; specifically that the total number of tractors in each country in a given year are NOT always being used entirely for cereal production, as they might be used in other agricultural production. The same problem occurs with fertilizer consumption.\n\nEach model does give us a good grounding in how to do multiple linear regression and how to determine good predictor variables from those that may end up being noise in our model. ", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "4f24c88dd95cccc816fdde754803a1dad729d397"}, "source": "# 2: Clustering #", "execution_count": null, "cell_type": "markdown", "outputs": []}]}