{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1527)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nimport gc, sys\ngc.enable()\n\n# Any results you write to the current directory are saved as output.\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a18a9cbe43e84d3eb25fa37999ed1c553e09a2a"},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ndef preProcess(is_train=True,debug=True):\n    test_idx = None\n    if is_train: \n        print(\"processing train.csv\")\n        if debug == True:\n            df = pd.read_csv('../input/train_V2.csv', nrows=10000)\n        else:\n            df = pd.read_csv('../input/train_V2.csv')           \n\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        df = pd.read_csv('../input/test_V2.csv')\n        test_idx = df.Id\n        \n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    df['kills_assists'] = (df['kills'] + df['assists'])    \n    df['killsWithoutMoving'] = ((df['kills'] > 0) & (df['totalDistance'] == 0))\n    df['healthitems'] = df['heals'] + df['boosts']\n    \n    df = reduce_mem_usage(df)\n\n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    \n    features.remove(\"matchType\")\n        \n    y = None\n    \n    \n    if is_train: \n        print(\"get target\")\n        y = np.array(df[target])\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df[['matchId','groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n        \n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n\n    \n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n        \n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    df_out= reduce_mem_usage(df_out)\n    print(\"get match Type feature\")\n    '''\n    agg=df[['matchId','matchType']]\n    agg = agg.drop_duplicates()\n    agg['matchType'] = agg['matchType'].astype('category')\n    agg['matchType'] = agg['matchType'].cat.codes\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    '''\n\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    X = df_out\n    \n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y, feature_names, test_idx\n\nx_train, y_train, train_columns, _ = preProcess(True,False)\nx_test, _, _ , test_idx = preProcess(False,True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ed1edea93684e2ad7f98f0829d06d9de85da41"},"cell_type":"code","source":"# Split the train and the validation set for the fitting\nrandom_seed=1\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=random_seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1875bc59cc56fd60d696262c62fde5aeac35d21b"},"cell_type":"code","source":"# Random Forest Model definition\nmodel = RandomForestRegressor(n_estimators=20, min_samples_leaf=5, max_features=0.5,\n                          n_jobs=-1,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25669011682155e86c0addda7a8121e63172e3ce"},"cell_type":"code","source":"%%time\nmodel.fit(x_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07fae7ae48f6239259b18c39c20c72fa5828f7fc"},"cell_type":"code","source":"# train set and validation set predictions\ny_train_pred = model.predict(x_train)\ny_val_pred = model.predict(x_val)\nprint('mae train: ', mean_absolute_error(y_train_pred, y_train))\nprint('mae validation: ', mean_absolute_error(y_val_pred, y_val))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e164e5ed395dccd12bfea6bdb7aad6c596e8601"},"cell_type":"code","source":"# Actual vs Prediction distribution histogram\nplt.subplot(2, 1, 1)\nplt.hist(y_val_pred)\n\nplt.subplot(2, 1, 2)\nplt.hist(y_val)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e516da002fe7a7e22a7c73fc41e81e49a0dabb39"},"cell_type":"code","source":"df_test = pd.read_csv('../input/' + 'test_V2.csv')\n\npred = model.predict(x_test)\n\ndf_test['winPlacePerc'] = pred\n\nsubmission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission_rf_feat1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"944cfca2d92bdadb2714bc459ba57b076b99f9e7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08365824d27300f70154e0a48b58d16581eb9fe0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}