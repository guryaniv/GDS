{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "63de4277-c079-1630-2e47-44d138f3e7bf"
      },
      "source": [
        "# Library Import #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "193a91a4-7404-a01b-2c78-6b7385979b9c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import six\n",
        "from abc import ABCMeta\n",
        "from scipy import sparse\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils import check_X_y, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f98318d0-c122-24a1-a9a6-62ab93985428"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c39b53ac-2080-bfb4-124d-5e775af2cec2"
      },
      "source": [
        "we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28217d5a-e580-fa92-fdc2-45a4842812e0"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('../input/Combined_News_DJIA.csv')\n",
        "train = data[data['Date'] < '2015-01-01']\n",
        "test = data[data['Date'] > '2014-12-31']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3d372c26-3c4c-7890-3089-c8ddb17ca81a"
      },
      "source": [
        "# Data Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b43212e0-9344-a216-b86b-37cfbfb37752"
      },
      "source": [
        "First, we transform the string of news into the  number of words as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a87f0d3d-80f3-af90-9368-dedde8c1154f"
      },
      "outputs": [],
      "source": [
        "trainheadlines = []\n",
        "for row in range(0,len(train.index)):\n",
        "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66913298-12f3-da89-030a-90ba38b1e11b"
      },
      "outputs": [],
      "source": [
        "basicvectorizer = CountVectorizer()\n",
        "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
        "print(basictrain.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"
      },
      "source": [
        "## Logic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ee566614-2f26-df99-3c54-700a07cf83bd"
      },
      "source": [
        "### Logic Regression 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"
      },
      "source": [
        "Algorithm: Logic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"
      },
      "source": [
        "Input: the counts of single words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5adfb2da-7467-505b-d44b-0a028e60227b"
      },
      "outputs": [],
      "source": [
        "basicmodel = LogisticRegression()\n",
        "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "56d1ea73-641b-6fdf-698f-32518fe99dec"
      },
      "outputs": [],
      "source": [
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "basictest = basicvectorizer.transform(testheadlines)\n",
        "preds1 = basicmodel.predict(basictest)\n",
        "acc1=accuracy_score(test['Label'], preds1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc13f327-70bd-2f8f-4ab7-6a9dfae356e9"
      },
      "outputs": [],
      "source": [
        "print('Logic Regression 1 accuracy: ',acc1 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c08a6f88-d723-97b5-dd09-a61b1f3f7829"
      },
      "source": [
        "The accuracy is only 0.42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2303559d-4ad8-7d30-ff3d-ee3bb91440ae"
      },
      "outputs": [],
      "source": [
        "basicwords = basicvectorizer.get_feature_names()\n",
        "basiccoeffs = basicmodel.coef_.tolist()[0]\n",
        "coeffdf = pd.DataFrame({'Word' : basicwords, \n",
        "                        'Coefficient' : basiccoeffs})\n",
        "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "coeffdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "949e33a5-f63d-0059-2ee0-17e625aede36"
      },
      "outputs": [],
      "source": [
        "coeffdf.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6aa2253b-0cf6-4853-4ff5-6143dae71fb1"
      },
      "source": [
        "### Logic Regression 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "65239831-43f2-290b-8e21-b4a9f82e5a54"
      },
      "source": [
        "Algorithm: Logic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a108baea-371d-d93f-7d65-213ecd4337ec"
      },
      "source": [
        "Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15df5336-f6f9-cd66-b020-6072aff3083b"
      },
      "source": [
        "We delete phrases of which frequency lower than 0.03 or higher than 0.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dbdf7b4f-4673-49f1-d510-377026c95611"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6bf747c-021c-eb7f-cd50-2240138ed555"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c225d22-d997-5db8-877e-03022a0785b4"
      },
      "outputs": [],
      "source": [
        "advancedmodel = LogisticRegression()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b3421b9-b326-4ef5-4235-2e764e5cd3a3"
      },
      "outputs": [],
      "source": [
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds2 = advancedmodel.predict(advancedtest)\n",
        "acc2=accuracy_score(test['Label'], preds2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "422fb025-f985-e010-3e9e-2ee8c4494c3e"
      },
      "outputs": [],
      "source": [
        "print('Logic Regression 2 accuracy: ', acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f5a4ca4-75a0-6a24-2c34-61855d2b102a"
      },
      "source": [
        "The accuracy is higher than input of single words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3af2a0fb-a8ef-84f2-2a80-3568d327baec"
      },
      "outputs": [],
      "source": [
        "advwords = advancedvectorizer.get_feature_names()\n",
        "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
        "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
        "                        'Coefficient' : advcoeffs})\n",
        "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
        "advcoeffdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f"
      },
      "outputs": [],
      "source": [
        "advcoeffdf.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39b0d7a4-cedc-d51c-11f4-d3a1d454682f"
      },
      "source": [
        "### Logic Regression 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4cf79ce1-930d-474a-f07b-dd397f9a72eb"
      },
      "source": [
        "Algorithm: Logic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a62cefba-3f14-ce21-49a5-1fdd09556dde"
      },
      "source": [
        "Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "886f33fe-d47f-d545-1b73-29f1e3618e01"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5437284a-b14e-96cd-e6d5-97c41790d42a"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e29bfca-0041-5b86-fdff-9c502f4aa1ad"
      },
      "outputs": [],
      "source": [
        "advancedmodel = LogisticRegression()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba"
      },
      "outputs": [],
      "source": [
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds3 = advancedmodel.predict(advancedtest)\n",
        "acc3 = accuracy_score(test['Label'], preds3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b700896c-936b-004d-2d70-50fefee2129c"
      },
      "outputs": [],
      "source": [
        "print('Logic Regression 3 accuracy: ', acc3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a438254-8acb-4db7-4f38-bb07be962dc2"
      },
      "source": [
        "The accuracy is lower than input of phrases with two connected words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66fee024-03b4-5964-f925-cfb3746a4741"
      },
      "outputs": [],
      "source": [
        "advwords = advancedvectorizer.get_feature_names()\n",
        "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
        "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
        "                        'Coefficient' : advcoeffs})\n",
        "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
        "advcoeffdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c50f97d8-a6f7-0a60-9edf-42bb8b7aa492"
      },
      "outputs": [],
      "source": [
        "advcoeffdf.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5ccc2e71-28c6-f425-f229-f1395293bffc"
      },
      "source": [
        "### NBayes 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "476ba54c-4b47-6029-e0de-29e6fcb64845"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4726d110-1821-ea71-dc5c-7a21ad628dd1"
      },
      "outputs": [],
      "source": [
        "advancedmodel = MultinomialNB(alpha=0.01)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds4 = advancedmodel.predict(advancedtest)\n",
        "acc4=accuracy_score(test['Label'], preds4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff1f469d-1c12-5a3d-1aed-148179c26824"
      },
      "outputs": [],
      "source": [
        "print('NBayes 1 accuracy: ', acc4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02a9958e-ab18-bf74-58f1-160c5b47cbff"
      },
      "outputs": [],
      "source": [
        "advwords = advancedvectorizer.get_feature_names()\n",
        "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
        "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
        "                        'Coefficient' : advcoeffs})\n",
        "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
        "advcoeffdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6223b466-48eb-0a0a-5d36-c082e50b1fe5"
      },
      "outputs": [],
      "source": [
        "advcoeffdf.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dcc814d2-014d-5cb9-b106-52ff648d9c18"
      },
      "source": [
        "### NBayes 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfe60202-6348-d724-c860-c3b46c66c447"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28b21c32-28de-1bc4-161e-dcf63737caca"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca"
      },
      "outputs": [],
      "source": [
        "advancedmodel = MultinomialNB(alpha=0.0001)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds5 = advancedmodel.predict(advancedtest)\n",
        "acc5 = accuracy_score(test['Label'], preds5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa7d48c3-60cc-0895-7ed3-ddbf796339ff"
      },
      "outputs": [],
      "source": [
        "print('NBayes 2 accuracy: ', acc5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f49cb3c4-b80b-48a8-69be-ac6c579e2ed4"
      },
      "outputs": [],
      "source": [
        "advwords = advancedvectorizer.get_feature_names()\n",
        "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
        "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
        "                        'Coefficient' : advcoeffs})\n",
        "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
        "advcoeffdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "087f9b18-c50b-17b5-60b6-2cf2dada7c3c"
      },
      "outputs": [],
      "source": [
        "advcoeffdf.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a77b2afd-5654-aaff-74a9-0013fe4e8164"
      },
      "source": [
        "### RF 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "91fe0b03-37a0-eb19-2632-cdc04067b12f"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "878756f6-f57d-88cb-fe82-5181c1e40901"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b561af90-db7d-ef58-26a2-5721d5432615"
      },
      "outputs": [],
      "source": [
        "advancedmodel = RandomForestClassifier()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds6 = advancedmodel.predict(advancedtest)\n",
        "acc6 = accuracy_score(test['Label'], preds6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "91fe3c5f-b005-97f0-da7c-597c29521877"
      },
      "outputs": [],
      "source": [
        "print('RF 1 accuracy: ', acc6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "76a971f3-878d-33e2-249a-a2b063aa5460"
      },
      "source": [
        "### RF 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1db5461b-1ee9-2196-6725-66a6c2376a19"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1fb8019-c981-9d18-8f54-2750387ec071"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bcddf034-4a10-2026-55ea-dd3d969a297b"
      },
      "outputs": [],
      "source": [
        "advancedmodel = RandomForestClassifier()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds7 = advancedmodel.predict(advancedtest)\n",
        "acc7 = accuracy_score(test['Label'], preds7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e395e3a6-7bc8-a434-16af-be13aebbabbd"
      },
      "outputs": [],
      "source": [
        "print('RF 2 accuracy: ', acc7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0a12329a-12c7-aa90-a138-025087dd76ba"
      },
      "source": [
        "## Gradient Boosting Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ea1681ab-166f-20a6-3c29-f34f84438a36"
      },
      "source": [
        "### GBM 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ec11f0e-6c50-8149-93c3-acd17d1e7caa"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c5d312d-555f-f353-606e-2f560631033a"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd"
      },
      "outputs": [],
      "source": [
        "advancedmodel = GradientBoostingClassifier()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
        "acc8 = accuracy_score(test['Label'], preds8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba8ae675-7e71-e24e-69bf-99d5d19590d8"
      },
      "outputs": [],
      "source": [
        "print('GBM 1 accuracy: ', acc8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e39653db-00b9-d1e0-d82e-87105e0eba1e"
      },
      "source": [
        "### GBM 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d35d6a94-69f4-5370-6d0f-5b98c91d2565"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b83bc122-5059-665a-28c1-85145787155e"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e98ca989-15e6-b785-f282-4e59f849cd1f"
      },
      "outputs": [],
      "source": [
        "advancedmodel = GradientBoostingClassifier()\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds9 = advancedmodel.predict(advancedtest.toarray())\n",
        "acc9 = accuracy_score(test['Label'], preds9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6df5f34f-a315-c1c4-fdbe-c54127468d5e"
      },
      "outputs": [],
      "source": [
        "print('GBM 2 accuracy: ', acc9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b7bd5500-e077-8225-31b5-a9c3f1015ca5"
      },
      "source": [
        "## Stochastic Gradient Descent Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f139d81-25c5-ba11-8924-e68b6e2e55f6"
      },
      "source": [
        "### SGDClassifier 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d071bb2f-df6a-1657-1af8-5d31ee8e4a19"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24d0156a-2705-fece-001e-0426da57d383"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30e4b846-af7e-1eda-dade-f1550d6e043b"
      },
      "outputs": [],
      "source": [
        "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
        "acc10 = accuracy_score(test['Label'], preds10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79a91d72-1cd5-4308-9b0e-9810314cdd1a"
      },
      "outputs": [],
      "source": [
        "print('SGDClassifier 1: ', acc10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0f46cdfe-20b2-9bc6-103a-1f44e2172789"
      },
      "source": [
        "### SGDClassifier 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c012b95d-8575-ef34-d699-d240ade631f0"
      },
      "outputs": [],
      "source": [
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e4f2994-e199-976d-f54b-0e676b6b11be"
      },
      "outputs": [],
      "source": [
        "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds11 = advancedmodel.predict(advancedtest.toarray())\n",
        "acc11 = accuracy_score(test['Label'], preds11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9d8499e-ae6a-6197-c86f-c31bf73f3f98"
      },
      "outputs": [],
      "source": [
        "print('SGDClassifier 2: ', acc11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d6aa0178-e7f3-6427-f88f-aa6e436fc87d"
      },
      "source": [
        "## Naive Bayes SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd1391ca-1b70-13c0-3a62-151fc84f6069"
      },
      "outputs": [],
      "source": [
        "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
        "\n",
        "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.C = C\n",
        "        self.svm_ = [] # fuggly\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = check_X_y(X, y, 'csr')\n",
        "        _, n_features = X.shape\n",
        "\n",
        "        labelbin = LabelBinarizer()\n",
        "        Y = labelbin.fit_transform(y)\n",
        "        self.classes_ = labelbin.classes_\n",
        "        if Y.shape[1] == 1:\n",
        "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
        "\n",
        "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
        "        # so we don't have to cast X to floating point\n",
        "        Y = Y.astype(np.float64)\n",
        "\n",
        "        # Count raw events from data\n",
        "        n_effective_classes = Y.shape[1]\n",
        "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
        "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
        "                                 dtype=np.float64)\n",
        "        self._compute_ratios(X, Y)\n",
        "\n",
        "        # flugglyness\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
        "            Y_i = Y[:,i]\n",
        "            svm.fit(X_i, Y_i)\n",
        "            self.svm_.append(svm) \n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_effective_classes = self.class_count_.shape[0]\n",
        "        n_examples = X.shape[0]\n",
        "\n",
        "        D = np.zeros((n_effective_classes, n_examples))\n",
        "\n",
        "        for i in range(n_effective_classes):\n",
        "            X_i = X.multiply(self.ratios_[i])\n",
        "            D[i] = self.svm_[i].decision_function(X_i)\n",
        "        \n",
        "        return self.classes_[np.argmax(D, axis=0)]\n",
        "        \n",
        "    def _compute_ratios(self, X, Y):\n",
        "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
        "        if np.any((X.data if issparse(X) else X) < 0):\n",
        "            raise ValueError(\"Input X must be non-negative\")\n",
        "\n",
        "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
        "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
        "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
        "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
        "        check_array(self.ratios_)\n",
        "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
        "\n",
        "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
        "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
        "\n",
        "\n",
        "def f1_class(pred, truth, class_val):\n",
        "    n = len(truth)\n",
        "\n",
        "    truth_class = 0\n",
        "    pred_class = 0\n",
        "    tp = 0\n",
        "\n",
        "    for ii in range(0, n):\n",
        "        if truth[ii] == class_val:\n",
        "            truth_class += 1\n",
        "            if truth[ii] == pred[ii]:\n",
        "                tp += 1\n",
        "                pred_class += 1\n",
        "                continue;\n",
        "        if pred[ii] == class_val:\n",
        "            pred_class += 1\n",
        "\n",
        "    precision = tp / float(pred_class)\n",
        "    recall = tp / float(truth_class)\n",
        "\n",
        "    return (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
        "\n",
        "    f1_pos = f1_class(pred, truth, pos)\n",
        "    f1_neg = f1_class(pred, truth, neg)\n",
        "\n",
        "    return (f1_pos + f1_neg) / 2.0;\n",
        "\n",
        "\n",
        "def main(train_file, test_file, ngram=(1, 3)):\n",
        "    print('loading...')\n",
        "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    # to shuffle:\n",
        "    #train.iloc[np.random.permutation(len(df))]\n",
        "\n",
        "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
        "                        names=['text', 'label'])\n",
        "\n",
        "    print('vectorizing...')\n",
        "    vect = CountVectorizer()\n",
        "    classifier = NBSVM()\n",
        "\n",
        "    # create pipeline\n",
        "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
        "    params = {\n",
        "        'vect__token_pattern': r\"\\S+\",\n",
        "        'vect__ngram_range': ngram, \n",
        "        'vect__binary': True\n",
        "    }\n",
        "    clf.set_params(**params)\n",
        "\n",
        "    #X_train = vect.fit_transform(train['text'])\n",
        "    #X_test = vect.transform(test['text'])\n",
        "\n",
        "    print('fitting...')\n",
        "    clf.fit(train['text'], train['label'])\n",
        "\n",
        "    print('classifying...')\n",
        "    pred = clf.predict(test['text'])\n",
        "   \n",
        "    print('testing...')\n",
        "    acc = accuracy_score(test['label'], pred)\n",
        "    f1 = semeval_senti_f1(pred, test['label'])\n",
        "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"
      },
      "source": [
        "### NBSVM 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e054e5ea-c8be-cb98-90a1-2512b3478b6e"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b57a2b30-d69f-cf0a-05d0-8a065d5a758b"
      },
      "outputs": [],
      "source": [
        "advancedmodel = NBSVM(C=0.01)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds12 = advancedmodel.predict(advancedtest)\n",
        "acc12 = accuracy_score(test['Label'], preds12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6acdade4-665e-5c06-95af-3e5ddcfd33d6"
      },
      "outputs": [],
      "source": [
        "print('NBSVM 1: ', acc12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aca410f2-118d-f973-6b14-a63048444f26"
      },
      "source": [
        "### NBSVM 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "663e5e98-7ccd-29a4-40a8-10a46327c06b"
      },
      "outputs": [],
      "source": [
        "advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c67fed4-8110-2a52-bb33-1da6f294b592"
      },
      "outputs": [],
      "source": [
        "advancedmodel = NBSVM(C=0.01)\n",
        "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "preds13 = advancedmodel.predict(advancedtest)\n",
        "acc13 = accuracy_score(test['Label'], preds13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "803861e3-d740-c2ed-2b2e-57221ba9b8c8"
      },
      "outputs": [],
      "source": [
        "print('NBSVM 2: ', acc13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "682e897f-169c-8694-d05d-66e9fc844158"
      },
      "source": [
        "## Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1de5a061-52d5-2578-9cb0-2602499ad87b"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac32628f-8080-5d53-e3a2-3d6986431f15"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
        "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
        "testheadlines = []\n",
        "for row in range(0,len(test.index)):\n",
        "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "advancedtest = advancedvectorizer.transform(testheadlines)\n",
        "print(advancedtrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
      },
      "outputs": [],
      "source": [
        "X_train = advancedtrain.toarray()\n",
        "X_test = advancedtest.toarray()\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "y_train = np.array(train[\"Label\"])\n",
        "y_test = np.array(test[\"Label\"])\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "# pre-processing: divide by max and substract mean\n",
        "scale = np.max(X_train)\n",
        "X_train /= scale\n",
        "X_test /= scale\n",
        "\n",
        "mean = np.mean(X_train)\n",
        "X_train -= mean\n",
        "X_test -= mean\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Here's a Deep Dumb MLP (DDMLP)\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_dim=input_dim))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "print(\"Training...\")\n",
        "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15, show_accuracy=True)\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds14 = model.predict_classes(X_test, verbose=0)\n",
        "acc14 = accuracy_score(test[\"Label\"], preds14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "39345ebd-2cf2-8332-6fb2-d00bc801af57"
      },
      "outputs": [],
      "source": [
        "print('prediction accuracy: ', acc14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "143c5d6d-4259-c945-f1dd-b07a4c388526"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db232b70-f3a0-0952-e7ab-9c0b768e8025"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.1\n",
        "maxlen = 200\n",
        "batch_size = 32\n",
        "nb_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5f6e70a-325d-5a94-0c16-c852f951d475"
      },
      "outputs": [],
      "source": [
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(nb_words=max_features)\n",
        "tokenizer.fit_on_texts(trainheadlines)\n",
        "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
        "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9224d6a-3bca-ef84-9817-a72b41241f54"
      },
      "outputs": [],
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29359f0d-8d42-2038-c04f-2cd5bc87bc93"
      },
      "outputs": [],
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, dropout=0.2))\n",
        "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
        "          validation_data=(X_test, Y_test))\n",
        "score, acc = model.evaluate(X_test, Y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds15 = model.predict_classes(X_test, verbose=0)\n",
        "acc15 = accuracy_score(test['Label'], preds15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "365d4d4b-57cd-c678-6e62-133aa32586fb"
      },
      "outputs": [],
      "source": [
        "print('prediction accuracy: ', acc15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8afb0d07-7d0d-dd18-b19c-be603f301b52"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7271a1d1-cde0-a26c-de99-edfb7dfbf918"
      },
      "outputs": [],
      "source": [
        "nb_filter = 120\n",
        "filter_length = 2\n",
        "hidden_dims = 120\n",
        "nb_epoch = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"
      },
      "outputs": [],
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, dropout=0.2))\n",
        "# we add a Convolution1D, which will learn nb_filter\n",
        "# word group filters of size filter_length:\n",
        "model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=filter_length,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1))\n",
        "\n",
        "def max_1d(X):\n",
        "    return K.max(X, axis=1)\n",
        "\n",
        "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
        "model.add(Dense(hidden_dims)) \n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61e07990-43ef-ff27-3b36-dca146f8b4a3"
      },
      "outputs": [],
      "source": [
        "print('Train...')\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "score, acc = model.evaluate(X_test, Y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds16 = model.predict_classes(X_test, verbose=0)\n",
        "acc16 = accuracy_score(test['Label'], preds16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49e9abbe-d224-b757-c17b-bd4cfa5da818"
      },
      "outputs": [],
      "source": [
        "print('prediction accuracy: ', acc16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21b86295-1a3b-a1b8-78cf-b032b9d74eaa"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "91aa05fb-b8fe-fe99-d191-c08b8dc5620c"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60fa6ce4-1658-11d2-c0ad-3b43a542fff0"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03273578-b025-4e87-6088-a993fce31e2f"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}