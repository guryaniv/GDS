{"nbformat_minor": 1, "cells": [{"source": ["import time\n", "start_time = time.time()\n", "\n", "import gc\n", "import numpy as np\n", "import pandas as pd\n", "from subprocess import check_output\n", "\n", "develop = True\n", "train_set_ratio = .99\n", "split_seed = 123\n", "\n", "train_path = '../input/mercari-price-suggestion-challenge/train.tsv'\n", "test_path = '../input/mercari-price-suggestion-challenge/test.tsv'\n", "\n", "def rmsle(y, y_pred):\n", "    assert y.shape == y_pred.shape\n", "    return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y + 1)).mean())"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "3f9b83b5977802c262f272613858c2ccb442df19", "collapsed": true, "_cell_guid": "95407d6e-a33b-4733-9936-58bdaa5f7cf6"}, "execution_count": null}, {"source": ["# # # # # # # # # # # # # # # # # # # # # # RNN # # # # # # # # # # # # # # # # # # # # # # # #\n", "# import modules\n", "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n", "from sklearn.cross_validation import train_test_split\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "# # load data\n", "train_df = pd.read_csv(train_path, sep='\\t')\n", "test_df = pd.read_csv(test_path, sep='\\t')\n", "print('size of train set:', train_df.shape)\n", "print('size of test set:', test_df.shape)\n", "\n", "# missing values imputation\n", "def impute_missing(data_df):\n", "    data_df.category_name.fillna(value=\"missing\", inplace=True)\n", "    data_df.brand_name.fillna(value=\"missing\", inplace=True)\n", "    data_df.item_description.fillna(value=\"missing\", inplace=True)\n", "\n", "impute_missing(train_df)\n", "impute_missing(test_df)\n", "\n", "# process categorical data\n", "le = LabelEncoder()\n", "le.fit(np.hstack([train_df.category_name, test_df.category_name]))\n", "train_df.category_name = le.transform(train_df.category_name)\n", "test_df.category_name = le.transform(test_df.category_name)\n", "\n", "le.fit(np.hstack([train_df.brand_name, test_df.brand_name]))\n", "train_df.brand_name = le.transform(train_df.brand_name)\n", "test_df.brand_name = le.transform(test_df.brand_name)\n", "del le\n", "\n", "# process text variable\n", "print(\"[{}] text to seq process...\".format(time.time() - start_time))\n", "raw_text = np.hstack([\n", "    train_df.item_description.str.lower(),\n", "    train_df.name.str.lower()\n", "])\n", "print(\"[{}] fitting tokenizer...\".format(time.time() - start_time))\n", "tok_raw = Tokenizer()\n", "tok_raw.fit_on_texts(raw_text)\n", "\n", "train_df[\"seq_item_description\"] = tok_raw.texts_to_sequences(train_df.item_description.str.lower())\n", "test_df[\"seq_item_description\"] = tok_raw.texts_to_sequences(test_df.item_description.str.lower())\n", "train_df[\"seq_name\"] = tok_raw.texts_to_sequences(train_df.name.str.lower())\n", "test_df[\"seq_name\"] = tok_raw.texts_to_sequences(test_df.name.str.lower())\n", "print(\"[{}] transforming text to seq...\".format(time.time() - start_time))\n", "\n", "# length of sequences\n", "max_name_seq = np.max([np.max(train_df.seq_name.apply(lambda x: len(x))),\n", "                       np.max(test_df.seq_name.apply(lambda x: len(x)))])\n", "max_seq_item_description = np.max([np.max(train_df.seq_item_description.apply(lambda x: len(x))),\n", "                                   np.max(test_df.seq_item_description.apply(lambda x: len(x)))])\n", "print('[{}] finish calculating MAX_seq'.format(time.time() - start_time))\n", "\n", "# EMBEDDINGS MAX VALUE\n", "MAX_NAME_SEQ = 10\n", "MAX_ITEM_DESC_SEQ = 75\n", "MAX_TEXT = np.max([np.max(train_df.seq_name.max()),\n", "                   np.max(test_df.seq_name.max()),\n", "                   np.max(train_df.seq_item_description.max()),\n", "                   np.max(test_df.seq_item_description.max())]) + 2\n", "MAX_CATEGORY = np.max([train_df.category_name.max(), test_df.category_name.max()]) + 1\n", "MAX_BRAND = np.max([train_df.brand_name.max(), test_df.brand_name.max()]) + 1\n", "MAX_CONDITION = np.max([train_df.item_condition_id.max(), test_df.item_condition_id.max()]) + 1\n", "\n", "# scale target variable\n", "train_df[\"target\"] = np.log(train_df.price + 1)\n", "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n", "train_df[\"target\"] = target_scaler.fit_transform(train_df.target.reshape(-1, 1))\n", "\n", "\n", "# keras data definition\n", "def get_keras_data(dataset):\n", "    X = {\n", "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n", "        # ,'category_name' : pad_sequences(dataset.seq_category_name, maxlen=MAX_CATEGORY_SEQ)\n", "        , 'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n", "        , 'brand_name': np.array(dataset.brand_name)\n", "        , 'category_name': np.array(dataset.category_name)\n", "        , 'item_condition': np.array(dataset.item_condition_id)\n", "        , 'num_vars': np.array(dataset[[\"shipping\"]])\n", "    }\n", "    return X\n", "\n", "\n", "# extract development test\n", "if develop:\n", "    dtrain, dvalid = train_test_split(train_df, train_size=train_set_ratio, random_state=split_seed)\n", "    X_train = get_keras_data(dtrain)\n", "    X_valid = get_keras_data(dvalid)\n", "    X_test = get_keras_data(test_df)\n", "else:\n", "    dtrain = train_df\n", "    dvalid = train_df   # this might be the part that make it slow\n", "    X_train = get_keras_data(dtrain)\n", "    X_valid = get_keras_data(dvalid)\n", "    X_test = get_keras_data(test_df)\n", "ids = test_df[[\"test_id\"]]\n", "del train_df, test_df; gc.collect()\n", "print('[{}] finish forming keras data input for rnn model'.format(time.time() - start_time))\n", "\n", "\n", "# keras model definition\n", "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, \\\n", "    BatchNormalization\n", "from keras.models import Model\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "from keras import backend as K\n", "from keras import regularizers\n", "\n", "dr_r = .1\n", "\n", "def get_callbacks(filepath, patience=2):\n", "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n", "    msave = ModelCheckpoint(filepath, save_best_only=True)\n", "    return [es, msave]\n", "\n", "\n", "def rmsle_cust(y_true, y_pred):\n", "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n", "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n", "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n", "\n", "\n", "def rmsle2(y_true, y_pred):\n", "    return K.sqrt(K.mean(K.square(y_true - y_pred), axis=-1))\n", "\n", "\n", "def get_model():\n", "    # Inputs\n", "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n", "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n", "    brand_name = Input(shape=[1], name=\"brand_name\")\n", "    category_name = Input(shape=[1], name=\"category_name\")\n", "    item_condition = Input(shape=[1], name=\"item_condition\")\n", "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n", "\n", "    # Embeddings layers\n", "    emb_name = Embedding(MAX_TEXT, 50)(name)\n", "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n", "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n", "    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n", "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n", "\n", "    # rnn layer\n", "    rnn_layer1 = GRU(16)(emb_item_desc)\n", "    rnn_layer2 = GRU(8)(emb_name)\n", "\n", "    # main layer\n", "    main_l = concatenate([\n", "        Flatten()(emb_brand_name)\n", "        , Flatten()(emb_category_name)\n", "        , Flatten()(emb_item_condition)\n", "        , rnn_layer1\n", "        , rnn_layer2\n", "        , num_vars\n", "    ])\n", "    main_l = Dropout(dr_r)(Dense(128, activation='relu')(main_l))\n", "\n", "    # output\n", "    output = Dense(1, activation=\"linear\")(main_l)\n", "\n", "    # model\n", "    model = Model([name, item_desc, brand_name, category_name, item_condition, num_vars], output)\n", "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n", "\n", "    return model\n", "\n", "model = get_model()\n", "#model.summary()\n", "\n", "# fitting the model\n", "BATCH_SIZE = 2000\n", "epochs = 2\n", "steps = int(len(X_train['name']) / BATCH_SIZE) * epochs\n", "lr_init, lr_fin = 0.009, 0.006\n", "exp_decay = lambda init, fin, steps: (init / fin) ** (1/(steps - 1)) - 1\n", "lr_decay = exp_decay(lr_init, lr_fin, steps)\n", "log_subdir = '_'.join(['ep', str(epochs),\n", "                       'bs', str(BATCH_SIZE),\n", "                       'lrI', str(lr_init),\n", "                       'lrF', str(lr_fin),\n", "                       'dr', str(dr_r)\n", "                       ])\n", "\n", "model = get_model()\n", "K.set_value(model.optimizer.lr, lr_init)\n", "K.set_value(model.optimizer.decay, lr_decay)\n", "\n", "model.fit(X_train, dtrain.target\n", "          , epochs=epochs\n", "          , batch_size=BATCH_SIZE\n", "          , validation_data=(X_valid, dvalid.target)\n", "          , verbose=1)\n", "print('[{}] finish fitting rnn model'.format(time.time() - start_time))\n", "\n", "# create predictions for validation set\n", "if develop:\n", "    val_preds_rnn = model.predict(X_valid)\n", "    val_preds_rnn = target_scaler.inverse_transform(val_preds_rnn)\n", "    val_preds_rnn = np.expm1(val_preds_rnn)\n", "    y_true = np.array(dvalid.price.values).reshape(dvalid.shape[0], 1)  # price, not the target column\n", "    v_rmsle = rmsle(y_true, val_preds_rnn)\n", "    print('[{}] RMSLE of validation set with RNN model is: {}'.format(\n", "        time.time() - start_time, v_rmsle))\n", "\n", "# create predictions for test set\n", "preds_rnn = model.predict(X_test, batch_size=BATCH_SIZE)\n", "preds_rnn = target_scaler.inverse_transform(preds_rnn)\n", "preds_rnn = np.expm1(preds_rnn)\n", "print('[{}] finish prediction with rnn'.format(time.time() - start_time))\n", "\n", "# clear\n", "del dtrain, dvalid, X_train, X_valid, X_test; gc.collect()"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "fc4b0ddcb07d2c7238d2e259981ac20095ce9160", "collapsed": true, "_cell_guid": "b37e8a4c-cb49-4882-9674-4226086c7f39"}, "execution_count": null}, {"source": ["# # # # # # # # # # # # # # # # # WordBatch Linear Models # # # # # # # # # # # # # # # # # # # #\n", "from scipy.sparse import csr_matrix, hstack\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "from sklearn.model_selection import train_test_split\n", "import sys\n", "\n", "sys.path.insert(0, '../input/wordbatch/wordbatch/')\n", "import wordbatch\n", "from wordbatch.extractors import WordBag, WordHash\n", "from wordbatch.models import FTRL, FM_FTRL\n", "from nltk.corpus import stopwords\n", "import re\n", "\n", "NUM_BRANDS = 4500\n", "NUM_CATEGORIES = 1250\n", "\n", "def rmsle(y, y0):\n", "    assert len(y) == len(y0)\n", "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n", "\n", "\n", "def split_cat(text):\n", "    try:\n", "        return text.split(\"/\")\n", "    except:\n", "        return (\"No Label\", \"No Label\", \"No Label\")\n", "\n", "\n", "def handle_missing_inplace(dataset):\n", "    dataset['general_cat'].fillna(value='missing', inplace=True)\n", "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n", "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n", "    dataset['brand_name'].fillna(value='missing', inplace=True)\n", "    dataset['item_description'].fillna(value='missing', inplace=True)\n", "\n", "\n", "def cutting(dataset):\n", "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n", "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n", "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n", "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n", "\n", "\n", "def to_categorical(dataset):\n", "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n", "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n", "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n", "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n", "\n", "\n", "# Define helpers for text normalization\n", "stopwords = {x: 1 for x in stopwords.words('english')}\n", "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n", "\n", "\n", "def normalize_text(text):\n", "    return u\" \".join(\n", "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n", "         if len(x) > 1 and x not in stopwords])\n", "\n", "\n", "def construct_features(start_time):\n", "    train = pd.read_table(train_path, engine='c')\n", "    test = pd.read_table(test_path, engine='c')\n", "    print('[{}] Finished to load data'.format(time.time() - start_time))\n", "    print('Train shape: ', train.shape)\n", "    print('Test shape: ', test.shape)\n", "    nrow_test = train.shape[0]  # -dftt.shape[0]\n", "    dftt = train[(train.price < 1.0)]\n", "    train = train.drop(train[(train.price < 1.0)].index)\n", "    del dftt['price']\n", "    nrow_train = train.shape[0]\n", "    # print(nrow_train, nrow_test)\n", "    y = np.log1p(train[\"price\"])\n", "    merge: pd.DataFrame = pd.concat([train, dftt, test])\n", "    #merge: pd.DataFrame = pd.concat([train, test])\n", "    #submission: pd.DataFrame = test[['test_id']]\n", "\n", "    del train\n", "    del test\n", "    gc.collect()\n", "\n", "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n", "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n", "    merge.drop('category_name', axis=1, inplace=True)\n", "    print('[{}] Split categories completed.'.format(time.time() - start_time))\n", "\n", "    handle_missing_inplace(merge)\n", "    print('[{}] Handle missing completed.'.format(time.time() - start_time))\n", "\n", "    cutting(merge)\n", "    print('[{}] Cut completed.'.format(time.time() - start_time))\n", "\n", "    to_categorical(merge)\n", "    print('[{}] Convert categorical completed'.format(time.time() - start_time))\n", "\n", "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n", "                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n", "                                                                  \"idf\": None,\n", "                                                                  }), procs=8)\n", "    wb.dictionary_freeze = True\n", "    X_name = wb.fit_transform(merge['name'])\n", "    del (wb)\n", "    X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n", "    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n", "\n", "    wb = CountVectorizer()\n", "    X_category1 = wb.fit_transform(merge['general_cat'])\n", "    X_category2 = wb.fit_transform(merge['subcat_1'])\n", "    X_category3 = wb.fit_transform(merge['subcat_2'])\n", "    print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n", "\n", "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n", "                                                                  \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n", "                                                                  \"idf\": None}), procs=8)\n", "    wb.dictionary_freeze = True\n", "    X_description = wb.fit_transform(merge['item_description'])\n", "    del (wb)\n", "    X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n", "    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n", "\n", "    lb = LabelBinarizer(sparse_output=True)\n", "    X_brand = lb.fit_transform(merge['brand_name'])\n", "    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n", "\n", "    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n", "    print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n", "          X_name.shape)\n", "    sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n", "\n", "    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n", "    return sparse_merge, y, nrow_train, nrow_test\n", "\n", "\n", "def main(start_time):\n", "    from time import gmtime, strftime\n", "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n", "\n", "    sparse_merge, y, nrow_train, nrow_test = construct_features(start_time)\n", "\n", "    # Remove features with document frequency <=1\n", "    print(sparse_merge.shape)\n", "    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n", "    sparse_merge = sparse_merge[:, mask]\n", "    X = sparse_merge[:nrow_train]\n", "    X_test = sparse_merge[nrow_test:]\n", "    print('X size:', X.shape)\n", "    print('X_test size:', X_test.shape)\n", "    print(sparse_merge.shape)\n", "\n", "    gc.collect()\n", "    train_X, train_y = X, y\n", "    if develop:\n", "        train_X, valid_X, train_y, valid_y = train_test_split(X, y, train_size=train_set_ratio, random_state=split_seed)\n", "\n", "    # # FTRL\n", "    # model = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=sparse_merge.shape[1], iters=50, inv_link=\"identity\",\n", "    #              threads=1)\n", "    # model.fit(train_X, train_y)\n", "    # print('[{}] Train FTRL completed'.format(time.time() - start_time))\n", "    # if develop:\n", "    #     val_predsF = model.predict(X=valid_X)\n", "    #     print(\"FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(val_predsF)))\n", "    # predsF = model.predict(X_test)\n", "    # print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n", "\n", "    # FM_FTRL\n", "    model = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, \n", "                    D=sparse_merge.shape[1], alpha_fm=0.01, L2_fm=0.0,\n", "                    init_fm=0.01, D_fm=200, e_noise=0.0001, iters=17, \n", "                    inv_link=\"identity\", threads=4)\n", "    model.fit(train_X, train_y)\n", "    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n", "    if develop:\n", "        val_predsFM = np.expm1(model.predict(X=valid_X))\n", "        print(\"FM_FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), val_predsFM))\n", "    predsFM = np.expm1(model.predict(X_test))\n", "    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n", "\n", "    # delete\n", "    del train_X, train_y, sparse_merge\n", "    if develop:\n", "        del valid_X, X, y\n", "    gc.collect()\n", "\n", "    if develop:\n", "        results = val_predsFM, predsFM, valid_y\n", "    else:\n", "        results = predsFM\n", "\n", "    return results\n", "\n", "results_linear_models = main(start_time)"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "42debf1c11faac44028df67a41552c0bc255adbd", "collapsed": true, "_cell_guid": "429a315b-3862-43cf-913b-58d313cd4d2f"}, "execution_count": null}, {"source": ["# combination\n", "if develop:\n", "    val_predsFM, predsFM, valid_y = results_linear_models\n", "    # validation sizeof two models still not the same yet\n", "    #val_preds_ensemble = val_preds_rnn.reshape(val_preds_rnn.shape[0],) * .8 + val_predsFM * .2\n", "    #print(\"ensemble model validation RMSLE: \", rmsle(np.expm1(valid_y).as_matrix(), val_preds_ensemble))\n", "    preds_ensemble = preds_rnn.reshape(preds_rnn.shape[0],) * .6 + predsFM * .4\n", "else:\n", "    predsFM = results_linear_models\n", "    preds_ensemble = preds_rnn.reshape(preds_rnn.shape[0],) * .6 + predsFM * .4"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "e7be123cce68857d21c86f95f2e7465409705207", "collapsed": true, "_cell_guid": "d404d8c7-461c-4d66-99a9-dbbf47e303f6"}, "execution_count": null}, {"source": ["# submit\n", "submission = ids\n", "submission[\"price\"] = preds_ensemble\n", "submission.to_csv(\"ensemble_rnn_wordbatch_ridge_submission_v3.csv\", index=False)\n", "print('[{}] finish everything'.format(time.time()-start_time))"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "c99ad0084399e65d8ea962598a8c7a0202ad08d3", "collapsed": true, "_cell_guid": "0a114c2e-2a03-4fb1-9f0f-f49031b8cb86"}, "execution_count": null}, {"source": [], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "9f554f42e2ce9ca88f4967b12d5bf95576539db9", "collapsed": true, "_cell_guid": "eea74ad8-d97a-43cb-ab40-37c3e5c90f1a"}, "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "version": "3.6.4", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}