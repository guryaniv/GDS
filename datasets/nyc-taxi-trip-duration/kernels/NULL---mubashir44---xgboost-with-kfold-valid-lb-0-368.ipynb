{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "045aaf43189604d7e6e5358b9118a87c6f6a4a3d", "_cell_guid": "b3efb3c3-072d-41ea-a3c4-164a4f52970a"}, "source": ["This is a simple implementation of the Xgboost using kfold cross validation. With a few minor tweaks, I have been able to use this implementation to score 0.3674 on the leaderboard. \n", "\n", "I encourage all the participants to fork this notebook and make their adjustments as they please to better adjust the error. "]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "8eb85edb33881d6b1579ade62564169bb2c29088", "_cell_guid": "f36534af-2d91-4915-be92-02db57d13d12"}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from datetime import timedelta\n", "import datetime as dt\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = [16, 10]\n", "import seaborn as sns\n", "import xgboost as xgb\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "import datetime as dt\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": 1}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "17477f473d0ee1cd3e7a3a6690e143d4e1ac0ba5", "_cell_guid": "0d102856-b123-458b-8ee0-4c669d6dd868"}, "outputs": [], "source": ["t0 = dt.datetime.now()\n", "train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\n", "test = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\n", "sample_submission = pd.read_csv('../input/nyc-taxi-trip-duration/sample_submission.csv')\n", "test_1 = test.copy()"], "execution_count": 2}, {"cell_type": "markdown", "metadata": {"_uuid": "0a5ab9e48951520ee44c2f74974293a98fadce2f", "_cell_guid": "78e50c7a-2f20-42ef-a4bd-36f7cd07aa2e"}, "source": ["### Feature Engineering"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a93d78252699cb1005dc6c4498e1b808192fc199", "_cell_guid": "d4f7b603-e426-48cf-94fd-b2c23cbc3281"}, "source": ["A lot of the features have been extracted using existing models, especially the model of \"beluga\" (Cheers, mate). I also tried using the weather information as a variable but it seems that they do not serve so much of a useful purpose as far the accuracy of the result is concerned. Perhaps, I would use some sort of ensemble learning later to calculate feature importance of variables"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5e31687f59b4eef4ebe035745136e2d0e7254cdd", "_cell_guid": "07b21106-6830-4edf-8d9b-169d9844f0f9"}, "source": ["### Conversion of DATETIME Features"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "af2910dee892104cde38824fdeb1a0134671be9a", "_cell_guid": "8d9e894f-d692-4880-b47a-34a04d84ec9f"}, "outputs": [], "source": ["train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n", "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n", "train.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\n", "test.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\n", "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n"], "execution_count": 3}, {"cell_type": "markdown", "metadata": {"_uuid": "f1c4e5c16586ce84c5cc855e145686592b5fdad6", "_cell_guid": "dd60678a-0d60-458a-a1f3-1fd681b91535"}, "source": ["### DateTime Features"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "bc96e37d272431d3b8cb52663f024a460b3da046", "_cell_guid": "8b075e7a-30db-40a2-b337-40a55820f514"}, "outputs": [], "source": ["train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\n", "train.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\n", "train.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\n", "train.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\n", "train.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "train.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n", "\n", "test.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\n", "test.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\n", "test.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\n", "test.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\n", "test.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n", "test.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n", "\n", "train.loc[:, 'pickup_dayofyear'] = train['pickup_datetime'].dt.dayofyear\n", "test.loc[:,'pickup_dayofyear'] = test['pickup_datetime'].dt.dayofyear"], "execution_count": 4}, {"cell_type": "markdown", "metadata": {"_uuid": "c77508b1859beec9b7de39b47af9bc13f53a7a7d", "_cell_guid": "75ba15bd-aa92-41c6-8068-a02718a260ba"}, "source": ["### Bearing Feature"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c84ffec14bc8d87023e7528682c733a9877ed947", "_cell_guid": "9643a807-df80-4794-b709-0afc6babf33d"}, "outputs": [], "source": ["def bearing_array(lat1, lng1, lat2, lng2):\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lng_delta_rad = np.radians(lng2 - lng1)\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n", "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n", "    return np.degrees(np.arctan2(y, x))\n", "\n", "train.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n", "                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "\n", "test.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n", "                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n"], "execution_count": 5}, {"cell_type": "markdown", "metadata": {"_uuid": "9b5a68218fdc2852fa69e2bf40ef3cac476f9fe2", "_cell_guid": "7dbabcde-8ad5-4d7d-ae73-32f7c817e83f"}, "source": ["### Distance Calculation"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "19cc8a3a2d4b2dd73ad42606e49fa5afa9da5ada", "_cell_guid": "e19d9a92-46ca-4f93-be0a-befbf7eb4d60"}, "outputs": [], "source": ["def haversine_array(lat1, lng1, lat2, lng2):\n", "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n", "    AVG_EARTH_RADIUS = 6371  # in km\n", "    lat = lat2 - lat1\n", "    lng = lng2 - lng1\n", "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n", "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n", "    return h\n", "\n", "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n", "    a = haversine_array(lat1, lng1, lat1, lng2)\n", "    b = haversine_array(lat1, lng1, lat2, lng1)\n", "    return a + b\n", "\n", "train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "train.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n", "\n", "\n", "test.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "test.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n", "\n", "\n", "\n", "train.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\n", "train.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\n", "test.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\n", "test.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2"], "execution_count": 6}, {"cell_type": "markdown", "metadata": {"_uuid": "5d31acbed3c2a215e724bdb593c815b438a950cd", "_cell_guid": "81a5a79e-9bbb-4c4f-89c7-ef738fe569c8"}, "source": ["### PCA Features"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f9c3ce5c5d4b25607b72230005f17a769f67d6e1", "_cell_guid": "b8f5e474-bfd0-4d4f-ab91-5443cb4db66b"}, "outputs": [], "source": ["coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n", "                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n", "                    test[['pickup_latitude', 'pickup_longitude']].values,\n", "                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n", "\n", "pca = PCA().fit(coords)\n"], "execution_count": 7}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a7de9daaaffa4a8be6f0836a21ebbf125d1173fa", "_cell_guid": "4b78ff78-6242-4ef8-9ac4-cd3e30735109"}, "outputs": [], "source": ["train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\n", "test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\n", "test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n", "test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n", "\n", "train.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n", "test.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])"], "execution_count": 8}, {"cell_type": "markdown", "metadata": {"_uuid": "d68f565f24ca1a7f12f078b1e4600d2ef583b6e7", "_cell_guid": "792caa6b-7f88-4f3d-818e-619a468500e9"}, "source": ["### Clustering Features"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "47457f290b2eca9e1e2d435b2223bf3c1173197a", "_cell_guid": "d64dcf59-fc8d-4fd0-9d40-15892b822c3e"}, "outputs": [], "source": ["sample_ind = np.random.permutation(len(coords))[:500000]\n", "kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])"], "execution_count": 9}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a1540023c5ecc7f240e7715b01ef0fc08163d5d1", "_cell_guid": "e824f392-eaff-40ef-b9af-b1a31639f39d"}, "outputs": [], "source": ["train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\n", "train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\n", "test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\n", "test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\n", "t1 = dt.datetime.now()"], "execution_count": 10}, {"cell_type": "markdown", "metadata": {"_uuid": "4e28d19de914f1f01c7ea41092f67bbae91b1723", "_cell_guid": "974af378-c328-451e-8486-981bbefb4464"}, "source": ["## OSRM Data"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "58fb8a1e16365bc25d49a9bace100e9fa52f5a93", "_cell_guid": "1528dc60-522a-4099-a43c-56c95a48a1d5"}, "outputs": [], "source": ["fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps', ])\n", "fr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n", "test_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_test.csv',\n", "                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])"], "execution_count": 11}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "8c826a60648eb0fe472053a0ab7c9f174acbe393", "_cell_guid": "f97fd3f7-d09a-49f3-bd1c-dc60f8f145e3"}, "outputs": [], "source": ["train_street_info = pd.concat((fr1, fr2))\n", "train = train.merge(train_street_info, how='left', on='id')\n", "test = test.merge(test_street_info, how='left', on='id')"], "execution_count": 12}, {"cell_type": "markdown", "metadata": {"_uuid": "22dacde008108af2f8338f22c7274bd6e46ee9d5", "_cell_guid": "01b39919-6cf9-4e6b-b89b-142fefa0951a"}, "source": ["### Features Checking"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "58890cbea7cf342b0d8ef86a72fb7a7494ff99cb", "_cell_guid": "53508e1f-93b8-4334-a650-a07fee6ed34d"}, "outputs": [], "source": ["train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)"], "execution_count": 13}, {"cell_type": "code", "metadata": {"_uuid": "0e8dd8ae20a18b43a10f4b699ef90ff1b5ee84ef", "_cell_guid": "7a6b2d5f-839a-4d47-8143-9134910305e5"}, "outputs": [], "source": ["feature_names = list(train.columns)\n", "print(np.setdiff1d(train.columns, test.columns))\n"], "execution_count": 14}, {"cell_type": "code", "metadata": {"_uuid": "4c093fa09204f16673e47185ffec3b6eeb40153f", "_cell_guid": "afa61e1c-97d4-4eac-b2cf-516228b4f50a"}, "outputs": [], "source": ["do_not_use_for_training = ['id', 'log_trip_duration', 'trip_duration', 'dropoff_datetime', 'pickup_date', \n", "                           'pickup_datetime', 'date']\n", "feature_names = [f for f in train.columns if f not in do_not_use_for_training]\n", "# print(feature_names)\n", "print('We have %i features.' % len(feature_names))\n", "train[feature_names].count()\n", "         "], "execution_count": 15}, {"cell_type": "markdown", "metadata": {"_uuid": "f8877c9ad4516368e7478859128129761cddfb86", "_cell_guid": "bae6ed5e-8248-45df-a234-7e01e37c576f"}, "source": ["### Features Encoding "]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "3bd1df273eb279576655d5ff1a4fcbb1c8a53161", "_cell_guid": "46e34e19-cad9-4db9-bfd5-0101c23b41d4"}, "outputs": [], "source": ["train['store_and_fwd_flag'] = train['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)"], "execution_count": 16}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "396b0e0ef56a83c4daf3b780567c584a58277fed", "_cell_guid": "5e79ef29-bc6a-4192-b12b-18b57c1e5494"}, "outputs": [], "source": ["test['store_and_fwd_flag'] = test['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)"], "execution_count": 17}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "4fb025c09b99834da9c821c7d264a75bcb893a41", "_cell_guid": "c2f874be-410a-4a01-b0c4-4a66b876b95c"}, "source": ["### K Fold Cross Validation"]}, {"cell_type": "code", "metadata": {"_uuid": "0578f508d5331705c0ae8ad74e044721384e2dbe", "_cell_guid": "72715a79-e072-448a-82db-b91171414a1b"}, "outputs": [], "source": ["from sklearn.model_selection import KFold\n", "\n", "X = train[feature_names].values\n", "y = np.log(train['trip_duration'].values + 1)  \n", "\n", "\n", "kf = KFold(n_splits=10)\n", "kf.get_n_splits(X)\n", "\n", "print(kf)  \n", "\n", "KFold(n_splits=10, random_state=None, shuffle=False)\n", "for train_index, test_index in kf.split(X):\n", "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n", "    X_train, X_test = X[train_index], X[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    \n", "    \n", " "], "execution_count": 18}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "8c9322eb02ff71a5d176df71950925f7ef66b337", "_cell_guid": "c5537638-11a3-4997-8a9c-6d3e0896bbc1"}, "source": ["### XgBoost Implementation"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "537c3ded92f5ecf30d640b58242c2bb60a3b254a", "_cell_guid": "3a7793c4-6fd9-41a9-97eb-682f35074207"}, "outputs": [], "source": ["   \n", "dtrain = xgb.DMatrix(X_train, label=y_train)\n", "dvalid = xgb.DMatrix(X_test, label=y_test)\n", "dtest = xgb.DMatrix(test[feature_names].values)\n", "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n", "\n", "# Try different parameters! My favorite is random search :)\n", "xgb_pars = {'min_child_weight': 10, 'eta': 0.04, 'colsample_bytree': 0.8, 'max_depth': 15,\n", "            'subsample': 0.75, 'lambda': 2, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n", "            'eval_metric': 'rmse', 'objective': 'reg:linear'}    "], "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "54d6183828377cd842ba6aa7a8bde18a34f6db4d", "_cell_guid": "c44a7935-7873-4f3b-8233-e18616ddf73b"}, "outputs": [], "source": ["model = xgb.train(xgb_pars, dtrain, 500, watchlist, early_stopping_rounds=250,\n", "                  maximize=False, verbose_eval=15)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "da3471a9cf011c5b5fd1d899ef2634068da9c88c", "_cell_guid": "1e44ac42-43ad-430c-8cab-f6ba4a053f08"}, "outputs": [], "source": ["ytest = model.predict(dtest)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "dafbc838ec6ee8ba604b2c3c774bbd1079becbf1", "_cell_guid": "6e15204d-cb84-4e80-9615-2671de8b5e60"}, "outputs": [], "source": ["print('Test shape OK.') if test.shape[0] == ytest.shape[0] else print('Oops')\n", "test['trip_duration'] = np.exp(ytest) - 1\n", "test[['id', 'trip_duration']].to_csv('xgb_submission.csv.gz', index=False, compression='gzip')\n", "\n", "print('Valid prediction mean: %.3f' % ypred.mean())\n", "print('Test prediction mean: %.3f' % ytest.mean())\n", "\n", "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n", "sns.distplot(ypred, ax=ax[0], color='red', label='validation prediction')\n", "sns.distplot(ytest, ax=ax[1], color='blue', label='test prediction')\n", "ax[0].legend(loc=0)\n", "ax[1].legend(loc=0)\n", "plt.show()\n", "\n", "t1 = dt.datetime.now()\n", "print('Total time: %i seconds' % (t1 - t0).seconds)"], "execution_count": null}], "nbformat_minor": 1}