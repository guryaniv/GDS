{"cells":[{"metadata":{"_uuid":"31e7643d2419f3680359de6933701d936ec241bd"},"cell_type":"markdown","source":"hi everyone, this is a baseline submission that haven't strated considering information from pictures. Fell free to ask me any questions or make suggestions. Thanks."},{"metadata":{"trusted":true,"_uuid":"b84bb17a873c7b041dab4b97b4ac454a3757371e"},"cell_type":"code","source":"#import cv2\nimport gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport warnings\nimport re\n\nimport numpy as np\nimport pandas as pd\n\n#from PIL import Image\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\npd.options.display.max_rows = 64\npd.options.display.max_columns = 512","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37f48d05ddde7685dc48cbf0c961f037da8c9daa"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true,"_uuid":"9a80fbcd36b724785a30091d114b2ede9d505fe0"},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntrain['AdoptionSpeed'].astype(np.int32)\ntest = pd.read_csv('../input/test/test.csv')\ndf = pd.concat([train,test],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc9ade4608f7c8bbb12af4a4ff2938c385d7a1f1"},"cell_type":"code","source":"train_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\nsentimental_analysis = train_sentiment_files + test_sentiment_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f630a02c48abc698c3371fd648b5092f98d93f"},"cell_type":"code","source":"score=[]\nmagnitude=[]\npetid=[]\nfor filename in sentimental_analysis:\n    with open(filename, 'r') as f:\n        sentiment_file = json.load(f)\n        file_sentiment = sentiment_file['documentSentiment']\n        file_score =  sentiment_file['documentSentiment']['score']\n        file_magnitude = sentiment_file['documentSentiment']['magnitude']\n        score.append(file_score)\n        magnitude.append(file_magnitude)\n        petid.append(filename.replace('.json','').replace('../input/train_sentiment/', '').replace('../input/test_sentiment/', ''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"568b9b9b67f2d32dc7186ab0262473672384b1f9"},"cell_type":"code","source":"score_dict = dict(zip(petid,score))\nmagnitude_dict = dict(zip(petid,magnitude))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b92b8c41905190fb8fd76b86b4c060cdf9deb828"},"cell_type":"code","source":"df['Score'] = df['PetID'].map(score_dict)\ndf['Score'][df.Score.isnull()] = 0\ndf['Magnitude'] = df['PetID'].map(magnitude_dict)\ndf['Magnitude'][df.Magnitude.isnull()] = 0\ndf.set_index('PetID',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea6f69b22dd9edba47eb1359c7fc5d808702b845"},"cell_type":"markdown","source":"## Core features"},{"metadata":{"_uuid":"49ded3c1f62c3009c3086fc510038cbefce8d2d7"},"cell_type":"markdown","source":"### Name \nCategorize to with meaningful name, with meaningless name and without name.\n\n#### Meaningless Rule\n1. 1 or 2 letters\n2. With the word \"NO\" \"NOT\" \"YET\" \"NAME\"\n3. Start with numbers"},{"metadata":{"trusted":true,"_uuid":"3be571107ff8c7a2ee46351cfdda508ef558119f"},"cell_type":"code","source":"def namevaild(name):\n    if name == np.nan:\n        return 0\n    elif len(str(name)) < 3:\n        return 1\n    elif re.match(u'[0-9]', str(name).lower()):\n        return 1\n    elif len(set(str(name).lower().split(' ')+['no','not','yet','male','female','unnamed'])) != len(set(str(name).lower().split(' ')))+6:\n        return 1\n    else:\n        return 2\ndf['Name_state'] = df['Name'].apply(namevaild)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46772786a849888e5a0da777a32caf597cadcc67"},"cell_type":"markdown","source":"### Fee\n\nBinning into 0, (0,50], (50,100], (100,200], (200,500], (500, +inf)"},{"metadata":{"trusted":true,"_uuid":"c3bc6678a05291ba024796249cb20dd2bb21da29"},"cell_type":"code","source":"df['Fee_per_pet'] = df.Fee/df.Quantity\n\ndf['Fee_Bin']=pd.factorize(pd.cut(df.Fee_per_pet,bins=[0,0.01,50,100,200,500,3000],right=False))[0]\nfee_bin_dummies_df = pd.get_dummies(df['Fee_Bin']).rename(columns=lambda x: 'Fee_Bin_' + str(x))\ndf = pd.concat([df, fee_bin_dummies_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bc0cb87587c0d1fbd45f6542c4db8f31530dc0"},"cell_type":"markdown","source":"### Quantity\n\nBinning to [1,2,4,22]"},{"metadata":{"trusted":true,"_uuid":"8527dc51b4d154058ed64430f7270f5b843d5bfc"},"cell_type":"code","source":"df['Quantity_Bin']=pd.factorize(pd.cut(df.Quantity,bins=[1,2,4,22],right=False))[0]\nquantity_bin_dummies_df = pd.get_dummies(df['Quantity_Bin']).rename(columns=lambda x: 'Quantity_Bin_' + str(x))\ndf = pd.concat([df, quantity_bin_dummies_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88075f0476c9281890c950bca01c522bac167efa"},"cell_type":"markdown","source":"### VideoAmt & PhotoAmt"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"9762d2352aad9847b403d75378e57c32a494d54f"},"cell_type":"code","source":"df.VideoAmt = df.VideoAmt.apply(lambda x: 1 if x > 0 else 0)\n\ndf['PhotoAmt_Bin']=pd.factorize(pd.cut(df.PhotoAmt,bins=[0,1,2,4,31],right=False))[0]\nphoto_bin_dummies_df = pd.get_dummies(df['PhotoAmt_Bin']).rename(columns=lambda x: 'PhotoAmt_Bin_' + str(x))\ndf = pd.concat([df, photo_bin_dummies_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5147b3ab368dea586618546b05606f97a7f10cb3"},"cell_type":"markdown","source":"### State"},{"metadata":{"trusted":true,"_uuid":"ab581c89e094df971f4b7a051afe228958ae48b6"},"cell_type":"code","source":"def map_state(state):\n    if state == 41326:\n        return 'Selangor'\n    elif state == 41401:\n        return 'Kuala_Lumpur'\n    else:\n        return 'Other_State'\ndf['State_Bin'] = df.State.apply(map_state)\nstate_bin_dummies_df = pd.get_dummies(df['State_Bin']).rename(columns=lambda x: 'State_' + str(x))\ndf = pd.concat([df, state_bin_dummies_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69f877e983fb473bd8a9a89ffb802bc9a5c21584"},"cell_type":"markdown","source":"### Rescuer\nBinning the saving number of animals in total"},{"metadata":{"trusted":true,"_uuid":"e0bdd9c3228a5678ca898a8a71d91fa07bc1b8f4"},"cell_type":"code","source":"rescuer_dict = df.RescuerID.value_counts().to_dict()\ndf['Rescuer_Num'] = df.RescuerID.map(rescuer_dict)\n#df['Rescuer_Bin']=pd.factorize(pd.cut(df.Rescuer_Num,bins=[1,2,5],right=False))[0]\n#df['Rescuer_Bin'].value_counts()\n#rescuer_bin_dummies_df = pd.get_dummies(df['Rescuer_Bin']).rename(columns=lambda x: 'Rescuer_Bin_' + str(x))\n#df = pd.concat([df, rescuer_bin_dummies_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd854160a6f13af8c964b2e3370ca11f502ef48"},"cell_type":"markdown","source":"### Breed"},{"metadata":{"_uuid":"f751f43cb0d7687d033a4b200fb63c399822aea3"},"cell_type":"markdown","source":"#### Mix or Pure\nWe consider a cat/dog is mixed breed if:\n1. Breed1_name or Breed2_name is Mixed_Breed\n2. Breed1_name is NA\n3. Breed1_name != Breed2_name"},{"metadata":{"trusted":true,"_uuid":"364ce73ef02c45f2a456eee41efa79f04dd806f6"},"cell_type":"code","source":"breeds = pd.read_csv('../input/breed_labels.csv')\nbreeds_dict = {k: v for k, v in zip(breeds['BreedID'], breeds['BreedName'])}\ndf['Breed1_name'] = df['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')\ndf['Breed2_name'] = df['Breed2'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a81e35dd813af05b0ff09e8d4c5c420dc60c2cb6"},"cell_type":"code","source":"df['Breed'] = df['Breed1_name'] + '--' + df['Breed2_name']\ndef mix_breed(string):\n    breed = string.split('--')\n    if breed[0] in ['Mixed_Breed','NA']:\n        return 1\n    elif breed[1] == 'Mixed_Breed':\n        return 1\n    elif breed[1] == 'NA':\n        return 0\n    elif breed[0] != breed[1]:\n        return 1\n    else:\n        return 0\ndf['Mixed_Breed'] = df.Breed.apply(mix_breed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b8bb4ef066c8289b3d401ac7d5688005b4a5e3"},"cell_type":"markdown","source":"### Description\n\nInterestingly, adding this text features actually damages the prediction. I have find any explainations, if you have any ideas, fell free to comment below."},{"metadata":{"trusted":true,"_uuid":"1a0a5b36e7ec7972611ca07119f2459156cf3e1f"},"cell_type":"code","source":"'''\ndf.Description[df.Description.isnull()] = ''\ndes_list = df.Description.values.tolist()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"877beeca8e8c27964a7ca2294b678396d6d4cf8e"},"cell_type":"code","source":"'''\nimport unicodedata\nimport re\n\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n'''\n\n'''\ndef replace_numbers(words):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words \n'''\n'''\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    #words = replace_numbers(words)\n    words = remove_stopwords(words)\n    #words = stem_words(words)\n    words = lemmatize_verbs(words)\n    return words\n\n'''\n'''\nword_bag = []\n\nfor i,item in enumerate(des_list):\n    words = word_tokenize(item)\n    words = normalize(words)\n    word_bag.append(words)\ndf['Word_bag'] = word_bag\n\ndef wordjoin(x):\n    return ' '.join(x)\n\ndf['Word_list'] = df['Word_bag'].apply(wordjoin)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71a0bad6795c5562121d3e025a6616498943551"},"cell_type":"code","source":"'''\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvectorizer = CountVectorizer(min_df = 0.02)\ntransformer=TfidfTransformer()\ntfidf = transformer.fit_transform(vectorizer.fit_transform(df.Word_list))\nweight=tfidf.toarray()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a93e64ae22e65f95a6c323922f3a8c7ff152a2ba"},"cell_type":"code","source":"'''\nfrom sklearn.decomposition import PCA\n\nn_components = 25\npca = PCA(n_components=n_components, random_state=42)\npca.fit(weight)\ntext_feature = pca.transform(weight)\n\ncolumns = []\nfor i in range(n_components):\n    columns.append('text_feature_'+str(i+1))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb775ca5b6d2cc89495338ca9c536f1e12363aef"},"cell_type":"code","source":"'''\ndf = pd.concat([df,pd.DataFrame(text_feature, index = df.index, columns = columns)],axis = 1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"904956b839cf9da129e8871f3a719efa80886341"},"cell_type":"markdown","source":"## Baseline"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b9174a831401f6193781c2d42c44302bc9e34a7a"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc77347829cb739dd5ecdf3ca002e4518537e89"},"cell_type":"code","source":"df_copy = df.drop(columns=['Description','Fee','Fee_per_pet','Name','PhotoAmt','Quantity','RescuerID','State','State_Bin','Fee_Bin','Quantity_Bin','PhotoAmt_Bin','Breed','Breed1_name','Breed2_name'])\ntrain = df_copy[df.AdoptionSpeed.notnull()]\ntest  = df_copy[df.AdoptionSpeed.isnull()]\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bed0725581e061efae6f21a802fc37a74b97841"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5313c2f613b561f786b1141d20159c138c5ce907"},"cell_type":"code","source":"X_train = train.drop(columns=['AdoptionSpeed'])\ny_train = train.AdoptionSpeed\nX_test = test.drop(columns=['AdoptionSpeed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8978fc5e6ece89820fbd88ed870997a01e853429"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 600, max_depth=None, criterion='gini')\nrf.fit(X_train,y_train)\ny_predict = rf.predict(X_test).astype(np.int32)\nsubmission = pd.DataFrame({'PetID': test.index, 'AdoptionSpeed': y_predict})\nsubmission = submission[['PetID','AdoptionSpeed']]\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"837c0d51845e0de0914061cea9a5277d4d3aab90"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}