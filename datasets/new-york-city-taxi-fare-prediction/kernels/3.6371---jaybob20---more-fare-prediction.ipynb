{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom scipy.spatial.distance import pdist\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import Birch\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"# Read data\ntrain = pd.read_csv('../input/train.csv', nrows = 1000000)\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b91c74370b1efe0324a39459fdb18f0e10ab106","trusted":true},"cell_type":"code","source":"# Let's start by checking for NaN values\nprint('Sum of NaN values for each column')\nprint(train.isnull().sum())\n\n# It seems like we lost some data for the dropoff. There are several ways of handling this, but I just go with removing the rows.\ntrain = train.dropna()\nprint('Sum of NaN values for each column after dropping NaN')\nprint(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee71ff8644f0fc938167562e1a3a2960674fd7d7","scrolled":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Seems like we have a few outliers. Let's visualize the data and see if we can spot the outliers.\n#columns_to_plot = ['fare_amount', 'passenger_count']\n#sns.pairplot(train.loc[:, train.columns != 'pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1eb1bfb629d4fa2433cf5af9f09cf6ac67d5124"},"cell_type":"code","source":"pickup_longitude_min = test.pickup_longitude.min()\npickup_longitude_max = test.pickup_latitude.max()\npickup_latitude_min = test.pickup_latitude.min()\npickup_latitude_max = test.pickup_latitude.max()\ndropoff_longitude_min = test.dropoff_longitude.min()\ndropoff_longitude_max = test.dropoff_longitude.max()\ndropoff_latitude_min = test.dropoff_latitude.min()\ndropoff_latitude_max = test.dropoff_latitude.max()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64a26f6003e700786451c62398864a0bd0e4e3ad","trusted":true},"cell_type":"code","source":"# So there seem to be a lot of outliers:\n\n# Latitude and longitude varies from -3116.28 to 2522.27 whereas the mean is around 40 (pickup_latitude, but goes for all the coordinates)\n# This is probably due to a typo when data was gathered. Let's select a more reasonable value (2 times the standard deviation)\n#columns_to_select = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n#for column in columns_to_select:\n#    train = train.loc[(train[column] > train[column].mean() - train[column].std() * 2) & (train[column] < train[column].mean() + train[column].std() * 2)]\n\n# Manually picking reasonable levels\ntrain = train.loc[(train['fare_amount'] > 0) & (train['fare_amount'] < 300)]\ntrain = train.loc[(train['pickup_longitude'] > pickup_longitude_min) & (train['pickup_longitude'] < pickup_longitude_max)]\ntrain = train.loc[(train['pickup_latitude'] > pickup_latitude_min) & (train['pickup_latitude'] < pickup_latitude_max)]\ntrain = train.loc[(train['dropoff_longitude'] > dropoff_longitude_min) & (train['dropoff_longitude'] < dropoff_longitude_max)]\ntrain = train.loc[(train['dropoff_latitude'] > dropoff_latitude_min) & (train['dropoff_latitude'] < dropoff_latitude_max)]\n# Let's assume taxa's can be mini-busses as well, so we select up to 8 passengers.\ntrain = train.loc[train['passenger_count'] <= 8]\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26ee773d9dc4b5e0cf1f91fb2a14a9616b884945","trusted":true,"collapsed":true},"cell_type":"code","source":"def haversine_np(a):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = a\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km\n\n\ndef cityblock(a,dist='cityblock'):\n    return pdist(a.reshape(2,2),dist)[0]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6835de4c3fc61e53316634b5752a8aad80b5f65","trusted":true,"collapsed":true},"cell_type":"code","source":"# from multiprocessing import Pool\n# pool = Pool()\n# coords = train[['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']].values\n# a = pool.map(haversine_np,coords)\n# del pool","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"dist_types = ['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\ndist_types = ['chebyshev','euclidean','canberra','sqeuclidean','braycurtis','minkowski','hamming','cityblock']\nfrom multiprocessing import Pool\npool = Pool()\ncombine = [train, test]\nfor dataset in combine:    \n    coords = dataset[['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']].values\n\n    dataset['haversine'] = pool.map(haversine_np,coords)\n        \n    \n    for dist_type in dist_types:\n        print(dist_type)\n        dataset[dist_type] = pool.starmap(cityblock,[(x,dist_type,) for x in coords],1000)\n    \n    # Maybe time of day matters? Obviously duration is a factor, but there is no data for time arrival\n    # Features: hour of day (night vs day), month (some months may be in higher demand) \n    dataset['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\n    dataset['hour_of_day'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['week'] = dataset.pickup_datetime.dt.week\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['day_of_year'] = dataset.pickup_datetime.dt.dayofyear\n    dataset['week_of_year'] = dataset.pickup_datetime.dt.weekofyear\npool.close()\npool.join()\ndel pool\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8803c4977a741d1e5ecbfc426a6f538f0b373844"},"cell_type":"code","source":"features = [ 'pickup_latitude',  'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude',\n             'haversine', 'chebyshev', 'euclidean', 'canberra', 'sqeuclidean', 'braycurtis',\n             'minkowski',  'hamming', 'cityblock']\n\nn = StandardScaler()\ntrain[features] = n.fit_transform(train[features])\ntest[features] = n.transform(test[features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78de80b19651f71e0d219a04b6903f008d2e4ec3","collapsed":true},"cell_type":"code","source":"coords = ['pickup_latitude',  'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude',]\nconcat = pd.concat([train[coords],test[coords]])\ndb = Birch(branching_factor=50, n_clusters=None, threshold=0.5,compute_labels=True).fit(concat)\nlabels = db.labels_\ntrain['cluster'] = labels[:train.shape[0]]\ntest['cluster'] = labels[train.shape[0]:]\n\ndb = Birch(branching_factor=50, n_clusters=None, threshold=0.5,compute_labels=True).fit(concat[['pickup_latitude',  'pickup_longitude']])\nlabels = db.labels_\ntrain['cluster1'] = labels[:train.shape[0]]\ntest['cluster1'] = labels[train.shape[0]:]\n\ndb = Birch(branching_factor=50, n_clusters=None, threshold=0.5,compute_labels=True).fit(concat[['dropoff_latitude', 'dropoff_longitude',]])\nlabels = db.labels_\ntrain['cluster2'] = labels[:train.shape[0]]\ntest['cluster2'] = labels[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"536e1474400283191fafc3047b657fcc2935d43b","collapsed":true},"cell_type":"code","source":"pca_features = [ \n#     'pickup_latitude',  'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude',\n             'haversine', 'chebyshev', 'euclidean', 'canberra',  'braycurtis',\n             'minkowski',  'hamming', 'cityblock']\n\npca = PCA(n_components=3)\np_result = pca.fit_transform(train[pca_features])\nfor x in range(p_result.shape[1]):\n    train['pca0' + str(x)] = p_result[:,x]\n  \np_result = pca.transform(test[pca_features])\nfor x in range(p_result.shape[1]):\n    test['pca0' + str(x)] = p_result[:,x]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39def2ffb283d945d6d1287c91cbe489ef1040d5","scrolled":false,"trusted":true},"cell_type":"code","source":"# Let's check how the features correlate\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cec893be34e814522c8945ba0e9959a712536d6","trusted":true,"collapsed":true},"cell_type":"code","source":"good_dist =  ['pca0','pca1','pca2','cluster',\n#               'haversine','chebyshev','euclidean','canberra','braycurtis','minkowski','hamming','cityblock'\n             ]\n# Let's drop all the irrelevant features\ntrain_features_to_keep = ['fare_amount',] + ['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'] + good_dist\ntrain.drop(train.columns.difference(train_features_to_keep), 1, inplace=True)\n\ntest_features_to_keep = ['key',] + ['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'] + good_dist\ntest.drop(test.columns.difference(test_features_to_keep), 1, inplace=True)\n\n# Let's prepare the test set\nx_pred = test.drop('key', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9be94caa63d8fee3a498ebb00faa883e91bc72a7","trusted":true,"collapsed":true},"cell_type":"code","source":"# Let's run XGBoost and predict those fares!\nx_train,x_test,y_train,y_test = train_test_split(train.drop('fare_amount',axis=1),train['fare_amount'],random_state=123,test_size=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"b5eb984e34364949f052e7a603be5cb428fe2222","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"def XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse',\n                            'eta':.3,\n                            'max_depth':4,\n                            'min_child_weight':3,\n                           }\n                    ,dtrain=matrix_train,num_boost_round=300,\n                    early_stopping_rounds=10,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e43e0253945333d179480d75634f4e0a55e3057","collapsed":true},"cell_type":"code","source":"xgb.plot_importance(model)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9728f5c59bc7a50a93a1d7b2d5e9e17914785d3","trusted":false,"collapsed":true},"cell_type":"code","source":"prediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)\n# Add to submission\nsubmission = pd.DataFrame({\n        \"key\": test['key'],\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('sub_fare.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f375bd4ce358439c382a14594d8f3b79e211140","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33edf3abbe767e557ca57079f8fa80fcb25c7fbe","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}