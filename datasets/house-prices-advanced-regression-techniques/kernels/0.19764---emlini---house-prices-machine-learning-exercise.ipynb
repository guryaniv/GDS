{"cells":[{"metadata":{"_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86","_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199"},"cell_type":"markdown","source":"# Introduction\n**This will be your workspace for Kaggle's Machine Learning education track.**\n\nYou will build and continually improve a model to predict housing prices as you work through each tutorial.  Fork this notebook and write your code in it.\n\nThe data from the tutorial, the Melbourne data, is not available in this workspace.  You will need to translate the concepts to work with the data in this notebook, the Iowa data.\n\nCome to the [Learn Discussion](https://www.kaggle.com/learn-forum) forum for any questions or comments. \n\n# Write Your Code Below\n\n"},{"metadata":{"_uuid":"48f69537fe221e294999417ca7e7b9f2c30e6f2f","_cell_guid":"a6b33d01-595d-48fc-9952-f27e765eae2f"},"cell_type":"markdown","source":"The steps in this Kernel follow those outlined in DanB's Learn Machine Learning tutorial (https://www.kaggle.com/dansbecker/starting-your-ml-project).\n\nI occasionally branch out from those steps when I want additional information about the dataframe."},{"metadata":{"_uuid":"8c4b2a8cd0e63909ba339a5d54e30807e6d2aaac","_cell_guid":"127dc0cb-3534-4a30-8714-dbef10e54eb9","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nmb_filepath = '../input/train.csv'\nprint('hello world')\nmb_data = pd.read_csv(mb_filepath)\nprint(mb_data.describe())","execution_count":258,"outputs":[]},{"metadata":{"_uuid":"184ca9238fca0c18bdac46d0623e4db904304fb0","_cell_guid":"24d2bc39-1a70-49e1-93e6-3e5199c2fcad"},"cell_type":"markdown","source":"The outut from the above code tells me that we have 38 variables.\n\nMy addition:  I can see that some of these seem to have more of a numerical spread than others, so I'd like to get the TYPE of variable (for each variable).  \n\nThere is a type() function, but I'd have to run it on each variable (or write a function that assessed and printed the type for each variable).  I found a function instead (select_dtypes) that allows you to specify certain kinds of variables.  So I can say \"I only want to see the variables that are integers,\" which is what I've done below:"},{"metadata":{"_uuid":"ec820ca72af62b63d12e7ea269f5be86b8400daa","scrolled":true,"_cell_guid":"b3f82855-de46-499e-91cf-031cf6904918","trusted":true},"cell_type":"code","source":"print(mb_data.select_dtypes(include='int')) #returns variables that are integers (useful for initial mb_pred with only continuous vars below)\n","execution_count":259,"outputs":[]},{"metadata":{"_uuid":"2b76bb18e2c674383786f6a45c21bf5e4b24b434","_cell_guid":"f00d3da6-3e55-4136-85e3-fe945651ef13"},"cell_type":"markdown","source":"The output from the above code is not as useful as it might be, because 35 out of 38 variables are integers (and some--like YrSold and MoSold) are either more categorical in nature, or not likely to have a direct impact on house prices. (Also, the output is abbreviated--it doesn't show me all the columns.)\n\nWhat would be more helpful is knowing whether certain integer-type variables are in fact Ordinal (as with OverallQual).  But we may be able to infer that based on the variable name.\n\nOn to the next step--practice selecting a single column (SalePrice) and returning just the head (first couple lines) from that column:"},{"metadata":{"_uuid":"1c728098629e1301643443b1341556a15c089b2b","_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","trusted":true},"cell_type":"code","source":"mb_price_data = mb_data.SalePrice\nprint(mb_price_data.head())","execution_count":260,"outputs":[]},{"metadata":{"_uuid":"bb5163c6e66776dcd21dc3eef774efdc5ecb43f7","_cell_guid":"2a171c40-5ff9-4004-8135-6cb7735ebd3f"},"cell_type":"markdown","source":"Then selecting multiple columns, saving them to a new variable, and giving me descriptives on them:"},{"metadata":{"_uuid":"9df3f91432706decbe164d46ebc69549d34b033f","_cell_guid":"1036afcc-1435-4526-b27c-472c00d5682a","trusted":true},"cell_type":"code","source":"interest_col = ['OverallCond', 'YearBuilt']\ntwocols = mb_data[interest_col]\ntwocols.describe()","execution_count":261,"outputs":[]},{"metadata":{"_uuid":"ed7912bbf4c78cf44520beea8435503cf0266f81","_cell_guid":"2fcf15b2-7f3c-4366-947c-ac1a5bb66694"},"cell_type":"markdown","source":"Identifying the prediction target (y) and the predictors (x)\n\nPredictors are based on the tutorial's recommendations"},{"metadata":{"_uuid":"fbab3b88d7f58b4bda87cf97a7d331a174c362c2","_cell_guid":"f57b7c87-e5af-4b3f-8e20-6a9f8abd29a2","trusted":true},"cell_type":"code","source":"y = mb_data.SalePrice\n\nmb_pred = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n\nX = mb_data[mb_pred]\n\nprint(X) #Just to check that my predictor list is correct","execution_count":262,"outputs":[]},{"metadata":{"_uuid":"a885a9aaac43785e72747aaa64b734b8943d33c8","_cell_guid":"0a3f93bd-311e-49c9-b33d-a5697d493eeb"},"cell_type":"markdown","source":"Now build the model (Define, Fit, Predict, and Evaluate):\n\nDefine type of model: decision tree regression (bc that's what the tutorial is having us practice)\n\nFit model: actually run the model with X and y"},{"metadata":{"_uuid":"bec2b9b94b17abf96fdb27810574659321e78b0b","scrolled":true,"_cell_guid":"0a2d842a-33cc-4f4f-8ec1-dda2c9cf2a1b","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# Define part\nmb_model = DecisionTreeRegressor()\n\n# Fit part\nmb_model.fit(X,y)\n","execution_count":263,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9c64b150a58b3b1f6329aaafc11d80cfe1eda942","_cell_guid":"2c4f10e7-c473-4f9a-8be4-fe8b83c7d721"},"cell_type":"markdown","source":"Predict values for the first 5 houses with the set of predictor variables we specified: "},{"metadata":{"_uuid":"8312e1adc85db60178131d150d5778f031100ae4","_cell_guid":"69d5f413-3ba0-4b7a-a3d8-6d05f0adac6a","trusted":true},"cell_type":"code","source":"print(\"Predicting sale price for the first 10 houses with our predictors:\")\nprint(X.head(10)) # gives the first 10 rows so we can see the X values for those houses\nprint(\"The predictions are:\")\nprint(mb_model.predict(X.head(10))) # shows model's predicted sale prices for the first 10 houses\n\n","execution_count":264,"outputs":[]},{"metadata":{"_uuid":"88b9f859c7ff776e8b05f684dc5769532d104106","_cell_guid":"9ae34ed4-f19f-41ff-b9c6-b5a380b701c8","trusted":true},"cell_type":"code","source":"print(\"Predicting sale price for the last 10 houses with our predictors:\")\nprint(X.tail(10)) # gives the last 10 rows so we can see the X values for those houses\nprint(\"The predictions are:\")\nprint(mb_model.predict(X.tail(10))) # shows model's predicted sale prices for the last 10 houses","execution_count":265,"outputs":[]},{"metadata":{"_uuid":"64e49f070072d6b4b2226cd6fa2b9aac54fc18f8","_cell_guid":"05bcf81d-612c-47be-8b5f-cdccdbe35896","trusted":true},"cell_type":"code","source":"print(\"Predicting sale price for a random sample of 10 houses with our predictors:\")\n\nimport _random as rnd # package that includes a variety of functions for generating/selecting random numbers/samples/objects/etc.\n\nrand_samp = X.sample(10) # selects a random sample of 10 rows)\nprint(rand_samp) # gives a random sample of 10 rows so we can see the X values for those houses\nprint(\"The predictions are:\")\nprint(mb_model.predict(X.sample(10))) # shows model's predicted sale prices for the randome cample of 10 houses","execution_count":266,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d92faae1a28a70a9f094e4341ba00243a1451792","_cell_guid":"5b02e21c-5c50-48fd-994a-7421c4ed3029"},"cell_type":"markdown","source":"**Examining the few predictions we've made (a first step I've added in assessing model fit):**\n\nThe 6th prediction in the list (ID #961) strikes me as somewhat odd. \n\n$164,500 is the predicted price for a house with 4 bedrooms, 2 full baths, 11 rooms above ground, on a lot of over 12,000 sq ft.  \n\nGranted, the house was built in 1977, so it is on the somewhat older side, but the fact that it is predicted to sell for less than the 5th house on the list (ID #387), built a year earlier (1976), with only 3 br and 1 full bath, a smaller lot, and no second floor, suggests that our current model may not be capturing all the variables relevant for predicting house prices. \n\nSo I'll move onto the next step in the tutorial, which is generating summary calculations of model fit:"},{"metadata":{"_uuid":"e67a7f3eba247dbecde30d00f2d30a48c678dccc","_cell_guid":"06e6e64a-da08-429f-953a-57ea58c886d0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nprice_pred = mb_model.predict(X) #gives \"in-sample\" score, which is bad, according to tutorial, bc it uses same set of data to generate model and assess model fit \nmean_absolute_error(y, price_pred)","execution_count":267,"outputs":[]},{"metadata":{"_uuid":"48635d682c81d68f6e2224ae78844db45c21e064","_cell_guid":"981ea30f-5323-40ce-9552-f4a348869377"},"cell_type":"markdown","source":"The above calculation of model fit is the Mean Absolute Error, but it's calculated based on the same data set used to generate the model, which is NOGOOD. (Tutorial explains the why in more detail here: https://www.kaggle.com/dansbecker/model-validation)\n\nTo better check model fit, we need to use validation data--a different set of data than we used to predict the model.  So we use a function to split the training dataset into two parts, generate the model from the first part, then calculate model fit based on the second part."},{"metadata":{"_uuid":"ea9caf68befcda1ef740b4d89ca58597dbbf4196","_cell_guid":"e89eb920-80fe-46ef-8e75-43ffcde4d619","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0) #splits both the predictors and the outcome into two sets (\"train\"--training--and \"val\"--validation)\nmb_model.fit(train_X, train_y)\n\n# print the mean absolute error (the average difference btwn the actual value of y and the value of y predicted by our model generated using the training set of data)\nval_model_pred = mb_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_model_pred))","execution_count":268,"outputs":[]},{"metadata":{"_uuid":"99758d92e4d86187be04d5efe3683973e3bc993c","_cell_guid":"0c1abf55-af65-49af-9636-313f740ff14f"},"cell_type":"markdown","source":"Above output tells us Mean Absolute Error for model is 33,315.93.  Need a point of comparison for whether this is good or bad.  \n\nNext step is to generate MORE models (of different types, possibly with different sets of predictors) and to compare the MAE from those models to each other.\n\n*Notes from next module*: want to avoid **overfitting** (matching training data too closely, too many branches in tree, makes poor predictions for new data) and **underfitting** (model doesn't have enough branches, doesn't accurately predict training or test data bc not specific enough) \n\nHave to figure out the RIGHT NUMBER OF BRANCHES (which we do by running the model several times with different numbers of nodes--branches--and comparing the MAE of the different models)"},{"metadata":{"_uuid":"2f1a1ba33d0f6a0c0d68e4a78b05e6c453d89cda","_cell_guid":"f1836b6f-ce90-4258-a7d1-5d3f85b5850a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Create a user-defined function that will generate a model \n# using a specified number of branches (max_leaf_nodes), \n# training- and test-set X values (train_pred and test_pred)\n# and training- and test-set Y values (train_val and test_val) values return the MAE for a model \n\ndef get_mae(max_leaf_nodes, train_pred, test_pred, train_val, test_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_pred, train_val) # fit a model using training set X and Y\n      ## this generates coefficients, presumably \n    pred_val = model.predict(test_pred) # uses coefficients from model generated on training data to predict test set Y values given test set X values\n    mae = mean_absolute_error(test_val, pred_val) # compare predicted test set Y values to ACTUAL test set Y values\n    return(mae)","execution_count":269,"outputs":[]},{"metadata":{"_uuid":"d99f023463ff68973151402def60799b380938c8","_cell_guid":"688e36e1-1c81-404f-be2e-b701d60c6453"},"cell_type":"markdown","source":"Now use the for loop to actually specify a few possible number of nodes (branches) and see how models run with those numbers of nodes compare in terms of MAE :"},{"metadata":{"_uuid":"29f01d2dfed0f3e3b79f268447846e83fb0e8e2c","_cell_guid":"54e1638b-9d37-4ca7-8adc-250e941d0624","trusted":true},"cell_type":"code","source":"for max_leaf_nodes in [5, 50, 100, 250, 500, 1000, 2000, 3000, 4000, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"With max leaf nodes: *%d* \\t\\t MAE: %d\" %(max_leaf_nodes, my_mae))","execution_count":270,"outputs":[]},{"metadata":{"_uuid":"6d05776c1a9d9f8c6d77a12ae40823c6954d417d","_cell_guid":"dc396711-b46d-42bd-b4da-3cfe78ee3eaa"},"cell_type":"markdown","source":"I see from the output above that the lowest two MAE values are for max_leaf_nodes= 50 or 100, so I'm going to rerun the above code with a different array of possible max_leaf_nodes values to see if I can pinpoint more specifically the optimal number of nodes."},{"metadata":{"_uuid":"5a7bf4daff511de06d7d0ec15edb46be9a8dbc5e","_cell_guid":"7e543434-5e1f-48fa-9063-9a495ba5c7cb","trusted":true},"cell_type":"code","source":"for max_leaf_nodes in [5, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"With max leaf nodes: *%d* \\t\\t MAE: %d\" %(max_leaf_nodes, my_mae))","execution_count":271,"outputs":[]},{"metadata":{"_uuid":"96106682145be4b08c57ae3269aa80d6e75962e1","_cell_guid":"c4ca9231-db4e-4401-8254-24143cacba89"},"cell_type":"markdown","source":"Again, optimal MAE is with around 50-75 nodes, so run again with decreased range:"},{"metadata":{"_uuid":"6d128ebbcfcb4fe8a7cefe55950d92b86acce666","_cell_guid":"c337e7c5-c776-4893-9d3c-5b5921f807c3","trusted":true},"cell_type":"code","source":"for max_leaf_nodes in [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"With max leaf nodes: *%d* \\t\\t MAE: %d\" %(max_leaf_nodes, my_mae))","execution_count":272,"outputs":[]},{"metadata":{"_uuid":"ff537c283a6f9062b00b11004020c17e981a4b22","_cell_guid":"f158a32a-cfe8-4a26-849e-a9e0810b6c0c"},"cell_type":"markdown","source":"**45** seems to be the optimal number of branches for this particular model.  So I'll re-run the model with the number of nodes set and double check that the MAE value of that model matches the value here."},{"metadata":{"_uuid":"6c5c6b90380cef7de6cccc7404d795943d608077","_cell_guid":"a2fb7f75-946e-4b26-ad64-c96783ede315","trusted":true},"cell_type":"code","source":"mb_model = DecisionTreeRegressor(max_leaf_nodes=45)\nmb_model.fit(train_X, train_y)\npred_y = mb_model.predict(val_X)\nprint(mean_absolute_error(pred_y, val_y))","execution_count":273,"outputs":[]},{"metadata":{"_uuid":"932992d3d993b5913d9fa59d98f8f06c7872c487","_cell_guid":"ebc4ffae-43d2-4cd7-806f-915c110c5a82"},"cell_type":"markdown","source":"The MAE is the same as what we got before--it should be, since it's essentially the same code, but it confirms that I can write the same thing in multiple ways (yay!).\n\nNow, to the Random Forests module (https://www.kaggle.com/dansbecker/random-forests):"},{"metadata":{"_uuid":"007f69e564e919132c3d3a61673e967a522b61a4","_cell_guid":"f82152c6-5249-4296-ad45-1f1871d82ef6","trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nRF_model = RandomForestRegressor()\nRF_model.fit(train_X, train_y)\npred_y = RF_model.predict(val_X)\nprint(mean_absolute_error(pred_y, val_y))","execution_count":275,"outputs":[]},{"metadata":{"_uuid":"1fd1a2c6254bdeae9779c507821cc0f8e5c09de6"},"cell_type":"markdown","source":"The MAE for the Random Forest model is about 3000 (units) less than the MAe for the Decision Tree model, which is good.\n\nThough it would be good to know more about what the standard values of MAEs are.\n\nNow to run another model with more predictors for submission:"},{"metadata":{"trusted":true,"_uuid":"02fea83d7ac518236b35ed7daaf6999dc67749c7"},"cell_type":"code","source":"#re-add packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\n#re-import data\ntrain = pd.read_csv(\"../input/train.csv\")\n\n#designate predictors and target\ntrain_y = train.SalePrice\n# print(train_y) ##Check\npredictor_cols = ['LotArea', 'OverallQual', 'OverallCond', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n#print(predictor_cols)  ##Check\ntrain_X = train[predictor_cols]\n\n#make the model\nsimple_model = RandomForestRegressor()\nsimple_model.fit(train_X, train_y)\n\n#import values from test dataset\ntest = pd.read_csv(\"../input/test.csv\")\n\n#use model to predict\ntest_X = test[predictor_cols] #USING SAME \"predictor_cols\" array here requires that test dataset have columns with EXACT same names\nsimple_pred = simple_model.predict(test_X)\nprint(simple_pred)","execution_count":287,"outputs":[]},{"metadata":{"_uuid":"c416f4eff92ed9b0ff028af2370f2544bc35878f"},"cell_type":"markdown","source":"Create submission file:"},{"metadata":{"_kg_hide-output":false,"trusted":true,"collapsed":true,"_uuid":"eaf4f67d3ef47acf17f46b747b71ee4340ff2063"},"cell_type":"code","source":"subm = pd.DataFrame({'Id': test.Id, 'SalePrice': simple_pred})\nsubm.to_csv('emlini_simplepred.csv', index=False)","execution_count":292,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d5ecd151f74d694785a604d2d977ad3ca8d0467"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}