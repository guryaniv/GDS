{"cells":[{"metadata":{"trusted":true,"_uuid":"49bbecd5ce61569a3cc13a7017a34504aa625981","scrolled":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport torch\nimport torchtext\nfrom torchtext import data\nimport spacy\nimport os\nimport re\n\n\nos.environ['OMP_NUM_THREADS'] = '4'\nmy_tok = spacy.load('en')\nmy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nmy_stopwords.update(['wikipedia','article','articles','im','page'])\n\ndef spacy_tok(x):\n    x= re.sub(r'[^a-zA-Z\\s]','',x)\n    x= re.sub(r'[\\n]',' ',x)\n    return [tok.text for tok in my_tok.tokenizer(x)]\n\n# print(spacy_tok(\"I solve Kaggle\"))\n# print(spacy_tok(\"I solve Kaggle5\"))\n# print(spacy_tok(\"I slove Kagle 5 43,....\"))\n\nTEXT = data.Field(lower=True, tokenize=spacy_tok,eos_token='EOS',stop_words=my_stopwords,include_lengths=True)\nLABEL = data.Field(sequential=False, \n                         use_vocab=False, \n                         pad_token=None, \n                            unk_token=None)\n\ndataFields = [(\"id\", None),\n                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n                 (\"obscene\", LABEL), (\"insult\", LABEL),\n                 (\"identity_hate\", LABEL)]\n\ndataset= data.TabularDataset(path='../input/train.csv', \n                                            format='csv',\n                                            fields=dataFields, \n                                            skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c9f064901f818dbab0845bcaf40fa008cbcfd59"},"cell_type":"code","source":"train,val= dataset.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b7b5a15d5c02eda2cbf3a19ff58631cb4cb8364"},"cell_type":"code","source":"TEXT.build_vocab(train,vectors='fasttext.simple.300d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a02a7e5f0a69af204327bab5a0e3cc35073d0dc"},"cell_type":"code","source":"traindl, valdl = torchtext.data.BucketIterator.splits(datasets=(train, val),\n                                            batch_sizes=(128,1024),\n                                            sort_key=lambda x: len(x.comment_text),\n                                            device=torch.device('cuda:0'),\n                                            sort_within_batch=True\n                                                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2db6e541ad3d414f194ca194d96d538846481cbe"},"cell_type":"code","source":"vectors= train.fields['comment_text'].vocab.vectors.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"025e85420e30f04fd048b9e739e2abae48fcf75b"},"cell_type":"code","source":"class BatchGenerator:\n    def __init__(self, dl):\n        self.dl = dl\n        self.yFields= ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n        self.x= 'comment_text'\n        \n    def __len__(self):\n        return len(self.dl)\n    \n    def __iter__(self):\n        for batch in self.dl:\n            X = getattr(batch, self.x)\n            y = torch.transpose( torch.stack([getattr(batch, y) for y in self.yFields]),0,1)\n            yield (X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0babf2adf42a715dbf339db4e5bf342e6b72cef"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fc150a912cb4e11c28b6e2690a7a9a35f11b07e"},"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self,op_size,n_tokens,pretrained_vectors,nl=2,bidirectional=True,emb_sz=300,n_hiddenUnits=100):\n        super(MyModel, self).__init__()\n        self.n_hidden= n_hiddenUnits\n        self.embeddings= nn.Embedding(n_tokens,emb_sz)\n        self.embeddings.weight.data.copy_(pretrained_vectors)\n#         self.embeddings.weight.requires_grad = False\n        self.rnn= nn.LSTM(emb_sz,n_hiddenUnits,num_layers=2,bidirectional=True,dropout=0.2)\n        self.lArr=[]\n        if bidirectional:\n            n_hiddenUnits= 2* n_hiddenUnits\n        self.bn1 = nn.BatchNorm1d(num_features=n_hiddenUnits)\n        for i in range(nl):\n            if i==0:\n                self.lArr.append(nn.Linear(n_hiddenUnits*3,n_hiddenUnits))\n            else:\n                self.lArr.append(nn.Linear(n_hiddenUnits,n_hiddenUnits))\n        self.lArr= nn.ModuleList(self.lArr)\n        self.l1= nn.Linear(n_hiddenUnits,op_size)\n        \n    def forward(self,data,lengths):\n        torch.cuda.empty_cache()\n        bs= data.shape[1]\n        self.h= self.init_hidden(bs)\n        embedded= self.embeddings(data)\n        embedded= nn.Dropout()(embedded)\n#         embedded = pack_padded_sequence(embedded, torch.as_tensor(lengths))\n        rnn_out, self.h = self.rnn(embedded, (self.h,self.h))\n#         rnn_out, lengths = pad_packed_sequence(rnn_out,padding_value=1)\n        avg_pool= F.adaptive_avg_pool1d(rnn_out.permute(1,2,0),1).view(bs,-1)\n        max_pool= F.adaptive_max_pool1d(rnn_out.permute(1,2,0),1).view(bs,-1)\n        ipForLinearLayer= torch.cat([avg_pool,max_pool,rnn_out[-1]],dim=1)\n        for linearlayer in self.lArr:\n            outp= linearlayer(ipForLinearLayer)\n            ipForLinearLayer= self.bn1(F.relu(outp))\n            ipForLinearLayer= nn.Dropout(p=0.6)(ipForLinearLayer)\n        outp = self.l1(ipForLinearLayer)\n        del embedded;del rnn_out;del self.h;\n        torch.cuda.empty_cache()\n        return outp\n        \n    def init_hidden(self, batch_size):\n        return torch.zeros((4,batch_size,self.n_hidden),device=\"cuda:0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550f4db545e01f07154e4b7060ede4f81b2f4c0c"},"cell_type":"code","source":"def getValidationLoss(valdl,model,loss_func):\n    model.eval()\n    runningLoss=0\n    valid_batch_it = BatchGenerator(valdl)\n    allPreds= []\n    allActualPreds= []\n    with torch.no_grad():\n        for i,obj in enumerate(valid_batch_it):\n            obj= ( (obj[0][0].cuda(),obj[0][1].cuda()),obj[1] )\n            preds = model(obj[0][0],obj[0][1])\n            loss = loss_func(preds,obj[1].float())\n            runningLoss+= loss.item()\n            allPreds.append(preds.detach().cpu().numpy())\n            allActualPreds.append(obj[1].detach().cpu().numpy())\n        rocLoss= roc_auc_score(np.vstack(allActualPreds),np.vstack(allPreds))\n        return runningLoss/len(valid_batch_it),rocLoss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31f575aa2949aa63542153eb1cd248e09869b467","scrolled":true},"cell_type":"code","source":"import torch.optim as optim\nfrom torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\ndef oneEpoch(lr):\n    train_batch_it = BatchGenerator(traindl)\n    opt = optim.Adam(model.parameters(),lr)\n    runningLoss= 0\n    allPreds=[]\n    allActualPreds=[]\n    for i,obj in enumerate(train_batch_it):\n        obj= ( (obj[0][0].cuda(),obj[0][1].cuda()),obj[1] )\n        model.train()\n        opt.zero_grad()\n        preds = model(obj[0][0],obj[0][1])\n        loss = loss_func(preds,obj[1].float())\n        runningLoss+= loss.item()\n        loss.backward()\n        opt.step()\n        allPreds.append(preds.detach().cpu().numpy())\n        allActualPreds.append(obj[1].detach().cpu().numpy())\n        del obj;del preds\n    trainRocLoss= roc_auc_score(np.vstack(allActualPreds),np.vstack(allPreds))\n    runningLoss= runningLoss/len(train_batch_it)\n    valLoss,valRocLoss= getValidationLoss(valdl,model,loss_func)\n    torch.cuda.empty_cache()\n    return runningLoss,valLoss,trainRocLoss,valRocLoss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"623a73b3e236f1e4ebb4b4cff0f443480256ec21","scrolled":true},"cell_type":"code","source":"epochs= 10\ntrainLossArr=[]\nvalLossArr=[]\nrocTrainLoss=[]\nrocValLoss=[]\nmodel= MyModel(6,len(TEXT.vocab),vectors,1)\nloss_func= torch.nn.BCEWithLogitsLoss()\nmodel = model.cuda()\nfor i in range(epochs):\n    %time tLoss,vLoss,tRocLoss,vRocLoss= oneEpoch(1e-4)\n    print(f\"Epoch - {i}\")\n    print(f\"Train Loss - {tLoss} vs Val Loss is {vLoss}\")\n    print(f\"Train ROC - {tRocLoss} vs Val ROC is {vRocLoss}\")\n    trainLossArr.append(tLoss)\n    valLossArr.append(vLoss)\n    rocTrainLoss.append(tRocLoss)\n    rocValLoss.append(vRocLoss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9d06ff2315c56eb999070afc5b4c5f1b03c5986","scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nplt.plot(trainLossArr,color='b')\nplt.plot(valLossArr,color='g')\nplt.plot(rocTrainLoss,color='r')\nplt.plot(rocValLoss,color='c')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33135cabe4844c92d97e1e41b45b30bba89932b3"},"cell_type":"code","source":"torch.save(model.state_dict(), \"myFirstModel1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0042ac1157cbc04deae076bb2cf8621bd2d25847"},"cell_type":"code","source":"dataFields = [(\"id\", None),\n                 (\"comment_text\", TEXT)\n             ]\n\ntestDataset= data.TabularDataset(path='../input/test.csv', \n                                            format='csv',\n                                            fields=dataFields, \n                                            skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e27e788427de804976c2a47af3f02a6dfee8a70"},"cell_type":"code","source":"test_iter1 = torchtext.data.Iterator(testDataset, batch_size=32, device=torch.device('cuda:0'), sort=False, sort_within_batch=False, repeat=False,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63f0074095fcd3cfa58187ee40dbb3d77bf74b2a"},"cell_type":"code","source":"testDF= pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8b5e385d5ab3cfc7c0dddb6d46fdd4f4f59399a"},"cell_type":"code","source":"myPreds=[]\nwith torch.no_grad():\n    model.eval()\n    for obj in test_iter1:\n#         print(torch.transpose(obj.comment_text[0],0,1)[:10].shape)\n#         text= torch.transpose(obj.comment_text[0],0,1)[:2]\n#         for t in text:\n#             print( ' '.join([TEXT.vocab.itos[i] for i in t]) )\n#         break\n        torch.cuda.empty_cache()\n        pred= model(obj.comment_text[0],obj.comment_text[1])\n        pred= torch.sigmoid(pred)\n        myPreds.append(pred.cpu().numpy())\n        del pred;del obj;\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b459956ad5ceca9fb8d5a527ea3b25a4ef6944fe"},"cell_type":"code","source":"myPreds= np.vstack(myPreds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f11298a9c9f0cfe11a4c4cac1281d7ecec0469dc"},"cell_type":"code","source":"for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n    testDF[col] = myPreds[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0e2df76358640a0d7515eff7cdd94e01b42693b"},"cell_type":"code","source":"testDF.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}