{"cells":[{"metadata":{"_uuid":"23f77d02be4027e9c629e69c9c262b99fd07db54"},"cell_type":"markdown","source":"# My first step in EDA using Plotly\n<br>\nIn this Kernel I am going to share with you my first step of EDA only on  training set:\n> - Plot target variable distribution\n> - Plot distribution for all features using histogram subplots\n> - Compare distribution of target variable for all features using histogram subplots\n\nThe main purpose of this kernel is not to extract insights which are very welcome in comments,\ninstead it was to understand how to use Plotly to create visualizations.\n\nI hope this can be helpful to you as well."},{"metadata":{"_uuid":"d72a93811d0de3cdfb8b32ef4ced4d1645c3e4d2"},"cell_type":"markdown","source":"## I) Setup the environment"},{"metadata":{"trusted":true,"_uuid":"7a37913ea77fc36385bf275e72e82e340272a8d0"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom math import ceil\n\nimport plotly\nimport plotly.plotly as py\n\n# Allows to plot offline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport plotly.graph_objs as go\n\n# Allows to plot into the notebook\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a267e4ceacdcacd3df89ac7634ad64d1713d3e"},"cell_type":"markdown","source":"To make code more readable, I need to use a module with some functions I created.<br>\nThe code is hidden in the next code cell. Since it is quite long, I suggest you to keep it hidden."},{"metadata":{"trusted":true,"_uuid":"7d6358c5b1ea64d525be2684159c5d2922ec4fea","_kg_hide-input":true},"cell_type":"code","source":"def hist_sub_plots(start, stop, overlayed = False):\n\n    # Set the default layout\n    layout = reset_layout()\n\n    # Create subplot grid\n    cols = 5\n        # this was to automatically define the number of nrows to plot all hist in one figure\n        # however, the plot got too much squeezed and is better\n#     nrows = ceil(len(train_df.columns[2 : ]) / 2)\n    rows = 5\n    from plotly import tools\n    fig = tools.make_subplots(rows=rows, cols=cols, shared_yaxes=True, print_grid = False,\n                          #subplot_titles=(train_df.columns[2 : 2 + (rows * cols)]))\n                            subplot_titles=(train_df.columns[1 + start : 1 + stop]))\n\n\n    if overlayed == False:\n        all_traces = {start + trace : create_hist(col) for trace, col in enumerate(train_df.columns[1 + start : 1 + stop])}\n\n        plot = 0\n        for i in range(rows):\n            for j in range(cols):\n                try:\n                    fig.append_trace(all_traces[start + plot], i + 1, j + 1)\n                    plot += 1\n#               if plot == len(train_df.columns[2 : ]):\n                except:\n                    fig['layout'].update(height=1000, width=1000,\n                                         title='Multiple Subplots with Shared Y-Axes',\n                                        showlegend = False)\n                    iplot(fig, filename='multiple-subplots-shared-yaxes')\n                    return\n\n        fig['layout'].update(height=1000, width=1000,\n                             title='Multiple Subplots with Shared Y-Axes',\n                             showlegend = False)\n        iplot(fig, filename='multiple-subplots-shared-yaxes')\n\n        return\n\n\n    elif overlayed == True:\n\n        traces_detection = {start + trace: create_hist(col, overlayed = True, target = 1) for trace, col in enumerate(train_df.columns[1 + start : 1 + stop])}\n        traces_no_detection = {start + trace: create_hist(col, overlayed = True, target = 0) for trace, col in enumerate(train_df.columns[1 + start : 1 + stop])}\n\n        plot = 0\n        for i in range(rows):\n            for j in range(cols):\n                try:\n                    fig.append_trace(traces_no_detection[start + plot], i + 1, j + 1)\n                    fig.append_trace(traces_detection[start + plot], i + 1, j + 1)\n                    plot += 1\n                except:\n                #if plot == len(train_df.columns[2 : ]):\n                    fig['layout'].update(height=1000, width=1000,\n                             title='Multiple Subplots with Shared Y-Axes',\n                             barmode = \"overlay\",\n                             showlegend = False)\n                    iplot(fig, filename='multiple-subplots-shared-yaxes')\n                    return\n\n        fig['layout'].update(height=1000, width=1000,\n                             title='Multiple Subplots with Shared Y-Axes',\n                             barmode = \"overlay\",\n                             showlegend = False)\n        iplot(fig, filename='multiple-subplots-shared-yaxes')\n\n        return\n\n################################################################################\n\ndef create_hist(col, overlayed = False, target = None):\n    if overlayed == False:\n        trace = go.Histogram(x = train_df[col],\n                    histnorm = \"probability\",\n                    marker = dict(color = 'rgb(191, 190, 190)'),\n                    name = col)\n    elif overlayed == True:\n        if target == 0:\n            trace = go.Histogram(x =  train_df[train_df[\"target\"] == 0][col],\n                      histnorm = \"probability\",\n                      marker = dict(color = 'rgb(191, 190, 190)'),\n                      opacity = 1,\n                    name = \"No Detection\"\n                     )\n        elif target == 1:\n            trace = go.Histogram(x = train_df[train_df[\"target\"] == 1][col],\n                      histnorm = \"probability\",\n                      marker = dict(color = \"rgba(255,255,255,0)\",\n                                    line = dict(width = 1, color = 'rgb(195, 81, 78)')),\n                      name = \"Detection\"\n                     )\n    return trace\n\n################################################################################\n\ndef reset_layout():\n    layout = go.Layout(\n        width=600,\n        height=800,\n        yaxis=dict(\n                        range = [0, 0.6],\n                        showline = True,\n                        tickmode = \"array\",\n                        tickvals = [val/100 for val in range(0,101,20)],\n                        ticktext = [str(text/100) + \"%\" for text in range(0,101,20)],\n                        color = \"grey\",\n                        tickfont = dict(\n                                        size = 15\n                                        ),\n                        zeroline = False\n        )\n    )\n\n    return layout\n\n################################################################################\n\ndef desc_stats(df, var_type):\n    '''\n    type df: pandas.core.frame.DataFrame\n    type var_type: string\n    type r: pandas.core.frame.DataFrame\n    '''\n    import sys\n    import pandas as pd\n    \n    if var_type not in [\"category\", \"number\"]:\n        if var_type.find(\"categ\") > -1:\n            raise ValueError(\"Maybe you meant category and not {}\".format(var_type))\n        elif var_type.find(\"num\") > -1:\n            raise ValueError(\"Maybe you meant number and not {}\".format(var_type))\n        raise ValueError(\"Summary statistics available only for {} variables\".\\\n                         format(\" or \".join([\"categorical\", \"numerical\"])))\n\n\n    if var_type == \"category\":\n\n        # Count number of variables to describe\n        n = len(df.select_dtypes(\"category\").columns)\n\n        # Create structure of the dataframe for categoricl descriptive statistics\n        desc_stats = pd.DataFrame(columns = [\"Feature\", \"Type\", \"% missing\", \"Unique values\", \"Mode\", \"% mode\"])\n\n        for i, colname in enumerate(df.select_dtypes(\"category\").columns):\n\n            # Compute freq in order to find biggest category\n            freq = df[colname].\\\n                                value_counts(normalize = True, dropna = False).\\\n                                to_frame().reset_index().\\\n                                iloc[0, : ]\n\n            # Create a temporary dictionary to append the desc_stats for categorical features\n            temp = {\"Feature\": colname,\n                   \"Type\" : str(df[colname].dtype),\n                   \"% missing\": round(df[colname].isnull().sum() * 100 / df.shape[0], 2),\n                   \"Unique values\": df[colname].nunique(),\n                    \"Mode\": freq[0],\n                    \"% mode\": round(freq[1] * 100, 2)\n                   }\n\n            # Append to desc_stats\n            desc_stats = desc_stats.append(temp, ignore_index = True)\n\n\n            # Print out progress\n            sys.stdout.write(\"\\rProgress: {:2.2%}\".format( (i + 1) / n) )\n            sys.stdout.flush()\n\n    elif var_type == \"number\":\n\n        # Import relevant packages for calculations\n        from  statistics import median, variance, stdev, mean\n        from scipy.stats import kurtosis, skew\n\n        # Count number of variables to describe\n        n = len(df.select_dtypes(\"number\").columns)\n\n        # Create structure of the dataframe for categoricl descriptive statistics\n        desc_stats = pd.DataFrame(columns = [\"Feature\", \"Type\", \"% missing\",\n                                             \"Mean\", \"Median\",\n                                             \"Min\", \"Max\", \"Range\", \"StDev\", \"Variance\",\n                                             \"Skewness\", \"Kurtosis\"])\n\n        for i, colname in enumerate(df.select_dtypes(\"number\").columns):\n\n            #Create a temporary dictionary to append the desc_stats for numeric features\n            temp = {\"Feature\": colname,\n                    \"Type\": str(df[colname].dtype),\n                    \"% missing\": round(df[colname].isnull().sum() * 100 / df.shape[0], 2),\n                    \"Mean\": round(df[colname].dropna().mean(), 2),\n                    \"Median\": median(df[colname].dropna()),\n                    \"Min\": min(df[colname].dropna()),\n                    \"Max\": max(df[colname].dropna()),\n                    \"Range\": max(df[colname].dropna()) -  min(df[colname].dropna()),\n                    \"StDev\": stdev(df[colname].dropna()),\n                    \"Variance\": variance(df[colname].dropna()),\n                    \"Skewness\": round(skew(df[colname].dropna()), 2),\n                    \"Kurtosis\": round(kurtosis(df[colname].dropna()), 2)\n                   }\n\n            # Append to desc_stats\n            desc_stats = desc_stats.append(temp, ignore_index = True)\n\n            # Print out progress\n            sys.stdout.write(\"\\rProgress: {:2.2%}\".format( (i + 1) / n) )\n            sys.stdout.flush()\n\n\n    return desc_stats.sort_values(by = \"% missing\", ascending = False)\n\n################################################################################\n\ndef write_csv(file, outdir : str):\n    filename = outdir.split(\"/\")[-1]\n    if outdir.find(\"/\"):\n        import os\n        directory = \"/\".join(outdir.split(\"/\")[ : -1])\n        if not os.path.exists(directory):\n                os.makedirs(directory)\n    file.to_csv(outdir, index = False)\n\n################################################################################\n\ndef reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    type df: pandas.core.frame.DataFrame\n    type r: pandas.core.frame.DataFrame\n    \"\"\"\n    # This function takes df as input and return a df for which numeric dtypes have been optimized\n    import numpy as np\n    import pandas as pd\n    import re\n\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n\n    # Memory occupied by df before dtypes transformation\n    start_mem_usg = df.memory_usage(deep=True).sum() / 1024**2\n    print(\"Memory usage of input dataframe is: {:5.2f} MB\".\n          format(start_mem_usg))\n\n    for col in df.columns:\n\n        # Extract dtypes as string and remove numbers\n        col_type = re.findall(\"([a-z]+)\", str(df[col].dtypes))[0]\n\n        if col_type == \"object\":\n            df[col] = df[col].astype(\"category\")\n\n        # use extract only root type from numerics list\n        elif col_type in [re.findall(\"([a-z]+)\", dtype)[0] for dtype in numerics]:\n            # Print current column type\n#             print(\"******************************\")\n#             print(\"Column: \", col)\n#             print(\"dtype before: \", df[col].dtype)\n\n            # Obtain minimum and maximum values for df[col]\n            # Used later to understand which is the most suitable dtype\n            has_nan = False if df[col].isnull().any() == False else True\n            c_Min, c_Max = df[col].\\\n                                        agg([\"min\", \"max\"]).\\\n                                        tolist()\n\n\n            # Since col_type already contains type str, no need of further transfomations\n            if col_type == \"int\":\n\n                # If df[col] is only positive, use np.uint classes\n                if c_Min >= 0:\n                    if c_Max < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif c_Max < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif c_Max < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif c_Max < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n\n                else:\n\n                  # Check the minimum int size that can store the column\n                    if c_Min > np.iinfo(np.int8).min and c_Max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_Min > np.iinfo(np.int16).min and c_Max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_Min > np.iinfo(np.int32).min and c_Max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_Min > np.iinfo(np.int64).min and c_Max < np.iinfo(np.int64).max:  #not clear why not use else:\n                        df[col] = df[col].astype(np.int64)\n\n            elif col_type == \"float\":\n\n                # Check the minimum float size that can store the column\n                if c_Min > np.finfo(np.float16).min and c_Max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_Min > np.finfo(np.float32).min and c_Max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n\n        # Print new column dtype\n#         print(\"dtype after: \", df[col].dtype)\n#         print(\"******************************\")\n\n    # Compute memory of the resulting df\n    end_mem_usg = df.memory_usage(deep = True).sum() / 1024**2\n\n    if verbose:\n        print(\"Mem. usage decreased to {:5.2f} MB ({:.1f}% reduction)\".\n             format(end_mem_usg, 100 * (start_mem_usg - end_mem_usg) / start_mem_usg))\n\n    write_csv(df.dtypes, \"dtypes/dtypes.csv\")\n\n################################################################################\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"909a4985fade9183f598592949a388082d12677a"},"cell_type":"markdown","source":"I want to see all rows and all columns."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 350\nprint(\"Display max rows set to 100\")\npd.set_option('display.max_columns', None)\nprint(\"Display all columns\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2000f65b370f9c50aa15fc7568c399875e5c052"},"cell_type":"markdown","source":"## II) Import the training dataset"},{"metadata":{"trusted":true,"_uuid":"a7a6ada596474d2be88eefb48d8da9568012df69"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', index_col = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b0cd0a2e1edf83c4ce4e72be9014542acb2f4270"},"cell_type":"code","source":"train_df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7874731774f21039e74845251a61a9945f14fee"},"cell_type":"markdown","source":"## III) First steps toward EDA"},{"metadata":{"_uuid":"3ac1364834adf4dcf4666620dd29b42b82927b88"},"cell_type":"markdown","source":"### 0) Summary Statistics"},{"metadata":{"_uuid":"8b1a99a63defeda27cbcea0931255fbe1cfc2739"},"cell_type":"markdown","source":"I decided to extract summary statistics describing each variable."},{"metadata":{"trusted":true,"_uuid":"070a8013965cbd3f39099d380b7c413239074feb","_kg_hide-input":true},"cell_type":"code","source":"try:\n    num_features = pd.read_csv(\"desc_stats/numerical.csv\")\nexcept:\n    num_features = desc_stats(train_df, \"number\")\n    write_csv(num_features, \"desc_stats/numerical.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"006eac4924a22de692e4fd343ac28d2d1d329426"},"cell_type":"code","source":"num_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2075b7dfd13e9abf04c158241af151158a753ac5"},"cell_type":"markdown","source":"### 1) Target variable distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"42732b1ece1967bc330fab838ac85a7c99300167"},"cell_type":"code","source":"temp =  (train_df[\"target\"].\\\n                    value_counts(normalize = True) * 100).\\\n                    to_frame().\\\n                    reset_index().\\\n                    sort_values(\"index\", ascending = True)\n\ntrace0 = go.Bar(\n        x = [\"HasDetections(0)\", \"HasDetections(1)\"],\n        y = temp[\"target\"],\n        width = [0.55, 0.55],\n        marker = dict(\n                        color=['rgb(191, 190, 190)', 'rgb(195, 81, 78)']\n                    )\n        )\n\ndata = [trace0]\n\nlayout = go.Layout(\n    width=500,\n    height=500,\n    title = dict(\n                    text = \"target\",\n                    x = 0.07,\n                    y = 0.90,\n                    font = dict(\n                                size = 25\n                                )\n                ),\n\n    xaxis = dict(\n                    showline = False,\n                    showgrid = False,\n                    zeroline = True,\n                    tickfont = dict(\n                                    size = 20\n                                    )\n                ),\n    yaxis=dict(\n                    range = [0, 100],\n                    showline = True,\n                    tickmode = \"array\",\n                    tickvals = [val for val in range(0,101,20)],\n                    ticktext = [str(text) + \"%\" for text in range(0,101,20)],\n                    color = \"grey\",\n                    tickfont = dict(\n                                    size = 15\n                                    ),\n                    zeroline = False\n                )\n    )\n\n\nannotations = []\nfor i in range(0, temp.shape[0]):\n    annotations.append(dict(x=temp[\"index\"][i], y=temp.target[i], text=str(round(temp.target[i], 2)) + \"%\",\n                            font=dict(family='Arial', size=25,\n                                      color='white'),\n                            showarrow=False,\n                            yshift = -15\n                                       ))\n    layout['annotations'] = annotations\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='target')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58e614896d5ca8ed1eefd4aa77df79b05394dc6"},"cell_type":"markdown","source":"### 2) Histogram subplots\ny-axis represents the relative frequency of the bin."},{"metadata":{"trusted":true,"_uuid":"13d571ab09ff05bdb6bfeaaaf77a1b14c74b05c1","scrolled":false},"cell_type":"code","source":"i = 0\nn_plots = 25\nwhile i < (len(train_df.columns[1 : ])):\n    hist_sub_plots(start = i + 0, stop = i + n_plots, overlayed = False)\n    i += n_plots","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f8e6ed0c923de852e69c9c077219269318afc8a"},"cell_type":"markdown","source":"### 3) Overlayed Histogram subplots\n<br>\nIn the next visualization, for each features, I am comparing the distribution of target 0 (gray histogram) to the distribution of target 1 (red line)<br>\ny-axis represents the relative frequency of the bin."},{"metadata":{"trusted":true,"_uuid":"721a9ee9ba1521ad34f2f76b9f3a00df0ba611f4"},"cell_type":"code","source":"i = 0\nn_plots = 25\nwhile i < (len(train_df.columns[1 : ])):\n    hist_sub_plots(start = i + 0, stop = i + n_plots, overlayed = True)\n    i += n_plots","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91067f06dd43b483b4fbd9433efc2437f5a514c9"},"cell_type":"markdown","source":"### IV) Next steps\n<br>\nThe next steps I want to follow are:\n1. plot correlation matrix (Pearson & Spearman)\n2. measure association of each variable to target variable\n3. try to cluster features based on Summary Statistics\n4. compute Summary Statistics by splitting each feature distribution depending on target value\n\n5. calculate results also on test set and understand how similar/dissimilar are the two datasets"},{"metadata":{"trusted":true,"_uuid":"58937d0728144ad666a873142b4699d424bb2063"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}