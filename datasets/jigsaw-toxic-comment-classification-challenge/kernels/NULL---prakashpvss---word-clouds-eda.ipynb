{"cells": [{"metadata": {}, "cell_type": "markdown", "source": ["This kernel is to show wordclouds associated with different types of Toxic behaviour in comments.\n", "More of an exploratory analysis"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "05817188bf4597f318bd5a182f8f104fb8c8b454", "collapsed": true, "_cell_guid": "81211fb8-5e3c-4b0d-8d20-048d3f057d19"}, "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "train = pd.read_csv('../input/train.csv')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["train.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": ["severe_toxic = train[train.severe_toxic==1]"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["severe_toxic.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["from keras.preprocessing.text import Tokenizer\n", "tokenizer = Tokenizer(num_words=5000)\n", "tokenizer.fit_on_texts(train.comment_text)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["word_to_id = tokenizer.word_index\n", "id_to_word = {value:key for key,value in word_to_id.items()}\n", "\n", "texts = train.comment_text.tolist()\n", "sequences = tokenizer.texts_to_sequences(texts)\n", "print(' '.join(id_to_word[id] for id in sequences[1] ))\n", "cleanText = []\n", "for seq in sequences:\n", "    c = ' '.join(id_to_word[id] for id in seq)\n", "    cleanText.append(c)\n", "train['comment_processed'] = cleanText \n", "\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": ["severe_toxic = train[train.severe_toxic==1]"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["severe_toxic.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": ["from wordcloud import WordCloud, STOPWORDS"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["import matplotlib.pyplot as plt\n", "plt.figure(figsize=(10,10))\n", "wc = WordCloud(background_color=\"black\", \n", "                stopwords=STOPWORDS,width=2500, height=1800)\n"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["wc.generate(\" \".join(severe_toxic.comment_processed))\n", "plt.title(\"Severe Toxic\", fontsize=50)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["toxic = train[(train.toxic==1) & (train.severe_toxic==0) & (train.obscene==0) &(train.threat == 0) &(train.identity_hate == 0) &(train.insult == 0)]\n", "wc.generate(\" \".join(toxic.comment_processed))\n", "plt.title(\"Toxic\", fontsize=50)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["obscene = train[(train.toxic==0) & (train.severe_toxic==0) & (train.obscene==1) &(train.threat == 0) &(train.identity_hate == 0) &(train.insult == 0)]\n", "wc.generate(\" \".join(obscene.comment_processed))\n", "plt.title(\"obscene\", fontsize=40)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17))\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["threat = train[(train.toxic==0) & (train.severe_toxic==0) & (train.obscene==0) &(train.threat == 1) &(train.identity_hate == 0) &(train.insult == 0)]\n", "wc.generate(\" \".join(threat.comment_processed))\n", "plt.title(\"Threat\", fontsize=40)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17))\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["ih = train[(train.toxic==0) & (train.severe_toxic==0) & (train.obscene==0) &(train.threat == 0) &(train.identity_hate == 1) &(train.insult == 0)]\n", "wc.generate(\" \".join(ih.comment_processed))\n", "plt.title(\"Identity Hate\", fontsize=40)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17))\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["insult = train[(train.toxic==0) & (train.severe_toxic==0) & (train.obscene==0) &(train.threat == 0) &(train.identity_hate == 0) &(train.insult == 1)]\n", "wc.generate(\" \".join(insult.comment_processed))\n", "plt.title(\"Insult\", fontsize=40)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17))\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {}, "source": ["NotToxic = train[(train.toxic==0) & (train.severe_toxic==0) & (train.obscene==0) &(train.threat == 0) &(train.identity_hate == 0) &(train.insult == 0)]\n", "wc.generate(\" \".join(NotToxic.comment_processed))\n", "plt.title(\"NotToxic\", fontsize=40)\n", "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17))\n", "plt.axis('off')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1}