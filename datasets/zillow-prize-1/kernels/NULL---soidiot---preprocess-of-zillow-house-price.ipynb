{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## 1. Business Understanding\n", "\n", "**The Overview**\n", "\n", "In this table dataset pre-processing and visualization lab, I will use the dataset that's from kaggle competition: Zillow\u2019s Home Value Prediction. Zillow's Zestimates are the estimate of home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. More details about zillow estimate's introduction can be found: https://www.zillow.com/zestimate/ . The final result and purpose of this dataset is the accurate prediction of log(Zillow estimate propeties' price)-log(actual transaction price). In this lab, I just focused on the preprocessing ,visulization of variables' relationships and dimensional reduction as preparation of following models'construction. The possible models to predict the logerror are Multi-Layer Perceptron , Convolutional Networks and Recurrent Networks\n", "\n", "#### The Bussiness Purposes:\n", "The accurate estimate of the logerror between zillow website's price and actual transaction price is very important. \n", "why? we can get the reasons from following cases:\n", "1. For Zillow website, if pepole can figure out a model, based on CNN or RNN, the Zillow website can modify their large error variable in their computational function by checking the weight of different variables. The Zillow website price function can be found here: https://www.zillow.com/research/zestimate-forecast-methodology/ . Moreover, the Zillow website can find a better way to model the estimate function after discovering this data for this data covers a lot of features related to properties. The Zillow webstite can delete duplicated or add features in Zestimate function in the process of data mining and machine learning. \n", "2. For some house sellers and investors, houses' price prediction is important. A better estimate of house price will help them to avoid loss of money in the properties' transaction. Usually, before investors make the decision to sell or buy a house, they will take the important estimate websites' results as criterion. Too big logerror will actually led to large amount loss.  Considering a house which can be sold at price 100,0000 after 6 monthes, if the Zestimate has 0.1 logerror under estimate, the seller will lost 20,0000 if they sell the house at the estimated price now.\n", "3. For families, they will be more careful about specific type of properties' estimate accuracy. Usually, they are just focused on houses or apartments that are suitable for family living. So the improvement of these kind of properties' price estimation will make more people get benefit from this website.\n", "4. For government officers, a better estimate of propeties' transaction price in different time will benefit them to calculate the properties of person or companies and also will be helpful for officers to forcast the financial development in different places and make more financial plans based on these predictional results.\n", "\n", "\n", "**The Steps for Lab one:**\n", "1. Analyze the data features' properties based on the information provied in the website. \n", "2. Analyze the data's quality based on the missing, duplicated values information.\n", "3. Analyze the disctribution of important variables.\n", "4. Discover relationships between important variables and do the dimension reduction to get most important variables.\n", "\n", "\n", "**The expectant result: How well the algorithm to perform to be useful:**\n", "\n", "Based on the information provied on the Zillow website: https://www.zillow.com/zestimate/#acc , we can know that the distribution of logerror is like bellowing: \n", "\n", "WITHIN 5% OF SALE PRICE:\n", "This is the percentage of transactions in a location for which the Zestimate was within 5% of the transaction price. Nationwide, Zestimates are currently within 5% of the final sale price 54.4% of the time.\n", "\n", "WITHIN 20% OF SALE PRICE:\n", "This is the percentage of transactions in a location for which the Zestimate was within 20% of the transaction price. Nationally, Zestimates are currently within 20% of the final sale price 86.9% of the time.\n", "\n", "So, if our target is to improve the estimate then the estimate of more than 86.9% properties have less than 20% error will be considered as better estimate. But for this problem, the comptetion holder want people to get accurate estimate of the error of Zestimate. That means the smaller difference of log(zillow estimate)-log(actual price) and my error estimate, the better result it will be. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Data Understanding "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### First Let's import data from dataset to python, which contains two tables: \n", "#### The dataset can be found in website: https://www.kaggle.com/c/zillow-prize-1/data  please download it to see the iploty pictures in jupyter notebook.\n", " 1) train_2016 (90275 rows \u00d7 3 columns): the result of log(zillowestimate price)-log(actual transaction price) in 2016 \n", "\n", " 2) properties_2016 (2985217 rows \u00d7 57features): table the features'data of different properties. some features are highly related and some are useless for our analysis. I will first make groups based on their definition,then do the distribution and relationship analysis."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["# import data from kaggle\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import dicom\n", "import os\n", "import seaborn as sns\n", "from operator import itemgetter\n", "from mpl_toolkits.mplot3d import Axes3D\n", "from scipy.stats import gaussian_kde\n", "import warnings\n", "import plotly \n", "from plotly import __version__\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "init_notebook_mode(connected=True)\n", "plotly.tools.set_credentials_file(username='ZhangAnyu', api_key='CogwNQlPuCThZLOjAOZQ')\n", "warnings.filterwarnings('ignore')\n", "train_2016=pd.read_csv('/Users/MacBook/Desktop/machinlearning/lab1/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\n", "properties_2016=pd.read_csv('/Users/MacBook/Desktop/machinlearning/lab1/properties_2016.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1) table of logerror in 2016"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["train_2016.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This table contains three types of data: \n", "\n", "1) parcelid (integer): the ID number of each properties\n", "\n", "2) logerror (float): log(Zillow estimate price)-log(actual transaction price)\n", "\n", "3) transectiondata (object): the year-month-date of actual transaction\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Table logerrror: Data Quality "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Data Quality check is an important step considering the missing values' impact to classification and prediction analysis. And we need to make sure our preprocessed data have unique houseID to avoid the mistake in mapping process with properties table. The properties table has unique Parcelid."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1) Missing Values"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true}, "source": ["train_2016.isnull().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There is no missing values after checking the missing values in logerror table in 2016."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2)Duplicate houses ID\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["duplicatedtrain=train_2016[train_2016.duplicated(['parcelid'], keep=False)]\n", "duplicatedtrain.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Why Duplicated data appear? "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Actually the Duplicated PracelID in this table dose not mean poor data quality, it means same house has more than one transaction in 2016. From the table we can know that for parcelid 13850164, it has two transcations in 2016. \n", "\n", "Next, let's count how many transactions each duplicated ID have."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["print(duplicatedtrain['parcelid'].value_counts().describe())\n", "print((duplicatedtrain['parcelid'].value_counts()==3).sum())\n", "print((duplicatedtrain['parcelid'].value_counts()==2).sum())\n", "print(duplicatedtrain['parcelid'].value_counts().head())\n", "doubleduplicated=duplicatedtrain[~(duplicatedtrain['parcelid']==11842707)]\n", "doubleduplicated1=pd.DataFrame(doubleduplicated['logerror'])\n", "errorchange=[]\n", "print (doubleduplicated1.head()) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["So,we can know that there are 124 houses have duplicated transection information which means same house was selled more than one time in 2016. There is only one house had three trasection history in 2016 and all of the other have twice transections in 2016. In the visualization section, I will do the statistical analysis of these duplicated selled houses and do the logerror analysis based on their properties like latitude and longitude in the later labs. And i will not delete these duplicated ID."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Distribution of Logerror"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["import plotly.graph_objs as go\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "import numpy as np\n", "print(train_2016.logerror.describe())\n", "x = train_2016.logerror\n", "data = [go.Histogram(x=x)]\n", "iplot(data, filename='basic histogram of logerror')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Basd on the statistic data, we can know that logerror data don't have the problem of outlier. The logerror distribution graph is close to normal distribution. The existence of large logerror makes us hard to know the most of logerror. So, I deleted the large logerrors which distance to mean are more than two stdandard deviations (SD)."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["import plotly\n", "import math\n", "print (plotly.__version__)     \n", "# version 1.9.x required\n", "plotly.offline\n", "from plotly.offline import iplot_mpl\n", "print(np.mean(train_2016.logerror))\n", "print(np.std(train_2016.logerror))\n", "def removeoutlier(data):\n", "    mean = np.mean(data)\n", "    std = np.std(data)\n", "    leave = [i for i in data if (mean - 2 * std < i < mean + 2 * std)]\n", "    return leave\n", "filtered_d = removeoutlier(train_2016.logerror)\n", "fig = plt.figure()\n", "sns.set_palette(\"hls\")\n", "sns.distplot(filtered_d);\n", "iplot_mpl(fig,strip_style=False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Much Better! this ditribution is close to normal distribution with mean=0.011457219606756682 and standard deviation(SD)= 0.16107794320832886. For finding the relationship between logerror and properties, i do the feature discretization to logerror in the next step.\n", "\n", "      abs(logerror) < abs(Mean-SD\uff09 => 'low error'\n", "\n", "      abs(logerror) >= abs(Mean-SD) => 'high error'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["   ### Feature Discretization: Logerror"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["meansd=abs(-0.16107794320832886/2+0.011457219606756682) # use this based on exprience to make lowerror number and high error number dont have huge difference.\n", "train_2016['class'] = pd.cut(abs(train_2016.logerror),[0,meansd,1e6],3,\n", "                                 labels=['lowerror','higherror']) # this creates a new variable\n", "train_2016.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2) Table of  Properties' Features"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["properties_2016.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is the table of first 5 instances in the file of houses' properties. Then let's check the data type of features and do the data quality analysis. I noticed there are a lot Null values in the table. "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["print(properties_2016.info())"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["properties_2016.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The describe() function gives us the distribution of all non object features. There are a lot of null values in some features. Some features are highly related and some features are useless for furture analysis of logerror.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1) Missing Values"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true}, "source": ["import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "from plotly.offline import iplot_mpl, iplot\n", "%matplotlib inline \n", "#matplotlib.style.use('ggplot')\n", "null_count=pd.DataFrame(properties_2016.isnull().sum())\n", "null_count.columns=['nullnumber']\n", "sortednull=null_count.sort_values(by='nullnumber')\n", "data = [go.Bar(\n", "            x=sortednull.index,\n", "            y=sortednull.nullnumber\n", "    )]\n", "\n", "iplot(data, filename='basic-bar')\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": false}, "source": ["print(properties_2016.shape)\n", "nullpercent=sortednull\n", "nullpercent.columns=['Null percent']\n", "print(nullpercent[:31]/2985217)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["we know that properties_2016 table dimension is 2985217 * 58. we totally have 58 features. If a feature has more than 2000000 null, it means this feature has more than more than 60% data are null. In this dataset, we have 29 features have more than 60% null data. So we can first consider 29 features that has less than 60% null data. In these 29 features, 25 features have less than 10% null data. Let's first consider these features' relationship and importance. Here, parcelID is the nominal feature which should not be considered relationship discover.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2) Duplicated Meaning Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By checking the meaning of these 30 features, we found that feature: rawcensustruactandblock  and censustractandblock have same meaning. Let's figure out if they can be represented by one of them.\n", "First, let's check the distribution of them."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["properties_2016[['rawcensustractandblock','censustractandblock']].describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Second, let's check if they have linear relationship by visulization."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["plt.scatter(properties_2016['rawcensustractandblock'], properties_2016['censustractandblock'])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These two variables have same meaning and are discrete. so we can pick one of them in further analysis. Here, i picked rawcensustractandblock\n", "in further relationship analysis."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3\uff09Duplicated ParcelID \n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["properties_2016[properties_2016.duplicated(['parcelid'], keep=False)]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["No duplicated ParcelID found in properties_2016 table."]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["# 3. Data Visulization \n", "## 1.Make subgroups based on feature meaning\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Considering we still have 31 features left which less than 60% null data. \n", "we first made the subgroups of features based on their meaning in table Zillow_data_dictionary. Bellowing are features that can be grouped as fllowing:\n", "\n", "\n", "\n", "\n", "### subgroup I :  Location of Properties:\n", "#### rawcensustractandblock, regionidcounty, longitude, latitude, regionidzip , regionidcity, regionidneighborhood.\n", "\n", "### subgroup II : Structrue and Condition of Properties:\n", "####  'parcelid','bedroomcnt','bathroomcnt','roomcnt','fullbathcnt','calculatedbathnbr','unitcnt','buildingqualitytypeid','heatingorsystemtypeid','logerror'\n", "\n", "### subgroup III: Time:\n", "\n", "#### 'yearbuilt','assessmentyear', ' transaction month'\n", "                       \n", "                                         \n", "### subgroup IV: Value and Area  of Proeperties and land:\n", "\n", "#### 'calculatedfinishedsquarefeet','finishedsquarefeet12','taxamount','taxvaluedollarcnt','structuretaxvaluedollarcnt'\n", "\n", "\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Relationships of Subgroups and Logerror"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### I. Location of Parcel VS logerror"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["properties_2016[['rawcensustractandblock','regionidcounty','longitude','latitude','regionidzip','regionidcity','regionidneighborhood']].describe()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["from sklearn import preprocessing\n", "from pandas.tools.plotting import scatter_matrix\n", "#mergelogerror_properties = pd.merge(properties_2016,train_2016, on='parcelid')\n", "%matplotlib inline \n", "locationproperties=mergelogerror_properties[['longitude','latitude','regionidzip','regionidcity','regionidneighborhood','class']].sample(5000)\n", "locationproperties['longitude']=locationproperties.longitude/1e6\n", "locationproperties['latitude']=locationproperties.latitude/1e6\n", "locationproperties1=locationproperties.dropna()\n", "sns.pairplot(locationproperties1,diag_kind=\"kde\",hue=\"class\",markers='+',plot_kws=dict(s=1,edgecolor=\"g\", linewidth=1),diag_kws=dict(shade=True))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the pairplot, we know that the city area which are in the middle or close to los angles has more chance to have high error. And the regionidzip in the range [96200-96500] have more chance to get low error."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["# plot the correlation matrix using seaborn\n", "%matplotlib inline \n", "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n", "sns.set(style=\"darkgrid\") # one of the many styles to plot using\n", "f, ax = plt.subplots(figsize=(7, 7))\n", "sns.heatmap(locationproperties.corr(), cmap=cmap, annot=True)\n", "f.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["we can say two variable have correlationship if correlationship number is in the range : r<-0.3 or r>0.3 more details can be found in this website: http://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/\n", "\n", "So, based on the calculation results we can know that latitude has correlationship with regionidneighborhood, longitude, regionidcounty and longtitude has correlationship with rawcensustractandblock, regionidcounty and latitude. Otherwise, based the meaning of these location features, we can know that longtitude and latitude will be good represents for location subgroups."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Properties' Density Distrbution"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["y=mergelogerror_properties.latitude.values/1e6\n", "x=mergelogerror_properties.longitude.values/1e6\n", "xy = np.vstack([x,y])\n", "z = gaussian_kde(xy)(xy)# Calculate the point density\n", "fig, ax = plt.subplots()\n", "ax.scatter(x, y, c=z, s=100, edgecolor='')\n", "ax.set_xlabel('longtitude')\n", "ax.set_ylabel('latitude')\n", "plt.title('Properties Density Distrbution')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Properties' Distribution in different cities by Google Map "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["%matplotlib inline \n", "import webbrowser\n", "url = \"file://\" + '/Users/MacBook/Desktop/machinlearning/lab1' + \"/\" + \"visual.html\"\n", "webbrowser.open(url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the dataset is huge for google map plot, i choose 1% of longtitude and latitude data: 29738 points to show the distribution of properties in different cities. For the html file is now located on my computer and can not be opened outside my computer, I put the picture of result of distribution of properties in different cities."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["from IPython.display import Image\n", "Image(\"/Users/MacBook/Desktop/machinlearning/lab1/distributionmap.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The number inside the circle means how many properies are in this area. From the picture we can know that los angles has most of properties and the Sana Ana area and long beach area are in second place. This result is not a big surprise but still gives us a more straightforward sense of properties' distribution. Next, let's check the distribution of logerror in different location(different longitude and altitude)."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["import pandas as pd\n", "import plotly.plotly as py\n", "import plotly.graph_objs as go\n", "from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis\n", "sample1=mergelogerror_properties.sample(5000)\n", "lowerror1 = sample1[(sample1['class'] =='lowerror')]\n", "higherror1 = sample1[(sample1['class']=='higherror')]\n", "\n", "trace_comp0 = go.Scatter(\n", "    y=lowerror1.latitude/1e6,\n", "    x=lowerror1.longitude/1e6,\n", "    mode='markers',\n", "    marker=Marker(size=lowerror1.logerror, sizemode='area', sizeref=0.01,color='navy'),\n", "    name='lowerror',\n", "    text=lowerror1.logerror,\n", "    )\n", "\n", "trace_comp1 = go.Scatter(\n", "    y=higherror1.latitude/1e6,\n", "    x=higherror1.longitude/1e6,\n", "    mode='markers',\n", "    marker=Marker(size=higherror1.logerror, sizemode='area', sizeref=0.01,color='red'),\n", "    name='higherror',\n", "    text=higherror1.logerror,\n", "        )\n", "\n", "data_comp = [trace_comp0, trace_comp1]\n", "layout_comp = go.Layout(\n", "    title='Location VS logerror',\n", "    hovermode='closest',\n", "    xaxis=dict(\n", "        title='longitude)',\n", "        ticklen=5,\n", "        zeroline=False,\n", "        gridwidth=2,\n", "    ),\n", "    yaxis=dict(\n", "        title='latitude',\n", "        ticklen=5,\n", "        gridwidth=2,\n", "    ),\n", ")\n", "fig_comp = go.Figure(data=data_comp, layout=layout_comp)\n", "py.iplot(fig_comp, filename='location vs logerror')\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This picture also shows that high error properties are more dense around los angles. And the biggest high error is in the location longitude: [-118.5,-118] and latitude: [34.2,33.8]."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### II. Structrue and Condition of Parcel VS Logerror"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["#from matplotlib import scatter\n", "structioncondition=mergelogerror_properties[['parcelid','bedroomcnt','bathroomcnt','roomcnt','fullbathcnt','calculatedbathnbr','unitcnt','buildingqualitytypeid','heatingorsystemtypeid','logerror','class']]\n", "structioncondition['lowerror']=[0 if i!='lowerror' else 1 for i in structioncondition['class']]\n", "\n", "#print(structioncondition['class'])\n", "errorgroup = pd.crosstab([structioncondition['heatingorsystemtypeid'],\n", "                    structioncondition['buildingqualitytypeid']], \n", "                    structioncondition['class'])\n", "print(errorgroup)\n", "#errorgroup.plot(kind='bar', stacked=True)\n", "error_rate = errorgroup.div(errorgroup.sum(1).astype(float),\n", "                             axis=0) # normalize the value\n", "\n", "# print survival_rate\n", "error_rate.plot(kind='barh', \n", "                   stacked=True)\n", "plt.title('lowerror and high error percentage by heating system type and building quality type')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["I made crosstab by class of heatingorsystemtypeid  and buildingqualitytypeid, here properties mainly have three types of heating systems(2: Central, 7: floor/wall and 20: Solar) and building quality id is from best (1) to worst (12). Here, True represents Low error and False represents High error.\n", "\n", "From the bar plot, we can know that over 75% properties which have central heating system and 4 buliding quality have low error. And central heating houses with worst buliding quality only have about 58% low error percent. it means the accuracy will be bad when the buliding quality is poor.\n", "Although for the floor/wall heating houses and solar heating houses, if they have poor buliding quality like 10, their low error percent will be less than 70%.\n", "\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["structionconditionnumber=structioncondition[['buildingqualitytypeid','bedroomcnt','bathroomcnt','roomcnt','fullbathcnt','calculatedbathnbr','unitcnt']]\n", "structionconditionnumber.describe()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["%matplotlib inline \n", "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n", "sns.set(style=\"darkgrid\") # one of the many styles to plot using\n", "f, ax = plt.subplots(figsize=(9, 9))\n", "sns.heatmap(structioncondition[['bedroomcnt','bathroomcnt','roomcnt','fullbathcnt','calculatedbathnbr','unitcnt','buildingqualitytypeid','heatingorsystemtypeid','logerror']].corr(), cmap=cmap, annot=True)\n", "f.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Based on the calculation results we can know that bedroom number, bathroom number , full bathroom number and calculated bath room number are highly related with correlation number more than 0.6. So, we can choose one variable to represent the imformation of these 4. Here, i choose the bathroom number. I also noticed that heating system type has a high positive relationship with room number."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### III Logerror VS Time"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Logerror VS Yearbuilt and Assessment year "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["timeproperties=mergelogerror_properties[['yearbuilt','assessmentyear','logerror','class']]\n", "timeproperties.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The properties table contains houses with built year from 1885 to 2015. they all have the same assessmentyear. So, the assessment year feature is useless in the future. Next, let's check how built year is related to logerror."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["# built year distribution\n", "timeproperties=timeproperties[['yearbuilt','logerror','class']]\n", "sns.distplot(properties_2016.yearbuilt[~np.isnan(properties_2016.yearbuilt)])\n", "plt.title('Distribution of parcels with builtyear ')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can know most of properties were built in 1950 to 1990.\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["#ax = scatter_matrix(timeproperties,figsize=(15, 10))\n", "logerrortimemean=timeproperties[['logerror','yearbuilt']].groupby('yearbuilt'). mean()\n", "plotly.offline.init_notebook_mode() # run at the start of every notebook\n", "plotly.offline.iplot({\n", "    \"data\": [{\n", "        \"x\": logerrortimemean.index,\n", "        \"y\": logerrortimemean.logerror\n", "    }],\n", "    \"layout\": {\n", "        \"title\": \"Mean of logerror VS Built year\"\n", "    }\n", "})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the mean of logerror Vs built year picture, we can know that with the time chage, mean of logerror is getting smaller and more stable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Transaction Month VS Logerror"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["#plot the hist of transection month to see the transaction tendency.\n", "import matplotlib.pyplot as plt\n", "from scipy.stats import norm\n", "train_2016['month']=train_2016['transactiondate'].dt.month\n", "plt.hist(train_2016['month'],bins=12)\n", "plt.title('hist of number of transactions in every month in 2016 ')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["There are less transaction after 10.25 for the transaction data are treated as test data. In the month 1-10, month in the middle of year have tendency to have most transactions like June. "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true}, "source": ["# show the mean of logerror for every month\n", "trainlogerrormean=train_2016[['logerror','month']].groupby('month'). mean()\n", "plotly.offline.init_notebook_mode() # run at the start of every notebook\n", "plotly.offline.iplot({\n", "    \"data\": [{\n", "        \"x\": [1,2,3,4,5,6,7,8,9,10,11,12],\n", "        \"y\": trainlogerrormean.logerror\n", "    }],\n", "    \"layout\": {\n", "        \"title\": \"Mean of logerror VS Transaction Month\"\n", "    }\n", "})"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["which is a suprise that, with more transaction in the middle of year, the mean of logerror is much smaller than other months. There are many reasons, one is for the transaction number is more than other months, June have more low error than other months."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["# show the mean of abs(logerror) for every month\n", "abstrain_2016=train_2016\n", "abstrain_2016['logerror']=abs(train_2016['logerror'])\n", "abstrain_2016mean=abstrain_2016[['logerror','month']].groupby('month'). mean()\n", "plotly.offline.init_notebook_mode() # run at the start of every notebook\n", "plotly.offline.iplot({\n", "    \"data\": [{\n", "        \"x\": [1,2,3,4,5,6,7,8,9,10,11,12],\n", "        \"y\": abstrain_2016mean.logerror\n", "    }],\n", "    \"layout\": {\n", "        \"title\": \"Mean of Abs(logerror) VS Transaction Month\"\n", "    }\n", "})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But if we plot the mean of abs(logerror), we can find that error decrease with transaction month. I think this is beacuase pepole start to collect information of properties in the begining of the year. To the end of the year people will avoid the error based on the tendency of previous months."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["mergelogerror_properties['month']=mergelogerror_properties['transactiondate'].dt.month\n", "sns.violinplot(x=\"month\", y=\"yearbuilt\", hue=\"class\", data=mergelogerror_properties,\n", "               split=True, inner=\"quart\")\n", "plt.title('Violion plot of logerror by built year and transaction month')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["By violin picture, we can know most of high error properties are built in older time than lowerror. The mean of built year of high error is about 1958 and the mean of built year of low error is about 1970. There is no big difference of low error and high error in each month."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### IV. Area and Value of Property and Land"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["areapropertyland=mergelogerror_properties[['parcelid','calculatedfinishedsquarefeet','finishedsquarefeet12','lotsizesquarefeet','taxamount','taxvaluedollarcnt','structuretaxvaluedollarcnt','propertycountylandusecode','propertyzoningdesc','landtaxvaluedollarcnt','logerror','class']]\n", "areapropertyland.describe()\n", "#sns.pairplot(areapropertyland, hue=\"class\", size=2)\n", "%matplotlib inline \n", "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n", "sns.set(style=\"darkgrid\") # one of the many styles to plot using\n", "f, ax = plt.subplots(figsize=(9, 9))\n", "sns.heatmap(areapropertyland[['calculatedfinishedsquarefeet','finishedsquarefeet12','lotsizesquarefeet','taxamount','taxvaluedollarcnt','structuretaxvaluedollarcnt','propertycountylandusecode','propertyzoningdesc','landtaxvaluedollarcnt','logerror']].corr(), cmap=cmap, annot=True)\n", "plt.title('Correlation Matrix Graph')\n", "f.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["calculatedfinishedsquarefeet','finishedsquarefeet12','taxamount','taxvaluedollarcnt','structuretaxvaluedollarcnt','propertycountylandusecode','propertyzoningdesc','landtaxvaluedollarcnt' are all high related. so we can choose one of them and lotsizesquarefeet to represent this group. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Exceptional Work"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["### House Properties Dimension Reduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1)PCA of Main Features\n", "\n", "Based on the analysis above, I picked out most important features: longitude, latitude, built year, heating system type, building quality, taxt amount, properties area."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["import numpy as np\n", "from sklearn.decomposition import PCA\n", "from sklearn.decomposition.pca import PCA\n", "from sklearn import preprocessing\n", "df=mergelogerror_properties[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','yearbuilt','taxamount','lotsizesquarefeet','logerror']].copy()\n", "df=df.dropna()\n", "df_scaled = preprocessing.scale(df[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','yearbuilt','taxamount','lotsizesquarefeet']])\n", "df_scaled=pd.DataFrame(df_scaled)\n", "df_scaled.columns=[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','yearbuilt','taxamount','lotsizesquarefeet']]\n", "\n", "print(df_scaled.head())\n", "y = df.logerror\n", "target_names='logerror'\n", "\n", "\n", " # fit data and then transform it\n", "for k in range(1,6):\n", "    pca=PCA(n_components=k)\n", "    X_pca = pca.fit(df_scaled).transform(df_scaled)\n", "    pca.fit(df_scaled)\n", "    print('explained_variance_ratio:',pca.explained_variance_ratio_)\n", "    print('pca:',pca.components_)\n", "\n"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"scrolled": true}, "source": ["import plotly.plotly as py\n", "from plotly.graph_objs import *\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\n", "import plotly.tools as tls\n", "from sklearn.preprocessing import StandardScaler\n", "X=df_scaled\n", "X_std = StandardScaler().fit_transform(X)\n", "mean_vec = np.mean(X_std, axis=0)\n", "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n", "#print('Covariance matrix \\n%s' %cov_mat)\n", "#print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))\n", "cov_mat = np.cov(X_std.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "cor_mat1 = np.corrcoef(X_std.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "cor_mat2 = np.corrcoef(X.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cor_mat2)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "u,s,v = np.linalg.svd(X_std.T)\n", "for ev in eig_vecs:\n", "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n", "# Make a list of (eigenvalue, eigenvector) tuples\n", "\n", "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n", "\n", "# Sort the (eigenvalue, eigenvector) tuples from high to low\n", "eig_pairs.sort()\n", "eig_pairs.reverse()\n", "\n", "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n", "print('Eigenvalues in descending order:')\n", "for i in eig_pairs:\n", "    print(i[0])\n", "\n", "tot = sum(eig_vals)\n", "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n", "cum_var_exp = np.cumsum(var_exp)\n", "print('the cum')\n", "print(cum_var_exp)\n", "trace1 = Bar(\n", "        x=['PC %s' %i for i in range(1,5)],\n", "        y=var_exp,\n", "        showlegend=False)\n", "\n", "trace2 = Scatter(\n", "        x=['PC %s' %i for i in range(1,5)], \n", "        y=cum_var_exp,\n", "        name='cumulative explained variance')\n", "\n", "data1 = Data([trace1, trace2])\n", "\n", "layout=Layout(\n", "        yaxis=YAxis(title='Explained variance in percent'),\n", "        title='Number of rooms variables: Explained variance by different principal components ')\n", "\n", "#fig = Figure(data=data1, layout=layout)\n", "py.iplot(Figure(data=data1, layout=layout))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Based on the cumulative explained variance we can know we need more than 4 principal component to get more than 70% cumulative explained variance. The reason is maybe we put too many variables as input. Let'delete some features like longitude , latitude, built year and calculate bath room number. "]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["X=df_scaled[['fullbathcnt','buildingqualitytypeid','heatingorsystemtypeid','taxamount','lotsizesquarefeet']]\n", "X_std = StandardScaler().fit_transform(X)\n", "mean_vec = np.mean(X_std, axis=0)\n", "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n", "#print('Covariance matrix \\n%s' %cov_mat)\n", "#print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))\n", "cov_mat = np.cov(X_std.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "cor_mat1 = np.corrcoef(X_std.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "cor_mat2 = np.corrcoef(X.T)\n", "eig_vals, eig_vecs = np.linalg.eig(cor_mat2)\n", "#print('Eigenvectors \\n%s' %eig_vecs)\n", "#print('\\nEigenvalues \\n%s' %eig_vals)\n", "u,s,v = np.linalg.svd(X_std.T)\n", "for ev in eig_vecs:\n", "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n", "# Make a list of (eigenvalue, eigenvector) tuples\n", "\n", "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n", "\n", "# Sort the (eigenvalue, eigenvector) tuples from high to low\n", "eig_pairs.sort()\n", "eig_pairs.reverse()\n", "\n", "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n", "print('Eigenvalues in descending order:')\n", "for i in eig_pairs:\n", "    print(i[0])\n", "\n", "tot = sum(eig_vals)\n", "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n", "cum_var_exp = np.cumsum(var_exp)\n", "print('the cum')\n", "print(cum_var_exp)\n", "trace1 = Bar(\n", "        x=['PC %s' %i for i in range(1,5)],\n", "        y=var_exp,\n", "        showlegend=False)\n", "\n", "trace2 = Scatter(\n", "        x=['PC %s' %i for i in range(1,5)], \n", "        y=cum_var_exp,\n", "        name='cumulative explained variance')\n", "\n", "data1 = Data([trace1, trace2])\n", "\n", "layout=Layout(\n", "        yaxis=YAxis(title='Explained variance in percent'),\n", "        title='Number of rooms variables: Explained variance by different principal components ')\n", "\n", "#fig = Figure(data=data1, layout=layout)\n", "py.iplot(Figure(data=data1, layout=layout))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Still no big change. Anyway, let's still check the 2D picture with logerror class."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["#this function is from given lab one example\n", "#print(df_scaled.head())\n", "meansd=abs(-0.16107794320832886/2+0.011457219606756682)\n", "train_2016['plotclass'] = pd.cut(abs(train_2016.logerror),[0,meansd,1e6],3,\n", "                                 labels=['0','1']) # this creates a new variable\n", "mergelogerror_properties = pd.merge(properties_2016,train_2016, on='parcelid')\n", "df=mergelogerror_properties[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','yearbuilt','taxamount','lotsizesquarefeet','logerror','plotclass']].sample(1000).copy()\n", "df=df.dropna()\n", "df_scaled = preprocessing.scale(df[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','taxamount','lotsizesquarefeet']])\n", "df_scaled=pd.DataFrame(df_scaled)\n", "df_scaled.columns=[['longitude','latitude','fullbathcnt','calculatedbathnbr','buildingqualitytypeid','heatingorsystemtypeid','taxamount','lotsizesquarefeet']]\n", "#print(df.plotclass.describe())\n", "y=df.plotclass\n", "\n", "target_names='logerror'\n", " # fit data and then transform it\n", "for k in range(1,6):\n", "    pca=PCA(n_components=k)\n", "    X_pca = pca.fit(df_scaled).transform(df_scaled)\n", "    pca.fit(df_scaled)\n", "    #print('explained_variance_ratio:',pca.explained_variance_ratio_)\n", "    #print('pca:',pca.components_)\n", "\n", "\n", "from pandas.tools.plotting import scatter_plot\n", "def get_feature_names_from_weights(weights, names):\n", "    tmp_array = []\n", "    for comp in weights:\n", "        tmp_string = ''\n", "        for fidx,f in enumerate(names):\n", "            if fidx>0 and comp[fidx]>=0:\n", "                tmp_string+='+'\n", "            tmp_string += '%.2f*%s ' % (comp[fidx],f[:-5])\n", "        tmp_array.append(tmp_string)\n", "    return tmp_array\n", "  \n", "pca_weight_strings = get_feature_names_from_weights(pca.components_, df_scaled.columns) \n", "df_pca = pd.DataFrame(X_pca,columns=[pca_weight_strings])\n", "y=preprocessing.scale(y)\n", "ax = scatter_plot(df_pca, pca_weight_strings[0], pca_weight_strings[1], c=y,s=(y+2)*10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the 2D PCA of 1000 properties sample, we can know that there is a possible cluster in the middle for white points. And we also can know the weight of differnt variables, here full bathroom number and calculated bath room number has biggest weight , the heating system type is in second place. Actually low error and high error are not clustered enough for us to cluster methods further. Next, let's check if LDA has the same results."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2) Dimension Reduction of buliding construction by PCA"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the variables relationship with logerror, we know that room number seems most important. so, let's check if building construction variables can be represented by 2 PCs."]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {}, "source": ["import numpy as np\n", "from sklearn.decomposition import PCA\n", "from sklearn.decomposition.pca import PCA\n", "from sklearn import preprocessing\n", "df=mergelogerror_properties[['bedroomcnt','bathroomcnt','fullbathcnt','calculatedbathnbr','unitcnt','plotclass']].copy()\n", "df=df.dropna()\n", "df_scaled = preprocessing.scale(df[['bedroomcnt','bathroomcnt','fullbathcnt','calculatedbathnbr','unitcnt']])\n", "df_scaled=pd.DataFrame(df_scaled)\n", "df_scaled.columns=[['bedroomcnt','bathroomcnt','fullbathcnt','calculatedbathnbr','unitcnt']]\n", "\n", "print(df_scaled.head())\n", "y = df.plotclass\n", "target_names='plotclass'\n", "\n", "for k in range(1,3):\n", "    pca=PCA(n_components=k)\n", "    X_pca = pca.fit(df_scaled).transform(df_scaled)\n", "    pca.fit(df_scaled)\n", "    print('explained_variance_ratio:',pca.explained_variance_ratio_)\n", "    print('pca:',pca.components_)\n", "\n", "from pandas.tools.plotting import scatter_plot\n", "def get_feature_names_from_weights(weights, names):\n", "    tmp_array = []\n", "    for comp in weights:\n", "        tmp_string = ''\n", "        for fidx,f in enumerate(names):\n", "            if fidx>0 and comp[fidx]>=0:\n", "                tmp_string+='+'\n", "            tmp_string += '%.2f*%s ' % (comp[fidx],f[:-5])\n", "        tmp_array.append(tmp_string)\n", "    return tmp_array\n", "  \n", "pca_weight_strings = get_feature_names_from_weights(pca.components_, df_scaled.columns) \n", "df_pca = pd.DataFrame(X_pca,columns=[pca_weight_strings])\n", "y=preprocessing.scale(y)\n", "ax = scatter_plot(df_pca, pca_weight_strings[0], pca_weight_strings[1], c=y,s=(y+2)*10)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At last, I found the 2 possible PCA components for number of rooms in houses. they have explained_variance_ratio: [ 0.70227567  0.19548756]. which means these two components can explain most of information in these five variables.   And consider the weight of each variables, we can know bathroomnumber, fullbathroomnumber and calculatedbathroomnumber most important variables in these room's number dataset considering the x direction component. If we consider the y direction component, then unit number will be the most important variable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Reference"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "https://pythonprogramming.net/matplotlib-3d-scatterplot-tutorial/\n", "\n", "https://plot.ly/ipython-notebooks/principal-component-analysis/#pca-vs-lda\n", "\n", "https://seaborn.pydata.org/tutorial/distributions.html\n", "\n", "import plotly\n", "print (plotly.__version__)\n", "plotly.offline.init_notebook_mode() # run at the start of every notebook\n", "from plotly.graph_objs import Scatter, Marker, Layout, XAxis, YAxis\n", "mergelogerror_properties = pd.merge(properties_2016,train_2016, on='parcelid')\n", "mergelogerror_properties1=mergelogerror_properties.sample(5000)\n", "plotly.offline.iplot({\n", "    'data':[\n", "        Scatter(y=mergelogerror_properties1.latitude.values/1e6,\n", "                x=mergelogerror_properties1.longitude.values/1e6,\n", "                text=mergelogerror_properties1.logerror.astype(str),\n", "                marker=Marker(size=mergelogerror_properties1.logerror, sizemode='area', sizeref=0.01,),\n", "                mode='markers')\n", "            ],\n", "    'layout': Layout(xaxis=XAxis(title='longitude'), yaxis=YAxis(title='latitude'))\n", "}, show_link=False)\n", "\n", "\n", "train_2016['positive or negtive']=train_2016['transactiondate'].dt.month\n", "for i in range(len(train_2016)):\n", "    if train_2016['logerror'][i]>0:\n", "        train_2016['positive or negtive'][i]='positive'\n", "    else:\n", "        train_2016['positive or negtive'][i]='negtive'\n", "print(train_2016)\n", "ax = sns.violinplot(x=\"month\", y=\"logerror\", hue=\"positive or negtive\",data=tips, palette=\"muted\", split=True)\n", "\n", "from matplotlib.colors import ListedColormap\n", "def plot_decision_regions(X, y, classifier, resolution=0.02):\n", "    markers = ('s', 'x', 'o', '^', 'v')\n", "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n", "    cmap = ListedColormap(colors[:len(np.unique(y))])\n", "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),np.arange(x2_min, x2_max, resolution))\n", "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n", "    Z = Z.reshape(xx1.shape)\n", "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n", "    plt.xlim(xx1.min(), xx1.max())\n", "    plt.ylim(xx2.min(), xx2.max())\n", "    for idx, cl in enumerate(np.unique(y)):\n", "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8, c=cmap(idx),marker=markers[idx], label=cl)\n", "from sklearn.lda import LDA\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.cross_validation import train_test_split\n", "lda=LDA(n_components=2)\n", "sc=StandardScaler()\n", "X_train,X_test,y_train,y_test=train_test_split(df_scaled,df.plotclass,test_size=0.2,random_state=0)\n", "X_train_std=sc.fit_transform(X_train)\n", "X_train_lda=lda.fit_transform(X_train_std,y_train)\n", "lr=LogisticRegression()\n", "lr=lr.fit(X_train_lda,y_train)\n", "print(lr)\n", "print(lr.class_weight)"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.0", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}