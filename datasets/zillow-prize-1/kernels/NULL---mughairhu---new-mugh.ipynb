{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"collapsed": true, "_uuid": "ba200d60fada725fe30f949cdfc60484b5e01963", "_cell_guid": "a9a81e69-0c62-46f7-a2b3-da2dcb2cea16"}, "source": ["#import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "color = sns.color_palette()\n", "\n", "\n", "pd.options.mode.chained_assignment = None\n", "pd.options.display.max_columns = 999\n", "\n", "train_df = pd.read_csv(\"input/train_2016.csv\", parse_dates=[\"transactiondate\"])\n", "train_df.shape\n", "train_df.head()\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(range(train_df.shape[0]), np.sort(train_df.logerror.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('logerror', fontsize=12)\n", "plt.show()\n", "\n", "#for missing values\n", "\n", "prop_df = pd.read_csv(\"input/properties_2016.csv\")\n", "prop_df.shape\n", "prop_df.head()\n", "missing_df = prop_df.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df = missing_df.loc[missing_df['missing_count']>0]\n", "missing_df = missing_df.sort_values(by='missing_count')\n", "\n", "ind = np.arange(missing_df.shape[0])\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Count of missing values\")\n", "ax.set_title(\"Number of missing values in each column\")\n", "plt.show()\n", "#to merge both tables\n", "train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')\n", "train_df.head()\n", "#find data type of var i.g text,float,int \n", "pd.options.display.max_rows = 65\n", "\n", "dtype_df = train_df.dtypes.reset_index()\n", "dtype_df.columns = [\"Count\", \"Column Type\"]\n", "dtype_df\n", "#not understand\n", "dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()\n", "#find missing in new data set\n", "missing_df = train_df.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\n", "missing_df.loc[missing_df['missing_ratio']>0.999]\n", "# Let us just impute the missing values with mean values to compute correlation coefficients #\n", "mean_values = train_df.mean(axis=0)\n", "train_df_new = train_df.fillna(mean_values, inplace=True)\n", "\n", "# Now let us look at the correlation coefficient of each of these variables #\n", "x_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n", "\n", "labels = []\n", "values = []\n", "for col in x_cols:\n", "    labels.append(col)\n", "    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0,1])\n", "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n", "corr_df = corr_df.sort_values(by='corr_values')\n", "    \n", "ind = np.arange(len(labels))\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,40))\n", "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n", "ax.set_xlabel(\"Correlation coefficient\")\n", "ax.set_title(\"Correlation coefficient of the variables\")\n", "#autolabel(rects)\n", "plt.show()\n", "#here are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.\n", "corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\n", "for col in corr_zero_cols:\n", "    print(col, len(train_df_new[col].unique()))\n", "#Let us take the variables with high correlation values and then do some analysis on them.\n", "corr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n", "corr_df_sel\n", "#\n", "cols_to_use = corr_df_sel.col_labels.tolist()\n", "\n", "temp_df = train_df[cols_to_use]\n", "corrmat = temp_df.corr(method='spearman')\n", "f, ax = plt.subplots(figsize=(8, 8))\n", "\n", "# Draw the heatmap using seaborn\n", "sns.heatmap(corrmat, vmax=1., square=True)\n", "plt.title(\"Important variables correlation map\", fontsize=15)\n", "plt.show()    This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": null, "cell_type": "code"}]}