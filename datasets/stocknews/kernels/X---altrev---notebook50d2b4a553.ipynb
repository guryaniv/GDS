{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0146a065-3841-bc0e-8b0d-1275ed8380b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from datetime import date\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv('../input/Combined_News_DJIA.csv')\n",
        "y = data.pop('Label').as_matrix()\n",
        "\n",
        "# Use RenMai's fancy joining code\n",
        "X_raw = data.filter(regex=(\"Top.*\")).apply(lambda x: ''.join(str(x.values)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7052814b-133c-db1e-9fdf-eddc7164decd"
      },
      "outputs": [],
      "source": [
        "# Let's remove other characters, stop words and lemmatize and turn into sequences of numbers\n",
        "# You might need this, I had them installed already:\n",
        "# nltk.download(['stopwords', 'wordnet'])\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatize = nltk.WordNetLemmatizer().lemmatize\n",
        "tokenize = nltk.tokenize.treebank.TreebankWordTokenizer().tokenize\n",
        "alphabet = '''abcdefghijklmnopqrstuvwxyz0123456789 '''\n",
        "# A dict for transforming into sequences:\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "vocab = {}\n",
        "count = 0\n",
        "def normalize(o):\n",
        "    global count\n",
        "    r = []\n",
        "    for t in tokenize(o):\n",
        "        if t not in stopwords and len(t) > 2:\n",
        "            t = lemmatize(t).lower()\n",
        "            t= ''.join(lc for lc in t if lc in alphabet)\n",
        "            if t in vocab:\n",
        "                n = vocab[t]\n",
        "            else:\n",
        "                vocab[t] = count\n",
        "                n = count\n",
        "                count += 1\n",
        "            r.append(n)\n",
        "    return r\n",
        "print('Normalize words and transform to sequences...')\n",
        "X_text = [normalize(x) for x in X_raw]\n",
        "\n",
        "# This vocabulary size might be a bit large, perhaps one should restrict it to the highest frequency scores\n",
        "vocab_size = len(vocab)\n",
        "print('Vocabulary size:' + str(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04433f89-1807-9d8f-64b9-1c8f6688c40d"
      },
      "outputs": [],
      "source": [
        "# Suggested train/test split. Kudos to RenMai, too\n",
        "TRAINING_END = date(2014,12,31)\n",
        "num_training = len(data[pd.to_datetime(data[\"Date\"]) <= TRAINING_END])\n",
        "X_train = X_text[:num_training]\n",
        "X_test = X_text[num_training:]\n",
        "y_train = y[:num_training]\n",
        "y_test = y[num_training:]\n",
        "\n",
        "# Pad sequences to maximum length.\n",
        "X_train = sequence.pad_sequences(X_train)\n",
        "X_test = sequence.pad_sequences(X_test)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89308501-e32b-6b3b-d1d9-48c8c3b40e76"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "# High dropout, because this quickly overfits.\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, dropout=0.75))\n",
        "model.add(LSTM(128, dropout_W=0.3, dropout_U=0.3))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5155f84b-589e-b1fb-7ad6-7327d069f070"
      },
      "outputs": [],
      "source": [
        "print('Train...')\n",
        "model.fit(X_train, y_train, batch_size=32, nb_epoch=11, validation_data=(X_test, y_test), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d96b8acc-84b2-2801-2a8f-ec4d20d8f12c"
      },
      "outputs": [],
      "source": [
        "# predict and evaluate predictions\n",
        "predictions = model.predict_proba(X_test)\n",
        "print('ROC-AUC yields ' + str(roc_auc_score(y_test, predictions)))"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}