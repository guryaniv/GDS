{"nbformat_minor": 1, "cells": [{"execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "metadata": {"_uuid": "27c47ece401b160c95fec0e626d9443be7a66afa", "_cell_guid": "812a429f-04bc-4b6d-9f95-562f285bfd87"}, "outputs": []}, {"execution_count": null, "source": ["# GBM prediction\n", "\n", "# inspirations:\n", "# https://www.kaggle.com/the1owl/surprise-me/\n", "import math\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import *\n", "import datetime as dt\n", "from sklearn import preprocessing\n", "from sklearn.model_selection import GridSearchCV\n", "import xgboost as xgb\n", "\n", "def RMSLE(y, pred):\n", "    return metrics.mean_squared_error(y, pred) ** 0.5\n", "\n", "start_time = dt.datetime.now()\n", "print(\"Started at \", start_time)"], "cell_type": "code", "metadata": {}, "outputs": []}, {"execution_count": null, "source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n", "#datetime transform+date feature\n", "for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n", "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n", "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n", "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n", "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n", "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n", "\n", "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n", "\n", "unique_stores = data['tes']['air_store_id'].unique()\n", "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i] * len(unique_stores)}) for i in range(7)],\n", "                   axis=0, ignore_index=True).reset_index(drop=True)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["def find_outliers(series):\n", "    return (series - series.mean())>2.4*series.std()\n", "\n", "data['tra']['is_outlier']=data['tra'].groupby('air_store_id').apply(lambda g: find_outliers(g['visitors'])).values"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["# mean max min sure it can be compressed... \n", "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].min().rename(\n", "    columns={'visitors': 'min_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n", "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].mean().rename(\n", "    columns={'visitors': 'mean_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n", "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].median().rename(\n", "    columns={'visitors': 'median_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n", "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].max().rename(\n", "    columns={'visitors': 'max_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n", "tmp = data['tra'].groupby(['air_store_id', 'dow'], as_index=False)['visitors'].count().rename(\n", "    columns={'visitors': 'count_observations'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\n", "\n", "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n", "\n", "print(\"Store df info:\")\n", "print(stores.info())"], "cell_type": "code", "metadata": {}, "outputs": []}, {"execution_count": null, "source": ["# NEW FEATURES FROM Georgii Vyshnia\n", "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n", "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n", "lbl = preprocessing.LabelEncoder()\n", "for i in range(10):\n", "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "\n", "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n", "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["# day_of_week label encode\n", "lbl = preprocessing.LabelEncoder()\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date'])\n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\n", "\n", "train = pd.merge(data['tra'], stores, how='left', on=['air_store_id', 'dow'])\n", "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id', 'dow'])\n", "\n", "for df in ['ar', 'hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id', 'visit_date'])\n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id', 'visit_date'])\n", "\n", "print(train.describe())\n", "print(train.head())"], "cell_type": "code", "metadata": {}, "outputs": []}, {"execution_count": null, "source": ["\n", "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n", "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n", "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n", "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n", "\n", "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n", "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n", "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n", "\n", "# NEW FEATURES FROM JMBULL\n", "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "\n", "# NEW FEATURES FROM Georgii Vyshnia\n", "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n", "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n", "\n", "lbl = preprocessing.LabelEncoder()\n", "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n", "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["train['x'] = np.cos(train['latitude']*math.pi/180.0) * np.cos(train['longitude']*math.pi/180.0)\n", "train['y'] = np.cos(train['latitude']*math.pi/180.0) * np.sin(train['longitude']*math.pi/180.0)\n", "test['x'] = np.cos(test['latitude']*math.pi/180.0) * np.cos(test['longitude']*math.pi/180.0)\n", "test['y'] = np.cos(test['latitude']*math.pi/180.0) * np.sin(test['longitude']*math.pi/180.0)\n", "train['monthday_int']=train['date_int']%10000\n", "train['day_int']=train['date_int']%100\n", "test['monthday_int']=test['date_int']%10000\n", "test['day_int']=test['date_int']%100\n", "# def calc_shifted_ewm(series, alpha, adjust=True):\n", "#     return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n", "\n", "# train['ewm'] = train.set_index('visit_date').groupby(['air_store_id', 'dow'])\\\n", "# .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1))\\\n", "# .sort_index(level=['air_store_id', 'visit_date']).values\n", "\n", "# test['ewm'] = test.set_index('visit_date').groupby(['air_store_id', 'dow'])\\\n", "# .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1))\\\n", "# .sort_index(level=['air_store_id', 'visit_date']).values"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["X = train[col]\n", "y = pd.DataFrame()\n", "y['visitors'] = np.log1p(train['visitors'].values)\n", "\n", "# print(X.info())\n", "\n", "y_test_pred = 0\n", "\n", "# do a hideout split for information leak-free last-minute check\n", "X, X_hideout, y, y_hideout = model_selection.train_test_split(X, y, test_size=0.13, random_state=42)\n", "\n", "print(\"Finished data pre-processing at \", dt.datetime.now())"], "cell_type": "code", "metadata": {}, "outputs": []}, {"execution_count": null, "source": ["X2=X[(X.month==4)&(X.year==2017)]\n", "y2=y[(X.month==4)&(X.year==2017)]\n", "X1=X[np.logical_not((X.month==4)&(X.year==2017))]\n", "y1=y[np.logical_not((X.month==4)&(X.year==2017))]"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["print(\"Start tuning at \", dt.datetime.now())\n", "param_grid = {\n", "   # 'max_depth':[5,7]\n", "    'reg_lambda':[0,1,2]\n", "               # 'min_child_weight':[1,3]\n", "    \n", "              # 'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n", "              }\n", "est = xgb.XGBRegressor(\n", "    learning_rate=0.01,\n", "    objective='reg:linear',\n", "    n_estimators=1000,\n", "    max_depth=6,\n", "    #min_child_weight=1,\n", "    subsample=0.8,\n", "    colsample_bytree=1,\n", "    scale_pos_weight=1,\n", "    gamma=0.1,\n", "    min_child_weight=3,\n", "    seed=1000)\n", "# this may take some minutes\n", "gs_cv = GridSearchCV(est, param_grid, cv=3, scoring = 'neg_mean_squared_error').fit(X_small, y_small)\n", "\n", "print(\"End up tuning at \", dt.datetime.now())\n", "# best hyperparameter setting\n", "gs_cv.grid_scores_"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["# Set up folds\n", "K = 3\n", "kf = model_selection.KFold(n_splits = K, random_state = 1, shuffle = True)\n", "np.random.seed(1)\n", "\n", "\n", "# model\n", "# xgboost\n", "boost_params = {'eval_metric': 'rmse'}\n", "model = xgb.XGBRegressor(\n", "    learning_rate=0.01,\n", "    objective='reg:linear',\n", "    n_estimators=10000,\n", "    max_depth=5,\n", "    min_child_weight=2,\n", "    subsample=0.8,\n", "    colsample_bytree=1,\n", "    scale_pos_weight=1,\n", "#     gamma=4,\n", "#     min_child_weight=3,\n", "    seed=1000)\n", "#seed=27"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n", "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n", "from sklearn import metrics\n", "from sklearn.decomposition import PCA\n", "from matplotlib import pyplot as plt\n", "import math\n", "\n", "def RMSLE(y, pred):\n", "    return metrics.mean_squared_error(y, pred)**0.5\n", "n_folds = 5\n", "def rmsle_cv(model):\n", "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train[col].values)\n", "    rmse= np.sqrt(-cross_val_score(model, train_x, train_y, scoring=\"neg_mean_squared_error\", cv = kf))\n", "    return(rmse)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["model.fit(X1,y1)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["pred=model.predict(X1)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["importances=model.feature_importances_\n", "fig, ax = plt.subplots(figsize=(20, 10))\n", "plt.bar(range(train[col].head(1000).shape[1]),importances.tolist())\n", "plt.xticks(range(train[col].head(1000).shape[1]),list(train[col].head(10)),fontsize=16, rotation=90)\n", "plt.show()"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["RMSLE(y, pred)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["RMSLE(y1, pred)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["pred_hideout=model.predict(X_hideout)\n", "RMSLE(y_hideout,pred_hideout)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["pred2=model.predict(X2)\n", "RMSLE(y2,pred2)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["\n", "print(\"Finished setting up CV folds and regressor at \", dt.datetime.now())\n", "# Run CV\n", "\n", "print(\"Started CV at \", dt.datetime.now())\n", "for i, (train_index, test_index) in enumerate(kf.split(X)):\n", "    # Create data for this fold\n", "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n", "    X_train, X_valid = X.iloc[train_index, :].copy(), X.iloc[test_index, :].copy()\n", "    X_test = test[col]\n", "    print(\"\\nFold \", i)\n", "\n", "    fit_model = model.fit(X_train, y_train)\n", "    pred = model.predict(X_valid)\n", "    print('RMSLE GBM Regressor, validation set, fold ', i, ': ', RMSLE(y_valid, pred))\n", "\n", "    pred_hideout = model.predict(X_hideout)\n", "    print('RMSLE GBM Regressor, hideout set, fold ', i, ': ', RMSLE(y_hideout, pred_hideout))\n", "    print('Prediction length on validation set, GBM Regressor, fold ', i, ': ', len(pred))\n", "    # Accumulate test set predictions\n", "\n", "    pred = model.predict(X_test)\n", "    print('Prediction length on test set, GBM Regressor, fold ', i, ': ', len(pred))\n", "    y_test_pred += pred\n", "\n", "    del X_test, X_train, X_valid, y_train\n", "\n", "print(\"Finished CV at \", dt.datetime.now())\n", "y_test_pred /= K  # Average test set predictions\n", "print(\"Finished average test set predictions at \", dt.datetime.now())"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["test['is_outlier']=np.nan"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["y_test_pred=model.predict(test[col])"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["test.head()"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "source": ["# Create submission file\n", "sub = pd.DataFrame()\n", "sub['id'] = test['id']\n", "sub['visitors'] = np.expm1(y_test_pred) # .clip(lower=0.)\n", "sub.to_csv('C:/Users/Administrator/Desktop/input/xgboost_submit2.csv', float_format='%.6f', index=False)\n", "\n", "print('We are done. That is all, folks!')\n", "finish_time = dt.datetime.now()\n", "print(\"Started at \", finish_time)\n", "elapsed = finish_time - start_time\n", "print(\"Elapsed time: \", elapsed)"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.4", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4}