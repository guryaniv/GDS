{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93f3e145-a484-3449-e2d9-0d0867f8fdb7"
      },
      "source": [
        "**Microsoft [LightGBM][1]** is a powerful, open-source boosted decision tree library similar to xgboost. In practice, it runs even faster than xgboost and achieves better performance in some cases.\n",
        "\n",
        "To install LightGBM, follow the [installation guide][2] to get the C++ distribution. The python API can then be easily built with these [instructions][3].\n",
        "\n",
        "Some useful resources for LightGBM python API and parameter tuning:\n",
        "\n",
        "**[Python API Documentation][4]:** this page includes all the functions and objects\n",
        "\n",
        "**[List of Parameters][5]:** all possible parameters for LightGBM functions and classes\n",
        "\n",
        "**[Parameter Tuning Guide][6]:** the advanced parameter tuning guide for LightGBM. Since most parameters in LightGBM are similar to those in XGBoost, it should be intuitive to follow.\n",
        "\n",
        "\n",
        "  [1]: https://github.com/Microsoft/LightGBM\n",
        "  [2]: https://github.com/Microsoft/LightGBM/wiki/Installation-Guide\n",
        "  [3]: https://github.com/Microsoft/LightGBM/tree/master/python-package\n",
        "  [4]: https://github.com/Microsoft/LightGBM/blob/master/docs/Python-API.md\n",
        "  [5]: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md\n",
        "  [6]: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7abe71f2-0ad1-4f9a-a86b-987dfe79a6e0"
      },
      "source": [
        "## Import Libraries, Preprocessing ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6fbbab02-330c-10b1-99af-e546690d46c1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import lightgbm as lgbm\n",
        "\n",
        "def preprocess_1(train_df, test_df):\n",
        "    \"\"\"Just a generic preprocessing function, feel free to substitute it with your custom function\"\"\"\n",
        "    # encode target variable\n",
        "    train_df['interest_level'] = train_df['interest_level'].apply(lambda x: {'high': 0, 'medium': 1, 'low': 2}[x])\n",
        "    test_df['interest_level'] = -1\n",
        "    train_index = train_df.index\n",
        "    test_index = test_df.index\n",
        "    data_df = pd.concat((train_df, test_df), axis=0)\n",
        "    del train_df, test_df\n",
        "    \n",
        "    # add counting features\n",
        "    data_df['num_photos'] = data_df['photos'].apply(len)\n",
        "    data_df['num_features'] = data_df['features'].apply(len)\n",
        "    data_df['num_description'] = data_df['description'].apply(lambda x: len(x.split(' ')))\n",
        "    data_df.drop('photos', axis=1, inplace=True)\n",
        "    \n",
        "    # naive feature engineering\n",
        "    data_df['room_difference'] = data_df['bedrooms'] - data_df['bathrooms']\n",
        "    data_df['total_rooms'] = data_df['bedrooms'] + data_df['bathrooms']\n",
        "    data_df['price_per_room'] = data_df['price'] / (data_df['total_rooms'] + 1)\n",
        "    \n",
        "    # add datetime features\n",
        "    data_df['created'] = pd.to_datetime(data_df['created'])\n",
        "    data_df['c_month'] = data_df['created'].dt.month\n",
        "    data_df['c_day'] = data_df['created'].dt.day\n",
        "    data_df['c_hour'] = data_df['created'].dt.hour\n",
        "    data_df['c_dayofyear'] = data_df['created'].dt.dayofyear\n",
        "    data_df.drop('created', axis=1, inplace=True)\n",
        "    \n",
        "    # encode categorical features\n",
        "    for col in ['display_address', 'street_address', 'manager_id', 'building_id']:\n",
        "        data_df[col] = LabelEncoder().fit_transform(data_df[col])\n",
        "       \n",
        "    data_df.drop('description', axis=1, inplace=True)\n",
        "    \n",
        "    # get text features\n",
        "    data_df['features'] = data_df['features'].apply(lambda x: ' '.join(['_'.join(i.split(' ')) for i in x]))\n",
        "    textcv = CountVectorizer(stop_words='english', max_features=200)\n",
        "    text_features = pd.DataFrame(textcv.fit_transform(data_df['features']).toarray(),\n",
        "                                 columns=['f_' + format(x, '03d') for x in range(1, 201)],\n",
        "                                 index=data_df.index)\n",
        "    data_df = pd.concat(objs=(data_df, text_features), axis=1)\n",
        "    data_df.drop('features', axis=1, inplace=True)\n",
        "    \n",
        "    feature_cols = [x for x in data_df.columns if x not in {'interest_level'}]\n",
        "    return data_df.loc[train_index, feature_cols], data_df.loc[train_index, 'interest_level'],\\\n",
        "        data_df.loc[test_index, feature_cols]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc1f3e65-0a99-9cb6-88c1-c988c1f62a3e"
      },
      "source": [
        "## Load Data ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd0afa7c-97ec-c6f4-aaaa-f1ad3cc65e4e"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json(open(\"../input/train.json\", \"r\"))\n",
        "test = pd.read_json(open(\"../input/test.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be284d13-56c8-7715-3fb1-b264da276660"
      },
      "source": [
        "## Define Hyperparameters for LightGBMClassifier ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a596eb31-d895-4386-71bf-551b6918e39b"
      },
      "outputs": [],
      "source": [
        "# the following dictionary contains most of the relavant hyperparameters for our task\n",
        "# I haven't tuned them yet, so they are mostly default\n",
        "t4_params = {\n",
        "    'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n",
        "    'num_leaves': 2**4, 'learning_rate': 0.05, 'max_depth': -1,\n",
        "    'max_bin': 255, 'subsample_for_bin': 50000,\n",
        "    'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha': 1, 'reg_lambda': 0,\n",
        "    'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n",
        "\n",
        "# they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\n",
        "t4 = lgbm.sklearn.LGBMClassifier(n_estimators=1000, seed=0, **t4_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa2c3f9b-21fd-be18-c760-cc337ce7d262"
      },
      "source": [
        "## Early Stopping with Cross Validation ##\n",
        "Similar to xgboost, we can use cross validation with early stopping to efficiently determine the optimal \"**n_estimators**\" value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bbdfc5d2-d933-cfc5-acff-817069bbf358"
      },
      "outputs": [],
      "source": [
        "def cross_validate_lgbm(filename_str, preprocess_func=preprocess_1):\n",
        "    lgbm_params = t4_params.copy()\n",
        "    lgbm_params['num_class'] = 3\n",
        "    train_X, train_y, test_df = preprocess_func(train, test)\n",
        "    dset = lgbm.Dataset(train_X, train_y, silent=True)\n",
        "    cv_results = lgbm.cv(\n",
        "        lgbm_params, dset, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='multi_logloss',\n",
        "        early_stopping_rounds=100, verbose_eval=50, show_stdv=True, seed=0)\n",
        "    # note: cv_results will look like: {\"multi_logloss-mean\": <a list of historical mean>,\n",
        "    # \"multi_logloss-stdv\": <a list of historical standard deviation>}\n",
        "    json.dump(cv_results, open(filename_str, 'w'))\n",
        "    print(filename_str)\n",
        "    print('best n_estimators:', len(cv_results['multi_logloss-mean']))\n",
        "    print('best cv score:', cv_results['multi_logloss-mean'][-1])\n",
        "\n",
        "# we simply have to run the following code each time we modify the hyperparameters:\n",
        "cross_validate_lgbm('lgbm_1.json')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}