{"cells":[{"metadata":{"_uuid":"ae847e0d691e75c4f5806fa66b7016fec7e85a0d"},"cell_type":"markdown","source":"## Google Analytics Customer Revenue Prediction\n\n### Predict how much GStore customers will spend\n"},{"metadata":{"_uuid":"9240f3f3b1db3f9ac8493344cdee9ea03c209283"},"cell_type":"markdown","source":"### 1.Business/Real World Problem:\nThe 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies\n"},{"metadata":{"_uuid":"a55fdb9ba57526a3c6fa5479d06d2b5009cb1d07"},"cell_type":"markdown","source":"### 2.Objectives \n#### Objective:\nwe are challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.\n\n"},{"metadata":{"_uuid":"e565fb660ec3b5008cc5d87e48f9eda3c43da6b4"},"cell_type":"markdown","source":"### 3.Data Information\n\"Train_v2.csv\" and \"Test_v2.csv\" contains the data necessary to make predictions for each \"fullVisitorId\" listed in \"sample_submission_v2.csv\"."},{"metadata":{"_uuid":"bf93c48fc3e9297b31f0f19d8ca800dde27f14ba"},"cell_type":"markdown","source":"### This is little about our data\nBoth train_v2.csv and test_v2.csv contain the columns (Features) . Each row in the dataset is one visit to the store. Because we are predicting the log of the total revenue per user, be aware that not all rows in test_v2.csv will correspond to a row in the submission, but all unique fullVisitorIds will correspond to a row in the submission.\n\nThere are multiple columns which contain JSON blobs of varying depth. In one of those JSON columns, totals, the sub-column transactionRevenue contains the revenue information we are trying to predict. This sub-column exists only for the training data.\n\n\n####  Where the data comes from\n\nThe sample dataset contains Google Analytics 360 data from the Google Merchandise Store, a real ecommerce store. The Google Merchandise Store sells Google branded merchandise. The data is typical of what you would see for an ecommerce website. It includes the following kinds of information:\n\n1.Traffic source data: information about where website visitors originate. This includes data about organic traffic, paid search traffic, display traffic, etc.\n\n2.Content data: information about the behavior of users on the site. This includes the URLs of pages that visitors look at, how they interact with content, etc.\n\n3.Transactional data: information about the transactions that occur on the Google Merchandise Store website."},{"metadata":{"trusted":true,"_uuid":"1e9576bddf4595236c66245a8f5f0c3af83e2487"},"cell_type":"code","source":"#IMPORTING LIBRARIES needed for this problem \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\nimport warnings \nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"b6028656f0e0ee4a38b5647d7f36ed8d3b7ddea5"},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca694bd9861ab5111e3ef07ed29169f01f010316"},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a9039a7b64fefeec51a85b32dc3a888101d67fcc"},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"96a15b1dad72859da4bc673c2ffbc3ab79940c4b"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1871b12f3c0c1811cb2d5fd136c54ef887bf9e6"},"cell_type":"markdown","source":"Few columns contains JSON objects , we will convert them into CSV files.So, that we can perform all our operations on data without much struggle"},{"metadata":{"_uuid":"f82b7ac811382fb5044e9c4dce87fcc5b23c191b"},"cell_type":"markdown","source":"### Features (Data Fields)\nEach row in the dataset is one visit to the store. We are predicting the natural log of the sum of all transactions per user.\n\n#### Data Fields\n\n**fullVisitorId**- A unique identifier for each user of the Google Merchandise Store.\n\n**channelGrouping** - The channel via which the user came to the Store.\n\n**date** - The date on which the user visited the Store.\n\n**device** - The specifications for the device used to access the Store.\n\n**geoNetwork** - This section contains information about the geography of the user.\n\n**sessionId** - A unique identifier for this visit to the store.\n\n**socialEngagementType** - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n\n**totals** - This section contains aggregate values across the session.\n\n**trafficSource** - This section contains information about the Traffic Source from which the session originated.\n\n**visitId** - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique \nto the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n\n**visitNumber** - The session number for this user. If this is the first session, then this is set to 1.\n\n**visitStartTime** - The timestamp (expressed as POSIX time)."},{"metadata":{"trusted":true,"_uuid":"4c3630ce13636309d5230a1e79fa1326de437944"},"cell_type":"code","source":"from pandas.io.json import json_normalize\nimport json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97bb9cf51469d2a5e6648b3a389f84dd9b9e2078","_kg_hide-input":true},"cell_type":"code","source":"\ncolumns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n\n#dir_path = \"all/train.csv\" # you can change to your local \n\n\n#Code to transform the json format columns in table\ndef json_read(df):\n    #joining the [ path + df received]\n    data_frame = '../input/train.csv'\n    \n    #Importing the dataset\n    df = pd.read_csv(data_frame, \n                     converters={column: json.loads for column in columns}, # loading the json columns properly\n                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n                     nrows = None\n                     )\n    \n    for column in columns: #loop to finally transform the columns in data frame\n        #It will normalize and set the json to a table\n        column_as_df = json_normalize(df[column]) \n        # here will be set the name using the category and subcategory of json columns\n        column_as_df.columns = [\"{column}.{subcolumn}\".format(column=column,subcolumn=subcolumn) for subcolumn in column_as_df.columns] \n        # after extracting the values, let drop the original columns\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n    # Printing the shape of dataframes that was imported     \n    #print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n    return df # returning the df after importing and transforming","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4567b560c3b75efdcfdef0af2023b0007b0d7d1f"},"cell_type":"code","source":"train_df = json_read(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8fc745c383d5f8d3c99321a7102be7a59847db3"},"cell_type":"code","source":"#let's see the shape after conversion of JSON values into columns\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6c198d819fcbaa2ed94f2906926f269e040596fa"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89356c194b86adf2a4e2eb667ee6683866edbb68"},"cell_type":"markdown","source":"### About values in the above data in 'train_df' dataframe\n\n**\"(not set)\"** means Google Analytics can't received any information. \nhttps://support.google.com/analytics/answer/2820717?hl=en\n\nand\n\n**\"not available in demo dataset\"** is shown only in Sample Dataset. This means that some real data are removed. https://support.google.com/analytics/answer/7586738?hl=en"},{"metadata":{"_uuid":"beda47212fc680e22c3ecee6d214144afe261d20"},"cell_type":"markdown","source":"### ----------------------------------------------------------------------------------------------------------"},{"metadata":{"_uuid":"b0c30fe123eb4853127d576e08c61ef5c745b383"},"cell_type":"markdown","source":"**Note :** Now applyig the same process(converting from json to columns ,which we have done for train_dataset) to test_datset also.\nWe will keep the test_dataset unseen but the transformations which we have done for train_datset should also be done for test_datset"},{"metadata":{"trusted":true,"_uuid":"91dd6989d5959e2b21947d0597b910080de9d9ea"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff4d2931bae926ed236f634e23818f043389601"},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"ceef087c44cef42ddb1ea9541b4bd433779c9231"},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59d0ec60a7022ee94c67eee9f6bb11b85f48d8e8","_kg_hide-input":true},"cell_type":"code","source":"\ncolumns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n\n#dir_path = \"all/train.csv\" # you can change to your local \n\n\n\n#Code to transform the json format columns in table\ndef json_read_test(df):\n    #joining the [ path + df received]\n    data_frame = '../input/test.csv'\n    \n    #Importing the dataset\n    df = pd.read_csv(data_frame, \n                     converters={column: json.loads for column in columns}, # loading the json columns properly\n                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n                     nrows = None\n                     )\n    \n    for column in columns: #loop to finally transform the columns in data frame\n        #It will normalize and set the json to a table\n        column_as_df = json_normalize(df[column]) \n        # here will be set the name using the category and subcategory of json columns\n        column_as_df.columns = [\"{column}.{subcolumn}\".format(column=column,subcolumn=subcolumn) for subcolumn in column_as_df.columns] \n        # after extracting the values, let drop the original columns\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n    # Printing the shape of dataframes that was imported     \n    #print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n    return df # returning the df after importing and transforming","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"916d549688d0f466849b098f57dc3cfc7b9b763a"},"cell_type":"code","source":"test_df = json_read_test(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a422e1e375d46b43e51c67c2cfff490afd23064"},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f6b6ed86aca245f48bfafe40a9e97f564c442db"},"cell_type":"markdown","source":"#### Missing Values percentage in Train Dataset"},{"metadata":{"_uuid":"5afd867e2488c986c62cbb9b7ce5c801b54d6b0f"},"cell_type":"markdown","source":"Let's plot the missing values percentage for columns having missing values.\n\nThe following graph shows only those columns having missing values, all other columns are fine."},{"metadata":{"trusted":true,"_uuid":"2a81268fa5cc265ae84077965819d230a21ed367"},"cell_type":"code","source":"missing_values_percentage = {}\nfor key, value in dict(train_df.isna().sum(axis=0)).items():\n    if value == 0:\n        continue\n    missing_values_percentage[key] = 100 * float(value) / len(train_df)\n    \nsorted_x = sorted(missing_values_percentage.items(), reverse=True)\nprint (\"There are \" + str(len(missing_values_percentage)) + \" columns with missing values\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0900308c6cb815ddd96e982beacf8cdc76865a16","_kg_hide-input":true},"cell_type":"code","source":"#Using plotly to plot the missing-values percentage in train dataset\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n\ndata = [go.Bar(                            #Bar chart\n            x=list(missing_values_percentage.values()),\n            y=list(missing_values_percentage.keys()),\n            orientation = 'h'\n)]\n\nlayout = go.Layout(title=\"Missing Values Percentage\",           #Layout for bar-chart\n                   xaxis=dict(title=\"Missing Percentage\"), \n                   height=400, margin=dict(l=250, r=200))\nfigure = go.Figure(data = data , layout = layout)\n\n\npy.iplot(figure,filename='missing values percentage')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e979d6a53b8d409744195de140c5244b1713e338"},"cell_type":"markdown","source":"So, from this plot we can see that some features have very large number of missing values"},{"metadata":{"_uuid":"1aaf6a8633a58850937c7bdc74c2cfa6815fec82"},"cell_type":"markdown","source":"## 4 Data Exploration\n### 4.1 Univariate Analysis\nAs there are many columns in the dataset. Many sub-columns related to one attribute. So, we will be analyzing each column with its sub column\n"},{"metadata":{"_uuid":"3f72c09f4984e61f9e74f90d939d7900a8adbe78"},"cell_type":"markdown","source":"First, Let's analyze the target variable. --- 'totals.transactionRevenue'\n\nAnalysis - Distribution (How it is distributed)"},{"metadata":{"_uuid":"0cb9262f7a69b73226e5315d45d573de253219d3"},"cell_type":"markdown","source":"#### 4.1.1 totals.transactionRevenue --- Target Variable"},{"metadata":{"trusted":true,"_uuid":"159d00403141ce123f79c9d695485f85ddb3c369"},"cell_type":"code","source":"train_df['totals.transactionRevenue'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ec87df9ec9a231e452417341ce866958b97e5c"},"cell_type":"markdown","source":"So, by this we can say that there are huge number of null values in the given column."},{"metadata":{"trusted":true,"_uuid":"43f198bca72df57f296df96d2d6b8f49b2bda94d"},"cell_type":"code","source":"train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f906bca4ab1800c73d0c9a5a438b41b06ccce96"},"cell_type":"code","source":"type(train_df['totals.transactionRevenue'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac19e8ca17dc3b90019bfbebf3953acb2fae1e45","_kg_hide-input":true},"cell_type":"code","source":"# Printing some statistics of our data\n\n#Min value of the transactionRevenue\nprint(\"Transaction Revenue Min Value: \", \n      train_df[train_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].min()) \n\n#Mean value of the transactionRevenue\nprint(\"Transaction Revenue Mean Value: \", \n      train_df[train_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].mean()) # mean value\n\n#Median value of the transactionRevenue\nprint(\"Transaction Revenue Median Value: \", \n      train_df[train_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].median()) # median value\n\n#Max value of the transactionRevenue\nprint(\"Transaction Revenue Max Value: \", \n      train_df[train_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"].max()) # the max value\n\n\n# seting the figure size of our plots\nplt.figure(figsize=(14,5))\n\n\n# ordering the total of users and seting the values of transactions to understanding \nplt.scatter(range(train_df.shape[0]), np.sort(train_df['totals.transactionRevenue'].values))\nplt.xlabel('Index', fontsize=15) # xlabel and size of words\nplt.ylabel('Revenue value', fontsize=15) # ylabel and size of words\nplt.title(\"Revenue Value Distribution\", fontsize=20) # Setting Title and fontsize\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04d9b387e5fb328c422b6a6b2a1d4081f5583c9c"},"cell_type":"markdown","source":"Here, in the above plot , as my \"max_TransactionRevenue\" values are so huge. In the below plot, I am taking log values of all transaction_revenue values. So, we can get a good plot.\n\nand also\n\nAs we are predicting the natural log of sum of all transactions of the user, let us sum up the transaction revenue at user level and take a log and then do a scatter plot"},{"metadata":{"trusted":true,"_uuid":"e5b4961b6d2e190362223ff0a475ab4e3756a324"},"cell_type":"code","source":"grouped_df = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(grouped_df.shape[0]), np.sort(np.log1p(grouped_df[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.title(\"Revenue Value Distribution\", fontsize=20) # Setting Title and fontsize\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3538c856ff6b1bf530ca702d255d05cdb63e39bc"},"cell_type":"code","source":"#Filling the NaN values with 0 in the totals.transactionRevenue column because as per the given rules when there is no transaction , the revenue generated will be zero.\ntrain_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6002394f44c757fa07b114cf2715697ab17e9c27"},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(train_df['totals.transactionRevenue'])\nplt.title(\"Distribution of Total TransactionRevenue\");\nplt.xlabel(\"total.TransactionsRevenue\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95b6a68bb74e26646305d5fc7c11781cd6642b78"},"cell_type":"markdown","source":"This seems like lognormal distribution (Power-law distribution) which is 80-20 rule \n  \n                                                          which confirms competition overview.  \n\n The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies\n"},{"metadata":{"_uuid":"bc08736b0fa9d206a76ab1e54ecdc4f57861b35e"},"cell_type":"markdown","source":"#### 4.1.2 Device Information"},{"metadata":{"trusted":true,"_uuid":"91232269817cc3e4a9d95e58374d74d93e54f3ac"},"cell_type":"code","source":"device_column = [col for col in train_df.columns if 'device' in col]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"32ba5cbf1392685cafc4744e6f9044fedbdf0bee"},"cell_type":"code","source":"#let's see the device_atribute part of the dataset to know which features which might the helpful\ntrain_df[device_column].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f048844c4d461bdd9cd3c46f4bffa67e6cf9cd15"},"cell_type":"markdown","source":"\nAmong all these , we will consider **\"device.browser , device.isMobile , device.deviceCategory , device.operatingSystem\"**-- these features for analysis as values for remaining features are not available in the given dataset"},{"metadata":{"trusted":true,"_uuid":"5a91ec10cd4bbb08f94d6917b6774481d7d94dd0"},"cell_type":"code","source":"#selected specific features from device attributes \ndevice_cols = ['device.browser' , 'device.isMobile' , 'device.deviceCategory' , 'device.operatingSystem']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc8c115ba64abb06d629a31b72110aeef0691a6","_kg_hide-input":true},"cell_type":"code","source":"from plotly.offline import iplot\nplots = []\ncolors = [\"green\",\"violet\",\"blue\",\"red\"]\nfor color,column in enumerate(device_cols):\n    each_column = train_df[column].value_counts()\n    plots.append(go.Bar(marker=dict(opacity=0.5,color=colors[color]),orientation=\"h\", y = each_column.index[:15][::-1], x = each_column.values[:15][::-1]))\n#each_column.index[:15][::-1] -------- #taking 15 items from column and also in reversing order [::-1]\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Visits: Browser\", \"Visits: Mobile\", \"Visits: Category\" ,\"Visits: OS\"], print_grid=False)\nfig.append_trace(plots[0], 1, 1)\nfig.append_trace(plots[1], 1, 2)\nfig.append_trace(plots[2], 2, 1)\nfig.append_trace(plots[3], 2, 2)\n\nfig['layout'].update(height=800, showlegend=False,xaxis=dict(title=\"no of users\"), title=\"Visits by Device Attributes\")\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a99a03489a52f3e364fbd8cd09a91992c58515d2"},"cell_type":"markdown","source":"#### Calculating the transaction revenues for each category in each features.\nInorder to know which one is category in which feature is impacting more."},{"metadata":{"trusted":true,"_uuid":"fce141c61bf87d26fdfe65d0297ef31c1c737673","_kg_hide-input":true},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\n\n\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Mean Transaction revenue: Browser\", \"Mean Transaction revenue: Mobile\", \"Mean Transaction revenue: Category\", \"Mean Transaction revenue: OS\"], print_grid=False)\n\ndevice_columns = ['device.browser' , 'device.isMobile' , 'device.deviceCategory' , 'device.operatingSystem']\n\ncolors = [\"green\",\"violet\",\"blue\",\"red\"]\nplts = []\nfor color, column in enumerate(device_columns):\n    temporary_var = train_df.groupby(column).agg({\"totals.transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals.transactionRevenue\" : \"Mean Revenue\"})\n    temporary_var = temporary_var.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    each_bar = go.Bar(x = temporary_var[\"Mean Revenue\"][::-1], orientation=\"h\", marker=dict(opacity=0.5, color=colors[color]), y = temporary_var[column][::-1])\n    plts.append(each_bar)\n\nfig.append_trace(plts[0], 1, 1)\nfig.append_trace(plts[1], 1, 2)\nfig.append_trace(plts[2], 2, 1)\nfig.append_trace(plts[3], 2, 2)\nfig['layout'].update(height=800, showlegend=False, title=\"Mean Transaction revenue by Device Attributes\")\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ecfd504f640e4e3fff89df365013985aa25175f"},"cell_type":"markdown","source":"### Observations:\n#### about device_attribute\n\n1.**Browser**:\n\n   ----> There is interesting results in this  feature analysis.\n  \n   ----> 1.Chrome browser is the one which is used by most of the visitors.\n   \n   ----> 2.FireFox browser is the one from which most of transactionRevenue is generated. This might be possible because all of them who visit the website doesn't mean they will purchase.We can assume as, the one who used firefox browser had made purchases.\n   \n2.**Mobile**:\n\n   ----> 1.Most of the people don't use the mobile to visit the Gstore (which is ecommercial store)\n  \n   ----> 2.As, most of the visitors don't use mobile. Most of the revenue will not be genearted through mobile devices.\n   \n3.**Category**:\n\n   ----> Which type of device is used by most of the visitors and from which type of device 'revenue' is generated.\n  \n   ----> Desktop is the one which is used by most of the visitors. Most of transaction is generated by it when compared to other devices.  \n   \n4.**OS (Operating system)**:\n\n   ----> There is interesting results in this  feature analysis.\n  \n   ----> 1.Windows Operating system is the one which is used by most of the visitors.\n   \n   ----> 2.Chrome OS is the one from which most of transactionRevenue is generated. This might be possible because all of them who visit the website doesn't mean they will purchase. We can assume as (our assumptions), the one who uses Chrome OS had made purchases and also as it is GStore , they are using ChromeOS which is Google product. They might have more trust and likely towards Google, they(People who use ChromeOS) are the one from which most of transaction_Revenue has been generated."},{"metadata":{"_uuid":"34540c2449d4b6e6cc496201512b3821eabad4e5"},"cell_type":"markdown","source":"#### 4.1.3 GeoNetwork attributes"},{"metadata":{"trusted":true,"_uuid":"db95229f27f9d5b89616bb291e56938f0cdb6521"},"cell_type":"code","source":"geo_net_column = [col for col in train_df.columns if 'geoNetwork' in col]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"046f656757ffe9da39079afdf508d3c12597cab1"},"cell_type":"code","source":"geo_net_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78cdb3e0c30fcc073d057af2e99f9c6abc4b2583"},"cell_type":"code","source":"train_df[geo_net_column].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1391cc7a5e21f26e4d1fd37eb8c602aeb68d4433"},"cell_type":"code","source":"geo_net_cols = ['geoNetwork.continent' ,'geoNetwork.subContinent', 'geoNetwork.country']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d08f8fd2efd832be9730fa38ea3f6134cb4eb7f"},"cell_type":"markdown","source":"Among all these , we will consider **\"geoNetwork.continent , geoNetwork.country , geoNetwork.subContinent**-- these features for analysis as values for remaining features are not available in the given dataset"},{"metadata":{"trusted":true,"_uuid":"f6420e869ceedd6d9b5bb87ed5d94c6c05f92bd7","_kg_hide-input":true},"cell_type":"code","source":"from plotly.offline import iplot\nplots = []\ncolors = [\"green\",\"blue\",\"red\"]\nfor color,column in enumerate(geo_net_cols):\n    each_column = train_df[column].value_counts()\n    plots.append(go.Bar(marker=dict(opacity=0.5,color=colors[color]),orientation=\"h\", y = each_column.index[:15][::-1], x = each_column.values[:15][::-1]))\n#each_column.index[:15][::-1] -------- #taking 15 items from column and also in reversing order [::-1]\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Visits: Continent\", \"Visits: subContinent\", \"Visits: country\" ], print_grid=False)\nfig.append_trace(plots[0], 1, 1)\nfig.append_trace(plots[1], 1, 2)\nfig.append_trace(plots[2], 2, 1)\n\nfig['layout'].update(height=800, showlegend=False,xaxis=dict(title=\"no of users\"), title=\"Visits by GeoNetwork Attributes\")\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a5694bc3e7d6c005a14fee49770bb014ef631bc"},"cell_type":"markdown","source":"#### Calculating the transaction revenues for each category in each features.\nInorder to know which one is category in which feature is impacting more."},{"metadata":{"trusted":true,"_uuid":"43a2a15ee289899deb5c7ba0e4d009d136ce3dc7","_kg_hide-input":true},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\n\n\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Mean Transaction revenue: Continent\", \"Mean Transaction revenue: SubContinent\", \"Mean Transaction revenue: Country\"], print_grid=False)\n\n\ncolors = [\"green\",\"blue\",\"red\"]\nplts = []\nfor color, column in enumerate(geo_net_cols):\n    temporary_var = train_df.groupby(column).agg({\"totals.transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals.transactionRevenue\" : \"Mean Revenue\"})\n    temporary_var = temporary_var.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    each_bar = go.Bar(x = temporary_var[\"Mean Revenue\"][::-1], orientation=\"h\", marker=dict(opacity=0.5, color=colors[color]), y = temporary_var[column][::-1])\n    plts.append(each_bar)\n\nfig.append_trace(plts[0], 1, 1)\nfig.append_trace(plts[1], 1, 2)\nfig.append_trace(plts[2], 2, 1)\n\nfig['layout'].update(height=800, showlegend=False, title=\"Mean Transaction revenue by GeoNetwork Attributes\")\n\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23459e52317da9f849abb4a94e7ab774dc22cc0","_kg_hide-input":true},"cell_type":"code","source":"#Jsut a fancy visualization for one of the above feature (Nothing new beyond that)\n\n# plotly globe credits - https://www.kaggle.com/arthurtok/generation-unemployed-interactive-plotly-visuals\ntemporary_var = train_df[\"geoNetwork.country\"].value_counts()\n\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = temporary_var.index,\n        z = temporary_var.values,\n        locationmode = 'country names',\n        text = temporary_var.values,\n        marker = dict(\n            line = dict(color = '#fff', width = 2)) )           ]\n\nlayout = dict(\n    height=500,\n    title = 'Visits by Country',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = '#222',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n\n\n\n########################----------------------MEAN TRANSACTION REVENUE FOR COUNTRIES----------------------------################\n\n\n\n# plotly globe credits - https://www.kaggle.com/arthurtok/generation-unemployed-interactive-plotly-visuals\n\ntemporary_var = train_df.groupby(\"geoNetwork.country\").agg({\"totals.transactionRevenue\" : \"mean\"}).reset_index()\n\ncolorscale = [[0, 'rgb(102,194,165)'], [0.005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = temporary_var['geoNetwork.country'],\n        z = temporary_var['totals.transactionRevenue'],\n        locationmode = 'country names',\n        text = temporary_var['totals.transactionRevenue'],\n        marker = dict(\n            line = dict(color = '#fff', width = 2)) ) ]\n\nlayout = dict(\n    height=500,\n    title = 'Mean Transaction Revenue by Countries',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = '#222',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 60,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = False,\n                gridcolor = 'rgb(102, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"328c6cb6245a14366a5cc3d8f4353f9999bf7a45"},"cell_type":"markdown","source":"### Observations:\n#### about GeoNetwork_attribute\n\n1.**Continent**:\n\n   ----> There is very interesting results in this feature analysis.\n  \n   ----> 1. American continent is the one which is used by most of the visitors.\n   \n   ----> 2. Number of Visitors to America is nearly 39times greater than Africa.But, Africa is the one from which most of transactionRevenue is generated followed by Asian continent. \n   \n2.**Sub-Continent**:\n\n   ----> This will be Similar to Continent Statistics as we can treat Sub-Continet is like a child class to Continent.\n   \n   ----> 1.NorthAmerica followed by SouthEast_Asia are the two sub-continents from where most of visitors come from.\n   \n   ----> 2.EasternAfrica and EastAsia are sub-Continents from where highest transaction revenue is generated from.\n   \n3.**Country**:\n\n   ----> As we discussed, This will be similar to above understanding because we can think Country is like a child class to Sub-Continent.\n  \n   ----> 1.USA has highest number of visitors. It has nearly 364k visitors. India is next highest number of visitor with 51k. \n   \n   ----> 2.Anguilla is the country from where highest revenue is generated.(Which is in Africa region) followed by Curaco."},{"metadata":{"_uuid":"9a5cb2ea3d5b3d212c5b2508e2db6bd2c4159655"},"cell_type":"markdown","source":"#### 4.1.4 trafficSource attributes"},{"metadata":{"trusted":true,"_uuid":"fa257af45dc59a05aaba318c91ef390fa58e490c"},"cell_type":"code","source":"trafficsource_columns = [col for col in train_df.columns if 'trafficSource' in col]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"a712de7939b551659be52ffe6f688608e46381a7"},"cell_type":"code","source":"trafficsource_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7896afdf4ed6a8b72094192b0297551c4377d5d8"},"cell_type":"code","source":"train_df[trafficsource_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc92e8ba4de89db6bea7c69332b5c655e7827697"},"cell_type":"code","source":"trafficSource_cols = ['trafficSource.campaign','trafficSource.medium','trafficSource.source']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e051282f0cc0ec4c924e2fad6e1d4736e48d96c8"},"cell_type":"markdown","source":"Among all these , we will consider **\"trafficSource.campaign','trafficSource.medium','trafficSource.source**-- these features for analysis as values for remaining features are not available or doesn't look useful in the given dataset"},{"metadata":{"trusted":true,"_uuid":"f3b35eba84c2280a808fb502cc74b470c98c5ff5","_kg_hide-input":true},"cell_type":"code","source":"plots = []\ncolors = [\"green\",\"blue\",\"red\"]\nfor color,column in enumerate(trafficSource_cols):\n    each_column = train_df[column].value_counts()\n    plots.append(go.Bar(marker=dict(opacity=0.5,color=colors[color]),orientation=\"h\", y = each_column.index[:15][::-1], x = each_column.values[:15][::-1]))\n#each_column.index[:15][::-1] -------- #taking 15 items from column and also in reversing order [::-1]\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"trafficSource: Campaign\", \"trafficSource: Medium\", \"trafficSource: Source\" ], print_grid=False)\nfig.append_trace(plots[0], 1, 1)\nfig.append_trace(plots[1], 1, 2)\nfig.append_trace(plots[2], 2, 1)\n\nfig['layout'].update(height=800, showlegend=False,xaxis=dict(title=\"no of users using trafficSource\"), title=\"Traffic Source\")\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4d666f5d2c601830e2661bf2e81cfbdffc4ecad"},"cell_type":"markdown","source":"#### Calculating the transaction revenues for each category in each features.\nInorder to know which one is category in which feature is impacting more."},{"metadata":{"trusted":true,"_uuid":"e07fc7b0e12df273af7513daae6ad191fd9a2a96","_kg_hide-input":true},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\n\n\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Mean Transaction revenue: Campaign\", \"Mean Transaction revenue: Medium\", \"Mean Transaction revenue: Source\"], print_grid=False)\n\n\ncolors = [\"green\",\"blue\",\"red\"]\nplts = []\nfor color, column in enumerate(trafficSource_cols):\n    temporary_var = train_df.groupby(column).agg({\"totals.transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals.transactionRevenue\" : \"Mean Revenue\"})\n    temporary_var = temporary_var.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    each_bar = go.Bar(x = temporary_var[\"Mean Revenue\"][::-1], orientation=\"h\", marker=dict(opacity=0.5, color=colors[color]), y = temporary_var[column][::-1])\n    plts.append(each_bar)\n\nfig.append_trace(plts[0], 1, 1)\nfig.append_trace(plts[1], 1, 2)\nfig.append_trace(plts[2], 2, 1)\n\nfig['layout'].update(height=800, showlegend=False, title=\"Mean Transaction revenue by Traffic_Source Attributes\")\n\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe727ca1e34ec2a81e367937aa3455a09de6e40f"},"cell_type":"markdown","source":"### Observations:\n#### about trafficSource Attributes\n\n1.**Campaign**:\n\n   ----> Most of the visitors trafficSource came through campaign 'not set' (that means through unknown source , this might be possible as there will be many unknown sources (all unknown sources are together taken as 'not set')). \n  \n   ----> Highest mean TransactionRevenue is generated from campaign called 'V-Accessories'.\n   \n   \n2.**Medium**:\n\n   ----> Organic is the medium through which many visitors have visited G-store.\n   \n   ----> But through 'cpm' medium , highest transactionRevenue is generated.\n   \n   \n3.**Source**:\n\n   ----> Most of the users have visited G-Store using Google (as a source). (Of high volume visitors)\n  \n   ----> But 'secamp.com' is the one source through which most of the revenue is generated. Surprisingly, in the revenue generation process , google hasn't been succeeded (it stands at 10th or far more places)."},{"metadata":{"_uuid":"f1148c2a8fef1489c48344ac7a3b4ae9d9a8e6ee"},"cell_type":"markdown","source":"#### 4.1.5 Channel Grouping."},{"metadata":{"trusted":true,"_uuid":"0f8f56d2eb36762b1032b628e34cf285b2fc873d","_kg_hide-input":true},"cell_type":"code","source":"channel_group_counts = train_df['channelGrouping'].value_counts()\nvalues_channel_group = channel_group_counts.values \nindex_channel_group = channel_group_counts.index\ndomain_channel_group = {'x': [0.2, 0.50], 'y': [0.0, 0.33]}\nfig = {\n  \"data\": [\n    {\n      \"values\": values_channel_group,\n      \"labels\": index_channel_group,\n      \"domain\": {\"x\": [0, .48]},\n    \"marker\" : dict(colors=[\"#ef86a2\" ,'#89edd4',  '#f7ee71']),\n      \"name\": \"Channel Grouping\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .7,\n      \"type\": \"pie\"\n    }\n   ],   \n  \"layout\": {\"title\":\"Channel Grouping\",\n      \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\": \"Channel Grouping\",\n                \"x\": 0.11,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\niplot(fig)\n#took from pavansangapati kernel","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cecc6f4c8bc0aebb34fc1e2c9ae397dcadda6d14"},"cell_type":"markdown","source":"### Observations:\n#### about channelGrouping\n\n1.Channel Grouping is something through which channel user came to the store (through which he/she had visited the G-store)\n\n2.Through Organic Search, 42.2% of the users have visited G-store then followed by Social channel (nearly 25% of the users visited the G-store)"},{"metadata":{"_uuid":"d21aa58772598b32238e40a31b777e0cc7f0bb9f"},"cell_type":"markdown","source":"#### 4.1.6 Date feature"},{"metadata":{"trusted":true,"_uuid":"d12cf81297493aa259cf4fd386d432f8fcaea4a1"},"cell_type":"code","source":"train_df['date'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aeacb393ba1b4230957e49dfb3cb3c22283c7d1"},"cell_type":"code","source":"#given date is not in the appropriate format. So we will try to convert that first using lambda function \nimport datetime\ntrain_df['date'] = train_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d5af1c573b23cd9c93fe062f02a17b38fc050c7","_kg_hide-input":true},"cell_type":"code","source":"temporary_var = train_df['date'].value_counts().to_frame().reset_index().sort_values('index')\ntemporary_var = temporary_var.rename(columns = {\"index\" : \"dateX\", \"date\" : \"visits\"})\n\npltt = go.Scatter(mode=\"lines\", x = temporary_var[\"dateX\"].astype(str), y = temporary_var[\"visits\"])\nlayout = go.Layout(title=\"User Visits by date(month wise)\", height=400)\nfig = go.Figure(data = [pltt], layout = layout)\niplot(fig)\n\n#calculating transactionRevenue\n\ntemporary_var = train_df.groupby(\"date\").agg({\"totals.transactionRevenue\" : \"mean\"}).reset_index()\ntemporary_var = temporary_var.rename(columns = {\"date\" : \"dateX\", \"totals.transactionRevenue\" : \"mean_revenue\"})\npltt = go.Scatter(mode=\"lines\", x = temporary_var[\"dateX\"].astype(str), y = temporary_var[\"mean_revenue\"])\nlayout = go.Layout(title=\"Monthly TransactionRevenue by date\", height=400)\nfig = go.Figure(data = [pltt], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d654ed596886ce65a85c970b94157529bed39f00"},"cell_type":"markdown","source":"### Observations:\n#### about  'date' feature\n--Interesting Observation-- . There is Contrast Observation from 'Visitors to Store' and 'Transaction revenue' mainly in date feature.\n\n1.From the plot, we can see that in Nov 2016 , there are high number of visitors to G-store.\n\n2.Surprising to see. In Nov2016, Despite of having more number of visitors, Monthly_Transaction_Revenue was almost least in that month.\n\n3.May2017 has generated highest transaction revenue despite of having least number of visits to store."},{"metadata":{"trusted":true,"_uuid":"10d75bf7a000c99052de8999443ef49abd7471e5"},"cell_type":"code","source":"#Applying same transformation to test dataset also\ntest_df['date'] = test_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"734d956b3d0c1dc107cefdb8a5263735b74d84ed"},"cell_type":"markdown","source":"#### 4.1.7 Number of Visitors and Common Visitors"},{"metadata":{"trusted":true,"_uuid":"8f48e55458d301840f2a860621d0982f4f10bc86"},"cell_type":"code","source":"print(\"Number of unique visitors in train set : \",train_df.fullVisitorId.nunique(), \" out of rows : \",train_df.shape[0])\nprint(\"Number of unique visitors in test set : \",test_df.fullVisitorId.nunique(), \" out of rows : \",test_df.shape[0])\nprint(\"Number of common visitors in train and test set : \",len(set(train_df.fullVisitorId.unique()).intersection(set(test_df.fullVisitorId.unique())) ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"282cf4621a86eec750184d5a1d6f38b05be94773"},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f916b6173b9e9da5d17e0aa55404e27d0c0ccb"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"4c3009de9ea6c861ace4c9b666141bad7f9fc423"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4be4c760e731fdfe289435d8188eb82e0471aac"},"cell_type":"markdown","source":"#### 4.1.7 Visitor Profile Attributes"},{"metadata":{"trusted":true,"_uuid":"70f45446442a1479176267f569e2cccbcc8d38cf"},"cell_type":"code","source":"total_cols = [col for col in train_df.columns if 'totals' in col]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"b1f4f9d5d9f8527b604dba7026c1912c5d0bd00e"},"cell_type":"code","source":"total_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19d8f53fa8e80fcdf88076d8a03b0c1200376b30"},"cell_type":"code","source":"ttl_cols = ['totals.hits','totals.pageviews']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29b9000d83c67e948128f845ac5d075643e80f51","_kg_hide-input":true},"cell_type":"code","source":"plots = []\ncolors = [\"green\",\"red\"]\nfor color,column in enumerate(ttl_cols):\n    each_column = train_df[column].value_counts()\n    plots.append(go.Bar(marker=dict(opacity=0.5,color=colors[color]),orientation=\"h\", y = each_column.index[:15][::-1], x = each_column.values[:15][::-1]))\n#each_column.index[:15][::-1] -------- #taking 15 items from column and also in reversing order [::-1]\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=[\"Visits: hits\", \"Visits: pageviews\"], print_grid=False)\nfig.append_trace(plots[0], 1, 1)\nfig.append_trace(plots[1], 1, 2)\n\nfig['layout'].update(height=800, showlegend=False,xaxis=dict(title=\"Visitor Profile hits & views\"), title=\"Visitor profile\")\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd37308cabc6a3a147e746a2eed0392e2f6bca55"},"cell_type":"markdown","source":"#### Calculating the transaction revenues for each category in each features.\nInorder to know which one is category in which feature is impacting more."},{"metadata":{"trusted":true,"_uuid":"b6078cc1bc394777b66a27475594a2c269d4a3f4","_kg_hide-input":true},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\n\n\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=[\"Mean Transaction revenue:hits\", \"Mean Transaction revenue:PageViews\"], print_grid=False)\n\n\ncolors = [\"green\",\"red\"]\nplts = []\nfor color, column in enumerate(ttl_cols):\n    temporary_var = train_df.groupby(column).agg({\"totals.transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals.transactionRevenue\" : \"Mean Revenue\"})\n    temporary_var = temporary_var.dropna().sort_values(\"Mean Revenue\", ascending = False)\n    each_bar = go.Bar(x = temporary_var[\"Mean Revenue\"][::-1], orientation=\"h\", marker=dict(opacity=0.5, color=colors[color]), y = temporary_var[column][::-1])\n    plts.append(each_bar)\n\nfig.append_trace(plts[0], 1, 1)\nfig.append_trace(plts[1], 1, 2)\n\n\nfig['layout'].update(height=800, showlegend=False, title=\"Mean Transaction revenue by Visitor profile attributes (totals)\")\n\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a003d4ca460805cbacdbbead4f9eb34bc5c173a1"},"cell_type":"markdown","source":"### Observations:\n#### about  'totals' feature (visitorProfileAttributes)\n\n1.Most of the visitors has only least number of hits and page views (they have hit or viewed mostly once).\n\n2.Count plot shows decreasing nature i.e. we have a very high total count for less number of hits and page views per visitor transaction and the overall count decreases when the number of hits per visitor transaction increases.\n\n3.we are unable to get any clear trend(or)pattern related to TransactionsRevenue as per the hits and pageViews"},{"metadata":{"_uuid":"776a2ba289a570e0e9c22658528fc3ea640685c7"},"cell_type":"markdown","source":"### 5.Creating a Baseline model\n\n**5.1 Preprocessing** \n\nInitially , we will remove the columns which is not useful for creating the model\n \n------>Drop Columns with constant values\n\n------>Drop Ids and other non relevant columns\n"},{"metadata":{"trusted":true,"_uuid":"f670cd590d5a9001dc0d62d5e41e9e13bf763b69"},"cell_type":"code","source":"#columns with constant values\nconstant_columns = [column for column in train_df.columns if train_df[column].nunique(dropna=False)==1 ]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"9822d455cf6d2728e113d9a8dda2f60cb9257e9e"},"cell_type":"code","source":"constant_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a29490f9bc32351818c382b4392cc8a21190fb82"},"cell_type":"code","source":"## non relevant columns\nnon_relevant = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d025e84cb45c92a93fb610f943fbe6449b45867"},"cell_type":"code","source":"train_df_model_columns = train_df.drop(columns=constant_columns)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"102249cc68304149ec6ca5a5cfa76fa011de44ba"},"cell_type":"code","source":"train_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31bb03625f291b8c2a7a9a70f2a1fbf087eaf515"},"cell_type":"code","source":"type(train_df_model_columns['date'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8bbd90e9017d9bfdd9b6be6e1f0c4f35bcb7cd1"},"cell_type":"code","source":"#sorting by date inorder to perform time-based slicing\nsorted_by_date_train_df_model_columns = train_df_model_columns.sort_values(by='date',ascending=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24c1f1a61f043dac297d34e0a22431f4b646953b"},"cell_type":"code","source":"sorted_by_date_train_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74b42baa54fc9c05bd7e8d3499412f7454a404cb"},"cell_type":"code","source":"train_df_model_columns = sorted_by_date_train_df_model_columns.drop(columns=non_relevant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7aa1e5da511459eb3838545604e4974c6033711"},"cell_type":"code","source":"train_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34dcb9e8caf64ae202162db41a5fd2839faa2db1"},"cell_type":"code","source":"train_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"755cafb9bd4b02e38291a9a7e30bbd2607c294a3"},"cell_type":"code","source":"train_df_model_columns.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237d6a67190d4a5fd80d00ee9fd9d01bd12f3ed7"},"cell_type":"markdown","source":"#### We will be doing same process for test data also\nremoving \"constant_columns\" and also \"non-relevant columns\" as well as we will be checking and comparing both train_df and test_df to check whether is there any columns(features) missing in test_df when compared to train_df."},{"metadata":{"trusted":true,"_uuid":"5f2130ec7880ee8a97c96e9754fef453546c4123"},"cell_type":"code","source":"test_df_model_columns = test_df.drop(columns=constant_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"956b33852b10bd89729b28c3486cfb084b76fc74"},"cell_type":"code","source":"test_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d550118cc7524030d2e45a5e0558e06aefc5182b"},"cell_type":"code","source":"test_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a95464b8c1740a78e62148b76fb4ca57bd9bab5f"},"cell_type":"code","source":"type(test_df_model_columns['fullVisitorId'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c98acd43f208ce1ec8706fd2f5b302cf5749080"},"cell_type":"code","source":"test_df_model_columns_with_id = test_df_model_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea897d88fc51231d08aec1a6fd9699aa84e0827c"},"cell_type":"code","source":"test_df_model_columns_with_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"201df7d443792d69dd4790aa208546a6cb4bf859"},"cell_type":"code","source":"type(test_df_model_columns_with_id['fullVisitorId'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c12a9eee9ba7e7799a36ec64ec795fd11f617047"},"cell_type":"code","source":"test_df_model_columns_with_id['fullVisitorId'] = test_df_model_columns_with_id['fullVisitorId'].astype('float') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beee54410bbd3677a7b8128391f647ab61d82345"},"cell_type":"code","source":"test_df_model_columns = test_df_model_columns.drop(columns=non_relevant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60d3e48a86b0bb9f45d274cd3bf3189fff97d2c6"},"cell_type":"code","source":"test_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e35ec455934eab661aa936d4f1f530a6c5dcd41"},"cell_type":"code","source":"#We will look at the variable names which are there in train dataset and not in test dataset.\nprint(\"Variables not in test_df_model_columns but in train_df_model_columns : \", set(train_df_model_columns.columns).difference(set(test_df_model_columns.columns)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c38874eea818c11fc7c19fbddf30058d36f09df7"},"cell_type":"markdown","source":"Anyway 'totals.transactionRevenue' will not be there in test_df_model_columns because it is the feature we need to predict.\nBut 'trafficSource.campaignCode' is not in train_df_model_columns. So, we will be removing this feature from 'trafficSource.campaignCode. So, we will be removing 'trafficSource.campaignCode' from the train_dataset"},{"metadata":{"trusted":true,"_uuid":"97c2df72f990619d1a79827484598b175e325553"},"cell_type":"code","source":"train_df_model_columns = train_df_model_columns.drop(columns='trafficSource.campaignCode')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d15fc051a5b99db2086eb4592381bc092c95516a"},"cell_type":"code","source":"train_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a65f0e245b9d8d2ec69ee9da65ef5b871d39117d"},"cell_type":"code","source":"#We will look at the variable names which are there in test dataset and not in train dataset.\nprint(\"Variables not in train_df_model_columns but in test_df_model_columns : \", set(test_df_model_columns.columns).difference(set(train_df_model_columns.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b41decdea085e6be843c0d846ed565227ef0f90b"},"cell_type":"code","source":"test_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57851616846bc487bf332a7ba8f9b146f570b288"},"cell_type":"markdown","source":"#### 5.2 Handling Categorical variables"},{"metadata":{"trusted":true,"_uuid":"7cb22099e7413ca2ca69bc8c301bdcd07ef2e010"},"cell_type":"code","source":"#Label encoding the Categorical variables (train_data)\nfrom sklearn.preprocessing import LabelEncoder\n\ncategorical_columns = [column for column in train_df_model_columns.columns if not column.startswith('total')]\ncategorical_columns = [column for column in categorical_columns if column not in constant_columns + non_relevant]\n\nfor column in categorical_columns:\n\n    le = LabelEncoder()\n    train_values = list(train_df_model_columns[column].values.astype(str))\n    test_values = list(test_df_model_columns[column].values.astype(str))\n    \n    le.fit(train_values + test_values)\n    \n    train_df_model_columns[column] = le.transform(train_values)\n    test_df_model_columns[column] = le.transform(test_values)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd1ab705cde25180d14fbec092de58625a5fdb1"},"cell_type":"code","source":"train_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20966066db68cefb31805db1b52d787e9373880"},"cell_type":"code","source":"test_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72725635e751b7bce7775a2c7cd7985ac5e3eb5e"},"cell_type":"markdown","source":"#### 5.3 Handling Numerical variables"},{"metadata":{"trusted":true,"_uuid":"f8816cc389b1e31074f41d4a311a50075729e318"},"cell_type":"code","source":"#filling the NA values in totals column\ntrain_df_model_columns['totals.bounces'] = train_df_model_columns['totals.bounces'].fillna(0.0)\ntrain_df_model_columns['totals.newVisits'] = train_df_model_columns['totals.newVisits'].fillna(0.0)\ntest_df_model_columns['totals.bounces'] = test_df_model_columns['totals.bounces'].fillna(0.0)\ntest_df_model_columns['totals.newVisits'] = test_df_model_columns['totals.newVisits'].fillna(0.0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28f020fa0df95eabc177ecda1b52b00afc3896c8"},"cell_type":"code","source":"def normalize_numerical_columns(dataframe, isTrainDataset = True):\n    dataframe[\"totals.hits\"] = dataframe[\"totals.hits\"].astype(float)\n    #dataframe[\"totals.hits\"] = (dataframe[\"totals.hits\"] - min(dataframe[\"totals.hits\"])) / (max(dataframe[\"totals.hits\"]) - min(dataframe[\"totals.hits\"]))\n\n    dataframe[\"totals.pageviews\"] = dataframe[\"totals.pageviews\"].astype(float)\n    #dataframe[\"totals.pageviews\"] = (dataframe[\"totals.pageviews\"] - min(dataframe[\"totals.pageviews\"])) / (max(dataframe[\"totals.pageviews\"]) - min(dataframe[\"totals.pageviews\"]))\n    \n    dataframe[\"totals.bounces\"] = dataframe[\"totals.bounces\"].astype(float)\n    dataframe[\"totals.newVisits\"] = dataframe[\"totals.newVisits\"].astype(float)\n    \n    \n    if isTrainDataset:\n        dataframe[\"totals.transactionRevenue\"] = dataframe[\"totals.transactionRevenue\"].fillna(0.0)\n    return dataframe ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"075bbecf6359f0b52ebc53a0185942c1ed205573"},"cell_type":"code","source":"train_df_model_columns = normalize_numerical_columns(train_df_model_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b63bd9822991247fffcfa3a078ea94e9e3aea479"},"cell_type":"code","source":"train_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3d8f52b8068989e956dcf84f30648e6104cf0bd"},"cell_type":"code","source":"test_df_model_columns = normalize_numerical_columns(test_df_model_columns,isTrainDataset=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b7a02f0d0c8e541d5fca1849b003084038a3bf4"},"cell_type":"code","source":"test_df_model_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8a34dbc95b0d6e33c1f2013920329add663b2b9"},"cell_type":"markdown","source":"#### 5.4 Generate Training and Validation Sets"},{"metadata":{"_uuid":"a4e28b86b261a22b73f2e871fddc1c1a1c86b470"},"cell_type":"markdown","source":"Now let us create development and validation splits based on time to build the model. We can take the last two months as validation sample."},{"metadata":{"_uuid":"a3e0ea8f83037c542eae319798437b5f9153fad0"},"cell_type":"markdown","source":"1. we will be splitting in the ratio of 70:30.\n    That too we will be performing time-based slicing because in order to get accurate results for future data , we need to know how our model is performing on recent data,so we can estimate our accuracy for future data. So, the most recent in the train dataset , we will be using for cross validation and older ones can be used as train_data"},{"metadata":{"trusted":true,"_uuid":"81aeef876c15d6d92f55963acace4e9cb4095326"},"cell_type":"code","source":"train_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"504c4eb982361ae480089d17d010e0791dc359b8"},"cell_type":"code","source":"num = 903653*70\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9a5b87447af1c7967c117a3abda54f5a9167e75"},"cell_type":"code","source":"num/100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfb6d5f921973789ff51972aaa864eca7da27f58"},"cell_type":"code","source":"train_df_model_columns_train = train_df_model_columns[:632557]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b727a2bb66dbcc805277ce888f9c96fc71d9dd9a"},"cell_type":"code","source":"train_df_model_columns_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a07ac26fdbc4c8332b86f2a05df5935531ed3e7b"},"cell_type":"markdown","source":"remaining in the train_df will be considered as validation data"},{"metadata":{"trusted":true,"_uuid":"3de883775faa32d42ab6abe3dfbcbc2085cb82be"},"cell_type":"code","source":"train_df_model_columns_cv = train_df_model_columns[632558:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69f421ddec18e21615ef72b7db13e8ca2868c03"},"cell_type":"code","source":"train_df_model_columns_cv.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e0d27de09cb944185a0113a9b3d512737d0907f"},"cell_type":"markdown","source":"Dividing into train and cross-validation data"},{"metadata":{"trusted":true,"_uuid":"2aea2484354561e2ed65aeda0c883b2289a7f46b"},"cell_type":"code","source":"train_X = train_df_model_columns_train.drop(columns='totals.transactionRevenue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c369cebdfe72720821f2bec7e3be9b9c24e1aa0"},"cell_type":"code","source":"train_Y = np.log1p(train_df_model_columns_train['totals.transactionRevenue'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c16d49800ad5238074eb385412ee960830ee00c"},"cell_type":"code","source":"train_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a72a992bbaa389859243e99c97894d21a2b2061c"},"cell_type":"code","source":"train_Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e865dfff71bc707d8d5cad8b0a671d87eef42460"},"cell_type":"code","source":"cv_X = train_df_model_columns_cv.drop(columns='totals.transactionRevenue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5220eafe1b47db962d2c1db9aa04d66d1056081"},"cell_type":"code","source":"cv_Y = np.log1p(train_df_model_columns_cv['totals.transactionRevenue'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"570683da37d33d02e87d61cd3f5d5de7e60f147c"},"cell_type":"code","source":"cv_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c8375f62c6486fd17b8ea5d28e55c563d2e727"},"cell_type":"code","source":"cv_Y.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1a560596a51349f9cf03b733354cec142d32a4e"},"cell_type":"markdown","source":"#### 5.5 Training the model\ntraining the model using lightgbm"},{"metadata":{"trusted":true,"_uuid":"cb1a8b5d2131dbd5e0a8c579af488220ffe70873"},"cell_type":"code","source":"import lightgbm as lightGBM \n\n#about parameters of lightgbm --- https://lightgbm.readthedocs.io/en/latest/Parameters.html\n\nlightGBM_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n              \"num_leaves\" : 100, \"learning_rate\" : 0.02, \n              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9,\"bagging_seed\" : 2019,\"use_best_model\":True,\"colsample_bytree\":0.9}\n\n\nlightGBM_train = lightGBM.Dataset(train_X, label=train_Y)\nlightGBM_crossVal = lightGBM.Dataset(cv_X, label=cv_Y)\nlightGBM_model = lightGBM.train(lightGBM_params, lightGBM_train, 700, valid_sets=[lightGBM_crossVal], early_stopping_rounds=150, verbose_eval=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c15b41ce0410e1fa4b68fd2961a94106b5cfde36"},"cell_type":"code","source":"test_df_model_columns.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91839b844cc50237f1fe1a96a743aa1a167b93f3"},"cell_type":"code","source":"prediction = lightGBM_model.predict(test_df_model_columns, num_iteration=lightGBM_model.best_iteration)\nfinal_df = pd.DataFrame({\"fullVisitorId\":test_df['fullVisitorId']})\nprediction[prediction<0] = 0\n\nfinal_df[\"PredictedLogRevenue\"] = np.expm1(prediction)\n\nfinal_df = final_df.groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"sum\"}).reset_index()\nfinal_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n\nfinal_df[\"PredictedLogRevenue\"] = np.log1p(final_df[\"PredictedLogRevenue\"])\n\nfinal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e8be001535aa80c468f6e3a50053ecdb25431dd"},"cell_type":"code","source":"final_df.to_csv(\"baseline_lightGBM_6.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b4b2c125999c99958e855c478f1699fdcf02192"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,18))\nlightGBM.plot_importance(lightGBM_model, max_num_features=30, height=0.8, ax=ax)\nplt.title(\"LightGBM-Feature Importance\", fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b7a7b1b06073917a532e47fc6692592899704c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd2eb0486a3bc68e02167a8622586b68eeb72c1b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7368e6c681a4d8962d640ea83b554b3d652275a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0503d26b18c8fc66a94856283bdf63c718a99aa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e44da809599a4dded7ee3affc7f05fcd3a651f06"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}