{"cells": [{"cell_type": "markdown", "source": ["I'm new to Kaggle competion. I have reviewed many public kernels before I aggregate many parts into this final working version. I'm mostly inspired by the follow kernels.\n", "\n", "https://www.kaggle.com/lopuhin/eli5-for-mercari\n", "\n", "https://www.kaggle.com/apapiu/ridge-script\n", "\n", "https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling\n", "\n", "https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44823"], "metadata": {}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import eli5\n", "import nltk\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.model_selection import KFold\n", "from sklearn.linear_model import Ridge\n", "from sklearn.pipeline import FeatureUnion\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.metrics import mean_squared_log_error\n", "from nltk.stem.porter import PorterStemmer\n", "import gc\n", "import re"], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["Load and preprocess data."], "metadata": {}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train = pd.read_table('../input/train.tsv')\n", "test = pd.read_table('../input/test.tsv')\n", "STOP_WORDS = frozenset([\n", "    \"a\", \"about\", \"after\", \"afterwards\", \"again\",\n", "    \"all\", \"almost\", \"along\", \"already\", \"also\", \"although\",\n", "    \"am\", \"among\", \"amongst\", \"amoungst\",  \"an\", \"and\", \"another\",\n", "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n", "    \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n", "    \"becomes\", \"becoming\", \"been\", \"before\",\"behind\", \"being\",\n", "    \"beside\", \"between\", \"both\",\n", "    \"but\", \"by\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n", "    \"could\", \"couldnt\", \"de\", \"do\",\n", "    \"each\", \"eg\", \"eight\", \"either\", \"else\",\n", "    \"elsewhere\", \"etc\", \"even\", \"ever\", \"every\",\n", "    \"everything\", \"everywhere\",  \"few\",\n", "    \"find\", \"for\",\n", "    \"from\", \"go\",\n", "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n", "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n", "    \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\"\n", "    \"into\", \"is\", \"it\", \"its\", \"itself\",\n", "    \"latterly\", \"ltd\", \"many\", \"may\", \"me\",\n", "    \"meanwhile\", \"might\", \"mill\", \"mine\",\"moreover\",\n", "    \"my\", \"myself\",\"neither\",\n", "    \"never\", \"nevertheless\", \"no\", \"nobody\", \"none\", \"noone\",\n", "    \"nor\", \"not\", \"now\", \"of\",  \"on\",\n", "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n", "    \"ours\", \"ourselves\", \"out\", \"per\", \"perhaps\",\n", "     \"re\",\n", "    \"seeming\", \"seems\", \"she\",\n", "    \"since\", \"so\", \"some\", \"somehow\", \"someone\",\n", "    \"something\", \"sometime\", \"sometimes\", \"somewhere\",\n", "    \"that\", \"the\", \"their\", \"them\",\n", "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n", "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n", "    \"this\", \"those\", \"though\",\n", "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n", "    \"twelve\",  \"un\", \"until\", \"up\", \"upon\", \"us\",\n", "     \"via\", \"was\", \"we\",  \"were\", \"what\", \"when\",\n", "    \"whence\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n", "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n", "    \"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"with\",\n", "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n", "    \"yourselves\"])\n", "\n", "NAME_MIN_DF = 10\n", "MAX_FEATURES_ITEM_NAME = 50000\n", "MAX_FEATURES_ITEM_DESCRIPTION = 100000\n", "\n", "transformerWeights={\n", "        'name': 1.0,\n", "        'general_cat': 1.0,\n", "        'subcat_1': 1.0,\n", "        'subcat_2': 1.0,\n", "        'brand_name': 1.2,\n", "        'shipping': 1.0,\n", "        'item_condition_id': 1.0,\n", "        'len_description': 1.0,\n", "        'item_description': 0.8\n", "    }\n", "\n", "\n", "train.drop(train[train.price < 1.0].index, inplace=True)\n", "train = train.reset_index(drop=True)\n", "nrow_train = train.shape[0]\n", "train_test : pd.DataFrame = pd.concat([train, test])\n", "\n", "y_train = np.log1p(train['price'])\n", "\n", "del train\n", "gc.collect()"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_test['category_name'] = train_test['category_name'].fillna('Other').astype(str)\n", "train_test['brand_name'] = train_test['brand_name'].fillna('missing').astype(str)\n", "train_test['shipping'] = train_test['shipping'].astype(str)\n", "train_test['item_condition_id'] = train_test['item_condition_id'].astype(str)\n", "train_test['item_description'] = train_test['item_description'].fillna('[ndy]')\n", "\n", "def replace_text(df, variable, text_to_replace, replacement):\n", "    df.loc[df[variable] == text_to_replace, variable] = replacement\n", "    \n", "    \n", "replace_text(train_test, 'item_description', 'No description yet', '[ndy]')\n", "\n", "def wordCount(text):\n", "    # convert to lower case and strip regex\n", "    try:\n", "         # convert to lower case and strip regex\n", "        text = text.lower()\n", "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n", "        txt = regex.sub(\" \", text)\n", "        # tokenize\n", "        # words = nltk.word_tokenize(clean_txt)\n", "        # remove words in stop words\n", "        words = [w for w in txt.split(\" \") \\\n", "                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n", "        \n", "        if len(words) < 30:\n", "            return \"1\"\n", "        elif len(words) < 90:\n", "            return \"2\"\n", "        elif len(words) < 120:\n", "            return \"3\"\n", "        else: \n", "            return \"4\"\n", "    except: \n", "        return \"1\"\n", "    \n", "train_test['len_description'] = train_test['item_description'].apply(lambda x: wordCount(x))\n", "\n", "train_test['general_cat'], train_test['subcat_1'], train_test['subcat_2'] = train_test['category_name'].str.split(\"/\", 2).str\n", "train_test.drop('category_name', axis=1, inplace=True)\n", "\n", "train_test['general_cat'] = train_test['general_cat'].fillna('Other').astype(str)\n", "train_test['subcat_1'] = train_test['subcat_1'].fillna('Other').astype(str)\n", "train_test['subcat_2'] = train_test['subcat_2'].fillna('Other').astype(str)"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train_test.head()"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["y_train.head()"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "default_preprocessor = CountVectorizer().build_preprocessor()\n", "\n", "def build_preprocessor(field):\n", "    field_idx = list(train_test.columns).index(field)\n", "    return lambda x: default_preprocessor(x[field_idx])\n", "\n", "def rex_tokenizer(text):\n", "    token_pattern = re.compile(r\"(?u)\\b\\w[\\w-]*\\w\\b\")\n", "    tokens = token_pattern.findall(text)\n", "    item_list = {\"1tb\" : \"1 tb\", \"2tb\" : \"2 tb\", \"4tb\" : \"4 tb\", \"4g\" : \"4 gb\",\"4gb\" : \"4 gb\",\"8g\" : \"8 gb\",\"8gb\" : \"8 gb\",\"16g\" : \"16 gb\", \"16gb\" : \"16 gb\", \"32gb\" : \"32 gb\", \"32g\" : \"32 gb\",\"64gb\" : \"64 gb\", \"64g\" : \"64 gb\", \"64gb\" : \"64 gb\", \"80gb\" : \"80 gb\", \"120gb\" : \"128 gb\", \"128gb\" : \"128 gb\", \"128g\" : \"128 gb\", \n", "                 \"160gb\" : \"160 gb\", \"250gb\" : \"256 gb\", \"256gb\" : \"256 gb\", \n", "                \"500g\" : \"512 gb\", \"500gb\" : \"512 gb\", \"512g\" : \"512 gb\",\"512gb\" : \"512 gb\", \"10k\" : \"10 k\", \"10kt\" : \"10 k\", \"12k\" : \"12 k\",\"14k\" : \"14 k\", \"14kt\" : \"14 k\" , \"18k\" : \"18 k\" ,\"18kt\" : \"18 k\" ,  \"1oz\" : \"1 oz\", \"4oz\" : \"4 oz\", \"5oz\" : \"5 oz\", \"8oz\" : \"8 oz\",\"36oz\" : \"36 oz\", \"64oz\" : \"64 oz\"}\n", "    postTokens = []\n", "    for item in tokens:\n", "        if item in item_list:\n", "            item = item_list[item]\n", "        postTokens.append(item)\n", "    return postTokens\n", "    \n", "vectorizer = FeatureUnion([\n", "    ('name', CountVectorizer(\n", "        ngram_range=(1, 2),\n", "        min_df=NAME_MIN_DF,\n", "        tokenizer=rex_tokenizer,\n", "        stop_words = 'english',\n", "        preprocessor=build_preprocessor('name'))),\n", "    ('general_cat', CountVectorizer(\n", "        token_pattern='.+',\n", "        preprocessor=build_preprocessor('general_cat'))),\n", "    ('subcat_1', CountVectorizer(\n", "        token_pattern='.+',\n", "        preprocessor=build_preprocessor('subcat_1'))),\n", "    ('subcat_2', CountVectorizer(\n", "        token_pattern='.+',\n", "        preprocessor=build_preprocessor('subcat_2'))),\n", "    ('brand_name', CountVectorizer(\n", "        token_pattern='.+',\n", "        preprocessor=build_preprocessor('brand_name'))),\n", "    ('shipping', CountVectorizer(\n", "        token_pattern='\\d+',\n", "        preprocessor=build_preprocessor('shipping'))),\n", "    ('item_condition_id', CountVectorizer(\n", "        token_pattern='\\d+',\n", "        preprocessor=build_preprocessor('item_condition_id'))),\n", "    ('item_description', TfidfVectorizer(\n", "        ngram_range=(1, 3),\n", "        max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n", "        stop_words = 'english',\n", "        analyzer = 'word',\n", "        tokenizer=rex_tokenizer,\n", "        preprocessor=build_preprocessor('item_description'))),\n", "    ('len_description', CountVectorizer(\n", "        token_pattern='\\d+',\n", "        preprocessor=build_preprocessor('len_description'))),\n", "], transformer_weights=transformerWeights)\n", "\n", "X_train_test = vectorizer.fit_transform(train_test.values)"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["%%time\n", "\n", "def get_rmsle(y_true, y_pred):\n", "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n", "\n", "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n", "for train_ids, valid_ids in cv.split(X_train_test[:nrow_train]):\n", "    model = Ridge(\n", "            solver='auto',\n", "            fit_intercept=True,\n", "            alpha=0.5,\n", "            max_iter=100,\n", "            normalize=False,\n", "            copy_X=True,\n", "            random_state=101,\n", "            tol=0.025)\n", "    model.fit(X_train_test[train_ids], y_train[train_ids])\n", "    y_pred_valid = model.predict(X_train_test[valid_ids])\n", "    rmsle = get_rmsle(y_pred_valid, y_train[valid_ids])\n", "    print(f'valid rmsle: {rmsle:.5f}')"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["eli5.show_weights(model, vec=vectorizer, top=100, feature_filter=lambda x: x != '<BIAS>')"], "metadata": {"collapsed": true}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["eli5.show_prediction(model, doc=train_test.values[1], vec=vectorizer)"], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["Make final prediction."], "metadata": {}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["preds = model.predict(X_train_test[nrow_train:])\n", "test[\"price\"] = np.expm1(preds)\n", "test[[\"test_id\", \"price\"]].to_csv(\"submission_ridge.csv\", index = False)"], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1}