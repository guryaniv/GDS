{"cells":[{"metadata":{"_uuid":"03ec0cae957157689a03d68209624c777855d927"},"cell_type":"markdown","source":"# Project: Final Report\nStudy by: Saurabh, Augustine, Gelesh, \nIndiana University\n\n### Changes to Phase 1 after review with Professor\n\n1. <a href='#context'>Adding data source, hyperlink, more context</a>\n2. <a href='#evaluation'>Additonal Metrics: precision, recall, FNR, f1, f0.5</a>\n3. <a href='#roc'>ROC Curve: ROC, AUC</a>\n4. <a href='#smpl_vary'>Impact of different sampling ratios</a>\n5. <a href='#feature_imp'>Variable importance:decision tree approach(not complete)</a>\n6. <a href='#feature_eng'>Feature Engineering(Investigating)</a>\n\n### Changes to Phase 2 report\n\n1. <a href='#feature_imp'>Variable importance:decision tree approach</a>\n2. <a href='#feature_sel'>Feature Selection</a>\n3. <a href='#pipeline_full'>Full Pipeline</a>\n4. <a href='#feature_eng'>Feature Engineering</a>\n5. <a href='#stats'>Statistical Significance</a>\n\n### Changes to Phase 3 report\n2. <a href='#feature_eng1'>New Features(Feature Engineering)</a>\n3. <a href='#stats1'>Final Statistical Test</a>"},{"metadata":{"_uuid":"eb59a340251edeb5aefadea1742954a5594194ba"},"cell_type":"markdown","source":"<a id='context'></a>\n# Credit Card Fraud Detection\n\nThe dataset used for this Machine Learning project is obained from Kaggle. A detailed description about the data set could be found at https://www.kaggle.com/agpickersgill/credit-card-fraud-detection/data . \n\n\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML.\n\nThe dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the dataset does not provide the original features or the background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA.  The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, which can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. "},{"metadata":{"_uuid":"c1a403d45ebfaf0106fb631e61e82c1da8f28464"},"cell_type":"markdown","source":"# Setup\nFirst, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline.\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3b0cbe3ae6e42c16d40a13d7da63e2f776d12728"},"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport os\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"7d05f75d664bd0ce4db829e49c5eeadbe64b059f"},"cell_type":"markdown","source":"# Explore Data\nThe data has been downloaded from the following website. \nhttps://www.kaggle.com/agpickersgill/credit-card-fraud-detection/data \n\nThe data is stored as creditcard.csv file, to a local folder where this notebook file is stored."},{"metadata":{"trusted":false,"_uuid":"5f4397b559d75a15449cb199fa35dea0c3416ded"},"cell_type":"code","source":"# Read data into a panda dataframe\n\ntrainFile = data = pd.read_csv(\"../input/creditcard.csv\") # pd.read_csv(\"creditcard.csv\")\ntrainFile.info()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"f048688524edc6321e03c0ba66705a905e2c9992"},"cell_type":"markdown","source":"There are 30 columns and 284807 rows where column class is the target variable. It is a binary value, which can have either 0 (not fraud) or 1 (fraud) value. Column \"Amount\" is the amount of the transaction and \"Time\" is the time of the transaction. The rest of the features V1 to V28 are not described. There are no missing values in the data\nset."},{"metadata":{"trusted":false,"_uuid":"52cdbdaae6b28e3090f88c68ab31b16690296d96"},"cell_type":"code","source":"trainFile.head()","execution_count":157,"outputs":[]},{"metadata":{"_uuid":"44ebb9aaadb71fc597bf5ab36ff738a2e70c41b5"},"cell_type":"markdown","source":"As shown above, the dataset has only one categorical variable \"Class\", which is the target variable. Features V1, V2, ... V28 are numerical values corresponding to the principal components obtained with PCA. Hence there is no scope for categorical pipeline in this case. Numerical pipelines are implemented as part of the ML pipelines, in later sections. "},{"metadata":{"_uuid":"397815bc98834ac5145f392c326e81c73c797025"},"cell_type":"markdown","source":"## Let us split fradulent and non-fraudlent transactions"},{"metadata":{"trusted":false,"_uuid":"188556a064d4e6e34797b025592eff97844df15c"},"cell_type":"code","source":"non_fraud = trainFile.loc[(trainFile[\"Class\"] ==0)]\nfraud = trainFile.loc[(trainFile[\"Class\"] ==1)]\nprint (\"Size of Fraud data:\", fraud.shape)\nprint (\"Size of non-fraud data: \", non_fraud.shape)\nclass_count = trainFile[\"Class\"].value_counts()\nclass_count.plot(kind = 'bar')\nplt.title(\"Transaction class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")","execution_count":158,"outputs":[]},{"metadata":{"_uuid":"d147681208f2ea6ae662858fc3820f87cbc95df7"},"cell_type":"markdown","source":"The data is highly unbalanced with respect of Class variable values i.e. Fraud transaction and Non-Fraud transactions. There are only 0.17% of the rows with value Class = 1."},{"metadata":{"_uuid":"f500f6b3fd0fb1b696883693d0c261d98f4e911f"},"cell_type":"markdown","source":"## Strategy"},{"metadata":{"_uuid":"090a14ac19a19f342b706d46d2a05b8861b7a7c3"},"cell_type":"markdown","source":"** There could be multiple approach for this classification problem taking into consideration the highly unbalanced data. **\n\n  1. OVER-SAMPLING: In this approach under-represented class are copied multiple times to match with the count of over\n     represented data (Class 0 in this case)\n               \n  2. UNDER-SAMPLING: In this approach instances of over-represented class are deleted.\n  \n  3. Ratio Match: In this approach each classes are split in 50-50 ratio from the dataset"},{"metadata":{"_uuid":"87130b2fb8e4991df29e134933b676ad469906cf"},"cell_type":"markdown","source":"## Approach\n\n1. We will use resampling by strategy 3 above i.e. Ratio matching and test this approach using a simple logistic regression classifier.\n\n2. After fitting the model, several performance metrics would be tested and analysed.\n\n3. We will repeat the best resampling, by tuning the parameters in the logistic regression classifier.\n\n4. We will finally perform classifications model using other classification algorithms."},{"metadata":{"_uuid":"61cbd2264432bee27c9a84bcf1bf8a0598623e27"},"cell_type":"markdown","source":"## Correlation matrix Visualization"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4bcbc2da0c1b8e31c14d4f54defdf14d3cc90439"},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import roc_auc_score","execution_count":159,"outputs":[]},{"metadata":{"_uuid":"aab92e9f8a0500f67db2574b46d93c74c8720bcf"},"cell_type":"markdown","source":"### Correlations for all the data"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c69a67700dea6f97aabd4cf538ab6ade2bebda4b"},"cell_type":"code","source":"trainFile[\"Class\"].astype('float')\ntrainFile_corr = trainFile.corr()\n","execution_count":160,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1452410ce9a0319e4b31cd84664b3a9f409d6251"},"cell_type":"code","source":"sns.heatmap(trainFile_corr, cbar = True,  square = True, annot=False, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')\nplt.show()","execution_count":161,"outputs":[]},{"metadata":{"_uuid":"83019a003d054202105fcb9151fc5c108ae82086"},"cell_type":"markdown","source":"** From the correlation matrix it can be observed that most of the data features are not correlated.This is because,most of the features (V1-V28) are the result of Principal Component Analysis (PCA) algorithm.** "},{"metadata":{"trusted":false,"_uuid":"f51461a386911f14f3f4d67e03d672ab26298609"},"cell_type":"code","source":"abs(trainFile_corr[\"Class\"]).sort_values(ascending=False)","execution_count":162,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c4ce3ecb6807f14b330829ff6da66ada40660525"},"cell_type":"code","source":"from pandas.tools.plotting import scatter_matrix\n\n# Top three correlated inputs with housing_median_age\nattributes = [\"Class\", \"V17\", \"V14\",\"V12\",\"V10\",\"V16\",\"V3\",\"V7\"]\n\nscatter_matrix(trainFile[attributes], figsize=(12, 8))","execution_count":163,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"459386e7fec379d51afefc400b1f5e2ebc6a3af6"},"cell_type":"code","source":"trainFile.plot(kind=\"scatter\", x=\"V11\", y=\"Class\",\n             alpha=0.1)","execution_count":164,"outputs":[]},{"metadata":{"_uuid":"177e415ac2fbd24bbda313e818399e3dacd95e5e"},"cell_type":"markdown","source":"### Correlations for non fraud class.\n\nAs shown above the correlation of PCA values in general is of little significance. So here we did a correlation of data separately for both the classes. \n"},{"metadata":{"trusted":false,"_uuid":"ee9cc71ca01981c7c8b4dc70e367ad41bcf8b8b4"},"cell_type":"code","source":"non_fraud_corr =  non_fraud.corr()\nsns.heatmap(non_fraud_corr, cbar = True,  square = True, annot=False, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')\nplt.show()","execution_count":165,"outputs":[]},{"metadata":{"_uuid":"eaf400b950adfa19087f794a7ccb093bcdc1cb2e"},"cell_type":"markdown","source":"### Correlations for fraud class.\n\nThe fraud transactions show significant correlations between several features which is contrast to the non-fraud class. "},{"metadata":{"trusted":false,"_uuid":"2a581b75ff9e4c9e98adf20e904a4a53f364a0e3"},"cell_type":"code","source":"fraud_corr =  fraud.corr()\nsns.heatmap(fraud_corr, cbar = True,  square = True, annot=False, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')\nplt.show()","execution_count":166,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4cd1637c96f8696a4e1cc7c437ef2a07e7bb2a52"},"cell_type":"code","source":"# Print Correlations above threshold of 0.15 for non fraud class\nrows, cols = non_fraud.shape\nflds = list(non_fraud.columns)\n\ncorr = non_fraud_corr.values\n\nfor i in range(cols):\n    for j in range(i+1, cols):\n        if abs(corr[i,j]) > 0.15:\n            print (flds[i], ' ', flds[j], ' ', corr[i,j])","execution_count":167,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b9a4a8a78bc629dd71520f38ff783deaa5c0f83e"},"cell_type":"code","source":"# Print Correlations above threshold of 0.8 for fraud class\nrows, cols = fraud.shape\nflds = list(fraud.columns)\n\ncorr = fraud_corr.values\n\nfor i in range(cols):\n    for j in range(i+1, cols):\n        if abs(corr[i,j]) > 0.8:\n            print (flds[i], ' ', flds[j], ' ', corr[i,j])","execution_count":168,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"03c3969182c11f21cf8ad0bb5904a54f2a539999"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfraud.hist( color='red', label='Fraud', bins=50, figsize=(20,15))\nnon_fraud.hist( color='blue', label='Non Fraud', bins=50, figsize=(20,15))\n\nplt.show()","execution_count":169,"outputs":[]},{"metadata":{"_uuid":"b0ed526f95c1664cf704343784f53a3dffce3283"},"cell_type":"markdown","source":"# Resampling imbalanced dataset with equal ratio of binary classes"},{"metadata":{"_uuid":"716f37d19b38288437c9efc861be97ec1ca226e4"},"cell_type":"markdown","source":"** Here we transform the dataset to have the minority class count match the majority class count.**"},{"metadata":{"trusted":false,"_uuid":"3b34d7ba6bb25aef1c92d90d8f07ca3501b8b859"},"cell_type":"code","source":"# random_state=42\n\nfraud_count = len(fraud)\n# fraud_count\nsmpl_non_fraud = non_fraud.sample(n=fraud_count, random_state=42)\n# len(smpl_non_fraud)\ntrain_data=smpl_non_fraud.append(fraud, ignore_index=True)\n\ntrain_data = shuffle(train_data)\ntrain_data.reset_index(drop=True)\n\nlen(train_data)","execution_count":170,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"19f5f0afb7b0137ec1c983f645757299ce4e55a0"},"cell_type":"code","source":"train_data.info()","execution_count":171,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"283152a63ec80d8699d43c14c91aabba0c6bba2b"},"cell_type":"code","source":"%matplotlib inline\nsns.countplot(x='Class', data=train_data)","execution_count":172,"outputs":[]},{"metadata":{"_uuid":"161cd26f8311ffdf6a512eafdeecd7e7fbc7effb"},"cell_type":"markdown","source":"# Preprocessing pipeline"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"01a61674d3ea80946fb435075f3d90917bb149fe"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":173,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"88d872bfbeb51141943bd7ae8499ba84006c3761"},"cell_type":"code","source":"# Select features to use for modeling.\n\ncc_num_attribs = list(train_data)[1:-1] # To select all features except Time\n\nnum_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cc_num_attribs)),\n        ('std_scaler', StandardScaler()),\n    ])\n","execution_count":174,"outputs":[]},{"metadata":{"_uuid":"5f18de63decdeb2c2c70ef7621ad363df7392b1c"},"cell_type":"markdown","source":"# Create a held out dataset \n\nCreating a held out dataset using the train_test_split(70 / 30)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d2bf37f9b38ec7e39190364a429fbf742397eb36"},"cell_type":"code","source":"X = train_data.loc[:,train_data.columns != 'Class']\ny = train_data.loc[:,train_data.columns == 'Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)","execution_count":175,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8844c10d3b7c01b7df5c4f424ebdd780c1580175"},"cell_type":"code","source":"cc_prepared = num_pipeline.fit_transform(X_train)\ncc_prepared","execution_count":176,"outputs":[]},{"metadata":{"_uuid":"ec104c42a9b7cd581130134c2a7656cff6699de2"},"cell_type":"markdown","source":"# Base model with logistic regression"},{"metadata":{"trusted":false,"_uuid":"36ef94ef8255ed664d0e55952708e4d429c2909e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(cc_prepared, y_train)","execution_count":177,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e616ef0eb71d300ec05fb64894852f81255c4d41"},"cell_type":"code","source":"# let's try the full pipeline on the testing data set\n\ntest_prepared = num_pipeline.fit_transform(X_test)\ny_pred = log_reg.predict(test_prepared)\n","execution_count":178,"outputs":[]},{"metadata":{"_uuid":"c513da2eb37b118d609e12e6f774741cb1b5f3c7"},"cell_type":"markdown","source":"<a id='evaluation'></a>\n## Evaluation\n\nWe plan to evaluate the ML model by analyzing the true positive (TP), true negative (TN), false positive (FP) and false negative (FN) predictions using the following metrics.\n\n1.\tPrecision : This shows the ability of the ML model to predict positive cases(fraud). This is expressed as TP / (TP+FP)\t\n2.\tRecall : This measures the sensitivity of the model to predict positive cases(fraud), This is expressed as TP / (TP+FN). This is an important metric for this ML problem and our objective is to maximize this metric.   \t\n3.\tFalse Negative Rate : This is shows the amount of fraud cases missed by the ML model. This is expressed as FN / (TP+FN). This is also an important metric for this ML problem and our objective is to minimize this metric.\n4.\tF1 Score : This is the harmonic mean of precision and recall. This is expressed as 2*TP / (2*TP+FP+FN)\t\n5.\tF0.5 Score : This is the F-beta score, where beta is 0.5. The expression to calculate the F-beta is shown below.\n\n6.\tAUC : The AUC represents a model’s ability to discriminate between fraud and non-fraud classes. An area of 1.0 represents a model that made all predictions perfectly. \n\nOur objective is to detect maximum number of fraud transactions(true positives). Hence we intend to maximize the recall and minimize the False Negative Rate. \n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"498f0a2b5a29a1b6beb3f4583c239571128373b4"},"cell_type":"code","source":"# Let us calculate the False negative rate (FNR), Miss rate\ndef fnr(y_test, y_pred):\n# from sklearn.metrics import confusion_matrix\n\n    log_cm = confusion_matrix(y_test, y_pred)\n    #tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n    tn, fp, fn, tp = log_cm.ravel()\n    # (tn, fp, fn, tp)\n    if isinstance(y_test, pd.DataFrame):\n        true_pos = len(y_test.loc[(y_test[\"Class\"] ==1)])\n    if isinstance(y_test, np.ndarray):\n        true_pos = np.count_nonzero(y_test == 1)\n    \n    #tp/true_pos # recall home_grown\n    #tp/(tp+fp) # precision home_grown\n    fnr = fn / true_pos\n    return fnr","execution_count":179,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"28fdc1ba37152147eb8ad88de68865daf54146a3"},"cell_type":"code","source":"metrics = [precision_score, \n           recall_score,\n           fnr,\n           f1_score,\n           lambda y_true, y_pred: fbeta_score(y_true, y_pred, beta=0.5),\n           roc_auc_score]\nmetrics_names = [\"Precision\", \n                 \"Recall\", \n                 \"False Negative\",\n                 \"F1\",\n                 \"F0.5\",\n                 \"AUC\"]","execution_count":180,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"69c2ff30449017a455f2c40f9215e1894f71ffa1"},"cell_type":"code","source":"samples = [(test_prepared, y_test)]\nmodels_names = [\"Logistic, Ratio(1F:1NF)\"]","execution_count":181,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"61a7a88737e1988c0f2132036323f63595f96dd5"},"cell_type":"code","source":"def evaluate(models, metrics, samples, metrics_names, models_names):\n    results = np.zeros((len(samples) * len(models), len(metrics)))\n    samples_names = []\n#     for m in models_names:\n#         samples_names.extend([m + \" Train\", m + \" Test\"])\n    for m_num, model in enumerate(models):\n        for row, sample in enumerate(samples):\n            for col, metric in enumerate(metrics):\n                results[row + m_num * 2, col] = metric(sample[1], model.predict(sample[0]))\n    results = pd.DataFrame(results, columns=metrics_names, index=models_names)\n    return results","execution_count":182,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ac68dd83cf5f56a0f61f22d9d7c060eda8b841d4"},"cell_type":"code","source":"models = [log_reg]","execution_count":183,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f6fcb98bd4e034e59012579c4d7fb717df5ede0"},"cell_type":"code","source":"res = evaluate(models, metrics, samples, metrics_names, models_names)\nres","execution_count":184,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4f18d7c0ebdbb25bd7b66dd75df524215261ca6b"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nlog_acc = accuracy_score(y_test, y_pred)\nlog_acc","execution_count":185,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc8d58928a57ab9e95bb6100b75715b8870058cd"},"cell_type":"code","source":"from sklearn.metrics import recall_score\n\nlog_recall = recall_score(y_test, y_pred)\nlog_recall","execution_count":186,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ec90a395fb22ce5322b7bd554a18492f9d8cc8c7"},"cell_type":"code","source":"from sklearn.metrics import precision_score\nlog_pre = precision_score(y_test, y_pred)\nlog_pre","execution_count":187,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"56437823a2fbbfe6d114f5806a7cb5bcd9d20497"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nlog_roc = roc_auc_score(y_test, y_pred)\nlog_roc","execution_count":188,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b15e69f2fe2d029f7acfabfa9542a74147e5730"},"cell_type":"code","source":"# Let us calculate the False negative rate (FNR), Miss rate\n# def fnr(y_test, y_pred):\n# from sklearn.metrics import confusion_matrix\nlog_cm = confusion_matrix(y_test, y_pred)\n#tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\ntn, fp, fn, tp = log_cm.ravel()\n# (tn, fp, fn, tp)\ntrue_pos = len(y_test.loc[(y_test[\"Class\"] ==1)])\n#tp/true_pos # recall home_grown\n#tp/(tp+fp) # precision home_grown\nfnr = fn / true_pos\nfnr","execution_count":189,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5b4a8d0cd8aedbcc1a701edb258f5fda767c10f6"},"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":190,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"111b3d48f17e50a586b61e7ce2e5089af9c114a6"},"cell_type":"code","source":"# Plot confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(log_cm\n                      , classes=class_names\n                      , title='Confusion matrix for base model')\nplt.show()","execution_count":191,"outputs":[]},{"metadata":{"_uuid":"5741b668db6d70ba3c98b31d43369b665246ffb5"},"cell_type":"markdown","source":"<a id='roc'></a>\n## ROC Curve\n\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"be35984aa71c605f0de319cd2ab4669752da1fa8"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)","execution_count":192,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"08503fcc913bae6b0fc23031fbd54a35bbfbef94"},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)","execution_count":193,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f7cff683d5deb6bb831b11c4d3ece5704dbe2cab"},"cell_type":"code","source":"\nplt.figure(figsize=(8, 6))\nplot_roc_curve(fpr, tpr)\nplt.grid()\nplt.show()","execution_count":194,"outputs":[]},{"metadata":{"_uuid":"08903df6ffd46ab2aaedd2f52ab0a4cc74238b2c"},"cell_type":"markdown","source":"<a id='smpl_vary'></a>\n# Impact of different sampling ratios\nHere we study the impact of the different sampling ratios on the model performance.\n\n## Sampling ratio of 1Fraud : 10 Non-fraud\n"},{"metadata":{"trusted":false,"_uuid":"b6a40bd6df09ff4db2f1fee622d7684efbc87ade"},"cell_type":"code","source":"\nfraud_count = len(fraud)\n# fraud_count\nsmpl_non_fraud = non_fraud.sample(n=fraud_count*10, random_state=42)\n# len(smpl_non_fraud)\ntrain_data=smpl_non_fraud.append(fraud, ignore_index=True)\n\ntrain_data = shuffle(train_data)\ntrain_data.reset_index(drop=True)\n\nlen(train_data)","execution_count":195,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c20019076ba3df331c658f87b6102822a1843a5b"},"cell_type":"code","source":"train_data.info()","execution_count":196,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bc9fcbc21a776c564f555c87ce4a653654266f58"},"cell_type":"code","source":"X = train_data.loc[:,train_data.columns != 'Class']\ny = train_data.loc[:,train_data.columns == 'Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)","execution_count":197,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3c513f7b76ad341678ffbe2bbb2f0f6d31e6160c"},"cell_type":"code","source":"cc_prepared = num_pipeline.fit_transform(X_train)\ntest_prepared = num_pipeline.fit_transform(X_test)\n","execution_count":198,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2e911cfaef33d5f090bda0176a2064fdd98ad06f"},"cell_type":"code","source":"log_reg.fit(cc_prepared, y_train)\ny_pred = log_reg.predict(test_prepared)","execution_count":199,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a33d953b97ec4238a37fb6a9baf62e9516cfbea7"},"cell_type":"code","source":"log_pre = precision_score(y_test, y_pred)\nlog_pre","execution_count":200,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4c0b83e4e3c679afd6df214ccf18d52305467439"},"cell_type":"code","source":"samples = [(test_prepared, y_test)]\nmodels_names = [\"Logistic, Ratio(1F:10NF)\"]","execution_count":201,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2787766d8ede323673c7b529a5f4058091a9a314"},"cell_type":"code","source":"models = [log_reg]","execution_count":202,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"11bc20750dfd2a193f6c713bd2acbcbb6776fa56"},"cell_type":"code","source":"res_10 = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_10)\nres","execution_count":203,"outputs":[]},{"metadata":{"_uuid":"aaec7a2b80eb91d449c6ccca2ca45e9ba929c367"},"cell_type":"markdown","source":"## Sampling ratio of 1Fraud : 20 Non-fraud\n"},{"metadata":{"trusted":false,"_uuid":"dcc0f091d122c03d9860d0b665401760fd1a313c"},"cell_type":"code","source":"\nfraud_count = len(fraud)\n# fraud_count\nsmpl_non_fraud = non_fraud.sample(n=fraud_count*20, random_state=42)\n# len(smpl_non_fraud)\ntrain_data=smpl_non_fraud.append(fraud, ignore_index=True)\n\ntrain_data = shuffle(train_data)\ntrain_data.reset_index(drop=True)\n\nlen(train_data)","execution_count":204,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3618bd02b356841c491bba9f45c7679848406cdf"},"cell_type":"code","source":"X = train_data.loc[:,train_data.columns != 'Class']\ny = train_data.loc[:,train_data.columns == 'Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)","execution_count":205,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f0555bd90c7916972bdd77943979e6f2b81713ef"},"cell_type":"code","source":"cc_prepared = num_pipeline.fit_transform(X_train)\ntest_prepared = num_pipeline.fit_transform(X_test)\n","execution_count":206,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e49aae5ba58cef0b926ed33437b9025bd7ac0a00"},"cell_type":"code","source":"log_reg.fit(cc_prepared, y_train)\ny_pred = log_reg.predict(test_prepared)","execution_count":207,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a54a98d15f9d0ed371a238007d6a5227e3e7c3b"},"cell_type":"code","source":"log_pre = precision_score(y_test, y_pred)\nlog_pre","execution_count":208,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"32c7508813b2fc0341369913d2b8f8cb4a86c660"},"cell_type":"code","source":"samples = [(test_prepared, y_test)]\nmodels_names = [\"Logistic, Ratio(1F:20NF)\"]","execution_count":209,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a8a59bd3420dcfc37cac2dafb3e7fad848690153"},"cell_type":"code","source":"res_20 = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_20)\nres","execution_count":210,"outputs":[]},{"metadata":{"_uuid":"ab9354c072c628ea1130ddc7fae2eeab4eb90d31"},"cell_type":"markdown","source":"** So far the best results are obtained \nwhen using equal number of fraud and non fraud classes. So we proceed with equal sampling of data.**"},{"metadata":{"_uuid":"fb67668991dc4fc9c1b8b74b3a5e3cc014f008fc"},"cell_type":"markdown","source":"# Train a Random Forest Classifier"},{"metadata":{"trusted":false,"_uuid":"977fe236bd271ef5c7f1733ef4db835624114a05"},"cell_type":"code","source":"from  sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(cc_prepared, y_train)","execution_count":211,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"96e1ea9d35abf0adaca5ac25a2ad19b1f416e9e2"},"cell_type":"code","source":"samples = [(test_prepared, y_test)]\nmodels_names = [\"RandomForest, Ratio(1F:1NF)\"]\nmodels = [rf_clf]\nres_rf = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_rf)\nres","execution_count":212,"outputs":[]},{"metadata":{"_uuid":"0d0b817554432ce4a1c00ab28913127bd2cc2a9e"},"cell_type":"markdown","source":"# Objective evaluation via K-fold crossfold validation"},{"metadata":{"_uuid":"ba3a54ba66ea47e91cfb256341bdcaab64807463"},"cell_type":"markdown","source":"## K-fold crossfold validation for Random Forest Classifier"},{"metadata":{"trusted":false,"_uuid":"b12ff346e32a9e5a431fa2e267e136ea4833fcf5"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n\nscore_randomforest = cross_val_score(rf_clf,test_prepared,y_test['Class'],scoring='recall',cv=10)\nscore_randomforest\n","execution_count":213,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c245d1be11703a0995ffc1a20273a688b3eaa1e"},"cell_type":"code","source":"validation_scores = pd.DataFrame(columns=['Model Name','mean','SD'])\n\nvalidation_scores.loc[0] = ['RF',score_randomforest.mean(),score_randomforest.std()]\n\nvalidation_scores","execution_count":214,"outputs":[]},{"metadata":{"_uuid":"3eeba24af8c164ac310bd3f0bbaeb5bf10eecf74"},"cell_type":"markdown","source":"## K-fold crossfold validation for Logistic Classifier"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"384e74a6aeeca620a531b30f8b3a9f5cb64cf3b1"},"cell_type":"code","source":"score_LR = cross_val_score(log_reg,test_prepared,y_test['Class'],scoring='recall',cv=10)\nscore_LR\nvalidation_scores.loc[1] = ['LC',score_LR.mean(),score_LR.std()]","execution_count":215,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77b994d47baa04269e464964575588866525dbe5"},"cell_type":"code","source":"validation_scores","execution_count":216,"outputs":[]},{"metadata":{"_uuid":"d9a8729c5d510f1db9065dd1021b777c56a71891"},"cell_type":"markdown","source":"<a id='stats'></a>\n# Statistical Significance"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"438beab33e9afe4cabfe26d36a7b473b7a395344"},"cell_type":"code","source":"def stat_test(control, treatment):\n    #paired t-test; two-tailed p-value      A   ,    B\n    (t_score, p_value) = stats.ttest_rel(control, treatment)\n\n    if p_value > 0.05/2:  #Two sided \n        print('There is no significant difference between the two machine learning pipelines (Accept H0)')\n    else:\n        print('The two machine learning pipelines are different (reject H0) \\n(t_score, p_value) = (%.2f, %.5f)'%(t_score, p_value) )\n        if t_score > 0.0: #in the case of regression lower RMSE is better; A is lower \n            print('Machine learning pipeline A is better than B')\n        else:\n            print('Machine learning pipeline B is better than A')\n    return p_value","execution_count":217,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"3f6120a69619f907ea3fbc7135fefdc551be39a9"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom scipy import stats\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.linear_model import LinearRegression\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\n# A sampling based bakeoff using *K-fold cross-validation*: \n# it randomly splits the training set into K distinct subsets (k=30)\n# this bakeoff framework can be used for regression or classification\n#Control system is a linear regression based pipeline\n\nkFolds=10\n\ny_test_ctrl = y_test\n# Logistic Regression as base\ncontrol = cross_val_score(log_reg, test_prepared, y_test_ctrl['Class'],\n                             scoring='recall', cv=kFolds)\n\n# control_acc = control.mean()\n# # control = control.mean()\n# display_scores(control)\n\n# display_scores(lin_rmse_scores)\n#Treatment system is a random forest based pipeline\n\ntreatment = cross_val_score(rf_clf, test_prepared, y_test['Class'],\n                         scoring='recall', cv=kFolds)\n\ntreatment_acc = treatment.mean()\n\npval = stat_test(control, treatment)\n\npval","execution_count":218,"outputs":[]},{"metadata":{"_uuid":"c81c2f8f05b3fe6f9e0836bed663a8043d110697"},"cell_type":"markdown","source":"\n# Finetune model/pipeline hyperparameters\n"},{"metadata":{"_uuid":"8574018024c219a0f953fffe3e11f6fa13570e54"},"cell_type":"markdown","source":"Let’s assume at this point that you now have a shortlist of promising models. You now need to fine-tune them. Let’s look at a few ways you can do that:\n\n* GridSearch\n* RandomSearch"},{"metadata":{"_uuid":"964a2eca33b8f029b8bd5266f9d7c8258b44008e"},"cell_type":"markdown","source":"## Finetune via GridSearch"},{"metadata":{"trusted":false,"_uuid":"aaf7558fee26085f1121d99ded85838d0c0b272c"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [5, 10, 20, 29]},\n    # then try 8 (2×4) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [10, 20], 'max_features': [5, 10, 20,29]},\n  ]\n\n# train across 5 folds, that's a total of (12+8)*5=100 rounds of training \ngrid_search = GridSearchCV(rf_clf, param_grid, cv=5,\n                           scoring='recall')\ngrid_search.fit(cc_prepared, y_train['Class'])","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"43b0516faf8620c83d9e0188aa9761c5dce3ed75"},"cell_type":"markdown","source":"The best hyperparameter combination found:"},{"metadata":{"trusted":false,"_uuid":"b7bc2d4e5d83cbe80f0ff69a4d96634b5cd726fc"},"cell_type":"code","source":"grid_search.best_params_","execution_count":62,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ae97dbbdcf62b09d6e45ea255c74969b1b64e66"},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"601bc40db89209dfdcadd410cc22a14a1258e0dd"},"cell_type":"markdown","source":"Let's look at the score of each hyperparameter combination tested during the grid search:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b6bee318fca6fa0604e9c59ab9919beaaaf5937a"},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)","execution_count":64,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"91d391af97d7b93b9b9e8b4e801a5cace98ffd0b"},"cell_type":"code","source":"pd.DataFrame(grid_search.cv_results_)","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"b67d083001eedca1dc97cb6c8eb6658e9a09d4b4"},"cell_type":"markdown","source":"<a id='feature_imp'></a>\n\n## Input variable importance"},{"metadata":{"trusted":false,"_uuid":"8837a0c7078659eed14e78db3d8ffb477a3bf83e"},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":66,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bab56209266ec50142f06a4aa8586a99158b6f57"},"cell_type":"code","source":"attributes = list(X_train.columns.values)\nattributes","execution_count":67,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"157eef3781155fd537503cb6607142f4dc8fb2db"},"cell_type":"code","source":"sortedFeatures = sorted(zip(feature_importances,attributes), reverse=False)\nsortedFeatures","execution_count":68,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"edfbda42a8f8b738dbb6c04b31f38ebca4ad4527"},"cell_type":"code","source":"np.array(sortedFeatures)[:, 0]","execution_count":69,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f0d0d2a7b0765ae63d3fcba2169d3431181d8c5"},"cell_type":"code","source":"# Plot the feature importances of the forest\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.figure() \nplt.title(\"Feature importances\")\nsortedNames = np.array(sortedFeatures)[:, 1]\nsortedImportances = np.array(sortedFeatures)[:, 0]\n\nplt.title('Feature Importances')\nplt.barh(range(len(sortedNames)), sortedImportances, color='b', align='center')\nplt.yticks(range(len(sortedNames)), sortedNames)\nplt.xlabel('Relative Importance')\nplt.grid()\nplt.show()","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"9760bb40a0d835f1c4ef1a691ef40bed194d43d4"},"cell_type":"markdown","source":"<a id='feature_sel'></a>\n## Feature Selection"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e3c08822cd2e6eb0d46d3963332402370707840a"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass BestFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","execution_count":71,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b6041d1685466275d332495f923a3107766b1b07"},"cell_type":"code","source":"k=5\ntop_k_feature_indices = indices_of_top_k(feature_importances, k)\ntop_k_feature_indices ","execution_count":72,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4dc9944d7e1ee97680d88fc3b927653297e4cded"},"cell_type":"code","source":"np.array(attributes)[top_k_feature_indices]","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"abcffedc1bc3b2a19acf2b739d928c592a2655a4"},"cell_type":"markdown","source":"Let's double check that these are indeed the top k features:"},{"metadata":{"trusted":false,"_uuid":"cdbb206b11f87701a360fdd873b28f3c9fa84cb0"},"cell_type":"code","source":"sorted(zip(feature_importances, attributes), reverse=True)[:k]","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"785ef783d5ff3f516cfce5a7b1ff1131425220b9"},"cell_type":"markdown","source":"** Let's create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection: **"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8891ed7a22d30f48de1819bd920613bc52e90041"},"cell_type":"code","source":"preparation_and_feature_selection_pipeline = Pipeline([\n    ('preparation', num_pipeline),\n    ('feature_selection', BestFeatureSelector(feature_importances, k))\n])","execution_count":75,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7e91dfa1d3511036322899b45a2f5b3a036bc267"},"cell_type":"code","source":"trainFile_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(X_train)","execution_count":76,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e2563f6fc86964aa143e2ef0d9062f79ad175d3f"},"cell_type":"code","source":"y_pred = preparation_and_feature_selection_pipeline.fit_transform(X_test)","execution_count":77,"outputs":[]},{"metadata":{"_uuid":"5cf07e05d9b58f23be2ed04008541dc338849127"},"cell_type":"markdown","source":"## Comparison of different models"},{"metadata":{"trusted":false,"_uuid":"cbb38295f1047fda4314da29b43d19685c5b86da"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom time import time\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n\nX=trainFile_prepared_top_k_features\nY=y_train\n\n\n\nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=1000)))\nmodels.append(('LR_L1', LogisticRegression(C=1,penalty='l1',max_iter=1000) ))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\n#models.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\n#models.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\nmodels.append(('RF_10',RandomForestClassifier(n_estimators=10)))\nmodels.append(('RF_100',RandomForestClassifier(n_estimators=100)))\n#models.append(('RF_5.21',RandomForestClassifier(max_features=5,n_estimators=21)))\n\n#models.append(('KNN_5', KNeighborsClassifier(n_neighbors=5,n_jobs=-1)))\n\n#models.append(('CART', DecisionTreeClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring ='recall'#'roc_auc' #'recall' #'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure(figsize=(8, 6))\nfig.suptitle('Classification Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.grid()\npyplot.show()","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"4c11d62337a60cb113829417959fc5f24f112aba"},"cell_type":"markdown","source":"<a id='pipeline_full'></a>\n## Full pipeline for data prep, feature selection and modeling"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d2e2376efb5aea96f381ce24b3964e0bd35017c1"},"cell_type":"code","source":"k=20\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', num_pipeline),\n    ('feature_selection', BestFeatureSelector(feature_importances, k)),\n    ('rf', RandomForestClassifier(bootstrap= False, n_estimators = 10, max_features=k))\n   \n])","execution_count":80,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3755e8894a76b492a0e588c6a87bda5c9a57b85"},"cell_type":"code","source":"prepare_select_and_predict_pipeline.fit(X_train, y_train)","execution_count":81,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"71659320460cf8910be74d973ac69a5c51a03092"},"cell_type":"code","source":"prepare_select_and_predict_pipeline.predict(X_test)","execution_count":82,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"98438f1967d58dcacb94f55991ccef88f5f68ba7"},"cell_type":"code","source":"\nsamples = [(X_test, y_test)]\nmodels_names = [\"RandomForest, Feautres=20, Ratio(1F:1NF)\"]\n","execution_count":83,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f4c75d65b0fda132214d6b63134053e01b17b7c6"},"cell_type":"code","source":"models = [prepare_select_and_predict_pipeline]","execution_count":84,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6aad2d9cca02df2116434584f8fd304e1b2799"},"cell_type":"code","source":"res_fe = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_fe)\nres","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a8dd15113adac4777afa4953a6ef7ae581c3f34a"},"cell_type":"code","source":"# res.drop(res.tail(1).index,inplace=True)\n# res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df8ff6149513ba882387afab3718ec2f5c6ed994"},"cell_type":"markdown","source":"## Kitchen Sink with VotingClassifier"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a3349314522928b8aed28884b61812bf15e48049"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\n\nvot_clf = VotingClassifier(\n    estimators=[\n                ('rf', RandomForestClassifier(bootstrap= False, n_estimators = 10, max_features=20)),\n#                 ('svc', SVC()),\n                ('lr', LogisticRegression()),\n                ('CART', DecisionTreeClassifier()),\n#                 ('LDA', LinearDiscriminantAnalysis())\n                ], \n    voting='hard')\n","execution_count":85,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"afd7aa688ccfc32fb94deb1e87b14bf22147e43d"},"cell_type":"code","source":"k=20\nprepare_select_and_predict_pipeline = Pipeline([\n    ('preparation', num_pipeline),\n    ('feature_selection', BestFeatureSelector(feature_importances, k)),\n    ('vot_clf', vot_clf)\n   \n])","execution_count":86,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1b0b27134857a556b7ea0da82d8eb8fafee76340"},"cell_type":"code","source":"prepare_select_and_predict_pipeline.fit(X_train, y_train)","execution_count":87,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fea120559c94f15a8a1901991b5250f440e1909c"},"cell_type":"code","source":"prepare_select_and_predict_pipeline.predict(X_test)","execution_count":88,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"597355ae1f4ce059ffe612177e5b90b198d6d641"},"cell_type":"code","source":"\nsamples = [(X_test, y_test)]\nmodels_names = [\"Voting(LR,RF,DT), Feautres=20(1F:1NF)\"]\n","execution_count":89,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4c70779f8a81406cc5c331a7006e42a8c886d79b"},"cell_type":"code","source":"models = [prepare_select_and_predict_pipeline]","execution_count":90,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"edbbc8188ec082d3f7db595c7e36711f2e47fcc0"},"cell_type":"code","source":"res_vot = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_vot)\nres","execution_count":481,"outputs":[]},{"metadata":{"_uuid":"50eb5eb1b832fa7fb85abc0ec9acdcdc8e8e1d30"},"cell_type":"markdown","source":"## Conclusions derived after Kitchen-sink analysis\nOur objective is to maximize the recall score and minimize the False Negative Rate. From the results table, it is clear that the simple model based on logistic regression is performing better. This is confirmed through statistical significance test. Models based on SVC, LDA, Random Forest, Decision Tree etc are not performing better even after several steps of feature selections. The kitchen sink model based on a VotingClassifier (Several ensembles of models based on LR, SVC, LDA, Random Forest and Decision Tree) might have improved the accuracy but did not improve the recall score. Here we used the kitchen sink analysis for demonstration purposes only. Hence we propose a feature engineering method where we generate new features after clustering analysis."},{"metadata":{"collapsed":true,"_uuid":"e21ce688adf50166c4f2781cee83e921bd794e67"},"cell_type":"markdown","source":"<a id='feature_eng'></a>\n# Feature Engineering (Experimental)\n\nThe original dataset contains 28 principal components identified as V1, V2, V3...,V28. The data is completely anonymized and hence domain specific feature engineering cannot be performed for this dataset. From pair plot analysis we could find that the feature \"Time\" is not significant in predicting the target variable. Also We could See that on a pair-wise comparison, the classes are forming clusters and some of them are even linearly seperable."},{"metadata":{"_uuid":"7164e9834349e81369507ceaf4d0656e79512bc9"},"cell_type":"markdown","source":"## Pair Plot Analysis"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1dc281c51611bcdae86c87c7118da5ea9ee134f8"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nsns.pairplot(trainFile, hue=\"Class\", size=2);\n# Dont Run it Again\n#Image(filename='.\\pair_plot.jpg', width=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad6544edcb7a90affa0df981ac0242dead79ee19"},"cell_type":"code","source":"Image(filename='.\\image2.jpeg', width=500)","execution_count":591,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad13c344c697c1e03deafbd7bae657436760ce48"},"cell_type":"code","source":"Image(filename='.\\image1.jpeg', width=500)","execution_count":592,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"235b3470cc0364241020dc8a4956655a88584d39"},"cell_type":"code","source":"non_fraud = trainFile.loc[(trainFile[\"Class\"] ==0)]\nnon_fraud = non_fraud.drop([\"Class\",\"Time\"],axis=1)\n\nfraud = trainFile.loc[(trainFile[\"Class\"] ==1)]\nfraud = fraud.drop([\"Class\",\"Time\"],axis=1)\n\nnfrdStd = num_pipeline.fit_transform(non_fraud)\nfrdStd = num_pipeline.fit_transform(fraud)\n#non_fraud","execution_count":219,"outputs":[]},{"metadata":{"_uuid":"8cd0d33898ef683d5c493d64dd9ea173552d79fb"},"cell_type":"markdown","source":"<a id='feature_eng1'></a>\n## Feature Extraction ( Gelesh G Omathil )\n\nHere we attempt to cluster the Fraud and non-fraud data into diffrent sets of clusters. Then we create features as distance to the centroids of the clusters."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b4cdc35f5987def173723c53d3810b8fe3576d71"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nnfrdCentro = KMeans(n_clusters=6, random_state=0).fit(nfrdStd)\nfrdCentro = KMeans(n_clusters=4, random_state=0).fit(frdStd)\n","execution_count":220,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d94d8780da0ffc1f05646e6e2131c1784df96ac8"},"cell_type":"code","source":"#kmeans = np.concatenate((nfrdCentro, frdCentro), axis=0)\nkmeans = np.concatenate((nfrdCentro.cluster_centers_, frdCentro.cluster_centers_), axis=0)\n","execution_count":221,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f666d7ddaca1bdcc33dac430c22b967861e33460"},"cell_type":"code","source":"#kmeans.labels_\n#kmeans.cluster_centers_\nimport numpy as np\nimport scipy\n#X=trainFile_0.values\ndist_0=[]\ndist_1=[]\ndist_2=[]\ndist_3=[]\ndist_4=[]\ndist_5=[]\ndist_6=[]\ndist_7=[]\ndist_8=[]\ndist_9=[]\nisFraud=[]\n\nfor x in frdStd:\n    dist_0.append(np.sqrt(np.sum((x-kmeans[0])**2,axis=0)))\n    dist_1.append(np.sqrt(np.sum((x-kmeans[1])**2,axis=0)))\n    dist_2.append(np.sqrt(np.sum((x-kmeans[2])**2,axis=0)))\n    dist_3.append(np.sqrt(np.sum((x-kmeans[3])**2,axis=0)))\n    dist_4.append(np.sqrt(np.sum((x-kmeans[4])**2,axis=0)))\n    dist_5.append(np.sqrt(np.sum((x-kmeans[5])**2,axis=0)))\n    dist_6.append(np.sqrt(np.sum((x-kmeans[6])**2,axis=0)))\n    dist_7.append(np.sqrt(np.sum((x-kmeans[7])**2,axis=0)))\n    dist_8.append(np.sqrt(np.sum((x-kmeans[8])**2,axis=0)))\n    dist_9.append(np.sqrt(np.sum((x-kmeans[9])**2,axis=0)))\n    isFraud.append(1)\n\ndistDf_frd = pd.DataFrame({\n        \"dist_0\": dist_0,\n        \"dist_1\": dist_1,\n        \"dist_2\": dist_2,\n        \"dist_3\": dist_3,\n        \"dist_4\": dist_4,\n        \"dist_5\": dist_5,\n        \"dist_6\": dist_6,\n        \"dist_7\": dist_7,\n        \"dist_8\": dist_9,\n        \"dist_9\": dist_9,\n        \"Class\": isFraud})\n\nndist_0=[]\nndist_1=[]\nndist_2=[]\nndist_3=[]\nndist_4=[]\nndist_5=[]\nndist_6=[]\nndist_7=[]\nndist_8=[]\nndist_9=[]\nnisFraud=[]\n\nfor x in nfrdStd:\n    ndist_0.append(np.sqrt(np.sum((x-kmeans[0])**2,axis=0)))\n    ndist_1.append(np.sqrt(np.sum((x-kmeans[1])**2,axis=0)))\n    ndist_2.append(np.sqrt(np.sum((x-kmeans[2])**2,axis=0)))\n    ndist_3.append(np.sqrt(np.sum((x-kmeans[3])**2,axis=0)))\n    ndist_4.append(np.sqrt(np.sum((x-kmeans[4])**2,axis=0)))\n    ndist_5.append(np.sqrt(np.sum((x-kmeans[5])**2,axis=0)))\n    ndist_6.append(np.sqrt(np.sum((x-kmeans[6])**2,axis=0)))\n    ndist_7.append(np.sqrt(np.sum((x-kmeans[7])**2,axis=0)))\n    ndist_8.append(np.sqrt(np.sum((x-kmeans[8])**2,axis=0)))\n    ndist_9.append(np.sqrt(np.sum((x-kmeans[9])**2,axis=0)))\n    nisFraud.append(0)\n    \ndistDf_nfrd = pd.DataFrame({\n        \"dist_0\": ndist_0,\n        \"dist_1\": ndist_1,\n        \"dist_2\": ndist_2,\n        \"dist_3\": ndist_3,\n        \"dist_4\": ndist_4,\n        \"dist_5\": ndist_5,\n        \"dist_6\": ndist_6,\n        \"dist_7\": ndist_7,\n        \"dist_8\": ndist_8,\n        \"dist_9\": ndist_9,\n        \"Class\": nisFraud})","execution_count":222,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ef91d6dc769dd8bf14d48bfd70915273a0291d11"},"cell_type":"code","source":"distDf_nfrd.describe()","execution_count":223,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8d3f2d83dbb38f277337576757d505e553126978"},"cell_type":"code","source":"distDf_frd.describe()","execution_count":224,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d0f763dbe5070cbe28ffb234efd12f52bfd1f818"},"cell_type":"code","source":"#dft_1=dft.loc[(dft[\"isFraud\"] ==1)]\n#print(len(dft_1))\n#dft_0=dft.loc[(dft[\"isFraud\"] ==0)]\ndistDf_nfrdSubSet=distDf_nfrd.sample(frac=0.03)\n#print(len(dft_0))\ntrainFile=distDf_nfrdSubSet.append(distDf_frd, ignore_index=True)\n\n#dft_1=dft_1.sample(frac=0.3)\n#trainFile=dft_1.append(dft_0, ignore_index=True)\n#trainFile=dft\ntrainFile= shuffle(trainFile)\n#trainFile.reset_index(drop=True)\ntrainFile= shuffle(trainFile)\n#trainFile.reset_index(drop=False)\ntrainFile= shuffle(trainFile)\ndataY=trainFile[(\"Class\")]\ndataX=trainFile.drop([\"Class\"],axis=1)\n#X,Y","execution_count":225,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f5da0ed4a77ed94aa016f10bdba6710031877ba"},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore')\n\n%matplotlib inline\n\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom time import time\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#from sklearn import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nY=dataY.values\nX=dataX.values\n\n\n#X_train=X\n#y_train=Y\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(max_iter=1000)))\nmodels.append(('LR_L1', LogisticRegression(C=1,penalty='l1',max_iter=1000) ))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\n#models.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\nmodels.append(('RF_10',RandomForestClassifier(n_estimators=10)))\nmodels.append(('RF_100',RandomForestClassifier(n_estimators=100)))\n#models.append(('RF_5.21',RandomForestClassifier(max_features=5,n_estimators=21)))\n\n#models.append(('KNN_5', KNeighborsClassifier(n_neighbors=5,n_jobs=-1)))\n\n#models.append(('CART', DecisionTreeClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring ='recall'#'roc_auc' #'recall' #'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure(figsize=(8, 8))\nfig.suptitle('Classification Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.grid()\npyplot.show()","execution_count":226,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fc129d44fff6516a7a6e7d67a92e7ab1ac64ae45"},"cell_type":"code","source":"from sklearn.metrics import recall_score\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_distribs = {\n        'penalty': ('l1','l2'),\n        'C': (0.001,0.01,1,10,100),\n    }\n\n# scoring ='roc_auc' #'recall'\nscoring ='recall'\nlog_reg = LogisticRegression(max_iter=10000)\nrnd_search = RandomizedSearchCV(log_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring=scoring, random_state=42) #'neg_mean_squared_error'","execution_count":227,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"73fdf32d6a397ccbed93d59c2190bf7f50ff0c1a"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dataX.values,dataY.values,test_size = 0.3, random_state = 42)\n","execution_count":228,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ddb966bf962b9029fbb2f3875ff105207c3b4c59"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3×4) combinations of hyperparameters\n    {'n_estimators': [3,10,30,100], 'max_features': [3,4,5,6,7,8,9]},\n    # then try 8 (2×4) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators':  [3,10,30,100], 'max_features': [3,4,5,6,7,8,9]},\n  ]\n\n# train across 5 folds, that's a total of (12+8)*5=100 rounds of training \ngrid_search = GridSearchCV(rf_clf, param_grid, cv=5,\n                           scoring='recall')\n#X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3, random_state = 42)\ngrid_search.fit(X_train, y_train)\n\nprint(\"BEST PARAMS\")\nprint(grid_search.best_params_)\ngrid_search.best_estimator_","execution_count":229,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"04865d0fe268114eb5abc620359fdd0860499b25"},"cell_type":"code","source":"cvres = grid_search.cv_results_\nprint(\"mean_score  params\")\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(mean_score, params)\n","execution_count":230,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dba1ccfd3bd1ee1de59e2002209bf17163b91786"},"cell_type":"code","source":"\npd.DataFrame(grid_search.cv_results_)","execution_count":231,"outputs":[]},{"metadata":{"_uuid":"85c6e606390e8443552075f7a387fb35feb2f3f2"},"cell_type":"markdown","source":"## Input features and their importance for best model\n\n We developed 10 new features after feature transformation. Their importance is shown in the plot below."},{"metadata":{"trusted":false,"_uuid":"6a9b4587844dbed87c00a7fdd6ccc24310cd611f"},"cell_type":"code","source":"\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances\n\nattributes = list(dataX.columns.values)\nattributes\n\nsortedFeatures = sorted(zip(feature_importances,attributes), reverse=False)\nsortedFeatures\n\n# Plot the feature importances of the forest\n%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.figure() \nplt.title(\"Feature importances\")\nsortedNames = np.array(sortedFeatures)[:, 1]\nsortedImportances = np.array(sortedFeatures)[:, 0]\n\nplt.title('Feature Importances')\nplt.barh(range(len(sortedNames)), sortedImportances, color='b', align='center')\nplt.yticks(range(len(sortedNames)), sortedNames)\nplt.xlabel('Relative Importance')\nplt.grid()\nplt.show()","execution_count":232,"outputs":[]},{"metadata":{"_uuid":"ea3fc7a4395d97ea72c2393ce4948e56cbffdec5"},"cell_type":"markdown","source":"## Evaluation of the best model"},{"metadata":{"trusted":false,"_uuid":"5da6f9b9d927dbed918b3a0b1ba93fdba429615b"},"cell_type":"code","source":"\ngrid_search.best_estimator_.fit(X_train, y_train)\n\n\nsamples = [(X_test, y_test)]\nmodels_names = [\"RandomForest, Feature Transform\"]\nmodels = [grid_search.best_estimator_]\nres_ft = evaluate(models, metrics, samples, metrics_names, models_names)\nres = res.append(res_ft)\nres","execution_count":233,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e7fbed0deead8fdc2a45146b63e3d012de4aa6a7"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ny_pred = grid_search.best_estimator_.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)","execution_count":234,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"be72fa8d38ee49f62b45be31c9c1ce29f3b2f1ce"},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplot_roc_curve(fpr, tpr)\nplt.grid()\nplt.show()","execution_count":235,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f10d1b59d5be2dde6a90155d82dfe781d77766aa"},"cell_type":"code","source":"\n# Plot confusion matrix\nbest_cm = confusion_matrix(y_test, y_pred)\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(best_cm\n                      , classes=class_names\n                      , title='Confusion matrix for best model')\nplt.show()","execution_count":236,"outputs":[]},{"metadata":{"_uuid":"8ba49025e33a18e6635119d42c1f5145054832ee"},"cell_type":"markdown","source":"<a id='stats1'></a>\n## Statistical Significance of the best model "},{"metadata":{"trusted":false,"_uuid":"f23c317c50761dbf3e41d144b447b45c7f5a0c20"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom scipy import stats\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.linear_model import LinearRegression\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\n# A sampling based bakeoff using *K-fold cross-validation*: \n# it randomly splits the training set into K distinct subsets (k=30)\n# this bakeoff framework can be used for regression or classification\n#Control system is a linear regression based pipeline\n\nkFolds=10\n# Logistic Regression as base\ncontrol = cross_val_score(log_reg, test_prepared, y_test_ctrl['Class'],\n                             scoring='recall', cv=kFolds)\n\n# control_acc = control.mean()\n# control = control.mean()\n# display_scores(lin_rmse_scores)\n# display_scores(control)\n\n\n#Treatment system is a random forest based pipeline\n\ntreatment = cross_val_score(grid_search.best_estimator_, X_test, y_test,\n                         scoring='recall', cv=kFolds)\n\n# treatment_acc = treatment.mean()\n# treatment = treatment.mean()\n# display_scores(treatment)\n# treatment = tree_rmse_scores = np.sqrt(-scores)\n# display_scores(tree_rmse_scores)\n\n\npval = stat_test(control, treatment)\n\npval\n    ","execution_count":237,"outputs":[]},{"metadata":{"_uuid":"1b97a8a26dd28a49f2ed212ecff0c15bbb8d5fc7"},"cell_type":"markdown","source":"## Discussion on best model\n\nThe results from base model(Logistic Regression) is below. \n\nPrecision: 0.937500\t\nRecall: 0.895522 \t\nFalse Negative Rate: 0.104478\t\nF1 Score: 0.916031  \t\nF0.5 Score: 0.928793\t\nAUC: 0.923070\n\nThe results from best model(Random Forest) after feature transformation is below.\n\nPrecision: 1.000000\t\nRecall: 0.979866\t\nFalse Negative Rate: 0.020134 \t\nF1 Score: 0.989831  \t\nF0.5 Score: 0.995907\t\nAUC: 0.989933\n\nTo detect maximum number of fraud transacations, we need to have model that maximizes the recall and minimizes the false negative rate. As you can see from the results, the best model reduced the false negative rate from 10% to 2%, while improving the recall from 0.89 to 0.97. The feature transformation is key factor in this achievement.  \t\t \t \t"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"350px","left":"0px","right":"781.333px","top":"106px","width":"152px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}