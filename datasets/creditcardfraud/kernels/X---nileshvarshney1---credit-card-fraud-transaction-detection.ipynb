{"cells":[{"metadata":{"_uuid":"8a3f1220c159408983e17b2243e7c04a7a4a47ee"},"cell_type":"markdown","source":"The world of electronics payment enables the card holder to  shop online with virtual money in the pocket. The growth of the online industry also increases the risk of fraudent transaction.  Every credit card provider is working hard in this area to reduce the fraud transaction.\n\nIt is important for thecredit card companies to recognize the fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nProvided dataset is PCA transformation except Time and Amount features. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n**Inspiration**\nIdentify fraudulent credit card transactions.\n\nProvided data is imbalanced, Precision-Recall Curve (AUPRC), Accuracy may not work out well. To improving the fraud trasaction detection, recall rate need to improve without much caring on  Precision and F-1 score."},{"metadata":{"trusted":true,"_uuid":"de924c81a778512ccb1a70115a433f4ca994df3b"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom collections import Counter\nfrom sklearn import metrics\nfrom sklearn.feature_selection import VarianceThreshold\nimport os\n\n%matplotlib inline\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85fc27d82494669d7904aa42d65c1e0e8a05fc1d"},"cell_type":"markdown","source":"## Read Dataset"},{"metadata":{"trusted":true,"_uuid":"97998f135625f67fdbdedf26ab232e54936f3e5b"},"cell_type":"code","source":"import os\nprint('Data File ==>\\t {}'.format(os.listdir(\"../input\")[0]))\ncreditcard = pd.read_csv(\"../input/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"creditcard.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fa73fa78c83c9c278db8cc9d6085897423d5326"},"cell_type":"code","source":"# Missing values\nprint('No of Missing values :\\t{}'.format(creditcard.isnull().sum().max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60ab58586b49aac7999c5bab840c0e78f96674d1"},"cell_type":"code","source":"sns.countplot(data=creditcard,x = 'Class')\nplt.title('Class Variables distribution', fontsize=14)\nplt.show()\n\ncreditcard['Class'].value_counts() *100 /len(creditcard)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beb9c5c5a1daac39c0237d276a86aab20bf4e62e"},"cell_type":"markdown","source":"Imbalance class, only 0.172% fraud case data against 99.83% none fraud case data. There are a couple of mathods that cane implemented to handle the imbalanced data.\n\n* Up-sampling\n* Down-sampling\n\nhttps://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis"},{"metadata":{"trusted":true,"_uuid":"d161b3e69eb3a515b11359831358def1b95d085e"},"cell_type":"code","source":"fig,ax = plt.subplots(nrows = 7, ncols=4, figsize=(12,21))\nrow = 0\ncol = 0\nfor i in range(len(creditcard.columns) -3):\n    if col > 3:\n        row += 1\n        col = 0\n    axes = ax[row,col]\n    sns.boxplot(x = creditcard['Class'], y = creditcard[creditcard.columns[i +1]],ax = axes)\n    col += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9853d705d291fa07ec9d96e94e93c84866aa755"},"cell_type":"markdown","source":"I am not intrested here to find the outliers, but there are few features those have big difference in mean and median values for fraudent and normal trasaction. I would prefer to remove the low variance variables along with Amount and Time for further analysis."},{"metadata":{"_uuid":"d0d26d793287088e064a3b2603abe4b25404b316"},"cell_type":"markdown","source":"**Remove Amount and Time**"},{"metadata":{"trusted":true,"_uuid":"08f619d6c0b87173d92f201a125784a7eab81189"},"cell_type":"code","source":"creditcard.drop(['Time','Amount'],axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"502e0b22371a58d5aa37481bebaefbcb4d88e017"},"cell_type":"code","source":"X = creditcard.iloc[:,range(0,28)].values\ny = creditcard['Class'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8692e40a3b43e013b40121f646256e0052e6f7b"},"cell_type":"markdown","source":"**Remove Low variance features**"},{"metadata":{"trusted":true,"_uuid":"94c8d7d0dcbf206db4ce121b5f97bb9780df4bd1"},"cell_type":"code","source":"sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nX = sel.fit_transform(X)\nprint('Remaining Features Count:\\t{}'.format(X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1160f74b6ee1b7395fb773d641f61ab46adf41fe"},"cell_type":"markdown","source":"Since data is imbalance, I would prefer under and oversampling along with normal data.\n\n**Model Metrics Method**"},{"metadata":{"trusted":true,"_uuid":"2274bdbea1e3279f0921aa12b7831da46d5bae26"},"cell_type":"code","source":"def print_result(title,actual, prediction,decision):\n    print('****************************************************')\n    print(title)\n    print('****************************************************')\n    print('Accuracy Score :\\t\\t{:.3}'.format(metrics.accuracy_score(actual, prediction)))\n    print('Recall Score :\\t\\t\\t{:.3}'.format(metrics.recall_score(actual, prediction)))\n    print('Average Precision Score :\\t{:.3}'.format(metrics.average_precision_score(actual, decision)))\n    print('ROC AUC Score :\\t\\t\\t{:.3}'.format(metrics.roc_auc_score(actual, decision)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa5c3c449a7cc96314cf4f9b85949b66c8c2fa6f"},"cell_type":"markdown","source":"**Training-Test dataset split**\n\nI will use 70% data to train model and rest 30% for validation purpose."},{"metadata":{"trusted":true,"_uuid":"c03bea3983846bd9f3b082b08a0efeb4016293ca"},"cell_type":"code","source":"# split data in training set and test set\nX_train, X_test, y_train, y_test = train_test_split(\\\n    X,y,test_size=0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61dbf1235c180029ac2c83d1c1834de9d362501b"},"cell_type":"markdown","source":"**Logistic Regression without any change in data (Normal)**\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"750e4fe436d92cb025dfe894ffd42d0a594f2170"},"cell_type":"code","source":"# Data Distribution \nprint('Normal Data Distribution {}'.format(Counter(creditcard['Class'])))\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    pipeline = make_pipeline(LogisticRegression(random_state=42, C = c_value))\n    model = pipeline.fit(X_train,y_train)\n    prediction = model.predict(X_test)\n    decision = model.decision_function(X_test)\n    # print(metrics.confusion_matrix(y_test,prediction))\n    print_result('Normal Data Logistic -> C ={}'.\\\n                 format(c_value), y_test,prediction,decision)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb72f86d56029cdb3df9f7566c9e4acce8431536"},"cell_type":"markdown","source":"As I doubted in the begining, model achieving almost 100% accuracy but recall score is not good enough. Best value for this model C = 1 which will able to catch only 62.6% fraud transaction. I will try upsampling and downsampling to improve the recall Score. "},{"metadata":{"_uuid":"b0a16363864671853cb676122fdf48fc47718675"},"cell_type":"markdown","source":"**SMOTE Oversampling Model**"},{"metadata":{"trusted":true,"_uuid":"2f3744fa2a08f9cec6d6daca32477fcbc39dfc13"},"cell_type":"code","source":"X_SMOTE,y_SMOTE = SMOTE().fit_sample(X,y)\nprint('SMOTE Data Distribution {}'.format(Counter(y_SMOTE)))\n\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    smote_pipeline = make_pipeline_imb(SMOTE(random_state=42),\\\n                                       LogisticRegression(random_state=42, C = c_value))\n    smote_model = smote_pipeline.fit(X_train,y_train)\n    smote_prediction = smote_model.predict(X_test)\n    smote_decision = smote_model.decision_function(X_test)\n    # print(metrics.confusion_matrix(y_test,smote_prediction))\n    print_result('SMOTE - Oversampling data(Logistic) -> C ={}'.\\\n                 format(c_value), y_test,smote_prediction,smote_decision)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea101d00e7feac6436542f134731ffad1dbaffc8"},"cell_type":"markdown","source":"As expected accuracy score is less as compares to normal data. Our purpose is to improve recall scrore. Oversampling with C value of 0.001 is providing recall score of 0.918 with accuracy of 0.977.\n\n**NearMiss Undersampling Model**"},{"metadata":{"trusted":true,"_uuid":"9a7021e0a0881673cfef8a37808095343cc91bfd"},"cell_type":"code","source":"X_NearMiss,y_NearMiss = NearMiss().fit_sample(X,y)\nprint('NearMiss Data Distribution {}'.format(Counter(y_NearMiss)))\n# build moodel with  - undersampling\nC_VALUES = [0.001,0.01,0.1,1]\n# build normal Model\nfor c_value in C_VALUES:\n    nearmiss =  LogisticRegression(random_state=42,C = c_value)\n    nearmiss_model = nearmiss.fit(X_NearMiss,y_NearMiss)\n    nearmiss_decision = nearmiss_model.decision_function(X_test)\n    nearmiss_prediction = nearmiss_model.predict(X_test)\n    print_result('NearMiss - Undersampling data(Logistic) -> C ={}'.\\\n                 format(c_value), y_test,nearmiss_prediction,nearmiss_decision)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17111bae45651bb52737c35bd627718ba378d4b5"},"cell_type":"markdown","source":"**Conclussion**\n\nUndersampling improves recall score but it also reduces accuracy drastically, therefore more normal trasaction are calssified as fraudulent transaction. I, personally, would prefer oversampling over other three models where model predicts approximately 92% of fraudulent transactions with overall accuracy  of 97.7% "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}