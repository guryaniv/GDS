{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\n\ntrain = pd.read_csv('../input/train.csv') # Test data\ntest = pd.read_csv('../input/test.csv') # Test data\n\ndatasets = [train, test]\n\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2ec17cfca1db392e4a73bb50b5147577e57062d"},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import re\nepic_titles = set(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'])\nfemale_titles = set(['Lady', 'Countess', 'Dona', 'Mlle', 'Ms', 'Mme'])\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n\ndef mapFare(fare):\n    if fare <= 7.91:\n        return 0\n    elif fare > 7.91 and fare <= 14.454:\n        return 1\n    elif fare > 14.454 and fare <= 31:\n        return 2\n    elif fare > 31:\n        return 3\n\ndef mapAge(age):\n    if age <= 16:\n        return 0\n    elif age > 16 and age <= 32:\n        return 1\n    elif age > 32 and age <= 48:\n        return 2\n    elif age > 48 and age <= 64:\n        return 3\n    elif age > 64:\n        return 4\n    \nfor dataset in datasets:\n    # Adding feature of family size\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    #dataset.drop([ 'SibSp', 'Parch'], axis=1, inplace=True)\n    \n    # Adding title as a feature\n    dataset['Title'] = dataset['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    dataset['Title'] = dataset['Title'].apply(lambda x: x if x not in epic_titles else 'Epic')\n    \n    # Fill NaN for Sex based on Title \n    dataset['Sex'] = dataset['Sex'].fillna(dataset['Title'].apply(lambda x: 'female' if x in female_titles else 'male'))\n\n    # Changing from old names to new names\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n    # Mapping title columns to int values\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Epic\": 5}\n    dataset['Title'] = dataset['Title'].apply(lambda x: title_mapping[x] if x in title_mapping else 5)\n\n    # Mapping Sex columns\n    dataset['Sex'] = dataset['Sex'].map( {\"female\": 0, \"male\": 1} ).astype(int)\n\n    # Mapping Age from 0 to 4\n    dataset['Age'] = dataset['Age'].fillna(dataset['Age'].mean()) # First replacing NaN with meanvalue\n    dataset['Age'] = dataset['Age'].apply(lambda x: mapAge(x))\n    dataset['Age'] = dataset['Age'].astype(int)\n\n    # Mapping Fare\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n    dataset['Fare'] = dataset['Fare'].apply(lambda x: mapFare(x))\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping deck feature\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    # we can now drop the cabin feature\n    dataset.drop(['Cabin'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50a7eb35289f34469316a0cb30244a22838b7ad6"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f9dd7e28698880d5e0ea8dbe1eebe4e35e698f6"},"cell_type":"code","source":"# Lookin at sex as a feature\ntrain[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e3ecbb3a86544c0715b43901c078fecc6c38f8a"},"cell_type":"code","source":"# Selecting columsn for splitting of data in next section\ny = train[['Survived']]\nx = train.drop(columns=['Name', 'Ticket', 'Survived', 'PassengerId'])\nx.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1db9b23caf81bcff50b8cb876a5dce6f1813c786"},"cell_type":"markdown","source":"### Correlation map"},{"metadata":{"trusted":true,"_uuid":"9f92f6af149de89733557ce30b7f343a8605cb64"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolormap = plt.cm.viridis\nplt.figure(figsize=(10,10))\nplt.title('Titanic Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.select_dtypes([np.number]).astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f126107c3b53c5096eeab05cef7bbf142df4e12"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Splitting data for training and validation\ntrain_x, test_x, train_y, test_y = train_test_split(x, np.ravel(y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b5ee3c1747089e08bfada611128fe2599af7a16"},"cell_type":"markdown","source":"### Testing with ownmade Decision Tree Classifier"},{"metadata":{"trusted":true,"_uuid":"44af24eac586ae0c8de7306190f8c318f88aded3"},"cell_type":"code","source":"\"\"\"\nHere I will make a decision-tree-algorithm which will trained by a set of training data,\nand then used to classify a set of test data. The algorithm adopts a greedy divide-and-conquer\nstrategy: always test the most important attribute/feature first. Most important means making\nthe most difference to the classification.\n\n\"\"\"\n\nimport pandas as pd\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # To ignore some future warnings from pandas\nimport math\nimport random\n\n\ndef plurality_value(parent_examples):\n    # Chooses random if tie, else the most common value for 'y'\n    # (value_counts() is sorted, therefore index = 0)\n    r = 0\n    if parent_examples['Survived'].value_counts().index[0] == parent_examples['Survived'].value_counts().index[1]:\n        r = random.randint(1)\n    return parent_examples['Survived'].value_counts().index[r]\n\ndef allValuesEqual(examples):\n    return len(examples['Survived'].value_counts()) == 1\n\n\ndef partition(examples, column, value):\n    \"\"\"  \n    :param examples: input dataset/rows \n    :param column: column to match \n    :param value: condition value to match each row with, equals 0 or 1\n    :return: matching rows and nonmatching rows\n    \"\"\"\n\n    true_rows, false_rows = examples.copy(), examples.copy()\n    for index, row in examples.iterrows():\n        if row[column] == value:\n            false_rows = false_rows.drop([index])\n        else:\n            true_rows = true_rows.drop([index])\n    return true_rows, false_rows\n\n\ndef get_entropy(examples):\n    # Calculating the number of the examples that have ouput (y) == 1\n    number_of_ones = 0\n    for index, row in examples.iterrows():\n        if row['Survived'] == 1:\n            number_of_ones += 1\n\n    q = number_of_ones/len(examples) if len(examples) != 0 else 0\n    return - ( q*math.log2(q) + (1-q)*math.log2((1-q)) ) if (q != 0 and q!= 1) else 0\n\n\ndef get_remainder(one_rows, two_rows, entropy1, entropy2):\n    total = len(one_rows) + len(two_rows)\n    p, n = len(one_rows), len(two_rows)\n    return (p/total)*entropy1 + (n/total)*entropy2\n\n\ndef find_best_split(examples, attributes, current_entropy):\n    \"\"\"\n    :param examples: examples to consider\n    :param attributes: attributes to consider\n    :param current_entropy: the entropy we are comparing the different new entropies against\n    :return: the attributes that makes a split which results in the most information gain\n    \"\"\"\n\n    # For each attribute, calculate info gain and choose attribute with highest info gain\n    best_gain = 0\n    best_attribute = attributes[0]\n    info_gains = []\n    for attribute in attributes:\n        # Partitions the examples based on whether or not their value for the attribute equals 1\n        ones, twoes = partition(examples, attribute, 1)\n        # print(ones.shape\n\n        # Calculating entropies for the two partitions\n        entropy1, entropy2 = get_entropy(ones), get_entropy(twoes)\n\n        # Calculating the remainder using the entropies\n        remainder = get_remainder(ones, twoes, entropy1, entropy2)\n\n        # Skip this split if it doesn't divide the dataset\n        if len(ones) == 0 and len(twoes) == 0:\n            continue\n\n        # Information gain\n        info_gain = current_entropy - remainder\n        info_gains.append(info_gain)\n        if info_gain >= best_gain and info_gain > 0.005:\n            best_gain, best_attribute = info_gain, attribute\n    #print(best_gain)\n    return (best_attribute, best_gain) if best_gain > 0 else (random.choice(attributes), best_gain)\n\n\ndef decision_tree_learning(examples, attributes, parent_examples, info_gain):\n    \"\"\"\n    :param examples: examples to consider in this iteration\n    :param attributes: attributes 'available' in this iteration, meaning not previously used in the path from \n    root to this node \n    :param parent_examples: the examples as they are before the split \n    :return: a complete Decision Tree (of class Tree) \n    \"\"\"\n    \n    # If examples are empty return most common output value among the parent examples (before the last split)\n    if len(examples) == 0:\n        return plurality_value(parent_examples)\n\n    # If all examples have the same output value, then the partition is pure and we return the classification\n    elif allValuesEqual(examples):\n        #print(examples.iloc[0]['y'])\n        return examples.iloc[0]['Survived']\n\n    # If attributes are empty (no more partition possible) return the plurality value of current examples\n    elif len(attributes) == 0:\n        return plurality_value(examples)\n    \n    elif info_gain < 0.08:\n        return plurality_value(examples)\n\n    # Else we continue the partition\n\n    current_entropy = get_entropy(examples)\n    best_attribute, info_gain = find_best_split(examples, attributes, current_entropy)[0], find_best_split(examples, attributes, current_entropy)[1] # Importance function version 1\n    #best_attribute = random.choice(attributes) # Importance function version 2\n\n    tree = Tree(best_attribute)\n    # Making a copy and then removing the attribute from that copy,\n    # as we need the attribute available in other nonsuccessor branches (when the recursion \"comes back\" again)\n    attr_copy = attributes.copy()\n    attr_copy = attr_copy.drop(labels=best_attribute)\n\n    for value in train[best_attribute].unique():\n        next_examples = examples.loc[examples[best_attribute] == value]\n        subtree = decision_tree_learning(next_examples, attr_copy, examples, info_gain)\n        tree.add_branch(value, subtree)\n    return tree\n\n\n\nclass Tree:\n    def __init__(self, root, branches=None):\n        self.root = root\n        if branches == None:\n            branches = {}\n        self.branches = branches\n\n    def add_branch(self, label, branch):\n        self.branches[label] = branch\n\n\ndef predict(tree, example):\n    value = example[tree.root]\n    if isinstance(tree.branches[value], Tree):\n        return predict(tree.branches[value], example)\n    else:\n        return tree.branches[value]\n    #else:\n     #   if isinstance(tree.branches[1], Tree):\n      #      return predict(tree.branches[1], example)\n       # else:\n        #    return tree.branches[1]\n\n\ndef print_tree(tree, value=\"\", level=0):\n    print(\"\\t\" * level + str(value), end = \" -> \")\n    attribute = tree.root if isinstance(tree, Tree) else str(tree)\n    print(attribute, end = \"\\n\")\n\n    if isinstance(tree, Tree):\n        for label, subtree in tree.branches.items():\n            print_tree(subtree, label, level + 1)\n\ndef main():\n\n    print(\"--------------------------------------------\")\n\n#     # Heatmap of correlations between attributes and 'y' (Class attribute)\n#     import seaborn as sns\n#     import matplotlib.pyplot as plt\n#     hm = sns.heatmap(train.corr(), annot=True, linewidth=.5, cmap='Blues')\n#     hm.set_title(label='Heatmap of correlations', fontsize=20)\n#     # plt.show()\n    train_new = train.copy()\n    train_new = train_new.drop(columns=['Name', 'Ticket', 'PassengerId', 'Deck'])\n    test_new = test.copy()\n    y = train_new['Survived']\n    train_x, test_x, train_y, test_y = train_test_split(train_new, y)\n    train_x_attributes = train_x.drop(columns=['Survived']).columns\n    \n    \n    # Build tree\n    tree = decision_tree_learning(train_x, train_x_attributes, train_x, 1)\n\n    # Visualize tree\n    #print_tree(tree)\n\n    # Check accuracy\n    from sklearn.metrics import accuracy_score\n    predictions = []\n    for index, example in test_x.iterrows():\n        try:\n            prediction = predict(tree, example)\n        except:\n            prediction = 0\n            print(\"her\")\n        predictions.append(prediction)\n    acc = accuracy_score(test_y, predictions)\n    print(\"Accuracy with my greedy Importance Decision Tree: \", acc)\n    \n    real_tree = decision_tree_learning(train, train_x_attributes, train, 1)\n    # Predict for the real test data\n    predictions = []\n    for index, example in test.iterrows():\n        try:\n            prediction = predict(real_tree, example)\n        except: \n            prediction = 0\n        predictions.append(prediction)\n    \n    pred_df = test[['PassengerId']].copy()\n    pred_df['Survived'] = predictions\n    pred_df.to_csv('submission_DTC.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4a51b60f0200b2088f3ebd83505a17d83a36cd6"},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"455088664a3b3948dfb89bdde449e3d6fa2feaf1"},"cell_type":"markdown","source":"### Random Forest Classifier with Grid Search"},{"metadata":{"trusted":true,"_uuid":"90c995d25cdaa700e62d3be344c736d16f46b8ae"},"cell_type":"code","source":"# # Parameters for grid search\n# parameters = {\n#     'n_estimators'      : [50, 100] + [x for x in range(200, 250, 30)],\n#     'max_depth'         : [x for x in range(5,10)] + [100],\n#     'random_state'      : [0, 1],\n#     'max_features'      : ['auto', 'log2'],\n# }\n# rf_vanilla = RandomForestClassifier(n_estimators=100, max_depth=100)\n# rf_vanilla = rf_vanilla.fit(train_x, train_y)\n\n# # Looking at importance of features\n# fti = rf_vanilla.feature_importances_\n# print(\"Feature imporantances out of the box:\")\n# for i, feat in enumerate(list(train_x.columns)):\n#     print('\\t{0:20s} : {1:>.6f}'.format(feat, fti[i]))\n\n    \n# GS_rf = GridSearchCV(RandomForestClassifier(), parameters, cv=10, n_jobs=-1)\n# GS_rf.fit(x, np.ravel(y))\n# preds = GS_rf.predict(test_x)\n# print('Accuracy with grid searched rf: ', accuracy_score(preds, test_y))\n# print('Best params', GS_rf.best_params_)\n# # Did return \"Best params {'max_depth': 6, 'max_features': 'auto', 'n_estimators': 240, 'random_state': 0}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3dd0de747b97f5b258804086bfd6f7e1360629d"},"cell_type":"code","source":"# # Testing all classifiers\n# rf = RandomForestClassifier(n_estimators=50, max_depth=100, n_jobs=-1)\n# rf.fit(x, np.ravel(y))\n# preds = rf.predict(test_x)\n# print('Accuracy with rf: ', accuracy_score(preds, test_y))\n\n# dt = DecisionTreeClassifier()\n# dt.fit(train_x, train_y)\n# preds = dt.predict(test_x)\n# print('Accuracy with dt: ', accuracy_score(preds, test_y))\n\n# neigh = KNeighborsClassifier(n_neighbors=5)\n# neigh.fit(train_x, train_y)\n# preds = neigh.predict(test_x)\n# print('Accuracy with kneigh: ', accuracy_score(preds, test_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c3d0ee59290f2746aeb6c632b71909ee2c0e460"},"cell_type":"markdown","source":"# Testing many different classifiers for benchmarking"},{"metadata":{"trusted":true,"_uuid":"240361b5c72300bd02957b446b4e0da23db11738"},"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.svm import SVC\n# from sklearn.gaussian_process import GaussianProcessClassifier\n# from sklearn.gaussian_process.kernels import RBF\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n#          \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n#          \"Naive Bayes\", \"QDA\"]\n\n# classifiers = [\n#     KNeighborsClassifier(3),\n#     SVC(kernel=\"linear\", C=0.025),\n#     SVC(gamma=2, C=1),\n#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n#     DecisionTreeClassifier(max_depth=5),\n#     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n#     MLPClassifier(alpha=1, max_iter=500),\n#     AdaBoostClassifier(),\n#     GaussianNB(),\n#     QuadraticDiscriminantAnalysis()]\n\n# for i, classifier in enumerate(classifiers):\n#     classifier.fit(train_x, train_y)\n#     preds = classifier.predict(test_x)\n#     print(f\"Predicting with {names[i]}, accuracy: {accuracy_score(preds, test_y)}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"098dfd48b315e4258e064c70f8a09a3826d0efde"},"cell_type":"markdown","source":"# Keras Binary classifier"},{"metadata":{"trusted":true,"_uuid":"0f5eddcf20c02a607bffe3ea1fbe287f29882a0f"},"cell_type":"code","source":"# # Importing libraries for building the neural network\n# import tensorflow as tf\n# from keras.models import Sequential\n# from keras.layers import Dense\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.model_selection import cross_val_score\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.pipeline import Pipeline\n# from sklearn.exceptions import DataConversionWarning\n# import warnings\n# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n# print(tf.test.gpu_device_name())\n# config = tf.ConfigProto()\n# config.gpu_options.allow_growth = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33161493eb5e70121cbea09147b9fe531e8c57bd"},"cell_type":"code","source":"# def create_baseline(optimizer='adam', init='uniform'):\n#     # create model\n#     if verbose: print(\"Create model with optimizer: %s; init: %s\" % (optimizer, init) )\n#     model = Sequential()\n#     model.add(Dense(16, input_dim=train_x.shape[1], kernel_initializer=init, activation='relu'))\n#     model.add(Dense(8, kernel_initializer=init, activation='relu'))\n#     model.add(Dense(4, kernel_initializer=init, activation='tanh'))\n#     model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n#     # Compile model\n#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbf0fca7754e646b4456cdcd39c7cabefec8abbd"},"cell_type":"code","source":"# # Training model\n# verbose = 1\n# gridsearch = False\n# with tf.device('/GPU:0'):\n#     if gridsearch:\n#         parameters = {\n#         'optimizer' : ['rmsprop', 'adam'],\n#         'init' : ['normal', 'uniform'],\n#         'epochs' : [100, 200],\n#         'batch_size' : [5, 10, 15],\n#             }\n\n#         model = KerasClassifier(build_fn=create_baseline, verbose=0)\n\n#         grid_model = GridSearchCV(estimator=model, param_grid=parameters)\n#         grid_result = grid_model.fit(train_x, train_y)\n\n#         # summarize results\n#         print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n#         means = grid_result.cv_results_['mean_test_score']\n#         stds = grid_result.cv_results_['std_test_score']\n#         params = grid_result.cv_results_['params']\n#         if verbose: \n#             for mean, stdev, param in zip(means, stds, params):\n#                 print(\"%f (%f) with: %r\" % (mean, stdev, param))\n#             elapsed_time = time.time() - start_time  \n#             print (\"Time elapsed: \",timedelta(seconds=elapsed_time))\n\n#         best_epochs = grid_result.best_params_['epochs']\n#         best_batch_size = grid_result.best_params_['batch_size']\n#         best_init = grid_result.best_params_['init']\n#         best_optimizer = grid_result.best_params_['optimizer']\n#     else:\n#         best_epochs = 200\n#         best_batch_size = 5\n#         best_init = 'uniform'\n#         best_optimizer = 'adam'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"513523e7406052a52fbc4bfc2323b5b5fe34498f"},"cell_type":"code","source":"# # Create a classifier with best parameters for whole dataset\n# with tf.device('/GPU:0'):\n#     model = KerasClassifier(build_fn=create_baseline, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n#     model.fit(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139a57a4b55d465decbeb286588d81265b479796"},"cell_type":"code","source":"# # Predicting values\n# with tf.device('/GPU:0'):\n#     preds = model.predict(test_x)\n#     print(f\"Keras Classifiers accuracy: {accuracy_score(preds, test_y)}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5157aba022350dbe0e7a37bd45bf3e8412188f3b"},"cell_type":"code","source":"# final_model = SVC(gamma=2, C=1)\n# final_model.fit(x, np.ravel(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe81497e6852abb8e277b728ad1d9d234921eef6"},"cell_type":"code","source":"# # Selecting columsn for splitting of data in next section\n# test_data = test.drop(columns=['Name', 'Ticket', 'PassengerId'])\n\n# # Predicting values from testing set\n# with tf.device('/GPU:0'):\n#     preds = final_model.predict(test_data)\n\n# #pred_df = test[['PassengerId']].copy()\n# #pred_df['Survived'] = preds\n# #pred_df.to_csv('submission_svc.csv', index=False)\n# pred_df.sample(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}