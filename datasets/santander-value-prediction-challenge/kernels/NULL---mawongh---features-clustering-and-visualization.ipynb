{"cells":[{"metadata":{"_uuid":"0bdfa29c01f28e5a322f6d461af1a95eb7efd631"},"cell_type":"markdown","source":"## Features clustering and visualization\nThis notebook attempts to cluster the features and visualize them for better understanding so an effective feature selection/enginering can be designed."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e9cbe72286af260b2c014f239accb3f8dcb8516a"},"cell_type":"code","source":"# the required imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AffinityPropagation\nfrom tqdm import tqdm\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4145ce3a294264dd6c4293a0dc30bf5b13652e2"},"cell_type":"markdown","source":"### Loading, Transforming and Scaling the data\n- The train and test set are loaded and concatenated into a single dataframe, it is important to get the total feature distribution not only from the train dataset.\n- the ID and target fields are dropped and log transformation is applied due the data's order of magnitudes.\n- StandardScale without the mean is applied, this is since for the visualization we'd like to better see the diference between the features.\n"},{"metadata":{"trusted":true,"_uuid":"9be07234ec4eaa6e0cc21be3fa6a1af9251d058d"},"cell_type":"code","source":"input_dir = '../input'\n\ntrain_df = pd.read_csv(input_dir + '/train.csv')\ntrain_df = train_df.drop(['ID', 'target'], axis = 1)\ntest_df = pd.read_csv(input_dir + '/test.csv')\ntest_df = test_df.drop(['ID'], axis = 1)\nfeatures = pd.concat([train_df, test_df], ignore_index=True)\n\n\ndata = np.log1p(features)\n\nscaler = StandardScaler(with_mean=False)\ndata = scaler.fit_transform(data)\nscaled_features = pd.DataFrame(data = data, columns=features.columns)\n\ndel train_df, test_df, data, features\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a91ca2cbee5ca374746223c4217f86b2272dfe40"},"cell_type":"markdown","source":"### Generating histograms for each one of the features\nIn this notebook the features are clustered based on their histograms, the main idea is to group those features with the most similar statistical distributions, this could be done also based on their descriptives (mean, std, kurtosis, etc) but doing directly by their histogram might be more effective in terms of their visualization.\n\nThe 'bins' parameter establishes the bins of the histograms being generated for each feature, also indicates the granularity and how many the histograms will differ from each other.\n\nA new dataframe with the histogram is generated, it's was implemented using a for loop, however this could be done with the apply method."},{"metadata":{"trusted":true,"_uuid":"cc54111e787858d926a0c7f4491c67ef827b6c26"},"cell_type":"code","source":"bins=300\n\nfeatures = scaled_features.columns\nranges = np.linspace(np.min(np.min(scaled_features,axis=0)), np.max(np.max(scaled_features,axis=0)), bins+1)\n\nfeat_hist_df =  pd.DataFrame(columns=ranges[:-1])\n\nfor feat in tqdm(features, ncols=110):\n    hist = pd.DataFrame(np.histogram(scaled_features[feat], bins=ranges)[0]\\\n                        .reshape(1,-1), columns=ranges[:-1], index= [feat])\n    feat_hist_df = feat_hist_df.append(hist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a548332a0d8c60abf2504800de8303705bd36a8d"},"cell_type":"markdown","source":"### Clustering the histograms\nThe zeros are removed and the features are clusterred using Affinity Propagation with default parameters, Affinity Propagation was used since it seems to be more effective with time series type data, although histograms are not time series, their shape is the property that is required to be kept."},{"metadata":{"trusted":true,"_uuid":"f48dc59e0121b39a2ae8812a2a6da99f2cbbab74"},"cell_type":"code","source":"feat_hist_nozero_df = feat_hist_df.drop([0],axis=1)\naf = AffinityPropagation().fit(feat_hist_nozero_df)\nfeat_hist_nozero_df['cluster'] = af.labels_\nprint('Using Affinity Propagation resulted in total of : {} clusters'.format(len(af.cluster_centers_indices_)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b23715cf55ef62817a129cb02795664a333403f"},"cell_type":"markdown","source":"### Exploring the clusters by visualization\nFrom the resulting clusters the top N cluster(s) will be selected and visualized. This might lead to some insight."},{"metadata":{"trusted":true,"_uuid":"499be2931d9bf37e1192a2edccf8c674b082ede9"},"cell_type":"code","source":"N = 10\ncluster_count = np.unique(af.labels_, return_counts=True)\n# as a sorted DataFrame\ncluster_count = pd.DataFrame({'cluster':cluster_count[0],'count':cluster_count[1]})\\\n                  .sort_values(by=['count'], ascending=False).reset_index(drop = True)\n# obtaining the top and bottom clusters\ntop_clusters = cluster_count.head(N)['cluster'].tolist()\ntop_counts = cluster_count.head(N)['cluster'].sum()\nbottom_clusters = cluster_count.tail(N)['cluster'].tolist()\ntop_accounts_percent = np.around(100 * top_counts / len(features),2)\nprint('Top {} cluster(s) accounts for {}% of total features'.format(N,top_accounts_percent))\nprint(cluster_count.head(N).T)\nprint('')\nprint('Bottom {} cluster(s)'.format(N))\nprint(cluster_count.tail(N).T)\n\n# the data will be converted into \"long\" format so it could be visualize using sns.tsplot\nfeat_hist_nozero_long = feat_hist_nozero_df.reset_index().melt(id_vars=['index','cluster'],\n                                                               var_name='bins', value_name='count')\nintop = np.in1d(feat_hist_nozero_long.cluster, top_clusters)\ninbottom = np.in1d(feat_hist_nozero_long.cluster, bottom_clusters)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1ddb5d45a4fd65d34fb480727576b307203a0217"},"cell_type":"markdown","source":"### Visualizing the clusters\nWhen plotting the histogram for each of the {{N}} top cluster using seaborn's tsplot, it can be observed that the Affinity Propagation does a nice job grouping these features, the confidence interval (68%) shadow is very narrow, which most of the feaures within a cluster have very similar histograms.\n"},{"metadata":{"trusted":true,"_uuid":"aa99b6b586c0f9e8bad43febde852cf76f4b2cdc"},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.tsplot(data=feat_hist_nozero_long[intop], time='bins',value='count',unit='index',condition ='cluster')\nplt.title('Histograms for top N cluster(s)')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17336ea812f856163b4b7f161bad7c7783c17a7d"},"cell_type":"markdown","source":"#### Some observations and thoughts\n- Between the top2 clusters, they group a total of 944 features, and they only have around 3 counts of values around their modes, indicating that those features have a lot of values that are zero. While the cluster 345 (with 99 features) have higher counts around its mode.\n- Histograms reveal that feature distributions are grouped around a mean/mode and follow a normal distribution-like shape. If normality can be confirmed. Then the original features would follow a log-normal distribution.\n- Most of the clusters histograms indicate *negative skewness* for the feature distributions. Can this be a consecuence of the log transformation?\n- If these features can be proved useful, using their distribution, can they be **sampled** so data augmentation can be implemented for the training set?"},{"metadata":{"trusted":true,"_uuid":"6d2f61c0905e2398c5b36500a4bc52340dc5d2e7"},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.tsplot(data=feat_hist_nozero_long[inbottom], time='bins',value='count',unit='index',condition ='cluster')\nplt.title('Histograms for bottom N cluster(s)')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ac8a1556f1a9db89c9b2f70bc42035b1aad8b4"},"cell_type":"markdown","source":"- The bottom clusters include only one feature and they group much higher counts of values around their mean/mode (higher than 100 counts). This indicates that there is strong relationship between the cluster and the number of zeros per feature.\n- The intuition might be lead us to think that the cluster with less features might be more relevant since they carry more information for the algorithms to make the predictions.\n\n#### The relationship between the features per cluster and numer of zeros per feature\n\nFollowing up on the intuition mentioned above, it is possible to select features based on their uniqueness of distribution and their count of zeros, following the visualization."},{"metadata":{"trusted":true,"_uuid":"cf0b34168634f674108068c9afb6983b3ac22199"},"cell_type":"code","source":"zeros_count = feat_hist_df[[0]].astype(int)\nzeros_count.columns = ['zero_count']\nzeros_count['cluster'] = af.labels_\nzeros_count = zeros_count.reset_index(drop=True).groupby('cluster')\\\n                         .agg([np.size, np.mean])['zero_count'].sort_values('mean')\n    \nx = zeros_count['mean'].values\ny1 = zeros_count['size'].values\ny2 = np.cumsum(y1)\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True,figsize = (16,8))\nplt.setp((ax1, ax2),xticks=np.arange(0,55000,500))\nfig.tight_layout()\nax1.scatter(x, y1, alpha = 0.2)\nax1.grid()\nax1.set_title('Features per Cluster (y) vs. number of zeros per feature (x)')\nax1.set_ylabel('features per cluster')\nax2.plot(x,y2, c = 'g')\nax2.set_xlabel('Mean number of zeros per feature')\nax2.set_ylabel('features')\nax2.set_title('Cumulative Features per Cluster (y) vs. number of zeros per feature (x)')\nax2.grid()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a3b5bb819513461b64337ea7389bc57ee492f281"},"cell_type":"markdown","source":"#### Additional Observations and thoughts\n- For example, if it is defined a \"cut value\" of 5200 zeros per feature, we will end of with 1000 features (~80% reduction of features), if we decide to include most of the cluster with few features, the number of features will increase exponentially, experiments with a model could be done in order to determine the added value of the rest of the features to the model's predictions.\n- Reducing the number of features might not improve the prediction score but might help the model to better generalize when predicting unseen data (test set)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}