{"cells":[{"metadata":{"_uuid":"3f091cd4010df40707255b7c34103c81328c2c55"},"cell_type":"markdown","source":"## Introduction\nThis is a learning and researching purpose kernel. In this journey, we will explore other kernels and works around the internet while will be trying to apply newly learned materials. Stay tuned. "},{"metadata":{"_uuid":"5a9e275b5622e772e95b4b6c9778dca5078117f1"},"cell_type":"markdown","source":"## Problem Statement: \nSeveral areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface.\n\nBut unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.\n\nTo create the most accurate seismic images and 3D renderings, TGS (the world’s leading geoscience data company) is hoping Kaggle’s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not."},{"metadata":{"_uuid":"2356c37257e9b6a3815be2d10ba6897c0ed47488"},"cell_type":"markdown","source":"## Submission Guideline\nSubmission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nThe metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.\n\nThe file should contain a header and have the following format. Each row in your submission represents a single predicted salt segmentation for the given image.\n\nid,                     rle_mask\n\n3e06571ef3,   1 1\n\na51b08d882,  1 1\n\nc32590b06f,  1 1\n\netc."},{"metadata":{"_uuid":"2c87256505d4a47115e0f9a057c67cfe4b39d7f8"},"cell_type":"markdown","source":"## Exploring Image Data\nWe will explore the image data in an ameture way following [this kernel](https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies), [this kernel](http://https://www.kaggle.com/skainkaryam/basic-data-visualization-using-pytorch-dataset). Special thanks to them. \n"},{"metadata":{"_uuid":"3d6da0d54ecfa4330d4b1b783aa244adee8d6f6f"},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pathlib\nimport imageio\nimport numpy as np\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\n\n#print(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad712a7504185fedffb478dfe918d52b477aad0f"},"cell_type":"markdown","source":"## Directory List"},{"metadata":{"trusted":true,"_uuid":"8602c47d02eaffac9fde50f60e64bdc4d23ef73b"},"cell_type":"code","source":"print('Parent Directory: ', os.listdir(\"../input\"))\nprint('train Directory:  ', os.listdir(\"../input/train\"))\nprint('test Directory:   ', os.listdir(\"../input/train\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f5ead6bea4f24c9d4dfa259ceb372f786a69ba"},"cell_type":"code","source":"depths_df = pd.read_csv('../input/depths.csv')\ntrain_df = pd.read_csv(\"../input/train.csv\")\nsample_submission_df = pd.read_csv(\"../input/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c90892d394823e968955b05a91169c0c21e5312"},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"trusted":true,"_uuid":"6be96b2c1572c43043e0d6074d47011c525007b6"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16a09eab06cc5b5da07ade4787a8b3fffc29f199"},"cell_type":"code","source":"depths_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff297f6f70fb7d16500b7b4442a30b959fe1bc07"},"cell_type":"code","source":"sample_submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a3e98c5e40201ecde6032098e61a7cf3c3bd819"},"cell_type":"markdown","source":"## Load & View Data using Pytorch Dataset Class\nPytorch Data Loading is neat. The official tutorial is [here](http://https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). The below work is taken from [this excellent kernel](http://https://www.kaggle.com/skainkaryam/basic-data-visualization-using-pytorch-dataset)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class TGSSaltDataset(Dataset):\n    \n    def __init__(self, root_path, file_list):\n        self.root_path = root_path\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        file_id = self.file_list[index]\n        \n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        \n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        \n        image = np.array(imageio.imread(image_path), dtype=np.uint8)\n        mask = np.array(imageio.imread(mask_path), dtype=np.uint8)\n        \n        return image, mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8ff74e237de491f5b027d4c7a8e9ee187319554"},"cell_type":"code","source":"depths_df = pd.read_csv('../input/train.csv')\n\ntrain_path = \"../input/train/\"\nfile_list = list(depths_df['id'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c9ed9e661052d014c166a5958263d5482e8f77b"},"cell_type":"code","source":"dataset = TGSSaltDataset(train_path, file_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e6ec27c02188888aa458a0a0c88e1cb74520c4"},"cell_type":"code","source":"def plot2x2Array(image, mask):\n    f, axarr = plt.subplots(1,2)\n    axarr[0].imshow(image)\n    axarr[1].imshow(mask)\n    axarr[0].grid()\n    axarr[1].grid()\n    axarr[0].set_title('Image')\n    axarr[1].set_title('Mask')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ad7daefa70bd7c1e1070759e4180a159bd813ce"},"cell_type":"code","source":"for i in range(5):\n    image, mask = dataset[np.random.randint(0, len(dataset))]\n    plot2x2Array(image, mask)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}