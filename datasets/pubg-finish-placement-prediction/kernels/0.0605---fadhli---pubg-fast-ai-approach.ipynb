{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae4310e58190d2a1149f91a5b4604cad804a3e07"},"cell_type":"markdown","source":"# Introduction\n\nThe dataset seems a good dataset to try the fastai approach. So, basically my aim is to build a model based on random forest classifier as fast as possible and work from there.\n\nSidenote : There's  a bunch of codes that I commented out. I followed the fastai lecture and notebook to a tee just for learning purposes. The code that I commented out are not important to get the result and submission. Feel free to try the codes out and uncommented it. "},{"metadata":{"trusted":true,"_uuid":"ea14796cfbabeb5e0fb6f920b8b6e5cff62fb4c0"},"cell_type":"code","source":"!pip install fastai==0.7.0\n!pip install torchtext==0.2.3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom IPython.display import display\n\nfrom sklearn import metrics\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f12a103dc1eb4f5581a34cc0fa32a4e3ea6f016"},"cell_type":"code","source":"df_raw = pd.read_csv('../input/train_V2.csv', low_memory=False)\ndf_raw_test = pd.read_csv('../input/test_V2.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ac69db2d600c2505e9cd4ae0049a11ae001fadd"},"cell_type":"code","source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000):\n        display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"552eb985b9a3aeea0063787b6483be959dc0273a"},"cell_type":"code","source":"display_all(df_raw.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a82e3094fe25b3a4e7ef98c16f98813829e7bdc2"},"cell_type":"code","source":"display_all(df_raw.describe(include='all'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"587dc607e7046c715183f44c90a2d8c7b5d25fe6"},"cell_type":"markdown","source":"## Initial Processing"},{"metadata":{"_uuid":"3a12296e446ee0fe15d10b494597eae4648a1573"},"cell_type":"markdown","source":"Let store Id, groupId, and matchId from the test dataset into an info dataset. This will be later use for submission. We can then drop this 3 fields from both the train and test dataset as it would not help us in building our model."},{"metadata":{"trusted":true,"_uuid":"9f46302576c97501f632cd5728399c4acaaeddf0"},"cell_type":"code","source":"# store test info\ndf_raw_test_info = df_raw_test[['Id', 'groupId', 'matchId']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"268a7443ae6031d04ad8cbf66fe06eec8cc11776"},"cell_type":"code","source":"df_raw.drop(['Id', 'groupId', 'matchId'], axis=1, inplace=True)\ndf_raw_test.drop(['Id', 'groupId', 'matchId'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eec35ad16bd3f8bcc79a181bfc79f870761ed516"},"cell_type":"markdown","source":"fastai library provide train_cats method which change any columns of strings in a dataframe to a column of categorical values. I also use apply_cats method on the test dataset to change any columns of strings into categorical variables using the train dataset (df_raw) as a template."},{"metadata":{"trusted":true,"_uuid":"e5cbfb531cb84c2595a1113ddc09e5b130733655"},"cell_type":"code","source":"train_cats(df_raw)\napply_cats(df_raw_test, df_raw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2bcf90b4322c0d6d45c924be182c34b1c518f11"},"cell_type":"code","source":"display_all(df_raw.isnull().sum().sort_index()/len(df_raw))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34bf637e689a603ee4da46069bf8402ff181b4ed"},"cell_type":"markdown","source":"Apparently, we have na values in the winPlacePerc column. This column is the target variable so it does not make sense to have an na values. Let's check the rows."},{"metadata":{"trusted":true,"_uuid":"c8346a177d3c9dab7351e02c3c3dd12f6921dc9e"},"cell_type":"code","source":"df_raw[pd.isna(df_raw['winPlacePerc'])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca9bc895d908dcfe0b8e39d7ac75783a15932282"},"cell_type":"markdown","source":"Oh, it's only one row. Let's get rid of the row."},{"metadata":{"trusted":true,"_uuid":"efd7072b03cf2fcbb15f73cd3692678cda35a879"},"cell_type":"code","source":"df_raw.dropna(subset=['winPlacePerc'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20e0a46fd9b4240f0a7fe73a26719267b56bd7a0"},"cell_type":"markdown","source":"Random forest classifier only take entirely numeric dataframe. To make sure our dataset is fit to be passed to the classifier, we can use proc_df to make sure the training and test dataset are set to numeric dataframe. "},{"metadata":{"trusted":true,"_uuid":"e81889993101dd95e83379eb74be1c9f8b4e07c2"},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(df_raw, 'winPlacePerc')\ndf_test, _, _ = proc_df(df_raw_test, na_dict=nas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f07c4959e09cc793d879f86ea60ac7b4f4674d77"},"cell_type":"code","source":"# m = RandomForestRegressor(n_jobs=-1)\n# m.fit(df_trn, y_trn)\n# m.score(df_trn,y_trn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e68b105ac9e19c855dca0c006e1d2ef7127bb275"},"cell_type":"markdown","source":"Running the classifier on the whole dataset give us an r^2 score of 0.986. Not bad. However, we don't have a way to see whether our model will do well for other data. The model is expected to do well on the training data.\n\nLet see if we can have another dataset to compare. Let split the dataset to get a validation dataset. In building model, validation dataset is a good way to make sure we does not overfit to the training dataset."},{"metadata":{"trusted":true,"_uuid":"1d1132a3def3319c067cf93cdbea8902510d52ff"},"cell_type":"code","source":"# split the data to train valid\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(df_trn, y_trn, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c913dd34892a725656eea37e6cd146d8dcc9a552"},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\ndef print_score(m):\n    res = [mean_absolute_error(m.predict(X_train), y_train), mean_absolute_error(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a6c95fec074e7d7d2cbe47a5be6a9069703b1f8"},"cell_type":"code","source":"# m = RandomForestRegressor(n_jobs=-1)\n# %time m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4ce945ce3de3a40d0c071269f9a9bf43eb32603"},"cell_type":"markdown","source":"Now we can see how good our model is doing. The model is posting an r^2 score of 0.9225 for the validation dataset. Not bad for our first try without much feature engineering. I'm going to use this model to get prediction from test dataset."},{"metadata":{"_uuid":"7421177251a0f96a2dad447a5e52f7030ba89255"},"cell_type":"markdown","source":"## Speeding things up"},{"metadata":{"trusted":true,"_uuid":"c1de28ab73b0fb8c716e23ad3aff4f485ed44db4"},"cell_type":"code","source":"# df_train, y_train, nas = proc_df(df_raw, 'winPlacePerc', subset=100000, na_dict=nas)\n# df_test, _, _ = proc_df(df_raw_test, na_dict=nas)\n\n# X_train, _, y_train, _ = train_test_split(df_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c0da51c38b28278c4497da01c7a0118672b31b8"},"cell_type":"code","source":"# m = RandomForestRegressor(n_jobs=-1)\n# %time m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62e916642b6a771aa29038169d3ba246eadb7628"},"cell_type":"markdown","source":"## Single tree"},{"metadata":{"trusted":true,"_uuid":"080103ff2b09c263f3ad801db5c9401137bc3704"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f10a32925f9db499d84bfa53512871307f8ec9"},"cell_type":"code","source":"# # from IPython import IPython\n# import graphviz\n\n# draw_tree(m.estimators_[0], df_trn, precision=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d8e12d669af3cafc42c431b20eff4c8b217e7f0"},"cell_type":"code","source":"# def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n#     \"\"\" Draws a representation of a random forest in IPython.\n\n#     Parameters:\n#     -----------\n#     t: The tree you wish to draw\n#     df: The data used to train the tree. This is used to get the names of the features.\n#     \"\"\"\n#     s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n#                       special_characters=True, rotate=True, precision=precision)\n#     IPython.display.display(graphviz.Source(re.sub('Tree {',\n#        f'Tree {{ size={size}; ratio={ratio}', s)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03f2283bf15d888484a519115f098733d784bb04"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4974b0fddff8c2286c9fa5480fc8426596536a2b"},"cell_type":"markdown","source":"Let's see what happens if we create a bigger tree. The training set result looks great! But the validation set is worse than our original model. This is why we need to use bagging of multiple trees to get more generalizable results."},{"metadata":{"_uuid":"2b23228b67fb4a1db7ae3448e686e3a0c459dc1c"},"cell_type":"markdown","source":"## Bagging\n\n### Intro to bagging\n\nTo learn about bagging in random forests, let's start with our basic model again."},{"metadata":{"trusted":true,"_uuid":"f0649cf9fc240cad826453527073f611fd4f53b1"},"cell_type":"code","source":"# m = RandomForestRegressor(n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"911626cfa124104d0c919813a25b385224f00cd8"},"cell_type":"markdown","source":"We'll grab the predictions for each individual tree, and look at one example."},{"metadata":{"trusted":true,"_uuid":"a7076062c83253224b335500337d0c60413bd87f"},"cell_type":"code","source":"# preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n# preds[:,0], np.mean(preds[:,0]), y_valid[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2c21b6c3820f5775ae5d3e796e6d5e067425b6"},"cell_type":"code","source":"# preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88455975fa495624d322ebfc8b181eb466b31535"},"cell_type":"code","source":"# plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8a2c29bfdc46e8b9481b093e9ec2ae8a66b2682"},"cell_type":"markdown","source":"The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)"},{"metadata":{"trusted":true,"_uuid":"5efdadd2183a7d1a408ae2ad022bd609113ad479"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74aed952f7d34346ad1c82927359eca0013cd388"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0582526c25e70b6253e07c67d7d2904ac5dd173"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a032cba6b1ea674bd054ce5c88d45c022c287ef"},"cell_type":"markdown","source":"### Out-of-bag (OOB) score\n\nIs our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!)\n\nThe idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was not included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n\nThis also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n\nThis is as simple as adding one more parameter to our model constructor. We print the OOB error last in our print_score function below."},{"metadata":{"trusted":true,"_uuid":"3af95955e6f4858d13d4b035e2489fab9ab4ae7f"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7abd93e6f4160085d0df3e5f4315e5933ecbc868"},"cell_type":"markdown","source":"## Reducing over-fitting\n\n### Subsampling\n\nIt turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: subsampling. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."},{"metadata":{"trusted":true,"_uuid":"784bad12f7386c83c6628446ad4872d0e1e39423"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_train, y_train, nas = proc_df(df_raw, 'winPlacePerc')\ndf_test, _, _ = proc_df(df_raw_test, na_dict=nas)\n\nX_train, X_valid, y_train, y_valid = train_test_split(df_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"632a8f020b92b985ee968bb71fcab4ea77e8305e"},"cell_type":"markdown","source":"The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a different random subset per tree. That way, given enough trees, the model can still see all the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."},{"metadata":{"trusted":true,"_uuid":"467ab0e4001bcae87cd6c35b6d957e85b0dca188"},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db8e8d6cd3f0c3e57d30b3abc5d7f679d88ee16"},"cell_type":"code","source":"m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3644cf2d58d4ef9428553d0f7787732f1dc3aea"},"cell_type":"markdown","source":"Since each additional tree allows the model to see more data, this approach can make additional trees more useful."},{"metadata":{"trusted":true,"_uuid":"87cc2bf7b695160be149fc40313f894f92ac1086"},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaed9d974993fc3a25a46184c13b402ba7cd72a8"},"cell_type":"markdown","source":"### Tree building parameters\n\nWe revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."},{"metadata":{"trusted":true,"_uuid":"8d149afa7ac1689a4a1aaf5fdeb350af461eb6c2"},"cell_type":"code","source":"# reset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db4993ae0412de3fa1269faed2da31f43e89d00"},"cell_type":"code","source":"# def dectree_max_depth(tree):\n#     children_left = tree.children_left\n#     children_right = tree.children_right\n\n#     def walk(node_id):\n#         if (children_left[node_id] != children_right[node_id]):\n#             left_max = 1 + walk(children_left[node_id])\n#             right_max = 1 + walk(children_right[node_id])\n#             return max(left_max, right_max)\n#         else: # leaf\n#             return 1\n\n#     root_node_id = 0\n#     return walk(root_node_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"032773c9d4b6ac2d6e09d257e7960e360c40d162"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f98ca4e88f5fdb83a5b1ecc3ae4a5e520758cdc"},"cell_type":"code","source":"# t=m.estimators_[0].tree_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"772a03ce31d4c89f53d1e6f61606e5022e836719"},"cell_type":"code","source":"# dectree_max_depth(t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49ea515fa9e48ca4b5d6ec2193e638eed99b2e5c"},"cell_type":"code","source":"# m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\n# m.fit(X_train, y_train)\n# print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"038f9faebb6142321375944297878d32becaa6da"},"cell_type":"code","source":"# t=m.estimators_[0].tree_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"706e91c5603ffa4d31980ec6c21fa6389cbeb4c5"},"cell_type":"code","source":"# dectree_max_depth(t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f98d5778b8765638144a4cf107f98ae73f126b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9fad1056407ddc432921e5d532b45fd74713714"},"cell_type":"code","source":"pred = m.predict(df_test)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39b597d1bb93b514df6ac0cfee1ef6345ee08cda"},"cell_type":"code","source":"df_sub = df_raw_test_info[['Id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54021fa9ef3e997ead5ca8f1267613ee5ef7a43f"},"cell_type":"code","source":"df_sub['winPlacePerc'] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd025f563ff12d91d0b73e8e0d698cd07066ab91"},"cell_type":"code","source":"df_sub.to_csv('PUBG_sub.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c70f4c28c0494ccd47b17961c00e2cf16416cd7"},"cell_type":"code","source":"df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50fe52f764f8dac40bf0782e78f299e4022fe1fc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}