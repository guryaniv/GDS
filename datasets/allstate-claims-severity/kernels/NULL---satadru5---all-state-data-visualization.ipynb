{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "76bf93a9-e163-9d2b-c237-1deba54cf083"
      },
      "source": [
        "This is data visualization part.Before doing ant ML work we need to understand the EDA first.This is my first work .Hope this will helpful to the beginners like me.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77ee853a-b8d5-5975-adc6-9757d7eb144c"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "from sklearn import preprocessing\n",
        "from sklearn import ensemble\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dba7ca21-8b97-03ac-0929-657e97577498"
      },
      "outputs": [],
      "source": [
        "train= pd.read_csv(\"../input/train.csv\")\n",
        "test= pd.read_csv(\"../input/test.csv\")\n",
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ffbb259-dd95-7cf6-71fb-40dd14bed81d"
      },
      "outputs": [],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4cc5a54e-f621-8d61-0898-d817cbe02b79"
      },
      "outputs": [],
      "source": [
        "#Take categorical columns#\n",
        "categorical_column=train.columns[1:117]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "384bd302-2785-248e-9b3f-4b35deef6a3f"
      },
      "outputs": [],
      "source": [
        "train_1=train.copy()\n",
        "test_1=test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "12cd1923-9da8-f8cc-b9ce-36e6e9bf7e40"
      },
      "outputs": [],
      "source": [
        "#Filler with missing Value#\n",
        "\n",
        "train_1=train_1.fillna(-999)\n",
        "test_1=test_1.fillna(-999)\n",
        "id=train_1[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa33dab3-c6b7-31e8-0591-71c1222bcb48"
      },
      "outputs": [],
      "source": [
        "#Encoding the data for processing#\n",
        "\n",
        "for var in categorical_column:\n",
        "    lb=preprocessing.LabelEncoder()\n",
        "    full_var_data = pd.concat((train_1[var],test_1[var]),axis=0).astype('str')\n",
        "    lb.fit(full_var_data)\n",
        "    train_1[var] = lb.transform(train_1[var].astype('str'))\n",
        "    test_1[var]=lb.transform(test_1[var].astype('str'))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c77a968b-82a4-180a-0d0b-37838fe7f0b7"
      },
      "outputs": [],
      "source": [
        "train_1['log_target']=np.log(train_1['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e1db3fb-551a-ae72-8d3f-3f891d082b29"
      },
      "outputs": [],
      "source": [
        "train_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a2bf52f0-4576-142c-f055-83ff482d9bff"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8faea66c-2551-76f3-c267-c20e0014dbfd"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "62cd5dc5-4999-0813-d7b0-7a5520dc4806"
      },
      "outputs": [],
      "source": [
        "X=np.array(X)\n",
        "Y=np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f211121e-2565-697f-95a7-1aaf9e26f893"
      },
      "outputs": [],
      "source": [
        "val_size = 0.1\n",
        "\n",
        "#Use a common seed in all experiments so that same chunk is used for validation\n",
        "seed = 0\n",
        "\n",
        "#Split the data into chunks\n",
        "from sklearn import cross_validation\n",
        "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "329aa6bb-66e3-a1e7-ed31-70b13b24bd97"
      },
      "outputs": [],
      "source": [
        "#Scoring parameter\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fcc4a1e-4aac-38d6-258b-dcbc00883080"
      },
      "outputs": [],
      "source": [
        "alpha =0.5\n",
        "seed = 0\n",
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge(alpha=alpha,random_state=seed)\n",
        "model.fit(X_train,Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f24b1c8-636b-3cdc-44e3-ba228f20d4ce"
      },
      "outputs": [],
      "source": [
        "    import numpy    \n",
        "    result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val)))\n",
        "    print(result)\n",
        "    \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6bf1978-a087-1658-e1e9-2fcb9e70f783"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X = numpy.concatenate((X_train,X_val),axis=0)\n",
        "#del X_train\n",
        "#del X_val\n",
        "Y = numpy.concatenate((Y_train,Y_val),axis=0)\n",
        "#del Y_train\n",
        "#del Y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97846ecb-1506-c49f-2bc9-01fac838c944"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "import pandas\n",
        "\n",
        "#X = numpy.concatenate((X_train,X_val),axis=0)\n",
        "#del X_train\n",
        "#del X_val\n",
        "#Y = numpy.concatenate((Y_train,Y_val),axis=0)\n",
        "#del Y_train\n",
        "#del Y_val\n",
        "\n",
        "n_estimators = 1000\n",
        "\n",
        "#Best model definition\n",
        "best_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n",
        "best_model.fit(X,Y)\n",
        "#del X\n",
        "#del Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "275da4c7-4c80-4d31-6a0b-499df1e102c7"
      },
      "outputs": [],
      "source": [
        "#Read test dataset\n",
        "dataset_test = pandas.read_csv(\"../input/test.csv\")\n",
        "#Drop unnecessary columns\n",
        "ID = dataset_test['id']\n",
        "dataset_test.drop('id',axis=1,inplace=True)\n",
        "\n",
        "#One hot encode all categorical attributes\n",
        "cats = []\n",
        "for i in range(0, split):\n",
        "    #Label encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(labels[i])\n",
        "    feature = label_encoder.transform(dataset_test.iloc[:,i])\n",
        "    feature = feature.reshape(dataset_test.shape[0], 1)\n",
        "    #One hot encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
        "    feature = onehot_encoder.fit_transform(feature)\n",
        "    cats.append(feature)\n",
        "\n",
        "# Make a 2D array from a list of 1D arrays\n",
        "encoded_cats = numpy.column_stack(cats)\n",
        "\n",
        "del cats\n",
        "\n",
        "#Concatenate encoded attributes with continuous attributes\n",
        "X_test = numpy.concatenate((encoded_cats,dataset_test.iloc[:,split:].values),axis=1)\n",
        "\n",
        "del encoded_cats\n",
        "del dataset_test\n",
        "\n",
        "#Make predictions using the best model\n",
        "predictions = numpy.expm1(best_model.predict(X_test))\n",
        "del X_test\n",
        "# Write submissions to output file in the correct format\n",
        "with open(\"submission.csv\", \"w\") as subfile:\n",
        "    subfile.write(\"id,loss\\n\")\n",
        "    for i, pred in enumerate(list(predictions)):\n",
        "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80f3ec62-33e7-4ce9-91c7-2fdae4496b7a"
      },
      "outputs": [],
      "source": [
        "#Pairplot to see the relation between veriable#\n",
        "\n",
        "sns.pairplot(data = train_1[[\"log_target\",\"cont1\",\"cont2\",\"cont3\",\"cont4\",\"cont5\"]].sample(1000),dropna=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c371136e-f70b-5049-7652-4ceeaff0b1c5"
      },
      "outputs": [],
      "source": [
        "#Get the veriable name #\n",
        "\n",
        "nolcolumn=[]\n",
        "for col in train_1.columns:\n",
        "    if col.startswith('cont'):\n",
        "        nolcolumn.append(col)\n",
        "    \n",
        "\n",
        "nolcolumn.append('loss') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a29a68a-4835-4650-d32a-39319c89cabd"
      },
      "outputs": [],
      "source": [
        "#Corelation plot to see relation among veriable#\n",
        "\n",
        "d = train[nolcolumn]\n",
        "corrmat = d.corr().abs()\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "#f, ax = plt.subplots(figsize=(12, 6))\n",
        "plt.subplots(figsize=(13, 9))\n",
        "\n",
        "# Draw the heatmap using seaborn\n",
        "sns.heatmap(corrmat, vmax=.8,annot=True, square=True)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}