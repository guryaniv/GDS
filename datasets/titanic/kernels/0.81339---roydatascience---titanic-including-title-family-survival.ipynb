{"cells":[{"metadata":{"trusted":true,"_uuid":"4774a9643da6425957d5b7f415cc35b734a41e61"},"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d2de954834d13f94734b3e1cd13d91b4fe0a78b"},"cell_type":"code","source":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2fe3d56d83eee9a58e185a78fedcf8577337191"},"cell_type":"code","source":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d9a16dfc9c9620fa6e1470d1f84d27af5a213f"},"cell_type":"code","source":"#merge the data for feature engineering and later split it, just before applying Data Pipeline\nTrainFile = pd.read_csv(\"../input/train.csv\") #read the data from the csv file.\nTestFile = pd.read_csv(\"../input/test.csv\")\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nDataFile = TrainFile.append(TestFile)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bda50fa08ddd5ae2227eb59c8a6d39dae4ad4e6"},"cell_type":"code","source":"TrainFile.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4ca6427c3702580ba5075c2d5c2a413a0fd2d00"},"cell_type":"code","source":"TestFile.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"577f7f60e50ae75f24cbb5292527083e761e6826"},"cell_type":"code","source":"DataFile.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b5a27d85d7b0727a324f216558d20a592df3998"},"cell_type":"code","source":"DataFile.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e6cf2fdca62876cea5c3fba89bea09847ae81e0"},"cell_type":"code","source":"# First Split the names to gt Mr. or Miss or Mrs.\n\nFirstName = DataFile[\"Name\"].str.split(\"[,.]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74893b247740f8cf575d41a541bcada6ce27b594"},"cell_type":"code","source":"# now strip the white spaces from the Salutation\ntitles = [str.strip(name[1]) for name in FirstName.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15e5bd4d20e9d308f418c0ce0feff413a3e049ad"},"cell_type":"code","source":"DataFile[\"Title\"] = titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45ea883d48d58901e66f6c8dbf3ffc104b5a508d"},"cell_type":"code","source":"# Now first we replace the extra titles to Mr and Mrs\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\nDataFile.replace({'Title': mapping}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0a5befdb9d226ababe47ac366937b4c2a16692d"},"cell_type":"code","source":"# get the imputed value for FARE\nDataFile['Fare'].fillna(DataFile['Fare'].median(), inplace=True)\n\n#based on the median taken to check fare the imputed value of Embarkment to be \"S\"\nDataFile['Embarked'].fillna(\"S\", inplace=True)\n\n#impute the age based on Titles \ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    imputed_age = DataFile.groupby('Title')['Age'].median()[titles.index(title)]\n    DataFile.loc[(DataFile['Age'].isnull()) & (DataFile['Title'] == title), 'Age'] = imputed_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93e87a5620a22743cafecb7ee63bba2b0b1d8ca2"},"cell_type":"code","source":"# Merge SibSp and Parch into one\nDataFile[\"Family Size\"] = DataFile[\"SibSp\"] + DataFile[\"Parch\"] + 1\n\n#Make Family Bins\nDataFile[\"Family Size\"] = pd.cut(DataFile[\"Family Size\"], bins=[0,1,4,20], labels=[1,2,3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6c432cd56ae4f6fbb08eaf803e3e667a46a9c14"},"cell_type":"code","source":"#Making Fare bins\n\nDataFile['FareBin'] = pd.qcut(DataFile['Fare'], 5)\n\nlabel = LabelEncoder()\nDataFile['FareBin'] = label.fit_transform(DataFile['FareBin'])\n\n#Making Age Bins\nDataFile['AgeBin'] = pd.qcut(DataFile['Age'], 4)\n\nlabel = LabelEncoder()\nDataFile['AgeBin'] = label.fit_transform(DataFile['AgeBin'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f52a2339ead0b19ca5fdd6a22377d5f4a209610"},"cell_type":"code","source":"#create a dummy for male and female\nDataFile['Sex'].replace(['male','female'],[0,1],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8d7611f1b034796feb9dd6517480fa5ba0823d"},"cell_type":"code","source":"##ADDED NEW CODE TO THE EXISTING PROGRAM\nDataFile['Last_Name'] = DataFile['Name'].apply(lambda x: str.split(x, \",\")[0])\n\nDEFAULT_SURVIVAL_VALUE = 0.5\nDataFile['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in DataFile[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                DataFile.loc[DataFile['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                DataFile.loc[DataFile['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      DataFile.loc[DataFile['Family_Survival']!=0.5].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa4706ee7d57aa884839c8edab1853116303a61b"},"cell_type":"code","source":"for _, grp_df in DataFile.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    DataFile.loc[DataFile['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    DataFile.loc[DataFile['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(DataFile[DataFile['Family_Survival']!=0.5].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f33a45b3992375333e5b73bdc5fe23017e7bdae6"},"cell_type":"code","source":"#drop SibSp and Parch\nDataFile = DataFile.drop(['SibSp','Parch','Age','Fare',\n                          'Last_Name','Name','PassengerId',\n                          \"Ticket\",\"Cabin\",\"Embarked\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e741d064dddbc4444db614dd15f0ddfae3e3f3"},"cell_type":"code","source":"#Now split Back The data to training and test set - before applying the pipeline\n\ntrain_set, test_set = train_test_split(DataFile, test_size=0.3193,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c10f3a64ca6b2175981d2c78676f2699ff6019d5"},"cell_type":"code","source":"train_set.shape # This exactly matches the original training set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1af8af481641ef07ba3aabfacba8e8db5f50928"},"cell_type":"code","source":"test_set.shape # This exactly matches the original test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be43ebf0dcbeccb92a95e52b5baca2324ddff0f7"},"cell_type":"code","source":"#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n\nobs = train_set.isnull().sum().sort_values(ascending = False)\npercent = round(train_set.isnull().sum().sort_values(ascending = False)/len(train_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c223a7295f39608bf4dfba077e586ff40601723"},"cell_type":"code","source":"#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n\nobs = test_set.isnull().sum().sort_values(ascending = False)\npercent = round(test_set.isnull().sum().sort_values(ascending = False)/len(test_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3161bb0b988709e5a87c8a4a66fbf22c44e0d2a"},"cell_type":"code","source":"# Now define x and y.\n\n#the Y Variable\ntrain_set_y = train_set[\"Survived\"].copy()\ntest_set_y = test_set[\"Survived\"].copy()\n\n#the X variables\ntrain_set_X = train_set.drop(\"Survived\", axis=1)\ntest_set_X = test_set.drop(\"Survived\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b91a35ef922e8691ebe6116f74434022efd959d7"},"cell_type":"code","source":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cb3e6936c4c508ecbe8e82461697874fb13d769"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5804c55394e28039fb6c10f422a63ff63294d85"},"cell_type":"code","source":"cat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['Title'])),\n        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n    ])\n\nno_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector([\"Sex\",\"Pclass\",\"FareBin\", \"AgeBin\",\"Family Size\",\"Family_Survival\"]))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b85b4916458faf1f03a9a1082160d39c19abf6a4"},"cell_type":"code","source":"full_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"no_pipeline\", no_pipeline),\n    ])\n\nfinal_train_X = full_pipeline.fit_transform(train_set_X)\nfinal_test_X = full_pipeline.transform(test_set_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38c15ea13962b86312e2b17e1b1afc5d5f9b136e"},"cell_type":"markdown","source":"#### Now We Build the Models"},{"metadata":{"_uuid":"4718644cf85ff675d743c574efb1a9f10ef45709"},"cell_type":"markdown","source":"#### KNN Classifier"},{"metadata":{"trusted":true,"_uuid":"7f19de60bf54f39fa4b6c9a6140dd0bdf9451a04"},"cell_type":"code","source":"#Introduce KNN Classifier \n\nKNeighbours = KNeighborsClassifier()\nleaf_size = list(range(1,25,5))\nn_neighbors = list(range(4,30,2))\n\nparam_grid_KNeighbours = {'n_neighbors' : n_neighbors,\n'algorithm' : ['auto'],\n'weights' : ['uniform', 'distance'],\n'leaf_size':leaf_size}\n\ngrid_search_KNeighbours = GridSearchCV(KNeighbours, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\ngrid_search_KNeighbours.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9ca00daeddb2cc8cae0bbad3990d8b26f019bf6"},"cell_type":"code","source":"neighbor_grid = grid_search_KNeighbours.best_estimator_\n\ny_pred_neighbor_grid = neighbor_grid.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_neighbor_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10c6e9ff40b495cdd9e8cf4008f2ab941c99adc8"},"cell_type":"code","source":"# now get the predictions\ny_pred_neigh_rand = neighbor_grid.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test1 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test1[\"PassengerId\"] = passenger_id_test\nresult_test1[\"Survived\"] = y_pred_neigh_rand\n\n# Export the predictions file\n#result_test1.to_csv(\"Titanic_prediction_ashish_KNN.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28a2f93ae50b0d0be338eefeee0f4389ab37ebae"},"cell_type":"markdown","source":"#### Another KNN Approach"},{"metadata":{"trusted":true,"_uuid":"2704ce92a34a561afc9a1b2e9999973f042b9e2a"},"cell_type":"code","source":"KNeighbours2 = KNeighborsClassifier()\nleaf_size2 = list(range(8,50,1))\nn_neighbors2 = list(range(5,20,1))\n\nparam_grid_KNeighbours = {'n_neighbors' : n_neighbors2,\n'algorithm' : ['auto'],\n'weights' : ['uniform', 'distance'],\n'leaf_size':leaf_size2}\n\ngrid_search_KNeighbours2 = GridSearchCV(KNeighbours2, param_grid_KNeighbours, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\ngrid_search_KNeighbours2.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7f5e565721e2c15de5fd605dce6673f185e587"},"cell_type":"code","source":"neighbor_grid2 = grid_search_KNeighbours2.best_estimator_\n\ny_pred_neighbor_grid2 = neighbor_grid2.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_neighbor_grid2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ee9cac572932265d51ffb4e5abcd4cf23afadf"},"cell_type":"markdown","source":"##### Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"ba23ff5dcf78f1b46c7a6f4f2477d6318b747368"},"cell_type":"code","source":"forest_class = RandomForestClassifier(random_state = 42)\n\nn_estimators = [50, 100, 400, 700, 1000]\nmax_features = [5, 7, 10]\nmax_depth = [10, 20] \noob_score = [True, False]\nmin_samples_split = [2, 4, 10, 12, 16]\nmin_samples_leaf = [1, 5, 10] \nmax_leaf_nodes = [2, 10, 20]\n\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features,\n                     'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                    'oob_score' : oob_score, 'min_samples_leaf': min_samples_leaf, \n                     'max_leaf_nodes' : max_leaf_nodes}\n\n\nrand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, scoring='roc_auc', refit = True,\n                                 n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b031e0c239065c39a548de045b3079182b29f0ef"},"cell_type":"code","source":"random_estimator = rand_search_forest.best_estimator_\n\ny_pred_random_estimator = random_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_random_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c22639161b996226cc2301312ab1894703e4d53"},"cell_type":"code","source":"# now get the predictions\ny_pred_forest_rand = random_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test2 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test2[\"PassengerId\"] = passenger_id_test\nresult_test2[\"Survived\"] = y_pred_forest_rand\n\n# Export the predictions file\n#result_test2.to_csv(\"Titanic_prediction_ashish_RF.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c3a5a8e8cba44805c8eb095ada45ced4704ff67"},"cell_type":"markdown","source":"###### Ada Boost Classifier"},{"metadata":{"trusted":true,"_uuid":"85a8415146b798c36220bc06aba1a4cb8ed2a1c5"},"cell_type":"code","source":"ada_boost = AdaBoostClassifier(random_state = 42)\n\nn_estimators = [50, 100, 400, 700, 1000]\nlearning_rate = [0.001, 0.01, 0.05, 0.09]\nalgorithm = ['SAMME', 'SAMME.R']\n\nparam_grid_ada = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate, 'algorithm' : algorithm}\n\nrand_search_ada = RandomizedSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='roc_auc', refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_ada.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"733b166a3cc6ef0d69f73c75718a1b1a62e1feb1"},"cell_type":"code","source":"ada_estimator = rand_search_ada.best_estimator_\n\ny_pred_ada_estimator = ada_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_ada_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb045ad1c5c0fa1d31aa0e0bfe63c0a38ed6170"},"cell_type":"code","source":"# now get the predictions\ny_pred_ada_rand = ada_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test3 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test3[\"PassengerId\"] = passenger_id_test\nresult_test3[\"Survived\"] = y_pred_ada_rand\n\n# Export the predictions file\n#result_test3.to_csv(\"Titanic_prediction_ashish_ada.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8211b9457b385073f75f4dae6741884b074da2e"},"cell_type":"markdown","source":"###### Extra Trees Classifier"},{"metadata":{"trusted":true,"_uuid":"45385a3d72da6ad439d8e5fc4917ad42ee6c8ed0"},"cell_type":"code","source":"extra_classifier = ExtraTreesClassifier(random_state = 42)\n\nn_estimators = [50, 100, 400, 700, 1000]\nmax_features = [5, 7, 10]\nmax_depth = [10, 20]\nmin_samples_split = [2, 4, 10, 12, 16]\nmin_samples_leaf = [1, 5, 10]  # Mhm, this one leads to accuracy of test and train sets being the same.\n\nparam_grid_extra_trees = {'n_estimators' : n_estimators, 'max_features' : max_features,\n                         'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                         'min_samples_leaf' : min_samples_leaf}\n\n\nrand_search_extra_trees = RandomizedSearchCV(extra_classifier, param_grid_extra_trees, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_extra_trees.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adfa02b82e27c952bd068229bf521b7fb3fb6b93"},"cell_type":"code","source":"extra_estimator = rand_search_extra_trees.best_estimator_\n\ny_pred_extra_estimator = extra_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_extra_estimator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0616559e19029000c30d7a5b62a386a09e42f45"},"cell_type":"markdown","source":"#### Support Vector Classifier"},{"metadata":{"trusted":true,"_uuid":"dccfea21c28b723ae0fc302485e915918075c73f"},"cell_type":"code","source":"SVC_Classifier = SVC(random_state = 42)\n\nparam_distributions = {\"gamma\": reciprocal(0.0001, 0.001), \"C\": uniform(100000, 1000000)}\n\nrand_search_svc = RandomizedSearchCV(SVC_Classifier, param_distributions, n_iter=10, verbose=2, n_jobs = -1)\n\nrand_search_svc.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c7b439298122e2379e393966988afbcce0f4abf"},"cell_type":"code","source":"svc_estimator = rand_search_svc.best_estimator_\n\ny_pred_svc_estimator = svc_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_svc_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2644638967251a581e518762c0a4fc899ddb176"},"cell_type":"code","source":"# now get the predictions\ny_pred_svc_rand = svc_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test4 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test4[\"PassengerId\"] = passenger_id_test\nresult_test4[\"Survived\"] = y_pred_svc_rand \n\n# Export the predictions file\n#result_test4.to_csv(\"Titanic_prediction_ashish_svc.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e95dfec0d8fbc6cf3de1a3b5dc9ad5c96b2c9c"},"cell_type":"markdown","source":"#### Gradient Boosting Classifier"},{"metadata":{"trusted":true,"_uuid":"d1d66d35fa7ce67d03e5c43057f685347e6a24cc"},"cell_type":"code","source":"GB_Classifier = GradientBoostingClassifier(random_state = 42)\n\nn_estimators = [50, 100, 400, 700, 1000]\nlearning_rate = [0.1, 0.5]\nmax_depth = [10, 20]\nmin_samples_split = [2, 4, 10, 12, 16]\nmin_samples_leaf = [1, 5, 10]\nmax_leaf_nodes = [2, 10, 20]\n                            \nparam_grid_grad_boost_class = {'n_estimators' : n_estimators, 'learning_rate' : learning_rate,\n                              'max_depth' : max_depth, 'min_samples_split' : min_samples_split,\n                              'min_samples_leaf' : min_samples_leaf, 'max_leaf_nodes' : max_leaf_nodes}\n\nrand_search_grad_boost_class = RandomizedSearchCV(GB_Classifier, param_grid_grad_boost_class, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_grad_boost_class.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdffd42537c70813fb97d82b7b27f3593f7a894d"},"cell_type":"code","source":"gb_estimator = rand_search_grad_boost_class.best_estimator_\n\ny_pred_gb_estimator = gb_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_gb_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02e8582ac61c834b6a26afd9bb715cab5fa2757d"},"cell_type":"code","source":"# now get the predictions\ny_pred_gb_rand = gb_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test5 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test5[\"PassengerId\"] = passenger_id_test\nresult_test5[\"Survived\"] = y_pred_gb_rand \n\n# Export the predictions file\n#result_test5.to_csv(\"Titanic_prediction_ashish_gb.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ebe4ad836c7cc569ae78d6dcc04edbc503e64e"},"cell_type":"markdown","source":"#### Logistic Classifier"},{"metadata":{"trusted":true,"_uuid":"011d0ad21e1f22b33d7d0e7b4634421fd58a7bdf"},"cell_type":"code","source":"log_reg = LogisticRegression(random_state = 42)\n\nC = np.array(list(range(1, 100)))/10\n                            \nparam_grid_log_reg = {'C' : C}\n\nrand_search_log_reg = RandomizedSearchCV(log_reg, param_grid_log_reg, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_log_reg.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3522f6755243bebf3632b3dda0ee8388425fc04d"},"cell_type":"code","source":"log_estimator = rand_search_log_reg.best_estimator_\n\ny_pred_log_estimator = log_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_log_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9654993f25eac64ba45d9f0e3e1d9cc62b6f2395"},"cell_type":"code","source":"# now get the predictions\ny_pred_log_rand = log_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test6 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test6[\"PassengerId\"] = passenger_id_test\nresult_test6[\"Survived\"] = y_pred_log_rand \n\n# Export the predictions file\n#result_test6.to_csv(\"Titanic_prediction_ashish_log.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61786451924f3fc94cc783acda6ba1ace190f4d8"},"cell_type":"markdown","source":"##### MLP Classifier - Nueral Networks"},{"metadata":{"trusted":true,"_uuid":"b75d2e1187056977e764288cc9ce8698d322287d"},"cell_type":"code","source":"mlp_clf = MLPClassifier(random_state = 42)\n\nalpha = [.0001,.001,.01,1]\nlearning_rate_init= [.0001,.001,.01,1]\nmax_iter = [50,70,100,200]\ntol = [.0001,.001,.01,1]\n\nparam_grid_mlp_clf = {'alpha':alpha, 'learning_rate_init':learning_rate_init, 'max_iter':max_iter,'tol':tol}\n\nrand_search_mlp_clf = RandomizedSearchCV(mlp_clf, param_grid_mlp_clf, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_mlp_clf.fit(final_train_X, train_set_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38aa1ddfb0eeac4e7949e8ca7e699d5390890da0"},"cell_type":"code","source":"mlp_estimator = rand_search_mlp_clf.best_estimator_\n\ny_pred_mlp_estimator = mlp_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_mlp_estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69f029833a5dc95402008edbac1d51f3b5f1f4dd"},"cell_type":"code","source":"# now get the predictions\ny_pred_mlp_rand = mlp_estimator.predict(final_test_X)\n\n# Prepare the predictions file\nresult_test7 = pd.DataFrame()\npassenger_id_test = TestFile[\"PassengerId\"].copy()\nresult_test7[\"PassengerId\"] = passenger_id_test\nresult_test7[\"Survived\"] = y_pred_mlp_rand \n\n# Export the predictions file\n#result_test7.to_csv(\"Titanic_prediction_ashish_mlp.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5634d758d893a0490c70a0d39ed1a56e28b5c0a"},"cell_type":"markdown","source":"#### Another Ensemble Technique : Bagging Model - Not the best model on the test data"},{"metadata":{"trusted":true,"_uuid":"8bc8f8b91710ebf8815f029372d848350b882f77"},"cell_type":"code","source":"Bag_Classifier = BaggingClassifier(DecisionTreeClassifier(random_state=42))\n\nn_estimators = [50,70,100,200,500]\nmax_samples = [10,50,100]\n\nparam_grid_bag_clf = {'n_estimators':n_estimators, 'max_samples':max_samples}\n\nrand_search_bag_clf = RandomizedSearchCV(Bag_Classifier, param_grid_bag_clf, cv = 4, scoring='roc_auc', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_bag_clf.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35b3f9b9c598a363babd62e1f390d25032a20638"},"cell_type":"code","source":"#Predict the y_pred to get accuracy score.\nbag_estimator = rand_search_bag_clf.best_estimator_\ny_pred_bag = bag_estimator.predict(final_train_X)\naccuracy_score(train_set_y, y_pred_bag)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e35912022f3d41565151e6c79aa5aa3cf2946c"},"cell_type":"markdown","source":"##### Voting Classifier - Ensemble the models."},{"metadata":{"trusted":true,"_uuid":"132145f99a7e6578496039c16e06733f4f2626e7"},"cell_type":"code","source":"voting_clf = VotingClassifier(\n    estimators=[('lr', log_estimator),('rf',random_estimator), ('ada',ada_estimator),('gb', gb_estimator), ('knn', neighbor_grid),\n                ('svc', svc_estimator), ('mlp', mlp_estimator), ('ext', extra_estimator),('bag',bag_estimator)],\n    voting='hard')\nvoting_clf.fit(final_train_X, train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9135db21a647715c5d77197d13ff98be10d46ac"},"cell_type":"code","source":"#Ensemble out the best models to get better accuracy score.\n\nfor clf in (log_estimator, random_estimator, ada_estimator, gb_estimator, neighbor_grid, svc_estimator, mlp_estimator, extra_estimator, bag_estimator,voting_clf):\n    clf.fit(final_train_X, train_set_y)\n    y_pred = clf.predict(final_train_X)\n    print(clf.__class__.__name__, accuracy_score(train_set_y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e9f9d7c22f4905827b6bae3e8472674486a164b"},"cell_type":"markdown","source":"SVC looks like a clear winner with 87% accuracy on training set. But Kaggle predicts Voting Classifier as the best classification algorithm on Test set."},{"metadata":{"trusted":true,"_uuid":"e62017342facc15907f46a90f5edbb39450e574d"},"cell_type":"code","source":"#predict using voting\ny_pred_voting = voting_clf.predict(final_test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38f221fed7e1992da0d682a1c410b909fad8b47d"},"cell_type":"code","source":"#Create the datafile for voting classifier 2\nresult_test8 = TestFile.copy()\nresult_test8[\"Survived\"] = y_pred_voting.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbcadba47313edcc94f603962915c4eed85358a4"},"cell_type":"code","source":"#Import Voting Classifier results\nresult_test8.to_csv('Titanic_prediction_ashish.csv', columns=['PassengerId', 'Survived'], index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}