{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lightgbm\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import recall_score\n\nmpl.style.use('seaborn')\nnp.set_printoptions(precision=4, suppress=True)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fc8a5f18-5e57-4c2c-988f-530ea4636eea","_uuid":"ea4b82f6ab9936ac988a1b369b8c7c606df369f1"},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_cell_guid":"5ca3b4e3-0131-4e51-85ee-f40a10c7209b","_uuid":"11dd5c88cf08ead0aec9d20744717a4cc96a0875"},"cell_type":"markdown","source":"[LightGBM](https://github.com/Microsoft/LightGBM) is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms open-sources by Microsoft. The library is used extensively in Kaggle competitions, and often forms part of the [winning solution](https://github.com/Microsoft/LightGBM/tree/master/examples).\n\nThis notebook gives an introduction to the using LightGBM, illustrating a few advance features and giving an overview of the parameters of the algorithm. For those interested in understanding gradient boosting a bit better, an overview of the technique is given here: [https://www.avanwyk.com/an-overview-of-lightgbm/](https://www.avanwyk.com/an-overview-of-lightgbm/).\n\nThe examples below will be at the hand of a classification task: we will attempt to detect credit card fraud. An overview of the dataset is given [here](https://www.kaggle.com/mlg-ulb/creditcardfraud). The dataset is highly imbalanced (there are very few positive examples, relative to the negative examples), an area within which GBDTs (Gradient Boosted Decision Trees) excel."},{"metadata":{"_cell_guid":"7c6191a8-08c8-47c6-956e-34f7749f31ce","_uuid":"0ac318d40a3f896d59eceb8b7871876c6f6f79c5"},"cell_type":"markdown","source":"## Load and pre-process data\n\nWe normalize the `Amount` feature and also drop the `Time` feature as it is not useful for our analysis. Additionally, the dataset is very imbalanced, however, GBDTs are well suited for imbalanced datasets, provided class weights are given. Finally we split our data into training and validation datasets."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler as StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/creditcard.csv')\ndata.head(10)\ndata['NormalizedAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Time', 'Amount'], axis=1)\n\npositive_percentage = data[data['Class'] == 1].shape[0]/data.shape[0] * 100\nprint(\"{:.2f}% of the data are positive examples (highly skewed).\".format(positive_percentage))\n\nX = data.drop('Class', axis=1)\ny = data['Class']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.33)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c10e430-85b1-4105-ac67-b55815c52107","_uuid":"1274883f40b2ed20c5abdbb33e79a0fb7cb0df42"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"_cell_guid":"06935674-ed99-4528-afb0-7a687bd9c832","_uuid":"4f60fea57eb1ef0a6c1eff33185bcd590869a916","collapsed":true,"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n# Wrap our training and validation sets in LightGBM Datasets.\nlgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\nlgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cc7391bc-74a3-4fb7-bda2-5a806175a5cf","_uuid":"d24dcc39130418f1392000a432c41e7d2931568b"},"cell_type":"markdown","source":"### Basic Usage"},{"metadata":{"_cell_guid":"8b4cc1bc-3597-46e6-a19a-7ec4ce6ae7f5","_uuid":"966ef782015005bc0eaf545e49a7dd1d581cb654"},"cell_type":"markdown","source":"Below is the core parameters driving the training of a gradient boosted machine, with a brief explanation of each. A full explanation of the core parameters are given [here](https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters)."},{"metadata":{"_cell_guid":"967e929a-2c79-4ddc-a0eb-3baa925c0613","_uuid":"13056eac1fae3ba8a7504f026d7d4fbf7e8b7335","collapsed":true,"trusted":true},"cell_type":"code","source":"core_params = {\n    'boosting_type': 'gbdt', # GBM type: gradient boosted decision tree, rf (random forest), dart, goss.\n    'objective': 'binary', # the optimization object: binary, regression, multiclass, xentropy.\n    'learning_rate': 0.05, # the gradient descent learning or shrinkage rate, controls the step size.\n    'num_leaves': 31, # the number of leaves in one tree.\n    'nthread': 4, # number of threads to use for LightGBM, best set to number of actual cores.\n    \n    'metric': 'auc' # an additional metric to calculate during validation: area under curve (auc).\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dccbc2b5-4768-4c13-a6a0-864a4bc48ab2","_uuid":"4bdd90f8b79e1508c80b987ced32fd5c87739bd4"},"cell_type":"markdown","source":"Now we can train a Gradient Boosted Decision Tree using LightGBM.\nWe wrap the training call in a function that trains the GBDT, plots the results of the training for us and returns the GBM and the validation results per iteration."},{"metadata":{"_cell_guid":"68bb2e6e-00ef-4e55-8ced-b0451fbe3c47","_uuid":"3e5ceb29dba76a050d22b000b39727dc48fb7f2b","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"def train_gbm(params, training_set, validation_set, init_gbm=None, boost_rounds=100, early_stopping_rounds=0, metric='auc'):\n    evals_result = {} \n\n    gbm = lgb.train(params, # parameter dict to use\n                    training_set,\n                    init_model=init_gbm, # initial model to use, for continuous training.\n                    num_boost_round=boost_rounds, # the boosting rounds or number of iterations.\n                    early_stopping_rounds=early_stopping_rounds, # early stopping iterations.\n                    # stop training if *no* metric improves on *any* validation data.\n                    valid_sets=validation_set,\n                    evals_result=evals_result, # dict to store evaluation results in.\n                    verbose_eval=False) # print evaluations during training.\n    \n    y_true = validation_set.label\n    y_pred = gbm.predict(validation_set.data)\n    fpr, tpr, threshold = roc_curve(y_true, y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.title(\"ROC Curve. Area under Curve: {:.3f}\".format(roc_auc))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    _ = plt.plot(fpr, tpr, 'r')\n    \n    return gbm, evals_result","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"258e8aa3-76d7-43fe-812f-c132eea17cfd","_uuid":"88b1b0a8b84c07d6a1d26e544cf25c3065274654","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"model, evals = train_gbm(core_params, lgb_train, lgb_val)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83438f94-cef5-4bee-9fb9-e01904b80d32","_uuid":"95992ea517d24b584fd9c64610ea57e99f248a82"},"cell_type":"markdown","source":"Our initial model isn't doing very well. However, there are many parameters that we could tune to improve performance, speed up the training or reduce overfitting."},{"metadata":{"_cell_guid":"40b980d1-b46d-40bb-a65a-82a089d5d0cb","_uuid":"2d24a8dbedf90f6296e5a440df6450c24abd5724"},"cell_type":"markdown","source":"### Parameters"},{"metadata":{"_cell_guid":"209fad5b-e4c8-4686-9cb6-e0788c00dc22","_uuid":"03e513d26fa4f87dbbae4faa3fd6ef3f899e919b"},"cell_type":"markdown","source":"Below we specify and explain a host of model parameters, with some being tuned to improve the model accuracy. A complete list of algorithm parameters is given [here](https://lightgbm.readthedocs.io/en/latest/Parameters.html)."},{"metadata":{"_cell_guid":"d0598689-5387-4daa-bb3b-d49779235754","_uuid":"73e847a804bfa42db02ed33ac246efb828d2c7e0","collapsed":true,"trusted":true},"cell_type":"code","source":"advanced_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \n    'learning_rate': 0.01,\n    'num_leaves': 41, # more leaves increases accuracy, but may lead to overfitting.\n    \n    'max_depth': 5, # the maximum tree depth. Shallower trees reduce overfitting.\n    'min_split_gain': 0, # minimal loss gain to perform a split\n    'min_child_samples': 21, # or min_data_in_leaf: specifies the minimum samples per leaf node.\n    'min_child_weight': 5, # minimal sum hessian in one leaf. Controls overfitting.\n    \n    'lambda_l1': 0.5, # L1 regularization\n    'lambda_l2': 0.5, # L2 regularization\n    \n    'feature_fraction': 0.5, # randomly select a fraction of the features before building each tree.\n    # Speeds up training and controls overfitting.\n    'bagging_fraction': 0.5, # allows for bagging or subsampling of data to speed up training.\n    'bagging_freq': 0, # perform bagging on every Kth iteration, disabled if 0.\n    \n    'scale_pos_weight': 99, # add a weight to the positive class examples (compensates for imbalance).\n    \n    'subsample_for_bin': 200000, # amount of data to sample to determine histogram bins\n    'max_bin': 1000, # the maximum number of bins to bucket feature values in.\n    # LightGBM autocompresses memory based on this value. Larger bins improves accuracy.\n    \n    'nthread': 4, # number of threads to use for LightGBM, best set to number of actual cores.\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f047b93-8ed9-4757-bcb7-7a97de3c8217","_uuid":"9cf8e4cfde7153380ce8fd7b0035a300035d6c52","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\nmodel, evals = train_gbm(advanced_params, lgb_train, lgb_val, boost_rounds=500)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04ee42aa-6d19-4475-98a2-39430ac461bc","_uuid":"06928e07f01217835cf9c0eb3826a38d86a57fc1"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"4d6309af-77ea-4e66-b4d3-ad7978c6056c","_uuid":"c1934455b90d57fa9b3c3890222c3accfb45bd8f","collapsed":true},"cell_type":"markdown","source":"Our parameter tuning helped and the model performs significantly better. The parameters used above improves the model's accuracy, but we could improve the model's speed by setting a `bagging_freq` and using a lower `max_bin`."},{"metadata":{"_cell_guid":"17b03965-9f64-4ba2-8d26-ea37c38ad198","_uuid":"98fe2c2d88a6b6352077be6f12ad12ec8f7231b8"},"cell_type":"markdown","source":"## Additional Features"},{"metadata":{"_cell_guid":"fc39d80d-6bca-4393-9e8f-0ea4f5bb7a4e","_uuid":"4a9d0df0327233ce8670420214725aa03a2a97bc"},"cell_type":"markdown","source":"### Continous Training\n\nA model's training can be continued by passing an existing model as the `init_model` parameter to the training function."},{"metadata":{"_cell_guid":"a20c4c20-beff-4dde-ab94-debcd55d3650","_uuid":"e6fec7723147e37ff4bbc40b2cb5a89da2980e3c","collapsed":true,"trusted":true},"cell_type":"code","source":"model, evals = train_gbm(advanced_params, lgb_train, lgb_val, init_gbm=model, boost_rounds=500)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f92d52f-b1cf-436c-ba82-fc54718419bd","_uuid":"b37604bed1f0498410edf00083c5532eb75f10d3"},"cell_type":"markdown","source":"### Plotting\n\nLightGBM has a number of useful plotting functions built in:"},{"metadata":{"_cell_guid":"9b4c18b8-b974-498f-bf87-eafe62bd2e7a","_uuid":"0b8d807c626ece4d6054343ccdf7f20e722782f1","collapsed":true,"trusted":true},"cell_type":"code","source":"_ = lgb.plot_metric(evals) # training metrics","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c2f60ac-2a7e-4b07-9eeb-b90939e9cef2","_uuid":"3b110faa96410f05300641669d620244747202ca","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"_ = lgb.plot_importance(model) # feature importance","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a871ae0-c8cb-4361-a306-3259f9b4170a","_uuid":"f690c262d292dbb7588fa488cd91bb2ceab25d8d","collapsed":true,"trusted":true},"cell_type":"code","source":"_ = lgb.plot_tree(model, figsize=(20, 20)) # built trees","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d69e9cde-772e-4eb9-81c8-eb59667db0dc","_uuid":"4667e3c62bc0508d31c03caca1ed88392457c1eb","collapsed":true},"cell_type":"markdown","source":"### Persistence\nLightGBM models can easily be saved and loaded to a file, or JSON:"},{"metadata":{"_cell_guid":"bf363cde-7195-40d6-a1ce-f7429fef5632","_uuid":"58df149f44b0eb79fab4d6365a204c2a54bb6045","collapsed":true,"trusted":true},"cell_type":"code","source":"model.save_model('cc_fraud_model.txt')\n\nloaded_model = lgb.Booster(model_file='cc_fraud_model.txt')\n\n# Output to JSON\nmodel_json = model.dump_model()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}