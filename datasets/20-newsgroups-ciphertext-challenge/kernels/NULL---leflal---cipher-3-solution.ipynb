{"cells":[{"metadata":{"_uuid":"71bcbc9b768cc8ab2d75b43f2d285fa904a9a049"},"cell_type":"markdown","source":"# Cipher #3 Solution\n\nTo help kagglers move forward on the cryptanalysis of cipher #3, this kernel wishes to provide material for a known-plaintext attack (KPA) as suggested by EtienneW (https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge/discussion/75407), that is to say matching cipher&plaintext (aka cribs) pairs. We provide the matched cipher&plaintext pairs in output of this kernel in a pickle (df_crib.pkl)\n\nAnd for those who's rather take the elevator than climbing the stairs, this kernel also provides the decryption of cipher #3 train & test sets in pickles (train_3.pkl & test_3.pkl).\n\n**If you find this material & hints useful, please upvote.**\n\n*Spoiler hint if you are in a hurry: focus only on the alphabetic characters once cipher #2 has been applied to the matching plaintext of a cipher # text* (see section 3.1)\n\nThis kernels proceeds as follows:\n1. It loads and pre-processes all the data for the task at hand:\n    * The plain-text set from scikit-learn\n    * The competition's train & test set from kaggle\n    * The cipher #2 map from another kernel (https://www.kaggle.com/leflal/cipher-1-cipher-2-full-solutions)\n1. It provides several angles to \"understand\" cipher #3:\n    * Matching ciphertexts and plaintexts which begin with \"From:\"\n    * Matching frequent words across ciphertexts (Subject:, Organization, Lines:)\n    * Matching ciphertexts and plaintexts by length of words sequence\n1. It provides the decryption of cipher #3 train & test sets\n\nHopefully this will help you move past the brick wall. See you on cipher #4 and all the best for 2019.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nimport re\n\nfrom collections import Counter\nfrom dask import delayed, compute\nfrom dask.diagnostics import ProgressBar\nfrom fuzzywuzzy import fuzz, process\nfrom IPython.core.display import display\nfrom itertools import cycle, islice\nfrom sklearn.datasets import fetch_20newsgroups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9b69453d8bce58ef3c7f6a18d247a4de4786a32"},"cell_type":"code","source":"ProgressBar().register()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f3ceecf3e0e425f01603f81bd9e4accaac4b4e7"},"cell_type":"code","source":"chunk_size = 300\npd.options.display.max_columns = chunk_size\npd.options.display.max_rows = chunk_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"646f3c525ce8b864b29a2e92480c3dc4ddf7b6a9"},"cell_type":"markdown","source":"# *1.* Loading and Preprocessing the Data"},{"metadata":{"_uuid":"bdbcd5331b41109a0c6b3b3174b6b6dc37580a86"},"cell_type":"markdown","source":"## *1.1* The Plaintexts from Scikit-learn"},{"metadata":{"trusted":true,"_uuid":"a82b86bc03364174d51bac601774c5085ac3ed64"},"cell_type":"code","source":"train_p = fetch_20newsgroups(subset='train')\ntest_p = fetch_20newsgroups(subset='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5299811620961583deef347f58965e075276272"},"cell_type":"code","source":"df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],\n                                   columns= ['text','target']),\n                      pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],\n                                   columns= ['text','target'])],\n                     axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e16df8c73eb4bb8a5f5c4bfc38c8d54dcbca0ce"},"cell_type":"code","source":"df_p['target'] = df_p['target'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a34901538f594fa2e205afd725d7d134e984243"},"cell_type":"markdown","source":"RS Turley insightfully pointed in his \"four tips from his experience\" (https://www.kaggle.com/c/20-newsgroups-ciphertext-challenge/discussion/75785) and in his comment regarding this kernel, we have added plaintext pre-processing and chunking. "},{"metadata":{"trusted":true,"_uuid":"a8ef27bcfab674500b270233ec6173747fa169ed"},"cell_type":"code","source":"df_p['text'] = df_p['text'].map(lambda x: x.replace('\\r\\n','\\n').replace('\\r','\\n').replace('\\n','\\n '))\ndf_p.loc[df_p['text'].str.endswith('\\n '),'text'] = df_p.loc[df_p['text'].str.endswith('\\n '),'text'].map(lambda x: x[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63a28655d44ee11965525d7960e83995af86a234"},"cell_type":"code","source":"p_text_chunk_list = []\np_text_index_list = []\n\nfor p_index, p_row in df_p.iterrows():\n    p_text = p_row['text']\n    p_text_len = len(p_text)\n    if p_text_len > chunk_size:\n        for j in range(p_text_len // chunk_size):\n            p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])\n            p_text_index_list.append(p_index)\n        if p_text_len%chunk_size > 0:\n            p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])\n            p_text_index_list.append(p_index)\n    else:\n        p_text_chunk_list.append(p_text)\n        p_text_index_list.append(p_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cab98a53dc90948816aaa9ff65ee5b507a1c2db6"},"cell_type":"code","source":"df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})\ndf_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')\n\ndf_p_chunked_list = []\nfor i in np.sort(df_p_chunked['target'].unique()):\n    df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68f0021fb8b38ecbfad12de66a951ee92de506bb"},"cell_type":"markdown","source":"## *1.2* The Ciphertexts From the Competition's Train & Test Set"},{"metadata":{"trusted":true,"_uuid":"99a7cba06fab1a586d9b09264ba57c0acc91a2b2"},"cell_type":"code","source":"competition_path = '../input/20-newsgroups-ciphertext-challenge/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af8a3b33ad2096ca88804de9c39621ad0fbee62d"},"cell_type":"code","source":"train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'})\ntest = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54a51c32a60e08ecb816ba9702c82c4ac45e79f0"},"cell_type":"code","source":"difficulty_level = 3\ntrain = train[train['difficulty'] == difficulty_level]\ntest = test[test['difficulty'] == difficulty_level]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b0c1c584eb30eb0bb2c28843fd0db2949336fcb"},"cell_type":"markdown","source":"## *1.3* The Cipher #2 Map from Another Kernel"},{"metadata":{"trusted":true,"_uuid":"6c533aee0408c7af241fb84fb5ad89998cadbfe8"},"cell_type":"code","source":"cipher_path = '../input/cipher-1-cipher-2-full-solutions/'\ncipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv')\ntranslation_2 = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a5d64e5417ef09295e4c70bc9ae9b14e46a4a10"},"cell_type":"code","source":"train['t_text'] = train.apply(lambda x: x['text'].translate(translation_2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f153d31f62d7507e25350c98bb7865c066f38143"},"cell_type":"markdown","source":"# *2.* Providing Several Angles to Understand Cipher #3"},{"metadata":{"_uuid":"f3abc1ebc7030466b697ac68b0e39591556dceb1"},"cell_type":"markdown","source":"## *2.1* Plaintexts Which Begin With \"From:\"\nAs suggested by Aman (https://www.kaggle.com/amansohane/level-3-with-partial-deciphering-0-94-level-3), we can focus on particular messages, namely those which start with From:"},{"metadata":{"trusted":true,"_uuid":"b3d63dc78474b14e6a57e0e03f0c42a351c305c3"},"cell_type":"code","source":"df_p_extract = df_p[df_p['text'].str.startswith('From:')].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ca1ceedcf9676fdca9c73dcca38db8ca31b66ac"},"cell_type":"code","source":"df_p_extract['text'] = df_p_extract['text'].map(lambda x: x[:300])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a779796ed6a913f057f6ba52dabebdff9cdd360"},"cell_type":"code","source":"df_p_extract['p_len'] = df_p_extract['text'].map(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d632e3e6103352443c5c4fb18dd7cefadcd61c8"},"cell_type":"code","source":"df_p_list = []\nfor i in np.sort(df_p_extract['target'].unique()):\n    df_p_list.append(df_p_extract[df_p_extract['target'] == i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f4d26223bf7113b3773b36a2de9128d5897eb1c"},"cell_type":"code","source":"df_c = train[train['t_text'].str.startswith('FrMmZ')].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fc0165d73e4fe727703159953524e73ca34a675"},"cell_type":"code","source":"len(df_c)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4173c7ddedfb7617fa7e1113cd9ca2eaecaa38c1"},"cell_type":"markdown","source":"So there are 1369 messages from cipher #3 which begin with \"From:\". Let's try to match them with plaintexts using their known target, their length and fuzzywuzzy."},{"metadata":{"trusted":true,"_uuid":"d6dba3a9e98d7f8b1d82c283236a488cf0a0485d"},"cell_type":"code","source":"def find_match(idx):\n    target = df_c.loc[idx,'target']\n    t_text = df_c.loc[idx,'t_text']\n    t_len = len(t_text)\n    df_p_match = df_p_list[target][df_p_list[target]['p_len']==t_len]\n    p_text, fscore, p_index =  process.extractOne(t_text, df_p_match['text'], scorer = fuzz.token_set_ratio)\n    return(p_text, fscore, p_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67edf6b263a2cb67bd5774e669d625eab02460a1"},"cell_type":"code","source":"par_compute = [delayed(find_match)(idx) for idx in df_c.index]\ncp_matches = compute(*par_compute, scheduler='processes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efae0bc892bc71991d229dd3a1d1fd645b760246"},"cell_type":"code","source":"cp_matches = pd.DataFrame(list(cp_matches),columns=['p_text','fscore','p_index'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9db063bc3809232564d03ae9e203b8542520b1c"},"cell_type":"code","source":"df_c = df_c[['target','text','t_text']].reset_index().rename(columns={'index' : 'c_index', 'text' : 'c_text'})\ndf_c = pd.concat([df_c,cp_matches],axis=1)\ndf_c.sort_values(by='fscore',ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8068935effaa793f702b767e7ce345dc2f7136b9"},"cell_type":"code","source":"df_c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9c9d7ebe70edead3e6b304557f9695a8efe82e0"},"cell_type":"code","source":"df_c_copy = df_c.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79a6e9d23cb54e49bcec74f973010aa3378a812a"},"cell_type":"markdown","source":"## *2.2* Frequent Words Across Ciphertexts (Subject:, Organization:, Lines:)"},{"metadata":{"trusted":true,"_uuid":"9a696d3a4e724ad9d72c7c1774d31ea03d059b35"},"cell_type":"code","source":"def word_freqs(s, seps):\n    words = list(filter(None, re.split('[' + ''.join(seps) + ']+',s)))\n    freqs = pd.Series(words).value_counts()\n    freqs = freqs.reset_index().rename(columns={'index' : 'word', 0:'count'})\n    freqs['word_len'] = freqs['word'].map(len)\n    freqs['abs_freq'] = 100 * freqs['count'] / len(words)\n    freqs = pd.merge(freqs,\n                     freqs.groupby('word_len')[['count']].sum().reset_index().rename(columns={'count' : 'word_len_count'}),\n                     on='word_len')\n    freqs['rel_freq'] = 100 * freqs['count'] / freqs['word_len_count']\n    freqs.sort_values(by='abs_freq',ascending=False,inplace=True)\n    return(freqs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83d734f8b33bfe32b5d1870822feef6294de8a82"},"cell_type":"code","source":"plaintext = ' '.join(df_p_extract['text'])\np_words = word_freqs(plaintext,[' '])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9854347acf51d2e116deb6a8986a696f9bb51a1"},"cell_type":"code","source":"p_words.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a54f733318ad3b1bf94dac908f9df1e99b9c904","scrolled":true},"cell_type":"code","source":"words = ['Subject:','Organization','Lines:'] \nt_words = [r'\\s*(..bje.t.)\\s', r'\\s*(Or..n...t..n.)\\s', r'\\s*(..nes.)\\s']\n#The above regular expressions have been inferred by manually looking at a few ciphertexts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ed3c8c8404718bc636b2f2281274d30ca066af4"},"cell_type":"code","source":"for i, t in enumerate(t_words):\n    w = words[i]\n    df_c[w + '_is'] = df_c['p_text'].map(lambda x: [match.span()[0] for match in re.finditer(w, x) if match is not None])\n    df_c[w + '_is_t'] = df_c['t_text'].map(lambda x: [match.span(1)[0] for match in re.finditer(t, x,re.DOTALL) if match is not None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"778d8f63f98945a5c988e4da4f7e4cc51ee20674"},"cell_type":"code","source":"df_c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6927dbe2523c6d5f951821fb3420f717463b2ba3"},"cell_type":"code","source":"def frequent_word_match(x):\n    res = True\n    for i, t in enumerate(t_words):\n        w = words[i]\n        res = res and (x[w + '_is'] == x[w + '_is_t'])\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a42cd5fcda3c7122c55a65a8c1682219ce706ce"},"cell_type":"code","source":"df_c['freq_word_match'] = df_c.apply(lambda x: frequent_word_match(x),axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56d036396c9c7eeed1569750382e80dcf1daf757"},"cell_type":"code","source":"len(df_c[~df_c['freq_word_match']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aaf577ef9a6853b108c6ba538af87efc43719b7"},"cell_type":"code","source":"df_crib = df_c[df_c['freq_word_match']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7acd801672a81b16fe29ef1f8393c9783cb61c98"},"cell_type":"code","source":"len(df_crib)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6db81ee0b2ed89cd3f71ad9aca2a75e78553418f"},"cell_type":"markdown","source":"## *2.3* Length of Words Sequence"},{"metadata":{"trusted":true,"_uuid":"3857fe5c09615e2ab8a3991171864dbd3e07abef"},"cell_type":"code","source":"def word_aligned(x):\n    t_text = x['t_text']\n    p_text = x['p_text']\n    t_list = t_text.split(' ')\n    p_list = p_text.split(' ')\n    return [len(s) for s in t_list] == [len(s) for s in p_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"944cccd57ad5ec908075687c24e3f21787787e39"},"cell_type":"code","source":"df_crib['word_aligned'] = df_crib.apply(lambda x: word_aligned(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e77bad5a17f76488e8040b35e98225ab448a7dc9"},"cell_type":"code","source":"df_crib_misaligned = df_crib[~df_crib['word_aligned']]\nlen(df_crib_misaligned)\n#We may investigate these misaligned cipher & plaintext pairs later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92c1baf0537214db2f0c2b2d284766d2db5fa65"},"cell_type":"code","source":"df_crib = df_crib[df_crib['word_aligned']]\nlen(df_crib)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92869a492a7f1f1ddab915d22d0a5932f05408f8"},"cell_type":"code","source":"df_crib = df_crib[['target','c_index','c_text','p_text','p_index']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1935580b490e447eea9828c3a44fb0976a72730a"},"cell_type":"markdown","source":"# *3* Decryption of Cipher #3\n## *3.1* Using the crib from plaintext to cipher"},{"metadata":{"_uuid":"1b2dab3f895fbee19be5ea61f2cb250f3ed764d0"},"cell_type":"markdown","source":"Previously we started by applying cipher #2 decryption to cipher #3 ciphertexts. Now let's apply cipher #2 encryption to the cipher #3 matching plaintext we have found"},{"metadata":{"trusted":true,"_uuid":"a1d30433e5aaa8ef1945d9910f7500066fdf4b95"},"cell_type":"code","source":"df_crib.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f85a5e9e67841fcf64315e00352c217b4d28e1b7"},"cell_type":"code","source":"translation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption\ntranslation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4101e1594600270f13f2f278a725549a71352ab9"},"cell_type":"code","source":"# Checking that no characters are missing in cipher #2 map to encrypt the cipher #3 plaintexts from the crib\n\ncipher2_plain_alphabet = set(''.join(cipher2_map['plain']))\ndf_crib['p_text_ok'] = df_crib['p_text'].map(lambda x: len(set(x).difference(cipher2_plain_alphabet)) == 0)\nlen(df_crib[~df_crib['p_text_ok']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7175f16895ab0f071eabeb2f05d68ffd121787e5"},"cell_type":"code","source":"df_crib.drop('p_text_ok',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a3ea0910dcfde60352be5a6f46d15bc5e234835"},"cell_type":"code","source":"df_crib['pt_text'] = df_crib['p_text'].map(lambda x: x.translate(translation_2_pt))\ndf_crib['ct_text'] = df_crib['c_text'].map(lambda x: x.translate(translation_2_ct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68803db5a653a9b2499c707faae57831b1909d64"},"cell_type":"code","source":"df_crib.to_pickle('df_crib.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87d66c2c9bdc5c4bad10ad45401e8eb06a8cdbab"},"cell_type":"markdown","source":"## *3.2* Zooming of a Few Cipher&Plaintext Pairs"},{"metadata":{"trusted":true,"_uuid":"f3a45d80e0da8118c2c1cc58ecf3c8aa170ccf7e"},"cell_type":"code","source":"def compare_ptc(idx):\n\n    p_text = df_crib['p_text'].loc[idx]\n    ct_text = df_crib['ct_text'].loc[idx]\n    \n    pt_text = df_crib['pt_text'].loc[idx]\n    c_text = df_crib['c_text'].loc[idx]\n    \n    c_split = c_text.split('8')\n    pt_split = pt_text.split('8')\n    ct_split = ct_text.split(' ')\n    p_split = p_text.split(' ')\n\n    return(pd.DataFrame([p_split,ct_split,pt_split, c_split],index=['p','ct','pt','c']).T)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"796cb7258356712edf15d9b0676bb02c46c46bc0"},"cell_type":"code","source":"def hide_ok_nok(x,pt = True, hide_ok = True):\n    pt_w = x['pt']\n    c_w = x['c']\n    if pt:\n        res = pt_w\n    else:\n        res = c_w\n    ok_i = set([i for i,(a,b) in enumerate(zip(pt_w,c_w)) if (ord(a) ^ ord(b) == 0)])\n    if hide_ok:\n        return(''.join(['.' if i in ok_i else res[i] for i in range(len(pt_w))]))\n    else:\n        return(''.join(['.' if i not in ok_i else res[i] for i in range(len(pt_w))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ac312c9e3628876b3a7c85a3f5164aa8651b787"},"cell_type":"code","source":"df_crib.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5a6c83bbc2f7282885e91c1eeb11a51e6dd53fd"},"cell_type":"code","source":"df_z = compare_ptc(846)\ndisplay(df_z.applymap(repr).T)\n\ndf_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1)\ndf_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1)\ndf_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1)\ndf_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1)\ndisplay(df_z.applymap(repr).T)\n\ndf_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok'])\ndisplay(df_zz)\n\npt_h = ''.join(df_zz.loc['pt_h_hide_ok'])\nprint('Characters to further encipher')\nprint(repr(pt_h))\npt_h_n = ''.join(df_zz.loc['pt_h_hide_nok'])\nprint('Characters of cipher#2 equal to cipher#3')\nprint(repr(pt_h_n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da235b146861660bf8143d0c5f868d58edf742bc"},"cell_type":"code","source":"df_z = compare_ptc(549)\ndisplay(df_z.applymap(repr).T)\n\ndf_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1)\ndf_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1)\ndf_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1)\ndf_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1)\ndisplay(df_z.applymap(repr).T)\n\ndf_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok'])\ndisplay(df_zz)\n\npt_h = ''.join(df_zz.loc['pt_h_hide_ok'])\nprint('Characters to further encipher')\nprint(repr(pt_h))\npt_h_n = ''.join(df_zz.loc['pt_h_hide_nok'])\nprint('Characters of cipher#2 equal to cipher#3')\nprint(repr(pt_h_n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5306b023177d90cdef2e1de9036a0a9c9666275"},"cell_type":"markdown","source":"We could keep on checking that ciper#3 only seems to re-encipher alphabetic characters after cipher #2 has been applied (the astute reader will have noticed that a few alphabetic characters in the sample above were interspersed among the non-alphabetic ones, however they should be coincidences as we will indeed show below, they correspond to the 'a' of the cipher #3 key applied to this alphabetic character)"},{"metadata":{"_uuid":"3d348cb08ff4f48dba072163dc40031996190165"},"cell_type":"markdown","source":"## *3.3* Cipher #3 Decryption"},{"metadata":{"_uuid":"0cadb0dfba66a0f90df8bbb4f6736d9ecd6c166a"},"cell_type":"markdown","source":"### *3.3.1* Identifying Cipher #3 and Recovering its Key\nAs discussed in the previous section, we focus on enciphering with cipher #3 the alphabetic characters output by cipher# encryption"},{"metadata":{"trusted":true,"_uuid":"90dd766970cca3d2ce110b5300f84fc2f3c3a12a"},"cell_type":"code","source":"df_z = compare_ptc(846)\npt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt']))\nc_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c']))\ndf_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c'])\ndf_3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ddb737914196f2f8a6d8209d31c141147046c50"},"cell_type":"markdown","source":"We can see that cipher #3 is a polyalphabetic cipher (two identical plaintext characters maybe enciphered to different ciphertext characters), so let's check the usual polyalphabetic suspects."},{"metadata":{"trusted":true,"_uuid":"921d62e20b72f43f0a0bf2f6b1373829a95454f8"},"cell_type":"code","source":"df_3_n = df_3.applymap(ord)\ndf_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0)\ndf_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0)\ndf_3_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edf06a90f97ca52b932875fe904f034a2715e8f9"},"cell_type":"markdown","source":"We can see for the above plain/cipher text pair that the encryption seems to be a vigenere using the following key: [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18]"},{"metadata":{"trusted":true,"_uuid":"da754d8f9ca8da380c4a6dbea080974d67e511ff"},"cell_type":"code","source":"key_ord = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c08c670cd2fe54bc773e8b44a5fa0fccd5a04835"},"cell_type":"code","source":"df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0)\ndf_3_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3d88bcfbd59da77a4bc9d97ce7858ed202aae18"},"cell_type":"code","source":"(df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe5c1c27ba7d106cd3fa3b715891a032f4b155a0"},"cell_type":"markdown","source":"Let us check this on another pair"},{"metadata":{"trusted":true,"_uuid":"27c31ba01f9dea375dc68d273b40edc23296e69e"},"cell_type":"code","source":"df_z = compare_ptc(549)\npt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt']))\nc_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c']))\ndf_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c'])\ndf_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c1b292c8e0fa687cdacf14a985a5cc675612456"},"cell_type":"code","source":"df_3_n = df_3.applymap(ord)\ndf_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0)\ndf_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0)\ndf_3_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33c465538110e3bdfec677b9ec9f9580f929211e"},"cell_type":"code","source":"df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0)\n(df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78060ea37ecf4f9c4de75f62a31677c5192fa8db"},"cell_type":"markdown","source":"Bingo!\n\nActually if we want to assign a meaning to the key [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18], we can do so:"},{"metadata":{"trusted":true,"_uuid":"b2f2ea8c16843f796d056f2bde0640c5f55aff7b"},"cell_type":"code","source":"key_char = [chr(i+ord('a')) if i>=0 else ' ' for i in key_ord]\n''.join(key_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a30cccbcd54d1f9bf2f61d9fa6fdc8894e7dc87"},"cell_type":"markdown","source":"### *3.3.2* Decrypting Cipher #3"},{"metadata":{"_uuid":"06e7394c477c4b9083cbb39225219e9e0828a4f7","trusted":true},"cell_type":"code","source":"train.drop('t_text',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da9105ff563691eaf973b1e99f6b71c37c0fb82e"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98f12eeee0715a9d2bddc7d7a629126a23c3e76f"},"cell_type":"code","source":"def shift_char(c,shift):\n    if c.islower():\n        return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))\n    else:\n        return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59119f1cd5821e4b631ef1d6962a0d10552334af"},"cell_type":"code","source":"def replace_alpha(l,l_alpha_new):\n    res = []\n    i_alpha = 0\n    for i in range(len(l)):\n        if l[i].isalpha():\n            res.append(l_alpha_new[i_alpha])\n            i_alpha += 1\n        else:\n            res.append(l[i])\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f56443a94f215b758ebf1e6323ffceb8222ada6"},"cell_type":"code","source":"def fractional_vigenere(s,key):\n    l = list(s)\n    l_alpha = [x for x in l if x.isalpha()]\n    l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key_ord), len(l_alpha))))]\n    return(''.join(replace_alpha(l,l_alpha_shifted)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c4de2cf230c0f5685a7e6940be7518bf8aead9"},"cell_type":"code","source":"train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3eda17efe2c5ff48be618ad2ff272e500572209"},"cell_type":"code","source":"target_list = np.sort(df_p_chunked['target'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e1c4f53249fb8e564073ba466450abe44f32f23"},"cell_type":"code","source":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in train[train['target'] == i].index[:]:\n        ct_text = train.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64ac0cc02c30d210843a4d48a90967a76f74969a"},"cell_type":"code","source":"train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de2e8e68330c65ce3b865c4848c91c5e06db719c"},"cell_type":"code","source":"print(train.shape[0])\nprint(train_p_indexes.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc62a85422d09fb569e6056fb2d825c4e8d18d81"},"cell_type":"code","source":"train = train.join(train_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a027569cfa54969be07bf43565e671da640d65bc"},"cell_type":"code","source":"train.to_pickle('train_3.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b428c1603859855e3b459b5cb133a4e000a3105"},"cell_type":"code","source":"test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b39ae9310f63549f03e483d38b296b761b1e0b"},"cell_type":"code","source":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in test.index[:]:\n        t_text = test.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97d52652c319004230bb43d39a0c172fe7d52f67"},"cell_type":"code","source":"test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edb54b9abc11ffff91e434092f593e32e7c43978"},"cell_type":"code","source":"print(test.shape[0])\nprint(test_p_indexes.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26227c561e7b143587766141d3011f798095b696"},"cell_type":"code","source":"test = test.join(test_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d155d5184f072b912b282581a776122787f0de46"},"cell_type":"code","source":"test.to_pickle('test_3.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}