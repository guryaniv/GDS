{"cells": [{"source": ["Hello, I started studying time series one week ago and I am not an expert in machine learning. This is my first kernel! I hope to get feedback, in particular I do not know yet how to interpret the Ljung-Box test on the bottom of this notebook.  \n", "\n", "For this competition I noticed a couple of kernels getting stuck with the number of visitors showing a sudden increase in July 2016. That happens simply because in July 2016 there are more restaurants in the data. For this reason I thought of using the mean of visitors and practice with a seasonal ARIMAX model."], "cell_type": "markdown", "metadata": {"_uuid": "2c2e5d08a847999dfd519f0ac76f586f4f6cca02", "_cell_guid": "05801232-1dd4-40ed-b900-91add9fa91ac"}}, {"source": ["## Recruit Restaurant Visitor Forecasting\n", "Forecast restaurant visits for establishments in Japan based on historical visits and reservation data from two websites - Hot Pepper Gourmet and AirREGI. We are also given some additional metadata on the restaurants such as genre and location. Find the competition on [this link at kaggle](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)\n", "\n"], "cell_type": "markdown", "metadata": {"_uuid": "c9c35677a30f3c666f8457e8134e4fd56d5874d5", "_cell_guid": "648b6c64-f31a-4214-9413-fca4c0b51a6d"}}, {"source": ["%matplotlib inline\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = \"all\""], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "107f112e9737900a22b4c8ee98738bd055162f2d", "collapsed": true, "_cell_guid": "0785a869-7f52-474b-be03-03a99e4af147"}, "outputs": []}, {"source": ["### Read the data\n", "\n", "NB: The number of stores with reservations = number of reservations because there's one reservation per store.\n", "    For this reason the mean is IMO a good normalized measure of visits, mean = # visits / # reserved stores"], "cell_type": "markdown", "metadata": {"_uuid": "ec2447787fcb39a5d38fe62775e02cbc42aecfc1", "_cell_guid": "cfc37583-9258-4448-a9f8-77d5c24acf65"}}, {"source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn import *\n", "from datetime import datetime\n", "import calendar\n", "# \n", "air = pd.read_csv('../input/air_visit_data.csv', parse_dates=[1])\n", "air.set_index(['visit_date'], inplace=True)\n", "air.index.name=None\n", "air.drop('air_store_id',axis=1,inplace=True)\n", "df2=pd.DataFrame()\n", "df2['visit_total'] = air.groupby(air.index,squeeze=True,sort=True)['visitors'].sum()\n", "df2['visit_mean'] = air.groupby(air.index,squeeze=True,sort=True)['visitors'].mean()\n", "df2['reserv_cnt'] = air.groupby(air.index,squeeze=True,sort=True)['visitors'].count()\n", "air=df2;del df2\n", "\n", "#Get the date info with dow and holidays\n", "hol=pd.read_csv('../input/date_info.csv', parse_dates=True).rename(columns={'calendar_date':'visit_date'})\n", "hol['visit_date'] = pd.to_datetime(hol['visit_date'])\n", "hol.set_index(['visit_date'], inplace=True)\n", "hol.index.name=None\n", "hol.day_of_week = hol.day_of_week.apply(list(calendar.day_name).index)\n", "\n", "#Get the test submission\n", "test = pd.read_csv('../input/sample_submission.csv')\n", "test['store_id'], test['visit_date'] = test['id'].str[:20], test['id'].str[21:]\n", "test.set_index('visit_date', drop=True, inplace=True)\n", "test.index.name=None"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9240bee771f31fdfe35b48cd201ebf54265029e2", "_cell_guid": "52735984-b48f-48fa-b0d9-80ee1b45a5c1"}, "outputs": []}, {"source": ["### Explorative analysis on cumulative visits\n", "I create a series with cumulative visits, that is the sum of visits to all stores/restaurants. Notice the huge increase in July 2016 (see above). The drop comes because many restaurants did not have any data before that month. Probably the registration system started \"monitoring\" many new restaurants/stores in that period.  \n", "There is also a drop in visits and reservation counts in the beginning of the years, probably due to stores being close around new year's eve.  "], "cell_type": "markdown", "metadata": {"_uuid": "e98af6ebe6f3d638247c750065c09f80552100fa", "_cell_guid": "03cceeca-e88b-4d0b-93de-573d5995f0e8"}}, {"source": ["#Plot the cumulative visits\n", "air['visit_total'].plot(legend=True);\n", "air['reserv_cnt'].plot(legend=True, figsize=(15,4), secondary_y=True,\n", "                      title='Visitors total and reservation count (with holidays)');\n", "for x in hol.query('holiday_flg==1').index:\n", "    _ = plt.axvline(x=x, color='k', alpha = 0.3);"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "39bd2d766ee5f5392fb62ed24bd54b4be5d4dcb6", "_cell_guid": "77887354-d3b0-49e8-89a0-367657519ee6"}, "outputs": []}, {"source": ["Instead of the sum of visits, a better measurement is **the mean** of the cumulative visits (sum of all visits divided by number of reservations or, which is the same, the number of \"active\" restaurants). The series does not show the gap in July 2016 anymore, though variance seems to become smaller for some months."], "cell_type": "markdown", "metadata": {"_uuid": "41fead117e58f4418d96d828b7b57106148e676f", "_cell_guid": "43ba8caa-b7cd-4fd0-8105-47307cd47be3"}}, {"source": ["air['visit_mean'].plot(figsize=(15,4), legend=True, title='Visitors mean (with holidays)')\n", "air['reserv_cnt'].plot(legend=True, figsize=(15,4), secondary_y=True, title='Visitors total and reservation count (with holidays)');\n", "for x in hol.query('holiday_flg==1').index:\n", "    _ = plt.axvline(x=x, color='k', alpha = 0.3);"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "be5ae08cb694de4e75a0c077b5eda526760825ac", "_cell_guid": "89161327-005a-48c0-a90a-e1f5183e8425"}, "outputs": []}, {"source": ["Applying a seasonal decomposition using moving averages. The trend captures three peaks in the data: \n", "\n", "* Around mid-March 2016, \n", "* April 2016\n", "* Just before the drop on new year's eve 2016, \n", "* Again mid-March 2017 \n", "I see on wikipedia that these are the vernal equinox (around March 20), the golden week starting on April 29, Emperor's birthday :( on December 23rd and new year's day on January 1st, vernal equinox again.  \n", "This sounds great, though some other holidays are hardly visible in the data. I can imagine that people has the tendency to stay home in the winter.\n", "\n", "The season plot shows a strong weekly period where the lowest number of visit happen on Monday. Below I show that people mostly do reservations between Friday and Sunday."], "cell_type": "markdown", "metadata": {"_uuid": "45dcf197743c5c30ea307a87a05114b932112b5d", "_cell_guid": "c1a21cb5-cd0a-407e-ae2c-340d0dbc5a25"}}, {"source": ["import statsmodels.api as sm  \n", "from statsmodels.tsa.stattools import acf  \n", "from statsmodels.tsa.stattools import pacf\n", "from statsmodels.tsa.seasonal import seasonal_decompose\n", "decomposition = seasonal_decompose(air.visit_mean, freq=12)  \n", "fig = plt.figure()  \n", "fig = decomposition.plot()  \n", "fig.set_size_inches(15, 8)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "692856805f609e753743d770e7b086a80450b886", "_cell_guid": "015be715-bf88-4148-83b0-8e07a5991946"}, "outputs": []}, {"source": ["df2=air.join(hol)\n", "df2[df2.holiday_flg==0].groupby(hol.day_of_week,squeeze=True,sort=True)['visit_mean'].sum()\n", "#df2.day_of_week=df3.day_of_week.apply(lambda x: list(calendar.day_name)[x]) # equiv to air.sum(0)"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["### Pre-ARIMA analysis: make the series stationary\n", "Stationary means that variance and autocorrelation structure not changing over time.  \n", "Changes in mean happen when the series has a overall trend, e.g. it increases/decreasing. In that case use differentiation: create a new series y with the first difference y=x$_t$-x<sub>t-1</sub> (this is the \"I\" in ARIMA models), and take seasonal differences (which you can account for in ARIMA model).  \n", "Changes is variance mean that oscillations change. To fix this a log of the series compresses oscillations.  \n", "Changes in autocorrelation means that oscillations become broader (or narrower). If you have this I think you are screwed..\n", "\n", "The Dickey-Fuller test helps us deciding whether a series is stationary. The test's null hypothesis is that the series is non-stationary. If the Test Statistic output of the Dickey-Fuller test is less than the Critical Value, we can reject the null hypothesis and say that the series is stationary.  \n", "Below we notice that the original time series (using the mean of visits) is fairly stationary: Test Statistic=-3.796104 > Critical Value (1%) = -3.444615. Applying 1st or seasonal differentiation greatly improves the test results (Test Statistic = -6.608968 and 7.196314, respectively). A weekly periodic structure is clearly visible in the data, so I lean towards using seasonal differentiation. \n", "Takign the log of the data helps but does not seem to be a main factor in improving stationarity. This means that variance is fairly stable. "], "cell_type": "markdown", "metadata": {"_uuid": "3789f91d2e3d81dfc5bca8b81c06f8ab37a6e156", "_cell_guid": "a740dc14-c1bc-419d-88b8-880528b27c8d"}}, {"source": ["from statsmodels.tsa.stattools import adfuller\n", "def test_stationarity(timeseries):\n", "    \n", "    #Determing rolling statistics\n", "    rolmean = timeseries.rolling(window=12,center=False).mean();\n", "    rolstd = timeseries.rolling(window=12,center=False).std();\n", "\n", "    #Plot rolling statistics:\n", "    fig = plt.figure(figsize=(15, 5))\n", "    orig = plt.plot(timeseries, color='blue',label='Original')\n", "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n", "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n", "    plt.legend(loc='best')\n", "    plt.title('Rolling Mean & Standard Deviation')\n", "    plt.show()\n", "    \n", "    #Perform Dickey-Fuller test:\n", "    print('Results of Dickey-Fuller Test:')\n", "    dftest = adfuller(timeseries, autolag='AIC')\n", "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n", "    for key,value in dftest[4].items():\n", "        dfoutput['Critical Value (%s)'%key] = value\n", "    print(dfoutput)\n", "\n", "test_stationarity(air.visit_mean); #-3.796104\n", "# Log is a minor improvement, meaning that the variance is stable\n", "air.visit_mean_log= air.visit_mean.apply(lambda x: np.log(x))  \n", "'''test_stationarity(air.visit_mean_log) #-3.830754'''\n", "# Although I see no real global trend, 1st difference strongly improves stationarity\n", "air['visit_mean_diff'] = air.visit_mean - air.visit_mean.shift(1)  \n", "test_stationarity(air.visit_mean_diff.dropna(inplace=False)) #-6.608968e+00\n", "# Seasonal difference: take a weekly season improves stationarity even more\n", "air['visit_mean_seasonal'] = air.visit_mean - air.visit_mean.shift(7)\n", "test_stationarity(air.visit_mean_seasonal.dropna(inplace=False)) #-7.196314e+00\n", "# Seasonal and 1st difference is even better, but we were already well within the 1% confidence interval\n", "air['visit_mean_seasonal_diff'] = air.visit_mean_diff - air.visit_mean_diff.shift(7)\n", "test_stationarity(air.visit_mean_seasonal_diff.dropna(inplace=False)) #-9.427797e+00"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "920485a3d879b6120a029fcd66507fdbc4d658f1", "_cell_guid": "54fe29a4-4cd4-4ac6-a513-b83707a0eaf7"}, "outputs": []}, {"source": ["mmm, let's stick to the seasonal series with **no 1st difference**. We will run auto-SARIMAX later and see if it makes sense. \n", "\n", "Let's run autocorrelation ACF and partial autocorrelation PACF for find the details fo the model. I will use the guidelines [here](http://people.duke.edu/~rnau/arimrule.htm).\n", "\n", "The following ACF on the mean visits has a repeated pattern. That's our 7-day seasonal term. I should add to the model seasonal differenciation in the term **seasonal\\_order=( , 1, ,7)**.  \n", "If ACF was positive and decreasing over time, the series would be the typical candidate for applying 1st difference. But not here. This confirms the observation we made on the Dickey-Fuller test where the 1st difference was more stationary but not that much. I should add a **order=( , 0, ) term** to the model. Also try a constant term **trend='c'** in the model for the non-zero mean value."], "cell_type": "markdown", "metadata": {"_uuid": "1415d4a970f4fe9caff4a4cb88903ab1b325489b", "_cell_guid": "0539b944-a2a1-44cb-a7e9-ac42ca8a3bbe"}}, {"source": ["fig = plt.figure(figsize=(12,8))\n", "ax1 = fig.add_subplot(211)\n", "fig = sm.graphics.tsa.plot_acf(air.visit_mean, lags=40, alpha=.05, ax=ax1)\n", "ax2 = fig.add_subplot(212)\n", "fig = sm.graphics.tsa.plot_pacf(air.visit_mean, lags=40, alpha=.05, ax=ax2)\n", "print(\"ACF and PACF of the visit mean:\")"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "84bd6e2be298d0631e95ad6d61881ac3efef1620", "_cell_guid": "352d0b07-ee19-47e9-bb37-0aee00381b6d"}, "outputs": []}, {"source": ["This is the ACF of the first difference, confriming the 7-day seasonal term. "], "cell_type": "markdown", "metadata": {"_uuid": "1f9645601b1d90c18ba4f8c1c400b3452d9ac81f", "_cell_guid": "748c2140-e8b7-406d-99b0-660d56602dec"}}, {"source": ["fig = plt.figure(figsize=(12,8))\n", "ax1 = fig.add_subplot(211)\n", "fig = sm.graphics.tsa.plot_acf(air.visit_mean_diff[1:], lags=40, alpha=.05, ax=ax1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a46e2c97e87718c03be513c4cf87555c6240cf24", "_cell_guid": "be69804a-40eb-46ed-b446-162f4cba9a93"}, "outputs": []}, {"source": ["The ACF and PACF below are plotted the seasonal difference data.\n", "\n", "The PACF of the seasonal differenced series is positive at lag=1 (the series appears slightly \"underdifferenced\"). I should probably add one AR term to the model **order=(2, , )**. Only one AR term because the term at lag=2 cuts off and enters the 95% confidence interval. \n", "\n", "As for the PACF, the ACF of the seasonal differenced series is overdifferentiated at lag = 1 and cuts off (which I looked on the internet and means \"go to zero\") at lag=2. I should probably add one MA term to the model **order=( , , 2)**.\n", "\n", "The ACF of the differenced series is negative at lag 7, suggesting to add a seasonal MA term **seasonal\\_order=(0, , 1, )** to the model. This situation is likely to occur if a seasonal difference has been used, which should be done if the data has a stable and logical seasonal pattern. If the peak was positive I would have added a season AR term. The link above suggests to avoid using more than one or two seasonal parameters (SAR+SMA) in the same model to avoid overfitting and/or problems in estimation.\n", "\n"], "cell_type": "markdown", "metadata": {"_uuid": "8a2f06fdb0350b9f939d83ee75c436d0504e3b87", "_cell_guid": "89d36654-7337-4187-a192-b143c7a3aa10"}}, {"source": ["print(\"ACF and PACF of the 7-day differenced visit mean:\")\n", "fig = plt.figure(figsize=(12,8))\n", "ax1 = fig.add_subplot(211)\n", "fig = sm.graphics.tsa.plot_acf(air.visit_mean_seasonal[8:], lags=40, alpha=.05, ax=ax1)\n", "ax2 = fig.add_subplot(212)\n", "fig = sm.graphics.tsa.plot_pacf(air.visit_mean_seasonal[8:], lags=40, alpha=.05, ax=ax2)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1678530ab9ed4c244329e1e8a7bfee27c1d9101a", "_cell_guid": "66a23d8a-7c6d-42b2-9233-b5615af80b38"}, "outputs": []}, {"source": ["More things to keep in mind:  \n", "Rule 9: If there is a unit root in the AR part of the model--i.e., if the sum of the AR coefficients is almost exactly 1--you should reduce the number of AR terms by one and increase the order of differencing by one.  \n", "Rule 10: If there is a unit root in the MA part of the model--i.e., if the sum of the MA coefficients is almost exactly 1--you should reduce the number of MA terms by one and reduce the order of differencing by one.  \n", "Rule 11: If the long-term forecasts* appear erratic or unstable, there may be a unit root in the AR or MA coefficients."], "cell_type": "markdown", "metadata": {"_uuid": "d0ed0bd9ca6f1f27e96d3b9002a7c31ddf2dca08", "_cell_guid": "b02983a6-c738-479e-a38b-da96937afdee"}}, {"source": ["### SARIMA model \n", "[SARIMAX](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html) = Seasonal Auto Regressive Integrated Moving Average (without eXogenous regressors).  \n", "First some helper functions to calculate errors, plot SARIMAX models, and do a grid search on the hyperparamenters of SARIMAX"], "cell_type": "markdown", "metadata": {"_uuid": "dd28cbfd13aa6b273cdace4ca87f4b8968f746f4", "_cell_guid": "51eaefbd-e3d1-427b-9515-2133519998e1"}}, {"source": ["#sklearn.metrics .mean_squared_log_error seems to exist but I cannot load it..\n", "from sklearn.metrics import mean_squared_error\n", "def mean_squared_log_error(y_pred, y_true, **dict):\n", "    '''Assume y_true starts earlier than y_pred, y_true is NaN free, and NaN in y_pred are only in the beginning'''\n", "    indafterNaN = y_pred.first_valid_index()\n", "    if (y_true.index[0] > y_pred.index[0]): return \"Check indices of prediction and true value\"\n", "    ind1stcommon = y_true.index[y_true.index==y_pred.index[0]]\n", "    indstart = max(indafterNaN, ind1stcommon)\n", "    indend = y_true.index[-1]\n", "    return mean_squared_error(np.log(y_true[indstart:indend]+1), \n", "                              np.log(y_pred[indstart:indend]+1) )**0.5\n", "\n", "def plotSARIMAX(labels, pred):\n", "    fig = plt.figure(figsize=(12, 8))\n", "    layout = (2, 2)\n", "    ax1 = plt.subplot2grid(layout, (0, 0), colspan=2)\n", "    ax3 = plt.subplot2grid(layout, (1, 0))\n", "    ax4 = plt.subplot2grid(layout, (1, 1))\n", "    labels.plot(ax=ax1);\n", "    pred.plot(ax=ax1, title='MSE: %.4f'% mean_squared_log_error(pred, labels))\n", "    ax3 = sm.graphics.tsa.plot_acf(results.resid, lags=40, alpha=.05, ax=ax3, title=\"ACF of residuals\")\n", "    ax4 = sm.graphics.tsa.plot_pacf(results.resid, lags=40, alpha=.05, ax=ax4, title=\"PACF of residuals\")\n", "    plt.tight_layout()\n", "    print(\"ACF and PACF of residuals\")"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2218de0b7b03799989cdfec348fd356f1218be85", "_cell_guid": "dd34faf4-ebcb-45e0-aa25-cfc4f8b20387"}, "outputs": []}, {"source": ["from scipy.optimize import brute\n", "from sklearn.metrics import mean_squared_error\n", "\n", "def autoSARIMAX(endog, exog=None, date_train_end=None, pred_days=[-12,12], verbose=True,\\\n", "        ranges=(slice(1,3),slice(0,1),slice(1,3),  slice(0,2),slice(1,2),slice(1,2),slice(7,8))):\n", "    #Instantiate my version of the grid with parameters and scores\n", "    global grid\n", "    grid = []\n", "    #Get indices up to which you do train and prediction \n", "    if date_train_end is None:\n", "        ind_train = endog.index[-1]\n", "    else:\n", "        ind_train = np.where(endog.index==date_train_end)[0][0]\n", "    #Brute optimization\n", "    resultsbrute = brute(runSARIMAX, ranges=ranges, args=(endog,exog,(ind_train,pred_days),), full_output=True, finish=None)\n", "    #First coefficients run two times for some reason or another\n", "    del grid[0]\n", "    #Print/Plot results\n", "    if verbose:\n", "        print(\"Best parameters: {}\".format([int(p) for p in resultsbrute[0]]))\n", "        print(\"Best score:          {}\".format(resultsbrute[1]))\n", "        gr = plotautoSARIMAX(resultsbrute, verbose)\n", "    return resultsbrute, gr\n", "\n", "def plotautoSARIMAX(resultsbrute, verbose=True):\n", "    #Print/Plot results\n", "    if not verbose: return None\n", "    #Plot scores by parameter values\n", "    gr = pd.DataFrame({'params':[''.join(str(n) for n in g[0]) for g in grid], 'score': [row[1] for row in grid], 'aic': [row[2] for row in grid]})\n", "    print(\"All parameters and scores: \\n\")\n", "    print(gr.head(1000).to_string())\n", "    ax1 = gr.plot('params','score',rot=90, grid=True, figsize=(15,4))\n", "    ax2 = gr.plot('params','aic',rot=90, secondary_y=True,ax=ax1)\n", "    ax1.set_ylabel('Score');ax2.set_ylabel('AIC');\n", "    plt.xticks(range(len(gr)), gr.params, rotation=90);\n", "    return gr\n", "\n", "def runSARIMAX(coeffs, *args):\n", "    endog = args[0]\n", "    exog = args[1]\n", "    #Process the row indices for training and prediction\n", "    ind_train = args[2][0]\n", "    pred_days = args[2][1]\n", "    ind_pred = [len(endog)+pred_days[0], len(endog)+pred_days[1]]\n", "    if ind_pred[0] > ind_train: \n", "        #ind_pred[0]=ind_train\n", "        raise ValueError('Make sure prediction bounds begin at least at len(endog): pred_days[0] must be <= %i ' % (ind_train-len(endog)))\n", "    exog_train, exog_pred, start_params = None, None, list()\n", "    if exog is not None:\n", "        if ind_pred[1] > len(exog):\n", "            raise ValueError('Make sure prediction bounds end  <= len(exog): pred_days[1] must be <= %i ' % (len(exog)-len(endog)))\n", "        exog_train = exog[:ind_train]\n", "        exog_cols = 1 if len(exog.shape) == 1 else exog.shape[1]\n", "        start_params.extend(0.1*np.ones(exog_cols-1))\n", "        exog_pred = exog[ind_pred[0]-1:ind_pred[1]]\n", "        exog_pred = pd.DataFrame(exog_pred)\n", "        \n", "    #Get the hyperparameters\n", "    order = coeffs[0:3].tolist()\n", "    seasonal_order = coeffs[3:7].tolist()\n", "    trend = 'c' if (order[1]==0) else 'n'\n", "    #Train SARIMAX and fit it on data, predict to get scores\n", "    try:        \n", "        mod = sm.tsa.statespace.SARIMAX(endog[:ind_train], exog_train, \\\n", "                                        trend=trend, order=order, seasonal_order=seasonal_order)\n", "        start_params.extend(0.1*np.ones( len(mod.params_complete)))\n", "        fit = mod.fit(start_params=start_params)\n", "        pred = fit.predict(start=ind_pred[0], end=ind_pred[1], exog=exog_pred)\n", "        aic = fit.aic\n", "        score = mean_squared_log_error(pred[:-pred_days[0]], endog[ind_pred[0]:])        \n", "        if np.isnan(aic): aic, score = np.inf, np.inf\n", "    except:  #Tip: Try to set starting paramenters in .fit()\n", "        import sys        \n", "        print(\"Error:\", sys.exc_info())        \n", "        print(\"{},{},'{}', len(start_params)={}\\n\".format(coeffs[0:3], coeffs[3:], trend, len(start_params)))\n", "        aic, score = np.inf, np.inf\n", "    #Sorry but I don't like the grid in the output of brute resultsbrute[2]\n", "    global grid\n", "    grid.append([coeffs,score,aic])\n", "    return score\n", "\n", "#Quick example\n", "#resbrute, gr = autoSARIMAX(endog=air.visit_mean, exog=None, date_train_end=\"2017-03-26\", pred_days=[-28,66],\\\n", "#                             ranges=(slice(1,2),slice(0,1),slice(1,2),  slice(0,2),slice(1,2),slice(1,2),slice(7,8)))\n", "\n", "#resbrute, gr=autoSARIMAX(endog=air.visit_mean, exog=hol.holiday_flg, date_train_end=\"2017-03-26\", pred_days = [-28,39],\\\n", "#                    ranges=(slice(1,2),slice(0,1),slice(1,2),  slice(0,1),slice(1,2),slice(1,2),slice(7,8)))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "87ba0ace8b9a41201eb241b6134e3780fe8b9376", "_cell_guid": "bc9c62f7-3846-4a3d-927e-bee79b9ae6ce"}, "outputs": []}, {"source": ["Run grid search on Seasonal ARIMA models to explore the best ones"], "cell_type": "markdown", "metadata": {}}, {"source": ["resbrute, gr = autoSARIMAX(endog=air.visit_mean, exog=None, date_train_end=\"2017-03-26\", pred_days = [-28,39],\\\n", "                             ranges=(slice(1,3),slice(0,2),slice(1,3),  slice(0,2),slice(1,2),slice(1,2),slice(7,8)))\n", "#Not shown, but the SMA parameter is important to keep to 1"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["The autoSARIMAX function evaluates models based on the mean squared log error used in the competition, but the aic Akaike information criterion estimating the quality of the statistical model is also plotted. The two scores seem to be in reasonable agreement.  \n", "Interestingly, 1st differentiation (the second 'd' hyperparamenter) does not seem to be necessary to obtain a good model. This confirms the analysis above. Because overdifferentiating is bad, I will stick to d=0: **( , 0, )x( , 1, , 7)**.   \n", "Although I do not show it the second to last term (SMA) is important and should be at least 1: **( , , )x( , , 1, )**. When I evaluate models with SMA=0 they are always worse and show an ugly zig zag plot.  \n", "The model we had from our analysis is **(2,0,2)x(0,1,1,7)**. Model **(2,0,2)x(1,1,1,7)** next to it in the plot has a slightly better Akaike Information Criterion score. Other models with more complexity (e.g. 2031117 or 3021117) are slightly better, but I'd rather keep the model simple.  \n", "I am tempted to choose model **(1,0,2)x(1,1,1,7)**, which is simpler and with a decent score. Probably in this model the first hyperparamenter AR=1 (instead of AR=2 from our analysis above) is compensated by the fourth hyperparamenter SAR=1 (instead of SAR=0 from our analysis). I will go for this model.   \n", "\n", "Running SARIMAX **(1,0,2)x(1,1,1,7)**: the weekly oscillations are nicely captured but the autocorrelation plots show some structure in the residuals. Notice that in my plots predictions (green time series) are entirely calculated on untrained region of the plot so that I can estimate predictions and true values where they overlap. Training the model on the whole labels would lead to better match in the overlapping region but the model would be prone to overfitting."], "cell_type": "markdown", "metadata": {}}, {"source": ["mod = sm.tsa.statespace.SARIMAX(air.visit_mean[:450], trend='c', order=(1,0,2), seasonal_order=(1,1,1,7))\n", "results = mod.fit()\n", "#Predict on future data and on time periods already known for evaluation with RMSLE\n", "pred = results.predict(start=450, end=516)\n", "print(results.summary())\n", "#Plot\n", "plotSARIMAX(air.visit_mean, pred)"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["Adding 1st order differentiation does not work (you get nan coefficients, not shown)\n", "\n", "### SARIMAX with eXogenous dataset\n", "\n", "Let's introduce holidays in the fit. The holidays column can be assed to the X of SARIMAX as eXogenous regressors. I first run the optimizer on SARIMAX\n"], "cell_type": "markdown", "metadata": {"_uuid": "748730b35a5b5f2bef533293f1ebca31db4a346b", "_cell_guid": "43a98170-54f4-4e92-86ab-ff8edd0cd5b7"}}, {"source": ["resbrute, gr=autoSARIMAX(endog=air.visit_mean, exog=hol.holiday_flg, date_train_end=\"2017-03-26\", pred_days=[-28,39],\\\n", "                    ranges=(slice(1,3),slice(0,2),slice(1,3),  slice(0,2),slice(1,2),slice(1,2),slice(7,8)))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ddec0eabab473fe36ad71e0b93da2db7d793e6bc", "_cell_guid": "54c804d1-0851-4be8-bfad-b9585c1ccaab"}, "outputs": []}, {"source": ["The results for the optimization using holidays as exogenous regressors shows a very similar behavior as before. I stick to model (2,0,2)x(1,1,1,7).  \n", "Now I fit that model. In the plot of the time series notice that the prediction is already much better. I don't show that but if you start the prediction earlier than shown below it would capture the peak on new year's eve. Our forecast now predicts a peak on the weekend of the golden week in April 2017, which is very plausible"], "cell_type": "markdown", "metadata": {"_uuid": "f855167ccfccaab3327206d8e850eddf361a0504", "_cell_guid": "3dc968f3-d120-4b5a-9e9a-304e16eb95d9"}}, {"source": ["modx = sm.tsa.statespace.SARIMAX(air.visit_mean[:450], trend='c', exog=hol.holiday_flg[:450],\\\n", "                                 order=(1,0,2), seasonal_order=(1,1,1,7))\n", "resultsx = modx.fit(start_params=0.1*np.ones( len(modx.param_terms)-2+ 2*2 ))\n", "#Predict on future data and on time periods already known for evaluation with RMSLE\n", "predx = resultsx.predict(start=450, end=516, exog=pd.DataFrame(hol.holiday_flg[450:]))\n", "print(resultsx.summary())\n", "plotSARIMAX(air.visit_mean, predx)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0f9f80a05dbe154ec522a11a7ea9f768272ee2b0", "_cell_guid": "c0f8ac98-cb66-40d5-a171-541c382913f9"}, "outputs": []}, {"source": ["resbrute, gr = autoSARIMAX(endog=air.visit_mean, exog=hol, date_train_end=\"2017-03-26\", pred_days=[-28,39],\\\n", "                    ranges=(slice(1,3),slice(0,2),slice(1,3),  slice(0,2),slice(1,2),slice(1,2),slice(7,8)))"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["From the plot of auto-SARIMAX above notice that the relative did not change much. The (1, 0, 2)x(1, 2, 1, 7) model used so far shows a dip and only more complex models get lower than that. \n", "The plot below shows that adding the day of the week column as exogenous regressors does not change the error and quality of the fit so much. Maybe that is because we already have a season of 7 days.  \n", "The autocorrelation of residuals has not improved and shows some structure."], "cell_type": "markdown", "metadata": {}}, {"source": ["modx2 = sm.tsa.statespace.SARIMAX(air.visit_mean[:450], trend='c', exog=hol[:450], order=(1,0,2), seasonal_order=(1,1,1,7))\n", "resultsx2 = modx2.fit(start_params=0.1*np.ones( len(modx2.params_complete)))\n", "#Predict on future data and on time periods already known for evaluation with RMSLE\n", "predx2 = resultsx2.predict(start=450, end=516, exog=hol[450:])\n", "print(resultsx2.summary())\n", "plotSARIMAX(air.visit_mean, predx2)"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["**Todo**\n", "\n", "* I have the feeling that including the number of open restaurants/reservations (or similar) will certainly be necessary when moving to the visitor data without mean, and it might help as exogenous model will improve modeling certain periods\n", "* I was expecting more reservations in the summer, but maybe Japanese people go on vacation? I thought they had to work all the time\n", "* Repeat the analysis after aggregating restaurants by regions and cuisine. How much will the models differ?\n", "* Better the ACF of residuals\n", "* Better understand Ljung-Box test: shouldn't there be a critical value? How to interpret Prob(Q)?\n"], "cell_type": "markdown", "metadata": {"_uuid": "6aafa02f08e3e3c372767d83af4870fe0a90b956", "_cell_guid": "73dd6c58-de3f-4a20-a433-fea86b1c2366"}}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}