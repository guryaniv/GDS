{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### This notebook will provide the ML techniques for sampling imbalanced datasets\nThis dataset is a HIGHLY unbalanced dataset, meaning the target classes are unbalanced\nClass = 0,1\n* 0 = Non-fraud\n* 1 = Fraud"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d49d48d743e80ddf62f7827287ef7ac59248f656"},"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"68166514f63cb8204e08f86f3f17ca6b7927dbbf"},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"726d88d51d116986b3a6fa50dad4265719f672b4"},"cell_type":"code","source":"data.head()\n#31 columns of data. As we can see,all the columns starting wiith 'v' seem to be data extracted from the Principal Components of the original data.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"919884fe185df2263c75e4322dd884be03b7b6a4"},"cell_type":"code","source":"#let's try to learn more about the data.\n#Target variable is the 'Class' variable, where 0 means 'fraudulent' and 1 means 'genuine'\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2f6220c94b893a31171e1b63881b84f52e5f2892"},"cell_type":"code","source":"len(data)\n#A total of 284,807 rows","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cf61a12fe9c3ba07c2731e419dae0d66f7046887"},"cell_type":"code","source":"#let's check the distribution of the target variable 'Class'\nCounter(data['Class'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8c912ee2109aeeceef5216e7952a01f2c32910e"},"cell_type":"markdown","source":"We can see that the data is HIGHLY unbalanced, meaning the data in class variable of 0 is very high compared to the class variable of 1 "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e628c05d4a56d1f7b275dc771f4e5be2e60a62af"},"cell_type":"code","source":"#plot a barplot to see how unbalanced the data looks graphically\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"703d5ae18d6cd244ce9f1b5c47b7e5d12807020e"},"cell_type":"code","source":"count_class = pd.value_counts(data['Class'])\nprint(count_class)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cfd34a8ff79285d8a277edbadab56a845ddcae60"},"cell_type":"code","source":"plt.figure(figsize=(16,10))\ncount_class.plot(kind='bar')\nplt.ylabel('Frequency')\nplt.xlabel('Class')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fcb4f6c65cb2b66222a7cb347eadd112d8f317b6"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b13c4f6db7535fb15e768a6f35663d595615cd9"},"cell_type":"markdown","source":"The columns starting with 'V' are already the Principal Components. So there is nothing much that we can do with these columns.\nNow, the column \"Time\" is the time elapsed between each transaction. Is this really an important field for us? No. It really will not have any significance in detecting whether a transaction is fraudulent or genuine. So let's drop this column.\nThe \"Amount\" column is necessary. So we shall standardise the Amount data, since all the other columns have been obtained through PCA, and PCA ALWAYS works on Standardised data."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6782efef715eb6f745bcea81509d067af1da5bc8"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata['s_Amount']=sc.fit_transform(data['Amount'].reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36a0cbaa245fc5f8f61aaf7a221e730121f3406"},"cell_type":"markdown","source":"Drop the \"Time\" field and the \"Amount\" field. We shall use the new \"s_Amount field\" with scaled data for our modelling"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"290cb7f1b280af853d6fabd333e157e5f0d17c51"},"cell_type":"code","source":"data = data.drop(['Time', 'Amount'], axis=1)\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3058f425826eb50b23348a14dfc4945c989348e"},"cell_type":"markdown","source":"\n\nSplit the data into x and y variables\n"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e8016c50e96ba99f19ffbd0e7a64cde673c909cf"},"cell_type":"code","source":"x = data.loc[:, data.columns!= 'Class']\ny = data.loc[:, data.columns == 'Class']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"373ebed8a8251cf474353f14db32d0616c9e1c5c"},"cell_type":"markdown","source":"Split the data into train and test data"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cdda6af906dd19ec0939f710213b05b8af3eecf3"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state = 62)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12713d73427df2d3f0a84841b154d2c3484594ec"},"cell_type":"markdown","source":"I will attempt to perform 3 methods of sampling to resolve this imbalanced dataset issue\n1. Undersampling \n2. Oversampling\n3. SMOTE - Synthetic Minority Oversampling Technique "},{"metadata":{"_uuid":"82c8fc2c2776e7ab95ac0e85758387e6ba7b0589"},"cell_type":"markdown","source":"### 1. Undersampling\nThe process of reducing the class instances of the MAJORITY class is called Undersampling. I will attempt to undersample the data and give a 50/50 ratio to each of the class's instances.\nFollowing are the steps for undersampling\n1. Find the number of the minority class\n2. Find the indices of the majority class\n3. Find the indices of the minority class\n4. Randomly sample the majority indices with respect to the minority numbers\n5. Concat the minority indices with the indices from step 4\n6. Get the balanced dataframe - This is the final undersampled data\n\n**Disadvantage** is you will lose critical data as you are reducing the instances of the majority class.\n"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"94562615a9ea02159980878a86430a4adc6efa17"},"cell_type":"code","source":"#1. Find the number of the minority class\nnumber_fraud = len(data[data['Class']==1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"40480666e06bab83ec27a23dfca55c0e184e4af6"},"cell_type":"code","source":"number_non_fraud = len(data[data['Class']==0])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7eb060379fa4a950b9dccdf5e95ab7c164881567"},"cell_type":"code","source":"print(number_fraud)\nprint(number_non_fraud)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5272f4851ecc8147801d2963e5a87c54185da2a6"},"cell_type":"code","source":"#2. Find the indices of the majority class\nindex_non_fraud = data[data['Class']==0].index","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5f9351fb9ea0f1052d1c56aac94e0af0441183a7"},"cell_type":"code","source":"#.3 Find the indices of the minority class\nindex_fraud = data[data['Class']==1].index","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"488671da85fd8a536deea88a2836e04e7deb1387"},"cell_type":"code","source":"#4. Randomly sample the majority indices with respect to the number of minority classes\nrandom_indices = np.random.choice(index_non_fraud, number_fraud,replace='False')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f030b307d2498aadbbb767566ba2e957769ea0af"},"cell_type":"code","source":"len(random_indices)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"19659fe8b61aff781fefe0558e0aaf90ed8389db"},"cell_type":"code","source":"#5. Concat the minority indices with the indices from step 4\nunder_sample_indices = np.concatenate([index_fraud,random_indices])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6f3a0413ce732642720d2f0da69ab32124fff120"},"cell_type":"code","source":"#Get the balanced dataframe - This is the final undersampled data\nunder_sample_df = data.iloc[under_sample_indices]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"49f09a607edea50280a579bff90b98f7488a3b26"},"cell_type":"code","source":"under_sample_df","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0e17a48a9b30fb0c852a6aff4c7df6657925fbd1"},"cell_type":"code","source":"Counter(under_sample_df['Class'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"05e7d6bc080efa9ea04419e9fcbca320619cd5e1"},"cell_type":"code","source":"under_sample_class_counts = pd.value_counts(under_sample_df['Class'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"be15407c21be5ea80fa790eeb8506949542b8d01"},"cell_type":"code","source":"under_sample_class_counts.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7744a7cf3d8920aad5e3c0389320a77676930cb"},"cell_type":"markdown","source":"We can see that the classes are now equally distributed. Now, split the data into x, y, train, and test"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3b3a952d5b2017f7e7054ef7d671e127ffc6a3d3"},"cell_type":"code","source":"x_under = under_sample_df.loc[:, under_sample_df.columns!='Class']\ny_under = under_sample_df.loc[:, under_sample_df.columns=='Class']\nx_under.columns\ny_under.columns","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e88e8c36ea1ee2b709070dcc6ec23bd22c5094d3"},"cell_type":"code","source":"x_under_train, x_under_test, y_under_train, y_under_test = train_test_split(x_under, y_under, test_size=0.25, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"d9bf7cf43c10091069e53c1941dfc50eebb2d7b2"},"cell_type":"code","source":"x_under_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"collapsed":true,"trusted":true,"_uuid":"de0477d427cf0a7c0f84a6f33c8d6e8930474447"},"cell_type":"code","source":"y_under_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c29e84e168cc106f8043bca4fa97f8d86e0581e9"},"cell_type":"markdown","source":"Run a Logistic Regression Classifer"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a11e54b9c5e64d9146c889750b4088bd383a9dd5"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr_under = LogisticRegression()\nlr_under.fit(x_under_train, y_under_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"de80d7b612d6ec1b05af678d9f2729374d2a12e7"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score\nlr_under_predict = lr_under.predict(x_under_test)\nlr_under_accuracy = accuracy_score(lr_under_predict, y_under_test)\nlr_recall = recall_score(lr_under_predict, y_under_test)\nprint(lr_under_accuracy)\nprint(lr_recall)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7a88823947089f820325d120be181820df746fd"},"cell_type":"markdown","source":"We can see that the recall is 98%, which is a great number. We can say that our model is correctly classifying data as 'fraudulent' with 98% accuracy.\nHowever, we see that accuracy is lesser than recall. This is normal, as we have undersampled our data."},{"metadata":{"_uuid":"3af27c2beae4072dab6bf6fa1dfcd691a9598f02"},"cell_type":"markdown","source":"### 2. Oversampling\nThe process of increasing the class instances of the MINORITY class is called Oversampling. \n**Disadvantage** is it will cause OVERFITTING as we are increasing the samples of the minority class.."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6b53ec07facf7e0c00cef0234f229fabc997c739"},"cell_type":"code","source":"fraud_sample = data[data['Class']==1].sample(number_non_fraud, replace=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4effb381396960db97beccb404a3b1df13f122b4"},"cell_type":"code","source":"#create a new dataframe containing only non-fraud data\ndf_fraud = data[data['Class']==0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"83f6a6715ea4c90f1546d98557bede057486af10"},"cell_type":"code","source":"over_sample_df = pd.concat([fraud_sample,df_fraud], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c78264d33e7c47fe6285ee9526da4ef399bed8d4"},"cell_type":"code","source":"over_sample_class_counts=pd.value_counts(over_sample_df['Class'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e4f039ab7b9f54c84b80631abc2395c0a4917e20"},"cell_type":"code","source":"over_sample_class_counts.plot(kind='bar')\nplt.xlabel = 'Class'\nplt.ylabel = 'Frequency'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f0045014e1d15edab7f0ac42e7305613aed57f"},"cell_type":"markdown","source":"We can now see that through oversampling, the counts of both the classes in the data set are equal.\nNow, we model using Logistic Regression.\nSplit the data into x,y, train, and test"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3680e7b8748dc608ba4d27c32b1b9de5c4ca7d55"},"cell_type":"code","source":"x_over = data.loc[:,over_sample_df.columns!='Class']\ny_over = data.loc[:,over_sample_df.columns=='Class']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e0fc17d9f6c4fac4e981b75b0aab6528876148bf"},"cell_type":"code","source":"x_over_train, x_over_test, y_over_train, y_over_test = train_test_split(x_over, y_over, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"03a673b87ae6495f0f18bc430e690329a3f28e22"},"cell_type":"code","source":"lr_over = LogisticRegression()\nlr_over.fit(x_over_train,y_over_train)\nlr_over_predict=lr_over.predict(x_over_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8ef1175cc683565fb2885acac174203d950c33f3"},"cell_type":"code","source":"lr_over_accuracy = accuracy_score(lr_over_predict, y_over_test)\nlr_over_recall = recall_score(lr_over_predict, y_over_test)\nprint(lr_over_accuracy)\nprint(lr_over_recall)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ed87e5ccc2e9e3ded036a58f8c5398f1e08876"},"cell_type":"markdown","source":"We can see that the accuracy is VERY high, but the recall is very low compared to what we saw for the undersampling recall. This is because oversampling causes OVERFITTING, as the data is multiplicated."},{"metadata":{"_uuid":"fc070328354933cdf945dc8621d44f0e95ff8140"},"cell_type":"markdown","source":"### 3. SMOTE - Syntetic Minority Over Sampling Technique\nThe right way to work on imbalanced data and SMOTE is to oversample only on the training data, and leave the test data unseen\n1. Split the training data further into train and validation data\n 1. original test data = x_test, y_test\n 2. original train_data = x_train, y_train\n2. I will further split x_train, y_train to x_val, y_val, x_train_new, y_train_new\n3. I will build the models on x_val and y_val, and check the model for performance on x_train_new, y_train_new\n4. Finally I will check the performace of the model on the unseen x_test, y_test"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1d24188db47979e4d951577e713c109830f90124"},"cell_type":"code","source":"import imblearn\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"af583c687be5dcc9fb0ea2b448018889148e3802"},"cell_type":"code","source":"x_val, x_train_new, y_val,y_train_new = train_test_split(x_train, y_train, test_size = 0.25, random_state=12)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"9c07214cda84b162da8f4345e0c66a74d2415de6"},"cell_type":"code","source":"sm = SMOTE()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"9ac1af7327767eb160c6786f7e833ffbe6b564ec"},"cell_type":"code","source":"x_train_res, y_train_res = sm.fit_sample(x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3b7034551f8f0f98b12085e1eb05ed02ff36238"},"cell_type":"markdown","source":"Here SMOTE.fit_sample gives me the resampled data i.e the oversampled data"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"24b8a50c6d7abcf10b4b517c94a73d5c5f988a02"},"cell_type":"code","source":"x_train_res, y_train_res = sm.fit_sample(x_val, y_val)\nCounter(y_train_res)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43203a0c7fb0a991db6351df6f9105f86a45a53e"},"cell_type":"markdown","source":"As seen above, the result of SMOTE gives us equal distribution of the 2 target classes."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fbbc067909da97ab4d457df2b269ce0c0194e7b6"},"cell_type":"code","source":"lr_smote = LogisticRegression()\nlr_smote.fit(x_train_res, y_train_res)\n#predict on the train data\nlr_smote_predict = lr_smote.predict(x_train_new)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b1285f244c1ae7659f25f009aa7554bb33d49242"},"cell_type":"code","source":"#print accuracy and recall on train data\nprint(accuracy_score(lr_smote_predict,y_train_new))\nprint(recall_score(lr_smote_predict,y_train_new))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"47bd3a3e3d99be8e95a25cb54e3406927b66dfad"},"cell_type":"code","source":"#predict on the test data\nlr_smote_predict_test = lr_smote.predict(x_test)\nprint(accuracy_score(lr_smote_predict_test,y_test))\nprint(recall_score(lr_smote_predict_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"798456b20ac8a0d4638af82724dddf5b2411ebb7"},"cell_type":"markdown","source":"We can see that the recall score for both the train data and the 'unseen' test data are almost similar. This means that we have built a good model using SMOTE"},{"metadata":{"_uuid":"25b88496587b59112e431646ed7c8d15f329ceec"},"cell_type":"markdown","source":"Using Random Forest Classifer"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e9c15c3810f89c6d0f8c2cb5c690ba56c3743d72"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_res, y_train_res)\nrf_smote_predict = rf.predict(x_train_new)\nrf_smote_predict_test = rf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bffd491a932368ceaf6861d33d9cc7eb7161ad7"},"cell_type":"markdown","source":"Check accuracy and recall on the train data"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"290fcdf0618c6f4b122ed04c74c0cdf308851aa9"},"cell_type":"code","source":"print(accuracy_score(rf_smote_predict,y_train_new))\nprint(recall_score(rf_smote_predict,y_train_new))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbae234ee30b7423392f93d4275a60d32251346b"},"cell_type":"markdown","source":"Check accuracy and recall on the unseen test data"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3a5a3082082a1fa8d2acd8f16ae336024b6fb8eb"},"cell_type":"code","source":"print(accuracy_score(rf_smote_predict_test,y_test))\nprint(recall_score(rf_smote_predict_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aba2a871f1c1c52e6411100203d34b140c823b34"},"cell_type":"markdown","source":"We can see that Random Forest performs MUCH BETTER in predicting the frauds in our dataset with the accuracy being 85% on the test data. Also, since the recall on the unseen test data is close to the recall of the train data, this model would perform well un production (as in prod, our model will be predicting on unseen test data)'"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1105703ef61e6784101651552d74da1ee5d9e6af"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}