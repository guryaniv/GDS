{"cells":[{"metadata":{"_uuid":"949662322d293a53825a79873fc6de86d345437b"},"cell_type":"markdown","source":"# Costa Rican Household Poverty Level Prediction\n\nTutorial : EASY EDA + Baseline Modeling for starters \n\n## *This is a work in progress.*\n  \n   \n### Contents \n0. Introduction  \n    0.0 Goal  \n    0.1 Core Data description from Kaggle  \n    0.2 reference  \n1. Load libraries and dataset\n2. An overview of dataset  \n    2.1 Glimpse of train and test dataset  \n    2.2 Summary statistics  \n    2.3 Check for missing values  \n    2.4 Target variables  \n3. Data exploration  \n    3.1 Visualization by datatype  \n    3.2 Household  \n    3.3 \n    \n---\n## 0. Introduction \n---\n\n### 0.0 Goal\nThe goal is to predict the poverty level of households.\n\n### 0.1  Core Data description from Kaggle\n  \n**Id** - a unique identifier for each row.  \n**Target** - the target is an **ordinal** variable indicating groups of income levels.   \n1 = extreme poverty  \n2 = moderate poverty   \n3 = vulnerable households  \n4 = non vulnerable households  \n  \n**idhogar** - this is a unique identifier for each household. This can be used to create household-wide features, etc.  All rows in a given household will have a matching value for this identifier.  \n**parentesco1** - indicates if this person is the head of the household.  \n\n### 0.2 reference\nI referred to several fabulous kernels. Thanks for the authors!  \nhttps://www.kaggle.com/shivamb/costa-rica-poverty-exploration-baseline-model\nhttps://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough\n\n"},{"metadata":{"_uuid":"f65fa57f1e1d7bd787091de94eaf0ae5b0a3e64b"},"cell_type":"markdown","source":"---\n## 1. Load libraries & dataset\n---\n\n\nLoad libraries and dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Load libraries\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(font_scale=2.2)\nplt.style.use('seaborn')\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, ShuffleSplit\nfrom sklearn.metrics import f1_score\nimport itertools\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport shap\nfrom tqdm import tqdm\nimport featuretools as ft\nimport time\nfrom datetime import date\nimport random \nimport warnings\nimport operator\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\nimport plotly.figure_factory as ff\n\nimport warnings \nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\n\n#Load dataset\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40dca8f9ec0c8cc9abf41b9509656f9e947b6aeb"},"cell_type":"markdown","source":"---\n## 2. An overview of dataset\n---"},{"metadata":{"_uuid":"0abe2f1397bb17b8bbed49049456452f934c55f7"},"cell_type":"markdown","source":"\n### 2.1 Glimpse of Train and Test Dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"581b2dd9e6357bd5150c54bb8f9a995aa2874592"},"cell_type":"code","source":"print('Train Dataset shape:', df_train.shape)\nprint('Test Dataset shape shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb31c1c9c25842e382040277bbc1b8988c084173"},"cell_type":"markdown","source":" **Train Dataset** : ID + 141 independent variables + Target variable  \n**Test Dataset** : ID + 141 independent variables"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"45edffbd07b2d5b22317626d6138cf4bd9131681"},"cell_type":"code","source":"print (\"Train Dataset: \")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a5bcb88e3db8d74d2789fd81c3078ae7023d968d"},"cell_type":"code","source":"print (\"Test Dataset: \")\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d5bcd33af8fb4d9382048b445c760e80e9c3b9e"},"cell_type":"markdown","source":"### 2.2 Summary statistics"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d37792218606b592337e329dd3f356d77edc53bc","_kg_hide-input":true},"cell_type":"code","source":"print (\"Summary Statistics of Train Dataset: \")\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afaafaffc45670d2a6531a94f072b584e66c69cc"},"cell_type":"markdown","source":"### 2.3 Check for missing values\n\nCheck the counts and percent of missing values both."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"125505892b92e6f7d14705621ee8ce46f326d26b"},"cell_type":"code","source":"print(\"Total Training Features with NaN values = \" + str(df_train.columns[df_train.isnull().sum() != 0].size))\nif (df_train.columns[df_train.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(df_train.columns[df_train.isnull().sum() != 0])))\n    df_train[df_train.columns[df_train.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"671b1c5458690c268f0a4f22918ef54421f40ed5"},"cell_type":"markdown","source":"5 columns( `v2a1`, `v18q1`, `rez_esc`, `meaneduc`, and `SQBmeaned`)are having missing values. Let's get the count and percentage of null values."},{"metadata":{"trusted":true,"_uuid":"6d0179cf4ca95e2f0ce0549aad5bfbbb13178ec7","_kg_hide-input":true},"cell_type":"code","source":"print (\"Top Columns having missing values\")\ncount = df_train.isnull().sum().sort_values(ascending = False)\npercent = 100 * (df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending=False)\nmissing_df = pd.concat([count, percent], axis=1, keys=['Count', 'Percent'])\nmissing_df.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da69b71e9eb5a66ebe8eb7c5871a1299645a6f0d","_kg_hide-input":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df_train[['v2a1', 'v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']], color=(0.42, 0.6, 0.4))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a96be74e37556e95897ba82ee1fd7b36aa89cadc"},"cell_type":"markdown","source":"### 2.4 Target variables \n  \nThe target is an ordinal variables representing poverty levels as follows:   \n1 = extreme poverty  \n2 = moderate poverty  \n3 = vulnerable households  \n4 = non vulnerable households\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"212e281e84f86cc70e9c88dfbd0c5b0c146dba46"},"cell_type":"code","source":"# Value counts of target\nprint (\"Value counts of target\")\ndf_train_target_counts = df_train['Target'].value_counts().sort_index()\ndf_train_target_counts","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"69e0ee35a34fb11b79c95f8f63f44c165e6e377f"},"cell_type":"code","source":"# Value counts of target - bar plot\nlevels = [\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"Non vulnerable\"]\ntrace = go.Bar(y=df_train_target_counts, x=levels, marker=dict(color='orange', opacity=0.6))\nlayout = dict(title=\"Household Poverty Levels\", margin=dict(l=200), width=800, height=400)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86ecc28c92504f58fe7dc4e5ce42dfcf2229920f"},"cell_type":"markdown","source":"Next, `idhogar` is a unique identifier for each household and `parentesco1` indicates if this person is the head of the household.  "},{"metadata":{"trusted":true,"_uuid":"1fbba18830bb3685d8a0defe9c0504ec331ef1f5","_kg_hide-input":false},"cell_type":"code","source":"df_train['idhogar'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5561b9f4a43a21f4088f1c079f441a60add97a9b"},"cell_type":"code","source":"df_train['idhogar'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98c7874e73782ace05c641a96b55b83453aa278b"},"cell_type":"code","source":"df_train['parentesco1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fda794baf0034ce941e1bf3dabc1b2d2713f21fe"},"cell_type":"markdown","source":"Consider the subset where the variable `parentesco1` is 1."},{"metadata":{"trusted":true,"_uuid":"7d162205b23425c2c1d0252a43801c9340893fa0","_kg_hide-input":true},"cell_type":"code","source":"# the subset with parentesco1 == 1\nprint (\"Value counts of target\")\ndf_train_head = df_train.loc[(df_train['Target'].notnull()) & (df_train['parentesco1'] == 1), ['Target', 'idhogar']]\n\n# Value counts of target when parentesco1 == 1\ndf_train_target_counts = df_train_head['Target'].value_counts().sort_index()\ndf_train_target_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70838a5045cfdf71e7e7fcc7d8cf29a711e0b844","_kg_hide-input":true},"cell_type":"code","source":"levels = [\"Extereme Poverty\", \"Moderate Poverty\", \"Vulnerable\", \"Non vulnerable\"]\ntrace = go.Bar(y=df_train_target_counts, x=levels, marker=dict(color='orange', opacity=0.6))\nlayout = dict(title=\"Household Poverty Levels\", margin=dict(l=100), width=800, height=400)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c5de4032b532f66b127c6e7d4958f2c38979b6"},"cell_type":"markdown","source":"---\n## 3. Data exploration\n---"},{"metadata":{"_uuid":"b95c77f5b7659e2194082f88c912c6ea1ab237b3"},"cell_type":"markdown","source":"We already know that there are 141 independent variables except `Id` and `Target` in train set and 140 independent variables except `Id` in test set. Let's start the exploration checking the type of variables."},{"metadata":{"trusted":true,"_uuid":"c95d1768315b02635edded92a916974d06719be3"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c605ca6e288eb24007363028e2007f80f8d0a1a4"},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d79ce8812f88ae2b4cc66b33b3bed5f8af8c1f80"},"cell_type":"markdown","source":"There are 130 integer columns, 8 float columns and 5 object columns in train set and 129 integer columns, 8 float columns and 5 object columns in test set. The integer columns would consist of boolean variables and ordinal variables.   \n \n To sum up,\n \n**Train Dataset** : total 143 columns  \n- 130 integer columns  (Target + other columns)\n- 8 float columns  \n- 5 object columns  (Id + other columns)\n  \n**Test Dataset** :  total 142 columns   \n- 129 integer column\n- 8 float columns\n- 5 object columns (Id + other columns)\n  \n "},{"metadata":{"_uuid":"f0aad741ae7e476df9c1315cd61c217e594ce17b"},"cell_type":"markdown","source":"Let's start with some overview plots for each data type to explore the data set. \n- Integer columns  \n- Float columns  \n- Object columns  \n  \n### 3.1 Integer Columns"},{"metadata":{"trusted":true,"_uuid":"f2f3b28f2a82b115488997847e05a0c513617e92"},"cell_type":"code","source":"# Count of Unique Values in Integer Columns\ndf_train_int_count = df_train.select_dtypes(np.int64).nunique().value_counts().sort_index()\ndf_train_int_count\n\ntrace = go.Bar(y=df_train_int_count, marker=dict(color='blue', opacity=0.8))\nlayout = dict(title=\"Count of Unique Values in Integer Columns\", margin=dict(l=100), width=800, height=400,\n              xaxis=dict(title='Number of Unique Values'), yaxis=dict(title=\"Count\")\n             )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84793fe0a882480f329efbeea3aafe955f09e4a1"},"cell_type":"markdown","source":"### 3.2 Float Columns"},{"metadata":{"_uuid":"70d8b314fb21dbd3e95aa42ed9e25c48e939f87e"},"cell_type":"markdown","source":"Floats columns represent continuous variables. We can create distribution plots to see if there is a significant difference in the variables depending on the household poverty level."},{"metadata":{"trusted":true,"_uuid":"4069060fd1428dc27077350a1168f2ccf80c0b3b"},"cell_type":"code","source":"# distributions of the float columns by the target \nfrom collections import OrderedDict # fix the keys and values in the same order\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# Color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'purple', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'Extreme', 2: 'Moderate', 3: 'Vulnerable', 4: 'Non Vulnerable'})\n\n# Iterate through the float columns\nfor i, col in enumerate(df_train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(df_train.loc[df_train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1a4aaf9e8721faa838bc0da24ca59615b0ce13"},"cell_type":"markdown","source":"We can guess the relationship between the variables and the Target on the graph. For example, the `meaneduc` which represents the average education of the adults in the household appears to be related to the Target: poverty level. This graph shows that a higher average adult education leads to higher values of the target which are less severe levels of poverty. "},{"metadata":{"trusted":true,"_uuid":"2e8095af39d143a0997d2ae8b514317fbb7549fa"},"cell_type":"markdown","source":"### 3.3 Object Columns"},{"metadata":{"trusted":true,"_uuid":"2dc0db56a766ecb0a176dca3511d2d5ca33cac55"},"cell_type":"code","source":"df_train.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d58eaebb2cadf940ec7bf833021fe78273a97eb"},"cell_type":"markdown","source":"The `Id` and `idhogar` columns are identifying variables. However others seem to be mixed columns of strings and numbers. We need to preprocess them before applying any machine learning techniques.  \n\nAccording to the data description from Kaggle, \n\n- dependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n- edjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n- edjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nThese explanations clear up the issue. For these three variables, \"yes\" = 1 and \"no\" = 0. We will correct the variables using a mapping and convert to floats."},{"metadata":{"trusted":true,"_uuid":"18e601defeb98d3b096768f61fee2a4c709fd28c"},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Apply same operation to both train and test\nfor df in [df_train, df_test]:\n    # Fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ndf_train[['dependency', 'edjefa', 'edjefe']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9a41df35174ac4a21a202a2857fd7a32a39c820"},"cell_type":"code","source":"plt.figure(figsize = (16, 12))\n\n# Iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    # Iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # Plot each poverty level as a separate line\n        sns.kdeplot(df_train.loc[df_train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b83c463d30ddfea61bf50a2ed828d29637e79e4c"},"cell_type":"markdown","source":"To make operations like that above a little easier, we'll join together the training and testing dataframes. This is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. Later we can separate out the sets based on the Target."},{"metadata":{"trusted":true,"_uuid":"b8dce9762b2584831b8fc073faaecce6032b5501"},"cell_type":"code","source":"# Add null Target column to test\ndf_test['Target'] = np.nan\ndata = df_train.append(df_test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b9dfccf9712d16385fda2b1f0a75024ff0e8b70"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}