{"cells":[{"metadata":{"_uuid":"c539a12d887303c45a2c70e66e6e03797a62a428"},"cell_type":"markdown","source":"This is my first competition but is an area that I'm very intersted in, using quantitative methods to solve social problems. I learned a lot by seeing how other people handled some of the challenges this dataset provided. Especially https://www.kaggle.com/skooch/lgbm-with-random-split . I also got some feature engineering ideas from https://www.kaggle.com/gaxxxx/exploratory-data-analysis-lightgbm .\n\nAs has ben mentioned in other kernels one of the problems with working with the data is the imbalance of the classes. There are different ways to account for this, including using the LightGBM balance option for class_weight, or using subsampling. Here I adopt the approach of assigning sample weights to the different target groups based on the inverse of their probability of being selected. Finally, you also want to make sure that your training and testing splits account for households and this can be done using Group Split where each idhogar is group.\n\nUltimately, the main problem is there is not enough information to separate the groups. There is too much overlap between the targets. Target 4 is clearly different. Target 1 is somewhat different. But 1,2,3 are very similar. I don't show this here, but examining polar plots of the variables (especially all the dummy coded variables) and you can see that targets 1-3 are very similar in their characteristics. \n\nAlso, the data is hierarchical in nature, individuals nested in households nested in regions. A better result might be achieved using a multilevel mixed effects model to account for the correlation inherent in this data. \n\nYou need to do some feature engineering. Without doing this you won't get a F1 score above .4 I think. Two that are consistantly important in whatever model I tried were age and education. In particular median household age, and then an Education Index for 25 and older derived from the UN's Multidimensional Poverty Index http://hdr.undp.org/en/faq-page. Other education features were pretty important too.\n\nFor the sampling weights, I initially fit separate models for each target as binary variables. For these model I used sample weights based on the individual's target. So target for was weighted the highest because it had the fewest samples. I then took a pseudo propensity score approach and assigned weights to each observation based on the probability of getting the binary model correct. These sample weights were used for the full multiclass model.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\n\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nsns.set()\n\n\n# Any results you write to the current directory are saved as output.\n%matplotlib inline\n\nlabels = {'v2a1':' Monthly rent payment','hacdor':' =1 Overcrowding by bedrooms','rooms':'  number of all rooms in the house','hacapo':' =1 Overcrowding by rooms','v14a':' =1 has toilet in the household','refrig':' =1 if the household has refrigerator','v18q':' owns a tablet','v18q1':' number of tablets household owns','r4h1':' Males younger than 12 years of age','r4h2':' Males 12 years of age and older','r4h3':' Total males in the household','r4m1':' Females younger than 12 years of age','r4m2':' Females 12 years of age and older','r4m3':' Total females in the household','r4t1':' persons younger than 12 years of age','r4t2':' persons 12 years of age and older','r4t3':' Total persons in the household','tamhog':' size of the household','tamviv':' TamViv','escolari':' years of schooling','rez_esc':' Years behind in school','hhsize':' household size','paredblolad':' =1 if predominant material on the outside wall is block or brick','paredzocalo':' \"=1 if predominant material on the outside wall is socket (wood  zinc or absbesto\"','paredpreb':' =1 if predominant material on the outside wall is prefabricated or cement','pareddes':' =1 if predominant material on the outside wall is waste material','paredmad':' =1 if predominant material on the outside wall is wood','paredzinc':' =1 if predominant material on the outside wall is zink','paredfibras':' =1 if predominant material on the outside wall is natural fibers','paredother':' =1 if predominant material on the outside wall is other','pisomoscer':' \"=1 if predominant material on the floor is mosaic   ceramic   terrazo\"','pisocemento':' =1 if predominant material on the floor is cement','pisoother':' =1 if predominant material on the floor is other','pisonatur':' =1 if predominant material on the floor is  natural material','pisonotiene':' =1 if no floor at the household','pisomadera':' =1 if predominant material on the floor is wood','techozinc':' =1 if predominant material on the roof is metal foil or zink','techoentrepiso':' \"=1 if predominant material on the roof is fiber cement   mezzanine \"','techocane':' =1 if predominant material on the roof is natural fibers','techootro':' =1 if predominant material on the roof is other','cielorazo':' =1 if the house has ceiling','abastaguadentro':' =1 if water provision inside the dwelling','abastaguafuera':' =1 if water provision outside the dwelling','abastaguano':' =1 if no water provision','public':' \"=1 electricity from CNFL   ICE   ESPH/JASEC\"','planpri':' =1 electricity from private plant','noelec':' =1 no electricity in the dwelling','coopele':' =1 electricity from cooperative','sanitario1':' =1 no toilet in the dwelling','sanitario2':' =1 toilet connected to sewer or cesspool','sanitario3':' =1 toilet connected to  septic tank','sanitario5':' =1 toilet connected to black hole or letrine','sanitario6':' =1 toilet connected to other system','energcocinar1':' =1 no main source of energy used for cooking (no kitchen)','energcocinar2':' =1 main source of energy used for cooking electricity','energcocinar3':' =1 main source of energy used for cooking gas','energcocinar4':' =1 main source of energy used for cooking wood charcoal','elimbasu1':' =1 if rubbish disposal mainly by tanker truck','elimbasu2':' =1 if rubbish disposal mainly by botan hollow or buried','elimbasu3':' =1 if rubbish disposal mainly by burning','elimbasu4':' =1 if rubbish disposal mainly by throwing in an unoccupied space','elimbasu5':' \"=1 if rubbish disposal mainly by throwing in river   creek or sea\"','elimbasu6':' =1 if rubbish disposal mainly other','epared1':' =1 if walls are bad','epared2':' =1 if walls are regular','epared3':' =1 if walls are good','etecho1':' =1 if roof are bad','etecho2':' =1 if roof are regular','etecho3':' =1 if roof are good','eviv1':' =1 if floor are bad','eviv2':' =1 if floor are regular','eviv3':' =1 if floor are good','dis':' =1 if disable person','male':' =1 if male','female':' =1 if female','estadocivil1':' =1 if less than 10 years old','estadocivil2':' =1 if free or coupled uunion','estadocivil3':' =1 if married','estadocivil4':' =1 if divorced','estadocivil5':' =1 if separated','estadocivil6':' =1 if widow/er','estadocivil7':' =1 if single','parentesco1':' =1 if household head','parentesco2':' =1 if spouse/partner','parentesco3':' =1 if son/doughter','parentesco4':' =1 if stepson/doughter','parentesco5':' =1 if son/doughter in law','parentesco6':' =1 if grandson/doughter','parentesco7':' =1 if mother/father','parentesco8':' =1 if father/mother in law','parentesco9':' =1 if brother/sister','parentesco10':' =1 if brother/sister in law','parentesco11':' =1 if other family member','parentesco12':' =1 if other non family member','idhogar':' Household level identifier','hogar_nin':' Number of children 0 to 19 in household','hogar_adul':' Number of adults in household','hogar_mayor':' # of individuals 65+ in the household','hogar_total':' # of total individuals in the household','dependency':' Dependency rate','edjefe':' years of education of male head of household','edjefa':' years of education of female head of household','meaneduc':'average years of education for adults (18+)','instlevel1':' =1 no level of education','instlevel2':' =1 incomplete primary','instlevel3':' =1 complete primary','instlevel4':' =1 incomplete academic secondary level','instlevel5':' =1 complete academic secondary level','instlevel6':' =1 incomplete technical secondary level','instlevel7':' =1 complete technical secondary level','instlevel8':' =1 undergraduate and higher education','instlevel9':' =1 postgraduate higher education','bedrooms':' number of bedrooms','overcrowding':' # persons per room','tipovivi1':' =1 own and fully paid house','tipovivi2':' \"=1 own   paying in installments\"','tipovivi3':' =1 rented','tipovivi4':' =1 precarious','tipovivi5':' \"=1 other(assigned   borrowed)\"','computer':' =1 if the household has notebook or desktop computer','television':' =1 if the household has TV','mobilephone':' =1 if mobile phone','qmobilephone':' # of mobile phones','lugar1':' =1 region Central','lugar2':' =1 region Chorotega','lugar3':' =1 region PacÃ­fico central','lugar4':' =1 region Brunca','lugar5':' =1 region Huetar AtlÃ¡ntica','lugar6':' =1 region Huetar Norte','area1':' =1 zona urbana','area2':' =2 zona rural','age':' Age in years','SQBescolari':' escolari squared','SQBage':' age squared','SQBhogar_total':' hogar_total squared','SQBedjefe':' edjefe squared','SQBhogar_nin':' hogar_nin squared','SQBovercrowding':' overcrowding squared','SQBdependency':' dependency squared','SQBmeaned':' meaned squared','agesq':' Age squared'}\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\n#print(df.head())\n\ndf['Target0']=df['Target']-1\nlocations = ['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']\n\nle = LabelEncoder()\ndf['idhogarnum'] = df[['idhogar']].apply(le.fit_transform)\ndf_test['idhogarnum'] = df_test[['idhogar']].apply(le.fit_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae1891428a1b141a50eebc99cc4741e5de33b19f"},"cell_type":"code","source":"\ndf = pd.concat([df,pd.get_dummies(df['Target'], prefix='Target')],axis=1)\ndf.head()\ndataframes = [df,df_test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"949f54ef4cff1fbf519334b0c30dd5f97335391a"},"cell_type":"markdown","source":"Calculate the weights for each type of target."},{"metadata":{"trusted":true,"_uuid":"cddc412d7e8eaaa2a3c5801c59ac072f58449377"},"cell_type":"code","source":"\ndf.groupby('Target')['Target'].count()\n#Reciprocal of the probability of selection\nT1p = 755.0/9557.0\nT2p = 1597.0/9557.0\nT3p = 1209.0/9557.0\nT4p = 5996.0/9557.0\n\nRT1p = 1.0/T1p\nRT2p = 1.0/T2p\nRT3p = 1.0/T3p\nRT4p = 1.0/T4p\nprint(RT1p,RT2p,RT3p,RT4p)\n\ndf['targetweight'] = 0.0\ndf.loc[df['Target']==1,'targetweight']=RT1p\ndf.loc[df['Target']==2,'targetweight']=RT2p\ndf.loc[df['Target']==3,'targetweight']=RT3p\ndf.loc[df['Target']==4,'targetweight']=RT4p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f89c7fea5b1609a3e2b9b4c806061d0d4e18ba22"},"cell_type":"markdown","source":"Feature engineering for individual level variables."},{"metadata":{"trusted":true,"_uuid":"aad443a7183bec941aaba7095cc5eb508a2b094a"},"cell_type":"code","source":"individual_variables = ['age','mobilephone','escolari','mobilephone','male','female','dis',\n                        'instlevel1','instlevel2','instlevel3','instlevel4','instlevel5',\n                        'instlevel6','instlevel7','instlevel8','instlevel9','estadocivil1',\n                        'estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6',\n                        'estadocivil7','parentesco1','parentesco2','parentesco3','parentesco4',\n                        'parentesco5','parentesco6','parentesco7','parentesco8','parentesco9',\n                        'parentesco10','parentesco11','parentesco12']\n#Target Group 2 as a baseline\n\n\nfor tdf in dataframes:\n    #education Index for individuals over the age of 25\n    tdf['indeduindex25'] = 0\n    tdf.loc[tdf['age']>=25,'indeduindex25'] = (tdf['escolari'] / 15.0 + (7/18.0))/2.0\n    tdf.loc[tdf['indeduindex25'].isna(),'indeduindex25'] = 0\n    individual_variables.append('indeduindex25')\n    tdf['agecentschool'] = tdf['age']-7\n    #calculate if 6 to 13 year olds are attending school\n    tdf['attendSchool'] = 0\n    tdf['attendSchool'] = np.where(tdf['escolari']>0,1,0)\n    tdf['attendSchool'] = np.where((tdf['age'] >=6) &(tdf['age'] <=13), tdf['attendSchool'],0)\n    individual_variables.append('attendSchool')\n    #attending school but behind for age 6 to 13\n    tdf['behindSchool'] = 0\n    tdf['behindSchool'] = np.where(tdf['agecentschool'] >tdf['escolari'],1,0)\n    tdf['behindSchool'] = np.where((tdf['age'] >=6) &(tdf['age'] <=13), tdf['behindSchool'],0)\n    individual_variables.append('behindSchool')\n    tdf['behindSchool18'] = 0\n    tdf['behindSchool18'] = np.where(tdf['agecentschool'] >tdf['escolari'],1,0)\n    tdf['behindSchool18'] = np.where((tdf['age'] >=6) &(tdf['age'] <=18), tdf['behindSchool18'],0)\n    individual_variables.append('behindSchool18')\n    tdf['portionofhh'] = 0\n    tdf['portionofhh'] = 1.0 / tdf['hhsize']\n    tdf.loc[tdf['parentesco1']==1,'portionofhh']=1.0\n    individual_variables.append('portionofhh')\n#df['sampweights'] = 0.0\n#df.loc[df['parentesco1']==1,'sampweights'] = 1.0\n#df.loc[df['Target0']==0,'sampweights'] = df[df['Target0']==0]['portionofhh']+0.9210003139060374\n#df.loc[df['Target0']==1,'sampweights'] = df[df['Target0']==1]['portionofhh']+0.8328973527257507\n#df.loc[df['Target0']==2,'sampweights'] = df[df['Target0']==2]['portionofhh']+0.8734958669038402\n#df.loc[df['Target0']==3,'sampweights'] = df[df['Target0']==3]['portionofhh']+0.3726064664643717\n    #individual_variables.append('portionofhh')\npd.set_option('display.max_columns', None)\n#df[individual_variables+['agecentschool']]\nindividual_variables = list(set(individual_variables)) #clear out duplicates","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d07cfc23f5455be1be88da31f9f8b99af3ee5c7"},"cell_type":"markdown","source":"Feature engineering for household level variables."},{"metadata":{"trusted":true,"_uuid":"4952d413a0e0539baba8228c1d66eabfd9205cda"},"cell_type":"code","source":"household_variables = ['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6','area1','area2',\n                      'paredblolad','paredzocalo','paredpreb','pareddes','paredmad','paredzinc',\n                       'paredfibras','paredother','pisomoscer','pisocemento','pisoother','pisonatur',\n                       'pisonotiene','pisomadera','techozinc','techoentrepiso','techocane','techootro',\n                       'cielorazo','abastaguadentro','abastaguafuera','abastaguano','public','planpri',\n                       'noelec','coopele','sanitario1','sanitario2','sanitario3','sanitario5','sanitario6',\n                       'energcocinar1','energcocinar2','energcocinar3','energcocinar4','elimbasu1','elimbasu2',\n                       'elimbasu3','elimbasu4','elimbasu5','elimbasu6','epared1','epared2','epared3','etecho1',\n                       'etecho2','etecho3','eviv1','eviv2','eviv3','tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5',\n                      'v18q1','qmobilephone','tamviv','tamhog','r4h1','r4h2','r4h3','r4m1','r4m2','r4m3','bedrooms', 'computer', 'hacapo', 'hacdor', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'r4t1',\n              'r4t2','r4t3', 'refrig', 'rooms', 'television', 'v14a', 'v18q']#\n#household_variables = ['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6','area1','area2',\n                      #'v18q1','qmobilephone','tamviv','tamhog']\nfor tdf in dataframes:\n    #change missing rent values to 0\n    tdf.loc[tdf['v2a1'].isna(),'v2a1'] = 0\n    tdf.loc[tdf['v18q1'].isna(),'v18q1'] = 0\n    tdf.loc[tdf['qmobilephone'].isna(),'qmobilephone'] = 0\n    \n    conditions = [(tdf['v2a1']==0),(tdf['v2a1']!=0)]\n    choices = [0,np.log(tdf['v2a1']+.0001)]\n    tdf['lv2a1']=np.select(conditions, choices)\n    household_variables.append('lv2a1')\n    \n    #Median Household Age\n    tdf['medhhage'] = tdf['age'].groupby(tdf['idhogar']).transform('median')\n    household_variables.append('medhhage')\n    #Create a household education index\n    #Based on UN Education Index for poverty. \n    #Household members 25 years and older mean education based on average 7 years education\n    tdf['eduindex'] = tdf[tdf['age']>=25]['escolari'].groupby(tdf['idhogar']).transform('mean')\n    tdf['eduindex'] = (tdf['eduindex'] / 15.0 + (8.3/18.0))/2.0\n    tdf['eduindex'] = tdf['eduindex'].groupby(tdf['idhogar']).transform('mean')\n    tdf.loc[tdf['eduindex'].isna(),'eduindex'] = 0\n    household_variables.append('eduindex')\n    #At least one household member with 8 years of education\n    tdf['atleast8'] = 0\n    tdf['atleast8'] = tdf[tdf['escolari']>=8]['escolari'].groupby(tdf['idhogar']).transform('count')\n    tdf['atleast8'] = tdf['atleast8'].groupby(tdf['idhogar']).transform('max')\n    tdf['atleast8'] = np.where(tdf['atleast8']>=1, 1, 0)\n    household_variables.append('atleast8')\n    \n    #At least one female household member with 8 years of education\n    tdf['atleast8F'] = 0\n    tdf['atleast8F'] = tdf[(tdf['escolari']>=6)&(tdf['female']>=1)]['escolari'].groupby(tdf['idhogar']).transform('count')\n    tdf['atleast8F'] = tdf['atleast8F'].groupby(tdf['idhogar']).transform('max')\n    tdf['atleast8F'] = np.where(tdf['atleast8F']>=1, 1, 0)\n    household_variables.append('atleast8F')\n    \n    #Number of household members below 14\n    tdf['nhhlt14'] = 0\n    tdf['nhhlt14'] = tdf[tdf['age']<=14]['age'].groupby(tdf['idhogar']).transform('count')\n    tdf['nhhlt14'] = tdf['nhhlt14'].groupby(tdf['idhogar']).transform('max')\n    tdf['nhhlt14'] = np.where(tdf['nhhlt14']>=1, tdf['nhhlt14'], 0)\n    household_variables.append('nhhlt14')\n    \n    #Number of household members below 14\n    tdf['perhhlt14'] = 0.0\n    tdf['perhhlt14'] = tdf['nhhlt14'] / tdf['hhsize']\n    household_variables.append('perhhlt14')\n    \n    tdf['perschoolage'] = 0.0\n    tdf['perschoolage'] = tdf[(tdf['age'] >=6) &(tdf['age'] <=13)].groupby(tdf['idhogar']).transform('count')\n    tdf['perschoolage'] = tdf['perschoolage'].groupby(tdf['idhogar']).transform('max')\n    tdf['perschoolage'] = np.where(tdf['perschoolage']>=1, tdf['perschoolage'], 0)\n    tdf['perschoolage'] = tdf['perschoolage'] / tdf['hhsize']\n    household_variables.append('perschoolage')\n    #parametric household consumption units\n    tdf['cupara'] = tdf['hogar_adul']+(0.5*tdf['nhhlt14'])\n    household_variables.append('cupara')\n    \n    #no household assets\n    tdf['assets2'] = 0\n    tdf.loc[(tdf['v18q']<=1)&(tdf['qmobilephone']<=1)&(tdf['computer']==0)&(tdf['television']==0),'assets2'] = 1 \n    household_variables.append('assets2')\n    \n    #some assets\n    tdf['assets3'] = 0\n    tdf.loc[(tdf['v18q']<=1)&(tdf['qmobilephone']>=1)&(tdf['computer']==0)&(tdf['television']==1),'assets3'] = 1 \n    household_variables.append('assets3')\n    #some assets\n    tdf['assets3b'] = 0\n    tdf.loc[(tdf['v18q']<=1)&(tdf['qmobilephone']>=1)&(tdf['computer']==0)&(tdf['television']==0),'assets3b'] = 1 \n    household_variables.append('assets3b')\n    \n    #a lot of assets\n    tdf['assets4'] = 0\n    tdf.loc[(tdf['v18q']>=1)&(tdf['qmobilephone']>=1)&(tdf['computer']==1)&(tdf['television']==1),'assets4'] = 1 \n    household_variables.append('assets4')\n    \n    #head of household single, divorced, separated, or widowed\n    conditions = [(tdf['parentesco1']==1)&(tdf['estadocivil4']==1),(tdf['parentesco1']==1)&(tdf['estadocivil5']==1),\n                  (tdf['parentesco1']==1)&(tdf['estadocivil6']==1),\n                  (tdf['parentesco1']==1)&(tdf['estadocivil7']==1),(tdf['parentesco1']==0)]\n    choices = [1,1,1,1,0]\n    tdf['headmarital']=np.select(conditions, choices)\n    tdf['headmarital'] = tdf['headmarital'].groupby(tdf['idhogar']).transform('count')\n    tdf['headmarital'] = np.where(tdf['headmarital']>=1, 1, 0)\n    household_variables.append('headmarital')\n    \n    #Ratio household children to Adults\n    conditions = [(tdf['hogar_adul']==0),(tdf['hogar_adul']>0)]\n    choices = [0, tdf['hogar_nin']/tdf['hogar_adul']]\n    tdf['radultchild']=np.select(conditions, choices)\n    household_variables.append('radultchild')\n    \n    #Ratio bedrooms to household size\n    conditions = [(tdf['tamhog']==0),(tdf['tamhog']>0)]\n    choices = [0, tdf['bedrooms']/tdf['tamhog']]\n    tdf['bedperperson']=np.select(conditions, choices)\n    household_variables.append('bedperperson')\n    \n    conditions = [(tdf['rooms']==0),(tdf['rooms']>0)]\n    choices = [0, tdf['bedrooms']/tdf['rooms']]\n    tdf['perbedrooms']=np.select(conditions, choices)\n    household_variables.append('perbedrooms')\n    \n    #ratio rooms to household size\n    conditions = [(tdf['rooms']==0),(tdf['rooms']>0)]\n    choices = [0, tdf['tamhog']/tdf['rooms']]\n    tdf['rperperson']=np.select(conditions, choices)\n    household_variables.append('rperperson')\n    \n    #ratio bedrooms to rooms\n    conditions = [(tdf['rooms']==0),(tdf['rooms']>0)]\n    choices = [0, tdf['bedrooms']/tdf['rooms']]\n    tdf['roomstobedrooms']=np.select(conditions, choices)\n    household_variables.append('roomstobedrooms')\n    \n    #ratio rent to adults\n    conditions = [(tdf['hogar_adul']==0),(tdf['hogar_adul']>0)]\n    choices = [0, tdf['lv2a1']/tdf['hogar_adul']]\n    tdf['rentpadult']=np.select(conditions, choices)\n    household_variables.append('rentpadult')\n    \n    #ratio phones to adults\n    conditions = [(tdf['hogar_adul']==0),(tdf['hogar_adul']>0)]\n    choices = [0, tdf['qmobilephone']/tdf['hogar_adul']]\n    tdf['phoneperadult']=np.select(conditions, choices)\n    household_variables.append('phoneperadult')\n    \n    #ratio phones to hhsize\n    conditions = [(tdf['hhsize']==0),(tdf['hhsize']>0)]\n    choices = [0, tdf['qmobilephone']/tdf['hhsize']]\n    tdf['phoneperhhsize']=np.select(conditions, choices)\n    household_variables.append('phoneperhhsize')\n    \n    #ratio of rent to rooms\n    conditions = [(tdf['rooms']==0),(tdf['rooms']>0)]\n    choices = [0, tdf['lv2a1']/tdf['rooms']]\n    tdf['rentproom']=np.select(conditions, choices)\n    household_variables.append('rentproom')\n    \n    #Dependency in the household\n    conditions = [(tdf['dependency']=='yes'),(tdf['dependency']=='no'),(tdf['dependency']!='yes')&(tdf['dependency']!='no')]\n    choices = [1,0,0]\n    tdf['dphhyesno']=np.select(conditions, choices)\n    household_variables.append('dphhyesno')\n    \n    #Dependency rate\n    conditions = [(tdf['dependency']=='yes'),(tdf['dependency']=='no'),(tdf['dependency']!='yes')&(tdf['dependency']!='no')]\n    choices = [0,0,tdf['dependency']]\n    tdf['dpnumeric']=np.select(conditions, choices)\n    household_variables.append('dpnumeric')\n    \n    #Female head of household\n    tdf['femhead'] = 0\n    tdf.loc[(tdf['parentesco1']==1)&(tdf['female']==1),'femhead'] = 1\n    tdf['femhead'] = tdf['femhead'].groupby(tdf['idhogar']).transform('count')\n    tdf['femhead'] = np.where(tdf['femhead']>=1, 1, 0)\n    household_variables.append('femhead')\n    \n    #Get the age of the head of household and the number of schooling missing values cleaned up later\n    tdf['hhheadeduindex'] = 0.0\n    tdf['hhheadeduindex'] = tdf[tdf['parentesco1']==1]['escolari'].groupby(tdf['idhogar']).transform('mean')\n    tdf['hhheadeduindex'] = (tdf['hhheadeduindex'] / 15.0 + (7.2/18.0))/2.0\n    tdf.loc[tdf['hhheadeduindex'].isna(),'hhheadeduindex'] = 0\n    tdf['hhheadeduindex'] = tdf['hhheadeduindex'].groupby(tdf['idhogar']).transform('mean')\n    tdf.loc[tdf['hhheadeduindex'].isna(),'hhheadeduindex'] = 0\n    household_variables.append('hhheadeduindex')\n\n    tdf['hhheadage'] =0\n    tdf['hhheadage'] = tdf[tdf['parentesco1']==1]['age'].groupby(tdf['idhogar']).transform('mean')\n    tdf['hhheadage'] = tdf['hhheadage'].groupby(tdf['idhogar']).transform('mean')\n    tdf.loc[tdf['hhheadage'].isna(),'hhheadage'] = 0\n    household_variables.append('hhheadage')\n    \n    \n    #missing head of household\n    tdf['misshead'] = 0\n    tdf['misshead'] = tdf['parentesco1'].groupby(tdf['idhogar']).transform('sum')\n    tdf['misshead'] = np.where(tdf['misshead']==0, 1, 0)\n    household_variables.append('misshead')\n    #get the missing value information for male head of household schooling\n    conditions = [(tdf['edjefe']=='yes'),(tdf['edjefe']=='no'),(tdf['edjefe']!='yes')&(tdf['edjefe']!='no')]\n    choices = [0,1,0]\n    tdf['edjefemiss']=np.select(conditions, choices)\n    household_variables.append('edjefemiss')\n    \n    conditions = [(tdf['edjefe']=='yes'),(tdf['edjefe']=='no'),(tdf['edjefe']!='yes')&(tdf['edjefe']!='no')]\n    choices = [0,0,tdf['edjefe']]\n    tdf['edjefeval']=np.select(conditions, choices) \n    household_variables.append('edjefeval')\n    \n    #get the missing value information for male head of household schooling\n    conditions = [(tdf['edjefa']=='yes'),(tdf['edjefa']=='no'),(tdf['edjefa']!='yes')&(tdf['edjefa']!='no')]\n    choices = [0,1,0]\n    tdf['edjefamiss']=np.select(conditions, choices)\n    household_variables.append('edjefamiss')\n    \n    conditions = [(tdf['edjefa']=='yes'),(tdf['edjefa']=='no'),(tdf['edjefa']!='yes')&(tdf['edjefa']!='no')]\n    choices = [0,0,tdf['edjefa']]\n    tdf['edjefaval']=np.select(conditions, choices) \n    household_variables.append('edjefaval')\n    \n    tdf['region'] = tdf['region'] = tdf[locations].idxmax(axis = 1)\n    \n    #credit for thse variables = https://www.kaggle.com/gaxxxx/exploratory-data-analysis-lightgbm\n    tdf['r4h1permale'] = tdf['r4h1'] / tdf['r4h3']\n    tdf.loc[tdf['r4h1permale'].isna(),'r4h1permale'] =0\n    household_variables.append('r4h1permale')\n    tdf['r4m1perfemale'] = tdf['r4m1'] / tdf['r4m3']\n    tdf.loc[tdf['r4m1perfemale'].isna(),'r4m1perfemale'] =0\n    household_variables.append('r4m1perfemale')\n    tdf['r4h1pertotal'] = tdf['r4h1'] / tdf['hhsize']\n    household_variables.append('r4h1pertotal')\n    tdf['r4m1pertotal'] = tdf['r4m1'] / tdf['hhsize']\n    household_variables.append('r4m1pertotal')\n    tdf['r4t1pertotal'] = tdf['r4t1'] / tdf['hhsize']\n    household_variables.append('r4t1pertotal')\n\n    \n    \n    #Assumed access to clean water\n    tdf['cleanwater'] = 0\n    tdf.loc[(tdf['abastaguafuera']==1)|(tdf['abastaguano']==1),'cleanwater'] = 1 \n    household_variables.append('cleanwater')\n    \n    #Assumed access to improved sanitation\n    tdf['improvedsan'] = 0\n    tdf.loc[(tdf['sanitario2']==1)|(tdf['sanitario3']==1)|(tdf['sanitario5']==1),'improvedsan'] = 1 \n    household_variables.append('improvedsan')\n    \n    #Assumed access to consistent electricity\n    tdf['electricity'] = 0\n    tdf.loc[(tdf['public']==1)|(tdf['planpri']==1)|(tdf['coopele']==1),'electricity'] = 1 \n    household_variables.append('electricity')\n    \n    #Combined flooring in household\n    tdf['flooring'] = 0\n    tdf.loc[(tdf['pisoother']==1)|(tdf['pisonatur']==1)|(tdf['pisonotiene']==1)|(tdf['pisomadera']==1),'flooring'] = 1 \n    household_variables.append('flooring')\n    \n    #Combined wall in household\n    tdf['wall'] = 0\n    tdf.loc[(tdf['paredzocalo']==1)|(tdf['pareddes']==1)|(tdf['paredfibras']==1)|(tdf['paredother']==1)|(tdf['paredmad']==1),'wall'] = 1 \n    household_variables.append('wall')\n    \n    #Combined roof in household\n    tdf['roof'] = 0\n    tdf.loc[(tdf['techocane']==1)|(tdf['techootro']==1)|(tdf['cielorazo']==0),'roof'] = 1 \n    household_variables.append('roof')\n    \n    #Cooking material combined\n    tdf['cooking'] = 0\n    tdf.loc[(tdf['energcocinar4']==1)|(tdf['energcocinar1']==1),'cooking'] = 1 \n    household_variables.append('cooking')\n    \n    tdf['rubbish'] = 0\n    tdf.loc[(tdf['elimbasu3']==1)|(tdf['elimbasu4']==1)|(tdf['elimbasu5']==1)|(tdf['elimbasu6']==1),'rubbish'] = 1 \n    household_variables.append('rubbish')\n    \n    tdf['condition'] = 0\n    tdf.loc[(tdf['epared1']==1)&(tdf['etecho1']==1)&(tdf['eviv1']==1),'condition'] = 1 \n    household_variables.append('condition')\n    \n    tdf['condition2'] = 0\n    tdf.loc[(tdf['epared2']==1)&(tdf['etecho2']==1)&(tdf['eviv2']==1),'condition2'] = 1 \n    household_variables.append('condition2')\n    \n    tdf['condition3'] = 0\n    tdf.loc[(tdf['epared3']==1)&(tdf['etecho3']==1)&(tdf['eviv3']==1),'condition3'] = 1 \n    household_variables.append('condition3')\n    \nhousehold_variables = list(set(household_variables)) #clear out duplicates\n#df[['behindSchool','perhhlt14','perschoolage','perbedrooms','assets3','assets4','assets3b','phoneperhhsize','phoneperadult']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2efb35fb88032ecfb46547576095fe058cf46522"},"cell_type":"markdown","source":"Feature engineering for regional level variables. Test and Train datasets were combined to calculate these."},{"metadata":{"trusted":true,"_uuid":"01aa33fc65fbdfbaa418ee2f9391cf2962e217ca"},"cell_type":"code","source":"#Create regional variables from all the data\ndf_regional = df.copy()\ndf_test_regional = df_test.copy()\ndf_test_regional[\"Target\"]=0\ndf_test_regional[\"Target0\"]=0\ndf_all = pd.concat([df_regional,df_test_regional],sort=False)\n#reindex to avoid problems\ndf_all.index = range(0,len(df_all['idhogar']))\n\n\nregional_variables = []\n\ndf_all['regionalMeanLogRent'] = 0.0\n\ndf_all['regionalMeanLogRent'] = df_all[df_all['lv2a1']>0][['lv2a1','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\ndf_all.loc[df_all['regionalMeanLogRent'].isna(),'regionalMeanLogRent'] = 0\nregional_variables.append('regionalMeanLogRent')\n\n#Median Age for the region\ndf_all['rmedianage'] = df_all[['age','idhogar','region']].groupby(['region']).transform('median')\nregional_variables.append('rmedianage')\n\n#Mean household education index for the region\ndf_all['reduindex'] = df_all[['eduindex','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\nregional_variables.append('reduindex')\n\n#print(pd.pivot_table(df_all,values=['reduindex'],index=['region']))\ndf_all['rnhhlt14'] = df_all[['nhhlt14','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\nregional_variables.append('rnhhlt14')\n\n#mean household size for the region\n#print(pd.pivot_table(df_all,values=['rhogar_adul'],index=['region']))\ndf_all['rhogar_total'] = df_all[['hogar_total','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\nregional_variables.append('rhogar_total')\n\n#mean number of bedrooms for the region\n#print(pd.pivot_table(df_all,values=['rhogar_total'],index=['region']))\ndf_all['rbedrooms'] = df_all[['bedrooms','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\nregional_variables.append('rbedrooms')\n#print(pd.pivot_table(df_all,values=['rbedrooms'],index=['region']))\n\n#percent of urban households for the region\ndf_all['urbanhouseholds']=df_all[['area1','idhogar','region']].groupby(['idhogar','region']).transform('mean').groupby(df_all['region']).transform('mean')\nregional_variables.append('urbanhouseholds')\n\nfor fld in regional_variables:\n    tempPiv = pd.pivot_table(df_all, values=fld, index=['region'], aggfunc=np.mean)\n    tempPiv.columns = [fld]\n    df = df.merge(tempPiv,on='region',how='left')\n    del tempPiv\nfor fld in regional_variables:\n    tempPiv = pd.pivot_table(df_all, values=fld, index=['region'], aggfunc=np.mean)\n    tempPiv.columns = [fld]\n    df_test = df_test.merge(tempPiv,on='region',how='left')\n    del tempPiv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e01134de89484ab6d97eda15972ffef6cb483e"},"cell_type":"markdown","source":"Check for any final missing or NAN values."},{"metadata":{"trusted":true,"_uuid":"6933fc29e29782b1e4eef265e1490abdc7ffc2e3"},"cell_type":"code","source":"for tdf in dataframes:\n    print (tdf.columns[tdf.isna().any()].tolist())\n    for f in tdf.columns[tdf.isna().any()].tolist():\n        if f in regional_variables:\n            print(f)\n    print (tdf.columns[tdf.isnull().any()].tolist())\n    for f in tdf.columns[tdf.isnull().any()].tolist():\n        if f in regional_variables:\n            print(f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91c423165555512b15bd1a5519301c48717b0616"},"cell_type":"markdown","source":"Develop the model using a gradient boosted classifier."},{"metadata":{"trusted":true,"_uuid":"bbf652dc3dd088a4b2a0072aed56adb5b25bc681"},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import metrics \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom  sklearn.model_selection import KFold\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport lightgbm as lgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\n#from mlxtend.classifier import EnsembleVoteClassifier\n\n\n#column ranges\n#column ranges\ncombo = individual_variables+household_variables+regional_variables\ncombo = list(set(combo))\nindex_individual_variables = list(range(0,len(individual_variables)))\nindex_household_variables = list(range(len(individual_variables),len(individual_variables)+len(household_variables)))\nindex_indiv_and_hh_variables = index_individual_variables+index_household_variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aaecd89096ce97c78c75ba649f24cbcafe4d0b1"},"cell_type":"markdown","source":"This iterates through each target to create a separate model. The weights are derived from these models. The sampling uses GroupShuffleSplit to maintain houshold groups."},{"metadata":{"trusted":true,"_uuid":"29dce21c39e6fda617cf7b4283154a4319095fcd"},"cell_type":"code","source":"keeplst = []\nindvmodels = {}\nfor t in [1,2,3,4]:\n    params = {'n_estimators': 500,\n                  'max_depth':None,\n                  'min_samples_split':2,\n                  'min_samples_leaf':90,\n                  'max_features':'auto',\n                  'loss':'deviance',\n                 \"learning_rate\":.01,\n                 \"subsample\":.5}\n\n    currentTarget = \"Target_\"+str(t)\n    print(currentTarget)\n    currentTarget_v = t\n    df_sub = df[df['parentesco1']==1][combo+[currentTarget,\"idhogarnum\",'targetweight']].copy()\n    X = df_sub[combo].values\n    y = df_sub[currentTarget].values.ravel()\n    weights = df_sub['targetweight'].values.ravel()\n    Y_strat = df_sub['idhogarnum'].values\n    gss = GroupShuffleSplit(test_size=.1, n_splits=1,random_state=314)\n    for train_index, test_index in gss.split(X, y, groups=Y_strat):\n        break\n    #print(train_index)\n    #print(test_index)\n    X_train, X_test = X[train_index],X[test_index]\n    y_train, y_test = y[train_index],y[test_index]\n    #w_train = weights[train_index]\n    w_train = weights[train_index]\n    strat_train = Y_strat[train_index]\n    print(X_train.shape)\n    print(X_test.shape)\n    reg = GradientBoostingClassifier(**params)\n    reg.fit(X_train,y_train,sample_weight=w_train)\n    indvmodels[t] = reg\n    y_pred = reg.predict(X_test)\n    print(f1_score(y_test, y_pred, average='macro'))\n    print(classification_report(y_test, y_pred))\n    fi = list(zip(range(0,len(reg.feature_importances_)),reg.feature_importances_))\n    fi.sort(key=lambda x: x[1])\n    vals = [x[1] for x in fi]\n    for i,important in fi:\n        if important >=np.percentile(reg.feature_importances_, 75):\n            keeplst.append(combo[i])\n            print(\"%s: %s\"%(combo[i],important))\n        #if important==0:\n            #removelst.append(combo[i])\n\n\n    probabilities = pd.DataFrame(reg.predict_proba(X),columns=[\"P0\",\"P1\"])\n    df[currentTarget+\"_p\"]=0.1\n    \n    df.loc[df['parentesco1']==1,currentTarget+\"_p\"]= probabilities[\"P1\"].values\nprint(list(set(keeplst)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a1aacb7045f1ce0218a91e4e3ef9d0a5f4dd01d"},"cell_type":"markdown","source":"Calculates the weight for each training sample. I also divide the weights by the sum of the weights, but this doesn't seem to make much of a difference."},{"metadata":{"trusted":true,"_uuid":"16ef29e0f91b39d54598cd8d9e2f7dabf8f44520"},"cell_type":"code","source":"df['pscore'] = 0.0\n#wghts={1:0.9210003139060374,2:0.8328973527257507,3:0.8734958669038402,4:0.3726064664643717}\nfor t in [1,2,3,4]:\n    df.loc[df['Target']==t,'pscore'] = df.loc[df['Target']==t][\"Target_\"+str(t)+'_p']\nprint(df[df['Target']==1].head())\ndf['indsampweight']= 100.0*df['pscore']\ndf['targetpscore'] = df['targetweight']*df['pscore']\ndf['targetpscore'] = df['targetpscore'] / df['targetpscore'].sum()\n\nprint(df[df['Target']==1].head())\nfrom sklearn.cluster import KMeans\ny_pred = KMeans(n_clusters=4, random_state=314).fit_predict(df['pscore'].values.reshape(-1, 1))\ndf['strata'] = y_pred\ndf[['Target','pscore','strata']]\ndf[df['strata']==1][['Target','pscore','strata']]\npd.pivot_table(df,values='pscore',index='strata',aggfunc=\"mean\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb4ece236d53cd44a305d1ba752bb511d99c64c4"},"cell_type":"markdown","source":"Final model"},{"metadata":{"trusted":true,"_uuid":"41a461cfe8cfaa42d874b6e76285ebb107f10ad8"},"cell_type":"code","source":"keeplst = list(set(keeplst))\n\n\ndf_sub = df[combo+[\"Target0\",\"idhogarnum\",'strata','indsampweight','targetweight','targetpscore']].copy()\nX = df_sub[combo].values\ny = df_sub[\"Target0\"].values.ravel()\nY_strat = df_sub[\"idhogarnum\"]\nweights_target = df_sub['targetweight'].values.ravel()\nweights_all = df_sub['indsampweight'].values.ravel()\nweights_tp = df_sub['targetpscore'].values.ravel()\ngss = GroupShuffleSplit(test_size=.1, n_splits=1,random_state=314)\ngss = ShuffleSplit(n_splits=1, test_size=.1, random_state=314)\n\n#for train_index, test_index in gss.split(X, y,Y_strat):\n    #break\nfor train_index, test_index in gss.split(X, y):\n    break\n\n\nX_train, X_test = X[train_index],X[test_index]\ny_train, y_test = y[train_index],y[test_index]\n\n\n\nwtar_train = weights_target[train_index]\nwall_train = weights_all[train_index]\nwtp_train = weights_tp[train_index]\nstrat_train = Y_strat[train_index]\nprint(X_train.shape)\nprint(X_test.shape)\nparams = {'n_estimators': 500,\n                  'max_depth': None,\n                  'min_samples_split':10,\n                  'min_samples_leaf':90,\n                  'max_features':'auto',\n                  'loss':'deviance',\n                 \"learning_rate\":.01,\n                 \"subsample\":.5}\n\nreg = GradientBoostingClassifier(**params)\nreg.fit(X_train,y_train,sample_weight=wtp_train)\ny_pred = reg.predict(X_test)\nprint(f1_score(y_test, y_pred, average='macro'))\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"418f42d7730c7f09903ef71a402e442a39cc7cc8"},"cell_type":"markdown","source":"Output."},{"metadata":{"trusted":true,"_uuid":"fba0ac39725fa3b6e1e7d5aee4a3c10fcc300671"},"cell_type":"code","source":"X_fin = df_test[combo].values\ndf_test['TargetP'] = reg.predict(X_fin)+1\nprint(df_test.TargetP)\ndf_output = pd.DataFrame({\"Id\":df_test['Id'],\"Target\":df_test['TargetP']})\ndf_output.to_csv(\"outputTargetPropensity.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"147787ab5eea43adefb314eb63800a5468fd1467"},"cell_type":"code","source":"#y_indiv = np.zeros([X_test.shape[0], 4], dtype=float)\n#for t in [1,2,3,4]:\n#    reg = indvmodels[t]\n#    df_prob = pd.DataFrame(reg.predict_proba(X_test),columns=['B0','B1'])\n#    #print(df_prob['B1'].values)\n#    y_indiv[:, t-1] = df_prob['B1'].values\n#    del df_prob\n#y_pred= np.zeros(X_test.shape[0])\n#for i in range(X_test.shape[0]):\n#    y_pred[i]=np.argmax(y_indiv[i])\n\n#print(f1_score(y_test, y_pred, average='macro'))\n#print(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e1c53baad8fbb98b116e53b7d77f91ba130d266"},"cell_type":"markdown","source":"## Prediction using separate Binary models\n\nYou can also use the individual models to predict the class. Which ever class has the highest prediction probability is assigned to that sample. This doesn't perform as well, but I few tries put it at about .420."},{"metadata":{"trusted":true,"_uuid":"9cd568547644d1a50074ab21d6905b96db6654e6"},"cell_type":"code","source":"X_fin = df_test[combo].values\ny_indiv = np.zeros([X_fin.shape[0], 4], dtype=float)\nfor t in [1,2,3,4]:\n    reg = indvmodels[t]\n    df_prob = pd.DataFrame(reg.predict_proba(X_fin),columns=['B0','B1'])\n    #print(df_prob['B1'].values)\n    y_indiv[:, t-1] = df_prob['B1'].values\n    del df_prob\ny_pred= np.zeros(X_fin.shape[0])\nfor i in range(X_fin.shape[0]):\n    y_pred[i]=np.argmax(y_indiv[i])\ny_pred = np.array(y_pred,dtype=int)\n#print(f1_score(y_test, y_pred, average='macro'))\n#print(classification_report(y_test, y_pred))\ndf_test['TargetI'] = y_pred+1\nprint(df_test.TargetI)\ndf_output = pd.DataFrame({\"Id\":df_test['Id'],\"Target\":df_test['TargetI']})\ndf_output.to_csv(\"outputTargetIndiv.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14f8c2e8f2ab40cb94f01cf3f0d306f59bc570d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}