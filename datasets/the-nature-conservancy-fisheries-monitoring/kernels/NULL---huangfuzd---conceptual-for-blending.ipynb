{"cells": [{"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"trusted": false, "_uuid": "4273b730ba51dae871a2ec26fac0d9057331fb19", "_cell_guid": "7f903488-acce-ad66-a3c4-e98d81350180"}, "source": ["import numpy as np\n", "import pandas as pd\n", "np.random.seed(2017)\n", "import os\n", "import glob\n", "import cv2\n", "import datetime\n", "import pandas as pd\n", "import time\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "from sklearn.cross_validation import KFold\n", "from keras.models import Sequential\n", "from keras.layers.core import Dense, Dropout, Flatten\n", "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n", "from keras.callbacks import EarlyStopping\n", "from keras.utils import np_utils\n", "from keras.constraints import maxnorm\n", "from sklearn.metrics import log_loss\n", "from keras import __version__ as keras_version\n", "\n", "from PIL import ImageFilter, ImageStat\n", "from PIL import Image, ImageDraw\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "min_max_s = MinMaxScaler(feature_range=(0, 255), copy=True)\n", "\n", "def im_stats(im_stats_df):\n", "    im_stats_d = {}\n", "    for i in range(len(im_stats_df)):\n", "        im_stats_im_ = Image.open(im_stats_df['path'][i])\n", "        im_stats_d[im_stats_df['path'][i]] = {'Stats': ImageStat.Stat(im_stats_im_), 'Size': im_stats_im_.size}\n", "    im_stats_df['size_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Size'][0])\n", "    im_stats_df['size_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Size'][1])\n", "    im_stats_df['sum_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[0])\n", "    im_stats_df['sum_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[1])\n", "    im_stats_df['sum_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].sum[2])\n", "    im_stats_df['mean_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[0])\n", "    im_stats_df['mean_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[1])\n", "    im_stats_df['mean_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].mean[2])\n", "    im_stats_df['rms_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[0])\n", "    im_stats_df['rms_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[1])\n", "    im_stats_df['rms_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].rms[2])\n", "    im_stats_df['var_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[0])\n", "    im_stats_df['var_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[1])\n", "    im_stats_df['var_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].var[2])\n", "    im_stats_df['stddev_0'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[0])\n", "    im_stats_df['stddev_1'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[1])\n", "    im_stats_df['stddev_2'] = im_stats_df['path'].map(lambda x: im_stats_d[x]['Stats'].stddev[2])\n", "    return im_stats_df\n", "\n", "def get_im_cv2(path):\n", "    img = cv2.imread(path)\n", "    resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n", "    return resized\n", "\n", "def load_train():\n", "    X_train = []\n", "    X_train_id = []\n", "    y_train = []\n", "    start_time = time.time()\n", "    train = []\n", "\n", "    print('Read train images')\n", "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n", "    for fld in folders:\n", "        index = folders.index(fld)\n", "        print('Load folder {} (Index: {})'.format(fld, index))\n", "        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n", "        files = glob.glob(path) #limited\n", "        train += files\n", "        for fl in files:\n", "            flbase = os.path.basename(fl)\n", "            img = get_im_cv2(fl)\n", "            X_train.append(img)\n", "            X_train_id.append(flbase)\n", "            y_train.append(index)\n", "    train = im_stats(pd.DataFrame(train, columns=['path']))\n", "    train = min_max_s.fit_transform(train[[c for c in train.columns if c not in ['path']]]).astype(int)\n", "    for im in range(len(X_train)):\n", "        for i in range(len(train[im])):\n", "            X_train[im][i,0] = [train[im][i],0,0]\n", "            X_train[im][i,1] = [train[im][i],0,0]\n", "            X_train[im][i,2] = [train[im][i],0,0]\n", "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n", "    return X_train, y_train, X_train_id\n", "\n", "def load_test():\n", "    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n", "    files = sorted(glob.glob(path))[:100] #limited\n", "\n", "    X_test = []\n", "    X_test_id = []\n", "    test = []\n", "    for fl in files:\n", "        flbase = os.path.basename(fl)\n", "        img = get_im_cv2(fl)\n", "        X_test.append(img)\n", "        X_test_id.append(flbase)\n", "    test = im_stats(pd.DataFrame(files, columns=['path']))\n", "    test = min_max_s.transform(test[[c for c in test.columns if c not in ['path']]]).astype(int)\n", "    for im in range(len(X_test)):\n", "        for i in range(len(test[im])):\n", "            X_test[im][i,0] = [test[im][i],0,0]\n", "            X_test[im][i,1] = [test[im][i],0,0]\n", "            X_test[im][i,2] = [test[im][i],0,0]\n", "    return X_test, X_test_id\n", "\n", "def create_submission(predictions, test_id, info):\n", "    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n", "    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n", "    now = datetime.datetime.now()\n", "    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n", "    result1.to_csv(sub_file, index=False)\n", "\n", "def read_and_normalize_train_data():\n", "    train_data, train_target, train_id = load_train()\n", "\n", "    print('Convert to numpy...')\n", "    train_data = np.array(train_data, dtype=np.uint8)\n", "    train_target = np.array(train_target, dtype=np.uint8)\n", "\n", "    print('Reshape...')\n", "    train_data = train_data.transpose((0, 3, 1, 2))\n", "\n", "    print('Convert to float...')\n", "    train_data = train_data.astype('float32')\n", "    train_data = train_data / 255\n", "    train_target = np_utils.to_categorical(train_target, 8)\n", "\n", "    print('Train shape:', train_data.shape)\n", "    print(train_data.shape[0], 'train samples')\n", "    return train_data, train_target, train_id\n", "\n", "def read_and_normalize_test_data():\n", "    start_time = time.time()\n", "    test_data, test_id = load_test()\n", "\n", "    test_data = np.array(test_data, dtype=np.uint8)\n", "    test_data = test_data.transpose((0, 3, 1, 2))\n", "\n", "    test_data = test_data.astype('float32')\n", "    test_data = test_data / 255\n", "\n", "    print('Test shape:', test_data.shape)\n", "    print(test_data.shape[0], 'test samples')\n", "    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n", "    return test_data, test_id\n", "\n", "def dict_to_list(d):\n", "    ret = []\n", "    for i in d.items():\n", "        ret.append(i[1])\n", "    return ret\n", "\n", "def merge_several_folds_mean(data, nfolds):\n", "    a = np.array(data[0])\n", "    for i in range(1, nfolds):\n", "        a += np.array(data[i])\n", "    a /= nfolds\n", "    return a.tolist()\n", "\n", "def create_model():\n", "    model = Sequential()\n", "    model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n", "    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n", "    model.add(Dropout(0.2))\n", "    \n", "    model.add(Flatten())\n", "    model.add(Dense(16, activation='relu',init='he_uniform'))\n", "    model.add(Dropout(0.2))\n", "    model.add(Dense(8, activation='softmax'))\n", "\n", "    model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n", "    return model\n", "\n", "def get_validation_predictions(train_data, predictions_valid):\n", "    pv = []\n", "    for i in range(len(train_data)):\n", "        pv.append(predictions_valid[i])\n", "    return pv\n", "\n", "def run_cross_validation_create_models(nfolds=5):\n", "    batch_size = 20\n", "    nb_epoch = 4\n", "    random_state = 0\n", "\n", "    train_data, train_target, train_id = read_and_normalize_train_data()\n", "\n", "    yfull_train = dict()\n", "    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n", "    num_fold = 0\n", "    sum_score = 0\n", "    models = []\n", "    for train_index, test_index in kf:\n", "        model = create_model()\n", "        X_train = train_data[train_index]\n", "        Y_train = train_target[train_index]\n", "        X_valid = train_data[test_index]\n", "        Y_valid = train_target[test_index]\n", "\n", "        num_fold += 1\n", "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n", "        print('Split train: ', len(X_train), len(Y_train))\n", "        print('Split valid: ', len(X_valid), len(Y_valid))\n", "\n", "        callbacks = [\n", "            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n", "        ]\n", "        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n", "              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n", "              callbacks=callbacks)\n", "\n", "        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n", "        score = log_loss(Y_valid, predictions_valid)\n", "        print('Score log_loss: ', score)\n", "        sum_score += score*len(test_index)\n", "\n", "        # Store valid predictions\n", "        for i in range(len(test_index)):\n", "            yfull_train[test_index[i]] = predictions_valid[i]\n", "\n", "        models.append(model)\n", "\n", "    score = sum_score/len(train_data)\n", "    print(\"Log_loss train independent avg: \", score)\n", "\n", "    info_string = '_' + str(np.round(score,3)) + '_flds_' + str(nfolds) + '_eps_' + str(nb_epoch)\n", "    return info_string, models\n", "\n", "def run_cross_validation_process_test(info_string, models):\n", "    batch_size = 20\n", "    num_fold = 0\n", "    yfull_test = []\n", "    test_id = []\n", "    nfolds = len(models)\n", "\n", "    for i in range(nfolds):\n", "        model = models[i]\n", "        num_fold += 1\n", "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n", "        test_data, test_id = read_and_normalize_test_data()\n", "        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n", "        yfull_test.append(test_prediction)\n", "\n", "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n", "    info_string = 'loss_' + info_string + '_folds_' + str(nfolds)\n", "    create_submission(test_res, test_id, info_string)\n", "\n", "if __name__ == '__main__':\n", "    print('Keras version: {}'.format(keras_version))\n", "    num_folds = 2\n", "    info_string, models = run_cross_validation_create_models(num_folds)\n", "    run_cross_validation_process_test(info_string, models)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"trusted": false, "_uuid": "8a3de32a22885642bc1934bcd96af2c46e48594e", "_cell_guid": "e71a8ca8-eca2-c5e9-775c-1f047322f373"}, "source": "#Now to blend use the following with your various results\n\nimport numpy as np\n\ndf1 = pd.read_csv('../input/sample_submission_stg1.csv') #change these\ndf2 = pd.read_csv('../input/sample_submission_stg1.csv') #change these\nc = [c+'_' if c !='image' else c for  c in df2.columns]\ndf2.columns = c\ndf = pd.merge(df1, df2, on='image', how='inner')\n\nfor c in df1.columns:\n    if c != 'image':\n        df[c] = (df[c] + df[c+'_'])/2\ndf[df1.columns].to_csv('z11_sub_blend01.csv', index=False)"}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "_is_fork": false, "_change_revision": 0, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 0}