{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.getcwd())\n#os.chdir('/Users/xianglongtan/Desktop/kaggle')\nprint(os.listdir(\"../input\"))\n#print(os.listdir())\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_activity = 'all'\n# Any results you write to the current directory are saved as output.","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\n#app_train = pd.read_csv('application_train.csv')\n#app_train.head()\napp_test = pd.read_csv('../input/application_test.csv')\n#app_test = pd.read_csv('application_test.csv')\n#app_test.head()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"53c0f2e2e5988afedda358fbd584a91b816ec2f6"},"cell_type":"markdown","source":"# Glimpse at raw dataset"},{"metadata":{"_uuid":"ae978742b3b42e55d4044196a5b12b62e21e5f52","trusted":true},"cell_type":"code","source":"train_Y = app_train['TARGET']\ntrain_X = app_train.drop('TARGET',axis=1)\ntest_X = app_test\nprint(train_Y.shape)\nprint(train_X.shape)\nprint(test_X.shape)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"f365a81e31c876bddc6365898b79956b658db344"},"cell_type":"markdown","source":"# Check missing value"},{"metadata":{"_uuid":"b0eeb2afe3edcb13242a8b5c309210ce6336fe98","trusted":true,"collapsed":true},"cell_type":"code","source":"# training set\n#train_Y.isnull().sum() # no missing value\ntrain_X_nonan = train_X.loc[:,train_X.isnull().sum() == 0] # no nan columns\ntrain_X_nan = train_X.loc[:,train_X.isnull().sum()>0] # columns that have nan\nnum_nan_train = train_X_nan.isnull().sum()\nnum_nan_train = pd.DataFrame(num_nan_train)\nnum_nan_train = num_nan_train.reset_index()\nnum_nan_train.columns = ['columns','number']\n#num_nan_train\n# test set\ntest_X_nonan = test_X.loc[:,test_X.isnull().sum() == 0] # no nan columns\ntest_X_nan = test_X.loc[:,test_X.isnull().sum()>0] # columns that have nan\nnum_nan_test = test_X_nan.isnull().sum()\nnum_nan_test = pd.DataFrame(num_nan_test)\nnum_nan_test = num_nan_test.reset_index()\nnum_nan_test.columns = ['columns','number']\n#num_nan_test","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"3e8a5cf33519c3bc810e337dbd7a71e9bbed08a5","trusted":true,"collapsed":true},"cell_type":"code","source":"# test sets has 64 features with missing value while trainging set has 67 features\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_columns',1000)\ndf = num_nan_train.set_index('columns').join(num_nan_test.set_index('columns'), how='left', lsuffix='_train', rsuffix='_test')\ntrain_X_nan = train_X[list(df.index)]","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"765fedd5466ad64ddf095cb887181d5a2b52bf73"},"cell_type":"code","source":"train_X_nan.head(10)","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"aeeebb7d46c33678f1159bc922ffccf29f035d5a"},"cell_type":"markdown","source":" ## Select columns where # of missing values is less than 10k. We will imputer those missing values later."},{"metadata":{"_uuid":"12d2d25a69df0be5412ec8f4703f9ebf03fd0998","trusted":true,"collapsed":true},"cell_type":"code","source":"less_10k_nan_train = num_nan_train[num_nan_train.number <= 10000] \ntrain_X_useful = train_X[less_10k_nan_train['columns']]# columns that have less than 10k nan\nless_10k_nan_test = num_nan_test[num_nan_test.number <= 8000]\ntest_X_useful = test_X[less_10k_nan_test['columns']]\ntrain_X_useful.isnull().sum()\ntest_X_useful.isnull().sum()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"9a8a967f7d4c9accfb1cbc0338d1f772f8ead6f3","collapsed":true},"cell_type":"markdown","source":"## Encoding Categorical data"},{"metadata":{"_uuid":"3de30a8cabe48451ab055f1a94ba445ca50d1885","trusted":true,"collapsed":true},"cell_type":"code","source":"# select columns that both not have missing values in training and test set\nnonan_columns =  train_X_nonan.columns.intersection(test_X_nonan.columns).drop('NAME_EDUCATION_TYPE')\ntrain_and_test = pd.concat([train_X_nonan[nonan_columns], test_X_nonan[nonan_columns]], axis=0)\ntrain_and_test_object = train_and_test.loc[:,train_and_test.dtypes==object]\nobject_col = train_and_test_object.columns\ntrain_X_nonan_obj = train_X_nonan[object_col]\ntest_X_nonan_obj = test_X_nonan[object_col]","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"bdb5f8a1cabfb80011b67bda1ecac2333caae5cb","trusted":true,"collapsed":true},"cell_type":"code","source":"train_X_nonan_dummies = pd.get_dummies(train_X_nonan_obj)\ntest_X_nonan_dummies = pd.get_dummies(test_X_nonan_obj)\ntrain_X_nonan_dummies,test_X_nonan_dummies = train_X_nonan_dummies.align(test_X_nonan_dummies, join='inner', axis=1)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"e2b019119da8d103e1ce73eddac32a542c49f272"},"cell_type":"markdown","source":"## Transform ordered categorical features"},{"metadata":{"_uuid":"c10a07aba1361fb0985c69fcfdd4a573941d6832","trusted":true,"collapsed":true},"cell_type":"code","source":"# education is order categorical features\ndef encode_edu(x):\n    if x == 'Secondary / secondary special':\n        return np.float(1)\n    elif x == 'Higher education':\n        return np.float(3)\n    elif x == 'Incomplete higher':\n        return np.float(2)\n    elif x == 'Lower secondary':\n        return np.float(0)\n    else:\n        return np.float(4)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"e1ac696efefec030b6e9f90ab95de997ddedfca5","trusted":true,"collapsed":true},"cell_type":"code","source":"education = test_X_nonan.NAME_EDUCATION_TYPE.map(lambda x: encode_edu(x))\ntest_X_nonan_dummies = pd.concat([test_X_nonan_dummies, education],axis=1)\neducation = train_X_nonan.NAME_EDUCATION_TYPE.map(lambda x: encode_edu(x))\ntrain_X_nonan_dummies = pd.concat([train_X_nonan_dummies, education],axis=1)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"e06a31e9759824d8ecb503eea058a6ef64633b6d","trusted":true,"collapsed":true},"cell_type":"code","source":"nonan_num_col = train_and_test.loc[:,train_and_test.dtypes!=object].columns\ntest_X_nonan_dummies = pd.concat([test_X_nonan_dummies, test_X_nonan[nonan_num_col]],axis=1)\ntrain_X_nonan_dummies = pd.concat([train_X_nonan_dummies, train_X_nonan[nonan_num_col]],axis=1)\n","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"3cacc9364a2f4d23d53c4e73e290de94ea9a5352","collapsed":true},"cell_type":"markdown","source":"# Train various model"},{"metadata":{"_uuid":"a6d8be9eda1600a0037101146786a029056b2bb9","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport time\nfrom plotnine import *","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"aab6c28ff0bd8548b9579aaa3ded4351b4a363d7","trusted":true,"collapsed":true},"cell_type":"code","source":"Y = train_Y\nX = train_X_nonan_dummies.drop('SK_ID_CURR',axis=1)\nseed = 4\ntest_X = test_X_nonan_dummies\ntest_ID = pd.DataFrame(test_X['SK_ID_CURR'])\ntest_X = test_X_nonan_dummies.drop('SK_ID_CURR',axis=1)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"cc4ca8f04dabafa4e64f67ef6ab3aea4426ef88b","trusted":true,"collapsed":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val = train_test_split(X,Y,random_state=seed,stratify=Y)\n#y_train = pd.DataFrame(y_train)\n#y_val = pd.DataFrame(y_val)\n#(ggplot(y_train)+geom_bar(aes(x='TARGET')))\n#(ggplot(y_val)+geom_bar(aes(x='TARGET')))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"048ada82bbf9d5583280350501c6d3b8ef8680b3","collapsed":true,"trusted":false},"cell_type":"code","source":"# KNN\nflag = 0\nif flag == 0:\n    pass\nelse:\n    start = time.time()\n    model = KNeighborsClassifier()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(X,Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_knn1.csv')","execution_count":104,"outputs":[]},{"metadata":{"_uuid":"d08a69045923daae3ff55e55a8c33b57538cf6b8","collapsed":true,"trusted":false},"cell_type":"code","source":"# SVM\nflag = 0\nif flag == 0:\n    pass\nelse:\n    start = time.time()\n    model = SVC()\n    model.fit(X_train, y_train)\n    pred_train = model.predict(X_train)\n    pred_val = model.predict(X_val)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_svm1.csv')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d63f5fe73abad3a08df15b40aa3680f9fa37912","collapsed":true,"trusted":false},"cell_type":"code","source":"# Logistic Regression\nflag = 0\nif flag == 0:\n    pass\nelse:\n    start = time.time()\n    model = LogisticRegression()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(X,Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_logit1.csv')","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"e841247c58623272ec74d3fc24ef70c431ebb6c6","collapsed":true,"trusted":false},"cell_type":"code","source":"# RandomForest\nflag = 0\nif flag == 0:\n    pass\nelse:\n    start = time.time()\n    model = RandomForestClassifier(class_weight = 'balanced')\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(X,Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_rf1.csv')\n\n","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"317be75b30dfde5e6f9b02ed9c35cf805c7a9fad","collapsed":true,"trusted":false},"cell_type":"code","source":"# Gradient Boosting\nflag = 0\nif flag == 0:\n    pass\nelse:\n    start = time.time()\n    model = XGBClassifier()\n    #model.fit(X_train, y_train)\n    #pred_train = model.predict(X_train)\n    #pred_val = model.predict(X_val)\n    model.fit(X,Y)\n    end  = time.time()\n    #print('F1 score of training set:',f1_score(pred_train, y_train, average='weighted'))\n    #print('F1 score of test set:',f1_score(pred_val, y_val, average='weighted'))\n    print('Done! Time spent:',end-start)\n    pred_test = pd.DataFrame(model.predict_proba(test_X)).loc[:,1]\n    result = pd.concat([test_ID,pred_test],axis=1)\n    result.columns = ['SK_ID_CURR','TARGET']\n    result = result.set_index('SK_ID_CURR')\n    result.to_csv('only_nonan_xgb1.csv')\n\n","execution_count":76,"outputs":[]},{"metadata":{"_uuid":"0854ddcdbcf3fdfd16e6ba92472acb71acbbc391","trusted":true,"collapsed":true},"cell_type":"code","source":"# Neural Network\n'''\nimport tensorflow as tf\n\n# Hyperparam\nLR = 0.000001 # learning rate\nITERATION = 10000\nBATCH_SIZE = 1500\nKEEP_PROB = 0.7\nNUM_FEAT = 135\nNUM_CLASS = 2\n\nclass DataIter():\n    def __init__(self, X,Y):\n        self.X = X\n        self.Y = Y\n        self.size = len(self.X)\n        self.epochs = 0\n        self.df = pd.concat([X,Y],axis=1)\n        self.pos = self.df.loc[self.Y == 1]\n        self.neg = self.df.loc[self.Y == 0]\n    def next_batch(self,n):\n        #X_train,X_val,y_train,y_val = train_test_split(X,Y,test_size = n/self.size,random_state=seed,stratify=Y)\n        #res = pd.concat([X_val,y_val],axis=1)\n        pos_sample = self.pos.sample(n, replace=True)\n        neg_sample = self.neg.sample(n, replace=True)\n        res = pd.concat([neg_sample, pos_sample],axis=0)\n        return res\n\n# build graph\ntf.reset_default_graph()\nx = tf.placeholder(tf.float32,[BATCH_SIZE*2, NUM_FEAT])\ny = tf.placeholder(tf.int32,[BATCH_SIZE*2])\nkeep_prob = tf.constant(KEEP_PROB)\nnn_inputs = tf.layers.dense(x, units = round(0.75*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid)# hidden layer 1\nnn_inputs = tf.nn.dropout(nn_inputs, KEEP_PROB)\n#print(nn_inputs.get_shape)\nnn_inputs = tf.layers.dense(x, units = round(0.5*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 2\nnn_inputs = tf.nn.dropout(nn_inputs, KEEP_PROB)\n#print(nn_inputs.get_shape)\nnn_inputs = tf.layers.dense(x, units = round(0.25*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 3\nnn_inputs = tf.nn.dropout(nn_inputs, KEEP_PROB)\n#print(nn_inputs.get_shape)\nnn_inputs = tf.layers.dense(x, units = 10, kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 4\nnn_inputs = tf.nn.dropout(nn_inputs, KEEP_PROB)\nwith tf.variable_scope('softmax'):\n    W = tf.get_variable('W', [10, NUM_CLASS],initializer = tf.truncated_normal_initializer())\n    b = tf.get_variable('b', [NUM_CLASS],initializer = tf.constant_initializer(0.0))\nlogits = tf.matmul(nn_inputs, W)+b\n#print(logits.get_shape)\npreds = tf.nn.softmax(logits)\nprediction = tf.cast(tf.argmax(preds,1), tf.int32)\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = preds))\nprecision, precision_op = tf.metrics.precision(y,prediction)\n#print(precision.get_shape)\nrecall, recall_op = tf.metrics.recall(y,prediction)\n#print(recall.get_shape)\nf1score = 2*precision*recall/(precision+recall)\ntrain_step = tf.train.AdamOptimizer(LR).minimize(loss)\n\n# session\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    tr = DataIter(X,Y)\n    for i in range(ITERATION):\n        batch = tr.next_batch(BATCH_SIZE)\n        sess.run(train_step, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n        if i%1000 == 0:\n            _,prec = sess.run([precision,precision_op], feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n            _,rec = sess.run([recall,recall_op], feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n            f1s = sess.run(f1score, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n            los = sess.run(loss, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']})\n            print('losss after',i,'round',los)\n            print('precision after',i,'round',prec)\n            print('recall after',i,'round',rec)\n            print('F1 score after',i,'round:',f1s)\n            print('\\n----------------------------------\\n')\n            print('logits:\\n',sess.run(logits,feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']}))\n            print('preds:\\n',sess.run(preds, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']}))\n            print('prediction:\\n',sess.run(prediction, feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']}))\n            print('y:\\n',sess.run(y,feed_dict={x:batch.iloc[:,0:-1], y:batch['TARGET']}))\n            print('\\n----------------------------------\\n')\n    cursor = 0\n    while cursor <= len(test_X):\n        if cursor+BATCH_SIZE <= len(test_X):\n            te = test_X.iloc[cursor:cursor+2*BATCH_SIZE]\n        else:\n            te = pd.concat([test_X.iloc[cursor:len(test_X)],test_X.iloc[0:2*BATCH_SIZE-len(test_X.iloc[cursor:len(test_X)])]])\n        results = sess.run(preds, feed_dict={x:te})\n        if cursor == 0:\n            prediction = pd.DataFrame(data=results, columns=['0','TARGET'])\n        else:\n            prediction = pd.concat([prediction, pd.DataFrame(data=results,columns=['0','TARGET'])])\n        cursor += 2*BATCH_SIZE\n        '''","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"37524e943171ee64bd248afba7a1c86f97692aa0","trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nresult = prediction.iloc[0:len(test_X)]\npred_test = pd.DataFrame(result['TARGET']).reset_index()\nresult_final = pd.concat([test_ID, pred_test],axis=1).drop('index',axis=1)\nresult_final.columns = ['SK_ID_CURR','TARGET']\nresult_final = result_final.set_index('SK_ID_CURR')\nresult_final.to_csv('only_nonan_NN1.csv')\n'''","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"5d0dc122ae0de7a92caa82a5581232c26f59359f","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ea724df155e4c656ea3bdb48431ebdc8ce7509b","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"246680f4a1da9866625aa74cb41b686ee46eac3d","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}