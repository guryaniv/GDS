{"cells":[{"metadata":{"_uuid":"015ea1dbb1513cc088fa0e0ceb56b62e9b8bb4ee"},"cell_type":"markdown","source":"# Content\n\n* FastText on 2-gram (loss=0.35, acc=0.86)\n* CNN on POS-Tags Only (loss=0.8, acc=0.66)\n* CNN on both words and POS-Tags (loss=0.36, acc=0.85)\n\nPS: I've tried, together with another kernel of conventional ML methods. But the best result is loss=0.35. Any advise is welcom. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom  keras.preprocessing.sequence import pad_sequences\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\n\ndf = pd.read_csv(\"../input/train.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f604d82ef7622ba7014bf6bb66f86440cc4b589b"},"cell_type":"markdown","source":"## Some Exploration"},{"metadata":{"trusted":true,"_uuid":"67b787883017f40c6fb817844a8dcdf79c553bf9"},"cell_type":"code","source":"#Number of sentences\nsentence_split = re.compile(\"[.!?'\\\";]\")\ndf['num_sentences'] = df['text'].apply(lambda x: len(sentence_split.split(x)))\ndf['num_sentences'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"607583479d4f1849d3da701e77ed7dba70f9cab5"},"cell_type":"code","source":"# Number of words\ndf['num_words'] = df['text'].apply(lambda x: len(x.split(' ')))\ndf['num_words'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a711e186ceef150d72ab642c493a775340fc4383"},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.countplot(x=\"author\", data=df);\nplt.subplot(1,3,2)\nsns.stripplot(x=\"author\", hue='author', y='num_sentences', data=df, jitter=True);\nplt.subplot(1,3,3)\nsns.stripplot(x=\"author\", hue='author', y='num_words', data=df, jitter=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74ef0e751a19e4835df2fa974e4c7a3f743528f7"},"cell_type":"markdown","source":"## Some Common Preprocessing"},{"metadata":{"trusted":true,"_uuid":"53eb119bf630cac7ad5ac60977953e2eb1405559"},"cell_type":"code","source":"labelEncoder = LabelEncoder().fit(df['author'])\ndf['author_id'] = labelEncoder.transform(df['author'])\ndef gen_ngram(tokens, n):\n    length = len(tokens)\n    for i in range(2, n+1):\n        for j in range(0, length+1-i):\n            tokens.append('+'.join(tokens[j:j+i]))\n    return tokens\ndef preprocess_text(text, stem_func=None, stop_words=set()):\n    text = nltk.tokenize.word_tokenize(text)\n    text = [w for w in text if not w in stop_words]\n    if stem_func!=None:\n        text = [stem_func(w) for w in text]\n    return ' '.join(text)\nwnl = nltk.stem.wordnet.WordNetLemmatizer()\n\ndf['text_proc'] = df['text'].apply(lambda x: preprocess_text(x, wnl.lemmatize, set()))\ndf['pos_tags'] = df['text_proc'].apply(lambda x: ' '.join([y[1] for y in nltk.pos_tag(x.split(' '))]))\ndf['text_proc'].head().apply(lambda x: len(x.split(' ')))\ndf['pos_tags'].head().apply(lambda x: len(x.split(' ')))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fadaf62a2788f3db9e5fb5955d400ee781c6bb5"},"cell_type":"markdown","source":"## Fasttext 2-gram\n\nRef: [simple-keras-fasttext](https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31)"},{"metadata":{"trusted":true,"_uuid":"616e15ef6af47c55be1cbf5c10a176e0d471d67f"},"cell_type":"code","source":"bigrams = [' '.join(gen_ngram(sentence.split(' '), 2)) for sentence in df['text_proc']]\ntokenizer = Tokenizer(filters='', lower=False, split=' ')\ntokenizer.fit_on_texts(bigrams)\ny = df['author_id'].values\nX = tokenizer.texts_to_sequences(bigrams)\nX = pad_sequences(X)\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25d1945bd29d7a96a96eb4735e94c8fced0a919b"},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(InputLayer((X.shape[1],)))\nmodel1.add(Embedding(input_dim=np.max(X)+1, output_dim=40, input_length=X.shape[1]))\nmodel1.add(GlobalAveragePooling1D())\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(3, activation='softmax'))\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d2030565f4c14de25d696e9d6a834efad867c69","scrolled":false},"cell_type":"code","source":"model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\nepochs=100\nhistory = model1.fit(X_train, y_train, \n          epochs=epochs, \n          validation_data=(X_validation, y_validation),\n          callbacks=[EarlyStopping(patience=3, monitor='val_loss')],\n          verbose=2)\nplt.figure(figsize=(10,2))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], 'r--')\nplt.plot(history.history['val_loss'], 'b-')\nplt.subplot(1,2,2)\nplt.plot(history.history['acc'], 'r--')\nplt.plot(history.history['val_acc'], 'b-')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80982d457df2d986953268a6a8e46a4891710995"},"cell_type":"markdown","source":"## A Model using POS tags only"},{"metadata":{"trusted":true,"_uuid":"fb2020dfd323a3d96b6adfecfb7d6fc1027527e5"},"cell_type":"code","source":"tokenizer = Tokenizer(filters='', lower=False, split=' ')\ntokenizer.fit_on_texts(df['pos_tags'])\ny = df['author_id'].values\nX = tokenizer.texts_to_sequences(df['pos_tags'])\nX = pad_sequences(X, 50)\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2)\n\nmodel2 = Sequential()\nmodel2.add(InputLayer((X.shape[1],)))\nmodel2.add(Embedding(input_dim=np.max(X)+1, output_dim=30, input_length=X.shape[1]))\nmodel2.add(Conv1D(100, kernel_size=10, activation='relu'))\nmodel2.add(GlobalAveragePooling1D())\nmodel2.add(Dropout(0.3))\nmodel2.add(Dense(3, activation='softmax'))\nmodel2.summary()\n\nmodel2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\nepochs=300\nhistory = model2.fit(X_train, y_train, \n          epochs=epochs, \n          validation_data=(X_validation, y_validation),\n          callbacks=[EarlyStopping(patience=5, monitor='val_loss')],\n          verbose=2)\nplt.figure(figsize=(10,2))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], 'r--')\nplt.plot(history.history['val_loss'], 'b-')\nplt.subplot(1,2,2)\nplt.plot(history.history['acc'], 'r--')\nplt.plot(history.history['val_acc'], 'b-')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3e3a57dd23ce5b1bdb5cc56a32b09addd9331b"},"cell_type":"markdown","source":"0.66 accuracy is better than chance. So POS tags do carry information."},{"metadata":{"_uuid":"0b035df492f8c7935e503ba54d7c7c3260a9e5a9"},"cell_type":"markdown","source":"## CNN Model that uses words and POS tags\n\nRef [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"},{"metadata":{"trusted":true,"_uuid":"9b734faaa66e2d509454f8f7fad4d0d9b97268d6"},"cell_type":"code","source":"tokenizer_text = Tokenizer(filters='', lower=False, split=' ')\ntokenizer_text.fit_on_texts(df['text_proc'])\ntokenizer_pos = Tokenizer(filters='', lower=False, split=' ')\ntokenizer_pos.fit_on_texts(df['pos_tags'])\ny = df['author_id'].values\nX_text = tokenizer_text.texts_to_sequences(df['text_proc'])\nX_text = pad_sequences(X_text)\nX_pos = tokenizer_pos.texts_to_sequences(df['pos_tags'])\nX_pos = pad_sequences(X_pos)\n\nX_text_train, X_text_validation, X_pos_train, X_pos_validation, y_train, y_validation = train_test_split(X_text, X_pos, y, test_size=0.1)\n\npos_input = Input(shape=(X_pos.shape[1],), name='pos_input')\npos_embd = Embedding(input_dim=np.max(X_pos)+1, output_dim=10, input_length=X_pos.shape[1])(pos_input)\ntext_input = Input(shape=(X_text.shape[1],), name='text_input')\ntext_embd = Embedding(input_dim=np.max(X_text)+1, output_dim=10, input_length=X_text.shape[1])(text_input)\nx = concatenate([pos_embd, text_embd], axis=-1)\nx1 = Conv1D(20, kernel_size=2, padding='same', activation='relu')(x)\nx1 = GlobalMaxPooling1D()(x1)\nx2 = Conv1D(20, kernel_size=3, padding='same', activation='relu')(x)\nx2 = GlobalMaxPooling1D()(x2)\nx3 = Conv1D(20, kernel_size=4, padding='same', activation='relu')(x)\nx3 = GlobalMaxPooling1D()(x3)\nx4 = Conv1D(20, kernel_size=5, padding='same', activation='relu')(x)\nx4 = GlobalMaxPooling1D()(x4)\nx = concatenate([x1,x2,x3,x4], axis=-1)\nx = Dropout(0.3)(x)\nx = Dense(3, activation='softmax')(x)\nmodel_combined = Model(inputs = [text_input, pos_input], outputs=x)\nmodel_combined.summary()\n\nmodel_combined.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\nepochs=100\nhistory = model_combined.fit({'pos_input':X_pos_train, 'text_input':X_text_train}, y_train, \n          epochs=epochs, \n          validation_data=({'pos_input':X_pos_validation, 'text_input':X_text_validation}, y_validation),\n          callbacks=[EarlyStopping(patience=2, monitor='val_loss')],\n          verbose=2)\nplt.figure(figsize=(10,2))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], 'r--')\nplt.plot(history.history['val_loss'], 'b-')\nplt.subplot(1,2,2)\nplt.plot(history.history['acc'], 'r--')\nplt.plot(history.history['val_acc'], 'b-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5bad527e9eead61145834dee2ec2f4e3dbad7f9b"},"cell_type":"markdown","source":"This model converges faster and is as good as the fastText model, but overfits the training set easily."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}