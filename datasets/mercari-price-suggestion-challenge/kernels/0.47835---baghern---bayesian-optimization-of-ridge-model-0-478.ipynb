{"nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "17c7d5f570c1ff82ffbaa5828e83869122436902", "_cell_guid": "a91ebe3d-8524-406c-9bad-3f57f2d143ab"}, "source": ["# Hyper Parameter Tuning With Bayesian Optimization\n", "\n", "Bayesian Optimization is a powerful way to tune hyper parameters in a model. Instead of a brute force approach like with sklearn's grid_search, it tries to optimize the loss function by intelligently exploring the underlying distribution.  This script can be run with the develop flag to find the optimal hyperparameters, then to deploy a final model, specify the optimal found parameters by hand. \n", "\n", "To see more of the documentation, check out the project github:\n", "https://github.com/fmfn/BayesianOptimization"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "302f8c8ad167a57b1c24ae36847b7c4a837e0dad", "_cell_guid": "8c0250d1-2573-4031-aa39-987f3fa81151"}, "source": ["import gc\n", "import time\n", "import numpy as np\n", "import pandas as pd\n", "import sys\n", "from scipy.sparse import csr_matrix, hstack\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "start_time = time.time()\n", "tcurrent   = start_time\n", "\n", "np.random.seed(54)   \n", "\n", "NUM_BRANDS = 4550\n", "NUM_CATEGORIES = 1280\n", "MAX_FEATURES_NAME = 10000\n", "MAX_FEATURES_DESC =10000\n", "\n", "develop = True"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "59fff548841ae3236461f610b3d7810203b4af01", "_cell_guid": "240314b7-f277-482d-aa09-1e463098727f"}, "source": ["from sklearn.metrics import make_scorer\n", "\n", "def rmsle(y_true, y_pred):\n", "    # Remember, we transformed price with log1p previously.\n", "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n", "\n", "neg_rmsle = make_scorer(rmsle, greater_is_better=False)\n", "\n", "def split_cat(text):\n", "    try:\n", "        return text.split(\"/\")\n", "    except:\n", "        return (\"No Label\", \"No Label\", \"No Label\")\n", "\n", "\n", "def handle_missing_inplace(dataset):\n", "    dataset['general_cat'].fillna(value='missing', inplace=True)\n", "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n", "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n", "    dataset['brand_name'].fillna(value='missing', inplace=True)\n", "    dataset['item_description'].fillna(value='missing', inplace=True)\n", "\n", "\n", "def cutting(dataset):\n", "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n", "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n", "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n", "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n", "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n", "\n", "\n", "def to_categorical(dataset):\n", "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n", "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n", "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n", "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "9c51600748c32975d862aa5739568735a7923d0b", "_cell_guid": "b991c7e0-a6bb-412e-9a8d-ebe25b5622db", "scrolled": true}, "source": ["from time import gmtime, strftime\n", "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n", "\n", "train = pd.read_table('../input/train.tsv', engine='c')\n", "test = pd.read_table('../input/test.tsv', engine='c')\n", "\n", "\n", "train = train.drop(train[(train.price == 0.0)].index)\n", "y = np.log1p(train[\"price\"])\n", "\n", "print('[{}] Finished to load data'.format(time.time() - start_time))\n", "print('Train shape: ', train.shape)\n", "print('Test shape: ', test.shape)\n", "\n", "submission: pd.DataFrame = test[['test_id']]\n", "    \n", "train.head()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "85221a9c0b86d80320605087267dd499e9ce5530", "_cell_guid": "f2f9571b-7720-4a09-8f03-e5ea5cbfbe50"}, "source": ["import re, string, timeit\n", "\n", "regex = re.compile('[%s]' % re.escape(string.punctuation))\n", "\n", "train['item_description'] = train['item_description'].apply(lambda x: regex.sub('',str(x).lower()))\n", "test['item_description'] = test['item_description'].apply(lambda x: regex.sub('',str(x).lower()))\n", "train['name'] = train['name'].apply(lambda x: regex.sub('',x.lower()))\n", "test['name'] = test['name'].apply(lambda x: regex.sub('',x.lower()))\n", "\n", "train.head()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "d3f3556034be61104dd454e64cc66bb0c9757b17", "_cell_guid": "8662eb45-cafb-4b75-9b7b-5332ad7b98b7"}, "source": ["train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n", "        zip(*train['category_name'].apply(lambda x: split_cat(x)))\n", "train.drop('category_name', axis=1, inplace=True)\n", "\n", "test['general_cat'], test['subcat_1'], test['subcat_2'] = \\\n", "        zip(*test['category_name'].apply(lambda x: split_cat(x)))\n", "test.drop('category_name', axis=1, inplace=True)\n", "print('[{}] Split categories completed.'.format(time.time() - start_time))\n", "\n", "\n", "handle_missing_inplace(train)\n", "handle_missing_inplace(test)\n", "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n", "\n", "\n", "cutting(train)\n", "cutting(test)\n", "print('[{}] Cut completed.'.format(time.time() - start_time))\n", "\n", "\n", "to_categorical(train)\n", "to_categorical(test)\n", "print('[{}] Convert categorical completed'.format(time.time() - start_time))\n", "train.head()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "f95943417650a0583834174993438e4a1846c85d", "_cell_guid": "b13b45e9-7a7e-425b-b443-4351d69ec029"}, "source": ["from sklearn.pipeline import FeatureUnion\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.feature_extraction import DictVectorizer\n", "\n", "\n", "class ItemSelector(BaseEstimator, TransformerMixin):\n", "    def __init__(self, key):\n", "        self.key = key\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, data_dict):\n", "        return data_dict[self.key]"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "f5fcee7680822564f33787213e9d02fa3db12c8c", "_cell_guid": "e43fa938-fc38-4fbd-a4d7-9f89e3e713f9"}, "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "pipeline = Pipeline([\n", "            ('selector', ItemSelector(key='name')),\n", "            ('tfidf', TfidfVectorizer(ngram_range = (1, 3),\n", "                            strip_accents = 'unicode', \n", "                            stop_words = 'english',\n", "                            min_df=20,\n", "                            max_df=.9,\n", "                            max_features = MAX_FEATURES_NAME))\n", "])\n", "\n", "X_name = pipeline.fit_transform(train, train['price'])\n", "X_name_test = pipeline.transform(test)\n", "\n", "X_name.shape"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "157f9aa1b59b5c1cf9b4fb129113c0dd2afac958", "_cell_guid": "8ecf3b43-f0ec-4a29-b73e-4e1c09c0cbf0"}, "source": ["pipeline = Pipeline([\n", "    ('item_description', Pipeline([\n", "            ('selector', ItemSelector(key='item_description')),\n", "            ('tfidf', TfidfVectorizer(ngram_range = (1, 3),\n", "                                stop_words = 'english',\n", "                                min_df=20,\n", "                                max_df=.9,\n", "                                max_features = MAX_FEATURES_DESC))\n", "            ]))\n", "])\n", "\n", "X_text = pipeline.fit_transform(train, train['price'])\n", "X_text_test = pipeline.transform(test)\n", "\n", "X_text.shape"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "5a2e3dd5010494363450982294f92be96b800325", "_cell_guid": "4fc7c1e7-197c-44ae-871a-494ac5b67d2b"}, "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "\n", "wb = CountVectorizer()\n", "X_category1 = wb.fit_transform(train['general_cat'])\n", "X_category1_test = wb.transform(test['general_cat'])\n", "X_category2 = wb.fit_transform(train['subcat_1'])\n", "X_category2_test = wb.transform(test['subcat_1'])\n", "X_category3 = wb.fit_transform(train['subcat_2'])\n", "X_category3_test = wb.transform(test['subcat_2'])\n", "print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n", "\n", "\n", "lb = LabelBinarizer(sparse_output=True)\n", "X_brand = lb.fit_transform(train['brand_name'])\n", "X_brand_test = lb.transform(test['brand_name'])\n", "print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "e04f361fbaa1e8fe87316baefa78d057d3263e60", "_cell_guid": "40e61dbc-2d43-4a8c-bbf5-9a50a77980ec"}, "source": ["X_dummies = csr_matrix(pd.get_dummies(train[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "X_dummies_test = csr_matrix(pd.get_dummies(test[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n", "print(X_dummies.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n", "          X_text.shape)\n", "\n", "\n", "sparse_merge = hstack((X_dummies, X_brand, X_name, X_text,\n", "                       X_category1, X_category2, X_category3)).tocsr()\n", "sparse_merge_test = hstack((X_dummies_test, X_brand_test, X_name_test, X_text_test,\n", "                            X_category1_test, X_category2_test, X_category3_test)).tocsr()\n", "\n", "print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n", "del X_dummies, lb, X_brand, X_category1, X_category2, X_category3\n", "del X_name, X_text\n", "del X_dummies_test, X_brand_test, X_category1_test, X_category2_test, X_category3_test\n", "del X_name_test, X_text_test\n", "\n", "gc.collect()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "b5449f17bf71ad932b2cd5755a8d770199bd1af1", "_cell_guid": "48279f3a-e911-41b0-8573-0e7c250eb4ce"}, "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X = sparse_merge\n", "X_test = sparse_merge_test\n", "\n", "train_X, train_y = X, y\n", "del X, sparse_merge, sparse_merge_test\n", "gc.collect()"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "d4e85b076b091a05c0375e9ed41c40d4b39c169a", "_cell_guid": "aabd3520-0511-43c1-b1a0-9fcf90cc99cd"}, "source": ["from bayes_opt import BayesianOptimization\n", "from sklearn.metrics import make_scorer, mean_squared_log_error\n", "from sklearn.model_selection import cross_val_score\n", "\n", "from sklearn.linear_model import Ridge\n", "\n", "seed = 101 # Lucky seed\n", "\n", "    \n", "def target(**params):\n", "    fit_intercept = int(params['fit_intercept'])\n", "    fit_intercept_dict = {0:False, 1:True}\n", "\n", "    model = Ridge(alpha = params['alpha'],\n", "                    fit_intercept = fit_intercept_dict[fit_intercept],\n", "                    copy_X = True)\n", "    \n", "    scores = cross_val_score(model, train_X, train_y, scoring=neg_rmsle, cv=3)\n", "    return scores.mean()\n", "    \n", "params = {'alpha':(1, 4),\n", "          'fit_intercept':(0,1.99)}\n", "if develop:\n", "    bo = BayesianOptimization(target, params, random_state=seed)\n", "    bo.gp.set_params(alpha=1e-8)\n", "    bo.maximize(init_points=5, n_iter=10, acq='ucb', kappa=2)\n", "    \n", "    print(bo.res['max']['max_params'])"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "234f0ac5ce49022be7c7bdc0748c1f92ac28908b", "_cell_guid": "ba51caf8-7eb7-4e88-b9c1-55cc80e91391"}, "source": ["model = Ridge(alpha = 3.0656,\n", "                  fit_intercept = True,\n", "                  copy_X = True)\n", "\n", "model.fit(train_X, train_y)\n", "predsR = model.predict(X_test)\n", "submission['price'] = np.expm1(predsR)\n", "submission.to_csv(\"Bayesian_Ridge.csv\", index=False)"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "_uuid": "6a25dea8789f02b3346d8e09eba222dda951432b", "_cell_guid": "48e473e5-f9e3-4c93-8793-e146d90ac2ef"}, "source": ["nm=(time.time() - start_time)/60\n", "print (\"Total processing time %s min\" % nm)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "version": "3.6.4", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python"}}, "nbformat_minor": 1}