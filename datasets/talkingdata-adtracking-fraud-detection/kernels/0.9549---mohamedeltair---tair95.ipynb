{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#imports\nimport pandas as pd\nimport numpy as np\nimport statistics as st\nimport matplotlib.pyplot as plt\nimport random\nfrom skimage import io\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.manifold import TSNE\n\nimport gc\nimport lightgbm as lgb\n\nimport time\n\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ea70a0e-9261-411e-9894-a6a85bd050d2","_uuid":"1c4a09b75c55137e696489737c6609af2eba44e4","collapsed":true,"trusted":true},"cell_type":"code","source":"dtypes = {\n        'ip'            : 'uint32',\n        'app'           : 'uint16',\n        'device'        : 'uint16',\n        'os'            : 'uint16',\n        'channel'       : 'uint16',\n        'is_attributed' : 'uint8',\n        'click_id'      : 'uint32'\n        }\nprint('about to read')\ntrain = pd.read_csv(\"../input/train.csv\", skiprows=range(1,179903890), nrows=5000000, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\ntest = pd.read_csv(\"../input/test.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\nprint('done reading')\nlen_train = len(train)\ntrain=train.append(test)\ndel test\ngc.collect()\ntrain['hour'] = pd.to_datetime(train['click_time']).dt.hour.astype('uint8')\ntrain['day'] = pd.to_datetime(train['click_time']).dt.day.astype('uint8')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55083f4d-bc22-4abf-8d9c-88c1c3bcbff5","_uuid":"22bbb4c1b03504d567b9de6a21d4016f8523e13c","collapsed":true,"trusted":true},"cell_type":"code","source":"def groupby_comb(comb, att, newname, opt):\n    print('about to add feature')\n    global train\n    combatt = np.concatenate((comb, att), axis=0)\n    if(opt == 1):\n        newdf = train[combatt].groupby(by=comb)[att].count().reset_index().rename(index=str, columns={att[0]: newname})\n    elif(opt == 2):\n        newdf = train[combatt].groupby(by=comb)[att].mean().reset_index().rename(index=str, columns={att[0]: newname})\n    else:\n        newdf = train[combatt].groupby(by=comb)[att].var().reset_index().rename(index=str, columns={att[0]: newname})\n    train = train.merge(newdf, on=comb, how='left')\n    del newdf\n    gc.collect\n    print('added feature')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cbb11549-24a3-4c41-9de8-fcd94093c5b5","_uuid":"5a58c1628281676440980faba342592231229388","collapsed":true,"trusted":true},"cell_type":"code","source":"groupby_comb(['ip'], ['channel'], 'a', 1)\ngroupby_comb(['ip','day','hour'], ['channel'], 'b', 1)\ngroupby_comb(['ip', 'app'], ['channel'], 'c', 1)\ngroupby_comb(['ip', 'app', 'os'], ['channel'], 'd', 1)\ngroupby_comb(['ip','day','channel'], ['hour'], 'e', 3)\ngroupby_comb(['ip', 'app', 'os'], ['hour'], 'f', 3)\ngroupby_comb(['ip', 'app', 'channel'], ['day'], 'g', 3)\ngroupby_comb(['ip', 'app', 'channel'], ['hour'], 'h', 2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3033a237-9722-494d-8a95-98ef07c7e7c3","_uuid":"af36057f86558f913001c6fd12756967d298d9a8","collapsed":true,"trusted":true},"cell_type":"code","source":"test = train[len_train:]\ntrain = train[:len_train]\ntarget = 'is_attributed'\npredictors = ['app','device','os', 'channel', 'hour', 'day', \n              'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\ncategorical = ['app', 'device', 'os', 'channel', 'hour', 'day']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9c63a84d-b320-4f85-94a3-6b6f1a1c9b30","_uuid":"ceec9afc5c5364b815779c506f1d7bc446f8d43d","collapsed":true,"trusted":true},"cell_type":"code","source":"lgbtrain = lgb.Dataset(train[predictors].values, label=train[target].values,\n                      feature_name=predictors,\n                      categorical_feature=categorical)\n                      \nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'subsample_for_bin': 200000,  # Number of samples for constructing bin\n    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n    'reg_alpha': 0,  # L1 regularization term on weights\n    'reg_lambda': 0,  # L2 regularization term on weights\n    'nthread': 4,\n    'verbose': 0,\n    'metric':'auc',     \n \n    'learning_rate': 0.15,\n    'num_leaves': 7,  # 2^max_depth - 1\n    'max_depth': 3,  # -1 means no limit\n    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n    'max_bin': 100,  # Number of bucketed bin for feature values\n    'subsample': 0.7,  # Subsample ratio of the training instance.\n    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n    'scale_pos_weight':99\n}\n\nevals_results = {}\nnum_boost_round = 250\nearly_stopping_rounds = 30\n\nprint('about to train')\nbooster = lgb.train(\n     lgb_params, \n     lgbtrain, \n     valid_sets=[lgbtrain], \n     valid_names=['train'], \n     evals_result=evals_results, \n     num_boost_round=num_boost_round,\n     early_stopping_rounds=early_stopping_rounds,\n     verbose_eval=1)\nprint('trained')\n\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53008b24-8984-4f1a-8ec2-12056c40ecd6","_uuid":"cca0a4d9ed3f68a5e3b37a07da34757129c7372e","collapsed":true,"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['click_id'] = test.click_id.astype('uint32')\nprint('about to predict')\nsubmission['is_attributed'] = booster.predict(test[predictors])\nprint('predicted')\nprint(submission.head())\nprint('about to write')\nsubmission.to_csv('sub.csv', index=False, float_format='%.10f')\nprint('written successfully')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}