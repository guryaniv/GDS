{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}}, "nbformat_minor": 1, "cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "40e61d60-c327-4480-b6e6-a6bc88bc10b3", "_uuid": "a2cf61f8bd08737bf0db912aa6640be877524a19", "collapsed": true}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "521c959d-ac20-4611-8f86-3529c7006588", "_uuid": "5dfde467ad051690b3c42c635ae61403759d9297", "collapsed": true}, "source": ["%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "\n", "import lightgbm as lgb\n", "import xgboost as xgb\n", "from sklearn.model_selection import cross_val_predict\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import log_loss, accuracy_score\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.svm import SVC\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "import nltk\n", "import re\n", "from nltk.corpus import stopwords\n", "import os"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "239282a6-4e44-4af3-8b1d-b85ff689f7e7", "_uuid": "02c1e884ae657acf6a29f2f0962d9a7560deca04", "collapsed": true}, "source": ["from random import shuffle\n", "from keras.wrappers.scikit_learn import KerasClassifier\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n", "from keras.utils import np_utils\n", "from keras.preprocessing import text, sequence"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "7a6ae9e0-8b2d-4f49-8713-964f364fc286", "_uuid": "29209129b5b4388af57f4fbb3a58244ff8dea001", "collapsed": true}, "source": ["df_train_txt = pd.read_csv('../input/training_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_train_var = pd.read_csv('../input/training_variants')\n", "df_test_txt = pd.read_csv('../input/test_text', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "df_test_var = pd.read_csv('../input/test_variants')\n", "training_merge_df = df_train_var.merge(df_train_txt,left_on=\"ID\",right_on=\"ID\")\n", "testing_merge_df = df_test_var.merge(df_test_txt,left_on=\"ID\",right_on=\"ID\")"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "9e5b667a-d686-41f8-8710-c94d0eec22f7", "_uuid": "258e66236cc49724a017ff27959ebdd27ffe1a6b", "collapsed": true}, "source": ["def textClean(text):\n", "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n", "    text = text.lower().split()\n", "    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n", "             'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n", "             'them', 'when', 'will', 'that', 'its', 'because', \n", "             'while', 'those', 'my', 'don', 'again', 'her', 'if',\n", "             'further', 'now', 'does', 'against', 'won', 'same', \n", "             'a', 'during', 'who', 'here', 'have', 'in', 'being', \n", "             'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n", "             'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n", "             'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n", "             'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n", "             'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n", "             'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n", "             'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n", "             'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n", "             'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n", "             'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n", "             'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n", "             'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n", "             'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n", "             'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n", "             'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n", "             'o', 'before'}\n", "    text = [w for w in text if not w in stops]    \n", "    text = \" \".join(text)\n", "    text = text.replace(\".\",\" \").replace(\",\",\" \")\n", "    return(text)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "efc7c780-2db2-47e8-9676-02a31004b238", "_uuid": "56a8d29ef45b876959b9c8bfe06966cd0cbdf6df", "collapsed": true}, "source": ["trainText = []\n", "for it in training_merge_df['Text']:\n", "    newT = textClean(it)\n", "    trainText.append(newT)\n", "testText = []\n", "for it in testing_merge_df['Text']:\n", "    newT = textClean(it)\n", "    testText.append(newT)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "9ac7d2ec-9f50-4050-9c88-4ef0573f7265", "_uuid": "4f3c715d075d41e9bb62300dd870b92d09f6a550", "collapsed": true}, "source": ["%%time\n", "count_vectorizer = TfidfVectorizer(ngram_range=(1,1), max_df=0.65,\n", "                        tokenizer=nltk.word_tokenize,\n", "                        strip_accents='unicode',\n", "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                        use_idf=True, smooth_idf=True, sublinear_tf=False, \n", "                        stop_words = 'english')\n", "bag_of_words = count_vectorizer.fit_transform(trainText)\n", "print(bag_of_words.shape)\n", "X_test = count_vectorizer.transform(testText)\n", "print(X_test.shape)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "7dc7750b-b3da-409c-ac6d-5a59625980bb", "_uuid": "9e836bc8bfc202fab905c38b8b9e0b3b00861ac6", "collapsed": true}, "source": ["%%time\n", "transformer = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False)\n", "transformer_bag_of_words = transformer.fit_transform(bag_of_words)\n", "X_test_transformer = transformer.transform(X_test)\n", "print (transformer_bag_of_words.shape)\n", "print (X_test_transformer.shape)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "38ed39a8-24db-476b-8679-9f984e6af368", "_uuid": "cf843f01a7eb9923178e57758c8ce947b6253016", "collapsed": true}, "source": ["train_y = training_merge_df['Class'].values\n", "label_encoder = LabelEncoder()\n", "label_encoder.fit(train_y)\n", "encoded_y = np_utils.to_categorical((label_encoder.transform(train_y)))"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "77becdcc-db80-48d8-9b9c-e07ece3e156e", "_uuid": "e6a37e8a4d097ee38bbecc70ce8530463384b05f", "collapsed": true}, "source": ["one_hot_gene = pd.get_dummies( np.hstack((training_merge_df['Gene'].values,testing_merge_df['Gene'].values)))\n", "one_hot_variation = pd.get_dummies( np.hstack((training_merge_df['Variation'].values,testing_merge_df['Variation'].values)))"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "f67381a9-1abc-44ae-8ed0-b46ba9f5ca9f", "_uuid": "f9f6c055505d4b7716c8112f799464305488eaea", "collapsed": true}, "source": ["from scipy.sparse import hstack"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "4f7b3853-3ccb-4ede-a985-5022afc5f2fb", "_uuid": "b06ebb12eaa431f068eda0669d5321b106a094a1", "collapsed": true}, "source": ["# define model\n", "def baseline_model():\n", "    model = Sequential()\n", "    model.add(Dense(512, input_dim=transformer_bag_of_words.shape[1]+one_hot_gene.shape[1]+one_hot_variation.shape[1], init='normal', activation='relu'))\n", "    model.add(Dropout(0.15))\n", "    model.add(Dense(512, init='normal', activation='relu'))\n", "    model.add(Dropout(0.15))\n", "    model.add(Dense(512, init='normal', activation='relu'))\n", "    model.add(Dropout(0.15))\n", "    model.add(Dense(512, init='normal', activation='relu'))\n", "    model.add(Dense(256, init='normal', activation='relu'))\n", "    model.add(Dense(64, init='normal', activation='relu'))\n", "    model.add(Dense(9, init='normal', activation=\"softmax\"))\n", "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy'])\n", "    return model\n", "\n", "\n", "estimator = KerasClassifier(build_fn=baseline_model, epochs=15, batch_size=64)\n", "estimator.fit(hstack((one_hot_gene[:training_merge_df.shape[0]], one_hot_variation[:training_merge_df.shape[0]], transformer_bag_of_words)).todense(), encoded_y, validation_split=0.05)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "dfecfcf3-93f7-430d-8eb8-ebce45bf0d2e", "_uuid": "03d028f81d04cc607423abd93ca3d0311e78da24", "collapsed": true}, "source": ["%%time\n", "results = estimator.predict_proba(hstack((one_hot_gene[training_merge_df.shape[0]:], one_hot_variation[training_merge_df.shape[0]:], X_test_transformer)).todense())"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "03a4ed07-d6a1-40da-a052-71559ef33dcb", "_uuid": "1216aec8bedc4731cfcebccc3e63486f26a07174", "collapsed": true}, "source": ["results_df = pd.read_csv(\"../input/submissionFile\")\n", "for i in range(1,10):\n", "    results_df['class'+str(i)] = results.transpose()[i-1]\n", "results_df.to_csv('output_tf_keras_version2',sep=',',header=True,index=None)\n", "results_df.head()"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "25b23162-676d-441d-9af6-bc22e6ce7603", "_uuid": "d98e95abde257056e9d36eee0e7c16c19efb89e6", "collapsed": true}, "source": [], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "47e2b727-254e-42f8-89d8-8843388196b8", "_uuid": "3e46176fd2249e84de710ec1cc1be7ca8742c87a", "collapsed": true}, "source": [], "outputs": []}], "nbformat": 4}