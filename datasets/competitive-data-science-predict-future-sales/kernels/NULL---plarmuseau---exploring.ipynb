{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n# Import all of them \nsales=pd.read_csv(\"../input/sales_train.csv\")\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nitem_cat=pd.read_csv(\"../input/item_categories.csv\")\nitem=pd.read_csv(\"../input/items.csv\")\nsub=pd.read_csv(\"../input/sample_submission.csv\")\nshops=pd.read_csv(\"../input/shops.csv\")\ntest=pd.read_csv(\"../input/test.csv\")\nshops.describe().T","execution_count":46,"outputs":[]},{"metadata":{"_cell_guid":"0258b7a8-b15b-49d0-9992-5116a33dbd35","_uuid":"cb4ceadf50b3e787c6bca5e2a6f1823a3b1dfdc7","trusted":true},"cell_type":"code","source":"Xtrain.head()","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"369138a1-e743-4562-8d33-0893624acf37","_uuid":"79b7aa5909d844f0fbdf9d82762c5aeee3620737","trusted":true},"cell_type":"code","source":"test.describe().T","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"f1b360fe81b04026947b5f3ef7103bbd163c5918"},"cell_type":"markdown","source":"# price * sales = turnover volume\n"},{"metadata":{"_cell_guid":"6caf548f-0be9-4c00-8367-c507f6621e8e","_uuid":"c208b64b8540ee038ca6b867b5d0955037a1c0ed","trusted":true},"cell_type":"code","source":"sales['vol']=sales['item_price']*sales['item_cnt_day']\nsales.describe().T","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"ec997e4dc075d760751f6f38abf9a1073e52136c"},"cell_type":"markdown","source":"# add category"},{"metadata":{"_cell_guid":"94984873-3318-4b9a-b7f2-62767cc5ff25","_uuid":"d86a37eb001a4b757d39077ab2e5b88ba4378bff","collapsed":true,"trusted":true},"cell_type":"code","source":"Train=pd.merge(sales, item, how='inner', on='item_id')","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"e92511a1-485e-44fb-96ba-0c01670e69e0","_uuid":"0822d33aebdb2be6845e33d008dba2380d225372","trusted":true},"cell_type":"code","source":"Train.describe().T","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"40ae1294bd9ff611735f726f9d158104074df3a5"},"cell_type":"markdown","source":"# pivot table to create product sales stats per month"},{"metadata":{"_cell_guid":"518c1aac-19dc-4714-87d4-3cb301da3e09","_uuid":"a4da6bf702f563e8b97b52b70c77082b8427228e","collapsed":true,"trusted":true},"cell_type":"code","source":"Xtrain=pd.pivot_table(sales, values='item_cnt_day', index=['shop_id', 'item_id'],columns=['date_block_num'], aggfunc=np.sum,fill_value=0)","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"62fc9737-f58e-40ce-a716-5ec4f4e865d9","_uuid":"52906fa687deb7aae4ebc312c523654d5adad0a3"},"cell_type":"markdown","source":"# declining number of product sales\n\n-50%\n\n# seasonal 12mont sales\n"},{"metadata":{"_cell_guid":"14e13786-2fc2-4196-9331-6f84290ad374","_uuid":"88534baaae7563e2193bae15cc4ada864a778db1","trusted":true},"cell_type":"code","source":"# library & dataset\nimport seaborn as sns\ndf = Xtrain.sum()\nprint(list(df))\n\n# use the function regplot to make a scatterplot\nsns.regplot(x=np.array(list(df.index)), y=np.array(list(df)), fit_reg=True)\n\nverkoop=Xtrain.sum()\n\nfor i in range(len(verkoop)-11):\n    average = sum(verkoop[i:i+12]) / 12\n    decline = sum(verkoop[i:i+12]) / (12.0*130227.0)*100 - 100.0 \n    print(i,\"-\",i+12,\"%.0f\" % average,\"%.2f\" % decline,'%')\n    \nprint( 'decline 32%/22 * 34 ', 32/22*34)","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"14a82370-d74e-47c7-bdee-33293f3e14b9","_uuid":"942f7e3584f2c75960bbb842b4efd27ca4805e0b"},"cell_type":"markdown","source":"# Correlogram top ten categories - sales volume\n\nthe top ten categories, and their relation in function of the monthly volumes is increasing, so they are drinving their sales volume with the top categories\n\ncat_month.sort_values(0)[-10:].loc[:,:12]"},{"metadata":{"_cell_guid":"3d510830-f623-4f0e-96b3-1441e3139bfc","_uuid":"edc261a86ed530a7e70b683df6fbd0914bfa00c1","trusted":true},"cell_type":"code","source":"cat_month=pd.pivot_table(Train, values='vol', index=['item_category_id'],columns=['date_block_num'], aggfunc=np.sum,fill_value=0)\n\nfor xi in range(12):\n    if xi+24<33:\n        cat_month[xi]=( cat_month[xi]+cat_month[xi+12]+cat_month[xi+24] )/3\n    else:\n        cat_month[xi]=( cat_month[xi]+cat_month[xi+12] )/2\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \n# with regression\nsns.pairplot(cat_month.sort_values(0)[-10:].loc[:,:12], kind=\"reg\")\nplt.show()\n","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"9217d01c411d670af90670c085719fdd74cf3b8f"},"cell_type":"markdown","source":"# pivot turnover per product"},{"metadata":{"_cell_guid":"252f3f0a-f17f-456b-8990-8db09b7cd77e","_uuid":"27a233d4a95331a7feb9b19e6f66f146c1e6d423","collapsed":true,"trusted":true},"cell_type":"code","source":"Ytrain=pd.pivot_table(sales, values='vol', index=['shop_id', 'item_id'],columns=['date_block_num'], aggfunc=np.sum,fill_value=0)","execution_count":55,"outputs":[]},{"metadata":{"_cell_guid":"552431ab-1eab-41c2-8ed4-f52a2cd994e6","_uuid":"0cae134f0b56d8e6e7ab43b00925356b4675419b"},"cell_type":"markdown","source":"# Constant Sales Volume\n\nthere was an 12% increase and then the sales volume decreased again"},{"metadata":{"_cell_guid":"35b696e2-67d3-4023-a718-0bf45e5702d3","_uuid":"47db4ab58ce229680f853a920fbbfc93e1065bca","trusted":true},"cell_type":"code","source":"# library & dataset\nimport seaborn as sns\ndf = Ytrain.sum()\nprint(list(df))\n\n# use the function regplot to make a scatterplot\nsns.regplot(x=np.array(list(df.index)), y=np.array(list(df)), fit_reg=True)\nplt.show()\n\nverkoop=Ytrain.sum()\n\nfor i in range(len(verkoop)-11):\n    average = sum(verkoop[i:i+12]) / 12\n    decline = sum(verkoop[i:i+12]) / (12.0*101460394)*100 - 100.0 \n    print(i,\"-\",i+12,\"%.0f\" % average,\"%.2f\" % decline,'%')\n    \n","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"aa6d664d12a23ed96305837ea0fff1db346a2643"},"cell_type":"markdown","source":"# lets do some ARIMA forecasting\n\n* 12month seasonal effect,thats clear\n* best models have AIC 300 and are of this type  ARIMA(1, 0, 0)x(1, 1, 0, 12)12\n"},{"metadata":{"_cell_guid":"3f6e830c-95ab-45df-8c4e-f8963346925a","_uuid":"ca90320b6f03bc20680e362f39dbcdff8501ac0c","trusted":true},"cell_type":"code","source":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(verkoop, nlags=120)\nlag_pacf = pacf(verkoop, nlags=12, method='ols')\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(verkoop)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(verkoop)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(verkoop)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(verkoop)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","execution_count":57,"outputs":[]},{"metadata":{"_cell_guid":"7f9256a1-5382-4c22-bf10-975e52ad1e74","_uuid":"062d34aaa4d76f71aa90983e13a07c67252d5d8b","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\nimport itertools\nimport statsmodels.api as sm\nvrkp=verkoop.diff().fillna(0).values\n\n# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\nwarnings.filterwarnings(\"ignore\")\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(vrkp,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue\n            \n\nmod = sm.tsa.statespace.SARIMAX(vrkp,\n                                order=(1, 1, 0),\n                                seasonal_order=(1, 1, 0, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\n\nprint(results.summary().tables[1])\n\nmodel = sm.tsa.statespace.SARIMAX(vrkp, order=(1, 1, 1),seasonal_order=(1, 1, 0, 12),enforce_stationarity=False,enforce_invertibility=False )  \nresults_AR = model.fit(disp=-1)  \nplt.plot(vrkp)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-vrkp)**2))\n\nresults.plot_diagnostics(figsize=(10, 8))\nplt.show()\n","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"da8fe663-4f8a-46b3-9386-3769872d55c4","_uuid":"95a13af465686fc698f039a73a95a8190dbd2f3f","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10eac744-d5b1-429d-8592-80f2df28a363","_uuid":"927c1e73dc6f2febd4f474da63ebaf28a99db524","trusted":true},"cell_type":"code","source":"#corr verkoop producten, comarketing\nmcor=Ytrain.corr()\nmcor=np.reshape(mcor,(34,34))\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\n\n# Make data.\nX = np.arange(0, 34, 1)\nY = np.arange(0,34,1)\nX, Y = np.meshgrid(X, Y)\nZ = mcor\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\n\n# Customize the z axis.\nax.set_zlim(0, 1)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=23)\n\nplt.show()","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"fddcfc336941cd5851a3bfcf983614ffa8f5922b"},"cell_type":"markdown","source":"# classification script"},{"metadata":{"_cell_guid":"e4f9dc72-e5a6-4cd3-a81e-0497457093e2","_uuid":"0691a5396c601e63733551cb4a953a5128a48bc7","collapsed":true,"trusted":true},"cell_type":"code","source":"def Klasseer(Mtrain,Mtest,Mlabel,klas,rank,start):\n    #data preparation\n    #print(Mtotal)\n    #Mtotal=Mtotal.fillna(-1)\n    #print(Mtotal)\n    #Mtrain=Mtotal[Mtotal[labelveld]!=-1]\n    #Mtest=Mtotal[Mtotal[labelveld]==-1]\n    #Mtest=Mtest.drop(labelveld,axis=1)\n    Mlabel=pd.DataFrame( Mlabel.values,columns=['label'] )  #[:len(Mtrain)]\n    labelveld='label'\n    print('shapes train',Mtrain.shape,'label',Mlabel.shape,'test',Mtest.shape)\n\n    \n    #totalA=Mtrain.append(Mtest)\n    totalA=np.concatenate((Mtrain,Mtest), axis=0)\n    predictionA=pd.DataFrame(Mlabel,columns=[labelveld])    \n    #totalA=totalA.drop(labelveld,axis=1)\n    #print(totalA.shape,predictionA.shape)\n    #print(prediction)\n    #faze 1\n    # dimmension reduction\n    from scipy.spatial.distance import cosine\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.preprocessing import Normalizer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, log_loss\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC, LinearSVC, NuSVC\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier,Perceptron\n\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    \n    for ira in range(rank-start,rank+1):\n        print('****20% sample test==',ira)\n        #Ulsa = lsa.fit_transform(Mtrain.values/255)  #train version\n        #print(total)\n        if ira!=0:\n            if ira<len(totalA.T):\n                print(\"lsa dimmension reduction\")                \n                svd = TruncatedSVD(ira)\n                normalizer = Normalizer(copy=False)\n                lsa = make_pipeline(svd, normalizer)\n                UlsA = lsa.fit_transform(totalA) #total version\n                explained_variance = svd.explained_variance_ratio_.sum()\n                print(\"Explained variance of the SVD step knowledge transfer: {}%\".format(\n                    int(explained_variance * 100)))    \n\n            else:\n                print(\"no reduction\")\n                UlsA=totalA\n        else:\n            print(\"3D-SVD dimmension reduction\")\n            u,s,vh=np.linalg.svd(totalA)\n            print(u.shape, s.shape, vh.shape)\n            UlsA=np.reshape(u, (len(totalA),28*28))            \n        #    UlsA = totalA\n        #    print(\"no LSA reduction\")\n        print('ulsa',UlsA.shape)\n\n\n        #faze2\n        #training model\n\n        #sample\n        samlen=int(len(Mlabel)/1)\n        X_train, X_test, y_train, y_test = train_test_split(UlsA[:samlen], Mlabel[:samlen],stratify=Mlabel[:samlen], test_size=0.5)\n        print(\"test on 20% sample\")\n        \n        if klas=='Logi':\n            classifiers = [\n    #    SVC(kernel=\"rbf\", C=0.025, probability=True),  20%\n    #    NuSVC(probability=True),\n                LogisticRegression(),\n                 ]\n        if klas=='Quad':\n            classifiers = [\n                QuadraticDiscriminantAnalysis(),\n                 ]           \n        if klas=='Rand':\n            classifiers = [\n                RandomForestClassifier(84),\n                 ]               \n        if klas=='Extr':\n            classifiers = [\n                ExtraTreesClassifier(verbose=1,n_jobs=3),\n                 ]             \n        if klas=='Adab':\n            classifiers = [\n                AdaBoostClassifier(),\n                 ]            \n        if klas=='Deci':\n            classifiers = [\n                DecisionTreeClassifier(),\n                 ]\n        if klas=='Grad':\n            classifiers = [\n                GradientBoostingClassifier(),\n                 ]            \n        if klas=='KNN':\n            classifiers = [\n                KNeighborsClassifier(n_jobs=4),  \n                 ]            \n        if klas=='Line':\n            classifiers = [\n                LinearDiscriminantAnalysis(), \n                 ]  \n        if klas=='Gaus':\n            classifiers = [\n                GaussianNB(),\n                 ] \n        if klas=='Perc':\n            classifiers = [\n                Perceptron(),\n                 ]      \n        if klas=='Elas':\n            classifiers = [\n                ElasticNet(random_state=0),\n                 ]                 \n    # Logging for Visual Comparison\n        log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n        log = pd.DataFrame(columns=log_cols)\n    \n        for clf in classifiers:\n            clf.fit(X_train,y_train)\n            name = clf.__class__.__name__\n        \n            print(\"=\"*30)\n            print(name)\n            \n            #print('****Results****')\n            train_predictions = clf.predict(X_test)\n            acc = accuracy_score(y_test, train_predictions)\n            print(\"Accuracy: {:.4%}\".format(acc))\n        \n            train_predictions = clf.predict_proba(X_test)\n            ll = log_loss(y_test, train_predictions)\n            print(\"Log Loss: {}\".format(ll))\n            \n            log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n            log = log.append(log_entry)\n    \n        print(\"=\"*30)\n\n    print('*** train complete set==',UlsA[:len(Mlabel)].shape)\n     \n    clf.fit(UlsA[:len(Mlabel)],Mlabel)\n    #on complete trainset\n\n    #pr2=pd.DataFrame(clf.predict_proba(Ulsa),index=list(range(0,len(Ulsa),1)))\n\n    predictionA=pd.DataFrame(clf.predict(UlsA),columns=['pred'],index=range(0,len(UlsA)))\n    predictionA[labelveld]=Mlabel \n    print('predict',predictionA.shape)\n    predictionA.fillna(-1)\n    predictionA['diff']=0\n    predictionA['next']=Mlabel\n    #abs(prediction[labelveld]-prediction['pred\n    collist=sorted( Mlabel.label.unique() )\n\n    print(collist)\n    if klas=='Logi':\n        predictionA[collist] = pd.DataFrame(clf.predict_log_proba(UlsA))\n    if klas!='Logi':\n        print(UlsA.shape)\n        temp=pd.DataFrame(clf.predict_proba(UlsA))\n        print(temp.shape)\n        predictionA[collist]=temp\n    \n    from sklearn.metrics import classification_report, confusion_matrix\n    true_labels=predictionA[labelveld][:len(Mtrain)].values.astype('float32')\n    predicted_labels = predictionA['pred'][:len(Mtrain)].values.astype('float32')\n\n    cm = confusion_matrix(true_labels, predicted_labels,labels=collist)\n    print(classification_report(true_labels, predicted_labels))\n    print(\"Confusion matrix\")\n    print(cm)\n    \n    corr=predictionA.drop(['pred','diff'],axis=1).corr()\n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(abs(corr), mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)\n    predictionA=predictionA.fillna('0')\n    #print('Prediction',prediction.head())\n    pred2=predictionA.drop(['pred',labelveld,'diff','next'],axis=1)\n    \n    print(predictionA.shape)\n\n\n    return predictionA #['next']","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"ca335ed0-5d9a-4642-bcdd-b6e612fb16c1","_uuid":"d88b5d446e61498234e1a5ba3418ab71d6cc7242"},"cell_type":"markdown","source":"# problem\nclasses where frequency is only 1\nso find them, and double them"},{"metadata":{"_cell_guid":"3a99589c-2786-4d70-9903-9c5b7df494a6","_uuid":"1f77d59b957044c5f4e52b935193ffe04832388d","trusted":true},"cell_type":"code","source":"train1=pd.DataFrame([])\nfor xi in range(3000):\n    lengte=len( Xtrain[Xtrain[33]==xi] )\n    if lengte==1:\n        train1=train1.append(Xtrain[Xtrain[33]==xi])\ntrain1","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"66d786a796bbd894543c9714c3b1e78f55fea1b4"},"cell_type":"markdown","source":"# merge the Xtest with the pivot Sales data\n\n* one of the problems is that 50% of the numbers don't have sales data, so probably the sales could be 0, maybe there are products where the sales can start... there we need to spillover knowhow from one shop to another"},{"metadata":{"trusted":true,"_uuid":"160ddeedad37a70a0dbf54bc89640a6df9f56e8e"},"cell_type":"code","source":"Xtest.reset_index().merge(Xtrain.reset_index(),how='left')","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"317e7e348332df685ce7a0fd2d5893cd4d0f77bd"},"cell_type":"markdown","source":"# Using the ARIMA knowhow\n\n* we found that there is a 12month lag, and a month lag\n* hence we need to know the 12 month difference, and the month difference\n* we add those columns to the training data"},{"metadata":{"_cell_guid":"6ccb9cb2-2a1d-4f43-aa92-09b17e9e3296","_uuid":"fb22874a85b3da49e164131fd19d186625acf2f5","trusted":true},"cell_type":"code","source":"Ttrain=Xtrain.append(train1)\n#append those unique double \n#Xtrain.diff(periods=12,axis=1).dropna(axis=1)\nAtrain=Ttrain.T.append(Ttrain.T.diff(periods=12).dropna() )\nAtrain=Atrain.append(Ttrain.T.diff(periods=1).dropna() )\nAlabel=pd.DataFrame(Ttrain.index,columns=['prid'])\n\n#Atrain.T\n\n\n","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"cb2d3dde-b41c-42c6-b16d-89cbc36e5c71","_uuid":"ca63f2132d04cacf05baeaf82f64f66fd2b2aae3","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"Klasseer(Atrain.fillna(0).T.values,Atrain.T[400000:].values,Ttrain[33],'Extr',40,0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}