{"nbformat_minor": 1, "cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "14d12b8f6e695864a14d404d3de8e0374cc06994", "_cell_guid": "7eb55977-219b-47d9-b1b5-6c34fe97fe74"}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e5415beaba2997b542af15d9341a09a354825f6d", "_cell_guid": "16164bb7-bb03-4ef9-8195-7c407e52e73c"}, "outputs": [], "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "print ('There are {0} rows and {1} attributes.'.format(train.shape[0], train.shape[1]))\n", "print ('There are {0} rows and {1} attributes.'.format(test.shape[0], test.shape[1]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e8b4bcc4d11b0292b208bff1899c9b80640cba57", "_cell_guid": "a4cbfc6b-c2da-4d3f-aa7a-3eae724b28fb"}, "outputs": [], "source": ["train.info()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "348b247c08b11cbdda957e14c2f9ca26a257c364", "_cell_guid": "86309d43-4d16-450c-a58c-c389b417bef2", "collapsed": true}, "outputs": [], "source": ["if len(train['id'].unique()) == train.shape[0]:\n", "    train.set_index(['id'], drop=False)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "43a5a934e3f8917725f8f12ae7e92f44295c0b7c", "_cell_guid": "2cfdde3f-6dc7-4ed0-a65f-2415e600a9df", "collapsed": true}, "outputs": [], "source": ["train_by_author = train.groupby('author')['text'].agg(lambda col: ''.join(col))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b97c3d897e74bb33fa42135c4618303961789ead", "_cell_guid": "baae5765-456e-4e6e-8067-4129aa37d90c"}, "outputs": [], "source": ["import os, re, math\n", "from collections import Counter\n", "from nltk.corpus import stopwords\n", "\n", "def tokenizer(contents):\n", "#     tokens = [] \n", "    parse = contents.replace('\"','').replace(',','').replace('.','')\n", "    tokens = re.sub('[^a-zA-Z0-9]', ' ', parse)\n", "    tokens = tokens.lower().split()\n", "    # remove remaining tokens that are not alphabetic\n", "    tokens = [word for word in tokens if word.isalpha()]\n", "    # filter out stop words\n", "    stop_words = set(stopwords.words('english'))\n", "    tokens = [w for w in tokens if not w in stop_words]\n", "    # filter out short tokens\n", "    tokens = [word for word in tokens if len(word) > 2]\n", "    return tokens\n", "   \n", "EAP = tokenizer(train_by_author['EAP'])\n", "MWS = tokenizer(train_by_author['MWS'])\n", "HPL = tokenizer(train_by_author['HPL'])\n", "\n", "vocab_EAP = Counter(list(EAP))\n", "vocab_MWS = Counter(list(MWS))\n", "vocab_HPL = Counter(list(HPL))\n", "\n", "# print the size of the vocab\n", "print ('EAP: {0}, MWS: {1}, HPL: {2} \\n\\n'.format(len(vocab_EAP), len(vocab_MWS), len(vocab_HPL)))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1718fe36e16b6812b92949b8c068ab6b66c27c70", "_cell_guid": "c2fa4f7e-e54f-45a6-b948-5bb23f524178", "collapsed": true}, "outputs": [], "source": ["def naive_bayes_classifier(x):\n", "    count = 0\n", "    prob = 0\n", "    for val in x:\n", "        if count == 0:\n", "#             prob = (math.log(0.33)) + (math.log(val))\n", "            prob = 0.33 * val\n", "        else:\n", "#             prob = prob + (math.log(val))\n", "            prob = prob * val\n", "\n", "        count += 1\n", "        \n", "    return prob\n", "\n", "def frequency_prob(x, couter, contents, smoothing):\n", "    word_counts = []\n", "    for word in x:\n", "        word_counts.append((couter.setdefault(word, 0) + (math.pow(10, -200))) / (len(contents) + smoothing))\n", "        \n", "    return naive_bayes_classifier(word_counts)\n", "\n", "sv_EAP = len(vocab_EAP) * (math.pow(10, -200))\n", "sv_MWS  = len(vocab_MWS) * (math.pow(10, -200))\n", "sv_HPL = len(vocab_HPL) * (math.pow(10, -200))\n", "    \n", "train['sentiment'] = train.apply(lambda row: tokenizer(row['text']), axis=1)\n", "train['EAP'] = train.apply(lambda row: frequency_prob(row['sentiment'], vocab_EAP, EAP, sv_EAP), axis=1) \n", "train['MWS'] = train.apply(lambda row: frequency_prob(row['sentiment'], vocab_MWS, MWS, sv_MWS), axis=1) \n", "train['HPL'] = train.apply(lambda row: frequency_prob(row['sentiment'], vocab_HPL, HPL, sv_HPL), axis=1) "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c3399adecac15a0a0ed2c2f1ed87afeeb74b2841", "_cell_guid": "fab5216f-4060-4cfb-9361-d2e4490f8ba4", "collapsed": true}, "outputs": [], "source": ["def classifier(x, y, z):\n", "    max_value = max(x, y, z)\n", "    if x == max_value:\n", "        return 'EAP'\n", "    if y == max_value:\n", "        return 'MWS'\n", "    else:\n", "        return 'HPL'\n", "\n", "train['classifier'] = train.apply(lambda row: classifier(row['EAP'], row['MWS'], row['HPL']), axis=1)"]}, {"execution_count": null, "cell_type": "code", "metadata": {}, "outputs": [], "source": ["# need to figure it out how to put into right probabilistic values (sum to 1)\n", "train[['EAP', 'MWS', 'HPL']].head(20)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "93ece166f25a40c5d07c38735d2273a417408d43", "_cell_guid": "2abcc326-cad5-4b57-ae24-822a86afae65"}, "outputs": [], "source": ["train['correctly_classified'] = np.where(train['author'] == train['classifier'], 1, 0)\n", "ans = train[['correctly_classified']].values.sum()\n", "print ('The accuracy of NB classifier model is {0}/{1}={2}.'.format(ans,\n", "                                                                   train.shape[0],\n", "                                                                   100*ans/train.shape[0]))"]}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}