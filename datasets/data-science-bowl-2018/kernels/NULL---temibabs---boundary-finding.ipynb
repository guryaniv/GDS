{"cells":[{"metadata":{"_uuid":"7d721db9016746c3714088b8ae51aa4337add18e","_cell_guid":"73ba0a7f-e61f-418d-b72b-598a210f0a61","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\n# from skimage.transform import AffineTr\nfrom random import randint, choice, shuffle\nfrom math import ceil\nimport matplotlib.pyplot as plt\nimport sys\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"402535c8e3d5361362504bb53ea31ca92c6b4325","_cell_guid":"00578239-0d5d-4b9f-b1d2-005aab24edfc","collapsed":true,"trusted":false},"cell_type":"code","source":"IMG_HEIGHT = 256\nIMG_WIDTH = 256\nIMG_CHANNELS = 3\nbatch_size = 16\nepoch = 50\ndata_path = '../input'\ntrain_ids = next(os.walk(data_path + '/stage1_train/'))[1]\nshuffle(train_ids)\ntest_ids = next(os.walk(data_path + '/stage1_test/'))[1]\nX = tf.placeholder(tf.float32, shape=(batch_size, IMG_HEIGHT,\n                IMG_WIDTH, IMG_CHANNELS))\ny = tf.placeholder(tf.float32, shape=(batch_size, IMG_HEIGHT,\n                IMG_WIDTH, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"312fbe1f09393076633ed82898a557d47da61aad","_cell_guid":"12369cc9-e419-4128-b6f8-fb9c715a7b7e","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_data(id_):\n    path = data_path + '/stage1_train/' + id_ \n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '/masks/'))[2]:\n        mask_ = imread(path + '/masks/' + mask_file)\n        res = resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n                                  preserve_range=True)\n        mask_ = np.expand_dims(res, axis=-1)\n        mask = np.maximum(mask, mask_)\n    return img, mask","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cca208d17f3e7ff80b50a5e4bb0b28792a55163d","_cell_guid":"253e9674-f825-412d-a0db-8de099dfae06","collapsed":true,"trusted":false},"cell_type":"code","source":"def convolution(x, y_dim, k_h=3, k_w=3, s_h=1, s_w=1, stddev=0.02,\n                activation=tf.nn.relu, name='conv2d'):\n    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n        w = tf.get_variable('w', [k_h, k_w, x.get_shape()[-1], y_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(x, w, strides=[1, s_h, s_w, 1], padding='SAME')\n        biases = tf.get_variable('biases', [y_dim],\n                                 initializer=tf.constant_initializer(0.0))\n        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n        if activation:\n            return activation(conv)\n        return conv\n    \ndef deconvolution(x, output_shape, k_h=2, k_w=2, s_h=2, s_w=2, std_dev=0.015,\n                  activation=tf.nn.relu, name='deconv2d'):\n    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n        w = tf.get_variable('weight', [k_h, k_w, output_shape[-1], x.get_shape()[-1]],\n                            tf.float32, initializer=tf.truncated_normal_initializer(stddev=std_dev))\n        deconv = tf.nn.conv2d_transpose(x, w, output_shape, [1, s_h, s_w, 1])\n        b = tf.get_variable('biases', [output_shape[-1]], \n                            initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, b), deconv.get_shape())\n        if activation:\n            return activation(deconv)\n        return deconv\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"587292d1d05c7e47c9a93d1b885168ad3c127ff2","_cell_guid":"8c00f763-0d2b-446f-980e-acb016a9873b","collapsed":true,"trusted":false},"cell_type":"code","source":"def build():\n    ## Sector 1\n    conv_1 = convolution(X, 16, name='conv_1')\n    drop_1 = tf.nn.dropout(conv_1, 0.06)\n    conv_2 = convolution(drop_1, 16, name='conv_2')\n    pool_1 = tf.nn.max_pool(conv_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n    \n    # Sector 2\n    conv_3 = convolution(pool_1, 32, name='conv_3')\n    drop_2 = tf.nn.dropout(conv_3, 0.06)\n    conv_4 = convolution(drop_2, 32, name='conv_4')\n    pool_2 = tf.nn.max_pool(conv_4, ksize=[1,1,1,1], strides=[1,2,2,1], padding='SAME')\n    \n    # Sector 3\n    conv_5 = convolution(pool_2, 64, name='conv_5')\n    drop_3 = tf.nn.dropout(conv_5, 0.06)\n    conv_6 = convolution(drop_3, 64, name='conv_6')\n    pool_3 = tf.nn.max_pool(conv_6, ksize=[1,1,1,1], strides=[1,2,2,1], padding='SAME')\n    \n    # Sector 4\n    conv_7 = convolution(pool_3, 128, name='conv_7')\n    drop_4 = tf.nn.dropout(conv_7, 0.06)\n    conv_8 = convolution(drop_4, 128, name='conv_8')\n    pool_4 = tf.nn.max_pool(conv_8, ksize=[1,1,1,1], strides=[1,2,2,1], padding='SAME')\n    \n    # Base\n    conv_9 = convolution(pool_4, 256, name='conv_9')\n    conv_10 = convolution(conv_9, 256, name='conv_10')\n    \n    # Sector 6\n    conv_11 = tf.concat([conv_8, deconvolution(conv_10,\n                                               [batch_size, 32, 32, 128], name='deconv_1')], 3)\n    conv_12 = convolution(conv_11, 128, name='conv_12')\n    conv_13 = convolution(conv_12, 128, name='conv_13')\n    \n    # Sector 7\n    conv_14 = tf.concat([conv_6, deconvolution(conv_13, \n                                               [batch_size, 64, 64, 64], name='deconv_2')], 3)\n    conv_15 = convolution(conv_14, 64, name='conv_15')\n    conv_16 = convolution(conv_15, 64, name='conv_16')\n    \n    # Sector 8\n    conv_17 = tf.concat([conv_4, deconvolution(conv_16,\n                                               [batch_size, 128, 128, 32], name='deconv_3')], 3)\n    conv_18 = convolution(conv_17, 32, name='conv_18')\n    conv_19 = convolution(conv_18, 32, name='conv_19')\n    \n    # Sector 9\n    conv_20 = tf.concat([conv_2, deconvolution(conv_19,\n                                               [batch_size, 256, 256, 16], name='deconv_4')], 3)\n    conv_21 = convolution(conv_20, 16, name='conv_21')\n    conv_22 = convolution(conv_21, 16, name='conv_22')\n    conv_23 = convolution(conv_22, 1, k_h=1, k_w=1,\n                          activation=tf.nn.sigmoid)\n    print ('conv_23 has shape: ', conv_23.shape)\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=conv_23, labels=y))\n    return conv_23, loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d00a1f4e369675c27f1e7ceecb6c21088d392805","_cell_guid":"b0df75eb-d23f-4de1-b52a-b7859ec026f5","collapsed":true,"trusted":false},"cell_type":"code","source":"def val(final):\n    with tf.Session():\n        final.eval()\n\ndef train(loss):\n    generator, g_loss = build()\n    with tf.variable_scope('optim', reuse=tf.AUTO_REUSE):\n        t_vars = tf.trainable_variables()\n        trainer = tf.train.AdamOptimizer(0.01).minimize(loss, var_list=t_vars)\n    input_ = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), \n                    dtype=np.uint8)\n    output_ = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1),\n                           dtype=np.uint8)\n    for i in range(0, len(train_ids)):\n        data = get_data(train_ids[i])\n        input_[i] = data[0]\n        output_[i] = data[1]\n    wtf_x = input_[0:16,:,:,:]\n    wtf_y = output_[0:16,:,:,:]\n        \n    with tf.Session() as sess:\n        def mean_iou(y_pred,y_true):\n            y_pred_ = tf.to_int64(y_pred > 0.5)\n            y_true_ = tf.to_int64(y_true > 0.5)\n            sess = tf.Session()\n            score, up_opt = tf.metrics.mean_iou(y_true_, y_pred_, 2)\n            with tf.control_dependencies([up_opt]):\n                score = tf.identity(score)\n                return score.eval(session=sess)\n        tf.global_variables_initializer().run()\n        for ep_id in range(0, epoch):\n            b_id = 0\n            batches = int(ceil(float(len(train_ids))/float(batch_size)))\n            for j in range(0, batches):\n                wtf_x = input_[b_id*batch_size:(b_id+1)*batch_size,:,:,:]\n                wtf_y = output_[b_id*batch_size:(b_id+1)*batch_size,:,:,:]\n                if j%16 == 0:\n                    s = randint(0,len(train_ids))\n                    wtf_x = input_[s:s+16]#\n                    wtf_y = output_[s:s+16]\n                    sht = logit.eval({X:wtf_x, y:wtf_y})\n                    print('validation: ', j/16 + 1, 'sht: ', tf.reduce_mean(sht).eval(), 'real: ', \n                          tf.reduce_mean(wtf_y).eval())\n                _ = sess.run(trainer,\n                             feed_dict={X: wtf_x,\n                                        y: wtf_y}\n                            )\n                ans = loss.eval({X:wtf_x, y:wtf_y})\n                \n                b_id += 1\n                print ('epoch: ', ep_id, ', loss: ', ans)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6778c27121d2778e8b4231e30dea8e822e42ba4","_cell_guid":"aca530b5-f029-4b3e-af8e-2a2e24786374","collapsed":true,"trusted":false},"cell_type":"code","source":"def mean_iou(y_pred,y_true):\n    y_pred_ = tf.to_int64(y_pred > 0.5)\n    y_true_ = tf.to_int64(y_true > 0.5)\n    sess = tf.Session()\n    score, up_opt = tf.metrics.mean_iou(y_true_, y_pred_, 2)\n    with tf.control_dependencies([up_opt]):\n        score = tf.identity(score)\n        tf.global_variables_initializer().run(session=sess)\n        return score.eval(session=sess)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bac02c28bca307b890756a922709970b33c70760","_cell_guid":"ac2528ae-19d5-416e-abe9-0728e3f52729","collapsed":true,"trusted":false},"cell_type":"code","source":"logit, loss = build()\ntrain(loss)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1}