{"nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.4", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "nbformat": 4, "cells": [{"source": ["# Wikipedia Traffic Forecasting\n", "## A starting kit\n", "**V. Croft, Y. Haddad**\n", "\n", "This is a very short kernel aiming to help on getting hands into time series analysis and forecasting. We will be using data from the [Wikipedia Traffic Forecasting challange](https://www.kaggle.com/c/web-traffic-time-series-forecasting) on Kaggle.\n", "\n", "This example is based on these kernels:\n", "* https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration\n", "* https://www.kaggle.com/screech/ensemble-of-arima-and-lstm-model-for-wiki-pages\n", "\n", "\n", "You can find this example on Github ([https://github.com/PythonDSGeneva/KaggleDojoTimeSeriesForcasting](https://github.com/PythonDSGeneva/KaggleDojoTimeSeriesForcasting))."], "cell_type": "markdown", "metadata": {}}, {"source": ["# Some basic libraries \n", "import pandas as pd\n", "import numpy as np\n", "from matplotlib import pyplot as plt\n", "import re\n", "%matplotlib inline"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["## Loading data"], "cell_type": "markdown", "metadata": {}}, {"source": ["df_train = pd.read_csv('./input/train_1.csv').fillna(0)\n", "df_train.head()"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["df_train.info()"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["## Feature Engineering\n", "\n", "### Language information\n", "\n", "We might reasonably assume different languages used in Wikipedia might affect the dataset. A simple regular expression can be used to search for the language code in the wikipedia URL. There are also a number of non-wikipedia URLs that will fail the regex search. These are wikimedia pages, so we give them the code 'na' since we can't determine their language. Many of these will be things like images that do not really have a language."], "cell_type": "markdown", "metadata": {}}, {"source": ["def get_language(page):\n", "    res = re.search('[a-z][a-z].wikipedia.org',page)\n", "    if res:\n", "        return res.group(0)[0:2]\n", "    return 'na'"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["df_train['lang'] = df_train.Page.map(get_language)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["df_train[df_train.Page == 'No\u00ebl_fr.wikipedia.org_all-access_all-agents']"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["There are 7 languages plus the media pages. The languages used here are: English, Japanese, German, French, Chinese, Russian, and Spanish. This will make any analysis of the URLs difficult since there are four different writing systems to be dealt with (Latin, Cyrillic, Chinese, and Japanese). \n", "\n", "First, let's create dataframes for the different types of entries and calculate the sum of all views."], "cell_type": "markdown", "metadata": {}}, {"source": ["lang_sets = {}\n", "lang_sets['en'] = df_train[df_train.lang=='en'].iloc[:,0:-1]\n", "lang_sets['ja'] = df_train[df_train.lang=='ja'].iloc[:,0:-1]\n", "lang_sets['de'] = df_train[df_train.lang=='de'].iloc[:,0:-1]\n", "lang_sets['na'] = df_train[df_train.lang=='na'].iloc[:,0:-1]\n", "lang_sets['fr'] = df_train[df_train.lang=='fr'].iloc[:,0:-1]\n", "lang_sets['zh'] = df_train[df_train.lang=='zh'].iloc[:,0:-1]\n", "lang_sets['ru'] = df_train[df_train.lang=='ru'].iloc[:,0:-1]\n", "lang_sets['es'] = df_train[df_train.lang=='es'].iloc[:,0:-1]\n", "\n", "sums = {}\n", "for key in lang_sets:\n", "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["and visualise"], "cell_type": "markdown", "metadata": {}}, {"source": ["days = [r for r in range(sums['en'].shape[0])]\n", "\n", "fig = plt.figure(1,figsize=[18,5])\n", "plt.ylabel('Views per Page')\n", "plt.xlabel('Day')\n", "plt.title('Pages in Different Languages')\n", "labels={'en':'English','ja':'Japanese','de':'German',\n", "        'na':'Media','fr':'French','zh':'Chinese',\n", "        'ru':'Russian','es':'Spanish'\n", "       }\n", "\n", "for key in sums:\n", "    plt.plot(days,sums[key],label = labels[key] )\n", "    \n", "plt.legend()\n", "plt.show()"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["### Looking at Periodic Structure with FFTs\n", "\n", "All solutions for time series forecasting are highly dependent on the length of the sequences in question. Some work very well for short sequences, up to 100-300 items but will forget information from older items on longer sequences. Competition time series is up to 700 days long, so it might be necessary to find some method to \"strengthen\" GRU memory.\n", "\n", "Looking at the Fast Fourier Transform (FFT) shows us the strongest frequencies in the periodic signal and can give a first estimate as to the length of some of the sequences."], "cell_type": "markdown", "metadata": {}}, {"source": ["from scipy.fftpack import fft\n", "def plot_with_fft(key):\n", "    f, ax = plt.subplots(2, figsize=(18,8))\n", "    ax[0].set_ylabel('Views per Page')\n", "    ax[0].set_xlabel('Day')\n", "    ax[0].set_title(labels[key])\n", "    ax[0].plot(days,sums[key],label = labels[key] )\n", "\n", "    fft_complex = fft(sums[key])\n", "    fft_mag = [np.sqrt(np.real(x)*np.real(x)+np.imag(x)*np.imag(x)) for x in fft_complex]\n", "    fft_xvals = [day / float(days[-1]) for day in days]\n", "    npts = len(fft_xvals) // 2 + 1\n", "    fft_mag = fft_mag[:npts]\n", "    fft_xvals = fft_xvals[:npts]\n", "        \n", "    ax[1].set_ylabel('FFT Magnitude')\n", "    ax[1].set_xlabel(r\"Frequency [days]$^{-1}$\")\n", "    ax[1].set_title('Fourier Transform')\n", "    ax[1].plot(fft_xvals[1:],fft_mag[1:],label = labels[key] )\n", "    ax[1].axvline(x=1./7,color='red',alpha=0.3)\n", "    ax[1].axvline(x=2./7,color='red',alpha=0.3)\n", "    ax[1].axvline(x=3./7,color='red',alpha=0.3)\n", "\n", "    plt.show()\n", "\n", "for key in sums:\n", "    plot_with_fft(key)"], "cell_type": "code", "outputs": [], "metadata": {"scrolled": false}, "execution_count": null}, {"source": ["# Individual Entry Data"], "cell_type": "markdown", "metadata": {}}, {"source": ["def plot_entry(key,idx, ax):\n", "    data = lang_sets[key].iloc[idx,1:]\n", "    ax.plot(days,data, label=df_train.iloc[lang_sets[key].index[idx],0])\n", "    ax.set_xlabel('day')\n", "    ax.set_ylabel('views')"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["f, ax = plt.subplots(figsize=(18,4))\n", "plot_entry(key='en', idx=4, ax=ax )\n", "plot_entry(key='en', idx=5, ax=ax )\n", "plot_entry(key='en', idx=6, ax=ax )\n", "plot_entry(key='en', idx=7, ax=ax )\n", "plot_entry(key='en', idx=8, ax=ax )\n", "ax.legend()"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["Nothing special about those pages as I selected them randomly. The only purpose is that you can explore individual entries, one by one, and see if there is any correlation with a social event, holiday, or a movie release. \n", "\n", "As a small exercise, try to find the Christmas page in French and explain the trends  ;) "], "cell_type": "markdown", "metadata": {}}, {"source": ["## Base prediction\n", "Many methods and tools can be used for time series forecasting. \n", "examples: RNN, autoencoders, Kernel methods, autoregressive integrated moving average models.\n", "\n", "For simplicity and to provide baseline model, we will only focus on two methods, the traditional autoregressive integrated moving average model and Recurrent Neural Network RNN. The later became is the trending method for prediction, will try to make a simple Keras model using LSTM cells.\n", "\n", "### Buidling an ARIMA forcasting model"], "cell_type": "markdown", "metadata": {}}, {"source": ["npages = 5\n", "top_pages = {}\n", "for key in lang_sets:\n", "    print(key)\n", "    sum_set = pd.DataFrame(lang_sets[key][['Page']])\n", "    sum_set['total'] = lang_sets[key].sum(axis=1)\n", "    sum_set = sum_set.sort_values('total',ascending=False)\n", "    print(sum_set.head(10))\n", "    top_pages[key] = sum_set.index[0]\n", "    print('\\n\\n')"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["from statsmodels.tsa.arima_model import ARIMA "], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["import warnings\n", "\n", "cols = df_train.columns[1:-1]\n", "for key in top_pages:\n", "    data = np.array(df_train.loc[top_pages[key],cols],'f')\n", "    result = None\n", "    with warnings.catch_warnings():\n", "        warnings.filterwarnings('ignore')\n", "        try:\n", "            arima = ARIMA(data,[2,1,4])\n", "            result = arima.fit(disp=False)\n", "        except:\n", "            try:\n", "                arima = ARIMA(data,[2,1,2])\n", "                result = arima.fit(disp=False)\n", "            except:\n", "                print(df_train.loc[top_pages[key],'Page'])\n", "                print('\\tARIMA failed')\n", "    #print(result.params)\n", "    pred = result.predict(2,599,typ='levels')\n", "    x = [i for i in range(600)]\n", "    i=0\n", "    \n", "    f, ax = plt.subplots(figsize=(18,4))\n", "    ax.plot(x[2:len(data)],data[2:] ,label='Data')\n", "    ax.plot(x[2:],pred,label='ARIMA Model')\n", "    print str(df_train.loc[top_pages[key],'Page'])\n", "    ax.set_title(str(df_train.loc[top_pages[key],'Page']).decode('utf-8'))\n", "    ax.set_xlabel('Days')\n", "    ax.set_ylabel('Views')\n", "    ax.legend()\n", "    plt.show()"], "cell_type": "code", "outputs": [], "metadata": {"scrolled": false}, "execution_count": null}, {"source": ["### Buidling an LSTM model using Keras"], "cell_type": "markdown", "metadata": {}}, {"source": ["in this part you need Keras installed. Instruction can be found on [keras.io](keras.io).\n", "In this section will try to train an LSTM models for top pages of all the languages for demonstration. We will, therefore, drop the `Page` column from the dataframe."], "cell_type": "markdown", "metadata": {}}, {"source": ["df_train = df_train.drop('Page',axis = 1)\n", "df_train.head()"], "cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null}, {"source": ["#Packages for pre processing\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", " # Importing the Keras libraries and packages for LSTM\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import LSTM"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["def keras_regressor():\n", "    regressor = Sequential()\n", "    # Adding the input layerand the LSTM layer\n", "    regressor.add(LSTM(units = 8, activation = 'relu', input_shape = (None, 1)))\n", "    # Adding the output layer\n", "    regressor.add(Dense(units = 1))\n", "    # Compiling the RNN\n", "    regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n", "    return regressor\n", "\n", "for key in sums:\n", "    f, ax = plt.subplots(figsize=(18,4))\n", "    \n", "    row = [0]*sums[key].shape[0]\n", "    for i in range(sums[key].shape[0]):\n", "        row[i] = sums[key][i]\n", "\n", "    #Using Data From Random Row for Training and Testing\n", "    X = row[0:549]\n", "    y = row[1:550]\n", "    \n", "    # Splitting the dataset into the Training set and Test set\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n", "    \n", "    # Feature Scaling\n", "    sc = MinMaxScaler()\n", "    X_train = np.reshape(X_train,(-1,1))\n", "    y_train = np.reshape(y_train,(-1,1))\n", "    X_train = sc.fit_transform(X_train)\n", "    y_train = sc.fit_transform(y_train)\n", "    #Reshaping Array\n", "    X_train = np.reshape(X_train, (384,1,1))\n", "\n", "    # Initialising the RNN\n", "    regressor = keras_regressor()\n", "\n", "    # Fitting the RNN to the Training set\n", "    regressor.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n", "\n", "    # Getting the predicted Web View\n", "    inputs = X\n", "    inputs = np.reshape(inputs,(-1,1))\n", "    inputs = sc.transform(inputs)\n", "    inputs = np.reshape(inputs, (549,1,1))\n", "    y_pred = regressor.predict(inputs)\n", "    y_pred = sc.inverse_transform(y_pred)\n", "\n", "    print(key)\n", "    #Visualising Result\n", "    ax.plot(y, color = 'red', label = 'Real Web View')\n", "    ax.plot(y_pred, color = 'blue', label = 'Predicted Web View')\n", "    ax.set_title ('Web View Forecasting: %s' % key)\n", "    ax.set_xlabel('Number of Days from Start')\n", "    ax.set_ylabel('Web View')\n", "    ax.legend()\n", "    plt.show()"], "cell_type": "code", "outputs": [], "metadata": {"scrolled": false}, "execution_count": null}, {"source": ["## Make submission\n", "\n", "The models above are very simple ones and need to be optimised. You could, therefore, consider a further features engineering and hyperparameters optimisation for the proposed models. You can also use other frameworks to build your forecasting models. Few examples are `pytorch`, `tensorflow`, `sklearn` ...etc. \n", "\n", "The last step will be to ma e a submission, and for that, you can refer to the documentation on Kaggle platform to make a submission."], "cell_type": "markdown", "metadata": {"collapsed": true}}]}