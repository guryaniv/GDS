{"cells":[{"metadata":{"_uuid":"b94731aead66611bd2d83f4dc1555b4ca82f071e"},"cell_type":"markdown","source":"# How Over Do You Fit?\n*Abstract*.  We create validation curves for some basic models trained on the data of the Don't Overfit! II competition.\n\n## Introduction\nThe [Dont't Overfit! II competition](https://www.kaggle.com/c/dont-overfit-ii) challenges us to model a binary target depending on 200 continuous variables without overfitting, using only 250 training samples.  For low-dimensional problems, whether a model underfits or overfits can be easily decided by visual inspection, as is nicely explained in the scikit-learn example [Underfitting vs. Overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html).\n![image](https://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png)\nQuoting from there, \"the plot shows the function that we want to approximate, which is a part of the cosine function.  In addition, the samples from the real function and the approximations of different models are displayed.  The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called underfitting. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will overfit the training data, i.e. it learns the noise of the training data.\"\n\nHowever, as stated in the scikit-learn [User Guide](https://scikit-learn.org/stable/modules/learning_curve.html), \"in the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance.  However, in high-dimensional spaces, models can become very difficult to visualize.''  Let's have a look at the scikit-learn example [Plotting Validation Curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html):\n![image](https://scikit-learn.org/stable/_images/sphx_glr_plot_validation_curve_001.png)\nQuoting from there, \"in this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma.  For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well.  If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.\"\n\nSo let's apply this idea to the Don't Overfit! II competition.\n\nAs usual, we start by loading some libraries (input hidden)."},{"metadata":{"trusted":true,"_uuid":"33007affcee437b75d15fd8d6bc9d03891984571","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy             as np\nimport pandas            as pd\n\nfrom scipy.stats                   import randint, lognorm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.dummy                 import DummyClassifier\nfrom sklearn.ensemble              import RandomForestClassifier, BaggingClassifier\nfrom sklearn.feature_selection     import RFE, SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.linear_model          import LogisticRegression, SGDClassifier\nfrom sklearn.metrics               import roc_auc_score\nfrom sklearn.model_selection       import cross_val_score, LeaveOneOut, ParameterGrid, GridSearchCV, RandomizedSearchCV, validation_curve, StratifiedShuffleSplit, RepeatedStratifiedKFold\nfrom sklearn.neighbors             import KNeighborsClassifier\nfrom sklearn.pipeline              import Pipeline, make_pipeline\nfrom sklearn.preprocessing         import StandardScaler\nfrom sklearn.svm                   import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daaa9a98c5dd4c03875ac657cb8ebeae24aac3cd"},"cell_type":"markdown","source":"Then we load the training data and convert it to scikit-learn's (X, y) format.  The output is hidden since everybody is probably already familiar with it."},{"metadata":{"trusted":true,"_uuid":"6115bad5a158640306a8c240d363e6efd8cafe2f","_kg_hide-output":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\nX = train.drop(['id', 'target'], axis=1)\ny = train['target']\ntrain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3646e54fc674f5586cf543782abd1884f5b15d96"},"cell_type":"markdown","source":"Let's use the area under the ROC curve as the scoring variable as in the competition, and use stratified 5 times cross validation repeated 10 times as the cross validation strategy."},{"metadata":{"trusted":true,"_uuid":"8db28be87a6a5e159b4331240fbf7a060c327cf4"},"cell_type":"code","source":"scoring='roc_auc'\ncv=RepeatedStratifiedKFold(5, 10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9be25644b6f7a42321355c92d56d9d4341ad85cb"},"cell_type":"markdown","source":"The following function is a wrapper around [validation_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html) that acually plots the output, slightly adapted from the scikit-learn example [Plotting Validation Curves](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html):"},{"metadata":{"trusted":true,"_uuid":"a59802ef9ae804c9e4c026758ddb2507d1373667"},"cell_type":"code","source":"def plot_validation_curve(estimator, param_name, param_range, X=X, y=y, groups=None, cv=cv, scoring=scoring, n_jobs=None, pre_dispatch='all', verbose=0, error_score='raise-deprecating'):\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, groups, cv, scoring, n_jobs, pre_dispatch, verbose, error_score)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std (train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores,  axis=1)\n    test_scores_std   = np.std (test_scores,  axis=1)\n    \n    plt.suptitle(estimator.__class__.__name__)\n    plt.xlabel(param_name)\n    if scoring is not None: plt.ylabel(scoring) \n    lw = 2\n    param_range = [str(x) for x in param_range] # make axis categorical so we don't have to think about scaling\n    plt.plot(param_range, train_scores_mean, label='Training score', color='darkorange', lw=lw)\n    plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\", lw=lw)\n    plt.plot(param_range, test_scores_mean, label='Cross-validation score', color='navy', lw=lw)\n    plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\", lw=lw)\n    plt.legend(loc='best')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12c1b62f0a64ca0a1aad8fa88349889340f6711a"},"cell_type":"markdown","source":"So let's have a look at some examples.\n\n## Dummy classifiers\nscikit-learn contains some [dummy classifiers](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators) that implement simple rules of thumb.  These should certainly not overfit, but let's verify:"},{"metadata":{"trusted":true,"_uuid":"468c76417874a6c6377c6ca4af31d039a91f9686"},"cell_type":"code","source":"plot_validation_curve(DummyClassifier(random_state=0),\n                      'strategy', ['stratified', 'most_frequent', 'prior', 'uniform'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e2a7db1e22a63b3835df329f7a003269a96cb25"},"cell_type":"markdown","source":"## $k$ nearest neighbors\nFor uniformly weighted [$k$ nearest neighbors](https://scikit-learn.org/stable/modules/neighbors.html#classification), overfitting reduces for large $k$, and vanishes around $k = 120$.  Note that for $k = 200$ for stratified 5 fold cross validation the classifier predicts simply the class probabilities in the entire test set, to the area under the ROC curve drops to $0$."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b47a72ae24d0e988f402b705b0e87d12fb94e8f4"},"cell_type":"code","source":"plot_validation_curve(KNeighborsClassifier(),\n                      'n_neighbors', [20, 40, 60, 80, 100, 120, 140, 160, 180, 200])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f4d5d01ace47f24cfccf7cdbd3ec31161234cf3"},"cell_type":"markdown","source":"Standardization does not substantially change this, but seems to add a bit of overfitting since the standardization adds more parameters to be fitted.  (Most public kernels in the competition that do standardization do this on the entire training set outside the cross validation loop, so they fail to detect this effect.)"},{"metadata":{"trusted":true,"_uuid":"ad5a2ee2dd82996f889fb9fc30a35e71fc428b86"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('std', StandardScaler()),\n                                ('clf', KNeighborsClassifier())]),\n                      'clf__n_neighbors', [20, 40, 60, 80, 100, 120, 140, 160, 180, 200])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af559f01c3dd3737c29d2642b840ce657ffd595d"},"cell_type":"markdown","source":"For distance weighted $k$ nearest neighbors, the model overfits for all $k$ as to be expected, but the cross-validation score increases for larger $k$."},{"metadata":{"trusted":true,"_uuid":"e541bb000c8721e81dc8e3dcbbdd15d125942625"},"cell_type":"code","source":"plot_validation_curve(KNeighborsClassifier(weights='distance'),\n                      'n_neighbors', [20, 40, 60, 80, 100, 120, 140, 160, 180, 200])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b622b165fafd87cee3004c53e1c3de71c13c3ac"},"cell_type":"markdown","source":"## Logistic regression\n[Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) overfits for the $l_2$ norm."},{"metadata":{"trusted":true,"_uuid":"46d7fa4e7c26a4288262b756449c65a27ec5ce09"},"cell_type":"code","source":"plot_validation_curve(LogisticRegression(penalty='l2',\n                                         solver='liblinear'),\n                      \"C\", [0.001, 0.01, 0.1, 1, 10, 100, 1000])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca7bf3cc4ca4cb6f415b49fe3e706cd2bc5365c6"},"cell_type":"markdown","source":"For the $l_1$ norm, overfitting starts for $C > 0.04$."},{"metadata":{"trusted":true,"_uuid":"fbeb43f965ea43c0f12ff33124391b368ffd1bee"},"cell_type":"code","source":"plot_validation_curve(LogisticRegression(penalty='l1',\n                                         solver='liblinear'),\n                      'C', [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b506f97feb958ad1b8973e33324b85e72093c71f"},"cell_type":"markdown","source":"Using balanced class weights does improve performance for $C = 0.03$ but still overfits for $C > 0.04$."},{"metadata":{"trusted":true,"_uuid":"c6ab1908e9ec72e293e9f8f20aa5e581f7ed2298"},"cell_type":"code","source":"plot_validation_curve(LogisticRegression(penalty='l1',\n                                         solver='liblinear',\n                                         class_weight='balanced'),\n                      'C', [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678b7929ce7d587a196847ca0488c220e5e7d534"},"cell_type":"markdown","source":"Standardization does not change the situation."},{"metadata":{"trusted":true,"_uuid":"ab423262cd30d559f18bc35537a3d06cb359a3b2"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('std', StandardScaler()),\n                                ('clf', LogisticRegression(penalty='l1',\n                                                           solver='liblinear',\n                                                           class_weight='balanced'))]),\n                      'clf__C', [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a6a89e204617040228e521313bb5a19b5904b20"},"cell_type":"markdown","source":"If $C$ is too large, overfitting starts as soon as we take more than the two most significant features into account.  For $C = 0.1$ it becomes pretty severe as $k$ increses:"},{"metadata":{"trusted":true,"_uuid":"f116b3555beddc173f277a8c22d9402d64cbf771"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('sel', SelectKBest()),\n                                ('clf', LogisticRegression(penalty=\"l1\",\n                                                           solver='liblinear',\n                                                           class_weight='balanced',\n                                                           C=0.1))]),\n                      'sel__k', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"243f05ebe107b91cc2a4f679d0f5dc713742052f"},"cell_type":"markdown","source":"If we keep $C = 0.05$, which is only slightly too large, training and cross-validation score still start to diverge when taking more than two features into account, but the model does not overfit as extremely:"},{"metadata":{"trusted":true,"_uuid":"b65f394ccc1241628c21a73ff8e1fd529770b54d"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('sel', SelectKBest()),\n                                ('clf', LogisticRegression(penalty=\"l1\",\n                                                           solver='liblinear',\n                                                           class_weight='balanced',\n                                                           C=0.05))]),\n                      'sel__k', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed4e7d3e718e56d8399087d757a84e3f1a5f3250"},"cell_type":"markdown","source":"For $C = 0.04$, since there was no severe overfitting even on the complete dataset, this also holds if we restrict to the $k$ most significant features.  However, the performance does not improve once we go beyond the two most significant features."},{"metadata":{"trusted":true,"_uuid":"b41732db43f65ac544ff27828e7d1f42192d9948"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('sel', SelectKBest()),\n                                ('clf', LogisticRegression(penalty=\"l1\",\n                                                           solver=\"liblinear\",\n                                                           class_weight=\"balanced\",\n                                                           C=0.04))]),\n                      'sel__k', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b343018ba30a49abffc4e6b41d92f6ba7a021f4"},"cell_type":"markdown","source":"Instead of selecting the featues upfront, in the public kernes in the competition it's pupular to use [recursive feature elimination](https://scikit-learn.org/stable/modules/feature_selection.html#rfe). However, this does not reduce overfitting."},{"metadata":{"trusted":true,"_uuid":"3aed149137e7991929bb3c2b4abfba908ba898d9"},"cell_type":"code","source":"plot_validation_curve(RFE(LogisticRegression(penalty=\"l1\",\n                                             solver='liblinear',\n                                             class_weight='balanced',\n                                             C=0.1)),\n                      'n_features_to_select', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c74acd9e4b5f568457789c9dcac06dcb4472d41"},"cell_type":"markdown","source":"## Linear discriminant analysis\nFor [linear discriminant analysis](https://scikit-learn.org/stable/modules/lda_qda.html#lda-qda), overfitting seems to occur for all shrinkages."},{"metadata":{"trusted":true,"_uuid":"c66ded3d75ec81ac83dfc92ab951918124f014dc"},"cell_type":"code","source":"plot_validation_curve(LinearDiscriminantAnalysis(solver='lsqr'),\n                      'shrinkage', [0.001, 0.001, .01, .1, 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3823072ec72f072e60fcc7d1dffa5e150a02fb8c"},"cell_type":"markdown","source":"## Support vector machines\nFor [support vector machines](https://scikit-learn.org/stable/modules/svm.html#svm-classification), it seems to be very difficult to find parameter combinatins that do not overfit.\nLet's first have a look at the different kernels with the default parameters:"},{"metadata":{"trusted":true,"_uuid":"6b91641f71c85b8c45d4fdc65d307c6ea2761f54"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma='scale',\n                          probability=True),\n                      'kernel', ['linear', 'poly', 'rbf', 'sigmoid'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c37f44be662578accf2d7a50e1ca306cec7c6edc"},"cell_type":"markdown","source":"### Linear kernel\nWe start by investigating the linear kernel, which has only one parameter $C$.  It overfits for all values of $C$."},{"metadata":{"trusted":true,"_uuid":"d55432a855635b17fc9352482fecd8935786df93"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma=\"auto\", # gamma is not really a parameter\n                                        # of the linear kernel, this is\n                                        # just to silence some warnings.\n                          probability=True), \n                      'C', [1e-3, 1e-2, 1e-1, 1e+0, 1e+1, 1e+2, 1e+3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7acacb34654c4101d0ec297c80c952372813d84"},"cell_type":"markdown","source":"### Sigmoid kernel\nNow let's have a look at the sigmoid kernel which has parameters $\\gamma$, $C$ and $\\mathrm{coef}_0$.\nFor $C = 10^{-4}$, overfitting reduces a bit."},{"metadata":{"trusted":true,"_uuid":"169c9d033ee89ca9a34202e8f5acf6c1da55a23d"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma=\"scale\",\n                          kernel=\"sigmoid\",\n                          probability=True),\n                      \"C\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e+0, 1e+1, 1e+2, 1e+3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c683325ed2c1007bc2161cb3b60d792500796de"},"cell_type":"markdown","source":"If we fix $C$, the choice of $\\mathrm{coef}_0 = 10$ seems to show some reduced overfitting:"},{"metadata":{"trusted":true,"_uuid":"33b5e697ad67ace1a290cf9871633bf823711f19"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma='scale',\n                          kernel='sigmoid',\n                          C=1e-4,\n                          probability=True),\n                      'coef0', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e+0, 1e+1, 1e+2, 1e+3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eb2fcba40a022a7fc2dfe2a68f77ff97620fda9"},"cell_type":"markdown","source":"Again, if we keep $C$ and $\\mathrm{coef}_0$, the choice of $\\gamma = 0.01$ shows some reduction in overfitting:"},{"metadata":{"trusted":true,"_uuid":"6b754f7a04a4ecebb967c748a4f3829c138cd3b5"},"cell_type":"code","source":"plot_validation_curve(SVC(kernel=\"sigmoid\",\n                          coef0=10,\n                          C=1e-4,\n                          probability=True),\n                      \"gamma\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d822e2f14d47b910ed6c186a9252c77b982e17b"},"cell_type":"markdown","source":"### Polynomial kernel\nNow let's have a look at polynomial kernels.  All degrees overfit with the default parameters:"},{"metadata":{"trusted":true,"_uuid":"954e0ffb5decb495915b7901162500a4fa8856a7"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma='scale',\n                          kernel='poly',\n                          probability=True),\n                      'degree', [1, 2, 3, 4, 5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"822c40376bc73983c6367560f9601763ec695cd2"},"cell_type":"markdown","source":"Let's investigate the quadratic case.  For $\\mathrm{coef}_0 = 0.05$ there is still overfitting but some improvement in cross-validation score:"},{"metadata":{"trusted":true,"_uuid":"acf079c7f6c3a694b9040e7ce38b3d5cf7aeb62a"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma=\"scale\",\n                          kernel=\"poly\",\n                          degree=2,\n                          probability=True),\n                      \"coef0\", [.001, .002, .005, .01, .02, 0.05, .1, .2, .5, 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47aee4ef314cb4c3ab3c8e804e981075cab4172c"},"cell_type":"markdown","source":"So we fix $\\mathrm{coeff}_0 = 0.05$ and investigate $C$.  Unfortunately, overfitting occurs for all values of $C$:"},{"metadata":{"trusted":true,"_uuid":"e7c074240d09c8e423eac606b501b1b13e9fb85d"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma=\"scale\",\n                          kernel=\"poly\",\n                          degree=2,\n                          coef0=0.05,\n                          probability=True),\n                      \"C\", [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1,10,100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"121590ee55fb470b5f228b08dc9ed62072d2d517"},"cell_type":"markdown","source":"Similarly, in the cubic case, for $\\mathrm{coef}_0 = 1$ there is still overfitting but some improvement in cross-validation score:"},{"metadata":{"trusted":true,"_uuid":"f8a1c8105f2436cb127ad08f7c7bdd46ae2c7a3e"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma='scale',\n                          kernel='poly',\n                          degree=3,\n                          probability=True),\n                      'coef0', [.01, .02, 0.05, .1, .2, .5, 1, 2, 5, 10, 20, 50])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbb95e6d7a0711f4aece4ddf4592a129a5a693bd"},"cell_type":"markdown","source":"Here, overfitting reduces slightly for $\\gamma < 10^{-6}$, but is still massive for all $\\gamma$:"},{"metadata":{"trusted":true,"_uuid":"72f95823604259608a98ac39a19bf02affc3771a"},"cell_type":"code","source":"plot_validation_curve(SVC(gamma=\"scale\",\n                          kernel=\"poly\",\n                          degree=3,\n                          coef0=1,\n                          probability=True),\n                      \"gamma\", [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3b696294345306b9cb42c90fa9c17de172e6127"},"cell_type":"markdown","source":"## Stochastic gradient descent\n\n[Stochastic gradient descent](https://scikit-learn.org/stable/modules/sgd.html#sgd) overfits for all $\\alpha$."},{"metadata":{"trusted":true,"_uuid":"5462de729948196859facd9b0a61fd5988056fc5"},"cell_type":"code","source":"plot_validation_curve(SGDClassifier(loss='modified_huber',\n                                    max_iter=1000,\n                                    tol=1e-3),\n                      'alpha', [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cf6d9cabd1a2ec2b4c48aa898dc58dbe62b2853"},"cell_type":"markdown","source":"Standardization does not change anything:"},{"metadata":{"trusted":true,"_uuid":"c90cfd9ddd6e0845b1f65734ab06a15be0af4bee"},"cell_type":"code","source":"plot_validation_curve(Pipeline([('std', StandardScaler()),\n                                ('clf', SGDClassifier(loss='modified_huber',\n                                                      max_iter=1000,\n                                                      tol=1e-3))]),\n                      'clf__alpha', [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f12f557f8dd0fa3ae0612a850cf17c563f0848d4"},"cell_type":"markdown","source":"## Random forest\nHow about [random forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)?  Even if we severely restict the maximal depth of trees, and look at the minimal number of samples per leaf, overfitting only stops when the AUC drops to $0.5$."},{"metadata":{"trusted":true,"_uuid":"25a838d6acd93301472aa38eff05973ec0f5b6e7","collapsed":true},"cell_type":"code","source":"plot_validation_curve(RandomForestClassifier(max_depth=2, n_estimators=10),\n                      'min_samples_leaf', [10, 20, 30, 40, 50, 60, 70, 80])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"046f465a0da945f81c4447c13d35b560bd8aa705"},"cell_type":"markdown","source":"## Bagging\nLet's have a look at [bagging](https://scikit-learn.org/stable/modules/ensemble.html#bagging).  This does still overfit:"},{"metadata":{"trusted":true,"_uuid":"983a3f492cecd3b9d76e990a39307bc666194493"},"cell_type":"code","source":"plot_validation_curve(BaggingClassifier(LogisticRegression(C=0.1,\n                                                           penalty='l1',\n                                                           class_weight='balanced',\n                                                           solver='liblinear',\n                                                           random_state=0),\n                                        max_samples=0.8,\n                                        bootstrap=True,\n                                        random_state=0),\n                      'n_estimators', [10, 20, 30, 40, 50, 60, 70, 80, 90, 100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0322d40b3669bb7483e13659e60fbef6de57fb20"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}