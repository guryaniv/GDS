{"nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "version": "3.6.1", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e1f8056a44615a5ee089b005634b7269c73c0be7", "_cell_guid": "d9cb3bfe-1590-4e8e-bb03-8f256a227407"}, "source": ["We create a ML pipeline for anomaly detection. The anomaly detection approach is suited best in this case as on the one hand the data is highly skewed. On the other hand an ordinary training method might detect fraudulent behaviour similar to the one that it has been trained on but fail to detect new anomalies.\n", "\n", "The first step is to read in and get a feeling for the data."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "aa41d981cde26af6c46bce887ca896aa9d604ed5", "_cell_guid": "c84bb969-e3ca-4c79-8a11-a8609e73ed4a", "collapsed": true}, "source": ["import numpy as np \n", "import pandas as pd \n", "\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ea8c5677f1a24667c881ff18506ee093eee171c2", "_cell_guid": "9953a1c5-df56-4779-b0aa-4bc2d458b751"}, "source": ["data = pd.read_csv('../input/creditcard.csv')\n", "data.head()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "d40e98c1e8a13730969d63f886941704d8a8d019", "_cell_guid": "f53fe7aa-1fca-4189-9362-a3c724a8cfe8"}, "source": ["+ 28 numerical features without further information -> check individual distribution, we expect them to be Gaussian\n", "+ one sorted feature \"Time\" \n", "+ one positive numerical feature \"Amount\" (most likely not Gaussian distributed) "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "aa39f14b076dd8b04ab785840f1065a037f8fdb1", "_cell_guid": "cf44c0b8-7569-4cf2-9e41-72831f91f288"}, "source": ["data.describe()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f65443ad59eb48388d59f69df8b168b88783cd24", "_cell_guid": "4a60a972-346e-4644-811d-125c5e671939"}, "source": ["+ The PCA features seem to be ordered by standard deviation\n", "+ Values are centered around zero and scaled within one order of magnitude"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "cecb76d66c41eb02da9bf804dca66715f1b346e8", "_cell_guid": "7d7ff060-9bf9-43c8-83d8-2d2b3d1187b3"}, "source": ["We check the distribution of the normal data."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "421d3debd69f19eaa0f0481dd939075cf0d20cb5", "_cell_guid": "0ea6f1c7-7fdc-403e-b553-b52551eae023"}, "source": ["normal_data = data.loc[data[\"Class\"] == 0]\n", "fraud_data = data.loc[data[\"Class\"] == 1]"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "ac4556fea900920349c7a0940598705313e83af5", "_cell_guid": "69ffda87-dce2-4c14-b60b-fa9ba714979b"}, "source": ["plt.figure();"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "b984a1adb9720e0d50125d95ddfc937a00cecc71", "_cell_guid": "dfd16e54-19fc-4286-ba89-3ad9f6b8b07d"}, "source": ["matplotlib.style.use('ggplot')\n", "pca_columns = list(data)[1:-2]\n", "normal_data[pca_columns].hist(stacked=False, bins=100, figsize=(12,30), layout=(14,2));"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "7f1cf2b623c55fc05ee28bfd1cb4b031bbbaebc1", "_cell_guid": "f513298a-3f10-4db5-90ea-450a27eb9689"}, "source": ["+ For first approximation features can be taken to be Gaussian distributed\n", "+ Note that some features have outliers in the normal data (we might want to engineer features that look at these separately later if they are)\n", "+ feature V1 seems to behave a little differently and has the most deviance from normal distribution on first sight\n", "\n", "Analysis of the amount of withdrawal\n", "==="]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "409406eb7507d27cce1cc012cf2741ef980cc8a3", "_cell_guid": "ab9087cd-82a0-4752-af62-1df3e51cc332"}, "source": ["normal_data[\"Amount\"].loc[normal_data[\"Amount\"] < 500].hist(bins=100);"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "aa65f5a537bee770c350d857d4268c5f54bc4d30", "_cell_guid": "d7ccf78b-2e77-4415-98f2-1bba359a9509"}, "source": ["+ The vast majority of withdrawals are in the sub 100$ range\n", "+ We expect the the fraudulent amounts to be higher "]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "6c42612cd53290eb1c3ad2063045651d40e33e51", "_cell_guid": "1e409224-aa6b-442b-8e56-142482bc686c"}, "source": ["print(\"Mean\", normal_data[\"Amount\"].mean(), fraud_data[\"Amount\"].mean())\n", "print(\"Median\", normal_data[\"Amount\"].median(), fraud_data[\"Amount\"].median())"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "745a7bc5bfd879f186761bf04086d3c29f8ac55b", "_cell_guid": "2563a903-780a-4a38-bb22-edf21ba3525a"}, "source": ["fraud_data[\"Amount\"].hist(bins=100);"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "409a2ced12d370337f278c4ee9418dce90d3464f", "_cell_guid": "9eae8c5d-e33e-465b-8d9e-e29dceca83ba"}, "source": ["+ Interestingly the median is lower but the mean is higher for the fraudulent cases.  This suggests there are some high value oriented criminals and some that focus on withdrawals \"below the radar\" to avoid detection\n", "+ This makes the amount of withdrawal another feature\n", "\n", "Analysis of time withdrawal\n", "====\n", "+ First we see if there are clusters of time with many normal/fraudulent withdrawals"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8586e1d3fdfe4bfd9e48637e6c8ac64f566b3cd0", "_cell_guid": "deb18118-084f-4f25-8585-d06878334106"}, "source": ["normal_data[\"Time\"].hist(bins=100);"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8abe67bdbfd8523d74fef5a07927cbe94cfffafa", "_cell_guid": "41a4e3d9-4feb-4bea-9d36-a3b3c8603518"}, "source": ["The two days the data is collected in are clearly visible in the normal transactions, being more or less constant during the day and falling sharply during the night.\n", "\n", "Are the criminals more active during the day or creatures of the night?"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "984a78768570866e75f91bd4abeda28b69613df5", "_cell_guid": "4eaad2a8-9af5-425c-8a44-5ad8d4e9a2cc"}, "source": ["fraud_data[\"Time\"].hist(bins=50);"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "fbc3bfb25b7fb11138d6c3bae94f404869152a8f", "_cell_guid": "26a47451-7549-4ba6-a93f-4a330e7c4fb8"}, "source": ["+ There seem to be some accumulation of frauds at the beginning of the night\n", "+ In general the day night activities are way less pronounced in the fraudulent cases"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "a9a24ff8cf5d559f161a2a76ac10269bef7398f5", "_cell_guid": "19226fc4-0826-4a2c-86ca-4745de8d2a8d"}, "source": ["Finally we check the correlations between the features to decide what anomaly detection model we start out with"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "fbb079b30a1addc1dc1fac15d01538b5f837bb1b", "_cell_guid": "a9ed412d-4916-472a-8c39-f66fa93eb1b8"}, "source": ["normal_pca_data = normal_data[pca_columns]\n", "fraud_pca_data = fraud_data[pca_columns]\n", "plt.matshow(normal_pca_data.corr());"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f4de946a050e094a9a649568a460f8b98b6b7fa0", "_cell_guid": "d4c11c5f-dd42-4a48-8f64-3225e70636dc"}, "source": ["+ There are correlations (albeit rather small) in the first 18 principal components\n", "+ As we have >200000 data points calculating the co-variance matrix will run into memory problems\n", "+ We start with uncorrelated Gaussians"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "75caeb9c215fc332a368678036b021fcd41b472d", "_cell_guid": "ed1ab74a-f57b-44f9-af04-8b2cab21fc42"}, "source": ["Multivariate Gaussian analysis of the normal data\n", "===\n", "+ First focus on the PCA features and see where it takes us\n", "+ Numpy built-in function for the covariance matrix cant handle 200000+ -> implement our own\n", "+ We start to train the model now, so split the data into training, cross validation and test set. The fraudulent cases are split between validation and test set."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "80b0eb8497946e10795e1926b5813494ccb40cfa"}, "source": ["num_test = 75000\n", "shuffled_data = normal_pca_data.sample(frac=1)[:-num_test].values\n", "\n", "X_train = shuffled_data[:-2*num_test]\n", "\n", "X_valid = np.concatenate([shuffled_data[-2*num_test:-num_test], fraud_pca_data[:246]])\n", "y_valid = np.concatenate([np.zeros(num_test), np.ones(246)])\n", "\n", "X_test = np.concatenate([shuffled_data[-num_test:], fraud_pca_data[246:]])\n", "y_test = np.concatenate([np.zeros(num_test), np.ones(246)])"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "d6490f7bfef6e11cd16769b80f3b1d6e1466308b", "_cell_guid": "ce3395bf-161a-4aa2-985d-96e9831a7082"}, "source": ["def covariance_matrix(X):\n", "    m, n = X.shape \n", "    tmp_mat = np.zeros((n, n))\n", "    mu = X.mean(axis=0)\n", "    for i in range(m):\n", "        tmp_mat += np.outer(X[i] - mu, X[i] - mu)\n", "    return tmp_mat / m"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "19b42a9bff6d23e40c5df63adeac19c6d1a9e069"}, "source": ["cov_mat = covariance_matrix(X_train)"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "busy", "_uuid": "ba6f6c32b3dabf5a2f92d68d37f5bdd861efb80b", "_cell_guid": "96674173-e874-48de-a954-018f8ad1db38"}, "source": ["cov_mat_inv = np.linalg.pinv(cov_mat)\n", "cov_mat_det = np.linalg.det(cov_mat)\n", "def multi_gauss(x):\n", "    n = len(cov_mat)\n", "    return (np.exp(-0.5 * np.dot(x, np.dot(cov_mat_inv, x.T))) \n", "            / (2. * np.pi)**(n/2.) \n", "            / np.sqrt(cov_mat_det))"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "73ae991a881eb5bacf433eef3304751b3a30260e"}, "source": ["from sklearn.metrics import confusion_matrix\n", "\n", "def stats(X_test, y_test, eps):\n", "    predictions = np.array([multi_gauss(x) <= eps for x in X_test], dtype=bool)\n", "    y_test = np.array(y_test, dtype=bool)\n", "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n", "    recall = tp / (tp + fn)\n", "    prec = tp / (tp + fp)\n", "    F1 = 2 * recall * prec / (recall + prec)\n", "    return recall, prec, F1"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "05424271c1d313c7732f36d395405f2320a77e8c"}, "source": ["To get an estimate for the threshold eps we take the maximum of the fraudulent cases. \n", "\n", "We should expect a recall of 100%"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "6608908de8d1268b8412e7bf6d8364bbc59f61c6"}, "source": ["eps = max([multi_gauss(x) for x in fraud_pca_data.values])\n", "print(eps)"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "fe53a78eb740622899706e58879cfa534784fc72"}, "source": ["recall, prec, F1 = stats(X_valid, y_valid, eps)\n", "print(\"For a boundary of:\", eps)\n", "print(\"Recall:\", recall)\n", "print(\"Precision:\", prec)\n", "print(\"F1-score:\", F1)"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "1a1413d393a7763836421803221d1f40c36e45df"}, "source": ["To compare different thresholds we score them with the F1 score against the cross validation data set"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "1f5e26f0abd6f9194ecbf972b6ee45d795281753"}, "source": ["validation = []\n", "for thresh in np.array([1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]) * eps:\n", "    recall, prec, F1 = stats(X_valid, y_valid, thresh)\n", "    validation.append([thresh, recall, prec, F1])"]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "692b123a567d0965074cb8ed58929d0599a2686f"}, "source": ["x = np.array(validation)[:, 0]\n", "y1 = np.array(validation)[:, 1]\n", "y2 = np.array(validation)[:, 2]\n", "y3 = np.array(validation)[:, 3]\n", "plt.plot(x, y1)\n", "plt.title(\"Recall\")\n", "plt.xscale('log')\n", "plt.show()\n", "plt.plot(x, y2)\n", "plt.title(\"Precision\")\n", "plt.xscale('log')\n", "plt.show()\n", "plt.plot(x, y3)\n", "plt.title(\"F1 score\")\n", "plt.xscale('log')\n", "plt.show()"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "66c4d34e41a2fb38485ddd906d4b40845ecddb2f"}, "source": ["Conclusion\n", "---\n", "+ High recall means (extremely) low precision\n", "    + this might be ok if on spot security measures are cheaply implemented. For example an extra verification online for cases that seem suspicious.\n", "    + problematic if the flagged cases have to be reviewed by hand. \n", "    \n", "    \n", "+ Need to include Time and Amount into data\n", "    + The Amount especially is important as most likely steal a high amount of money should be penalized stronger \n", "    \n", "    \n", "+ Need to analyze data further to check if non-fraudulent outliers play an important role\n", "    + Check if non-fraudulent outliers cluster in different manner -> engineer feature\n", "    \n", "The two last cases are problematic within the Multivariate Gaussian approach as the provided data are not normal distributed."]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "1cce03072cf4b10b9a3cc6c2bb006f70069e00f9"}, "source": ["Follow up\n", "==\n", "Looking at the first couple of principal component dimensions we see that the data is rather awkardly scattered. In particular the positive cases are not real outliers but seem to cluster in certain areas. This makes a supervised classifier algorithm usable."]}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_uuid": "c84c3fad9fdc566dbcc2a9d317543aad368e3223"}, "source": ["data.plot.scatter(\"V1\",\"V2\", c=\"Class\")\n", "data.plot.scatter(\"V2\",\"V3\", c=\"Class\")\n", "data.plot.scatter(\"V1\",\"V3\", c=\"Class\")"]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "893f2299bd89222a3c637268503f66a6b1b360e3"}, "source": ["A multivariate gaussian draws ellipses around the negative data points. From the above pictures it is evident that any ellipse with a large recall also must have low precision. In particular as the ellipses are not learned per se."]}, {"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_uuid": "9f8bda14b4cb2dcb999e49b4e9230b9197bf739f"}, "source": []}]}