{"nbformat": 4, "nbformat_minor": 0, "metadata": {"language_info": {"name": "python", "version": "3.6.1", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "c58e3612-da56-4ee2-9710-58f2046ab780", "_uuid": "723685daa10cce81ec77773ea436661ee4accc29"}, "source": "The other part of my solution is there:\n\n1. https://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094\n2. https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script/notebook\n3. https://www.kaggle.com/schoolpal/nn-model-lb-0-306-to-0-308\n4. The stacking model which combines everything https://www.kaggle.com/schoolpal/nn-stacking-magic-no-magic-30409-private-31063\n\nThe \"fake price\" properties are the investment properties with price <=1e6,  2e6, 3e6 at least. It is the community's conclusion that fake price prediction is impossible.  had a script to predict a few of them. I think this is one of few kernel works on this problem. This problem is the reason all our score is around 0.3. The bound of this game is at 0.29 if we don't solve the \"fake price\" problem (more info here https://www.kaggle.com/schoolpal/upper-bound-of-the-game)\n\nThe script is risky, since it involves a huge downscale for the \"false price\" properties. Prediction errors would have a huge negative impact on the private LB.  I used this as my final improvement, because Alijs and Evgeny were approaching me on the last day of this competition.\n\nThe script works on local cross validation, and works well on public LB.  I showed the top 10 and top 30's precision.  I think the precision is good for 2014/2015, 0.8/0.9 for top 10, 0.5/0.6 for top 30. In the competition, I used the top 15 results for the year 2015 in the test data. Those prices were downscaled by 0.5. That give me a huge improvement on public LB from 0.2965 to 0.29518. The sad part is that it did not improve the private LB but did not hurt much either. I guess the good and bad entries were cancelling out each other(unlikely?), or those \"fake prices\" I found are mostly in public LB?  But why? Can I even infer that the organizer actually just put most of those fake prices in the public LB and training data? This is the most confusing part for me.", "cell_type": "markdown", "outputs": []}, {"execution_count": null, "metadata": {"_execution_state": "busy", "trusted": false, "_cell_guid": "36d23ba2-ef5d-4323-a146-a5e78bea9708", "_uuid": "c6876a7dcdd27a08e99092069cdc4f89e4bbcd0c"}, "source": "from sklearn.model_selection import train_test_split,KFold\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import plot_tree\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pdb\nimport lightgbm as lgb\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport datetime\n#now = datetime.datetime.now()\nfrom collections import Counter\nimport pickle\n\ndef prepare_data():\n    train = pd.read_csv('../input/train.csv',parse_dates=['timestamp'])\n    test = pd.read_csv('../input/test.csv',parse_dates=['timestamp'])\n\n    # In the actual script, I used these scores which can be produced from\n    # kernels:\n    # https://www.kaggle.com/schoolpal/nn-model-lb-0-306-to-0-308\n    # https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script\n    # (xgb_train,xgb_test)=pickle.load(open('xgb_predicted.pkl'))\n    # (nn_train,nn_test)=pickle.load(open('nn_predicted.pkl'))\n    # (lgb_train,lgb_test)=pickle.load(open('lgb_predicted.pkl'))\n    # train['nn_score']=nn_train\n    # train['nn_score_log']=np.log(nn_train)\n    # train['xgb_score']=xgb_train\n    # train['xgb_score_log']=np.log(xgb_train)\n    \n    # test['nn_score']=nn_test\n    # test['nn_score_log']=np.log(nn_test)\n    # test['xgb_score']=xgb_test\n    # test['xgb_score_log']=np.log(xgb_test)\n    \n    train = train[(train.product_type=='Investment') & (train.timestamp>pd.to_datetime('2013-01-01'))]\n    test = test[(test.product_type=='Investment')]\n    id_test = test.id\n    full_sq=train.full_sq.copy()\n    full_sq[full_sq<5]=np.NaN\n    price_sq=train.price_doc/full_sq\n    y_train = ((train[\"price_doc\"]<=1e6) |  (train[\"price_doc\"]==2e6) |  (train[\"price_doc\"]==3e6) | (price_sq<30000) ).astype(int)\n    #can't merge train with test because the kernel run for very long time\n\n    num_train=len(train)\n    times=pd.concat([train.timestamp,test.timestamp])\n    x_train = train.drop(['id','timestamp','price_doc','product_type'], axis=1)\n    x_test = test.drop(['id','timestamp', 'product_type'], axis=1)\n    df_all=pd.concat([x_train,x_test])\n    df_all['olds']=times.dt.year-df_all.build_year\n    da=df_all\n    to_remove=[]\n    df_cat=None\n    for c in da.columns:\n        if da[c].dtype=='object':\n            oh=pd.get_dummies(da[c],prefix=c)\n            \n            if df_cat is None:\n                df_cat=oh\n            else:\n                df_cat=pd.concat([df_cat,oh],axis=1)\n            to_remove.append(c)\n    da.drop(to_remove,inplace=True,axis=1)\n    to_remove=[]\n    if df_cat is not None:\n        sums=df_cat.sum(axis=0)\n        to_remove=sums[sums<200].index.values\n        df_cat=df_cat.loc[:,df_cat.columns.difference(to_remove)]\n        da = pd.concat([da, df_cat], axis=1)\n\n\n    x_train=df_all[:len(x_train)]\n    x_test=df_all[len(x_train):]\n    return x_train,x_test,y_train,times\n\n\ndef model(x_train,y_train,x_test):\n    RS=1\n    np.random.seed(RS)\n    ROUNDS = 1500\n    params = {\n        'objective': 'binary',\n            'boosting': 'gbdt',\n            'learning_rate': 0.01 ,\n            'verbose': 0,\n            'num_leaves': 2 ** 5,\n            'bagging_fraction': 0.95,\n            'bagging_freq': 1,\n            'bagging_seed': RS,\n            'feature_fraction': 0.7,\n            'feature_fraction_seed': RS,\n            'max_bin': 100,\n            'max_depth': 7,\n            'num_rounds': ROUNDS,\n        }\n    train_lgb=lgb.Dataset(x_train,y_train)\n    model=lgb.train(params,train_lgb,num_boost_round=ROUNDS)\n    predict=model.predict(x_test)\n    return predict\ndef precision(predict,gold):\n    correct=np.count_nonzero((predict==1) & (gold==1))\n    p=float(correct)/np.count_nonzero(predict)\n    return p,correct\nx_train,x_test,y_train,times=prepare_data()\n\n# 5-fold cross validation\nskf = KFold(5,shuffle=False,random_state=1)\np_mean=0\ntop10=[]\ntop30=[]\nfor i,(train_inds,test_inds) in enumerate(skf.split(x_train)):\n    print('Working on CV '+str(i))\n    val=x_train.iloc[test_inds]\n    y_val=y_train.iloc[test_inds]\n    cv_train=x_train.iloc[train_inds]\n    start_time=times.iloc[test_inds[0]]\n    cv_y_train=y_train.iloc[train_inds]\n    y_predict=model(cv_train,cv_y_train,val)\n    sorted_inds=np.argsort(y_predict)[::-1]\n    labels=np.zeros(len(y_predict))\n    labels[sorted_inds[0:10]]=1\n    p,count=precision(labels,y_val)\n    top10.append((p,start_time))\n#    p_mean+=p\n#    print(str(i)+'. TOP 10 precision '+str(p)+' '+str(start_time))\n    labels=np.zeros(len(y_predict))\n    labels[sorted_inds[0:30]]=1\n    p,count=precision(labels,y_val)\n    top30.append((p,start_time))\n\nfor i,r in enumerate(top10):\n    print('Fold '+str(i)+'. TOP 10 precision '+str(r[0])+' '+str(r[1]))\n    \nfor i,r in enumerate(top30):\n    print('Fold '+str(i)+'. TOP 30 precision '+str(r[0])+' '+str(r[1]))\n\n", "cell_type": "code", "outputs": []}]}