{"cells":[{"metadata":{"_uuid":"59802896f3564cd41b68fac1a1f2eac7db92a37c"},"cell_type":"markdown","source":"**Fast and Basic Solution to Movie Review Sentiment Analysis using LSTM (forked from Ahmet Erdem)\n**\n\nI have used some of my previous code from Quora Duplicate Question Competition. https://github.com/aerdem4/kaggle-quora-dup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\nprint(\"Loading data...\")\ntrain = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\nprint(\"Train shape:\", train.shape)\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\nprint(\"Test shape:\", test.shape)\n\nenc = OneHotEncoder(sparse=False)\nenc.fit(train[\"Sentiment\"].values.reshape(-1, 1))\nprint(\"Number of classes:\", enc.n_values_[0])\n\nprint(\"Class distribution:\\n{}\".format(train[\"Sentiment\"].value_counts()/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db45eb314d6840613f1646cc1267093da740b436"},"cell_type":"markdown","source":"For the examples which occur in both sets, we can directly use the labels from train set as our prediction. (* Is it cheating ??? *)"},{"metadata":{"trusted":true,"_uuid":"0c15a8123acb6cc1d1967e47fb15d7c4333b9370"},"cell_type":"code","source":"print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(set(train[\"Phrase\"]).intersection(set(test[\"Phrase\"])))/test.shape[0]))\ntest = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb90d1b5026427f996075ea037b0972f32409dc"},"cell_type":"code","source":"print(\"Number of unique sentence in train: \", train.SentenceId.nunique())\nprint(\"Number of unique sentence in test: \", test.SentenceId.nunique())\n\nsentence_distribution = train.groupby(['SentenceId'])['Sentiment'].agg(['min','max','count','nunique']).reset_index().sort_values(\"nunique\", ascending = False)\nsentence_distribution[sentence_distribution['nunique'] >= 4]['SentenceId'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1facd8c4f934ba164ef2b60599f4b48da1fe7358"},"cell_type":"code","source":"train[train['SentenceId']==4268]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffe13579c17385c5d07459cf441346ed38ecf978"},"cell_type":"markdown","source":"There are multiple sentences with more thatn one unique sentiment"},{"metadata":{"_uuid":"f23154aa33cb61e925d0c9fb557915ceafc226c5"},"cell_type":"markdown","source":"Let's see if all the words in the test set occurs in the train set:"},{"metadata":{"trusted":true,"_uuid":"04bea1b921fb53f77ee8b069d864d2c103fec0ff"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv1 = CountVectorizer()\ncv1.fit(train[\"Phrase\"])\n\ncv2 = CountVectorizer()\ncv2.fit(test[\"Phrase\"])\n\nprint(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\nprint(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\nprint(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1710fd6e3ee2943ffad75f515c883b971f886c4a"},"cell_type":"markdown","source":"**TF-IDF**"},{"metadata":{"trusted":true,"_uuid":"1deb2c50b3e720eee550462e46db2f1d1236972d"},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c1797a493c52c468bc8cca8a657edc10cae6f0c"},"cell_type":"code","source":"def computeTF():\n    dicts = []\n    for i in range(0, 5):\n        tf_dict = {}\n        sentences = list(train[train['Sentiment']==i]['Phrase'].values)\n        for sentence in sentences:\n            sentence = sentence.lower()\n            words = sentence.split()\n            for word in words:\n                if word not in tf_dict:\n                    tf_dict[word] = 1\n                else:\n                    tf_dict[word] += 1\n        total_words = sum(tf_dict.values())\n        for word, val in tf_dict.items():\n            tf_dict[word] = val * 1.0/total_words\n        dicts.append(tf_dict)\n    return dicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"744778e47f760338c8ab10be2cb1dd7fe21fd56a"},"cell_type":"code","source":"def computeIDF(tf_dicts):\n    keys_all = []\n    idf_dict = {}\n    for i in range(0, 5):\n        keys_all += list(tf_dicts[i].keys())\n    for key in keys_all:\n        if key not in idf_dict:\n            idf_dict[key] = 1\n        else:\n            idf_dict[key] += 1\n    for word, val in idf_dict.items():\n            idf_dict[word] = math.log(5.0 / idf_dict[word])\n    return idf_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33e5d935e9847ecaf91f040e80c55a9eaaee4ddf"},"cell_type":"code","source":"def computeTFIDF(tf_dicts, idf_dict):\n    tfidf_dicts = []\n    for i in range(0, 5):\n        tfidf_dict = {}\n        for word, val in tf_dicts[i].items():\n            tfidf_dict[word] = tf_dicts[i][word] * idf_dict[word]\n        tfidf_dicts.append(tfidf_dict)\n    return tfidf_dicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"20770c856d9efbb66e544a227bbbf401c95752ec"},"cell_type":"code","source":"tf_dicts = computeTF()\nidf_dict = computeIDF(tf_dicts)\ntfidf_dicts = computeTFIDF(tf_dicts, idf_dict)\n\nkeywords = []\nfor i in range(0, 5):\n    important_words = sorted(tfidf_dicts[i].items(), key=lambda x: x[1], reverse=True)[1:100]\n    keywords.append([item[0] for item in important_words])\nall_keywords = [keyword for keyword_list in keywords for keyword in keyword_list ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d0cefa184547a5e02f5f389216e1a496f90e982"},"cell_type":"markdown","source":"**Numerical Feature Extraction**"},{"metadata":{"trusted":true,"_uuid":"2b4043b8831b8a42bd946f762375a3a97516dfc2"},"cell_type":"code","source":"def transform(df):\n    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n    df[\"sentence_start\"] = df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n    df[\"sentiment0_words\"] = df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[0]))))\n    df[\"sentiment1_words\"] = df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[1]))))\n    df[\"sentiment3_words\"] = df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[3]))))\n    df[\"sentiment4_words\"] = df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[4]))))\n    df[\"no_sentiment_words\"] = df[\"Phrase\"].apply(lambda x: len(set(x.split()))-len(set(x.split()).intersection(set(all_keywords))))\n    return df\n\ntrain = transform(train)\ntest = transform(test)\n\ndense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\", \n                  \"sentiment0_words\", \"sentiment1_words\",\"sentiment3_words\", \"sentiment4_words\", \"no_sentiment_words\"]\n\ntrain.groupby(\"Sentiment\")[dense_features].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8173232d306f83f14b5a03a7940caa89d2434f17"},"cell_type":"markdown","source":"**Splitting Data into folds**\n\nIf we split the data totally random, we may bias our validation set because the phrases in the same sentence may be distributed to train and validation sets. We need to guarantee that all phrases of one sentence is in one fold. We can assume that SentenceId%NUM_FOLDS preserves this while splitting the data randomly."},{"metadata":{"trusted":true,"_uuid":"628d31125d7bd1e3d5c3592466bcc1cfcc9e2b80"},"cell_type":"code","source":"print(max(train[\"phrase_count\"]))\nprint(max(train[\"word_count\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfaeff0ff308a70c2b254c1c6d90a97491f567c4"},"cell_type":"code","source":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"PhraseId\"].apply(lambda x: x%NUM_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"572a4bda2d8b1cd035f9c6fffa403e7f1b0fbbe4"},"cell_type":"markdown","source":"**Transfer Learning Using GLOVE Embeddings**"},{"metadata":{"trusted":true,"_uuid":"3e18956c6b5c5c88e7097c41138a2afb3f4bc1e1"},"cell_type":"code","source":"EMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\nEMBEDDING_DIM = 100\n\nall_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n\ndef get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()\nprint(\"Number of words in total:\", len(all_words))\nprint(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)), \", {0:.2f}\".format(len(all_words - set(embeddings_index))/len(all_words)), \"of all words\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f93b76d710ea94ed48627babc49bc5261647789"},"cell_type":"markdown","source":"**Prepare the sequences for LSTM**\n\n- Tokenizer: word to index\n- GloVe Embedding: word to vector"},{"metadata":{"trusted":true,"_uuid":"5d2f6c292c46b367b0ae7f98f8a1fbafc163a0e0"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer(filters=\"\")\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7f3d8de4369b676ffab460533ace55ed08a10ab"},"cell_type":"markdown","source":"**Define the Model**"},{"metadata":{"trusted":true,"_uuid":"878174c7d497cf794531afbcc8d81b5f59c803dd"},"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\ndef build_model():\n    embedding_layer = Embedding(nb_words,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    dropout = SpatialDropout1D(0.2)\n    mask_layer = Masking()\n    lstm_layer = LSTM(50)\n    \n    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n    dense_input = Input(shape=(len(dense_features),))\n    \n    dense_vector = BatchNormalization()(dense_input)\n    \n    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n    \n    feature_vector = concatenate([phrase_vector, dense_vector])\n    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n    \n    output = Dense(5, activation=\"softmax\")(feature_vector)\n    \n    model = Model(inputs=[seq_input, dense_input], outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6879dab4a2dc2daa0a63ab689a7a579a4c4baa5e"},"cell_type":"markdown","source":"**Train the Model:**"},{"metadata":{"trusted":true,"_uuid":"e901073b2456ce9f321f8c084fbb0e5084e8c0f4"},"cell_type":"code","source":"# Resampling\n\n# print(train_seq.shape)\n# print(train_dense.shape)\n# new_dataframe = pd.concat([\n#     train_dense,\n#     pd.DataFrame(train_seq, dtype=np.float64)\n# ], axis=1, ignore_index=True)\n# new_dataframe.head()\n\n# from imblearn.over_sampling import SMOTE, ADASYN\n# X_resampled, y_resampled = SMOTE().fit_sample(train_seq, train[train[\"fold_id\"] != 0][\"Sentiment\"].values)\n# from collections import Counter\n# print(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d4cdf9226840afc92b25b0ca75b4ff520dabf7b","scrolled":false},"cell_type":"code","source":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    \n    y_train = train[train[\"fold_id\"] != i][\"Sentiment\"].values\n    y_val = train[train[\"fold_id\"] == i][\"Sentiment\"].values\n    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    ## Early stopping\n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=2) # verbose = 1 ==> print out every line, which takes time\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=2)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=2)\n    print()\n    \ntest_preds /= NUM_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2be054d1df9f4e6732bc7c27d4837722bb7668e3"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfc473082e4f349ceb59a6df2e96f173f762195c"},"cell_type":"markdown","source":"**Making submission...**"},{"metadata":{"trusted":true,"_uuid":"39ed4370163acc2daa2fa48a43549c84166bdcb7"},"cell_type":"code","source":"print(\"Select the class with the highest probability as prediction...\")\ntest[\"pred\"] = test_preds.argmax(axis=1)\n\nprint(\"Use these predictions for the phrases which don't exist in train set...\")\ntest.loc[test[\"Sentiment\"].isnull(), \"Sentiment\"] = test.loc[test[\"Sentiment\"].isnull(), \"pred\"]\n\nprint(\"Make the submission ready...\")\ntest[\"Sentiment\"] = test[\"Sentiment\"].astype(int)\ntest[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d09df424c7ffb4e471f3f10cb1f3f5d340467ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}