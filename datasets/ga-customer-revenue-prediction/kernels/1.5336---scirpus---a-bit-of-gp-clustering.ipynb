{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport numpy as np \nimport pandas as pd \nimport json\nfrom pandas.io.json import json_normalize\nimport seaborn as sns \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"PATH=\"../input/\"\n \ncols_to_parse = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\ndef read_parse_dataframe(file_name):\n    #full path for the data file\n    path = PATH + file_name\n    #read the data file, convert the columns in the list of columns to parse using json loader,\n    #convert the `fullVisitorId` field as a string\n    data_df = pd.read_csv(path, \n        converters={column: json.loads for column in cols_to_parse}, \n        dtype={'fullVisitorId': 'str'})\n    #parse the json-type columns\n    for col in cols_to_parse:\n        #each column became a dataset, with the columns the fields of the Json type object\n        json_col_df = json_normalize(data_df[col])\n        json_col_df.columns = [f\"{col}.{sub_col}\" for sub_col in json_col_df.columns]\n        #we drop the object column processed and we add the columns created from the json fields\n        data_df = data_df.drop(col, axis=1).merge(json_col_df, right_index=True, left_index=True)\n\n    return data_df\n    \ndef process_date_time(data_df):\n\n    data_df['date'] = data_df['date'].astype(str)\n    data_df[\"date\"] = data_df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n    data_df[\"date\"] = pd.to_datetime(data_df[\"date\"])   \n    data_df[\"year\"] = data_df['date'].dt.year\n    data_df[\"month\"] = data_df['date'].dt.month\n    data_df[\"day\"] = data_df['date'].dt.day\n    data_df[\"weekday\"] = data_df['date'].dt.weekday\n    data_df['weekofyear'] = data_df['date'].dt.weekofyear\n    data_df['month.unique.user.count'] = data_df.groupby('month')['fullVisitorId'].transform('nunique')\n    data_df['day.unique.user.count'] = data_df.groupby('day')['fullVisitorId'].transform('nunique')\n    data_df['weekday.unique.user.count'] = data_df.groupby('weekday')['fullVisitorId'].transform('nunique')\n\n    \n    return data_df\n\ndef process_format(data_df):\n\n    for col in ['visitNumber', 'totals.hits', 'totals.pageviews']:\n        data_df[col] = data_df[col].astype(float)\n    data_df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\n    data_df['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n\n    return data_df\n    \ndef process_device(data_df):\n\n    data_df['browser.category'] = data_df['device.browser'] + '.' + data_df['device.deviceCategory']\n    data_df['browser.os'] = data_df['device.browser'] + '.' + data_df['device.operatingSystem']\n\n    return data_df\n\ndef process_totals(data_df):\n\n    data_df['visitNumber'] = (data_df['visitNumber'])\n    data_df['totals.hits'] = (data_df['totals.hits'])\n    data_df['totals.pageviews'] = (data_df['totals.pageviews'].fillna(0))\n    data_df['mean.hits.per.day'] = data_df.groupby(['day'])['totals.hits'].transform('mean')\n    data_df['sum.hits.per.day'] = data_df.groupby(['day'])['totals.hits'].transform('sum')\n    data_df['max.hits.per.day'] = data_df.groupby(['day'])['totals.hits'].transform('max')\n    data_df['min.hits.per.day'] = data_df.groupby(['day'])['totals.hits'].transform('min')\n    data_df['var.hits.per.day'] = data_df.groupby(['day'])['totals.hits'].transform('var')\n    data_df['mean.pageviews.per.day'] = data_df.groupby(['day'])['totals.pageviews'].transform('mean')\n    data_df['sum.pageviews.per.day'] = data_df.groupby(['day'])['totals.pageviews'].transform('sum')\n    data_df['max.pageviews.per.day'] = data_df.groupby(['day'])['totals.pageviews'].transform('max')\n    data_df['min.pageviews.per.day'] = data_df.groupby(['day'])['totals.pageviews'].transform('min')    \n\n    return data_df\n\ndef process_geo_network(data_df):\n\n    data_df['sum.pageviews.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    data_df['count.pageviews.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    data_df['mean.pageviews.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n    data_df['sum.hits.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    data_df['count.hits.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    data_df['mean.hits.per.network.domain'] = data_df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\n    return data_df\n\ndef process_traffic_source(data_df):\n\n    data_df['source.country'] = data_df['trafficSource.source'] + '.' + data_df['geoNetwork.country']\n    data_df['campaign.medium'] = data_df['trafficSource.campaign'] + '.' + data_df['trafficSource.medium']\n    data_df['medium.hits.mean'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('mean')\n    data_df['medium.hits.max'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('max')\n    data_df['medium.hits.min'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('min')\n    data_df['medium.hits.sum'] = data_df.groupby(['trafficSource.medium'])['totals.hits'].transform('sum')\n\n    return data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c80ad34587a2762bdc0006cf4dd01af9c8daa4fc"},"cell_type":"code","source":"train_df = read_parse_dataframe('train.csv')\ntest_df = read_parse_dataframe('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aac2944964ef1842b1672d4ab530971d93704d8"},"cell_type":"code","source":"train_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].astype(float)\ntrain_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].fillna(0)\ntrain_df['totals.transactionRevenue'] = np.log1p(train_df['totals.transactionRevenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24e7a938ea339e3d9c6063e3011fbbad2991b4ce"},"cell_type":"code","source":"cols_to_drop = [col for col in train_df.columns if train_df[col].nunique(dropna=False) == 1]\ntrain_df.drop(cols_to_drop, axis=1, inplace=True)\ntest_df.drop([col for col in cols_to_drop if col in test_df.columns], axis=1, inplace=True)\ntrain_df.drop(['trafficSource.campaignCode'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc03ed35fb751560c593d7b2ba8e21025da47ffd"},"cell_type":"code","source":"train_df = process_date_time(train_df)\ntrain_df = process_format(train_df)\ntrain_df = process_device(train_df)\ntrain_df = process_totals(train_df)\ntrain_df = process_geo_network(train_df)\ntrain_df = process_traffic_source(train_df)\n\ntest_df = process_date_time(test_df)\ntest_df = process_format(test_df)\ntest_df = process_device(test_df)\ntest_df = process_totals(test_df)\ntest_df = process_geo_network(test_df)\ntest_df = process_traffic_source(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18e21ace0dbc0af635d9720f2124e3c8c40233b5"},"cell_type":"code","source":"num_cols = ['month.unique.user.count', 'day.unique.user.count', 'weekday.unique.user.count',\n            'visitNumber', 'totals.hits', 'totals.pageviews', \n            'mean.hits.per.day', 'sum.hits.per.day', 'min.hits.per.day', 'max.hits.per.day', 'var.hits.per.day',\n            'mean.pageviews.per.day', 'sum.pageviews.per.day', 'min.pageviews.per.day', 'max.pageviews.per.day',\n            'sum.pageviews.per.network.domain', 'count.pageviews.per.network.domain', 'mean.pageviews.per.network.domain',\n            'sum.hits.per.network.domain', 'count.hits.per.network.domain', 'mean.hits.per.network.domain',\n            'medium.hits.mean','medium.hits.min','medium.hits.max','medium.hits.sum']\n                \nnot_used_cols = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \n        \"visitId\", \"visitStartTime\", 'totals.transactionRevenue', 'trafficSource.referralPath']\ncat_cols = [col for col in train_df.columns if col not in num_cols and col not in not_used_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6370a805da2f4690437815c286d3ab4b6de3295"},"cell_type":"code","source":"for col in num_cols:\n    train_df[col] = np.log1p((train_df[col].values)).astype('float32')\n    test_df[col] = np.log1p((test_df[col].values)).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"471f5744139f7a98c3f0954d9c95a25aaaf9750c"},"cell_type":"code","source":"test_df['totals.transactionRevenue'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f54ea2675f4b030e15c88f4c8ee2c82e0120faa"},"cell_type":"code","source":"for col in cat_cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str'))).astype('float32')\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str'))).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c5b414c1225c869057fedb01560794362fd8bbf"},"cell_type":"code","source":"train_df.fillna(-1,inplace=True,axis=1)\ntest_df.fillna(-1,inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd7cdaa55b77dda740725308ca909ded4925d4c3"},"cell_type":"code","source":"df = pd.concat([train_df,test_df],sort=False)\ndf = df.reset_index(drop=True)\nfor col in num_cols:\n    df.loc[:,col] = pd.cut(df[col], 50,labels=False)\ngp_test_df = df[df['totals.transactionRevenue']==-1].copy().reset_index(drop=True)\ngp_train_df = df[df['totals.transactionRevenue']!=-1].copy().reset_index(drop=True)\ndel df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e96599d44db883142e7595ec3d8d6b961a542d0"},"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a4f01a0131156afe3794a41ad10e4575e87b1e0"},"cell_type":"code","source":"features = num_cols+cat_cols\nfor c in features:\n    print(c)\n    tr,te =target_encode(gp_train_df[c], \n                         gp_test_df[c], \n                         target=(gp_train_df['totals.transactionRevenue']>0).astype(int), \n                         min_samples_leaf=50,\n                         smoothing=0,\n                         noise_level=0.0)\n    gp_train_df[c] = tr\n    gp_test_df[c] = te","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e6593efde5c88d6b583ec7ed991330c0c95aa5e"},"cell_type":"code","source":"df = pd.concat([gp_train_df,gp_test_df],sort=False)\ndel gp_train_df\ndel gp_test_df\ngc.collect()\nfrom sklearn.preprocessing import StandardScaler\nfor c in features:\n    ss = StandardScaler()\n    df.loc[~np.isfinite(df[c]),c] = np.nan\n    df.loc[~df[c].isnull(),c] = ss.fit_transform(df.loc[~df[c].isnull(),c].values.reshape(-1,1))\n    df[c].fillna(-99999,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60f7cade740918881f92199876625f813db9262c"},"cell_type":"code","source":"gp_test_df = df[df['totals.transactionRevenue']==-1].copy().reset_index(drop=True)\ngp_train_df = df[df['totals.transactionRevenue']!=-1].copy().reset_index(drop=True)\ndel df\ngc.collect()\ngp_test_df.drop('totals.transactionRevenue',axis=1,inplace=True)\ntest_df.drop('totals.transactionRevenue',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f8a831202018f08ad519d481487358c3f6d26f6"},"cell_type":"code","source":"def Output(p):\n    return 1.0/(1.0+np.exp(-p))\n\ndef GP1(data):\n    v = pd.DataFrame()\n    v[\"i0\"] = 0.100000*np.tanh(np.minimum(((3.0)), ((data[\"totals.pageviews\"])))) \n    v[\"i1\"] = 0.100000*np.tanh((((data[\"geoNetwork.region\"]) > (-1.0))*1.)) \n    v[\"i2\"] = 0.100000*np.tanh(np.minimum(((np.maximum(((data[\"mean.hits.per.network.domain\"])), ((3.0))))), ((3.0)))) \n    v[\"i3\"] = 0.100000*np.tanh(((2.0) + (data[\"geoNetwork.region\"]))) \n    v[\"i4\"] = 0.100000*np.tanh(((-1.0) * (((data[\"totals.pageviews\"]) * ((14.40841960906982422)))))) \n    v[\"i5\"] = 0.100000*np.tanh(((((((1.0) < (-2.0))*1.)) < (((data[\"geoNetwork.subContinent\"]) - (data[\"sum.pageviews.per.network.domain\"]))))*1.)) \n    v[\"i6\"] = 0.100000*np.tanh(np.maximum((((((data[\"totals.hits\"]) + (data[\"totals.pageviews\"]))/2.0))), ((data[\"totals.hits\"])))) \n    v[\"i7\"] = 0.100000*np.tanh(((2.0) * (data[\"totals.pageviews\"]))) \n    v[\"i8\"] = 0.100000*np.tanh(((0.0) - (data[\"totals.pageviews\"]))) \n    v[\"i9\"] = 0.100000*np.tanh(np.maximum(((0.0)), ((data[\"geoNetwork.continent\"])))) \n    v[\"i10\"] = 0.100000*np.tanh(((-1.0) * (data[\"totals.bounces\"]))) \n    v[\"i11\"] = 0.100000*np.tanh((((-3.0) < (((3.0) - ((7.0)))))*1.)) \n    v[\"i12\"] = 0.100000*np.tanh(((np.maximum(((data[\"mean.pageviews.per.network.domain\"])), (((((-3.0) > (data[\"totals.hits\"]))*1.))))) - (data[\"totals.hits\"]))) \n    v[\"i13\"] = 0.100000*np.tanh((((-1.0*((-1.0)))) / 2.0)) \n    v[\"i14\"] = 0.100000*np.tanh((((np.maximum(((-1.0)), ((np.maximum(((data[\"browser.category\"])), ((1.0))))))) > ((-1.0*((data[\"totals.pageviews\"])))))*1.)) \n    v[\"i15\"] = 0.100000*np.tanh(np.minimum(((data[\"totals.pageviews\"])), ((data[\"source.country\"])))) \n    v[\"i16\"] = 0.100000*np.tanh(((((1.0)) > (data[\"trafficSource.isTrueDirect\"]))*1.)) \n    v[\"i17\"] = 0.100000*np.tanh((((data[\"geoNetwork.region\"]) + (data[\"totals.pageviews\"]))/2.0)) \n    v[\"i18\"] = 0.100000*np.tanh((((data[\"mean.hits.per.network.domain\"]) + ((((data[\"totals.pageviews\"]) > (3.0))*1.)))/2.0)) \n    v[\"i19\"] = 0.100000*np.tanh((((data[\"sum.hits.per.network.domain\"]) < ((((9.0)) + (data[\"geoNetwork.city\"]))))*1.)) \n    v[\"i20\"] = 0.100000*np.tanh(np.minimum(((data[\"totals.pageviews\"])), (((((data[\"visitNumber\"]) + (((data[\"source.country\"]) * (data[\"totals.pageviews\"]))))/2.0))))) \n    v[\"i21\"] = 0.100000*np.tanh((((((((((data[\"totals.pageviews\"]) + (-1.0))/2.0)) + (((data[\"totals.pageviews\"]) + (data[\"totals.pageviews\"]))))/2.0)) + ((((data[\"totals.pageviews\"]) + (data[\"totals.bounces\"]))/2.0)))/2.0)) \n    v[\"i22\"] = 0.100000*np.tanh((((data[\"trafficSource.source\"]) < ((((np.tanh((data[\"mean.pageviews.per.network.domain\"]))) < ((((data[\"geoNetwork.subContinent\"]) < ((-1.0*((data[\"geoNetwork.networkDomain\"])))))*1.)))*1.)))*1.)) \n    v[\"i23\"] = 0.100000*np.tanh(np.minimum(((data[\"totals.pageviews\"])), ((data[\"geoNetwork.subContinent\"])))) \n    v[\"i24\"] = 0.100000*np.tanh(np.maximum(((-3.0)), ((np.where(((data[\"totals.pageviews\"]) * 2.0)<0, (1.0), -3.0 ))))) \n    v[\"i25\"] = 0.100000*np.tanh((((((((data[\"totals.pageviews\"]) - (data[\"totals.hits\"]))) + (data[\"totals.pageviews\"]))/2.0)) / 2.0)) \n    v[\"i26\"] = 0.100000*np.tanh(np.where(data[\"totals.pageviews\"]>0, data[\"source.country\"], np.minimum(((data[\"source.country\"])), ((data[\"totals.pageviews\"]))) )) \n    v[\"i27\"] = 0.099951*np.tanh(((np.minimum((((((data[\"totals.pageviews\"]) + (data[\"totals.pageviews\"]))/2.0))), ((data[\"source.country\"])))) + (data[\"totals.pageviews\"]))) \n    v[\"i28\"] = 0.100000*np.tanh(((((np.minimum(((data[\"source.country\"])), (((((data[\"source.country\"]) + (data[\"totals.pageviews\"]))/2.0))))) + ((((data[\"totals.pageviews\"]) + (-3.0))/2.0)))) * 2.0)) \n    v[\"i29\"] = 0.100000*np.tanh((((((data[\"totals.pageviews\"]) * 2.0)) < ((((data[\"medium.hits.sum\"]) < (data[\"totals.hits\"]))*1.)))*1.)) \n    v[\"i30\"] = 0.100000*np.tanh(np.tanh((((np.where(-1.0>0, 1.0, data[\"totals.hits\"] )) * 2.0)))) \n    v[\"i31\"] = 0.100000*np.tanh(((((((data[\"totals.pageviews\"]) + (((data[\"trafficSource.source\"]) - (3.0))))/2.0)) + (((data[\"totals.pageviews\"]) - (data[\"totals.hits\"]))))/2.0)) \n    v[\"i32\"] = 0.100000*np.tanh(((((((np.minimum(((data[\"totals.pageviews\"])), ((data[\"totals.newVisits\"])))) * 2.0)) * 2.0)) + ((((data[\"totals.pageviews\"]) + (np.minimum(((data[\"totals.newVisits\"])), ((data[\"totals.newVisits\"])))))/2.0)))) \n    v[\"i33\"] = 0.100000*np.tanh(((((np.minimum(((data[\"totals.hits\"])), ((((data[\"browser.os\"]) * 2.0))))) * 2.0)) + ((((data[\"geoNetwork.networkDomain\"]) + (data[\"totals.pageviews\"]))/2.0)))) \n    v[\"i34\"] = 0.100000*np.tanh((((-1.0*((np.minimum(((data[\"source.country\"])), ((((((data[\"totals.pageviews\"]) / 2.0)) + (-2.0))))))))) - (data[\"visitNumber\"]))) \n    v[\"i35\"] = 0.100000*np.tanh(((np.minimum(((data[\"totals.pageviews\"])), ((data[\"totals.pageviews\"])))) + (np.where(((np.minimum(((data[\"totals.bounces\"])), ((data[\"totals.newVisits\"])))) * 2.0)>0, data[\"totals.bounces\"], -3.0 )))) \n    v[\"i36\"] = 0.100000*np.tanh(np.where(np.where((-1.0*((data[\"geoNetwork.country\"]))) < -99998, data[\"geoNetwork.metro\"], data[\"geoNetwork.country\"] )>0, (-1.0*((data[\"totals.bounces\"]))), (-1.0*((data[\"geoNetwork.city\"]))) )) \n    v[\"i37\"] = 0.100000*np.tanh(((np.minimum(((data[\"device.deviceCategory\"])), ((data[\"source.country\"])))) + (((((np.minimum(((data[\"device.deviceCategory\"])), ((((data[\"totals.pageviews\"]) * 2.0))))) * 2.0)) * 2.0)))) \n    v[\"i38\"] = 0.100000*np.tanh(np.minimum(((data[\"totals.newVisits\"])), ((data[\"totals.newVisits\"])))) \n    v[\"i39\"] = 0.100000*np.tanh(((((((np.tanh(((-1.0*((data[\"browser.os\"])))))) * 2.0)) * 2.0)) * 2.0)) \n    v[\"i40\"] = 0.100000*np.tanh((((((((((data[\"totals.hits\"]) / 2.0)) < (data[\"totals.pageviews\"]))*1.)) + (data[\"source.country\"]))) * 2.0)) \n    v[\"i41\"] = 0.100000*np.tanh(((data[\"totals.hits\"]) - (data[\"totals.pageviews\"]))) \n    v[\"i42\"] = 0.100000*np.tanh(((((data[\"totals.pageviews\"]) - (data[\"totals.hits\"]))) * (data[\"totals.hits\"]))) \n    v[\"i43\"] = 0.100000*np.tanh(((np.minimum(((data[\"month.unique.user.count\"])), ((data[\"source.country\"])))) + (((np.minimum(((data[\"month.unique.user.count\"])), ((((np.minimum(((data[\"month.unique.user.count\"])), ((data[\"totals.pageviews\"])))) * 2.0))))) * 2.0)))) \n    v[\"i44\"] = 0.100000*np.tanh(np.tanh(((-1.0*((data[\"totals.hits\"])))))) \n    v[\"i45\"] = 0.100000*np.tanh(np.minimum(((data[\"device.deviceCategory\"])), ((np.where(((data[\"device.deviceCategory\"]) * 2.0)>0, data[\"totals.pageviews\"], data[\"totals.pageviews\"] ))))) \n    v[\"i46\"] = 0.100000*np.tanh(((data[\"device.operatingSystem\"]) + (np.where(data[\"geoNetwork.city\"]>0, ((data[\"browser.os\"]) + (data[\"trafficSource.source\"])), ((data[\"trafficSource.source\"]) + (data[\"geoNetwork.country\"])) )))) \n    v[\"i47\"] = 0.100000*np.tanh((((((data[\"geoNetwork.country\"]) + (data[\"totals.pageviews\"]))) + (data[\"totals.bounces\"]))/2.0)) \n    v[\"i48\"] = 0.100000*np.tanh(((data[\"totals.hits\"]) - ((4.81635427474975586)))) \n    v[\"i49\"] = 0.100000*np.tanh(((np.minimum(((data[\"month.unique.user.count\"])), ((data[\"totals.pageviews\"])))) * 2.0)) \n    v[\"i50\"] = 0.097362*np.tanh((((data[\"geoNetwork.metro\"]) < (((0.0) * (data[\"geoNetwork.city\"]))))*1.)) \n    v[\"i51\"] = 0.100000*np.tanh(((((((((((((((data[\"geoNetwork.subContinent\"]) * (data[\"geoNetwork.region\"]))) * 2.0)) * (data[\"totals.pageviews\"]))) * 2.0)) * 2.0)) * (data[\"totals.pageviews\"]))) * 2.0)) \n    v[\"i52\"] = 0.100000*np.tanh(((((np.where(data[\"totals.pageviews\"]<0, data[\"totals.pageviews\"], data[\"day\"] )) * 2.0)) * 2.0)) \n    v[\"i53\"] = 0.100000*np.tanh((((((((data[\"totals.pageviews\"]) + (data[\"mean.pageviews.per.network.domain\"]))) < (data[\"source.country\"]))*1.)) * (((((((data[\"browser.category\"]) + (data[\"mean.pageviews.per.network.domain\"]))/2.0)) > (3.0))*1.)))) \n    v[\"i54\"] = 0.100000*np.tanh((((((((((data[\"browser.os\"]) > (data[\"totals.hits\"]))*1.)) > (((((9.0)) < (data[\"channelGrouping\"]))*1.)))*1.)) < ((((data[\"totals.pageviews\"]) < (data[\"trafficSource.source\"]))*1.)))*1.)) \n    v[\"i55\"] = 0.099951*np.tanh((((-2.0) + (data[\"device.deviceCategory\"]))/2.0)) \n    v[\"i56\"] = 0.099951*np.tanh(((np.tanh((data[\"totals.hits\"]))) + (((((data[\"totals.hits\"]) - (data[\"totals.pageviews\"]))) + (data[\"totals.hits\"]))))) \n    v[\"i57\"] = 0.100000*np.tanh(((((((((data[\"trafficSource.campaign\"]) + (data[\"count.hits.per.network.domain\"]))) + (data[\"trafficSource.campaign\"]))/2.0)) < (data[\"geoNetwork.continent\"]))*1.)) \n    v[\"i58\"] = 0.099902*np.tanh(((((np.minimum(((np.minimum(((data[\"weekday\"])), ((data[\"totals.bounces\"]))))), ((data[\"totals.bounces\"])))) * 2.0)) * 2.0)) \n    v[\"i59\"] = 0.100000*np.tanh(((data[\"geoNetwork.country\"]) * (data[\"totals.hits\"]))) \n    v[\"i60\"] = 0.100000*np.tanh((((-3.0) + (((((((-3.0) + (data[\"geoNetwork.city\"]))) + ((((data[\"totals.pageviews\"]) + (data[\"geoNetwork.city\"]))/2.0)))) + (data[\"totals.pageviews\"]))))/2.0)) \n    v[\"i61\"] = 0.100000*np.tanh(((-2.0) * (((data[\"totals.pageviews\"]) - (data[\"totals.hits\"])))))\n    return v.sum(axis=1)\n\ndef GP2(data):\n    v = pd.DataFrame()\n    v[\"i0\"] = 0.100000*np.tanh(((((-1.0*((data[\"device.isMobile\"])))) < (np.where(-1.0 < -99998, data[\"totals.hits\"], (((-1.0*((2.0)))) - (3.0)) )))*1.)) \n    v[\"i1\"] = 0.100000*np.tanh((((data[\"totals.pageviews\"]) > (-3.0))*1.)) \n    v[\"i2\"] = 0.100000*np.tanh(((np.where(np.minimum(((2.0)), ((data[\"geoNetwork.region\"])))<0, np.maximum(((data[\"totals.hits\"])), ((-2.0))), 2.0 )) - (data[\"mean.hits.per.network.domain\"]))) \n    v[\"i3\"] = 0.100000*np.tanh(((3.0) - (data[\"totals.hits\"]))) \n    v[\"i4\"] = 0.100000*np.tanh(((data[\"totals.pageviews\"]) * ((6.41743326187133789)))) \n    v[\"i5\"] = 0.100000*np.tanh(np.minimum(((2.0)), ((data[\"geoNetwork.country\"])))) \n    v[\"i6\"] = 0.100000*np.tanh(((data[\"totals.pageviews\"]) * ((3.0)))) \n    v[\"i7\"] = 0.100000*np.tanh((((-2.0) > (np.where(2.0<0, 1.0, (((data[\"geoNetwork.networkDomain\"]) + (data[\"browser.os\"]))/2.0) )))*1.)) \n    v[\"i8\"] = 0.100000*np.tanh((((((0.0) - (data[\"totals.hits\"]))) < (-3.0))*1.)) \n    v[\"i9\"] = 0.100000*np.tanh(((data[\"totals.pageviews\"]) * (data[\"totals.hits\"]))) \n    v[\"i10\"] = 0.100000*np.tanh(((data[\"geoNetwork.country\"]) + (data[\"totals.hits\"]))) \n    v[\"i11\"] = 0.100000*np.tanh(np.where(np.tanh((2.0))<0, (-1.0*((((2.0) / 2.0)))), data[\"totals.pageviews\"] )) \n    v[\"i12\"] = 0.100000*np.tanh(((((((data[\"totals.hits\"]) > (0.0))*1.)) > ((((2.0) < (data[\"totals.pageviews\"]))*1.)))*1.)) \n    v[\"i13\"] = 0.100000*np.tanh(((np.where(data[\"geoNetwork.continent\"]>0, data[\"totals.pageviews\"], (((np.tanh((data[\"sum.hits.per.network.domain\"]))) < (((data[\"geoNetwork.metro\"]) / 2.0)))*1.) )) / 2.0)) \n    v[\"i14\"] = 0.100000*np.tanh(((data[\"geoNetwork.country\"]) + (data[\"totals.pageviews\"]))) \n    v[\"i15\"] = 0.100000*np.tanh((-1.0*((((data[\"totals.pageviews\"]) - (1.0)))))) \n    v[\"i16\"] = 0.100000*np.tanh((((((8.0)) * (np.minimum(((((data[\"geoNetwork.continent\"]) / 2.0))), ((np.minimum(((data[\"geoNetwork.continent\"])), ((data[\"totals.pageviews\"]))))))))) + (((data[\"geoNetwork.continent\"]) / 2.0)))) \n    v[\"i17\"] = 0.100000*np.tanh(np.minimum((((((data[\"source.country\"]) + (np.minimum((((((data[\"source.country\"]) + ((((-1.0) + (data[\"totals.pageviews\"]))/2.0)))/2.0))), ((data[\"totals.pageviews\"])))))/2.0))), ((data[\"totals.pageviews\"])))) \n    v[\"i18\"] = 0.100000*np.tanh((((((np.minimum(((data[\"geoNetwork.country\"])), ((data[\"totals.pageviews\"])))) * 2.0)) + (((np.minimum(((data[\"geoNetwork.country\"])), ((data[\"device.isMobile\"])))) + (data[\"totals.pageviews\"]))))/2.0)) \n    v[\"i19\"] = 0.100000*np.tanh(np.minimum((((1.24994897842407227))), (((((0.0) > ((((-3.0) < (data[\"trafficSource.source\"]))*1.)))*1.))))) \n    v[\"i20\"] = 0.100000*np.tanh(np.maximum(((data[\"totals.newVisits\"])), ((data[\"totals.hits\"])))) \n    v[\"i21\"] = 0.100000*np.tanh((((data[\"totals.newVisits\"]) < ((((2.0) < ((((data[\"browser.os\"]) > (data[\"visitNumber\"]))*1.)))*1.)))*1.)) \n    v[\"i22\"] = 0.100000*np.tanh(np.maximum(((np.minimum(((np.minimum(((data[\"totals.hits\"])), ((data[\"geoNetwork.subContinent\"]))))), ((data[\"totals.hits\"]))))), ((-1.0)))) \n    v[\"i23\"] = 0.100000*np.tanh((((((data[\"geoNetwork.subContinent\"]) * (np.where(data[\"totals.pageviews\"]<0, data[\"totals.pageviews\"], data[\"totals.pageviews\"] )))) + ((((data[\"totals.pageviews\"]) + (data[\"totals.pageviews\"]))/2.0)))/2.0)) \n    v[\"i24\"] = 0.100000*np.tanh(np.minimum((((5.0))), ((data[\"trafficSource.adwordsClickInfo.page\"])))) \n    v[\"i25\"] = 0.100000*np.tanh(np.tanh((np.minimum((((3.0))), ((np.where(2.0<0, data[\"count.pageviews.per.network.domain\"], -1.0 ))))))) \n    v[\"i26\"] = 0.100000*np.tanh(np.where(((data[\"totals.pageviews\"]) * 2.0)<0, -3.0, (((((data[\"totals.pageviews\"]) * 2.0)) + (((data[\"totals.hits\"]) * 2.0)))/2.0) )) \n    v[\"i27\"] = 0.099951*np.tanh(np.maximum(((data[\"totals.hits\"])), ((np.tanh(((((data[\"totals.hits\"]) + (np.minimum(((np.tanh((((1.0) * 2.0))))), ((data[\"totals.pageviews\"])))))/2.0))))))) \n    v[\"i28\"] = 0.100000*np.tanh(np.minimum(((data[\"source.country\"])), ((np.tanh((((((data[\"totals.pageviews\"]) - ((1.56775867938995361)))) + (data[\"totals.newVisits\"])))))))) \n    v[\"i29\"] = 0.100000*np.tanh(np.where(data[\"totals.pageviews\"]>0, ((data[\"totals.pageviews\"]) - ((((data[\"totals.bounces\"]) > (((data[\"trafficSource.keyword\"]) * 2.0)))*1.))), (((data[\"trafficSource.adwordsClickInfo.isVideoAd\"]) < (data[\"totals.pageviews\"]))*1.) )) \n    v[\"i30\"] = 0.100000*np.tanh(((((((data[\"totals.pageviews\"]) + (data[\"trafficSource.source\"]))) + (-3.0))) + (((data[\"trafficSource.isTrueDirect\"]) + (data[\"trafficSource.isTrueDirect\"]))))) \n    v[\"i31\"] = 0.100000*np.tanh(((np.minimum(((np.where(data[\"geoNetwork.continent\"]>0, data[\"totals.pageviews\"], ((data[\"geoNetwork.region\"]) * 2.0) ))), ((((((data[\"geoNetwork.continent\"]) * 2.0)) * 2.0))))) * 2.0)) \n    v[\"i32\"] = 0.100000*np.tanh((((((((((((((-3.0) / 2.0)) < (-2.0))*1.)) < (((-3.0) / 2.0)))*1.)) * 2.0)) > ((13.35829830169677734)))*1.)) \n    v[\"i33\"] = 0.100000*np.tanh(np.minimum(((((np.minimum(((data[\"totals.pageviews\"])), ((data[\"source.country\"])))) + ((((data[\"totals.pageviews\"]) + (((-3.0) + (data[\"source.country\"]))))/2.0))))), ((data[\"source.country\"])))) \n    v[\"i34\"] = 0.100000*np.tanh(((np.maximum(((data[\"totals.pageviews\"])), ((data[\"trafficSource.isTrueDirect\"])))) - (np.maximum(((data[\"trafficSource.isTrueDirect\"])), ((data[\"totals.pageviews\"])))))) \n    v[\"i35\"] = 0.100000*np.tanh((((3.0) < (data[\"totals.hits\"]))*1.)) \n    v[\"i36\"] = 0.100000*np.tanh(((np.minimum(((((((((data[\"source.country\"]) * 2.0)) * 2.0)) + (data[\"trafficSource.keyword\"])))), ((data[\"totals.pageviews\"])))) * 2.0)) \n    v[\"i37\"] = 0.100000*np.tanh(((((((((-1.0) > ((((np.tanh((np.tanh((np.tanh(((6.0)))))))) > (-3.0))*1.)))*1.)) / 2.0)) > (data[\"trafficSource.source\"]))*1.)) \n    v[\"i38\"] = 0.100000*np.tanh(np.minimum(((((data[\"visitNumber\"]) + (data[\"browser.os\"])))), ((((((-3.0) + (data[\"totals.pageviews\"]))) + (data[\"geoNetwork.region\"])))))) \n    v[\"i39\"] = 0.100000*np.tanh(((np.maximum(((((2.0) + (data[\"geoNetwork.continent\"])))), ((0.0)))) * (((data[\"totals.hits\"]) - (data[\"totals.pageviews\"]))))) \n    v[\"i40\"] = 0.100000*np.tanh(np.where(((data[\"trafficSource.isTrueDirect\"]) + (((data[\"geoNetwork.metro\"]) + (((data[\"trafficSource.isTrueDirect\"]) + (data[\"visitNumber\"]))))))<0, -2.0, data[\"geoNetwork.country\"] )) \n    v[\"i41\"] = 0.100000*np.tanh((((((((np.minimum(((((data[\"totals.bounces\"]) * 2.0))), ((data[\"totals.newVisits\"])))) * 2.0)) * 2.0)) + (data[\"geoNetwork.metro\"]))/2.0)) \n    v[\"i42\"] = 0.100000*np.tanh(np.where(((data[\"source.country\"]) + (data[\"visitNumber\"]))>0, data[\"totals.bounces\"], -2.0 )) \n    v[\"i43\"] = 0.100000*np.tanh((((data[\"channelGrouping\"]) > (data[\"totals.pageviews\"]))*1.)) \n    v[\"i44\"] = 0.100000*np.tanh(((data[\"source.country\"]) + (((((data[\"source.country\"]) * 2.0)) + (np.where(data[\"trafficSource.isTrueDirect\"]<0, ((data[\"source.country\"]) * 2.0), ((data[\"device.operatingSystem\"]) * 2.0) )))))) \n    v[\"i45\"] = 0.100000*np.tanh(np.where(data[\"geoNetwork.country\"]>0, ((((2.0)) > (data[\"trafficSource.source\"]))*1.), 3.0 )) \n    v[\"i46\"] = 0.100000*np.tanh(((((data[\"totals.pageviews\"]) - ((5.0)))) * 2.0)) \n    v[\"i47\"] = 0.100000*np.tanh((-1.0*((((((data[\"trafficSource.source\"]) * (((data[\"month.unique.user.count\"]) / 2.0)))) + (((data[\"source.country\"]) * (data[\"month.unique.user.count\"])))))))) \n    v[\"i48\"] = 0.097362*np.tanh((((data[\"browser.os\"]) + (((data[\"totals.pageviews\"]) - (np.where(data[\"totals.hits\"]>0, data[\"totals.hits\"], data[\"device.operatingSystem\"] )))))/2.0)) \n    v[\"i49\"] = 0.100000*np.tanh(np.where(data[\"geoNetwork.continent\"]<0, ((np.minimum(((data[\"totals.bounces\"])), ((data[\"geoNetwork.continent\"])))) * 2.0), data[\"totals.bounces\"] )) \n    v[\"i50\"] = 0.100000*np.tanh(((-3.0) + (((np.where(((data[\"geoNetwork.metro\"]) * 2.0)<0, np.where(-2.0<0, data[\"totals.newVisits\"], data[\"geoNetwork.metro\"] ), data[\"geoNetwork.metro\"] )) * 2.0)))) \n    v[\"i51\"] = 0.100000*np.tanh(np.tanh((np.tanh((-1.0))))) \n    v[\"i52\"] = 0.100000*np.tanh(((data[\"totals.bounces\"]) + (((((((data[\"totals.pageviews\"]) - (data[\"totals.hits\"]))) * 2.0)) * (data[\"totals.bounces\"]))))) \n    v[\"i53\"] = 0.100000*np.tanh((((((np.minimum(((data[\"visitNumber\"])), ((data[\"geoNetwork.networkDomain\"])))) + (data[\"totals.bounces\"]))/2.0)) * 2.0)) \n    v[\"i54\"] = 0.099951*np.tanh((((2.0)) - (data[\"geoNetwork.region\"]))) \n    v[\"i55\"] = 0.099951*np.tanh(((data[\"totals.pageviews\"]) + (np.minimum(((data[\"mean.pageviews.per.network.domain\"])), ((np.where(data[\"geoNetwork.continent\"]<0, (5.0), ((np.minimum(((data[\"geoNetwork.continent\"])), ((data[\"geoNetwork.networkDomain\"])))) * 2.0) ))))))) \n    v[\"i56\"] = 0.100000*np.tanh(((data[\"source.country\"]) + (((np.where(data[\"totals.pageviews\"]<0, data[\"geoNetwork.city\"], ((data[\"totals.pageviews\"]) - (data[\"source.country\"])) )) * 2.0)))) \n    v[\"i57\"] = 0.100000*np.tanh((((-1.0*((((data[\"totals.pageviews\"]) - (np.where(np.tanh((data[\"count.hits.per.network.domain\"])) < -99998, data[\"totals.pageviews\"], data[\"totals.hits\"] ))))))) * (((data[\"totals.hits\"]) * 2.0)))) \n    v[\"i58\"] = 0.100000*np.tanh((((data[\"source.country\"]) + (((np.maximum(((data[\"device.operatingSystem\"])), ((((data[\"geoNetwork.region\"]) + (data[\"source.country\"])))))) + (((data[\"source.country\"]) + (data[\"source.country\"]))))))/2.0)) \n    v[\"i59\"] = 0.100000*np.tanh(((((((data[\"channelGrouping\"]) - (data[\"totals.pageviews\"]))) + (3.0))) / 2.0)) \n    v[\"i60\"] = 0.100000*np.tanh(((((((-2.0) < (((data[\"month.unique.user.count\"]) - ((-1.0*((0.0)))))))*1.)) > (((((7.0)) < (data[\"geoNetwork.city\"]))*1.)))*1.)) \n    v[\"i61\"] = 0.100000*np.tanh(((((((np.where(data[\"totals.pageviews\"]>0, ((data[\"totals.pageviews\"]) - (data[\"channelGrouping\"])), data[\"geoNetwork.country\"] )) - (data[\"totals.hits\"]))) * 2.0)) - (data[\"channelGrouping\"]))) \n    v[\"i62\"] = 0.100000*np.tanh(np.tanh((np.maximum((((-1.0*((data[\"geoNetwork.country\"]))))), (((5.0)))))))\n    return v.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96a90b6f00b15ae6d17eb3a147d030d8b9b20407"},"cell_type":"code","source":"cm = plt.cm.get_cmap('Greys')\nfig, axes = plt.subplots(1, 1, figsize=(15, 15))\nsc = axes.scatter(GP1(gp_train_df),\n                  GP2(gp_train_df),\n                  alpha=.5,\n                  c=(gp_train_df['totals.transactionRevenue']),\n                  cmap=cm,\n                  s=30)\ncbar = fig.colorbar(sc, ax=axes)\ncbar.set_label('Target')\n_ = axes.set_title(\"Clustering colored by target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8352d571980d2210f79ebb4d84fd5e1f05000c49"},"cell_type":"code","source":"train_df['gp1'] = GP1(gp_train_df).values\ntrain_df['gp2'] = GP2(gp_train_df).values\ntest_df['gp1'] = GP1(gp_test_df).values\ntest_df['gp2'] = GP2(gp_test_df).values\ndel gp_train_df\ndel gp_test_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d2da20ec81a047e7dda9a8f00c4e9ce24293e54"},"cell_type":"code","source":"train_df = train_df.sort_values('date')\nX = train_df.drop(not_used_cols, axis=1)\ny = train_df['totals.transactionRevenue']\nX_test = test_df.drop([col for col in not_used_cols if col in test_df.columns], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa8efd62fa6c078d8b7e53cde54cdfee2e364df2"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import model_selection, preprocessing, metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlgb_params = {'num_leaves': 300,\n             'min_data_in_leaf': 30, \n             'objective':'regression',\n             'max_depth': -1,\n             'learning_rate': 0.01,\n             \"min_child_samples\": 50,\n             \"boosting\": \"rf\",\n             \"feature_fraction\": 0.9,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.8,\n             \"bagging_seed\": 11,\n             \"metric\": 'rmse',\n             \"lambda_l1\": 1,\n             \"verbosity\": -1\n             }\n             \nlgb_params1 = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \"max_depth\": 5, \"min_child_samples\": 100, \"reg_alpha\": 1, \"reg_lambda\": 1,\n        \"num_leaves\" : 257, \"learning_rate\" : 0.01, \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \"verbosity\": -1}\n\nrun_lgb = True\nprint('LGB : ', run_lgb)\n# modeling\n#--------------------------------------------------------------------------\nif run_lgb:\n    import lightgbm as lgb\n    def kfold_lgb_xgb():\n        folds = KFold(n_splits=5, shuffle=True, random_state=7)\n        \n        oof_lgb = np.zeros(len(train_df))\n        predictions_lgb = np.zeros(len(test_df))\n\n        features_lgb = list(X.columns)\n        feature_importance_df_lgb = pd.DataFrame()\n\n        for fold_, (trn_idx, val_idx) in enumerate(folds.split(X)):\n            trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n            val_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n            \n            print(\"LGB \" + str(fold_) + \"-\" * 50)\n            num_round = 20000\n            clf = lgb.train(lgb_params1, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n            oof_lgb[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n            \n            fold_importance_df_lgb = pd.DataFrame()\n            fold_importance_df_lgb[\"feature\"] = features_lgb\n            fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n            fold_importance_df_lgb[\"fold\"] = fold_ + 1\n            feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n            predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n        \n        #lgb.plot_importance(clf, max_num_features=30)    \n        cols = feature_importance_df_lgb[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:50].index\n        best_features_lgb = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n        plt.figure(figsize=(14,10))\n        sns.barplot(x=\"importance\", y=\"feature\", data=best_features_lgb.sort_values(by=\"importance\", ascending=False))\n        plt.title('LightGBM Features (avg over folds)')\n        plt.tight_layout()\n        plt.savefig('lgbm_importances.png')\n        x = []\n        for i in oof_lgb:\n            if i < 0:\n                x.append(0.0)\n            else:\n                x.append(i)\n        cv_lgb = mean_squared_error(x, y)**0.5\n        cv_lgb = str(cv_lgb)\n        cv_lgb = cv_lgb[:10]\n        \n        pd.DataFrame({'preds': x}).to_csv('lgb_oof_' + cv_lgb + '.csv', index = False)\n        \n        print(\"CV_LGB : \", cv_lgb)\n        return cv_lgb, predictions_lgb\n        \n    cv_lgb, lgb_ans = kfold_lgb_xgb()\n    x = []\n    for i in lgb_ans:\n        if i < 0:\n            x.append(0.0)\n        else:\n            x.append(i)\n    np.save('lgb_ans.npy', x)\n    submission = test_df[['fullVisitorId']].copy()\n    submission.loc[:, 'PredictedLogRevenue'] = x\n    submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\n    submission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\n    grouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\n    grouped_test.to_csv('lgb_' + cv_lgb + '.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}