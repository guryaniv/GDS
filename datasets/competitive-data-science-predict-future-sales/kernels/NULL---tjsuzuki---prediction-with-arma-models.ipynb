{"cells":[{"metadata":{"_cell_guid":"db403c26-b257-47c1-bd44-e6251d133da4","_uuid":"e228981f6f49cbbd50ceceadec89b4bcfff2cd7d"},"cell_type":"markdown","source":"# Contents\n\nIn this kernel, I will predict the total amount of monthly seles by using ARMA models. This is my first contribution to kaggle kernels, and any advises, questions and comments are welcomed."},{"metadata":{"_cell_guid":"4f64f934-4cc0-4c07-adcc-683f31c751a9","_uuid":"1fac9d66f308035415fa0ed512ecdbf94dd6cc72"},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"_uuid":"79c39642f341d0154b43f033194660d0b1e2e2e2","_cell_guid":"7c2f993c-1e5f-4421-abb6-c3f5ba572135","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv('../input/sales_train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubmission = pd.read_csv('../input/sample_submission.csv')\nitems = pd.read_csv('../input/items.csv')\nitem_cats = pd.read_csv('../input/item_categories.csv')\nshops = pd.read_csv('../input/shops.csv')\nprint(\"train:\", train.shape)\nprint(train.head())\nprint(\"test:\", test.shape)\nprint(test.head())\nprint(\"submission:\",submission.shape)\nprint(submission.head())\nprint(\"items:\",items.shape)\n#print(items.head)\nprint(\"item_cats:\",item_cats.shape)\n#print(item_cats)\nprint(\"shops:\",shops.shape)\n#print(shops)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b62bf46f-6082-48bc-b688-0bb1aec285ed","_uuid":"f76323182cde7e05d2c030964a85b34291cbee10"},"cell_type":"markdown","source":"In this competition, we are required to predict total sales for every product and store in the next month. The first step is to analyze and visualize the shop- and item-dependence of the sales."},{"metadata":{"_cell_guid":"d390b9c4-6b13-493e-b4fe-2235e4255895","_uuid":"a38b66f7aa17496eeb78154866d4d4ff248909b3","trusted":false,"collapsed":true},"cell_type":"code","source":"### Sales for each shops ###\n\nSaleEachShop = train.groupby([\"shop_id\"],as_index=False)[\"item_cnt_day\"].sum()\nSaleEachShop = SaleEachShop.sort_values(by='item_cnt_day', ascending = False)\nprint(SaleEachShop.head())\nprint(SaleEachShop.tail())\nax = SaleEachShop.plot(y=[\"item_cnt_day\"], bins=10, alpha=0.5, figsize=(16,4), kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"121b14f5-e33f-4496-a6f5-5e899e39ef4d","_uuid":"3c4928056812236d1453d7c83ce655db162a304b"},"cell_type":"markdown","source":"It seems that the data is composed of a few dominant shops and other small shops with a threshold around 100000. This implies that we need at least two different models to predict the future sale. How about the item dependence?"},{"metadata":{"_cell_guid":"bbdc50e7-f9ae-4ba9-904b-ecad79b938dc","_uuid":"ff711c6d4f9cdcfc6beec703cef7631bca9c9e81","trusted":false,"collapsed":true},"cell_type":"code","source":"### Sales for each item ###\n\nSaleEachItem = train.groupby([\"item_id\"],as_index=False)[\"item_cnt_day\"].sum()\nSaleEachItem = SaleEachItem.sort_values(by='item_cnt_day', ascending = False)\nprint(SaleEachItem[\"item_cnt_day\"].describe())\nprint(SaleEachItem.head())\nMaxSale = SaleEachItem[\"item_cnt_day\"].max()\nQ95 = SaleEachItem[\"item_cnt_day\"].quantile(.95)\nprint(\"quantile 0.95 = {0}\".format(Q95))\nSaleEachItem[SaleEachItem[\"item_cnt_day\"]< Q95].plot(y=[\"item_cnt_day\"], bins=30, alpha=0.5, figsize=(16,4), title=\"~95%\",kind='hist')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"37bc1b64-89bb-4a3a-bae1-e163f7019f42","_uuid":"4a7dd4f4e6d8edc8ed18f168035b776c9bebdd19"},"cell_type":"markdown","source":"A quick view of the result revealed that the number of the sales of the top selling item is an order of magnitude larger than that of the second. In contrast, the sale of 95% of the items is less than 653, and the histogram has a long tail. I'm not familiar with techniques to incorporate this information into the model, and I will postpone this issue for the moment."},{"metadata":{"_cell_guid":"7f88fab0-5f60-4363-96da-cd9e25f75ee2","_uuid":"202f84821f092864ad7653943de7bbfc6588ad03"},"cell_type":"markdown","source":"# Time Series Data Visualization\nWe would like to see the time series data to gain further insights of the data. Here are the graphs of the monthly sale for each shop."},{"metadata":{"_cell_guid":"ca99ccd5-9cfd-4589-8688-a01c76bacadd","_uuid":"dcebfd7ebbb3a76f2104ce0d5b790bf34c06c72f","trusted":false,"collapsed":true},"cell_type":"code","source":"Monthly_sale_shop = train.groupby([\"date_block_num\",\"shop_id\"],as_index=False)[\"item_cnt_day\"].sum()\nMaxSpan = Monthly_sale_shop[\"date_block_num\"].max()\nfor i in range(len(shops)):\n    Monthly_sale_shop[Monthly_sale_shop[\"shop_id\"] == i].plot(x=\"date_block_num\",y=\"item_cnt_day\",xlim=[0, MaxSpan],\n                                                              title=\"shop {0}\".format(i))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e95a04ea-e0f1-4a43-b6f0-3fb5fe093992","_uuid":"4abe7a0c9af82e9f0a9a5ebbe4a2cfd25e9e6926"},"cell_type":"markdown","source":"These graphs offer useful insights to predict the sales.\n1. Some shops were opened recetnly, and others were already closed. In particular, identifing the closed shops may be very helpful because the sale in the next month is obviously 0.\n2. If we take a glance at the data of the shops which are not closed during the span (e.g. shop 59),there seems to be periodic peaks at date_block_num=11 and 23, which correspond to December. Oh, I see. Christmas.\n\nThese observations bring us a promising strategy to predict the sale of each shop: First, examine the general tendency of the monthly sale, and then incorporate the shop features.\n\nThe next step we should take is to analyze the monthly sales summed over the shops."},{"metadata":{"_cell_guid":"7f7dcd6a-a0de-4601-895d-fe3077ba3179","_uuid":"c366949b9d9c50758ca9bd76bec94302cb56db74","trusted":false,"collapsed":true},"cell_type":"code","source":"Monthly_sale_total = train.groupby([\"date_block_num\"],as_index=False)[\"item_cnt_day\"].sum()\nMonthly_sale_total.plot(x=\"date_block_num\",y=\"item_cnt_day\")\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb0245ce-585f-4046-8c5d-e2e9bb9d85fc","_uuid":"a144b90006636a4231876bfe3c2d353c767c26e1"},"cell_type":"markdown","source":"We can visualize the (partial) autocorrelation and the seasonal effect of the time series data as follows."},{"metadata":{"_cell_guid":"fc0eb2fa-be86-4d05-9a57-98bec1abdb93","_uuid":"314f77073374e869bd2426a9b03214bb4beebe05","trusted":false,"collapsed":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n### The index should be given in Datetime format to use seasonal_decompose\nMonthly_sale_total[\"item_cnt_day\"].index = pd.DatetimeIndex(freq=\"m\",start='2013-1',periods=len(Monthly_sale_total))\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(Monthly_sale_total[\"item_cnt_day\"], lags=20, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(Monthly_sale_total[\"item_cnt_day\"], lags=20, ax=ax2)\n\nres = sm.tsa.seasonal_decompose(Monthly_sale_total[\"item_cnt_day\"], freq=3)\nres.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"027341af-5bda-4d92-a0eb-89ffdd0b32e2","_uuid":"40e245c081bf072982c67a62d3da37cfb10f2e93"},"cell_type":"markdown","source":"It is evident that there is an overall decreasing trend in the data. Moreover, we can find clear peaks with pediod 12, which indicates the seasonal effects. In order to confirm the nonstationarity, we will plot the rolling meaan and variance. Let's define the function to test the stationarity for later usage. A nice function has been already proposed here https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/."},{"metadata":{"_cell_guid":"0f84082e-4acb-439f-8d4d-ac59905807e0","_uuid":"c7dc349d1c829e6c70324e6bba3ffa7cffe418ce","trusted":false,"collapsed":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(timeseries):\n\n    #Plot rolling statistics:\n    NWindow = 5\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(pd.rolling_mean(timeseries, window=NWindow), color='red', label='Rolling Mean')\n    plt.plot(pd.rolling_std(timeseries,  window=NWindow), color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.show(block=False)\n    \n    #Dickey-Fuller test:\n    print(\"Results of Dickey-Fuller Test:\")\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n\ntest_stationarity(Monthly_sale_total[\"item_cnt_day\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a4326550-436f-4cc3-9f9f-599ff7c69335","_uuid":"a2884da5a872bff11965b1059549349606234473"},"cell_type":"markdown","source":"Both the rolling mean and the variance have peak at date_block_num=11, 23. Besides, the null hypothesis of the nonstationarity cannot be rejected according to Dickey-Fuller test. Sounds reasonable."},{"metadata":{"_cell_guid":"e8a61198-a50c-4eed-8e47-5c80afd20790","_uuid":"d505963f03e7b2404cd979c7834c6a385e9135f5"},"cell_type":"markdown","source":"# Data processing\nNow, it is found that the original time series is not stationary. The next step is to remove the trend and the seasonal effect in order to apply statistical models to time series. We will try to remove the trend and the seasonal effect by subtracting the data with the one shifted by 12, which is the period found above.\n"},{"metadata":{"_cell_guid":"6b971eb3-57d7-4d31-9269-92a507d53888","_uuid":"6b2600f91a2fc1f1c0d3888d61b939178683c144","trusted":false,"collapsed":true},"cell_type":"code","source":"ModData = Monthly_sale_total - Monthly_sale_total.shift(periods=12)\nModData = ModData.dropna()\ntest_stationarity(ModData[\"item_cnt_day\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55394d45-da84-4892-80e8-bda74ba66959","_uuid":"1f989acc21de22ee55b10a2e9264a5bee339de45","trusted":false,"collapsed":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n### The index should be given in Datetime format to use seasonal_decompose\nModData.index = pd.DatetimeIndex(freq=\"m\",start='2014-1',periods=len(ModData))\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(ModData[\"item_cnt_day\"], lags=20, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(ModData[\"item_cnt_day\"], lags=20, ax=ax2)\n\nres = sm.tsa.seasonal_decompose(ModData[\"item_cnt_day\"], freq=3)\nres.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10823e06-9488-4956-8e3c-6b52778f786a","_uuid":"306c6de03506879a38c90881849e6615a80d8621"},"cell_type":"markdown","source":"# Time Series Analysis: ARMA Model\nNow both the overall trend and the seasonal effect look removed and p-value is actually less than 0.05. We are ready to use ARMA models, which is one of the most popular and widely used statistical method for time series prediction. First, we will determine the best hyperparameters of the ARMA model by using a function offered by statsmodels library."},{"metadata":{"_cell_guid":"f046e2d4-4ff2-44c2-a8cd-006307f42d25","_uuid":"86b27a419f5e615a098694d3896ec90205a155da","trusted":false,"collapsed":true},"cell_type":"code","source":"res = sm.tsa.arma_order_select_ic(ModData[\"item_cnt_day\"], ic='aic', trend='nc')\nres","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43992265-ea7d-45c2-81f9-5814c27bfe14","_uuid":"79018d48b525d6dfaadc72c563b4040f1589ac48"},"cell_type":"markdown","source":"It is found that Akaike's Information Criterion is minimized with parameter (3,1). Let's make the model and examine the fitting."},{"metadata":{"_uuid":"3dffe94a591a9201e51788111ee32abf42694ab9","_cell_guid":"574471f0-15cc-4c78-86ad-393035fe5c75","collapsed":true,"trusted":false},"cell_type":"code","source":"ARMA_3_1 = sm.tsa.ARMA(ModData[\"item_cnt_day\"], (3,1)).fit()\nprint(ARMA_3_1.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ecf56ce-1f91-407a-b708-91f640955a42","_uuid":"f5a23a82578d93211b01aae6e1c612679c47d016","trusted":false,"collapsed":true},"cell_type":"code","source":"### Autocorrelation function of the residual ###\n\nresid = ARMA_3_1.resid\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=20, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=20, ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83501c7a-74e0-4461-afad-fdbe81715cb8","_uuid":"76642d6f1b41d15a2e1e5336f1c310f6214a6ee8"},"cell_type":"markdown","source":"The fitting is well converged, and the autocorrelation of the residue seems to be weak. Finally, we will predict the future total sale. "},{"metadata":{"_cell_guid":"3904fd1f-26df-4268-9904-936bf830915b","_uuid":"c327ae7dfc38831612f62c0c9ec7665dfe8798ab","trusted":false,"collapsed":true},"cell_type":"code","source":"### prediction with ARMA model\n\npred = ARMA_3_1.predict('2015-1-31', '2016-010-31')\n \nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(211)\nax.set_xlim([datetime.date(2013, 1, 31), datetime.date(2016, 10, 31)])\nShifted = Monthly_sale_total[\"item_cnt_day\"].copy()\nShifted.index = pd.date_range('2014-01-01', periods = len(Shifted), freq = 'M')\nax.plot(Monthly_sale_total[\"item_cnt_day\"],label=\"orig\")\nax.plot(pred+Shifted, \"r\",label=\"pred\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a1a6d8063dfdf1a6f1e376ca8e32d7d7e86cba9","_cell_guid":"69b3d6fd-043f-42a8-89c5-f9553f517e9c","collapsed":true},"cell_type":"markdown","source":"Good. ARMA model predicts both the prominent peak with period 12 and the overall decreasing trend."},{"metadata":{"_cell_guid":"e1fdc8dc-40da-4201-a63d-f26cddb1a286","_uuid":"1fda40848ccb2904180b7e8a72ace82971c1b304"},"cell_type":"markdown","source":"# Perspective\nIt is important not to forget our final goal. Although we just focused on the total amount of sales, we need to seriously think about the shop- and item-dependence in order to correctly predict total sales for every product and store. Moreover, we have completely ignored prices and categories, which may carry quite useful information. Still, I belive that sharing this kernel may benefit both of us because the basic procedures made abov works as a template of more sophisticated and detailed analysis. I would apprecite it if this contribution helps you to perform nice analysis. Thank you for reading my article."},{"metadata":{"_uuid":"c2f7b4673fb3c4a469b14f1779d30b09bfa5007e","_cell_guid":"0b869f30-cb10-4a00-970c-e75a8b7b77d1","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}