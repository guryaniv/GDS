{"nbformat_minor": 1, "metadata": {"language_info": {"version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "cells": [{"metadata": {"_uuid": "1010815f52d2195d8e92d78ddb841ad3c7460385", "_cell_guid": "7ba4023b-1fc8-4be6-92e3-6e29794d3804"}, "source": ["This kernel is building and training convolutional network for text classification. The architecture of the netwok is the same as in the paper of Kim https://arxiv.org/pdf/1408.5882.pdf"], "cell_type": "raw"}, {"metadata": {"_uuid": "089876c2d029abb38330ccbdf33545ae031c39e5", "_cell_guid": "96b710c9-7a89-4b90-a119-6ff2724aa971"}, "source": ["### Let's load data to see what we are dealing with ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "78bb9ac640d6625586229d046c67a7c2461c5242", "collapsed": true, "_cell_guid": "fd1f69c4-82bc-4d9f-9e8d-943100b19382"}, "source": ["import numpy as np\n", "import pandas as pd"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "84556514b1f02ead3472578ae471e5fef0761f09", "_cell_guid": "50771083-1694-4319-be99-2d41b83ac492"}, "source": ["train_data=pd.read_csv('../input/train.csv')\n", "test_data=pd.read_csv('../input/test.csv')"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "663f6daba8931118803fdd688cd73fc363e7b0d3", "_cell_guid": "29e22d03-3aca-4ffd-a832-84baa7a18c9a"}, "source": ["train_data.head(3)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b566fe50ebd12e87e4ddfcd7d6a8395b01d4869e", "_cell_guid": "e18aa2f8-4795-4af7-ae02-369b14663a81"}, "source": ["test_data.head(3)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b64134fa96c9556ee5e2ee989b1ef077e30a268c", "_cell_guid": "89c33848-6339-48c7-86ef-63881e4b1940"}, "source": ["### Lets see the size, and if we have missing values in the datasets ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4c040610445465e7a206129f7920dc1cbae2d98f", "_cell_guid": "a89365e0-b3b3-44c1-8c7e-eeeaaa4aaaf6"}, "source": ["print(train_data.shape,test_data.shape)\n", "print(train_data.isnull().sum())\n", "print(test_data.isnull().sum())"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "2ac19cb93b0baf1703c670e6ac96031d5d9804a9", "_cell_guid": "69eb323f-8978-4eed-8534-0eed635856ec"}, "source": ["### Ok, seems clean ###\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "97cbcd59c301c8225d5fb22e70878bf4190bd32e", "_cell_guid": "760bf481-0f8e-4aea-a5a6-478c87ab8e90"}, "source": ["### Lets make categories of authors ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "c48fed101812eaea561e1333a1c9158cd73ae128", "collapsed": true, "_cell_guid": "6951f4bb-1519-4683-aa03-15789ad76668"}, "source": ["authors=train_data.author.unique()\n", "dic={}\n", "for i,author in enumerate(authors):\n", "    dic[author]=i\n", "labels=train_data.author.apply(lambda x:dic[x])"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8a56c49cdedaba88d515945f486374888e2dc71f", "_cell_guid": "52e669fb-03cc-4feb-af2c-f9d7e6cee600"}, "source": ["### Lets divide our training data to train and validation ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "c42864d9c3033c9b77632b869fbcecfc6a0a3b16", "collapsed": true, "_cell_guid": "39119655-fc12-41eb-b739-c0f68cdf57a3"}, "source": ["val_data=train_data.sample(frac=0.2,random_state=200)\n", "train_data=train_data.drop(val_data.index)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4e3a192bd5a5aae37bd6a2fcb527e9855e760f47", "_cell_guid": "8a86b60b-7303-42fe-bf6d-1e6bbd6c5e77"}, "source": ["### Tokenize text of the training data with keras text preprocessing functions ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "de2ae8c8e7548657bac8ece11ca928205779bd33", "_cell_guid": "67810d98-bc8c-49ea-8285-55cf14c74100"}, "source": ["from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.utils import to_categorical"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c8d1f2f7dc08f93c34d91bedc9002b5fd5524c1d", "collapsed": true, "_cell_guid": "e7381224-7e75-4733-9de1-5e60d353bc7f"}, "source": ["texts=train_data.text"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "d8a393afae2528f97f24fa1d49179340212582bf", "_cell_guid": "73f85346-389c-481e-b182-07436c5004cc"}, "source": ["NUM_WORDS=20000\n", "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n", "                      lower=True)\n", "tokenizer.fit_on_texts(texts)\n", "sequences_train = tokenizer.texts_to_sequences(texts)\n", "sequences_valid=tokenizer.texts_to_sequences(val_data.text)\n", "word_index = tokenizer.word_index\n", "print('Found %s unique tokens.' % len(word_index))"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e453033d2b7514d2d184d623e5d1acbd537f9487", "_cell_guid": "2af3b95b-1560-4a2e-89c4-921a31fb9120"}, "source": ["X_train = pad_sequences(sequences_train)\n", "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n", "y_train = to_categorical(np.asarray(labels[train_data.index]))\n", "y_val = to_categorical(np.asarray(labels[val_data.index]))\n", "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n", "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)\n", "\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "08b2df3d3ad03e9d2472c7b6ba757dee91292199", "_cell_guid": "64d4a5f7-2d3c-487d-9689-5761e79be267"}, "source": ["### word embedding ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "7df6473b83620f055e7ef090b91d967e9cd63618", "_cell_guid": "2fb012c6-8166-4124-b8dd-f8c69f289e9b"}, "source": ["### lets load the pretrain Word2Vec model from Google https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit ###\n", "### It might take time since it contains contains 300-dimensional vectors for 3 million words and phrases ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "6478a820872e903b7d5a3a8b0b3a543c773eb926", "collapsed": true, "_cell_guid": "b5b196d3-a98e-4eb4-b731-c3f0650567ed"}, "source": ["import gensim\n", "from gensim.models import Word2Vec\n", "from gensim.utils import simple_preprocess\n", "\n", "from gensim.models.keyedvectors import KeyedVectors\n", "\n", "word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n", "\n", "EMBEDDING_DIM=300\n", "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n", "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n", "for word, i in word_index.items():\n", "    if i>=NUM_WORDS:\n", "        continue\n", "    try:\n", "        embedding_vector = word_vectors[word]\n", "        embedding_matrix[i] = embedding_vector\n", "    except KeyError:\n", "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n", "\n", "del(word_vectors)\n", "\n", "from keras.layers import Embedding\n", "embedding_layer = Embedding(vocabulary_size,\n", "                            EMBEDDING_DIM,\n", "                            weights=[embedding_matrix],\n", "                            trainable=True)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "f632644d6ea3d74261786848b3fe117a238bb955", "_cell_guid": "40c23df6-23b2-4b31-9036-3376b583cee1"}, "source": ["### Without pretrained data we can just initalize embedding matrixs as: ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "77ef76ef55d7a8d63ab10c131739559a09ff7654", "_cell_guid": "286d1203-7f84-4e2f-a086-945062fdfa01"}, "source": ["from keras.layers import Embedding\n", "EMBEDDING_DIM=300\n", "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n", "\n", "embedding_layer = Embedding(vocabulary_size,\n", "                            EMBEDDING_DIM)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["### Lets create the network and train it as long as validation loss goes down  ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "6b15fab22852ebbe324b9b0143baa98b58074dff", "_cell_guid": "e2f3c6e4-ff65-4129-8860-3ead1a5dcb4b"}, "source": ["from keras.layers import Dense, Input, GlobalMaxPooling1D\n", "from keras.layers import Conv1D, MaxPooling1D, Embedding\n", "from keras.models import Model\n", "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n", "from keras.layers.core import Reshape, Flatten\n", "from keras.callbacks import EarlyStopping\n", "from keras.optimizers import Adam\n", "from keras.models import Model\n", "from keras import regularizers\n", "sequence_length = X_train.shape[1]\n", "filter_sizes = [3,4,5]\n", "num_filters = 100\n", "drop = 0.5\n", "\n", "\n", "\n", "inputs = Input(shape=(sequence_length,))\n", "embedding = embedding_layer(inputs)\n", "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n", "\n", "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n", "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n", "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n", "\n", "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n", "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n", "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n", "\n", "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n", "flatten = Flatten()(merged_tensor)\n", "reshape = Reshape((3*num_filters,))(flatten)\n", "dropout = Dropout(drop)(flatten)\n", "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n", "\n", "# this creates a model that includes\n", "model = Model(inputs, output)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "203f236bfd9d22392cf8de296ba162e6ea1fc341", "_cell_guid": "dcfdc51e-7c75-4314-b34f-25bf2e47b3de"}, "source": ["adam = Adam(lr=1e-3)\n", "\n", "model.compile(loss='categorical_crossentropy',\n", "              optimizer=adam,\n", "              metrics=['acc'])\n", "callbacks = [EarlyStopping(monitor='val_loss')]\n", "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n", "         callbacks=callbacks)  # starts training\n", "\n", "\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "89ced26b55b7fb07ee6ee6f5d866b1a54e27b612", "collapsed": true, "_cell_guid": "048734be-7046-4f3d-bb66-aa4321f56205"}, "source": ["### now lets use our model to predict test data ###"], "cell_type": "markdown"}, {"metadata": {"_uuid": "b1ccc147faeb6eb331be53d3d70489cb24b5d294", "collapsed": true, "_cell_guid": "aab8ced3-1224-4155-9d3d-f7a33e9575dc"}, "source": ["sequences_test=tokenizer.texts_to_sequences(test_data.text)\n", "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n", "y_pred=model.predict(X_test)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "57402e363150b39eea3af18f97e7ad73594a40ac", "collapsed": true, "_cell_guid": "8fd503ca-579a-499d-a3d9-3e1f52fb380d"}, "source": ["to_submit=pd.DataFrame(index=test_data.id,data={'EAP':y_pred[:,dic['EAP']],\n", "                                                'HPL':y_pred[:,dic['HPL']],\n", "                                                'MWS':y_pred[:,dic['MWS']]})"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "da218d48054d2fa0d6c67cd77a1dba36c1497e2c", "collapsed": true, "_cell_guid": "68af55f4-eafc-4b2f-b327-34fb6d21e6be"}, "source": ["to_submit.to_csv('submit.csv')"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "2a7dafacf61e979c315b05c1f4b7c590d9aeda5b", "collapsed": true, "_cell_guid": "a2446c4f-10fa-486d-913a-bae4dbe3d34a"}, "source": [], "outputs": [], "execution_count": null, "cell_type": "code"}], "nbformat": 4}