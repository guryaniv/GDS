{"cells":[{"metadata":{"trusted":true,"_uuid":"975086f8db96559661ff03268937889d8f116a62"},"cell_type":"code","source":"!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1138b177f5cc2da0392db15254d787e931fdcbd5"},"cell_type":"code","source":"!pip install pytorch-pretrained-bert\n!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7387ea27e178399cfa652448870475715fc96bc6"},"cell_type":"code","source":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"33223\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom helperbot import BaseBot, TriangularLR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe4cc33befb140639dda83a35ede5eba5cfa803"},"cell_type":"code","source":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02aeb4b14cee88a377c2b1ee04e4c634487191c5"},"cell_type":"code","source":"def insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n        # Extracts the tokens and offsets(positions of A, B, and P)\n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            text = insert_tag(row)\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx], None\n    \ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels\n\nclass Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.bert_hidden_size = bert_hidden_size\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),\n            nn.Dropout(0.5),\n            nn.Linear(bert_hidden_size * 3, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.5),\n            nn.Linear(512, 3)\n        )\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n        ).view(bert_outputs.size(0), -1)\n        return self.fc(extracted_outputs)\n\n    \nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            \n\n    \ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir / \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c41e0e9b211d8891dae8203fedef220c5610a956"},"cell_type":"code","source":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e04366bdbfe4b23776ed48ee4bfe722d6dc537b5"},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n)\n# These tokens are not actually used, so we can assign arbitrary values.\ntokenizer.vocab[\"[A]\"] = -1\ntokenizer.vocab[\"[B]\"] = -1\ntokenizer.vocab[\"[P]\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b45036936d0134162f8d932e3051fa2a98c9c444"},"cell_type":"code","source":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=20,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True,\n    drop_last=True\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beeb22589e972a702eb25d3abf2262af058ab9b7"},"cell_type":"code","source":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f67abef579026c9f220992fc41b378ee7c1c57f3"},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b64aca7032023cb30a2df5c4f47fcfb658e6c959"},"cell_type":"code","source":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 5\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch // 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"482ea0f17b31c3fb46d2f1593f7175ac390b2ba2"},"cell_type":"code","source":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87b8247369e444a38faca06e6414868cf4e9972"},"cell_type":"code","source":"# Evaluate on the test dataset\nbot.eval(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"383eb98c1ac0da2da346ea3f88184dc0b65c953a"},"cell_type":"code","source":"# Extract predictions to the test dataset\npreds = bot.predict(test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d716a54dfbf744d82f7bcf4b4ae26cad7984e769"},"cell_type":"code","source":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"cache/sub.csv\", index=False)\ndf_sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}