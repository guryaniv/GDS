{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["**UPDATE after finish**\n", "The final score of this model on the **private leaderboard was 0.1639**.  My best ensemble private score was 0.1477. However I choosed another model for the final solution and my final result is a bit dissapointment. But I learned a lot and this is why we all here :-) \n", "\n", "This was my first competition here at Kaggle. I learned a lot from the discussions and the kernels gladly published by more experienced Kagglers. \n", "\n", "Therefore I decided to give something back to the community. Here is my single best model scoring 0.1541 on the public leaderboard. \n", "\n", "Maybe it can help someone as a part of the final stacked solution. Thanks for upvoting if so ;-)\n", "\n", "This work was built on those two great Kernels.\n", "\n", "TheGruffalo\n", "Keras CNN - StatOil Iceberg LB 0.1995 (now 0.1516)\n", "[https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516](http://https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516)\n", "\n", "QuantScientist\n", "Statoil CSV PyTorch SENet ensemble LB 0.1520\n", "[https://www.kaggle.com/solomonk/statoil-csv-pytorch-senet-ensemble-lb-0-1520](http://https://www.kaggle.com/solomonk/statoil-csv-pytorch-senet-ensemble-lb-0-1520)"], "cell_type": "markdown", "metadata": {"_uuid": "a84233f5cbe98d402d74f46e87e7d745540b2cc8", "_cell_guid": "c825acee-1835-4fa0-a14a-0c40a589577a"}}, {"source": ["## Imports"], "cell_type": "markdown", "metadata": {"_uuid": "fb4f08cce0aaddbda853b5c86f8b66e6189b10a4", "_cell_guid": "3742280f-b7f8-4639-a1dc-fe34c5d0bbe5"}}, {"outputs": [], "execution_count": null, "source": ["import pandas as pd \n", "import numpy as np \n", "import cv2 # Used to manipulated the images \n", "seed = 1234\n", "np.random.seed(seed) # The seed I used - pick your own or comment out for a random seed. A constant seed allows for better comparisons though\n", "\n", "# Kfold\n", "from sklearn.model_selection import StratifiedKFold\n", "\n", "# Import Keras \n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Flatten, Activation\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.optimizers import Adam"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a6025efc05dbb5d0a9f30e1b70e6178a52f7bb5b", "_cell_guid": "06ed8576-67b0-407a-bec3-a8919432eb4b"}}, {"outputs": [], "execution_count": null, "source": ["def get_scaled_imgs(df):\n", "    \"\"\"\n", "    basic function for reshaping and rescaling data as images\n", "    \"\"\"\n", "    imgs = []\n", "    \n", "    for i, row in df.iterrows():\n", "        #make 75x75 image\n", "        band_1 = np.array(row['band_1']).reshape(75, 75)\n", "        band_2 = np.array(row['band_2']).reshape(75, 75)\n", "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n", "        \n", "        # Rescale\n", "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n", "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n", "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n", "\n", "        imgs.append(np.dstack((a, b, c)))\n", "\n", "    return np.array(imgs)    \n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "2782430813d6deda5a6f8aae1cc385586538f5c5", "_cell_guid": "715ebc9b-0999-456e-8ed0-bc96e05e0128"}}, {"source": ["## Adding images for training"], "cell_type": "markdown", "metadata": {"_uuid": "8da34b46d1f4c96d9f32239df3b61573cde9d0e1", "_cell_guid": "306fe2f3-f417-4f2d-956b-2cc98c8b955b"}}, {"source": ["Now, the biggest improvement I had was by adding more data to train on. I did this by simply including horizontally and vertically flipped data. Using OpenCV this is easily done."], "cell_type": "markdown", "metadata": {"_uuid": "fba1fe217316a5ca70887c67d2c0c158cb8c02dc", "_cell_guid": "49117bf4-dbdd-4ea5-95ca-af23ed35053e"}}, {"outputs": [], "execution_count": null, "source": ["def get_more_images(imgs):\n", "    \"\"\"\n", "    augmentation for more data\n", "    \"\"\"    \n", "\n", "\n", "    more_images = []\n", "    vert_flip_imgs = []\n", "    hori_flip_imgs = []\n", "      \n", "    for i in range(0,imgs.shape[0]):\n", "        a=imgs[i,:,:,0]\n", "        b=imgs[i,:,:,1]\n", "        c=imgs[i,:,:,2]\n", "        \n", "        av=cv2.flip(a,1)\n", "        ah=cv2.flip(a,0)\n", "        bv=cv2.flip(b,1)\n", "        bh=cv2.flip(b,0)\n", "        cv=cv2.flip(c,1)\n", "        ch=cv2.flip(c,0)\n", "        \n", "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n", "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n", "      \n", "    v = np.array(vert_flip_imgs)\n", "    h = np.array(hori_flip_imgs)\n", "       \n", "    more_images = np.concatenate((imgs,v,h))\n", "    \n", "    return more_images"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "760fa192e912111b39d3d0ee3fadd9c7fd0eb628", "_cell_guid": "f069618d-46f0-42f0-8e83-e5a4716bb965"}}, {"source": ["## CNN Keras Model"], "cell_type": "markdown", "metadata": {"_uuid": "f2b22fcec3493ce4df71f0dd21ed97fd1938e3f7", "_cell_guid": "54a72cfc-dbc1-4136-bb31-2ea75115e1cf"}}, {"outputs": [], "execution_count": null, "source": ["\n", "def get_model():\n", "    \n", "    \"\"\"\n", "    Keras Sequential model\n", "\n", "    \"\"\"\n", "    \n", "    model=Sequential()\n", "    \n", "    # Conv block 1\n", "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n", "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n", "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n", "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n", "   \n", "    # Conv block 2\n", "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n", "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n", "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n", "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "   \n", "    # Conv block 3\n", "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n", "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "   \n", "    #Conv block 4\n", "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n", "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "   \n", "    # Flatten before dense\n", "    model.add(Flatten())\n", "\n", "    #Dense 1\n", "    model.add(Dense(1024, activation='relu'))\n", "    model.add(Dropout(0.4))\n", "\n", "    #Dense 2\n", "    model.add(Dense(512, activation='relu'))\n", "    model.add(Dropout(0.2))\n", "\n", "    # Output \n", "    model.add(Dense(1, activation=\"sigmoid\"))\n", "\n", "    optimizer = Adam(lr=0.0001, decay=0.0)\n", "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n", "    \n", "    return model\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "edd8ce229d5914f9f57bf147dccb4810c06dbf09", "_cell_guid": "c3d1f84c-44e0-45df-9363-13f5b8abd6ef"}}, {"source": ["## Load the data, and train the model usign K-fold CV\n", "\n", "The traing here at Kaggle will be very slow. Better fork the notebook, download and run the code locally. Don't forget to set the epochs."], "cell_type": "markdown", "metadata": {"_uuid": "3ea6bdb4b84b3a7810305165a322c29a71d8c018", "_cell_guid": "a6102e0d-9059-4daa-8483-c1449b67ff4f"}}, {"outputs": [], "execution_count": null, "source": ["\n", "# Training Data\n", "df_train = pd.read_json('../input/train.json') # this is a dataframe\n", "\n", "\n", "Xtrain = get_scaled_imgs(df_train)\n", "Ytrain = np.array(df_train['is_iceberg'])\n", "df_train.inc_angle = df_train.inc_angle.replace('na',0)\n", "idx_tr = np.where(df_train.inc_angle>0)\n", "\n", "Ytrain = Ytrain[idx_tr[0]]\n", "Xtrain = Xtrain[idx_tr[0],...]\n", "\n", "Xtr_more = get_more_images(Xtrain) \n", "Ytr_more = np.concatenate((Ytrain,Ytrain,Ytrain))\n", "\n", "# K fold CV training\n", "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n", "for fold_n, (train, test) in enumerate(kfold.split(Xtr_more, Ytr_more)):\n", "    print(\"FOLD nr: \", fold_n)\n", "    model = get_model()\n", "    \n", "    MODEL_FILE = 'mdl_simple_k{}_wght.hdf5'.format(fold_n)\n", "    batch_size = 32\n", "    mcp_save = ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_loss', mode='min')\n", "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n", "\n", "    # set the epochs to 30 before training on your GPU\n", "    model.fit(Xtr_more[train], Ytr_more[train],\n", "        batch_size=batch_size,\n", "        epochs=1,\n", "        verbose=1,\n", "        validation_data=(Xtr_more[test], Ytr_more[test]),\n", "        callbacks=[mcp_save, reduce_lr_loss])\n", "    \n", "    model.load_weights(filepath = MODEL_FILE)\n", "\n", "    score = model.evaluate(Xtr_more[test], Ytr_more[test], verbose=1)\n", "    print('\\n Val score:', score[0])\n", "    print('\\n Val accuracy:', score[1])\n", "\n", "    SUBMISSION = './result/simplenet/sub_simple_v1_{}.csv'.format(fold_n)\n", "\n", "    df_test = pd.read_json('../input/test.json')\n", "    df_test.inc_angle = df_test.inc_angle.replace('na',0)\n", "    Xtest = (get_scaled_imgs(df_test))\n", "    pred_test = model.predict(Xtest)\n", "\n", "    submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n", "    print(submission.head(10))\n", "\n", "    submission.to_csv(SUBMISSION, index=False)\n", "    print(\"submission saved\")\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "4c2c471152ef251ae12e2589d0c7da7b4bcce369", "_cell_guid": "5dc8dcec-7ed5-4ba7-b3da-b458b26f32ca"}}, {"source": ["## Combine the K-fold solutions together "], "cell_type": "markdown", "metadata": {"_uuid": "ac5554f356765781e6d3c40439ac0c288dbf9b4d", "_cell_guid": "916dd785-f600-450b-b2d7-2661d0c936be"}}, {"outputs": [], "execution_count": null, "source": ["wdir = './result/simplenet/'\n", "stacked_1 = pd.read_csv(wdir + 'sub_simple_v1_0.csv')\n", "stacked_2 = pd.read_csv(wdir + 'sub_simple_v1_1.csv')\n", "stacked_3 = pd.read_csv(wdir + 'sub_simple_v1_2.csv')\n", "stacked_4 = pd.read_csv(wdir + 'sub_simple_v1_3.csv')\n", "stacked_5 = pd.read_csv(wdir + 'sub_simple_v1_4.csv')\n", "stacked_6 = pd.read_csv(wdir + 'sub_simple_v1_5.csv')\n", "stacked_7 = pd.read_csv(wdir + 'sub_simple_v1_6.csv')\n", "stacked_8 = pd.read_csv(wdir + 'sub_simple_v1_7.csv')\n", "stacked_9 = pd.read_csv(wdir + 'sub_simple_v1_8.csv')\n", "stacked_10 = pd.read_csv(wdir + 'sub_simple_v1_9.csv')\n", "sub = pd.DataFrame()\n", "sub['id'] = stacked_1['id']\n", "sub['is_iceberg'] = np.exp(np.mean(\n", "    [\n", "        stacked_1['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_2['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_3['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_4['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_5['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_6['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_7['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_8['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_9['is_iceberg'].apply(lambda x: np.log(x)),\n", "        stacked_10['is_iceberg'].apply(lambda x: np.log(x)),\n", "        ], axis=0))\n", "\n", "sub.to_csv(wdir + 'final_ensemble.csv', index=False, float_format='%.6f')    \n", "\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "34350475a7d6078541692599f7f306697fda4b70", "_cell_guid": "4f8f68da-bd58-4d54-9b6a-c2bb8d5f3a7e"}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"file_extension": ".py", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.4", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}}}