{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c72453017c6c93b9e9b02899cf5a03abfb197ad8"},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2ead2bf30554784311cc507d1be39e0906c78a78"},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"768029b3e54120189a869709cf0f17360fca0d79"},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76b72c479c001bedc8adc371165a5ded76a14aa5"},"cell_type":"code","source":"df_train = pd.read_json('../input/train.json')\ndf_test = pd.read_json('../input/test.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f400bff120060aa2f1be4ea1f2c2de9bdf65819f"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13b471e7f84fa9019a1d9f8ac0981b3a080be8ae"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f7ac2c4c22e0646442ee892446f74d0abf5d1b"},"cell_type":"markdown","source":"## Handle ingredients column"},{"metadata":{"_uuid":"8db758152c34988a77c6cf462befdbc3d5bed1c3"},"cell_type":"markdown","source":"#### Create list of words in each recipe row"},{"metadata":{"trusted":true,"_uuid":"17d3d1a1b5efc09da35e47812fff7dadf4f47aa3"},"cell_type":"code","source":"#Train set\ningredients_train = df_train.ingredients\nwords_train = [' '.join(x) for x in ingredients_train]\nprint(len(words_train), words_train[0])\n\n#Test set\ningredients_test = df_test.ingredients\nwords_test = [' '.join(x) for x in ingredients_test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31785c18efe6c511dfd9fd7882c0a88c1347ec1a"},"cell_type":"markdown","source":"#### Create a word vector based on the training set"},{"metadata":{"trusted":true,"_uuid":"7f5f089a07d3df831471d3add64cd6a27d807b9c"},"cell_type":"code","source":"vectorizer = CountVectorizer(max_features = 1000)\nbag_of_words = vectorizer.fit(words_train)\nbag_of_words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9029b9ae9849568eb2f48e9e10448396e4e447dd"},"cell_type":"markdown","source":"#### Transform the word lists into vectors using the vectorizer trained on the training data"},{"metadata":{"trusted":true,"_uuid":"e0855592e88a5c5c67e35cbcdcccb8557681e1e3"},"cell_type":"code","source":"ing_array_train = bag_of_words.transform(words_train).toarray()\ning_array_test = bag_of_words.transform(words_test).toarray()\ning_array_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d0d29748a2092791dfc118cc84df8ba509eae2c"},"cell_type":"markdown","source":"#### Incorporate the word vectors into the train and test dataframes"},{"metadata":{"trusted":true,"_uuid":"0a4df08ddf5092657a266846623cda33fbd57c6e"},"cell_type":"code","source":"df_ing_train = pd.DataFrame(ing_array_train, columns=vectorizer.vocabulary_)\ndf_ing_test = pd.DataFrame(ing_array_test, columns=vectorizer.vocabulary_)\ndf_ing_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61f331b4bb1cf3c8e090d6f4f4c1b2108e430efb"},"cell_type":"code","source":"df_train_new = df_train.merge(df_ing_train, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\ndf_train_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02684eb300182c1faabdf1561b5cbacdbe14bf62"},"cell_type":"code","source":"df_test_new = df_test.merge(df_ing_test, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\ndf_test_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"943bf4b48515c9629b90009fc05634236f594122"},"cell_type":"markdown","source":"## Create sets"},{"metadata":{"trusted":true,"_uuid":"8f7cc7e9542677aea4de05e9ff4933fe95619ed1"},"cell_type":"code","source":"X = df_train_new.drop(['id', 'cuisine'], axis=1)\ny = df_train_new.cuisine\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b8b13ebbaf421345a2977abbe68b8d68c80ca98"},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.85)\nX_train.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aca8fc140198cf71e031f40184c8ce91b6d39e8"},"cell_type":"markdown","source":"## Create Random Forest"},{"metadata":{"trusted":true,"_uuid":"b6df14b6f174fc04beabcda2e7fa5939ad09b3a6"},"cell_type":"code","source":"m = RandomForestClassifier(oob_score=True)\nm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b6b1cb96582a35e1fd5fb26d11675c245627356"},"cell_type":"code","source":"m.oob_score_, m.score(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31cbe56a9799e51efef5f2a20c8cad649b53b824"},"cell_type":"markdown","source":"## Structuring the above into helper functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2f127714f45bfdc1ef35929aab43fb3c463b6802"},"cell_type":"code","source":"def create_model(n_words, n_trees, train, test, words=None):\n    #create vectorized df's\n    df_train, df_test = vect_train_test(train, test, n_words, words)\n    \n    X = df_train.drop(['id', 'cuisine'], axis=1)\n    y = df_train.cuisine\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.85)\n    \n    m = RandomForestClassifier(n_estimators=n_trees, oob_score=True)\n    m.fit(X_train, y_train)\n    \n    moob_score = m.oob_score_\n    score = m.score(X_val, y_val)\n    model = m\n    \n    return moob_score, score, model\n\ndef vect_train_test(dftrain, dftest, n_words=1000, words=None):\n    vectorizer = CountVectorizer(max_features = n_words)\n    ingredients_train = dftrain.ingredients\n    words_train = [' '.join(x) for x in ingredients_train]\n    ingredients_test = dftest.ingredients\n    words_test = [' '.join(x) for x in ingredients_test]\n    if isinstance(words, pd.Series):\n        bag_of_words = vectorizer.fit(words)\n    else:\n        bag_of_words = vectorizer.fit(words_train)\n\n    ing_array_train = bag_of_words.transform(words_train).toarray()\n    ing_array_test = bag_of_words.transform(words_test).toarray()\n\n    df_ing_train = pd.DataFrame(ing_array_train, columns=vectorizer.vocabulary_)\n    df_ing_test = pd.DataFrame(ing_array_test, columns=vectorizer.vocabulary_)\n\n    df_train = dftrain.merge(df_ing_train, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\n    df_test= dftest.merge(df_ing_test, \n                          left_index=True, \n                          right_index=True).drop('ingredients', axis=1)\n    return df_train, df_test\n\ndef run_variations(variations, target):\n    models = []\n    for var in variations:\n        moob_score, score, model = create_model(var[0], var[1], df_train, df_test)\n        models.append({'n_vectors': var[0],\n                       'n_trees': var[1],\n                       'moob_score': moob_score,\n                      'score': score,\n                      'model': model})\n        print(var, moob_score, score)\n    if target == 'vector':\n        plot_vector_score(models)\n    elif target == 'trees':\n        plot_ntree_score(models)\n    return models\n\ndef plot_vector_score(models):\n    plt.plot([x['n_vectors'] for x in models], [y['moob_score'] for y in models])\n    plt.title('Score increase from Vector increase')\n    plt.xlabel('Word vector size')\n    plt.ylabel('Score')\n    plt.show()\n    return\n\ndef plot_ntree_score(models):\n    plt.plot([x['n_trees'] for x in models], [y['moob_score'] for y in models])\n    plt.title('Score increase from number of estimators (trees) increase')\n    plt.xlabel('Number of Estimators')\n    plt.ylabel('Score (350 word vector)')\n    plt.show()\n    return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"554691b9bd16571cef09e8baae7d4845f72cbe2f"},"cell_type":"markdown","source":"### Check feature importance"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a6d064b01a81e1eb47f20ddca2cd61f60581cf9d"},"cell_type":"code","source":"top = sorted(list(zip(X_val.columns, \n                      m.feature_importances_)), key=lambda x: x[1], reverse=True)[:200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80ed79280acf3ef142f355dde89fa7ed925bf78e"},"cell_type":"code","source":"df_imp = pd.DataFrame(top, columns=['feat', 'imp'])\ndf_imp.imp = df_imp.imp.astype(float)\ndf_imp[df_imp.imp > 0.002].plot('feat', 'imp', kind='barh', figsize=(12,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad219ed743daea306b66baf01222568a067fbe1d"},"cell_type":"code","source":"df_keep = df_imp[df_imp.imp > 0.004]\nnew_ing = df_keep.feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88ded6eeebea15818fc55b463332a1eef9b698c1"},"cell_type":"code","source":"#Reducing features to the most significants did not improve results...\nmoob_score, score, m = create_model(350, 30, df_train, df_test, new_ing)\nmoob_score, score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb12f07293308f479b5558b1571e95b54ca93b4b"},"cell_type":"markdown","source":"## Create variations"},{"metadata":{"trusted":true,"_uuid":"ba807c33f9fb35f5a3f993d792867b11dd29486a"},"cell_type":"code","source":"variations_1 = [(100, 30),\n             (150, 30),\n             (200, 30),\n             (300, 30),\n             (350, 30),\n             (500, 30),\n             (700, 30),\n             (1000, 30),\n             (1300, 30)]\n\nmodels_1 = run_variations(variations_1, 'vector')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc97b2a543fb8cd0e7a91e2d7df268b36c8c10ea"},"cell_type":"markdown","source":"We can see that size 1000 for word vectors is ideal."},{"metadata":{"_uuid":"9dc13cbb0b09365ee90d6a0a40bf23b06837a50d"},"cell_type":"markdown","source":"How about number of trees? How does the score improve with changes to that?"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"700e08f6ca53f7f176c29a712c58238e9a1ceeda"},"cell_type":"code","source":"# variations_2 = [(350, 10),\n#                (350, 20),\n#                (350, 25),\n#                (350, 30),\n#                (350, 35),\n#                (350, 50),\n#                (350, 75),\n#                (350, 100),\n#                (350, 150),\n#                (350, 200),\n#                (350, 300)]\n\n# models_2 = run_variations(variations_2, 'trees')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"624803b126e31ac14bcdb063b12e29138c6fa743"},"cell_type":"code","source":"# variations_3 = [(1000, 50),\n#                 (1000, 100),\n#                (1000, 150)]\n\n# models_3 = run_variations(variations_3, 'trees')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efc2ff344b8a748df21a5627453851c74dcfc0b4"},"cell_type":"markdown","source":"## Optimal model (current)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2ab0144f47b23758485aa00e6bce1ff301ea3ed1"},"cell_type":"code","source":"moob_score, score, m = create_model(1000, 300, df_train, df_test) #after running variations these values seemed the best right now..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9949d9749743a60824852b42d3b764026d64a51a"},"cell_type":"code","source":"moob_score, score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf0720d734df124516e762efee57887449be6830"},"cell_type":"markdown","source":"## Create test set file"},{"metadata":{"trusted":true,"_uuid":"008776e5d79c1ff6fa7a48624a390b8a660a5ff4"},"cell_type":"code","source":"X_test = df_test_new.drop('id', axis=1)\ny_test = m.predict(X_test)\ny_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55f799a24f2af414a4a551d2555c8a828ed8c9c5"},"cell_type":"code","source":"df_sub = pd.DataFrame(np.array([df_test.id, y_test]).T, \n                      columns=['id', 'cuisine']).set_index('id')\n\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a018003f069a2d4ee95c7352acaa748bb9231c0"},"cell_type":"code","source":"df_sub.to_csv(f'submission_{m.n_estimators}_V4Kernel.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ae07e2f78654f0ca570c73db6cb9db57e566dd86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}