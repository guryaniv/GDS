{"nbformat": 4, "metadata": {"language_info": {"version": "3.6.3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"metadata": {"_cell_guid": "53a31210-a396-40ce-8f4b-6ac00e476fff", "_uuid": "6897a60cb6a96bfb7610acaa186b76b7d4fa8307"}, "cell_type": "markdown", "source": ["This notebook is meant to be a proof of concept for ideas discussed in [__this thread__](https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/45291). Hopefully it shows that cross-validation can be done even within existing time limits, and that it can be done even better than here with multi-processing and other tricks. In my hands cross-validation has produced scores that agree very well with LB scores - the difference is usually on the order of 0.001. That should give us comfort compared to single runs of individual methods on fold splits and random seeds that are crafted to give the best LB score.\n", "\n", "Further down the line there is a brief example of how to use out-of-fold predictions to calculate linear blending weights by minimization. Several plots closer to the end will show how individual methods differ in types of prediction errors. Lastly, all out-of-fold files and submission files will be saved if you wish to play with them locally.\n", "\n", "Fairness warning: This script will not give you a 0.43xx score at the push of a button. You will have to work on your own and modify concepts presented here in order to get that kind of score.\n", "\n", "A __big thank you__ to [__@apapiu__](https://www.kaggle.com/apapiu) for his original [__Ridge script__](https://www.kaggle.com/apapiu/ridge-script), as the initial data processing here is based on his work. It is a shame that his script has less than 40 votes, while other scripts using his strategy have more votes just because they advertise better LB scores. You know what to do if you agree with me."]}, {"metadata": {"collapsed": true, "_cell_guid": "e51428dc-f5be-4b4d-8afc-b47b2771d388", "_uuid": "921ca593f6750227dec7315ab5b2b9ce389ea179"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["from __future__ import print_function\n", "import pandas as pd\n", "import numpy as np\n", "import scipy\n", "\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.linear_model import SGDRegressor, Ridge\n", "from sklearn.model_selection import KFold\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "import matplotlib.pyplot as plt\n", "from datetime import datetime\n", "import lightgbm as lgb\n", "from scipy.optimize import minimize\n", "%matplotlib inline\n", "\n", "import gc\n", "\n", "\n", "def timer(start_time=None):\n", "    if not start_time:\n", "        start_time = datetime.now()\n", "        return start_time\n", "    elif start_time:\n", "        thour, temp_sec = divmod(\n", "            (datetime.now() - start_time).total_seconds(), 3600)\n", "        tmin, tsec = divmod(temp_sec, 60)\n", "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n", "              (thour, tmin, round(tsec, 2)))\n", "\n", "\n", "def rmse_min_func(weights):\n", "    final_prediction = 0\n", "    for weight, prediction in zip(weights, blend_train):\n", "        final_prediction += weight * prediction\n", "    return np.sqrt(mean_squared_error(y_train, final_prediction))\n", "\n", "\n", "#starttime = timer(None)\n", "\n", "start_time = timer(None)\n", "NUM_BRANDS = 2500\n", "NAME_MIN_DF = 10\n", "MAX_FEAT_DESCP = 50000\n", "print('Reading in Data')\n", "\n", "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n", "df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n", "y_train = np.log1p(df_train['price'])\n", "tr_ids = df_train['train_id'].values.astype(np.int32)\n", "df_train.drop(['price', 'train_id'], axis=1, inplace=True)\n", "te_ids = df_test['test_id'].values.astype(np.int32)\n", "df_test.drop(['test_id'], axis=1, inplace=True)\n", "df = pd.concat([df_train, df_test])\n", "nrow_train = df_train.shape[0]\n", "\n", "del df_train, df_test\n", "gc.collect()\n", "\n", "df['category_name'] = df['category_name'].fillna('Other').astype('category')\n", "df['brand_name'] = df['brand_name'].fillna('unknown')\n", "\n", "pop_brands = df['brand_name'].value_counts().index[:NUM_BRANDS]\n", "df.loc[~df['brand_name'].isin(pop_brands), 'brand_name'] = 'Other'\n", "\n", "df['item_description'] = df['item_description'].fillna('None')\n", "df['item_condition_id'] = df['item_condition_id'].astype('category')\n", "df['brand_name'] = df['brand_name'].astype('category')\n", "\n", "print('Encodings')\n", "count = CountVectorizer(min_df=NAME_MIN_DF)\n", "X_name = count.fit_transform(df['name'])\n", "\n", "print('Category Encoders')\n", "unique_categories = pd.Series('/'.join(df['category_name'].unique().astype(\n", "    'str')).split('/')).unique()\n", "count_category = CountVectorizer()\n", "X_category = count_category.fit_transform(df['category_name'])\n", "\n", "print('Descp encoders')\n", "count_descp = TfidfVectorizer(max_features=MAX_FEAT_DESCP,\n", "                              ngram_range=(1, 3),\n", "                              stop_words='english')\n", "X_descp = count_descp.fit_transform(df['item_description'])\n", "\n", "print('Brand encoders')\n", "vect_brand = LabelBinarizer(sparse_output=True)\n", "X_brand = vect_brand.fit_transform(df['brand_name'])\n", "\n", "print('Dummy Encoders')\n", "X_dummies = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n", "    'item_condition_id', 'shipping'\n", "]], sparse=True).values)\n", "\n", "X = scipy.sparse.hstack((X_dummies, X_descp, X_brand, X_category,\n", "                         X_name)).tocsr()\n", "\n", "print([X_dummies.shape, X_category.shape, X_name.shape, X_descp.shape,\n", "       X_brand.shape])\n", "\n", "X_train = X[:nrow_train]\n", "X_test = X[nrow_train:]\n", "timer(start_time)"]}, {"metadata": {"_cell_guid": "7beaa2fd-7540-49ab-a7ab-3ec614463233", "_uuid": "538544f1af3787f704f8466c41e963970df8926e"}, "cell_type": "markdown", "source": ["Doing cross-validation in this block. Time-permitting, we'd be doing 5- or 10-fold CV and few parameters would have been different in individual models. It should not be a problem at all to include 2-3 additional models that have training times similar to Ridge or SGDRegressor."]}, {"metadata": {"collapsed": true, "_cell_guid": "380c1c46-2464-448b-ab2a-13513b5cc7f0", "_uuid": "893c0b531d894b4bdbdca6524dbbfe1416209e79"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["# This number of folds is forced by time limit\n", "folds = 3\n", "\n", "sgd_cv_sum = 0\n", "ridge_cv_sum = 0\n", "lgb_cv_sum = 0\n", "lgb_pred = []\n", "sgd_pred = []\n", "ridge_pred = []\n", "lgb_fpred = []\n", "sgd_fpred = []\n", "ridge_fpred = []\n", "\n", "avreal = y_train\n", "lgb_avpred = np.zeros(X_train.shape[0])\n", "sgd_avpred = np.zeros(X_train.shape[0])\n", "ridge_avpred = np.zeros(X_train.shape[0])\n", "idpred = tr_ids\n", "\n", "blend_train = []\n", "blend_test = []\n", "\n", "train_time = timer(None)\n", "kf = KFold(n_splits=folds, random_state=1001)\n", "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n", "    start_time = timer(None)\n", "    Xtrain, Xval = X_train[train_index], X_train[val_index]\n", "    ytrain, yval = y_train[train_index], y_train[val_index]\n", "\n", "    model = SGDRegressor(penalty='l2',\n", "                         loss='squared_epsilon_insensitive',\n", "                         max_iter=200,\n", "                         tol=0.00001,\n", "                         epsilon=0.0001,\n", "                         learning_rate='invscaling',\n", "                         fit_intercept=False,\n", "                         alpha=1e-10,\n", "                         l1_ratio=0.09,\n", "                         shuffle=True,\n", "                         verbose=0,\n", "                         random_state=1001)\n", "    model.fit(Xtrain, ytrain)\n", "    sgd_scores_val = model.predict(Xval)\n", "    sgd_RMSLE = np.sqrt(mean_squared_error(yval, sgd_scores_val))\n", "    print('\\n Fold %02d SGD RMSLE: %.6f' % ((i + 1), sgd_RMSLE))\n", "    sgd_y_pred = model.predict(X_test)\n", "\n", "    model = Ridge(alpha=4.75,\n", "                  solver='sag',\n", "                  fit_intercept=False,\n", "                  random_state=1001,\n", "                  max_iter=1000)\n", "    model.fit(Xtrain, ytrain)\n", "    ridge_scores_val = model.predict(Xval)\n", "    ridge_RMSLE = np.sqrt(mean_squared_error(yval, ridge_scores_val))\n", "    print(' Fold %02d Ridge RMSLE: %.6f' % ((i + 1), ridge_RMSLE))\n", "    ridge_y_pred = model.predict(X_test)\n", "\n", "    params = {\n", "        'boosting': 'gbdt',\n", "#        'max_bin'          : 511,\n", "        'max_depth': 7,\n", "        'min_data_in_leaf': 80,\n", "        'num_leaves': 40,\n", "        'learning_rate': 0.75,\n", "        'objective': 'regression',\n", "        'metric': 'rmse',\n", "        'nthread': 4,\n", "        'bagging_freq': 1,\n", "        'subsample': 0.94,\n", "        'colsample_bytree': 0.7,\n", "        'min_child_weight': 17,\n", "        'is_unbalance': False,\n", "        'verbose': -1,\n", "        'seed': 1001\n", "    }\n", "\n", "    dtrain = lgb.Dataset(Xtrain, label=ytrain, max_bin=4095)\n", "    dval = lgb.Dataset(Xval, label=yval, max_bin=4095)\n", "    watchlist = [dtrain, dval]\n", "    watchlist_names = ['train', 'val']\n", "\n", "    model = lgb.train(params,\n", "                      train_set=dtrain,\n", "                      num_boost_round=1800,\n", "                      valid_sets=watchlist,\n", "                      valid_names=watchlist_names,\n", "                      early_stopping_rounds=100,\n", "                      verbose_eval=50)\n", "    lgb_scores_val = model.predict(Xval)\n", "    lgb_RMSLE = np.sqrt(mean_squared_error(yval, lgb_scores_val))\n", "    print(' Fold %02d LightGBM RMSLE: %.6f' % ((i + 1), lgb_RMSLE))\n", "    lgb_y_pred = model.predict(X_test)\n", "\n", "    del Xtrain, Xval\n", "    gc.collect()\n", "\n", "    timer(start_time)\n", "\n", "    sgd_avpred[val_index] = sgd_scores_val\n", "    ridge_avpred[val_index] = ridge_scores_val\n", "    lgb_avpred[val_index] = lgb_scores_val\n", "\n", "    if i > 0:\n", "        sgd_fpred = sgd_pred + sgd_y_pred\n", "        ridge_fpred = ridge_pred + ridge_y_pred\n", "        lgb_fpred = lgb_pred + lgb_y_pred\n", "    else:\n", "        sgd_fpred = sgd_y_pred\n", "        ridge_fpred = ridge_y_pred\n", "        lgb_fpred = lgb_y_pred\n", "    sgd_pred = sgd_fpred\n", "    ridge_pred = ridge_fpred\n", "    lgb_pred = lgb_fpred\n", "    sgd_cv_sum = sgd_cv_sum + sgd_RMSLE\n", "    ridge_cv_sum = ridge_cv_sum + ridge_RMSLE\n", "    lgb_cv_sum = lgb_cv_sum + lgb_RMSLE\n", "\n", "timer(train_time)\n", "\n", "sgd_cv_score = (sgd_cv_sum / folds)\n", "ridge_cv_score = (ridge_cv_sum / folds)\n", "lgb_cv_score = (lgb_cv_sum / folds)\n", "sgd_oof_RMSLE = np.sqrt(mean_squared_error(avreal, sgd_avpred))\n", "ridge_oof_RMSLE = np.sqrt(mean_squared_error(avreal, ridge_avpred))\n", "lgb_oof_RMSLE = np.sqrt(mean_squared_error(avreal, lgb_avpred))\n", "\n", "print('\\n Average SGD RMSLE:\\t%.6f' % sgd_cv_score)\n", "print(' Out-of-fold SGD RMSLE:\\t%.6f' % sgd_oof_RMSLE)\n", "print('\\n Average Ridge RMSLE:\\t%.6f' % ridge_cv_score)\n", "print(' Out-of-fold Ridge RMSLE:\\t%.6f' % ridge_oof_RMSLE)\n", "print('\\n Average LightGBM RMSLE:\\t%.6f' % lgb_cv_score)\n", "print(' Out-of-fold LightGBM RMSLE:\\t%.6f' % lgb_oof_RMSLE)\n", "sgd_score = round(sgd_oof_RMSLE, 6)\n", "ridge_score = round(ridge_oof_RMSLE, 6)\n", "lgb_score = round(lgb_oof_RMSLE, 6)\n", "\n", "sgd_mpred = sgd_pred / folds\n", "ridge_mpred = ridge_pred / folds\n", "lgb_mpred = lgb_pred / folds"]}, {"metadata": {"_cell_guid": "2e5d66ce-9fc1-488d-94d7-b99eca1a98b5", "_uuid": "e47c52bfd466a3d277f545388d7e5c3689eedc04"}, "cell_type": "markdown", "source": ["This procedure finds linear blending weights from out-of-fold predictions by simple minimization. It doesn't take very long to complete, and I would normally use N-fold validation with the same folds as above. However, for the purpose of this competition the gain from doing so is negligible compared to extra time it takes.\n", "\n", "It may be worth your while to read the inline comments below about negative weights, and to try this notebook with and without them."]}, {"metadata": {"collapsed": true, "_cell_guid": "2fb0c7e1-682c-49d0-a760-f5d86e59b163", "_uuid": "dc4d898167a1d4a95889c0e8dff1eb322dad1aef"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["blend_time = timer(None)\n", "\n", "blend_train.append(sgd_avpred)\n", "blend_train.append(ridge_avpred)\n", "blend_train.append(lgb_avpred)\n", "blend_train = np.array(blend_train)\n", "\n", "blend_test.append(sgd_mpred)\n", "blend_test.append(ridge_mpred)\n", "blend_test.append(lgb_mpred)\n", "blend_test = np.array(blend_test)\n", "\n", "print('\\n Finding Blending Weights ...')\n", "res_list = []\n", "weights_list = []\n", "#for k in range(1000):\n", "for k in range(20):\n", "    starting_values = np.random.uniform(size=len(blend_train))\n", "\n", "    #######\n", "    # I used to think that weights should not be negative - many agree with that.\n", "    # I've come around on that issues as negative weights sometimes do help.\n", "    # If you don't think so, just swap the two lines below.\n", "    #######\n", "\n", "#    bounds = [(0, 1)]*len(blend_train)\n", "    bounds = [(-1, 1)] * len(blend_train)\n", "\n", "    res = minimize(rmse_min_func,\n", "                   starting_values,\n", "                   method='L-BFGS-B',\n", "                   bounds=bounds,\n", "                   options={'disp': False,\n", "                            'maxiter': 100000})\n", "    res_list.append(res['fun'])\n", "    weights_list.append(res['x'])\n", "    print('{iter}\\tScore: {score}\\tWeights: {weights}'.format(\n", "        iter=(k + 1),\n", "        score=res['fun'],\n", "        weights='\\t'.join([str(item) for item in res['x']])))\n", "\n", "bestSC = np.min(res_list)\n", "bestWght = weights_list[np.argmin(res_list)]\n", "weights = bestWght\n", "blend_score = round(bestSC, 6)\n", "\n", "print('\\n Ensemble Score: {best_score}'.format(best_score=bestSC))\n", "print('\\n Best Weights: {weights}'.format(weights=bestWght))\n", "\n", "train_prices = np.zeros(len(blend_train[0]))\n", "test_prices = np.zeros(len(blend_test[0]))\n", "\n", "print('\\n Your final model:')\n", "for k in range(len(blend_test)):\n", "    print(' %.6f * model-%d' % (weights[k], (k + 1)))\n", "    test_prices += blend_test[k] * weights[k]\n", "\n", "for k in range(len(blend_train)):\n", "    train_prices += blend_train[k] * weights[k]\n", "\n", "timer(blend_time)"]}, {"metadata": {"_cell_guid": "a2f14ec9-bdd0-44b0-be46-635c88f7516c", "_uuid": "0f81225dba88e2bc1ad2a0d7bb77f69e293fb925"}, "cell_type": "markdown", "source": ["Hopefully the plots show that Ridge and SGDRegressor tend to under-predict prices as a general rule. That is especially severe for many items priced at \\$1000+. LightGBM does better when it comes to under-predicting \\$1000+ items, but over-predicts a good number of items that cost \\$250-\\$1000."]}, {"metadata": {"collapsed": true, "_cell_guid": "61cd0a8b-7e34-47b0-9b65-406c454646da", "_uuid": "7dcd9a08eedcb2b2cd0c434641691bc757ec3345"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["# Plotting\n", "print('\\n Making scatter plots of actual vs. predicted prices ...')\n", "x_true = np.expm1(avreal)\n", "\n", "x_pred = np.expm1(sgd_avpred)\n", "#cm = plt.cm.get_cmap('RdYlBu')\n", "cm = plt.cm.get_cmap('winter')\n", "# Normalized prediction error clipped so the color-coding covers -75% to 75% range\n", "x_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\n", "plt.figure(1, figsize=(12, 10))\n", "plt.title('Actual vs. Predicted Prices - SGD Regressor')\n", "plt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\n", "plt.colorbar()\n", "plt.plot([x_true.min() - 50, x_true.max() + 50],\n", "         [x_true.min() - 50, x_true.max() + 50],\n", "         'k--',\n", "         lw=1)\n", "plt.xlabel('Prices')\n", "plt.ylabel('Predicted Prices')\n", "plt.xlim(-50, 2050)\n", "plt.ylim(-50, 2050)\n", "plt.tight_layout()\n", "plt.savefig('Mercari-SGDRegressor-3fold-train-predictions-00-v1.png')\n", "plt.show()\n", "#plt.close(1)\n", "\n", "x_pred = np.expm1(ridge_avpred)\n", "#cm = plt.cm.get_cmap('RdYlBu')\n", "cm = plt.cm.get_cmap('winter')\n", "# Normalized prediction error clipped so the color-coding covers -75% to 75% range\n", "x_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\n", "plt.figure(2, figsize=(12, 10))\n", "plt.title('Actual vs. Predicted Prices - Ridge Regressor')\n", "plt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\n", "plt.colorbar()\n", "plt.plot([x_true.min() - 50, x_true.max() + 50],\n", "         [x_true.min() - 50, x_true.max() + 50],\n", "         'k--',\n", "         lw=1)\n", "plt.xlabel('Prices')\n", "plt.ylabel('Predicted Prices')\n", "plt.xlim(-50, 2050)\n", "plt.ylim(-50, 2050)\n", "plt.tight_layout()\n", "plt.savefig('Mercari-Ridge-3fold-train-predictions-00-v1.png')\n", "plt.show()\n", "#plt.close(2)\n", "\n", "x_pred = np.expm1(lgb_avpred)\n", "#cm = plt.cm.get_cmap('RdYlBu')\n", "cm = plt.cm.get_cmap('winter')\n", "# Normalized prediction error clipped so the color-coding covers -75% to 75% range\n", "x_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\n", "plt.figure(3, figsize=(12, 10))\n", "plt.title('Actual vs. Predicted Prices - LightGBM')\n", "plt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\n", "plt.colorbar()\n", "plt.plot([x_true.min() - 50, x_true.max() + 50],\n", "         [x_true.min() - 50, x_true.max() + 50],\n", "         'k--',\n", "         lw=1)\n", "plt.xlabel('Prices')\n", "plt.ylabel('Predicted Prices')\n", "plt.xlim(-50, 2050)\n", "plt.ylim(-50, 2050)\n", "plt.tight_layout()\n", "plt.savefig('Mercari-LightGBM-3fold-train-predictions-00-v1.png')\n", "plt.show()\n", "#plt.close(3)\n", "\n", "x_pred = np.expm1(train_prices)\n", "#cm = plt.cm.get_cmap('RdYlBu')\n", "cm = plt.cm.get_cmap('winter')\n", "# Normalized prediction error clipped so the color-coding covers -75% to 75% range\n", "x_diff = np.clip(100 * ((x_pred - x_true) / x_true), -75, 75)\n", "plt.figure(4, figsize=(12, 10))\n", "plt.title('Actual vs. Predicted Prices - Blend')\n", "plt.scatter(x_true, x_pred, c=x_diff, s=10, cmap=cm)\n", "plt.colorbar()\n", "plt.plot([x_true.min() - 50, x_true.max() + 50],\n", "         [x_true.min() - 50, x_true.max() + 50],\n", "         'k--',\n", "         lw=1)\n", "plt.xlabel('Prices')\n", "plt.ylabel('Predicted Prices')\n", "plt.xlim(-50, 2050)\n", "plt.ylim(-50, 2050)\n", "plt.tight_layout()\n", "plt.savefig('Mercari-blend-3fold-train-predictions-00-v1.png')\n", "plt.show()\n", "#plt.close(4)"]}, {"metadata": {"_cell_guid": "9dd390fe-0100-4503-94a8-d2abce3e943f", "_uuid": "553949e57ba506b37ae62c54fa4fd7f3bc01e7d8"}, "cell_type": "markdown", "source": ["Saving all the files in this block. Note that file names have corresponding out-of-fold scores in them, as well as unique time stamps. That's my usual practice, but it should be avoided for production scripts per Kaggle guidelines."]}, {"metadata": {"collapsed": true, "_cell_guid": "408626a0-d620-49fe-be5f-d51e3a77159b", "_uuid": "57318c62b99a290970d7116e1c2eb25a952cab48"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["now = datetime.now()\n", "oof_result = pd.DataFrame(np.expm1(avreal), columns=['price'])\n", "oof_result['prediction'] = np.expm1(sgd_avpred)\n", "oof_result['train_id'] = idpred\n", "oof_result['train_id'] = oof_result['train_id'].astype(np.int32)\n", "oof_result.sort_values('train_id', inplace=True)\n", "oof_result = oof_result[['train_id', 'price', 'prediction']]\n", "sub_file = 'train_SGDRegressor-3fold-00-v1-oof_' + str(sgd_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing out-of-fold train file::  %s' % sub_file)\n", "oof_result.to_csv(sub_file, index=False, float_format='%.6f')\n", "\n", "result = pd.DataFrame(np.expm1(sgd_mpred), columns=['price'])\n", "result['test_id'] = te_ids\n", "result['test_id'] = result['test_id'].astype(np.int32)\n", "result = result.set_index('test_id')\n", "print('\\n First 10 lines of your SGD Regressor prediction:')\n", "print(result.head(10))\n", "sub_file = 'submission_SGDRegressor-3fold-00-v1_' + str(sgd_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing submission:  %s' % sub_file)\n", "result.to_csv(sub_file, index=True, index_label='test_id', float_format='%.6f')\n", "\n", "oof_result = pd.DataFrame(np.expm1(avreal), columns=['price'])\n", "oof_result['prediction'] = np.expm1(ridge_avpred)\n", "oof_result['train_id'] = idpred\n", "oof_result['train_id'] = oof_result['train_id'].astype(np.int32)\n", "oof_result.sort_values('train_id', inplace=True)\n", "oof_result = oof_result[['train_id', 'price', 'prediction']]\n", "sub_file = 'train_Ridge-3fold-00-v1-oof_' + str(ridge_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing out-of-fold train file::  %s' % sub_file)\n", "oof_result.to_csv(sub_file, index=False, float_format='%.6f')\n", "\n", "result = pd.DataFrame(np.expm1(ridge_mpred), columns=['price'])\n", "result['test_id'] = te_ids\n", "result['test_id'] = result['test_id'].astype(np.int32)\n", "result = result.set_index('test_id')\n", "print('\\n First 10 lines of your Ridge Regressor prediction:')\n", "print(result.head(10))\n", "sub_file = 'submission_Ridge-3fold-00-v1_' + str(ridge_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing submission:  %s' % sub_file)\n", "result.to_csv(sub_file, index=True, index_label='test_id', float_format='%.6f')\n", "\n", "oof_result = pd.DataFrame(np.expm1(avreal), columns=['price'])\n", "oof_result['prediction'] = np.expm1(lgb_avpred)\n", "oof_result['train_id'] = idpred\n", "oof_result['train_id'] = oof_result['train_id'].astype(np.int32)\n", "oof_result.sort_values('train_id', inplace=True)\n", "oof_result = oof_result[['train_id', 'price', 'prediction']]\n", "sub_file = 'train_LightGBM-3fold-00-v1-oof_' + str(lgb_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing out-of-fold train file::  %s' % sub_file)\n", "oof_result.to_csv(sub_file, index=False, float_format='%.6f')\n", "\n", "result = pd.DataFrame(np.expm1(lgb_mpred), columns=['price'])\n", "result['test_id'] = te_ids\n", "result['test_id'] = result['test_id'].astype(np.int32)\n", "result = result.set_index('test_id')\n", "print('\\n First 10 lines of your LightGBM prediction:')\n", "print(result.head(10))\n", "sub_file = 'submission_LightGBM-3fold-00-v1_' + str(lgb_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing submission:  %s' % sub_file)\n", "result.to_csv(sub_file, index=True, index_label='test_id', float_format='%.6f')\n", "\n", "oof_result = pd.DataFrame(np.expm1(avreal), columns=['price'])\n", "oof_result['prediction'] = np.expm1(train_prices)\n", "oof_result['train_id'] = idpred\n", "oof_result['train_id'] = oof_result['train_id'].astype(np.int32)\n", "oof_result.sort_values('train_id', inplace=True)\n", "oof_result = oof_result[['train_id', 'price', 'prediction']]\n", "sub_file = 'train_blend-3fold-00-v1-oof_' + str(blend_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing out-of-fold train file::  %s' % sub_file)\n", "oof_result.to_csv(sub_file, index=False, float_format='%.6f')\n", "\n", "result = pd.DataFrame(np.expm1(test_prices), columns=['price'])\n", "result['test_id'] = te_ids\n", "result['test_id'] = result['test_id'].astype(np.int32)\n", "result = result.set_index('test_id')\n", "print('\\n First 10 lines of your blended prediction:')\n", "print(result.head(10))\n", "sub_file = 'submission_blend-3fold-00-v1_' + str(blend_score) + '_' + str(\n", "    now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\n", "print('\\n Writing submission:  %s' % sub_file)\n", "result.to_csv(sub_file, index=True, index_label='test_id', float_format='%.6f')\n", "\n", "#timer(starttime)\n"]}], "nbformat_minor": 1}