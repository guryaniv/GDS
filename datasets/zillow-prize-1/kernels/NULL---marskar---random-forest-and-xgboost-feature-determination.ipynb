{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["# Introduction"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Zillow Prize"], "metadata": {}, "cell_type": "markdown"}, {"source": ["The [Zillow Prize](https://www.zillow.com/promo/Zillow-prize/) is a data science and machine learning competition organized by the [Zillow real estate company](https://www.zillow.com/). \n", "The competition is host by [Kaggle](https://www.kaggle.com/), a company that provides datasets and computational kernels for data science challenges. Kaggle was [purchased by Google](https://techcrunch.com/2017/03/07/google-is-acquiring-data-science-community-kaggle/) earlier this year. \n", "The Zillow Prize competition has recently declared to be the [second largest data science challenge on Kaggle](https://www.prnewswire.com/news-releases/zillow-prize-ranks-as-one-of-most-popular-machine-learning-contests-of-all-time-300539347.html).\n", "The purpose of the Zillow prize is to inspire data scientists around the world to work on improving the accuracy of the Zillow \"Zestimate\" <cite data-cite=\"5251998/F6YV2BYE\"></cite> home price estimate algorithm. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Zestimate Algorithm"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Zillow\u2019s Zestimate home valuation was first released in 2006 <cite data-cite=\"5251998/6ZB7HZKT\"></cite> and has since had a major impact on the United States real estate industry. The Zestimate algorithm created a new standard of providing free, publicly available housing data and home price estimates. \n", "The Zestimate algorithm relies on [\"7.5 million statistical and machine learning models\"](https://www.prnewswire.com/news-releases/zillow-prize-ranks-as-one-of-most-popular-machine-learning-contests-of-all-time-300539347.html) that have been refined over the years to have a [margin of error on only 5%](https://www.kaggle.com/c/zillow-prize-1).\n", "\n", "The real estate industry is a major contributor to the U.S economy and was worth roughly [30 trillion dollars in 2016](https://www.zillow.com/research/2016-total-home-value-rents-14028/). Homeownership equity is a major form of wealth that Americans hold, while mortgages are a major type of American private debt. It is extremely important for U.S. homeowners and for the U.S. economy that home prices are estimated correctly. Without proper estimates, lender and borrowers cannot confidently monitor their assets and liabilities. Overevaluation of real estate assets and the potential returns on real estate debt for lenders has been linked to the great recession. <cite data-cite=\"5251998/GKWP3DLG\"></cite> "], "metadata": {}, "cell_type": "markdown"}, {"source": ["## My Goal"], "metadata": {}, "cell_type": "markdown"}, {"source": ["My goal in working with the Zillow Prize data was to answer the question of which features in the dataset are the most important determinants of the major evaluation metric for the Zillow Prize. This metric is called \"logerror\" in the data and is defined as the difference between the log of Zestimate price and the log of the actual sales price. To achieve this goal, I cleaned the provided housing valuation data and employed two different machine learning methods, random forest and xgboost, to calculate importance scores for each of the features in the cleaned dataset."], "metadata": {}, "cell_type": "markdown"}, {"source": ["# Methods"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Data"], "metadata": {}, "cell_type": "markdown"}, {"source": ["I obtained the data from [Kaggle website](https://www.kaggle.com/c/zillow-prize-1/data). The data consisted of the following files:\n", "\n", "- properties_2016.csv.zip\n", "- properties_2017.csv.zip\n", "- sample_submission.csv\n", "- train_2016_v2.csv.zip\n", "- train_2017.csv.zip\n", "- zillow_data_dictionary.xlsx\n", "\n", "The `zillow_data_dictionary.xlsx` is a code book that explains the data.\n", "The data are available on the [Kaggle website](https://www.kaggle.com/c/zillow-prize-1/data), but I also made the data available on [figshare](https://figshare.com/s/a54a364682b4b02caa5a) where they can be accessed without the need to create/enter a username and password. The data can also be accessed as part of my [Zillow Kaggle kernel](https://www.kaggle.com/marskar/random-forest-and-xgboost-feature-determination)."], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Analysis"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Data analysis was done in Jupyter Notebook (formerly known as IPython Notebook) <cite data-cite=\"5251998/SH25XT8L\"></cite> Integrated Development Environment using the Python language <cite data-cite=\"5251998/FGTD82L2\"></cite> and a number of software packages. I used NumPy <cite data-cite=\"5251998/3SWILWGR\"></cite> and Pandas <cite data-cite=\"5251998/K3NZPGU9\"></cite> for data wrangling. To calculate the importance scores, I used the Scikit-learn <cite data-cite=\"5251998/QUC7G24H\"></cite> and XGBoost <cite data-cite=\"5251998/9VNLRITL\"></cite> machine learning libraries. Finally, I visualized the data with the Matplotlib <cite data-cite=\"5251998/WP5LZ6AZ\"></cite> and Seaborn <cite data-cite=\"5251998/Z5Z8R3J8\"></cite> libraries.\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Reproducibility"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Reproducibility is extremely important in scientific research yet many examples of problematic studies exist in the literature <cite data-cite=\"5251998/UXR4ZTUS\"></cite>.\n", "To make the analysis reproducible for users of the [Anaconda distribution of Python](https://www.anaconda.com/download/). The names and versions of each package used herein are listed in the accompanying `env.yml` file in the `config` folder.\n", "The computational environment used to analyze the data can be recreated using this `env.yml` file and the [`conda` package and environment manager](https://conda.io/docs/using/envs.html).\n", "Additionally, details on how to setup a Docker image capable of running the analysis is included in the `README.md` file in the `config` folder.\n", "The code in the form of a jupyter notebook can also be run on the Kaggle website (this requires logging in with a username and password) by accessing the [Zillow Kaggle kernel](https://www.kaggle.com/marskar/random-forest-and-xgboost-feature-determination) I created.\n", "More information on the details of how this project was created and the computational environment was configured can be found in the accompanying `README.md` file."], "metadata": {}, "cell_type": "markdown"}, {"source": ["# Results"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Missing values"], "metadata": {}, "cell_type": "markdown"}, {"source": ["The dataset provided for the Zillow challenge consisted of 58 features. I first determined the percent representation of missing for each of these features (Figure 1).\n", "I then used median imputation to fill in the missing values for the all of the features. I decided not to remove any features based on the percent missing values."], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Feature Importance"], "metadata": {}, "cell_type": "markdown"}, {"source": ["Next, I used two prediction methods: Random Forest and XGBoost to obtain importance scores for each of the remaining features (Figure 2). I made a final list of features by removing any features that had an importance of less than 0.001 from Random Forest model or an XGBoost F score importance of less than 10. Of the initial 58 features, 39 features remained in final feature list. "], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # data visualization\n", "import seaborn as sns\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.ensemble import ExtraTreesRegressor;"], "execution_count": null, "metadata": {"_uuid": "da163d263ba61685860530a25d26fa8282f04c42", "collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# Set plot parameters\n", "from IPython.display import set_matplotlib_formats\n", "\n", "plt.rcParams['savefig.dpi'] = 300\n", "\n", "%matplotlib inline\n", "### Seaborn style\n", "sns.set_style(\"whitegrid\")"], "execution_count": null, "metadata": {"_uuid": "da163d263ba61685860530a25d26fa8282f04c42", "collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# Input data files are available in the \"../input/\" directory, on Kaggle and the GitHub repo for this project.\n", "prop = pd.read_csv(\"../input/properties_2016.csv\", low_memory=False)\n", "prop.shape;"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# Now I will calculate the percent missing values(NaN)\n", "nan = prop.isnull().sum()/len(prop)*100"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["### Plotting NaN counts\n", "nan_sorted = nan.sort_values(ascending=False).to_frame().reset_index()\n", "nan_sorted.columns = ['Column', 'percentNaN']\n", "nan_sorted.head();"], "execution_count": null, "metadata": {"code_folding": [], "collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])"], "execution_count": null, "metadata": {"scrolled": true, "collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["train['transaction_month'] = pd.DatetimeIndex(train['transactiondate']).month\n", "train.sort_values('transaction_month', axis=0, ascending=True, inplace=True)"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# Here I will merge the train and properties datasets\n", "train = pd.merge(train, prop, on='parcelid', how='left')"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["# Now I will impute the missing values with median values to compute the importance scores\n", "median_values_train = train.median(axis=0)\n", "train = train.fillna(median_values_train, inplace=True)"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["for c in train[['transactiondate', 'hashottuborspa', 'propertycountylandusecode', 'propertyzoningdesc', 'fireplaceflag', 'taxdelinquencyflag']]:\n", "    label = LabelEncoder()\n", "    label.fit(list(train[c].values))\n", "    train[c] = label.transform(list(train[c].values))\n", "\n", "x_train = train.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n", "y_train = train['logerror']"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["rf = RandomForestRegressor(n_estimators=30, max_features=None)\n", "rf.fit(x_train, y_train);"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["rf_importance = rf.feature_importances_\n", "rf_importance_df = pd.DataFrame()\n", "rf_importance_df['features'] = x_train.columns\n", "rf_importance_df['importance'] = rf_importance\n", "rf_importance_df.head();"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["rf_importance_df.sort_values('importance', axis=0, inplace=True, ascending=False)\n", "\n", "rf_importance_df_trim = rf_importance_df[rf_importance_df.importance>0.001]\n", "\n", "rf_importance_df_trim.tail()\n", "\n", "rf_feature_list = rf_importance_df_trim.features"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["xgb_params = {\n", "    'eta': 0.05,\n", "    'max_depth': 8,\n", "    'subsample': 0.7,\n", "    'colsample_bytree': 0.7,\n", "    'objective': 'reg:linear',\n", "    'silent': 1,\n", "    'seed' : 0\n", "}\n", "dtrain = xgb.DMatrix(x_train, y_train, feature_names=x_train.columns.values)\n", "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["xgbdict = model.get_score()\n", "\n", "xgb_importance_df = pd.DataFrame()\n", "xgb_importance_df['features'] = xgbdict.keys()\n", "xgb_importance_df['importance'] = xgbdict.values()\n", "\n", "xgb_importance_df.sort_values('importance', axis=0, inplace=True, ascending=False)\n", "\n", "xgb_importance_df_trim = xgb_importance_df[xgb_importance_df.importance>=10]\n", "\n", "xgb_feature_list = xgb_importance_df_trim.features\n", "\n", "feature_list = xgb_feature_list.append(rf_feature_list)\n", "\n", "feature_list = feature_list.unique()"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["feature_list = list(feature_list)"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "source": ["fig, ax = plt.subplots(figsize=(48, 100), dpi=300)\n", "sns.set_context(\"poster\", font_scale=6)\n", "# [1]\n", "sns.barplot(x=\"importance\", y=\"features\", data=rf_importance_df, color='Green', ax=ax)\n", "ax.set(xlabel=\"Importance (Variance explained)\", ylabel=\"\")\n", "ax.set_title('Random Forest Importance', fontsize= 96)\n", "plt.show()"], "execution_count": null, "metadata": {"caption": "Figure 1. Missing values in the Zillow dataset. Features from the Zillow dataset were sorted by percentage of missing values (NaN) and then plotted with the corresponding missing value percentage represented as red bars. Some features consist almost entirely of missing values.", "widefigure": true, "label": "fig:somelabel"}, "cell_type": "code"}, {"outputs": [], "source": ["fig, ax = plt.subplots(figsize=(48, 100), dpi=300)\n", "sns.set_context(\"poster\", font_scale=6)\n", "# [2]\n", "xgb.plot_importance(model, height=0.85, grid = False, color=\"blue\", ax=ax)\n", "ax.xaxis.grid()\n", "ax.set_title('XGBoost Importance', fontsize= 96)\n", "ax.set(xlabel=\"Importance (F score)\", ylabel=\"\")\n", "plt.show()"], "execution_count": null, "metadata": {"caption": "Figure 2. XGBoost Importance Scores. XGBoost method was used to obtain importance scores for each of the features in the Zillow dataset. The features were ranked by XGBoost importance and plotted with the corresponding XGBoost score. The XGBoost method calculates importance as an F score.", "widefigure": true, "label": "fig:somelabel"}, "cell_type": "code"}, {"source": ["# Conclusions"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Limitations"], "metadata": {}, "cell_type": "markdown"}, {"source": ["For my first Kaggle competition, I was only interested in how to determine which features would be most likely to be useful in making a prediction. One major limitation of the analysis I did was that I did not validate my calculated importance scores using data set aside for this purpose. Another major limitation was that I did not show that a model with a smaller set of high-importance features could perform at a similar level as a model with all of the features. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Lessons Learned"], "metadata": {}, "cell_type": "markdown"}, {"source": ["In addition to learning, how to calculate importance scores using the Random Forest and XGBoost methods. I learned a great deal about Kaggle competition while working on this project. For example, I learned how to use the Kaggle Jupyter Notebook interface and make and publish [my own Kaggle kernel](https://www.kaggle.com/marskar/random-forest-and-xgboost-feature-determination). This knowledge will allow me to compete in future Kaggle challenges and share my code in the form of Kaggle kernels. I also learned how to use Docker images for reproducibility, which is a useful means of recreating the environment in which a data analysis was completed. Most importantly, I learned how to create a reproducible report that contains citations, links, plots, code and text in a single Jupyter Notebook source file that can generate various types of output files including a LaTeX formatted PDF."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Bibliography is added in post-processing"], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["<div class=\"cite2c-biblio\"></div>"], "metadata": {}, "cell_type": "markdown"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "toc": {"toc_position": {}, "toc_window_display": false, "number_sections": true, "toc_cell": true, "nav_menu": {}, "sideBar": true, "skip_h1_title": true, "toc_section_display": "block"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.3"}, "cite2c": {"citations": {"5251998/UXR4ZTUS": {"volume": "329", "id": "5251998/UXR4ZTUS", "issue": "5992", "issued": {"year": "2010"}, "container-title": "Science", "type": "article-journal", "language": "eng", "URL": "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?cmd=prlinks&dbfrom=pubmed&retmode=ref&id=20688986", "title": "Cancer research. As questions grow, Duke halts trials, launches investigation.", "page": "614\u20135", "author": [{"family": "Couzin-Frankel", "given": "J."}], "page-first": "614"}, "5251998/GKWP3DLG": {"author": [{"family": "Verick", "given": "Sher"}, {"family": "Islam", "given": "Iyanatul"}], "id": "5251998/GKWP3DLG", "title": "The great recession of 2008-2009: causes, consequences and policy responses", "type": "article-journal", "issued": {"year": "2010"}}, "5251998/6ZB7HZKT": {"id": "5251998/6ZB7HZKT", "title": "How Good Are Zillows Estimates?", "issued": {"year": "2007"}, "container-title": "Wall Street Journal", "author": [{"family": "Hagerty", "given": "James R"}], "type": "article-journal"}, "5251998/K3NZPGU9": {"event-place": "Austin, Texas", "publisher-place": "Austin, Texas", "id": "5251998/K3NZPGU9", "editor": [{"family": "Walt", "given": "S. J. van der"}, {"family": "Millman", "given": "K. J."}], "issued": {"year": "2010", "month": "7"}, "container-title": "proceedings of the 9th Python in Science Conference", "title": "Data structures for statistical computing in Python", "author": [{"family": "McKinney", "given": "W."}], "type": "paper-conference"}, "5251998/FGTD82L2": {"volume": "13", "id": "5251998/FGTD82L2", "issue": "2", "issued": {"year": "2011"}, "container-title": "Computing in Science & Engineering", "page": "13\u201321", "URL": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5582063", "author": [{"family": "P\u00e9rez", "given": "F."}, {"family": "Granger", "given": "B. E."}, {"family": "Hunter", "given": "J. D."}], "page-first": "13", "type": "article-journal", "title": "Python: an ecosystem for scientific computing"}, "5251998/Z5Z8R3J8": {"volume": "27", "page": "2016", "id": "5251998/Z5Z8R3J8", "issued": {"year": "2015"}, "container-title": "O\u2019Reilly Media. Retrieved April", "title": "Data visualization with Seaborn", "author": [{"family": "VanderPlas", "given": "J"}], "page-first": "2016", "type": "article-journal"}, "5251998/F6YV2BYE": {"id": "5251998/F6YV2BYE", "title": "What is a Zestimate?", "issued": {"year": "2013"}, "author": [{"family": "Tuman", "given": "Diane"}], "type": "book", "publisher": "Apr"}, "5251998/3SWILWGR": {"volume": "13", "id": "5251998/3SWILWGR", "issue": "2", "issued": {"year": "2011", "month": "3", "month_end": "4", "year_end": "2011"}, "container-title": "Computing in Science & Engineering", "page": "22\u201330", "author": [{"family": "van der Walt", "given": "S."}, {"family": "Colbert", "given": "S. C."}, {"family": "Varoquaux", "given": "G."}], "page-first": "22", "type": "article-journal", "title": "The NumPy Array: A Structure for Efficient Numerical Computation", "DOI": "10.1109/MCSE.2011.37"}, "5251998/WP5LZ6AZ": {"volume": "9", "abstract": "Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.", "id": "5251998/WP5LZ6AZ", "issue": "3", "issued": {"year": "2007", "month": "5"}, "container-title": "Computing In Science & Engineering", "page": "90\u201395", "author": [{"family": "Hunter", "given": "J. D."}], "page-first": "90", "type": "article-journal", "title": "Matplotlib: A 2D graphics environment"}, "5251998/9VNLRITL": {"page": "785\u2013794", "id": "5251998/9VNLRITL", "issued": {"year": "2016"}, "container-title": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining", "title": "Xgboost: A scalable tree boosting system", "author": [{"family": "Chen", "given": "Tianqi"}, {"family": "Guestrin", "given": "Carlos"}], "page-first": "785", "publisher": "ACM", "type": "paper-conference"}, "5251998/SH25XT8L": {"volume": "9", "id": "5251998/SH25XT8L", "issue": "3", "issued": {"year": "2007", "month": "5"}, "container-title": "Computing in Science & Engineering", "page": "21\u201329", "author": [{"family": "P\u00e9rez", "given": "F."}, {"family": "Granger", "given": "B. E."}], "page-first": "21", "type": "article-journal", "title": "IPython: a System for Interactive Scientific Computing"}, "5251998/QUC7G24H": {"id": "5251998/QUC7G24H", "title": "Mastering Machine Learning with scikit-learn", "issued": {"year": "2014"}, "author": [{"family": "Hackeling", "given": "Gavin"}], "type": "book", "publisher": "Packt Publishing Ltd"}}}, "hide_input": false, "celltoolbar": "Edit Metadata", "latex_metadata": {"author": "Martin Skarzynski", "title": "Random Forest and XGBoost determination of Zillow Prize dataset feature importance", "affiliation": "Johns Hopkins University"}}}