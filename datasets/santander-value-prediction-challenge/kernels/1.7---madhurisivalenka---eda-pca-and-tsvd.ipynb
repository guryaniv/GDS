{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25a462be61022a1c248ed7f5eef60a4c0e24216b","collapsed":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"59937f8b6a648f488e8a27ee87060ec640347e1e","collapsed":true},"cell_type":"code","source":"#remove the ID field from the train data\ntrain = train.drop('ID', axis = 1)\ntest = test.drop('ID', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebd70b64f871110a4ea61e55457561fca81744b4","collapsed":true},"cell_type":"code","source":"#check columns of train data\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e70f45af93614476e09d4863b59f082357a6fece"},"cell_type":"markdown","source":"Initial test and train data exploration"},{"metadata":{"trusted":true,"_uuid":"c3ef44c72a4dd939ed91d71c8333ffa7ff88bf3a","collapsed":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a874809334fa7741a0528c96d6e5a430504422d","scrolled":true,"collapsed":true},"cell_type":"code","source":"#We have no categorical variables.\ntrain.select_dtypes(include=['object']).dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73e7b2751a641a61ee6404b0f9c5e7d0d8813eda","collapsed":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e7d79ee0bb75572bdb4a4c8b3b0f9149b45c6c0","collapsed":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a59720e80bb211d393f2f39e5fcf878f90ee75","scrolled":true,"collapsed":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fb978cf4ddd6bea5f33497246e198410811590f","collapsed":true},"cell_type":"code","source":"train.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc936573cc11180adf5c7476b020e5f34d6cd17","collapsed":true},"cell_type":"code","source":"test.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b228a4771ddc55dba0ce610b7a9f431834f878a0"},"cell_type":"markdown","source":"**Observations**\n1. Same number of columns between the test and train data (train has 1 extra \"target field\" column)\n2. Most importantly, the number of rows in the train data are **MUCH** lesser than the number of columns! We have around 3000 columns and around 4500 rows. Test data is better.. \n3. Columns names do not make sense, so we will have to perform feature extraction for this data to make sense.\n4. **No missing data** in the train and test sets! "},{"metadata":{"_uuid":"2c5004c1a9dcbe6fe65bcb91aed111bfb3421ec4"},"cell_type":"markdown","source":"**UNIVARIATE ANALYSIS OF THE TARGET FIELD**\n   Let's pick the target field and try to analyse it.\n"},{"metadata":{"trusted":true,"_uuid":"71b36f17ec9462be97b7d8d0d24d81a8743826cb","collapsed":true},"cell_type":"code","source":"train['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"932c28f5f8876ad4c0322c4197c7fa8c23159c00","collapsed":true},"cell_type":"code","source":"#plot a distribution plot to see the distribution of the target field\nplt.figure(figsize=(8,5))\nsns.distplot(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe732c7d6adda2e53dfca9853afe5c83046f0387"},"cell_type":"markdown","source":"This seems to be a highly skewed target variable. Let's take the log of it to check the distribution."},{"metadata":{"trusted":true,"_uuid":"e1e3c4a6e9c7c4c8b8011cbc2d094498d6f66f65","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.distplot(np.log1p(train['target']), kde='False')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d93ec5fd6b525b6cd0bd2808f302da7b846fb986"},"cell_type":"markdown","source":"Better distributed now! Let's check the train.info after taking the log"},{"metadata":{"trusted":true,"_uuid":"43f5262201ab0b0ba2122e4cf45e55a0c65eec15","collapsed":true},"cell_type":"code","source":"np.log1p(train['target']).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a18a583cffa0a6958c239c1b65c33d6b34cd271b"},"cell_type":"markdown","source":"This is a LOT better! Helps us better understand the distribution of the 'target' column. Lets plot some scatter plots with a few other variabls to see its spread.\nLet's pick the first column '48df886f9'"},{"metadata":{"trusted":true,"_uuid":"e7a1e3a37f484f24c66d6001d0b87c442530abf3","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.scatter(np.log(train['48df886f9']),np.log(train['target']))\nplt.xlabel('48df886f9')\nplt.ylabel('Target')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b275fecbf1521ab0b0b7ac94d6386de567c5d041"},"cell_type":"markdown","source":"Perform some basic checks on the target column."},{"metadata":{"trusted":true,"_uuid":"3acdddebee13ad25f9791dc663bd9194b6aefcd5","_kg_hide-output":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"train['target'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87c23cc536415c767d7ec5a5fefac8e3ab976fb"},"cell_type":"markdown","source":"*  We can see that \"target\" variable ranges from values of 10^5 to 10^9. \n*  Next Check the count of the most common target value"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"7636a6c2e6a47b5009c6cb8dbccbd5295e402192","collapsed":true},"cell_type":"code","source":"Counter(train['target']).most_common()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"839f862ed2eeb1f77d18bd7fc652e15c6a60c5cf","scrolled":true,"collapsed":true},"cell_type":"code","source":"#Plot a boxplot\nplt.figure(figsize=(8,8))\nsns.boxplot(train['target'], orient='v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a34d1c9a28d055bc0dfb30255ce56bcd6f8573e4","collapsed":true},"cell_type":"code","source":"#separate the x and y variables for the train and test data\n#taking the log of the target variable as it is not well distributed.\nx_train = train.iloc[:,train.columns!='target']\ny_train = np.log1p(train.iloc[:,train.columns=='target'])\nx_test = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3928b13fa7ee5f9c451b081e1fe3e7fc45b0cef9"},"cell_type":"code","source":"#copy the x_train, y_train, and x_test datasets\nx_train_copy= x_train.copy()\nx_test_copy= x_test.copy()\ny_train_copy= y_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"b1f8123fa57d1e19793fb6a23485764be4d51ca8","collapsed":true},"cell_type":"code","source":"x_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"758286bf9eb66190b237ed267ba0411862dfce03","collapsed":true},"cell_type":"code","source":"print(y_train.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d62fb1a1600725d4ebf101b3de774ee12ec4e3f0","collapsed":true},"cell_type":"code","source":"print(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"006de9ce532bec192496534769f1c0997a22e8e3","collapsed":true},"cell_type":"code","source":"print(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7c0e30b4ad5dbc8d226960fc15f53101f09a308","collapsed":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93c11ea7a250a4a774da9da2c273f5cf4c9baf19","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09f42d6dbc0627f41dcea09089338f4a0d9377b3"},"cell_type":"markdown","source":"**Remove the columns with standard deviation = 0 from test and train set.**\n- Standard Deviation = 0 means that **every data point in a column is equal to its mean**. Also means that all of a column's values are **identical**.\n- Such columns really do not help us in prediction. So we will drop them"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"d94a7143d7bca3c84782ce430e8750b70e1dfbb5"},"cell_type":"code","source":"drop_cols=[]\nfor cols in x_train.columns:\n    if x_train[cols].std()==0:\n        drop_cols.append(cols)\nprint(\"Number of constant columns to be dropped: \", len(drop_cols))\nprint(drop_cols)\nx_train.drop(drop_cols,axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9be261444309326f48dad98dfd110b027676667c"},"cell_type":"markdown","source":"Check for constant columns on the test data"},{"metadata":{"trusted":true,"_uuid":"143da1488d05fa70f70540c559e4a4d33ea19ddf"},"cell_type":"code","source":"drop_cols_test=[]\nfor cols in x_test.columns:\n    if x_test[cols].std()==0:\n        drop_cols_test.append(cols)\nprint(\"Number of constant columns to be dropped: \", len(drop_cols_test))\nprint(drop_cols_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a45eebe2674e15f4b8ae0642fd2bf31d2ad64d2"},"cell_type":"markdown","source":"There are no constant columns from the test data. However, we still need to drop them as the shapes of the test and the train data need to be the same for modelling."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d5e14e64fdae2ebc220d1dbe7d995058f48f97a"},"cell_type":"code","source":"x_test.drop(drop_cols,axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3d8174e77460dc61ab2c4ac568b8f57f2fc20ac"},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8a80a368e7f9ab174b59c38dff1c2708805622a"},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6b65f48bf79f5ddc253a2c0781e3a5c0df33258"},"cell_type":"markdown","source":"**DIMENSIONALITY REDUCTION**\n- One of the major problems with this dataset is that it has too many predictors (almost 4900+). To go through each of these predictors and see which ones are significant for the model is going to be a tedious task. Instead, we can use one of the all-time favourite dimensionality reduction technique - Principle Component Analysis.\n- Before we can use PCA, we need to **STANDARDISE** the data (Standardisation and Normalization are used inter-dependently. Standardisation is moulding the data to between -1 and +1 data points. Normalisation is normalising the data so that the data points lie along the mean.)"},{"metadata":{"trusted":true,"_uuid":"a94777ebb109a615af26b54b5f06ae8eab27853c","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"818d7756c3a2e74d3dab534dd2c9981c5cd30636","collapsed":true},"cell_type":"code","source":"x_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a36f1bde2153342be308692f634b0f01299e004","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print(x_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c15d9cee1ce614cebfbcbfc4b0d452d563c63bb7"},"cell_type":"markdown","source":"Now that the data is scaled, we shall use PCA"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2080f592383108156d1915749ca53748b4e4719f"},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"e7b29df5e92ecd01397c01b8122e36040e94e238","collapsed":true},"cell_type":"code","source":"pca_x = PCA(0.95).fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdd4f64a191378c2f8b4c8b25060c053f9ee373e"},"cell_type":"code","source":"print('%d components explain 95%% of the variation in data' % pca_x.n_components_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f81d849eb3cf72ddf0a853ce4e320082d36dc498"},"cell_type":"markdown","source":"We can see that the first 1527 Principal Components attribute for about 95% variation in the data. We shall use these 1527 for our prediction "},{"metadata":{"trusted":true,"_uuid":"1a18fc0f1c7b1e7d3299ec5461e2df58d63e582a","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pca = PCA(n_components=1527)\n#fit with 1527 components on train data\npca.fit(x_train)\n#transform on train data\nx_train_pca = pca.transform(x_train)\n#transform on test data\nx_test_pca = pca.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c972ee5461a5fbf00398fca8c6f2188f85e507b2"},"cell_type":"markdown","source":"**MODELLING AND PREDICTION**\nWe shall use the following classifiers for our prediction \n- Random Forest"},{"metadata":{"trusted":true,"_uuid":"d4b44d04443387bd2cb78884ed726f3c21f18106"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train_pca, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e04ca4498b43f5e3eccd9ec982afef9aa578f95","collapsed":true},"cell_type":"code","source":"rf_pca_predict = rf.predict(x_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"5192ed9713a6aacdd8c527616ebd209a07cdc9a1"},"cell_type":"code","source":"rf_pca_predict = np.expm1(rf_pca_predict)\nprint(rf_pca_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83fb1d5193f6c11e5bf73b529b04669f90f4a6d5","collapsed":true},"cell_type":"code","source":"print(len(rf_pca_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19996b23a92b25831575d535b06f821c3241de5b"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission[\"target\"] = rf_pca_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9163d3e77db44906d0c47d86780ac3fcd94344eb"},"cell_type":"code","source":"print(submission.head())\nsubmission.to_csv('sub_PCA_LR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"925520f5fc752e1ac1c567884f61689ee29b80eb","collapsed":true},"cell_type":"code","source":"print(submission['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97f0575e30e1274a74661787896c68895bb58885"},"cell_type":"markdown","source":"\n**USING TSVD**\n- TSVD, which stands for Truncated Single Vector Decomposition is a dimensonality reduction methodology. Unlike PCA, we do not need to standardise the data before we pass it through a TSVD.\n- One of the main parameters is n_components which should be LESS THAN the number of dimensions.\n- For the sake of this problem, I shall randomly pick n_components as 1500, and then write a code to choose those components which attribute for 95% of variation in the data.\n- I am going to use the copies of the x_train, x_test, y_train datasets for TSVD."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"866a2484512e8a8aef928d36d0ab10e370dbdfce"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9802b8a9db169c223a0257d4d8c8b52b462bc031","collapsed":true},"cell_type":"code","source":"svd_x = TruncatedSVD(n_components=1500,n_iter=20, random_state=42)\nsvd_x.fit(x_train_copy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ac36862e38d8622f8d82a4907e60708daac5f01","collapsed":true},"cell_type":"code","source":"#code to select those components which attribute for 95% of variance in data\ncount = 0\nfor index, cumsum in enumerate(np.cumsum(svd_x.explained_variance_ratio_)):\n    if cumsum <=0.95:\n      count+=1  \n    else:\n        break\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55fbbd5c9c04a952793266eca056670700b74d32"},"cell_type":"markdown","source":"From the above result we can see that the first 601 components attrribte for 95% of the variation in data. We shall use these 601 components"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6978abc5c0ed7439cb1f32cabae7354cbfd9a450","collapsed":true},"cell_type":"code","source":"for index, cumsum in enumerate(np.cumsum(svd_x.explained_variance_ratio_)):\n    print(index, cumsum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1557090c51726c23aca440f1b3968b3d5c7b219","collapsed":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=601, random_state=42)\n#fit the TSVD on the train data\nsvd.fit(x_train_copy)\n#transform on the x_train data\nx_train_svd = svd.transform(x_train_copy)\n#transform on the x_test data\nx_test_svd = svd.transform(x_test_copy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd0de6d0472df4c48e665261de6a5f70b3fbca5f"},"cell_type":"markdown","source":"Use a Random Forest Regressor for modelling"},{"metadata":{"trusted":true,"_uuid":"bfe4559a255fe93f8afb68f4acc4a0060f780a12","collapsed":true},"cell_type":"code","source":"rf.fit(x_train_svd, y_train_copy)\nrf_tsvd_predict = rf.predict(x_test_svd)\nrf_tsvd_predict = np.expm1(rf_tsvd_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0709902f1893d2a6ff56c67f6c6d5fcd5a087e6","collapsed":true},"cell_type":"code","source":"print(rf_tsvd_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b18f9235bd5773e2bb8116304bafa0659d1e1754","collapsed":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission[\"target\"] = rf_tsvd_predict\nprint(submission.head())\nsubmission.to_csv('sub_TSVD_LR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e280e05ca01bb4b2b29019bda8cc76d5552b1e4c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}