{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25863b15-14a6-3e4d-c4b1-d4b5f6ce68fb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "data = pd.read_csv('../input/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4eca4d3-297e-b387-7946-969617f6cda0"
      },
      "outputs": [],
      "source": [
        "# Lets get a basic idea of the data.\n",
        "\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "55737896-991b-d25d-2b4f-20199cd83927"
      },
      "source": [
        "If you see the feature 'Class' which is basically the column  which indicates if fraud was done or not, we can see that the mean is ~= 0 (0.001) which indicates that the no of frauds committed are minimal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1e01892-c332-c130-7d99-b6e583b0f52b"
      },
      "outputs": [],
      "source": [
        "data['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "056011de-86b3-9428-bcdf-7c635fa5c7d0"
      },
      "outputs": [],
      "source": [
        "# We can see that the no of frauds is quite low. \n",
        "\n",
        "print('fraud_percentage:', (492/(492 + 284315)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a7a04cc-2176-3ccc-8860-67ff11d3e880"
      },
      "outputs": [],
      "source": [
        "# Guessing 0 would give us an accuracy of\n",
        "\n",
        "print('guessing 0 accuracy:', (284315/(284315 + 492)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45691c63-1e58-b0ef-6cba-6a87ee3ebf82"
      },
      "outputs": [],
      "source": [
        "# Lets look at the correlation of the data with the Class feature\n",
        "\n",
        "sr = data.corr()['Class']\n",
        "sr.drop(['Class'], inplace=True)\n",
        "df = pd.DataFrame({'feature': sr.index, 'correlation': sr.values})\n",
        "sns.barplot(y='correlation', x='feature', data=df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45158e86-f8f2-5a39-28f6-97f93c6223c7"
      },
      "outputs": [],
      "source": [
        "# Lets sort this and make all values positive\n",
        "\n",
        "df['correlation'] = df['correlation'].apply(lambda x: abs(x))\n",
        "df.sort_values(['correlation'], axis=0, ascending=False, inplace=True)\n",
        "sns.barplot(y='correlation', x='feature', data=df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c069bdab-f30e-b6aa-e655-902c95264ae5"
      },
      "outputs": [],
      "source": [
        "# Drop the last few features with really low correlation\n",
        "\n",
        "features_to_drop = df[-5:]['feature'].values\n",
        "data.drop(features_to_drop, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ee886bf-8316-efb2-d965-9a9bf8c15004"
      },
      "outputs": [],
      "source": [
        "# Let's split the data now\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(['Class'], axis=1), data['Class'], test_size=0.3,\n",
        "                                                    random_state=32, stratify=data['Class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb595af8-4877-411a-53e1-20942951167d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = AdaBoostClassifier(n_estimators=400, learning_rate=1.1)\n",
        "clf.fit(X_train, y_train)\n",
        "print(classification_report(y_test, clf.predict(X_test)))\n",
        "\n",
        "array = confusion_matrix(y_test, clf.predict(X_test))\n",
        "df_cm = pd.DataFrame(array, index=['Actually Negative', 'Actually Positive'], columns=['Predicted Negative', 'Predicted Postive'])\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc40b5b0-56ea-ea90-7947-79d31312b189"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(classification_report(y_test, clf.predict(X_test)))\n",
        "\n",
        "array = confusion_matrix(y_test, clf.predict(X_test))\n",
        "df_cm = pd.DataFrame(array, index=['Actually Negative', 'Actually Positive'], columns=['Predicted Negative', 'Predicted Postive'])\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a8036cfd-506a-5372-4e41-ef48c66a36c6"
      },
      "source": [
        "We can see that we're getting fairly decent results using RandomForestClassifier and AdaBoostClassifier.  \n",
        "Although the latter gives a lower f1_score, it's recall is better, which holds more weight here. \n",
        "\n",
        "Todo:  \n",
        "1. Use GridSearch and optimize to get better _recall_ values. "
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}