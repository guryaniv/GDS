{"cells":[{"metadata":{"_uuid":"eb20a4b389b5eab9c26026948a749dc38abf5dc8"},"cell_type":"markdown","source":"# Notebook for implementing Kaggle Machine Learning Education Course"},{"metadata":{"_uuid":"8b0b9ce9fc7f314be90a909d6ceaf158cd6962c0"},"cell_type":"markdown","source":"## ** Level 1 **"},{"metadata":{"_uuid":"50d708cb425c62492ca05f078cfd4a3fdb94e287"},"cell_type":"markdown","source":"## 1. The Data\n### 1.1 Loading dataset"},{"metadata":{"_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","_uuid":"1c728098629e1301643443b1341556a15c089b2b","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nmain_file_path = '../input/train.csv'\ndata = pd.read_csv(main_file_path)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"4daad73e1cec7592bdd52c10b58b74d5e13d0a35"},"cell_type":"markdown","source":"### 1.2 Inspecting the dataset </br>\n### 1.2.1 Using Describe\n#### Describe gives you the the statistical information of the dataset's numerical data. It gives us the information about the count, mean, min, max, standard deviation and data under various quartiles .describe() method needs to be called on the loaded dataframe"},{"metadata":{"_cell_guid":"9ccc553c-e3a7-4f8c-8118-b97711ea974d","_uuid":"011e454870a3c8af934cf823da85300a75127afd","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"cd729ebc37d7e9df855413d4578d21881a6f6516"},"cell_type":"markdown","source":"### 1.2.2 Using columns\n#### Columns helps us in identifying the columns and we can observe that the data shown by the describe method doesn't match the number of columns shown below. it's because, we can find the mean, min, max, etc., for Non-numeric data. So these columns are not shown by the describe method. Unlike the describe shown above, columns isn't a method. \n\n#### As we now know that there are other columns to be checked, let's check the datatypes of these columns using the .info() method"},{"metadata":{"_cell_guid":"69f05a25-e377-4dcb-b4da-9ec87cf29a07","_uuid":"94a398f4fd4920d21c4104d3f6168fa10eb17de5","trusted":true},"cell_type":"code","source":"data.columns","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"29e256483720c6d06107c67793581b99c24e8b3f"},"cell_type":"markdown","source":"### 1.2.3 Using Info\n#### Info is another method which can be used for knowing about the dataset. It shows the number of entries in total and number of entries for each column. This helps us to think that there is a need to fill in these gaps\n\n#### It also shows that there are different types of data to be handled. we can infer something based on the columns name and the datatypes."},{"metadata":{"trusted":true,"_uuid":"fc8e3270a321fa9adb98ffeb3c3f1b4e330525a9"},"cell_type":"code","source":"data.info()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"d302f8874f8f1b9e4e42bbf94c7d79c2c92c86ee"},"cell_type":"markdown","source":"### 1.2.4 Using head\n\n#### Like the Linux command for checking a file, head is present here to check the content of the loaded dataset. Number of lines to be shown can be passed as input parameter. Default value is 5."},{"metadata":{"trusted":true,"_uuid":"e47261303e5114e0988c3db830fd12b4c3063cb9"},"cell_type":"code","source":"data.head()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"f1bce867b9362d5fc77392c72d9537208d68c1a5"},"cell_type":"markdown","source":"### 1.2.5 Using correlation matrix or corr in short\n#### Correlation is one of the statistical method of finding the relation between two variables. The values range from -1 to +1\n#### one being a the highest. positive and negative signs helps us in identifying how tightly or loosely coupled are the variables, Positivie correlation is the change in one variable in one direction will have a change in the other variable. If something increases, the other increases too under positive correlation. Negative correlation is the opposite of it.\n\n#### This is better understood when we draw the graphs betweens these attributes or variables. As it's still level 1, let's skip it for now.\n\n##### Note: Correlation matrix altogether might frighten, but if we take one column and inspect it, it will be very clear. Mostly the target value's column is chosen, here it's the saleprice. "},{"metadata":{"trusted":true,"_uuid":"ef572c2f2a28304d691c631ccf18c05107d89f4e"},"cell_type":"code","source":"corr = data.corr()\ncorr","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6043299015003112561c057b1e06736ed2727ff0"},"cell_type":"code","source":"# Correlation in the descending order. Obviously, the SalePrice will have a positive correlation with it's own and will be 1.\ncorr.SalePrice.sort_values(ascending=False)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"9fd44c9a-cafe-4e9a-8be5-dbdc6ba76249","_uuid":"ee6c958be93443f69a522a3ebf92fc7a1de9a601"},"cell_type":"markdown","source":"## 2. The Model"},{"metadata":{"_uuid":"bdeb1bddedcfa1e1d80a94321cc19a57daf8f559"},"cell_type":"markdown","source":"### 2.1 Selecting columns for training the model\n#### Obviously not all the columns are useful, we can skip some of them or experiment with few and add more based on the results. we can also choose based on the correlation for testing the model.\n\n#### There is a strong need for removing the target attribute from the data before training it. X is for predictor values and y is for target value."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3890026d37cb4c7e6b33beb7a560d476c5839728"},"cell_type":"code","source":"y = data.SalePrice\nprediction_columns = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr','TotRmsAbvGrd']\nX = data[prediction_columns]","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"f955c73d6cb66aabd5418a56674002511dd9b441"},"cell_type":"markdown","source":"### 2.2 Splitting the data to train and test set\n#### Before implementing any model on our dataset, we need to be sure that the model isn't perfectly tuning to the dataset, when it's in perfect tune with the dataset, it will fail for new data. To prevent it, we need to have a test set for evaluating the model after training. This helps in checking the model's performance.****"},{"metadata":{"trusted":true,"_uuid":"701370a1c25fbd65ac3013367c91aa1106055c0a"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=0)","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"636dbdd0-a4b9-4315-a933-975aa0409de6","_uuid":"8f8b487e3319ed2692e369b1f3db75e26f57a54f"},"cell_type":"markdown","source":"### 2.3 DecisionTreeRegressor as the model\n#### we have the train and test sets, we have the columns to be used, then next step is to choose a model. For now, we will use DecisionTreeRegressor as instructed by the Instructor DanB."},{"metadata":{"_cell_guid":"98fe3a94-5675-480a-b590-0ba8f4375726","_uuid":"ad44b9ffb9bccfba65c51c4b02c98f23484c48ec"},"cell_type":"markdown","source":"### 2.3.1 Defining and fitting the model\n#### Cool thing about the SciKit learn is that all the model follow similar methods: \n* #### fit is for fitting the data, we pass the inputs as X and y. it's like training the model.\n* #### predict is for predicting the expected values for a new X.\n* #### tranform is another method, which is used in pipelines. When the data has to go through a procedures before feeding it to the model. transform method is used. Let's ignore this method for now. "},{"metadata":{"_cell_guid":"819f922d-4927-493c-bd25-35b197406704","_uuid":"72913aac1eda49fcaddeee35cac769bd8a4418c2","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndec_tree_reg = DecisionTreeRegressor()\ndec_tree_reg.fit(train_X,train_y)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"5354c1ec-4406-426b-9827-cd33b8283138","_uuid":"2090bc23d7f97434d1add8f0c82c5eae61355f5a"},"cell_type":"markdown","source":"### 2.3.2 Predicting using the model trained\n#### Training the model using fit method won't be of any use unless we put it to work or test. We test the model using the test data we created using train_test_split class of sklearn's model_selection module."},{"metadata":{"_cell_guid":"422cd88a-f5b6-4c63-80da-35f6efb1b186","_uuid":"41156f0441ced61cb08795b7e37f8dc3a89aaeba","trusted":true},"cell_type":"code","source":"prediction_y = dec_tree_reg.predict(val_X)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"f20792fe219b5441d41e8644a02246e1efc799b1"},"cell_type":"markdown","source":"### 2.3.3 Evaluating the model's predictions\n#### As we cannot evaluated all the values predicted, we have use some method to go through all of the results and tell us how it performed. Imagine going through the predictions having 1000 results. Time  taking and prone to human errors."},{"metadata":{"trusted":true,"_uuid":"090010036edf484240578b128fcead99be1a5098"},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint(mean_absolute_error(val_y, prediction_y))","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"50a31107-81cc-490b-ae09-b338efa71e35","_uuid":"19763130541c0c3fb3de0f16f0c3b0a78ed88d00","collapsed":true},"cell_type":"markdown","source":"### 2.3.4 Exploring options available for model's performance improvement\n#### DecisionTreeRegressor has input parameter of max_leaf_nodes, which will tell the model how deep the tree can be, we are exploring that option by passing different number of max_leaf_nodes"},{"metadata":{"_cell_guid":"634b339e-6cfd-4345-865b-5ef4fa426549","_uuid":"f33727b32cb02dafc7e7a241df26a7cb41b9ed6a","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"e4568fab-fcf1-4513-a977-bd4cae252a09","_uuid":"b99c1383a533b9324082d546f07a1293b01a445e","trusted":true},"cell_type":"code","source":"for max_leaf_nodes in [5,50,500,5000]:\n    mae_res = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, mae_res))","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"cbd833cfc28553dbd29ac6e7dadb5affe688b071"},"cell_type":"markdown","source":"### 2.4 Trying out other models. \n#### On an online shopping site, It's always a best idea to check a deal's authenticity, compare price before purchasing it. Similarly experimenting helps us in finding the model's performance and scope of improvement. Here we are using RandomForestRegressor, I think it's kind of a group of Decision Trees atleast based on both the names, we can infer that. "},{"metadata":{"_cell_guid":"5203e406-d8cc-4443-986f-bb4d0c9463dc","_uuid":"30b2448d658fc2f1a0fbb67021582eede877e7e8","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nrand_forest_reg = RandomForestRegressor()\nrand_forest_reg.fit(train_X, train_y)\npred_y = rand_forest_reg.predict(val_X)\nprint(\"Mean Absolute Error using the RandomForestRegressor is: %d\", mean_absolute_error(val_y, pred_y))","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"3227021d-148b-4ee7-ad18-f1508ca15332","_uuid":"f9ef40e96fbdabfd5961075858751a38f0196044","collapsed":true},"cell_type":"markdown","source":"## 3. The Submission"},{"metadata":{"_uuid":"3a61c92421e732d514aa3ceed2546050da1e877a"},"cell_type":"markdown","source":"### 3.1 Navigating the files - The Linux Way"},{"metadata":{"_cell_guid":"d76f1c34-9d17-4455-8020-26495d2ae78f","_uuid":"4d5826c8c6cc39cffe6048168d27f431097076f6","collapsed":true,"trusted":true},"cell_type":"code","source":"ll ../input/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d618217310fca9d71be49bf27e93d8ae80314f0a"},"cell_type":"markdown","source":"### 3.2 The whole thing again.\n#### We are doing the same thing again, imports, reading the data, training, but this time we use the train and test data provided and doesn't have to split the data."},{"metadata":{"_cell_guid":"9240a83b-f8a7-4afc-90ec-183c08d63190","_uuid":"0565b95146caaa2aa1fbec5917676e3ea345b6a6","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\n#reading the train.csv and test.csv file\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\n#fetching the target value and predictors value based on columns for training data\npredictor_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']\ntrain_X = train_data[predictor_cols]\ntrain_y = train_data.SalePrice\n\n#fetching the predictors value based on columns for test data. will be the same columns as training data\ntest_X = test_data[predictor_cols]\n\n#defining the RandomForestRegressor\nrand_forest_reg = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9218abe4-519e-44d3-a727-fa776f995269","_uuid":"4dde3f43cf43249d009f0bbe6584b4fb819fdac7","collapsed":true,"trusted":true},"cell_type":"code","source":"#fitting the training predictors and target to the model\nrand_forest_reg.fit(train_X, train_y)\n\n#predicting using the test data predcitors\npredicted_prices = rand_forest_reg.predict(test_X)\nprint(predicted_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd26f1817828a5dac55ec2953ba0e5f26c90c13"},"cell_type":"markdown","source":"### 3.3 The Final Call\n#### We will be creating a dataframe using pandas, as the number of predicted values should match the test data provided. We are passing the id field of the test data and our prediction's results to the dataframe constructor\n\n#### to_csv is the Pandas way of writing to csv. Ideally,  it should have been write_csv contradicting the read_csv method used during section 1.1 but it's to_csv and we should stick with it."},{"metadata":{"_cell_guid":"e3d92984-3124-48e9-8415-fb275d46cfdd","_kg_hide-output":true,"_uuid":"47134a286a33b0013928757791c2c3b618740152","collapsed":true,"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26dc013a98d085933f7d2aac27c257e59891bb3"},"cell_type":"markdown","source":"## **Level 1 - The End**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5bc101d4975c5221658edf38db378a15b180fde3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}