{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#importing required libraries.....\n\nimport os \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re # regular expression\nimport matplotlib.pyplot as plt\nimport seaborn\nimport nltk\nfrom nltk import PorterStemmer # natural language toolkit\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier \nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2010d2d8ccb2a67715b8a9a0fd6d93ba04160b5"},"cell_type":"code","source":"os.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e9b6bfe9fb8a575952e19a8c17b0d064598847f"},"cell_type":"code","source":"#read the data \ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\nsample=pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f2759d607dacac36b5b0de0c41f94f15763eaac"},"cell_type":"code","source":"#printing some upper rows of our training data\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e6072ace4cf591cd5f25fb4bd54f85366f18d1d"},"cell_type":"code","source":"#some information about data\nprint('No. of training examples : ',len(train))\nprint(\"No. of test data : \",len(test))\nprint(train.columns[2:]) #columns_name\nrow=train.iloc[:,2:].sum(axis=1)\nprint(\"No. of examples with no labels : \",(row==0).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca1be5aa5bf35c26c6a602b87f60f75e11c4d2ab"},"cell_type":"code","source":"#fill blank values with unknown otherwise model gives problem\nprint(\"Check for missing values in Train dataset\")\nnull_check=train.isnull().sum()\nprint(null_check)\nprint(\"Check for missing values in Test dataset\")\nnull_check=test.isnull().sum()\nprint(null_check)\nprint(\"filling NA with \\\"unknown\\\"\")\ntrain[\"comment_text\"].fillna(\"unknown\", inplace=True)\ntest[\"comment_text\"].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8cc99c2ee2bcd662248663faac48a633a68676","scrolled":true},"cell_type":"code","source":"#plot\nx=train.iloc[:,2:].sum()\nplt.figure(figsize=(8,4))\nax= seaborn.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd3f4988cb257c6314df024a2a3024f9b84d5baf"},"cell_type":"code","source":"# creating train-validation split\nx_train, x_val, y_train, y_val = train_test_split(train.comment_text, train.iloc[:,2:8], test_size=0.3, random_state=19)\nx_test = test.comment_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5561dffcda357ddc8300a29c88e03a7872129308"},"cell_type":"code","source":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)\n    \n    words = [w for w in words if not w in stopwords.words('english')]\n    words=[lem.lemmatize(word, \"v\") for word in words]\n    \n    clean_sent=\" \".join(words)\n    return(clean_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4adf293ff9a256e65631d8778cdd504ce1861e8"},"cell_type":"code","source":"# preparing training text to pass in count vectorizer\ncorpus=[]\nfor text in x_train:\n    text = clean(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"944db1ae7e863069338486a96bfb2c2552dc56cd"},"cell_type":"code","source":"# build Count Vectorizer, to convert a collection of text documents to a matrix of token counts\ncount_vect = CountVectorizer(ngram_range=(1,2))\nX_train_counts = count_vect.fit_transform(corpus)\n\n# build TFIDF Transformer, to transform a count matrix to a normalized tf or tf-idf representation\n# tfidf - term frequency inverse document frequency\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dff76cfacd192362086c8c043df4c185ee4c34e"},"cell_type":"code","source":"# preparing validation text to pass in count vectorizer\nX_val_set = []\nfor text in x_val:\n    text = clean(text)\n    X_val_set.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f51881d5733e6fd50a61d7c9f9258408222346ff"},"cell_type":"code","source":"# tranforming validation data using count vectorizer followed by tfidf transformer\nX_val_counts = count_vect.transform(X_val_set)\nX_val_tfidf = tfidf_transformer.transform(X_val_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00feb9cdc2a5e9c67a4cc894f041e76a97f8a47c"},"cell_type":"code","source":"# preparing test text to pass in count vectorizer\nX_test_set = []\nfor text in x_test:\n    text=clean(text)\n    X_test_set.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b91897ed88926e6feeac735e5329c9f916812244"},"cell_type":"code","source":"# tranforming validation data using count vectorizer followed by tfidf transformer\nX_test_counts = count_vect.transform(X_test_set)\nX_test_tfidf = tfidf_transformer.transform(X_test_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"302acc808a042e7fb8bd6524b53564593c2c5279"},"cell_type":"code","source":"# creating dictionary to store prediction results\nresult_test = dict()\nresult_val =  dict()\nresult_train =  dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9da7add340b0f5219c095b731f6b02c1de62b2d3"},"cell_type":"code","source":"#Applying Model\n# Multinomial Naive Bayes Model\nMNB_classifier = OneVsRestClassifier(MultinomialNB())\nMNB_classifier.fit(X_train_tfidf, y_train)\ny_pred_train=MNB_classifier.predict(X_train_tfidf)\nresult_train['Multinomial_NB'] = y_pred_train\nprint (\"Accurary of Multinomial Naive Bayes Classifier on Training Data:\",accuracy_score(y_pred_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50f9ae93273711cf55babfae3cec7100ba0de4a"},"cell_type":"code","source":"# Bernoulli Naive Bayes Model\nBNB_model = OneVsRestClassifier(BernoulliNB())\nBNB_model.fit(X_train_tfidf, y_train)\ny_pred_train=BNB_model.predict(X_train_tfidf)\nresult_train['Bernoulli_NB'] = y_pred_train\nprint('Accurary of Bernoulli Naive Bayes Classifier on Training Data:',accuracy_score(y_pred_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f193b1fa96ad3f24c34975c52599af222085dc7f"},"cell_type":"code","source":"#Ridge Classifier Model\nridge_model = OneVsRestClassifier(RidgeClassifier(normalize=True))\nridge_model.fit(X_train_tfidf, y_train)\ny_pred_train=ridge_model.predict(X_train_tfidf)\nresult_train['Ridge_Classifier'] = y_pred_train\nprint('Accurary of Ridge Classifier on Training Data:',accuracy_score(y_pred_train,y_train))                          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c13c29fd4b04bca4ca1dc2df3229ff4a52416d5b"},"cell_type":"code","source":"# Logistic Regression Model\nlog_model = OneVsRestClassifier(LogisticRegression(multi_class='ovr'))\nlog_model.fit(X_train_tfidf, y_train)\ny_pred_train=log_model.predict(X_train_tfidf)\nresult_train['Logistic_Regression'] = y_pred_train\nprint('Accurary of Logistic Regression on Training Data:',accuracy_score(y_pred_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e4d5fbe63c64f3ab668235f61611001a2d2111","scrolled":true},"cell_type":"code","source":"# SVM Classifier Model\nsvm_model = OneVsRestClassifier(LinearSVC(multi_class='ovr'))\nsvm_model.fit(X_train_tfidf, y_train)\ny_pred_train=svm_model.predict(X_train_tfidf)\nresult_train['SVM'] = y_pred_train\nprint('Accurary of SVM Classifier on Training Data:',accuracy_score(y_pred_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"345bde1273d7b6aafc1febdf53a93abfd9077364"},"cell_type":"code","source":"#visualizations\n\nD=result_train[:]\nplt.figure(figsize=(20, 7))\nplt.yticks( fontsize=20)\nplt.xticks(range(len(D)), list(D.keys()), fontsize=20)\nax=plt.bar(range(len(D)), list(D.values()), align='center',width=0.8)\nplt.title(\"# Accuracy Score on Training Set by different Models\\n\", fontsize=40)\nplt.ylabel('# Accuracy Range', fontsize=30)\nplt.xlabel('\\n#Model type ', fontsize=30)\n#adding the text labels\nfor rect in ax:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()/2.0, height, '%f' % float(height), ha='center', va='bottom',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e879fa932bde1d33b907bcf34e6951f30e487f9"},"cell_type":"code","source":"#Picking the model with highest accuracy rate on train data and now do hyperparameter tuning on cross-validation set\n#Hyperparameter Tuning\ngrid_values = {'estimator__C': [0.3, 1.0, 30.0]}\nsvm_grid = GridSearchCV(svm_model, param_grid = grid_values, scoring = 'roc_auc')\nsvm_grid.fit(X_train_tfidf, y_train)\nprint('Accurary of SVM Classifier on Training Data: {:.3f}' .format(svm_grid.score(X_train_tfidf, y_train)))\nprint('Accurary of SVM Classifier on Validation Data: {:.3f}' .format(svm_grid.score(X_val_tfidf, y_val)))\nprint('Grid best parameter (max. accuracy): ', svm_grid.best_params_)\nprint('Grid best score (accuracy): ', svm_grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35d2f8423d7fb98e209ad20f69aa20bf2d63e779"},"cell_type":"code","source":"\n#predict for the test data\nresult_test['SVM']=svm_grid.predict(X_test_tfidf)\n# storing results of SVM Classifier as our result\ny_test = result_test['SVM']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da116bf9fa5a8072c4e8b27c8ac34bb35b6912c2"},"cell_type":"code","source":"# combining final results with the original test data set\noutput = pd.DataFrame(y_test, columns = train.columns[2:8], index = test.index)\noutput = pd.concat([test, output], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"573d8fe90dcee0681832433a229315c51f55e791"},"cell_type":"code","source":"#Sample Submission\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"643eb96cc7080a042368ce6631ec43b7cc78a446"},"cell_type":"code","source":"# verifing data\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e972dbc6ca5f008c249559e4056160a8aee90ef0"},"cell_type":"code","source":"# verifing select random case, as per index from above code chunk\noutput.comment_text[5902]\noutput.iloc[5902,:]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"29acaac446163814c367f4a04a03047bc8a8b54b"},"cell_type":"code","source":"# quick summary for training, validation and test set respectively\ny_train.sum(axis=0)\ny_val.sum(axis=0)\noutput.iloc[:,2:8].sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb9e81ab67ea3bef39514eb9dd2b3d0e62038957"},"cell_type":"code","source":"#Final Submission\nmy_submission = output.drop(['comment_text'], axis = 1, inplace = False)\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e517c0c62e558b7d680236ae494caf2f2604c8f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}