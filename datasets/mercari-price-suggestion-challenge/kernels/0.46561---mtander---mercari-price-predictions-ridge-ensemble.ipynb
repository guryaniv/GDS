{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["Relied heavily on all of the kernels below. Thank you all for sharing your work!\n", "\n", "Credits: \n", "https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl  <br>\n", "https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling  <Br>\n", "https://www.kaggle.com/rakeshbhat9/mercari-simple-data-exploration  <br>\n", "https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55 <br> \n", "https://www.kaggle.com/lopuhin/eli5-for-mercari <br>\n", "https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44341-2 <br>\n", "https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/ "]}, {"metadata": {"_uuid": "6f32c006d0236da808159c2c662eab14047d4584", "_cell_guid": "5839037b-7eb6-4ebc-9831-0def88d72056"}, "execution_count": null, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "\n", "from scipy.sparse import csr_matrix, hstack\n", "\n", "import time\n", "import re\n", "import math\n", "\n", "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler,LabelBinarizer\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.metrics import mean_squared_log_error\n", "from sklearn.linear_model import Ridge\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "\n", "import xgboost as xgb\n", "\n", "seed = 90"], "outputs": []}, {"metadata": {"_uuid": "475c7ce2081132a6fd884665be2c9084f5f621fb", "collapsed": true, "_cell_guid": "1b566e59-6062-40b4-938e-98d5e5e8969f"}, "execution_count": null, "cell_type": "code", "source": ["Time_0 = time.time()\n", "train = pd.read_csv('../input/train.tsv',sep='\\t')\n", "test = pd.read_csv('../input/test.tsv',sep='\\t')"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["#Get log price\n", "y_train = train['log_price'] = np.log((train['price'] + 1))"], "outputs": []}, {"metadata": {"_uuid": "b170004bcff1d02b65791ec3bc4ed103b665772c", "_cell_guid": "1671206f-f122-47ed-aa6d-f1d5f1f6aa02"}, "execution_count": null, "cell_type": "code", "source": ["#Data prep functions\n", "def handle_missing(dataset):\n", "    dataset['category_name'].fillna(value=\"NA/NA/NA\", inplace=True)\n", "    dataset['brand_name'].fillna(value=\"missing\", inplace=True)\n", "    dataset['item_description'].fillna(value=\"missing\", inplace=True)\n", "    return (dataset)\n", "\n", "def split_cat(dataset):\n", "    dataset['cat1'], dataset['cat2'], dataset['cat3'] =  zip(*dataset['category_name'].str.split(\"/\",2))\n", "    return dataset\n", "\n", "def label_maker(dataset):\n", "    \n", "    lb = LabelBinarizer(sparse_output=True)\n", "    \n", "    cat1 = lb.fit_transform(dataset['cat1'])\n", "    cat2 = lb.fit_transform(dataset['cat2'])\n", "    cat3 = lb.fit_transform(dataset['cat3'])\n", "    brand_name = lb.fit_transform(dataset['brand_name'])\n", "    \n", "    del lb\n", "    \n", "    return cat1,cat2,cat3,brand_name\n", "\n", "def get_dums(dataset):\n", "    X_dummies = csr_matrix(pd.get_dummies(dataset[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "    \n", "    return X_dummies\n", "\n", "def text_processing(dataset):\n", "    MIN_DF_COUNT = 10\n", "    MAX_DF_COUNT = 10000\n", "    cv = CountVectorizer(min_df = MIN_DF_COUNT, max_df = MAX_DF_COUNT)\n", "    name = cv.fit_transform(dataset['name'])\n", "    \n", "    MIN_DF_TF = 10\n", "    MAX_DF_TF = 51000\n", "    MAX_FEATURES_TF = 51000\n", "    \n", "    tv = TfidfVectorizer(max_features=MAX_FEATURES_TF,\n", "                         min_df = MIN_DF_TF,\n", "                         max_df = MAX_DF_TF,\n", "                         ngram_range=(1, 3),\n", "                         stop_words='english')\n", "    description = tv.fit_transform(dataset['item_description'])\n", "    \n", "    del cv, tv\n", "    \n", "    return name, description\n", "\n"], "outputs": []}, {"metadata": {"_uuid": "058a2376502c1f3ca7ca3dcc155a95575c7b0714", "_cell_guid": "3b5e958d-c6b9-4180-88b1-16c9094ed15c"}, "execution_count": null, "cell_type": "code", "source": ["#Merge dataset\n", "nrow_train = train.shape[0]\n", "merge: pd.DataFrame = pd.concat([train, test])\n", "submission: pd.DataFrame = test[['test_id']]\n", "    \n", "del train\n", "del test"], "outputs": []}, {"metadata": {"_uuid": "5829d27cf4bf37640ed5572e8f3f00387c9659e3", "_cell_guid": "0be6367a-4131-41d8-a732-1161ea83c9ec"}, "execution_count": null, "cell_type": "code", "source": ["#Preparing training data\n", "# Time ~ 9 mins\n", "start_time = time.time()\n", "\n", "print(\"Handle Missing...\")\n", "merge = handle_missing(merge)\n", "\n", "print(\"splitting cat...\")\n", "merge = split_cat(merge)\n", "\n", "print(\"making labels...\")\n", "cat1,cat2,cat3,brand_name = label_maker(merge)\n", "\n", "print(\"getting dummies...\")\n", "X_dummies = get_dums(merge)\n", "\n", "print(\"processing text...\")\n", "name,description = text_processing(merge)\n", "\n", "print(\"stacking train...\")\n", "sparse_merge = hstack((cat1,cat3,cat3,brand_name,X_dummies,name,description)).tocsr()\n", "\n", "print(\"TIME:\", time.time() - start_time)"], "outputs": []}, {"metadata": {"_uuid": "9a6fa92fdc03510efb0b41b1b4419cf2c9a5e035", "collapsed": true, "_cell_guid": "1b24f7aa-4514-40c1-af48-9d92085433a2"}, "execution_count": null, "cell_type": "code", "source": ["#Split data\n", "X_train = sparse_merge[:nrow_train]\n", "X_test = sparse_merge[nrow_train:]"], "outputs": []}, {"metadata": {"_uuid": "a0fa99633411503c71869302aaa4cc63c27d6a14", "_cell_guid": "6053c3f1-05b2-4ef6-b3eb-beecdca16b7a"}, "execution_count": null, "cell_type": "code", "source": ["#Model building functions\n", "def model_testing(model,X_test, y_test):\n", "    y_pred = model.predict(X_test)\n", "    error = rmsle(y_test, y_pred)\n", "    print(error)\n", "    \n", "\n", "def rmsle(y, y0):\n", "    assert len(y) == len(y0)\n", "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n", "\n"], "outputs": []}, {"metadata": {"_uuid": "5a83f724cc238478aa971f8cfa311160a0e36aa7", "_cell_guid": "e761c163-7525-4075-83b7-6918d032e77c"}, "execution_count": null, "cell_type": "code", "source": ["#Initiating models\n", "ridge_model_1 = Ridge(alpha=5.0, fit_intercept=True, normalize=False, copy_X=True, \n", "                    max_iter=None, tol=0.001, solver='auto', random_state=None)\n", "ridge_model_2 = Ridge(alpha=5.0, fit_intercept=True, normalize=False, copy_X=True, \n", "                    max_iter=None, tol=0.001, solver='sag', random_state=None)\n", "ridge_model_3 = Ridge(alpha=5.0, fit_intercept=True, normalize=False, copy_X=True, \n", "                    max_iter=None, tol=0.001, solver='lsqr', random_state=None)\n", "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 5, \n", "                                 learning_rate = 0.9,subsample=0.9)"], "outputs": []}, {"metadata": {"_uuid": "5ca0121e6481e4939500f3e83a5e051850284839", "_cell_guid": "5153889f-2a94-4d5c-958f-ae8c993460b0"}, "execution_count": null, "cell_type": "code", "source": ["#Model execution\n", "#Time ~ 6mins\n", "start_time = time.time()\n", "\n", "print(\"train test splitting...\")\n", "X_t, X_v, y_t, y_v = train_test_split(X_train, y_train,test_size = 0.20)\n", "\n", "print(\"training model...\")\n", "print(\"1\")\n", "ridge_model_1.fit(X_train, y_train)\n", "model_testing(ridge_model_1, X_test = X_v, y_test = y_v)\n", "#Current best: .1233\n", "\n", "print(\"training model...\")\n", "print(\"2\")\n", "#ridge_model_2.fit(X_t, y_t)\n", "#model_testing(ridge_model_2, X_test = X_v, y_test = y_v)\n", "\n", "print(\"training model...\")\n", "print(\"3\")\n", "#ridge_model_3.fit(X_t, y_t)\n", "#model_testing(ridge_model_3, X_test = X_v, y_test = y_v)\n", "\n", "print(\"TIME:\", time.time() - start_time)"], "outputs": []}, {"metadata": {"_uuid": "83c45021279e9af237ae998847d3a67718c12039", "collapsed": true, "_cell_guid": "1832e70e-70c0-4f9f-9816-276054a38044"}, "execution_count": null, "cell_type": "code", "source": ["#Submission functions\n", "def create_submission(model,test = X_test, submission=submission,path=\"./predictions.csv\"):\n", "    predictions = model.predict(test)\n", "    predictions = pd.Series(np.exp(predictions) - 1)\n", "    \n", "    submission['price'] = predictions\n", "    \n", "    submission.to_csv(path, index=False)\n", "    \n", "    print(submission.describe())"], "outputs": []}, {"metadata": {"_uuid": "ef367b66276bd25059be3373a1c51768d1d7805b", "_cell_guid": "1cc932cb-7aa5-492e-a3f0-6ebd81934137"}, "execution_count": null, "cell_type": "code", "source": ["#Generating submission\n", "#Time ~ 15 secs\n", "start_time = time.time()\n", "\n", "create_submission(ridge_model_1)\n", "\n", "print(\"TIME:\", time.time() - start_time)\n", "print(\"TOTAL TIME:\", time.time() - Time_0)\n", "#Total Time ~ "], "outputs": []}, {"metadata": {"_uuid": "e23905dcd843a509ca1f8f78e40e96df74fa801b", "collapsed": true, "_cell_guid": "ed460728-0461-46fa-95b2-1797dacd9252"}, "execution_count": null, "cell_type": "code", "source": [], "outputs": []}]}