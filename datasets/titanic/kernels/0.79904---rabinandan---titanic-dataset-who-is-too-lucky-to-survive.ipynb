{"cells":[{"metadata":{"_cell_guid":"3882341a-f742-4b69-ac66-f984925ceb79","_uuid":"9539d57981812ea48ad7a73cbe92a0349d9a1ea1","collapsed":true,"trusted":true},"cell_type":"code","source":"\n####IMPORT PACKAGES ####################################\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08d19a60-4380-4862-ba2a-a9ace5183f99","_uuid":"aaafbd8c63646c1eabf9314ef57ff53a10f6692c","collapsed":true,"trusted":true},"cell_type":"code","source":"###LOAD DATA####################\ndata_train = pd.read_csv('../input/train.csv')\ndata_test  = pd.read_csv('../input/test.csv')\ndataset = [data_train, data_test]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9d74cc1-4ab4-4cc5-bade-aee9537c6811","_uuid":"6984e543cacb80532c980004ee8c037b2115cf06","scrolled":true,"trusted":true},"cell_type":"code","source":"####DESCRIBE DATA AND ANALYSE\nprint(data_train.describe()) ##HELP\nprint(data_train.info())\nprint(data_train.sample())\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19a6d401-7987-4112-9395-bc8e5d212523","_uuid":"4c63055090842cac169083f13217fa1605e4a396"},"cell_type":"markdown","source":"**##LET'S Start looking into each feature #####**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d01d09fc-c3ec-4e9d-8ac9-25bf65975b2f","_uuid":"975516501ece5fb3bb541f450b71f3f91fac1245","trusted":true},"cell_type":"code","source":"#PassengerId does never contribute into servival... hence simply drop it\nfor data in dataset:\n    data.drop(['PassengerId'],axis=1, inplace= True)\n#data_test.drop(['PassengerId'],axis=1)\nprint(data_test.describe())\n   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f0ce1a8-becd-480f-81e8-c0262dcab3bc","_uuid":"c7c4453640ef0ffed04753b401abc628a72507c7","collapsed":true,"trusted":true},"cell_type":"code","source":"####Survive column is output/target and not contain any null entries, looks good\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bd4e3f1-ca3a-44ac-af2e-0279560c2142","_uuid":"6911469d2511f2a9cdb036e42d20e37cd54ceed2","collapsed":true,"trusted":true},"cell_type":"code","source":"##Pclass- is passenger class 1st class passentger got higher precidence of servival respective to 3rd one. \n##Just remain as it is -(HELP)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eab33352-5f55-43f3-a9eb-1d32faff54ae","_uuid":"2c7d5e5d209e09c9c77b49e95f17b082a250fdbc","collapsed":true,"trusted":true},"cell_type":"code","source":"##NAME does not any null value, we simply extract title from this column\nfor data in dataset:\n    data['Title'] = data['Name'].str.split(\",\",expand=True)[1].str.split(\".\",expand=True)[0]\n#data_train['Title'].unique()\ndata_train['Title'].value_counts()\n##So, Just create Miss title and put all other titles except whose count >10\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in \n#statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\nfor data in dataset:\n    title_names = (data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    data['Title'] = data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n#print(data_train['Title'].unique())\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0b15038-78f6-425b-878e-1dbc8fb2dd1a","_uuid":"c6642c60f7cd4a1bbd80ee3e7f522c8d48fa299e","collapsed":true,"trusted":true},"cell_type":"code","source":"##Now Remove NAME \nfor data in dataset:\n    data.drop(['Name'],axis=1, inplace=True)\n#data_train.head()    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76fbaf37-4c77-4c3a-925e-453191d0a629","_uuid":"f8c813f65fe38208312fb5b5e452dfa0cf767b63","trusted":true},"cell_type":"code","source":"##NOW code categorical data\n#code categorical Title column and remove Title column\nlabel = LabelEncoder()\nfor data in dataset: \n    data['Title_Code'] = label.fit_transform(data['Title'])\n    data.drop(['Title'], axis=1, inplace=True )\ndata_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce787c03-abe6-4439-a62a-12c268193377","_uuid":"fedb3615611a9befc7518b1477596b2a2732169b","trusted":true},"cell_type":"code","source":"###Sex column is categorical text, just code it similar to Title code\nfor data in dataset:\n    data[\"Sex_Code\"] = label.fit_transform(data['Sex'])\n    data.drop(['Sex'], axis=1, inplace=True)\ndata_train.head()    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"372ca8ed-5c27-4e87-9d18-89f95a24a0d0","_uuid":"075c81d0f6f9683ef51d70d0e6ae2da528c48104","collapsed":true,"trusted":true},"cell_type":"code","source":"##Now, Age column : Hands on - has missing data as well as continuons data\n## A basic approach to fill missing continuous data is to median \nfor data in dataset:\n    data['Age'].fillna(data['Age'].median(), inplace = True)\ndata['Age'].isnull().sum()\n##Lets convert continuous age into bin and then categorize it using label encoder(HELP)\nfor data in dataset:\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    data['AgeBin'] = pd.cut(data['Age'].astype(int), 5)\n    data['AgeBin_Code'] = label.fit_transform(data['AgeBin'])\n    data.drop(['Age'], axis=1, inplace=True)\n    data.drop(['AgeBin'], axis=1, inplace=True)\n    data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46bccaf4-a205-4ce0-a4ef-a5aa476f1c48","_uuid":"7ef106ccc558c37b50de0f63f5a932d3f8bef011","trusted":true},"cell_type":"code","source":"##Here no. of sibling and parents sibsp and parch column makes family_size and IsAlone\nfor data in dataset:\n    #Discrete variables\n    data['FamilySize'] = data ['SibSp'] + data['Parch'] + 1\n    data['IsAlone'] = 1 #initialize to yes/1 is alone\n    data.loc[data['FamilySize'] > 1, 'IsAlone'] = 0\n    data.drop(['SibSp'], axis=1, inplace=True)\n    data.drop(['Parch'], axis=1, inplace=True)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5281d3c8-ea02-426d-8a43-92eb051c409d","_uuid":"c98af53c8aacd9b74d06ad17fa06cb69b1cf510b","trusted":true},"cell_type":"code","source":"##Ticket is almost not contributing in survival, you may consider, i am dopping for now\nfor data in dataset:\n    data.drop(['Ticket'], axis=1, inplace=True)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7feb136c-a86d-4cfb-9973-0592d3817a5a","_uuid":"e3c13f5f1ad7457352b7918718a8236dbc4f6796","trusted":true},"cell_type":"code","source":"##Fare may contribute to survivavl, one paying more got nearby lifebot cabin and other facility\n##For fare create bin and encode as categorical------------\n##Just a quick fix data_test conains 1 missing entry, fill using median\nfor data in dataset:\n    data['Fare'].fillna(data['Fare'].median(), inplace=True)\n#Now for rest part of fare\nfor data in dataset:\n    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    data['FareBin'] = pd.qcut(data['Fare'], 4)\n    #print(data['Fare'].isnull().sum())\n    data['FareBin_Code'] = label.fit_transform(data['FareBin'])\n    data.drop(['Fare'], axis=1, inplace=True)\n    data.drop(['FareBin'], axis=1, inplace=True)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0c362c5-f403-4b02-b84c-5c7c313d8606","_uuid":"99cca8598b4d3d975faf30a9939d163ffef735b5","trusted":true},"cell_type":"code","source":"##ofcourse cabin contribute into survival, however 78% of total is null entries in this data, so good\n## is to drop it and somehow fare may help in place of it\nfor data in dataset:\n    data.drop(['Cabin'], axis=1, inplace=True)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f357eb67-9d9d-4b6c-b448-512cda0a56a6","_uuid":"3926c3c53a20c29536000f160cf2638e459ad92e","scrolled":false,"trusted":true},"cell_type":"code","source":"##For Embarked- it is categorical char value treat as Sex\n##Before it just only 2 missing entries (among all 891 entries) is filled with mode\nfor data in dataset:\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n    \nfor data in dataset:\n    data['Embarked_Code'] = label.fit_transform(data['Embarked'])\n    data.drop(['Embarked'], axis=1, inplace=True)\ndata_train.head(10)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"82b4fef8-7631-45cc-8645-0dc2acb048e7","_uuid":"06ce1b026563452eae1218263bd65c9769d97999"},"cell_type":"markdown","source":"**My Cleaned dataset looks like ****","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ec29afed-b9d8-4215-a927-c118979c016f","_uuid":"5845f45462a85527ae3da719601772e6bc34293c","trusted":true},"cell_type":"code","source":"for data in dataset:\n    print(data.describe())\n    print(\"--\"*10)\n    print(data.head(8))\n    print(\"--\"*10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a624e09-b8e2-4133-9633-95e4dc78c957","_uuid":"0b5de4786102c88524fec5cec08880c11bc683cc","trusted":true},"cell_type":"code","source":"#Quick check for corelation (HELP)\nfrom scipy.stats.stats import pearsonr\n\nprint(pearsonr(data_train[['FamilySize']],data_train[['IsAlone']] ))\nprint(pearsonr(data_test[['FamilySize']],data_test[['IsAlone']] ))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a1f0a3a-9c73-4ef6-a1ab-4c6c707a2587","_uuid":"be8792f0326a79f2e3e0740d21542cdcc9d101a0","collapsed":true,"trusted":true},"cell_type":"code","source":"# separating our independent and dependent variable\nX = data_train.drop(['Survived'], axis=1)\nY = data_train[\"Survived\"]\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"284e98bf-fbe4-456d-b0cd-757c14119705","_uuid":"c5583fc395c1541fa54718757d5f194cdf50dcc8","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = .33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07387680-8886-470b-b064-ecb1a82ad8e7","_uuid":"8b6c5eb0d9cf7624d7cf9f2221a6ba601b78f6d0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdce2932-c7ce-477a-8a0d-b008dcc305ef","_uuid":"61649152187e55661c4f8e31361fe50a8cc5042f","collapsed":true,"trusted":true},"cell_type":"code","source":"data_test_scaled = sc.transform(data_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f817119-60d6-402f-a9f1-996ea187af04","_uuid":"4acbee110f3102171d4d070567a6ae580be82818","trusted":true},"cell_type":"code","source":"#Logistic regression \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix\nlogreg = LogisticRegression(solver='liblinear', penalty='l1')\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nlogreg_accy = round(accuracy_score(y_pred,y_test), 3)\nprint (logreg_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ba36da3-fee5-4328-b09d-6ce6d925f044","_uuid":"6a9281b92af1efcb1a1a3e8d9e172aa75d331772","trusted":true},"cell_type":"code","source":"print (classification_report(y_test, y_pred, labels=logreg.classes_))\nprint (confusion_matrix(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a8aeb3e-a197-42c5-9397-e48cd07c67c8","_uuid":"25a9230ee8b4c7fd2850bdc1bd2c498cf67f5358","trusted":true},"cell_type":"code","source":"#KNN approach\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(weights=\"distance\", n_neighbors=45,  metric='minkowski', p =2 )\n#n_neighbors: specifies how many neighbors will vote on the class\n#weights: uniform weights indicate that all neighbors have the same weight while \"distance\" indicates\n        # that points closest to the \n#metric and p: when distance is minkowski (the default) and p == 2 (the default), this is equivalent to the euclidean distance metric\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nknn_accy = round(accuracy_score(y_test, y_pred), 3)\nprint (knn_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef582994-af3f-4f8e-b489-974d484c9602","_uuid":"eb902b5bd6035efa6172621f55cc0cd468c8f854","trusted":true},"cell_type":"code","source":"#Naive Bayes\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d29a569c-a1b2-4e3c-9e6d-3a2372e04233","_uuid":"6c00604c0e51cf13bf60517b45033d2468b913c2","scrolled":true,"trusted":true},"cell_type":"code","source":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC(probability=True)\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nsvc_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(svc_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"665e8a5b-7809-49ee-b142-f74fbec5c8f9","_uuid":"2289ac95d2754dfe82a61b6294b2a52d13752eb2","trusted":true},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree = DecisionTreeClassifier( max_depth=5, \n                                class_weight = 'balanced',\n                                min_weight_fraction_leaf = 0.01)\ndectree.fit(x_train, y_train)\ny_pred = dectree.predict(x_test)\ndectree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(dectree_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f83115d-2c2a-4e6f-ab6e-17b4dab0194e","_uuid":"1c4b033426d5bdbb8458e54310fcf7e91f35ac8d","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100,max_depth=9,min_samples_split=6, min_samples_leaf=4)\n#randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nrandom_accy = round(accuracy_score(y_pred, y_test), 3)\nprint (random_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b07e8b25-58e3-4f05-8704-8ddfd209df5d","_uuid":"80f33164e454f5de5571673b1edbcd28943735c9","trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient = GradientBoostingClassifier()\ngradient.fit(x_train, y_train)\ny_pred = gradient.predict(x_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe70894d-b88d-4473-a491-cd4942dd0941","_uuid":"2671ec5ad14ac39fd80e79d2f697519801af220f"},"cell_type":"markdown","source":"**#Submit Result**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e7cbaae0-8c46-4257-8405-89593e33188e","_uuid":"dd1eb23af8a87113ea11bca814a310fe6ce9b151","collapsed":true,"trusted":true},"cell_type":"code","source":"##Here GradientBoostingClassifer has maximum accuracy.. go with it\n#PassengerId1 = data_test['PassengerId']\ntest_result = gradient.predict(data_test_scaled)\ndata_test  = pd.read_csv('../input/test.csv')\ntest_df = pd.DataFrame(columns = ['PassengerId', 'Survived'])\ntest_df['PassengerId'] = data_test['PassengerId']\ntest_df['Survived'] = test_result\ntest_df.to_csv('test_result.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}