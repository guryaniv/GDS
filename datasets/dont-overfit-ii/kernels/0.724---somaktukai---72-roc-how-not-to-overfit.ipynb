{"cells":[{"metadata":{"_uuid":"a6da9e7341d3da367ac4fa9ce677eedc3b0c4d94"},"cell_type":"markdown","source":"# Do Not Overfit Challenge"},{"metadata":{"_uuid":"b66a543f806a71e432d49ec356ac81b66aa3055b"},"cell_type":"markdown","source":"#### Kaggle recently launched a challenge to run classification models on datasets with a very particular goal - do not overfit!!\n\n#### Datasets had only 250 rows and features around 320. Since our number of independent features is greater than sample size, we are faced with a very pressing overfitting challenge.\n\n##### Below I enumerate the methodology to reduce overfitting :\n\n###### 1. Reduce feature size using two specific techniques: pearsons  correlation coefficient and dimensionality reduction techniques\n###### 2. Create handful of samples with goal on simplicity. eg less number of estimators,less number of iterations etc\n###### 3. Provide regularization parameters where applicable\n###### 4. Plot precision-recall curves and roc-auc curves for each model. Check learning curve as well.\n###### 5. Use StratifiedKFold cross validation to parameter tune\n###### 6. Deal with imbalance in target classes using under sampling and over sampling techniques\n\n\n\nNote: baseline model creation(using all features + regularization + tsne dimensionality reduction + over/under sampling) technique provided no real use. CV scores for roc auc did not exceed 0.6 in every scenario possible.\n\n\n\n\n"},{"metadata":{"trusted":false,"_uuid":"ff990c0604609f6f4d6430d0c3c86a337d81964d"},"cell_type":"code","source":"import numpy as np\nimport pandas as panda\nfrom matplotlib import pyplot as plot\nimport seaborn as sns\n\nimport pandas as panda\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,label_binarize\n\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RepeatedStratifiedKFold,learning_curve\n\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, \\\n        classification_report,confusion_matrix,average_precision_score\nfrom sklearn.linear_model import Perceptron, LogisticRegression,RidgeClassifier,SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot as plot\nfrom itertools import cycle\nimport numpy as np \nfrom scipy import interp\nimport seaborn as sns\nimport itertools, time, datetime\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom numpy import bincount, linspace, mean, std, arange, squeeze\n\nimport warnings\n\nwarnings.simplefilter('ignore')\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b4f56eeb2530d7ab2f4670c503ae7009d0dd2e3"},"cell_type":"code","source":"np.random.seed(143)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"02c3cbe353e4033911145f5b1c9ab23177045b66"},"cell_type":"code","source":"train_data = panda.read_csv('../input/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6dd04af9c7eeba8a13134108bc45402cbe7cc97c"},"cell_type":"code","source":"train_data.target.value_counts(), train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f5369a0ab966e88261738794f54c071e9de981ef"},"cell_type":"code","source":"train_data['target'] = train_data.target.astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e45c02fc5b94c4ffa9ae5306dc8d5ac76ac89857"},"cell_type":"code","source":"plot.figure(figsize=(10,4))\ntrain_data.target.value_counts().plot(kind='bar')\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d177af2d285a42a5466622d352ac83543a976c8"},"cell_type":"code","source":"train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]].describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b4cb28e1dfaa9ac95271d33f0f11b4f8eb0af2d"},"cell_type":"code","source":"data_type = train_data.dtypes.to_frame().reset_index()\ndata_type.columns  = ['col_name','col_type']\ndata_type[data_type.col_type==np.object].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f84b9fa345755037acc8b8d2b5069110315927b"},"cell_type":"code","source":"train_data.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8bc12a1c68a0941f76f80dccf67a4e25464fa5af"},"cell_type":"code","source":"col_names = [i for i in train_data.columns if i not in ['target','id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"112d8206c4a26d329be8c7ace9d050d7407f46ab"},"cell_type":"code","source":"def draw_relevant_plots(table, columns):\n    \n    \n    fig, axes = plot.subplots(1, 5, figsize=(10, 7), sharex=True)\n    count = 0\n    for col in columns:\n        \n        ax = axes[count]\n        sns.distplot(table[col], ax = axes[count])\n        \n        count+= 1\n    plot.show()  \n    \n    plot.figure(figsize = (10,4))\n    data = table[columns]\n    data = panda.concat([data,table['target']], axis = 1)\n    correlation_map = np.corrcoef(data.values.T)\n    sns.heatmap(correlation_map,\n                cbar = True,\n                annot=True,\n                square = True,\n                fmt = '.2f',\n                annot_kws = {'size':15},\n                yticklabels = data.columns.tolist(),\n                xticklabels = data.columns.tolist(),\n               )\n    plot.show()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6fcef6cb064ee610e264cddee47ae5a4412ab3d0"},"cell_type":"code","source":"col_chunks = [col_names[i:i+5] for i in range(0,len(col_names),5)]\n\nfor item in col_chunks:\n    draw_relevant_plots(train_data, item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7855c5870408ca972e126a6ef9cd552e96c45c36"},"cell_type":"code","source":"from scipy import stats\ndef calculateCorrelationCoefficientsAndpValues(x_data, y_data, xlabel):\n    \n    pearson_coef, p_value = stats.pearsonr(x_data, y_data)\n    print(\"The Pearson Correlation Coefficient for %s is %s with a P-value of P = %s\" %(xlabel,pearson_coef, p_value))\n    \n    return (pearson_coef,p_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8558fa370deb1201fa09653eda853cf23149d770"},"cell_type":"code","source":"pearson_coeff = []\np_value = []\ncol_name = []\n\nfor col in [i for i in train_data.columns.tolist() if i not in ['id','target']]:\n    \n    x,y = calculateCorrelationCoefficientsAndpValues(train_data[col], train_data['target'], col)\n    pearson_coeff.append(x)\n    p_value.append(y)\n    col_name.append(col)\n    \npearson_table = panda.DataFrame({'column_name':col_name , 'pearson_coeff':pearson_coeff, 'p_value': p_value})\npearson_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d2f9f9900c29990282875199159dbb15353137b2"},"cell_type":"code","source":"pearson_table[(pearson_table.pearson_coeff>0.1) | (pearson_table.pearson_coeff<-0.1)].sort_values(by=['pearson_coeff'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7fd5af9462d47e17f2b5b6cb230fb8e84f480001"},"cell_type":"code","source":"reqd_columns = pearson_table[(pearson_table.pearson_coeff>0.1) | (pearson_table.pearson_coeff<-0.1)].sort_values(by=['pearson_coeff'], ascending=False).column_name.values.tolist()\nreqd_columns[:5], len(reqd_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19f1a2d9e31cca05dda0a401ee66fa4c16daba68"},"cell_type":"markdown","source":"There are a couple of observations that we can make out of the above diagrams:\n\n1. ALmost all independent variables are distributed within -2.5 and 2.5\n2. Almost all have near normal distributions\n3. Almost none have any significant outliers\n4. Pearsons correlation coeff for each indedepent variable with target variable is pretty low\n5. There are no categorical data\n6. pearsons correlation coefficient scores are pretty less and we will take all whose scores are above 0.1 and below -0.1"},{"metadata":{"_uuid":"f68617a2643c9a3e32a2a0b06027fe1eafc7dd8d"},"cell_type":"markdown","source":"#### Based on above data analysis, we are going to attempt 3 things\n\n1. create a baseline model out of gridsearch for the data using the usual suspects of classifiers. \n\n\n2. Plot learning curve and auc/roc scores for each\n\n\n3. try dimensionality reduction techniques such as PCA,LDA and t-SNE and run on baseline models created in step 1.\n\n\n4. try SMOTE and run on the same model created in point 1 and compare scores\n\n"},{"metadata":{"_uuid":"e9963ea4e8f83f83ba9d57db79931b6e15fb5fd3"},"cell_type":"markdown","source":"### STEP 1: Run handful of usual suspect classifiers using selected 50 features giving highest coeff scores\n\n<br><br>"},{"metadata":{"trusted":false,"_uuid":"07491b49959c79c02437b22a7d038be28eba0e4c"},"cell_type":"code","source":"class CodeTimer:\n    \n    \"\"\"\n        Utility custom contextual class for calculating the time \n        taken for a certain code block to execute\n    \n    \"\"\"\n    def __init__(self, name=None):\n        self.name = \" '\"  + name + \"'\" if name else ''\n\n    def __enter__(self):\n        self.start = time.clock()\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (time.clock() - self.start) * 1000.0\n        time_taken = datetime.timedelta(milliseconds = self.took)\n        print('Code block' + self.name + ' took(HH:MM:SS): ' + str(time_taken))\n        \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plot.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plot.imshow(cm, interpolation='nearest', cmap=cmap)\n    plot.title(title)\n    plot.colorbar()\n    tick_marks = arange(len(classes))\n    plot.xticks(tick_marks, classes, rotation=45)\n    plot.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plot.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plot.ylabel('True label')\n    plot.xlabel('Predicted label')\n#     plot.tight_layout()\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a92c8670f4eff5ba7f9c04c9070804741b1a511a"},"cell_type":"code","source":"\ndef plotLearningCurve(_x_train, _y_train, learning_model_pipeline,  model_name, k_fold = 10, training_sample_sizes = linspace(0.1,1.0,10), jobsInParallel = 1):\n    \n    training_size, training_score, testing_score = learning_curve(estimator = learning_model_pipeline, \\\n                                                                X = _x_train, \\\n                                                                y = _y_train, \\\n                                                                train_sizes = training_sample_sizes, \\\n                                                                cv = k_fold, \\\n                                                                n_jobs = jobsInParallel) \n\n\n    training_mean = mean(training_score, axis = 1)\n    training_std_deviation = std(training_score, axis = 1)\n    testing_std_deviation = std(testing_score, axis = 1)\n    testing_mean = mean(testing_score, axis = 1 )\n\n    ## we have got the estimator in this case the perceptron running in 10 fold validation with \n    ## equal division of sizes betwwen .1 and 1. After execution, we get the number of training sizes used, \n    ## the training scores for those sizes and the test scores for those sizes. we will plot a scatter plot \n    ## to see the accuracy results and check for bias vs variance\n\n    # training_size : essentially 10 sets of say a1, a2, a3,,...a10 sizes (this comes from train_size parameter, here we have given linespace for equal distribution betwwen 0.1 and 1 for 10 such values)\n    # training_score : training score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    # testing_score : testing score for the a1 samples, a2 samples...a10 samples, each samples run 10 times since cv value is 10\n    ## the mean and std deviation for each are calculated simply to show ranges in the graph\n\n    plot.plot(training_size, training_mean, label= \"Training Data\", marker= '+', color = 'blue', markersize = 8)\n    plot.fill_between(training_size, training_mean+ training_std_deviation, training_mean-training_std_deviation, color='blue', alpha =0.12 )\n\n    plot.plot(training_size, testing_mean, label= \"Testing/Validation Data\", marker= '*', color = 'green', markersize = 8)\n    plot.fill_between(training_size, testing_mean+ training_std_deviation, testing_mean-training_std_deviation, color='green', alpha =0.14 )\n\n    plot.title(\"Scoring of our training and testing data vs sample sizes for model:\"+model_name)\n    plot.xlabel(\"Number of Samples\")\n    plot.ylabel(\"Accuracy\")\n    plot.legend(loc= 'best')\n    plot.show()\n    \ndef plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name):\n        \n    plot.figure(figsize=(10,3))\n    plot.plot(list(false_positive_rate), list(true_positive_rate),  label = \"ROC Curve for model: \"+model_name)     \n    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n    plot.plot([0, 0, 1], [0,1, 1], ':', label = 'Perfect Score')\n    auc_score = auc(false_positive_rate, true_positive_rate)\n    plot.title('ROC Curve for model: %s with AUC %.2f'%(model_name, auc_score))\n    plot.xlabel('False Positive Rate')\n    plot.ylabel('True Positive Rate')\n    plot.legend(loc='best')\n    plot.show()\n    \n    \ndef plot_precision_recall_curve(precision, recall, model_name):\n    \n    plot.figure(figsize=(10,3))\n    plot.plot(list(recall), list(precision),  label = \"Precision/Recall Curve for model: \"+model_name)     \n#     plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n    plot.title('Precision Recall Curve for model: %s'%model_name)\n    plot.xlabel('Recall')\n    plot.ylabel('Precision')\n    plot.legend(loc='best')\n    plot.show()\n\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea56f0eda52d5cf00f7fc8730e8a5442cac42ed9"},"cell_type":"code","source":"def runGridSearchAndPredict(pipeline,model_name, x_train, y_train, x_test, y_test, param_grid, n_jobs = 1, cv = 10, score = 'accuracy'):\n#     pass\n\n    response =  {}\n    training_timer       = CodeTimer('training')\n    testing_timer        = CodeTimer('testing')\n    learning_curve_timer = CodeTimer('learning_curve')\n    predict_proba_timer  = CodeTimer('predict_proba')\n    \n    with training_timer:\n        \n        gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv = cv, n_jobs = n_jobs, scoring = score)\n\n        search = gridsearch.fit(x_train,y_train)\n\n        print(\"Grid Search Best parameters \", search.best_params_)\n        print(\"Grid Search Best score \", search.best_score_)\n\n    with testing_timer:\n        y_prediction = gridsearch.predict(x_test)\n            \n    print(\"F1 score %s\" %f1_score(y_test,y_prediction, average ='weighted'))\n    print(\"Classification report  \\n %s\" %(classification_report(y_test, y_prediction)))\n    \n    with learning_curve_timer:\n        plotLearningCurve(x_train, y_train, search.best_estimator_, model_name)\n#         _matrix = confusion_matrix(y_true = _y_test ,y_pred = y_prediction, labels = list(range(_y_test.shape[1])))\n        _matrix = confusion_matrix(y_true = y_test ,y_pred = y_prediction, labels = list(set(y_test)))\n        classes = list(set(y_test))\n        plot_confusion_matrix(_matrix, classes, title = \"Confusion matrix for model:\"+model_name)\n        \n    with predict_proba_timer:\n\n        if hasattr(gridsearch.best_estimator_, 'predict_proba'):\n            \n            print('inside decision function')\n            y_probability = gridsearch.predict_proba(x_test)\n            number_of_classes = len(np.unique(y_train))\n            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probability[:, 1])\n            response['roc_auc_score'] = roc_auc_score(y_test, y_probability[:,1])\n            response['roc_curve'] = (false_positive_rate, true_positive_rate)  \n            response['roc_curve_false_positive_rate'] = false_positive_rate\n            response['roc_curve_true_positive_rate'] = true_positive_rate\n            precision, recall, _ = precision_recall_curve(y_test, y_probability[:,1])\n            plot_roc_auc_curve(false_positive_rate, true_positive_rate, model_name)\n            plot_precision_recall_curve(precision, recall, model_name)\n            \n        else: ## eg SVM, Perceptron doesnt have predict_proba method\n            \n            response['roc_auc_score'] = 0\n            response['roc_curve'] = 0\n            response['roc_curve_false_positive_rate'] = 0\n            response['roc_curve_true_positive_rate'] = 0\n    \n    response['learning_curve_time'] = learning_curve_timer.took\n    response['testing_time'] = testing_timer.took\n    response['_y_prediction'] = y_prediction\n#     response['accuracy_score'] = accuracy_score(y_test,y_prediction)\n    response['training_time'] = training_timer.took\n    response['f1_score']  = f1_score(y_test, y_prediction, average ='weighted')\n    response['f1_score_micro']  = f1_score(y_test, y_prediction, average ='micro')\n    response['f1_score_macro']  = f1_score(y_test, y_prediction, average ='macro')\n    response['best_estimator'] = search.best_estimator_\n    response['confusion_matrix'] = _matrix\n    \n    return response\n\n\ndef plotROCCurveAcrossModels(positive_rates_sequence, model_name):\n    \n    plot.figure(figsize=(10,5))\n    for plot_values, label_name in zip(positive_rates_sequence, model_name):\n        \n        plot.plot(list(plot_values[0]), list(plot_values[1]),  label = \"ROC Curve for model: \"+label_name)\n        \n    plot.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing') #\n    plot.title('ROC Curve across models')\n    plot.xlabel('False Positive Rate')\n    plot.ylabel('True Positive Rate')\n    plot.legend(loc='best')\n    plot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"33233a1be7b5727e74f7a632ecf6104f0d718ac7"},"cell_type":"code","source":"\ndef execute( _x_train,\n             _y_train,\n             _x_test,\n             _y_test, \n            classifiers, \n            classifier_names, \n            classifier_param_grid,\n            cv  = 10 , \n            score = 'accuracy',\n            scaler = StandardScaler()\n           ):\n    \n    '''\n    This method will run your data sets against the model specified \n    Models will be fed through a pipeline where the first step would be to\n    execute a scaling operation.\n    \n    Method will also call additional lower level methods in order to plot\n    precision curve, roc curve, learning curve and will also prepare a confusion matrix\n    \n    :returns: dict containing execution metrics such as time taken, accuracy scores\n    :returntype: dict\n    \n    '''\n\n    timer = CodeTimer(name='overalltime')\n    model_metrics = {}\n\n    with timer:\n        for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid):\n\n            pipeline_steps = [('scaler', scaler),(model_name, model)] if scaler is not None else [(model_name, model)]\n            pipeline = Pipeline(pipeline_steps)\n\n            result = runGridSearchAndPredict(pipeline, \n                                             model_name,\n                                             _x_train,\n                                             _y_train,\n                                             _x_test,\n                                             _y_test, \n                                             model_param_grid ,\n                                             cv = cv,\n                                             score = score)\n\n            _y_prediction = result['_y_prediction']\n\n            model_metrics[model_name] = {}\n            model_metrics[model_name]['confusion_matrix'] = result.get('confusion_matrix')\n            model_metrics[model_name]['training_time'] = result.get('training_time')\n            model_metrics[model_name]['testing_time'] = result.get('testing_time')\n            model_metrics[model_name]['learning_curve_time'] = result.get('learning_curve_time')\n            model_metrics[model_name]['f1_score'] = result.get('f1_score')\n            model_metrics[model_name]['f1_score_macro'] = result.get('f1_score_macro')\n            model_metrics[model_name]['f1_score_micro'] = result.get('f1_score_micro')\n            model_metrics[model_name]['roc_auc_score'] = result.get('roc_auc_score')\n            model_metrics[model_name]['roc_curve_true_positive_rate'] = result.get('roc_curve_true_positive_rate')\n            model_metrics[model_name]['roc_curve_false_positive_rate'] = result.get('roc_curve_false_positive_rate')\n\n            model_metrics[model_name]['best_estimator'] = result.get('best_estimator')\n\n\n    print(timer.took)\n    \n    return model_metrics\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"78ac0596156ea7bd5978bb3361509cf61f8552f5"},"cell_type":"code","source":"\nclassifiers = [\n    Perceptron(random_state = 1),\n    LogisticRegression(random_state = 1),\n    LogisticRegression(random_state = 1, solver='liblinear'),\n    LogisticRegression(random_state = 1, solver='newton-cg'),\n    LogisticRegression(random_state = 1, solver='sag'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(random_state = 1),\n    KNeighborsClassifier(metric = 'minkowski'),\n    RidgeClassifier(random_state = 123), \n    SVC(kernel=\"linear\"),\n    SVC(),\n    ExtraTreeClassifier(random_state = 123),\n    GaussianProcessClassifier(random_state = 123),\n    BernoulliNB(),\n    BaggingClassifier(base_estimator = LogisticRegression(random_state = 1)),\n    BaggingClassifier(base_estimator = BernoulliNB()),\n    GradientBoostingClassifier(random_state= 123),\n    LGBMClassifier(objective = 'binary'),\n    XGBClassifier(objective = 'binary:logistic')\n]\n\n\nclassifier_names = [\n            'perceptron',\n            'logisticregression',\n            'logisticregression_liblinear_l2',\n            'logisticregression_newton_cg',\n            'logisticregression_sag',\n            'decisiontreeclassifier',\n            'randomforestclassifier',\n            'kneighborsclassifier',\n            'ridge',\n            'linear_svc',\n            'gamma_svc',\n            'extra_trees',\n            'gaussian_process',\n            'bernoulli',\n            'bagging_logistic',\n            'bagging_bernoulli',\n            'gradient_boosting_classifier',\n            'lgbm_classifier',\n            'xgb'\n]\n\nclassifier_param_grid = [\n            \n            {'perceptron__max_iter': [5,10,30], 'perceptron__eta0': [.1]},\n            {\n             'logisticregression__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression__penalty':['l1','l2'],\n             'logisticregression__solver':['saga','liblinear']\n            },\n            {\n             'logisticregression_liblinear_l2__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_liblinear_l2__penalty':['l2'],\n             'logisticregression_liblinear_l2__dual':[True]\n            },\n            {\n             'logisticregression_newton_cg__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_newton_cg__penalty':['l2'],\n            },\n            {\n             'logisticregression_sag__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_sag__penalty':['l2'],\n            },\n    \n            {'decisiontreeclassifier__max_depth':[6,8,10],\n             'decisiontreeclassifier__criterion':['gini','entropy'],\n             'decisiontreeclassifier__max_features':['auto','sqrt','log2'],\n            },\n            {'randomforestclassifier__n_estimators':[6,8,12],'randomforestclassifier__criterion': ['gini','entropy']} ,\n            {'kneighborsclassifier__n_neighbors':[4,6,10]},\n            {'ridge__alpha':[1,1.2,0.9],'ridge__max_iter':[100,300,500]},\n            {'linear_svc__C':[0.025]},\n            {'gamma_svc__gamma':[2,4],'gamma_svc__C':[1,5]},\n            {'extra_trees__max_depth':[6,8,12],'extra_trees__criterion': ['gini','entropy']} ,\n            {'gaussian_process__max_iter_predict':[200,400]} ,\n            {'bernoulli__alpha':[0.2,0.6,1.2]} ,\n            {'bagging_logistic__base_estimator__C':[1.2,0.02,2.2,4], \n             'bagging_logistic__base_estimator__penalty':['l1','l2'],\n             'bagging_logistic__n_estimators': [5,8,10]\n            },\n            {'bagging_bernoulli__base_estimator__alpha':[1.2,0.02,2.2,4], \n             'bagging_bernoulli__n_estimators': [5,8,10]\n            },\n            {\n                'gradient_boosting_classifier__loss':['deviance','exponential'],\n                'gradient_boosting_classifier__learning_rate':[0.5,1.2],\n                'gradient_boosting_classifier__n_estimators':[100,500,1000],\n                'gradient_boosting_classifier__criterion':['friedman_mse','mse','mae'],\n                'gradient_boosting_classifier__max_depth':[6,8,16,20],\n            },\n            {\n                 'lgbm_classifier__num_leaves':[25,], \\\n#                  'lgbm_classifier__min_data_in_leaf':[20],\\\n                 'lgbm_classifier__max_depth':[20,], \\\n                 'lgbm_classifier__learning_rate' : [0.01,],\\\n                 'lgbm_classifier__min_child_samples' :[2,], \\\n                 'lgbm_classifier__n_estimators' : [5000,], \\\n                 'lgbm_classifier__num_boost_round' : [100], \\\n                 'lgbm_classifier__feature_fraction' : [0.9,], \\\n                 'lgbm_classifier__bagging_freq' : [1,], \\\n                 'lgbm_classifier__bagging_seed' : [123], \\\n            },\n             {\n                'xgb__max_depth':[6,8,10],\n                 'xgb__learning_rate':[0.1,0.5,1,2],\n                 'xgb__n_estimators':[100,400,1000],             \n                 'xgb__booster':['gbtree','dart'],\n                 'xgb__subsample':[0.5, 0.2,0.8]\n            },\n    \n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b84239e841d9fc5a7d5ca766edc993c5874b8c91"},"cell_type":"code","source":"# x = train_data[[i for i in train_data.columns.tolist() if i not in ['target','id']]]\nx = train_data[reqd_columns[:51]]\ny = train_data['target']\n\nx_train,x_test,y_train,y_test = train_test_split(x,y , stratify = y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"492c3968e65fc6278514bde18b1cb17faed6c4c7"},"cell_type":"code","source":"cv = StratifiedKFold(n_splits = 5, shuffle= True, random_state =123)\nscore= 'roc_auc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4130534ccfc70e30eb1b7daebd63c474529ef64b"},"cell_type":"code","source":"response = execute(\n        x_train,\n        y_train,\n        x_test,\n        y_test,\n        classifiers,\n        classifier_names,\n        classifier_param_grid,\n        cv=cv,\n        score=score,\n        scaler=StandardScaler())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd7d82e0ec67be8e26c19723db66a7546085d830"},"cell_type":"code","source":"results = panda.DataFrame(response).transpose()\nresults.head()\nresults[['f1_score',\n         'f1_score_macro',\n         'f1_score_micro',\n         'learning_curve_time',\n         'roc_auc_score',\n         'testing_time',\n         'training_time',\n        ]]\\\n.sort_values(by=['roc_auc_score',],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48bf62d475f7d7acc704bb95231f20acf0504b5f"},"cell_type":"code","source":"\nroc_rates = []\nmodel_name = []\nfor index, key in enumerate(response):\n    \n    \n    estimator = response.get(key)\n    if estimator.get('roc_auc_score')!=0:\n        roc_curve_true_positive_rate = estimator.get('roc_curve_true_positive_rate')\n        roc_curve_false_positive_rate = estimator.get('roc_curve_false_positive_rate')\n        roc_rates.append([roc_curve_false_positive_rate,roc_curve_true_positive_rate])\n        model_name.append(key)\n\nplotROCCurveAcrossModels(roc_rates,model_name) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81a505c315e8519141eb48785cfa97f59a49e388"},"cell_type":"code","source":"results['learning_curve_time'] = results['learning_curve_time'].astype('float64')\nresults['testing_time'] = results['testing_time'].astype('float64')\nresults['training_time'] = results['training_time'].astype('float64')\nresults['f1_score'] = results['f1_score'].astype('float64')\nresults['f1_score_micro'] = results['f1_score_micro'].astype('float64')\nresults['f1_score_macro'] = results['f1_score_macro'].astype('float64')\nresults['roc_auc_score'] = results['roc_auc_score'].astype('float64')\n# results['roc_auc_macro'] = results['roc_auc_macro'].astype('float64')\n\n#scaling time parameters between 0 and 1\nresults['learning_curve_time'] = (results['learning_curve_time']- results['learning_curve_time'].min())/(results['learning_curve_time'].max()- results['learning_curve_time'].min())\nresults['testing_time'] = (results['testing_time']- results['testing_time'].min())/(results['testing_time'].max()- results['testing_time'].min())\nresults['training_time'] = (results['training_time']- results['training_time'].min())/(results['training_time'].max()- results['training_time'].min())\n\nresults.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acc37c302d267b4e6e79188ce2574a1490f84071"},"cell_type":"markdown","source":"### Conclusion:\n\n1. We got highest scores of ROC/AUC 0.92 in logistic regression with l2 parameter\n\n2. From the learning curve, we also see that overfitting tendency is less\n"},{"metadata":{"trusted":false,"_uuid":"e8aade1ea7b17e2447a201c1688d8a5890bfe6f9"},"cell_type":"code","source":"test_data = panda.read_csv('../input/test.csv')\n\ntest_data_x = test_data[reqd_columns]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcbcab0145debc4d458649b0bc6b483012ad40ea"},"cell_type":"markdown","source":"### STEP 2: Run Dimensionality technique, specifically t-SNE\n\n<br><br>"},{"metadata":{"trusted":false,"_uuid":"cd8df7b1b53c40cdf168f3ab902df15077e8ce88"},"cell_type":"code","source":"\n## perplexity parameters were tuned\ntsne = TSNE(perplexity=35, learning_rate=15)\nscaler = StandardScaler()\nx_train_tsne = tsne.fit_transform(scaler.fit_transform(x_train))\nx_test_tsne = tsne.fit_transform(scaler.fit_transform(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c656ca4be19f75fbc734f222fd75d02254d4188b"},"cell_type":"code","source":"# plot.scatter(x_train_tsne[:,0]x_train_tsne[:,0], x_train_tsne[:,1])\nplot.scatter(x_train_tsne[:,0], y_train,  marker='^', c='blue')\nplot.scatter(x_train_tsne[:,1], y_train,  marker='o', c='red')\nplot.show()\n# x_train_tsne[:,1].shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"a03e65748b120706a557c9f54bf37c07421f50de"},"cell_type":"code","source":"\n##runing without scaling, since scaling was already done prior to tsne\nresponse = execute(\n        x_train_tsne,\n        y_train,\n        x_test_tsne,\n        y_test,\n        classifiers,\n        classifier_names,\n        classifier_param_grid,\n        cv=cv,\n        score=score,\n        scaler=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4873a4537b78a2a608e8f3e05e68427ffa20ac85"},"cell_type":"code","source":"results = panda.DataFrame(response).transpose()\nresults.head()\nresults[['f1_score',\n         'f1_score_macro',\n         'f1_score_micro',\n         'learning_curve_time',\n         'roc_auc_score',\n         'testing_time',\n         'training_time',\n        ]]\\\n.sort_values(by=['roc_auc_score',],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b0ba46d2e9430586af0778b5838f5284d7e0afbd"},"cell_type":"code","source":"results['learning_curve_time'] = results['learning_curve_time'].astype('float64')\nresults['testing_time'] = results['testing_time'].astype('float64')\nresults['training_time'] = results['training_time'].astype('float64')\nresults['f1_score'] = results['f1_score'].astype('float64')\nresults['f1_score_micro'] = results['f1_score_micro'].astype('float64')\nresults['f1_score_macro'] = results['f1_score_macro'].astype('float64')\nresults['roc_auc_score'] = results['roc_auc_score'].astype('float64')\n# results['roc_auc_macro'] = results['roc_auc_macro'].astype('float64')\n\n#scaling time parameters between 0 and 1\nresults['learning_curve_time'] = (results['learning_curve_time']- results['learning_curve_time'].min())/(results['learning_curve_time'].max()- results['learning_curve_time'].min())\nresults['testing_time'] = (results['testing_time']- results['testing_time'].min())/(results['testing_time'].max()- results['testing_time'].min())\nresults['training_time'] = (results['training_time']- results['training_time'].min())/(results['training_time'].max()- results['training_time'].min())\n\nresults.plot(kind='barh',figsize=(12, 10))\nplot.title(\"Scaled Estimates across different classifiers used\")\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aecaff4f34f3181f0184a6fdfdef941ee5e70c58"},"cell_type":"markdown","source":"### Conclusion: dimensionality reduction gives worse predictions"},{"metadata":{"_uuid":"88a5f3a5641aa7899d6dbd558b803ee0b846a6a6"},"cell_type":"markdown","source":"### STEP 3: Upsampling / Downsampling Techniques\n\n<br><br>"},{"metadata":{"trusted":false,"_uuid":"20af802a48245943c4cd267c403ec422e7754700"},"cell_type":"code","source":"\nclassifiers = [\n    Perceptron(random_state = 1),\n    LogisticRegression(random_state = 1),\n    LogisticRegression(random_state = 1, solver='liblinear'),\n    LogisticRegression(random_state = 1, solver='newton-cg'),\n    LogisticRegression(random_state = 1, solver='sag'),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(random_state = 1),\n    KNeighborsClassifier(metric = 'minkowski'),\n    RidgeClassifier(random_state = 123), \n    SVC(kernel=\"linear\"),\n    SVC(),\n    ExtraTreeClassifier(random_state = 123),\n    GaussianProcessClassifier(random_state = 123),\n    BernoulliNB(),\n    BaggingClassifier(base_estimator = LogisticRegression(random_state = 1)),\n    BaggingClassifier(base_estimator = BernoulliNB()),\n \n]\n\n\nclassifier_names = [\n            'perceptron',\n            'logisticregression',\n            'logisticregression_liblinear_l2',\n            'logisticregression_newton_cg',\n            'logisticregression_sag',\n            'decisiontreeclassifier',\n            'randomforestclassifier',\n            'kneighborsclassifier',\n            'ridge',\n            'linear_svc',\n            'gamma_svc',\n            'extra_trees',\n            'gaussian_process',\n            'bernoulli',\n            'bagging_logistic',\n            'bagging_bernoulli',\n \n]\n\nclassifier_param_grid = [\n            \n            {'perceptron__max_iter': [5,10,30], 'perceptron__eta0': [.1]},\n            {\n             'logisticregression__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression__penalty':['l1','l2'],\n             'logisticregression__solver':['saga','liblinear']\n            },\n            {\n             'logisticregression_liblinear_l2__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_liblinear_l2__penalty':['l2'],\n             'logisticregression_liblinear_l2__dual':[True]\n            },\n            {\n             'logisticregression_newton_cg__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_newton_cg__penalty':['l2'],\n            },\n            {\n             'logisticregression_sag__C':[1.2,0.02,2.2,4, 0.01, 0.05], \n             'logisticregression_sag__penalty':['l2'],\n            },\n    \n            {'decisiontreeclassifier__max_depth':[6,8,10],\n             'decisiontreeclassifier__criterion':['gini','entropy'],\n             'decisiontreeclassifier__max_features':['auto','sqrt','log2'],\n            },\n            {'randomforestclassifier__n_estimators':[6,8,12],'randomforestclassifier__criterion': ['gini','entropy']} ,\n            {'kneighborsclassifier__n_neighbors':[4,6,10]},\n            {'ridge__alpha':[1,1.2,0.9],'ridge__max_iter':[100,300,500]},\n            {'linear_svc__C':[0.025]},\n            {'gamma_svc__gamma':[2,4],'gamma_svc__C':[1,5]},\n            {'extra_trees__max_depth':[6,8,12],'extra_trees__criterion': ['gini','entropy']} ,\n            {'gaussian_process__max_iter_predict':[200,400]} ,\n            {'bernoulli__alpha':[0.2,0.6,1.2]} ,\n            {'bagging_logistic__base_estimator__C':[1.2,0.02,2.2,4], \n             'bagging_logistic__base_estimator__penalty':['l1','l2'],\n             'bagging_logistic__n_estimators': [5,8,10]\n            },\n            {'bagging_bernoulli__base_estimator__alpha':[1.2,0.02,2.2,4], \n             'bagging_bernoulli__n_estimators': [5,8,10]\n            },\n    \n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ac04babe4565db2d90ab55a6cf216e943f521856"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f04a05494200f545bd901612a85bb6040b20949b"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import TomekLinks\n\nfrom imblearn.combine import SMOTETomek","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b6d4b3749537998784627007434e442484f757c"},"cell_type":"code","source":"\ntl = TomekLinks(return_indices=True, ratio='majority')\n_x_train_tomek, _y_train_tomek, id_tl = tl.fit_sample(x_train, y_train)\n\nsmt = SMOTETomek(ratio='auto')\n_x_train_smt, _y_train_smt = smt.fit_sample(x_train, y_train)\n\nsmote = SMOTE(ratio='minority')\nx_train_smote, y_train_smote = smote.fit_sample(x_train,y_train)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd9fdb058605a8d740d277bda014fe55e67531ef"},"cell_type":"code","source":"_x_train_tomek.shape, _y_train_tomek.shape,x_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"828488d0c263f745f438a877f963f6d221b253b1"},"cell_type":"code","source":"response1 = execute(\n        _x_train_tomek,\n        _y_train_tomek,\n        x_test,\n        y_test,\n        classifiers,\n        classifier_names,\n        classifier_param_grid,\n        cv=cv,\n        score=score,\n        scaler=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c1b1487d69bf92419ab6abf1bed8c7740ae68f35"},"cell_type":"code","source":"results = panda.DataFrame(response1).transpose()\nresults.head()\nresults[['f1_score',\n         'f1_score_macro',\n         'f1_score_micro',\n         'learning_curve_time',\n         'roc_auc_score',\n         'testing_time',\n         'training_time',\n        ]]\\\n.sort_values(by=['roc_auc_score',],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"eb6fdc1920ec89eacda97653ac0bde1ddd8a7477"},"cell_type":"code","source":"response = execute(\n        _x_train_smt,\n        _y_train_smt ,\n        x_test,\n        y_test,\n        classifiers,\n        classifier_names,\n        classifier_param_grid,\n        cv=cv,\n        score=score,\n        scaler=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4e7c1c1b9d1e17b162d0e0955120538688680a13"},"cell_type":"code","source":"results = panda.DataFrame(response).transpose()\nresults.head()\nresults[['f1_score',\n         'f1_score_macro',\n         'f1_score_micro',\n         'learning_curve_time',\n         'roc_auc_score',\n         'testing_time',\n         'training_time',\n        ]]\\\n.sort_values(by=['roc_auc_score',],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d6a004a090180a539397b4da97c12f27a9361985"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"dfc69d02943c83076204ab58fa36d60f81b80173"},"cell_type":"code","source":"response2 = execute(\n        x_train_smote,\n        y_train_smote ,\n        x_test,\n        y_test,\n        classifiers,\n        classifier_names,\n        classifier_param_grid,\n        cv=cv,\n        score=score,\n        scaler=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d131498f982c1570454f02a9764009f16762fb78"},"cell_type":"code","source":"results = panda.DataFrame(response2).transpose()\nresults.head()\nresults[['f1_score',\n         'f1_score_macro',\n         'f1_score_micro',\n         'learning_curve_time',\n         'roc_auc_score',\n         'testing_time',\n         'training_time',\n        ]]\\\n.sort_values(by=['roc_auc_score',],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58376bbeaa2758a0c5b7aaa32774190637a4d6af"},"cell_type":"markdown","source":"### Conclusion : We got the best performing model as Liblinear L2 Logistic Regression with a ROC-AUC score of 0.93 and f1 score of 0.82 using TomekLink downsampling"},{"metadata":{"trusted":false,"_uuid":"b2f489785845b6125f813b1d7bae3bb22eaaa21a"},"cell_type":"code","source":"logistic_regression_liblinear_2 = response1.get('logisticregression_liblinear_l2',{}).get('best_estimator')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"973b0da7f021ce23a02b0bc0ad08876f6b63e364"},"cell_type":"code","source":"# _x_tomek, _y_tomek, id_tl = tl.fit_sample(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"24081f33a98d6a3ace2be02aae7fec986cd1b61b"},"cell_type":"code","source":"test_data_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5731e05e773ef440c6f6e89a9ecdab3794f0fa6e"},"cell_type":"code","source":"# logistic_regression_liblinear_2.fit(_x_tomek, _y_tomek)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b3bbeec432297c893b254d6a6f2395a80c5baff"},"cell_type":"code","source":"test_target = logistic_regression_liblinear_2.predict(test_data_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"01e70773b1ed5058aece105ac35582da1fe9fcf1"},"cell_type":"code","source":"np.bincount(test_target)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31395abb56d7ca52ba6a09877a1393db72d846ce"},"cell_type":"code","source":"final_submission = panda.DataFrame({'target':test_target})\nfinal_submission['id'] = test_data['id']\nfinal_submission[['id','target']].head() \n\n# np.bincount(test_target)\n\nfinal_submission[['id','target']].to_csv('sample_submission_8.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}