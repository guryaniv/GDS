{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["#Introduction\n", "This project is to help explore the potential pattens behind the **Quora question pairs**. We are going to explore the data and try to find the most obvious patterns then have the ability to predict the unseen questions. In this kernel, the following points will be made:\n", "\n", " - *The pipeline to solve this problem*\n", " - *Data Exploration Analysis*\n", " - *Feature Engineering*\n", " - *Construct single best model*\n", " - *Stacking*\n", " - *Final Prediction*"], "metadata": {"_active": false, "_uuid": "9156e02c6de262ed4e981f18453598a0c6a0e24a", "_cell_guid": "ad4be0f1-41cc-03d9-41dc-e83f6ae9469e"}}, {"cell_type": "markdown", "source": ["#Pipeline\n", "The pipeline can be seen as the plans or the processes to solve the problem. The process can be:\n", "\n", "***Raw Data -> Features -> Stacking (Consists of best models) -> Final Prediction***\n", "\n", "The further explanation is:\n", "\n", " - ***Raw Data:*** This is the first step to deal with the data. The loading process of data, data exploration analysis as well as the basic analysis will be run.\n", " - ***Features:*** This is the further step to explore the data. This step can also be called as feature engineering, which deals with how to construct the new features, or how to transfer the previous raw data into the features, which can be used by the models. \n", " - ***Stacking:*** In this part, there are two kinds of processes. Choose the best single model and stack the best models into a final model. \n", "      - *Choose best single model:* There are a lot of parameters in the model. We should choose the most suitable parameters for the single model and then stack them together. \n", "      - *Stacking the models:* Stacking is one method of ensembling algorithm. Easily to say, the results from previous model can be seen as the input of the next model. Then, the combination of several models can be seen as a final model to be used to predict the unseen dataset. "], "metadata": {"_active": false, "_uuid": "c71806191c89de927a3fdc04aa54bf624dfb1f10", "_cell_guid": "cb4ace18-def3-ba90-d668-78ff4c482a0b"}}, {"cell_type": "markdown", "source": ["#Raw Data"], "metadata": {"_active": false, "_uuid": "7be461533ed95b394fc1d7fcc76d7772366d991e", "_cell_guid": "861d1bf3-03a4-a3aa-a215-0c38a0cc31d2"}}, {"cell_type": "code", "outputs": [], "source": ["#Import the important packages. \n", "import numpy as np \n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams\n", "eng_stopwords = set(stopwords.words('english'))\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "from nltk.corpus import wordnet as wn\n", "from nltk.corpus import wordnet\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams"], "execution_count": null, "metadata": {"_execution_state": "idle", "_active": false, "_uuid": "4bc9506b93c42b7ea185e38ebaf00b4f15a1086a", "collapsed": true, "_cell_guid": "37f40ec2-04d0-0b51-142f-fa5cd3f064eb"}}, {"cell_type": "markdown", "source": ["##Import data\n", "In the following text is the overview of the **training data set**:"], "metadata": {"_active": false, "_uuid": "26b8796e1e9a0cd799076f3e1be5dc53ce038559", "_cell_guid": "36568bd1-e3ac-171b-aae8-fe71af855a85"}}, {"cell_type": "code", "outputs": [], "source": ["train_data = pd.read_csv(\"../input/q_quora.csv\")\n", "print(train_data.shape)\n", "train_data.head()"], "execution_count": null, "metadata": {"_execution_state": "idle", "_active": true, "_uuid": "28dfba54eab84ebb24bf07fc42fdc36325ec2cbf", "_cell_guid": "d5162da9-0996-83f3-370c-d42a3c4ac80d"}}, {"cell_type": "markdown", "source": ["In the following text is the overview of the **test data set**:"], "metadata": {"_active": false, "_uuid": "3bc5d5866d8f4db1a9f425c6283029434b20b7f1", "_cell_guid": "b8040a0d-a261-337e-d609-1c89de6d3aaa"}}, {"cell_type": "code", "outputs": [], "source": ["test_data = pd.read_csv(\"../input/test.csv\")\n", "print(test_data.shape)\n", "test_data.head()"], "execution_count": null, "metadata": {"_active": false, "_uuid": "8fcd3054f884911d6a460a411be0a1bf436974da", "collapsed": true, "_cell_guid": "2a969925-aba9-4faa-8fb5-edce88be0531"}}, {"cell_type": "markdown", "source": ["## Basic EDA"], "metadata": {"_active": false, "_uuid": "25ea1790dc7d1894cc0d747a6006d72994f3e33e", "_cell_guid": "8c190b8b-6549-e2c8-39ed-2c2aa7190b77"}}, {"cell_type": "code", "outputs": [], "source": ["is_dup = train_data['is_duplicate'].value_counts()\n", "sns.barplot(is_dup.index, is_dup.values)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "b8d00434d33a124ff81bd7354b1ad1ed22a37fa0", "collapsed": true, "_cell_guid": "c0e805f9-04b9-61d5-6d05-ffca55d3de27"}}, {"cell_type": "code", "outputs": [], "source": ["train_q1 = train_data['question1']\n", "train_q2 = train_data['question2']\n", "train_q1_length = [len(i) for i in train_q1]\n", "sns.histplot(train_q1_length)"], "execution_count": null, "metadata": {"_execution_state": "idle", "_active": false, "_uuid": "73c6f667f3c11e2b1274cc45e4568e8573f9c1da", "collapsed": true, "_cell_guid": "18a1275c-dd9b-e40c-fdc7-765f357bbf06"}}, {"cell_type": "markdown", "source": ["#Features\n", "As mentioned, we need to transfer the data into the features, which can be used in the further model construction. Just as a summary, the possible interesting features can be:\n", "\n", "* **Words of the question**\n", "\n", "* **Number of the Noun words**\n", "\n", "* **Number of the Capital words - 1**\n", "\n", "* **tf**\n", "\n", "* **dif**\n", "\n", "* **tf/dif**\n", "\n", "* **Sentiment analysis**\n", "\n", "In the following part, not all the features will be constructed, only the most important ones will be explained. The codes are shown as following. "], "metadata": {"_active": false, "_uuid": "4fa9c67bcb1ace85304b1fd134452292f05e2ed5", "_cell_guid": "5393bd9d-9972-6ce2-7278-8ce59fc835e4"}}, {"cell_type": "code", "outputs": [], "source": ["from nltk.corpus import wordnet as wn\n", "from nltk.corpus import wordnet\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams\n", "import os\n", "import numpy as np \n", "import pandas as pd \n", "import csv\n", "import re\n", "from collections import Counter\n", "\n", "eng_stopwords = set(stopwords.words('english'))\n", "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n", "\n", "def getWords(text):\n", "    return re.compile('\\w+').findall(text)\n", "\n", "def getWords_0(text):\n", "    temp = re.compile('\\w+').findall(str(text))\n", "    temp = \" \".join(temp).lower()\n", "    return(temp)\n", "\n", "trans_Q1_AllWords = pd.Series(train_data['question1'].tolist()).astype(str)\n", "df_train_Q1 = train_data['question1'].apply(lambda row: getWords_0(row))\n", "words_1 = (\" \".join(df_train_Q1)).split()\n", "counts_Q1 = Counter(words_1)\n", "\n", "\n", "trans_Q2_AllWords = pd.Series(train_data['question2'].tolist()).astype(str)\n", "df_train_Q2 = train_data['question2'].apply(lambda row: getWords_0(row))\n", "words_2 = (\" \".join(df_train_Q2)).split()\n", "counts_Q2 = Counter(words_2)\n", "\n", " \n", "def feature_extraction(text):\n", "    \n", "    que1 = str(text['question1']).lower()\n", "    que2 = str(text['question2']).lower()\n", "    que1 = getWords(que1)\n", "    que2 = getWords(que2)\n", "    \n", "    feature = []\n", "    feature.extend([len(que1),len(que2)])\n", "    \n", "    Simplified_que1 = [word for word in que1 if word not in eng_stopwords]\n", "    Simplified_que2 = [word for word in que2 if word not in eng_stopwords]\n", "    Length_que1 = len(Simplified_que1)\n", "    Length_que2 = len(Simplified_que2)\n", "    feature.extend([Length_que1,Length_que2])\n", "\n", "    Unique_que1 = [word for word in Simplified_que1 if word not in Simplified_que2]\n", "    Unique_que2 = [word for word in Simplified_que2 if word not in Simplified_que1]\n", "    feature.extend([len(Unique_que1),len(Unique_que2)])\n", "    \n", "    Unique_que1_Nouns = [word for word in Unique_que1 if word in nouns]\n", "    Unique_que2_Nouns = [word for word in Unique_que2 if word in nouns]\n", "    feature.extend([len(Unique_que1_Nouns), len(Unique_que2_Nouns)])\n", "    \n", "    # Tfdif\n", "    df_Q1_1 = 0\n", "    df_Q1_2 = 0\n", "    df_Q2_1 = 0\n", "    df_Q2_2 = 0\n", "    for i in Unique_que1:\n", "        df_Q1_1 = df_Q1_1 + counts_Q1[i] / len(words_1)\n", "        df_Q1_2 = df_Q1_2 + counts_Q2[i] / len(words_2)\n", "    for i in Unique_que2:\n", "        df_Q2_1 = df_Q2_1 + counts_Q1[i] / len(words_1)\n", "        df_Q2_2 = df_Q2_2 + counts_Q2[i] / len(words_2)\n", "        \n", "    feature.extend([df_Q1_1, df_Q1_2, df_Q2_1, df_Q2_2])\n", "\n", "    return(feature)\n", "\n", "df_train_Questions = train_data[['question1','question2']]\n", "train_X = np.vstack( np.array(df_train_Questions.apply(lambda row: feature_extraction(row), axis=1)) )\n", "\n", "train_Y = train_data['is_duplicate']\n", "\n", "pos_train = train_X[train_Y == 1]\n", "neg_train = train_X[train_Y == 0]\n", "index = np.random.choice(len(pos_train), int(0.17 * 255027), replace = False)\n", "pos_train = pos_train[index]\n", "train_X = np.concatenate([pos_train, neg_train])\n", "train_Y = np.concatenate([np.zeros(len(pos_train)) + 1, np.zeros(len(neg_train))])"], "execution_count": null, "metadata": {"_active": false, "_uuid": "31d39f3da05106bd38114bb21227328e5ad3600b", "collapsed": true, "_cell_guid": "d78074b8-b471-e558-4fff-962e3740992a"}}, {"cell_type": "code", "outputs": [], "source": ["train_X"], "execution_count": null, "metadata": {"_active": false, "_uuid": "b3727a2efbe9d79bb610a21029ff0dd25cdc1a18", "collapsed": true, "_cell_guid": "29a94708-3ce5-fef2-2acd-40b70750ad92"}}, {"cell_type": "markdown", "source": ["# Model Construction\n", "There are a lot of models can be used in this problem. We should test them and find the best parameter of these models, then stack them together to give a final prediction of the problem. "], "metadata": {"_active": false, "_uuid": "d9b36f052bf903640af9e02287269678600d0949", "_cell_guid": "84e599d1-71c2-5937-e41d-a8b7242de939"}}, {"cell_type": "markdown", "source": ["##Single best model \n", "The basic things for the model is the single model. We should find the best parameters for the single model and then use the same method to construct the other models. "], "metadata": {"_active": false, "_uuid": "c21e81ec6ca0212010eb0e55e753b00546ca12c2", "_cell_guid": "56c34e9a-c2d2-b363-f589-a6690972870d"}}, {"cell_type": "code", "outputs": [], "source": ["# Here one simple example will show the basic way to find the best model. The RandomForest algorithm will be used here\n", "from sklearn.ensemble import RandomForestClassifier\n", "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "\n", "# Here the train_X is the features which are generated from the raw data. Train_Y is the results\n", "X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234)\n", "\n", "def objective(space):\n", "    clf = RandomForestClassifier(**space)\n", "    logloss = cross_val_score(clf,train_X,train_Y).mean()\n", "    print (\"SCORE:\", logloss)\n", "    return{'loss':logloss, 'status': STATUS_OK }\n", "\n", "space4rf = {\n", "    'max_depth': hp.choice('max_depth', range(1,20)),\n", "    'max_features': hp.choice('max_features', range(1,5)),\n", "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n", "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n", "    'scale': hp.choice('scale', [0, 1]),\n", "    'normalize': hp.choice('normalize', [0, 1])\n", "}\n", "\n", "trials = Trials()\n", "best = fmin(fn=objective,\n", "            space=space4rf,\n", "            algo=tpe.suggest,\n", "            max_evals=10,\n", "            trials=trials)\n", "\n", "print (best)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "768899e8d7aafff629c5c3b11af2aee96caf59a4", "collapsed": true, "_cell_guid": "8d0b44f3-8bab-2c32-1685-3fedf4974a5b"}}, {"cell_type": "markdown", "source": ["## Stacking\n", "After we have the basic best model from the previous process, now we can begin to stack them together. Stacking is one method of the ensambling algorithm, which aims to combine several models together to get a better model. The principles of stacking is not so hard, just use the model to create results, which can be seen as the new features for the next stacking. "], "metadata": {"_active": false, "_uuid": "c79981db2b7d3b494566c6305e538c118a93cd47", "_cell_guid": "f3e1f4aa-3724-5b96-45ec-fc22b031c011"}}, {"cell_type": "code", "outputs": [], "source": ["# The code for the stacking. \n", "\n", "clfs = [RandomForestClassifier(n_estimators=100, criterion='gini'),\n", "        RandomForestClassifier(n_estimators=100, criterion='entropy'),\n", "        RandomForestClassifier(n_estimators=10, criterion='gini'),\n", "        RandomForestClassifier(n_estimators=10, criterion='entropy'),\n", "        ExtraTreesClassifier(n_estimators=100, criterion='gini'),\n", "        ExtraTreesClassifier(n_estimators=100, criterion='entropy'),\n", "        ExtraTreesClassifier(n_estimators=10, criterion='gini'),\n", "        ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n", "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50),\n", "        KNeighborsClassifier(n_neighbors=5),\n", "        KNeighborsClassifier(n_neighbors=10),\n", "        GaussianNB(),\n", "        LogisticRegression()]\n", "\n", "n_folds = 5\n", "skf = list(StratifiedKFold(train_Y, n_folds))\n", "\n", "dataset_blend_train = np.zeros((train_X.shape[0], len(clfs)))\n", "dataset_blend_test = np.zeros((test_X.shape[0], len(clfs)))\n", "    \n", "for j, clf in enumerate(clfs):\n", "    print (j, clf)\n", "    dataset_blend_test_j = np.zeros((test_X.shape[0], len(skf)))\n", "    for i, (train, test) in enumerate(skf):\n", "        print (\"Fold\", i)\n", "        X_train = train_X[train]\n", "        y_train = train_Y[train]\n", "        X_test = train_X[test]\n", "        y_test = train_Y[test]\n", "        clf.fit(X_train, y_train)\n", "        y_submission = clf.predict_proba(X_test)[:, 1]\n", "        dataset_blend_train[test, j] = y_submission\n", "        dataset_blend_test_j[:, i] = clf.predict_proba(test_X)[:, 1]\n", "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "eb499ebeef21362dde0785c1c69bc23f3334ec49", "collapsed": true, "_cell_guid": "1fe75d4f-201b-cd95-bedc-e8e561ea7e83"}}, {"cell_type": "markdown", "source": ["###Final Prediction\n", "After the stacking, we have new features, which can be used to make the final prediction. Here I choose XGBoost as the algorithm to make the final prediction. The process is almost similar with the previous steps. "], "metadata": {"_active": false, "_uuid": "49e5a8a13757c5298d76fa54fc5b74fe2a2ac40f", "_cell_guid": "8dcaa780-7a1e-2408-bd2f-ecb01953b14f"}}, {"cell_type": "code", "outputs": [], "source": ["kf = KFold(n_splits=5, shuffle=True, random_state=2016)\n", "\n", "for dev_index, val_index in kf.split(range(dataset_blend_train_L2.shape[0])):\n", "    \n", "    params = {}\n", "    params[\"objective\"] = \"binary:logistic\"\n", "    params['eval_metric'] = 'logloss'\n", "    params[\"eta\"] = 0.3\n", "    params[\"subsample\"] = 0.7\n", "    params[\"min_child_weight\"] = 2\n", "    params[\"colsample_bytree\"] = 0.7\n", "    params[\"max_depth\"] = 5\n", "    params[\"silent\"] = 1\n", "    dev_X, val_X = dataset_blend_train_L2[dev_index,:], dataset_blend_train_L2[val_index,:]\n", "    dev_y, val_y = train_Y[dev_index], train_Y[val_index]\n", "    d_train = xgb.DMatrix(dev_X, label = dev_y)\n", "    d_test = xgb.DMatrix(val_X, label = val_y)\n", "    watchlist = [ (d_train,'train'), (d_test, 'test') ]\n", "    bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n", "    break\n", "\n", "# Prediction\n", "dataset_blend_test = xgb.DMatrix(dataset_blend_test_L2)\n", "data_XGB = xgb.DMatrix(test_X)\n", "predict_y_XGB = bst.predict(data_XGB)\n", "data_prediction = predict_y_XGB\n", "\n", "def Prediction(training_data, testing_data, clfs):\n", "    for i in clfs:\n", "        temp_model = i.fit(training_data, train_Y)\n", "        temp_prediction = temp_model.predict(testing_data)\n", "        data_prediction = np.vstack([data_prediction,temp_prediction])\n", "        print(i)\n", "        \n", "for i in clfs_prediction:\n", "    temp_model = i.fit(dataset_blend_train_L2, train_Y)\n", "    temp_prediction = temp_model.predict(dataset_blend_test_L2)\n", "    data_prediction = np.vstack([data_prediction,temp_prediction])\n", "    print(i)\n", "\n", "sub = pd.DataFrame()\n", "sub['test_id'] = df_test['test_id']\n", "sub['is_duplicate'] = data_prediction\n", "sub.to_csv(\"submission.csv\", index=False)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "9c1c5c6ed3a14c8576d676061994635370a7c3f9", "collapsed": true, "_cell_guid": "5fdf14c6-02f4-80fc-f41e-fcb677400e3d"}}, {"cell_type": "markdown", "source": ["#Introduction\n", "This project is to help explore the potential pattens behind the **Quora question pairs**. We are going to explore the data and try to find the most obvious patterns then have the ability to predict the unseen questions. In this kernel, the following points will be made:\n", "\n", " - *The pipeline to solve this problem*\n", " - *Data Exploration Analysis*\n", " - *Feature Engineering*\n", " - *Construct single best model*\n", " - *Stacking*\n", " - *Final Prediction*"], "metadata": {"_active": false, "_uuid": "71d01fb15ea05c7575fc8ec267df211dd95f09e4", "_cell_guid": "ea5ef5eb-88db-8aee-1ffe-31ff148e4981"}}, {"cell_type": "markdown", "source": ["#Pipeline\n", "The pipeline can be seen as the plans or the processes to solve the problem. The process can be:\n", "\n", "***Raw Data -> Features -> Stacking (Consists of best models) -> Final Prediction***\n", "\n", "The further explanation is:\n", "\n", " - ***Raw Data:*** This is the first step to deal with the data. The loading process of data, data exploration analysis as well as the basic analysis will be run.\n", " - ***Features:*** This is the further step to explore the data. This step can also be called as feature engineering, which deals with how to construct the new features, or how to transfer the previous raw data into the features, which can be used by the models. \n", " - ***Stacking:*** In this part, there are two kinds of processes. Choose the best single model and stack the best models into a final model. \n", "      - *Choose best single model:* There are a lot of parameters in the model. We should choose the most suitable parameters for the single model and then stack them together. \n", "      - *Stacking the models:* Stacking is one method of ensembling algorithm. Easily to say, the results from previous model can be seen as the input of the next model. Then, the combination of several models can be seen as a final model to be used to predict the unseen dataset. "], "metadata": {"_active": false, "_uuid": "30ad00160847be931acfe50f3ee7952cf96ea979", "_cell_guid": "497a90b8-c5f8-8d5f-4e89-7e56dac34025"}}, {"cell_type": "markdown", "source": ["#Raw Data"], "metadata": {"_active": false, "_uuid": "af1897b1916ee9f101596b335d65b07632fb2f20", "_cell_guid": "d582f0f6-0a2c-5df3-37c6-988afb7b3f88"}}, {"cell_type": "code", "outputs": [], "source": ["#Import the important packages. \n", "import numpy as np \n", "import pandas as pd \n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams\n", "eng_stopwords = set(stopwords.words('english'))\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "from nltk.corpus import wordnet as wn\n", "from nltk.corpus import wordnet\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams"], "execution_count": null, "metadata": {"_active": false, "_uuid": "9b3b9ed443cf7ae8a328b016c38937cea4b798f2", "collapsed": true, "_cell_guid": "2ce447ec-c34e-320f-f3a5-f65016eb9fe8"}}, {"cell_type": "markdown", "source": ["##Import data\n", "In the following text is the overview of the **training data set**:"], "metadata": {"_active": false, "_uuid": "9887c9a5f035f3daa9fa0c52754c17d96b30e2dd", "_cell_guid": "663e2b6b-8455-b07b-afc2-48311ee9e593"}}, {"cell_type": "code", "outputs": [], "source": ["train_data = pd.read_csv(\"../input/train.csv\")\n", "print(train_data.shape)\n", "train_data.head()"], "execution_count": null, "metadata": {"_active": false, "_uuid": "e6c38fec5627d96e473937497c4b19d11e6c0c4a", "collapsed": true, "_cell_guid": "f7f6475b-84c2-abf1-9ded-e6709b7cc6c5"}}, {"cell_type": "markdown", "source": ["In the following text is the overview of the **test data set**:"], "metadata": {"_active": false, "_uuid": "54f6532121f488df68b12171b636b9512fe63e12", "_cell_guid": "e4bec55c-68ad-ff15-8950-1620d339e2ae"}}, {"cell_type": "code", "outputs": [], "source": ["test_data = pd.read_csv(\"../input/test.csv\")\n", "print(test_data.shape)\n", "test_data.head()"], "execution_count": null, "metadata": {"_active": false, "_uuid": "e1881abfda40e6603baad6705af135d6a69c6e7b", "collapsed": true, "_cell_guid": "f05b3776-2681-1a57-7e1c-ef680524550f"}}, {"cell_type": "markdown", "source": ["## Basic EDA"], "metadata": {"_active": false, "_uuid": "e4c25ecd8e251c8f9d63b3d5ac231f1648cf63b5", "_cell_guid": "7df316c6-30c7-ff6e-ebcf-64c045fd6212"}}, {"cell_type": "code", "outputs": [], "source": ["is_dup = train_data['is_duplicate'].value_counts()\n", "sns.barplot(is_dup.index, is_dup.values)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "18f80c8d8e67c5601d05ca99a7aa5b5b7258dc87", "collapsed": true, "_cell_guid": "9751f7a5-86fd-6285-2e02-dfbd5ed7d843"}}, {"cell_type": "code", "outputs": [], "source": ["train_q1 = train_data['question1']\n", "train_q2 = train_data['question2']\n", "train_q1_length = [len(i) for i in train_q1]\n", "sns.distplot(train_q1_length)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "71eccd8a4196f8e3802b5838ac9720701acfadb1", "collapsed": true, "_cell_guid": "c2fd2202-4280-7b40-e054-3e054aa95283"}}, {"cell_type": "markdown", "source": ["#Features\n", "As mentioned, we need to transfer the data into the features, which can be used in the further model construction. Just as a summary, the possible interesting features can be:\n", "\n", "* **Words of the question**\n", "\n", "* **Number of the Noun words**\n", "\n", "* **Number of the Capital words - 1**\n", "\n", "* **tf**\n", "\n", "* **dif**\n", "\n", "* **tf/dif**\n", "\n", "* **Sentiment analysis**\n", "\n", "In the following part, not all the features will be constructed, only the most important ones will be explained. The codes are shown as following. "], "metadata": {"_active": false, "_uuid": "fb6ccc5062aabb58614f0314c89ea138c56bf6ea", "_cell_guid": "c07ae313-843a-027f-7378-17d4bbe3db46"}}, {"cell_type": "code", "outputs": [], "source": ["from nltk.corpus import wordnet as wn\n", "from nltk.corpus import wordnet\n", "from nltk.corpus import stopwords\n", "from nltk import word_tokenize, ngrams\n", "import os\n", "import numpy as np \n", "import pandas as pd \n", "import csv\n", "import re\n", "from collections import Counter\n", "\n", "eng_stopwords = set(stopwords.words('english'))\n", "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n", "\n", "def getWords(text):\n", "    return re.compile('\\w+').findall(text)\n", "\n", "def getWords_0(text):\n", "    temp = re.compile('\\w+').findall(str(text))\n", "    temp = \" \".join(temp).lower()\n", "    return(temp)\n", "\n", "trans_Q1_AllWords = pd.Series(train_data['question1'].tolist()).astype(str)\n", "df_train_Q1 = train_data['question1'].apply(lambda row: getWords_0(row))\n", "words_1 = (\" \".join(df_train_Q1)).split()\n", "counts_Q1 = Counter(words_1)\n", "\n", "\n", "trans_Q2_AllWords = pd.Series(train_data['question2'].tolist()).astype(str)\n", "df_train_Q2 = train_data['question2'].apply(lambda row: getWords_0(row))\n", "words_2 = (\" \".join(df_train_Q2)).split()\n", "counts_Q2 = Counter(words_2)\n", "\n", " \n", "def feature_extraction(text):\n", "    \n", "    que1 = str(text['question1']).lower()\n", "    que2 = str(text['question2']).lower()\n", "    que1 = getWords(que1)\n", "    que2 = getWords(que2)\n", "    \n", "    feature = []\n", "    feature.extend([len(que1),len(que2)])\n", "    \n", "    Simplified_que1 = [word for word in que1 if word not in eng_stopwords]\n", "    Simplified_que2 = [word for word in que2 if word not in eng_stopwords]\n", "    Length_que1 = len(Simplified_que1)\n", "    Length_que2 = len(Simplified_que2)\n", "    feature.extend([Length_que1,Length_que2])\n", "\n", "    Unique_que1 = [word for word in Simplified_que1 if word not in Simplified_que2]\n", "    Unique_que2 = [word for word in Simplified_que2 if word not in Simplified_que1]\n", "    feature.extend([len(Unique_que1),len(Unique_que2)])\n", "    \n", "    Unique_que1_Nouns = [word for word in Unique_que1 if word in nouns]\n", "    Unique_que2_Nouns = [word for word in Unique_que2 if word in nouns]\n", "    feature.extend([len(Unique_que1_Nouns), len(Unique_que2_Nouns)])\n", "    \n", "    # Tfdif\n", "    df_Q1_1 = 0\n", "    df_Q1_2 = 0\n", "    df_Q2_1 = 0\n", "    df_Q2_2 = 0\n", "    for i in Unique_que1:\n", "        df_Q1_1 = df_Q1_1 + counts_Q1[i] / len(words_1)\n", "        df_Q1_2 = df_Q1_2 + counts_Q2[i] / len(words_2)\n", "    for i in Unique_que2:\n", "        df_Q2_1 = df_Q2_1 + counts_Q1[i] / len(words_1)\n", "        df_Q2_2 = df_Q2_2 + counts_Q2[i] / len(words_2)\n", "        \n", "    feature.extend([df_Q1_1, df_Q1_2, df_Q2_1, df_Q2_2])\n", "\n", "    return(feature)\n", "\n", "df_train_Questions = train_data[['question1','question2']]\n", "train_X = np.vstack( np.array(df_train_Questions.apply(lambda row: feature_extraction(row), axis=1)) )\n", "\n", "train_Y = train_data['is_duplicate']\n", "\n", "pos_train = train_X[train_Y == 1]\n", "neg_train = train_X[train_Y == 0]\n", "index = np.random.choice(len(pos_train), int(0.17 * 255027), replace = False)\n", "pos_train = pos_train[index]\n", "train_X = np.concatenate([pos_train, neg_train])\n", "train_Y = np.concatenate([np.zeros(len(pos_train)) + 1, np.zeros(len(neg_train))])"], "execution_count": null, "metadata": {"_active": false, "_uuid": "e20a96d5b8ba1d5faf100eb29197b7d72fd97f91", "collapsed": true, "_cell_guid": "7c456d4d-bb80-24ec-ef1e-0953e86a5860"}}, {"cell_type": "code", "outputs": [], "source": ["train_X"], "execution_count": null, "metadata": {"_active": false, "_uuid": "e3e48a8c13bd6cf96a07e0d07126bfcf8791f2d8", "collapsed": true, "_cell_guid": "0ba251d3-39fd-2387-c12b-3f7b87aef138"}}, {"cell_type": "markdown", "source": ["# Model Construction\n", "There are a lot of models can be used in this problem. We should test them and find the best parameter of these models, then stack them together to give a final prediction of the problem. "], "metadata": {"_active": false, "_uuid": "07db1a6f3b685580fe4884bd16314735080d6dbd", "_cell_guid": "99ad593b-954a-ef71-e0ff-2cdf69b9ae64"}}, {"cell_type": "markdown", "source": ["##Single best model \n", "The basic things for the model is the single model. We should find the best parameters for the single model and then use the same method to construct the other models. "], "metadata": {"_active": false, "_uuid": "0c3412adb1f49faf690e30f4dbd2801f71b95ae0", "_cell_guid": "a9f64926-c001-e3cd-14d0-45e52c173f72"}}, {"cell_type": "code", "outputs": [], "source": ["# Here one simple example will show the basic way to find the best model. The RandomForest algorithm will be used here\n", "from sklearn.ensemble import RandomForestClassifier\n", "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "\n", "# Here the train_X is the features which are generated from the raw data. Train_Y is the results\n", "X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234)\n", "\n", "def objective(space):\n", "    clf = RandomForestClassifier(**space)\n", "    logloss = cross_val_score(clf,train_X,train_Y).mean()\n", "    print (\"SCORE:\", logloss)\n", "    return{'loss':logloss, 'status': STATUS_OK }\n", "\n", "space4rf = {\n", "    'max_depth': hp.choice('max_depth', range(1,20)),\n", "    'max_features': hp.choice('max_features', range(1,5)),\n", "    'n_estimators': hp.choice('n_estimators', range(1,20)),\n", "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n", "}\n", "\n", "trials = Trials()\n", "best = fmin(fn=objective,\n", "            space=space4rf,\n", "            algo=tpe.suggest,\n", "            max_evals=10,\n", "            trials=trials)\n", "\n", "print (best)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "259663d9c218aa14dd74029adaaca860bc9f7887", "collapsed": true, "_cell_guid": "5edd62d4-3bba-e64b-dd80-348e7da633a9"}}, {"cell_type": "markdown", "source": ["## Stacking\n", "After we have the basic best model from the previous process, now we can begin to stack them together. Stacking is one method of the ensambling algorithm, which aims to combine several models together to get a better model. The principles of stacking is not so hard, just use the model to create results, which can be seen as the new features for the next stacking. "], "metadata": {"_active": false, "_uuid": "84ec713a4cbe9122bf4de387ad01c324c37a7646", "_cell_guid": "8f1ab29a-acbd-0845-5c58-ea7f0ea0525d"}}, {"cell_type": "code", "outputs": [], "source": ["# The code for the stacking. It will take long time to train the model, but it works.  \n", "import numpy as np\n", "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n", "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,ExtraTreesClassifier,ExtraTreesRegressor\n", "from sklearn.model_selection import cross_val_predict, GridSearchCV, train_test_split\n", "from sklearn.cross_validation import StratifiedKFold\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.linear_model import Ridge\n", "\n", "clfs = [RandomForestClassifier(n_estimators=100, criterion='gini'),\n", "        RandomForestClassifier(n_estimators=100, criterion='entropy'),\n", "        RandomForestClassifier(n_estimators=10, criterion='gini'),\n", "        RandomForestClassifier(n_estimators=10, criterion='entropy'),\n", "        ExtraTreesClassifier(n_estimators=100, criterion='gini'),\n", "        ExtraTreesClassifier(n_estimators=100, criterion='entropy'),\n", "        ExtraTreesClassifier(n_estimators=10, criterion='gini'),\n", "        ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n", "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50),\n", "        KNeighborsClassifier(n_neighbors=5),\n", "        KNeighborsClassifier(n_neighbors=10),\n", "        GaussianNB(),\n", "        LogisticRegression()]\n", "\n", "test_X = np.vstack( np.array(test_data.apply(lambda row: feature_extraction(row), axis=1)) )\n", "X_train, X_test, Y_train, Y_test = train_test_split(train_X, train_Y, test_size = 0.3)\n", "\n", "\n", "n_folds = 5\n", "skf = list(StratifiedKFold(train_Y, n_folds))\n", "\n", "dataset_blend_train = np.zeros((train_X.shape[0], len(clfs)))\n", "dataset_blend_test = np.zeros((test_X.shape[0], len(clfs)))\n", "    \n", "for j, clf in enumerate(clfs):\n", "    print (j, clf)\n", "    dataset_blend_test_j = np.zeros((test_X.shape[0], len(skf)))\n", "    for i, (train, test) in enumerate(skf):\n", "        print (\"Fold\", i)\n", "        X_train = train_X[train]\n", "        y_train = train_Y[train]\n", "        X_test = train_X[test]\n", "        y_test = train_Y[test]\n", "        clf.fit(X_train, y_train)\n", "        y_submission = clf.predict_proba(X_test)[:, 1]\n", "        dataset_blend_train[test, j] = y_submission\n", "        dataset_blend_test_j[:, i] = clf.predict_proba(test_X)[:, 1]\n", "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "30796566cd15f0e4715df26ac405684af5aab490", "collapsed": true, "_cell_guid": "c6aecf5d-acb1-3b52-f4f5-9b7ca7ad58df"}}, {"cell_type": "markdown", "source": ["###Final Prediction\n", "After the stacking, we have new features, which can be used to make the final prediction. Here I choose XGBoost as the algorithm to make the final prediction. The process is almost similar with the previous steps. "], "metadata": {"_active": false, "_uuid": "eedd69e0b3a123c2e8803abc717cb73030203605", "_cell_guid": "ad14025c-8b4c-7549-00ee-1b2fd6f5cac5"}}, {"cell_type": "code", "outputs": [], "source": ["import xgboost as xgb\n", "from sklearn.model_selection import KFold\n", "kf = KFold(n_splits=5, shuffle=True, random_state=2016)\n", "for dev_index, val_index in kf.split(range(dataset_blend_train.shape[0])):\n", "    \n", "    params = {}\n", "    params[\"objective\"] = \"binary:logistic\"\n", "    params['eval_metric'] = 'logloss'\n", "    params[\"eta\"] = 0.3\n", "    params[\"subsample\"] = 0.7\n", "    params[\"min_child_weight\"] = 2\n", "    params[\"colsample_bytree\"] = 0.7\n", "    params[\"max_depth\"] = 5\n", "    params[\"silent\"] = 1\n", "    dev_X, val_X = dataset_blend_train[dev_index,:], dataset_blend_train[val_index,:]\n", "    dev_y, val_y = train_Y[dev_index], train_Y[val_index]\n", "    d_train = xgb.DMatrix(dev_X, label = dev_y)\n", "    d_test = xgb.DMatrix(val_X, label = val_y)\n", "    watchlist = [ (d_train,'train'), (d_test, 'test') ]\n", "    bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n", "    break\n", "\n", "# Prediction\n", "dataset_blend_test = xgb.DMatrix(dataset_blend_test)\n", "predict_y_XGB = bst.predict(dataset_blend_test)\n", "data_prediction = predict_y_XGB\n", "\n", "sub = pd.DataFrame()\n", "sub['test_id'] = df_test['test_id']\n", "sub['is_duplicate'] = data_prediction\n", "sub.to_csv(\"submission.csv\", index=False)"], "execution_count": null, "metadata": {"_active": false, "_uuid": "49d197180222ebd7bf7929b97e94364cbf70c76c", "collapsed": true, "_cell_guid": "2c3cfe2e-c917-4c2b-1fbe-4a38c2cf0cee"}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.3", "name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}}}}