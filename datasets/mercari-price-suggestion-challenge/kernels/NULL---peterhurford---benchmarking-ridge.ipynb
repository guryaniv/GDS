{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["Inspired by https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/45160, I seek to benchmark different solvers and alpha levels for the `Ridge` classifier in SKLearn.\n", "\n", "Some resources:\n", "\n", "* Read more about `Ridge`: http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\n", "* Read more about the individual solvers: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge\n", "* Here's a bit on how `alpha` works (higher `alpha` = lower model complexity = lower risk of overfitting): https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"], "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["import gc\n", "import time\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from scipy.sparse import csr_matrix, hstack\n", "\n", "from sklearn.linear_model import Ridge\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "import lightgbm as lgb\n", "\n", "NUM_BRANDS = 4000\n", "NUM_CATS = 4000\n", "NAME_MIN_DF = 10\n", "MAX_FEATURES_ITEM_DESCRIPTION = 50000\n", "\n", "\n", "def handle_missing_inplace(dataset):\n", "    dataset['category_name'].fillna(value='missing', inplace=True)\n", "    dataset['brand_name'].fillna(value='missing', inplace=True)\n", "    dataset['item_description'].fillna(value='missing', inplace=True)\n", "\n", "def cutting(dataset):\n", "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n", "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATS]\n", "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n", "\n", "def to_categorical(dataset):\n", "    dataset['category_name'] = dataset['category_name'].astype('category')\n", "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n", "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n", "\n", "start_time = time.time()\n", "train = pd.read_table('../input/train.tsv', engine='c')\n", "test = pd.read_table('../input/test.tsv', engine='c')\n", "print('[{}] Finished to load data'.format(time.time() - start_time))\n", "\n", "nrow_train = train.shape[0]\n", "y = np.log1p(train[\"price\"])\n", "merge: pd.DataFrame = pd.concat([train, test])\n", "submission: pd.DataFrame = test[['test_id']]\n", "del train\n", "del test\n", "gc.collect()\n", "\n", "handle_missing_inplace(merge)\n", "print('[{}] Finished to handle missing'.format(time.time() - start_time))\n", "cutting(merge)\n", "print('[{}] Finished to cut'.format(time.time() - start_time))\n", "to_categorical(merge)\n", "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n", "cv = CountVectorizer(min_df=NAME_MIN_DF)\n", "X_name = cv.fit_transform(merge['name'])\n", "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n", "\n", "cv = CountVectorizer()\n", "X_category = cv.fit_transform(merge['category_name'])\n", "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n", "\n", "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n", "                     ngram_range=(1, 3),\n", "                     stop_words='english')\n", "X_description = tv.fit_transform(merge['item_description'])\n", "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n", "\n", "lb = LabelBinarizer(sparse_output=True)\n", "X_brand = lb.fit_transform(merge['brand_name'])\n", "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n", "\n", "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n", "                                      sparse=True).values)\n", "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n", "\n", "sparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n", "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n", "\n", "X = sparse_merge[:nrow_train]\n", "X_test = sparse_merge[nrow_train:]"], "metadata": {"_uuid": "8620e08a140638728b18fa4b22058d34d0c89e7d", "scrolled": true, "_cell_guid": "20dd83b2-f62a-4ab6-8bed-d03bda4351de"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["# Make a 20% holdout set so that we can benchmark our implementations.\n", "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 144)\n", "# I've found that in practice that Public Leaderboard scores are about 0.003 better than holdout\n", "# scores (e.g., 0.468 in holdout will be about 0.465 on Public Leadearboard).\n", "#\n", "# I've also found locally on my laptop that the holdout score is very close to the 5-fold\n", "# CV score and that fold variance is quite low. Good for us!\n", "\n", "def rmse(predicted, actual):\n", "    return np.sqrt(((predicted - actual) ** 2).mean())\n", "\n", "def test_model(model):\n", "    start_time = time.time()\n", "    model.fit(X_train, y_train)\n", "    train_finished = time.time()\n", "    preds = model.predict(X_valid)\n", "    print('RMSLE %f, train in %.4f sec, predict in %.4f sec' % (rmse(preds, y_valid), (train_finished - start_time), (time.time() - train_finished)))\n", "\n", "for solver in ['lsqr', 'sparse_cg', 'sag', 'saga']:\n", "    print('Solver =', solver)\n", "    intercept = True if solver == 'sag' else False # Sklearn says only sag can fit intercept. Maybe saga can too?\n", "    for alpha in [0.01, 0.03, 0.05, 0.1, 0.5, 1, 3, 5, 10]:\n", "        print('Alpha =', alpha)\n", "        model = Ridge(alpha=alpha, copy_X=True, fit_intercept=intercept, max_iter=100,\n", "                      normalize=False, random_state=101, solver=solver, tol=0.01)\n", "        test_model(model)"], "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["# To be more precise, focus on the best alphas with lower tolerance and no iteration cap.\n", "\n", "# Though it looks like the same relationship between alpha and score does not hold, and that\n", "# scores are not actually guaranteed to be better!\n", "\n", "for solver in ['lsqr', 'sag', 'saga']:\n", "    print('Solver =', solver)\n", "    intercept = True if solver == 'sag' else False\n", "    for alpha in [1, 3, 5]:\n", "        print('Alpha =', alpha)\n", "        model = Ridge(alpha=alpha, copy_X=True, fit_intercept=intercept, max_iter=None,\n", "                      normalize=False, random_state=101, solver=solver, tol=0.001)\n", "        test_model(model)\n", "    \n", "# Room for further improvement is left as an excercise for the reader. :D"], "metadata": {}}], "metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}}