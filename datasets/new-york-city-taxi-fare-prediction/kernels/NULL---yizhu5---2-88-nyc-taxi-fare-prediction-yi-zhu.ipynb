{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1f52ed61741554b2bd261496e734d399592253f"},"cell_type":"markdown","source":"**Import the training and testing data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import time\n# import training data\nstart_time = time.time() # track time\ntrain_df =  pd.read_csv('../input/train.csv', nrows=15000000, usecols = [1,2,3,4,5,6,7]) # the first column 'key' is not useful \nprint(\"%s seconds\" % (time.time() - start_time))\n\n# insert testing data\nstart_time = time.time() # track time\ntest_df =  pd.read_csv('../input/test.csv', usecols = [1,2,3,4,5,6]) # the first column 'key' is not useful\nprint(\"%s seconds\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"097f4d7ca02c289bda14104cd9945c044a1856c7"},"cell_type":"markdown","source":"> **Compare the train and test data set**"},{"metadata":{"trusted":true,"_uuid":"b2fe8aa65b46a5659304cc62555cf063232dacc3"},"cell_type":"code","source":"# check the shape \nprint (test_df.shape)\nprint (train_df.shape)\n\n# check the head\nprint (test_df.head())\nprint (train_df.head())\n\n# as expected the test data lack one column --- 'fare_amount', which is what we are going to predict --- comparing with train data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd8e7bbca65cff5922335ec0cd8c266e42d0f788"},"cell_type":"markdown","source":"**Explore the test data** \n\nIt is very important to explore the test data before the train data, as we can decide the cleaning metric for the train data base on the data range in the test data.[](http://) "},{"metadata":{"trusted":true,"_uuid":"2804690bd1c15496af7a75f3dc820dcc49a9ec0d"},"cell_type":"code","source":"#check null value\nprint(test_df.isnull().sum())\n#check zero value\nprint((test_df == 0).astype(int).sum(axis=0))\n# the test data is very clean, with no null value or zero value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"849545c2713c0df4ac0677f291ce77ac196dd9cd"},"cell_type":"code","source":"# check description\ntest_df.describe()\n# By checking the description of test data, we can see the min and max value of each feature, \n# so we can choose the clean the train data base on these value. In other word, we can delete \n# the values that are out of these boundaries in the train data, as they are using in training \n# the model for prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68e987ca269002b7884143ce8093cbacd67ef76e"},"cell_type":"markdown","source":"**Explore the train data**"},{"metadata":{"trusted":true,"_uuid":"22cf26acc27e313904ac76cff742c76840fc1012"},"cell_type":"code","source":"#check null value \nprint(train_df.isnull().sum())\n#check zero value \nprint((train_df == 0).astype(int).sum(axis=0))\n# There are some null and zero values in the train data. This step is very import, \n# as these values can influence the training result significantly\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a3d1cb7ba6a81f4cfeab2fa8cd78d67d583afcd"},"cell_type":"code","source":"# check description\ntrain_df.describe()\n# There are some values that are apparently to be wrong. E.g. the min of fare_amount is negative, but it can't be.\n# The max value of passenger count is 208, which is too exagerating. We have to delete this values. But it doesn't \n# matter, we will delete the useless value base on the value boundary in the test data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c174696f538185b958ff311588cc6336261af66f"},"cell_type":"code","source":"# One problem is the test data don't have the 'fare_amount' column, so we don't have a boundary for cleaning.\n# Therefore, we want to use the visulization to see the data distribution of this paticular column value.\nimport matplotlib.pyplot as plt\ntrain_df.fare_amount.hist(bins=100, figsize = (16,8))\nplt.xlabel(\"Fare Amount\")\nplt.ylabel(\"Frequency\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adea5eadd8d9e267ce36c1d5ce57ffb76c0bb39e"},"cell_type":"code","source":"# Check quantile\ntrain_df[['fare_amount']].quantile([0.001, 0.999])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4a900a39c206ed474bbb5e5832015889c3f2e4a8"},"cell_type":"code","source":"# Most the data are distributed below 200, so check again the data distribution below 200\ntrain_df[train_df.fare_amount <200 ].fare_amount.hist(bins=100, figsize = (16,8))\nplt.xlabel(\"Fare Amount\")\nplt.ylabel(\"Frequency\")\n# Boundary between 0 and 200 could be a good choice\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc8ce1a89349ba25d71e304874189e1ed67197ad"},"cell_type":"markdown","source":"**Data cleaning**"},{"metadata":{"trusted":true,"_uuid":"b15ad682e3f698f5b59cadb19383cac35ef6c85d"},"cell_type":"code","source":"# Delete null value\nprint(\"old: %d\" %len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint(\"new: %d\" %len(train_df)) # track data amount before and after deletion\n\n# Delete zero value\nprint(\"old: %d\" %len(train_df))\ntrain_df = train_df[~(train_df == 0).any(axis=1)]\nprint(\"new: %d\" %len(train_df)) # track data amount before and after deletion\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"877db9dd06ccf1a4e3949927a43b9661d1beaaa6"},"cell_type":"code","source":"# Remove observations with useless values base on the test data boundary\nmask = train_df['pickup_longitude'].between(-74.3, -72.9)\nmask &= train_df['dropoff_longitude'].between(-74.3, -72.9)\nmask &= train_df['pickup_latitude'].between(40.5, 41.8)\nmask &= train_df['dropoff_latitude'].between(40.5, 41.7)\nmask &= train_df['passenger_count'].between(0, 6)\nmask &= train_df['fare_amount'].between(2, 200)\n\n\nprint(\"old: %d\" %len(train_df))\ntrain_df = train_df[mask]\nprint(\"new: %d\" %len(train_df)) # track data amount before and after deletion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae64f548921da496f0619a8794200274e04633bf"},"cell_type":"code","source":"# Strip the 'pickup_datetime' column\nimport datetime as dt\ndef convert_to_datetime(df):\n    test_time = df['pickup_datetime'].astype(str).str[:-4]\n    df['date_time'] =  pd.to_datetime(test_time, format='%Y%m%d %H:%M:%S')\n    return df \n\n# Apply to both train and test data   \ntrain_df = convert_to_datetime(train_df)\ntest_df = convert_to_datetime(test_df)\n\n# Chek shape\nprint (test_df.shape)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c1ae7b2bd4f26a94cd33756011752800c4f946d","scrolled":true},"cell_type":"code","source":"# Extract date attributes and then drop the pickup_datetime column\ndef extract_date(data):\n    data['hour'] = data['date_time'].dt.hour\n    data['day'] = data['date_time'].dt.day\n    data['month'] = data['date_time'].dt.month\n    data['year'] = data['date_time'].dt.year\n    data['weekday'] = data['date_time'].dt.weekday\n    data = data.drop(['date_time','pickup_datetime'], axis=1)\n    return data\n\n# Apply to both train and test data      \ntrain_df = extract_date(train_df)\ntest_df = extract_date(test_df)\n\n# Chek shape\nprint (test_df.shape)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2207e0e6e42641226014ed9179ac13abf6481912"},"cell_type":"markdown","source":" **Feature Engineering**\n\nAdd more features that are likely to be helpful for predicting the fare amount\n"},{"metadata":{"trusted":true,"_uuid":"8148f29d69ade65faa6243b058e7fa1cc0b4da62"},"cell_type":"code","source":"# A trick for this step is we have to refer to the fare computing machanism for NYC taxi. \n# I referred to this website and apply each computing criteria to the engineering. \n# http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml\n\n# There is no doubt that the factor that affects the fare amout the most is the distance of\n# the trip, so we have the define the metric for computing distance first.\n# Here is the means for computing earth surface distace base on two points' longitude and latitude.\n\n# Define distance\ndef dist(pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude):\n    pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude = map(np.radians, [pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude])\n    dlon = dropoff_longitude - pickup_longitude\n    dlat = dropoff_latitude - pickup_latitude\n    a = np.sin(dlat/2.0)**2 + np.cos(pickup_latitude) * np.cos(dropoff_latitude) * np.sin(dlon/2.0)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    distance = 6367 * c\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad8d09281f2644caaccd93568546ba9bc8059d53"},"cell_type":"code","source":"# There are extra charges if trip ends in 3 nearby aiports and 7 nearby counties from the NYC center,\n# so these location points and there distances to pickup and dropoff points are key factors \n\n# Distances to nearby city center, airports, and other ny counties\ndef transform(data):\n    # Distances to nearby airports, city center and other counties\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    \n    # city center\n    nyc = (-74.0060, 40.7128)\n    \n    # county\n    Nassau = (-73.5594, 40.6546)\n    Suffolk = (-72.6151, 40.9849)\n    Westchester = (-73.7949, 41.1220)\n    Rockland = (-73.9830, 41.1489)\n    Dutchess = (-73.7478, 41.7784)\n    Orange = (-74.3118, 41.3912)\n    Putnam = (-73.7949, 41.4351) \n\n    # airport\n    jfk = (-73.7781, 40.6413)\n    ewr = (-74.1745, 40.6895)\n    lgr = (-73.8740, 40.7769)\n    \n    \n    # county\n    data['pickup_distance_to_center'] = dist(nyc[0], nyc[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_center'] = dist(nyc[0], nyc[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Nassau'] = dist(Nassau[0], Nassau[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Nassau'] = dist(Nassau[0], Nassau[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Suffolk'] = dist(Suffolk[0], Suffolk[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Suffolk'] = dist(Suffolk[0], Suffolk[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Westchester'] = dist(Westchester[0], Westchester[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Westchester'] = dist(Westchester[0], Westchester[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Rockland'] = dist(Rockland[0], Rockland[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Rockland'] = dist(Rockland[0], Rockland[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Dutchess'] = dist(Dutchess[0], Dutchess[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Dutchess'] = dist(Dutchess[0], Dutchess[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Orange'] = dist(Orange[0], Orange[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Orange'] = dist(Orange[0], Orange[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_Putnam'] = dist(Putnam[0], Putnam[1],\n                                      data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_Putnam'] = dist(Putnam[0], Putnam[1],\n                                      data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    # airports\n    data['pickup_distance_to_jfk'] = dist(jfk[0], jfk[1],\n                                         data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[0], jfk[1],\n                                           data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_ewr'] = dist(ewr[0], ewr[1], \n                                          data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_ewr'] = dist(ewr[0], ewr[1],\n                                           data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    data['pickup_distance_to_lgr'] = dist(lgr[0], lgr[1],\n                                          data['pickup_longitude'], data['pickup_latitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[0], lgr[1],\n                                           data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    # point distance\n    data['distance'] = dist(data['pickup_longitude'], data['pickup_latitude'],\n                            data['dropoff_longitude'], data['dropoff_latitude'])\n    \n    return data\n\n# Apply to both train and test data      \ntrain_df = transform(train_df)\ntest_df = transform(test_df)\n\n# Chek shape\nprint (test_df.shape)\nprint (train_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63c6707b0602dc5f5b706e56483be3bb99c43886"},"cell_type":"code","source":"# Consider extra charges\ndef final_convert(df):\n\n    # There is a 50-cent MTA State Surcharge for all trips that end in New York City or \n    # Nassau, Suffolk, Westchester, Rockland, Dutchess, Orange or Putnam Counties.\n    # The following two variables can be merged into one.\n    # The following only considers trips that starts in city center and ends in nearby counties,\n    # while the opposite direction could also be considered\n    # counties\n    df['county_dropoff_1'] = np.where((df['pickup_distance_to_center'] <= 5) &\n                                     ((df['dropoff_distance_to_Nassau'] <= 21.3) |\n                                      (df['dropoff_distance_to_Westchester'] <= 22.4)), 1, 0)\n    \n    df['county_dropoff_2'] = np.where((df['pickup_distance_to_center'] <= 5) &                  \n                                     ((df['dropoff_distance_to_Suffolk'] <= 48.7) |           \n                                      (df['dropoff_distance_to_Rockland'] <= 14.1) |\n                                      (df['dropoff_distance_to_Dutchess'] <= 28.7) |\n                                      (df['dropoff_distance_to_Orange'] <= 29) |\n                                      (df['dropoff_distance_to_Putnam'] <= 15.7)), 1, 0)\n    \n    # There is a daily 50-cent surcharge from 8pm to 6am.\n    df['night_hour'] = np.where((df['hour'] >= 20) |\n                                (df['hour'] <= 6) , 1, 0)\n    \n    # There is a $1 surcharge from 4pm to 8pm on weekdays, excluding holidays.\n    df['peak_hour'] = np.where((df['hour'] >= 16) &\n                                (df['hour'] <= 20) & \n                                (df['weekday'] >=0) &\n                                (df['weekday'] <=4) , 1, 0)\n    \n    # This is a flat fare of $52 plus tolls, the 50-cent MTA State Surcharge, the 30-cent Improvement Surcharge, \n    # to/from JFK and any location in Manhattan:\n    df['to_from_jfk'] = np.where(((df['pickup_distance_to_jfk'] <= 2) & (df['dropoff_distance_to_center'] <= 5)) | \n                                 ((df['pickup_distance_to_center'] <= 5) & (df['dropoff_distance_to_jfk'] <= 2)) ,1, 0)\n\n    # There is a $4.50 rush hour surcharge (4 PM to 8 PM weekdays, excluding legal holidays). o/from JFK and any location in Manhattan:\n    df['jfk_rush_hour'] = np.where((df['to_from_jfk'] == 1) & \n                                   (df['hour'] >= 16) &\n                                   (df['hour'] <= 20) ,1, 0)\n    \n    # There is a $17.50 Newark Surcharge to Newark Airport:\n    df['ewr'] = np.where((df['pickup_distance_to_center'] <= 5) &\n                         (df['dropoff_distance_to_ewr'] <= 1) ,1, 0)\n    \n    return df\n\n\n# Apply to both train and test data      \ntrain_df = final_convert(train_df)\ntest_df = final_convert(test_df)\n\n# Chek shape\nprint (test_df.shape)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c2175c7c268fcbaa4f870459d276ef683bf037c"},"cell_type":"code","source":"# Check column names\nprint(train_df.columns)\n# Check corr of 'fare_amount' to all the other variables\nprint(train_df.corrwith(train_df['fare_amount']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60d5319501e29f7e052160b58a93f256b52da701"},"cell_type":"code","source":"# We can choose to remove the variables that has the corr less than 0.1\ntrain_df = train_df.drop(['passenger_count','hour','day','month','weekday'], axis = 1)\ntest_df = test_df.drop(['passenger_count','hour','day','month','weekday'], axis = 1)\n\n# Chek shape\nprint (test_df.shape)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96c683a6415f6e683a391a758a266e90e15e4cc0"},"cell_type":"markdown","source":"**Prediction**"},{"metadata":{"trusted":true,"_uuid":"23698096f077089afde53f4fbd6a5b0f26b5c5b1"},"cell_type":"code","source":"# Split the train data for model training\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(train_df.drop('fare_amount', axis=1),\n                                                    train_df['fare_amount'], test_size=0.15, random_state = 111)\n\n# Check shape\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67c5e48091f6c05fc40b268a7228e3c3884da315"},"cell_type":"code","source":"# Use lightgbm model to do the training. Need to install the lightgbm package first\n# An instruction for installing can be found here:\n# https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html#windows\n\nimport lightgbm as lgbm\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,\n        'num_rounds':50000\n    }\n\n\ndef LGBMmodel(X_train,X_test,y_train,y_test,params):\n    matrix_train = lgbm.Dataset(X_train, y_train)\n    matrix_test = lgbm.Dataset(X_test, y_test)\n    model=lgbm.train(params=params,\n                    train_set=matrix_train,\n                    num_boost_round=100000, \n                    early_stopping_rounds=500,\n                    verbose_eval=100,\n                    valid_sets=matrix_test)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"444e34e68cc751cf6c3b4afdcc73bac7913c99fe"},"cell_type":"code","source":"# Train the model\n\n# model = LGBMmodel(X_train,X_test,y_train,y_test,params)\n\n# Training RMSE best iteration is 3.25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d687751e803df4c1e7b0be7faad34845cb9e6cec"},"cell_type":"code","source":"# Predicte the 'fare_amount' and save file\n\n# prediction = model.predict(test_df, num_iteration = model.best_iteration) \n \n# submission = pd.DataFrame(\n#     {'key': test_df_initial.key, 'fare_amount': prediction},\n#     columns = ['key', 'fare_amount'])\n# submission.to_csv('xxx', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b7af11ab01581c1677e3803a56d6cb823793c74"},"cell_type":"markdown","source":"The final RMSE result for prediction is** 2.88**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}