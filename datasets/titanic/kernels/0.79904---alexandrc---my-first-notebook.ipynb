{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e2f595b-edaf-40b5-050f-965ca860ee30",
        "_active": false
      },
      "source": "#Test\nHere I try to work out some ideas of preparing data for classifiers, use different classifiers on Titanic Dataset and do some simple research in order to find the best of them.",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "1b01b053-6ceb-cafe-a45d-8160c97900a3",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output \n# Any results you write to the current directory are saved as output.",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "197bfbce-3cfc-b8fb-b048-80bf12f924b2",
        "_active": false
      },
      "outputs": [],
      "source": "# Load data\ndata_train = pd.read_csv(\"../input/train.csv\")\ndata_test = pd.read_csv(\"../input/test.csv\")\n\ny_train = data_train['Survived']\nX_train = data_train.drop(labels=[\"Survived\"], axis=1)\nX_test = data_test\nn_train = X_train.shape[0]\nn_test = X_test.shape[0]\nprint (\"train size\", n_train)\nprint (\"test_size\", n_test)\n\nX_train.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "f573debb-5366-d40a-aabb-5ba340cfca35",
        "_active": false
      },
      "outputs": [],
      "source": "X_all = X_train.append(X_test, ignore_index=True)\nprint (X_all.info())",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "dfc81768-ec5b-248e-d66d-20e1927db3d6",
        "_active": false
      },
      "outputs": [],
      "source": "# Name, Ticket and Cabin are decided to be useless. Drop them\nX_train = X_train.drop(labels=[\"Name\", \"Ticket\", \"Cabin\"], axis=1)\nX_test = X_test.drop(labels=[\"Name\", \"Ticket\", \"Cabin\"], axis=1)\nX_all = X_all.drop(labels=[\"Name\", \"Ticket\", \"Cabin\"], axis=1)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "74b454dc-c28e-6c9d-07aa-2cdf617b3e6f",
        "_active": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Methods of filling NAN values\n# ------------------------------------------------------------------------\ndef my_fillna(X, method):\n    if method==\"bfill\":\n        res = X.fillna(method=\"bfill\")\n        res = res.fillna(method=\"ffill\")  # if last one is nan\n    elif method==\"ffill\":\n        res = X.fillna(method=\"ffill\")\n        res = res.fillna(method=\"bfill\")  # if first one is nan\n    elif method==\"zero\":\n        res = X.fillna({\"Age\":0.0, \"Fare\":0.0, \"Embarked\":\"<NAN>\"})\n    elif method==\"avg\":\n        res = X.fillna({\"Embarked\":\"<NAN>\"})\n        age_avg = res['Age'].mean()\n        fare_avg = res['Fare'].mean()\n        res = res.fillna({\"Age\":age_avg, \"Fare\":fare_avg})\n    elif method==\"drop\":\n        res = X.dropna()\n    return res   ",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_cell_guid": "7ae5d07b-2ee5-c58c-9d4d-f536b8b6baa9",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Convert all categorical features to numeric/real\n# ------------------------------------------------------------------------\nfrom sklearn.feature_extraction import DictVectorizer as DV\n\ndef encode_cat(X):\n    encoder = DV(sparse = False)\n    X_cat = encoder.fit_transform(X.T.to_dict().values())\n    return X_cat\n    \ndef encode_cat_test(X):\n    print('\\nSource data:\\n')\n    print(X.shape)\n    print(X[:10])\n    encoder = DV(sparse = False)\n    X_cat = encoder.fit_transform(X.T.to_dict().values())\n    print('\\nEncoded data:\\n')\n    print(X_cat.shape)\n    print(X_cat[:10])\n    print('\\nVocabulary:\\n')\n    print(encoder.vocabulary_)\n    print(encoder.feature_names_)\n    return X_cat",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "0a734313-5115-099f-621b-03bba1bfaa7d",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "zzz = encode_cat_test(my_fillna(X_train,\"bfill\"))\n\n#make list for non-cat column of features (manually, for a while)\nreal_ind_list_A = [0,5,6,7,8,11]  # if fillna=\"avg\" or \"zero\"\nreal_ind_list_B = [0,4,5,6,7,10]  # if fillna=\"bfill\" or \"ffill\"",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "5d148644-df2a-a6f0-c53c-b2b62300b5b3",
        "_active": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Feature scaling\n# ------------------------------------------------------------------------\nfrom sklearn.preprocessing import StandardScaler\n\n#create standard scaler\ndef scale_features(X_tr, X_tt, real_ind_list, y):\n    X_tr_scaled = np.array(X_tr)\n    X_tt_scaled = np.array(X_tt)\n    \n    X_tr_real = X_tr[:,real_ind_list]\n    X_tt_real = X_tt[:,real_ind_list]\n    \n    scaler = StandardScaler()    \n    scaler.fit(X_tr_real, y)  # set scaled parameters relatively to train data\n    \n    X_tr_scaled[:,real_ind_list] = scaler.transform(X_tr_real)\n    X_tt_scaled[:,real_ind_list] = scaler.transform(X_tt_real)  \n    \n    return X_tr_scaled, X_tt_scaled",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "2782e52a-1573-1e93-a83e-f120d34503d5",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Implementation of \"greedy\" algorithm for feature selection \n# ------------------------------------------------------------------------\nfrom sklearn.cross_validation import cross_val_score\n\ndef reduce_features(X, y, cls):    \n    n_features = X.shape[1]\n    list_features_ind = [] #initial\n    max_prev_score = 0.0\n    list_iter_score = []    \n    flag_stop = False\n    flag_del = False\n    n_iter = 0\n    \n    while flag_stop == False:\n        \n        n_iter = n_iter + 1\n        \n        # try to add one feature\n        res_score = np.zeros(n_features)        \n        for i in range(n_features):\n            if i in list_features_ind:\n                pass\n            else:\n                list_tmp_ind = list(list_features_ind)\n                list_tmp_ind.append(i)\n                X_tmp = X[:,list_tmp_ind]\n                res_cv = cross_val_score(cls, X_tmp, y)    # cross validation score\n                res_score[i] = res_cv.mean()\n                \n        #print n_iter, \"add\", res_score\n        max_ind = np.argmax(res_score)\n        max_val = res_score[max_ind]\n        #print max_ind, max_val\n        if max_val > max_prev_score:\n            list_features_ind.append(max_ind)\n            max_prev_score = max_val\n            list_iter_score.append(max_val)\n            flag_del = False\n        else:            \n            flag_del = True        \n            \n        # if adding one feature wasn't effective, try to delete one chosen feature\n        if flag_del == True:\n            if len(list_features_ind) <= 1:\n                flag_stop = True\n                break\n                \n            res_score = np.zeros(n_features)        \n            for i in range(n_features):\n                if i in list_features_ind:\n                    list_tmp_ind = list(list_features_ind)\n                    list_tmp_ind.remove(i)\n                    X_tmp = X[:,list_tmp_ind]\n                    res_cv = cross_val_score(cls, X_tmp, y)   # cross validation score\n                    res_score[i] = res_cv.mean()                        \n\n            #print n_iter, \"del\", res_score\n            max_ind = np.argmax(res_score)\n            max_val = res_score[max_ind]\n            #print max_ind, max_val\n            if max_val > max_prev_score:\n                list_features_ind.remove(max_ind)\n                max_prev_score = max_val\n                list_iter_score.append(max_val)\n            else:              \n                flag_stop = True            \n                break       \n\n    return list_features_ind, list_iter_score                 ",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_cell_guid": "4ae2c1e8-59af-a5f9-233e-9f7ef58f5303",
        "_active": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Reduce number of features using PCA\n# ------------------------------------------------------------------------\nfrom sklearn.decomposition import PCA\ndef reduce_features_PCA(X, y):\n    mdl = PCA()\n    mdl.fit(X)\n    print (mdl.explained_variance_ratio_)\n    #TODO:\n    # define the optimal number of principal components\n    # do some tests for three suitable numbers via 'cross_val_score'    \n    # return reduced_dataset, list of remained features, and list of dropped features",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_cell_guid": "56a51c93-f7d6-4d36-6842-3ef74bf8b42e",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# Classifier initialization\n# ------------------------------------------------------------------------\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import RandomForestClassifier as RandomForest\nfrom sklearn.ensemble import BaggingClassifier as Bagging\nfrom sklearn.tree import DecisionTreeClassifier as DecisionTree\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neural_network import MLPClassifier as MLP\n\ndef get_classifier(cls, param):\n    if cls==\"LR\":\n        return LR(C=param, random_state=123)\n    elif cls==\"KNN\":\n        return KNN(n_neighbors=param)\n    elif cls==\"RForest\":\n        return RandomForest(n_estimators=75, max_depth=param, random_state=123)\n    elif cls==\"BagTree\":\n        return Bagging(base_estimator=DecisionTree(max_depth=param, random_state=123), random_state=123)\n    elif cls==\"Perceptron\":    \n        return Perceptron(eta0=param, random_state=123)\n    elif cls==\"MLP\":\n        return MLP(hidden_layer_sizes=(20,), alpha=param, max_iter=40, solver='lbfgs') #too slow\n    else:\n        pass",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_cell_guid": "fe922508-6f27-2bca-65c5-30a9c66d87c9",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "#----------------------------------------------------------------------\n# Main cell: Research !\n#----------------------------------------------------------------------\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nresearch_list = [     \n{    \n    'classifier' : \"LR\",    \n    'param_values' : [0.07,0.1,0.13,0.17,0.2],\n    'param_name' : \"C\" \n},\n{    \n    'classifier' : \"KNN\",    \n    'param_values' : [1,2,3,4,5],\n    'param_name' : \"K\" \n},    \n{    \n    'classifier' : \"RForest\",    \n    'param_values' : [3,4,5,7,10],\n    'param_name' : \"max_depth\" \n},    \n{    \n    'classifier' : \"BagTree\",    \n    'param_values' : [3,4,5,7,10],\n    'param_name' : \"max_depth\" \n},\n{    \n    'classifier' : \"Perceptron\",    \n    'param_values' : [0.001,0.01,0.1,1],\n    'param_name' : \"eta0\" \n}]\n\nfillna_meth_list = [\"bfill\", \"ffill\", \"zero\", \"avg\"]\n\nview_log = False\nscaling = True\n\nfor item in research_list:\n    \n    cls_max_score = 0.0\n    cls_max_ind = []\n    cls_max_param = 0.0\n\n    for meth in fillna_meth_list:    \n        X_train_na = my_fillna(X_train, meth)\n        X_test_na = my_fillna(X_test, meth)\n        X_all_na = X_train_na.append(X_test_na, ignore_index=True)\n        X_all_cat = encode_cat(X_all_na)         \n        X_train_cat = X_all_cat[:n_train]\n        X_test_cat = X_all_cat[n_train:] \n\n        if scaling == True:\n            if meth==\"zero\" or meth==\"avg\":\n                X_train_cat, X_test_cat = scale_features(X_train_cat, X_test_cat, y_train, real_ind_list_A)\n            else:\n                X_train_cat, X_test_cat = scale_features(X_train_cat, X_test_cat, y_train, real_ind_list_B)        \n\n        for pval in item['param_values']:\n            cls = get_classifier(item['classifier'], pval)\n            l_ind, l_scores = reduce_features(X_train_cat, y_train, cls)\n            if view_log == True:\n                print (\"---\", meth, \",\", item['param_name'], \"=\", pval, \"---\")\n                print (l_ind)\n                print (l_scores[-1:][0])\n            if cls_max_score <= max(l_scores):\n                cls_max_meth = meth\n                cls_max_score = l_scores[-1:][0]  # last one must be the best\n                cls_max_ind = list(l_ind)\n                cls_max_param = pval\n\n    print (\"---\", item['classifier'], \":\", cls_max_meth, \",\", item['param_name'], \"=\", cls_max_param, \"---\")\n    print (\"features: \", cls_max_ind)\n    print (\"score: \", cls_max_score)  # <- cross_val_score().mean",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_cell_guid": "495e7e3b-f2a0-ac16-86a2-58d58207f643",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "#----------------------------------------------------------------------\n# BONUS: Visualization of dataset\n#----------------------------------------------------------------------\nfrom sklearn.manifold import TSNE, MDS\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.preprocessing import scale\n%matplotlib inline\n\n# prepare data\nX_train_na = my_fillna(X_train, 'avg')\nX_test_na = my_fillna(X_test, 'avg')\nX_all_na = X_train_na.append(X_test_na, ignore_index=True)\nX_all_cat = encode_cat(X_all_na)         \nX_train_cat = X_all_cat[:n_train]\nX_test_cat = X_all_cat[n_train:]\n\nX_train_cat, X_test_cat = scale_features(X_train_cat, X_test_cat, y_train, real_ind_list_A) #avg\n\n# calculate projection on 2 dimensions using three different algorithms\nX_tsne_view = TSNE().fit_transform(X_train_cat)\nX_mds_view = MDS().fit_transform(X_train_cat)\nX_pca_view = PCA(n_components=2).fit_transform(X_train_cat)\n\n# draw plots\nfig = plt.figure(figsize=(12, 5))\nplt.subplot(131)\nfor response, color in zip([0,1],['red', 'blue']):\n    plt.scatter(X_tsne_view[y_train.values==response, 0], \n                X_tsne_view[y_train.values==response, 1], c=color, alpha=1)\nplt.legend([\"died\",\"surv.\"])\nplt.xlabel(\"t-NSE algoritm\")\n    \nplt.plot()\nplt.subplot(132)\nfor response, color in zip([0,1],['red', 'blue']):\n    plt.scatter(X_mds_view[y_train.values==response, 0], \n                X_mds_view[y_train.values==response, 1], c=color, alpha=1)    \nplt.legend([\"died\",\"surv.\"])    \nplt.xlabel(\"MDS algorithm (metric=cos)\")\n\nplt.plot()\nplt.subplot(133)\nfor response, color in zip([0,1],['red', 'blue']):\n    plt.scatter(X_pca_view[y_train.values==response, 0], \n                X_pca_view[y_train.values==response, 1], c=color, alpha=1)    \nplt.legend([\"died\",\"surv.\"])    \nplt.xlabel(\"PCA method (n=2)\")",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "_cell_guid": "91267cf5-02dd-56a0-f4ea-bb2af516c5b3",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# use the best scored model and calculate prediction for y_test (submission)\nmeth = \"avg\"                          # method of filling nan values\nfeature_list = [9, 11, 0, 8, 5, 4]    # used encoded features\ncls = get_classifier(\"RForest\", 7)    # used classifier \n\n# prepare data with chosen params\nX_train_na = my_fillna(X_train, meth)\nX_test_na = my_fillna(X_test, meth)\nX_all_na = X_train_na.append(X_test_na, ignore_index=True)\nX_all_cat = encode_cat(X_all_na)         \nX_train_cat = X_all_cat[:n_train]\nX_test_cat = X_all_cat[n_train:]\n\nX_train_cat, X_test_cat = scale_features(X_train_cat, X_test_cat, y_train, real_ind_list_A) #avg\n\n# use chosen classifier\ncls.fit(X_train_cat[:, feature_list], y_train)\ny_test = cls.predict(X_test_cat[:, feature_list])\nprint(\"cross_val_score\", cross_val_score(cls, X_train_cat[:, feature_list], y_train).mean())\nprint(\"train score\", cls.score(X_train_cat[:, feature_list], y_train))",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "_cell_guid": "76b455c8-98bd-b032-0a10-07e13ab1a5dc",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# write submission to file\nfout = open(\"answer_1.csv\", \"w\")\ni = 891\nfout.write(\"PassengerId,Survived\\n\")\nfor y in y_test:\n    i = i + 1\n    fout.write(str(i)+\",\"+str(y)+\"\\n\")\nfout.close()",
      "execution_state": "idle"
    }
  ]
}