{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8ff9901d-97f0-cdbc-fda2-bd583869ad71"
      },
      "source": [
        "**Import dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad51bff4-3832-d2e2-5649-f8fed94ada4d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.cross_validation import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "84335cfc-17f9-d23f-9bd9-552684a8672a"
      },
      "source": [
        "**Function to clean the data set and get X and Y. X is hyper parameters, Y is last column in csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "427b25a0-13bd-157d-ec66-65c061054631"
      },
      "outputs": [],
      "source": [
        "def get_data(data_location, split_dataset):\n",
        "    dataset = pd.read_csv(data_location)\n",
        "\n",
        "    # 0 shape to get total of rows, 1 to get total of columns\n",
        "    rows = dataset.shape[0]\n",
        "    print (\"there are \", rows, \" rows before cleaning\\n\")\n",
        "\n",
        "    # removing unimportant columns\n",
        "    columns = ['ID']\n",
        "    for text in columns:\n",
        "        del dataset[text]\n",
        "\n",
        "    # get all data except last column\n",
        "    x = dataset.ix[: , :-1].values\n",
        "\n",
        "    # get all data on last column only\n",
        "    y = dataset.ix[: , -1:].values\n",
        "\n",
        "    # split our dataset to reduce overfitting\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = split_dataset)\n",
        "    \n",
        "    return x_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93332ce4-33bb-ddfd-56d1-4168204d8e16"
      },
      "source": [
        "**Function to return one-hot-label our Y for softmax cross entropy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27e4ac66-5ea3-24a6-4f7c-d2cd4ffb313d"
      },
      "outputs": [],
      "source": [
        "def return_embedded(x):\n",
        "\n",
        "    data = np.zeros((x.shape[0], np.unique(x).shape[0]), dtype = np.float32)\n",
        "    \n",
        "    for i in range(x.shape[0]):\n",
        "        data[i][x[i][0]] = 1.0\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dff3fcbf-8edf-9319-ba32-8a5e069ccfda"
      },
      "source": [
        "**Our global variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38811c3c-4fad-dbda-8c45-ec4876811fe4"
      },
      "outputs": [],
      "source": [
        "data_location = '../input/UCI_Credit_Card.csv'\n",
        "\n",
        "# not included input and output layer\n",
        "num_layers = 5\n",
        "# all hidden layers are same wide size\n",
        "size_layer = 64\n",
        "learning_rate = 0.01\n",
        "# batch mini size for training\n",
        "batch_size = 100\n",
        "\n",
        "# beta for regularizer, learn from penalty value\n",
        "beta = 0.05\n",
        "\n",
        "# probability to disconnect connection between nodes\n",
        "prob_dropout = 1.0\n",
        "\n",
        "biased_node = True\n",
        "\n",
        "split_dataset = 0.7\n",
        "\n",
        "# iteration for training\n",
        "epoch = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "92b6df36-7dcc-05e7-b1cc-fee4e919428e"
      },
      "source": [
        "**You can choose what type of activation function, I prefer RELu because it does not have upper boundary, and to put penalty later**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a2a3e567-7c9d-751f-5a26-64b6aff1fa48"
      },
      "outputs": [],
      "source": [
        "# got sigmoid, softmax, tanh\n",
        "activation = 'relu'\n",
        "\n",
        "if activation == 'sigmoid':\n",
        "    activation = tf.nn.sigmoid\n",
        "elif activation == 'tanh':\n",
        "    activation = tf.nn.tanh\n",
        "elif activation == 'relu':\n",
        "    activation = tf.nn.relu\n",
        "else:\n",
        "    raise Exception(\"model type not supported\")\n",
        "    \n",
        "x_train, x_test, y_train, y_test = get_data(data_location, split_dataset)\n",
        "\n",
        "y_train = return_embedded(y_train)\n",
        "y_test = return_embedded(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ba705cc7-26b9-d089-1241-5edf921cca0e"
      },
      "source": [
        "**Our Deep Dynamic Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e0b7720-4928-f66a-79cf-73942e501546"
      },
      "outputs": [],
      "source": [
        "# Neural Network pipelining ===========================================================================\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, x_train.shape[1]])\n",
        "Y = tf.placeholder(\"float\", [None, y_train.shape[1]])\n",
        "        \n",
        "input_layer = tf.Variable(tf.random_normal([x_train.shape[1], size_layer]))\n",
        "\n",
        "if biased_node:\n",
        "    biased_input_layer = tf.Variable(tf.random_normal([size_layer]))\n",
        "    biased = []\n",
        "    for i in range(num_layers):\n",
        "        biased.append(tf.Variable(tf.random_normal([size_layer])))\n",
        "\n",
        "layers = []\n",
        "for i in range(num_layers):\n",
        "    layers.append(tf.Variable(tf.random_normal([size_layer, size_layer])))\n",
        "\n",
        "output_layer = tf.Variable(tf.random_normal([size_layer, y_train.shape[1]]))\n",
        "\n",
        "if biased_node:\n",
        "    first_l = activation(tf.add(tf.matmul(X, input_layer), biased_input_layer))\n",
        "    \n",
        "    # reduce nodes connection\n",
        "    first_l = tf.nn.dropout(first_l, prob_dropout)\n",
        "    \n",
        "    next_l = activation(tf.add(tf.matmul(first_l, layers[0]), biased[0]))\n",
        "    # reduce nodes connection\n",
        "    next_l = tf.nn.dropout(next_l, prob_dropout)\n",
        "    \n",
        "    for i in range(1, num_layers - 1):\n",
        "        next_l = activation(tf.add(tf.matmul(next_l, layers[i]), biased[i]))\n",
        "        \n",
        "        # reduce nodes connection\n",
        "        next_l = tf.nn.dropout(next_l, prob_dropout)\n",
        "else:\n",
        "    first_l = activation(tf.matmul(X, input_layer))\n",
        "    \n",
        "    # reduce nodes connection\n",
        "    first_l = tf.nn.dropout(first_l, prob_dropout)\n",
        "    \n",
        "    next_l = activation(tf.matmul(first_l, layers[0]))\n",
        "    \n",
        "    # reduce nodes connection\n",
        "    next_l = tf.nn.dropout(next_l, prob_dropout)\n",
        "    \n",
        "    for i in range(1, num_layers - 1):\n",
        "        next_l = activation(tf.matmul(next_l, layers[i]))\n",
        "        \n",
        "        # reduce nodes connection\n",
        "        next_l = tf.nn.dropout(next_l, prob_dropout)\n",
        "    \n",
        "last_l = tf.matmul(next_l, output_layer)\n",
        "\n",
        "# adding up all penalties values\n",
        "regularizers = tf.nn.l2_loss(input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = last_l, labels = Y))\n",
        "\n",
        "# included penalty values\n",
        "cost = tf.reduce_mean(cost + beta * regularizers)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(last_l, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "416db8c3-636d-2c40-3782-c6bae62cb92b"
      },
      "source": [
        "**Start our session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0349549-b1bc-69b2-81b7-a4021a94ffae"
      },
      "outputs": [],
      "source": [
        "# start the session graph\n",
        "sess = tf.InteractiveSession()\n",
        "    \n",
        "# initialize global variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print (\"Train for \", epoch, \" iteration\")\n",
        "print (\"There are \", x_train.shape[0], \" of rows for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "87177496-63c7-1c39-caa3-d17cb27f3255"
      },
      "source": [
        "**Training session begin**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89c6ba56-4943-d80b-4ec8-4d381dbcb9eb"
      },
      "outputs": [],
      "source": [
        "for i in range(epoch):\n",
        "    last_time = time.time()\n",
        "    total_lost = 0\n",
        "    total_accuracy = 0\n",
        "    \n",
        "    for n in range(0, x_train.shape[0], batch_size):\n",
        "        out, _, loss = sess.run([accuracy, optimizer, cost], feed_dict={X: x_train[n : n + batch_size, :], Y: y_train[n : n + batch_size, :]})\n",
        "        total_accuracy += out\n",
        "        total_lost += loss\n",
        "    \n",
        "    print (\"total accuracy: \", total_accuracy / (x_train.shape[0] / batch_size * 1.0))\n",
        "    diff = time.time() - last_time\n",
        "    print (\"batch: \", i + 1, \", loss: \", total_lost/x_train.shape[0], \", speed: \", diff, \" s / epoch\")\n",
        "    total_lost = 0\n",
        "    total_accuracy = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ff2a35c6-1769-8b80-ae84-a3b8943e0d8b"
      },
      "source": [
        "Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e01fe1c7-d463-76b9-01d1-edf7eb2684d3"
      },
      "outputs": [],
      "source": [
        "total_correct = 0\n",
        "total_positive = 0\n",
        "total_correct_positive = 0\n",
        "for n in range(x_test.shape[0]):\n",
        "    \n",
        "    correct = sess.run(accuracy, feed_dict={X: x_test[n : n + 1, :], Y: y_test[n : n + 1 , :]})\n",
        "    total_correct += correct\n",
        "    if y_test[n][1] == 1:\n",
        "        total_positive += 1\n",
        "        if correct == 1:\n",
        "            total_correct_positive += 1\n",
        "    \n",
        "print (\"total correct positive: \", total_correct_positive, \" / \", total_positive)\n",
        "print (\"total correct: \", int(total_correct), \" / \", x_test.shape[0]) \n",
        "print (\"total accuracy: \", total_correct / (x_test.shape[0] * 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c2899b8-1966-36e8-d6c5-f8f866f02b19"
      },
      "source": [
        "**The output generated overly biased/under fitted. You need to increase the iteration atleast one thousand if want better result.**"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}