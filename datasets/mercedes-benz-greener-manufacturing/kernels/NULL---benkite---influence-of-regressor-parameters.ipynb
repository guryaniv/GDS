{"nbformat_minor": 0, "nbformat": 4, "cells": [{"execution_count": null, "source": "I'd share to share my cross validation work showing how model parameters influence cross-validation test performance. I define a few functions before showing some results. ", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "4abe086869b41dea6774e3d31215742746d8e772", "collapsed": false, "_cell_guid": "641d217e-68d3-42e8-97b4-4ed07bc4e8a1"}, "outputs": []}, {"execution_count": null, "source": "import pandas, numpy\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom datetime import datetime\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the \"../input/\" directory.\ntrain = pandas.read_csv(\"../input/train.csv\")\ntest = pandas.read_csv(\"../input/test.csv\")", "cell_type": "code", "metadata": {"trusted": false, "_execution_state": "busy", "_uuid": "88abd238a0b84d23969b8bfe63e4ce5e0d668e0a", "_cell_guid": "82880426-4adf-4555-9ecb-f6e0b7021cac"}, "outputs": []}, {"execution_count": null, "source": "This function randomly splits the training data into training and validation sets. I define the arguments for the next function which serves as a wrapper.", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "c681a47f8f81e0aeedaaf45408c1af4d0792a264", "collapsed": false, "_cell_guid": "a53c02d0-39c3-4528-ac6a-d8ef0bf98fea"}, "outputs": []}, {"execution_count": null, "source": "def regCheck(data, propTrain, models, features, outcome, regNames = None):\n    ind = data.index.values\n    size = int(numpy.round(len(ind)*propTrain))\n    use = numpy.random.choice(ind, size, replace = False)\n    train = data.loc[use]\n    test = data.loc[set(ind) - set(use)]\n    regmeas = []\n    if regNames == None:\n        names = []\n    for m in models:\n        if regNames == None:\n            names.append(str(m).split(\"(\")[0])\n        trained = m.fit(train[features], train[outcome])\n        test[\"prediction\"] = trained.predict(test[features])\n        out = r2_score(test[outcome], test[\"prediction\"])\n        regmeas.append(out)\n    regmeas = pandas.DataFrame(regmeas)\n    regmeas = regmeas.transpose()\n    if regNames == None:\n        regmeas.columns = names\n    else:\n        regmeas.columns = regNames\n    return(regmeas)", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "c96a5d619da1f3aea9f41a747e10c044e6d186f3", "collapsed": false, "_cell_guid": "f2818dc6-6aa1-48d3-b4a6-89d673d656c9"}, "outputs": []}, {"execution_count": null, "source": "The function below is used to compare competing models.\n\nThe data argument is a data frame with the features and outcome.\n\nnsamples is the number of replications of spliting the data into training and test.\n\npropTrain is the proportion of cases assigned to the training set.\n\nmodels is a list of sklearn regressors (even a single classifier needs to be in a list).\n\nfeatures is a list of predictor variables.\n\noutcome is the continuous outcome of interest.\n\nregNames allows the user to specific names for the models to display in the output.\n\nmaxTime is the maximum number of minutes the function should be allowed to run.\n\nThis returns a data frame summarizing how the models performed.\n", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "171047ed6b7f0b41ad900e1a9e62bd8241312abd", "collapsed": false, "_cell_guid": "52fcce83-b30f-4d2d-a051-91524758261c"}, "outputs": []}, {"execution_count": null, "source": "def simmer(data, models, features, outcome, nsamples = 100, propTrain = .8, regNames = None, maxTime = 1440):\n    tstart = datetime.now()\n    sd = dict()\n    for i in range(0, nsamples):\n        sd[i] = regCheck(data, propTrain, models, features, outcome, regNames)\n        if (datetime.now() - tstart).seconds/60 > maxTime:\n            print(\"Stopped at \" + str(i + 1) + \" replications to keep things under \" + str(maxTime) + \" minutes\")\n            break\n    output = pandas.concat(sd)\n    output = output.reset_index(drop = True)\n    return(output)", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "4b215e0a0db434822cd636f11594b78da32944e9", "collapsed": false, "_cell_guid": "d08db447-34dd-413d-af16-d77a1b7c84d9"}, "outputs": []}, {"execution_count": null, "source": "This is used to determine how changing a parameter of the model nfluences cross validation accuracy.\n\nparam indicates what parameter should be varied (e.g., alpha for Lasso).\n\nmodel is a string indicating what model should be fitted (e.g., \"Lasso\" or \"MLPRegressor\").\n\nvalues is a list which indicates what levels of the parameter indicated by param should be used.\n\nthe remaining arguments are passed to the simmer function defined above. Note that this function has different defaults.", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "84e27a8ddef14ba8966cfc99a723ac5cd98837a7", "collapsed": false, "_cell_guid": "ea95ded8-985b-4cea-9a98-91887364e75f"}, "outputs": []}, {"execution_count": null, "source": "def paramTester(param, model, values, features, outcome, data, nsamples = 100, propTrain = .5, maxTime = 10):\n    models = []\n    names = []\n    for v in values:\n        models.append(eval(model + \"(\" + param + \"=\" + str(v) + \")\"))\n        names.append(param + \" = \" + str(v))\n    out = simmer(data, models, features, outcome, nsamples, propTrain, regNames = names, maxTime = maxTime)\n    return(out)", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "7e6cddecaa9637386cf4bfb5f6e4aa8bfc37121d", "collapsed": false, "_cell_guid": "9a2622b7-bdef-48d6-a2dd-99fdb6bcb187"}, "outputs": []}, {"execution_count": null, "source": "Now I need to do some quick data preparation.", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "b4dbaca7fdf196414caf8e4cc88c89c15261ed6f", "collapsed": false, "_cell_guid": "d1d39724-6e7d-4f54-abe4-af8fc4006ee7"}, "outputs": []}, {"execution_count": null, "source": "train = pandas.get_dummies(train, drop_first = True)\ntest = pandas.get_dummies(test, drop_first = True)\n\nusevars = list(set(train.columns).intersection(test.columns))\n\nusevars = numpy.sort(usevars)  \n\npreds = usevars[1:]\n", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "8f9947573b6061f0a2ccd7dd2123bc4f20723ce6", "collapsed": false, "_cell_guid": "8511b65a-e935-4e9b-bd28-0dffb8d23ca7"}, "outputs": []}, {"execution_count": null, "source": "And now I can use my functions to see how changing alpha with Lasso influences test performance on random validation sets. \n\nHere I am using 80% of the training data to train a model, and then the remaining 20% to gauge prediction accuracy with R-squared. I specify maxTime as .5 so that this will run in about 30 seconds (it can be more, never less).", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "6507e5ee583d401c1b62df7cc7a89227c67f9203", "collapsed": false, "_cell_guid": "ed029d5e-ace4-4660-84eb-c4feab3c2ae2"}, "outputs": []}, {"execution_count": null, "source": "values = [.5, .25, .1, .01]\nmodel = \"Lasso\"\nparam = \"alpha\"\n        \noutput = paramTester(\"alpha\", \"Lasso\", values, preds, \"y\", train, propTrain = .8, maxTime = .5)\noutput", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "460f06e3a25c95e59c31f27bc99650182f29dac6", "collapsed": false, "_cell_guid": "1c4579d4-c2ee-45d5-8f3a-b215ed4d82b4"}, "outputs": []}, {"execution_count": null, "source": "Each row in output shows the results from a single split of the data. I am concerned about within row differences between the columns.  Because alpha = 0.01 column has the highest R-squared value in every row, I favor .01 over the other alpha values.\n\nWhat if we look at other low values?", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "7b7d9db06c20c789becf697ec4e6cb9ca48377c8", "collapsed": false, "_cell_guid": "c1ab5429-2d8d-453d-8ba9-d89855267393"}, "outputs": []}, {"execution_count": null, "source": "values = [.05, .025, .01, .001]\nmodel = \"Lasso\"\nparam = \"alpha\"\n        \noutput = paramTester(\"alpha\", \"Lasso\", values, preds, \"y\", train, propTrain = .8, maxTime = .5)\noutput", "cell_type": "code", "metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "a1e5b880e4739dbbde0ff14f3206689221140fe3", "collapsed": false, "_cell_guid": "79620b86-5ece-48de-a6ba-fd0949e2fdda"}, "outputs": []}, {"execution_count": null, "source": "Based on the results shown above, it looks like an alpha in the .025 to .010 range should be preferred.", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "fd294b299cc4f02c1afaca813ace075aa1bd9ebe", "collapsed": false, "_cell_guid": "17ee2108-0c8b-4a08-9121-6f2af7ed2be4"}, "outputs": []}, {"execution_count": null, "source": "The code shown here provides an example of how to probe models and determine how to increase test model accuracy. \n\nThis can be used to do a series of tests to influence the decision of what model should be used and what parameters should be specified.\n\nThe results presented here are simply an example, I am not saying that using Lasso with an alpha of .025 will get you a .590 on the leaderboard (around .550, maybe).\n\nI hope this helps someone!", "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "63c2c36c8d1cd60da057de7f083bb0c1a44275c1", "collapsed": false, "_cell_guid": "6f345af7-c743-4dfa-9204-fa65a8d0dc43"}, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}}