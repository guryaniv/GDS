{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Adversarial validation is a mean to check if train and test datasets have significant differences. The idea is to use the dataset features to try and separate train and test samples.\n\nSo you would create a binary target that would be 1 for train samples and 0 for test samples and fit a classifier on the features to predict if a given sample is in train or test datasets!\n\nHere we will use a LogisticRegression and a TF-IDF vectorizer to check if text features distributions are different and see if we can separate the samples. \n\nThe best kernel on this is certainly [here](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) by [Konrad Banachewicz](https://www.kaggle.com/konradb)\n\nOther resources can be found [on fastML](http://fastml.com/adversarial-validation-part-one/)\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrn = pd.read_csv(\"../input/train.csv\", encoding=\"utf-8\")\nsub = pd.read_csv(\"../input/test.csv\", encoding=\"utf-8\")","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"356ea9e9-126b-4cf3-abe5-9eb4f9a0f5af","_uuid":"a9d76507e3a7805b16b2bf6e902223ee48ff1d8b","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport regex\nvectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    max_features=20000\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_\nall_idf = vectorizer.fit_transform(pd.concat([trn.comment_text, sub.comment_text], axis=0))\nall_vocab = vectorizer.vocabulary_","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"61f32cac-cd19-445c-bd04-3ffd9ccdcc1d","_uuid":"de3f629a93be69f4079ad1de465482ec9de4ba05"},"cell_type":"markdown","source":"Convert vocab dictionnaries to list of words"},{"metadata":{"_cell_guid":"c8539cd9-b7b2-47fa-9cdd-0de5498ac0c6","_uuid":"578f39976d38377dd72adc9062bf200d7e715e77","collapsed":true,"trusted":true},"cell_type":"code","source":"trn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nall_words = [word for word in all_vocab.keys()]","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"c1d48036-0873-48ea-b712-db51b607684f","_uuid":"2681c121a8366fdda44d681c6708e809e8e1da2f"},"cell_type":"markdown","source":"Check a few figures on words not in train or test"},{"metadata":{"_cell_guid":"8dc68da6-c0ef-44ab-9ce8-4aa6a4425574","_uuid":"acc03c89faa3578c8b7c6a249a384f4b443ce4d7","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"common_words = set(trn_words).intersection(set(sub_words)) \nprint(\"number of words in both train and test : %d \"\n      % len(common_words))\nprint(\"number of words in all_words not in train : %d \"\n      % (len(trn_words) - len(set(trn_words).intersection(set(all_words)))))\nprint(\"number of words in all_words not in test : %d \"\n      % (len(sub_words) - len(set(sub_words).intersection(set(all_words)))))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"819aee86-98a6-4f24-82cb-4133a8d0dd13","_uuid":"5d17d63b9c61e83516b77c74a0a208171ec38848"},"cell_type":"markdown","source":"This means there are substantial differences between train and test vocabularies or term frequencies\n\nLet's check if a LinearRegression can make a difference between train and test using this.\n\nWe would take the output of the TF-IDF vectorizer fitted on train + test."},{"metadata":{"_cell_guid":"92863c3d-4600-4fe5-8287-e2777b693f70","_uuid":"94884ef5129ad259ecc0034fa8bc328da14aa47c","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n# Create target where all train samples are ones and all test samples are zeros\ntarget = np.hstack((np.ones(trn.shape[0]), np.zeros(sub.shape[0])))\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"00977781-2ae1-45f0-9fcd-a1e461a18573","_uuid":"1df84a6b320e7aff77356b497077e3f924cf200c"},"cell_type":"markdown","source":"I must say I find these results alarming. There are significant differences between train and test sets even with  a vectorizer fitted on train and test samples. \n\nI did not see any wrong doing in the code so far but if you do please shout as loud as possible !\n\nThis to me means we may have surprises on the private LB.\n\nAnother experiment is to see if using common words (i.e. words in both train and test) would help reduce difference in words distribution. I will use a min_df of 3 "},{"metadata":{"_cell_guid":"97a569c8-ab36-4a90-9a29-dfec0122b1c2","_uuid":"9e9866001c5b122b5b71510b60ebdf4b27b7b0cb","collapsed":true,"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    max_features=20000,\n    min_df=3\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"d6f4737e-7143-40f3-ae21-757ed917ce3a","_uuid":"139e613cb25ae0175e82b9a3a6e141e378f51970"},"cell_type":"markdown","source":"Now use common vocab in TFIDF"},{"metadata":{"_cell_guid":"c94dc73b-16b4-4e03-9d5e-6fdab3276a8c","_uuid":"8ca35316bf350a1aaf1d17ca4e21196205319de6","trusted":true},"cell_type":"code","source":"trn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nprint(\"Number of words in common : \", len(set(trn_words).intersection(set(sub_words))))\nvectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    vocabulary=list(set(trn_words).intersection(set(sub_words)))\n)\nall_idf = vectorizer.fit_transform(pd.concat([trn.comment_text, sub.comment_text], axis=0))\n# Create target where all train samples are ones and all test samples are zeros\ntarget = np.hstack((np.ones(trn.shape[0]), np.zeros(sub.shape[0])))\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"e2bcb412-aaf1-47ce-81a3-ea2bd41f81cf","_uuid":"9cfe59cde4820d5664e5a37e29c2cef685666775"},"cell_type":"markdown","source":"It's not really better.\n\nWould using a smaller vocab finally help ?"},{"metadata":{"_cell_guid":"e3bbeebc-eec7-41b2-a956-ff4f6b9af6ef","_uuid":"aaeb50124fef60353278e0740ba41b00d314e10a","trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    max_features=500,\n    min_df=3\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_\n\ntrn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nprint(\"Number of words in common : \", len(set(trn_words).intersection(set(sub_words))))\nvectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 1), \n    vocabulary=list(set(trn_words).intersection(set(sub_words)))\n)\nall_idf = vectorizer.fit_transform(pd.concat([trn.comment_text, sub.comment_text], axis=0))\n# Create target where all train samples are ones and all test samples are zeros\ntarget = np.hstack((np.ones(trn.shape[0]), np.zeros(sub.shape[0])))\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"ec13d78c-ae96-4918-a03b-8aa66da8d3b2","_uuid":"6157b57b2d378ffce12b6c432d8b9f8190fd8e0e"},"cell_type":"markdown","source":"Even with an extreme reduction in vocabulary we still have significant differences between train and test. \n\nMy belief is that it is due to the wide range of topics covered by the dataset with only 300000 samples in total.\n\nNow you need to be aware that tokenization is important. To demonstrate that I'm going to use CountVectorizer and 2 different tokenization methods: a very simple oe and the TweetTokenizer from the nltk package.\n\nThe very simple tokenization will display a 0.60 adversarial AUC when TweetTokenizer will reach 0.80+ !\n\nLet's start with the simple Tokenization"},{"metadata":{"_cell_guid":"ff4df680-cd24-4232-bbda-1e9cff98a537","_uuid":"9f6f1bcdd654bf688af2b62ec511d2132669d152","trusted":true},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\nvectorizer = CountVectorizer(\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 2), \n    max_features=500,\n    min_df=3\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_\n\ntrn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nprint(\"Number of words in common : \", len(set(trn_words).intersection(set(sub_words))))\nvectorizer = CountVectorizer(\n    strip_accents='unicode',\n    tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 2), \n    vocabulary=list(set(trn_words).intersection(set(sub_words)))\n)\nall_idf = vectorizer.fit_transform(pd.concat([trn.comment_text, sub.comment_text], axis=0))\n# Create target where all train samples are ones and all test samples are zeros\ntarget = np.hstack((np.ones(trn.shape[0]), np.zeros(sub.shape[0])))\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"675520f3442ccc541838af2c1410805ebfc49483"},"cell_type":"markdown","source":"Here is the corresponding list of words"},{"metadata":{"_uuid":"154547195b8cac77b9647d8f08477f432b8f9298","_cell_guid":"4f19e630-f78d-423b-8be0-736be44498be","trusted":true},"cell_type":"code","source":"list(set(trn_words).intersection(set(sub_words)))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"8e36dd2ce16e86c91145be03ebdb4554b7438049"},"cell_type":"markdown","source":"Now let's use TweetTokenizer"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a9d454971a2d932ebb4349ef0e89b6cce6e74ffe"},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\nvectorizer = CountVectorizer(\n    strip_accents='unicode',\n    tokenizer=TweetTokenizer().tokenize,\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 2), \n    max_features=500,\n    min_df=3\n)\ntrn_idf = vectorizer.fit_transform(trn.comment_text)\ntrn_vocab = vectorizer.vocabulary_\nsub_idf = vectorizer.fit_transform(sub.comment_text)\nsub_vocab = vectorizer.vocabulary_\n\ntrn_words = [word for word in trn_vocab.keys()]\nsub_words = [word for word in sub_vocab.keys()]\nprint(\"Number of words in common : \", len(set(trn_words).intersection(set(sub_words))))\nvectorizer = CountVectorizer(\n    strip_accents='unicode',\n    tokenizer=TweetTokenizer().tokenize,\n    analyzer='word',\n    token_pattern=None,\n    stop_words='english',\n    ngram_range=(1, 2), \n    vocabulary=list(set(trn_words).intersection(set(sub_words)))\n)\nall_idf = vectorizer.fit_transform(pd.concat([trn.comment_text, sub.comment_text], axis=0))\n# Create target where all train samples are ones and all test samples are zeros\ntarget = np.hstack((np.ones(trn.shape[0]), np.zeros(sub.shape[0])))\n# Shuffle samples to mix zeros and ones\nidx = np.arange(all_idf.shape[0])\nnp.random.seed(1)\nnp.random.shuffle(idx)\nall_idf = all_idf[idx]\ntarget = target[idx]\n# Train a Logistic Regression\nfolds = StratifiedKFold(5, True, 1)\nfor trn_idx, val_idx in folds.split(all_idf, target):\n    lr = LogisticRegression()\n    lr.fit(all_idf[trn_idx], target[trn_idx])\n    print(roc_auc_score(target[val_idx], lr.predict_proba(all_idf[val_idx])[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96be87a676862d4921f79778a4bf66ae9dd75c1d"},"cell_type":"markdown","source":"Here is the corresponding list of words"},{"metadata":{"trusted":true,"_uuid":"f3333f4079eeff83305e55aea738b7c4917ba18a"},"cell_type":"code","source":"print(list(set(trn_words).intersection(set(sub_words))))","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"dbca68987f00217b6fc2b53f47ac42e24218f639"},"cell_type":"markdown","source":"You may want to push all this further and see the impact of a cleaner dataset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"830fd4bf9ec8078b0d992187f26bbb3cb0d4447c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}