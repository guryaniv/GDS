{"cells":[{"metadata":{"_uuid":"f29c2f904c5ca9e18e7c74ade7ec00c9b2c8bb13","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nimport pandas as pd\n\n# fix dimension ordering issue\n# https://stackoverflow.com/questions/39547279/loading-weights-in-th-format-when-keras-is-set-to-tf-format\nfrom keras import backend as K\nK.set_image_dim_ordering('th')","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"1b93444fff56719351db079ba541907918ca6496","trusted":true,"collapsed":true},"cell_type":"code","source":"# Let's read our training and test set from the attached dataset\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\").values\n\n# The first column of the dataset tell us the actual value\nx_train = train[:, 1:].astype('float32')\ny_train = train[:, 0]\n\n# So far we have loaded the 2 variables that we need\n# 1. x_train -> has the images to train on\n# 2. y_train -> has the value for those images","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb2883740dbdf18ab73c9164d2faee022948cf35","collapsed":true},"cell_type":"code","source":"# Let's normalize from 0 to 255 to 0 to 1. It's a good practice when dealing with \n# neural networks that the inputs have short range of values.\nx_train /= 255","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8eaaa453cec31754d9f9ef65287bc976f78ddabf"},"cell_type":"code","source":"print (x_train.shape)\nprint (y_train.shape)","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2ca7fa1042315b18d2b2152d24fd1527ed790e6b","trusted":true},"cell_type":"code","source":"y_train = np_utils.to_categorical(y_train)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"241d7558a407bc06cee27400a873f19937a47812"},"cell_type":"code","source":"num_pixels = 28 * 28\nnum_classes = 10\n\nmodel = Sequential()\n\nmodel.add(Dense(num_pixels, input_dim=num_pixels, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint (model.summary())","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07bb27f0fbd2760fe37563c71942a7df34356b6d"},"cell_type":"code","source":"model.fit(x_train, y_train, epochs=10, batch_size=200, verbose=2, validation_split=0.2)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d8cf8aeb98af87ea761e1bde59ed23642c16890"},"cell_type":"code","source":"# reshape to be [samples][channels][width][height]\nx_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\nprint (x_train.shape)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"b11d44ba386024b9533af2de96e1c959e0b1113d","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5), input_shape=(1, 28, 28), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint (model.summary())","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"6a2e8a541ce119e9bd418604fca4d881b27769fa","trusted":true,"scrolled":true},"cell_type":"code","source":"model.fit(x_train, y_train, epochs=10, batch_size=200, verbose=2, validation_split=0.2)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"2da055166606b1f0374313bd2ffbbaf347296637"},"cell_type":"markdown","source":"**Notice that this an accuracy of 98.80% !**"},{"metadata":{"_uuid":"fe8df8e03f41944dad185087eac57d778734cc81","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=30, kernel_size=(5, 5), input_shape=(1, 28, 28), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(filters=15, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint (model.summary())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"f2cc91ce78909bfc03a407b608e69a0fc6929851","trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train, epochs=10, batch_size=200, verbose=2, validation_split=0.2)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"901ee568434f8bef80924d40e68527ef26e4da98"},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nweights = '../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\nmodel = InceptionV3(weights=weights)\nprint (model.summary())","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8ad98e245d38c1a72898c5a8c38598d641f6d0","collapsed":true},"cell_type":"code","source":"from os import makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n!cp  ../input/inceptionv3/* ~/.keras/models/","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b30c44303e5560674a213ae8ee409053f1a87f2","collapsed":true},"cell_type":"code","source":"from keras.applications.inception_v3 import preprocess_input, decode_predictions\nimport time\n\ncurrent_milli_time = lambda: int(round(time.time() * 1000))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbb61f8392e2a6813669c7659eb0ded25d76ff05"},"cell_type":"code","source":"from keras.preprocessing import image\nimg_path = \"../input/dogs-vs-cats-redux-kernels-edition/train/dog.4444.jpg\"\nimg = image.load_img(img_path, target_size=(224, 224))\nimg","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"944e8a95333ef7a5cd33acdf3bcc7c66a6848989"},"cell_type":"code","source":"img = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\n\nstart = current_milli_time()\npreds = model.predict(img)\nend = current_milli_time()\nprint('Predicted in {} ms: {}'.format(end-start, decode_predictions(preds, top=3)[0]))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58899ee64f6fc8bead14957e7fc7e891acfd1497"},"cell_type":"code","source":"from keras.preprocessing import image\nimg_path = \"../input/dogs-vs-cats-redux-kernels-edition/train/cat.5555.jpg\"\nimg = image.load_img(img_path, target_size=(224, 224))\nimg","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b816d5627b7d63883ffbe13419453d16730c7301"},"cell_type":"code","source":"img = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\n\nstart = current_milli_time()\npreds = model.predict(img)\nend = current_milli_time()\nprint('Predicted in {} ms: {}'.format(end-start, decode_predictions(preds, top=3)[0]))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caa1f0f4d97b46dc86088c4f6a0f3c9caad409ef"},"cell_type":"code","source":"from keras.preprocessing import image\nimg_path = \"../input/flowers-recognition/flowers/flowers/daisy/34539556222_f7ba32f704_n.jpg\"\nimg = image.load_img(img_path, target_size=(224, 224))\nimg","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f78e8150882cd0ed8c939aa407f4562c234a47a"},"cell_type":"code","source":"img = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\n\nstart = current_milli_time()\npreds = model.predict(img)\nend = current_milli_time()\nprint('Predicted in {} ms: {}'.format(end-start, decode_predictions(preds, top=3)[0]))","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8261a3fdce3109632db141a35a13acd36da6ee6"},"cell_type":"code","source":"from keras.preprocessing import image\nimg_path = \"../input/flowers-recognition/flowers/flowers/rose/9609569441_eeb8566e94.jpg\"\nimg = image.load_img(img_path, target_size=(224, 224))\nimg","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9eb863072fc186f7735ed3bccf8d23ead6f411b9"},"cell_type":"code","source":"img = image.img_to_array(img)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\n\nstart = current_milli_time()\npreds = model.predict(img)\nend = current_milli_time()\nprint('Predicted in {} ms: {}'.format(end-start, decode_predictions(preds, top=3)[0]))","execution_count":23,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}