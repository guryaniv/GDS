{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4bae4126-27f5-c1be-bb25-fa0ea0210b4e",
        "_active": true
      },
      "outputs": [],
      "source": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#NLTK functioncs\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\n\nstop = set(stopwords.words('english'))   #_____________ import stop words\n\n# timing function\nimport time   \nstart = time.clock() #_________________ measure efficiency timing\n\n\n# read data\ntrain = pd.read_csv('../input/train.csv',encoding='utf8')[:25000]  #_______________________ open data files\ntest = pd.read_csv('../input/test.csv',encoding='utf8')[:25000]  #_______________________ open data files\nprint(train.head(3))\ntrain.fillna(value='leeg',inplace=True)\n#train=train.dropna(axis=0, how='any')  #clean empty rows that give trouble\nend = time.clock()\nprint('open:',end-start)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c929cf3f-3631-68b2-b755-e713a89d9a44",
        "_active": false
      },
      "outputs": [],
      "source": "def cleantxt(q1,q2):\n    #print(q1,q2)\n    q1words = nltk.word_tokenize(q1)\n    q2words = nltk.word_tokenize(q2)\n    equq1 = [w for w in q1words if w in q2words]\n    #difq1 = [stemmer.stem(w.decode(\"utf8\")) for w in q1words if w not in q2words]  # stemming sometimes simplifies things\n    #difq2 = [stemmer.stem(w.decode(\"utf8\")) for w in q2words if w not in q1words ]\n    difq1 = [w for w in q1words if w not in q2words] \n    difq2 = [w for w in q2words if w not in q1words ]\n    #print(difq1,difq2)\n    #wsq1=[wordnet.synsets(w.decode(\"utf8\")) for w in difq1]\n    #wsq2=[wordnet.synsets(w.decode(\"utf8\")) for w in difq2]    #synsetting seems to find only stemmable words...\n    netto=list(set(difq1+difq2))\n    return q1words,q2words,difq1,difq2,equq1\n    \n\nq1=[]\nq2=[]\ndi1=[]\ndi2=[]\neq=[]\nfor xi in range(len(train)):\n    q1words,q2words,difq1,difq2,equq1=cleantxt(train.iloc[xi].question1,train.iloc[xi].question2)\n    q1.append(q1words)\n    q2.append(q2words)\n    di1.append(difq1)\n    di2.append(difq2)\n    eq.append(equq1)\n\ntrain['q1']=q1\ntrain['q2']=q2\ntrain['di1']=di1\ntrain['di2']=di2\ntrain['eq']=eq\n\nq1=[]\nq2=[]\ndi1=[]\ndi2=[]\neq=[]\nfor xi in range(len(test)):\n    q1words,q2words,difq1,difq2,equq1=cleantxt(test.iloc[xi].question1,test.iloc[xi].question2)\n    q1.append(q1words)\n    q2.append(q2words)\n    di1.append(difq1)\n    di2.append(difq2)\n    eq.append(equq1)\n\ntest['q1']=q1\ntest['q2']=q2\ntest['di1']=di1\ntest['di2']=di2\ntest['eq']=eq\n\nprint(train.head())\n    \nend = time.clock()\nprint('cleaned:',end-start)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59c52309-8e2e-9c3f-4e8f-fa19e2d733d1",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "1# Lets redo it but splitted... and use the existing vocabulary\nfrom nltk.tokenize import word_tokenize\n\ns1=train['q1'].map(lambda x: ' '.join(x),na_action=None)\ns2=train['q2'].map(lambda x: ' '.join(x),na_action=None)\nsd3=train['di1'].map(lambda x: ' '.join(x),na_action=None)                  \nsd4=train['di2'].map(lambda x: ' '.join(x),na_action=None)  \nse5=train['eq'].map(lambda x: ' '.join(x),na_action=None)  \nst1=test['q1'].map(lambda x: ' '.join(x),na_action=None)\nst2=test['q2'].map(lambda x: ' '.join(x),na_action=None)\ntemp=s1.append(s2)\ntemp=temp.append(st1)\ntemp=temp.append(st2)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df=0.00001)\ncount_vectorizer.fit(temp)  #Learn vocabulary and idf, return term-document matrix.\n\ncount1_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount1_vectorizer.fit_transform(s1)  #Learn vocabulary and idf, return term-document matrix.\nfreq1_term_matrix = count_vectorizer.transform(s1)\ncount2_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount2_vectorizer.fit_transform(s2)\n#s2>s1\nfreq2_term_matrix = count_vectorizer.transform(s2) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount3_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount3_vectorizer.fit_transform(sd3)\nfreq3_term_matrix = count_vectorizer.transform(sd3) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount4_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount4_vectorizer.fit_transform(sd4)\nfreq4_term_matrix = count_vectorizer.transform(sd4) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount5_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount5_vectorizer.fit_transform(se5)\nfreq5_term_matrix = count_vectorizer.transform(se5) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n#se5>s1\n\ntfidf1 = TfidfTransformer(norm=\"l2\")\ntf1_idf_matrix = tfidf1.fit_transform(freq1_term_matrix)\ntfidf2 = TfidfTransformer(norm=\"l2\")\ntf2_idf_matrix = tfidf2.fit_transform(freq2_term_matrix)\ntfidf3 = TfidfTransformer(norm=\"l2\")\ntf3_idf_matrix = tfidf3.fit_transform(freq3_term_matrix)\ntfidf4 = TfidfTransformer(norm=\"l2\")\ntf4_idf_matrix = tfidf4.fit_transform(freq4_term_matrix)\ntfidf5 = TfidfTransformer(norm=\"l2\")\ntf5_idf_matrix = tfidf5.fit_transform(freq5_term_matrix)\n\n\nprint('Questions1 x Words', tf1_idf_matrix.shape)\nprint('Questions2 x Words', tf2_idf_matrix.shape)\n#print('Differenc1 x Words', tf3_idf_matrix.shape)\nprint('Differenc2 x Words', tf4_idf_matrix.shape)\nprint('Equality x Words', tf5_idf_matrix.shape)\n\n#als je similariteit wilt zien...\n#print('Q similarity',tf1_idf_matrix[:10].dot(tf2_idf_matrix[:10].T) )\n\nprint('example first 10 questions similarity')\ncorr1=tf1_idf_matrix[:30].dot(tf2_idf_matrix[:30].T).diagonal().round(1)\nprint(corr1)\nprint('example equality 1 - eq ')\ncorr2=tf1_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\nprint(corr2)\nprint('example equality 2 - eq')\ncorr3=tf2_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\nprint(corr3)\nprint('example difference 1 - dif1 ')\ncorr4=tf1_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\nprint(corr4)\nprint('example difference 2 - dif2')\ncorr5=tf2_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\nprint(corr5)\nprint('example difference 1 - dif2')\ncorr6=tf1_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\nprint(corr6)\nprint('example difference 2 - dif1')\ncorr7=tf2_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\nprint(corr7)\n\nctrain=pd.DataFrame([])\nctrain['q12']=tf1_idf_matrix[:].dot(tf2_idf_matrix[:].T).diagonal().round(2)\nctrain['q1eq']=tf1_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\nctrain['q2eq']=tf2_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\nctrain['q1d1']=tf1_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(2)\nctrain['q2d2']=tf2_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(2)\nctrain['q1d2']=tf1_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(2)\nctrain['q2d1']=tf2_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(2)\n\n    \nprint(ctrain.head())\n\nend = time.clock()\nprint('train tfidf:',end-start)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6206871b-ad49-7e2e-b1e7-790fd1449d93",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "#the same for the test\nfrom nltk.tokenize import word_tokenize\n\ns1=test['q1'].map(lambda x: ' '.join(x),na_action=None)\ns2=test['q2'].map(lambda x: ' '.join(x),na_action=None)\nsd3=test['di1'].map(lambda x: ' '.join(x),na_action=None)                  \nsd4=test['di2'].map(lambda x: ' '.join(x),na_action=None)  \nse5=test['eq'].map(lambda x: ' '.join(x),na_action=None)  \n\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df=0.00001)\ncount_vectorizer.fit(temp)  #Learn vocabulary and idf, return term-document matrix.\n\n\ncount1_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount1_vectorizer.fit_transform(s1)  #Learn vocabulary and idf, return term-document matrix.\nfreq1_term_matrix = count_vectorizer.transform(s1)\ncount2_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount2_vectorizer.fit_transform(s2)\n#s2>s1\nfreq2_term_matrix = count_vectorizer.transform(s2) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount3_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount3_vectorizer.fit_transform(sd3)\nfreq3_term_matrix = count_vectorizer.transform(sd3) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount4_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount4_vectorizer.fit_transform(sd4)\nfreq4_term_matrix = count_vectorizer.transform(sd4) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\ncount5_vectorizer = CountVectorizer(ngram_range=(1, 2),vocabulary=count_vectorizer.vocabulary_)\ncount5_vectorizer.fit_transform(se5)\nfreq5_term_matrix = count_vectorizer.transform(se5) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n#se5>s1\n\ntfidf1 = TfidfTransformer(norm=\"l2\")\ntf1_idf_matrix = tfidf1.fit_transform(freq1_term_matrix)\ntfidf2 = TfidfTransformer(norm=\"l2\")\ntf2_idf_matrix = tfidf2.fit_transform(freq2_term_matrix)\ntfidf3 = TfidfTransformer(norm=\"l2\")\ntf3_idf_matrix = tfidf3.fit_transform(freq3_term_matrix)\ntfidf4 = TfidfTransformer(norm=\"l2\")\ntf4_idf_matrix = tfidf4.fit_transform(freq4_term_matrix)\ntfidf5 = TfidfTransformer(norm=\"l2\")\ntf5_idf_matrix = tfidf5.fit_transform(freq5_term_matrix)\n\n\nprint('Questions1 x Words', tf1_idf_matrix.shape)\nprint('Questions2 x Words', tf2_idf_matrix.shape)\n#print('Differenc1 x Words', tf3_idf_matrix.shape)\nprint('Differenc2 x Words', tf4_idf_matrix.shape)\nprint('Equality x Words', tf5_idf_matrix.shape)\n\n#als je similariteit wilt zien...\n#print('Q similarity',tf1_idf_matrix[:10].dot(tf2_idf_matrix[:10].T) )\n\nprint('example first 10 questions similarity')\ncorr1=tf1_idf_matrix[:30].dot(tf2_idf_matrix[:30].T).diagonal().round(1)\nprint(corr1)\nprint('example equality 1 - eq ')\ncorr2=tf1_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\nprint(corr2)\nprint('example equality 2 - eq')\ncorr3=tf2_idf_matrix[:30].dot(tf5_idf_matrix[:30].T).diagonal().round(1)\nprint(corr3)\nprint('example difference 1 - dif1 ')\ncorr4=tf1_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\nprint(corr4)\nprint('example difference 2 - dif2')\ncorr5=tf2_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\nprint(corr5)\nprint('example difference 1 - dif2')\ncorr6=tf1_idf_matrix[:30].dot(tf4_idf_matrix[:30].T).diagonal().round(1)\nprint(corr6)\nprint('example difference 2 - dif1')\ncorr7=tf2_idf_matrix[:30].dot(tf3_idf_matrix[:30].T).diagonal().round(1)\nprint(corr7)\n\nttrain=pd.DataFrame([])\nttrain['q12']=tf1_idf_matrix[:].dot(tf2_idf_matrix[:].T).diagonal().round(1)\nttrain['q1eq']=tf1_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(1)\nttrain['q2eq']=tf2_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(1)\nttrain['q1d1']=tf1_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(1)\nttrain['q2d2']=tf2_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(1)\nttrain['q1d2']=tf1_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(1)\nttrain['q2d1']=tf2_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(1)\n\nprint(ttrain.head())\nend = time.clock()\nprint('test tfidf:',end-start)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1de557bf-d187-165f-7d9a-dd4e8ea02611",
        "_active": false
      },
      "outputs": [],
      "source": "import xgboost as xgb\n\ngbm = xgb.XGBClassifier(silent=False).fit(ctrain,train['is_duplicate'])\npredictor = gbm.predict(ttrain)\nprint(gbm)\nprint('model prediction first 20')\nfor xi in range (0,40):\n    print(predictor[xi],test.iloc[xi].question2)\n\nend = time.clock()\nprint('XGB model trained:',end-start)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9986f2cd-9e07-320b-5548-9938d6e06695",
        "_active": false
      },
      "source": "Estimating all similarities\n----\nin batches of 1000, since its pure TF_IDF it doesn't matter",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f69b00ee-a822-cc0f-e541-362064743a95",
        "_active": false
      },
      "outputs": [],
      "source": "submit=[]\nco1=[]\nco2=[]\nco3=[]\nco4=[]\nco5=[]\nco6=[]\nco7=[]\nbatch=1000\nfor xi in range(0,len(train),batch):\n    if xi+batch>len(train):\n        batch=len(train)-xi\n    corr1=tf1_idf_matrix[xi:xi+batch].dot(tf2_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr2=tf1_idf_matrix[xi:xi+batch].dot(tf5_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr3=tf2_idf_matrix[xi:xi+batch].dot(tf5_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr4=tf1_idf_matrix[xi:xi+batch].dot(tf3_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr5=tf2_idf_matrix[xi:xi+batch].dot(tf4_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr6=tf1_idf_matrix[xi:xi+batch].dot(tf4_idf_matrix[xi:xi+batch].T).diagonal().round(2)\n    corr7=tf2_idf_matrix[xi:xi+batch].dot(tf3_idf_matrix[xi:xi+batch].T).diagonal().round(2)    \n    co1.extend(corr1)\n    co2.extend(corr2)    \n    co3.extend(corr3)\n    co4.extend(corr4)\n    co5.extend(corr5)    \n    co6.extend(corr6)\n    co7.extend(corr7)    \n    #submit.extend(corr6/corr3)\n    submit.extend(corr1+(corr6/corr3-0.3))\ntrain['len']= train['eq'].map(lambda x: len(x))\ntrain['dif']=train['di2'].map(lambda x:len(x))\n\nend = time.clock()\nprint('estimate if similarity of q1-diff2 is bigger then equality of q2 with common stem:',end-start) "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "63062423-4e9a-be6c-ee90-11edb3bf5bd1",
        "_active": false
      },
      "source": "with the similarity we submit\n-----\n\n 1. similXY['isDUP']=(np.asarray(co1)+np.asarray(co2)+np.asarray(co3))/3\n---\nthe first is simply the average similarity of Q1-Q2 + Q1-EQ + Q2-EQ\nthis gives on the LB a score of 0.823\n\n 2. similXY['isNOT']=(np.asarray(co4)+np.asarray(co5))/2\n---\nthis estimates the similarity between a Q1 and all the non common words. If two questions are 100% different you get 1, if two questions are identical, you get a 0 or nan\nSo if we want to use this similarity, we have to reverse the value thats ABS( isNOT - 1 )  what makes the identical question a 1, and the 100% different question a 0\nthis gives on the LB a score of 0.55\n\n3.similXY['is_duplicate']=similXY['isDUP']>similXY['isNOT']\n---\nthis estimate gave me the idea i would score very well... **how disappointing... If you answer binary... the LB is 11....**",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0e2ca1c-98eb-012d-f5a4-c0cdd3b26a07",
        "_active": false
      },
      "outputs": [],
      "source": "import numpy as np\nsimilXY=pd.DataFrame([])\nsimilXY['q12']=co1\nsimilXY['q1e12']=co2\nsimilXY['q2e12']=co3\nsimilXY['q1d1']=co4\nsimilXY['q2d2']=co5\nsimilXY['q1d2']=co6\nsimilXY['q2d1']=co7\nsimilXY['isDUP']=(np.asarray(co1)+np.asarray(co2)+np.asarray(co3))/3\nsimilXY['isNOT']=(np.asarray(co4)+np.asarray(co5))/2\nsimilXY['is_duplicate']=similXY['isDUP']>similXY['isNOT']\nsimilXY['is_duplicate']=similXY['is_duplicate']*1\n\nsimilXY.fillna(value=0)\nprint(similXY.head(30))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f2c3b370-3cf3-7de2-ca8c-37795ef37c50",
        "_active": false
      },
      "source": "Visualize\n----\nMaking a visual shows a preview how good the model could score\nThe better the separation the better the model should work\n\n - Beware: the **isduplicate is here my estimate not the one in the training dat**a !",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea0cea74-c816-c97c-3914-735239b53987",
        "_active": false
      },
      "outputs": [],
      "source": "import seaborn as sns\nimport numpy as np\n\n\n\n\nsns.set(style=\"white\", color_codes=True)\nsimilsample=similXY.sample(n=300)\nsns.pairplot(similsample, hue=\"is_duplicate\", size=3)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e8ab18b9-c445-debc-4a2c-81a453f00ecf",
        "_active": false
      },
      "outputs": [],
      "source": "submiss=pd.DataFrame(abs(similXY['isNOT']-1))\nsubmiss.fillna(value=0,inplace=True)\nsubmiss.to_csv('TfIdf_submission.csv')\nprint(submiss.describe())\n\n# DUP/NOTDUP = 11\n# pure DUP   = 0.8233\n\nend = time.clock()\nprint('cleaned:',end-start)"
    }
  ]
}