{"cells":[{"metadata":{"_uuid":"c9907da029a24c0c0bc75a1088f571b0152459b0"},"cell_type":"markdown","source":"# Credit Card Fraud Detection"},{"metadata":{"trusted":true,"_uuid":"5b1adfb828d8b3367cca5db07dad211ce5c16b90"},"cell_type":"code","source":"# Carrega as libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1707ff7676a71620cb097e39be623a6c7574711"},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"trusted":true,"_uuid":"81120781c6e19c98b875f58b5b4183e89b26c4fd","scrolled":true},"cell_type":"code","source":"# diretorio de trabalho\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4208ef9ad2ec16c6c7c04e78abac784d7d2b934f"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0aba1a68c13e2d06b9146adae1324deea2e8343d"},"cell_type":"markdown","source":"## Análise das Variáveis"},{"metadata":{"_uuid":"8fe72fc37617824dd67326b0134d10fdee785156"},"cell_type":"markdown","source":"### Análise da Variável Resposta - Classes 0 e 1\n- 0 = Transação Legítima\n- 1 = Transação Fraudulenta"},{"metadata":{"trusted":true,"_uuid":"086843ac7c9d43d321b35057cf9908a844908470"},"cell_type":"code","source":"# value_counts == table() do R\nclass_count = pd.value_counts(df['Class'])\nprint(class_count)\n\n# Plota as classes\nax = sns.countplot('Class', data = df)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(df['Class'])), (p.get_x() + 0.35, p.get_height() + 3000))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d60fb7e5d28d31e608e80ba1293fca2d355323b"},"cell_type":"markdown","source":"### Análise das Variáveis de Interesse\nAs variáveis apresentadas estão de forma anônimas, exceto a varável **Time** (medida em segundos da transação a partir da primeira) e **Amount** (valor da transação). As demais 28 variáveis são numéricas."},{"metadata":{"_uuid":"354c4c87c1d6e4c2e19e0cf1c27f7dff117110e7"},"cell_type":"markdown","source":"___\n- **Variável Time**"},{"metadata":{"trusted":true,"_uuid":"dd4dec19981695f3e423b5d1c9bd492a6d04372b"},"cell_type":"code","source":"# Para analisar a variável Time, vou reajustá-la para observar em Horas\ndf['Time_Hours'] = df['Time']/3600","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7400c14f9481e900b5b0741197714edc0f8ae178"},"cell_type":"code","source":"df['Time_Hours'].tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34045f35fb40d39ce68e390cd7749d13472ffc72"},"cell_type":"markdown","source":"Como podemos observar acima, o dataset contém infos de 2 dias (48h)\n\nObs: Mais para frente vamos observar melhor o comportamento das variáveis de acordo com o tempo"},{"metadata":{"_uuid":"492055e75efde410151f2e03a88c813d45f8e800"},"cell_type":"markdown","source":"___\n- **Variável Amount**"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"c368a61546cf11054aec1856d4772a00a6f05d57"},"cell_type":"code","source":"plt.figure(figsize=(12,3))\nsns.distplot(df['Amount'], kde = False);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb3bed02634a2fb76025d0a391864711f97abcbc"},"cell_type":"markdown","source":"Observamos acima que as transações de um valor de até 1000 representam a maior parte dos dados."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"13a17c617cdb54debf6c585e33b326d1f930d483"},"cell_type":"code","source":"# Vamos dar um zoom nessas transações\nplt.figure(figsize=(20,5))\nplt.subplot().set_xlim(-10, 1000)   # seleciona o zoom do valor 0 até 2000\nsns.distplot(df[df['Amount']<=1000].Amount);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b8fd5a0724b098c6efa0fdd410ece3fb4fefb91"},"cell_type":"markdown","source":"Mais ainda, estão altamente concentradas entre 0 e 200\n___\nPara resolver isso vai ser necessário normalizar os dados. Existem diversas formas de realizar isso, aqui vou usar o StandardScaler do sklearn."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ad9feaf1e0facaec873867797e4754c4f55d5c1b"},"cell_type":"code","source":"# StandardScaler deixa a média = 0, e dp = 1\nfrom sklearn.preprocessing import StandardScaler\n\ndf['Amount_Norm'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\nprint('Média: ' + str(np.round(np.mean(df['Amount_Norm']))))\nprint('Var: ' + str(np.round(np.var(df['Amount_Norm']))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73aa2b61c9119d3c22cfc1360f9797cdc8340431"},"cell_type":"markdown","source":"Uma outra maneira de tratar esses valores é criando ranges, assim transformando em variável categorica."},{"metadata":{"_uuid":"64e14611954daf4bce560d33f50ea87fc71f5dfe"},"cell_type":"markdown","source":"___\n- **Variáveis Descaracterizadas (V1 - V28)**\n\nAqui vamos observar a distribuição de todas as variáveis, assim tendo uma visão geral de tudo.\n\nComo vimos nos plots, as distribuições estão parecidas."},{"metadata":{"trusted":true,"_uuid":"b641d37003e585a2aa42841f993af5450ecb5bf4"},"cell_type":"code","source":"######\n# Vou plotar as distribuições de todas as variáveis\nimport itertools\n\n# Define o espaço para os 28 plots\nfig, axes = plt.subplots(7, 4, sharex=True, sharey=True, figsize=(20,10))\n\nvar = 1   # vai ser usado para o nome das variaveis V1 - V28\n# loop para plotar os graficos\nfor i, j in itertools.product(range(7), range(4)):\n    axes[i,j].set(xlim=(-10, 10), ylim=(0, 1.2))   # ajusta a escala das dimensoes x e y\n    sns.distplot(df['V' + str(var)], ax=axes[i,j])\n    var = var + 1","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5cb89f5cdfd283507e51b0e9b59b60c31da7b668"},"cell_type":"markdown","source":"## Modelos de Regressão Logística"},{"metadata":{"trusted":true,"_uuid":"5028feff34ecb2f35150bbad107b53d2e853d15e"},"cell_type":"code","source":"# Cria funcao para plotar a matriz de confusao\ndef PlotCM(y_test, pred, plot = True):\n    \n    from sklearn.metrics import confusion_matrix, classification_report\n    \n    # Cria CM e CM normalizada\n    cm_matrix = confusion_matrix(y_test, pred)\n    cm_matrix_norm = cm_matrix / cm_matrix.astype(np.float).sum(axis=1)\n    \n    fig = plt.figure(figsize=(12, 3))    \n    # Plota CM\n    ax = fig.add_subplot(1,2,1)\n    sns.heatmap(cm_matrix, cmap='coolwarm_r', linewidths=0.5, annot=True, fmt='g', ax=ax)\n    plt.title('Confusion Matrix')\n    plt.ylabel('Real Classes')\n    plt.xlabel('Predicted Classes')\n    \n    # Plota CM Normalizada\n    # variavel para controlar se plota o grafico ou somente gera os valores\n    if (plot == True):\n        ax = fig.add_subplot(1,2,2)\n        sns.heatmap(cm_matrix_norm, cmap='coolwarm_r', linewidths=0.5, annot=True, fmt='g', ax=ax)\n        plt.title('Normalized Confusion Matrix')\n        plt.ylabel('Real Classes')\n        plt.xlabel('Predicted Classes')\n        plt.show()\n    \n    print('---Classification Report---')\n    TP = cm_matrix[1,1]\n    FN = cm_matrix[1,0]\n    FP = cm_matrix[0,1]\n    TN = cm_matrix[0,0]\n    T = TP+FN+FP+TN\n    \n    print('Accuracy =      {:.3f}'.format((TP+TN)/T))\n    print('Specificity =   {:.3f}'.format(TN/(TN+FP)))\n    print('Precision =     {:.3f}'.format(TP/(TP+FP)))\n    print('Recall (TPR) =  {:.3f}'.format(TP/(TP+FN)))\n    print('Fallout (FPR) = {:.3e}'.format(FP/(FP+TN)))\n    print('F1 Score =      {:.3f}'.format(2*TP/(2*TP+FP+FN)))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71c0b2cb3986d4b4db6ec0c964a772d0e93d5ac6"},"cell_type":"code","source":"# Cria Treino e Teste\ndef cross_val_model(X, y, model, n_splits=3, CM = True):\n    # Cria os \n    \n    from sklearn.model_selection import StratifiedKFold\n    \n    X = np.array(X)\n    y = np.array(y)\n\n    folds = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123).split(X, y))\n\n    for j, (train_idx, test_idx) in enumerate(folds):\n        X_train = X[train_idx]\n        y_train = y[train_idx]\n        X_test = X[test_idx]\n        y_test = y[test_idx]\n\n        print (\"Fit %s fold %d\" % (str(model).split('(')[0], j+1))\n        model.fit(X_train, y_train)\n        \n        pred = model.predict(X_test)\n        \n        PlotCM(y_test, pred, CM)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d45a68f07585b8e4d9e189b8cbbeddcd439e3be0"},"cell_type":"markdown","source":"### Exemplo com CV"},{"metadata":{"trusted":true,"_uuid":"68870494e847a68b49a06f57dc002e878e89eaa9"},"cell_type":"code","source":"# Separa a base em treino/validação e teste\nfrom sklearn.model_selection import train_test_split\n\n# Cria X e y\nvar_x = ['Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14',\n         'V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount']\nvar_y = ['Class']\n\nX = np.array(df.loc[:,var_x])\ny = np.array(df.loc[:,var_y])\n\n# Separa em treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1db4145060a5190b1b7b0514136e4b2397675d7e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\n\n# Treina o modelo\ncross_val_model(X_train, y_train, lr_model, n_splits=3, CM = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"518262669a5cd42ba798398d9743ee756a99b6b6"},"cell_type":"markdown","source":"### Feature Selection (Recursive feature elimination)\nMais especificamente o Feature ranking with recursive feature elimination and cross-validation (RFECV).\n\nAlém de técnicas de feature selection, as análises exploratórias são fundamentais para conseguir escolher as variáveis finais no modelo, e ter insights sobre algum detalhe, transformação ou criação das variáveis."},{"metadata":{"trusted":true,"_uuid":"9d4ce82e3247435efbf8076555c27b835b7205ef"},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=5, scoring='precision')\nrfecv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b2bba7283d2d4604ac9ba73739b945a0453516"},"cell_type":"code","source":"print(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(df.loc[:,var_x].columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c34afe85462dbc9ae3151e75ac76a36b7502cbe"},"cell_type":"code","source":"# Atualiza as bases com as variáveis selecionadas\nX_train = X_train[:,rfecv.support_]\nX_test = X_test[:,rfecv.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"13fdba6176403d805a76491cdbb093f5bbca8462"},"cell_type":"code","source":"# Treina o modelo\ncross_val_model(X_train, y_train, lr_model, n_splits=3, CM = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51212f09a500f5c2da27cb5437fa3a8543624d83"},"cell_type":"markdown","source":"Já conseguimos ver que o modelo consegue melhorar a performance, principalemente observando a medida Precision.\nAlém disso podemos melhor ainda mais nosso modelo, repetindo esses passos, explorando outras variáveis e com insights das análises e gráficos."},{"metadata":{"trusted":true,"_uuid":"a7d6b64480ae3f879169b903d299b5ed41c84f32"},"cell_type":"code","source":"# Plota a correlação das variáveis selecionadas e variável resposta\nselected_features = np.array(df.loc[:,np.array(var_x)[rfecv.support_]].columns)\nX_corr = df[np.concatenate([selected_features, np.array(var_y)])]\n\nplt.subplots(figsize=(20, 5))\nsns.heatmap(X_corr.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89c23bf6c782601d7d0fc59a110eff7c2aaead74"},"cell_type":"markdown","source":"### Undersampling\n\nAqui vou utilizar a técnica de undersampling para tratar o desbalanceamento das classes. O undersampling 'retira' observações da classe predominante para igualar a classe pouco presente.\n\nEstou utilizando essa técnica, pois nesse problema é mais fácil dizer que a transação não é fraude, então tirar observações dessa classe não afetará na identificação da mesma. Já a classe de 'fraude' é mais sensível e difícil de identificar, então um oversampling poderia atrapalhar na criação do modelo, incluindo uma maior possibilidade de overfitting."},{"metadata":{"_uuid":"f777059d0d3edd5b4675861b24f7b8c96c5ea0c8"},"cell_type":"markdown","source":"#### Preparação dos Dados\nPrepara os dataframe para tirar observações de classe 0 com proporções 3:1 e 2:1.\n\n**OBS: Essa análise é apenas para exemplo. Para verificar a eficácia da técnica é necessário treinar e testar o modelo de uma forma mais exaustiva, pois muitos dados foram deixados de fora.\nAlém disso, o df é muito desbalanceado, então proporções maiores já podem trazer resultados bons.**"},{"metadata":{"trusted":true,"_uuid":"3b9257201908c765b0788c014b62b9925c54852f"},"cell_type":"code","source":"######\n# Vou criar 2 datasets, o primeiro com classes 3:1 e 2:1\nid_1 = np.where(y_train==1)[0] # ids das linhas com variavel resposta 1\nid_0 = np.where(y_train==0)[0] # ids das linhas com variavel resposta 0\nn_class_1 = len(id_1)  # numero de linhas com variavel resposta 1\n\n# 100:1\nid_subset = np.concatenate([np.random.choice(id_0, size=n_class_1*100), id_1])\nX_train_100_1 = X_train[id_subset]\ny_train_100_1 = y_train[id_subset]\n\n# 10:1\nid_subset = np.concatenate([np.random.choice(id_0, size=n_class_1*10), id_1])\nX_train_10_1 = X_train[id_subset]\ny_train_10_1 = y_train[id_subset]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"399e2f15ab4a7d05b6bcf7fab5b4c13518a8998d"},"cell_type":"markdown","source":"- **Logistic Regression - Undersampling**"},{"metadata":{"_uuid":"67da09eee6e4304c094d5c09b0dae5aa86ad1637"},"cell_type":"markdown","source":"**Data 100:1**"},{"metadata":{"trusted":true,"_uuid":"308af96598988c0c72ca75c826790ed59f5e20d9"},"cell_type":"code","source":"cross_val_model(X_train_100_1, y_train_100_1, lr_model, n_splits=3, CM = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfb6679766ad7efbd8e6abc204bbc705a75dd627"},"cell_type":"markdown","source":"**Data 10:1**"},{"metadata":{"trusted":true,"_uuid":"b023bb6e44b7b9517ad5f3ae801088ea4c74be13"},"cell_type":"code","source":"cross_val_model(X_train_10_1, y_train_10_1, lr_model, n_splits=3, CM = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56d340de50cc15f781359a789a899750dcdc9c00"},"cell_type":"markdown","source":"- Os restultados são aparentemente ótimos!\nMas deve-se tomar muito cuidado pois muitos dados foram inutilizados, então é necessário fazer as validações na base de teste e tentar simular outros diferentes cenários para utilizar um modelo com undersampling."},{"metadata":{"_uuid":"6e05cc71f3ac12d8c9d96fe7b61a4cc1c571d58c"},"cell_type":"markdown","source":"### Criação do Modelos Finais (SKLearn X StatsModels)\nApós serem selecionadas as variáveis, também pode ser realizado a tunagem de parametros por Grid Search, que nada mais é doq uma seleção dos parâmetros por força bruta (loop para testar diferentes valores e analisar o output). Além disso, também é importante decidir a métrica de erro a ser analisada, mas como são questões que dependem muito da base de dados em que se está trabalhando e do problema que queremos resolver (decisão de negócio), não vou explorar.\n\nAlém disso, nesse caso eu utilizei treino/validação e teste, somente treino e validação.\nNovamente pelos motivos de ser apenas um exemplo, pois isso varia muito de acordo com os dados em questão. No caso real, pode se usar o último mês (ou qualquer período em que faça sentido para o negócio) para a base de teste, e todos os outros dados para treino e validação."},{"metadata":{"trusted":true,"_uuid":"046fe3f5bf2360dd7bdcd3e9342ea357a8e2b7a9"},"cell_type":"code","source":"# Variáveis finais após o Feature Selection\nX = np.array(df.loc[:,np.array(var_x)[rfecv.support_]])\ny = np.array(df.loc[:,var_y])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2076d19a0125919643ae774080c8846924606e77"},"cell_type":"markdown","source":"- **SKLearn**"},{"metadata":{"trusted":true,"_uuid":"5d6be9141ed9b59f04934ce9fccca93df6694f74"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(X_train, y_train)\n\npred = lr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec63321f7f4dd7165839199a83be4b2cf88149a0","scrolled":true},"cell_type":"code","source":"PlotCM(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bc1b16fd16d5180b2b7f2056fbed50dd2c50873"},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nskplt.metrics.plot_roc_curve(y_test, lr_model.predict_proba(X_test))\nplt.show()\n\nskplt.metrics.plot_precision_recall_curve(y_test, lr_model.predict_proba(X_test))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171bf02ec14b470ecad5719efad3b2e95d6d9eb5"},"cell_type":"markdown","source":"- **StatsModels**\n___\n**Métodos:**\n- logitreg.summary2()            # summary of the model\n- logitreg.fittedvalues             # fitted value from the model\n- logitreg.predict()                  # predict\n- logfitreg.pred_table()           # confusion matrix"},{"metadata":{"trusted":true,"_uuid":"b50fe57524e47ee312fcfdab5831a888503c3194"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"5a19b165a48d7d61700657fb1ee75cd35f653c76"},"cell_type":"code","source":"import statsmodels.api as sm\n\nlr_model = sm.Logit(y_train, X_train).fit()\npred_proba = lr_model.predict(X_test)\nprint(lr_model.summary2())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42431ea53d3dbc5cd6a8746b355601e4d107546a"},"cell_type":"code","source":"# Odds ratio dos parametros\nprint(\"odds ratio\")\nprint(np.exp(lr_model.params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e8e6bd2fdfcb286c24448ff8a9b10de4846ea79"},"cell_type":"code","source":"# odds ratios and 95% CI\nparams = lr_model.params\nconf = pd.DataFrame(lr_model.conf_int())\nconf['OR'] = params\nconf.columns = ['2.5%', '97.5%', 'OR']\nprint(np.exp(conf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ebbe2d1e599623366cd90cf448f9c846e302686"},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thresholds = roc_curve(y_test, pred_proba)\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbddefb7883d3a20a50e2983906f01578b974ab7"},"cell_type":"markdown","source":"#### Plota a curva ROC e linha 1-fpr\nTambém acha o valor ótimo para o threshold"},{"metadata":{"trusted":true,"_uuid":"8b13e2fd00ba5588c734497f8a3602095d873367"},"cell_type":"code","source":"import pylab as pl\n####################################\n# The optimal cut off would be where tpr is high and fpr is low\n# tpr - (1-fpr) is zero or near to zero is the optimal cut off point\n####################################\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\nprint(roc.ix[(roc.tf-0).abs().argsort()[:1]])\n\n# Plot tpr vs 1-fpr\nfig, ax = pl.subplots()\npl.plot(roc['tpr'])\npl.plot(roc['1-fpr'], color = 'red')\npl.xlabel('1-False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('Receiver operating characteristic')\nax.set_xticklabels([])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed06c55677520d76748b3cd3162891a1f7a0a427"},"cell_type":"markdown","source":"#### Threshold"},{"metadata":{"trusted":true,"_uuid":"d28948b72185d8655b49e2cb42b1518ec57ee98a"},"cell_type":"code","source":"# Função que encontra o valor ótimo do threshold\ndef Find_Optimal_Cutoff(target, predicted):\n    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n    Parameters\n    ----------\n    target : Matrix with dependent or target data, where rows are observations\n\n    predicted : Matrix with predicted data, where rows are observations\n\n    Returns\n    -------     \n    list type, with optimal cutoff value\n\n    \"\"\"\n    fpr, tpr, threshold = roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4f700eb1985cbadb217b104ede7727e88bb4e9d"},"cell_type":"code","source":"# Find optimal probability threshold\nthreshold = Find_Optimal_Cutoff(y_test, pred_proba)\nprint(threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0813fa9718dd89ec0a148748fa6f6908675d2eca"},"cell_type":"code","source":"def applyThresh(x, thresh):\n    if x > thresh:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b779f6f780968195ed1fb42a22e815483acce5e"},"cell_type":"code","source":"# Acha as predicoes (0 e 1) aplicando o pred_proba\npred = np.vectorize(applyThresh)(pred_proba, threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee039d03eaee1ee065ea23e970061e62212fd852"},"cell_type":"code","source":"PlotCM(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b77ebc51e6f5fcbc377db6572b78dcda86e6918"},"cell_type":"markdown","source":"Vemos que a Precision está muito baixa, mas podemos testar outros threshold, além de refinar o modelo analisando os outputs"},{"metadata":{"trusted":true,"_uuid":"bd32a5240e433a0c57c46b9c28b312d9c3ad18f4"},"cell_type":"code","source":"# Acha as predicoes (0 e 1) aplicando o pred_proba\npred = np.vectorize(applyThresh)(pred_proba, 0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4f29efd6c56fc522476444145589f566b55c935"},"cell_type":"code","source":"PlotCM(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f639ea1db88a0308043782818b7a394a6c868101"},"cell_type":"markdown","source":"### Cross Validation no StatsModels\nEncontrei no SO esse \"wraper\" para usar o StatsModels com funcionalidades do SKLearn.\nAqui são somente alguns testes."},{"metadata":{"trusted":true,"_uuid":"c9c6ac868c36cf125f6792d74708da9c3084d59d"},"cell_type":"code","source":"import statsmodels.api as sm\nfrom sklearn.base import BaseEstimator, RegressorMixin\n\nclass SMWrapper(BaseEstimator, RegressorMixin):\n    \"\"\" A universal sklearn-style wrapper for statsmodels regressors \"\"\"\n    def __init__(self, model_class, fit_intercept=True):\n        self.model_class = model_class\n        self.fit_intercept = fit_intercept\n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = sm.add_constant(X)\n        self.model_ = self.model_class(y, X)\n        self.results_ = self.model_.fit()\n    def predict(self, X):\n        if self.fit_intercept:\n            X = sm.add_constant(X)\n        return self.results_.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3998e3a63d56cf0c6faafc8c74b58d0b084dc8d7"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nprint(cross_val_score(SMWrapper(sm.Logit), X_train, y_train, scoring='neg_mean_squared_error'))\n\nteste=cross_val_score(SMWrapper(sm.Logit), X_train, y_train, scoring='neg_mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbd637eba5e266bd4a4a68043883be1ed57d8bb7"},"cell_type":"code","source":"teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9c5c9a34a5d992fd779dd288b6dac5ad9e6e624"},"cell_type":"code","source":"lr_model = SMWrapper(sm.Logit)\nlr_model.fit(X_train, y_train)\n\npred_proba = lr_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea68d1d575eb01f017b7cef74362c60cdf906f2"},"cell_type":"code","source":"Find_Optimal_Cutoff(y_test, pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d6f439452c3c25d172b5cce214524a9806e202c"},"cell_type":"code","source":"# Acha as predicoes (0 e 1) aplicando o pred_proba\npred = np.vectorize(applyThresh)(pred_proba, 0.0011455324286474824)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49b024ca685f1b8236ad96c09e5f0109f85198ec"},"cell_type":"code","source":"PlotCM(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aafa7818c67d563a013d2d44c5995a2fd256366"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}