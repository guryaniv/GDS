{"cells":[{"metadata":{"_uuid":"b4578d48b219735043a4d2102119fb307d2fc83f"},"cell_type":"markdown","source":"# This is a basic Starter Kernel for the New York City Taxi Fare Prediction Playground Competition \nHere we'll use a simple linear model based on the travel vector from the taxi's pickup location to dropoff location which predicts the `fare_amount` of each ride.\n\nThis kernel uses some `pandas` and mostly `numpy` for the critical work.  There are many higher-level libraries you could use instead, for example `sklearn` or `statsmodels`.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Initial Python environment setup...\nimport numpy as np # linear algebra\nimport pandas as pd # CSV file I/O (e.g. pd.read_csv)\nimport os # reading the input files we have access to\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb969a26e52931bcaced3cbb7a36d8d8b1b04556"},"cell_type":"markdown","source":"### Setup training data\nFirst let's read in our training data.  Kernels do not yet support enough memory to load the whole dataset at once, at least using `pd.read_csv`.  The entire dataset is about 55M rows, so we're skipping a good portion of the data, but it's certainly possible to build a model using all the data."},{"metadata":{"trusted":true,"_uuid":"56d80f9e8d4344d4708583b36c395df75c6bde5d","collapsed":true},"cell_type":"code","source":"# df = pd.DataFrame(index=pd.DatetimeIndex(start=dt.datetime(2016,1,1,0,0,1),\n#     end=dt.datetime(2016,1,2,0,0,1), freq='H'))\\\n#     .reset_index().rename(columns={'index':'datetime'})\n\n# df.head()\n# df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f6a52b26a4b46a2090a31a471a2a6c39473bf61","collapsed":true},"cell_type":"code","source":"# df['ts'] = df.datetime.values.astype(np.int64) // 10 ** 9\n# df.head()\n# df.dtypes\n# # t = pd.Timestamp(2017, 1, 1, 12)\n# # ts = t.astye","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv('../input/train.csv', nrows = 10_000_00)\ntrain_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"441087ba449cdfceabcabc12e9b10a32a6cde27a","collapsed":true},"cell_type":"code","source":"#Reverse GeoCoding\n# import pygeocoder\n# from pygeocoder import Geocoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25df18156ed90f583efdbbc028c58a9d2bdfdc7b"},"cell_type":"markdown","source":"Let's create two new features in our training set representing the \"travel vector\" between the start and end points of the taxi ride, in both longitude and latitude coordinates.  We'll take the absolute value since we're only interested in distance traveled. Use a helper function since we'll want to do the same thing for the test set later."},{"metadata":{"trusted":true,"_uuid":"0610376b9d39279e9a11f935be15aa2db39d492b"},"cell_type":"code","source":"#striping the timezone\ndef convert_to_datetime(df):\n    test_time = df['pickup_datetime'].astype(str).str[:-4]\n    df['date_time'] =  pd.to_datetime(test_time, format='%Y%m%d %H:%M:%S')\n    return df\n    \n# converting the object to date time\ntrain_df = convert_to_datetime(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"554db4bc6b4b3af98b3fc2cd3dbbca1fccf1fa8e"},"cell_type":"markdown","source":"Lets describe the df."},{"metadata":{"trusted":true,"_uuid":"e2ab67d8059e0937fb8219e1079af411a6c81bce"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa93b159bdfedabd8d461db9f107b3fad3fa4197"},"cell_type":"markdown","source":"[](http://)Min value of the fare_amount is less than zero and min passenger count is zero. We should discard those values."},{"metadata":{"trusted":true,"_uuid":"e164df495005614341bef8fcd1c54686430de8b7"},"cell_type":"code","source":"def normalize_fare_passenger(df):\n    if 'fare_amount' in df.columns:\n        print(\"old lenght: %d\" %len(df))\n        df = df[df.fare_amount>0]\n    print(\"length after fare_amount normalization: %d\" %len(df))\n    df = df[df.passenger_count>0]\n    print(\"length after passenger_count normalization: %d\" %len(df))\n    return df\n\ntrain_df = normalize_fare_passenger(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59f0595db44dd60044cfd0404824651a7c2bee87","collapsed":true},"cell_type":"code","source":"# Given a dataframe, add two new features 'abs_diff_longitude' and\n# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n# the pickup location to the dropoff location.\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n    return df\n\ntrain_df = add_travel_vector_features(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1dbc7610bd467f1dfaf9042b5ec638eb2014aaf"},"cell_type":"markdown","source":"### Explore and prune outliers\nFirst let's see if there are any `NaN`s in the dataset."},{"metadata":{"trusted":true,"_uuid":"e808c7e75338b45ca30f9f261dfbc90845700624"},"cell_type":"code","source":"print(train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29bc86f2fa8baa37f0c4eb4300f77a8cb69f12aa"},"cell_type":"markdown","source":"There are a small amount, so let's remove them from the dataset."},{"metadata":{"trusted":true,"_uuid":"9d8f28e24f3d4ca55ad93692329680774c341376"},"cell_type":"code","source":"print('Old size: %d' % len(train_df))\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\nprint('New size: %d' % len(train_df))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a045ef14c636ec726a5e8c349ca7e5fbb3a87c1"},"cell_type":"markdown","source":"Now let's quickly plot a subset of our travel vector features to see its distribution."},{"metadata":{"trusted":true,"_uuid":"97d0aa1deab1c6cf0c97a4a3a12ba7007aada6c5"},"cell_type":"code","source":"plot = train_df.iloc[1:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22277d77f75e3177a5acaec9b820e0de6e869663"},"cell_type":"markdown","source":"We expect most of these values to be very small (likely between 0 and 1) since it should all be differences between GPS coordinates within one city.  For reference, one degree of latitude is about 69 miles.  However, we can see the dataset has extreme values which do not make sense.  Let's remove those values from our training set. Based on the scatterplot, it looks like we can safely exclude values above 5 (though remember the scatterplot is only showing the first 2000 rows...)"},{"metadata":{"trusted":true,"_uuid":"9703895e6c7e67b32c504f843b5ef19be2023964"},"cell_type":"code","source":"print('Old size: %d' % len(train_df))\ntrain_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\nprint('New size: %d' % len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b00bddf5fcf370f53e8a69f2fc69dbb620f18bc8"},"cell_type":"markdown","source":"Lets draw some histogram to get an idea about the fare_amount range"},{"metadata":{"trusted":true,"_uuid":"683f19b9274dfdcc1246fe8fc0be1a978424762a"},"cell_type":"code","source":"train_df[train_df.fare_amount<60].fare_amount.hist(bins=200, figsize=(14,3))\nplt.xlabel('fare $USD')\nplt.title('Histogram');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b772c054a6f248d68fd4d32d2541a79404353278"},"cell_type":"markdown","source":"Now fare_amout should be directly related to the distance covered in the trip. Lets try to find out the distance covered in each trip\n"},{"metadata":{"trusted":true,"_uuid":"32e901704e21c6312b2db1ca63f9b9f129f4b65d"},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc36a1b634b4bb8c6e3c1874a0bb27c700ed1a08"},"cell_type":"code","source":"#this is a kind of haversine formula to calculate the spherical distance\ndef haversine_distance_calculation(df):\n    lon1, lat1, lon2, lat2 = df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude']\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    distance = 6367 * c\n    df['distance'] = distance\n    return df\ntrain_df = haversine_distance_calculation(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e151bfa1d36b5fdf0a32266898d5b76527f6e43d"},"cell_type":"markdown","source":"Now lets try to plot the relation between distance and fare_amoutn"},{"metadata":{"trusted":true,"_uuid":"d8d46d0105f76d267d9b1563589e142bdc1642f3"},"cell_type":"code","source":"train_df.plot(x='distance', y='fare_amount', style = 'o')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"432814498d1309ce2ac5cc254ecebda5951aa65e"},"cell_type":"markdown","source":"Now we are seeing more outliers. Like distance more than 40km but fare_amount is very less and also distance very less but fare_amount is high. We should prune thoes also. So we will round up the distance within 50km and fare within 100 USD"},{"metadata":{"trusted":true,"_uuid":"028ef22884870500021e44f3c0e3dad70e0efb35"},"cell_type":"code","source":"def distance_fare_normalization(df):\n    print(\"old lenght with distance greated than 50km: %d\" %len(df))\n    df = df[df.distance<50]\n    print(\"length after distance normalization: %d\" %len(df))\n    if 'fare_amount' in df.columns:\n        df = df[df.fare_amount<100]\n        print(\"length after fare_amount normalization: %d\" %len(df))\n    return df\n\ntrain_df = distance_fare_normalization(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0a58ff9a893e679410ca8504de239b3193dc421"},"cell_type":"markdown","source":"Now lets check the graph again"},{"metadata":{"trusted":true,"_uuid":"e9cf97dadf3ead56c375a34d40802e2059c8563d"},"cell_type":"code","source":"train_df.plot(x='distance', y='fare_amount', style = 'o')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20b5c95a3bd626d89b73d936bdbdc82efb074d3c"},"cell_type":"markdown","source":"Now we can try to find any relation between time and fare_amount. We can find the day of year and day of week from date_time and correlate them with. Lets try."},{"metadata":{"trusted":true,"_uuid":"04545d0e2b3583ea1b6f41dab285e9e79430d4d8"},"cell_type":"code","source":"def day_converter(df):\n    day_of_year = df['date_time'].dt.dayofyear\n    day_of_week = df['date_time'].dt.dayofweek\n    hour_of_day = df['date_time'].dt.hour\n    df['day_of_year'] = day_of_year\n    df['day_of_week'] = day_of_week\n    df['hour_of_day'] = hour_of_day\n    return df\n\n    \ntrain_df = day_converter(train_df)\ntrain_df.plot(x='day_of_year', y='fare_amount', style = 'o')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2151480a168d291bc2f4fd014fdac4ab7b5f6560"},"cell_type":"markdown","source":"### Train our model\nOur model will take the form $X \\cdot w = y$ where $X$ is a matrix of input features, and $y$ is a column of the target variable, `fare_amount`, for each row. The weight column $w$ is what we will \"learn\".\n\nFirst let's setup our input matrix $X$ and target column $y$ from our training set.  The matrix $X$ should consist of the two GPS coordinate differences, plus a third term of 1 to allow the model to learn a constant bias term.  The column $y$ should consist of the target `fare_amount` values."},{"metadata":{"trusted":true,"_uuid":"fb752441a1c1ce3e01d78452389ec48c95d52dc6"},"cell_type":"code","source":"# Construct and return an Nx3 input matrix for our linear model\n# using the travel vector, plus a 1.0 for a constant bias term.\n##MH:adding pickup_datetime in train_df\ndef get_input_matrix(df):\n    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, df.distance, df.day_of_year, df.day_of_week,df.hour_of_day, np.ones(len(df))))\n\ntrain_X = get_input_matrix(train_df)\ntrain_y = np.array(train_df['fare_amount'])\n\nprint(train_X.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dd0d8fe3f6478c050079df934381317a65f7d2b"},"cell_type":"markdown","source":"Now let's use `numpy`'s `lstsq` library function to find the optimal weight column $w$."},{"metadata":{"trusted":true,"_uuid":"85abbb09a27d2e1e2a15b261264b3c7cbdde39e4"},"cell_type":"code","source":"# The lstsq function returns several things, and we only care about the actual weight vector w.\n(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\nprint(w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c11c9993467cd31c6be525f864eae24b0da364d"},"cell_type":"markdown","source":"These weights pass a quick sanity check, since we'd expect the first two values -- the weights for the absolute longitude and latitude differences -- to be positive, as more distance should imply a higher fare, and we'd expect the bias term to loosely represent the cost of a very short ride.\n\nSidenote:  we can actually calculate the weight column $w$ directly using the [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) method:\n$w = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$"},{"metadata":{"trusted":true,"_uuid":"4a629cdacdddd48a7ba9e8492b0e748cde819829"},"cell_type":"code","source":"w_OLS = np.matmul(np.matmul(np.linalg.inv(np.matmul(train_X.T, train_X)), train_X.T), train_y)\nprint(w_OLS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00fb113f2e7dda9ba84e85f6d8c29f9103be61b1"},"cell_type":"markdown","source":"* Using a neural network with tensorflowframework"},{"metadata":{"trusted":true,"_uuid":"dc0109bb1bfbdcf8e295f8adef8e2c0789561b8e"},"cell_type":"code","source":"import tensorflow as tf\nimport math\nimport matplotlib.pyplot as plt\nimport h5py\n\n\n#Creating placeholders\nx = tf.placeholder(tf.float32, [7, None])\ny = tf.placeholder(tf.float32, [1, None])\n\n#defining hyper params\nlearning_rate = 0.0001\nnum_epochs = 2000\nminibatch_size = 1024\n\n#defining trainabl variables\nW1 = tf.Variable(tf.random_normal([10, 7], stddev=0.03), name='W1')\nb1 = tf.Variable(tf.random_normal([10,1]), name='b1')\n# and the weights connecting the hidden layer to the output layer\nW2 = tf.Variable(tf.random_normal([6, 10], stddev=0.03), name='W2')\nb2 = tf.Variable(tf.random_normal([6,1]), name='b2')\n\nW3 = tf.Variable(tf.random_normal([4, 6], stddev=0.03), name='W3')\nb3 = tf.Variable(tf.random_normal([4,1]), name='b3')\n\nparameters = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3\n             }\n\n#transposing to adjust size\ntrain_X = np.transpose(train_X)\ntrain_y = np.transpose(train_y)\n\n\nprint(np.shape(train_X))\nprint(np.shape(train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"490fb1caf20d13c013855f1e4142f016629fad7c","collapsed":true},"cell_type":"code","source":"\ndef random_mini_batches(X, Y, mini_batch_size):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[1]                  # number of training examples\n#     print(m)\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n#     print(np.shape(permutation))\n    #print(permutation)\n    shuffled_X = X[:, permutation]\n#     print(np.shape(shuffled_X))\n    shuffled_Y = Y[permutation].reshape((1,m))\n#     print(np.shape(shuffled_Y))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n#     print(num_complete_minibatches)\n    for k in range(0, num_complete_minibatches):\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:, (k*mini_batch_size):((k+1)*mini_batch_size)]\n        mini_batch_Y = shuffled_Y[:, (k*mini_batch_size):((k+1)*mini_batch_size)]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:,(num_complete_minibatches*mini_batch_size):m]\n        mini_batch_Y = shuffled_Y[:,(num_complete_minibatches*mini_batch_size):m]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\n# minibatches = random_mini_batches(train_X,train_y,minibatch_size)\n# for minibatch in minibatches:\n#     (minibatch_X, minibatch_Y) = minibatch\n#     print(np.shape(minibatch_X))\n#     print(np.shape(minibatch_Y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"482a3fad2c19d852d865890fc02bf00a5bd8515a"},"cell_type":"code","source":"# now declare the weights connecting the input to the hidden layer\n\n# calculate the output of the hidden layer\ndef forward_propagation(X, W1,b1,W2,b2, W3, b3):\n    Z1 = tf.add(tf.matmul(W1, X),b1)                                              # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)  \n#     print(np.shape(A1))\n#     print(np.shape(W2))\n    Z2 = tf.add(tf.matmul(W2, A1),b2) \n    A2 = tf.nn.relu(Z2)\n    \n    Z3 = tf.add(tf.matmul(W3,A2),b3)\n    return Z3\n\ndef compute_cost(Y_h, Y):\n    logits = tf.transpose(Y_h)\n    labels = tf.transpose(Y)\n#     logits = Y_h\n#     labels = Y\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits, labels = labels))\n    ### END CODE HERE ###    \n    return cost\n\n\n\n# finally setup the initialisation operator\n# init_op = tf.global_variables_initializer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"390b00f9d74da6fe3a4b550f9f161d33580ad812"},"cell_type":"code","source":"# start the session\ncosts =[]\nZ3 = forward_propagation(x, W1,b1,W2,b2, W3, b3)\nS = (tf.nn.l2_loss(W1) +tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3))*0.01\ncost = compute_cost(Z3, y) + S\nm = train_X.shape[1]\n# m = train_X.shape[1]\n#Defining optimizer\noptimizer =tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\ninit = tf.global_variables_initializer()\nprint_cost = True\n\n# print(minibatches)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(num_epochs):\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            minibatches = random_mini_batches(train_X, train_y, minibatch_size)\n\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch                \n                _ , minibatch_cost = sess.run([optimizer, cost],feed_dict={x:minibatch_X,y:minibatch_Y}) #here x,y are the defined placeholders\n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 500 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n            \n            \n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n\n    # lets save the parameters in a variable\n    parameters = sess.run(parameters)\n    print (\"Parameters have been trained!\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"37d56e88cf0e5b85bfe0e75c9efdc65cb462ac8d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a70ed21b43d720282bbae70e934b1188be2bc382"},"cell_type":"markdown","source":"### Make predictions on the test set\nNow let's load up our test inputs and predict the `fare_amount`s for them using our learned weights!"},{"metadata":{"_uuid":"75dcc05a5e9216191a0565d1593934206705e7cb"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"3cbf4836cf8c71dfb67d13a9621b18a8d487197e"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e0a966354a3476219ea8600651eaa40c032d2695"},"cell_type":"code","source":"def predict(X,parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    Z3 = forward_propagate(X,W1,b1,W2,b2,W3,b3)\n    out = tf.nn.sigmoid(Z3)\n    return out\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddba4a856ff617411a641dfdf7635e47f969dff8"},"cell_type":"code","source":"# Reuse the above helper functions to add our features and generate the input matrix.\nadd_travel_vector_features(test_df)\n#\n##converting to date time\ntest_df = convert_to_datetime(test_df)\n##Normalizing fare_amount>0 & passenger_count>0\ntest_df = normalize_fare_passenger(test_df)\n## dropping null values\ntest_df = test_df.dropna(how = 'any', axis = 'rows')\ntest_df =haversine_distance_calculation(test_df)\n##normalization distance>50km and fare>100usd\ntest_df = distance_fare_normalization(test_df)\n#converting date time to day vactor\ntest_df = day_converter(test_df)\n\ntest_X = get_input_matrix(test_df)\n# Predict fare_amount on the test set using our model (w) trained on the training set.\n# test_y_predictions = np.matmul(test_X, w).round(decimals = 2)\ntest_y_predictions = predict(test_X, parameters).round(decimals = 2)\n\n# Write the predictions to a CSV file which we can submit to the competition.\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_y_predictions},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)\n\nprint(os.listdir('.'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80ed89470e25d75c0b99008b9c88861be9739da3"},"cell_type":"markdown","source":"## Ideas for Improvement\nThe output here will score an RMSE of $5.74, but you can do better than that!  Here are some suggestions:\n\n* Use more columns from the input data.  Here we're only using the start/end GPS points from columns `[pickup|dropoff]_[latitude|longitude]`.  Try to see if the other columns -- `pickup_datetime` and `passenger_count` -- can help improve your results.\n* Use absolute location data rather than relative.  Here we're only looking at the difference between the start and end points, but maybe the actual values -- indicating where in NYC the taxi is traveling -- would be useful.\n* Use a non-linear model to capture more intricacies within the data.\n* Try to find more outliers to prune, or construct useful feature crosses.\n* Use the entire dataset -- here we're only using about 20% of the training data!"},{"metadata":{"_uuid":"8fd559ff5ca72a73091d5dfd5b7032522832e999"},"cell_type":"markdown","source":"Special thanks to Dan Becker, Will Cukierski, and Julia Elliot for reviewing this Kernel and providing suggestions!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}