{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"'''\nCredit to this kernel:\nhttps://www.kaggle.com/remidi/neural-compression-auto-encoder-lb-0-55/code\n\nI do some change and make it work for Santander . It is my first time to use denoising autoencoder.\nPlease provide feedback and upvote if you like it :)\n'''"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import NMF\nfrom sklearn.cluster import FeatureAgglomeration\nimport scipy\nfrom sklearn.ensemble import RandomForestRegressor\nimport random\n\nrandom.seed(1234)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# copy from https://www.kaggle.com/mathormad/knowledge-distillation-with-nn-rankgauss\nclass GaussRankScaler():\n\n    def __init__( self ):\n        self.epsilon = 1e-9\n        self.lower = -1 + self.epsilon\n        self.upper =  1 - self.epsilon\n        self.range = self.upper - self.lower\n\n    def fit_transform( self, X ):\n\n        i = np.argsort( X, axis = 0 )\n        j = np.argsort( i, axis = 0 )\n\n        assert ( j.min() == 0 ).all()\n        assert ( j.max() == len( j ) - 1 ).all()\n\n        j_range = len( j ) - 1\n        self.divider = j_range / self.range\n\n        transformed = j / self.divider\n        transformed = transformed - self.upper\n        transformed = scipy.special.erfinv( transformed )\n        ############\n        # transformed = transformed - np.mean(transformed)\n\n        return transformed\n\ntarget_col='target'\nid_col='ID_code'\nsubmission = pd.read_csv('../input/sample_submission.csv')\nid_test = submission[id_col].values\n# function for auto encoder with a compressed components n_comp = 12\ndef neural_compression_v2(train, test):\n    dataset = pd.concat([train.drop(target_col, axis=1), test], axis=0)\n    ids = dataset[id_col]\n    dataset.drop(id_col, axis=1, inplace=True)\n    y_train = train[target_col]\n    \n    cat_vars = [c for c in dataset.columns if dataset[c].dtype == 'object']\n    for c in cat_vars:\n        t_data = pd.get_dummies(dataset[c], prefix=c)\n        dataset = pd.concat([dataset, t_data], axis=1)\n\n    dataset.drop(cat_vars, axis=1, inplace=True)\n    # We scale both train and test data so that our NN works better.\n    sc = StandardScaler()\n#     sc = GaussRankScaler()# Gauss Rank does not work...\n    sc.fit_transform(dataset)\n\n    dataset = sc.fit_transform(dataset)\n\n    train = dataset[:train.shape[0]]\n    test = dataset[train.shape[0]:]\n\n    print(\"one hot encoded train shape :: {}\".format(train.shape))\n    print(\"one hot encoded test shape :: {}\".format(test.shape))\n    \n    ''' neural network compression code '''\n    \n    import keras\n    from keras import regularizers\n    from keras.layers import Input, Dense,BatchNormalization,Dropout\n    from keras.models import Model\n    from keras.regularizers import l2\n    # adding some noise to data before feed them to nn\n    train = train + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=train.shape) \n    test = test + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=test.shape)\n    l2_reg_embedding = 1e-5\n    print(keras.__version__)\n    init_dim = train.shape[1]\n\n    input_row = Input(shape=(init_dim, ))\n    encoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(input_row)\n    encoded = Dropout(0.2)(encoded)\n    encoded=BatchNormalization()(encoded)\n    encoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dropout(0.2)(encoded)\n    encoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dropout(0.2)(encoded)\n    encoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dropout(0.2)(encoded)\n    encoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n\n    encoded = Dense(16, activation='elu')(encoded)\n    \n    decoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dropout(0.2)(encoded)\n    decoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    encoded = Dropout(0.2)(encoded)\n    decoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    encoded = Dropout(0.2)(encoded)\n    decoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    encoded = Dropout(0.2)(encoded)\n    encoded = BatchNormalization()(encoded)\n    decoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    decoded = Dense(init_dim, activation='sigmoid')(decoded)\n\n    autoencoder = Model(inputs=input_row, outputs=decoded)\n    autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n    #we use the train data to train\n    autoencoder.fit(train, train,\n                    batch_size=512,verbose=2,\n                    shuffle=True, validation_data=(test, test), epochs=4)\n\n    # compressing the data\n    encoder = Model(inputs=input_row, outputs=encoded)\n    train_compress = encoder.predict(train,batch_size=4048)\n    test_compress = encoder.predict(test,batch_size=4048)\n\n    # denoising the data\n    denoised_train = autoencoder.predict(train,batch_size=4048)\n    denoised_test = autoencoder.predict(test,batch_size=4048)\n    \n    return train_compress, test_compress, denoised_train, denoised_test\n\ntrain_compress, test_compress, denoised_train, denoised_test = neural_compression_v2(train, test)\n\n\n\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n        \n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train.drop([target_col], axis=1))\ntsvd_results_test = tsvd.transform(test)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train.drop([target_col], axis=1))\npca2_results_test = pca.transform(test)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train.drop([target_col], axis=1))\nica2_results_test = ica.transform(test)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train.drop([target_col], axis=1))\ngrp_results_test = grp.transform(test)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train.drop([target_col], axis=1))\nsrp_results_test = srp.transform(test)\n\n# NMF\n# nmf = NMF(n_components=n_comp, init='nndsvdar', random_state=420)\n# nmf_results_train = nmf.fit_transform(train.drop([target_col], axis=1))\n# nmf_results_test = nmf.transform(test)\n\n# FAG\nfag = FeatureAgglomeration(n_clusters=n_comp, linkage='ward')\nfag_results_train = fag.fit_transform(train.drop([target_col], axis=1))\nfag_results_test = fag.transform(test)\n\nusable_columns = list(set(train.columns) - set([target_col]))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n    \n    # train['nmf_' + str(i)] = nmf_results_train[:, i - 1]\n    # test['nmf_' + str(i)] = nmf_results_test[:, i - 1]\n    \n#     train['fag_' + str(i)] = fag_results_train[:, i - 1]\n#     test['fag_' + str(i)] = fag_results_test[:, i - 1]\n\nfor j in range(1, train_compress.shape[1]):\n#     train['aen_' + str(j)] = train_compress[:, j-1]\n#     test['aen_' + str(j)] = test_compress[:, j-1]\n    train['aen_' + str(j)] = denoised_train[:, j-1]\n    test['aen_' + str(j)] = denoised_test[:, j-1]\n    \n    \n    \n\n\ny = train[target_col].values\n\n\n# finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \nfinaltrainset = train[usable_columns].values\nfinaltestset = test[usable_columns].values\n\n#--training & test stratified split\nX=train.drop(target_col, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bc7facb09c4f4d27137b363826edaf8e078127f"},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,stratify=y, test_size=0.10,random_state=2019)\n\n\n'''Train the xgb model then predict the test data'''\nprint('running ....... ')\n\n# lgb_params = {\n#     'num_leaves': 25,\n#     'learning_rate': 0.02,\n#     'max_depth': 2,#7\n#     'subsample': 0.8,\n#     'colsample_bytree':0.8,\n#     'objective': 'binary',\n#     'eval_freq':100,\n#     'verbose': 0,\n#     'metric': 'auc',\n\n# }\nlgb_params = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }\n\n\nimport lightgbm as lgb\ndtrain = lgb.Dataset(X_train, y_train)\ndvalid = lgb.Dataset(X_valid, y_valid)\n# dtest = lgb.Dataset(test)\n\nnum_boost_rounds = 50000# maybe overfitting\n# train model\nmodel = lgb.train(lgb_params, dtrain,valid_sets=dvalid, num_boost_round=num_boost_rounds,early_stopping_rounds=500)\ny_pred = model.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041f182522d563ffed5c0d9faf7058a8a37135d9"},"cell_type":"code","source":"'''Train the stacked models then predict the test data'''\n\nstacked_pipeline = make_pipeline(\n    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n    LassoLarsCV()\n)\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(finaltrainset, y,stratify=y, test_size=0.10,random_state=2019)\nstacked_pipeline.fit(X_train2, y_train2)\nresults = stacked_pipeline.predict(finaltestset)\n\n'''auc Score on the entire Train data when averaging'''\n\nprint('auc score on train data:')\nlgb_preds=model.predict(X_valid)\nstack_score=roc_auc_score(y_valid2,stacked_pipeline.predict(X_valid2)*0.2855 + lgb_preds*0.7145)\nprint('stack_score:{}'.format(stack_score))\nscore = roc_auc_score(y_valid, lgb_preds)\nprint(\"lgboost score : {}\".format(score))\n\n\n'''Save submission to csv file'''\nsub = pd.DataFrame()\nsub[id_col] = id_test\n# sub[target_col] = y_pred*0.75 + results*0.25\nsub[target_col] = y_pred\nsub.to_csv('lgb_stack_submission{}.csv'.format(stack_score), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a42338605bd66612d9815cbd973b5d2312ad0ea5"},"cell_type":"markdown","source":"It seems that stacked_pipeline make performance worse. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}