{"cells":[{"metadata":{"_uuid":"a8a7c6528e9923407a2ac42471bffeaf3ee32679"},"cell_type":"markdown","source":"# Predicting points based on description (NLP) and other features with Catboost"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34db8811fbdfe85242c9c86ff7f36ca9067f85bb"},"cell_type":"markdown","source":"First of all, we are going to load our data and clean the dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/winemag-data-130k-v2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a46224f97fffd44581a0ba2154a18ceddf3d44fc","collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b9055bd2dbe718f8806b487bf1705cb55afc512"},"cell_type":"markdown","source":"We can see that we have a lot of null objects. Let's print some percentage."},{"metadata":{"trusted":true,"_uuid":"6c96bb9077039c78a6f18b45db2fe888a6601a8c","collapsed":true},"cell_type":"code","source":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2e7241ebd3b8e9586c119111538618b8b81c82"},"cell_type":"markdown","source":"I'm worried the most about wines with NaN in price columns. We don't want to predict points for wines which price are undeclared. We will drop rows with NaN value in this column. Another technique is to fill that values with mean, but my approch is to deal with only price taged wines."},{"metadata":{"trusted":true,"_uuid":"88695da049b1cfbfee5fa5f37b58323a5e8ff5c6","collapsed":true},"cell_type":"code","source":"data=data.dropna(subset=['price'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7c28f0c6ccabfc64c5ae664f2c82f2648dcb08"},"cell_type":"markdown","source":"We can easily see that there are a lot of duplicates in the data, which we want to rid of."},{"metadata":{"trusted":true,"_uuid":"51838c6a33f2405f9d983b54055a981c87ac6140","collapsed":true},"cell_type":"code","source":"print(\"Total number of examples: \", data.shape[0])\nprint(\"Number of examples with the same title and description: \", data[data.duplicated(['description','title'])].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fba0b411165819103f0ddc93a8d4afb6ac48b8dd"},"cell_type":"code","source":"data=data.drop_duplicates(['description','title'])\ndata=data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efbebd7f3e446c39c8a3936562d8fd83d08df9e2"},"cell_type":"markdown","source":"Fill all missing values with -1. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05de805d22e0bb89ebfa953a13b18e078674a3e4"},"cell_type":"code","source":"data=data.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"187c7786e47b00ab0c94bdbba2fe027f191ff5f0"},"cell_type":"markdown","source":"# NLP\nOur basic features are ready, so now we start to create features from description with using NLTK library.\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\n"},{"metadata":{"trusted":true,"_uuid":"6816ed0f935779feec1522abc9f31b08a3ef8395","collapsed":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport string\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\n\nfrom nltk.tokenize import RegexpTokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7df6a941dd49db86a274736bf0bcb41f42440c"},"cell_type":"markdown","source":"We have to turn evry word into lowercase because there is no meaning diffrence between 'This' and 'this' term. We also get rid of irrelevent term."},{"metadata":{"trusted":true,"_uuid":"d0a950d978c6b3206376b30905042b9f3d046e02","collapsed":true},"cell_type":"code","source":"data['description']= data['description'].str.lower()\ndata['description']= data['description'].apply(lambda elem: re.sub('[^a-zA-Z]',' ', elem))  \ndata['description']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c88d0cae1616cfb2dfeb6c29c9dbc927c891fa6"},"cell_type":"markdown","source":"We can't analyze whole sentences, we will use regex to tokenize sentences to list of words."},{"metadata":{"trusted":true,"_uuid":"bb18ae1ab19fc6b6b2d8f53f2f1db04bc5ae2433","collapsed":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nwords_descriptions = data['description'].apply(tokenizer.tokenize)\nwords_descriptions.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf73aaf3d935b0934d8d84c4d6bf74f8fdd66917"},"cell_type":"markdown","source":"When we split description into individual words, we have to create vocabulary and additionaly we can add new feature - description lengths."},{"metadata":{"trusted":true,"_uuid":"c90fcac6315bd41fdf62c359db6a84d410dc07cb","collapsed":true},"cell_type":"code","source":"all_words = [word for tokens in words_descriptions for word in tokens]\ndata['description_lengths']= [len(tokens) for tokens in words_descriptions]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16f8a4a5ea21e877d0a847f48e548a1f66e332bc"},"cell_type":"markdown","source":"Let's check what are our most common words in our dictionary."},{"metadata":{"trusted":true,"_uuid":"2e36207a4148bf41368ed6ec66226f00a5767373","collapsed":true},"cell_type":"code","source":"from collections import Counter\ncount_all_words = Counter(all_words)\ncount_all_words.most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d9aeebd7feeb929088ea55d07124626cc9fda4"},"cell_type":"markdown","source":"We can see that there are many stop words and words which can't help us with our goal - predict points. \nNow we want to\n1. Convert words with same meaning to the one word(example run, running, runned -> run). We will use PorterStemmer from NLTK library.\n2. Delete all stopwords.\n"},{"metadata":{"trusted":true,"_uuid":"d0b244e4a7f750c7d4146269a7e73e79d0d36e0a","collapsed":true},"cell_type":"code","source":"stopword_list = stopwords.words('english')\nps = PorterStemmer()\nwords_descriptions = words_descriptions.apply(lambda elem: [word for word in elem if not word in stopword_list])\nwords_descriptions = words_descriptions.apply(lambda elem: [ps.stem(word) for word in elem])\ndata['description_cleaned'] = words_descriptions.apply(lambda elem: ' '.join(elem))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b39a3090275ea9829feb4ac0c4818a98c31840bf","collapsed":true},"cell_type":"code","source":"all_words = [word for tokens in words_descriptions for word in tokens]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\ncount_all_words = Counter(all_words)\ncount_all_words.most_common(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"526e0112ec031590725aec4ba54372617fb5a7fd"},"cell_type":"markdown","source":"As we can see we deleted almost 9k words and now words from description are much more meaningful.\nNow we can 3 diffrent ways to represent our description\n\n1. **Bag of Words Counts** - embeds each sentences as a list of 0 or 1,  1 represent containing word. \n2. **TF-IDF (Term Frequency, Inverse Document Frequency)** - weighing words by how frequent they are in our dataset, discounting words that are too frequent.\n3. **Word2Vec **- Capturing semantic meaning. We won't use it in this kernel.\n\nWe will check which types perform better in our case, Bag of Words Counts or TF-IDF Bag of Words.\n\nFirst we will test Bag of Words Counts.\n\nLet's define some useful function and then test our picked techniques.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"263347ba58de75c86b65886357729fa21e6ed87a"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostRegressor, cv\n\ndef prepare_dataframe(vect, data, features=True):\n    vectorized=vect.fit_transform(data['description_cleaned']).toarray()\n    vectorized=pd.DataFrame(vectorized)\n    if features == True:\n        X=data.drop(columns=['points','Unnamed: 0','description','description_cleaned'])\n        X=X.fillna(-1)\n        print(X.columns)\n        X=pd.concat([X.reset_index(drop=True),vectorized.reset_index(drop=True)],axis=1)\n        categorical_features_indices =[0,1,3,4,5,6,7,8,9,10]\n    else:\n        X=vectorized\n        categorical_features_indices =[]\n    y=data['points']\n    return X,y,categorical_features_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8ded5683a180a94a5c262c474fb76bc33c93824","collapsed":true},"cell_type":"code","source":"#model definintion and training.\ndef perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test,categorical_features_indices,name):\n    model = CatBoostRegressor(\n        random_seed = 100,\n        loss_function = 'RMSE',\n        iterations=800,\n    )\n    \n    model.fit(\n        X_train, y_train,\n        cat_features = categorical_features_indices,\n        verbose=False,\n        eval_set=(X_valid, y_valid)\n    )\n    \n    print(name+\" technique RMSE on training data: \"+ model.score(X_train, y_train).astype(str))\n    print(name+\" technique RMSE on test data: \"+ model.score(X_test, y_test).astype(str))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6027dd40421e3e0d24a504e46cfc7256e82cf0e"},"cell_type":"code","source":"def prepare_variable(vect, data, features_append=True):\n    X, y , categorical_features_indices = prepare_dataframe(vect, data,features_append)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, \n                                                        random_state=42)\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, \n                                                        random_state=52)\n    return X_train, y_train,X_valid, y_valid,X_test, y_test, categorical_features_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"020c5f00f339e8a0ce1890cae5d8d52dd7382caf","collapsed":true},"cell_type":"code","source":"vect= CountVectorizer(analyzer='word', token_pattern=r'\\w+',max_features=500)\ntraining_variable=prepare_variable(vect, data)\nperform_model(*training_variable, 'Bag of Words Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5a433523b8bc7029e9996d127fe7fae4ab04017"},"cell_type":"markdown","source":"Now we can try TF-IDF."},{"metadata":{"trusted":true,"_uuid":"7e68b71556689df5e1802d457f50bf2d6cc3076e","collapsed":true},"cell_type":"code","source":"vect= TfidfVectorizer(analyzer='word', token_pattern=r'\\w+',max_features=500)\ntraining_variable=prepare_variable(vect, data)\nperform_model(*training_variable, 'TF-IDF')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5be9e5ab37f54035a5896456758c0a01a97e363b"},"cell_type":"markdown","source":"Yeah, but beyond description we used also meaningful features, let's drop all of our features and do prediction based ONLY on descriptions. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cae27aacac029dcd1861f6ff48d5599c9c6b8210"},"cell_type":"code","source":"vect= CountVectorizer(analyzer='word', token_pattern=r'\\w+',max_features=500)\ntraining_variable=prepare_variable(vect, data, False)\nperform_model(*training_variable, 'Bag of Words Counts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"39047ebc7236ef0bfe480ee65c82d994e1d30978"},"cell_type":"code","source":"vect= TfidfVectorizer(analyzer='word', token_pattern=r'\\w+',max_features=500)\ntraining_variable=prepare_variable(vect, data, False)\nperform_model(*training_variable, 'TF-IDF')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6ad44cef4cf684a11217dd8a96b93fbbbd562df1"},"cell_type":"markdown","source":"As we can see our scores are similar, but it really outperformet technique without any NLP operations (about 2.09 test score) \n* 1. link to EDA +  Catboost without NLP : https://www.kaggle.com/mistrzuniu1/eda-catboost-feature-importance/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}