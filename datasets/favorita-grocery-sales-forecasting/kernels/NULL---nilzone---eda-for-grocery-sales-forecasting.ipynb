{"cells": [{"metadata": {"collapsed": true, "_uuid": "47721e28d447a468bef7bab49108318d9d16343b", "_cell_guid": "7df94848-3543-4447-b769-85587a74da6a"}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "77b0bbbaa1fd76554e81ba527ae26d35e141b796", "_cell_guid": "562062ca-bef3-4724-bc01-98ce48193171"}, "source": ["## Training-set\n", "---"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "abd2d1348a32851ffd8963627868a64058dad18c", "_cell_guid": "17312225-72ad-49d2-8d3c-a17e1100c983"}, "outputs": [], "source": ["#Changed the data-types in order to minimize the file-size\n", "types = {\n", "    'id': 'uint32',\n", "    'item_nbr': 'uint32',\n", "    'store_nbr': 'uint16',\n", "    'unit_sales': 'float32',\n", "    'onpromotion': bool,\n", "}\n", "df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['date'], dtype=types, infer_datetime_format=True)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "374188739756395a335a91b2287b6b432bc2dbac", "_cell_guid": "61152734-393d-4bb0-8986-1c2aa0c02214"}, "outputs": [], "source": ["df_train.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "639b7b3b7c2c4ffacf15c126d3805b5abf59a3a0", "_cell_guid": "7b055753-bafd-485b-b33e-017e5a2fca10"}, "outputs": [], "source": ["print(\"{} datapoints and {} features\".format(*df_train.shape))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "eede1ebc951d6ca0b4a1307b9cdd4b23d9029fa3", "_cell_guid": "d1ef6159-645f-493a-94f6-92f6f0ae3722"}, "source": ["That is quite a few datapoints. But first, let's cast the date-feature to a pandas datetime so we can extract day, week, month, year and so on, as well as do some sampling :"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "853245281a94ed161050e765aa0579f42405090e", "_cell_guid": "5f3a2535-c4e4-4e3e-b53b-b08c04597aed"}, "outputs": [], "source": ["df_train.loc[:, \"year\"] = df_train[\"date\"].dt.year.astype(\"uint16\")\n", "df_train.loc[:, \"week\"] = df_train[\"date\"].dt.week.astype(\"uint16\")\n", "df_train.loc[:, \"weekday\"] = df_train[\"date\"].dt.weekday.astype(\"uint16\")\n", "df_train.loc[:, \"month\"] = df_train[\"date\"].dt.month.astype(\"uint16\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "8ba14631efc23d5f64da4435dbec30a723cf3131", "_cell_guid": "6e0de55d-6237-4b43-b2b2-66fddd3e8a07"}, "outputs": [], "source": ["_ = df_train.groupby(\"year\").aggregate({\"id\": \"count\"}).plot(kind=\"bar\", alpha=.5, figsize=(12, 5))\n", "_ = plt.title(\"Distribution of data pr. year\")\n", "_ = plt.ylabel(\"Count\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "38726c175aa9870da64e304f5efed82a7c339b30", "_cell_guid": "c5582319-c9e6-4b30-bfea-f4721e305cf5"}, "source": ["Alright. Now that we have the yearly distribution - let's break it down to a monthly distribution pr. year."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "3e6937c00c9c370a3866b4a0da2193154f0cd4b7", "_cell_guid": "0f240b8d-976e-4c91-ae66-db1f2c8ea114"}, "outputs": [], "source": ["data = df_train.groupby([\"year\", \"month\"], as_index=False).aggregate({\"id\": \"count\"})\n", "_ = sns.factorplot(x=\"month\", y=\"id\", col=\"year\", col_wrap=3, data=data)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "a86da02d58f135203b0910ef49adf48000f2e427", "_cell_guid": "e1afa5f5-2450-4c4e-b14e-125aa99e88a3"}, "outputs": [], "source": ["_ = df_train.groupby(\"store_nbr\").aggregate({\"id\": \"count\"}).plot(kind=\"bar\", figsize=(12, 6), alpha=.5, width=1)\n", "_ = plt.title(\"Distribution of datapoints pr. store\")\n", "_ = plt.ylabel(\"Count\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "fca377c818814f5b79883e1959a7c3e658061715", "_cell_guid": "7daa5745-eb9a-4b52-9f87-4f516c1fb0d1"}, "outputs": [], "source": ["ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"M\").sum().plot(figsize=(12, 6))\n", "_ = ax.set_ylabel(\"unit_sales\")\n", "_ = ax.set_title(\"Monthly Sales Volum\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "4b1e4dac97178bd99047f367195769064c9f76ea", "_cell_guid": "bc0b1bfc-bc97-40e6-a8e7-5e9628eaf59c"}, "outputs": [], "source": ["ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"W\").sum().plot(figsize=(12, 6))\n", "_ = ax.set_ylabel(\"unit_sales\")\n", "_ = ax.set_title(\"Weekly Sales Volum\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "69fe7b7488e8ed00b05369073396f2b2a5beffe2", "_cell_guid": "73cabd9e-8209-4570-afb9-3de49d4f14e3"}, "outputs": [], "source": ["ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"D\").sum().plot(figsize=(12, 6))\n", "_ = ax.set_ylabel(\"unit_sales\")\n", "_ = ax.set_title(\"Daily Sales Volum\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0637ea612b890b7d1d99929ce9dba5e312f9dfb4", "_cell_guid": "1a30f686-92b1-4dac-969e-6d1bbe971554"}, "source": ["#### We find:\n", "\n", "- A generally increasing trend, with a couple of significant dips specifically in june/july of 2014 and early 2015. The answer to that is probably somewhere in the other datasets we were provided"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "25cf7090e94d691263440160d2bed4970cc8b061", "_cell_guid": "f66fc1cd-0a78-4972-aae5-37b98d0b945e"}, "outputs": [], "source": ["ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"W\").mean().plot(figsize=(12, 6), label=\"Mean\")\n", "ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"W\").median().plot(figsize=(12, 6), label=\"Median\")\n", "ax = df_train.set_index(\"date\")[\"unit_sales\"].resample(\"W\").std().plot(figsize=(12, 6), label=\"Std\")\n", "\n", "_ = ax.set_ylabel(\"unit_sales\")\n", "_ = plt.legend(loc=\"best\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "9340c790049a3f2d94a90b21e2e7f7e3ea2d1b81", "_cell_guid": "7af9d3c0-85bf-4f2f-96a9-3ae9a3e9e4d6"}, "source": ["#### We find:\n", "- Mean and median is fairly close all the way. The standard deviation has a couple of solid spikes early to mid and late 2016. A high std generally means high variation in the data. Maybe this was due to new products, or new stores opened or existing stores closed down."], "cell_type": "markdown"}, {"metadata": {"_uuid": "c7ad099ee9134ecbc429d77d8f59411aa45b74c4", "_cell_guid": "38a8c9b5-31e4-4fd1-8afd-9de720dd588c"}, "source": ["#### Let's very quickly check how many potential outliars there are in our training-data\n", "---"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c5826ca6ba483ee537444ac7100d6c288d9c4161", "_cell_guid": "df770fcc-b98d-46f4-88e0-dcf0ebe60c5f"}, "outputs": [], "source": ["def detect_outliars(col, df):\n", "    Q1 = np.percentile(df[col], 25)\n", "    Q3 = np.percentile(df[col], 75)\n", "    step = 1.5 * (Q3 - Q1)\n", "    outliar_mask = ~((df[col] >= Q1 - step) & (df[col] <= Q3 + step))\n", "    \n", "    return outliar_mask\n", "\n", "\n", "outliar_mask = detect_outliars(\"unit_sales\", df_train)\n", "\n", "print(\"Percentage of training-data that is classified as outliars: {}%\".format(round(len(df_train[outliar_mask]) / float(len(df_train)) * 100, 2)))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "6d61a91292cef780eb400676950ae5fe4cc475fa", "_cell_guid": "929f86ab-a66d-4b09-be0f-776bff880cca"}, "outputs": [], "source": ["df_train[outliar_mask][\"store_nbr\"].value_counts().plot(kind=\"bar\", figsize=(12, 6), width=1, alpha=.5)\n", "_ = plt.title(\"Distribution of outliars accross the different stores\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "84bc2c685b1fb150a6158da0bc43e31f3178e73c", "_cell_guid": "8e1cedf6-ab8d-478f-a055-79ad7e50529a"}, "outputs": [], "source": ["_ = df_train[\"unit_sales\"].apply(np.log).hist(bins=25, range=(-2, 6), figsize=(12, 6), alpha=.5)\n", "_ = plt.xlabel(\"unit_sales log-transformed\")\n", "_ = plt.ylabel(\"Count\")\n", "_ = plt.title(\"Distribution of sales-volume\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "522c99692f82a770d614fb1da38bbdbd974c1695", "_cell_guid": "edd6d4ec-8ec0-47e7-987d-d0b270201856"}, "outputs": [], "source": ["data = pd.crosstab(df_train[\"year\"], df_train[\"week\"], df_train[\"unit_sales\"], aggfunc=\"sum\", normalize=True)\n", "\n", "_ = plt.figure(figsize=(14, 3))\n", "_ = sns.heatmap(data, cmap=\"viridis\")\n", "_ = plt.title(\"Heatmap of Sales Volume Year vs. Week\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7f03a24c6d1253fbd0aeb2866dd1de0fe19f721c", "_cell_guid": "7342ff43-3863-49fa-b4c3-a56983fb54e4"}, "source": ["#### We find\n", "- Other than a generally increasing trend, there doesn't seem to be spesific weeks where interesting stuff happens, aside from the expected behaviour when for example holidays kicks into effect"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "e937115134596be0ed78bf4ad3daee27ec093594", "_cell_guid": "98700501-02dc-49fc-a3a7-e1d29aa62d41"}, "outputs": [], "source": ["data = pd.crosstab(df_train[\"year\"], df_train[\"weekday\"], df_train[\"unit_sales\"], aggfunc=\"sum\", normalize=True)\n", "weekday_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n", "\n", "_ = plt.figure(figsize=(14, 3))\n", "_ = sns.heatmap(data, cmap=\"viridis\")\n", "_ = plt.xticks(range(7), weekday_names)\n", "_ = plt.title(\"Heatmap of Sales Volume Year vs. Weekday\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0ff65fc3ccd3461a0bd58fdb61d327a4f686bf8b", "_cell_guid": "2b4949a0-0bed-422e-b4a1-4e96d4a46faa"}, "source": ["#### We find\n", "- People grocery-shop the most during the weekends. This is pretty consistently every year, although it was particularly much in 2016 "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "1408a35379ac0c76a683107537cd794ff743e66a", "_cell_guid": "a5dec0e0-951b-4c09-bc7f-92bda0445b00"}, "outputs": [], "source": ["data = pd.crosstab(df_train[\"year\"], df_train[\"month\"], df_train[\"unit_sales\"], aggfunc=\"sum\", normalize=True)\n", "month_names = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n", "\n", "_ = plt.figure(figsize=(14, 3))\n", "_ = sns.heatmap(data, cmap=\"viridis\")\n", "_ = plt.xticks(range(12), month_names)\n", "_ = plt.title(\"Heatmap of Sales Volume Year vs. Month\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "91aa3a709fc3cb89a55984f78f8a259e219fb392", "_cell_guid": "b0ea412f-53d6-4593-a2d5-816fe9d15691"}, "source": ["#### We find\n", "- An increased sales-volume in the month of december every year."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "b2e885d9e991a0420226dc63a717823419b84777", "_cell_guid": "edd5b6e0-6374-465d-b152-135db48d5731"}, "outputs": [], "source": ["data = pd.crosstab(df_train[\"weekday\"], df_train[\"month\"], df_train[\"unit_sales\"], aggfunc=\"sum\", normalize=True)\n", "\n", "_ = plt.figure(figsize=(14, 3))\n", "_ = sns.heatmap(data, cmap=\"viridis\")\n", "_ = plt.xticks(range(12), month_names)\n", "_ = plt.yticks(range(7), weekday_names, rotation=0)\n", "_ = plt.title(\"Heatmap of Sales Volume Weekday vs. Month\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "360b6f0d682fff7dd6b434b79348d9a5e72d7bdf", "_cell_guid": "9cd628d9-7484-4eb7-8931-e3b6b3cd144a"}, "outputs": [], "source": ["def on_promotion(x):\n", "    if pd.isnull(x):\n", "        return -1\n", "    elif x == True:\n", "        return 1\n", "    else:\n", "        return 0\n", "\n", "df_train.loc[:, \"onpromotion\"] = df_train[\"onpromotion\"].apply(on_promotion)\n", "df_train.loc[:, \"onpromotion\"] = df_train[\"onpromotion\"].astype(\"int8\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "18dae494caf2203f0983376168215fc41b2fdc9d", "_cell_guid": "474a7b5b-de79-49ff-93ea-7a50d39a7e10"}, "outputs": [], "source": ["df_train[\"onpromotion\"].value_counts()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "0f6600cc8c97b17e4cce995c5964203c6cee5e40", "_cell_guid": "f9897718-21b8-4c44-9d81-b2ba8df2333b"}, "outputs": [], "source": ["promotion = len(df_train[df_train[\"onpromotion\"] == 1])\n", "no_promotion = len(df_train[df_train[\"onpromotion\"] == 0])\n", "unknown = len(df_train[df_train[\"onpromotion\"] == -1])\n", "\n", "print(\"{}% of traning_set is on promotion\".format(round(promotion / float(len(df_train)) * 100, 2)))\n", "print(\"{}% of traning_set is not on promotion\".format(round(no_promotion / float(len(df_train)) * 100, 2)))\n", "print(\"{}% of traning_set is unkown with regards to promotion\".format(round(unknown / float(len(df_train)) * 100, 2)))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "0c50f18b62e8ae033d0cdc37efaf59f68f8d0327", "_cell_guid": "05f78d70-6082-441c-99cd-7c7c8c2ccfeb"}, "outputs": [], "source": ["data = df_train \\\n", "    .groupby([\"store_nbr\", \"onpromotion\"], as_index=False) \\\n", "    .aggregate({\"id\": \"count\"}) \\\n", "    .pivot(index=\"store_nbr\", columns=\"onpromotion\",values=\"id\")\n", "    \n", "_ = data \\\n", "    .apply(lambda x: x / data.sum(axis=1) * 100) \\\n", "    .fillna(0) \\\n", "    .plot \\\n", "    .bar(stacked=True, width=1, figsize=(14, 6), alpha=.5)   \n", "    \n", "_ = plt.legend([\"Unkown\", \"No Promotion\", \"Promotion\"], bbox_to_anchor=(1.2, 1.0))\n", "_ = plt.title(\"Which store has the most products on promotion\")\n", "_ = plt.ylim(0, 100)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "db6405aa5f667078963d45e582b751eea3cc3c65", "_cell_guid": "81ca65b9-dceb-48ed-af81-9118a51cd0d2"}, "source": ["## Items\n", "---"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "38326daad1f6a571409bcc69d7ac15cf3401adca", "_cell_guid": "c2adfe4d-640f-4940-b18f-281867000b5b"}, "outputs": [], "source": ["df_items = pd.read_csv(\"../input/items.csv\")\n", "df_items.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "137da4aef0cd463e715cb6b5fe9d2aabba07c9af", "_cell_guid": "119a600b-5964-47a7-b21e-746669150265"}, "outputs": [], "source": ["_ = plt.figure(figsize=(6,10))\n", "ax = sns.countplot(y=df_items[\"family\"])\n", "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=45)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "scrolled": false, "_uuid": "07ffa590857fd26960d134fee42679ca26084dbc", "_cell_guid": "e44c9d59-a0c4-469c-aab7-cf18f37d4f2f"}, "outputs": [], "source": ["plt.figure(figsize=(10, 3))\n", "sns.kdeplot(df_items[df_items[\"perishable\"] == 1][\"class\"], shade=True, color=\"g\")\n", "sns.kdeplot(df_items[df_items[\"perishable\"] == 0][\"class\"], shade=True, color=\"r\")\n", "plt.legend([\"Perishable\", \"Non perishable\"])\n", "_ = plt.xlabel(\"family [id]\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "c755c42177ea618e485970600e531bf074dc3c10", "_cell_guid": "f580d1cf-71e9-4fcc-83a0-8f44987a3aaf"}, "outputs": [], "source": ["plt.figure(figsize=(10, 3))\n", "sns.kdeplot(df_items[df_items[\"perishable\"] == 1][\"item_nbr\"], shade=True, color=\"g\")\n", "sns.kdeplot(df_items[df_items[\"perishable\"] == 0][\"item_nbr\"], shade=True, color=\"r\")\n", "plt.legend([\"Perishable\", \"Non perishable\"])\n", "_ = plt.xlabel(\"item_nbr [id]\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b73bc23576870349c5e75ddcd63fec509c1fed1b", "_cell_guid": "00840298-9335-4f82-883e-3bcc689c89e3"}, "source": ["## Transactions\n", "---"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "a9a5c49be41769797952339bbe2b4e3f03222faf", "_cell_guid": "7c358cd3-e9e0-4709-8afb-89b9ff9c092d"}, "outputs": [], "source": ["df_transactions = pd.read_csv(\"../input/transactions.csv\", parse_dates=[\"date\"], infer_datetime_format=True)\n", "df_transactions.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "7da3a45d73e2d26106dbc7d938458231ab1b719e", "_cell_guid": "01786dd0-1db8-4c63-8e09-778288a95fbc"}, "outputs": [], "source": ["_ = plt.figure(figsize=(12, 5))\n", "_ = plt.hist(df_transactions[\"transactions\"], bins=50, alpha=.5)\n", "_ = plt.xlabel(\"Number of Transactions\")\n", "_ = plt.ylabel(\"Count\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "44f71e0a970de2bf7899e32d1dc4071aa0e59efc", "_cell_guid": "5137c34c-2817-4c58-a2e8-ed615ea8c229"}, "outputs": [], "source": ["data = df_transactions.groupby(\"date\").aggregate({\"transactions\": \"sum\"})\n", "_ = data.resample(\"W\").mean().plot(figsize=(12, 6))\n", "_ = plt.ylabel(\"Transactions\")\n", "_ = plt.title(\"Mean Transactions Weekly\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "768c8c3f32df156a4f34157dfec7fbbfaf5daa95", "_cell_guid": "760a8c98-0f68-4aef-a266-40e4ad5e4018"}, "source": ["#### We find\n", "- Every year around christmast/new-year we see a spike in mean transaction-amounts which makes sense"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "321079baa2d3a8b1b351047f839fe06e72d12635", "_cell_guid": "76d3a372-3280-44df-9909-bc9e641c7b03"}, "outputs": [], "source": ["outliar_mask = detect_outliars(\"transactions\", df_transactions)\n", "print(\"Percentage of outliars in transactions: {}%\".format(round(len(df_transactions[outliar_mask]) / float(len(df_transactions)) * 100, 2) ))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "d7420ae37ee8dd86c4dba209420a139b3e3d251b", "_cell_guid": "bb86a5ec-30de-4c73-9bb6-beb9b33323c1"}, "outputs": [], "source": ["df_transactions[outliar_mask][\"store_nbr\"].value_counts().plot(kind=\"bar\", figsize=(12, 6), width=1, alpha=.5)\n", "_ = plt.ylabel(\"Number of Transactions\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "ea3c9a6103fe1b1af8bd870b4db0cb6da16ed0fd", "_cell_guid": "2da5e372-424e-4812-9eca-c44f98b7c59a"}, "source": ["More or less the same outliar-stores we found when checking the training-set."], "cell_type": "markdown"}, {"metadata": {"_uuid": "d74b45bae0089fb187fea060826af771fe5f50a5", "_cell_guid": "9db8393c-e733-488b-9e6f-c9e9d7a0c913"}, "source": ["## Stores\n", "---"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "67de3d7867b270e5addc078e1b98ce2f27126115", "_cell_guid": "a0c4d013-89f5-430c-be3b-b7bc264fd827"}, "outputs": [], "source": ["df_stores = pd.read_csv(\"../input/stores.csv\")\n", "df_stores.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "c57ec07280230b8720469a421e65561e4128caa6", "_cell_guid": "7c2f8409-22ed-474e-a169-8a265657db0c"}, "outputs": [], "source": ["f, ax = plt.subplots(1, 2, figsize=(12, 4))\n", "_ = sns.countplot(df_stores[\"type\"].sort_values(), ax=ax[0])\n", "_ = sns.countplot(df_stores[\"cluster\"], ax=ax[1])"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "66198e198e0ceedd045df79db40efa3568a8aea7", "_cell_guid": "b6bee4a6-8a2f-43ca-920e-15560b4b36d5"}, "outputs": [], "source": ["data = df_stores \\\n", "    .groupby([\"cluster\", \"type\"]) \\\n", "    .aggregate({\"type\": \"count\"}) \\\n", "    .rename(columns={'type': 'type_count'}) \\\n", "    .unstack(level=0) \\\n", "    .fillna(0)\n", "\n", "\n", "ax = data.plot(kind=\"bar\", stacked=True, figsize=(12, 4))\n", "_ = plt.legend([\"Cluster %d\" % i for i in range(1, 18)], bbox_to_anchor=(1.3, 1.2))\n", "_ = plt.ylabel(\"Count\")\n", "_ = plt.title(\"Distribution of Clusters pr. Type\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "5db269e9a9ed15904544ad31f80c717024e26223", "_cell_guid": "28d20d42-d6b1-4e74-b672-ca77c22c7691"}, "outputs": [], "source": ["data = df_stores \\\n", "    .groupby([\"city\", \"type\"]) \\\n", "    .aggregate({\"type\": \"count\"}) \\\n", "    .unstack(level=1) \\\n", "    .fillna(0)\n", "    \n", "_ = data.plot(kind=\"bar\", stacked=True, figsize=(12, 4))\n", "_ = plt.legend([\"Type A\", \"Type B\", \"Type C\", \"Type D\", \"Type E\"], loc=\"best\")\n", "_ = plt.ylabel(\"Count\")\n", "_ = plt.title(\"Distribution of Types pr. City\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "a9fe62cf1a6630d7ca8185a3692d8a8f5d9d8d8f", "_cell_guid": "8366e01e-2ebc-4d64-8b74-0f3b3181b259"}, "outputs": [], "source": ["_ = plt.figure(figsize=(14, 4))\n", "_ = sns.heatmap(pd.crosstab(df_stores[\"cluster\"], df_stores[\"store_nbr\"]), cmap=\"viridis\")\n", "_ = plt.yticks(rotation=0)\n", "_ = plt.title(\"Which store belongs to which cluster\")"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "9f015606d51589888d4150f61f0bddbf3c27a43c", "_cell_guid": "8e4cb46b-8837-41cc-9d93-bc24eaf8c345"}, "outputs": [], "source": [], "execution_count": null, "cell_type": "code"}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "nbformat": 4}