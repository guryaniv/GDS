{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "552dc3e7-9937-d5e8-1d43-8375b57ce090"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "343171de-08a3-1bf2-875c-597bf0e57c8e"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "## set directories and parameters\n",
        "########################################\n",
        "BASE_DIR = '../input/'\n",
        "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
        "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
        "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "MAX_NB_WORDS = 200000\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "num_lstm = np.random.randint(175, 275)\n",
        "num_dense = np.random.randint(100, 150)\n",
        "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
        "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
        "\n",
        "act = 'relu'\n",
        "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
        "\n",
        "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
        "        rate_drop_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49311fec-cf40-477e-b589-0907161c0e9c"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "## index word vectors\n",
        "########################################\n",
        "print('Indexing word vectors')\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
        "        binary=True)\n",
        "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0369355b-40a5-455b-05b4-3f7db6cfcc66"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################################\n",
        "## process texts in datasets\n",
        "########################################\n",
        "print('Processing text dataset')\n",
        "\n",
        "# The function \"text_to_wordlist\" is from\n",
        "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)\n",
        "\n",
        "texts_1 = [] \n",
        "texts_2 = []\n",
        "labels = []\n",
        "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        texts_1.append(text_to_wordlist(values[3]))\n",
        "        texts_2.append(text_to_wordlist(values[4]))\n",
        "        labels.append(int(values[5]))\n",
        "print('Found %s texts in train.csv' % len(texts_1))\n",
        "\n",
        "test_texts_1 = []\n",
        "test_texts_2 = []\n",
        "test_ids = []\n",
        "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        test_texts_1.append(text_to_wordlist(values[1]))\n",
        "        test_texts_2.append(text_to_wordlist(values[2]))\n",
        "        test_ids.append(values[0])\n",
        "print('Found %s texts in test.csv' % len(test_texts_1))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
        "\n",
        "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
        "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
        "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
        "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = np.array(labels)\n",
        "print('Shape of data tensor:', data_1.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_ids = np.array(test_ids)\n",
        "\n",
        "########################################\n",
        "## prepare embeddings\n",
        "########################################\n",
        "print('Preparing embedding matrix')\n",
        "\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embedding_matrix[i] = word2vec.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "########################################\n",
        "## sample train/validation data\n",
        "########################################\n",
        "#np.random.seed(1234)\n",
        "perm = np.random.permutation(len(data_1))\n",
        "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
        "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
        "\n",
        "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
        "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
        "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
        "\n",
        "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
        "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
        "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
        "\n",
        "weight_val = np.ones(len(labels_val))\n",
        "if re_weight:\n",
        "    weight_val *= 0.472001959\n",
        "    weight_val[labels_val==0] = 1.309028344\n",
        "\n",
        "########################################\n",
        "## define the model structure\n",
        "########################################\n",
        "embedding_layer = Embedding(nb_words,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False)\n",
        "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
        "\n",
        "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
        "x1 = lstm_layer(embedded_sequences_1)\n",
        "\n",
        "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
        "y1 = lstm_layer(embedded_sequences_2)\n",
        "\n",
        "merged = concatenate([x1, y1])\n",
        "merged = Dropout(rate_drop_dense)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "merged = Dense(num_dense, activation=act)(merged)\n",
        "merged = Dropout(rate_drop_dense)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "preds = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "########################################\n",
        "## add class weight\n",
        "########################################\n",
        "if re_weight:\n",
        "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
        "else:\n",
        "    class_weight = None\n",
        "\n",
        "########################################\n",
        "## train the model\n",
        "########################################\n",
        "model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
        "        outputs=preds)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "        optimizer='nadam',\n",
        "        metrics=['acc'])\n",
        "#model.summary()\n",
        "print(STAMP)\n",
        "\n",
        "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
        "bst_model_path = STAMP + '.h5'\n",
        "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
        "        validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
        "        epochs=200, batch_size=2048, shuffle=True, \\\n",
        "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "model.load_weights(bst_model_path)\n",
        "bst_val_score = min(hist.history['val_loss'])\n",
        "\n",
        "########################################\n",
        "## make the submission\n",
        "########################################\n",
        "print('Start making the submission before fine-tuning')\n",
        "\n",
        "preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
        "preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
        "preds /= 2\n",
        "\n",
        "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
        "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}