{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#using tensorflow and keras high-level api to build our model\nimport tensorflow as tf # \nimport tensorflow.keras as keras\ntf.logging.set_verbosity('DEBUG')\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7157947239cab1cacb9076d6d07bd644765eac1"},"cell_type":"code","source":"#load train and test data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#set frac = 1.0 will randomize the whole train data\ntrain.sample(frac=1.0)\nprint(train.info())\nprint(train.describe())\nprint(test.info())\nprint(test.describe())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"_uuid":"bf1fe3f000e1dc47f42a83f64ed9c550c64f078a"},"cell_type":"code","source":"#for text embedding, we will use tensorflow api word2vec from google\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import text, sequence\ntrain_sentences = train['comment_text'].fillna('N/A').values\ntest_sentences = test['comment_text'].fillna('N/A').values\n\n#Fig : sentences length distribution\nall_sentences = list(train_sentences) + list(test_sentences)\nall_sentences_len = [len(text.text_to_word_sequence(sentence))\n                     for sentence in all_sentences]\n\nplt.hist(all_sentences_len, bins=200)\nplt.xlim(0, 500)\nplt.xlabel('sentence length')\nplt.ylabel('number')\nplt.table(cellText=[[np.mean(all_sentences_len), np.median(all_sentences_len),\n                     np.percentile(all_sentences_len, 95), np.max(all_sentences_len)]],\n          rowLabels=['value'],\n          colLabels=['Mean', 'Median(50%)', '95%', 'Max(100%)'],\n          loc='top')\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80b5baa8fc270706ae301888519dfe0735a07dcc"},"cell_type":"markdown","source":"as we can see , the histgram of sentence length in train and test data, 200 should be a apprapriate number , which will cover most of the samples"},{"metadata":{"_uuid":"93dc2a3f6c528d77496675ec437ccbeb1b9a8c91"},"cell_type":"markdown","source":"\n"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"8af82cdc1688ada4eea2fe97bdba1b62829afac4"},"cell_type":"code","source":"MAX_FEATURES = 20000\nMAX_SENTENCE_LENGTH = 200\ntrain_classes = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\n\n#transform text into integer sentences with same length\ntokenizer = text.Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(list(train_sentences))\n\ntrain_sentences = tokenizer.texts_to_sequences(train_sentences)\ntest_sentences = tokenizer.texts_to_sequences(test_sentences)\ntrain_sentences = sequence.pad_sequences(train_sentences, maxlen=MAX_SENTENCE_LENGTH)\ntest_sentences = sequence.pad_sequences(test_sentences, maxlen=MAX_SENTENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99d26dcff1a606678035d545fa67a849c4bf823a"},"cell_type":"code","source":"def my_model(lstm_units, dropout_rate, optimizer):\n    model = keras.models.Sequential([\n        keras.layers.InputLayer(input_shape=(MAX_SENTENCE_LENGTH, )),\n        keras.layers.Embedding(input_dim=MAX_FEATURES, output_dim=int(MAX_FEATURES**0.25)),\n        keras.layers.LSTM(units=lstm_units),\n        #keras.layers.GlobalMaxPool1D(),\n        keras.layers.Dense(lstm_units, activation=tf.nn.relu),\n        keras.layers.Dropout(dropout_rate),\n        keras.layers.Dense(6, activation=tf.nn.sigmoid)\n    ])\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.binary_crossentropy,\n        metrics=['accuracy']\n    )\n    return model\n\n#define some hyper parameter\nbatch_size = 32\nepochs = 2\nlstm_units = 50\ndropout_rate = 0.5\noptimizer = tf.train.AdamOptimizer()\n\n#define the model\nmodel = my_model(lstm_units, dropout_rate, optimizer)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"169618b41f9471f30d744616f564a7d5ec350ce4"},"cell_type":"code","source":"#define some callbacks\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import TensorBoard\ncheckpoint_cb = ModelCheckpoint(filepath='model_toxic/cp.ckpt',\n                                save_weights_only=True,\n                                mode='min',\n                                 verbose=1,\n                                save_best_only=True)\ntensorboard_cb = TensorBoard(log_dir='model_toxic',\n                             histogram_freq=1,\n                             write_grads=True)\n\n#start train and evaluate\nmodel.fit(\n    x=train_sentences,\n    y=train_classes,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2,\n    callbacks=[checkpoint_cb, tensorboard_cb],\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ed27cdcc011ba3839554f95161ee4219a241b9"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}