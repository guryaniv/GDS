{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1bf553727bf6e2b4b53e9485950699aa8f3130c"},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold, KFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"072fb4101023bc24225491ebc79e650b414bc294"},"cell_type":"code","source":"#load the training file\ntrain_data = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fec03af28959fe0a5eb6177395f74f1651863a0"},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a36da9fdc4d6a955eb1db9cc56bdcd9de6b85570"},"cell_type":"code","source":"value_count = train_data['target'].value_counts()\nprint(f'THE VALUE COUNTS OF THE TARGET VARIABLE : \\n{value_count}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04def36557e7d8f6488cc307654323d28abaa2d7"},"cell_type":"code","source":"sns.set(style = 'darkgrid')\nplt.figure(figsize = (12,10))\nsns.countplot(y = train_data['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e54347037e7bca8ada5b88af22621cd5a0ee1f71"},"cell_type":"code","source":"#load in the testing file\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90c5758ea540bfa614273944597a738cedcbcfe4"},"cell_type":"code","source":"test_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"621555e93c7f28223f6dab0d3f5d72ca30bd56cd"},"cell_type":"code","source":"#spearated the dataset into input features and labels\nX = train_data.drop(['target', 'ID_code'], axis = 1)\ny = train_data['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a435971d3d3aecb77d32219e6f4d81a7d6449b6"},"cell_type":"markdown","source":"## ENSEMBLE NEURAL NETWORK"},{"metadata":{"trusted":true,"_uuid":"663d71a4fc717ee20215c9fa68e59775ecc07c53"},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras import optimizers\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bff6ef1c158099a14a9afe2b3d176eae6c4617b"},"cell_type":"code","source":"kernel_init = 'normal'\ndef SimpleFFNN(input_dim, activation, classes):\n    model = Sequential()\n\n    model.add(Dense(512, kernel_initializer = kernel_init, input_dim = input_dim))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dense(512, kernel_initializer = kernel_init, input_dim = input_dim))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(256, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(256, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(128, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(128, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(classes, kernel_initializer = kernel_init))    \n    model.add(Activation('sigmoid'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f7d3b80399512f7a6c1a1c5dddf0da5336dbef3"},"cell_type":"code","source":"#we will also flatten our output label\ny_flatten = y.ravel()\nprint(f\"THE SIZE OF THE OTUPUT LABELS : {y_flatten.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cebd9f91291a5a375edb90e457a2df38a5c0e1ba"},"cell_type":"code","source":"#let's scale the original training data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X)\n\nX_scaled = sc.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b472cffa5ed83d1abd26898aea968ef76faf1ab"},"cell_type":"code","source":"#scaling the testing data\ntest = test_data.drop('ID_code', axis = 1)\ntest_scaled = sc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6e6a059792b64d8ed4f36bdb31890cf48ec3776","_kg_hide-output":true},"cell_type":"code","source":"input_dim = X_scaled.shape[1]\nactivation = 'relu'\nclasses = 1\n\nhistory = dict() #dictionery to store the history of individual models for later visualization\nprediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n\n#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\nfor i in np.arange(0, 5):\n    optim = optimizers.Adam(lr = 0.001)\n    ensemble_model = SimpleFFNN(input_dim = input_dim, activation = activation, classes = classes)\n    ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])\n    print('TRAINING MODEL NO : {}'.format(i))\n    H = ensemble_model.fit(X_scaled, y_flatten,\n                           batch_size = 128,\n                           epochs = 200,\n                           verbose = 1)\n    history[i] = H\n    \n    ensemble_model.save('MODEL_{}.model'.format(i))\n    \n    predictions = ensemble_model.predict(test_scaled, verbose = 1, batch_size = 128)\n    prediction_scores[i] = predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9af297109517f973e29b16b025bd37ce107bd490"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true,"_uuid":"2366f722d7549741b762eaa519901ba5d2b44c9b"},"cell_type":"code","source":"#we will considering all the features except 'ID_code' and 'target'\nfeatures = [value for value in train_data.columns if value not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ab0e061edd5546767ca69833c341c2c435a4b96"},"cell_type":"code","source":"#defining the parameters\nparam = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"070c3348ad956093e64b3a16d9ffd8795c6ca299"},"cell_type":"code","source":"folds = StratifiedKFold(n_splits = 12, shuffle = False, random_state = 101)\ntrain_mat = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, y.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features], label = y.iloc[trn_idx])\n    val_data = lgb.Dataset(train_data.iloc[val_idx][features], label = y.iloc[val_idx])\n\n    num_round = 500000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval = 1000, early_stopping_rounds = 500)\n    train_mat[val_idx] = clf.predict(train_data.iloc[val_idx][features], num_iteration = clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis = 0)\n    \n    predictions += clf.predict(test_data[features], num_iteration = clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, train_mat)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4101144646ad77cf1ee8a467f0fadba285f01f7"},"cell_type":"markdown","source":"## CATBOOST"},{"metadata":{"trusted":true,"_uuid":"eae19db6a689e4cb456d24f0826f5d3cb76e4b60"},"cell_type":"code","source":"## Catboost : https://www.kaggle.com/wakamezake/starter-code-catboost-baseline\nfrom catboost import Pool, CatBoostClassifier\nmodel = CatBoostClassifier(loss_function = \"Logloss\", eval_metric = \"AUC\")\nkf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n\ny_valid_pred = 0 * y\ny_test_pred = 0\n\nfor idx, (train_index, valid_index) in enumerate(kf.split(train_data)):\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    X_train, X_valid = train_data[features].iloc[train_index,:], train_data[features].iloc[valid_index,:]\n    _train = Pool(X_train, label = y_train)\n    _valid = Pool(X_valid, label = y_valid)\n    print( \"\\nFold \", idx)\n    fit_model = model.fit(_train,\n                          eval_set = _valid,\n                          use_best_model = True,\n                          verbose = 1\n                         )\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict_proba(test_data[features])[:,1]\ny_test_pred /= 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"419817b98c5166eb21a443c9ff8f343dd10fe186"},"cell_type":"markdown","source":"## CREATING SUBMISSION FILE"},{"metadata":{"_uuid":"14f9ca42b20b9fa61790daa02baebbd6a5d374e5"},"cell_type":"markdown","source":"1. LGBM "},{"metadata":{"trusted":true,"_uuid":"12834d5d54cb97a66f23964d98e6ee683a77894f"},"cell_type":"code","source":"#submission for LGBM\ndf_lgbm = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_lgbm[\"target\"] = predictions\ndf_lgbm.to_csv(\"lgbm_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d7090f398fc1477bbff40cf1ff8248c31143b64"},"cell_type":"markdown","source":"2. CATBOOST"},{"metadata":{"trusted":true,"_uuid":"90ba5d29e7411b5eda7d6c18c615df21ce40c229"},"cell_type":"code","source":"#submission for CAT\ndf_cat = pd.DataFrame({\"ID_code\": test_data[\"ID_code\"].values})\ndf_cat[\"target\"] = y_test_pred\ndf_cat.to_csv(\"cat_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79f747ff8a70022ea95c3dd5176a83a6609ddc9"},"cell_type":"markdown","source":"3. ENSEMBLE"},{"metadata":{"trusted":true,"_uuid":"2c77cb32689c18574287f6c68e20081836ee2ae7"},"cell_type":"code","source":"#making predictions\nprediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\npredictions_ensemble = np.mean(prediction, axis = 1)\n\nprint(predictions_ensemble.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6633730b469102df5e09385f0f397cb56d0d758c"},"cell_type":"code","source":"#submission for ENSEMBLE\ndf_ensemble = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_ensemble[\"target\"] = predictions_ensemble\ndf_ensemble.to_csv(\"ensemble_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"407c9c86ada5736462b92b1607b002671ed0d7eb"},"cell_type":"markdown","source":"4. COMBINED MODEL"},{"metadata":{"trusted":true,"_uuid":"2d210e94fabb8497d11460cf2d06164cce4b4842"},"cell_type":"code","source":"##submission of combined model\ndf_total = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_total[\"target\"] = 0.4*df_lgbm[\"target\"] + 0.4*df_cat[\"target\"] + 0.2*df_ensemble['target']\ndf_total.to_csv(\"lgbm_cat_ensemble_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1b3bca3a6b82264406da717b180c0eb366fa1d4"},"cell_type":"markdown","source":"5. CATBOOST AND LGBM"},{"metadata":{"trusted":true,"_uuid":"f42e6f450a5e631b14d0daab53fd6f3d106244f6"},"cell_type":"code","source":"df1 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf1[\"target\"] = 0.5*df_lgbm[\"target\"] + 0.5*df_cat[\"target\"]\ndf1.to_csv(\"lgbm_cat_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f953beba831ceeb33b7238a22859fa51b64d156c"},"cell_type":"markdown","source":"6. LGBM AND ENSEMBLE"},{"metadata":{"trusted":true,"_uuid":"09813e6afabae1b91c022945a0269a558c668138"},"cell_type":"code","source":"df2 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf2[\"target\"] = 0.5*df_lgbm[\"target\"] + 0.5*df_ensemble[\"target\"]\ndf2.to_csv(\"lgbm_ensemble_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"015a16839af956bcac1dd5c3d0ec5f19be735d02"},"cell_type":"markdown","source":"7. CATBOOST AND ENSEMBLE"},{"metadata":{"trusted":true,"_uuid":"c6487b12711c983e03c4e783cf664a493a0ddd7c"},"cell_type":"code","source":"df3 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf3[\"target\"] = 0.5*df_cat[\"target\"] + 0.5*df_ensemble[\"target\"]\ndf3.to_csv(\"cat_ensemble_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03913047f07f80e29f6b5e16708a87e48e20a044"},"cell_type":"markdown","source":"8. LGBM, CATBOOST AND ENSEMBLE"},{"metadata":{"trusted":true,"_uuid":"fe52cf6887114855c241ce498c41c08ac909fe37"},"cell_type":"code","source":"df4 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf4[\"target\"] = 0.2*df_cat[\"target\"] + 0.4*df_ensemble[\"target\"] + 0.4*df_lgbm[\"target\"]\ndf4.to_csv(\"0.2cat_0.4ensemble_0.4lgbm_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d971a01c896527588850558c6ad30bf07dba6ad"},"cell_type":"markdown","source":"9. LGBM, CATBOOST AND ENSEMBLE"},{"metadata":{"trusted":true,"_uuid":"c32fbd97cce264fb46ead05a5338c281c53b099c"},"cell_type":"code","source":"df5 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf5[\"target\"] = 0.4*df_cat[\"target\"] + 0.4*df_ensemble[\"target\"] + 0.2*df_lgbm[\"target\"]\ndf5.to_csv(\"0.4cat_0.4ensemble_0.2lgbm_submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}