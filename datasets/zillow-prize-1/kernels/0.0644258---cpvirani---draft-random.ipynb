{"cells":[{"metadata":{"_kg_hide-output":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"883b8936371e8ecb2a5bc4e8cb7cf5f291f79ce7"},"cell_type":"code","source":"%matplotlib inline\n### Seaborn style\nsb.set_style(\"whitegrid\")\n## Dictionary of feature dtypes\nints = ['parcelid']\n\nfloats = ['basementsqft', 'bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'finishedfloor1squarefeet', \n          'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13',\n          'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'fireplacecnt',\n          'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude',\n          'lotsizesquarefeet', 'poolcnt', 'poolsizesum', 'roomcnt', 'threequarterbathnbr', 'unitcnt',\n          'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt', 'numberofstories',\n          'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear',\n          'landtaxvaluedollarcnt', 'taxamount', 'taxdelinquencyyear']\n\nobjects = ['airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid',\n           'buildingqualitytypeid', 'decktypeid', 'fips', 'hashottuborspa', 'heatingorsystemtypeid',\n           'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'propertycountylandusecode',\n           'propertylandusetypeid', 'propertyzoningdesc', 'rawcensustractandblock', 'regionidcity',\n           'regionidcounty', 'regionidneighborhood', 'regionidzip', 'storytypeid',\n           'typeconstructiontypeid', 'fireplaceflag', 'taxdelinquencyflag', 'censustractandblock']\n\nfeature_dtypes = {col: col_type for type_list, col_type in zip([ints, floats, objects],\n                                                               ['int64', 'float64', 'object']) \n                                  for col in type_list}\n### Let's import our data\ndata = pd.read_csv('../input/properties_2016.csv' , dtype = feature_dtypes)\n### and test if everything OK\ndata.head()\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8344fb4f8ae32a7b40f5b4338e8ac38635d09576"},"cell_type":"code","source":"### ... check for NaNs\nnan = data.isnull().sum()\n#nan\n\n### Plotting NaN counts\nprint( \"\\nNan column name and data type\" )\nnan_sorted = nan.sort_values(ascending=False).to_frame().reset_index()\nnan_sorted.columns = ['Column', 'Number of NaNs']\n\n\nfig, ax = plt.subplots(figsize=(12, 25))\nsb.barplot(x=\"Number of NaNs\", y=\"Column\", data=nan_sorted, color='purple', ax=ax);\nax.set(xlabel=\"Number of NaNs\", ylabel=\"\", title=\"Total Nimber of NaNs in each column\");\n\ndata.dtypes","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98d6814b46fd9bc5b2f8d5985a88ca29a5560894"},"cell_type":"code","source":"continuous = ['basementsqft', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', \n              'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15',\n              'finishedsquarefeet50', 'finishedsquarefeet6', 'garagetotalsqft', 'latitude',\n              'longitude', 'lotsizesquarefeet', 'poolsizesum',  'yardbuildingsqft17',\n              'yardbuildingsqft26', 'yearbuilt', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt',\n              'landtaxvaluedollarcnt', 'taxamount']\n\ndiscrete = ['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'fireplacecnt', 'fullbathcnt',\n            'garagecarcnt', 'poolcnt', 'roomcnt', 'threequarterbathnbr', 'unitcnt',\n            'numberofstories', 'assessmentyear', 'taxdelinquencyyear']\n\n### Continuous variable plots\nprint( \"\\nContinuous variable plots\\n\" )\nfor col in continuous:\n    values = data[col].dropna()\n    lower = np.percentile(values, 1)\n    upper = np.percentile(values, 99)\n    fig = plt.figure(figsize=(18,9));\n    sb.distplot(values[(values>lower) & (values<upper)], color='purple', ax = plt.subplot(121));\n    plt.suptitle(col, fontsize=16)     ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c0b45758b0694e668b91ff56da81d1cbea0a746"},"cell_type":"code","source":"### Discrete variable plots\nprint( \"\\nDiscrete variable plots\\n\" )\nNanAsZero = ['fireplacecnt', 'poolcnt', 'threequarterbathnbr']\nfor col in discrete:\n    if col in NanAsZero:\n        data[col].fillna(0, inplace=True)\n    values = data[col].dropna()   \n    fig = plt.figure(figsize=(18,9));\n    sb.countplot(x=values, color='purple', ax = plt.subplot(121));\n    plt.suptitle(col, fontsize=16)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"bb8389a3-52b8-457a-a67d-da3c0c043c34","_uuid":"c37103c7f3c0f6a737ee5230866236197e5f3be8","trusted":true},"cell_type":"code","source":"##### READ IN RAW DATA\n\nprint( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\n\n# Parameters\nXGB_WEIGHT = 0.6415\nBASELINE_WEIGHT = 0.0050\nXGB1_WEIGHT = 0.8083  \nBASELINE_PRED = 0.0115  \n\n##  XGBoost   ##\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n\n## PROCESS DATA FOR XGBOOST\n\nprint( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\n\ntrain_df[\"transactiondate\"] = pd.to_datetime(train_df[\"transactiondate\"])\ntrain_df[\"Month\"] = train_df[\"transactiondate\"].dt.month\n\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n\nx_test[\"transactiondate\"] = '2016-07-01'\nx_test[\"transactiondate\"] = pd.to_datetime(x_test[\"transactiondate\"])\nx_test[\"Month\"] = x_test[\"transactiondate\"].dt.month \nx_test = x_test.drop(['transactiondate'], axis=1)\n\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n# drop out ouliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n\n\n\n## RUN XGBOOST\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )\n\n\n\n## RUN XGBOOST AGAIN\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )\n\n\n\n## COMBINE XGBOOST RESULTS\nxgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )\n\ndel train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d951416f06c53cf5f2783f160fec56b5160feb6"},"cell_type":"code","source":"### Reading train file\nerrors = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'])\nerrors.head()\n\n#### Merging tables\nprint( \"\\nMerging tables\\n\" )\ndata_sold = data.merge(errors, how='inner', on='parcelid')\ndata_sold.head()\n\n### Checking logerror\nprint( \"\\nChecking logerror\\n\" )\ncol = 'logerror'\n\nvalues = data_sold[col].dropna()\nlower = np.percentile(values, 1)\nupper = np.percentile(values, 99)\nfig = plt.figure(figsize=(18,9));\nsb.distplot(values[(values>lower) & (values<upper)], color='purple', ax = plt.subplot(121));\nsb.boxplot(y=values, color='purple', ax = plt.subplot(122));\nplt.suptitle(col, fontsize=16);\n\n### Adding some new features from transactiondate\ndata_sold['month'] = data_sold['transactiondate'].dt.month\ndata_sold['day_of_week'] = data_sold['transactiondate'].dt.weekday_name\ndata_sold['week_number'] = data_sold['transactiondate'].dt.week\ndata_sold.head()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"3850b06f-1e79-4433-9422-8727c186b9c7","_uuid":"8a908d887de045ec7684742d289e46d3514619d7","trusted":true},"cell_type":"code","source":"## READ IN RAW DATA\n\nprint( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\n\n# Parameters\nOLS_WEIGHT = 0.0856 \n\n##    OLS     ##\n\nnp.random.seed(17)\nrandom.seed(17)\n\nprint(len(train),len(properties),len(submission))\n\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n\ntrain = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' \ntest = get_features(test[col])\n\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f100473aa6efe41602e2f74ef05c07a6a10c23e"},"cell_type":"code","source":"### Scrutinizing transactiondate\nprint( \"\\nScrutinizing transaction date\\n\" )\nfig = plt.figure(figsize=(18, 18));\nsb.countplot(x='transactiondate', color='purple', data=data_sold, ax = plt.subplot(221));\nsb.countplot(x='month', color='purple', data=data_sold, ax = plt.subplot(222));\nsb.countplot(x='day_of_week', color='purple', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday',\n                                                      'Friday', 'Saturday', 'Sunday'], \n              data=data_sold, ax = plt.subplot(223));\nsb.countplot(x='week_number', color='purple', data=data_sold, ax = plt.subplot(224));\nplt.suptitle('Transaction Date', fontsize=20);","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"082132f1-bc2d-468a-a230-e38898bbd86c","_uuid":"66c0a665a2e275e95c9a49e23c42db25bc0bc324","trusted":true},"cell_type":"code","source":"## READ IN RAW DATA\n\nprint( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\n\n# Parameters\nXGB_WEIGHT = 0.6415\nBASELINE_WEIGHT = 0.0050\nXGB1_WEIGHT = 0.8083  \nBASELINE_PRED = 0.0115  \n\n\n\n##  LightGBM  ##\n \n\n# PROCESS DATA FOR LIGHTGBM\n\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\n#add month feature\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"Month\"] = df_train[\"transactiondate\"].dt.month\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\n\n\n\n## RUN LIGHTGBM\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          \nparams['sub_feature'] = 0.345    \nparams['bagging_fraction'] = 0.85 \nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        \nparams['min_data'] = 500         \nparams['min_hessian'] = 0.05     \nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\n#add month feature assuming 2016-10-01\ndf_test[\"transactiondate\"] = '2016-07-01'\ndf_test[\"transactiondate\"] = pd.to_datetime(df_test[\"transactiondate\"])\ndf_test[\"Month\"] = df_test[\"transactiondate\"].dt.month \ndf_test = df_test.drop(['transactiondate'], axis=1)\n\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"51a4c6aa-4240-4cd3-b903-cefb878cf0c9","_uuid":"3a60dd2292eb8fadf6fb6ffa41983dbea6451788","trusted":true},"cell_type":"code","source":"##  Combine and Save  ##\n\n### COMBINE PREDICTIONS\n\nprint( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\nlgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\npred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n\nprint( \"\\nCombined XGB/LGB/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB/LGB/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\nprint( submission.head() )","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3912fbd8c94b336a222f060662b5d4945802e54"},"cell_type":"code","source":"### Creating 5 equal size logerror bins \nprint( \"\\nCreating 5 equal size logerror bins \\n\" )\ndata_sold['logerror_bin'] = pd.qcut(data_sold['logerror'], 5, \n                                    labels=['Large Negative Error', 'Medium Negative Error',\n                                            'Small Error', 'Medium Positive Error',\n                                            'Large Positive Error'])\nprint(data_sold.logerror_bin.value_counts())\n\n### Continuous variable vs logerror plots\nprint( \"\\nContinuous variable vs logerror plots\\n\" )\nfor col in continuous:     \n    fig = plt.figure(figsize=(18,9));\n    sb.barplot(x='logerror_bin', y=col, data=data_sold, ax = plt.subplot(121),\n                order=['Large Negative Error', 'Medium Negative Error','Small Error',\n                       'Medium Positive Error', 'Large Positive Error']);\n    plt.xlabel('LogError Bin');\n    plt.ylabel('Average {}'.format(col));\n    sb.regplot(x='logerror', y=col, data=data_sold, color='purple', ax = plt.subplot(122));\n    plt.suptitle('LogError vs {}'.format(col), fontsize=16)   ","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"34107247-ad42-4ba5-bf7c-3bc03c18f9a9","_uuid":"7ffc12f2744e345e8ca75ef4c0c4bcaeacbf089a","trusted":true},"cell_type":"code","source":"### WRITE THE RESULTS\n\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('TEm07sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n","execution_count":14,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}