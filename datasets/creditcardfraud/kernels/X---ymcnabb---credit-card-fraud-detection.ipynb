{"cells":[{"metadata":{"_uuid":"4c17418c276f91e7f635119e0121a76de0855152"},"cell_type":"markdown","source":"# Credit card fraud detection\n\nThe goal of this project is to implement a machine learning algorithm to detect credit card fraud based on a dataset that contains credit card transactions made by european cardholders. This dataset includes transactions that occurred in the course of two days in September 2013, with 492 fradulent transactions out of a total of 284,315 transactions. The dataset is thus highly unbalanced with the positive class (frauds) accounting for just 0.17% of all transactions. This will be addressed later in the discussion of the best model and sampling trategy. The dataset has 30 input features, 28 of which anonymized, and 1 target variable."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\nsns.set()\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"984974c50029ef16061eee9ce0479d19971b7a88"},"cell_type":"markdown","source":"## Data description\n\nThe loaded dataset below includes 30 input features with only two of which, `Time` and `Amount`, being labeled. This doesn't allow us to do EDA on most of the features."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the dataset\ndata = pd.read_csv('../input/creditcard.csv')\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6a216543b61975c104c02cdd1a9f203079d5987"},"cell_type":"code","source":"# identify the features and the target in the data set\nfeatures = data.columns[:-1]\nprint('The features are as follows: {}'.format(features))\ntarget = data.columns[-1]\nprint('The target is: ' + target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6fa57db693fce0ac4fb567a6f43081e4f7a83b8"},"cell_type":"code","source":"# Create an X variable containing the features and a y variable containing only the target variable\nX = data[features]\ny = data[target]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"400ba206f412b684166a8810fd88c35f03bce90c"},"cell_type":"markdown","source":"Once we've created the feature variable (X) and the target variable (y), we next view the histograms of each of the features below. We see that the unlabeled feature values have been transformed --- they're the result of a PCA transformation. `Amount` isn't and neither is `Time` (between transaction), the latter displaying a bimodal distribution (with two modes: one mode around 50K seconds, or 13.89 hours and the other around 150k seconds, or 41.67 hours)."},{"metadata":{"trusted":true,"_uuid":"da866c188110b85e499b99dca49fa3de91e22ed6"},"cell_type":"code","source":"# Plot histograms of each parameter \nX.hist(figsize = (20, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e10e3ba40974e7d10a148d5e3ee0b1dd4853b01f"},"cell_type":"markdown","source":"Next let's look at the distribution of class types. It looks like the daraset we have is drastically biased toward non-fradulent transactions (284,315) in comparison with fradulent transactions (492)."},{"metadata":{"trusted":true,"_uuid":"31e5584d047de0525b4058ea4784b1278e34024d"},"cell_type":"code","source":"# Count frequency of target class types\ncount_target = pd.value_counts(y, sort = True).sort_index()\ncount_target.plot(kind = 'bar')\nplt.xticks(rotation=0)\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e930527d5bfca19dbfcdaae83ce8d6a88d010d1"},"cell_type":"markdown","source":"## Data preparation\n\nNext we prepare the features for the machine learning algorithm. The machine learning algorithm requires standard normally distributed data. In the histograms above we saw that the anomymized features were all scaled (via PCA transformation) but not the `Time` and `Amount` ones. We then scale these two features."},{"metadata":{"trusted":true,"_uuid":"1867f789863b1498d84554e4f1341c01afc59dc7"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Scale `Amount` and `Time` column values and assign to new columns\nX['normAmount'] = scaler.fit_transform(X['Amount'].values.reshape(-1, 1)) \nX['normTime'] = scaler.fit_transform(X['Time'].values.reshape(-1, 1))\n\n# Drop pre-scaled `Time` and `Amount` values from the feature dataset\nX = X.drop(['Time','Amount'],axis=1)\n\n# Plot histograms of each parameter \nX.hist(figsize = (20, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"022f2d2e467f58ee6ce92af26ba02380e4db0527"},"cell_type":"markdown","source":"## Supervised machine learning model - Logistic regression\n\nThe first model we'll consider will be a Logistic Regression model. We split the dataset into training and test set and train our model. We then predict the target on the test set and produce a classification report, an accuracy score, and confusion matrix."},{"metadata":{"trusted":true,"_uuid":"61cd694cfdeafa278c1b4bf05ded965a23b82184"},"cell_type":"code","source":"# Split the data set using 'train_test_split' function\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Instantiate the model\nfrom sklearn.linear_model import LogisticRegression\nlg_model = LogisticRegression(solver = 'liblinear')\n\n# Train the model using 'fit' method\nlg_model.fit(X_train, y_train)\n\n# Test the model using 'predict' method\ny_pred = lg_model.predict(X_test)\n\n# Print the classification report \nprint(classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_test,y_pred)\n\n# print confusion matrix\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt = 'd', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Logistic regression: Confusion Matrix'); \nax.xaxis.set_ticklabels(['Non-fradulent', 'Fradulent']); ax.yaxis.set_ticklabels(['Non-fradulent', 'Fradulent']);\n\nlg_model_accuracy = lg_model.score(X_test, y_test)\nprint(\"Model accuracy: \", lg_model_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acde7a05b383ba8f3fc0cbb7af4e5d31a9a110a4"},"cell_type":"markdown","source":"Our goal is to have the highest success rate possible in detecting fradulent trasactions. This means is that we want to have 100% success rate for TPs (true positives) and the lowest error rate for FNs (false negatives), the latter so we don't miss any fradulent transactions. This means that the most important score in the confusion matrix above is the recall rate (TP/(TP+FN)). The recall score is quite shabby, but this is not surprising given that the training set is skewed toward non-fradulent transactions.\n\nThe ROC curve below shows that our model is quite good for detecting true positives and minimizing false positives, but it doesn't say anything about the false negatives, i.e., those fadulent transactions that fly under the radar. We'll move on to the precision-recall scores to get a better idea of how the model fares with respect to false negatives."},{"metadata":{"trusted":true,"_uuid":"eae726b21e4a401c470a8834e0eb221628784822"},"cell_type":"code","source":"y_pred_prob = lg_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title('Logistic Regression ROC Curve')\nplt.show()\n\nprint('The ROC AUC score is: {}'.format(roc_auc_score(y_test, y_pred_prob)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"922c4d62f4eeb6ada970f71466718879b618ecda"},"cell_type":"markdown","source":"The AUPRC (Area Under the Precision-Recall Curve) shows the trade-off between precision and recall: As recall increases, precision plumets to a point that above 0.5 of recall precision is no better than an unskilled model, depicted by the 0.5 line. "},{"metadata":{"trusted":true,"_uuid":"4d390300a3fa7ea518b17203e7f507903f774416"},"cell_type":"code","source":"#Area Under the Precision-Recall Curve (AUPRC)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n# Plot comparison to no-skill model\nplt.plot([0, 1], [0.5, 0.5], linestyle='--', label='Unskilled model')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title('Logistic Regression Precision-Recall Curve')\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b3958b2afedcb67e4f9af723621a80c03867bda"},"cell_type":"markdown","source":"## Resampling and tuning the classifier's parameters\n\nIn order to address the skewed sample, I've adopted [oparga3's](https://www.kaggle.com/joparga3) [code for under-sampling](https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now). The idea behind undersampling in this case is creating a 50/50 ratio for class 1 (fadulent) and class 0 (non-fadulent), but randomly selecting a number of observations from the majority class (class 0 in this case) that equals that of the number of observations from the minority class (class 1)."},{"metadata":{"trusted":true,"_uuid":"96192687a177dc8c0605009bef9531a95928575b"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Number of data points in the minority class\nnumber_records_fraud = len(y[y.values == 1])\nfraud_indices = np.array(y[y.values == 1].index)\n\n# Picking the indices of the normal classes\nnormal_indices = y[y.values == 0].index\n\n# Out of the indices we picked, randomly select an x number (== number_records_fraud)\nrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Appending the 2 indices\nunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n\n# Under sample dataset\nunder_sample_data = data.iloc[under_sample_indices,:]\n\n#X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n#y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n\n\n# Under sample dataset\nX_undersample = X.iloc[under_sample_indices,:]\ny_undersample = y.iloc[under_sample_indices]\n\n# Showing ratio\nprint(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\nprint(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\nprint(\"Total number of transactions in resampled data: \", len(under_sample_data))\n\n# Undersampled dataset\nX_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n                                                                                                   ,y_undersample\n                                                                                                   ,test_size = 0.3\n                                                                                                   ,random_state = 1)\nprint(\"\")\nprint(\"Number transactions training dataset: \", len(X_train_undersample))\nprint(\"Number transactions test dataset: \", len(X_test_undersample))\nprint(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87010d517075983fe0df9df5f11e751c4b161b80"},"cell_type":"markdown","source":"## Logistic regression on undersampled data\n\nBelow the logistic model is run on the undersampled yet balanced in terms of the representation of the two classes. Precision for the detection of fradulent transaction is now 0.93 and recall is 0.94, much better results than with the much larger but highly unbalanced set before. Obviously, we want an even better model, one that catches those extra 6% fradulent transaction."},{"metadata":{"trusted":true,"_uuid":"08ffdad2e80eae31e143927cb66edb5e327b0c78"},"cell_type":"code","source":"# Instantiate the model\nfrom sklearn.linear_model import LogisticRegression\nlg_model = LogisticRegression(solver = 'liblinear')\n\n# Train the model using 'fit' method\nlg_model.fit(X_train_undersample, y_train_undersample)\n\n# Test the model using 'predict' method\ny_pred_undersample = lg_model.predict(X_test_undersample)\n\n# Print the classification report \nprint(classification_report(y_test_undersample, y_pred_undersample))\n\ncm_undersample = confusion_matrix(y_test_undersample,y_pred_undersample)\n\n# print confusion matrix\nax= plt.subplot()\nsns.heatmap(cm_undersample, annot=True, fmt = 'd', ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Logistic regression: Confusion Matrix'); \nax.xaxis.set_ticklabels(['Non-fradulent', 'Fradulent']); ax.yaxis.set_ticklabels(['Non-fradulent', 'Fradulent']);\n\nlg_model_accuracy_undersample = lg_model.score(X_test_undersample, y_test_undersample)\nprint(\"Model accuracy: \", lg_model_accuracy_undersample)\n\n\n#cv_scores = cross_val_score(lg_model, X, y, cv=5, scoring='roc_auc')\n#print(\"The scores of 5-fold cross-validation are: {}\".format(cv_scores))\n#print(\"The mean cross-validation score is: {}\".format(np.mean(cv_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1786c651a93b93d32f5edfe4d3fbee13c6b661b"},"cell_type":"code","source":"y_pred_prob_undersample = lg_model.predict_proba(X_test_undersample)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test_undersample, y_pred_prob_undersample)\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title('Logistic Regression ROC Curve')\nplt.show()\n\nprint('The ROC AUC score is: {}'.format(roc_auc_score(y_test_undersample, y_pred_prob_undersample)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"903ffd95c4ec4122f81b6106419070cae00265d3"},"cell_type":"markdown","source":"The model's precision-recall curve is what we're looking for: No huge trade off between precision and recall but rather a similarly high rates for both precision and recall."},{"metadata":{"trusted":true,"_uuid":"31d4f827bb86dcf4c55e1943b21bdef4578ce7a1"},"cell_type":"code","source":"#Area Under the Precision-Recall Curve (AUPRC)\nprecision, recall, thresholds = precision_recall_curve(y_test_undersample, y_pred_prob_undersample)\n# Plot comparison to no-skill model\nplt.plot([0, 1], [0.5, 0.5], linestyle='--', label='Unskilled model')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title('Logistic Regression Precision-Recall Curve')\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8fc3cf6eac5c0841f88aea75661c7ba1bc89895"},"cell_type":"markdown","source":"## Looking ahead\n\nNext I'd like to look at SVM and Decision Tree models, but for now I'm quite happy with the the logistic regression model here."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}