{"cells": [{"outputs": [], "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom IPython.display import display\n%matplotlib inline\n\n#This keeps the \"middle\" columns from being omitted when wide dataframes are being displayed\npd.options.display.max_columns = None\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "74e5636f18986a261388f18e15d1add3de2ae03d", "_cell_guid": "4b6a494a-e682-415f-96ed-6e2a0cc3ae2d", "trusted": false}}, {"outputs": [], "source": "#customers = pd.read_csv('../input/train.csv', na_values='-1')\ncustomers = pd.read_csv('../input/train.csv')", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8e0c6627dfcdab9f8e2970ad196e8a214761beb9", "_cell_guid": "9e1cbf2d-9dc5-4198-bdbd-edbae688cd68", "trusted": false, "collapsed": true}}, {"outputs": [], "source": "customers.head()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "92a47b2112953d20a21d660a0620de3c81d5a26e", "_cell_guid": "2edfc88d-e197-4f0c-83a4-8ce73ac955a4", "trusted": false}}, {"outputs": [], "source": "for colname in customers.columns:\n    print (colname, customers[colname].dtype)", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "453a4f500dc5d2335206db00c3092e9e0c5f8c34", "_cell_guid": "044f434b-b2de-4b2d-9be3-f37d55f837c0", "trusted": false}}, {"outputs": [], "source": "bin_cols = ['target']\ncat_cols = []\ncont_cols = []\n\nfor colname in customers.columns:\n        if 'bin' in colname:\n            bin_cols.append(colname)\n        elif 'cat' in colname:\n            cat_cols.append(colname)\n        else:\n            cont_cols.append(colname)", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f5eac2891a8de174109f284200daae5ea9072609", "_cell_guid": "7f2b994e-fb81-437d-a1a1-8c3cb3a6ba2d", "trusted": false, "collapsed": true}}, {"source": "**Exploration of the Binary Features**", "cell_type": "markdown", "metadata": {"_uuid": "b7a63f80f13cc57604c4ddf3a0c2d1bb0e02048b", "_cell_guid": "5ae3a79d-835b-4307-a53f-cac2944dd148"}}, {"outputs": [], "source": "print (len(customers))\n\ncustomers[bin_cols].describe()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "854ad03d586c9b76646148ba48ef31831c57479f", "_cell_guid": "5c1dac05-58fe-4f1e-9112-3d30d6e918c3", "trusted": false}}, {"source": "Noteworthy things in the description matrix for the binary features:\n* There are no missing values (none of the minimums are -1)\n* The target only has a mean of .036448. This means that the positive and negative classes are imbalanced. We will need to deal with this later by undersampling or oversampling.\n* With means below 0.01, features ps_ind_10_bin through ps_ind_13_bin are quite sparse, with very few positive entries (even fewer than the target)\n* Conversely, there are no features that are heavily positive\n\nBased on this, it seems unlikely that any of these invidivual features will show much correlation at all with the target. There's a chance, though, that the sparse features might line up pretty well with the sparse targets. A correlation matrix will show this.\n\nSo let's take a look:", "cell_type": "markdown", "metadata": {"_uuid": "5c0dc9c9e01958a92921555afd45184be4b842a9", "_cell_guid": "4893ccf6-5399-448a-a79b-9da50ffee407"}}, {"outputs": [], "source": "customers[bin_cols].corr()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b21b6bfe0e977ff1433cdfbdc8ac56a36b903772", "_cell_guid": "0f355a44-fd8a-45a0-995d-ca409d196016", "trusted": false}}, {"source": "Not a lot to see here:\n* Features ps_ind_06_bin through  ps_ind_09_bin have weak levels of correlation with each other (~0.20 - 0.50).\n* Features ps_ind_11_bin and ps_ind_12_bin are weakly correlated with each other (0.25), and ps_ind_12_bin and ps_ind_13_bin even less so. As stated earlier, this could be useful if the \"sparsities\" line up. But they don't....there's no correlation with the target.\n* Features ps_ind_16_bin shows strong correlation (0.50+) with both ps_ind_17_bin and ps_ind_18_bin. But ps_ind_17_bin and ps_ind_18 show little correlation with each other (0.158)...that seems odd.\n* The \"calc\" binary features (ps_calc_15_bin through ps_calc_20_bin) are noteworthy for how little correlation there is, with either each other or with the target.\n* The features that are correlated with each other are numbered sequentially...likely these features represent attributes of the customer that are similar/related.\n* None of the binary features show anything close to correlation with the target. The only features with a correlation coefficient with magnitude over 0.03 are ps_ind_06_bin, ps_ind_07_bin, and ps_ind_17_bin, which happen to be the features that show some correlation to each other. Is it meaningful that the features with the highest intercorrelation are also the features with the highest (but still very low) correlation with the target? Maybe...(<-- Warning: confirmation bias). Perhaps some form of PCA on these features would improve the correlation. (A quick Google search shows there are valid techniques for doing PCA-like dimensionality reduction on binary features.)\n\nThat's it for the binary features. A few of them look like they *might* be useful, but most of them don't look very informative. I'll start maintaining a list of features that are worth investigating further, and then let's move on to the categorical features.", "cell_type": "markdown", "metadata": {"_uuid": "dafe540b2b3cb2ff3afeb2af41aa7179d0c18e77", "_cell_guid": "afb697d1-0f15-4945-927b-fd16042feb59"}}, {"outputs": [], "source": "# Adding features to the list with a correlation magnitude of 0.01 or greater. Tiny, I know...\npotential_features = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_16_bin', 'ps_ind_17_bin']", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "69882d9156d353f180632a46977e0afad16cfe22", "_cell_guid": "22fa9ee0-ae80-44cd-924e-834773f98edc", "trusted": false, "collapsed": true}}, {"source": "**Exploration of the Categorical Features**", "cell_type": "markdown", "metadata": {"_uuid": "7e6a35d33fc6df522c5836ab35189ef5553694f5", "_cell_guid": "d141be25-1e90-44ed-a3a5-99cc620dab18"}}, {"outputs": [], "source": "customers[cat_cols].describe()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "76e6de810841539adc4a16034194e49a836951bc", "_cell_guid": "16c45574-83d4-4f2b-a5d4-afbcddbed868", "trusted": false}}, {"source": "Things to note:\n* 7 of the features are missing values. However, since these are categorical I'll plan to just treat a missing variable as an additional category for now \n* ps_car_11_cat has high cardinality, with over 100 different categories. One-hot encoding should be OK for the other variables, but I will want to test alternative approaches (vs. OHE) for ps_car_11_cat\n* ps_car_10_cat has low variance\n* ps_car_08_cat is binary-like and is not missing any data. Even though the contest sponsor said these values really are categorical, I don't see a meaningful difference between a two-category feature and a binary feature, so I'll treat this as binary\n\nLet's look at how ps_car_08_cat is correlated to the target before moving on to the categorical features:", "cell_type": "markdown", "metadata": {"_uuid": "7598ef9076ba0e13fbec27c8148bf9dd34733232", "_cell_guid": "f77e92e2-9b3e-452e-8414-9b4345fcd40a"}}, {"outputs": [], "source": "customers[['target', 'ps_car_08_cat']].corr()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ff8cd1bc3de41eb3c91afbfebd6e5cfedff564e2", "_cell_guid": "69040fa2-cf2f-4f55-9f75-ff762c124059", "trusted": false}}, {"source": "This feature's correlation is above the (tiny) 0.01 threshold I established earlier. I'll add it to the potential features list and the binary columns list.", "cell_type": "markdown", "metadata": {"_uuid": "58b8b755b1b3e15152b7ab1400029a01db6988bb", "_cell_guid": "802ec047-f78d-4217-89c2-e6e5a6db3ce5"}}, {"outputs": [], "source": "# remove the binary-like feature\ncat_cols = list(set(cat_cols) - set(['ps_car_08_cat']))\n# add it to the previous binary features list\nbin_cols += ['ps_car_08_cat']\n# add it to the potential features list\npotential_features += ['ps_car_08_cat']", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5ad59a52d08fe0d6db7df5cb4312e3ced611dc74", "_cell_guid": "82e38181-64e4-45a6-96b9-442257add5dd", "trusted": false, "collapsed": true}}, {"outputs": [], "source": "#sort by correlation to target and save correlation dataframe\nbin_corr = customers[bin_cols].corr().sort_values(['target'], ascending=0) \n#reorder the dataframe columns so we have the nice symmetry of the 1 correlations down the diagonal\nbin_corr = bin_corr[list(bin_corr.index.values)]\nbin_corr", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b7df54578efbfec0d7530c60d267d03351dffef0", "_cell_guid": "dc079c58-91f3-4b48-bfb5-0df797da1831", "trusted": false}}, {"outputs": [], "source": "plt.figure(figsize=(18, 14))\nsns.heatmap(bin_corr, cmap=\"YlGnBu\", annot=True, fmt='03.2f')", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6ec00e76f7590fbd0373fce9b28da45e5d8b20fd", "_cell_guid": "f0d01b57-a403-4f6a-86e7-b63a1f5fb0a7", "trusted": false}}, {"source": "\n\nNow we'll return to the categorical data. Ideally, I would want to visualize the categorical data with a histogram broken down by category value, but since the data is so imbalanced it will likely be difficult to \"eyeball\" anything meaningful. Still, I'll take a look just in case something jumps out:", "cell_type": "markdown", "metadata": {"_uuid": "b2e224476079e0c2d4d3180acfe9b3cf96252816", "_cell_guid": "337078ec-af38-47da-9973-c0991120819c"}}, {"outputs": [], "source": "for col in cat_cols:\n    sns.countplot(x=col, hue=\"target\", data=customers)\n    plt.show()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1043b9ed403b0a235f55a6f8e4c7bddc5c86cf4f", "_cell_guid": "76d3d1fc-e11c-4d97-8cb5-15b376d18881", "trusted": false}}, {"source": "As suspected, not much is easily visible, although it's easy to see that ps_car_10_cat has very little variance (as previously mentioned). \n\nInstead of \"eyeballing it\", I can do a Chi-square Test of Independence to see if that reveals potential relationships to the target.", "cell_type": "markdown", "metadata": {"_uuid": "bff0873db9d07559f4fdb620900b48181743c423", "_cell_guid": "2367d0fb-604d-47d6-bbbe-e1746275f525"}}, {"outputs": [], "source": "for col in cat_cols:\n    cont_table = pd.crosstab(customers['target'], customers[col])\n    print (\"Feature:\", col, \"P-value:\", stats.chi2_contingency(observed= cont_table)[1])", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a117a15aa2a783c9e3ebf4b1b82281c6e6fc1a97", "_cell_guid": "58779122-df59-44b6-b5dd-e7cb5b613468", "trusted": false}}, {"source": "These results are surprising. The extremely low p-values for all of but one of these features (our boring, low-variance friend ps_car_10_cat) suggest that all of these features could be useful. I'll add them to the potential features list, and move on to the remaining ordinal/continuous features.", "cell_type": "markdown", "metadata": {"_uuid": "6c1d2951f3afc5cfa7bfdebe733f5e3504943d71", "_cell_guid": "e748dfd9-b71d-44f9-b204-1a6c9ce51bca"}}, {"outputs": [], "source": "cat_cols = list(set(cat_cols) - set(['ps_car_10_cat']))\n\npotential_features += cat_cols", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "99e4750cb463fed66de39290510b4259591680e6", "_cell_guid": "b62669e9-66ac-4006-b91c-1e0804399c6b", "trusted": false, "collapsed": true}}, {"source": "**Exploration of the Continuous/Ordinal Features**", "cell_type": "markdown", "metadata": {"_uuid": "24298b1c0075a676e4774541c4177af4c6bddfb6", "_cell_guid": "92e7c4dd-1975-48ae-8178-2139635b40aa"}}, {"outputs": [], "source": "cont_cols = list(set(cont_cols) - set(['id']) - set(['target']))\n\ncustomers[cont_cols].describe()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "12dc889429dd81a3ddc395c79a22dd151b1aaa85", "_cell_guid": "3f796397-5563-4b96-8c4b-669445b9fa89", "trusted": false}}, {"outputs": [], "source": "\nplt.figure(figsize=(18, 14))\nplt.xticks(rotation=90)\nsns.boxplot(data=customers[cont_cols])", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "51206af3f62b01eeb2d47650161fb071edb9f3a6", "_cell_guid": "0fdcb22b-4b92-427b-a5f3-e433c0978013", "trusted": false}}, {"source": "Things to note from the description matrix and box plot of continuous/ordinal features:\n* ps_reg_01, ps_calc_01, ps_calc_03, ps_calc_02, ps_car_12, and ps_car_14 initially look like low-variance features but they are all continous features with small-scale values, so the low-variance is only relative to larger scale features like ps_calc_11, ps_calc_14, and ps_calc_10\n* Although the scales of the features are not dramatically different (only 1 order of magnitude difference), given the imbalanced dataset and scale of the binary and encoded categorical features, it may also make sense to scale the continuous features.\n* Four features are missing data: ps_reg_03, ps_car_12, ps_car_14, and ps_car_11. \n\nFor the purposes of this analysis, I'm not going to treat continuous features any differently than ordinal features, except when it comes to handling missing data (if necessary). Let's take a look at how much data is missing:", "cell_type": "markdown", "metadata": {"_uuid": "68010afb964344b662014b65a65fda8655623672", "_cell_guid": "58a1e93c-6b6f-4b4f-b816-03f199e780c7"}}, {"outputs": [], "source": "print (\"Missing Feature Counts:\")\nprint (\"ps_reg_03: \", customers['ps_reg_03'].value_counts().loc[-1])\nprint (\"ps_car_12: \", customers['ps_car_12'].value_counts().loc[-1])\nprint (\"ps_car_14: \", customers['ps_car_14'].value_counts().loc[-1])\nprint (\"ps_car_11: \", customers['ps_car_11'].value_counts().loc[-1])", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c09b443d87f54e990905b8a83d9c9ff3294f1968", "_cell_guid": "cf740e3e-a4f2-4f6f-a392-f7afeffb595e", "trusted": false}}, {"source": "ps_reg_03 and ps_car_14 are missing quite a bit of data, and are continuous features. If each feature has another feature that is strongly correlated with it, these could be used to provide a better replacement value than a simple mean or median.\n\nThe correlation matrix (for rows where both features are not null) is shown below:", "cell_type": "markdown", "metadata": {"_uuid": "d07361f67c93328ede19e88d1658239bf8f58397", "_cell_guid": "436faa07-c469-48d9-b7aa-29449471fa0b"}}, {"outputs": [], "source": "plt.figure(figsize=(18, 14))\nsns.heatmap(customers[(customers['ps_reg_03'] != -1) & (customers['ps_car_14'] != -1)][cont_cols].corr(), cmap=\"YlGnBu\", annot=True, fmt='03.2f')", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fabefd439356023cc19c315b3b5f72b856f5c215", "_cell_guid": "4a8dbdd3-ff3b-49ab-abb6-a0f6e6c7ed37", "trusted": false}}, {"source": "ps_reg_03 is strongly correlated (0.74) with ps_reg_02 and, ps_car_14 is fairly well correlated with ps_car_13 and ps_car_12 (0.44 and 0.59 respectively). Let's take a quick look at some scatter plots:", "cell_type": "markdown", "metadata": {"_uuid": "4c12ae2c4bf566368e205cc205821df595e62feb", "_cell_guid": "520e471d-c989-417d-883a-eb5a364ad72f"}}, {"outputs": [], "source": "temp = customers[(customers['ps_reg_03'] != -1) & (customers['ps_car_14'] != -1)][cont_cols]\nsns.regplot(data=temp, x='ps_reg_02', y='ps_reg_03')\nplt.show()\nsns.regplot(data=temp, x='ps_car_13', y='ps_car_14')\nplt.show()\nsns.regplot(data=temp, x='ps_car_12', y='ps_car_14')\nplt.show()", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a9e7acc2eb2587719a2f71eb21e5acb67bb2eb96", "_cell_guid": "1899a199-4d8c-4625-af25-5b97c990bf51", "trusted": false}}, {"source": "Since these features look like they have a relationship to the target, then I should be able to use them to make a better guess when replacing the missing values.\n\nAs a final step, I'll do simple one-variable linear regressions for each continuous column against the target variable to see if potential relationships are revealed:", "cell_type": "markdown", "metadata": {"_uuid": "d587b464b8b0e56f1a33c3db7bbe4e1199968a3d", "_cell_guid": "8378123c-eb01-4125-96c0-9978ad194d71"}}, {"outputs": [], "source": "lst = []\n\nfor col in cont_cols:\n    if (col == 'ps_reg_03' or col == 'ps_car_14'):\n        slope, intercept, r_value, p_value, std_err = stats.linregress(customers[customers[col] != -1][col], customers[customers[col] != -1]['target'])\n        lst.append([col, slope, p_value])              \n    else:\n        slope, intercept, r_value, p_value, std_err = stats.linregress(customers[col], customers['target'])\n        lst.append([col, slope, p_value])\n        \ncont_features = pd.DataFrame(lst, columns=['Feature', 'Slope', 'P-value'])\ncont_features.sort_values(['P-value'], inplace=True)\n                   \ncont_features", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0c17d4583bac6f7a29db6217d11ffc57f466f057", "_cell_guid": "74ea276c-6219-47c8-a497-3e5cac7c0d57", "trusted": false}}, {"source": "Observations:\n* None of the \"_calc_\" features show a significant relationship with the target and also have tiny slope coefficients\n* At the top of the list, the ps_car_12 and ps_car_13 variables have relatively large slope cofficients and extremely low p-values (the p-value for ps_car_13 is reported to be exactly zero, which is very odd).\n\nI will add the 11 features with a p-value over 0.01 to my list of potential features.", "cell_type": "markdown", "metadata": {"_uuid": "b0e2b0bcdb48503d2a7be50684575f17f0419865", "_cell_guid": "e747335c-6dfc-4486-b0f0-ca996e7e7b7a"}}, {"outputs": [], "source": "#Add features with p-value over .01 to potential features list\npotential_features += list(cont_features[cont_features['P-value'] < .01]['Feature'])", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "619b2d4300cd1b450331f7654704beff17c80b35", "_cell_guid": "b705ccd6-4843-457a-a51e-e5449c41a2fb", "trusted": false}}, {"outputs": [], "source": "print (\"Count: \", len(potential_features))\npotential_features", "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "bf6b5d23de68318a36d85756ed853261cf654d60", "_cell_guid": "b4a20c9b-b255-4b9e-b27f-8b5b850f5601", "trusted": false}}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "file_extension": ".py"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}