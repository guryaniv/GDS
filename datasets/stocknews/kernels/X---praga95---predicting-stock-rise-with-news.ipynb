{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bcc9b9183aadbc4638869256d2c959a7f1d9434"},"cell_type":"code","source":"data = pd.read_csv('../input/Combined_News_DJIA.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c0ac15bb86dc7e88ef0346e7e7cce72d13ee01d"},"cell_type":"markdown","source":"The **Label** variable will be a **1** if the DJIA stayed the same or rose on that date or **0** if the DJIA fell on that date.\n"},{"metadata":{"_uuid":"bc927298ee3f35716cc2370d82c1295a5d96747e"},"cell_type":"markdown","source":"#### Train and Test Split"},{"metadata":{"_uuid":"e5c99eb9980a17a72f7e32edf8d5dec1c8bce5a3"},"cell_type":"markdown","source":"Lets first join all the headlines for each row together."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc484879ca1b2e44eb6c77d6c2ad886e7c9d2723"},"cell_type":"code","source":"combined=data.copy()\ncombined['Combined']=combined.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b871eac4ce6e61a172aee54fd5b681c7fdeb98a9"},"cell_type":"code","source":"train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf4199576ecf4176332c5dd22235868d2cb339a4"},"cell_type":"code","source":"print(\"Length of train is\",len(train))\nprint(\"Length of test is\", len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2619946855897a271aa90cc7ac7936785691435e"},"cell_type":"code","source":"trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4afb846037b035e7f50a2c1af176e1a35e2f2107"},"cell_type":"markdown","source":"#### Simple EDA"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ae613dbe7c11bbb2223d2ba9dbd3f04df7556d73"},"cell_type":"code","source":"train = combined[combined['Date'] < '2015-01-01']\ntest = combined[combined['Date'] > '2014-12-31']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"120f831e91ea212392b52d7fd7801dfe04a75227"},"cell_type":"code","source":"non_decrease = train[train['Label']==1]\ndecrease = train[train['Label']==0]\nprint(len(non_decrease)/len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21387d2c8141ad272640e4362c5901939fccfbb2"},"cell_type":"code","source":"non_decrease_test = test[test['Label']==1]\ndecrease_test = test[test['Label']==0]\nprint(len(non_decrease_test)/len(test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8655d383eb40fbf8ccc687ceb50a51889e1327e1"},"cell_type":"markdown","source":"We can see that the occurrence of non-decrease situation is almost equal to that of a decrease market."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eca38a46ed3ba1f55f3fbd4e8cdebd1dbf1dd26c"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\ndef to_words(content): ### function to clean the words\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", content) ### get only letters\n    words = letters_only.lower().split()             ### lowercase       \n    stops = set(stopwords.words(\"english\"))         ### remove stopwords such as 'the', 'and' etc.         \n    meaningful_words = [w for w in words if not w in stops] ### get meaningful words\n    return( \" \".join( meaningful_words )) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28db7eb20d26e7507e14f04e99bed96c89d6bc2e","collapsed":true},"cell_type":"code","source":"non_decrease_word=[]\ndecrease_word=[]\nfor each in non_decrease['Combined']:\n    non_decrease_word.append(to_words(each))\n\nfor each in decrease['Combined']:\n    decrease_word.append(to_words(each))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82e3d92f5e2cc2a161fdb62d6b0489b2ac29559"},"cell_type":"code","source":"wordcloud1 = WordCloud(background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(decrease_word[1])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Words which indicate a fall in DJIA \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b933252e8ea91b54f6eddc5451404d44b964b76"},"cell_type":"code","source":"wordcloud2 = WordCloud(background_color='green',\n                      width=3000,\n                      height=2500\n                     ).generate(non_decrease_word[3])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Words which indicate a rise/stable DJIA \")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6dac893598c29d3d5395ab5d6c4511f250706d5"},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{"trusted":true,"_uuid":"e6dcdb892d5cef71bb92dbcdc0d1f54aa7a8efb8"},"cell_type":"code","source":"example = train.iloc[3,3]\nprint(example)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c87f39d06ad7fa6341d320c06a0917c87b83625a"},"cell_type":"markdown","source":"##### Lower Case"},{"metadata":{"trusted":true,"_uuid":"59e40343ea3f54a8bf7887146fcd46c0ca5facb7"},"cell_type":"code","source":"example2 = example.lower()\nprint(example2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1983178fa74c17f7509e934ae4a9fd67a6c325ea"},"cell_type":"markdown","source":"##### Count Vectorizer"},{"metadata":{"trusted":true,"_uuid":"6320e7b7be011be9107a2b41f77dbb359e32377e"},"cell_type":"code","source":"example3 = CountVectorizer().build_tokenizer()(example2)\nprint(example3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b429cfdd2901430468ded3b51a585c478a004104"},"cell_type":"code","source":"pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f34de5fb8c9079a781187884719e935cc03190e"},"cell_type":"markdown","source":"The process involved:\n\n- Converting the headline to lowercase letters\n- Splitting the sentence into a list of words\n- Removing punctuation and meaningless words\n"},{"metadata":{"_uuid":"96f0590880059b70897f2be578f472cccd387d01"},"cell_type":"markdown","source":"### Basic Model Training and Testing"},{"metadata":{"_uuid":"3da26a45a237cf697f8c61b1c86cd3ed68184b6a"},"cell_type":"markdown","source":"The tool we'll be using is CountVectorizer, which takes a single list of strings as input, and produces word counts for each one."},{"metadata":{"trusted":true,"_uuid":"b44d226c15ff1813e6cca0d0107ed870955416f1"},"cell_type":"code","source":"basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"87c4f05bd97fa7c86991732f93bc494407390b58"},"cell_type":"code","source":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe3b8ac0d65a481e58bda1e2d80db5f5bd543142"},"cell_type":"code","source":"basictest = basicvectorizer.transform(testheadlines)\nprint(basictest.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87c83c9dd0fef740b8fe008211c09f5b5a1692c5"},"cell_type":"markdown","source":"Our resulting table contains counts for 31,675 different words!"},{"metadata":{"_uuid":"372a67cc928d742ed6a5cccbdb0aba5f6b345e2b"},"cell_type":"markdown","source":"**Model Fitting**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"28dda42cbecae7cf02874c2125c901c602453694"},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c57c2f1557828630d206fa656eb6c0cdc41ab787","collapsed":true},"cell_type":"code","source":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    except Exception:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8296151bea3027194d4090646acd955210670e1"},"cell_type":"code","source":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2efc405e77d15288fd9a479e109369e549b6b42a"},"cell_type":"markdown","source":"### Feature Extraction"},{"metadata":{"_uuid":"b9a936b84d908a34abaf532f7c62f146c966bd82"},"cell_type":"markdown","source":"**TFIDF Model**\n"},{"metadata":{"_uuid":"9236595b099a40c48581be4a81fcc5ea4b75bb79"},"cell_type":"markdown","source":"Lets try to improve the score with more models and feature Selection."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"11f1d39b8e9228b141ab51fce3a83373280b42f2"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\ntrain_text = []\ntest_text = []\nfor each in train['Combined']:\n    train_text.append(to_words(each))\n\nfor each in test['Combined']:\n    test_text.append(to_words(each))\ntrain_features = tfidf.fit_transform(train_text)\ntest_features = tfidf.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"628d5fb41a0efcabfd669da9256d7bbab5b4dab4"},"cell_type":"markdown","source":"## Model fitting"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da3234ad10f680222e132a602b5e1facf62484c6"},"cell_type":"code","source":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.25, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    AdaBoostClassifier(),\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2fe443226b57f3fc16c4b4c5bfee5d0dbdd84bd"},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['Label'])\n        pred = fit.predict(test_features)\n        prob = fit.predict_proba(test_features)[:,1]\n    except Exception:\n        fit = classifier.fit(dense_features,train['Label'])\n        pred = fit.predict(dense_test)\n        prob = fit.predict_proba(dense_test)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ab05983606f263158b6badf2ac0b8df9dc62ae0"},"cell_type":"code","source":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e00a92b5b21d27b4fd7ee15929e7815aebbdb3"},"cell_type":"markdown","source":"As we can see, there has been a slight improvement from the previous scores."},{"metadata":{"_uuid":"02e13e46636ac6f583a16bd5169753178a6ed2d2"},"cell_type":"markdown","source":"## Advanced Modeling"},{"metadata":{"_uuid":"06776c7a4894d002f802e736cbefc7a8b8203269"},"cell_type":"markdown","source":"The technique we just used is known as a **bag-of-words** model. We essentially placed all of our headlines into a \"bag\" and counted the words as we pulled them out.\n\nHowever,  a single word doesn't always have enough meaning by itself.\n\nWe need to consider the rest of the words in the sentence as well!"},{"metadata":{"_uuid":"23ba94ce36143b4833e6ed701cb2eb31f46ae552"},"cell_type":"markdown","source":"### N - gram model"},{"metadata":{"_uuid":"0ee5ec7c3d3e6abacd0dbcb5cae72b9b0d42a368"},"cell_type":"markdown","source":"##### n=2"},{"metadata":{"trusted":true,"_uuid":"9b209902e468acf6ed8b21b60b3d8b1afa2076d5"},"cell_type":"code","source":"advancedvectorizer = CountVectorizer(ngram_range=(2,2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7841f8860c8e61602ae1f8229d22e96a2df199da"},"cell_type":"markdown","source":"This time we have 366,721 unique variables representing two-word combinations!"},{"metadata":{"trusted":true,"_uuid":"3e41f7ae5a74ae9fcef5da541fe65e6794e7d8f2","collapsed":true},"cell_type":"code","source":"advancedtest = advancedvectorizer.transform(testheadlines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64c29d62de40db908767b023c1d4ee623e969f37"},"cell_type":"code","source":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd546dbe5b0fb26b628175eeb0cb83128c5fbaec"},"cell_type":"code","source":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4b17ee87188f30eacc0302a313528918bdbfc48"},"cell_type":"markdown","source":"We are getting much better results now and we are getting an accuracy of 56.08%."},{"metadata":{"_uuid":"3476bd4cce8037c01f18223d0707109a4cc457f9"},"cell_type":"markdown","source":"##### n=3"},{"metadata":{"trusted":true,"_uuid":"4ff5bec0c85c2ac703aa7973a641cb6360ba1d75"},"cell_type":"code","source":"advancedvectorizer = CountVectorizer(ngram_range=(3,3))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\nadvancedtest = advancedvectorizer.transform(testheadlines)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f163fd7a8c2a58f816b4363dbdf6230f26ab3c7"},"cell_type":"markdown","source":"This time we have 611,140 unique variables representing three-word combinations!"},{"metadata":{"trusted":true,"_uuid":"f5d048dd184046abe0bb156401e94349361a113e"},"cell_type":"code","source":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1f1c4afc55101623823a06365c79f8add80d3d4"},"cell_type":"code","source":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed7a7cbc550f9c59cb2530d917516898b1cc053"},"cell_type":"markdown","source":"The accuracy does not seem to increase and it looks like we have hit our maximum accuracy point at 56%."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}