{"cells": [{"outputs": [], "execution_count": null, "source": ["import numpy as np\n", "from numpy import argmax\n", "import pandas as pd\n", "\n", "# Keras\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import LSTM\n", "from keras.layers import TimeDistributed\n", "from keras.layers import RepeatVector\n", "from keras.preprocessing.text import text_to_word_sequence"], "cell_type": "code", "metadata": {"_uuid": "29199138dd9a3d6ecdc298050840941ea26171dd", "_cell_guid": "ec342be6-7c0d-4717-8c85-b39f362a69c6"}}, {"outputs": [], "execution_count": null, "source": ["# Max columns for display\n", "pd.options.display.max_columns = 999\n", "\n", "# Disable warning message\n", "import warnings\n", "warnings.filterwarnings('ignore')"], "cell_type": "code", "metadata": {"_uuid": "e2b1f7610936c5cfee6109fd86d6f1b16023b2f0", "collapsed": true, "_cell_guid": "818d16d8-f3e6-4508-b344-b4ff31bbcfe8"}}, {"outputs": [], "execution_count": null, "source": ["df = pd.read_csv(\"../input/en_train.csv\")"], "cell_type": "code", "metadata": {"_uuid": "e30520ec064a2640ff4929791306ecc2b56f7188", "collapsed": true, "_cell_guid": "e1fb3943-adde-40a1-a2b4-fbbed059cc18"}}, {"outputs": [], "execution_count": null, "source": ["# Refer to the ebook long_short_term_memory_networks_with_python\n", "\n", "def integer_encode_X(X, vocabs, reverse_order=True, max_len=50, pad_default_char=' '):\n", "    char_to_int = dict((c, i) for i, c in enumerate(vocabs))\n", "    \n", "    Xenc = list()\n", "    for pattern in X:\n", "        pattern = pattern[:max_len]\n", "        pattern = pattern + (pad_default_char * (max_len - len(pattern)))\n", "        if reverse_order:\n", "            pattern = pattern[::-1]\n", "        integer_encoded = [char_to_int[char] for char in pattern]\n", "        Xenc.append(integer_encoded)\n", "    return Xenc\n", "\n", "\n", "def text_to_word_sequence_fixed(text):\n", "    list_words = text_to_word_sequence(text)\n", "    return list_words if len(list_words) > 0 else [text]\n", "\n", "\n", "def integer_encode_Y(y, vocabs, max_len=50, pad_default_char='PAD'):\n", "    idx_mapping = dict((c, i) for i, c in enumerate(vocabs))\n", "    \n", "    yenc = list()\n", "    for pattern in y:\n", "        pattern = text_to_word_sequence_fixed(pattern)[:max_len]\n", "        pattern = pattern + ([pad_default_char] * (max_len - len(pattern)))\n", "        integer_encoded = [idx_mapping[word] for word in pattern]\n", "        yenc.append(integer_encoded)\n", "    return yenc\n", "\n", "def one_hot_encode(X, y, vec_size_x, vec_size_y):\n", "    Xenc = list()\n", "    for seq in X:\n", "        pattern = list()\n", "        for index in seq:\n", "            vector = [0 for _ in range(vec_size_x)]\n", "            vector[index] = 1\n", "            pattern.append(vector)\n", "        Xenc.append(pattern)\n", "    yenc = list()\n", "    for seq in y:\n", "        pattern = list()\n", "        for index in seq:\n", "            vector = [0 for _ in range(vec_size_y)]\n", "            vector[index] = 1\n", "            pattern.append(vector)\n", "        yenc.append(pattern)\n", "    return np.array(Xenc), np.array(yenc)\n", "\n", "\n", "def invert(seq, vocabs, join_char=''):\n", "    idx_mapping = dict((i, c) for i, c in enumerate(vocabs))\n", "\n", "    strings = list()\n", "    for pattern in seq:\n", "        string = idx_mapping[argmax(pattern)]\n", "        strings.append(string)\n", "    return join_char.join(strings)\n", "\n", "\n", "def make_transform_train_data(\n", "        df, X_vocabs, y_vocabs,\n", "        n_in_seq_length, n_out_seq_length,\n", "        n_in_terms, n_out_terms\n", "    ):\n", "    X_small = df['before']\n", "    y_small = df['after']\n", "    \n", "    x_transformed = integer_encode_X(X_small, X_vocabs, max_len=n_in_seq_length)\n", "    y_transformed = integer_encode_Y(y_small, y_vocabs, max_len=n_out_seq_length)\n", "    \n", "    return one_hot_encode(\n", "        x_transformed,\n", "        y_transformed,\n", "        n_in_terms,\n", "        n_out_terms\n", "    )"], "cell_type": "code", "metadata": {"_uuid": "1c982a76123ef9c7aabf24f3ce4a4ebef5026d5b", "collapsed": true, "_cell_guid": "006b4ec3-a8c7-4b4d-8ef9-894f4c5e942f"}}, {"source": ["### Prepare data"], "cell_type": "markdown", "metadata": {"_uuid": "96ad9e006739d9ffe818e433d0380709c001a3d4", "_cell_guid": "e87bde98-df08-4abd-987b-c90fe90d8818"}}, {"outputs": [], "execution_count": null, "source": ["# Only try out with the CARDINAL\n", "df_filtered = df[df['class'] == 'CARDINAL']\n", "df_filtered[['before', 'after']] = df_filtered[['before', 'after']].astype(str)"], "cell_type": "code", "metadata": {"_uuid": "25cd86f3d796e0909c9a4aed8a6e81061d5923f8", "collapsed": true, "_cell_guid": "30e46eb5-0669-4ee1-bbaa-6a4b207984ba"}}, {"outputs": [], "execution_count": null, "source": ["X = df_filtered['before']\n", "y = df_filtered['after']\n", "\n", "X_vocabs = set([' '])\n", "for words in X:\n", "    X_vocabs.update(list(words))\n", "X_vocabs = [' '] + [X_vocab for X_vocab in list(X_vocabs) if X_vocab != ' ']\n", "\n", "y_vocabs = set()\n", "for words in y:\n", "    y_vocabs.update(text_to_word_sequence_fixed(words))\n", "y_vocabs = ['PAD'] + list(y_vocabs)"], "cell_type": "code", "metadata": {"_uuid": "03cd85bc81c2e17f1f5bd1299c375437503c6b1e", "collapsed": true, "_cell_guid": "b1976dff-a991-4ac2-a3e3-3122b028c524"}}, {"outputs": [], "execution_count": null, "source": ["print('Index of empty word in X %s' % X_vocabs.index(' '))\n", "print('Index of empty word in y %s' % y_vocabs.index('PAD'))"], "cell_type": "code", "metadata": {"_uuid": "c124c0f55e583b68c5e1385dab7da99fee018d10", "_cell_guid": "fc0b77f1-e107-4254-b7e0-51b1ead961ee"}}, {"outputs": [], "execution_count": null, "source": ["print('Size of vocabs X: %s' % len(X_vocabs))\n", "print('Size of vocabs y: %s' % len(y_vocabs))"], "cell_type": "code", "metadata": {"_uuid": "0b04805b428d2fdd52df5af4939c12a02fd97df7", "_cell_guid": "dd29b943-ac85-492d-b380-9426e8d95f0e"}}, {"outputs": [], "execution_count": null, "source": ["n_in_seq_length = np.min([50, len(X_vocabs)])\n", "n_out_seq_length = np.min([50, len(y_vocabs)])\n", "\n", "n_in_terms = len(X_vocabs)\n", "n_out_terms = len(y_vocabs)"], "cell_type": "code", "metadata": {"_uuid": "820456e145f444f37fbdc722048c402e16d9b921", "collapsed": true, "_cell_guid": "c785aee0-8a5d-4ffa-880f-b819dc5f324b"}}, {"source": ["### Train model"], "cell_type": "markdown", "metadata": {"_uuid": "933584148984096e2246f27c3998cb6b3851beb8", "_cell_guid": "bf69ec60-6a54-4074-999d-6194ef10f806"}}, {"outputs": [], "execution_count": null, "source": ["model = Sequential()\n", "model.add(LSTM(75, input_shape=(n_in_seq_length, n_in_terms)))\n", "model.add(RepeatVector(n_out_seq_length))\n", "model.add(LSTM(50, return_sequences=True))\n", "model.add(TimeDistributed(Dense(n_out_terms, activation='softmax')))\n", "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "print(model.summary())"], "cell_type": "code", "metadata": {"_uuid": "8b725ba4a9eac8d24e4231623d255c6cc697e4fc", "_cell_guid": "10850064-acf0-41af-b58b-09802d3a6735"}}, {"outputs": [], "execution_count": null, "source": ["%%time\n", "x_transformed, y_transformed = make_transform_train_data(\n", "    df_filtered, X_vocabs, y_vocabs,\n", "    n_in_seq_length, n_out_seq_length,\n", "    n_in_terms, n_out_terms\n", ")"], "cell_type": "code", "metadata": {"_uuid": "b96b011a0bec1f1c3fa022dc04e97fd39180c65d", "_cell_guid": "bd22432a-3172-4de5-b69d-072121d1c9c2"}}, {"outputs": [], "execution_count": null, "source": ["model.fit(x_transformed, y_transformed, epochs=1, batch_size=32)"], "cell_type": "code", "metadata": {"_uuid": "76a911412240e8604539b79e90bb7b9709c6c0ca", "_cell_guid": "92211b09-94fa-49c4-93d2-67cc7edbb95f"}}, {"source": ["### Evaluate"], "cell_type": "markdown", "metadata": {"_uuid": "a2a87879e81839a306f95491f1ea7bea21c7ddcb", "_cell_guid": "921f9ad1-d994-4dba-b6f4-5392cd142a26"}}, {"outputs": [], "execution_count": null, "source": ["# Get first 2000 rows for verify the model performance\n", "x_transformed_test, y_transformed_test = make_transform_train_data(\n", "    df_filtered.iloc[:2000], X_vocabs, y_vocabs,\n", "    n_in_seq_length, n_out_seq_length,\n", "    n_in_terms, n_out_terms\n", ")"], "cell_type": "code", "metadata": {"_uuid": "fa733b6dfa176cf5fa19b35f3a3881307a91fe2e", "collapsed": true, "_cell_guid": "5afbfd1a-ae24-4215-b8cb-4d6fc5c834ea"}}, {"outputs": [], "execution_count": null, "source": ["yhat = model.predict(x_transformed_test, verbose=0)"], "cell_type": "code", "metadata": {"_uuid": "6e69fd77e00fded9ebae06fa00edbd14f355686e", "collapsed": true, "_cell_guid": "7e2476b3-df7c-4793-ae9f-a22fe804d8c0"}}, {"outputs": [], "execution_count": null, "source": ["for idx, yh in enumerate(yhat[:50]):\n", "    yh_inverted = invert(yh, vocabs=y_vocabs, join_char=' ').replace(' PAD', '')\n", "    in_seq = invert(x_transformed_test[idx], X_vocabs).replace(' ', '')[::-1]\n", "    out_seq = invert(y_transformed_test[idx], y_vocabs, join_char=' ').replace(' PAD', '')\n", "    print('%s = %s (%s expect: %s)' % (in_seq, yh_inverted, ('TRUE' if out_seq == yh_inverted else \"FALSE\"), out_seq))"], "cell_type": "code", "metadata": {"_uuid": "d7fd648eb019331f344aaebe757136c82181957b", "_cell_guid": "6465008f-b023-4414-9eac-43d6ee738983"}}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.3", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}}