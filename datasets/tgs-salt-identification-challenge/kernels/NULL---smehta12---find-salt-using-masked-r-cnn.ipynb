{"cells":[{"metadata":{"_uuid":"b6578b357c35f5b9aa68abf287fa2e9d17cc4bd7"},"cell_type":"markdown","source":"This notebook explains how to use Masked RCNN to identify the salt from given images and its masks. It uses the matterport's [implemetation](https://github.com/matterport/Mask_RCNN) which is well known in the Kaggle community. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input/train\"))\n\nroot_dir = \"../input\"\ntrain_img_dir= \"train/images\"\ntrain_masks_dir = \"train/masks\"\ntests_dir = \"../input/test/images\" #\"../input/test/images\"\n#masks_csv = \"..input/test/train.csv\"\n# Any results you write to the current directory are saved as output.\n\nORIG_IMG_HEIGHT=101\nORIG_IMG_WIDTH=101","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Install the matterport's masked R-CNN on Kaggle kernel\nimport subprocess\nsubprocess.call([\"pip\" ,\"install\", \"git+git://github.com/matterport/Mask_RCNN.git\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4253ed71a8c96435bb50d03402f5e43a7a95812","trusted":true},"cell_type":"code","source":"# import required package from Masked R-CNN\n\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3df58cb697cfd2a1579c8d7bb88f6e6f2b35e10f"},"cell_type":"markdown","source":"## Data Exploration\n\nLet's look at some data. We can see that TGS chose to use very varied data by inspecting."},{"metadata":{"trusted":true,"_uuid":"5608470c709df9d008b75a9cb188bc9dbec0db6c"},"cell_type":"code","source":"ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\nplt.figure(figsize=(20,10))\nfor j, img_name in enumerate(ids):\n    q = j+1\n    img = plt.imread(os.path.join(root_dir, train_img_dir, img_name + '.png'))\n    img_mask = plt.imread(os.path.join(root_dir, train_masks_dir, img_name + '.png'))\n    \n    plt.subplot(1,2*(1+len(ids)),q*2-1)\n    plt.imshow(img)\n    plt.subplot(1,2*(1+len(ids)),q*2)\n    plt.imshow(img_mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92ecd81c8a3b0e31b626fc3658945af862c6fef3"},"cell_type":"markdown","source":"The CSV below contains the id of the train images and the rle masks. From this csv we'll only use ids and the masks will be loaded from the train mask dir."},{"metadata":{"_uuid":"554e7198c39de01a6294d37d887a5397afff3c53","trusted":true},"cell_type":"code","source":"# Load csv\ntrain_csv=os.path.join(root_dir, \"train.csv\")\ntrain_img_info = pd.read_csv(train_csv)\ntrain_img_info.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ebca2409ff6c593438e05bedb34e05d3f990fd7"},"cell_type":"markdown","source":"### Configurations for the training the masked RCNN"},{"metadata":{"_uuid":"088dcab813dfbbd54a5476e576012fac1722d162","trusted":true},"cell_type":"code","source":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These may not be optimal.\n\nclass SaltConfig(Config):\n     # Give the configuration a recognizable name  \n    NAME = 'find_salt'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8 \n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # no mask + 1 pneumonia classes\n    \n    IMAGE_MIN_DIM = 128\n    IMAGE_MAX_DIM = 128\n    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n    TRAIN_ROIS_PER_IMAGE = 16\n    MAX_GT_INSTANCES = 1\n    DETECTION_MAX_INSTANCES = 3\n    DETECTION_MIN_CONFIDENCE = 0.9\n    DETECTION_NMS_THRESHOLD = 0.1\n    TOP_DOWN_PYRAMID_SIZE = 128\n\n    STEPS_PER_EPOCH = 100\n    \nconfig = SaltConfig()\nconfig.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3c8c04c6b986583a8ce5bc0f1975a9341eb1fa2"},"cell_type":"code","source":"# identify whether the image has salt present or not based on the RLE masks. \n# If RLE mask present in the train df then it has salt else it doesn't has it.\n\ntrain_img_info[\"has_salt\"] = ~train_img_info[\"rle_mask\"].isnull()\ntrain_img_info.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2881b0d5c100da1f3379577d6b234de3f2150e3a"},"cell_type":"markdown","source":"### Provide the information of the dataset to the algorithm\n\nThe class below retains the information of the data set including the images and the masks. It also acts as the utility to read back the image info and masks info."},{"metadata":{"_uuid":"536fda5f209edebb594cf4ab0d9e9c254bf11c2a","trusted":true},"cell_type":"code","source":"class SaltDataSet(utils.Dataset):\n    \n    def __init__(self, image_files, root_dir, raw_img_dir, mask_img_dir, has_mask, orig_height, orig_width):\n        super(SaltDataSet, self).__init__(self)\n        \n        self.add_class('salt_shape',1, 'salt')\n        \n        for i, image_id in enumerate(image_files):\n            fp = os.path.join(root_dir, raw_img_dir, image_id+\".png\")\n            mask_fp=os.path.join(root_dir, mask_img_dir, image_id+\".png\")\n            self.add_image('salt_shape', image_id=i, path=fp,\n                           orig_height=orig_height, orig_width=orig_width, mask_fp=mask_fp, has_mask=has_mask[i])\n    \n    def get_img_info(self, img_id):\n        return self.image_info[img_id]\n    \n    def get_random_img_id(self):\n        return random.choice(list(range(len(self.image_info))))\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        has_mask=info[\"has_mask\"]\n        fp = info['mask_fp']\n        \n        mask_img = plt.imread(fp)\n        mask=np.reshape(mask_img, mask_img.shape + (1,))\n        \n        if not has_mask:\n            class_ids=np.zeros((1,), dtype=np.int32)\n        else:\n            class_ids=np.ones((1,), dtype=np.int32)\n        \n        return mask, class_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b85a706a3a8c5de9ee8b9960928b834eefa4b3"},"cell_type":"code","source":"image_ids=train_img_info[\"id\"].tolist()\n######################################################################\n# Modify this line to use more or fewer images for training/validation. \n# To use all images, do: image_fps_list = list(image_fps)\nimage_id_list = list(image_ids[:1000]) \n#####################################################################\n\n# split dataset into training vs. validation dataset \n# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\nvalidation_split = 0.1\n\nsorted(image_id_list)\nrandom.seed(42)\nrandom.shuffle(image_id_list)\nsplit_index = int((1 - validation_split) * len(image_id_list))\n\nimage_id_train = image_id_list[:split_index]\nimage_id_val = image_id_list[split_index:]\n\nprint(len(image_id_train), len(image_id_val))\nprint(image_id_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02ef61d63cddb42fbb9817e48e4c678765b28f19"},"cell_type":"code","source":"df_image_id_train = train_img_info[train_img_info[\"id\"].isin(image_id_train)]\ndf_image_id_train.reset_index(drop=True)\nprint(df_image_id_train.shape)\n\n\ndf_image_id_val = train_img_info[train_img_info[\"id\"].isin(image_id_val)]\ndf_image_id_val.reset_index(drop=True)\nprint(df_image_id_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e77646c915c2f783cac59ee35b32ebbf0a35060"},"cell_type":"code","source":"# Provide info about train and validation images to the SaltDataSet class\n\ndataset_train = SaltDataSet(df_image_id_train[\"id\"].tolist(), root_dir, train_img_dir, train_masks_dir,\n                            df_image_id_train[\"has_salt\"].tolist(), ORIG_IMG_HEIGHT, ORIG_IMG_WIDTH)\ndataset_train.prepare()\n\ndataset_val = SaltDataSet(df_image_id_val[\"id\"].tolist(), root_dir, train_img_dir, train_masks_dir,\n                            df_image_id_val[\"has_salt\"].tolist(), ORIG_IMG_HEIGHT, ORIG_IMG_WIDTH)\ndataset_val.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63e510e2243310fbbcfc34cd97311fbc3a32da6a"},"cell_type":"code","source":"# show info about random image\n\nimage_id = dataset_train.get_random_img_id()\ndataset_train.get_img_info(image_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32cc3418af34e19a0f3926ff3e211a4cdb43a7de"},"cell_type":"code","source":"# Load and display random samples and their bounding boxes\n# Suggestion: Run this a few times to see different examples. \n\nimage_id = dataset_train.get_random_img_id()\nimage_fp = dataset_train.image_reference(image_id)\nimage = dataset_train.load_image(image_id)\nmask, class_ids = dataset_train.load_mask(image_id)\n\nprint(dataset_train.get_img_info(image_id))\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image[:, :, 0], cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += image[:, :, 0] * mask[:, :, i]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530380ceb963a2b38cea56b56bd8d8ff83181842"},"cell_type":"markdown","source":"## Training\n\n\n\nNote: the following model is for demonstration purpose only. The epochs is limited to 5 but it can be modified or extend to generate the models based on multiple epochs. "},{"metadata":{"trusted":true,"_uuid":"53283787a7dfe5b43a95b52643ed7f4511e34bb2"},"cell_type":"code","source":"model_dir= os.path.join(\"model\")\nprint(model_dir)\nif not os.path.exists(model_dir):\n    os.mkdir(model_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"84f009c55b72dc734ca6138f5839eba6cecf0c24"},"cell_type":"code","source":"NUM_EPOCHS = [5]#, 10]\n\nimport warnings \nimport time\n\n\nfor epoch in NUM_EPOCHS:\n    current_model_dir= os.path.join(model_dir, \"model_{}\".format(epoch))\n    print(current_model_dir)\n    if not os.path.exists(current_model_dir):\n        os.mkdir(current_model_dir)\n    \n    model = modellib.MaskRCNN(mode='training', config=config, model_dir=current_model_dir)\n\n    # Train Mask-RCNN Model \n    start_time = time.time()\n    warnings.filterwarnings(\"ignore\")\n    \n    model.train(dataset_train, dataset_val, \n                learning_rate=config.LEARNING_RATE, \n                epochs=epoch, \n                layers='all'\n               )\n    end_time = time.time()\n\n    print(\"completion time:{}\".format((start_time-end_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff0fd1a4811cb595287d17ce684b43ca00cd9c28"},"cell_type":"markdown","source":"### Inference"},{"metadata":{"trusted":true,"_uuid":"38a7aba7d8e11c25fd85a56ad7ee1168fc87cf7a"},"cell_type":"code","source":"MODEL_DIR=os.path.join(model_dir, \"model_{}\".format(epoch[0]))\n\nclass InferenceConfig(SaltConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode=\"inference\", \n                          config=inference_config,\n                          model_dir=MODEL_DIR)\n\nmodel_path = model.find_last()\nprint(model_path)\n\n# Load trained weights\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eae074ca51cde2db86d732e354b6a7899e1513d5"},"cell_type":"code","source":"image_id = random.choice(dataset_val.image_ids)\nprint(dataset_val.get_img_info(image_id))\noriginal_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n    modellib.load_image_gt(dataset_val, inference_config, \n                           image_id, use_mini_mask=False)\n\nlog(\"original_image\", original_image)\nlog(\"image_meta\", image_meta)\n\nvisualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                            dataset_train.class_names, figsize=(8, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63ddd87658adb518bf9fd6022d35ee56dc52e85b"},"cell_type":"code","source":"results = model.detect([original_image], verbose=1)\n\n\nr = results[0]\nvisualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                            dataset_val.class_names, r['scores'], figsize=(8, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11b3e177aecdec89c336188a02290b906b460145"},"cell_type":"code","source":"from itertools import groupby\n\ndef rle_encode(binary_mask):\n    rle = {'counts': [], 'size': list(binary_mask.shape)}\n    counts = rle.get('counts')\n    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n        if i == 0 and value == 1:\n            counts.append(0)\n        counts.append(len(list(elements)))\n    print(rle['counts'])\n    return rle\n\nprint(rle_encode(r[\"masks\"]))\n\nplt.imshow(r[\"masks\"].squeeze())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdffdff4956c67f19b6940b4020668f6762c4534"},"cell_type":"code","source":"test_images=os.listdir(tests_dir)\ntest_images = list(map(lambda x:os.path.join(tests_dir, x), test_images))\ntest_images[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9caa1c799d40d14b8e7cface944d553942c174cb"},"cell_type":"markdown","source":"### Predict the multiple image and Prepare for submission\n\nHere, the salt identification is derieved based on the model generated above and it converts the binary mask to RLE. It will generate the dataframe for each provided test image id and the RLE mask."},{"metadata":{"trusted":true,"_uuid":"ed76b2a36af48aabb0c0c356def5bda835386ee7"},"cell_type":"code","source":"def predict(image_paths, min_conf=0.95):\n    \n    submission_dict=[]\n       \n    # assume square image\n    resize_factor = ORIG_IMG_HEIGHT / config.IMAGE_SHAPE[0]\n    \n    prev_mask=None\n    #resize_factor = ORIG_SIZE \n    for image_id in tqdm(image_paths): \n        image = plt.imread(image_id)\n        \n        print(image_id)\n        \n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1) \n        image, window, scale, padding, crop = utils.resize_image(\n            image,\n            min_dim=config.IMAGE_MIN_DIM,\n            min_scale=config.IMAGE_MIN_SCALE,\n            max_dim=config.IMAGE_MAX_DIM,\n            mode=config.IMAGE_RESIZE_MODE)\n            \n        print(image.shape)\n        salt_img_id = os.path.basename(image_id).split(\".\")[0]\n        \n        results = model.detect([image])\n        r = results[0]\n        \n        print(r[\"masks\"].shape)\n        \n#         if prev_mask:\n#             print(\"array equal:{}\".format(np.array_equal(r[\"masks\"], prev_mask)))\n        prev_mask = r[\"masks\"]\n        \n        plt.imshow(np.squeeze(r[\"masks\"]))\n        \n        assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n        \n        rle_mask=rle_encode(r['masks'])[\"counts\"]\n        \n        rle_mask=\" \".join(map(str, rle_mask))\n                \n        submission_dict.append({\"id\":salt_img_id, \"rle_mask\":rle_mask})\n\n                               \n    submission_df=pd.DataFrame(submission_dict)\n    \n    print(submission_df.head())\n    \n    return submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb9d017dce315c95abaa0a262c0ff3479d64b6f"},"cell_type":"code","source":"submission_df = predict(test_images[:5])\n#submission_df.to_csv(os.path.join(root_dir, \"submission.csv\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}