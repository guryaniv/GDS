{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["# Overview\n", "Here we try to focus on just the emojis to figure out which of them are toxic"], "metadata": {"_cell_guid": "88b36a6b-6bb5-4cdd-8fd4-0f5c43bf1903", "_uuid": "3ae4f06c309793cb1b206300f9c0f56c5fd2e85c"}}, {"cell_type": "code", "execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from keras.preprocessing import text as keras_text, sequence as keras_seq\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint"], "outputs": [], "metadata": {"_cell_guid": "d06b6ce7-d93b-424b-ace1-2464e13f90ae", "_uuid": "e41e62413248c1a87934d5c5e8bb0390b2481add"}}, {"cell_type": "markdown", "source": ["# Load and Preprocessing Steps\n", "Here we load the data and fill in the misisng values"], "metadata": {"_cell_guid": "6ddb5d07-801c-4c82-8fb4-24fd71567dc0", "_uuid": "96d5f7c706b6a11fbc2a2530c97dfefb4faf209d"}}, {"cell_type": "code", "execution_count": null, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = train.sample(frac=1)\n", "\n", "list_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "y = train[list_classes].values\n", "list_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values\n", "\n", "all_sentences = np.concatenate([list_sentences_train, list_sentences_test])"], "outputs": [], "metadata": {"_cell_guid": "c8bbaffa-b53b-4840-b47f-2d5bb117fc53", "_uuid": "d6b8d61f1c16e0a7e1713d31428c5d01097a985c", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# We add all of the possible categories together as a proxy for level of toxicity\n", "total_toxicity = np.sum(y,1)\n", "print('Distribution of Total Toxicity Labels (important for validation)')\n", "print(pd.value_counts(total_toxicity))"], "outputs": [], "metadata": {"_cell_guid": "a121ad8b-d39d-4f38-8f0d-06a50710689a", "_uuid": "84805fa62ace58a65ff7d2b26dfa5b9fadad4678"}}, {"cell_type": "markdown", "source": ["## Sequence Generation\n", "Here we take the data and generate sequences from the data. We have an unusual preprocessing step because we want to remove everything that is normal text or number"], "metadata": {"_cell_guid": "ebe64376-9126-4f7f-b10d-388a8f3bc77d", "_uuid": "ee41735cbf0933aec3590d0da6a8b6d59c7ea608"}}, {"cell_type": "code", "execution_count": null, "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "import re, string\n", "nums_chars = ''.join(['{}'.format(i) for i in range(10)])\n", "re_prep = re.compile(f'([{string.ascii_letters}{nums_chars} ;:.,\\t!?\\-_\\(\\)\\[\\]])') # non-character things\n", "re_prep = re.compile(r'[^\\u263a-\\U0001f645]') # just emojis\n", "def prep_func(s): return re_prep.sub('', s)\n", "print('Verify we are keeping the right things:',\n", "      prep_func('Hello my name (\u00b5-998)\\t and I [\ud83d\ude0d] emojis ;-)'))\n", "vec = CountVectorizer(preprocessor=prep_func, \n", "                      analyzer = 'char', \n", "                     binary = True)"], "outputs": [], "metadata": {"_cell_guid": "2d715de6-d361-4a47-9082-76b3e6c1b3c9", "_uuid": "f312edcbb9afbe08523e3fdeb08e558a1ef1fa87"}}, {"cell_type": "code", "execution_count": null, "source": ["%%time\n", "vec.fit(all_sentences)\n", "vocab_lookup = {idx: k for k,idx in vec.vocabulary_.items()}\n", "print(len(vocab_lookup), 'unique characters found')"], "outputs": [], "metadata": {"_cell_guid": "7a472046-efc7-4941-a084-7d240b9de292", "_uuid": "de0935723205e23a7b7838c396eef9debc984929"}}, {"cell_type": "code", "execution_count": null, "source": ["X_train = vec.transform(list_sentences_train)\n", "X_test = vec.transform(list_sentences_test)"], "outputs": [], "metadata": {"_cell_guid": "d20fc51b-a2af-435b-9baf-47c60d954407", "_uuid": "674a4fe6d153f1cfc8f2e29ae2fbaa351a18641b", "collapsed": true}}, {"cell_type": "markdown", "source": ["# Simple Model\n", "Here we create a simple random forest model for determining which characters relate to toxicity. We make a simple split to start and then fit the model on one group and then validate it on the other"], "metadata": {"_cell_guid": "a6c4a179-4049-44bf-87c0-5e095670cce9", "_uuid": "c133153f5b6b8200878f22bce39a216d479c017a"}}, {"cell_type": "code", "execution_count": null, "source": ["from sklearn.model_selection import train_test_split\n", "X_t_train, X_t_test, y_train, y_test = train_test_split(X_train, \n", "                                                        total_toxicity, \n", "                                                        test_size = 0.2, \n", "                                                        stratify = total_toxicity,\n", "                                                       random_state = 2017)\n", "print('Training:', X_t_train.shape)\n", "print('Testing:', X_t_test.shape)"], "outputs": [], "metadata": {"_cell_guid": "00db0eea-f8c1-4d74-82ee-499d8d63ab3c", "_uuid": "0c7d698963639d37463d7256438dc85518865706"}}, {"cell_type": "code", "execution_count": null, "source": ["from sklearn.ensemble import RandomForestRegressor\n", "basic_rf = RandomForestRegressor()\n", "basic_rf.fit(X_t_train, y_train)"], "outputs": [], "metadata": {"_cell_guid": "41cd3c6b-72c1-48d3-b6b1-6b6e33772d97", "_uuid": "fce7b3b19204451da59402cdc1452db6addb64e6"}}, {"cell_type": "code", "execution_count": null, "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "from sklearn.metrics import roc_curve, roc_auc_score\n", "y_pred = basic_rf.predict(X_t_test)\n", "fig, ax1 = plt.subplots(1,1)\n", "tpr, fpr, _ = roc_curve(y_test>0, y_pred)\n", "ax1.plot(tpr, fpr, 'b.', label = 'ROC Curve')\n", "ax1.plot(tpr, tpr, 'r-', label = 'Random Guessing')\n", "ax1.set_ylabel('True Positive Rate')\n", "ax1.set_xlabel('False Positive Rate')"], "outputs": [], "metadata": {"_cell_guid": "1c4ce644-94cc-487e-9971-3096699ce34d", "_uuid": "6f5b4848c9eadc7c179c226f745897013eb85a58"}}, {"cell_type": "markdown", "source": ["# Character Importance\n", "We can now show the importance of each character for deciding if a message is toxic or not"], "metadata": {"_cell_guid": "fdb59682-a1d7-4eda-8c28-5309beab8549", "_uuid": "2d57d28db3dfe93a3a282dacec6dc4f050b0d0c6"}}, {"cell_type": "code", "execution_count": null, "source": ["show_characters = 100\n", "for i in np.argsort(-1*basic_rf.feature_importances_)[:show_characters]:\n", "    print(vocab_lookup[i], '\\t%2.2f%%' % (100*basic_rf.feature_importances_[i]))"], "outputs": [], "metadata": {"_cell_guid": "9f1ff657-6129-4c00-9756-1915b6db7389", "_uuid": "467cc9819d7545bff9eb2a18dc51780d92408f8e"}}, {"cell_type": "markdown", "source": ["# Logistic Regression\n", "Here we apply cross-validated logistic regression to determine which characters are important"], "metadata": {"_cell_guid": "05032baa-bade-426a-b87c-d8d64300256e", "_uuid": "4de322cf1453258d0019c9b0a9b70ad77cfbb743"}}, {"cell_type": "code", "execution_count": null, "source": ["%%time\n", "from sklearn.linear_model import LogisticRegressionCV\n", "basic_logreg = LogisticRegressionCV()\n", "basic_logreg.fit(X_train, total_toxicity>0)"], "outputs": [], "metadata": {"_cell_guid": "90de75ae-3a9c-4b62-b165-ced65afbe463", "_uuid": "1dbe8f9f111c55e385d917c591a6c27fcdcc1906"}}, {"cell_type": "markdown", "source": ["# Show the most significant characters\n", "Here we show the characters by the ones with the largest coefficients"], "metadata": {"_cell_guid": "6fb8d7a0-7229-4f39-b56b-bdac0425cdac", "_uuid": "c681624bd3b66835f846df047f843dc492f2bf71"}}, {"cell_type": "code", "execution_count": null, "source": ["show_characters = 100\n", "for i in np.argsort(-1*np.abs(basic_logreg.coef_[0,:]))[:show_characters]:\n", "    print(vocab_lookup[i], '\\t%03.2f%%' % (100*basic_logreg.coef_[0,i]))"], "outputs": [], "metadata": {"_cell_guid": "b08d6f92-990d-4eff-a1dd-584ad2704416", "scrolled": false, "_uuid": "4007ed23f0065529f39279ed1c0cb275ed414dbc"}}, {"cell_type": "code", "execution_count": null, "source": [], "outputs": [], "metadata": {"_cell_guid": "b4ac252d-7daa-4ab6-a9cf-38fc6fa88223", "_uuid": "a80ee3203a429bdcd13fd5765ff4f0fdcfc7dbbe", "collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"file_extension": ".py", "name": "python", "mimetype": "text/x-python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}