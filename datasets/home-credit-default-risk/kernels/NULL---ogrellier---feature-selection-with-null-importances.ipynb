{"cells":[{"metadata":{"_uuid":"2f0688e506bb25201d67f33b3ff9c9a2f5bd5e30"},"cell_type":"markdown","source":"### Feature selecture using target permutation\n\nThe notebook uses a procedure described in [this article]( https://academic.oup.com/bioinformatics/article/26/10/1340/193348).\n\nFeature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target).\n\nThe notebook implements the following steps  :\n - Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n - Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution\n - for each feature test the actual importance:\n    - Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n    - Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.\n\nFor processing time reasons, the notebook will only cover application_train.csv but you can extend it as you wish.\n"},{"metadata":{"_uuid":"b7e08c832a50bf4fbd4476d1cebd943ee230be3f"},"cell_type":"markdown","source":"### Import a few packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport time\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore', UserWarning)\n\nimport gc\ngc.enable()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"7421e2121aa6c9229bd4cfeb961912cfefc88e5d"},"cell_type":"markdown","source":"### Read application_train\n\nRead data and take care of categorical features"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"data = pd.read_csv('../input/application_train.csv')\n\ncategorical_feats = [\n    f for f in data.columns if data[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    data[f_], _ = pd.factorize(data[f_])\n    # Set feature type as categorical\n    data[f_] = data[f_].astype('category')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d800326c1a3fcb3e8be2d0051cbde2c1927d9ab3"},"cell_type":"markdown","source":"### Create a scoring function\n\nCoring function uses LightGBM in RandomForest mode fitted on the full dataset "},{"metadata":{"trusted":true,"_uuid":"d78a5c60c870b15590476088da546be03e2fa6ee","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def get_feature_importances(data, shuffle, seed=None):\n    # Gather real features\n    train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']]\n    # Go over fold and keep track of CV score (train and valid) and feature importances\n    \n    # Shuffle target if required\n    y = data['TARGET'].copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = data['TARGET'].copy().sample(frac=1.0)\n    \n    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'binary',\n        'boosting_type': 'rf',\n        'subsample': 0.623,\n        'colsample_bytree': 0.7,\n        'num_leaves': 127,\n        'max_depth': 8,\n        'seed': seed,\n        'bagging_freq': 1,\n        'n_jobs': 4\n    }\n    \n    # Fit the model\n    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(train_features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n    \n    return imp_df","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"1e6f4d74f63ec0961d9b76b8b3b60b4fb7b47511"},"cell_type":"markdown","source":"### Build the benchmark for feature importance\n\n![](http://)The original paper does not talk about this but I think it makes sense to have a distribution of actual importances as well"},{"metadata":{"trusted":true,"_uuid":"38c932d1b6422bcedcc0ab378f8cc4e062393478","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Seed the unexpected randomness of this world\nnp.random.seed(123)\n# Get the actual importance, i.e. without shuffling\nactual_imp_df = get_feature_importances(data=data, shuffle=False)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fce92f2e7c467aec384d21584eb271e88fc4d4b"},"cell_type":"code","source":"actual_imp_df.head()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"93791d0a1bd477a19a1fbf7c61e0f2f621bc78bb"},"cell_type":"markdown","source":"### Build Null Importances distribution"},{"metadata":{"trusted":true,"_uuid":"2332391f11aa881124c49e072c71eb58297c6813","_kg_hide-input":true},"cell_type":"code","source":"null_imp_df = pd.DataFrame()\nnb_runs = 80\nimport time\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=data, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    # Erase previous message\n    for l in range(len(dsp)):\n        print('\\b', end='', flush=True)\n    # Display current run and time used\n    spent = (time.time() - start) / 60\n    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n    print(dsp, end='', flush=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96d51a41b38921ac84ba0853b935a3645afe3939"},"cell_type":"code","source":"null_imp_df.head()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"efb49112d2adb44a3019aa142f8efccc83743425"},"cell_type":"markdown","source":"### Display distribution examples\n\nA few plots are better than any words"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0905c7e06c936a8cdbc0012bcd1a3a13d1cd5803","_kg_hide-input":true},"cell_type":"code","source":"def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())\n        ","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ed28ef87f45a1b9fbf9681250ffd2948c45cb46"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG')","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"556dd360ce384a54eb1babb92f085403d2a45433"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='CODE_GENDER')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2024cc04ab4e53db4cc50db4efe527030b54ee5e"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_1')","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d7ebb2e105f7ac759a42c54228d1cd7c97fe484"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_2')","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cacee961a02ee3a9cd1b5ee56ec9b2239714129"},"cell_type":"code","source":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_3')","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"30a61af86daa613a6b86a980639b212a6065be2b"},"cell_type":"markdown","source":"From the above plot I believe the power of the exposed feature selection method is demonstrated. In particular it is well known that :\n - Any feature sufficient variance can be used and made sense of by tree models. You can always find splits that help scoring better\n - Correlated features have decaying importances once one of them is used by the model. The chosen feature will have strong importance and its correlated suite will have decaying importances\n \n The current method allows to :\n  - Drop high variance features if they are not really related to the target\n  - Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)\n"},{"metadata":{"_uuid":"a99b7b4914185cc4307f17b7d325bdde359101bf"},"cell_type":"markdown","source":"### Score features\n\nThere are several ways to score features : \n - Compute the number of samples in the actual importances that are away from the null importances recorded distribution.\n - Compute ratios like Actual / Null Max, Actual  / Null Mean,  Actual Mean / Null Max\n \nIn a first step I will use the log actual feature importance divided by the 75 percentile of null distribution."},{"metadata":{"trusted":true,"_uuid":"c95234084a71f516361b685ec2a5872b54322639","_kg_hide-input":true},"cell_type":"code","source":"feature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"25512458d6d6b173d04b19e2f0c7e70bdc4fd71c"},"cell_type":"markdown","source":"### Save data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9ddcbced0a061e0cdd867ae09f53b822d22a624"},"cell_type":"code","source":"null_imp_df.to_csv('null_importances_distribution_rf.csv')\nactual_imp_df.to_csv('actual_importances_ditribution_rf.csv')","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"b434ed7f33710271315545c39bae87d9d0cbeee6"},"cell_type":"markdown","source":"### Check the impact of removing uncorrelated features\n\nHere I'll use a different metric to asses correlation to the target"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d8a5d1b03e26f61dee356ac5661ade25e1ef155a"},"cell_type":"code","source":"correlation_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n    correlation_scores.append((_f, split_score, gain_score))\n\ncorr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n\nfig = plt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\nfig.subplots_adjust(top=0.93)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"71cb5349912200b11e2c9a386c7cd2a308558781"},"cell_type":"markdown","source":"### Score feature removal for different thresholds"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"35f30fb61b917e93701ff63b545836d27626a748"},"cell_type":"code","source":"def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n    # Fit LightGBM \n    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'learning_rate': .1,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'seed': 13,\n        'n_jobs': 4,\n        'min_split_gain': .00001,\n        'reg_alpha': .00001,\n        'reg_lambda': .00001,\n        'metric': 'auc'\n    }\n    \n    # Fit the model\n    hist = lgb.cv(\n        params=lgb_params, \n        train_set=dtrain, \n        num_boost_round=2000,\n        categorical_feature=cat_feats,\n        nfold=5,\n        stratified=True,\n        shuffle=True,\n        early_stopping_rounds=50,\n        verbose_eval=0,\n        seed=17\n    )\n    # Return the last mean / std values \n    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n\n# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]\n# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])\n\nfor threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]:\n    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n    split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n    gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n                                                                                             \n    print('Results for threshold %3d' % threshold)\n    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET'])\n    print('\\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1]))\n    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET'])\n    print('\\t GAIN  : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))\n","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"244b4799bb7f88477370beb378b197a1278595da"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}