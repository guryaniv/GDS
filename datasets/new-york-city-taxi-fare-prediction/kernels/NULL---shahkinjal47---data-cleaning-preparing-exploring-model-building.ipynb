{"cells":[{"metadata":{"trusted":false,"_uuid":"0eeef2513a0a203ff97292e94e3a29a9362d128e"},"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt \n#import googlemaps\nimport random\nfrom math import cos, asin, sqrt\nimport numpy as np\nimport datetime\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n#gmaps = googlemaps.Client(key='AIzaSyAm_TEkXHyBqBAKJXca9N_JrZyy6K7dpoU') \nTYPE = \"\"\n#function to load dataset into dataframe, reducing the datatypes from float64 and int64 to float32 and uint8\ndef load_ny_taxi_data(type = TYPE):\n    if(type == \"train\"):\n        data_types = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\n        cols = list(data_types.keys())\n        file = \"../input/train.csv\"\n    if(type == \"test\"):\n        data_types = {'key': 'str',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n        cols = list(data_types.keys())\n        file = \"../input/test.csv\"\n    return pd.read_csv(file, usecols=cols, dtype=data_types) \n    \ntrain = load_ny_taxi_data(\"train\")\n\ntrain = train.sample(n=1000000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"52b10a16d85c12dd3478734b2aef45c68e03ec8b"},"cell_type":"code","source":"\ntrain.describe() #take a look at the data\ntrain.isnull().sum() #check for the null values\nprint(\"Minimum fare = \" , train[\"fare_amount\"].min())\nprint(\"Maximum fare = \" , train[\"fare_amount\"].max())\nprint(\"Negative fare rows = \" , sum(train[\"fare_amount\"] < 0))\nprint(\"0 fare rows = \" , sum(train[\"fare_amount\"] == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd0a9ad4c80d1f805c16ed0f7a71126fe5eaa860"},"cell_type":"code","source":"#Data Cleaning\n\n# Currently, minimum taxi fare in NYC is $2.5. Since, we have data from 2009, assuming that minimum fare \n# would be atleast $1, removing rows with fare < $1.5\n# print(\"Rows with fare<1 are \", sum(train[\"fare_amount\"] < 1.5)) #Number of rows with fare < $1.5\n# print(\"Rows with fare>450 are \", sum(train[\"fare_amount\"] > 450)) #Number of rows with fare > $450\n\n#Maximum fare between two farthest points in NY city is $320 as per the current uber fare. \n#So considering all the data with fare>$450 and fare<1.5 as bad data, we will remove those values\ntrain = train[(train[\"fare_amount\"] > 1.5) & (train[\"fare_amount\"] <= 450)]\n\n#NY City latitude is between 40.4965, 40.9159 and longitude is between -74.25 , -73.7016\n#Identify latitudes and lngitudes which does not belong to NY City, adding some grace distance\n\n# print(train[(train[\"pickup_latitude\"] <39.8) | (train[\"pickup_latitude\"] > 41.3)].shape[0])\n# print(train[(train[\"pickup_longitude\"] < -75) | (train[\"pickup_longitude\"] > -71.8)].shape[0])\n# print(train[(train[\"dropoff_latitude\"] <39.8) | (train[\"dropoff_latitude\"] > 41.3)].shape[0])\n# print(train[(train[\"dropoff_longitude\"] < -75) | (train[\"dropoff_longitude\"] > -71.8)].shape[0])\n\n#Remove rows with bad latitude and longitude values\ntrain = train[(train[\"pickup_latitude\"] >39.8) & (train[\"pickup_latitude\"] < 41.3)]\ntrain = train[(train[\"pickup_longitude\"] > -75) & (train[\"pickup_longitude\"] < -71.8)]\ntrain = train[(train[\"dropoff_latitude\"] >39.8) & (train[\"dropoff_latitude\"] < 41.3)]\ntrain = train[(train[\"dropoff_longitude\"] > -75) & (train[\"dropoff_longitude\"] < -71.8)]\n\n#Removing 195532 rows with passenger count more than 6 and less than 1\n# print(train[(train[\"passenger_count\"] > 6 ) | (train[\"passenger_count\"] < 1)].shape[0])\n\ntrain = train[(train[\"passenger_count\"] <= 6 ) & (train[\"passenger_count\"] >= 1)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa4ec2a208e79de428dceff0afdde7a706d51df1"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain.hist(bins=50,figsize=(15,15)) #Look at the histograms\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f4679a92c96f69c52226e203728bcb5d54d3efb"},"cell_type":"code","source":"#Visualizing data\n# train_sample = train.sample(n=100000) #take a sample of 100,000 randon rows\ntrain.plot(kind=\"scatter\", x=\"pickup_longitude\", y=\"pickup_latitude\", c=\"red\", alpha=0.1)\ntrain.plot(kind=\"scatter\", x=\"dropoff_longitude\", y=\"dropoff_latitude\", c=\"blue\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8e0a2aba639d8a436769a7dbdccbd503f6757f3c"},"cell_type":"code","source":"#Dual axis plot\n# fig, ax1 = plt.subplots()\n\n# ax2 = ax1.twinx()\n# ax1.plot(kind=\"scatter\", x=\"pickup_longitude\", y=\"pickup_latitude\", c=\"red\", alpha=0.1)\n# ax2.plot(kind=\"scatter\", x=\"dropoff_longitude\", y=\"dropoff_latitude\", c=\"blue\", alpha=0.1)\n# plt.show()\n# ax1.set_xlabel(train_sample[\"pickup_longitude\"])\n# ax1.set_ylabel(train_sample[\"pickup_latitude\"], color='g')\n# ax2.set_ylabel(train_sample[\"dropoff_latitude\"], color='b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f9e8506291446426719e77e00a466311611e6be9"},"cell_type":"code","source":"#Look for correlations anmong variables\ncorr = train_sample.corr()\ncorr[\"fare_amount\"].sort_values(ascending=False)\nfrom pandas.tools.plotting import scatter_matrix\nscatter_matrix(train_sample,figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e5208bf4c379423a850c58d0803d5767e9aed9d"},"cell_type":"code","source":"#Calculate distance and duration for given pickup and drop locations using Google's Distance Matrix API\n#Could not use it because of the limitations on the number of free API calls and processing speed for \n#such large dataset \n\n# <--- !!!!  DO NOT EXECUTE THIS STEP  !!!! --->\n\n# def calc_dist_dur(df):\n\n#     pickup = df.pickup_latitude.astype(str) + ',' + df.pickup_longitude.astype(str)\n#     drop = df.dropoff_latitude.astype(str) + ',' + df.dropoff_longitude.astype(str)\n\n#     distance = []\n#     duration = []\n#     for i in range(len(df)):\n#         dist = gmaps.distance_matrix(origins = pickup[i],destinations = drop[i])\n#         dist = dist['rows'][0]\n#         dist = dist['elements'][0]\n#         dur = dist['duration']['value']\n#         dist = dist['distance']['value']\n#         distance.append(dist)\n#         duration.append(dur)\n    \n#     return distance, duration\n\n# distance, duration = calc_dist_dur(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"918f24058dc9383c5edaee0c1742e56023496f3d"},"cell_type":"code","source":"# Define function to calculate distance between two points by latitude and longitude\n# Faced some errors in this function as described below\n# 1. lat1, lon1 etc were Panda series and so it won't allow its conversion to float.Typecasted to list when calling the function\n# 2.Python won't allow operations between two list, so converted list to np.array\n# 3.Math.cos,asin etc functions does not support np.array. So used np.cos, np.asin functions\n\ndef distance(lat1, lon1, lat2, lon2):\n    lat1 = np.array(lat1)\n    lon1 = np.array(lon1)\n    lat2 = np.array(lat2)\n    lon2 = np.array(lon2)\n    p = 0.017453292519943295     #Pi/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2\n    return (7917.512 * np.arcsin(np.sqrt(a)))\n\n# Assigning dummy value 1.1 to column \"distance\" to change its datatype to float64 from object\ntrain[\"distance\"] = 1.1\n\n#Changing datatype of train.Distance from float64 to float16\ntrain[\"distance\"] = train[\"distance\"].astype(np.float16)\n\n#Calculating distance between pickup and drop points using Haversine formula\ntrain['distance'] = distance(train.pickup_latitude.tolist(), train.pickup_longitude.tolist(),\n                             train.dropoff_latitude.tolist(), train.dropoff_longitude.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2dcca7aaa7181a733f1d7e9cab779920fcadf723"},"cell_type":"code","source":"#Since we have distance now, let's see its relation with fare amount\n# We can see a clear linear relationship between distance and fare. Also, there are some trips with zero\n# distance by non-zero fare amount. We can also see some stright lines around fare 40-60, may be fixed fare\n# to airports\n\ntrain.plot(kind=\"scatter\", x=\"distance\", y=\"fare_amount\", c=\"red\", alpha=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c9d62a0aad34ad61b50099cd4ffb1048d4dcb3f4"},"cell_type":"code","source":"# Create three new variables for storing year,time and hour of pickup\nimport datetime\ntrain[\"pickup_datetime\"] = pd.to_datetime(train[\"pickup_datetime\"])\ntrain[\"year\"] = train[\"pickup_datetime\"].dt.year\ntrain[\"time\"] = train[\"pickup_datetime\"].dt.time\ntrain[\"hour\"] = train[\"pickup_datetime\"].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"62bb3f62f8e9d8703c497440e1c011fe0aaf7a8e"},"cell_type":"code","source":"# train[\"weekday\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a9960c099228c94758445ff8b86c822685c7de8"},"cell_type":"raw","source":""},{"metadata":{"trusted":false,"_uuid":"c93b879ce18e0ad138458f4d7a90214411e14710"},"cell_type":"code","source":"# Define Function to polulate new column \"weekday\" where 1 = weekday, 0 = weekend\ndef weekday(pickup_date):\n    weekday = []\n    for index,val in pickup_date.iteritems():\n        val = pd.to_datetime(val)\n        if(val.weekday() == 5 or val.weekday() == 6):\n            weekday.append(0)\n        else:\n            weekday.append(1)\n    return weekday\n\ntrain[\"weekday\"] = 0\ntrain[\"weekday\"] = weekday(train[\"pickup_datetime\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ed20e8b5a50f2fe2103f9f77b5f5c35604d89a5"},"cell_type":"code","source":"train[\"hour\"] = train[\"pickup_datetime\"].dt.hour\n#Histogram on train.hour to see when highest number of cabs are booked\nplt.hist(train[\"hour\"], bins=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8cf60623626245a79485263b8ff3a6328dc533ab"},"cell_type":"code","source":"# Let's categorize time data into morning, afternoon, evening and night in new column \"part_of_day\"\n\ndef time_in_range(start, end, x):\n    # Return true if x is in the range [start, end]\n    if start <= end:\n        return start <= x <= end\n    else:\n        return start <= x or x <= end\n    \ndef assign_day_part(pickup_date):\n    day_part = []\n    # Morning = 0600-1000\n    mornStart = datetime.time(6, 0, 1)\n    mornEnd = datetime.time(10, 0, 0)\n\n    # Midday = 1000-1600\n    midStart = datetime.time(10, 0, 1)\n    midEnd = datetime.time(16, 0, 0)\n\n    # Evening = 1600-2000\n    eveStart = datetime.time(16, 0, 1)\n    eveEnd = datetime.time(20, 0, 0)\n    \n    # Night = 2000-0000\n    nightStart = datetime.time(20, 0, 1)\n    nightEnd = datetime.time(0, 0, 0)\n\n    # Late Night = 0000-0600\n    lateStart = datetime.time(0, 0, 1)\n    lateEnd = datetime.time(6, 0, 0)\n    \n    for index,val in pickup_date.iteritems():\n        if time_in_range(mornStart, mornEnd, val.time()):\n            day_part.append(\"morning\")\n        elif time_in_range(midStart, midEnd, val.time()):\n            day_part.append(\"midday\")\n        elif time_in_range(eveStart, eveEnd, val.time()):\n            day_part.append(\"evening\")\n        elif time_in_range(nightStart, nightEnd, val.time()):\n            day_part.append(\"night\")\n        elif time_in_range(lateStart, lateEnd, val.time()):\n            day_part.append(\"lateNight\")\n\n    return day_part\n\ntrain[\"part_of_day\"] = assign_day_part(train[\"pickup_datetime\"])\n\n# We do not need time and hour variables now. So, dropping them\ntrain = train.drop([\"time\", \"hour\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3fc77c7d290f0d174f436e25f14d10a940009359"},"cell_type":"code","source":"#Since we have distance and classification of weekday now, let's see the correlation\n\ntrain.plot(kind=\"scatter\", x=\"distance\", y=\"fare_amount\", c=\"red\", alpha=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"be25b1157fc86ed2b185c90a852999744f48cf43"},"cell_type":"code","source":"# REMOVE COMMENTS\n\n# #Since we have distance and classification of weekday now, let's see the correlation\n# # We see that distance is highly related to fare amount positively. Year is also positively \n# # corelated to fare amount\n\n# print(train.corr()[\"fare_amount\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3226d0f4dc5bc24aaa3b146f4ef3f94e5257250"},"cell_type":"code","source":"# We can see negative correlation of fare amount with pickup and drop off latitude and \n# positive correlation with both longitudes. To explore that further, let's check if fare varies with \n# the direction of the trip\n\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n\n            \ntrain['diff_lon'] = train.pickup_longitude - train.dropoff_longitude\ntrain['diff_lat'] = train.pickup_latitude - train.dropoff_latitude\n\n# Select trips in Manhattan\nBB_manhattan = (-74.025, -73.925, 40.7, 40.8)\nidx_manhattan = select_within_boundingbox(train, BB_manhattan)\n\nplt.figure(figsize=(14,8))\nplt.scatter(train[idx_manhattan].diff_lon, train[idx_manhattan].diff_lat, s=0.5, alpha=1.0, \n            c=np.log1p(train[idx_manhattan].fare_amount), cmap='viridis')\nplt.colorbar()\nplt.xlabel('pickup_longitude - dropoff_longitude')\nplt.ylabel('pickup_latitude - dropoff_latidue')\nplt.title('log1p(fare_amount)');\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8a75404cec758b4c17444a14b2ffc2c08df42b3c"},"cell_type":"code","source":"# We can clearly see that direction has positive linear correlation with fare amount\n# So let's calculate exact direction of a trip, from 180 to -180 degrees. Horizontal axes = 0 degrees\n\ndef calculate_direction(d_lon, d_lat):\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180/np.pi)*np.arcsin(d_lat[d_lon>0]/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180/np.pi)*np.arcsin(d_lat[idx]/l[idx])\n    return result\n\ntrain['direction'] = calculate_direction(train.diff_lon, train.diff_lat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9ee8c7f98f17c30d42aaa02edec77d3aaf560b37"},"cell_type":"code","source":"\ntrain.plot(kind=\"scatter\", x=\"direction\", y=\"fare_amount\", c=\"blue\", alpha=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"34fb083a6b9807cbca1d3d1320a0acb9ac9a9d5f"},"cell_type":"code","source":"# Convert categorical varibale \"part_of_day\" from to numerical value using sklearn LabelBinarizer\n# This will create 5 new columns - evening, lateNight, midday, morning, night\n\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nlb_results = lb.fit_transform(train[\"part_of_day\"])\nlb_results_df = pd.DataFrame(lb_results, columns=lb.classes_)\n\ntrain = pd.merge(train, lb_results_df, left_index=True, right_index=True) #Merge output with training set\n\ntrain.drop([\"part_of_day\"], axis=1, inplace=True) # Dropping part_of_day variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd4b06071df2f9a2d082d01e73fe355436765538"},"cell_type":"code","source":"#Let us separate the response variable fare_amount before we standardize the training set\n\ntrain_labels = train[\"fare_amount\"]\ntrain.drop([\"fare_amount\"], axis=1, inplace=True)\n\n# Let us also drop pickup_datetime variable as we have included variables like year and part_of_day\ntrain_std = train.drop([\"pickup_datetime\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ff8be97d8565620f4540d9402b3073d3c5da113"},"cell_type":"code","source":"# Feature Scaling - Let's standardize our attributes so that they are in the same scales\n\nfrom sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\ntrain_std = std_scaler.fit_transform(train_std)\ntrain_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bfe40bb0312ce640a29d459e32e2f7a355a539b3"},"cell_type":"code","source":"# Create and train a Linear Regression Model\ntrain_no_std = train.drop([\"pickup_datetime\"], axis=1)\n\ntrain_prepared = train_no_std.copy()\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(train_prepared, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9c6fa80373a68607fb77a1ccda82405a1f7482f1"},"cell_type":"code","source":"#Let us test our preductions on first 3 rows\nsome_data = train_prepared.iloc[:3]\nsome_labels = train_labels.iloc[:3]\nprint(\"Predictions:\\t\", lin_reg.predict(some_data))\nprint(\"Labels\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"30ca7772c3d07ce39468c070b71bbe2f5f379c86"},"cell_type":"code","source":"# Measuring this linear regression model's RMSE on the whole training set\n\nfrom sklearn.metrics import mean_squared_error\nfare_predictions = lin_reg.predict(train_prepared)\nlin_mse = mean_squared_error(train_labels,fare_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse # We got RMSE of 4.964 which is not very good as mean of fare_amount is $11. So\n# prediction error of $5 is not acceptable","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4bde897d7c10f2c6a0acb94fb5c657d20e775bc7"},"cell_type":"code","source":"# Function to display model scores\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"36f6d9ca9258bc27e4ac97a23a8ee2c78d2a704f"},"cell_type":"code","source":"# Let's train a new DecisionTreeRegressor model and compare the rmse with earlier model\n\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(train_prepared, train_labels)\nfare_predictions_DTR = tree_reg.predict(train_prepared)\ntree_mse = mean_squared_error(train_labels,fare_predictions_DTR)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse # This gives RMSE of 0.011 which means this model is overfitting\n\n# Let's use cross-validation for the better evaluation of the model\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, train_prepared, train_labels, \n                         scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\n\n#Take a look at the DecisionTreeRegressor's score from cross-validation\n    \ndisplay_scores(rmse_scores)\n\nWe get mean of 5.6 with SD of 0.35. The variation of $5.6+-0.35 is not a very good prediction model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48d57fb8f5290eef4b0056f1516d6f670ba09f52"},"cell_type":"code","source":"# Checking score for Linear Regression model created earlier using cross-validation\nscores_lin_reg = cross_val_score(lin_reg, train_prepared, train_labels, \n                         scoring=\"neg_mean_squared_error\", cv=10)\n\nlin_reg_rmse_scores = np.sqrt(-scores_lin_reg)\n\ndisplay_scores(lin_reg_rmse_scores)\n\n#We get mean of 4.96 with sd of 0.24 which is better then DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2bb318cb7acd06616b0b44cb839fa98582bf1d04"},"cell_type":"code","source":"# Let's train a RandomForestRegressor and check how well it fits the data\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(train_prepared, train_labels)\nfare_predictions_rfg = forest_reg.predict(train_prepared)\nrfg_mse = mean_squared_error(train_labels,fare_predictions_rfg)\nrfg_rmse = np.sqrt(rfg_mse)\nrfg_rmse # RMSE = 1.83 which is quite good compared to the previous two models\n\n# Let's train this model using Cross-validation \n\nscores_rfg = cross_val_score(forest_reg, train_prepared, train_labels, \n                         scoring=\"neg_mean_squared_error\", cv=10)\n\nrfg_rmse_scores = np.sqrt(-scores_rfg)\n\ndisplay_scores(rfg_rmse_scores) # We got mean error of 4.31 with sd of 0.18, not very good but better\n                                # than linear regression model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a6c76498b78336a1bd00999e4f307f62e542de43"},"cell_type":"code","source":"# Let us fine tune the RandomForestRegressor model\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error')\ngrid_search.fit(train_prepared, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c0b4ee7be802fa9e24a84435989995fb91ad4cb"},"cell_type":"code","source":"print(\"Best parameters combination: \",grid_search.best_params_)\nprint(\"Best estimator: \",grid_search.best_estimator_)\ncvres = grid_search.cv_results_\nprint(\"Evaluation scores: \")\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n\n# We get below results:\n# Best parameters combination:  {'max_features': 4, 'n_estimators': 30}\n# Best estimator:  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n#            max_features=4, max_leaf_nodes=None, min_impurity_decrease=0.0,\n#            min_impurity_split=None, min_samples_leaf=1,\n#            min_samples_split=2, min_weight_fraction_leaf=0.0,\n#            n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n#            verbose=0, warm_start=False)\n# Evaluation scores: \n# 4.844166768045483 {'max_features': 2, 'n_estimators': 3}\n# 4.384999498554626 {'max_features': 2, 'n_estimators': 10}\n# 4.210352917368231 {'max_features': 2, 'n_estimators': 30}\n# 4.744536239308798 {'max_features': 4, 'n_estimators': 3}\n# 4.285080545281859 {'max_features': 4, 'n_estimators': 10}\n# 4.116223038942821 {'max_features': 4, 'n_estimators': 30} <--- Best solution\n# 4.686922101415325 {'max_features': 6, 'n_estimators': 3}\n# 4.278610842446552 {'max_features': 6, 'n_estimators': 10}\n# 4.139147097263857 {'max_features': 6, 'n_estimators': 30}\n# 4.6503502986524 {'max_features': 8, 'n_estimators': 3}\n# 4.270118164505793 {'max_features': 8, 'n_estimators': 10}\n# 4.145219647426304 {'max_features': 8, 'n_estimators': 30}\n# 4.776605716185325 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n# 4.3499492989752495 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n# 4.794243834697975 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n# 4.307106350189692 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n# 4.706772084360594 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n# 4.29509148366207 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c54bc9f16ba31e0295340bc488681d34f9fd34a"},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8208ade2301aa9010910e5b7559547a2d9a51df3"},"cell_type":"code","source":"# Check the relative importance of each attribute for making accurate predictions\nattributes = list(train_prepared)\nsorted(zip(feature_importances, attributes), reverse=True)\n\n# Output\n# [(0.3841124284168225, 'distance'),\n#  (0.17239442282118714, 'diff_lon'),\n#  (0.10769124244350045, 'dropoff_longitude'),\n#  (0.09648335922154312, 'diff_lat'),\n#  (0.07543540514386286, 'pickup_longitude'),\n#  (0.05130816936350101, 'pickup_latitude'),\n#  (0.045930285019949633, 'dropoff_latitude'),\n#  (0.025204936787226056, 'direction'),\n#  (0.022248352241158183, 'year'),\n#  (0.004941632952827758, 'passenger_count'),\n#  (0.0026084883883504065, 'midday'),\n#  (0.0025747338852137552, 'weekday'),\n#  (0.0024459892828226667, 'evening'),\n#  (0.0024339192827895037, 'morning'),\n#  (0.0023701585993039075, 'night'),\n#  (0.0018164761499410416, 'lateNight')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ab058587fafbb8196e0817918ff7326d56ae19eb"},"cell_type":"code","source":"# Let us try dropping some less useful features as per the output of last command\n\n# train_prepared.drop([\"direction\",\"year\",\"passenger_count\",\"midday\",\"weekday\",\"evening\",\"morning\",\n#                      \"night\",\"lateNight\"], axis=1, inplace=True)\n\nforest_reg2 = RandomForestRegressor()\nforest_reg2.fit(train_prepared, train_labels)\nfare_predictions_rfg2 = forest_reg2.predict(train_prepared)\nrfg2_mse = mean_squared_error(train_labels,fare_predictions_rfg2)\nrfg2_rmse = np.sqrt(rfg2_mse)\nrfg2_rmse\n\nscores_rfg2 = cross_val_score(forest_reg2, train_prepared, train_labels, \n                         scoring=\"neg_mean_squared_error\", cv=10)\n\nrfg2_rmse_scores = np.sqrt(-scores_rfg2)\n\ndisplay_scores(rfg2_rmse_scores)\n\ntrain_prepared = train_no_std.copy() # Adding removed features back\n\n# We get below scores after removing some features which is bad compared to earlier mean error of 4.3\n# So we will go back to adding the removed features\n\n# Scores: [4.09936639 4.53226684 4.5280771  4.60757191 4.69303811 4.67236903\n#  4.97136344 4.31146722 4.52617438 4.58548337]\n# Mean: 4.55271777832126\n# Standard deviation: 0.21881778766341559","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e1975af0fd0dad537042d5a908a164f1e5f843f"},"cell_type":"code","source":"# Let's test our module on the given test data\n\n# Read the test data\n\ntest = load_ny_taxi_data(\"test\")\n\n#Perform transformations on the test data\n\ntest[\"distance\"] = 1.1\n\n#Changing datatype of train.Distance from float64 to float16\ntest[\"distance\"] = test[\"distance\"].astype(np.float16)\n\n#Calculating distance between pickup and drop points using Haversine formula\ntest[\"distance\"] = distance(test.pickup_latitude.tolist(), test.pickup_longitude.tolist(),\n                             test.dropoff_latitude.tolist(), test.dropoff_longitude.tolist())\n\ntest[\"pickup_datetime\"] = pd.to_datetime(test[\"pickup_datetime\"])\ntest[\"year\"] = test[\"pickup_datetime\"].dt.year\ntest[\"time\"] = test[\"pickup_datetime\"].dt.time\ntest[\"hour\"] = test[\"pickup_datetime\"].dt.hour\n\ntest[\"weekday\"] = 0\ntest[\"weekday\"] = weekday(test[\"pickup_datetime\"])\n\ntest[\"part_of_day\"] = assign_day_part(test[\"pickup_datetime\"])\n\n# We do not need time and hour variables now. So, dropping them\ntest = test.drop([\"time\", \"hour\"], axis=1)\n\ntest['diff_lon'] = test.pickup_longitude - test.dropoff_longitude\ntest['diff_lat'] = test.pickup_latitude - test.dropoff_latitude\n\n#Add variable \"direction\"\ntest['direction'] = calculate_direction(test.diff_lon, test.diff_lat)\n\ntest.drop([\"pickup_datetime\"], axis=1, inplace=True)\n\nlb_results = lb.fit_transform(test[\"part_of_day\"])\nlb_results_df = pd.DataFrame(lb_results, columns=lb.classes_)\n\ntest = pd.merge(test, lb_results_df, left_index=True, right_index=True) #Merge output with testing set\n\ntest.drop([\"part_of_day\"], axis=1, inplace=True)\n\n# Removing extra \"key\" column in test data and storing it for later merging\ntest_keys = test[\"key\"]\ntest.drop([\"key\"], axis=1, inplace=True)\n\ntest_keys = test_keys.to_frame()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3d2929b5e3a6e08d9c711b7f56d0b31391652977"},"cell_type":"code","source":"# Since training and test set have same variables now, we can run test data \n# through our model to predict the fare\n\ntest_fare_predictions = forest_reg.predict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20070168f869759d530b8b6e47d74f9cd3f7ab38"},"cell_type":"code","source":"type(test_fare_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"72c13c2323edd339bffa68ce5a52b34f2678c1d2","_kg_hide-output":false},"cell_type":"code","source":"# Merge test data keys and predictions for kaggle submission\n\ntest_keys[\"fare_amount\"] = 0\n\nfor i in range(len(test_keys)):\n    test_keys.loc[i,\"fare_amount\"] = test_fare_predictions[i]\n    \ntest_keys = test_keys.round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5849222ae7480702950f734a6f83016b8558e700","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# This dataframe test_keys contains our final submission data. \n\ntest_keys.to_csv(\"../output/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}