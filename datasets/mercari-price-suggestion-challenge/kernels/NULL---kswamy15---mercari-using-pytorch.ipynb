{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"version": "3.6.3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["### This is my first competition entry to Kaggle.  The Deep learning model has been developed using Pytorch\n", "The exploration and data transformation has been heavily borrowed from Noobhound's 'A simple nn solution with Keras (~0.48611 PL)' and ThyKhueLy 'Mercari Interactive EDA + Topic Modelling' Kernels.  All the Pytorch model is mine.  \n", "#### I am not having success with the deep learning model - very poor validation performance.  I have tried different types of convolution types on the item_description and name embeddings.  Any pointers to a better Deep learning model from the forum would be helpful.  Not sure If I am missing something basic in my models.  \n", "\n"], "metadata": {"_cell_guid": "82f39a30-ee1e-45b2-9459-60a125482583", "_uuid": "7ce08ead6d501f7b5656ac0539e13caad5e2328c"}}, {"cell_type": "code", "execution_count": null, "source": ["import os\n", "os.environ['OMP_NUM_THREADS'] = '4'"], "metadata": {"collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# used for developing deep learning models\n", "import torch\n", "from torch.autograd import Variable\n", "from torch import optim\n", "#from torch.optim import lr_scheduler\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "from torchvision import transforms, utils\n", "import torch.nn.functional as F"], "metadata": {"_cell_guid": "9e37e074-c8fc-4500-9e1d-3b8431508592", "_uuid": "1f0ccc272d903402971028ae495a328a7a2621a8", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "#from subprocess import check_output\n", "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "import time          #to get the system time"], "metadata": {"_cell_guid": "a59bdcf6-6b8d-4770-937f-7f28313689c1", "_uuid": "4e940299821bd76c4e282c7df63a2d852eb24e3e", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["import matplotlib.pyplot as plt\n", "import matplotlib.gridspec as gridspec\n", "import seaborn as sns\n", "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "import math"], "metadata": {"_cell_guid": "bac395c4-c9ee-4c24-8b5f-031b71e71a51", "_uuid": "ec4449b28f2620166dda32548c54cf83e3e99716", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train = pd.read_csv('../input/train.tsv', sep='\\t')\n", "test = pd.read_csv('../input/test.tsv', sep='\\t')"], "metadata": {"_cell_guid": "8ae75f70-24e5-4b3e-93ee-83d11c66933b", "_uuid": "27bb165573488b2f5bc280e229b3fd28a1694182", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.describe()"], "metadata": {"_cell_guid": "7dc0f6c8-14f2-40b7-a506-a69ea324341c", "_uuid": "4bfb741d3b8426bcc6648dedc5496e4d5732e981"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["test.describe()"], "metadata": {"_cell_guid": "f4e176c5-8af2-481c-a633-6e430def2f9d", "_uuid": "e1693a2746f6188b863fc1f2e27570fecf6aa448"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.head()"], "metadata": {"_cell_guid": "125f3ed9-c986-4148-a958-9dcfd20c0ceb", "_uuid": "e6b9688f567fee71014101a14765060cf458594c", "scrolled": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.isnull().sum()"], "metadata": {"_cell_guid": "bcbd53d8-460c-42e7-ac2c-a51728dca4a5", "_uuid": "cf8c363782f41ee8e0ead993dc829ad34ed633a3"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["test.isnull().sum()"], "metadata": {"_cell_guid": "5c70df20-2b39-424f-bcb8-01fdd8e09e31", "_uuid": "6f2361484c52f7ad6b28b849a0c80c32715b4785"}, "outputs": []}, {"cell_type": "markdown", "source": ["## Item Category"], "metadata": {"_cell_guid": "01e6a625-445d-4fc1-8e95-db2e5d68510b", "_uuid": "de8044486fe39eca54dd442887d8ce9e001abc83"}}, {"cell_type": "code", "execution_count": null, "source": ["print(\"There are %d unique values in the category column.\" % train['category_name'].nunique())\n", "# TOP 5 RAW CATEGORIES\n", "print(train['category_name'].value_counts()[:5])\n", "# missing categories\n", "print(\"There are %d items that do not have a label.\" % train['category_name'].isnull().sum())"], "metadata": {"_cell_guid": "16ccb481-3021-4e5a-8280-a5f86a75b392", "_uuid": "a363310e785f898dcc8ae2fba409287c37cf4fe7"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\n", "def split_cat(text):\n", "    try: return text.split(\"/\")\n", "    except: return (\"None\", \"None\", \"None\")"], "metadata": {"_cell_guid": "1ecf8884-7cd0-4c62-9242-f4864d726736", "_uuid": "82a15cb8c0919cffc7b0f49b054775980a9b3d4d", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train['general_cat'], train['subcat_1'], train['subcat_2'] = \\\n", "zip(*train['category_name'].apply(lambda x: split_cat(x)))\n", "train.head()\n", "test['general_cat'], test['subcat_1'], test['subcat_2'] = \\\n", "zip(*test['category_name'].apply(lambda x: split_cat(x)))\n", "test.head()"], "metadata": {"_cell_guid": "d9dced68-7725-4122-b9b4-aef302703761", "_uuid": "b9dad4c414541686e620c5bd19d7f326d68babe5"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["print(\"There are %d unique first sub-categories.\" % train['general_cat'].nunique())\n", "print(\"There are %d unique first sub-categories.\" % train['subcat_1'].nunique())\n", "print(\"There are %d unique second sub-categories.\" % train['subcat_2'].nunique())"], "metadata": {"_cell_guid": "35de890c-b897-434b-8e9b-4915a2269487", "_uuid": "6f1180aa574cd367c5ed5b7107f45f528587c5a6"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["### Plotting some histograms of categorical Variables\n", "plt.figure(figsize=(10,10))\n", "plt.subplot(3,3,1)\n", "count_classes_general_cat = pd.value_counts(train.general_cat, sort = True)\n", "count_classes_general_cat.plot(kind = 'bar')\n", "plt.title(\"General Category histogram\")\n", "plt.xlabel(\"Class\")\n", "plt.ylabel(\"Frequency\")\n", "# subcategory 1\n", "plt.subplot(3,3,3)\n", "count_classes_subcat_1 = pd.value_counts(train.subcat_1, sort = True)[:15]\n", "count_classes_subcat_1.plot(kind = 'bar')\n", "plt.title(\"Sub Category 1 histogram\")\n", "plt.xlabel(\"Class\")\n", "plt.ylabel(\"Frequency\")\n", "# subcategory 2\n", "plt.subplot(3,3,9)\n", "count_classes_subcat_2 = pd.value_counts(train.subcat_2, sort = True)[:15]\n", "count_classes_subcat_2.plot(kind = 'bar')\n", "plt.title(\"Sub Category 2 histogram\")\n", "plt.xlabel(\"Class\")\n", "plt.ylabel(\"Frequency\")"], "metadata": {"_cell_guid": "f3e15f03-a995-4141-91b8-a5e6ccdbacb7", "_uuid": "a545faa28091ca5c51e5976a3185f9350b3884cf"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["print(\"There are %d unique brand names in the training dataset.\" % train['brand_name'].nunique())"], "metadata": {"_cell_guid": "4b47b7df-8dcb-4a0d-b152-e37825ead776", "_uuid": "3c83bf0fd305cbf3e192cfd2505b53f0e55ad0a7"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["fig, ax = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\n", "sns.distplot(np.log(train['price'].values+1))"], "metadata": {"_cell_guid": "22eb387f-1cf7-4f50-94cf-a99264fa3d90", "_uuid": "acf4daebd05f74393da5fcac5e9dd8c5c2916132"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["def rmsle(y, y_pred):\n", "    assert len(y) == len(y_pred)\n", "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n", "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n", "#Source: https://www.kaggle.com/marknagelberg/rmsle-function"], "metadata": {"_cell_guid": "72815e21-754f-4f24-ac2e-23ecf9570c20", "_uuid": "8d952c094e926299c303f2e63c8f5d055eed67e6", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#HANDLE MISSING VALUES\n", "print(\"Handling missing values...\")\n", "def handle_missing(dataset):\n", "    #dataset.category_name.fillna(value=\"na\", inplace=True)\n", "    dataset.brand_name.fillna(value=\"None\", inplace=True)\n", "    dataset.item_description.fillna(value=\"None\", inplace=True)\n", "    dataset.category_name.fillna(value=\"None\", inplace=True)\n", "    return (dataset)\n", "\n", "train = handle_missing(train)\n", "test = handle_missing(test)\n", "print(train.shape)\n", "print(test.shape)"], "metadata": {"_cell_guid": "1f2c32c8-5afc-430e-a4fe-51e93e898158", "_uuid": "71934ea38805cd853fad55881094aebe3f6a0e86"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.isnull().sum()\n", "# Not to worry about the nulls in Category name as the nulls have been taken care of earlier when splitting the\n", "# category name into general, sub cat1 and sub cat2"], "metadata": {"_cell_guid": "badd29b8-147b-454d-a183-b6a028f5ac16", "_uuid": "7516e1ff4712c7f097fe97b0d6f190bf976077a4", "scrolled": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#PROCESS CATEGORICAL DATA\n", "#print(\"Handling categorical variables...\")\n", "def encode_text(column):\n", "    le = LabelEncoder()\n", "    le.fit(np.hstack([train[column], test[column]]))\n", "    train[column+'_index'] = le.transform(train[column])\n", "    test[column+'_index'] = le.transform(test[column])"], "metadata": {"_cell_guid": "c87fd9ee-73a5-4abe-956b-500dd17b0eaf", "_uuid": "676a061137ef0205812ea5ce3d6cc83fb6f28942", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["encode_text('brand_name')\n", "encode_text('general_cat')\n", "encode_text(\"subcat_1\")\n", "encode_text('subcat_2')\n", "encode_text('category_name')"], "metadata": {"_cell_guid": "9ff4177a-5e91-48fc-bba7-f494acd93cfe", "_uuid": "813d2be5c578677e5c40803af37b15a5f46d48d7", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["test.head()"], "metadata": {"_cell_guid": "67b8e6a9-fd24-4a32-afd7-4b9ffd6eb25b", "_uuid": "af8d4f70330e67e1c47d92ec41dbea1734c47361", "scrolled": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["class Category:\n", "    def __init__(self, name):\n", "        self.name = name\n", "        self.word2index = {}\n", "        self.word2count = {}\n", "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n", "        self.n_words = 2  # Count SOS and EOS\n", "\n", "    def addSentence(self, sentence):\n", "        for word in sentence.split(' '):\n", "            self.addWord(word)\n", "\n", "    def addWord(self, word):\n", "        if word not in self.word2index:\n", "            self.word2index[word] = self.n_words\n", "            self.word2count[word] = 1\n", "            self.index2word[self.n_words] = word\n", "            self.n_words += 1\n", "        else:\n", "            self.word2count[word] += 1"], "metadata": {"_cell_guid": "964a227b-4879-4bf6-9c03-bb4168a6ad61", "_uuid": "79a493df9aed6da3c22e690eecea1b1e7e0fe8f8", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Turn a Unicode string to plain ASCII, thanks to\n", "# http://stackoverflow.com/a/518232/2809427\n", "import unicodedata\n", "import re\n", "def unicodeToAscii(s):\n", "    return ''.join(\n", "        c for c in unicodedata.normalize('NFD', s)\n", "        if unicodedata.category(c) != 'Mn'\n", "    )\n", "\n", "# Lowercase, trim, and remove non-letter characters\n", "\n", "\n", "def normalizeString(s):\n", "    #s = unicodeToAscii(s.lower().strip())\n", "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n", "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n", "    return s\n", "\n", "def normalizeLine(sentence):\n", "    return [normalizeString(s) for s in sentence.split('\\t')]"], "metadata": {"_cell_guid": "3942f2c2-bf5d-48d0-b3ed-fb9126d11862", "_uuid": "41a41167b6ce0cbba4a1b0fd72d1d7798bd004cf", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["def prepareData(lang1,data):\n", "    input_cat = Category(lang1)\n", "    for sentence in data:\n", "        normalize_line = [normalizeString(s) for s in sentence.split('\\t')]\n", "        input_cat.addSentence(normalize_line[0])\n", "        \n", "    print(\"Counted words:\")\n", "    print(input_cat.name, input_cat.n_words)\n", "    return input_cat"], "metadata": {"_cell_guid": "505bc246-4ba7-42e9-9b4d-44664774d6b9", "_uuid": "53375cd0d74ab9fa3f5e9a2a5af6ca1804cdf43d", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["def indexesFromSentence(lang, sentence):\n", "    return [lang.word2index[word] for word in sentence.split(' ')]\n", "\n", "def variableFromSentence(lang, sentence):\n", "    indexes = indexesFromSentence(lang, sentence)\n", "    #indexes.append(EOS_token)\n", "    return indexes"], "metadata": {"_cell_guid": "2ee7605c-3330-449a-9bc7-dd5cf17c7e2b", "_uuid": "c6cf3bfdb1159184d08eb4a9e1829c036838344a", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["def token_fit(column):\n", "    raw_text = np.hstack([(train[column]).str.lower(), (test[column]).str.lower()])\n", "    cat1 = prepareData(column,raw_text)\n", "    print (\"adding train data\")\n", "    train[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) \\\n", "                                                      for sentence in train[column]]\n", "    print (\"adding test data\")\n", "    test[column + '_seq'] = [variableFromSentence(cat1,normalizeLine(sentence.lower())[0]) \\\n", "                                                      for sentence in test[column]]"], "metadata": {"_cell_guid": "58ecfbdd-e227-44d7-a514-06c4a999ae38", "_uuid": "a664d2d36dbba147b59bd88961dd8f5f37145372", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["token_fit('name')"], "metadata": {"_cell_guid": "9713321d-1025-4c93-9b3f-082c9b176658", "_uuid": "34eac7d199502e3ab2f1fe5a595fad8ac34df86f"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["token_fit('item_description')"], "metadata": {"_cell_guid": "f85d9b87-622b-4e94-b161-6be11446f23d", "_uuid": "0c62a18f670b45d60ef08322d6e288ead93beca4"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.head()"], "metadata": {"_cell_guid": "03d71534-c3c1-4deb-ac62-f2c5fef8a310", "_uuid": "6e0e13c069c3a0b3410e78073026de21221af8e4"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#SEQUENCES VARIABLES ANALYSIS\n", "max_name_seq = np.max([np.max(train.name_seq.apply(lambda x: len(x))), np.max(test.name_seq.apply(lambda x: len(x)))])\n", "max_item_description_seq = np.max([np.max(train.item_description_seq.apply(lambda x: len(x)))\n", "                                   , np.max(test.item_description_seq.apply(lambda x: len(x)))])\n", "print(\"max name seq \"+str(max_name_seq))\n", "print(\"max item desc seq \"+str(max_item_description_seq))"], "metadata": {"_cell_guid": "0a176cff-2d54-4edd-a249-e43f2d3e150b", "_uuid": "d886ad29ef02387cabaad8ee0bf17a4f6f92256a"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#EMBEDDINGS MAX VALUE\n", "#Base on the histograms, we select the next lengths\n", "MAX_NAME_SEQ = 10\n", "MAX_ITEM_DESC_SEQ = 75\n", "MAX_TEXT = np.max([np.max(train.name_seq.max()) \n", "                   , np.max(test.name_seq.max())\n", "                  , np.max(train.item_description_seq.max())\n", "                  , np.max(test.item_description_seq.max())])+2\n", "MAX_GEN_CATEGORY = np.max([train.general_cat_index.max(), test.general_cat_index.max()])+1\n", "MAX_SUB_CAT1_CATEGORY = np.max([train.subcat_1_index.max(), test.subcat_1_index.max()])+1\n", "MAX_SUB_CAT2_CATEGORY = np.max([train.subcat_2_index.max(), test.subcat_2_index.max()])+1\n", "MAX_BRAND = np.max([train.brand_name_index.max(), test.brand_name_index.max()])+1\n", "MAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1\n", "MAX_CATEGORY_NAME = np.max([train.category_name_index.max(), test.category_name_index.max()])+1"], "metadata": {"_cell_guid": "e76573c0-5ace-4d97-994a-76c17662d86a", "_uuid": "0ad1c53d6c119e90076c90c551c642f7ef9ea711", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["MAX_BRAND"], "metadata": {"_cell_guid": "3dce0552-9087-4296-b0a7-157741ddee0e", "_uuid": "423232dce1bfd9681fa7ae4c39a7eb909459ce8d"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#SCALE target variable\n", "train[\"target\"] = np.log(train.price+1)\n", "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n", "train[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\n", "pd.DataFrame(train.target).hist()"], "metadata": {"_cell_guid": "f223fa02-4ece-460f-9f37-9730e3d92b82", "_uuid": "4330e64622eed4c594faecc52f45747adb084a8d"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["print (target_scaler.scale_,target_scaler.min_,target_scaler.data_min_,target_scaler.data_max_)"], "metadata": {"_cell_guid": "4154b237-fe8a-4a80-99af-e08965274bc0", "_uuid": "f550174fb85fbe0ced6dd0bd9c514f249d53a11b"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["train.head()"], "metadata": {"_cell_guid": "21a0a9b4-7c75-4a82-b600-ea0badef165d", "_uuid": "ecaf6a5f895f39db5b1680b1e75411ed814667a4"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["#EXTRACT DEVELOPTMENT TEST\n", "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\n", "print(dtrain.shape)\n", "print(dvalid.shape)"], "metadata": {"_cell_guid": "01a3c80e-75f7-4f61-9f83-eb6b2db67544", "_uuid": "8ba1292583a7304ef2bf7475bae824367e7431f6"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["def pad(tensor, length):\n", "    if length > tensor.size(0):\n", "        return torch.cat([tensor, tensor.new(length - tensor.size(0), *tensor.size()[1:]).zero_()])\n", "    else:\n", "        return torch.split(tensor, length, dim=0)[0]"], "metadata": {"_cell_guid": "e1a88f0c-fe31-4018-933d-ce9de7d9dabc", "_uuid": "420b0fa8befc77e4dd957e9287fba8c58adbec36", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["class ToTensor(object):\n", "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n", "\n", "    def __call__(self, sample):\n", "        name, item_desc,brand_name,cat_name,general_category,subcat1_category,subcat2_category, \\\n", "        item_condition,shipping,target = sample['name'], sample['item_desc'], sample['brand_name'], \\\n", "        sample['cat_name'], sample['general_category'], sample['subcat1_category'], sample['subcat2_category'], \\\n", "        sample['item_condition'], sample['shipping'],sample['target']\n", "        #item_desc, brand_name = sample['item_desc'], sample['brand_name']       \n", "        return {'name': pad(torch.from_numpy(np.asarray(name)).long().view(-1),MAX_NAME_SEQ),\n", "                'item_desc': pad(torch.from_numpy(np.asarray(item_desc)).long().view(-1),MAX_ITEM_DESC_SEQ),\n", "               'brand_name':torch.from_numpy(np.asarray(brand_name)),\n", "               'cat_name':torch.from_numpy(np.asarray(cat_name)),\n", "               'general_category':torch.from_numpy(np.asarray(general_category)),\n", "               'subcat1_category':torch.from_numpy(np.asarray(subcat1_category)),\n", "               'subcat2_category':torch.from_numpy(np.asarray(subcat2_category)),\n", "               'item_condition':torch.from_numpy(np.asarray(item_condition)),\n", "               'shipping':torch.torch.from_numpy(np.asarray(shipping)),\n", "               'target':torch.from_numpy(np.asarray(target))}"], "metadata": {"_cell_guid": "98ad9abe-1016-495b-9f27-d1755dcc9071", "_uuid": "3e2e1bd7684e392d0ed6a688a668bc1ab7249d4b", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Define the Dataset to use in a DataLoader\n", "class MercariDataset(Dataset):\n", "    \"\"\"Mercari Challenge dataset.\"\"\"\n", "\n", "    def __init__(self, data_pd, transform=None):\n", "        \"\"\"\n", "        Args:\n", "            data_pd: Data frame with the used columns.\n", "            transform (callable, optional): Optional transform to be applied\n", "                on a sample.\n", "        \"\"\"\n", "        self.mercari_frame = data_pd\n", "        self.transform = transform\n", "\n", "    def __len__(self):\n", "        return len(self.mercari_frame)\n", "\n", "    def __getitem__(self, idx):\n", "        name = [self.mercari_frame.name_seq.iloc[idx]]\n", "        item_desc = [self.mercari_frame.item_description_seq.iloc[idx]]\n", "        brand_name = [self.mercari_frame.brand_name_index.iloc[idx]]\n", "        cat_name = [self.mercari_frame.category_name_index.iloc[idx]]\n", "        general_category = [self.mercari_frame.general_cat_index.iloc[idx]]\n", "        subcat1_category = [self.mercari_frame.subcat_1_index.iloc[idx]]\n", "        subcat2_category = [self.mercari_frame.subcat_2_index.iloc[idx]]\n", "        item_condition = [self.mercari_frame.item_condition_id.iloc[idx]]\n", "        shipping = [self.mercari_frame.shipping.iloc[idx]]\n", "        target = [self.mercari_frame.target.iloc[idx]]\n", "        sample = {'name': name,\n", "                'item_desc': item_desc,\n", "               'brand_name': brand_name,\n", "               'cat_name': cat_name,   \n", "               'general_category': general_category,\n", "               'subcat1_category': subcat1_category,\n", "               'subcat2_category': subcat2_category,\n", "               'item_condition': item_condition,\n", "               'shipping': shipping,\n", "               'target': target}\n", "\n", "        if self.transform:\n", "            sample = self.transform(sample)\n", "\n", "        return sample"], "metadata": {"_cell_guid": "efda5415-835a-4907-be1b-64bead9a6a37", "_uuid": "8c0c28606e2812999bacadca20a8bd62d420828e", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["mercari_datasets = {'train': MercariDataset(dtrain,transform=transforms.Compose([ToTensor()])), \n", "                    'val': MercariDataset(dvalid,transform=transforms.Compose([ToTensor()]))\n", "                   }\n", "dataset_sizes = {x: len(mercari_datasets[x]) for x in ['train', 'val']}"], "metadata": {"_cell_guid": "e922eca9-965f-4b6a-b972-310ced15dd5c", "_uuid": "aeb7ccf983c4662bbfd0da85a6e712d4176219b4", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["mercari_dataloaders = {x: torch.utils.data.DataLoader(mercari_datasets[x], batch_size=50, shuffle=True) \n", "                                                           for x in ['train', 'val']}"], "metadata": {"_cell_guid": "ae44ba30-5227-4da1-b47f-58a8edf5df6a", "_uuid": "293dc2f35a85f2d51faac9b3f00fccf8c2d0f9bb", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["mercari_dataloaders"], "metadata": {"_cell_guid": "d4bb792c-e93d-47e4-a48f-2566a5ea2d86", "_uuid": "f8792fa7717d3677d21dbd5fd87ec24678503be2"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Some Useful Time functions\n", "def asMinutes(s):\n", "    m = math.floor(s / 60)\n", "    s -= m * 60\n", "    return '%dm %ds' % (m, s)\n", "\n", "\n", "def timeSince(since, percent):\n", "    now = time.time()\n", "    s = now - since\n", "    es = s / (percent)\n", "    rs = es - s\n", "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"], "metadata": {"_cell_guid": "7a4746bd-31cd-42ec-b2a1-409ab45b329d", "_uuid": "64e05c17ff41c7caceca0a9ca4416d0a59bd032d", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Definition of the Pytorch Model\n", "class RegressionNeural(nn.Module):\n", "    def __init__(self, max_sizes):\n", "        super(RegressionNeural, self).__init__()\n", "        # declaring all the embedding for the various items\n", "        self.name_embedding = nn.Embedding(np.asscalar(max_sizes['max_text']), 50)\n", "        self.item_embedding = nn.Embedding(np.asscalar(max_sizes['max_text']), 50)\n", "        self.brand_embedding = nn.Embedding(np.asscalar(max_sizes['max_brand']), 10)\n", "        self.gencat_embedding = nn.Embedding(np.asscalar(max_sizes['max_gen_category']), 10)\n", "        self.subcat1_embedding = nn.Embedding(np.asscalar(max_sizes['max_subcat1_category']), 10)\n", "        self.subcat2_embedding = nn.Embedding(np.asscalar(max_sizes['max_subcat2_category']), 10)\n", "        self.condition_embedding = nn.Embedding(np.asscalar(max_sizes['max_condition']), 5)\n", "        # I am adding an embedding just based on Category name without separating it into the 3 pieces\n", "        self.catname_embedding = nn.Embedding(np.asscalar(max_sizes['max_cat_name']), 10)\n", "        \n", "        ## I am trying to throw all types of convolutional model on the name and item embedding and haven't\n", "        ## had any luck.  \n", "        #self.conv1_name = nn.Conv1d(max_sizes['max_name_seq'], 1, 3, stride=1)\n", "        #self.conv1_item_desc = nn.Conv1d(max_sizes['max_item_desc_seq'], 1, 5, stride=5) \n", "        \n", "        self.conv1_name = nn.Conv1d(50, 1, 2, stride=1)\n", "        # I am not using these other convolutions as they didn't seem to improve my result\n", "        self.conv2_name = nn.Conv1d(16, 8, 2, stride=1)\n", "        self.conv3_name = nn.Conv1d(8, 4, 2, stride=1)\n", "        \n", "        self.conv1_item_desc = nn.Conv1d(50, 1, 5, stride=5) \n", "        # I am not using these other convolutions as they didn't see to improve my result\n", "        self.conv2_item_desc = nn.Conv1d(64, 16, 5, stride=1)\n", "        self.conv3_item_desc = nn.Conv1d(16, 4, 5, stride=1)\n", "        \n", "        #self.conv1 = nn.Conv1d(64, 32, 3, stride=1)\n", "        #self.conv2 = nn.Conv1d(32, 16, 3, stride=1)\n", "        self.dropout = nn.Dropout(p=0.2)\n", "        \n", "        self.input_fc1_count = 50 #1214 #206 #16+10+10+10+10+5+1\n", "        self.fc1 = nn.Linear(self.input_fc1_count, 64)\n", "        self.fc2 = nn.Linear(64,32)\n", "        self.fc3 = nn.Linear(32,1)\n", "        \n", "        self.relu = nn.ReLU()  \n", "            \n", "    def forward(self, x, batchsize):\n", "        embed_name = self.name_embedding(x['name'])\n", "        #print (\"embed_name size\",embed_name.size())\n", "        #embed_name = (self.conv1_name(embed_name))\n", "        \n", "        # I am swapping the Embedding size and the sequence length so that convolution is done across multiple words\n", "        # using all the embeddings.  Without this, the 1-D convolution was doing convolution using all the words but\n", "        # a slice of embeddings.  I don't think that is the correct way to do 1D convolution.  \n", "        embed_name = F.relu(self.conv1_name(embed_name.transpose(1,2)))\n", "        #print (\"embed_name after 1st conv\",embed_name.size())\n", "        #embed_name = F.relu(self.conv2_name(embed_name))\n", "        #print (\"embed_name after 2nd conv\",embed_name.size())\n", "        #embed_name = self.conv3_name(embed_name)\n", "        #print (\"embed_name after 3rd conv\",embed_name.size())\n", "        \n", "        embed_item = self.item_embedding(x['item_desc'])\n", "        #print (\"embed_item size\",embed_item.size())\n", "        #embed_item = (self.conv1_item_desc(embed_item))\n", "        embed_item = F.relu(self.conv1_item_desc(embed_item.transpose(1,2)))\n", "        #print (\"embed_item after 1 conv\",embed_item.size())\n", "        #embed_item = F.relu(self.conv2_item_desc(embed_item))\n", "        #print (\"embed_item after 2 conv\",embed_item.size())\n", "        #embed_item = self.conv3_item_desc(embed_item)\n", "        #print (\"embed_item after 3rd conv\",embed_item.size())\n", "        \n", "        embed_brand = self.brand_embedding(x['brand_name'])\n", "        embed_gencat = self.gencat_embedding(x['general_category'])\n", "        embed_subcat1 = self.subcat1_embedding(x['subcat1_category'])\n", "        embed_subcat2 = self.subcat2_embedding(x['subcat2_category'])\n", "        embed_condition = self.condition_embedding(x['item_condition'])\n", "        embed_catname = self.catname_embedding(x['cat_name'])\n", "        \n", "        #out = torch.cat((embed_brand.view(batchsize,-1),embed_gencat.view(batchsize,-1), \\\n", "        #                 embed_subcat1.view(batchsize,-1), embed_subcat2.view(batchsize,-1), \\\n", "        #                 embed_condition.view(batchsize,-1),embed_name.view(batchsize,-1), \\\n", "        #                 embed_item.view(batchsize,-1),x['shipping']),1)\n", "        out = torch.cat((embed_brand.view(batchsize,-1), embed_catname.view(batchsize,-1), \\\n", "                         embed_condition.view(batchsize,-1),embed_name.view(batchsize,-1), \\\n", "                         embed_item.view(batchsize,-1),x['shipping']),1)\n", "        #out = self.dropout(out)\n", "        \n", "        out = (self.fc1(out))\n", "        out = F.relu(self.dropout(out))\n", "        out = (self.fc2(out))\n", "        out = (self.dropout(out))\n", "        out = self.fc3(out)\n", "        return out\n", "\n", "max_sizes = {'max_text':MAX_TEXT,'max_name_seq':MAX_NAME_SEQ,'max_item_desc_seq':MAX_ITEM_DESC_SEQ, \\\n", "             'max_brand':MAX_BRAND,'max_cat_name':MAX_CATEGORY_NAME,'max_gen_category':MAX_GEN_CATEGORY,\\\n", "             'max_subcat1_category':MAX_SUB_CAT1_CATEGORY,'max_subcat2_category':MAX_SUB_CAT2_CATEGORY,\\\n", "             'max_condition':MAX_CONDITION} \n", "\n", "deep_learn_model = RegressionNeural(max_sizes)"], "metadata": {"_cell_guid": "78b43262-eaef-495b-8151-76edf3005257", "_uuid": "a26cb52ca3564583b02b357439300a1a0080be0a", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Training model function that uses the dataloader to load the data by Batch\n", "def train_model(model, criterion, optimizer, num_epochs=1, print_every = 100):\n", "    start = time.time()\n", "\n", "    best_acc = 0.0\n", "    print_loss_total = 0  # Reset every print_every\n", "\n", "    for epoch in range(num_epochs):\n", "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n", "        print('-' * 10)\n", "\n", "        # Each epoch has a training and validation phase\n", "        for phase in ['train', 'val']:\n", "            if phase == 'train':\n", "                #scheduler.step()\n", "                model.train(True)  # Set model to training mode\n", "            else:\n", "                model.train(False)  # Set model to evaluate mode\n", "\n", "            running_loss = 0.0\n", "            num_batches = dataset_sizes[phase]/50.\n", "            #running_corrects = 0\n", "\n", "            # Iterate over data.\n", "            for i_batch, sample_batched in enumerate(mercari_dataloaders[phase]): \n", "            # get the inputs\n", "                inputs = {'name':Variable(sample_batched['name']), 'item_desc':Variable(sample_batched['item_desc']), \\\n", "                    'brand_name':Variable(sample_batched['brand_name']), \\\n", "                    'cat_name':Variable(sample_batched['cat_name']), \\\n", "                    'general_category':Variable(sample_batched['general_category']), \\\n", "                    'subcat1_category':Variable(sample_batched['subcat1_category']), \\\n", "                    'subcat2_category':Variable(sample_batched['subcat2_category']), \\\n", "                    'item_condition':Variable(sample_batched['item_condition']), \\\n", "                    'shipping':Variable(sample_batched['shipping'].float())}\n", "                prices = Variable(sample_batched['target'].float())   \n", "                batch_size = len(sample_batched['shipping'])   \n", "                \n", "\n", "                # zero the parameter gradients\n", "                optimizer.zero_grad()\n", "\n", "                # forward\n", "                outputs = model(inputs, batch_size)\n", "                #_, preds = torch.max(outputs.data, 1)\n", "                loss = criterion(outputs, prices)\n", "\n", "                # backward + optimize only if in training phase\n", "                if phase == 'train':\n", "                    loss.backward()\n", "                    optimizer.step()\n", "\n", "                # statistics\n", "                running_loss += loss.data[0]\n", "                print_loss_total += loss.data[0]\n", "                #running_corrects += torch.sum(preds == labels.data)\n", "                \n", "                \n", "                if (i_batch+1) % print_every == 0:\n", "                    print_loss_avg = print_loss_total / print_every\n", "                    print_loss_total = 0\n", "                    #print (i_batch / num_batches, i_batch, num_batches)\n", "                    print('%s (%d %d%%) %.4f' % (timeSince(start, i_batch / num_batches), \\\n", "                                                 i_batch, i_batch / num_batches*100, print_loss_avg))\n", "                \n", "                # I have put this just so that the Kernel will run and allow me to publish\n", "                if (i_batch) > 500:\n", "                    break\n", "\n", "            epoch_loss = running_loss / num_batches\n", "            #epoch_acc = running_corrects / dataset_sizes[phase]\n", "\n", "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n", "            \n", "        print()\n", "\n", "    time_elapsed = time.time() - start\n", "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n", "    # load best model weights\n", "    #model.load_state_dict(best_model_wts)\n", "    return model                         "], "metadata": {"_cell_guid": "d2177546-089a-4418-a712-b83424877405", "_uuid": "a0cb2930aa67b9fb3da2be53258b8dd45c120f72", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Set the optimizer Criterion and train the model\n", "criterion = nn.MSELoss()\n", "\n", "optimizer_ft = optim.SGD(deep_learn_model.parameters(), lr=0.001, momentum=0.9)\n", "#optimizer_ft = optim.SGD(deep_learn_model.parameters(), lr=0.005)\n", "\n", "# Decay LR by a factor of 0.1 every 7 epochs\n", "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n", "train_model(deep_learn_model,criterion,optimizer_ft)\n", "# I have run the model a lot of times with different combination of deep learning configs and I am not able to\n", "# get a loss below 0.0341.  "], "metadata": {"_cell_guid": "9c8a68da-43cc-4523-bec8-e21ec3fe4940", "_uuid": "ee1f92fdcc7d617c818256f48a2fc214d4b4cda6", "scrolled": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Function to calculate the RMSLE on the validation data\n", "def rmsle(y, y_pred):\n", "    assert len(y) == len(y_pred)\n", "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n", "    return (sum(to_sum) * (1.0/len(y))) ** 0.5"], "metadata": {"_cell_guid": "31eeeba8-680e-4d6a-9b77-7d775190204a", "_uuid": "9d0f8997fe12874d6edb3f34175721125570030f", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Validate the model results against validation data\n", "def validate(model, print_every = 20, phase = 'val'):\n", "    start = time.time()\n", "    running_loss = 0\n", "    print_loss_total = 0\n", "    num_batches = dataset_sizes[phase]/50.\n", "    y_pred_full = np.array([])\n", "    y_true_full = np.array([])\n", "    for i_batch, sample_batched in enumerate(mercari_dataloaders[phase]): \n", "    # get the inputs\n", "        inputs = {'name':Variable(sample_batched['name']), 'item_desc':Variable(sample_batched['item_desc']), \\\n", "            'brand_name':Variable(sample_batched['brand_name']), \\\n", "            'cat_name':Variable(sample_batched['cat_name']), \\\n", "            'general_category':Variable(sample_batched['general_category']), \\\n", "            'subcat1_category':Variable(sample_batched['subcat1_category']), \\\n", "            'subcat2_category':Variable(sample_batched['subcat2_category']), \\\n", "            'item_condition':Variable(sample_batched['item_condition']), \\\n", "            'shipping':Variable(sample_batched['shipping'].float())}\n", "        prices = Variable(sample_batched['target'].float())   \n", "        batch_size = len(sample_batched['shipping'])\n", "\n", "        # forward\n", "        outputs = model(inputs,batch_size)\n", "        val_preds = target_scaler.inverse_transform(outputs.data.numpy())\n", "        val_preds = np.exp(val_preds)-1\n", "        val_true =  target_scaler.inverse_transform(prices.data.numpy())\n", "        val_true = np.exp(val_true)-1\n", "\n", "        #mean_absolute_error, mean_squared_log_error\n", "        y_true = val_true[:,0]\n", "        y_pred = val_preds[:,0]\n", "        y_true_full = np.append(y_true_full,y_true)\n", "        y_pred_full= np.append(y_pred_full,y_pred)\n", "        \n", "        loss = criterion(outputs, prices)\n", "        #print (\"output size\", val_preds.shape)\n", "        #print (\"ypred_full\",len(y_pred_full))\n", "\n", "        # statistics\n", "        running_loss += loss.data[0]\n", "        print_loss_total += loss.data[0]\n", "        #print(\"loss data shape\", loss.data.size())\n", "        #print(\"running loss\", running_loss)\n", "        #running_corrects += torch.sum(preds == labels.data)\n", "\n", "\n", "        if (i_batch+1) % print_every == 0:\n", "            print_loss_avg = print_loss_total / print_every\n", "            print_loss_total = 0\n", "            #print (i_batch / num_batches, i_batch, num_batches)\n", "            print('%s (%d %d%%) %.4f' % (timeSince(start, i_batch / num_batches), \\\n", "                                         i_batch, i_batch / num_batches*100, print_loss_avg))\n", "\n", "    v_rmsle = rmsle(y_true_full, y_pred_full)\n", "    print(\" RMSLE error on dev validate: \"+str(v_rmsle))\n", "    print(\"total loss\", running_loss / num_batches ) \n", "    return y_pred_full, y_true_full"], "metadata": {"_cell_guid": "68420bdc-589f-4b72-9171-b6509a392abc", "_uuid": "fa46959511b86b3faad119367b6bc508d3ae1717", "collapsed": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# You can see the RMSE loss on validation data is very poor.  \n", "y_pred_val, y_true_val = validate(deep_learn_model)"], "metadata": {"_cell_guid": "b2c0a398-c452-43b6-995d-c4f604bad90d", "_uuid": "878bcd06b0209c6d2f5bab329f9dedd44b701819", "scrolled": true}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["axes = plt.gca()\n", "axes.set_ylim([0,100])\n", "plt.scatter(y_pred_val,y_true_val)"], "metadata": {"_cell_guid": "f37882b3-7376-4b26-9468-f2e12dcbeec2", "_uuid": "04ab76b184c683628585a03373ba09465b746266"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["y_pred_val.mean()"], "metadata": {"_cell_guid": "3a48c239-a46b-40a1-8325-a2da3d52d226", "_uuid": "26ee3dc8472b4ce02cbef7ef3bcfdec54c56f615"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["y_true_val.mean()"], "metadata": {"_cell_guid": "b0e98621-b464-41a9-b5e2-8d1c8b25b234", "_uuid": "6ae47bc2b1569ae3acea31c4bfa9a53ef0a90dc5"}, "outputs": []}]}