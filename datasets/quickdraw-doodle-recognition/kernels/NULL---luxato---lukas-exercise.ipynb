{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"##################################### KERNEL#1\nimport os\nfrom glob import glob\nimport re\nimport ast\nimport numpy as np \nimport pandas as pd\nimport cv2\nimport urllib\nfrom tqdm import tqdm\nfrom dask import bag, threaded\nfrom dask.diagnostics import ProgressBar\nimport gc\n\nimport matplotlib\nimport matplotlib.pyplot as pltc\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, model_from_json\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport random\n\nprint(os.listdir(\"../\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d13dcce894377a1ed9f19014cfd370b0f6117474"},"cell_type":"code","source":"##################################### KERNEL#2\n# Read the strokes .csv files.\n\ntraindir = \"../input/train_simplified/\"\n\ntrain_dataset = {}\ntrain_images = 0\n\nk = 0\ncountry_list = set()\n#showcase = []\ndrawlist = []\nfor file in os.listdir(traindir):\n    k += 1\n    # Iterate through all categories.\n    if not file.endswith(\".csv\"):\n        continue\n    \n    #print(\"Current file is: \" + file)\n    filename = file.split('.')[0]\n    draw_df = None\n    draw_df = pd.read_csv(traindir + file, nrows=500)\n\n    for i in range(0, len(draw_df)):\n        country_list.add(draw_df.countrycode[0])\n        if(draw_df.recognized[i] == True):\n            #print(draw_df.recognized[i])\n            drawlist.append(draw_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b73426251f0fa80bd7267d52d450aa33bc1c42ac"},"cell_type":"code","source":"##################################### KERNEL#3\n# A JSON containing a Class and a Number of images belonging to the Class.\n\n{\n   \"airplane\":151623,\n   \"alarm clock\":123399,\n   \"ambulance\":148004,\n   \"angel\":149736,\n   \"animal migration\":137847,\n   \"ant\":124612,\n   \"anvil\":126231,\n   \"apple\":144722,\n   \"arm\":120951,\n   \"asparagus\":168102,\n   \"axe\":124122,\n   \"backpack\":125801,\n   \"banana\":307936,\n   \"bandage\":147614,\n   \"barn\":151139,\n   \"baseball bat\":123809,\n   \"baseball\":135375,\n   \"basket\":118458,\n   \"basketball\":133793,\n   \"bat\":118114,\n   \"bathtub\":174336,\n   \"beach\":124938,\n   \"bear\":134762,\n   \"beard\":165202,\n   \"bed\":113862,\n   \"bee\":120890,\n   \"belt\":191119,\n   \"bench\":128695,\n   \"bicycle\":126527,\n   \"binoculars\":124190,\n   \"bird\":133572,\n   \"birthday cake\":144982,\n   \"blackberry\":128153,\n   \"blueberry\":127878,\n   \"book\":119364,\n   \"boomerang\":142682,\n   \"bottlecap\":153207,\n   \"bowtie\":130283,\n   \"bracelet\":119416,\n   \"brain\":143033,\n   \"bread\":120570,\n   \"bridge\":133010,\n   \"broccoli\":132826,\n   \"broom\":116927,\n   \"bucket\":124064,\n   \"bulldozer\":187409,\n   \"bus\":166208,\n   \"bush\":120520,\n   \"butterfly\":117999,\n   \"cactus\":131676,\n   \"cake\":124905,\n   \"calculator\":128375,\n   \"calendar\":321981,\n   \"camel\":121399,\n   \"camera\":128772,\n   \"camouflage\":172710,\n   \"campfire\":133395,\n   \"candle\":141545,\n   \"cannon\":141394,\n   \"canoe\":123767,\n   \"car\":182764,\n   \"carrot\":132459,\n   \"castle\":122534,\n   \"cat\":123202,\n   \"ceiling fan\":115413,\n   \"cell phone\":121130,\n   \"cello\":149725,\n   \"chair\":222706,\n   \"chandelier\":167502,\n   \"church\":164225,\n   \"circle\":122876,\n   \"clarinet\":126214,\n   \"clock\":120536,\n   \"cloud\":120265,\n   \"coffee cup\":183432,\n   \"compass\":127609,\n   \"computer\":123885,\n   \"cookie\":131353,\n   \"cooler\":271444,\n   \"couch\":119662,\n   \"cow\":123083,\n   \"crab\":126930,\n   \"crayon\":129953,\n   \"crocodile\":127932,\n   \"crown\":134089,\n   \"cruise ship\":123410,\n   \"cup\":130721,\n   \"diamond\":131587,\n   \"dishwasher\":169759,\n   \"diving board\":290239,\n   \"dog\":152159,\n   \"dolphin\":121613,\n   \"donut\":140751,\n   \"door\":120230,\n   \"dragon\":124362,\n   \"dresser\":123395,\n   \"drill\":136786,\n   \"drums\":137299,\n   \"duck\":135480,\n   \"dumbbell\":157975,\n   \"ear\":122897,\n   \"elbow\":126253,\n   \"elephant\":126969,\n   \"envelope\":134863,\n   \"eraser\":118339,\n   \"eye\":125888,\n   \"eyeglasses\":225762,\n   \"face\":161666,\n   \"fan\":136158,\n   \"feather\":119910,\n   \"fence\":129426,\n   \"finger\":167957,\n   \"fire hydrant\":137242,\n   \"fireplace\":155570,\n   \"firetruck\":220695,\n   \"fish\":134150,\n   \"flamingo\":124569,\n   \"flashlight\":239763,\n   \"flip flops\":121518,\n   \"floor lamp\":166355,\n   \"flower\":144818,\n   \"flying saucer\":151966,\n   \"foot\":203086,\n   \"fork\":126077,\n   \"frog\":159047,\n   \"frying pan\":123824,\n   \"garden hose\":121843,\n   \"garden\":158527,\n   \"giraffe\":127182,\n   \"goatee\":190002,\n   \"golf club\":194843,\n   \"grapes\":155305,\n   \"grass\":123071,\n   \"guitar\":120451,\n   \"hamburger\":129672,\n   \"hammer\":119012,\n   \"hand\":291773,\n   \"harp\":285403,\n   \"hat\":222610,\n   \"headphones\":118906,\n   \"hedgehog\":120527,\n   \"helicopter\":159938,\n   \"helmet\":121899,\n   \"hexagon\":142435,\n   \"hockey puck\":203301,\n   \"hockey stick\":130110,\n   \"horse\":178286,\n   \"hospital\":167448,\n   \"hot air balloon\":126350,\n   \"hot dog\":181999,\n   \"hot tub\":120279,\n   \"hourglass\":135957,\n   \"house plant\":122996,\n   \"house\":135420,\n   \"hurricane\":136245,\n   \"ice cream\":123133,\n   \"jacket\":214124,\n   \"jail\":120131,\n   \"kangaroo\":174470,\n   \"key\":160965,\n   \"keyboard\":187766,\n   \"knee\":267540,\n   \"ladder\":125389,\n   \"lantern\":149912,\n   \"laptop\":261501,\n   \"leaf\":125571,\n   \"leg\":116804,\n   \"light bulb\":120879,\n   \"lighthouse\":160903,\n   \"lightning\":151560,\n   \"line\":143549,\n   \"lion\":120949,\n   \"lipstick\":127623,\n   \"lobster\":140175,\n   \"lollipop\":128849,\n   \"mailbox\":130053,\n   \"map\":120629,\n   \"marker\":319136,\n   \"matches\":143969,\n   \"megaphone\":137334,\n   \"mermaid\":180304,\n   \"microphone\":120570,\n   \"microwave\":130533,\n   \"moon\":121661,\n   \"mosquito\":123029,\n   \"motorbike\":169931,\n   \"mountain\":128540,\n   \"mouse\":178826,\n   \"moustache\":179924,\n   \"mouth\":134135,\n   \"mug\":152918,\n   \"mushroom\":142167,\n   \"nail\":158593,\n   \"necklace\":120580,\n   \"nose\":197573,\n   \"ocean\":131493,\n   \"octagon\":159474,\n   \"octopus\":150152,\n   \"onion\":132297,\n   \"oven\":206910,\n   \"owl\":169632,\n   \"paint can\":123446,\n   \"paintbrush\":187002,\n   \"palm tree\":121959,\n   \"panda\":113613,\n   \"pants\":144264,\n   \"paper clip\":127129,\n   \"parachute\":127319,\n   \"parrot\":185530,\n   \"passport\":150265,\n   \"peanut\":126626,\n   \"pear\":116904,\n   \"peas\":161656,\n   \"pencil\":122001,\n   \"penguin\":253791,\n   \"piano\":116870,\n   \"pickup truck\":130740,\n   \"picture frame\":122371,\n   \"pig\":186770,\n   \"pillow\":118753,\n   \"pineapple\":125071,\n   \"pizza\":130371,\n   \"pliers\":172549,\n   \"police car\":130024,\n   \"pond\":121620,\n   \"pool\":133439,\n   \"popsicle\":126707,\n   \"postcard\":125706,\n   \"potato\":329204,\n   \"power outlet\":169462,\n   \"purse\":123320,\n   \"rabbit\":155288,\n   \"raccoon\":119588,\n   \"radio\":135728,\n   \"rain\":134680,\n   \"rainbow\":126845,\n   \"rake\":154639,\n   \"remote control\":119644,\n   \"rhinoceros\":188484,\n   \"river\":133271,\n   \"roller coaster\":143570,\n   \"rollerskates\":119772,\n   \"sailboat\":136506,\n   \"sandwich\":131732,\n   \"saw\":121256,\n   \"saxophone\":118107,\n   \"school bus\":122041,\n   \"scissors\":123390,\n   \"scorpion\":165689,\n   \"screwdriver\":116313,\n   \"sea turtle\":119876,\n   \"see saw\":131936,\n   \"shark\":126050,\n   \"sheep\":126121,\n   \"shoe\":120231,\n   \"shorts\":124970,\n   \"shovel\":117194,\n   \"sink\":208410,\n   \"skateboard\":128733,\n   \"skull\":126174,\n   \"skyscraper\":183709,\n   \"sleeping bag\":119691,\n   \"smiley face\":124386,\n   \"snail\":133757,\n   \"snake\":122273,\n   \"snorkel\":154533,\n   \"snowflake\":116685,\n   \"snowman\":340029,\n   \"soccer ball\":125349,\n   \"sock\":205715,\n   \"speedboat\":188580,\n   \"spider\":209447,\n   \"spoon\":125028,\n   \"spreadsheet\":170200,\n   \"square\":125145,\n   \"squiggle\":118441,\n   \"squirrel\":156883,\n   \"stairs\":128981,\n   \"star\":137619,\n   \"steak\":122042,\n   \"stereo\":122444,\n   \"stethoscope\":153794,\n   \"stitches\":125192,\n   \"stop sign\":119814,\n   \"stove\":116535,\n   \"strawberry\":122301,\n   \"streetlight\":123280,\n   \"string bean\":119083,\n   \"submarine\":124362,\n   \"suitcase\":126442,\n   \"sun\":133781,\n   \"swan\":152088,\n   \"sweater\":120184,\n   \"swing set\":119357,\n   \"sword\":123802,\n   \"t-shirt\":125233,\n   \"table\":128021,\n   \"teapot\":126804,\n   \"teddy-bear\":179568,\n   \"telephone\":127885,\n   \"television\":123137,\n   \"tennis racquet\":231151,\n   \"tent\":131527,\n   \"The Eiffel Tower\":134801,\n   \"The Great Wall of China\":193015,\n   \"The Mona Lisa\":121383,\n   \"tiger\":121067,\n   \"toaster\":122434,\n   \"toe\":153652,\n   \"toilet\":129888,\n   \"tooth\":125064,\n   \"toothbrush\":124828,\n   \"toothpaste\":131037,\n   \"tornado\":143271,\n   \"tractor\":116677,\n   \"traffic light\":125321,\n   \"train\":127948,\n   \"tree\":144721,\n   \"triangle\":123170,\n   \"trombone\":184759,\n   \"truck\":131354,\n   \"trumpet\":169547,\n   \"umbrella\":124084,\n   \"underwear\":124548,\n   \"van\":165909,\n   \"vase\":126475,\n   \"violin\":217260,\n   \"washing machine\":120851,\n   \"watermelon\":132939,\n   \"waterslide\":185364,\n   \"whale\":116502,\n   \"wheel\":136659,\n   \"windmill\":120644,\n   \"wine bottle\":126373,\n   \"wine glass\":132302,\n   \"wristwatch\":162645,\n   \"yoga\":280442,\n   \"zebra\":144608,\n   \"zigzag\":120072\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29a11f23009780355eb4bdae9f7107d859611d7f","scrolled":false},"cell_type":"code","source":"##################################### KERNEL#4\n# This kernel splits the data into train and val set.\n\ndef empty_folder(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n        except Exception as e:\n            print(e)\n\n    \n\nclass_files = os.listdir('../input/train_simplified/')\nclass_dict = {x[:-4].replace(\" \", \"_\"):i for i, x in enumerate(class_files)}\nreverse_dict = {v: k for k, v in class_dict.items()}\n\nimg_rows, img_cols = 28, 28\nims_per_class = 100\n\nnum_classes = len(class_files)\n\ndef get_ims(strokes): # Get image out of strokes.\n    strokes = ast.literal_eval(strokes) \n    fig, ax = plt.subplots()\n    for x,y in strokes:\n        ax.plot(x, y, linewidth=12.)\n    ax.axis('off')\n    \n    #### DATA Augumentation - Random erasing\n    erasing_probability = 0\n    rand_probability = random.uniform(0, 1)\n    if rand_probability < erasing_probability:\n        # Let's erase something\n        random_width = random.randint(20,50)\n        random_height = random.randint(20,50) \n        someX, someY = 100, 0.5\n        randomX = random.randint(1,255)\n        randomY = random.randint(1,60)\n        currentAxis = plt.gca()\n        currentAxis.add_patch(Rectangle((randomX, randomY), random_width, random_height, facecolor=\"white\", alpha=1))\n    \n    fig.canvas.draw()\n    X = np.array(fig.canvas.renderer._renderer)\n    plt.close(\"all\")\n    plt.clf()\n    X = (cv2.resize(X, (img_rows, img_cols)) / 255.)[::-1]\n    X = X[:, :, 3]\n    return X\n\n# get train arrays\ntrain_grandarray = np.empty((num_classes, ims_per_class, img_rows, img_cols))\nlabelarray = np.empty((num_classes, ims_per_class))\n\nclass_paths = glob('../input/train_simplified/*.csv')\nlm = 0\nfor i,c in enumerate(tqdm(class_paths[0: num_classes])):\n    train = pd.read_csv(c, usecols=['word', 'drawing', 'countrycode'], nrows=ims_per_class)\n    train['word'] = train.word.str.replace(\" \", \"_\")\n    for m in range(0, len(train.drawing)):\n        lm += 1\n    imagebag = bag.from_sequence(train.drawing.values).map(get_ims)\n    trainarray = np.array(imagebag.compute())\n    train_grandarray[i] = trainarray\n    label = train.word.map(class_dict)\n    labelarray[i] = label\n\nlabelarray = np.expand_dims(np.concatenate(labelarray), 1)\ntrain_grandarray = np.concatenate(train_grandarray)\ntrain_grandarray = np.reshape(train_grandarray, (ims_per_class*num_classes, -1))\ntrain_grandarray = np.concatenate((labelarray, train_grandarray), axis=1)\n#labelarray","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6f94c267482b2e26a17805200cbe36e732c6787"},"cell_type":"code","source":"##################################### KERNEL#5\n# This kernel prepares the test dataset.\n\nif not os.path.exists('../my/test'):\n    #empty_folder('../my/test/')\n    os.makedirs('../my/test')\n    \nif not os.path.exists('../my/checkpoint'):\n    os.makedirs('../my/checkpoint')\n\nclass_paths = glob('../input/train_simplified/*.csv')\ntraindir = \"../input/train_simplified/\"\n\nfor index in range(0, len(class_paths)): # For every class.\n    file = class_paths[index]\n    \n    if not file.endswith(\".csv\"):\n        continue\n    \n    filename = file.split('.')[0]\n    draw_df = None\n    draw_df = pd.read_csv(file, nrows=50, skiprows=range(1, 500)) # Skip first 500 rows which were used for training\n\n    size_of_set = len(draw_df)\n    \n    label = draw_df.word[0].replace(\" \", \"_\")\n\n    k = 0\n    for i in range (0, size_of_set):\n        if(draw_df.recognized[i] == False):\n            continue\n        file = \"../my/test/\" + label + \"-\" + str(i) + \"-\" + draw_df.countrycode[0]  + \".txt\"\n        f = open(file, \"w\")\n        f.write(draw_df.drawing[i])\n        f.close()\n        k += 1\n        if k == 15: # take 15 images per cat.\n            break\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c085e6500e42cf59face2204442456f0302d0e"},"cell_type":"code","source":"##################################### KERNEL#6\n# This kernel shows 20 images from at least 8 classes here. Overall 160 images.\n\nfig=plt.figure(figsize=(20, 120))\nfrom matplotlib import pyplot, transforms\n\nb = 0\ndrawinglist = []\n\nfiles = os.listdir(\"../my/test\")\nfiles.sort()\n\nimage_count = 1\nfor file in files:\n    \n    F = open(\"../my/test/\" + file,\"r\")\n    \n    drawing = F.read()\n    drawing = ast.literal_eval(drawing)\n    \n    label = file.split('-')[0]\n\n    \n    if (drawinglist.count(label) == 20):\n        continue\n    \n    if (len(set(drawinglist)) == 8 and drawinglist.count(label) == 0):\n        break\n    \n    drawinglist.append(label)\n    \n    if image_count == 21:\n        image_count = 1\n    \n    fig.add_subplot(32, 5, b + 1)\n    for x, y in drawing: \n        # Rotate the picture because it's by default reversed.\n        base = pyplot.gca().transData\n        rot = transforms.Affine2D().rotate_deg(180)\n        plt.plot(x, y,linewidth=12. , marker='+', transform= rot + base)\n        plt.xticks(rotation=90)\n        plt.title(str(image_count) + \". \" + label)\n        plt.axis('off')\n    image_count += 1    \n    b += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c794ff3fb097017bc6d6ba3194f597aba407231d"},"cell_type":"code","source":"##################################### KERNEL#7\n# This kernel splits the data into train and val.\n\nnp.random.shuffle(train_grandarray)\nsplit_x = int(len(train_grandarray) * 0.6) # Split train:val dataset 60:40\n\nx_train = train_grandarray[0:split_x, 1:]\ny_train = train_grandarray[0:split_x, 0]\nx_val = train_grandarray[split_x: , 1:]\ny_val = train_grandarray[split_x: , 0]\n\n# todo left something for test as well\n\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n\ny_train_onehot = keras.utils.to_categorical(y_train, num_classes)\ny_val_onehot = keras.utils.to_categorical(y_val, num_classes)\n\nprint(x_train.shape, \"\\n\",\n      y_train_onehot.shape, \"\\n\",\n      x_val.shape, \"\\n\",\n      y_val_onehot.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b3d12a486ef1a1ad2f0444fe7a876c8d2ad3a8"},"cell_type":"code","source":"##################################### KERNEL#8\n# In this kernel a deep learning model is defined. \n\nfrom keras.layers.normalization import BatchNormalization\n\n# model = Sequential()\n# model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, 1)))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dense(256, activation='relu')) # Remove then two of these\n# model.add(Dense(256, activation='relu')) # Remove then two of these\n# model.add(Dropout(0.2))\n# model.add(Flatten())\n# model.add(Dense(256, activation='relu')) # Make this layer here twice so they will be 3\n# #model.add(BatchNormalization())\n# #model.add(Dropout(0.05)) # TODO remove one of these, try 0.9\n# model.add(Dense(256, activation='relu')) # Remove then two of these\n# #model.add(Dropout(0.05)) # TODO remove one of these, try 0.9\n# model.add(Dense(256, activation='relu')) # Make this layer here twice so they will be 3\n# model.add(Dropout(0.7)) # TODO remove one of these, try 0.9\n# model.add(Dense(256, activation='relu')) # Remove then two of these\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.65))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"000e8bf95572011b419901b7e1c5fb8c5bdc4662","scrolled":false},"cell_type":"code","source":"##################################### KERNEL#9\n# In this kernel the training is done and model evaluated on the validation set.\n\nfrom keras.callbacks import EarlyStopping\n\ncallbacks = [EarlyStopping(monitor='acc', patience=3)]\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(lr=0.001),\n              metrics=['accuracy'])\nbatchsize = 16\nmodel.fit(x_train, \n          y_train_onehot,\n          batch_size = batchsize,\n          epochs = 20,\n          callbacks = callbacks,\n          verbose = 1)\n\nscore = model.evaluate(x_val, y_val_onehot, verbose = 1, batch_size = batchsize)\n\n# Clean up the memory\n# x_train = None\n# y_val_onehot = None\n# y_train = None\n# train_grandarray = None\n# draw_df = None\n# del x_train\n# del y_val_onehot\n# del y_train\n# del train_grandarray\n# del draw_df\n# gc.collect()\n\nprint(\"\\nAccuracy {}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"437e956edb05d12f824523ab4859d456d4777d04","scrolled":true},"cell_type":"code","source":"##################################### KERNEL#10\n# In this kernel, the trained model is used to make predictions for the test dataset.\n# Also, computes data used for calculating the accuracy by country.\n\nfrom os import listdir\nfrom os.path import isfile, join\nimport operator\n\n# Load the test dataset\ntest_path = '../my/test'\ntest_files = [f for f in listdir(test_path) if isfile(join(test_path, f))]\n\n#test_files.sort()\n\nlabels = {}\nfor l in range(0, len(reverse_dict)):\n    labels[reverse_dict[l]] = 0\n\nfinal_predictions = {}\n\nsuccess_collector = []\nunsuccess_collector = []\n\ncountry_dict = dict([ (elem, [0, 0]) for elem in country_list ]) # Convert Set to dict.\n\n# Take all test files and make prediction in for loop to save RAM.\nfor i in range(0, len(test_files)):\n    R = []\n    F = open(\"../my/test/\" + test_files[i],\"r\") \n    test_drawing = F.read()\n    F.close()\n    \n    drawing = ast.literal_eval(test_drawing) \n    fig, ax = plt.subplots()\n    for x,y in drawing:\n        ax.plot(x, y, linewidth=12.)\n    ax.axis('off')\n    fig.canvas.draw()\n    X = np.array(fig.canvas.renderer._renderer)\n    plt.close(\"all\")\n    plt.clf()\n    X = (cv2.resize(X, (img_rows, img_cols)) / 255.)[::-1]\n    X = X[:, :, 3]         \n    X = np.array(X)\n    X = np.reshape(X, (img_rows, img_cols, 1))    \n    R.append(X) # I have to append it like this so it fits the dimensions.\n    \n    R = np.array(R)\n    prediction = model.predict_classes(R, batch_size=batchsize, verbose=0)\n    \n    X = None\n    R = None\n    del X\n    del R\n    gc.collect()\n\n    groundtruth = test_files[i].split(\"-\")[0]\n    country = test_files[i].split(\"-\")[2].split('.')[0]\n\n    #print(str(i + 1) + \". Prediction for this image is: \" + reverse_dict[prediction[0]] + \", Groundtruth is: \" + groundtruth + \", Country: \" + country) \n    \n    # hardcode hotfix, I did notice issues with dash in the label names too late.\n    if groundtruth == \"teddy\":\n        groundtruth = \"teddy-bear\"\n    if groundtruth == \"t\":\n        groundtruth = \"t-shirt\"\n    \n    if reverse_dict[prediction[0]] == groundtruth:\n        # Was classified correctly? increase the score.\n        labels[groundtruth] += 1 # Increase score for computing category complexity.\n        if (country not in country_dict):\n            country_dict[country] = [0,0]\n        country_dict[country][0] += 1 # First index of this dict represents number of correct guesses.\n        if len(success_collector) < 5:\n            success_collector.append([test_drawing, groundtruth, reverse_dict[prediction[0]]])\n    else:\n        # If classify incorrectly then decrese the score.\n        labels[groundtruth] -= 1 # Decrease score for computing category complexity.\n        if (country not in country_dict):\n            country_dict[country] = [0,0]\n        country_dict[country][1] += 1 # Second index of this dict represents number of incorrect guesses.\n        if len(unsuccess_collector) < 5:\n            unsuccess_collector.append([test_drawing, groundtruth, reverse_dict[prediction[0]]])\n    \n    if (i % 100 == 0):\n        print(str(i) + \" out of \" + str(len(test_files)))\n        \n    #if i==50:\n        #break\n        \nsorted_labels = sorted(labels.items(), key=operator.itemgetter(1))  \nsorted_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60d961f30d5945b9b3dc16aa3738af6e130c719b","scrolled":true},"cell_type":"code","source":"##################################### KERNEL#11\n# This kernel finds top 5 categories where classification went wrong and where it wents well. \n# Lastly, it visualise 5 examples where prediction was correct and 5 where it failed.\n\nprint(\"Top most difficult categories to classify are: \")    \ni = 0    \nfor key in sorted_labels:\n    if(i < 5):\n        print(str(i + 1) + \". \" + key[0])\n    else:\n        break\n    i +=1\n        \nprint(\"Top most easiest categories to classify are: \")\nj = 1\nfor i in range(len(sorted_labels) - 1, len(sorted_labels) - 6, -1):\n    print(str(j) + \". \" + sorted_labels[i][0])\n    j += 1\n    \n\n\nfig = plt.figure(figsize=(15, 15))\nfor i in range(0, len(success_collector)):\n    drawing = ast.literal_eval(success_collector[i][0])\n    fig.add_subplot(5, 5, i + 1)\n    for x, y in drawing: \n        base = pyplot.gca().transData\n        rot = transforms.Affine2D().rotate_deg(180)\n        plt.plot(x, y,linewidth=12. , marker='+', transform= rot + base)\n        plt.xticks(rotation=90)\n        plt.title(str(i + 1) + \". \" + success_collector[i][1])\n        plt.axis('off')\n\nprint(\"\\n\")\nprint(\"These are predictions labels for the second row (why weren't classfied correctly):\")\nfig = plt.figure(figsize=(15, 15))        \nfor i in range(0, len(unsuccess_collector)):\n    drawing = ast.literal_eval(unsuccess_collector[i][0])\n    fig.add_subplot(5, 5, i + 1)\n    for x, y in drawing: \n        base = pyplot.gca().transData\n        rot = transforms.Affine2D().rotate_deg(180)\n        plt.plot(x, y,linewidth=12. , marker='+', transform= rot + base)\n        plt.xticks(rotation=90)\n        plt.title(str(i + 1) + \". \" + unsuccess_collector[i][1])\n        plt.axis('off')  \n    print(str(i + 1) + \". \" + unsuccess_collector[i][1] + \" was classified as \" + unsuccess_collector[i][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd947bdf1ac4847f17e82e3b540923aea4395a5c"},"cell_type":"code","source":"##################################### KERNEL #12\n# In this kernel, the accuracy by country is presented \n\nprint(\"                         Accuracy by Country                  \")\nprint(\"___________________________________________________________________\")\nprint(\"|Country |Number of guesses |Correct |Incorrect | Accuracy [%]|\")\nprint(\"___________________________________________________________________\")\nfor key, value in country_dict.items():\n    number_of_guesses = value[0] + value[1]\n    if number_of_guesses == 0:\n        accuracy = 0\n    else:\n        accuracy = value[0] / number_of_guesses\n    print(\"|   \" + str(key) + \"   |        \" + str(number_of_guesses) + \"         |    \" +  str(value[0]) + \"   |    \" + str(value[1]) + \"     |      \" +  str(accuracy) + \"      |\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"5e8db48a228626fce3e9d1424c95ecd78ebbcf63"},"cell_type":"markdown","source":"Now, to the actual mini-project assignment:\n\n1.\n\na.       How many images does the dataset consist of? \n\t\t\n\t\t49707919 (Headers in .csv files were not counted)\n\n\nb.       How many classes? How many images per class?\n\n\t\t340 classes. Number of images per class is shown as JSON in Kernel#3 \n\n\nc.       Show 20 sample images from at least 8 different classes\n\t\t\n\t\tThis is done in Kernel #6\n\n\nd.       Consider if/how the data distribution will affect training of a classifier.\n\n\t\tI noticed that there is a noise which could cause some issues for my model. First of all, dataset is not evenly distributed. For example class airplane has ~10% more images than class alarm. This could present unwanted bias for the model. However, because of the memory limitation in Kaggle and the point that I haven't manage to make generator running in time before submition, I've decided to use fixed amount of samples for every class.\n\t\tSome of the images are empty or very bad drawn. Reason behind this is that in the quick draw game there is only 20 seconds to draw an image. While reviewing the .csv file I noticed that there is a column called \"recognized\" which contains boolean values. I've used only those which were recognized, in order to not use empty images, bad drawing and overall to decrease the noise which improved accuracy eventually.\n\n2.       Use a network of your own choice (UNet, Classical CNN, ResNet, Faster R-CNN, etc.) to classify the dataset. You may take one of the existing kernels as point of departure. We expect that you document your experiences. Please experiment with:\n\n\t\tI have decided to use a variation of convnet - a deep-learning model almost universally used in CV applications. Convnet is basically stack of Conv2D and MaxPooling2D layers. First layer takes an input tensor of shape (image_height, image_width, image_channels) - not including the batch dimension. \n\t\tNext the series of Conv2D and MaxPooling2D layers which output a 3D tensors of shape (height, width, channels). Width and hight dimensions shrinks as we go deeper to the network.\n\t\tThe last step is to feed the output tensor into a densely connected classifier network.\n\n--------------------------------------\n\na.       Hyper-parameters\n\n\t\tTraining was done on 102 000 examples (300 images per category), validation dataset consisted of 68 000 images. Test set contained 14 960 images. This was later change because of Kaggle dying and going slow to 20 400 images for training and 13 600 for validation and 5 100.\n\n\t\tLearning rate - With low learning rates the loss curve will be close to linear line. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss. Loss curve of higher learning rate will also look more exponencial.\n\n\t\tFirstly I have used Adam as an optimizer with learning rate 0.1. This didn't show up as an optimal solution because model stopped \n\n\t\tlearning rate 0.01 and model achieved 0.4470 acc on train and 0.3072 on validation set.\n\n\t\tThen I tried learning rate 0.005, which achieved slightly worse accuracy 0.3883/0.3563.\n\n\t\tThe training time was still increasing so I was forced to decrese number of epochs to 20.\n\t\tFinally I set learning rate to 0.001 and result was 0.5921/0.4034, however I saw that after 20 epochs the network is still learning (acc was increasing and loss decreasing) so I re-run the training with 50 epochs and the result was 0.7364/0.3427, meaning that undesirable overfitting occured. \n\n\t\tLR = 0.001 was chosen as the best one with 40 epochs.\n\n\t\tThere are other optimizers to consider, such as Adamax, Nadam, SGDNesterov and so on. Adam was chosen because it has the best performance on MNIST dataset which is a similar CV problem.\n\t\t\n\nb.       Batch sizes\n\t\t\tWith higher batch sizes (512), the training goes much faster. However, it requires to iterate through much more epochs to get higher accuracy.\n\t\t\tThe test was done with batch size 64. Then I tried following batch sizes:\n\t\t\t1. 64 - 0.7364/0.3427 40 epochs\n\t\t\t2. 512 - 0.6615/0.4075 40 epochs\n\t\t\t3. 128 - 0.6748/0.3736 40 epochs\n\n\n\n\t\t\tAt this point I was forced to change my approach because the kernel was constantly dying in random times (https://www.kaggle.com/product-feedback/41221). I had to decrese number of images per category to just 100, so overall 340 000 images.\n\t\t\tHaving batch size 32 and 40 epoch caused a huge overfitting 0.7/0.34. Thus I changed batch size back to 64 and put the training epochs to 20 achieving ~ 0.7 / ~ 0.25.\n\n\n\n\t\t\n\nc.       Number of network layers, adding/removing batch normalization, increasing/decreasing dropout (if applicable)\n\t\t\n\t\tThe overall architecture looked like this: Conv2D -> Maxpooling -> Conv2D -> Maxpooling -> Conv2D -> Flatten -> Dense -> Dense (sofmax).\n\n\t\tDroput layer takes care of removing neurons in order to make network generalize better and increase the validation accuracy. \n\t\tCurrent accuracy is 0.7685/0.2561 In order to remove overfitting I will try to add a dropout layer before the last Dense layer with droput rate, 0.2 with result 0.4772/0.2868.\n\n\t\tI also tried to replace the dropout layer with another Dense layer with 256 neurons leading, as expected, to even more overfitting 0.8377/0.277.\n\t\tI have decided to keep the 256 neurons dense layer but after that add the droput layer with dropout rate 0.65 leading to 0.4375/0.2926. \n\n\t\n\t\tBecause of the data decrease I decreased batch normalization to 16 and I added one more droput layer with 0.2 rate, because still the training probability is much higher then validation one. Resulting in 0.30/0.273.\n\nd.       Data augmentation techniques (random cropping, normalization, random erasing, etc.)\n\n\t\tI have found this paper about random erasing (https://arxiv.org/abs/1708.04896) from which I took some inspiration.\n\t\tAlgorithm works as follows: Generates random probability in interval <0, 1>, if this probability is smaller then the erasing probability, then we are going to generate one white rectangle over the canvas.\n\t\tRectangle with and high are randomly generated from an interval <20, 50>.\n\t\tThen random position is generated where the rectangle will be places. X axis from interval <1, 255> and Y from <1, 60>. \n\t\tThese intervals were manually estimated so the result is similar to examples in paper.\n\t\tLastly, the canvas is merged utilizing matplotlib and numpy array acquired used in training.\n\n\t\tThe acuraccy with augumentation probability 0 (no augumentation) was 0.7313/0.25 Then, I have tried to set the augumentation probability to 0.7 which led to accuraccy 0.6807/0.2553.\n\n\t\tRandom erasing didn't show up as optimal augumentation technique in this situation. Probably because I'm working with very small number of data and there are many outliers in the dataset.\n\t\tIt's worth noting that sometimes a generated rectangle did not erase any part of a drawing. This is because it was either too small or was positioned in a blank area.\n\n\n3.       Break down the results of your classifier. Some suggestions:\n\t\t\n\t\tReason why accuracy is not that high is that I have used only small amount of data for training (100* 0.6 images per category). By general rule of thumb it is reccomended to use at least 2000 samples per category. My intention was to use as much data as possible, but I was bounded with Kaggle RAM memory of 14 GB. I wanted to counter this by using a data generator, unfortunately my attemts failed. \n\n\t\tIn deep learning, having a more data is always better. Despite the Kaggle memory limits there are workarounds how to use as much data as possible. \n\t\tMethods such as writing data to disk so they are not taking place in RAM, using generators instead of loading whole dataset at once, converting images into even smaller ones can be option as well. Then it is also possible to generalize better by creating new training images with data augmentation techniques such as (except mentioned above) flipping image verically or horizontally, rotating, zooming in or out, varying the color of an image. However, because of the time constrains and faulty behaviour of kaggle I couldn't make it on time.\n\t\tModel classifies well the drawing which are unique and can be easily distinguished from other drawings such as flower or Eifell tower. On the other hand drawings which share certain degree of similarity present a problem. These are for example Giraffe and horse which can be easily distinguished in real life by color, however in this particular drawing where you can draw just with one color it's very difficult.\n\t\tAlso objects which have wheels, e.g. tractor and roller scates, pants and shorts.\n\t\tThen there were categories like which are basically subsets of the other one , for example bus and School bus.\n\t\tFurthermore, it classified violin as a guitar, cactus as a snake. Then classes which are very abstract were classified wrongly such as animal_migration and so on.\n\n\t\tTo make neural network more accurate it is necessary to use more training data, identify outliers and see why were they classified incorrectly. It could be allowed to use colors in order to differentiate from school_bus and bus.\n\n\t\tA bit overfitting occured in my training. If I'd more time I would increase the dropout rates and increase number of epochs. Although, I used data augumentation technique, I use to to replace original data, not to expand the dataset as it was intended at first place.\n\t\tI should have use more training parameters. Also for tackling an overfitting a good technique is cross-validation.\n\na.       Which category is the easiest to classify? And the hardest? Did this correlate with the distribution of training data?\n\n\t\tTop most difficult categories to classify are (Number of samples in original dataset): \n\t\t1. key (160 965) \n\t\t2. marker (319 136)\n\t\t3. bridge (133 010)\n\t\t4. toothpaste (131 037)\n\t\t5. mosquito (123 029)\n\n\t\tTop most easiest categories to classify are (Number of samples in original dataset): \n\t\t1. sailboat (136 506)\n\t\t2. circle (122 876)\n\t\t3. stairs (128 981)\n\t\t4. headphones (118 906)\n\t\t5. bowtie (130 283)\n\n\t\tMy initial guess was that categories which have the most amount of guesses are easier to classify. This would have been because when a person doesn't know how to draw an object or it is very difficult, they might skip the round and move to drawing something else. However, my hypothesis wasn't confirmed. \n\n\n\t\tFor training, I made sure that every category has the same amount of samples (100*0.6), so different number of sample couldn't cause the network to be biased against some categories. Difference between categories which are easier to classify than the hard ones is that easy ones are just simple geometric shapes, whereas, the hard ones can be drawn from different angles, in different sizes and so on. \n\n\nb.       Are there any differences in classification accuracy from country to country?\n\t\t\n\t\tTable is done in Kernel#12.\n\n\t\tAs it can be seen in the table not all the countries have same amount of guesses, some countries don't even have a guess.\n\t\tIf we look only on those where number of guesses are higher than 250 there are US with 2490 and acc 0.30, GB with 390 and acc 0.29 and DE with 285 and acc 0.25.\n\n\t\tFor making conclusions for this it is necessary to use much more data. From the results I got I can just conclude that most players are from US. \n\nc.       Visualize five examples where classification went well, and five where classification failed, consider what went wrong in case of the latter.\n\t\t\n\t\tVisualisation is done in KERNEL#11.\n\n\n\t\tFive examples where classification failed:\n\n\t\t1. squiggle was classified as animal_migration - This is because animal_migration was most of the time drawn just as some lines which should represent many animals going somewhere.\n\t\t2. pants was classified as pliers - I can see why, those pants actually look like pliers. \n\t\t3. snowman was classified as pineapple - How can a carrot has the same size as whole snowman? This is pineapple. \n\t\t4. airplane was classified as dragon - The backside of the airplane looks like dragon's had.\n\t\t5. piano was classified as jail - Those piano's keys look like jail bars."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}