{"cells":[{"metadata":{"trusted":true,"_uuid":"dddabb983cddc6cc669dfede6c3cb434a6d269cf"},"cell_type":"markdown","source":"Data are generated from this script : https://www.kaggle.com/qnkhuat/make-data-ready"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '../input/make-data-ready'\nprint(os.listdir(data_root))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fed79ee0e6bd26d348d02b8c7b59886c3afe22c2"},"cell_type":"markdown","source":"# Import and load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor as RFF\nimport xgboost as xgb\n\nfrom pprint import pprint\nimport math\n\nfrom scipy.stats import kurtosis, skew\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42d57caae8977328d06ce1191632cd93f93cceff"},"cell_type":"code","source":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\nplt.rcParams['figure.figsize'] = (12,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3cc9e31f9c51f1fd66593d2fd0cb3963e880f12"},"cell_type":"code","source":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"711b2ff1d23636474cbf8cbd63ed852d4853499d","scrolled":false},"cell_type":"code","source":"df_train = load_data(n=9)\ndf_test = load_data('test',n=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a55c64cf5c42b8c9afdf8d48373384c8d34fd3b"},"cell_type":"code","source":"# df_test.date.min(),df_test.date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e79d13741fe0ec227d976fc3a6c0011c71dd5370"},"cell_type":"code","source":"# start ,end = datetime.strptime('2017-05-01','%Y-%m-%d'),datetime.strptime('2017-10-15','%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"14fc2d029612c5d6672b13fe38f946b5a5add4d4"},"cell_type":"code","source":"# df_train = df_train[(df_train['date']>start) & (df_train['date'] <end)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48b7ceda57c96d02894bbb765244313a3ca9c077"},"cell_type":"code","source":"# df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"660a0bb0b6ac94d469e290bddbf0b32ca55bf0ff"},"cell_type":"code","source":"print(f'# of columns has na value: {(df_test.isnull().sum().sort_values(ascending=False) > 0).sum()}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3609ea5ecb4ea0a721136c3fc804b48877df40fd"},"cell_type":"markdown","source":"# Base model"},{"metadata":{"trusted":true,"_uuid":"6c8fa14fd59dfe63ee72f2b5f8f1f8768894170e"},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n\ndef split_data(df=df_train,rate=.8):\n    # sort the date first\n    df = df.sort_values('date').copy()\n    \n    df.drop(['fullVisitorId','visitId','visitStartTime'],axis=1,inplace=True)\n    df['Revenue'] = np.log1p(df['Revenue'])\n    \n    global X_train,X_valid,y_train,y_valid\n    \n    n_train = int(len(df)*rate)\n    X_train = df.drop(['Revenue','date'],axis=1).iloc[:n_train]\n    X_valid = df.drop(['Revenue','date'],axis=1).iloc[n_train:]\n    \n    y_train = df['Revenue'].iloc[:n_train]\n    y_valid = df['Revenue'].iloc[n_train:]\n    \n    print(X_train.shape,X_valid.shape)\n    \n    \n\ndef encode_data(verbose=False):\n    global df_train_encoded,df_test_encoded\n    df_train_encoded = df_train.copy()\n    df_test_encoded = df_test.copy()\n    for col in df_train.columns:\n        if df_train_encoded[col].dtype == 'object' and col not in ['fullVisitorId','visitId','visitStartTime','date']:\n            if verbose:\n                print(col)\n            lb = LabelEncoder()\n            lb.fit( list(df_train_encoded[col].unique()) + list(df_test_encoded[col].unique()))\n            df_train_encoded[col] = lb.transform(df_train_encoded[col])\n            df_test_encoded[col] = lb.transform(df_test_encoded[col])\n        \ndef run_xgb():\n   \n    params = {\n        'objective':'reg:linear',\n        'eval_metric':'rmse',\n        'learning_rate':.01,\n        'eta': 0.15, # Step size shrinkage used in update to prevents overfitting\n#         'max_depth': 10, # V3 : 1.0471 on LB\n#         'max_depth':5, # V5 : 0.9331 on LB\n        'subsample': 0.6, # sample of rows\n        'colsample_bytree': 0.6, # sample of features\n#         'alpha':0.001, \n        'lambda':1, # l2 regu\n        'random_state': 42,\n        'silent':True\n        \n    }\n    \n    \n    # got params from https://www.kaggle.com/kailex/group-xgb-for-gstore-v2\n    params['n_thread'] = -1\n    params['max_depth'] = 8\n    params['min_child_weight'] = 100\n    params['gamma'] = 5\n    params['subsample'] = 1\n    params['colsample_bytree'] = .95\n    params['colsample_bylevel'] = 0.35\n    params['alpha'] = 25\n    params['lambda'] = 25\n    \n    xgb_train_data = xgb.DMatrix(X_train, y_train)\n    xgb_val_data = xgb.DMatrix(X_valid, y_valid)\n    \n    model = xgb.train(params, xgb_train_data,\n#           num_boost_round=1000, # V3 : 1.0471 on LB\n#           num_boost_round=200, # 1.0471 on LB\n          num_boost_round = 200,\n          evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n#           early_stopping_rounds=10, # V11 0.9301 on LB\n          early_stopping_rounds=50, \n          verbose_eval=20\n         )\n    return model\n\ndef submit():\n    test_matrix = xgb.DMatrix(X_test)\n    y_pred = clf.predict(test_matrix,ntree_limit=clf.best_ntree_limit)\n    df_test['PredictedLogRevenue'] = y_pred\n    engineer_prediction\n    print('rmse after engineer prediction')\n    print(rmse(y_pred,df_test['PredictedLogRevenue']))\n    submit = df_test[['PredictedLogRevenue','fullVisitorId']].groupby('fullVisitorId').PredictedLogRevenue.sum().reset_index()\n    submit.to_csv('submit.csv',index=False)\n    \n    test(y_pred)\n    \n    \ndef engineer_prediction(df_test):\n    df_test[df_test['totals_hits'] == 1].PredictedLogRevenue = 0\n    df_test[df_test['totals_timeOnSite'] == 0].PredictedLogRevenue = 0\n    df_test[df_test['totals_bouces'] == 1].PredictedLogRevenue = 0\n    return dftest\n    \n    \ndef test(predict):\n    y_test = np.log1p(df_test['totals_transactionRevenue'])\n    print(rmse(y_test,predict))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c803f389fc7a88ef267e7ed031e0b0649a910ce8"},"cell_type":"code","source":"def prepare_data(df_train,df_test,\n                 del_col=['fullVisitorId','visitId','visitStartTime','date'],to_log=None):\n    df_train = df_train.sort_values('date').copy()\n    \n    df_train = df_train.drop(del_col,axis=1).copy()\n    df_test = df_test.drop(del_col,axis=1).copy()\n    \n    # Log some column\n    if to_log is not None:\n        df_train[to_log] = np.log1p(df_train[to_log])\n        df_test[to_log] = np.log1p(df_test[to_log])\n    \n    # totals_transactionRevenue\n    df_train['totals_transactionRevenue'] = np.log1p(df_train['totals_transactionRevenue'])\n    df_test['totals_transactionRevenue'] = np.log1p(df_test['totals_transactionRevenue'])\n    \n    global X_train,X_valid,y_train,y_valid,X_test,y_test\n    # 80/20 : train/valid\n    n_train = int(len(df_train)*.8)\n    \n    # split\n    X_train = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[:n_train]\n    X_valid = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[n_train:]\n    \n    y_train = df_train['totals_transactionRevenue'].iloc[:n_train]\n    y_valid = df_train['totals_transactionRevenue'].iloc[n_train:]\n    \n    X_test = df_test.drop(['totals_transactionRevenue'],axis=1)\n    y_test = df_test['totals_transactionRevenue']\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66568ffac07e4667abb96a0dd46bd640477ff086"},"cell_type":"code","source":"def feature_engineering(df):\n    df = df.copy()\n    # Copy from : https://www.kaggle.com/qnkhuat/base-model-v2-with-with-full-features/edit\n    \n    # time based\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    df['browser_category'] = df['device_browser'] + '_' + df['device_deviceCategory']\n    df['browser_operatingSystem'] = df['device_browser'] + '_' + df['device_operatingSystem']\n\n    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n    \n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n    \n    df['mean_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('mean')\n    df['sum_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('sum')\n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n\n    df['sum_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('sum')\n    df['count_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('count')\n    df['mean_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('mean')\n    \n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n\n    df['sum_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('sum')\n    df['count_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('count')\n    df['mean_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('mean')\n\n    df['sum_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('sum')\n    df['count_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('count')\n    df['mean_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('mean')\n    \n    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('sum')\n    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals_hits'].transform('sum')\n    \n    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('count')\n    df['user_hits_count'] = df.groupby('fullVisitorId')['totals_hits'].transform('count')\n\n    \n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] / df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] / df['user_hits_sum'].mean()\n\n    df['user_pageviews_to_region'] = df['user_pageviews_sum'] / df['mean_pageviews_per_region']\n    df['user_hits_to_region'] = df['user_hits_sum'] / df['mean_hits_per_region']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0013141da2777f0ac9d4b2feffed3c153930c44e"},"cell_type":"code","source":"df_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"16605dd219a0e49bb3ab9ddd0629c34ea7f948e1"},"cell_type":"code","source":"encode_data(verbose=True)\nprepare_data(df_train_encoded,df_test_encoded,del_col=['fullVisitorId','visitId',\n            'visitStartTime','date','totals_transactions','totals_totalTransactionRevenue'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"c26b1e30bf504d9c9cec95146656358ed844970e"},"cell_type":"code","source":"# clf = run_xgb()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b3094980c0ec926539821a462d0ee2d65abe9ea"},"cell_type":"code","source":"# try to find a good validation set\n# Why our score so different with the leader board?\n# check with the target in dataset first","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b6b3c573dded5838368dc8eb349a6ad447d49a9"},"cell_type":"markdown","source":"# GRID SEARCH"},{"metadata":{"trusted":true,"_uuid":"d09c884f19b77a855bec329adeb416f4de41eafd"},"cell_type":"code","source":"gs_params = {\n    'max_depth':[3,5,7,10,15,20],\n    'learning_rate':[.01,.1,.5],\n    'n_estimators':[10,50,100,150,200],\n    'n_jobs':[-1],\n    'gamma':[0,5],\n    'min_child_weight':[1,5,7],\n    'subsample': [0.6,1], # sample of rows\n    'colsample_bytree': [0.5,1], # Subsample ratio of columns when constructing each tree.\n    'colsample_bylevel':[0.35,.5,.7],# Subsample ratio of columns for each split, in each level.\n    'reg_alpha':[1,5,10,25],\n    'reg_lambda':[1,5,10,25],\n    'objective':['reg:linear'],\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9a17014fa6e86de5cbe0cda5c6d509d4f160748"},"cell_type":"code","source":"fit_params = {\n    'num_boost_round':200,\n    'early_stopping_rounds':50, \n    'verbose_eval':20,\n    \n    \n}\nmodel = xgb.XGBRegressor()\nmodel_cv = model_selection.GridSearchCV(model,param_grid=gs_params,fit_params= fit_params,scoring = 'neg_mean_squared_error',cv=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9b6758cff19e922c8156eb9a4f598f73bec870c"},"cell_type":"code","source":"%%time\nbest = model_cv.fit(X_train,y_train,eval_metric='rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06c2cb595d17e15d19fa104d3efc3998d605fc01"},"cell_type":"code","source":"best.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe662971d4793ae82f7b575fdfff590ea85d9a15"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"392938eb9284e3f800c7f1254891827b39c68d7d"},"cell_type":"markdown","source":"# Feature important"},{"metadata":{"_uuid":"0101c08148886885c07d41dfa152fb94fc8e604d"},"cell_type":"markdown","source":"\"gain\" is the average gain of splits which use the feature"},{"metadata":{"trusted":true,"_uuid":"63cb30a84b68bec37fe1ffa53d6c708b489731fd"},"cell_type":"code","source":"# xgb.plot_importance(clf,importance_type='gain',max_num_features=20)\n# plt.title('Gain Feature important')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81d96c5a110b876e555d19e538fc888d0b051001"},"cell_type":"markdown","source":"\"cover\" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"34b2a8c2186d69867ae376671ff291fce662fc99"},"cell_type":"code","source":"# xgb.plot_importance(clf,importance_type='cover',max_num_features=20)\n# plt.title('Cover Feature important')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22b8e759c46de5a8031ad3512f7d0b7e679b0a84"},"cell_type":"markdown","source":"\"weight\" is the number of times a feature appears in a tree"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e32fac1da52c2a2e7644e1af7c9a196f573f59dc"},"cell_type":"code","source":"# xgb.plot_importance(clf,importance_type='weight',max_num_features=20)\n# plt.title('Weight Feature important')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dc30a7c4a402a664bcade5046b382ec160a9268"},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true,"_uuid":"a5c973b9c7ef1d533fcb4c807b7d39a123dfd876"},"cell_type":"code","source":"# submit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f587453cb66d50c15822707425190512ddab4d0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b4c795987399a376ff7f856b2a43d62e76ef57e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"171b4868f331037343e0930111a72edb4c29860a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e32583cc487663dd61799cb1960f872eff20bd2c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d48f9ea4532b68da11401a92d6dc020f33c791a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24e84945849d90581707fd71c587ead4ba709583"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}