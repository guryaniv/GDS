{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Seismic data is a neat thing. You can imagine it like an ultra-sound of the subsurface. However, in an ultra-sound, we use much smaller wavelengths to image our body. Seismic data usually has wavelengths around 1m to 100m. That has some physical implications, but for now, we don't have to deal with that. It's just something to keep in mind while thinking about resolution. \n\nImaging salt has been a huge topic in the seismic industry, basically since they imaged salt the first time. The Society of Exploration geophysicist alone has over 10,000 publications with the [keyword salt](https://library.seg.org/action/doSearch?AllField=salt). Salt bodies are important for the hydrocarbon industry, as they usually form nice oil traps. So there's a clear motivation to delineate salt bodies in the subsurface. If you would like to do a deep dive, you can see [this publication](https://www.iongeo.com/content/documents/Resource%20Center/Articles/INT_Imaging_Salt_tutorial_141101.pdf)\n\nSeismic data interpreters are used to interpreting on 2D or 3D images that have been heavily processed. The standard work of [seismic data analysis](https://wiki.seg.org/wiki/Seismic_Data_Analysis) is open access.\nYou'll find sections on Salt in there as well (https://wiki.seg.org/wiki/Salt-flank_reflections and https://wiki.seg.org/wiki/Salt_flanks). The seismic itself is pretty \"old\" in the publication, and you're dealing with data that is less noisy here, which is nice.\n\n[![Seismic Data with salt CC-BY-SA Yilmaz](https://wiki.seg.org/images/1/14/Ch05_fig0-1.png)](https://wiki.seg.org/wiki/Salt-flank_reflections#/media/File:Ch05_fig0-1.png)\nCaption: Figure 5.0-1  Conflicting dips associated with salt flanks: (a) CMP stack without dip-moveout correction; (b) time migration of the stack in (a); (c) the stack with dip-moveout correction; (d) time migration of the stack in (c). CC-BY-SA Yilmaz.\n\nInterpretation on seismic images has long used texture attributes, to identify better and highlight areas of interest. These can be seen like feature maps on the texture of the seismic. For salt, you will notice that the texture in the salt masks is rather chaotic, where the surrounding seismic is more \"striped\". You can think of Earth as layered. Sand gets deposited on top of existing sand. In comes salt, which is behaving very much, unlike other rocks. There is an entire research branch dedicated to salt tectonics, that is the movement of salt in the subsurface. To give you the gist, these salt diapirs form from salt layers somewhere else that were under much pressure. These started to flow (behave ductile) and find a way into other layers above. I have written a bit about salt on [my blog](http://the-geophysicist.com/the-showroom-data-for-my-thesis).\n\nOne common seismic attribute is called \"chaos\" or \"seismic disorder\". So if you talk to cynic geophysicists, you'll hear \"that deep learning better outperform the Chaos attribute\". A good starting point is [this publication](http://www.chopraseismic.com/wp-content/uploads/2016/08/Chopra_Marfurt_TLE_Aug2016-LowRes.pdf).\n\nRecently, geoscience has started to adopt deep learning, and it has seen a clear boom, particularly in imaging salt. Code for automatic seismic interpretation can be found here: \n\n+ https://github.com/waldeland/CNN-for-ASI\n+ https://github.com/bolgebrygg/MalenoV\n+ https://github.com/crild/facies_net\n\nYou will notice that these solutions load a specific SEG-Y file, which luckily we don't have to bother with. TGS provided some nice PNG files instead. However, you can glean some information from them how to approach seismic data. If you find you need some geophysical helpers, you can [import Bruges](https://github.com/agile-geoscience/bruges)\n\nLet's dive in for now.\n"},{"metadata":{"trusted":true,"_uuid":"00833d394e3069216af171fd979c814e7e1e430d","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\n\nfrom tqdm import tqdm_notebook, tnrange\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda\nfrom keras.layers.core import Dropout, Flatten\nfrom keras.layers import BatchNormalization,Concatenate\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e26e21ff39e8b2afc0003fec4e4f5269f61aa4c","_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"# Set some parameters\nim_width = 128\nim_height = 128\nim_chan = 3\npath_train = '../input/train/'\npath_test = '../input/test/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89455be399a79910334eb76beafc40bcdab08f83"},"cell_type":"markdown","source":"# Data Exploration\nLet's look at some data. We can see that TGS chose to use very varied data by inspecting. That is great and adresses a problem in deep learning geoscience at the moment. We build models on one type of seismic and have no idea whether it generalizes."},{"metadata":{"trusted":true,"_uuid":"2dccf3274ff5ea9bddc430bae092342c871f41ac","collapsed":true},"cell_type":"code","source":"ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\nplt.figure(figsize=(20,10))\nfor j, img_name in enumerate(ids):\n    q = j+1\n    img = load_img('../input/train/images/' + img_name + '.png')\n    img_mask = load_img('../input/train/masks/' + img_name + '.png')\n    \n    plt.subplot(1,2*(1+len(ids)),q*2-1)\n    plt.imshow(img)\n    plt.subplot(1,2*(1+len(ids)),q*2)\n    plt.imshow(img_mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61fa14e7421411a0ca943a1a36e77cbef2ddcbd2"},"cell_type":"markdown","source":"We have many examples without salt, as you can see by the masks that are entirely dark. That's great, an algorithm we build will then know that patches exist entirely without salt. Talk about biasing your data.\n\nWe can draw heavily on other work, instead of regurgitating the geophysics work that has been done before. I mentioned that seismic is kind of like ultrasound. So I had a look at https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n\nLet's throw a Unet at our data. I am blatanly stealing from Ketil at this point. All credit goes to him and his nice code.\nFirst we'll need to get our data into a shape that works for U-Nets. That means, it should be a power of 2. Let's do it quick and dirty for now, but eventually, consider aliasing and all that fun."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"97114b7b4f28347130dc3e44af5469d6efdf7ab1","_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"train_ids = next(os.walk(path_train+\"images\"))[2]\ntest_ids = next(os.walk(path_test+\"images\"))[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ee3d0fb98094a4a6c5604093c5706a7db96df97"},"cell_type":"code","source":"# Image Data Augmentation\n# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n# https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\n# https://towardsdatascience.com/image-augmentation-for-deep-learning-using-keras-and-histogram-equalization-9329f6ae5085","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8f02165966489c8a21bb7127bb88e7cf607599d","_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# Get and resize train images and masks\nR_chan = 0\nG_chan = 1\nB_chan = 2\n\nX_train_orig_img = [None]\nX_train_gray_img = [None]\nX_train_rgb_img = np.zeros((len(train_ids), im_height, im_width, 3), dtype=np.uint8)\nY_train_mask_img = np.zeros((len(train_ids), im_height, im_width, 3), dtype=np.uint8)\nY_train_orig_mask_img = [None]\n\nX_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.bool)\n\nX_train_new = np.zeros((len(train_ids), im_height, im_width, 3), dtype=np.uint8)\nY_train_new = np.zeros((len(train_ids), im_height, im_width, 3), dtype=np.uint8)\n\nimg_ids = []\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n    path = path_train\n    img_ids.append(id_)\n    img = load_img(path + '/images/' + id_ )\n    \n#     r_id = random.randint(0, 100)\n#     id_ = train_ids[r_id]\n#     print(r_id)\n#     img = load_img(path + '/images/' + id_ )\n\n    x_img = img_to_array(img)\n    X_train_orig_img.append(x_img)\n    x_img = resize(x_img, (im_width, im_height, 3), mode='constant', preserve_range=True)\n    X_train_rgb_img[n] = x_img\n    x = img_to_array(img)[:,:,1]\n    #X_train_gray_img.append(x)\n    x = resize(x, (im_width, im_height, 1), mode='constant', preserve_range=True)\n    X_train[n] = x\n    mask_img = load_img(path + '/masks/' + id_)\n    Y_train_orig_mask_img.append(mask_img)\n    mask = img_to_array(mask_img)\n    mask_img = resize(mask, (im_width, im_height, 3), mode='constant', preserve_range=True)\n    Y_train_mask_img[n] = mask_img\n    Y_train[n] = resize(mask[:,:,1], (im_width, im_height, 1), mode='constant', preserve_range=True)\n    #res = cv2.bitwise_and(img, img, mask = mask)\n    \n    res1 = mask_img * x_img\n    mask_inv = 255.0 - mask_img\n    res2 = mask_inv * x_img\n\n    for x1 in range(im_width):\n        for x2 in range(im_height):\n            if x1 < im_width - 1 and x2 < im_height - 1:\n                if  res2[x1,x2, R_chan] == 0 and res2[x1,x2, G_chan] == 0 and res2[x1,x2, B_chan] == 0: # when green channel is 0, color with red\n                    res2[x1,x2, G_chan] = 100\n                #else:\n                    #res2[x1, x2, R_chan] = 100\n                \n\n    res3 = res1+res2\n    res4 = res3 * mask_img\n    res5 = res2 * mask_img\n    \n    X_train_new[n] = res2\n    Y_train_new[n] = res5\n    \n#     break\n    \n# plt.subplot(1,5,1)\n# plt.imshow(res1)\n# plt.subplot(1,5,2)\n# plt.imshow(res2)\n# plt.subplot(1,5,3)\n# plt.imshow(res3)\n# plt.subplot(1,5,4)\n# plt.imshow(res4)\n# plt.subplot(1,5,5)\n# plt.imshow(res5)\n# plt.show()\n\n# plt.subplot(1,2,1)\n# plt.imshow(x_img)\n# plt.subplot(1,2,2)\n# plt.imshow(mask_img)\n# plt.show()\n\n# plt.subplot(1,2,1)\n# plt.imshow(res3)\n# plt.subplot(1,2,2)\n# plt.imshow(res5)\n# plt.show()\n    \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"faf6ea42655fb0f5ee8994a65a7c3bef888ef1ae","collapsed":true},"cell_type":"code","source":"# Check if training data looks all right\nix = random.randint(0, len(train_ids))\nprint(X_train[ix].shape)\n#plt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\nplt.imshow(X_train_new[ix])\nplt.show()\n\nprint(Y_train[ix].shape)\ntmp = np.squeeze(Y_train_new[ix]).astype(np.float32)\n#plt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.imshow(tmp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d66a11a8d8d48e16640307185062f5494c1f5b6"},"cell_type":"markdown","source":"# Train Model\nOur task, just like the segmentation task for nuclei, is evaluated on the mean IoU metric. This one isn't in keras, but obviously, we're stealing this one too from Ketil."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b4716a2112dfb71c75e60bff90cb17836f78bf66"},"cell_type":"code","source":"smooth = 1.\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\n# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e7c423ccf5145d6ac991dad85262540735e4dfe"},"cell_type":"markdown","source":"This is the fun part. Building the sequential Model. The U-Net is basically looking like an Auto-Encoder with shortcuts. \n\nWe're also sprinkling in some earlystopping to prevent overfitting. If you're running this on kaggle, this is the point, you want to have GPU support."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb09cfb0643c3f10e87fbb325821b217da4d9dcf"},"cell_type":"code","source":"def conv_block(m, dim, acti, bn, res, do=0):\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n\tn = BatchNormalization()(n) if bn else n\n\tn = Dropout(do)(n) if do else n\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n\tn = BatchNormalization()(n) if bn else n\n\treturn Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n\tif depth > 0:\n\t\tn = conv_block(m, dim, acti, bn, res)\n\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n\t\tif up:\n\t\t\tm = UpSampling2D()(m)\n\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n\t\telse:\n\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n\t\tn = Concatenate()([n, m])\n\t\tm = conv_block(n, dim, acti, bn, res)\n\telse:\n\t\tm = conv_block(m, dim, acti, bn, res, do) # do dropout for last layer\n\treturn m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n\ti = Input(shape=img_shape)\n\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n\to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n\treturn Model(inputs=i, outputs=o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fb6dedf8b50a8150c2177f5b2175559e9b9e985","collapsed":true},"cell_type":"code","source":"# Build U-Net model\n#img_shape = (im_height, im_width, im_chan)\nimg_shape = (im_height, im_width, 3)\ninputs = Input(img_shape)\ns = Lambda(lambda x: x / 255) (inputs)\n\nmodel = UNet(img_shape, out_ch = 3, start_ch=8, depth=4, dropout=0.5, batchnorm=True)\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mean_iou'])\nmodel.compile(optimizer='adam', loss=dice_coef_loss, metrics = [dice_coef])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a32ece6c94c69191b26ba8eabfe501cb3480cd6a","collapsed":true},"cell_type":"code","source":"print(X_train_new.shape)\nprint(Y_train_new.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58969e2e3bdca3b94da4ebd4e513a83455adf00a","_kg_hide-output":true,"_kg_hide-input":true,"scrolled":false,"collapsed":true},"cell_type":"code","source":"earlystopper = EarlyStopping(patience=5, verbose=1)\n#checkpointer = ModelCheckpoint('model-tgs-salt-3-dropout-0.5-batchnorm-accuracy.h5', verbose=1, save_best_only=True)\ncheckpointer = ModelCheckpoint('model-tgs-salt-3-dropout-0.5-batchnorm-dice_coef.h5', verbose=1, save_best_only=True)\nresults = model.fit(X_train_new, Y_train_new, validation_split=0.1, batch_size=8, epochs=100, \n                     callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f3ba4fe34672a2c8b4687293861f5ce59e2c26e","collapsed":true},"cell_type":"code","source":"loss = results.history['loss']\nval_loss = results.history['val_loss']\n\nacc = results.history['acc']\nval_acc = results.history['val_acc']\n\nepochs = range(len(loss))\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training vs Validation loss')\nplt.legend()\n\nplt.figure()\nplt.show()\n\nplt.plot(epochs, acc, 'bo', label='Training Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Training vs Validation acc')\nplt.legend()\n\nplt.figure()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07f33fc4ad0abec843770c65831a21e586d88a49","collapsed":true},"cell_type":"code","source":"\n# val_loss = results.history['val_loss']\n# val_mean_iou = results.history['val_mean_iou']\n# mean_iou = results.history['mean_iou']\n\n# epochs = range(len(val_loss))\n\n# plt.plot(epochs, mean_iou, 'bo', label='Validation Loss')\n# plt.plot(epochs, val_mean_iou, 'b', label='Validation mean IOU')\n# plt.title('')\n# plt.legend()\n\n# plt.figure()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22c52fa0216935778bad80956dcb3e2342a6909f","collapsed":true},"cell_type":"code","source":"loss = results.history['loss']\nval_loss = results.history['val_loss']\ndice_coef = results.history['dice_coef']\nval_dice_coef = results.history['val_dice_coef']\n\nepochs = range(len(val_loss))\n\nplt.plot(epochs, loss, 'bo', label='Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\n\nplt.figure()\nplt.show()\n\nplt.plot(epochs, dice_coef, 'bo', label='Dice Coef')\nplt.plot(epochs, val_dice_coef, 'b', label='Validation Dice Coef')\nplt.title('Training vs Validation Dice Coef')\nplt.legend()\n\nplt.figure()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab8516fb8ab135872dd4f4b895b5d76206df1fa"},"cell_type":"markdown","source":"# Test Data\nFirst we'll get the test data. This takes a while, it's 18000 samples."},{"metadata":{"trusted":true,"_uuid":"c6d376a5ed9fa0ff708299f55a0a8ed8b8471137","collapsed":true},"cell_type":"code","source":"# Get and resize test images\n#X_test = np.zeros((len(test_ids), im_height, im_width, 1), dtype=np.uint8)\nX_test_new = np.zeros((len(test_ids), im_height, im_width, 3), dtype=np.uint8)\n\nsizes_test = []\nprint('Getting and resizing test images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm_notebook(enumerate(test_ids), total=len(test_ids)):\n    path = path_test\n    img = load_img(path + '/images/' + id_)\n    x = img_to_array(img)\n    x_img = x\n    x = x[:,:,1]\n    sizes_test.append([x.shape[0], x.shape[1]])\n    x = resize(x, (128, 128, 1), mode='constant', preserve_range=True)\n    x_new = resize(x_img, (128, 128, 3), mode='constant', preserve_range=True)\n    #X_test[n] = x\n    X_test_new[n] = x_new\n\nprint(X_test_new.shape)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2316034edcb7227673fd9b69264ca9c0d0e87f14","collapsed":true},"cell_type":"code","source":"# Define IoU metric\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\n# Predict on train, val and test\n#model = load_model('model-tgs-salt-3-dropout-0.5-batchnorm.h5', custom_objects={'mean_iou': mean_iou})\n#model = load_model('model-tgs-salt-3-dropout-0.5-batchnorm-accuracy.h5')\nmodel = load_model('model-tgs-salt-3-dropout-0.5-batchnorm-dice_coef.h5', \n                   custom_objects={'dice_coef': dice_coef, 'dice_coef_loss': dice_coef_loss})\n\npreds_train = model.predict(X_train_new[:int(X_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(X_train_new[int(X_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(X_test_new, verbose=1)\n\n# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58bc6bef25dabc0a0a833f8c926eb95c0771ad8b"},"cell_type":"code","source":"np.save('../working/Outputpreds_test', preds_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f2edb5ece371531a14c2c188b4cb09f03079b4c"},"cell_type":"code","source":"#preds_test_l = np.load('../working/preds_test.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af64790cdb7e5beb05fc34635cdf092124d7dc20","collapsed":true},"cell_type":"code","source":"# del X_train\n# del X_train_orig_img\n# del X_train_rgb_img\n# del X_train_gray_img\n# del Y_train\n\n# Create list of upsampled test masks\n# preds_test_upsampled = []\n# for i in tnrange(len(preds_test)):\n#     preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n#                                        (sizes_test[i][0], sizes_test[i][1]), \n#                                        mode='constant', preserve_range=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7da5a47444df98205dd7039223868b5d67f15400","collapsed":true},"cell_type":"code","source":"#preds_test_upsampled[0].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24defa25c00d0d91b38e559515e78c63f4d26e2b"},"cell_type":"markdown","source":"We'll look at it again, just to be sure."},{"metadata":{"trusted":true,"_uuid":"6302c46fc76d8a43cb87d01c43c60c3c8f0ac98b","collapsed":true},"cell_type":"code","source":"# # Perform a sanity check on some random training samples\n# ix = random.randint(0, len(preds_train_t))\n# #plt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\n# plt.imshow(X_train_new[ix])\n# plt.show()\n# # tmp = np.squeeze(Y_train[ix]).astype(np.float32)\n# # plt.imshow(np.dstack((tmp,tmp,tmp)))\n# plt.imshow(Y_train_new[ix])\n# plt.show()\n\n# tmp = np.squeeze(preds_train_t[ix]).astype(np.float32)\n# #plt.imshow(np.dstack((tmp,tmp,tmp)))\n# plt.imshow(tmp)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"844cded40edc71652bc5b26852245e37f46f6448"},"cell_type":"markdown","source":"# Prepare Submission\nWe need to prepare the submission. A nice CSV with predictions. All of this is one to one from Ketil and does not differ from any of the other segmentation tasks. Check them out to improve on this."},{"metadata":{"trusted":true,"_uuid":"73336f76166028ba39c8164083c9564a0d5afe40","collapsed":true},"cell_type":"code","source":"def RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n\n# pred_dict = {fn[:-4]:RLenc(np.round(preds_test_upsampled[i])) for i,fn in tqdm_notebook(enumerate(test_ids))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eaf7acaf4a0678638c5e40732c6533816777637","collapsed":true},"cell_type":"code","source":"# sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n# sub.index.names = ['id']\n# sub.columns = ['rle_mask']\n# sub.to_csv('submission-dropout-0.5-batchnorm-dice-coef.csv')\n#model-tgs-salt-3-dropout-0.5-batchnorm.h5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}