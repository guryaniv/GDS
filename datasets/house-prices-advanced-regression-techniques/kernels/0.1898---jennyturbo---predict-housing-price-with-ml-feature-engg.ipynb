{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"scrolled":true},"cell_type":"code","source":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\n#read in data \n#training set consists house id 1- 1460\ndata= pd.read_csv('../input/train.csv',index_col=0)\n#testing set consists house id 1461 - 2919\ntestdata=pd.read_csv('../input/test.csv', index_col=0)\n#insert saleprice column in testdata just so the they have the same # of columns\ntestdata[\"SalePrice\"]=0\n","execution_count":1,"outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#concate the dataframes together so I can clean the data together. They will be seperated later\ntotal=pd.concat([data,testdata])\n#Fill NAN with 0's\ntotal=total.apply(lambda x: x.fillna(0))\n# conver object to dummy variables\ncolumns=total.columns[data.dtypes == 'object']\ntotal=pd.get_dummies(total, columns =columns)\nprint(total.head())\n","execution_count":12,"outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"741b56bb-d902-4919-b7c7-6dbb2c51de29","_uuid":"38199d9cd1bd0b6cb6a7d4e1a1456e6d3ea047f0","trusted":true},"cell_type":"code","source":"#now split the data again\ndata = total.iloc[0:1460,:]\ntestdata= total.iloc[1460:, :]\n# Now remove SalePrice column from test set\ntestdata.drop([\"SalePrice\"],axis=1)\nprint(data.head())\nprint(testdata.head())","execution_count":13,"outputs":[]},{"metadata":{"scrolled":true,"_cell_guid":"baa34691-1d5f-4bc0-9eca-afc4058840d3","_uuid":"7b463f8ce8f7c1ed9a1ff73f27698fab556929dc","trusted":true},"cell_type":"code","source":"#split out validation data set\nY=data[['SalePrice']]\ndata.drop([\"SalePrice\"], axis=1)\nX=data\ntestdata.drop([\"SalePrice\"], axis=1)\nprint(testdata)\nprint(X)\nprint(Y)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"d2a42eac-c8cb-4d7c-8457-5390aad018a9","_uuid":"a7e69e6077f21ccb509cef95f57327b5a864fe1c","trusted":true,"scrolled":true},"cell_type":"code","source":"#Start Testing a couple  models' performance\n#Try Lasso, ElasticNet, Ridge, SVR(kernel ='rbf') etc\n#import packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor \n#feature extraction with SelectKBest\nfrom sklearn.feature_selection import SelectKBest \n#Statistical tests can be used to select those features that have the strongest relationship with the output variable. \n#The scikit-learn library provides the SelectKBest class2 that can be used with a suite of different statistical tests to select a specifit number of features.\nfrom sklearn.feature_selection import chi2\nfrom sklearn.decomposition import PCA\n\n#Split out validation dataset\nX_train,X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size= 0.2,random_state=7)\nnum_folds=10\nscoring = 'mean_squared_error'\nseed=7\n\nprint('test')\n","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"60757ce7-ff97-4a2a-a328-4176c1a973ad","_uuid":"806baa2721d6dcb92fa96ad9b9a4553be941975c","trusted":true,"scrolled":false},"cell_type":"code","source":"#Using pipelines and for loops to find the best model(s)\nfrom sklearn.pipeline import Pipeline\npipelines= []\npipelines.append(('scaledLasso', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('Lasso', Lasso())])))\npipelines.append(('scaledRidge', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('Ridge', Ridge())])))\npipelines.append(('scaledEN', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('EN', ElasticNet())])))\npipelines.append(('scaledSVR', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('SVR', SVR())])))\npipelines.append(('scaledRFR', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('RFR', RandomForestRegressor())])))\npipelines.append(('scaledGBR', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('GBR', GradientBoostingRegressor())])))\npipelines.append(('scaledABR', Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('ABR', AdaBoostRegressor())])))\nresults=[]\nnames=[]\nfor name, model in pipelines:\n    kfold=KFold(n_splits=num_folds, random_state = seed)\n    cv_results= cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, cv_results.mean(),cv_results.std()))\n\n    #scaledGBR: -1044598568.165649 (737380927.867408)\n    #scaledRFR: -1054476883.359236 (620683514.287574)\n#scaledEN: -1335370698.879367 (624704717.051779)\n#scaledLasso: -1320054330.183205 (640006413.003961)\n#scaledRidge: -1319943605.816805 (639756928.179752)\n#scaledSVR: -6369795963.599862 (1838658478.481387)\n#scaledABR: -1500073218.467181 (969069134.458727)\n\n   ","execution_count":46,"outputs":[]},{"metadata":{"scrolled":true,"_cell_guid":"3745d01d-4b75-4154-9b46-9da97c4c1f2b","_uuid":"62a90e416dc17659456f38d9ca9489441703df22","trusted":true},"cell_type":"code","source":"#Fine tuning GradientBoostingRegressor model (the best model)\nscaler=Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5))]).fit(X_train)\nrescaledX=scaler.transform(X_train)\nlearning_rate =[0.01,0.1]\nn_estimators = [200, 500]\nmax_depth = [10,50]\nmin_samples_leaf = [1, 2]\n\nparam_grid = {'n_estimators': n_estimators, \n              'learning_rate': learning_rate,\n             'max_depth': max_depth,\n             'min_samples_leaf': min_samples_leaf}\n\nmodel=GradientBoostingRegressor()\nkfold=KFold(n_splits=num_folds, random_state=seed)\ngrid=GridSearchCV(estimator=model, param_grid=param_grid, scoring= scoring, cv=kfold)\ngrid_result=grid.fit(rescaledX,Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_,grid_result.best_params_))\n\n#Best: -1027985518.359070 using {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 2, 'n_estimators': 200}\n\n\n","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"d13fd279-eb8b-4315-80f1-eea9ef6724a5","_uuid":"39c3cc0d2640ccc740fa6032e12fa58afe519802","trusted":true},"cell_type":"code","source":"#Finalize the model\n\nscaler = Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5))]).fit(X_train)\nrescaledX=scaler.transform(X_train)\n##Best: -1027985518.359070 using {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 2, 'n_estimators': 200}\n# Pipeline([('Scaler',StandardScaler()),('pca',PCA(n_components=5)),('GBR', GradientBoostingRegressor())])\nmodel=GradientBoostingRegressor(learning_rate=0.1, max_depth=10, min_samples_leaf=2, n_estimators=200)\nmodel.fit(rescaledX,Y_train)\nrescaledTestdataX=scaler.transform(testdata)\n\nprediction=model.predict(rescaledTestdataX)\nprint(prediction)\n\ntestdata[\"SalePrice\"]=prediction\noutput=testdata[[\"SalePrice\"]]\nprint(output.head(20))\n\noutput.to_csv(\"prediction_output_ML_test3.csv\")\n","execution_count":50,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}