{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\nimport nltk\nfrom sklearn import *\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntest = pd.read_csv('../input/test_stage_1.tsv', delimiter='\\t').rename(columns={'A': 'A_Noun', 'B': 'B_Noun'})\nsub = pd.read_csv('../input/sample_submission_stage_1.csv')\ntest.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd7cc6a7b02d4155074c8f9111315a582e46a40"},"cell_type":"code","source":"# True test here:\n#gh_train = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\", delimiter='\\t')\n\ngh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\ngh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\ntrain = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f6933d5524239e2954db2706a6392f2aa92a78"},"cell_type":"code","source":"def name_replace(s, r1, r2):\n    s = str(s).replace(r1,r2)\n    for r3 in r1.split(' '):\n        s = str(s).replace(r3,r2)\n    return s\n\ndef get_features(df):\n    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n    #df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'subjectone'), axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'subjecttwo'), axis=1)\n    \n    \n    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n    return(df)\n\ntrain = get_features(train)\ntest = get_features(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfa12b1a81855f4a88347cb7f489c071b7f355dc","trusted":true},"cell_type":"code","source":"def get_nlp_features(s, w):\n    doc = nlp(str(s))\n    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\n\ntrain['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntrain['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\ntest['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntest['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b113656f16ed8c6b34d4a0f3916e0e0d85277c1b"},"cell_type":"code","source":"train = train.rename(columns={'A-coref':'A', 'B-coref':'B'})\ntrain['A'] = train['A'].astype(int)\ntrain['B'] = train['B'].astype(int)\ntrain['NEITHER'] = 1.0 - (train['A'] + train['B'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f42bc5c2ad912aa9b69c538ac3b171d4856be7"},"cell_type":"code","source":"col = ['Pronoun-offset', 'A-offset', 'B-offset', 'section_min', 'Pronoun-offset2', 'A-offset2', 'B-offset2', 'section_max', 'A-poss', 'B-poss', 'A-dist', 'B-dist']\nx1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\nx1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"714def76447082fbf1d595685d54e014033ddc97"},"cell_type":"code","source":"model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n\n# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n\nmodel.fit(x1, y1)\nprint('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\nmodel.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\nresults = model.predict_proba(test[col])\ntest['A'] = results[:,0]\ntest['B'] = results[:,1]\ntest['NEITHER'] = results[:,2]\ntest[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}