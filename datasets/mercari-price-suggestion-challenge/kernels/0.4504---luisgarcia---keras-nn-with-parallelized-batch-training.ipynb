{"metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "0f3ab6b4-5b89-447c-841b-82ea3139295d", "_uuid": "4dd86826913b2f8779885ec7536aa9a483a01ce7"}, "cell_type": "markdown", "source": ["I start with [Alex Papiu's](https://www.kaggle.com/apapiu/ridge-script) script where he preprocesses the data into a ```scipy.sparse``` matrix and train a neural network. Keras does not like ```scipy.sparse``` matrices and converting the entire training set to a matrix will lead to computer memory issues; so the model is trained in batches: 32 samples at a time, and these few samples can be converted to matrices and fed into the network. \n", "\n", "This requieres a batch generator, which I pieced together from this [stack overflow question](https://stackoverflow.com/questions/41538692/using-sparse-matrices-with-keras-and-tensorflow) and I set up an iterator to make it threadsafe for parallelization. ~~Kaggle allows the use of 32 cores which speeds up the training~~. Seems like kaggle only allows four cores.  \n", "\n", "I have been tuning the network and it seems like a smaller network with longer epochs yields better results. Currently I have a two hidden layers with 25  and 10 nodes. This is quite small but, with the input layer considered, this network still yields approximately 1.5M parameters!\n", "\n", "Give it a try and let me know what you think. There are still plenty of things on can try:\n", "* Add a validation set for early stopping. \n", "* Tune `batch_size`, `samples_per_epoch`, and nodes in hidden layers.\n", "* Add dropout.\n", "* Add L1 and/or L2 regularization.\n", "   \n", "\n"]}, {"metadata": {"_cell_guid": "4a79269a-c302-48af-bd96-1a426a651c18", "_uuid": "bddce000193062c3a129f2125d253e18926a2f71", "collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import scipy\n", "import gc\n", "import time\n", "import threading\n", "import multiprocessing\n", "\n", "from sklearn import metrics\n", "from sklearn.linear_model import Ridge, LogisticRegression\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "\n", "from keras import regularizers \n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation\n", "from keras.utils import np_utils\n", "from keras.callbacks import EarlyStopping\n", "\n", "\n", "def hms_string(sec_elapsed):\n", "    h = int(sec_elapsed / (60 * 60))\n", "    m = int((sec_elapsed % (60 * 60)) / 60)\n", "    s = sec_elapsed % 60\n", "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n", "\n", "\n", "# the following functions allow for a parallelized batch generator\n", "class threadsafe_iter(object):\n", "    \"\"\"\n", "    Takes an iterator/generator and makes it thread-safe by\n", "    serializing call to the `next` method of given iterator/generator.\n", "    \"\"\"\n", "    def __init__(self, it):\n", "        self.it = it\n", "        self.lock = threading.Lock()\n", "    def __iter__(self):\n", "        return self\n", "\n", "    def __next__(self):\n", "        with self.lock:\n", "            return next(self.it)\n", "\n", "def threadsafe_generator(f):\n", "    \"\"\"\n", "    A decorator that takes a generator function and makes it thread-safe.\n", "    \"\"\"\n", "    def g(*a, **kw):\n", "        return threadsafe_iter(f(*a, **kw))\n", "    return g\n", "\n", "@threadsafe_generator\n", "def batch_generator(X_data, y_data, batch_size):\n", "    \n", "    #index = np.random.permutation(X_data.shape[0])    \n", "    #X_data = X_data[index]\n", "    #y_data = y_data[index]\n", "    \n", "    samples_per_epoch = X_data.shape[0]\n", "    number_of_batches = samples_per_epoch/batch_size\n", "    counter=0\n", "    index = np.arange(np.shape(y_data)[0])\n", "    #idx = 1\n", "    while 1:\n", "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n", "        X_batch = X_data[index_batch,:].todense()\n", "        y_batch = y_data[index_batch]\n", "        counter += 1\n", "        yield np.array(X_batch),y_batch\n", "        #print(\"\")\n", "        #print(X_batch.shape)\n", "        #print(\"\")\n", "        #print('generator yielded a batch %d' % idx)\n", "        #idx += 1\n", "        if (counter > number_of_batches):\n", "            counter=0\n", "            \n", "            \n", "@threadsafe_generator\n", "def batch_generator_x(X_data,batch_size):\n", "    samples_per_epoch = X_data.shape[0]\n", "    number_of_batches = samples_per_epoch/batch_size\n", "    counter=0\n", "    index = np.arange(np.shape(X_data)[0])\n", "    while 1:\n", "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n", "        X_batch = X_data[index_batch,:].todense()\n", "        counter += 1\n", "        yield np.array(X_batch)\n", "        if (counter > number_of_batches):\n", "            counter=0\n", "\n", "np.random.seed(1337)  # for reproducibility\n", "start_time = time.time()\n", "\n", "NUM_BRANDS = 2500\n", "NAME_MIN_DF = 10\n", "MAX_FEAT_DESCP = 50000\n", "\n", "print(\"Reading in Data\")\n", "\n", "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n", "df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n", "\n", "df = pd.concat([df_train, df_test], 0)\n", "nrow_train = df_train.shape[0]\n", "y_train = np.log1p(df_train[\"price\"])\n", "\n", "del df_train\n", "gc.collect()\n", "\n", "#print(df.memory_usage(deep = True))\n", "\n", "df[\"category_name\"] = df[\"category_name\"].fillna(\"Other\").astype(\"category\")\n", "df[\"brand_name\"] = df[\"brand_name\"].fillna(\"unknown\")\n", "\n", "pop_brands = df[\"brand_name\"].value_counts().index[:NUM_BRANDS]\n", "df.loc[~df[\"brand_name\"].isin(pop_brands), \"brand_name\"] = \"Other\"\n", "\n", "df[\"item_description\"] = df[\"item_description\"].fillna(\"None\")\n", "df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(\"category\")\n", "df[\"brand_name\"] = df[\"brand_name\"].astype(\"category\")\n", "\n", "#print(df.memory_usage(deep = True))\n", "\n", "print(\"Encodings...\")\n", "count = CountVectorizer(min_df=NAME_MIN_DF)\n", "X_name = count.fit_transform(df[\"name\"])\n", "\n", "print(\"Category Encoders...\")\n", "unique_categories = pd.Series(\"/\".join(df[\"category_name\"].unique().astype(\"str\")).split(\"/\")).unique()\n", "count_category = CountVectorizer()\n", "X_category = count_category.fit_transform(df[\"category_name\"])\n", "\n", "print(\"Descp encoders...\")\n", "count_descp = TfidfVectorizer(max_features = MAX_FEAT_DESCP, \n", "                              ngram_range = (1,3),\n", "                              stop_words = \"english\")\n", "X_descp = count_descp.fit_transform(df[\"item_description\"])\n", "\n", "print(\"Brand encoders...\")\n", "vect_brand = LabelBinarizer(sparse_output=True)\n", "X_brand = vect_brand.fit_transform(df[\"brand_name\"])\n", "\n", "print(\"Dummy Encoders..\")\n", "X_dummies = scipy.sparse.csr_matrix(pd.get_dummies(df[[\n", "    \"item_condition_id\", \"shipping\"]], sparse = True).values)\n", "\n", "X = scipy.sparse.hstack((X_dummies, \n", "                         X_descp,\n", "                         X_brand,\n", "                         X_category,\n", "                         X_name)).tocsr()\n", "\n", "#print([X_dummies.shape, X_category.shape, X_name.shape, X_descp.shape, X_brand.shape])\n", "\n", "X_train = X[:nrow_train]\n", "\n", "tpoint1 = time.time()\n", "print(\"Time for Preprocessing: {}\".format(hms_string(tpoint1-start_time)))\n", "\n", "print(\"Train dimensions:{}\".format(X_train.shape))\n", "\n", "\n", "\n", "\n", "print(\"Fitting Model\")\n", "\n", "n_workers = multiprocessing.cpu_count() #it's 32 on kaggle, 4 on my personal machine\n", "batch_size = 32\n", "            \n", "model = Sequential()\n", "model.add(Dense(25,\n", "                input_dim=X_train.shape[1],\n", "                kernel_initializer='normal',\n", "                activation='relu',\n", "                kernel_regularizer=regularizers.l2(0.01),\n", "                activity_regularizer=regularizers.l1(0.01),\n", "               ))\n", "model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n", "#model.add(Dropout(0.01))\n", "model.add(Dense(1, kernel_initializer='normal'))\n", "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n", "model.compile(loss='mean_squared_error', optimizer='adam')\n", "\n", "model.fit_generator(generator=batch_generator(X_train, y_train, batch_size),\n", "                    workers=n_workers, \n", "                    steps_per_epoch=1024, #samples_per_epoch=1024,\n", "                    max_queue_size=128,\n", "                    epochs=37, \n", "                    verbose=1,\n", "                    use_multiprocessing=False\n", "                   )\n", "\n", "tpoint2 = time.time()\n", "print(\"Time for Training: {}\".format(hms_string(tpoint2-tpoint1)))\n", "\n", "\n", "X_test = X[nrow_train:]\n", "print(\"Test dimensions:{}\".format(X_test.shape))\n", "\n", "s = np.floor(X_test.shape[0]/batch_size)+1\n", "print(s)\n", "\n", "# unfortunately, predicting is tougher to parallelize...\n", "pred = model.predict_generator(generator=batch_generator_x(X_test, batch_size),\n", "                               steps=s,\n", "                               #workers=n_workers, \n", "                               #max_queue_size=32, \n", "                               use_multiprocessing=False,\n", "                               verbose=1\n", "                              )\n", "\n", "tpoint3 = time.time()\n", "print(\"Predict dimensions:{}\".format(pred.shape))\n", "print(\"Time for Predicting: {}\".format(hms_string(tpoint3-tpoint2)))\n", "\n", "\n", "\n", "df_test[\"price\"] = np.expm1(pred)\n", "df_test[[\"test_id\", \"price\"]].to_csv(\"submission_NN.csv\", index = False)\n", "\n", "elapsed_time = time.time() - start_time\n", "print(\"Total Time: {}\".format(hms_string(elapsed_time)))\n", "\n", "\n", "\n", "\n"]}], "nbformat_minor": 1}