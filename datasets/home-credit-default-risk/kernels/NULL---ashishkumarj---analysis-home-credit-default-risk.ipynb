{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8895135dede0f478c2a021125ea444e1a9718e59"},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"232d5ac97970503a737064e73379516be6809556","collapsed":true},"cell_type":"code","source":"#importing all the data files to dataset.\ndf1 = pd.read_csv('../input/application_test.csv')\ndf2 = pd.read_csv('../input/application_train.csv')\ndf3 = pd.read_csv('../input/bureau.csv')\ndf4 = pd.read_csv('../input/bureau_balance.csv')\ndf5 = pd.read_csv('../input/credit_card_balance.csv')\ndf6 = pd.read_csv('../input/installments_payments.csv')\ndf7 = pd.read_csv('../input/POS_CASH_balance.csv')\ndf8 = pd.read_csv('../input/previous_application.csv')\n","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e1eac4ba23884888b8523519fa8a2dcdbb456b27"},"cell_type":"code","source":"## check if column has more than 30% data missing, then reject that variables.\ndef accept_reject(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    NaN_Percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    final = pd.concat([total,NaN_Percent], axis=1, keys=['Total', 'NaN_Percent'])\n    not_required =  final.drop(final[final.NaN_Percent < 30].index)\n    print('Missing Value Greater Than 30%, so variable has been dropped')\n    return not_required","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f436505baa095d7218cfe7bbf230aad0ebb25039"},"cell_type":"code","source":"## plot the graph of all the Catagorical variables\ndef cat_graph(data):\n    print(' Catogirical fileds vs its count graph')\n    catag= list(data.select_dtypes(exclude=['int64','float']).columns)\n    for col in catag:\n        #plt.title(col)\n        sns.countplot(y= data[col])\n        plt.show()","execution_count":45,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ca9581ffef774e89193902b5869f0c5b9fe1d990"},"cell_type":"code","source":"## If data is missing, then fill Catgorical with MODE and Continous with MEDIAN\ndef misfill(data):\n    catag= pd.DataFrame([data.select_dtypes(exclude=['int64','float']).columns])\n    contn= pd.DataFrame([data.select_dtypes(exclude=['object']).columns])\n    for row in contn.iteritems():\n        col =row[1] \n        data[col]=data[col].fillna(data[col].median())\n    for roww in catag.iteritems():\n        col =roww[1] \n        data[col]=data[col].fillna(data[col].mode().loc[0])","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c8f4c1bd391e33013706d172b94337cd0734280","collapsed":true},"cell_type":"code","source":"## Pre processing Application Train (df1)\ndf1.drop_duplicates(['SK_ID_CURR'], keep='first',inplace=True)\ndf1.sort_values(by=['SK_ID_CURR'],inplace=True)\n\naccept_reject(df1)\n\n## based on above function result, deleting below variable as it has more than 30% data missing.\n\ndf1.drop(['COMMONAREA_MEDI' , 'COMMONAREA_AVG', 'COMMONAREA_MODE','NONLIVINGAPARTMENTS_MODE',\n          'NONLIVINGAPARTMENTS_MEDI','NONLIVINGAPARTMENTS_AVG','FONDKAPREMONT_MODE',\\\n          'LIVINGAPARTMENTS_MEDI','LIVINGAPARTMENTS_MODE','LIVINGAPARTMENTS_AVG','FLOORSMIN_MEDI',\\\n          'FLOORSMIN_MODE','FLOORSMIN_AVG','YEARS_BUILD_MEDI','YEARS_BUILD_AVG',\\\n          'YEARS_BUILD_MODE','OWN_CAR_AGE','LANDAREA_MODE','LANDAREA_AVG','LANDAREA_MEDI',\\\n          'BASEMENTAREA_MEDI','BASEMENTAREA_AVG','BASEMENTAREA_MODE','EXT_SOURCE_1',\\\n          'NONLIVINGAREA_MEDI','NONLIVINGAREA_AVG','NONLIVINGAREA_MODE','ELEVATORS_MODE',\\\n          'ELEVATORS_AVG','ELEVATORS_MEDI','WALLSMATERIAL_MODE','APARTMENTS_MODE','APARTMENTS_AVG',\\\n          'APARTMENTS_MEDI','ENTRANCES_MEDI','ENTRANCES_MODE','ENTRANCES_AVG','LIVINGAREA_MEDI',\\\n          'LIVINGAREA_MODE','LIVINGAREA_AVG','HOUSETYPE_MODE','FLOORSMAX_MODE','FLOORSMAX_MEDI',\\\n          'FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BEGINEXPLUATATION_AVG',\\\n          'YEARS_BEGINEXPLUATATION_MODE','TOTALAREA_MODE','EMERGENCYSTATE_MODE',\\\n          'OCCUPATION_TYPE'],axis=1, inplace=True)\n\n## plot the graphs for catagorical Variables\n\ncat_graph(df1)\n\n## fill the missing value \n\nmisfill(df1)\n\n# convert the string data to binary data.\n\ndf1 = pd.get_dummies(df1)\n","execution_count":47,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0432d952085838275627ac520e5532048ef23196"},"cell_type":"code","source":"#preprocessing Training data (DF2)\n\ndf2.drop_duplicates(['SK_ID_CURR'], keep='first',inplace=True)\ndf1.sort_values(by=['SK_ID_CURR'],inplace=True)\n\naccept_reject(df2)\n\ndf2.drop(['COMMONAREA_MEDI' , 'COMMONAREA_AVG', 'COMMONAREA_MODE','NONLIVINGAPARTMENTS_MODE',\n          'NONLIVINGAPARTMENTS_MEDI','NONLIVINGAPARTMENTS_AVG','FONDKAPREMONT_MODE',\\\n          'LIVINGAPARTMENTS_MEDI','LIVINGAPARTMENTS_MODE','LIVINGAPARTMENTS_AVG','FLOORSMIN_MEDI',\\\n          'FLOORSMIN_MODE','FLOORSMIN_AVG','YEARS_BUILD_MEDI','YEARS_BUILD_AVG',\\\n          'YEARS_BUILD_MODE','OWN_CAR_AGE','LANDAREA_MODE','LANDAREA_AVG','LANDAREA_MEDI',\\\n          'BASEMENTAREA_MEDI','BASEMENTAREA_AVG','BASEMENTAREA_MODE','EXT_SOURCE_1',\\\n          'NONLIVINGAREA_MEDI','NONLIVINGAREA_AVG','NONLIVINGAREA_MODE','ELEVATORS_MODE',\\\n          'ELEVATORS_AVG','ELEVATORS_MEDI','WALLSMATERIAL_MODE','APARTMENTS_MODE','APARTMENTS_AVG',\\\n          'APARTMENTS_MEDI','ENTRANCES_MEDI','ENTRANCES_MODE','ENTRANCES_AVG','LIVINGAREA_MEDI',\\\n          'LIVINGAREA_MODE','LIVINGAREA_AVG','HOUSETYPE_MODE','FLOORSMAX_MODE','FLOORSMAX_MEDI',\\\n          'FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MEDI','YEARS_BEGINEXPLUATATION_AVG',\\\n          'YEARS_BEGINEXPLUATATION_MODE','TOTALAREA_MODE','EMERGENCYSTATE_MODE',\\\n          'OCCUPATION_TYPE'],axis=1, inplace=True)\n\ncat_graph(df2)\n\nmisfill(df2)\n\ndf2 = pd.get_dummies(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a4d009a6804b70a25be842bf49497c8530a9ade0"},"cell_type":"code","source":"# Pre processing the Bureau Dataset (df3)\n\ndf3.drop_duplicates(['SK_ID_BUREAU','SK_ID_CURR'], keep='first',inplace=True)\ndf3.sort_values(by=['SK_ID_BUREAU','SK_ID_CURR'],inplace=True)\n\naccept_reject(df3)\n\n#Based on above result, delete below variables\ndf3.drop(['CREDIT_CURRENCY', 'AMT_ANNUITY','AMT_CREDIT_MAX_OVERDUE','DAYS_ENDDATE_FACT',\\\n          'AMT_CREDIT_SUM_LIMIT','CREDIT_ACTIVE'], axis=1, inplace=True)\n\ncat_graph(df3)\n\nmisfill(df3)\n\ndf3 = pd.get_dummies(df3) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23c06023f01a369670fd32113233bdee4a8e8155"},"cell_type":"code","source":"# Pre processing the Bureau Balance Dataset (df4)\n\ndf4.drop_duplicates(['SK_ID_BUREAU'], keep='first',inplace=True)\ndf4.sort_values(by=['SK_ID_BUREAU'],inplace=True)\n\naccept_reject(df4)\n\ncat_graph(df4)\n\nmisfill(df4)\n\ndf4 = pd.get_dummies(df4) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2dddc887f2f2bc063e7ee7b13ca5cf82f59b9c7"},"cell_type":"code","source":"# Pre processing Credit card balance Dataset (df5)\n\ndf5.drop_duplicates(['SK_ID_PREV','SK_ID_CURR'], keep='first',inplace=True)\ndf5.sort_values(by=['SK_ID_PREV','SK_ID_CURR'],inplace=True)\n\naccept_reject(df5)\n\ndf5.drop(['NAME_CONTRACT_STATUS','AMT_RECEIVABLE_PRINCIPAL','AMT_RECIVABLE',\\\n          'AMT_TOTAL_RECEIVABLE','AMT_PAYMENT_CURRENT','AMT_DRAWINGS_ATM_CURRENT',\\\n         'CNT_DRAWINGS_POS_CURRENT','CNT_DRAWINGS_OTHER_CURRENT','CNT_DRAWINGS_ATM_CURRENT',\\\n         'AMT_DRAWINGS_POS_CURRENT','AMT_DRAWINGS_OTHER_CURRENT'],axis=1, inplace=True)\n\ncat_graph(df5)\n\nmisfill(df5)\n\ndf5 = pd.get_dummies(df5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"77acc9bb8f650ef052b16fc2fe880e3bd09285b9"},"cell_type":"code","source":"## Pre processing Instalment_Payment datasets (df6)\n\ndf6.drop_duplicates(['SK_ID_PREV','SK_ID_CURR'], keep='first',inplace=True)\ndf6.sort_values(by=['SK_ID_PREV','SK_ID_CURR'],inplace=True)\n\naccept_reject(df6)\n\ncat_graph(df6)\n\nmisfill(df6)\n\ndf6 = pd.get_dummies(df6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ad14632faaf5165253f52242bfb01845e879e1f"},"cell_type":"code","source":"## Pre processing Posh Cash Balance dataset (df7)\n\ndf7.drop_duplicates(['SK_ID_PREV','SK_ID_CURR'], keep='first',inplace=True)\ndf7.sort_values(by=['SK_ID_PREV','SK_ID_CURR'],inplace=True)\n\naccept_reject(df7)\n\ncat_graph(df7)\n\nmisfill(df7)\n\ndf7 = pd.get_dummies(df7)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"324bae89363fc9feb35197b3f4706cb97769d0fe"},"cell_type":"code","source":"## Pre processing Previous Applicataion dataset (df8)\n\ndf8.drop_duplicates(['SK_ID_PREV','SK_ID_CURR'], keep='first',inplace=True)\ndf8.sort_values(by=['SK_ID_PREV','SK_ID_CURR'],inplace=True)\n\naccept_reject(df8)\n\n# Drop below fileds based on above function result.\ndf8.drop(['RATE_INTEREST_PRIVILEGED','RATE_INTEREST_PRIMARY','RATE_DOWN_PAYMENT','AMT_DOWN_PAYMENT',\\\n          'NAME_TYPE_SUITE', 'DAYS_TERMINATION','NFLAG_INSURED_ON_APPROVAL','DAYS_FIRST_DRAWING',\\\n          'DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE'], axis=1, inplace=True)\n\ncat_graph(df8)\n\ndf8 = pd.get_dummies(df8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c7349114c2948c8c3028ab9a69d8c02465b3000"},"cell_type":"code","source":"# MERGE  beauro and beauro balance on 'SK_ID_BUREAU'\n\ndf34= pd.merge(df3, df4, on='SK_ID_BUREAU')\n\n# Merge credit card balance and installation payment on 'SK_ID_PREV','SK_ID_CURR'\n\ndf56= pd.merge(df5, df6, on=['SK_ID_PREV','SK_ID_CURR'],how='inner')\n\n#pos cash balance is not matching at all with above tables, so ignored that complete table.\n\n# Merge prev application, credit card balance and installation payment on 'SK_ID_PREV','SK_ID_CURR'\n\ndf856 = pd.merge(df8, df56, on=['SK_ID_PREV','SK_ID_CURR'])\n\n## Merge beauro,beauro balance,prev application, credit card balance and installation payment on 'SK_ID_CURR'\n\ndf_sub_final= pd.merge(df34, df856, on='SK_ID_CURR')\n\n# Merge the Training dataset with all other dataset on 'SK_ID_CURR'\nactual_train_data = pd.merge(df2, df_sub_final, on='SK_ID_CURR',how = 'inner')\n\n# Merge the Test dataset with all other dataset on 'SK_ID_CURR'\n\nactual_test_data = pd.merge(df1, df_sub_final, on='SK_ID_CURR',how = 'left')\n\n# to match the variables in test and train data. Below variable has to be removed as these fields are not in Test dataset \n\nactual_train_data.drop(['CODE_GENDER_XNA','NAME_INCOME_TYPE_Maternity leave','NAME_FAMILY_STATUS_Unknown'],axis=1,inplace=True)\n\nmisfill(actual_test_data)\nmisfill(actual_train_data)\nactual_test_data = pd.get_dummies(actual_test_data)\nactual_train_data = pd.get_dummies(actual_train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"103995c746540b2f2b8a68b6137d065ad83a90ae"},"cell_type":"code","source":"# PREDICT THE BEST FIT MODEL\n# first preditict using the Training data (as target also given to check accuracy), then we will fit the best model in Test.\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ny = actual_train_data['TARGET']\nX=  actual_train_data.drop(['TARGET'],axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n\n\n#1. DECISION TREE CLASSIFIER \nfrom sklearn.tree import DecisionTreeClassifier \nDtree = DecisionTreeClassifier()\ndt= Dtree.fit(X_train,y_train)\npredict = dt.predict(X_test)\ndt_acc = accuracy_score(y_test, predict)\ndt_conf = confusion_matrix(y_test, predict)\nprint('Decision Tree accuracy',dt_acc)\nprint('Decision Tree Confusion Matrix',dt_conf)\n\n#2. RANDOM FOREST CLASSIFIER\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_acc = accuracy_score(y_test, Y_prediction)\nrandom_conf = confusion_matrix(y_test, Y_prediction)\nprint('Random Forest accuracy',random_acc)\nprint('Random Forest Confusion Matrix',random_conf)\n\n#3. K-Neighbour Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nY_knn_Pred = knn.predict(X_test)\nknn_acc = accuracy_score(y_test, Y_knn_Pred)\nknn_conf = confusion_matrix(y_test, Y_knn_Pred)\nprint('K-Neighbour accuracy',knn_acc)\nprint('K-Neighbour Confusion Matrix',knn_conf)\n\n#4. LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_log_Pred = logreg.predict(X_test)\nlogistic_acc = accuracy_score(y_test, Y_log_Pred)\nlogistic_conf = confusion_matrix(y_test, Y_log_Pred)\nprint('Logistic accuracy',logistic_acc)\nprint('Logistic Confusion Matrix',logistic_conf)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"59c7be9becf0e94c1472c9e2601c92f4799f2ee9"},"cell_type":"code","source":"## APPLY THE BEST MODEL ON \"ACTUAL TEST DATA \" - (DF1)\n\n# Best accuracy we got for DecisionTreeClassifier, so we can apply the same to our actual test data\n\npred = dt.predict(actual_test_data)\n\npred = pd.DataFrame(pred)\n\nResult = pd.concat([actual_test_data['SK_ID_CURR'], pred], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f047bf20a889584eb8d6fba3a5a292dad6d1c66e","collapsed":true},"cell_type":"code","source":"Result","execution_count":48,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}