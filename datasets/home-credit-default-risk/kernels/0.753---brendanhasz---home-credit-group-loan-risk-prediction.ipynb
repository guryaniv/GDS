{"cells":[{"metadata":{"_uuid":"be4820cca6d91fa5b86e64729a28be8529eb4b9d"},"cell_type":"markdown","source":"# Home Credit Group Loan Risk Prediction\n\n[Home Credit Group](http://www.homecredit.net/) is a financial institution which specializes in consumer lending, especially to people with little credit history.  In order to determine what a reasonable principal is for applicants, and a repayment schedule which will help their clients sucessfully repay their loans, Home Credit Group wants to use data about the applicant to predict how likely they are to be able to repay their loan.  Home Credit Group recently hosted a [kaggle competition](https://www.kaggle.com/c/home-credit-default-risk) to predict loan repayment probability from (anonymized) applicant information.  In this kernel, we'll try and predict loan repayment ability.\n\n## Outline\n\n* [Data Loading and Cleaning](#data-loading-and-cleaning)\n* [Manual Feature Engineering](#manual-feature-engineering)\n* [Feature Encoding](#feature-encoding)\n* [Baseline Predictions](#baseline-predictions)\n* [Calibration](#calibration)\n* [Resampling](#resampling)\n* [Final Predictions and Feature Importance](#final-predictions)\n\nFirst let's load the packages we'll use."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom hashlib import sha256\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import make_pipeline\n\n# Plot settings\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5402661119519f4267bfbd3d53713ed2c1aa73bb"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data-loading-and-cleaning\"></a>\n## Data Loading and Cleaning\n\nLet's load both the training and test data."},{"metadata":{"trusted":true,"_uuid":"dbafb8dcf1af4e7d9c3b4bf16161327a6ae695dd"},"cell_type":"code","source":"# Load applications data\ntrain = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c8ad85ea70fbcbc4474b0000ae9a6c39645167"},"cell_type":"markdown","source":"And now we can take a look at the data we're working with.  "},{"metadata":{"trusted":true,"_uuid":"569b63d0d9d83e59de73fb8dbcbf98c70a51ad7f"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c83bc5b0363e361864d48367e1b416fa37254300"},"cell_type":"code","source":"# Print info about each column in the train dataset\nfor col in train:\n    print(col)\n    Nnan = train[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/train.shape[0])\n    print(train[col].describe())\n    if train[col].dtype==object:\n        print('Categories and Count:')\n        print(train[col].value_counts().to_string(header=None))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da59786e9376f69b3b179ba596428f12818a5ad4"},"cell_type":"code","source":"# Print info about each column in the test dataset\nfor col in test:\n    print(col)\n    Nnan = test[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/test.shape[0])\n    print(test[col].describe())\n    if test[col].dtype==object:\n        print('Categories and Count:')\n        print(test[col].value_counts().to_string(header=None))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33f2e6988f3fb8f05023b55a6a9d74e47bc67958"},"cell_type":"markdown","source":"The column containing the values we are trying to predict, `TARGET`, doesn't contain any missing values.  The value of `TARGET` is $0$ when the loan was repayed sucessfully, and $1$ when there were problems repaying the loan.  Many more loans were succesfully repayed than not, which means that the dataset is imbalanced in terms of our dependent variable, which is something we'll have to watch out for when we build a predictive model later:"},{"metadata":{"trusted":true,"_uuid":"d73aae2ef303cdce678f87bc49c164073f26b081"},"cell_type":"code","source":"# Show target distribution\ntrain['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae4313940302e607227540defc3302c6a32d856c"},"cell_type":"markdown","source":"There's a lot of categorical columns - let's check that, for each column, all the categories we see in the training set we also see in the test set, and vice-versa."},{"metadata":{"trusted":true,"_uuid":"fc7177f28a14c85162c608d9e85bee10ffe432a4"},"cell_type":"code","source":"for col in test:\n    if test[col].dtype==object:\n        print(col)\n        print('Num Unique in Train:', train[col].nunique())\n        print('Num Unique in Test: ', test[col].nunique())\n        print('Unique in Train:', sorted([str(e) for e in train[col].unique().tolist()]))\n        print('Unique in Test: ', sorted([str(e) for e in test[col].unique().tolist()]))\n        print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c50a37f6abe6ab3b456ca79118394805abcab60e"},"cell_type":"markdown","source":"We'll merge the test and training dataset, and create a column which indicates whether a sample is in the test or train dataset.  That way, we can perform operations (label encoding, one-hot encoding, etc) to all the data together instead of doing it once to the training data and once to the test data."},{"metadata":{"trusted":true,"_uuid":"0346e412e98f1f7124a54b39903ea9057348e5e9"},"cell_type":"code","source":"# Merge test and train into all application data\ntrain_o = train.copy()\ntrain['Test'] = False\ntest['Test'] = True\ntest['TARGET'] = np.nan\napp = train.append(test, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17020939b123b2d49ac558c0ef14e6ef6da4bfb9"},"cell_type":"markdown","source":"The gender column contains whether the loan applicant was male or female.  The training datset contains 4 values which weren't empty but were labelled `XNA`.  Normally we would want to create a new column to represent when the gender value is null.  However,  since the test dataset has only `M` and `F` entries, and because there are only 4 entries with a gender of `XNA` in the training set, we'll remove those entries from the training set."},{"metadata":{"trusted":true,"_uuid":"734bd2ec150be7b5a1764f1bda3e0ced9dcd7e96"},"cell_type":"code","source":"# Remove entries with gender = XNA\napp = app[app['CODE_GENDER'] != 'XNA']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88a9836bde9985a765da6194384abc5b76273371"},"cell_type":"markdown","source":"The `NAME_INCOME_TYPE` column also contained entries for applicants who were on Maternity leave, but no such applicants were in the test set.  There were only 5 such applicants in the training set, so we'll remove these from the training set."},{"metadata":{"trusted":true,"_uuid":"aa8f509cda7c020400a0c5718d6afe30bc17dd2c"},"cell_type":"code","source":"# Remove entries with income type = maternity leave\napp = app[app['NAME_INCOME_TYPE'] != 'Maternity leave']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c249acebaa36a060854c7d624ef765342fc41366"},"cell_type":"markdown","source":"Similarly, in the `NAME_FAMILY_STATUS` column, there were 2 entries in the training set with values of `Unknown`, and no entries with that value in the test set.  So, we'll remove those too."},{"metadata":{"trusted":true,"_uuid":"340b9ffbae4a9fe9762c12944f24e5bef62cf478"},"cell_type":"code","source":"# Remove entries with unknown family status\napp = app[app['NAME_FAMILY_STATUS'] != 'Unknown']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1abcf81abdd12bac518b9ba6dbfae5549920d9b5"},"cell_type":"markdown","source":"There were some funky values in the `DAYS_EMPLOYED` column:"},{"metadata":{"trusted":true,"_uuid":"c77e621099e7159f337fe6c498156658b3ab1ef3"},"cell_type":"code","source":"app['DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f7109b6142ea10aba85c8b66f70b8c87ada27d"},"cell_type":"markdown","source":"$350,000$ days?  That's like $1,000$ years!  Looks like all the reasonable values represent the number of days between when the applicant was employed and the date of the loan application.  The unreasonable values are all exactly $365,243$, so we'll set those to `NaN`."},{"metadata":{"trusted":true,"_uuid":"88c253d19d78167320e5c5f6d802c73e8d88e74b"},"cell_type":"code","source":"# Show distribution of reasonable values\napp.loc[app['DAYS_EMPLOYED']<200000, 'DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED (which are less than 200,000)')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbc994f55797034195413a8220f94a6316e34839"},"cell_type":"code","source":"# Show all unique outlier values\napp.loc[app['DAYS_EMPLOYED']>200000, 'DAYS_EMPLOYED'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b65921e206aba230ddb2c63c6e1eeb52352bfd"},"cell_type":"code","source":"# Set unreasonable values to nan\napp['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f527d11e598260a8ccd37994b0359a43687fb95c"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"manual-feature-engineering\"></a>\n## Manual Feature Engineering\n\nWe'll add some features which may be informative as to how likely an applicant is to repay their loan:\n\n- The proportion of the applicant's life they have been employed.  If a 23-year-old has only been employed for 4 years, this is fine.  If a 50-year-old has only ever been employed for 4 years, they may have trouble repaying their loan.\n- The ratio of credit to income.  More income than credit will likely help an applicant be able to repay their loan.\n- The ratio of income to annuity.\n- The ratio of income to annuity scaled by age.\n- The ratio of credit to annuity.  If an applicant has a high level of credit relative to their annuity, they may have trouble repaying their loan.\n- The ratio of credit to annuity, scaled by age.  If a young person doesn't have much annuity this doesn't really mean they're less likely to repay their loan."},{"metadata":{"trusted":true,"_uuid":"bc307395a9461e1a3ade2bb98f799c5aef3bcc95"},"cell_type":"code","source":"app['PROPORTION_LIFE_EMPLOYED'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\napp['INCOME_TO_CREDIT_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_CREDIT'] \napp['INCOME_TO_ANNUITY_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_ANNUITY']\napp['INCOME_TO_ANNUITY_RATIO_BY_AGE'] = app['INCOME_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\napp['CREDIT_TO_ANNUITY_RATIO'] = app['AMT_CREDIT'] / app['AMT_ANNUITY']\napp['CREDIT_TO_ANNUITY_RATIO_BY_AGE'] = app['CREDIT_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\napp['INCOME_TO_FAMILYSIZE_RATIO'] = app['AMT_INCOME_TOTAL'] / app['CNT_FAM_MEMBERS']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1494ed2198f37d78528546fb590bbba561fc8df2"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"feature-encoding\"></a>\n## Feature Encoding\n\nSome columns are non-numeric and will have to be encoded to numeric types so that our predictive algorithm can handle them.  We'll encode cyclical variables (like day of the week) into 2 dimensions, encode features with only two possible classes by assigning them 0 or 1, and one-hot encode categorical features with more than two classes.\n\nThe column `WEEKDAY_APPR_PROCESS_START` contains categorical information corresponding to the day of the week.  We could encode these categories as the values 1-7, but this would imply that Sunday and Monday are more similar than, say Tuesday and Sunday.  We could also one-hot encode the column into 7 new columns, but that would create 7 additional dimensions.  Seeing as the week is cyclical, we'll encode this information into two dimensions by encoding them using polar coordinates.  That is, we'll represent the days of the week as a circle.  That way, we can encode the days of the week independently, but only add two dimensions."},{"metadata":{"trusted":true,"_uuid":"2d86fc449a9ae5faa208bd35dc91cd509c5334a0"},"cell_type":"code","source":"# Create map from categories to polar projection\nDOW_map = {\n    'MONDAY':    0,\n    'TUESDAY':   1,\n    'WEDNESDAY': 2,\n    'THURSDAY':  3,\n    'FRIDAY':    4,\n    'SATURDAY':  5,\n    'SUNDAY':    6,\n}\nDOW_map1 = {k: np.cos(2*np.pi*v/7.0) for k, v in DOW_map.items()}\nDOW_map2 = {k: np.sin(2*np.pi*v/7.0) for k, v in DOW_map.items()}\n\n# Show encoding of days of week -> circle\ndays = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']\ntt = np.linspace(0, 2*np.pi, 200)\nxx = np.cos(tt)\nyy = np.sin(tt)\nplt.plot(xx,yy)\nplt.gca().axis('equal')\nplt.xlabel('Encoded Dimension 1')\nplt.ylabel('Encoded Dimension 2')\nplt.title('2D Projection of days of the week')\nfor day in days:\n    plt.text(DOW_map1[day], DOW_map2[day], day, ha='center')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ac9340cde811b82e637d51d57b3f2c3e6c07655"},"cell_type":"code","source":"# WEEKDAY_APPR_PROCESS_START to polar coords\ncol = 'WEEKDAY_APPR_PROCESS_START'\napp[col+'_1'] = app[col].map(DOW_map1)\napp[col+'_2'] = app[col].map(DOW_map2)\napp.drop(columns=col, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08fa08c097ace5bda19092d7250fa445b89217fb"},"cell_type":"markdown","source":"For the housing-related features (e.g. `LIVINGAPARTMENTS_MODE`, `BASEMENTAREA_AVG`, etc) there are combinations of some PREFIX (e.g. `LIVINGAPARTMENTS`,  `BASEMENTAREA`, etc) and some POSTFIX (e.g. `MODE`, `MEDI`, `AVG`, etc) into a variable `PREFIX_POSTFIX`.  However, if one value for a given PREFIX is empty, the other values for that PREFIX will also be empty.  \n\nFor each column which has some empty values, we want to add an indicator column which is 1 if the value in the corresponding column is empty, and 0 otherwise.  However, if we do this with the housing-related features, we'll end up with a bunch of duplicate columns!  This is because the same samples have null values across all the POSTFIX columns for a given PREFIX.   The same problem crops up with the CREDIT_BUREAU-related features. To handle this problem, after creating the null indicator columns, we'll check for duplicate columns and merge them.\n\nSo, first we'll add columns to indicate where there are empty values in each other column."},{"metadata":{"trusted":true,"_uuid":"02acbadff9f44a022fc008c8aeba9ba72b8bd553"},"cell_type":"code","source":"# Add indicator columns for empty values\nfor col in app:\n    if col!='Test' and col!='TARGET':\n        app_null = app[col].isnull()\n        if app_null.sum()>0:\n            app[col+'_ISNULL'] = app_null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d64ad0c6824827fae7a5a04fce45d7768360d71"},"cell_type":"markdown","source":"Then we can label encode categorical features with only 2 possible values (that is, turn the labels into either 0 or 1)."},{"metadata":{"trusted":true,"_uuid":"0167a10c72607282e6c6722d829be3beb146cc06"},"cell_type":"code","source":"# Label encoder\nle = LabelEncoder()\n\n# Label encode binary fearures in training set\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()==2:\n        if col+'_ISNULL' in app.columns: #missing values here?\n            app.loc[app[col+'_ISNULL'], col] = 'NaN'\n        app[col] = le.fit_transform(app[col])\n        if col+'_ISNULL' in app.columns: #re-remove missing vals\n            app.loc[app[col+'_ISNULL'], col] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0627f1939645184f0abc88b39f13367d92264a5"},"cell_type":"markdown","source":"Then we'll one-hot encode the categorical features which have more than 2 possible values."},{"metadata":{"trusted":true,"_uuid":"bff906ff8e92bbf846d43e59fcb0b01150c1545f"},"cell_type":"code","source":"# Get categorical features to encode\ncat_features = []\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()>2:\n        cat_features.append(col)\n\n# One-hot encode categorical features in train set\napp = pd.get_dummies(app, columns=cat_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d531e3fb93e4eddc167aba65efad839f68a6774"},"cell_type":"markdown","source":"And finally we'll remove duplicate columns.  We'll hash the columns and check if the hashes match before checking if all the values actually match, because it's a lot faster than comparing $O(N^2)$ columns elementwise."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9f97dbdb0e4ec6885e533d34891573d57b2b3a73"},"cell_type":"code","source":"# Hash columns\nhashes = dict()\nfor col in app:\n    hashes[col] = sha256(app[col].values).hexdigest()\n    \n# Get list of duplicate column lists\nNcol = app.shape[1] #number of columns\ndup_list = []\ndup_labels = -np.ones(Ncol)\nfor i1 in range(Ncol):\n    if dup_labels[i1]<0: #if not already merged,\n        col1 = app.columns[i1]\n        t_dup = [] #list of duplicates matching col1\n        for i2 in range(i1+1, Ncol):\n            col2 = app.columns[i2]\n            if ( dup_labels[i2]<0 #not already merged\n                 and hashes[col1]==hashes[col2] #hashes match\n                 and app[col1].equals(app[col2])): #cols are equal\n                #then this is actually a duplicate\n                t_dup.append(col2)\n                dup_labels[i2] = i1\n        if len(t_dup)>0: #duplicates of col1 were found!\n            t_dup.append(col1)\n            dup_list.append(t_dup)\n        \n# Merge duplicate columns\nfor iM in range(len(dup_list)):\n    new_name = 'Merged'+str(iM)\n    app[new_name] = app[dup_list[iM][0]].copy()\n    app.drop(columns=dup_list[iM], inplace=True)\n    print('Merged', dup_list[iM], 'into', new_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac970aaf98fba223fd890e7147a0ceefb9a79af7"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"baseline-predictions\"></a>\n## Baseline Predictions\n\nAs a baseline, let's use XGBoost with all the default parameters to predict the probabilities of applicants having trouble repaying their loans."},{"metadata":{"trusted":true,"_uuid":"d06d46ac51599ae674c72da0484bbd29a46e2d96"},"cell_type":"code","source":"# Split data back into test + train\ntrain = app.loc[~app['Test'], :]\ntest = app.loc[app['Test'], :]\n\n# Make SK_ID_CURR the index\ntrain.set_index('SK_ID_CURR', inplace=True)\ntest.set_index('SK_ID_CURR', inplace=True)\n\n# Ensure all data is stored as floats\ntrain = train.astype(np.float32)\ntest = test.astype(np.float32)\n\n# Target labels\ntrain_y = train['TARGET']\n\n# Remove test/train indicator column and target column\ntrain.drop(columns=['Test', 'TARGET'], inplace=True)\ntest.drop(columns=['Test', 'TARGET'], inplace=True)\n\n# Classification pipeline\nxgb_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', XGBClassifier())\n])\n\n# Cross-validated AUROC\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(xgb_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint('Mean AUROC:', scores.mean())\n\n# Fit to training data\nxgb_fit = xgb_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = xgb_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_baseline.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8dfeaa47cb8f1f1d09aa94235ed6a77787a18da"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"calibration\"></a>\n## Calibration\n\nOne problem with the tree-based model is that the predicted probabilities tend to be overconfident.  That is, when the actual probability of class=1 is closer to 0.5, the model predicts probabilities closer to 0 or 1 than 0.5.  We can measure the extent of this overconfidence (or underconfidence) of our classifier by looking at its calibration curve.  The calibration curve plots the probability predicted by our model against the actual probability of samples in that bin.  A model which is perfectly calibrated should show a calibration curve which lies on the identity (y=x) line."},{"metadata":{"trusted":true,"_uuid":"131cb3ca0f16c4709b25ca09429d75b00c5569e1"},"cell_type":"code","source":"# Predict probabilities for the training data\ntrain_pred = cross_val_predict(xgb_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ntrain_pred = train_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfraction_of_positives, mean_predicted_value = \\\n    calibration_curve(train_y, train_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for baseline XGBoost model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a91eeb0cc697aa35f96e779ca53ab603d8681c6"},"cell_type":"markdown","source":"The model is pretty well calibrated as is, exept for at higher predicted probabilities.  We can better calibrate our model by adjusting predicted probabilities to more accurately reflect the probability of loan default.  \n\nThere are two commonly-used methods for model calibration:\n\n1. Sigmoid calibration (aka Platt's scaling, which transforms the model's predictions using a sigmoid so they more accurately reflect the actual probabilities)\n1. Isotonic calibration (which calibrates the model's predictions using a method based on isotonic regression)\n\nWe'll try both methods, and see if either betters the calibration of our model."},{"metadata":{"trusted":true,"_uuid":"ea1cf2f6cf67d8f4ac02d47b63407b1e38c2d0d7"},"cell_type":"code","source":"# Classification pipeline w/ isotonic calibration\ncalib_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='isotonic'))\n])\n\n# Classification pipeline w/ sigmoid calibration\nsig_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='sigmoid'))\n])\n\n# Predict probabilities w/ isotonic calibration\ncalib_pred = cross_val_predict(calib_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ncalib_pred = calib_pred[:,1] #only want p(default)\n\n# Predict probabilities w/ sigmoid calibration\nsig_pred = cross_val_predict(sig_pipeline, \n                             train, \n                             y=train_y,\n                             method='predict_proba')\nsig_pred = sig_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfop_calib, mpv_calib = \\\n    calibration_curve(train_y, calib_pred, n_bins=10)\nfop_sig, mpv_sig = \\\n    calibration_curve(train_y, sig_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.plot(mpv_calib, fop_calib, 's-',\n         label='Calibrated Predictions - isotonic')\nplt.plot(mpv_sig, fop_sig, 's-',\n         label='Calibrated Predictions - sigmoid')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for Calibrated XGBoost model')\nplt.show()\n\n# Cross-validated AUROC for isotonic\nprint('Mean AUROC with isotonic calibration:', \n      roc_auc_score(train_y, calib_pred))\n\n# Cross-validated AUROC for sigmoid\nprint('Mean AUROC with sigmoid calibration:',\n      roc_auc_score(train_y, sig_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5fc4cc8555d7411e5cb5e35ce93e9c86e56a637"},"cell_type":"markdown","source":"Sigmoid calibration didn't appear to work very well in this case...  Isotonic calibration didn't work perfectly either, however it did appear to improve the model's discrimination a small bit (the model without calibration has slightly poorer discrimination in that it is more likely to predict probabilities which are close to 0.5).  Isotonic calibration is usually only recommended if one has $>>1000$ datapoints, which we do (the training set contains around 300,000 datapoins), so we'll go ahead and use isotonic calibration.  Now we can output our predictions after calibrating."},{"metadata":{"trusted":true,"_uuid":"d2e32d52ae44dd8a8999348efc46b51a51c47727"},"cell_type":"code","source":"# Fit to the training data\ncalib_fit = calib_pipeline.fit(train, train_y)\n\n# Predict default probabilities of the test data\ntest_pred = calib_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_calibrated.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c0e74b6db206fee84203dbb9692ff5972b6fdc"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"resampling\"></a>\n## Resampling\n\nThe target class is very imbalanced: many more people successfully repaid their loans than had trouble repaying."},{"metadata":{"trusted":true,"_uuid":"4c3b18085c1a7de46051f4db32047b18f48b6ef0"},"cell_type":"code","source":"# Show distribution of target variable\nsns.countplot(x='TARGET', data=app)\nplt.title('Number of applicants who had trouble repaying')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58811a46c0572302841db86e72147a7066929741"},"cell_type":"markdown","source":"We'll use the [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html) package to re-sample our dataset such that the classes are balanced.  There are several different common methods we could use for re-sampling: \n\n1. Random over-sampling (randomly repeat minority class examples in the training data)\n1. Random under-sampling (randomly drop majority class examples from the training data)\n1. Synthetic minority oversampling technique (SMOTE, generate additional synthetic training examples which are similar to the minority class)\n\nWe'll try all three techniques, and see if any of the techniques give better predictive performance in terms of the AUROC."},{"metadata":{"trusted":true,"_uuid":"e95b3c42f930e5fd8375242cc6108726550e5f80"},"cell_type":"code","source":"# A sampler that doesn't re-sample!\nclass DummySampler(object):\n    def sample(self, X, y):\n        return X, y\n    def fit(self, X, y):\n        return self\n    def fit_sample(self, X, y):\n        return self.sample(X, y)\n    \n# List of samplers to test\nsamplers = [\n    ['Oversampling', RandomOverSampler()], \n    ['Undersampling', RandomUnderSampler()], \n    ['SMOTE', SMOTE()],\n    ['No resampling', DummySampler()]\n]\n\n# Preprocessing pipeline\npre_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\n# Classifier\nclassifier = CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='isotonic')\n\n# Compute AUROC and plot ROC for each type of sampler\nplt.figure()\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\ncv = StratifiedKFold(n_splits=3)\nfor name, sampler in samplers:\n    \n    # Make the sampling and classification pipeline\n    pipeline = make_pipeline(sampler, calib_pipeline)\n\n    # Cross-validated predictions on training set\n    probas = np.zeros(train.shape[0]) # to store predicted probabilities\n    for tr, te in cv.split(train, train_y):\n        test_pre = pre_pipeline.fit_transform(train.iloc[te])  #preprocess test fold\n        train_pre = pre_pipeline.fit_transform(train.iloc[tr]) #preprocess training fold\n        train_s, train_y_s = sampler.fit_sample(train_pre, train_y.iloc[tr]) #resample train fold\n        probas_ = classifier.fit(train_s, train_y_s).predict_proba(test_pre) #predict test fold\n        probas[te] = probas_[:,1]\n    \n    # Print AUROC value\n    print(name, 'AUROC:', roc_auc_score(train_y, probas))\n    \n    # Plot ROC curve for this sampler\n    fpr, tpr, threshs = roc_curve(train_y, probas)\n    plt.plot(fpr, tpr, label=name)\n\nplt.plot([0, 1], [0, 1], label='Chance')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"826e600e54c5fadc7cd65b89f36ee28d61a958eb"},"cell_type":"markdown","source":"Unfortunately it looks like none of the sampling techniques actually helped improve the AUROC score!  The SMOTE resampling technique did even more poorly than simply under- or over-sampling.  This is probably because SMOTE generates samples by interpolating between training samples in feature-space, but most of our features are binary.  So, interpolation isn't really adding diversity to the training data, it's just adding noise and making it more difficult for our classification algorithm to decide where to put a threshold in that dimension."},{"metadata":{"_uuid":"5225207564540f0f72f89efb54c6da209826b7d6"},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"feature-importance\"></a>\n## Feature Importance\n\nFor our final predictions, we'll use the isotonic calibrated model with no resampling (which we've already used to make predictions on the test data, back in the [calibration](#calibration) section), since resampling didn't appear to help increase the preformance of our model.  We can view how important each feature was to the model's predictions by using XGBoost's `plot_importance` function."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e8f23a733cb422b860d0cd3ae1d6561cb8b406c1"},"cell_type":"code","source":"# Fit XGBoost model on the training data\ntrain_pre = pre_pipeline.fit_transform(train) #preprocess training data\nmodel = XGBClassifier()\nmodel.fit(train, train_y)\n\n# Show feature importances\nplt.figure(figsize=(6, 15))\nplot_importance(model, height=0.5, ax=plt.gca())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"059c7827f543d09b385cde6000b334f03428d58d"},"cell_type":"markdown","source":"The three most important factors by far were the three \"external sources.\"  Presumably these were credit scores or some other similar reliability measure from sources outside Home Credit.  The credit-to-annuity ration was also very important, and other factors such as employment length, age, gender - *gender*?"},{"metadata":{"trusted":true,"_uuid":"be1a1e3e1f7ca290515ab6dbbafc8fe687b0fa93"},"cell_type":"code","source":"# Show default probability by gender\nplt.figure()\nsns.barplot(x='CODE_GENDER', y=\"TARGET\", data=train_o)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce83928f36b9e427ef48a9606377612759e2810d"},"cell_type":"markdown","source":"Indeed female applicants only default on their loans around 7% of the time, while male applicants default around 10% of the time."},{"metadata":{"_uuid":"358413ca39f21feb4fdb9ea17a35b697672f81b5"},"cell_type":"markdown","source":"## Conclusion\n\nAfter building a predictive model, there are a few things Home Credit should consider before putting it in to production.  Firstly, because this model could have a direct effect on large number of individuals' financial lives, we need to ensure our model is being equitable, and isn't [discriminating by proxy](https://digitalcommons.law.umaryland.edu/fac_pubs/285/) against certain groups based on race, gender, ethnicity, etc.  Also, because we used applicants' personal information to train the model, we should ensure that those applicants have been informed their information would be used for such a purpose, that they have given consent, and that we have minimized the personally identifiable information present in the dataset.  Still other ethical issues exist which we would want to address before putting our model into production.  There are tools to help us ensure our model and data practices more ethical, such as checklists like [Deon](http://deon.drivendata.org/) and toolkits like the [Ethics and Algorithms Toolkit](https://ethicstoolkit.ai/).\n\nAnother thing to prepare for when considering putting a predictive model into deployment is the possibility of covariate shift or concept drift.  We would want to have a monitoring system in place for a deployed model which could alert us when our data inputs appear to be changing over time, or when our model is no longer fitting the data as well as it used to (or, generally, when things are changing unexpectedly).  For example, if the `EXT_SOURCE` features are indeed credit scores from external agencies, if one of those agencies were to change their scoring system, the score distribution would change, and this would have a large effect on *our* system's predictions.\n\nFinally, remember that the point of building a predictive model to estimate how likely applicants are to pay back their loans is not just for Home Credit Group to use that information to accept or reject applicants.  Rather, they want to be able to predict which principal and payment plan would be the best option for each applicant.  An even more useful model would be one which predicted loan repayment probability given not only the applicant information, but also information about the proposed principal and payment schedule.  This way, Home Credit Group could use the model as a tool to decide not only whether to accept or reject applicants, but to determine the specifics of a loan which would be best for each of their applicants. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}