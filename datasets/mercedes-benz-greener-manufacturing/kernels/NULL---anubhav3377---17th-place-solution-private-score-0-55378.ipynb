{"nbformat_minor": 0, "cells": [{"execution_count": null, "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "4380f32b47860250bd74c5cba49afb683614c615", "_cell_guid": "96c3cd91-d458-4464-8a57-3d15d35a3ac0"}, "source": "This notebook contains my solution that scored 0.58286 on public LB and 0.55378 on private LB, which scores 17th on private leaderboard."}, {"metadata": {"_execution_state": "idle", "_uuid": "e0e061c896b6c6d1d6a7848402d2c2b6171e3e6a", "_cell_guid": "c521dbc9-13b0-4dd4-8ac0-e264726b9587", "trusted": false}, "outputs": [], "cell_type": "code", "execution_count": null, "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom sklearn.linear_model import LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "2a33130ac520ed88176d6cc732c8ac7b712a3ceb", "_cell_guid": "86105479-78c9-4201-b27f-99da340ce4dd", "trusted": false}, "source": "class StackingEstimator(BaseEstimator, TransformerMixin):\n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "c9002e0daccc523d137710b0b0fd8d4dd8415492", "_cell_guid": "24671d37-2a9c-4d2d-97df-a539dbbedb11", "trusted": false}, "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "6e26444514067d6cfc036b55f5b517185cba9f2d", "_cell_guid": "27a1876b-f695-4d6e-b7f5-f8997d85a234", "trusted": false}, "source": "for c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\ntsvd_results_test = tsvd.transform(test)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\npca2_results_test = pca.transform(test)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\nica2_results_test = ica.transform(test)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\ngrp_results_test = grp.transform(test)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\nsrp_results_test = srp.transform(test)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "9cabc608293c7fd0e3ae05bc742ce7df854c7d5b", "_cell_guid": "19adb1ff-61a6-4235-bde5-3ea7b6a84cbc", "trusted": false}, "source": "#save columns list before adding the decomposition components\n\nusable_columns = list(set(train.columns) - set(['y']))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "4a14f2f492338c3f7119821a88943dd1b0337c23", "_cell_guid": "8b199c96-f106-4613-8a3e-696ee9ccd996", "trusted": false}, "source": "leaks = {\n    1:71.34112,\n    12:109.30903,\n    23:115.21953,\n    28:92.00675,\n    42:87.73572,\n    43:129.79876,\n    45:99.55671,\n    57:116.02167,\n    3977:132.08556,\n    88:90.33211,\n    89:130.55165,\n    93:105.79792,\n    94:103.04672,\n    1001:111.65212,\n    104:92.37968,\n    72:110.54742,\n    78:125.28849,\n    105:108.5069,\n    110:83.31692,\n    1004:91.472,\n    1008:106.71967,\n    1009:108.21841,\n    973:106.76189,\n    8002:95.84858,\n    8007:87.44019,\n    1644:99.14157,\n    337:101.23135,\n    253:115.93724,\n    8416:96.84773,\n    259:93.33662,\n    262:75.35182,\n    1652:89.77625\n}\n\nleaky_df = test.ix[(test.ID == 1) | (test.ID==8002) | (test.ID == 259) | (test.ID==262) |\n                   (test.ID == 8007) | (test.ID==72) | (test.ID == 3977) | (test.ID==12) |\n                   (test.ID == 973) | (test.ID==78) | (test.ID == 337) | (test.ID==23) | \n                   (test.ID == 88) | (test.ID==89) | (test.ID == 28) | (test.ID==93) |\n                   (test.ID == 94) | (test.ID==8416) (test.ID == 1644) | (test.ID==104) |\n                   (test.ID == 1001) | (test.ID==42) | (test.ID == 43) | (test.ID==1004) |\n                   (test.ID == 45) | (test.ID==110) | (test.ID == 1008) | (test.ID==1009) |\n                   (test.ID == 1652) | (test.ID==105) | (test.ID == 57) | (test.ID==253)]\n\nleaky_df['y'] = leaky_df.apply(lambda x: leaks[x.ID], axis=1)\n\ntrain = pd.concat([train, leaky_df], axis=0)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "ed97c7e9d56482cd10d6cd8e9e51409133d0e997", "_cell_guid": "68e6cda0-5ba1-4432-90c5-8c2e85011cb1", "trusted": false}, "source": "y_train = train['y'].values\ny_mean = np.mean(y_train)\nid_test = test['ID'].values\n\n#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \nfinaltrainset = train[usable_columns].values\nfinaltestset = test[usable_columns].values"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "83825b7e3569dbcf760886dcec2ba729ac5cf5b7", "_cell_guid": "4eed1b5e-d2b4-489a-9867-41fee9481a60", "trusted": false}, "source": "'''Train the xgb model then predict the test data'''\n\nxgb_params = {\n    'n_trees': 520, \n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.93,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': 100.92, #y_mean, # base prediction = mean(target)\n    'silent': 1\n}"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "d5c86d458112255eed0003c6e4784678b55642d4", "_cell_guid": "fe0bd1e7-a599-4082-acf3-c7cfc6f3b830", "trusted": false}, "source": "train = train.reindex_axis(sorted(train.columns), axis=1)\ntest = test.reindex_axis(sorted(test.columns), axis=1)\n\ndtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\ndtest = xgb.DMatrix(test)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "4428e4beadfe078943b95c2bf0c5f58c77fa3e3c", "_cell_guid": "245cda14-0c8b-4553-9b3b-d242e3441eba", "trusted": false}, "source": "num_boost_rounds = 1250+500\n\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\ny_pred = model.predict(dtest)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "3c9c42e34aea4b7581cf1acca7046dca1e439d65", "_cell_guid": "91c4f500-df1c-4998-8e55-dca7476144ab", "trusted": false}, "source": "'''Train the stacked models then predict the test data'''\n\nstacked_pipeline = make_pipeline(\n    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=5, max_features=0.7,\n                                                          min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n    LassoLarsCV()\n\n)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "31bd33a8454e94283506fedc27f4f9d6d4455dff", "_cell_guid": "2b394441-6055-42fa-92aa-ebee819d4191", "trusted": false}, "source": "stacked_pipeline.fit(finaltrainset, y_train)\nresults = stacked_pipeline.predict(finaltestset)"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "4e524c4df5943eb6325d16786aa0dd7ea2bdd3b5", "_cell_guid": "6ff86285-fed2-4762-ad25-81e25ce5ef0e", "trusted": false}, "source": "sub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = y_pred*0.55 + results*0.45\n\nsub['y'] = sub.apply(lambda x: leaks[x.ID] if x.ID in leaks.keys() else x['y'], axis=1)\n\nsub.to_csv('submission-stacked-models.csv', index=False)"}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}}}