{"cells":[{"metadata":{"_uuid":"ffdb4e921d22ddb2651bcf58f7b86964debc8ad2"},"cell_type":"markdown","source":"# Introduction\nThe purpose of doing this version is for me to get a hang of uNet and the various libraries needed to get a headstart on this project.\n \nI will be copying the code from *https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification* with the purpose of understanding the implementation first."},{"metadata":{"_uuid":"9e56000d5019f686e795e5778d4fc2c566f45a88"},"cell_type":"markdown","source":"# Step 1- Setting up of libraries\nThe code below is the set up of libraries for this project."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n    \nfrom random import randint\nimport matplotlib.pyplot as plt\n# seaborn-white and white control the aesthetics of the plot. They give the background of the plots.\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\n# Trivially, to resize our image to a power of 2 for uNet.\nfrom skimage.transform import resize\n\n# Keras is a Python deep learning library, capable of running on top of TensorFlow, CNTK, or Theano.\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\n# Adam is a stochastic optimization\n# https://arxiv.org/abs/1412.6980v8\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\n# Visualize loops as progress bars\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f89978a806391c225f90e1bbd7bd1e9fe34fd93"},"cell_type":"markdown","source":"**Parameters and helpers**\n\nThere exists 2 methods in the following block of code- upsample and downsample. Trivially, upsample can be understood as resizing the image to become larger (img_size_target), and downsample to be resizing the image to become smaller (img_size_ori)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9ec25d6bbcfdac4878c08000006e4adf9f9f49"},"cell_type":"markdown","source":"# Loading of train and test DataFrames\nThe third line appends the original train_df with depths_df by 'id' indices. The fourth line does a similar thing, though now it does that to train_df. \n\nI'm not sure if this is what we want to do- I was thinking of inputting the depth as a feature in another model but we will see what this does first."},{"metadata":{"trusted":true,"_uuid":"53b8896d7f992daa5db9779fe77decd4ce8d5a8c"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f14fcdb67c73729e6ce350d4b324fceb2892a2e6"},"cell_type":"markdown","source":"# Loading of images\nNote how we divide by 255. This is because in greyscale the range is 0-255, where 0 denotes white and 255 black. We divide by 255 so that our pixel intensity is represented as a percentage between 0-1."},{"metadata":{"trusted":true,"_uuid":"6c90169ac995b940bb347dd54b1810c63f31f62e"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), color_mode=\"grayscale\")) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7159d9273f9a3cf1bd6e907283b7dde90815cab6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only. (I actually count only 10 classes, double check this.)\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"trusted":true,"_uuid":"08a452ac35d553bcee29c3384af8f9fc61646547"},"cell_type":"code","source":"# Calculate the fraction of salt coverage in a mask\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee79ef4143f3ee60cccd7f8d2b148649ffb519cb"},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b26183cde7458c8b9a6c1f1c160c0602f30a56b2"},"cell_type":"code","source":"# Code here giving some warning, let's ignore it for now.\nfig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec209b001d296e0ddb1cb865df3918a51a047300"},"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c832ccf67de365e98a02ec1966d983ddbda7b70d"},"cell_type":"markdown","source":"# Plotting of depth distribution\nPlots of depth distribution for both train and test data."},{"metadata":{"trusted":true,"_uuid":"7aac1c5b50b0954aee0f1e4267ed3082c060203c"},"cell_type":"code","source":"# z is depth in feet of each image (id)\nsns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc60edd8b03198dbbf5bf26dcb5f25ee396ddf4e"},"cell_type":"markdown","source":"# Show some images"},{"metadata":{"trusted":true,"_uuid":"5a068a1dcea66a2559af7dfbd072a7ae3a009a29"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ce1a876fbbb104e6d8d82fbe85bca01353f846b"},"cell_type":"markdown","source":"# Create train and validation split with salt coverage as stratification criteria\nI'm not 100% sure about this but what this method does is that it creates a train and validation data set as we are used to. In order to ensure that the salt coverage in the train and data set has the same representation, we use the \"stratify\" parameter in train_test_split."},{"metadata":{"trusted":true,"_uuid":"c70ecc8dee20b1825900185cacd0a0b02797bb82"},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1338371cd36e780563316075345da7963cdc15b2"},"cell_type":"markdown","source":"# Create an image to check that our upsampling (resizing to a bigger size) works properly\nWe create a matrix filled with 0s (tmp_img), then copy a random image in train_df (the one at index 10 in this case) to this matrix. We then plot this matrix, along with our rescaled one."},{"metadata":{"trusted":true,"_uuid":"202ca2afb908ef54cc2274c4540615271930738c"},"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"961c5898fbe78cf8acfd7a3ee58b811118236acb"},"cell_type":"markdown","source":"# Model building\nWe are using the Unet model. For those who are unfamiliar with NN (or have never built a NN model before), what this section of code achieves is we dictate the parameters of each layer, then append them together to form our NN. (I am unsure about the exact structure of this NN though, will appreciate some help.)\n\nDropout is a form of regularization technique to prevent overfitting. What it does is that it set some of the weights to 0 so that those weights are not used for the forward propagation of that particular layer.\n\nWhat maxpooling does is to apply a sliding window across the image so that we can get a similar representation at a lower computation cost. Follow this link here:\n[www.quora.com/What-is-max-pooling-in-convolutional-neural-networks](http://)"},{"metadata":{"trusted":true,"_uuid":"b2897680cdb4936080c69c39f85e657bdb8bdd05"},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5208a8dcee001e47316d875f111d1266ea044599"},"cell_type":"code","source":"model = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c85538ed982dd969f32fb39b85b76abbecd90f"},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1cc9b56c0090f56aa1d7ab67fd50e3454f71d59"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"591ebf9918b700a044cdcdc624da444787fb4ac0"},"cell_type":"markdown","source":"# Data augmentation\nData augmentation is a popular method in computer vision whereby we \"replicate\" an image (often by flipping or translating) so that our model has more data to train on. This typically allows our model to train better."},{"metadata":{"trusted":true,"_uuid":"2ce8098426d08489b02c246092b5eaafeb9b3fba"},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be6f079d1c4a1062fa7ee5a3c5f4f76a0ebf83d"},"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae873918cf7663a862daada89eda601d0c5cf6ef"},"cell_type":"markdown","source":"# Training\nSeveral common parameters here that you may be unfamiliar with. \n* Early stopping: As its name suggests, the model stops training when certain criteria are met. This is to prevent waste of computation when no obvious improvement to the model is being made. In this case, patience refers to \"number of epochs with no improvement after which training will be stopped\". Verbose refers to the option of seeing the training information (log) being printed to screen.\n* Model checkpoint: Allows us to save the model after every epoch.\n* ReduceLROnPlateau: Reduces our learning rate (factor * old LR) after 5 epochs which show no improvement.\n* Epoch: One epoch refers to 1 forward and backward pass of all training examples."},{"metadata":{"trusted":true,"_uuid":"87bf5e35464b37dcdb4473fb2267e38d2a7feafa"},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2205c52546633890af09c6a4c19fbe3846c59f"},"cell_type":"markdown","source":"# Checking our train and validation loss/accuracy\nNote that parameters like \"loss\", \"val_acc\" are actually given when we train our model."},{"metadata":{"trusted":true,"_uuid":"4824cf11e94aea1aa0de4ad77b8e099c498e8bc8"},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dee64d0d4a2d818d1bd87b5fa99e684932820d1"},"cell_type":"code","source":"# I'm a little unclear on this model. Is this the model that we trained earlier?\nmodel = load_model(\"./keras.model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9fb61086325051e678cf9829a412596cead9618"},"cell_type":"markdown","source":"# Predicting on our validation set\nWe want to plot our predictions out, just to check if our model makes sense. Note that we shade the predicted salt region with red and we actually state our salt coverage (top right of each image)."},{"metadata":{"trusted":true,"_uuid":"13e3f5af657ea9cf1da99cd18e40922c0fb9d175"},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57a8b4577c67dc121b09b6a79f53b6cd97b57d5f"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d1ee4bcff08e2a8e9c87f6c6ce8a69b27a163bb"},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU.\n\nSome terms to clarify:\n* Threshold optimization- The threshold that we learnt in class for classification. \n* IoU- Intersection over union. A metric to gauge accuracy of prediction. More here: [www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/](http://)"},{"metadata":{"trusted":true,"_uuid":"3103d93df25f6d02d449ed3e75746252ee7cdd8c"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4e27db9cde011429a3ea85c93ec72e61a100d02"},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"548036832c4c83bf83863be1888fbafbcc3828a5"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3abd425efc30ea63bc8fa28606d778a1824c5afa"},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e591cd68c572685480b5b87d363ebe51590de79f"},"cell_type":"markdown","source":"# Sanity check using our updated threshold from previous step"},{"metadata":{"trusted":true,"_uuid":"5ceb2ffee99c2d2507ed2e1cd8cac0abaca4bdef"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6effc3ecca67e808e3a224a1f4cec2eca52001c3"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"trusted":true,"_uuid":"b2eaa16bbe68d06104a14e273ecbfc4142c39435"},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1c17e1f62ed665db630611b994fc0ac00c647f"},"cell_type":"code","source":"x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fed6865cc16689444d24a71e4ab05b5214f8de24"},"cell_type":"code","source":"preds_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe68d98b104df7533ede45a9e7ecc967088ee3bf"},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452331b7a4247e686b13de8a8907c8f87d0e39db"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f63013fb3e681a88517efe371d65138510971c0"},"cell_type":"markdown","source":"# And that's it!\nFor our first learning session. Next we will submit our predictions, then try and implement histogram equalization into the pictures."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}