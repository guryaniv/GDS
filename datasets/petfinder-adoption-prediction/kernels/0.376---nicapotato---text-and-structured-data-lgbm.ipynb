{"cells":[{"metadata":{"_uuid":"468d6b0166d1257bb1f5acbcd8425c5ab357da9b"},"cell_type":"markdown","source":"# Text and Structured Data LGBM\n_By Nick Brooks, December 2018_"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas.io.json import json_normalize\nimport os\nimport gc\nimport time\nnotebookstart= time.time()\nprint(\"Data:\\n\",os.listdir(\"../input\"))\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import ensemble\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom nltk.corpus import stopwords \n\n# Viz\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom contextlib import contextmanager\n\n\nimport json\n\nimport scipy as sp\n\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom collections import Counter\n\nimport lightgbm as lgb\nnp.random.seed(369)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e856bca091e26629f23dcb6f28194b950644739"},"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    \"\"\"\n    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    \"\"\"\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\n# Data Visualization\ndef cloud(text, title, size = (10,7)):\n    # Processing Text\n    wordcloud = WordCloud(width=800, height=400,\n                          collocations=True\n                         ).generate(\" \".join(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='w')\n    plt.tight_layout(pad=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60cba250cdab936fcab8ddfe82cb09d80f64a283"},"cell_type":"markdown","source":"#### Main Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/train\"))\ntrain = pd.read_csv(\"../input/train/train.csv\")\ntest = pd.read_csv(\"../input/test/test.csv\")\nbreed_label = pd.read_csv('../input/breed_labels.csv')\n\n\ntrain.Name = train.Name.astype(str)\ntest.Name = test.Name.astype(str)\n\ntrain.Description = train.Description.astype(str)\ntest.Description = test.Description.astype(str)\n\ndisplay(train.sample(4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"796aac36ad3695c81b6a40eeac3806f7000aec47"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54d486dbe7bc8baacf33515b58f493be2d6e3903"},"cell_type":"code","source":"cloud(train.loc[:,\"Description\"].str.title(), title=\"{}\".format(\"Descriptions Word Cloud\"), size=[12,8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f020fbdc63c2ecdd2c5061cd46b635fa81a684be"},"cell_type":"code","source":"for i,name in [(1,\"Male\"),(2,\"Female\")]:\n     cloud(train.loc[train.Gender == i,\"Name\"].str.title(), title=\"{} Pet Names\".format(name), size=[12,8])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ade1548875028ffdf7fc36b04617c2872ebb37e"},"cell_type":"markdown","source":"***\n\n### Sentiment Data"},{"metadata":{"trusted":true,"_uuid":"d8fa585b6c9c550d925d46b125307079721c2b3c"},"cell_type":"code","source":"samplesentiment = pd.read_json('../input/train_sentiment/{}'.format(\"4fdebca57.json\"), orient='index', typ='series')\nprint(\"Document Sentiment\")\nprint(samplesentiment[\"documentSentiment\"])\nprint(\"\\nEntities\")\nprint(samplesentiment[\"entities\"][0])\nprint(\"\\nSentences\")\nprint(samplesentiment['sentences'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ca564943607658222ca1054559f85444d869a4f"},"cell_type":"code","source":"with timer(\"Process Sentiment Train Data\"):\n    sentiment_list = os.listdir('../input/train_sentiment')\n    train_sentiment_df = pd.DataFrame()\n    for i,x in enumerate(sentiment_list):\n        samplesentiment = pd.read_json('../input/train_sentiment/{}'.format(x), orient='index', typ='series')\n\n        sentences = json_normalize(samplesentiment.sentences).loc[:,['sentiment.magnitude', 'sentiment.score']].agg(\n                        {\n                           'sentiment.magnitude' : ['count','mean','std'],\n                           'sentiment.score' : ['mean','std', 'sum']\n\n                        }).unstack().to_frame().sort_index(level=1).T\n        sentences.columns = sentences.columns.map('_'.join)\n\n#         words_salience_type = json_normalize(samplesentiment.entities).loc[:,['name','salience','type']].set_index('name')\\\n#             .unstack().to_frame().sort_index(level=1).T\n#         words_salience_type.columns = words_salience_type.columns.map('_'.join)\n\n        sentiment = pd.concat([json_normalize(samplesentiment[\"documentSentiment\"]),\n                               sentences,\n#                                words_salience_type\n                              ], axis =1)\n#         train_sentiment_df[x[:9]] = sentiment\n        sentiment.index = [x[:9]]\n        train_sentiment_df = pd.concat([train_sentiment_df, sentiment], axis =0)\n\n    display(train_sentiment_df.sample(5))\n    train = train.join(train_sentiment_df, on = \"PetID\")\n    del train_sentiment_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afd9ad50266d284c76ea936cfef462fe3364d4b8"},"cell_type":"code","source":"with timer(\"Process Sentiment Test Data\"):\n    sentiment_list = os.listdir('../input/test_sentiment/')\n    test_sentiment_df = pd.DataFrame()\n    for i,x in enumerate(sentiment_list):\n        samplesentiment = pd.read_json('../input/test_sentiment/{}'.format(x), orient='index', typ='series')\n        sentences = json_normalize(samplesentiment.sentences).loc[:,['sentiment.magnitude', 'sentiment.score']].agg(\n                        {\n                           'sentiment.magnitude' : ['count','mean','std'],\n                           'sentiment.score' : ['mean','std', 'sum']\n\n                        }).unstack().to_frame().sort_index(level=1).T\n        sentences.columns = sentences.columns.map('_'.join)\n\n#         words_salience_type = json_normalize(samplesentiment.entities).loc[:,['name','salience','type']].set_index('name')\\\n#             .unstack().to_frame().sort_index(level=1).T\n#         words_salience_type.columns = words_salience_type.columns.map('_'.join)\n\n        sentiment = pd.concat([json_normalize(samplesentiment[\"documentSentiment\"]),\n                               sentences,\n#                                words_salience_type\n                              ], axis =1)\n#         test_sentiment_df[x[:9]] = sentiment\n        sentiment.index = [x[:9]]\n        test_sentiment_df = pd.concat([test_sentiment_df, sentiment], axis =0)\n\n    display(test_sentiment_df.sample(5))\n\n    test = test.join(test_sentiment_df, on = \"PetID\")\n    del test_sentiment_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4214b8bc6a90e9b549dd93e58cb0ae95d7f4432"},"cell_type":"markdown","source":"### Image Metadata\n\nNot using this, just for exploration."},{"metadata":{"trusted":true,"_uuid":"fa068a8120ca35cc6676d72436ed76e6b66abc6a","scrolled":false},"cell_type":"code","source":"sample_image_metadata = pd.read_json('../input/train_metadata/{}'.format(\"c161afd26-3.json\"), orient='index', typ='series')\n\nprint(\"Label Annotations\")\nprint(sample_image_metadata['labelAnnotations'])\nprint(\"\\nImage Properties Annotations\")\nprint(sample_image_metadata['imagePropertiesAnnotation'])\nprint(\"\\nCrop Hints Annotation\")\nprint(sample_image_metadata['cropHintsAnnotation'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1f3ea2c5ec92bcb1497464b585b254de849adb8"},"cell_type":"markdown","source":"### Text and Breed Processing"},{"metadata":{"trusted":true,"_uuid":"dd286804b6dbf7404e9da5a681fc6b51b0e6f238"},"cell_type":"code","source":"train.set_index(\"PetID\", inplace= True)\ntest.set_index(\"PetID\", inplace= True)\n\ntraindex = train.index\ntestdex = test.index\n\ny = train.AdoptionSpeed\ntrain.drop('AdoptionSpeed', axis =1 , inplace=True)\n\ndf = pd.concat([train,test], axis = 0)\n\n# Join Breed Dataset\ndf = df.merge(breed_label, left_on = 'Breed1', right_on = 'BreedID', how = 'left', suffixes = ['','b1']).drop('BreedID',axis=1)\ndf.rename(columns = {'BreedName':'b1name'}, inplace=True)\ndf = df.merge(breed_label, left_on = 'Breed2', right_on = 'BreedID', how = 'left', suffixes = ['','b2']).drop('BreedID',axis=1)\ndf.rename(columns = {'BreedName':'b2name'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1665c5738afc44fe6bb30b3bea85694d7791c42"},"cell_type":"markdown","source":"#### Label Encoding"},{"metadata":{"trusted":true,"_uuid":"4488f952b45c7036398a6149701928c1b9a4d80c"},"cell_type":"code","source":"print(\"Encode Variables\")\ncategorical = [\"Name\"]\nprint(\"Encoding :\",categorical)\n\ndf.Name = df.Name.str.lower()\n\n# Encoder:\nlbl = preprocessing.LabelEncoder()\nfor col in categorical:\n    df[\"encoded_\" + col] = lbl.fit_transform(df[col].astype(str))\n    \ndf.drop('RescuerID', axis =1 ,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1220b463be83447f07a5eaf52b0afd2a0487dcf8"},"cell_type":"markdown","source":"#### Meta Text Feautres"},{"metadata":{"trusted":true,"_uuid":"e80f555db85050bb7ffcc4bfc1e41fefbe40f645"},"cell_type":"code","source":"textfeats = [\"Description\"]\n\nfor cols in textfeats:\n    df[cols] = df[cols].astype(str) \n    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5dc336032f117cd8a4372c95f931f25915e80b"},"cell_type":"code","source":"# Combine Secondary Text Features into one\ndf['name_and_breeds'] = df['Name'].astype(str) + ' ' +  df['b1name'].astype(str) + ' ' + df['b2name'].astype(str)\ndf['name_and_breeds'] = df['name_and_breeds'].str.lower()\n\ndf.drop(['Name', 'b1name','b2name'], axis =1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"715536ddb9c9a4e85df0c1a8ad423699462ac98a"},"cell_type":"markdown","source":"#### TF-IDF"},{"metadata":{"trusted":true,"_uuid":"a8725403d295cd880a18210127cfe257b2f7fdd8"},"cell_type":"code","source":"# Distinguish Train / Test\ntrain = df.iloc[0:len(traindex),:]\ntest = df.iloc[len(traindex):,:]\ndel df; gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2532e0bb4c9880cba7eb472e77ca7781e43c82bd"},"cell_type":"code","source":"word_vect = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_features=20000)\n\nchar_vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            ngram_range=(2,5),\n            max_features=3000)\n\nwith timer(\"Word Grams TFIDF\"):\n    word_vect.fit(train['Description'])\n    train_word_features  = word_vect.transform(train['Description'])\n    test_word_features  = word_vect.transform(test['Description'])\n\nwith timer(\"Character Grams TFIDF\"):\n    char_vectorizer.fit(train['name_and_breeds'])\n    train_char_features = char_vectorizer.transform(train['name_and_breeds'])\n    test_char_features = char_vectorizer.transform(test['name_and_breeds'])\n\n# Get Structured Data    \nnum_features = [f_ for f_ in train.columns\n            if f_ not in [\"Description\", \"name_and_breeds\"]]\n    \n# Get Sparse Matrix Feature Names..\nfeature_names = (word_vect.get_feature_names() +\n                char_vectorizer.get_feature_names() +\n                num_features)\n\nwith timer(\"Sparse Combine\"):\n    X = hstack(\n        [\n            train_char_features,\n            train_word_features,\n            train[num_features]\n        ]\n    ).tocsr()\n\n    del train_char_features\n    gc.collect()\n\n    testing = hstack(\n        [\n            test_char_features,\n            test_word_features,\n            test[num_features]\n        ]\n    ).tocsr()\n# del train, test\ndel test_char_features; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fdb11ffc47bd49f8e877810c657554cd7703ad4"},"cell_type":"markdown","source":"## LGBM"},{"metadata":{"trusted":true,"_uuid":"90b9a870cc5e322b5e74b26afc96c3b52dfd54be"},"cell_type":"code","source":"for shape in [X,testing]:\n    print(\"{} Rows and {} Cols\".format(*shape.shape))\n    \nprint(\"Feature Count:\", len(feature_names))\nprint(\"Dependent Variable Len\", y.shape[0])\n\ncontinuous_vars = ['Age','Quantity', 'Fee',\n       'VideoAmt', 'PhotoAmt', 'magnitude', 'score',\n       'sentiment.magnitude_count', 'sentiment.score_count',\n       'sentiment.magnitude_mean', 'sentiment.score_mean',\n       'sentiment.magnitude_std', 'sentiment.score_std',\n       'sentiment.magnitude_sum', 'sentiment.score_sum', 'Description_num_words',\n       'Description_num_unique_words','Description_words_vs_unique']\ncategorical_vars = [x for x in num_features if x not in continuous_vars]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e49759da5300bec6e089d2980b711c05fedc7827","scrolled":false},"cell_type":"code","source":"lgtrain = lgb.Dataset(X, label=y, feature_name= feature_names,\n                      free_raw_data=False, categorical_feature = categorical_vars)\nfeature_importance_df = pd.DataFrame()\nn_fold = 0\nprint(\"Light Gradient Boosting Classifier: \")\n\nlgbm_params = {\n          'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 150,\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.01,\n          'min_child_samples': 150,\n          'min_child_weight': 0.1,\n          'verbosity': -1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 3000}\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nfold_preds = np.zeros((testing.shape[0],))\noof_preds = np.zeros((X.shape[0],))\nall_coefficients = np.zeros((5, 4))\nlgtrain.construct()\n\n# Fit 5 Folds\nmodelstart = time.time()\nfor trn_idx, val_idx in folds.split(X, y):\n    clf = lgb.train(\n        params=lgbm_params,\n        train_set=lgtrain.subset(trn_idx),\n        valid_sets=lgtrain.subset(val_idx),\n        categorical_feature = categorical_vars,\n        num_boost_round=1500, \n        early_stopping_rounds=150,\n        verbose_eval=300\n    )\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feature_names\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = n_fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # Creds to Abhishek\n    # https://www.kaggle.com/abhishek/maybe-something-interesting-here/notebook\n    pred_test_y = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(pred_test_y, y.iloc[val_idx].values)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    all_coefficients[n_fold, :] = coefficients\n    print(\"Valid Counts = \", Counter(y.iloc[val_idx].values))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(y.iloc[val_idx].values, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n#     pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    fold_preds += clf.predict(testing, num_iteration=clf.best_iteration) / folds.n_splits\n    n_fold += 1\n\nprint(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"882be695fb36c82f294643e04535d8355a9bb48e"},"cell_type":"code","source":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout\n    plt.savefig('lgbm_importances01.png')\ndisplay_importances(feature_importance_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd2ba0d33b60e9b34807fec53dd82094fdd996cc"},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(all_coefficients, axis=0)\nprint(coefficients_)\n\noptR = OptimizedRounder()\ntest_predictions = optR.predict(fold_preds, coefficients_).astype(int)\nCounter(test_predictions)\n\n# quadratic_weighted_kappa(target, train_predictions)\n# rmse(target, [r[0] for r in results['train']])\nsubmission = pd.DataFrame({'PetID': testdex, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a6de5fae4ba47b52bfecc075f615b7d0c34163d"},"cell_type":"code","source":"print(\"Train Dependent Variable Class Distribution:\\n\", y.value_counts())\nprint(\"Prediction Dependent Variable Class Distribution:\\n\", submission['AdoptionSpeed'].value_counts())\n\nprint(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}