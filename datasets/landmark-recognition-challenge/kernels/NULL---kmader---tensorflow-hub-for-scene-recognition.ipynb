{"cells":[{"metadata":{"_uuid":"85d48c57413d4575b427febbb334694794a9d08e"},"cell_type":"markdown","source":"# Overview\nThe kaggle kernel-based version of the tf-hub notebook [here](https://github.com/tensorflow/hub/blob/master/examples/colab/tf_hub_delf_module.ipynb). The example uses the pretrained [DELF](https://github.com/tensorflow/models/tree/master/research/delf) feature detector for the recognition\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image, ImageOps\nfrom scipy.spatial import cKDTree\nfrom skimage.feature import plot_matches\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\nfrom six import BytesIO\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom six.moves.urllib.request import urlopen","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f04c9a5bed8833f0058ea8b3d802b3246685c91"},"cell_type":"markdown","source":"# Demo Images"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"MAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/2/28/Bridge_of_Sighs%2C_Oxford.jpg'\nIMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/c/c3/The_Bridge_of_Sighs_and_Sheldonian_Theatre%2C_Oxford.jpg'\n\nIMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/1/1e/Golden_gate2.jpg'\nIMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/3/3e/GoldenGateBridge.jpg'\n\nIMAGE_1_URL = 'https://upload.wikimedia.org/wikipedia/commons/c/ce/2006_01_21_Ath%C3%A8nes_Parth%C3%A9non.JPG'\nIMAGE_2_URL = 'https://upload.wikimedia.org/wikipedia/commons/5/5c/ACROPOLIS_1969_-_panoramio_-_jean_melis.jpg'\n\nIMAGE_1_JPG = 'image_1.jpg'\nIMAGE_2_JPG = 'image_2.jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d2b5c7330214adf9b6f43d977e14f5510ac1ec4"},"cell_type":"code","source":"def download_and_resize_image(url, filename, new_width=256, new_height=256):\n    response = urlopen(url)\n    image_data = response.read()\n    pil_image = Image.open(BytesIO(image_data))\n    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n    pil_image_rgb = pil_image.convert('RGB')\n    pil_image_rgb.save(filename, format='JPEG', quality=90)\n\ndownload_and_resize_image(IMAGE_1_URL, IMAGE_1_JPG)\ndownload_and_resize_image(IMAGE_2_URL, IMAGE_2_JPG)\n\ndef show_images(image_path_list):\n    plt.figure()\n    for i, image_path in enumerate(image_path_list):\n        plt.subplot(1, len(image_path_list), i+1)\n        plt.imshow(np.asarray(Image.open(image_path)))\n        plt.title(image_path)\n        plt.grid(False)\n        plt.yticks([])\n        plt.xticks([])\n    plt.show()\n\nshow_images([IMAGE_1_JPG, IMAGE_2_JPG])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74db27d63dd51eb9f7695671fdf74e8d9e646401"},"cell_type":"code","source":"def image_input_fn():\n    filename_queue = tf.train.string_input_producer(\n      [IMAGE_1_JPG, IMAGE_2_JPG], shuffle=False)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    image_tf = tf.image.decode_jpeg(value, channels=3)\n    return tf.image.convert_image_dtype(image_tf, tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3958818082495ab928fd1458274b46677eb2fdf1"},"cell_type":"markdown","source":"# Apply the DELF module to the data\nThe DELF module takes an image as input and will describe noteworthy points with vectors. "},{"metadata":{"trusted":true,"_uuid":"67df8b2cdcadde0b38a8ea75610f07e245ddc285"},"cell_type":"code","source":"tf.reset_default_graph()\ntf.logging.set_verbosity(tf.logging.FATAL)\n\nm = hub.Module('https://tfhub.dev/google/delf/1')\n\n# The module operates on a single image at a time, so define a placeholder to\n# feed an arbitrary image in.\nimage_placeholder = tf.placeholder(\n    tf.float32, shape=(None, None, 3), name='input_image')\n\nmodule_inputs = {\n    'image': image_placeholder,\n    'score_threshold': 100.0,\n    'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\n    'max_feature_num': 1000,\n}\n\nmodule_outputs = m(module_inputs, as_dict=True)\n\nimage_tf = image_input_fn()\n\nwith tf.train.MonitoredSession() as sess:\n    results_dict = {}  # Stores the locations and their descriptors for each image\n    for image_path in [IMAGE_1_JPG, IMAGE_2_JPG]:\n        image = sess.run(image_tf)\n        print('Extracting locations and descriptors from %s' % image_path)\n        results_dict[image_path] = sess.run(\n            [module_outputs['locations'], module_outputs['descriptors']],\n            feed_dict={image_placeholder: image})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"430c095536220c717e08449a341927188f53af1c"},"cell_type":"markdown","source":"## Post-processing and visualization"},{"metadata":{"trusted":true,"_uuid":"a14c583beac094f2cdf9306514cd644e57f15fb2"},"cell_type":"code","source":"def match_images(results_dict, image_1_path, image_2_path):\n    distance_threshold = 0.8\n\n    # Read features.\n    locations_1, descriptors_1 = results_dict[image_1_path]\n    num_features_1 = locations_1.shape[0]\n    print(\"Loaded image 1's %d features\" % num_features_1)\n    locations_2, descriptors_2 = results_dict[image_2_path]\n    num_features_2 = locations_2.shape[0]\n    print(\"Loaded image 2's %d features\" % num_features_2)\n\n    # Find nearest-neighbor matches using a KD tree.\n    d1_tree = cKDTree(descriptors_1)\n    _, indices = d1_tree.query(\n      descriptors_2, distance_upper_bound=distance_threshold)\n\n    # Select feature locations for putative matches.\n    locations_2_to_use = np.array([\n      locations_2[i,]\n      for i in range(num_features_2)\n      if indices[i] != num_features_1\n    ])\n    locations_1_to_use = np.array([\n      locations_1[indices[i],]\n      for i in range(num_features_2)\n      if indices[i] != num_features_1\n    ])\n\n    # Perform geometric verification using RANSAC.\n    _, inliers = ransac(\n      (locations_1_to_use, locations_2_to_use),\n      AffineTransform,\n      min_samples=3,\n      residual_threshold=20,\n      max_trials=1000)\n\n    print('Found %d inliers' % sum(inliers))\n\n    # Visualize correspondences.\n    _, ax = plt.subplots()\n    img_1 = mpimg.imread(image_1_path)\n    img_2 = mpimg.imread(image_2_path)\n    inlier_idxs = np.nonzero(inliers)[0]\n    plot_matches(\n      ax,\n      img_1,\n      img_2,\n      locations_1_to_use,\n      locations_2_to_use,\n      np.column_stack((inlier_idxs, inlier_idxs)),\n      matches_color='b')\n    ax.axis('off')\n    ax.set_title('DELF correspondences')\n\nmatch_images(results_dict, IMAGE_1_JPG, IMAGE_2_JPG)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0954a481ca99232dce23fdfae2c01083fc4f0f6e"},"cell_type":"markdown","source":"## Apply to Contest Data\nHere we can try to apply the model to some contest data"},{"metadata":{"trusted":true,"_uuid":"02e6b82454e7c6145b78355ea404b01586cd19f3"},"cell_type":"code","source":"import pandas as pd\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4145c76bbd2d2651cc766a30dfed06513548fd75"},"cell_type":"code","source":"for _, (c_id, c_df) in zip(range(3), train_df.groupby('landmark_id')):\n    print(c_id)\n    # take the first 2 urls\n    url_1, url_2, *_ = c_df['url'].values\n    download_and_resize_image(url_1, IMAGE_1_JPG)\n    download_and_resize_image(url_2, IMAGE_2_JPG)\n    with tf.train.MonitoredSession() as sess:\n        results_dict = {}  # Stores the locations and their descriptors for each image\n        for image_path in [IMAGE_1_JPG, IMAGE_2_JPG]:\n            image = sess.run(image_tf)\n            print('Extracting locations and descriptors from %s' % image_path)\n            results_dict[image_path] = sess.run(\n                [module_outputs['locations'], module_outputs['descriptors']],\n                feed_dict={image_placeholder: image})\n    match_images(results_dict, IMAGE_1_JPG, IMAGE_2_JPG)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9d763719ed48d0c89e87c43da64ff1b66301467"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0a5e09e20cac029a1bded52840cfe7816e188167"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}