{"nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "version": "3.6.4", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1, "cells": [{"source": ["## Hello!\n", "\n", "In this simple baseline notebook I will generate simple features from text to try predict the labels. I will use a combination of TfIdfVectorizer, SVD and XGBoost."], "cell_type": "markdown", "metadata": {"_cell_guid": "b398f7d2-a0f6-4dd2-ab1b-0e49c29ea496", "_uuid": "cbf4397e8f23371c3c0f234e2d9d4856de8d6ed2"}}, {"source": ["import numpy as np\n", "import pandas as pd \n", "from subprocess import check_output\n", "from gensim.models import Word2Vec\n", "\n", "from nltk.tokenize import RegexpTokenizer\n", "from nltk import WordNetLemmatizer\n", "from nltk.corpus import stopwords\n", "\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.preprocessing import Normalizer\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import KFold, train_test_split\n", "\n", "from sklearn.metrics import f1_score, accuracy_score\n", "\n", "import xgboost as xgb\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n", "\n", "alpha_tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n", "lemmatizer = WordNetLemmatizer()\n", "stop = stopwords.words('english')\n", "\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns"], "metadata": {"_cell_guid": "3ebd5d8f-a710-42f7-8cac-b7f6561e996c", "_uuid": "e51dc9c2f054c7b6f0422bd5831f255a17047c5d", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "submissions = pd.read_csv('../input/sample_submission.csv')\n", "\n", "train.comment_text.fillna('', inplace=True)\n", "test.comment_text.fillna('', inplace=True)"], "metadata": {"_cell_guid": "be0df950-015a-43b3-904d-9c7248913ecd", "_uuid": "dfc122975a7c7905f842d75387020e2675497944", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["How does the data look?"], "cell_type": "markdown", "metadata": {"_cell_guid": "77356f64-0cfe-46b9-b74a-3ca1b2601840", "_uuid": "8168f91219cbce511abde4d1e428560f2f298d4d"}}, {"source": ["train.head()"], "metadata": {"_cell_guid": "84161fd2-a950-4064-a4ba-91bcf6d14965", "_uuid": "6cf335242b797a2a48ce0c3b227d384ce784d57a", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["plt.figure(figsize=(12,6))\n", "sns.barplot(train.toxic.value_counts().index, train.toxic.value_counts().values, alpha=0.8)\n", "plt.ylabel('Amount of class instances', fontsize=16)\n", "plt.xlabel('Class', fontsize=16)\n", "plt.show();"], "metadata": {"_cell_guid": "7e4a3a62-15fd-4e77-b7f9-8a248764403f", "_uuid": "2fbb3b48f0c968f3010499769d713ee9fa56dfad", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["plt.figure(figsize=(12,6))\n", "sns.barplot(train.obscene.value_counts().index, train.obscene.value_counts().values, alpha=0.8)\n", "plt.ylabel('Amount of class instances', fontsize=16)\n", "plt.xlabel('Class', fontsize=16)\n", "plt.show();"], "metadata": {"_cell_guid": "f24a272d-940a-4b39-964c-92fcaca24e59", "_uuid": "82c489e6bb33b3583080fe7d42e59be76ad1a648", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["There are a lot of different ways to generate features through different vectorizers based on count-matrices and co-ocurrence-matrices."], "cell_type": "markdown", "metadata": {"_cell_guid": "5d924eb4-55e5-4098-9df4-a2b1a8b4c54e", "_uuid": "87f74a58c3af0f4ce7cc5a6161f10931b526730f"}}, {"source": ["vectorizers = [ ('3-gram TF-IDF Vectorizer on words', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n", "               ('3-gram Count Vectorizer on words', CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n", "               ('3-gram Hashing Vectorizer on words', HashingVectorizer(ngram_range=(1, 5), analyzer='word', binary=False)),\n", "                ('TF-IDF + SVD', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n", "                                ('svd', TruncatedSVD(n_components=150)),\n", "                               ])),\n", "               ('TF-IDF + SVD + Normalizer', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n", "                                ('svd', TruncatedSVD(n_components=150)),\n", "                                ('norm', Normalizer()),\n", "                               ]))\n", "              ]"], "metadata": {"_cell_guid": "184ce1f4-d100-497f-9b60-90b88243166a", "_uuid": "23c413d42c1c01a1382ed45d250c5baaf7e383b1", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["estimators = [\n", "              (KNeighborsClassifier(n_neighbors=3), 'K-Nearest Neighbors', 'yellow'),\n", "              (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False), 'Support Vector Machine', 'red'),\n", "              (LogisticRegression(tol=1e-8, penalty='l2', C=0.1), 'Logistic Regression', 'green'),\n", "              (MultinomialNB(), 'Naive Bayes', 'magenta'),\n", "              (RandomForestClassifier(n_estimators=10, criterion='gini'), 'Random Forest', 'gray'),\n", "              (None, 'XGBoost', 'pink')\n", "]"], "metadata": {"_cell_guid": "5b5175e9-061f-4952-b3a1-43fe186c9ea9", "_uuid": "8ddf69b73342cf8570eec6b78e6c27cec25da503", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["params = {}\n", "params['objective'] = 'multi:softprob'\n", "params['eta'] = 0.1\n", "params['max_depth'] = 3\n", "params['silent'] = 1\n", "params['num_class'] = 3\n", "params['eval_metric'] = 'mlogloss'\n", "params['min_child_weight'] = 1\n", "params['subsample'] = 0.8\n", "params['colsample_bytree'] = 0.3\n", "params['seed'] = 0"], "metadata": {"_cell_guid": "6640e1e7-41be-4c95-8d2e-84f7f1bcf00a", "_uuid": "5b87eede05110bf8ba94226bf4a5f5bd71474f2c", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["I will try to compare all of them splitting training set to the train chunk and to the test chunk."], "cell_type": "markdown", "metadata": {"_cell_guid": "f6edf577-e57d-43c9-94df-ebe3d710d799", "_uuid": "5c0b1a4667c39aa4b8c6cf85da98f8d8e7f98f8b"}}, {"source": ["def vectorize():\n", "\n", "    test_size = 0.3\n", "\n", "    train_split, test_split = train_test_split(train[:10], test_size=test_size)\n", "\n", "    for column in range(2, len(train.axes[1])):\n", "        for vectorizer in vectorizers:\n", "            print(vectorizer[0] + '\\n')\n", "            X = vectorizer[1].fit_transform(train.comment_text.values)\n", "            X_train, X_test = train_test_split(X, test_size=test_size)\n", "            for estimator in estimators:\n", "                if estimator[1] == 'XGBoost': \n", "                    xgtrain = xgb.DMatrix(X_train, train_split.iloc[:,column].values)\n", "                    xgtest = xgb.DMatrix(X_test)\n", "                    model = xgb.train(params=list(params.items()), dtrain=xgtrain,  num_boost_round=40)\n", "                    predictions = model.predict(xgtest, ntree_limit=model.best_ntree_limit).argmax(axis=1)\n", "                else:\n", "                    estimator[0].fit(X_train, train_split.iloc[:,column].values)\n", "                    predictions = estimator[0].predict(X_test)\n", "                print(accuracy_score(predictions, test_split.iloc[:,column].values), estimator[1])"], "metadata": {"_cell_guid": "60f9b52f-6eea-4040-bce5-4e78cfef890e", "_uuid": "38a490080c3318930d05e43465c0c20f9d3dc94a", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["To make a quick submission I will just make a simple pre-processing on the given text data and use a TfIDfVectorizer on words uni-grams."], "cell_type": "markdown", "metadata": {"_cell_guid": "a271b8d3-b860-4a34-98a4-169b544b8d72", "_uuid": "cf01e586e19b1f6cb98b62e618c9f1b9f498df1b"}}, {"source": ["train_text = [' '.join([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop]) for sent in train.comment_text.values]\n", "test_text = [' '.join([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop]) for sent in test.comment_text.values]"], "metadata": {"_cell_guid": "990c52ac-5c44-4723-bb32-6a29854da207", "_uuid": "3ebe0b2f1a94b59e830f0920cb325d2ff87a0e01", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["vectorizer = TfidfVectorizer(ngram_range=(1,1), analyzer='word')\n", "\n", "full = vectorizer.fit_transform(train_text + test_text)\n", "X_train = vectorizer.transform(train_text)\n", "X_test = vectorizer.transform(test_text)\n", "\n", "# NUM_FEATURES = 100\n", "\n", "# model = Word2Vec(train_text + test_text, min_count=2, size=NUM_FEATURES, window=4, sg=1, alpha=1e-4, workers=4)\n", "\n", "# def get_feature_vec(tokens, num_features, model):\n", "#     featureVec = np.zeros(shape=(1, num_features), dtype='float32')\n", "#     missed = 0\n", "#     for word in tokens:\n", "#         try:\n", "#             featureVec = np.add(featureVec, model[word])\n", "#         except KeyError:\n", "#             missed += 1\n", "#             pass\n", "#     if len(tokens) - missed == 0:\n", "#         return np.zeros(shape=(num_features), dtype='float32')\n", "#     return np.divide(featureVec, len(tokens) - missed).squeeze()\n", "\n", "# train_vectors = []\n", "# for i in train_text:\n", "#     train_vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))\n", "    \n", "# test_vectors = []\n", "# for i in test_text:\n", "#     test_vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))"], "metadata": {"_cell_guid": "9dbbba0d-0e8d-4c7f-9bfb-9fc7cebcce31", "_uuid": "94338e873ffb1c23de4159cbaca75d9d8e044d64", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["X_train = train_vectors\n", "X_test = test_vectors"], "metadata": {"_cell_guid": "aae4915a-8338-4a63-98b0-57e5fd0628ab", "_uuid": "2e01ff4008cc3f3a9cc6e608b11e96fba0b4a9a8", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Then I will be able to give predictions using gradient boosting:"], "cell_type": "markdown", "metadata": {"_cell_guid": "d970b464-3305-469f-b48f-8d7d3f078c65", "_uuid": "5ba7c44abc7508a948648fef365c43022b0b160b"}}, {"source": ["start = 2\n", "predictions = np.zeros((len(test), len(train.axes[1]) - start))\n", "\n", "for column in range(start, len(train.axes[1])):\n", "    y_train = train.iloc[:,column].values\n", "    # estimator = LogisticRegression(C = 0.01)\n", "    # estimator.fit(X_train, y_train)\n", "    # predictions[:,column - start] = estimator.predict_proba(X_test)[:,1]\n", "    xgtrain = xgb.DMatrix(X_train, y_train)\n", "    xgtest = xgb.DMatrix(X_test)\n", "    model = xgb.train(params=list(params.items()), dtrain=xgtrain, num_boost_round=500)\n", "    predictions[:,column - start]  = model.predict(xgtest, ntree_limit=model.best_ntree_limit)[:,1]"], "metadata": {"_cell_guid": "247147a3-e704-4aea-8540-1c58bd7aeef0", "_uuid": "bb8358e7b3d2cbde09c51806811c52a65ea36b48", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["result = pd.concat([pd.DataFrame({'id': submissions['id']}), pd.DataFrame(predictions, columns = train.columns.values[2:])], axis=1)\n", "result.to_csv('submission.csv', index=False)"], "metadata": {"_cell_guid": "280bc746-ad91-476a-ba49-6e252c61ace0", "_uuid": "ba99d8c5e37cafa727514bc7343a06bb74bdfd25", "collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Of course, this is not all, and this kernel is just a simple baseline. I am working on a more interesting model right now, and I will start with more powerful methods of vectorizing the text data like Doc2Vec or FastText -- stay tuned! I will be glad to hear your comments and suggestions!"], "cell_type": "markdown", "metadata": {"_cell_guid": "74750127-85a2-4bd5-99fe-e5d5f92421a0", "_uuid": "48c985e6ae08814ca5369e5644be6153e2cdefe1"}}]}