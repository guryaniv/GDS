{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "_is_fork": false, "language_info": {"name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "file_extension": ".py"}, "_change_revision": 0}, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "8d8bcfdaf0c2104442af8cb7458fc7aade4a7d83", "_cell_guid": "d0a037a6-6e20-ab59-2455-c6e28ae3d056"}, "execution_count": null, "source": "# Different regularized regression tests\n#### Some functions and examples used are courtesy of Datacamp (www.datacamp.com)", "outputs": [], "cell_type": "markdown"}, {"metadata": {"trusted": false, "_uuid": "3eb4a5eee649555efc61d8fd77da2f9318025d42", "_cell_guid": "cf98585a-c141-0f8b-ba57-53b9c1ca280f"}, "execution_count": null, "source": "# Importing main packages and settings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "cdf16df1ee6fc832ccbb58daef2a5a4dbf19c814", "_cell_guid": "6f89960e-22b1-4d20-c90a-d1c1cb4fa08c"}, "execution_count": null, "source": "# Import the relevant sklearn packages\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel, VarianceThreshold, SelectKBest, f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV, LarsCV, LassoLarsCV\nfrom sklearn.linear_model import MultiTaskElasticNetCV, MultiTaskLassoCV, OrthogonalMatchingPursuitCV\nfrom sklearn.metrics import mean_squared_error", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "b371c750c97e98942c1bc3f3fb96e28bf7398025", "_cell_guid": "d9ca82b5-c8e4-b47d-2019-702e543e7ed9"}, "execution_count": null, "source": "# Function for plotting the scores for different alphas used in Ridge regression\ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "34995653b70a20f6bf7d57dff1cf659ab8a4b1ec", "_cell_guid": "63babe32-7650-0ab2-ccaa-4c7ba2655366"}, "execution_count": null, "source": "# Loading the training dataset\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "46185cabcb424d1b3b65cc9c5a3374e8ada9c423", "_cell_guid": "7aa8abd6-5992-a0ed-79c6-f78fe3cd0cfc"}, "execution_count": null, "source": "# turning object features into dummy variables\ndf_train_dummies = pd.get_dummies(df_train, drop_first=True)\ndf_test_dummies = pd.get_dummies(df_test, drop_first=True)\n\n# dropping ID and the target variable\ndf_train_dummies = df_train_dummies.drop(['ID','y'], axis=1)\ndf_test_dummies = df_test_dummies.drop('ID', axis=1)\n\nprint(\"Clean Train DataFrame With Dummy Variables: {}\".format(df_train_dummies.shape))\nprint(\"Clean Test DataFrame With Dummy Variables: {}\".format(df_test_dummies.shape))", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "3c5f6a65b672d8060d47d9561c7fff047291cb6d", "_cell_guid": "213c956f-57a7-d2b9-ba1d-3a78f1c16423"}, "execution_count": null, "source": "# concatenate to only include columns in both data sets\n# the number should be based on the number of columns. Original is 30471. Now set to 15471 after outlier handling etc.\ndf_temp = pd.concat([df_train_dummies, df_test_dummies], join='inner')\ndf_temp_train = df_temp[:len(df_train.index)]\ndf_temp_test = df_temp[len(df_train.index):]\n\n# check shapes of combined df and split out again\nprint(df_temp.shape)\nprint(df_temp_train.shape)\nprint(df_temp_test.shape)", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "8eec549641b9144871e6238c3d740d19d68298ea", "_cell_guid": "f7f3b97d-ca55-a9d9-d65e-6c51e56a826b"}, "execution_count": null, "source": "# defining X and y\nX = df_temp_train\ntest_X = df_temp_test\ny = df_train['y']", "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "1bd54c7b8d961bcca6d0127005fe64a1dc9dd3f4", "_cell_guid": "3a96e212-eb9a-03fe-82ce-35f3a4934b2c"}, "execution_count": null, "source": "# Determining best alpha for Ridge regression", "outputs": [], "cell_type": "markdown"}, {"metadata": {"trusted": false, "_uuid": "92333cf23e1e8639dbc2530df520665b91258add", "_cell_guid": "6fd39e93-a2d1-0b3c-d639-993c88b75ed3"}, "execution_count": null, "source": "# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 20)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X, y, cv=5)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "17dd41312d2da199f7f02b56c847ed299e0003d4", "_cell_guid": "7dab635b-edf2-b706-9732-22b4bab396b3"}, "execution_count": null, "source": "ridgescores = pd.DataFrame({'alpha':alpha_space, 'score':ridge_scores})\nridgescores", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "1b43e46966e00d133902d6b19c484a4d3dfee9ad", "_cell_guid": "9f2e72a2-13e0-34f3-3633-9cd8e49adca9"}, "execution_count": null, "source": "# Setup the hyperparameter grid\nalpha_space = np.logspace(-4, 0, 20)\nparam_grid = {'alpha': alpha_space}\n\n# Instantiate a logistic regression classifier: ridge\nridge = Ridge()\n\n# Instantiate the GridSearchCV object: ridge_cv\nridge_cv = GridSearchCV(ridge, param_grid, cv=5)\n\n# Fit it to the data\nridge_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(ridge_cv.best_params_)) \nprint(\"Best score is {}\".format(ridge_cv.best_score_))", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "82ce06b61a4f4a6a2b86c2cc38bc2eb039fe304b", "_cell_guid": "fc0fb45e-54cb-71eb-b7cf-dedd1cd15a61"}, "execution_count": null, "source": "# instantiating\nrcv = RidgeCV()\n\n# setting up steps for the pipeline\nsteps = [('RidgeCV', rcv)]\n\n# instantiating the pipeline\npipe = Pipeline(steps)\n\n# creating train and test sets using train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# fitting and predicting\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(pipe.score(X_test, y_test)))\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error: {}\".format(mse))", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "22f05582e5b1ad1b72e8d9366f70c2d5a312424a", "_cell_guid": "a97de349-7bea-193c-4bf3-db87cb3d38ea"}, "execution_count": null, "source": "'''\n# Setup the hyperparameter grid\nalpha_space = np.logspace(-4, 0, 5)\nl1_l2_space = np.linspace(0,1,11)\n\nparam_grid = {'alpha': alpha_space,\n             'l1_ratio': l1_l2_space}\n\n# Instantiate a logistic regression classifier: elas\nelas = ElasticNet()\n\n# Instantiate the GridSearchCV object: elas_cv\nelas_cv = GridSearchCV(elas, param_grid, cv=5)\n\n# Fit it to the data\nelas_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(elas_cv.best_params_)) \nprint(\"Best score is {}\".format(elas_cv.best_score_))\n'''", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "10b1910ff8494494bc34fd8140eba154f145d749", "_cell_guid": "d4a45cee-020d-2f65-a9e5-7f0bf0ec87c3"}, "execution_count": null, "source": "# instantiating different regressors\nrcv = RidgeCV()\nlcv = LassoCV()\nllrcv = LassoLarsCV()\necv = ElasticNetCV()\nompcv = OrthogonalMatchingPursuitCV()", "outputs": [], "cell_type": "code"}, {"metadata": {"trusted": false, "_uuid": "8863a8e6c736afaf65072ca87d92c3e70f8df401", "_cell_guid": "11904e90-290c-26f2-e3a8-04a2a7ba6fa5"}, "execution_count": null, "source": "# bad for but just for now:\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Compute 10-fold cross-validation scores: cv_scores\ncv_scores_rcv = cross_val_score(rcv, X, y, cv=10)\ncv_scores_lcv = cross_val_score(lcv, X, y, cv=10)\ncv_scores_llrcv = cross_val_score(llrcv, X, y, cv=10)\ncv_scores_ecv = cross_val_score(ecv, X, y, cv=10)\ncv_scores_ompcv = cross_val_score(ompcv, X, y, cv=10)\n\n# Print the 10-fold cross-validation scores\nprint(cv_scores_rcv)\nprint(cv_scores_lcv)\nprint(cv_scores_llrcv)\nprint(cv_scores_ecv)\nprint(cv_scores_ompcv)\n\nprint(\"Average 10-Fold RidgeCV CV Score: {}\".format(np.mean(cv_scores_rcv)))\nprint(\"Average 10-Fold LassoCV CV Score: {}\".format(np.mean(cv_scores_lcv)))\nprint(\"Average 10-Fold LassoLarsCV CV Score: {}\".format(np.mean(cv_scores_llrcv)))\nprint(\"Average 10-Fold ElasticNetCV CV Score: {}\".format(np.mean(cv_scores_ecv)))\nprint(\"Average 10-Fold OrthogonalMatchingPursuitCV CV Score: {}\".format(np.mean(cv_scores_ompcv)))", "outputs": [], "cell_type": "code"}], "nbformat": 4}