{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "84508db8-6c14-866c-00b7-ac48fb7e42d4"
      },
      "source": [
        "There is a lot of juice to be had combining the raw numeric parameters.  This is for illustrative purposes and scores only 0.664 with a train score of 0.661.  Coming up with decent combos will certainly improve your scores. As will by thinking vertically with rolling parameters based on dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5e3fa92-9e2d-7c20-230e-89bb154f3d55"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2a41e87-7b7e-d4a0-5b31-2597fb77aaa2"
      },
      "outputs": [],
      "source": [
        "def Outputs(p):\n",
        "    return 1.0/(1.0+np.exp(-p))\n",
        "\n",
        "def GPLow(data):\n",
        "    p = (np.tanh((((2.275860 + (((2.275860 + (19.666700 + ((data[\"num_photos\"] * 2.0) * 2.0))) + data[\"bedrooms\"]) * data[\"price\"]))/2.0) * 2.0)) +\n",
        "         np.tanh((((((((data[\"price\"] * 2.0) * 2.0) * 2.0) - ((data[\"num_features\"] + data[\"bedrooms\"])/2.0)) * 2.0) + (data[\"num_photos\"] * data[\"num_photos\"]))/2.0)) +\n",
        "         np.tanh(((((data[\"price\"] + data[\"latitude\"]) * 2.0) * 2.0) - (((data[\"created_hour\"] * 2.0) + ((((data[\"num_description_words\"] + data[\"bedrooms\"])/2.0) + data[\"bedrooms\"])/2.0))/2.0))) +\n",
        "         np.tanh((((0.220930 + ((data[\"created_hour\"] + ((-((data[\"created_hour\"] / 2.0))) * (data[\"created_hour\"] / 2.0)))/2.0))/2.0) * (data[\"num_features\"] + data[\"created_hour\"]))) +\n",
        "         np.tanh((((data[\"bathrooms\"] - (data[\"bedrooms\"] * 2.0)) * data[\"price\"]) + (data[\"latitude\"] * ((data[\"price\"] + data[\"latitude\"]) * 31.0)))) +\n",
        "         np.tanh(((data[\"longitude\"] * (data[\"longitude\"] + ((5.0) + (3.750000 * (data[\"bedrooms\"] * 2.0))))) + (data[\"price\"] + data[\"latitude\"]))) +\n",
        "         np.tanh(((0.065574 * (data[\"num_description_words\"] * (((((data[\"created_hour\"] * 2.0) + data[\"num_features\"])/2.0) + (data[\"num_description_words\"] - 1.022220))/2.0))) * 2.0)) +\n",
        "         np.tanh(((((data[\"price\"] - (data[\"bathrooms\"] / 2.0)) / 2.0) + (((0.065574 + data[\"price\"]) + data[\"longitude\"]) * 2.0)) + data[\"longitude\"])))\n",
        "    return Outputs(p)\n",
        "\n",
        "def GPMedium(data):\n",
        "    p = (np.tanh(((data[\"num_features\"] + ((data[\"num_features\"] * (3.857140 - data[\"num_features\"])) - (data[\"created_day\"] + 10.0)))/2.0)) +\n",
        "         np.tanh((((((data[\"num_photos\"] + data[\"created_hour\"])/2.0) + ((-(data[\"latitude\"])) - (data[\"num_photos\"] * data[\"num_photos\"])))/2.0) - ((data[\"price\"] * 2.0) * 2.0))) +\n",
        "         np.tanh((((((((data[\"bedrooms\"] + data[\"num_features\"])/2.0) + data[\"num_features\"])/2.0) + (data[\"bedrooms\"] - 0.591837))/2.0) - (((data[\"price\"] * 2.0) * 2.0) * 2.0))) +\n",
        "         np.tanh((((data[\"price\"] * (((7.0) + ((-(data[\"bathrooms\"])) * (data[\"price\"] * (7.0))))/2.0)) * data[\"bedrooms\"]) - 0.220930)) +\n",
        "         np.tanh(((((0.090909 - data[\"latitude\"]) * (((data[\"latitude\"] + data[\"num_description_words\"])/2.0) + data[\"bedrooms\"])) - data[\"latitude\"]) - data[\"price\"])) +\n",
        "         np.tanh((((data[\"latitude\"] * (data[\"latitude\"] * ((-((10.0 + data[\"bathrooms\"]))) * 2.0))) - data[\"longitude\"]) - (data[\"longitude\"] * 2.0))) +\n",
        "         np.tanh((0.090909 * ((data[\"created_hour\"] + 1.169230) + ((-((data[\"num_features\"] + data[\"created_hour\"]))) * (data[\"created_hour\"] + data[\"num_description_words\"]))))) +\n",
        "         np.tanh((((data[\"num_photos\"] + (((-(data[\"num_photos\"])) / 2.0) * (data[\"num_photos\"] / 2.0)))/2.0) * ((-1.0 + (data[\"num_photos\"] * data[\"num_photos\"]))/2.0))))\n",
        "    return Outputs(p)\n",
        "\n",
        "def GPHigh(data):\n",
        "    p = (np.tanh((19.666700 * (-1.0 - ((data[\"price\"] * (((((19.666700 + data[\"created_hour\"])/2.0) * 2.0) + 5.764710)/2.0)) / 2.0)))) +\n",
        "         np.tanh((-((((1.514290 + (((((5.764710 * 2.0) * 2.0) + data[\"bedrooms\"])/2.0) * data[\"price\"])) * 2.0) * 2.0)))) +\n",
        "         np.tanh(((((-((data[\"price\"] * 31.0))) + (-(3.071430))) + (data[\"bedrooms\"] + (data[\"created_hour\"] + data[\"bedrooms\"])))/2.0)) +\n",
        "         np.tanh((((data[\"num_features\"] / 2.0) / 2.0) - (data[\"price\"] + ((data[\"price\"] * 2.0) + (((data[\"num_photos\"] * data[\"num_photos\"]) + 0.090909)/2.0))))) +\n",
        "         np.tanh((((data[\"latitude\"] * 2.0) + data[\"price\"]) * ((data[\"created_hour\"] - (1.653850 + (31.0 * data[\"latitude\"]))) - 1.653850))) +\n",
        "         np.tanh((((((((0.090909 + data[\"price\"])/2.0) * 2.0) * 2.0) * ((-(1.362070)) + (data[\"bedrooms\"] * 2.0))) - data[\"longitude\"]) * 2.0)) +\n",
        "         np.tanh((((((-(data[\"price\"])) + (((-1.0 + data[\"num_photos\"])/2.0) - (data[\"longitude\"] * 19.666700)))/2.0) - data[\"price\"]) - data[\"price\"])) +\n",
        "         np.tanh((data[\"latitude\"] + (((-(((data[\"created_day\"] + ((((data[\"created_hour\"] + data[\"bedrooms\"])/2.0) * data[\"bedrooms\"]) - data[\"bedrooms\"]))/2.0))) / 2.0) / 2.0))))\n",
        "    return Outputs(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "81b3168d-ef8c-a5c4-0d3b-5afdd6787d00"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json('../input/train.json')\n",
        "test = pd.read_json('../input/test.json')\n",
        "train.loc[train.interest_level=='low','interest_level'] = 0\n",
        "train.loc[train.interest_level=='medium','interest_level'] = 1\n",
        "train.loc[train.interest_level=='high','interest_level'] = 2\n",
        "train.interest_level = train.interest_level.astype(float)\n",
        "train[\"created\"] = pd.to_datetime(train[\"created\"])\n",
        "test[\"created\"] = pd.to_datetime(test[\"created\"])\n",
        "train[\"num_photos\"] = train[\"photos\"].apply(len)\n",
        "test[\"num_photos\"] = test[\"photos\"].apply(len)\n",
        "train[\"num_features\"] = train[\"features\"].apply(len)\n",
        "test[\"num_features\"] = test[\"features\"].apply(len)\n",
        "train[\"created_year\"] = train[\"created\"].dt.year\n",
        "test[\"created_year\"] = test[\"created\"].dt.year\n",
        "train[\"created_month\"] = train[\"created\"].dt.month\n",
        "test[\"created_month\"] = test[\"created\"].dt.month\n",
        "train[\"created_day\"] = train[\"created\"].dt.day\n",
        "test[\"created_day\"] = test[\"created\"].dt.day\n",
        "train[\"created_hour\"] = train[\"created\"].dt.hour\n",
        "test[\"created_hour\"] = test[\"created\"].dt.hour\n",
        "train[\"num_description_words\"] = train[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "test[\"num_description_words\"] = test[\"description\"].apply(lambda x: len(x.split(\" \")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "894df8ec-adfd-121e-4326-86df6aa83c2a"
      },
      "outputs": [],
      "source": [
        "actualcolumns = ['bathrooms', 'bedrooms',\n",
        "                 'num_photos', 'num_features', 'num_description_words',\n",
        "                 'latitude','longitude',\n",
        "                 'price',\n",
        "                 'created_year',\n",
        "                 'created_month',\n",
        "                 'created_day',\n",
        "                 'created_hour']\n",
        "classes = ['low','medium','high']\n",
        "ss = StandardScaler()\n",
        "ss.fit(pd.concat([train[actualcolumns],test[actualcolumns]]))\n",
        "predictions  = np.zeros((train.shape[0],3))\n",
        "gptrain = train[actualcolumns].copy()\n",
        "gptrain[actualcolumns] = ss.transform(train[actualcolumns])\n",
        "for i in range(3):\n",
        "    if(i==0):\n",
        "        predictions[:,0] = GPLow(gptrain)\n",
        "    elif(i==1):\n",
        "        predictions[:,1] = GPMedium(gptrain)\n",
        "    else:\n",
        "        predictions[:,2] = GPHigh(gptrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b99c239-3bb5-9c5a-023a-e65c123f62eb"
      },
      "outputs": [],
      "source": [
        "print('Log Loss', log_loss(train.interest_level,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71661550-1c43-51e0-6ce9-d06880d1e58a"
      },
      "outputs": [],
      "source": [
        "predictions  = np.zeros((test.shape[0],3))\n",
        "gptest = test[actualcolumns].copy()\n",
        "gptest[actualcolumns] = ss.transform(test[actualcolumns])\n",
        "for i in range(3):\n",
        "    if(i==0):\n",
        "        predictions[:,0] = GPLow(gptest)\n",
        "    elif(i==1):\n",
        "        predictions[:,1] = GPMedium(gptest)\n",
        "    else:\n",
        "        predictions[:,2] = GPHigh(gptest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4a4abba-4681-de9f-9462-da8fc1ef347c"
      },
      "outputs": [],
      "source": [
        "print('Started producing Submission File')\n",
        "out_df = pd.DataFrame(predictions)\n",
        "out_df.columns = [\"low\", \"medium\", \"high\" ]\n",
        "out_df[[\"low\", \"medium\", \"high\" ]] = out_df[[\"low\", \"medium\", \"high\" ]].div(out_df[[\"low\", \"medium\", \"high\" ]].sum(axis=1), axis=0)\n",
        "out_df[\"listing_id\"] = test.listing_id.values\n",
        "out_df = out_df[['high', 'medium', 'low','listing_id']]\n",
        "out_df.to_csv(\"loo_xgb_starter.csv\", index=False)\n",
        "print('Finished producing Submission File')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}