{"cells":[{"metadata":{"_uuid":"9615588ff15ee992f488696fa0a5fa1394bb2707"},"cell_type":"markdown","source":"# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_import)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Datasets samples](#eda_ds_samples)  \n&nbsp; [3.2 Datasets numerical statistics](#eda_ds_desc)  \n&nbsp; [3.3 Datasets comparisons](#eda_ds_comparison)  \n&nbsp; [3.4 Target Label](#eda_app_train_target)  \n&nbsp; [3.5 Amounts comparison](#eda_amts)  \n&nbsp; [3.6 Distribution of DAYS_BIRTH](#eda_days_birth)  \n&nbsp; [3.7 Distribution of AMT_CREDIT](#eda_amt_credit)  \n&nbsp; [3.8 Distribution of DAYS_ID_PUBLISH](#eda_days_id_publish)  \n&nbsp; [3.9 Distribution of DAYS_REGISTRATION](#eda_days_registration)  \n&nbsp; [3.10 Distribution of DAYS_EMPLOYED](#eda_days_employed)  \n[4. Data Preprocessing](#4)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "},{"metadata":{"_uuid":"922c9a490d610b6b4d927a90ff25c15e208ae8ac"},"cell_type":"markdown","source":"Acknowledgements:\n- Dataset flattening, feature engineering, LGBM parameters: https://www.kaggle.com/shep312/lightgbm-harder-better-slower\n- Dataset flattening, LGBM model starting point: https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772\n- General ideas: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code\n- Reducing memory footprint: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage"},{"metadata":{"_uuid":"75f2d953491f905a4c3e01991275125d419519bb"},"cell_type":"markdown","source":"# <a id=\"import\">1 Import Libraries and create support functions</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport seaborn as sns\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n# https://github.com/Microsoft/LightGBM\nimport lightgbm as lgb\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\nimport itertools\n\n#warnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0dc45408902e93d5cc4e3b7825abe8d9d5d5914","collapsed":true},"cell_type":"code","source":"# Support functions\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef bar_hor_noagg(x, y, title, color, w=None, h=None, lm=0, limit=100, rt=False):\n    trace = go.Bar(y=x, x=y, orientation = 'h', marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ndef bar_ver_noagg(x, y, title, color, w=None, h=None, lm=0, rt = False):\n    trace = go.Bar(y=y, x=x, marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n    \ndef gp(col, title):\n    df1 = app_train[app_train[\"TARGET\"] == 1]\n    df0 = app_train[app_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(app_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"85a7a3fd4c8cad2c91d88d8df268022bc00c6c30"},"cell_type":"code","source":"# This implementation was copied from: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"953d520a8eb9c6fb820c57ea10ed79a691ecb8c8"},"cell_type":"markdown","source":"# <a id=\"data_import\">2 Dataset Import</a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"# List available data files\n#print(os.listdir(\"../input\"))\nprint(\"Loading data files...\")\n\nstart = time()\n# Load all the datasets and reduce the memory usage\nposc_bal = reduce_mem_usage(pd.read_csv(\"../input/POS_CASH_balance.csv\"))\nbureau_bal = reduce_mem_usage(pd.read_csv(\"../input/bureau_balance.csv\"))\napp_train = reduce_mem_usage(pd.read_csv(\"../input/application_train.csv\"))\nprev_app = reduce_mem_usage(pd.read_csv(\"../input/previous_application.csv\"))\ninst_pay = reduce_mem_usage(pd.read_csv(\"../input/installments_payments.csv\"))\ncc_bal = reduce_mem_usage(pd.read_csv(\"../input/credit_card_balance.csv\"))\napp_test = reduce_mem_usage(pd.read_csv(\"../input/application_test.csv\"))\nbureau = reduce_mem_usage(pd.read_csv(\"../input/bureau.csv\"))\nend = time()\n\nprint(\"Finished loading data files and running memory optimization in {} seconds.\".format(int(round(end - start))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bfe6928a00aaceccc2af19ccee97ec86604c55d"},"cell_type":"markdown","source":"# <a id=\"eda\">3 Exploratory Data Analysis (EDA)</a>"},{"metadata":{"_uuid":"47cfd4cf3ed8981e479ac2f96514c4a8fe041dce"},"cell_type":"markdown","source":"## <a id=\"eda_ds_samples\">3.1 Datasets samples</a>"},{"metadata":{"trusted":true,"_uuid":"6b82025cecf9889944744cba768355190a1eb761","collapsed":true},"cell_type":"code","source":"# Show first 5 rows of each dataset\nprint('Point of Sale Cash Balance')\ndisplay(posc_bal.head())\nprint('Buereau Balance')\ndisplay(bureau_bal.head())\nprint('Applications Train')\ndisplay(app_train.head())\nprint('Previous Applications')\ndisplay(prev_app.head())\nprint('Installment Payments')\ndisplay(inst_pay.head())\nprint('Credit Card Balance')\ndisplay(cc_bal.head())\nprint('Applications Test')\ndisplay(app_test.head())\nprint('Bureau')\ndisplay(bureau.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a5294b17f55554285224823b48f1148b636f3d2"},"cell_type":"markdown","source":"## <a id=\"eda_ds_desc\">3.2 Datasets numerical statistics</a>"},{"metadata":{"trusted":true,"_uuid":"d4a59026b303ed01a8264b8804886b50984d1b90","collapsed":true},"cell_type":"code","source":"# Show dataset descriptive statistics\nprint('Point of Sale Cash Balance')\ndisplay(posc_bal.describe(exclude=['category']))\nprint('Buereau Balance')\ndisplay(bureau_bal.describe(exclude=['category']))\nprint('Applications Train')\ndisplay(app_train.describe(exclude=['category']))\nprint('Previous Applications')\ndisplay(prev_app.describe(exclude=['category']))\nprint('Installment Payments')\ndisplay(inst_pay.describe(exclude=['category']))\nprint('Credit Card Balance')\ndisplay(cc_bal.describe(exclude=['category']))\nprint('Applications Test')\ndisplay(app_test.describe(exclude=['category']))\nprint('Bureau')\ndisplay(bureau.describe(exclude=['category']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7bace1861211baace24dd5c3890e30e1e78d69e"},"cell_type":"markdown","source":"## <a id=\"eda_ds_comparison\">3.3 Datasets comparisons</a>"},{"metadata":{"trusted":true,"_uuid":"61d33facd34c5856f6f3562228ddd474ba93df5b","collapsed":true},"cell_type":"code","source":"eda = pd.DataFrame(\n    [\n        ['Point of Sale Cash Balance', posc_bal.shape[0], posc_bal.shape[1] - 2, np.sum(posc_bal.dtypes=='category'), \n            np.sum(posc_bal.isnull().sum() > 0), posc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Bureau Balance', bureau_bal.shape[0], bureau_bal.shape[1] - 1, np.sum(bureau_bal.dtypes=='category'), \n            np.sum(bureau_bal.isnull().sum() > 0), bureau_bal.isnull().sum().sum()], # Features don't include SK_ID_BUREAU\n        ['Applications Train', app_train.shape[0], app_train.shape[1] - 2, np.sum(app_train.dtypes=='category'), \n            np.sum(app_train.isnull().sum() > 0), app_train.isnull().sum().sum()], # Features don't include SK_ID_CURR or TARGET\n        ['Previous Applications', prev_app.shape[0], prev_app.shape[1] - 2, np.sum(prev_app.dtypes=='category'), \n            np.sum(prev_app.isnull().sum() > 0), prev_app.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Installment Payments', inst_pay.shape[0], inst_pay.shape[1] - 2, np.sum(inst_pay.dtypes=='category'), \n            np.sum(inst_pay.isnull().sum() > 0), inst_pay.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Credit Card Balance', cc_bal.shape[0], cc_bal.shape[1] - 2, np.sum(cc_bal.dtypes=='category'), \n            np.sum(cc_bal.isnull().sum() > 0), cc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Applications Test', app_test.shape[0], app_test.shape[1] - 1, np.sum(app_test.dtypes=='category'), \n            np.sum(app_test.isnull().sum() > 0), app_test.isnull().sum().sum()], # Features don't include SK_ID_CURR\n        ['Bureau', bureau.shape[0], bureau.shape[1] - 2, np.sum(bureau.dtypes=='category'), \n            np.sum(bureau.isnull().sum() > 0), bureau.isnull().sum().sum()], # Features don't include SK_ID_CURR and SK_ID_BUREAU\n    ],\n    columns=['Dataset', 'samples', 'number_features', 'number_categorical_features', 'number_features_missing_values', \n                'total_number_missing_values']\n)\n\ndisplay(eda.head(8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07cf91e31284eee977d60508585ddb962665bffc"},"cell_type":"markdown","source":"## <a id=\"eda_app_train_target\">3.4 Target Label</a>"},{"metadata":{"trusted":true,"_uuid":"ead6e5fa6d72df540a56afd02b3a133950ae20d7","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(18,9))\nplt.subplot(121)\napp_train[\"TARGET\"].value_counts().plot(fontsize = 16,\n                                        kind = 'pie',\n                                        autopct = \"%1.0f%%\",\n                                        colors = sns.color_palette(\"prism\",8),\n                                        startangle = 90,\n                                        labels=[\"1 - Repayer\",\"0 - Defaulter\"],\n                                        explode=[.1,0],\n                                       )\nplt.title(\"Distribution of Target Label for Applications Train dataset\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dec41452d26d50fd0c71402d0b695d142e7c8b58"},"cell_type":"markdown","source":"## <a id=\"eda_amts\">3.5 Amounts comparison</a>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9a4d907f25c5ffdc84af6fe79ea5e89811795fad","collapsed":true},"cell_type":"code","source":"# Implementation source: https://www.kaggleusercontent.com/kf/4442153/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Df_QZcauc2BVOOmdHRjw1Q.ruvOVG8p44cAqUgN2tZHTPK-y8DwzYtkIoGA39JWR938aOHRdCqQRYjQj1U8AAiXqRfoRScRMjXH_DrMDqBWO9JIBKjTxS7yQyC3ouVc-MuExzzH0lGZdfJT2HJGkjvqSVLm4gYg7ML3r_jmJ3dP--6dmgHGsW1TQ6D04GnZzk6xwZseKGjCzeIYavlz44Qj.WDYyfq5ILj9HsKasnQ37uA/__results__.html#Comparing-summary-statistics-between-defaulters-and-non---defaulters-for-loan-amounts-.\ncols = [ 'AMT_INCOME_TOTAL', 'AMT_CREDIT','AMT_ANNUITY', 'AMT_GOODS_PRICE']\n\ndf = app_train.groupby(\"TARGET\")[cols].describe().transpose().reset_index()\ndisplay(df)\ndf = df[df[\"level_1\"].isin([ 'mean', 'std', 'min', 'max'])] \ndf_x = df[[\"level_0\",\"level_1\",0]]\ndf_y = df[[\"level_0\",\"level_1\",1]]\ndf_x = df_x.rename(columns={'level_0':\"amount_type\", 'level_1':\"statistic\", 0:\"amount\"})\ndf_x[\"type\"] = \"1 - Repayer\"\ndf_y = df_y.rename(columns={'level_0':\"amount_type\", 'level_1':\"statistic\", 1:\"amount\"})\ndf_y[\"type\"] = \"0 - Defaulter\"\ndf_new = pd.concat([df_x,df_y],axis = 0)\n\nstat = df_new[\"statistic\"].unique().tolist()\nlength = len(stat)\n\nplt.figure(figsize=(13,15))\n\nfor i,j in itertools.zip_longest(stat,range(length)):\n    plt.subplot(2,2,j+1)\n    fig = sns.barplot(df_new[df_new[\"statistic\"] == i][\"amount_type\"],df_new[df_new[\"statistic\"] == i][\"amount\"],\n                hue=df_new[df_new[\"statistic\"] == i][\"type\"],palette=[\"g\",\"r\"])\n    plt.title(i + \"--Defaulters vs Non defaulters\")\n    plt.subplots_adjust(hspace = .4)\n    fig.set_facecolor(\"lightgrey\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d67fa46db3d4bbbc14459d0e60b66ef627c022d7"},"cell_type":"markdown","source":"## <a id=\"eda_days_birth\">3.6 Distribution of DAYS_BIRTH</a>"},{"metadata":{"trusted":true,"_uuid":"3b9db5c728f1ecc76dbc3034aae55339ef96e471","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_BIRTH\")\nax = sns.distplot(app_train[\"DAYS_BIRTH\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcfd0ef91ca9808e27da32145c45371beff0c80c"},"cell_type":"markdown","source":"## <a id=\"eda_amt_credit\">3.7 Distribution of AMT_CREDIT</a>"},{"metadata":{"trusted":true,"_uuid":"d9c1f7a52f3f0fb9305ca4249cd7d9dcf0368d37","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_CREDIT\")\nax = sns.distplot(app_train[\"AMT_CREDIT\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c752a1fa63fdfc935821070be0d8c2dfca2e65b"},"cell_type":"markdown","source":"## <a id=\"eda_days_id_publish\">3.8 Distribution of DAYS_ID_PUBLISH</a>"},{"metadata":{"trusted":true,"_uuid":"79b55cb1b04b535c2d19624aa5033985b83ae5d1","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_ID_PUBLISH\")\nax = sns.distplot(app_train[\"DAYS_ID_PUBLISH\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05162a010684cd8479b53b8d79945671bc0ad958"},"cell_type":"markdown","source":"## <a id=\"eda_days_registration\">3.9 Distribution of DAYS_REGISTRATION</a>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2c483d1556ed945ca5556e700574fb21b8ee2e04","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_REGISTRATION\")\nax = sns.distplot(app_train[\"DAYS_REGISTRATION\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aefa0e29999b40980185954882728281c85e2dca"},"cell_type":"markdown","source":"## <a id=\"eda_days_employed\">3.10 Distribution of DAYS_EMPLOYED</a>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"108ad9d9f5eb38cb48af41300482afa7550a95b3","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(app_train[\"DAYS_EMPLOYED\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b45c23eb5bd269ec30f30d863cd80a7bb35d9dd"},"cell_type":"markdown","source":"# <a id=\"4\">4 Data Preprocessing</a>"},{"metadata":{"trusted":true,"_uuid":"6e62700bba51c07029d90fd9510d2053b56ed1f4","collapsed":true},"cell_type":"code","source":"# Merge training and testing datasets - This will help in two ways:\n# When handling categorical variables it will ensure both datasets end up with the same features\n# When handling missing values, if we use the mean to fill in missing values, they will be more representative\napp_train['is_train'] = 1\napp_train['is_test'] = 0\napp_test['is_train'] = 0\napp_test['is_test'] = 1\nprint(\"\\nJoining the training(app_train) and testing(app_test) dataset for pre-processing into pandas DataFrame 'data'.\")\n\n# data = pd.concat([app_train, app_test], axis=0, sort=False)\n# ERROR: TypeError: concat() got an unexpected keyword argument 'sort'\ndata = pd.concat([app_train, app_test], axis=0)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' for app_train\n# And substract 3 for app_test, as it doesn't have a 'TARGET' column\nprint(\"app_train has {0:,} samples and {1} features.\".format(app_train.shape[0], app_train.shape[1]-4))\nprint(\"app_test has {0:,} samples and {1} features.\".format(app_test.shape[0], app_test.shape[1]-3))\nprint(\"data has {0:,} samples and {1} features BEFORE one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nassert(data.shape[0] == app_train.shape[0] + app_test.shape[0])\nassert(data.shape[1] >= max(app_train.shape[1], app_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f1b7d135accee867bddc35318ddffdd15b622ee","collapsed":true},"cell_type":"code","source":"# Handle Categorical variables - Turn categorical variables into numerical features using the one-hot encoding scheme\n# Support function for one-hot encoding\ndef _one_hot_encoding(data):\n    # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n    return pd.get_dummies(data)\n\n# Handle categorical variables\nprint(\"\\nPerforming one-hot encoding on {} dataset.\".format('data'))\ndata = _one_hot_encoding(data)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test'\nprint(\"app has {0:,} samples and {1} features AFTER one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nposc_bal = _one_hot_encoding(posc_bal)\n#bureau_bal = _one_hot_encoding(bureau_bal)\nprev_app = _one_hot_encoding(prev_app)\ninst_pay = _one_hot_encoding(inst_pay)\ncc_bal = _one_hot_encoding(cc_bal)\nbureau = _one_hot_encoding(bureau)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bda85dd8056b42949510007471a53d5a34439312"},"cell_type":"code","source":"# Keep a copy of the application_train & application_test datasets without merging with the rest of the datasets\ndata_train_test = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"270ef25dfa972d28a962bae4981248fb1c7e3ce3","collapsed":true},"cell_type":"code","source":"# Merge Point of Sale Cash Balance dataset\nprint(\"Merge 'Point of Sale Cash Balance' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nposc_bal_count = posc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nposc_bal['POSC_BAL_COUNT'] = posc_bal['SK_ID_CURR'].map(posc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nposc_bal = posc_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nposc_bal_avg = posc_bal.groupby('SK_ID_CURR').mean()\nposc_bal_avg.columns = ['pcb_' + col for col in posc_bal_avg.columns]\ndata = data.merge(right=posc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"379a233789f277ca8630dae8c5d5232f3ff2f793","collapsed":true},"cell_type":"code","source":"'''\n# Merge Bureau Balance dataset\nprint(\"Merge 'Bureau Balance' dataset.\")\n#'SK_ID_BUREAU'\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73ce63a2eadd2d575ea52b317a55c24acf09951d","collapsed":true},"cell_type":"code","source":"# Merge Previous Applications dataset\nprint(\"Merge 'Previous Applications' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['PREV_COUNT'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nprev_app = prev_app.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['pa_' + col for col in prev_app_avg.columns]\ndata = data.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848e06dea27f5efb1847debbd5ca9c485ca61908","collapsed":true},"cell_type":"code","source":"# Merge Installments Payments dataset\nprint(\"Merge 'Installments Payments' dataset.\")\n# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature\ninst_pay_count = inst_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_pay['INST_PAY_COUNT'] = inst_pay['SK_ID_CURR'].map(inst_pay_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ninst_pay = inst_pay.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ninst_pay_avg = inst_pay.groupby('SK_ID_CURR').mean()\ninst_pay_avg.columns = ['ip_' + col for col in inst_pay_avg.columns]\ndata = data.merge(right=inst_pay_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70a191e709a4ba349fea674b039fc40fa9cb1c9e","collapsed":true},"cell_type":"code","source":"# Merge Credit Card Balance dataset\nprint(\"Merge 'Credit Card Balance' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\ncc_bal_count = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncc_bal['CC_BAL_COUNT'] = cc_bal['SK_ID_CURR'].map(cc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ncc_bal = cc_bal.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ncc_bal_avg = cc_bal.groupby('SK_ID_CURR').mean()\ncc_bal_avg.columns = ['ccb_' + col for col in cc_bal_avg.columns]\ndata = data.merge(right=cc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a85c005254d9976eba6042afd2b06bda36cdea1a","collapsed":true},"cell_type":"code","source":"# Merge Bureau dataset\nprint(\"Merge 'Bureau' dataset.\")\n# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau['BUREAU_COUNT'] = bureau['SK_ID_CURR'].map(bureau_count['SK_ID_BUREAU'])\n# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value\nbureau = bureau.drop(['SK_ID_BUREAU'], axis=1)\n\n## Average values for all other features in previous applications\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['b_' + col for col in bureau_avg.columns]\ndata = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c9e7f293121fd26492be2e8d0334f5b6551e624","collapsed":true},"cell_type":"code","source":"# Transforming skewed continuous features\n#skewed = ['DAYS_EMPLOYED']\n#data[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n# I need to handle negative numbers, if x = -1 then it will throw an error; log(0) = Inf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cbcbd68b606500375a635ee12286daa28ae8e24","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(data[\"DAYS_EMPLOYED\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10916d50c700ec898123f3b422bb24064f2e4711","collapsed":true},"cell_type":"code","source":"# Normalizing numerical features\nfrom sklearn.preprocessing import MinMaxScaler\n#app_train_copy = app_train.copy()\n\nscaler = MinMaxScaler()\n# Full list of top ten features, discounting EXT_SOURCE_X becuase they are already normalizaed:\n# ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH', 'pcb_CNT_INSTALMENT_FUTURE', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED']\n\n# numerical = ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']\n# 12 entries in 'AMT_ANNUITY' are NaN - I need to fix that first before Normalizing\n\n# 'pcb_CNT_INSTALMENT_FUTURE' belongs to a different dataset\n\n# numerical = ['DAYS_BIRTH', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']\n\nnumerical = ['DAYS_BIRTH', 'AMT_CREDIT', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED']\ndata[numerical] = scaler.fit_transform(data[numerical])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e96594845b10101ad32583f79665412b3994733","collapsed":true},"cell_type":"code","source":"data_to_use = 'ALL' # 'ALL' or 'data_train_test'\nif data_to_use == 'data_train_test':\n    data = data_train_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8673e3d4c80bcae3d653f7d8b87f095381fcc0f2","collapsed":true},"cell_type":"code","source":"# Handle missing data\n# https://pandas.pydata.org/pandas-docs/stable/missing_data.html#filling-with-a-pandasobject\n# https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\nprint(\"\\nFilling NaN values in the dataset using pandas.fillna() using the column mean() value.\")\nprint(\"Number of NaN values in the dataset BEFORE running pandas.fillna(): {:,}\".format(data.isnull().sum().sum()))\ndata = data.fillna(data.mean())\nnan_after = data.isnull().sum().sum()\nprint(\"Number of NaN values in the dataset AFTER running pandas.fillna(): {:,}\".format(nan_after))\nassert(nan_after == 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26279a753dfbd18414c2498179a72663386d1fea","collapsed":true},"cell_type":"code","source":"import gc\n# Clean variables that are no longer needed\n# Not used yet: bureau_bal_count, bureau_bal_avg\ndel posc_bal, posc_bal_count, posc_bal_avg, bureau_bal, app_train, app_test\ndel prev_app, prev_app_count, prev_app_avg, inst_pay, inst_pay_count, inst_pay_avg, cc_bal, cc_bal_count, cc_bal_avg\ndel bureau, bureau_count, bureau_avg, data_train_test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96481d0c2fe0b029c2f2cc30341ef82ebf6bdaca","collapsed":true},"cell_type":"code","source":"# Separate the data into the original test and training datasets\n# Remove columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' as they are not features\nprint(\"\\nSeparating the training and testing dataset after completing pre-processing.\")\ntrain = data[data['is_train'] == 1]\n\n# Separate the 'target label' from the training dataset\ntarget = train['TARGET']\ntrain = train.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\ntest = data[data['is_test'] == 1]\n\n# To be used when preparing the submission\ntest_id = test['SK_ID_CURR']\ntest = test.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\nprint(\"train has {:,} samples and {} features.\".format(train.shape[0], train.shape[1]))\nprint(\"test has {:,} samples and {} features.\".format(test.shape[0], test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bdd148493034a86d6f4a84007c5e91e9170362d"},"cell_type":"markdown","source":"# <a id=\"5\">5 Split Data into Training and Validation</a>"},{"metadata":{"trusted":true,"_uuid":"385ad2709b08ae3f67967b290b78ed48532a9384","collapsed":true},"cell_type":"code","source":"# Split 'features' and 'target label' data into training and validation data using train_test_split\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nprint(\"\\nSplitting the training dataset into actual training and validation datasets\")\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=42)\nassert(train.shape[0] == X_train.shape[0] + X_val.shape[0])\nassert(X_train.shape[1] == train.shape[1])\nassert(X_val.shape[1] == train.shape[1])\nassert(target.shape[0] == y_train.shape[0] + y_val.shape[0])\nprint(\"training dataset has {0:,} samples and {1} features.\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"validating dataset has {0:,} samples and {1} features.\".format(X_val.shape[0], X_val.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"51ba916b1d62b15d1d2fc1315a5f7647903e454f"},"cell_type":"code","source":"# Run GridSearchCV or fully train an estimator\n# 'grid_search_RFR', 'grid_search_LGBM', 'train_estimators', 'train_estimator_LGBM', 'train_estimator_RFR', 'LGBM_KFold'\nrun_mode = 'LGBM_KFold'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c943bbf3525a6d995f88f633f98ee7fd062f0ea1"},"cell_type":"markdown","source":"# <a id=\"6\">6 Hyperparameter Tuning</a>"},{"metadata":{"trusted":true,"_uuid":"450cd4b1642b4882fdc0be63473598366f243b6a","collapsed":true},"cell_type":"code","source":"# Run GridSearchCV on LGBM\nif run_mode == 'grid_search_LGBM':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    X_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    y_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    X_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    y_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    \n    estimator = lgb.LGBMClassifier(\n          objective='binary',\n          metric='auc',\n          num_iteration=5000, # num_boost_round=5000,\n          verbose=1,\n          silent=False,\n          colsample_bytree=.8,\n          subsample=.9,\n          reg_alpha=.1,\n          reg_lambda=.1,\n          min_split_gain=.01,\n          min_child_weight=1,\n          # early_stopping_rounds=100\n          # ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n    )\n    \n    '''\n    parameters = {\n          'task': ['train'],\n          'boosting_type': ['gbdt'],\n          'objective': ['binary'],\n          'metric': ['auc'],\n          'learning_rate': [0.01],\n          'num_leaves': [48],\n          'num_iteration': [5000],\n          'verbose': 0,\n          'colsample_bytree': [.8],\n          'subsample': [.9],\n          'max_depth': [7],\n          'reg_alpha': [.1],\n          'reg_lambda': [.1],\n          'min_split_gain': [.01],\n          'min_child_weight': [1]\n        }\n    '''\n    parameters = {\n          'boosting_type': ['gbdt'], # 'dart'\n          'num_leaves': [35, 48, 80],\n          'min_data_in_leaf': [20], # [15, 20, 25],\n          'learning_rate': [0.005],\n          'max_depth': [7], # [6, 7, 8],\n        }\n    \n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(X_train_small, y_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    print(\"\\nParameters used for tunning: \\n{}\".format(parameters))\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    #pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    #print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(X_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val_small, best_pred_val)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263","collapsed":true},"cell_type":"code","source":"# Run GridSearchCV\nif run_mode == 'grid_search_RFR':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [130, 135, 145],\n        'min_samples_leaf': [55, 62, 75],\n        'max_features': [0.2], # [0.18, 0.2, 0.23]\n        'min_samples_split': [2], # [2, 3]\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65738a59e8d918d371f8c717f247d94d15b7f482"},"cell_type":"markdown","source":"# <a id=\"7\">7 Model Fitting & Prediction</a>"},{"metadata":{"trusted":true,"_uuid":"5590b6b3ffee50b6734fb9f81a85b72b7b1e33a3","collapsed":true},"cell_type":"code","source":"# Train estimator LGBM\nif run_mode == 'train_estimator_LGBM':\n    params = {\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': 0.1,\n        'min_data_in_leaf': 30,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'feature_fraction': 0.5,\n        'scale_pos_weight': 2,\n        'drop_rate': 0.02,\n        'metric': 'auc',\n        'num_boost_round': 200,\n    }\n\n    data_split = 'kfold' # Possible values: 'kfold' or 'train_test_split'\n    if data_split == 'train_test_split':\n        # Using split merged datasets with train_test_split\n        lgb_train = lgb.Dataset(data=X_train, label=y_train)\n        lgb_eval = lgb.Dataset(data=X_val, label=y_val)\n        start = time()\n        estimator = lgb.train(\n            params = params,\n            train_set = lgb_train,\n            valid_sets = lgb_eval,\n            early_stopping_rounds = 350,\n            verbose_eval = 200\n        )\n        end = time()\n    elif data_split == 'kfold':   \n        # Using KFolds to split the merged dataset for cross-validation\n        lgb_train_cv = lgb.Dataset(data=train, label=target)\n        start = time()\n        cv_results = lgb.cv(\n            params = params,\n            train_set = lgb_train_cv,\n            nfold = 2,\n            early_stopping_rounds = 50,\n            stratified = True,\n            verbose_eval = 50\n        )\n        optimum_boost_rounds = np.argmax(cv_results['auc-mean'])\n        print('Optimum boost rounds = {}'.format(optimum_boost_rounds))\n        print('Best LGBM CV result = {}'.format(np.max(cv_results['auc-mean'])))\n        estimator = lgb.train(\n            params = params,\n            train_set = lgb_train_cv,\n            num_boost_round = optimum_boost_rounds,\n            verbose_eval = 50\n        )\n        end = time()\n    \n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n\n    lgb.plot_importance(estimator, figsize=(12, 12), max_num_features=30);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff0dc9c1c519aaf74e787fefa2ab085401452458","collapsed":true},"cell_type":"code","source":"print(estimator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c0f6d05dd9f75c4564bda6142fb33da74b6bdf1","collapsed":true},"cell_type":"code","source":"# Parameters from Aguiar https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features/code\n# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    from sklearn.model_selection import KFold, StratifiedKFold\n\n    folds = KFold(n_splits=10, shuffle=True, random_state=1024)\n\n    oof_preds = np.zeros(train.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    feature_importance = pd.DataFrame()\n    feats = train.columns\n\n    start = time()\n    for n_fold, (train_index, valid_index) in enumerate(folds.split(train, target)):\n        train_x, train_y = train.iloc[train_index], target.iloc[train_index]\n        valid_x, valid_y = train.iloc[valid_index], target.iloc[valid_index]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = lgb.LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, \n        )\n\n        clf.fit(\n            train_x,\n            train_y,\n            eval_set = [(valid_x, valid_y)],\n            eval_metric = 'auc',\n            verbose = 200,\n            early_stopping_rounds = 500,\n        )\n\n        oof_preds[valid_index] = clf.predict_proba(valid_x, num_iterations=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test, num_iterations=clf.best_iteration_)[:, 1] / folds.n_splits\n\n        fold_importance = pd.DataFrame()\n        fold_importance['feature'] = feats\n        fold_importance['importance'] = clf.feature_importances_\n        fold_importance['fold'] = n_fold + 1\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n        print('Fold {:02d} AUC: {:.6f}'.format(n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_index])))\n\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n    print('Full AUC score: {:.6f}'.format(roc_auc_score(target, oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e969760dd0383b3c1ac7489a08eef7b312364c","collapsed":true},"cell_type":"code","source":"# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    # Display feature importance\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deaa5d4d685c66431fb180f730ba7816a791c3be","collapsed":true},"cell_type":"code","source":"# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    # Prepare submission file\n    submission = pd.DataFrame()\n    submission['SK_ID_CURR'] = test_id\n    submission['TARGET'] = sub_preds\n    submission.to_csv('LGBM_SKFold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adf25ba4e24d459ac9e0fe1631a73609a6989e99","collapsed":true},"cell_type":"code","source":"# Train estimator RandonForrestRegressor\nif run_mode == 'train_estimator_RFR':\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    # Alternative: n_estimators=135, max_features=0.2, min_samples_split=2, min_samples_leaf=62\n    # Alternative2: criterion='mae', # default='mse', VERY SLOW\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(estimator))\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(X_train, y_train)\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(X_val)\n    print(\"\\nEstimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val, pred_val)))\n    \n    # Determine the feature importance\n    fi = pd.DataFrame()\n    fi['feature'] = X_train.columns\n    fi['importance'] = estimator.feature_importances_\n    display(fi.sort_values(by=['importance'], ascending=False).head(10))\n\n    # TODO: GRAPH THE FEATURE IMPORTANCE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85c8c3c19174276cdc1ee4adca42d90b059479ad","collapsed":true},"cell_type":"code","source":"if 'train_estimator_' in run_mode:\n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(test)\n    \n    # Prepare prediction for submission\n    print(\"\\nPreparing prediction for submission.\")\n    submission = pd.DataFrame()\n    submission['SK_ID_CURR'] = test_id\n    # Replace any negative number with zero, required for https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code\n    # pred_test[pred_test < 0] = 0\n    submission['TARGET'] = pred_test\n    submission.head()\n    file_name = run_mode.split('train_estimator_')[1] + '.csv'\n    submission.to_csv(file_name, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2db77807318d1973299dab343a37cc63aaed327"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}