{"cells":[{"metadata":{"_cell_guid":"7182d808-a918-472a-92bd-de48544e49d3","_uuid":"ddabc7c31f4073319a1d39834405a201d0710b53","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5946065-fd1e-44d5-a60d-5d1baaced2e1","_uuid":"0761fef7cc2510d068c6aff0c993499592fc9b38","trusted":true},"cell_type":"code","source":"'''Example script showing how to use a stateful LSTM model\nand how its stateless counterpart performs.\nMore documentation about the Keras LSTM model can be found at\nhttps://keras.io/layers/recurrent/#lstm\nThe models are trained on an input/output pair, where\nthe input is a generated uniformly distributed\nrandom sequence of length = \"input_len\",\nand the output is a moving average of the input with window length = \"tsteps\".\nBoth \"input_len\" and \"tsteps\" are defined in the \"editable parameters\" section.\nA larger \"tsteps\" value means that the LSTM will need more memory\nto figure out the input-output relationship.\nThis memory length is controlled by the \"lahead\" variable (more details below).\nThe rest of the parameters are:\n- input_len: the length of the generated input sequence\n- lahead: the input sequence length that the LSTM\n  is trained on for each output point\n- batch_size, epochs: same parameters as in the model.fit(...) function\nWhen lahead > 1, the model input is preprocessed to a \"rolling window view\"\nof the data, with the window length = \"lahead\".\nThis is similar to sklearn's \"view_as_windows\"\nwith \"window_shape\" being a single number\nRef: http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows\nWhen lahead < tsteps, only the stateful LSTM converges because its\nstatefulness allows it to see beyond the capability that lahead\ngave it to fit the n-point average. The stateless LSTM does not have\nthis capability, and hence is limited by its \"lahead\" parameter,\nwhich is not sufficient to see the n-point average.\nWhen lahead >= tsteps, both the stateful and stateless LSTM converge.\n'''\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n# ----------------------------------------------------------\n# EDITABLE PARAMETERS\n# Read the documentation in the script head for more details\n# ----------------------------------------------------------\n\n# length of input\ninput_len = 1000\n\n# The window length of the moving average used to generate\n# the output from the input in the input/output pair used\n# to train the LSTM\n# e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],\n#      then output=[1.5, 2.5, 3.5, 4.5]\ntsteps = 2\n\n# The input sequence length that the LSTM is trained on for each output point\nlahead = 1\n\n# training parameters passed to \"model.fit(...)\"\nbatch_size = 1\nepochs = 10\n\n# ------------\n# MAIN PROGRAM\n# ------------\n\nprint(\"*\" * 33)\nif lahead >= tsteps:\n    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\nelse:\n    print(\"STATELESS LSTM WILL NOT CONVERGE\")\nprint(\"*\" * 33)\n\nnp.random.seed(1986)\n\nprint('Generating Data...')\n\n\ndef gen_uniform_amp(amp=1, xn=10000):\n    \"\"\"Generates uniform random data between\n    -amp and +amp\n    and of length xn\n    Arguments:\n        amp: maximum/minimum range of uniform data\n        xn: length of series\n    \"\"\"\n    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\n    data_input = pd.DataFrame(data_input)\n    return data_input\n\n# Since the output is a moving average of the input,\n# the first few points of output will be NaN\n# and will be dropped from the generated data\n# before training the LSTM.\n# Also, when lahead > 1,\n# the preprocessing step later of \"rolling window view\"\n# will also cause some points to be lost.\n# For aesthetic reasons,\n# in order to maintain generated data length = input_len after pre-processing,\n# add a few points to account for the values that will be lost.\nto_drop = max(tsteps - 1, lahead - 1)\ndata_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\n\n# set the target to be a N-point average of the input\nexpected_output = data_input.rolling(window=tsteps, center=False).mean()\n\n# when lahead > 1, need to convert the input to \"rolling window view\"\n# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\nif lahead > 1:\n    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\n    data_input = pd.DataFrame(data_input)\n    for i, c in enumerate(data_input.columns):\n        data_input[c] = data_input[c].shift(i)\n\n# drop the nan\nexpected_output = expected_output[to_drop:]\ndata_input = data_input[to_drop:]\n\nprint('Input shape:', data_input.shape)\nprint('Output shape:', expected_output.shape)\nprint('Input head: ')\nprint(data_input.head())\nprint('Output head: ')\nprint(expected_output.head())\nprint('Input tail: ')\nprint(data_input.tail())\nprint('Output tail: ')\nprint(expected_output.tail())\n\nprint('Plotting input and expected output')\nplt.plot(data_input[0][:10], '.')\nplt.plot(expected_output[0][:10], '-')\nplt.legend(['Input', 'Expected output'])\nplt.title('Input')\nplt.show()\n\n\ndef create_model(stateful):\n    model = Sequential()\n    model.add(LSTM(20,\n              input_shape=(lahead, 1),\n              batch_size=batch_size,\n              stateful=stateful))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\nprint('Creating Stateful Model...')\nmodel_stateful = create_model(stateful=True)\n\n\n# split train/test data\ndef split_data(x, y, ratio=0.8):\n    to_train = int(input_len * ratio)\n    # tweak to match with batch_size\n    to_train -= to_train % batch_size\n\n    x_train = x[:to_train]\n    y_train = y[:to_train]\n    x_test = x[to_train:]\n    y_test = y[to_train:]\n\n    # tweak to match with batch_size\n    to_drop = x.shape[0] % batch_size\n    if to_drop > 0:\n        x_test = x_test[:-1 * to_drop]\n        y_test = y_test[:-1 * to_drop]\n\n    # some reshaping\n    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n    x_train = reshape_3(x_train)\n    x_test = reshape_3(x_test)\n\n    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\n    y_train = reshape_2(y_train)\n    y_test = reshape_2(y_test)\n\n    return (x_train, y_train), (x_test, y_test)\n\n\n(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\nprint('x_train.shape: ', x_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('x_test.shape: ', x_test.shape)\nprint('y_test.shape: ', y_test.shape)\n\nprint('Training')\nfor i in range(epochs):\n    print('Epoch', i + 1, '/', epochs)\n    # Note that the last state for sample i in a batch will\n    # be used as initial state for sample i in the next batch.\n    # Thus we are simultaneously training on batch_size series with\n    # lower resolution than the original series contained in data_input.\n    # Each of these series are offset by one step and can be\n    # extracted with data_input[i::batch_size].\n    model_stateful.fit(x_train,\n                       y_train,\n                       batch_size=batch_size,\n                       epochs=1,\n                       verbose=1,\n                       validation_data=(x_test, y_test),\n                       shuffle=False)\n    model_stateful.reset_states()\n\nprint('Predicting')\npredicted_stateful = model_stateful.predict(x_test, batch_size=batch_size)\n\nprint('Creating Stateless Model...')\nmodel_stateless = create_model(stateful=False)\n\nprint('Training')\nmodel_stateless.fit(x_train,\n                    y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test),\n                    shuffle=False)\n\nprint('Predicting')\npredicted_stateless = model_stateless.predict(x_test, batch_size=batch_size)\n\n# ----------------------------\n\nprint('Plotting Results')\nplt.subplot(3, 1, 1)\nplt.plot(y_test)\nplt.title('Expected')\nplt.subplot(3, 1, 2)\n# drop the first \"tsteps-1\" because it is not possible to predict them\n# since the \"previous\" timesteps to use do not exist\nplt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:])\nplt.title('Stateful: Expected - Predicted')\nplt.subplot(3, 1, 3)\nplt.plot((y_test - predicted_stateless).flatten())\nplt.title('Stateless: Expected - Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ad1c043-92e1-4c71-99d5-86f5fa644e05","collapsed":true,"_uuid":"479f17325a3019035c1908a863e592c186a61374","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}