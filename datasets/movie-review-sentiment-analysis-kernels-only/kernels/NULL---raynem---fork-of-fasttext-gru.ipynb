{"cells":[{"metadata":{"trusted":false,"_uuid":"678d4a90dd0e7183e950d906cc513e2345e6e92f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.utils.np_utils import to_categorical\nfrom keras.initializers import Constant\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nimport tensorflow as tf\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe0b01a106a9bcf8b179a91840e534a2992cd8f1"},"cell_type":"code","source":"df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', delimiter='\\t')\ndf = df[['Phrase', 'Sentiment']]\n\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ed2f2224d7b901a7f896528335131d87c88c4d62"},"cell_type":"code","source":"def clean_str(in_str):\n    in_str = str(in_str)\n    # replace urls with 'url'\n    in_str = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", in_str)\n    in_str = re.sub(r'([^\\s\\w]|_)+', '', in_str)\n    return in_str.strip().lower()\n\n\ndf['text'] = df['Phrase'].apply(clean_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"22ffd6f11eb4a800aa006ea131158224c268ad36"},"cell_type":"code","source":"df_0 = df[df['Sentiment'] == 0].sample(frac=1)\ndf_1 = df[df['Sentiment'] == 1].sample(frac=1)\ndf_2 = df[df['Sentiment'] == 2].sample(frac=1)\ndf_3 = df[df['Sentiment'] == 3].sample(frac=1)\ndf_4 = df[df['Sentiment'] == 4].sample(frac=1)\n\nsample_size = min(len(df_0), len(df_1), len(df_2), len(df_3), len(df_4))\n\ndata = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4e803751632fb6f0b6abfc566a5628583383ffea"},"cell_type":"code","source":"data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))\nprint(\"mean length of sentence: \" + str(data.l.mean()))\nprint(\"max length of sentence: \" + str(data.l.max()))\nprint(\"std dev length of sentence: \" + str(data.l.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"96f6e6a55101d3444ae6790ee0438f503141f33b"},"cell_type":"code","source":"sequence_length = 52","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae2dd3ebb061f3542119622b2ae69feee8319aa2"},"cell_type":"code","source":"max_features = 20000\n\ntokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\ntokenizer.fit_on_texts(data['Phrase'].values)\nX = tokenizer.texts_to_sequences(data['Phrase'].values)\nX = pad_sequences(X, sequence_length)\n\ny = pd.get_dummies(data['Sentiment']).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nprint(\"test set size \" + str(len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b0c7d84c8f6ec794f983e0a702476a91a8db5c9d"},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a29d4be1412da3068f6fe4c6e0a930ee08957767"},"cell_type":"code","source":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('wiki-news-300d-1M.vec'))\n\nmax_features = 20000\nmaxlen = 52\nembed_size = 300\nnb_words = min(max_features, len(word_index)) + 1\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3c899012e6b2bda3fec6d2daa2195d8771cdd29b"},"cell_type":"code","source":"def get_model():\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(14102, embed_size, weights=[embedding_matrix])(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(80, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(5, activation=\"softmax\")(conc)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65f485d39367ac0284149b5a7daa00e22738ff24"},"cell_type":"code","source":"callbacks = [ModelCheckpoint('weights_text1.model',\n                                monitor='val_acc',\n                                verbose=1,\n                                save_best_only=True,\n                                save_weights_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e1ae570b70c571d7601e5bf1e48fba00c4905d36"},"cell_type":"code","source":"hist = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test),\n                 callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8a3cca6f0a8854ba881ca4a3bd07f89975d3a560"},"cell_type":"code","source":"df_test = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', delimiter='\\t')\n\ndf_test['text'] = df_test['Phrase'].apply(clean_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4755b1961be533ee0cd4bf974d196c001d7aea69"},"cell_type":"code","source":"x = tokenizer.texts_to_sequences(df_test['text'].values)\nx = pad_sequences(x, sequence_length)\n\ny_hat = model.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f11f30b1556f45e7ed4687732e33cdc37efbd61"},"cell_type":"code","source":"df_results = pd.DataFrame(list(zip(df_test['PhraseId'].values, list(map(lambda x: np.argmax(x), y_hat)))), columns=['PhraseId', 'Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e1afc0284e4d9f005cd818d1f830255a3390804f"},"cell_type":"code","source":"df_results.to_csv('res.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ff4beaef784af64a0a1f12e4fbcce3914a41b1c2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}