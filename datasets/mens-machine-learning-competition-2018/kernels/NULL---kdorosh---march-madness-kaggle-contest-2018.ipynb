{"cells":[{"metadata":{"_cell_guid":"63f97ada-21c8-4755-9fee-0a0cfbff8aa7","_uuid":"9853586a0dc75ce39e7c7ffcde1eb4d47c6fb02e"},"cell_type":"markdown","source":"# Overview ##\n\nThis notebook creates numerous models, eventually settling on a simple yet effective logistic regression model. The models are trained on the seed differences between teams and season average metric differences (e.g., FG%, PPG, Opp. PPG) between teams. \n\nNote that the model is trained entirely on data from 2003-2017 and their known outcomes. The resulting classifier is then used on 2018 data to generate predictions for this year's tournament on March 11."},{"metadata":{"_cell_guid":"0c233e05-c63d-4866-96dc-bb38d444bf84","_uuid":"5464dc4b196dc4c8dd0323bbd71b75724113e2af","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"9eecd909-c6e5-4a88-8481-bcbca7aef1df","_uuid":"819472385a23f3fd5aaf4172b4f8db227cf5271f"},"cell_type":"markdown","source":"## Load the training data ##\nWe're keeping it relatively simple & using only a handful of files for this model: the tourney seeds, tourney results, and a detailed results dataset to calculate our other features."},{"metadata":{"_cell_guid":"087d26ad-591c-4ff4-bd13-be6aaf436832","_uuid":"bf8ee168a0372e883332d6bb0ce5c89c13143650","trusted":false},"cell_type":"code","source":"data_dir = '../input/'\ndf_seeds = pd.read_csv(data_dir + 'NCAATourneySeeds.csv')\ndf_tour = pd.read_csv(data_dir + 'NCAATourneyCompactResults.csv')\n\n# We load detailed season data to calculate season average statistics for each team\ndf_reg_season_detailed = pd.read_csv(data_dir + 'RegularSeasonDetailedResults.csv')\ndf_reg_season_detailed.drop(labels=['WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WDR', 'WAst', \n                'WStl', 'WBlk', 'WPF', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LDR', \n                'LAst', 'LStl', 'LBlk', 'LPF', 'WLoc', 'NumOT', 'WOR', 'LOR'], \n                            inplace=True, axis=1)\ndf_reg_season_detailed.head()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"2eda8ba4a0ccfcbce6d10921c78a94bfd8750f4c"},"cell_type":"markdown","source":"## Create a new data frame with season average metrics ##\nWe are creating a new data frame with season average statistics for each team for use as features in our machine learning algorithm."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"1cd96f10ac17fdd89fe96091a87f40adcdae2141"},"cell_type":"code","source":"yearList = range(2003,2019) #2003 is the first year we have detailed data for\nteams_pd = pd.read_csv(data_dir + 'Teams.csv')\nteamIDs = teams_pd['TeamID'].tolist()\n\nrows = list()\n\nfor year in yearList:\n    for team in teamIDs:\n        df_curr_season = df_reg_season_detailed[df_reg_season_detailed.Season == year]       \n\n        df_curr_team_wins = df_curr_season[df_curr_season.WTeamID == team]\n        df_curr_team_losses = df_curr_season[df_curr_season.LTeamID == team]\n        \n        # no games played by them this year.. skip (current team didn't win or lose any games)\n        if df_curr_team_wins.shape[0] == 0 and df_curr_team_losses.shape[0] == 0:\n            continue;\n        \n        df_winteam = df_curr_team_wins.rename(columns={'WTeamID':'TeamID', 'WFGM':'FGM', \n                    'WFGA':'FGA', 'WTO':'TO', 'WScore':'Score', 'LScore':'OppScore'})\n        \n        # drop all columns except the ones we are using\n        df_winteam = df_winteam[['TeamID', 'FGM', 'FGA', 'TO', 'Score', 'OppScore']]\n\n        df_loseteam = df_curr_team_losses.rename(columns={'LTeamID':'TeamID', 'LFGM':'FGM',\n                    'LFGA':'FGA', 'LTO':'TO', 'LScore':'Score', 'WScore':'OppScore'})\n        # drop all columns except the ones we are using\n        df_loseteam = df_loseteam[['TeamID', 'FGM', 'FGA', 'TO', 'Score', 'OppScore']] \n\n        # dataframe w/ all relevant stats from current year for current team\n        df_curr_team = pd.concat((df_winteam, df_loseteam)) \n\n        wins = df_winteam.shape[0]\n        FGPercent = df_curr_team['FGM'].sum() / df_curr_team['FGA'].sum()\n        TurnoverAvg = df_curr_team['TO'].sum() / len(df_curr_team['TO'].values)\n        PPG = df_curr_team['Score'].sum() / len(df_curr_team['Score'].values)\n        OppPPG = df_curr_team['OppScore'].sum() / len(df_curr_team['OppScore'].values)\n\n        # collect all data in rows list first for effeciency\n        rows.append([year, team, wins, FGPercent, TurnoverAvg, PPG, OppPPG])\n\ndf_training_data = pd.DataFrame(rows, columns=['Season', 'TeamID', 'Wins', 'FGPercent', \n                                               'TOAvg', 'PPG', 'OppPPG'])\ndf_training_data.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d9b6c64fe7718e17b75992866af9d9706782ea6c"},"cell_type":"markdown","source":"Here we show the contents of our seeding and tournament results data frames. These, combined with the stats calculated above (df_training_data) will form the final X_train matrix."},{"metadata":{"_cell_guid":"e5412069-c89b-4fed-9d2d-4690d6fd71b4","_uuid":"9f32e5f9104b7f10d3de7b38d3f292aef045c30f","trusted":false},"cell_type":"code","source":"df_seeds.head()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"3eaeb447-2e68-4790-9bcd-423ccdb7a117","_uuid":"dcb3b4cc84f09ea5af4d52da4fd970928e14bfc1","trusted":false},"cell_type":"code","source":"df_tour.head()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"ac5dd6af-b871-47c6-b7b0-ef6d85954971","_uuid":"42f99f53dd385e23b09378e0de9d3fce5eb1a2e9"},"cell_type":"markdown","source":"First, we'll simplify the datasets to remove the columns we won't be using and convert the seedings to the needed format (stripping the regional abbreviation in front of the seed)."},{"metadata":{"_cell_guid":"4e397ac8-7ac8-4ba7-b92f-571a0e75da18","_uuid":"fcb18269a41cfa257bd97c40664e43e701251bed","trusted":false},"cell_type":"code","source":"def seed_to_int(seed):\n    #Get just the digits from the seeding. Return as int\n    s_int = int(seed[1:3])\n    return s_int\ndf_seeds['seed_int'] = df_seeds.Seed.apply(seed_to_int)\ndf_seeds.drop(labels=['Seed'], inplace=True, axis=1) # This is the string label\ndf_seeds.head()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"90db3f9f-7d11-4bac-8d37-3331e4416c6a","_uuid":"1f6ecb82fa587f5a95a6833cd224b01407f5c90a","trusted":false},"cell_type":"code","source":"df_tour.drop(labels=['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], inplace=True, axis=1)\ndf_tour.head()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"88e7ab21-0380-42b2-9979-e30fef6e856a","_uuid":"3f223cdf4446d6e9c77ab8319237f05393d1a822"},"cell_type":"markdown","source":"## Merge seed for each team ##\nMerge the Seeds with their corresponding TeamIDs in the compact results dataframe."},{"metadata":{"_cell_guid":"a22a595b-a6cb-4291-81ae-903a9548cd37","_uuid":"53638c1ae27cfb24d47e02007c293d5ee19ebdac","trusted":false},"cell_type":"code","source":"df_winseeds = df_seeds.rename(columns={'TeamID':'WTeamID', 'seed_int':'WSeed'})\ndf_lossseeds = df_seeds.rename(columns={'TeamID':'LTeamID', 'seed_int':'LSeed'})\ndf_dummy = pd.merge(left=df_tour, right=df_winseeds, how='left', on=['Season', 'WTeamID'])\ndf_concat = pd.merge(left=df_dummy, right=df_lossseeds, on=['Season', 'LTeamID'])\ndf_concat['SeedDiff'] = df_concat.WSeed - df_concat.LSeed\ndf_concat.head()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"20ea1e39461941ed0756341326496d02199bbff2"},"cell_type":"markdown","source":"Now we'll combine our advanced season statistics and merge them into the df_concat data frame."},{"metadata":{"trusted":false,"_uuid":"be792971b37931e390ed5d1c266bbc4b1aa892dd"},"cell_type":"code","source":"df_winstats = df_training_data.rename(columns={'TeamID':'WTeamID', 'FGPercent':'WFGPercent', \n                            'TOAvg':'WTOAvg', 'PPG':'WPPG', 'OppPPG':'WOppPPG', 'Wins':'WWins'})\ndf_lossstats = df_training_data.rename(columns={'TeamID':'LTeamID', 'FGPercent':'LFGPercent',\n                            'TOAvg':'LTOAvg', 'PPG':'LPPG', 'OppPPG':'LOppPPG', 'Wins':'LWins'})\ndf_dummy = pd.merge(left=df_concat, right=df_winstats, on=['Season', 'WTeamID'])\ndf_concat = pd.merge(left=df_dummy, right=df_lossstats, on=['Season', 'LTeamID'])\ndf_concat['FGPercentDiff'] = df_concat.WFGPercent - df_concat.LFGPercent\ndf_concat['TOAvgDiff'] = df_concat.WTOAvg - df_concat.LTOAvg\ndf_concat['PPGDiff'] = df_concat.WPPG - df_concat.LPPG\ndf_concat['OppPPGDiff'] = df_concat.WOppPPG - df_concat.LOppPPG\ndf_concat['WWinMargin'] = df_concat.WPPG - df_concat.WOppPPG\ndf_concat['LWinMargin'] = df_concat.LPPG - df_concat.LOppPPG\ndf_concat['WinMarginDiff'] = df_concat.WWinMargin - df_concat.LWinMargin\ndf_concat['WinDiff'] = df_concat.WWins - df_concat.LWins\n # drop all columns except the ones we are using\ndf_concat = df_concat[['Season', 'WTeamID', 'LTeamID', 'SeedDiff', 'FGPercentDiff', \n                       'TOAvgDiff', 'PPGDiff', 'OppPPGDiff', 'WinMarginDiff', 'WinDiff']]\n\n# Note: We can have SeedDiff == 0 due to the First Four (68 teams)! Also Final Four onwards!\n# Note: Pandas merges tossed out data from before 2003!\ndf_concat.head()","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"72274b95-581c-4938-88a4-b3e8cb1787d4","_uuid":"1c82f60c02545c8c46ab090cb8cefca48e48e434"},"cell_type":"markdown","source":"Now we'll create a dataframe that summarizes wins & losses along with their corresponding seed differences, FG% differences, and turnover differences. This is the meat of what we'll be creating our model on."},{"metadata":{"_cell_guid":"4279fa78-4700-43d6-a92e-3a372715e0f3","_uuid":"1a40000e85c0dd9d2be6850a767acd736bf5f182","trusted":false},"cell_type":"code","source":"# We create positive and negative versions of the data so the \n# supervised learning algorithm has sample data of each class to classify\n\ndf_wins = pd.DataFrame()\ndf_wins['SeedDiff'] = df_concat['SeedDiff']\ndf_wins['FGPercentDiff'] = df_concat['FGPercentDiff']\ndf_wins['TOAvgDiff'] = df_concat['TOAvgDiff']\ndf_wins['PPGDiff'] = df_concat['PPGDiff']\ndf_wins['OppPPGDiff'] = df_concat['OppPPGDiff']\ndf_wins['WinMarginDiff'] = df_concat['WinMarginDiff']\ndf_wins['WinDiff'] = df_concat['WinDiff']\ndf_wins['Result'] = 1\n\ndf_losses = pd.DataFrame()\ndf_losses['SeedDiff'] = -df_concat['SeedDiff']\ndf_losses['FGPercentDiff'] = -df_concat['FGPercentDiff']\ndf_losses['TOAvgDiff'] = -df_concat['TOAvgDiff']\ndf_losses['PPGDiff'] = -df_concat['PPGDiff']\ndf_losses['OppPPGDiff'] = -df_concat['OppPPGDiff']\ndf_losses['WinMarginDiff'] = -df_concat['WinMarginDiff']\ndf_losses['WinDiff'] = -df_concat['WinDiff']\ndf_losses['Result'] = 0\n\ndf_predictions = pd.concat((df_wins, df_losses))\ndf_predictions.head()","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"e6eca4b5-2b22-4abf-9145-7a8d284faab7","_uuid":"3cf1b39303c44e73a3fa0f813a9580e91eca6b0b","collapsed":true,"trusted":false},"cell_type":"code","source":"X_train = [list(a) for a in zip(df_predictions.SeedDiff.values, df_predictions.FGPercentDiff.values, \n                                df_predictions.TOAvgDiff.values, df_predictions.PPGDiff.values,\n                                df_predictions.OppPPGDiff.values, df_predictions.WinMarginDiff.values,\n                                df_predictions.WinDiff.values)]\nX_train = np.array(X_train)\ny_train = df_predictions.Result.values\nX_train, y_train = shuffle(X_train, y_train)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"4b0751b0-a04a-4586-b612-6feb201cde7d","_uuid":"563937f42bcccd2bbfb8fc1a66a72a9ca1351f43"},"cell_type":"markdown","source":"## Train the model ##\nTrain the a variety of models. Tune the hyperparameters for each algorithm and perform cross validation. Logistic regression and SVC perform the best."},{"metadata":{"_cell_guid":"34a69207-0b92-4b28-8ad7-5d2cabec878a","_uuid":"95f817451eae9b72dc237e734e19c929be136d50","trusted":false},"cell_type":"code","source":"# Neural Network\nparams = {'hidden_layer_sizes': [(256,), (512,), (128, 256, 128,)]}\nmlp = MLPClassifier(learning_rate='adaptive')\nclf = GridSearchCV(mlp, params, scoring='neg_log_loss')\nclf.fit(X_train, y_train)\nprint('Best log_loss Multi Layer Perceptron Classifier: {}'.format(clf.best_score_))\n\n# Gradient Boosted Classifier\nGBC = GradientBoostingClassifier()\nparam_grid_GBC = {\n    \"n_estimators\" : [100],\n    \"learning_rate\" : [0.1, 0.05, 0.02, 0.01],\n    \"max_depth\" : [1,2,3],\n    \"min_samples_leaf\" : [1,3,5],\n    \"max_features\" : [1.0, 0.3, 0.1]\n}\nclf = GridSearchCV(GBC, param_grid_GBC, scoring='neg_log_loss')\nclf.fit(X_train, y_train)\nprint('Best log_loss Gradient Boosting Classifier: {}'.format(clf.best_score_))\n\n# Random Forest Classifier\nRFC = RandomForestClassifier()\nparam_grid_RFC = { \n    'n_estimators': [60, 120, 240],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\nclf = GridSearchCV(RFC, param_grid_RFC, scoring='neg_log_loss')\nclf.fit(X_train, y_train)\nprint('Best log_loss Random Forest Classifier: {}'.format(clf.best_score_))\n\n# K Nearest Neighbors Classifier\nknn = KNeighborsClassifier()\nk = np.arange(80)+1\nparameters = {'n_neighbors': k}\nclf = GridSearchCV(knn, parameters, scoring='neg_log_loss')\nclf.fit(X_train, y_train)\nprint('Best log_loss K-Nearest Neighbors Classifier: {}'.format(clf.best_score_))\n\n# SVC\nSVC = svm.SVC(probability=True)\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\ntuned_parameters_preselected = [{'kernel': ['linear'], 'C': [10]}]\nclf = GridSearchCV(SVC, tuned_parameters_preselected, scoring='neg_log_loss')\nclf.fit(X_train, y_train)\nprint('Best log_loss Support Vector Classification: {}'.format(clf.best_score_))\n\n# Logistic Regression\nlogreg = LogisticRegression()\nparams = {'C': np.logspace(start=-15, stop=15, num=31)} # {C: array[1^-15 , 1^-14, ... 1^15] }\nclf = GridSearchCV(logreg, params, scoring='neg_log_loss', refit=True) #sklearn model selection\nclf.fit(X_train, y_train)\nprint('Best log_loss Logistic Regression: {}, with best C: {}'.format(clf.best_score_, \n                                                                      clf.best_params_['C']))\n\n# Logistic Regression is typically the top-performer. We compute it last, and use \n# this classifier to make future predictions.\n\n# SVC is typically a close second. Comment out Logistic Regression to use \n# the SVC classifier instead to make future predictions\n\n# Keep in mind, the provided values are a single representation of our classifier's\n# success! Depending on how the data is shuffled, each run of the program may yield\n# a slightly different classifier (and thus different predictions/success rate)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"2bf46a87-8bbe-4964-935a-89e307f700b9","_uuid":"37e01a2b50f69e1f6a0aeaa50c7593d8cae15b1b","trusted":false},"cell_type":"code","source":"# Create training data with the seeds varying from -10, 10\n# All other features are zeroed out so the plot only shows\n# the relationship between seed and P(team1 wins)\nX1 = np.arange(-10, 10)\nX2 = np.zeros(20, dtype=np.int)\nX = [list(a) for a in zip(X1, X2, X2, X2, X2, X2, X2)]\nX = np.array(X)\n\npreds = clf.predict_proba(X)[:,1]\n\nplt.plot(X1, preds)\nplt.xlabel('Team1 seed - Team2 seed')\nplt.ylabel('P(Team1 will win)')","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"2e9a55cf-b938-426f-b92c-e92ded0a663f","_uuid":"3e8270e8638b6f78317b7f787cc0259af682dba7"},"cell_type":"markdown","source":"Plotting validates our intuition, that the probability a team will win decreases as the seed differential to its opponent decreases."},{"metadata":{"_cell_guid":"16e4b2a7-91a4-420f-9eee-abaa49ea028b","_uuid":"cd5a427eca09adda4e9a42a88208b683020a1f8d","collapsed":true,"trusted":false},"cell_type":"code","source":"df_sample_sub = pd.read_csv(data_dir + 'SampleSubmissionStage2.csv')\nn_test_games = len(df_sample_sub)\n\ndef get_year_t1_t2(ID):\n    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n    return (int(x) for x in ID.split('_'))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"f307a242c1b52b30f064aff94bec245e635dbd0e"},"cell_type":"markdown","source":"Now we create our X_test matrix with the expected dimensions for the Kaggle contest and fill it with zeroes. Then we loop over the sample submission, and initialize X_test with the correct features for 2018 teams. This X_test matrix is our test set for our previously trained classifier to make predictions about this year's tournament."},{"metadata":{"_cell_guid":"3d842b3e-783b-4ab5-94c5-97cedc9d08c4","_uuid":"72d64ebc20c903660108ae9c529be07859396909","collapsed":true,"trusted":false},"cell_type":"code","source":"X_test = np.zeros(shape=(n_test_games, 7))\n\nfor ii, row in df_sample_sub.iterrows():\n    year, t1, t2 = get_year_t1_t2(row.ID)\n    t1_seed = df_seeds[(df_seeds.TeamID == t1) & (df_seeds.Season == year)].seed_int.values[0]\n    t2_seed = df_seeds[(df_seeds.TeamID == t2) & (df_seeds.Season == year)].seed_int.values[0]\n    diff_seed = t1_seed - t2_seed\n    X_test[ii, 0] = diff_seed\n    \n    t1_FGPercent = df_training_data[(df_training_data.TeamID == t1) & \n                                    (df_training_data.Season == year)].FGPercent.values[0]\n    t2_FGPercent = df_training_data[(df_training_data.TeamID == t2) & \n                                    (df_training_data.Season == year)].FGPercent.values[0]\n    diff_FGPercent = t1_FGPercent - t2_FGPercent\n    X_test[ii, 1] = diff_FGPercent\n    \n    t1_TOAvg = df_training_data[(df_training_data.TeamID == t1) & \n                                (df_training_data.Season == year)].TOAvg.values[0]\n    t2_TOAvg = df_training_data[(df_training_data.TeamID == t2) & \n                                (df_training_data.Season == year)].TOAvg.values[0]\n    diff_TOAvg = t1_TOAvg - t2_TOAvg\n    X_test[ii, 2] = diff_TOAvg\n    \n    t1_PPG = df_training_data[(df_training_data.TeamID == t1) & \n                              (df_training_data.Season == year)].PPG.values[0]\n    t2_PPG = df_training_data[(df_training_data.TeamID == t2) & \n                              (df_training_data.Season == year)].PPG.values[0]\n    diff_PPG = t1_PPG - t2_PPG\n    X_test[ii, 3] = diff_PPG\n    \n    t1_OppPPG = df_training_data[(df_training_data.TeamID == t1) & \n                                 (df_training_data.Season == year)].OppPPG.values[0]\n    t2_OppPPG = df_training_data[(df_training_data.TeamID == t2) & \n                                 (df_training_data.Season == year)].OppPPG.values[0]\n    diff_OppPPG = t1_OppPPG - t2_OppPPG\n    X_test[ii, 4] = diff_OppPPG\n    \n    X_test[ii, 5] = diff_PPG - diff_OppPPG # Win Margin\n    \n    t1_Wins = df_training_data[(df_training_data.TeamID == t1) & \n                                 (df_training_data.Season == year)].Wins.values[0]\n    t2_Wins = df_training_data[(df_training_data.TeamID == t2) & \n                                 (df_training_data.Season == year)].Wins.values[0]\n    X_test[ii, 6] = t1_Wins - t2_Wins","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"8bacc197-297f-465d-a49c-9b988619c166","_uuid":"375748512c55520e00ffd5701c82704856478370"},"cell_type":"markdown","source":"## Make Predictions ##\nCreate predictions using the logistic regression model we trained."},{"metadata":{"_cell_guid":"b3da5070-e0e3-445a-9af1-ced10032bcc7","_uuid":"65dc063a2e9c5e447d800556f7cf67b26b7cbedb","trusted":false},"cell_type":"code","source":"preds = clf.predict_proba(X_test)[:,1]\n\nclipped_preds = np.clip(preds, 0.05, 0.95)\ndf_sample_sub.Pred = clipped_preds\ndf_sample_sub.head()","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"72494058-d8c5-4ab4-99cd-4b50d7d7135c","_uuid":"3f4ef6ab893953a811462d240778205c2fdecf97"},"cell_type":"markdown","source":"Lastly, create your submission file!"},{"metadata":{"_cell_guid":"78ded09c-4ee1-49a2-901b-0cbe2ac3f114","_uuid":"7c784a9b62d889e83493b70efa17bd233f9abff4","collapsed":true,"trusted":false},"cell_type":"code","source":"df_sample_sub.to_csv('predictions.csv', index=False)","execution_count":17,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}