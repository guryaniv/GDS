{"nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "cells": [{"metadata": {"_cell_guid": "79e16cd8-aa8a-466d-aef0-5cd415ccae71", "_uuid": "5b6ee3740a35177df75730c3fd93c8d1058fd1d7"}, "cell_type": "markdown", "source": ["# CNN for text classification\n", "I'm late to this party (as always), but I will share this with you in case if you find it useful.  \n", "This is my first kernel, I'm new to ML and text mining, I've started python two months ago (expect some unusual code) and I will never close it back :)  \n", "  \n", "This notebook is set to run on kaggle, but with large sacrifice on model performance and with overfit as training data is truncated. You should run this on your local env and reset all variables as suggested.  You should reach ~0.8 log loss with ease on this fast training model.\n", "  \n", "Good luck!"]}, {"metadata": {"_cell_guid": "4d56cfbf-0315-4253-a155-61b09935be18", "_uuid": "11d23c04081a092dc6c1a45d8aa36a20cd3d16af"}, "cell_type": "markdown", "source": ["### Common includes"]}, {"metadata": {"_cell_guid": "face5577-4513-4d2f-8caf-e9190e043ed8", "collapsed": true, "_uuid": "f6df16a6121c5b646f75f7e5b917c4a656b39a7d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from IPython.display import display"]}, {"metadata": {"_cell_guid": "12221acf-7a28-4aec-9c0d-f4ba3f46fc10", "_uuid": "8f3e7563b47a660198c86c212114a878cbda2b79"}, "cell_type": "markdown", "source": ["### Prepare data\n", "training_variants+training_text+test_variants[stage1_solution]+test_text[stage1_solution]"]}, {"metadata": {"_kg_hide-output": false, "_cell_guid": "94a89b6d-1055-4685-9070-73a74bf47364", "_kg_hide-input": false, "_uuid": "5c9cd3e5fe706df91333b97a05d667731ebad887"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# load training variants\n", "train = pd.read_csv('../input/training_variants')\n", "# load training text\n", "train_txt_ = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "# merge text & variants\n", "train = pd.merge(train, train_txt_, how='left', on='ID').fillna('')\n", "# clean up\n", "del train_txt_\n", "# print train data info\n", "display(train.info())\n", "\n", "# load test variants from stage 1\n", "testold_var_ = pd.read_csv('../input/test_variants')\n", "# load test text from stage 1\n", "testold_txt_ = pd.read_csv('../input/test_text', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "# merge text & variants\n", "testold_ = pd.merge(testold_var_, testold_txt_, how='left', on='ID').fillna('')\n", "# clean up\n", "del testold_var_\n", "del testold_txt_\n", "\n", "# load stage1 solutions\n", "stage1sol_ = pd.read_csv('../input/stage1_solution_filtered.csv')\n", "# get class\n", "stage1sol_['Class'] = pd.to_numeric(stage1sol_.drop('ID', axis=1).idxmax(axis=1).str[5:]).fillna(0).astype(np.int64)\n", "# drop records from testold_ if they are not in stage1sol_\n", "testold_ = testold_[testold_.index.isin(stage1sol_['ID'])]\n", "# merge class to testold_ from stage1sol_\n", "newtraindata_ = testold_.merge(stage1sol_[['ID', 'Class']], on='ID', how='left')\n", "# reindex columns\n", "newtraindata_ = newtraindata_.reindex_axis(['ID','Gene','Variation','Class','Text'], axis=1)\n", "# clean up\n", "del stage1sol_\n", "del testold_\n", "\n", "# append new train data\n", "train = train.append(newtraindata_)\n", "# clean up\n", "del newtraindata_\n", "\n", "# print train data info\n", "display(train.info())"]}, {"metadata": {"_cell_guid": "cac68d4f-f92c-4015-bbf0-d417dc9ab091", "_uuid": "548cae4dc15de777fabed4598cd5b6bb92444b83"}, "cell_type": "markdown", "source": ["### Load word2vec\n", "When running on local env download word2vec binary file from [bio.nlplab.org](http://bio.nlplab.org/).  \n", "**Word2vec can drastically improve model performance.**"]}, {"metadata": {"_cell_guid": "ec70fd13-cb3d-41c9-9dea-0621080ae54b", "_uuid": "0eb26447b002fe423511a8088f586daaa651098e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print('Indexing word vectors.')\n", "import os\n", "from gensim.models import KeyedVectors\n", "word2vec = None\n", "# make sure you load this on your local env and uncomment the line\n", "#word2vec = KeyedVectors.load_word2vec_format('PubMed-and-PMC-w2v.bin', binary=True)\n", "if (word2vec == None):\n", "    print(\"word2vec not loaded!\")\n", "else:\n", "    print(\"Found {} word vectors of word2vec\".format(len(word2vec.vocab)))"]}, {"metadata": {"_cell_guid": "969120dd-8a2d-44ee-b72f-ec7862d8298b", "_uuid": "cab5ec97be51c28e7e6977772353b13e741e7ad3"}, "cell_type": "markdown", "source": ["### Truncate dataset for kaggle limit (or faster model hyperparameter search)\n", "Split is done by truncating data equally by classes and corrects original dataset class distrubution."]}, {"metadata": {"_cell_guid": "7ef2fb06-50e2-4e56-b078-43ea6fee2909", "_uuid": "eff43d8e87be0ac4bd74460bc846d6352a37fba7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# due kaggle limit we will truncate train database, remove this block if running on local env\n", "# debug msg\n", "print('Split dataset.')\n", "# set to max value of orig dataset\n", "maxsize = len(train)\n", "# check class distrubution and find min sample size\n", "for c in range(1,10):\n", "    _ = len(train[train['Class']==c])\n", "    if (_ < maxsize):\n", "        maxsize = _\n", "# debug msg\n", "print('max size', maxsize)\n", "# create new dataframe\n", "train_ = pd.DataFrame(columns=train.columns)\n", "for c in range(1,10):\n", "    # append samples from train of length maxsize\n", "    train_ = train_.append(train[train['Class']==c][:maxsize], ignore_index=True)\n", "# display truncated data\n", "display(train_.head())\n", "# debug msg\n", "print('Train dataset old size {} new size {}'.format(len(train),len(train_)))\n", "# overwrite train with truncated train data\n", "train = train_\n", "# debug msg\n", "print('Split dataset done')"]}, {"metadata": {"_cell_guid": "7934df5f-f277-4870-8ca5-c1638d23013a", "_uuid": "8b079953fc90db70559cb48d691b440d5e39ba1b"}, "cell_type": "markdown", "source": ["### Expand dataset by seperation of sentences in batches\n", "I found this increases model performance, keeps the model small and helps with small dataset."]}, {"metadata": {"_cell_guid": "6e715cd0-7fc1-4208-87fb-30debc86bfe6", "_uuid": "a923fa4f347f37427dfd25019dfff5c49f3e3161"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import nltk\n", "\n", "# Create a function called \"chunks\" with two arguments, l and n:\n", "def chunks(l, n):\n", "    # For item i in a range that is a length of l,\n", "    for i in range(0, len(l), n):\n", "        # Create an index range for l of n items:\n", "        yield l[i:i+n]\n", "\n", "print('Expand records to sentences.')\n", "# increase maxnumberofsentecs on local env to 400\n", "maxnumberofsentences = 200\n", "# increase splitbysenteces on local env to 10\n", "splitbysentences = 2\n", "# temp dict for new train set\n", "tmpdf_ = {'Text': [], 'Class': [], 'ID': [], 'Gene': [], 'Variation': []}\n", "for index, row in train.iterrows():\n", "    # get sentences nltk\n", "    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n", "    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n", "    if (len(sent_tokenize_list) > maxnumberofsentences):\n", "        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n", "    # split sentences to batch\n", "    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n", "    for chunk in sent_chunk:\n", "        # join sentences in text\n", "        tmpdf_['Text'].append(\" \".join(chunk))\n", "        # assign class\n", "        tmpdf_['Class'].append(row['Class'])\n", "        # assign ID\n", "        tmpdf_['ID'].append(row['ID'])\n", "        # assign Gene\n", "        tmpdf_['Gene'].append(row['Gene'])\n", "        # assign Variation\n", "        tmpdf_['Variation'].append(row['Variation'])\n", "# create new train set from temp dict\n", "origtrainlen = len(train)\n", "train = pd.DataFrame(tmpdf_)\n", "# clean up\n", "del tmpdf_\n", "# display head\n", "display(train.head())\n", "# display \n", "print('expanded from {} to {}'.format(origtrainlen,len(train)))"]}, {"metadata": {"_cell_guid": "0407e2fd-6f60-4a81-8ae4-515adc1ba446", "_uuid": "7676af3fa2f381434303cc658541f393be2a35e8"}, "cell_type": "markdown", "source": ["### Tokenizer and embedding\n", "When no word2vec is loaded model will try to learn weights by itself."]}, {"metadata": {"_cell_guid": "ed1343bd-8d82-4d82-ad27-a864f044dae2", "_uuid": "5092e93cd3033d134ff4aa4f2dd3b9797ab9873a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "# max top words, increase on local env to 100000\n", "num_words = 5000\n", "# max sequence length, increase on local env to 500\n", "sequencelength = 200\n", "# init tokenizer\n", "tokenizer = Tokenizer(num_words=num_words)\n", "# fit tokenizer\n", "tokenizer.fit_on_texts(train['Text'])\n", "# get sequences\n", "X = tokenizer.texts_to_sequences(train['Text'])\n", "# unique words in text\n", "word_index = tokenizer.word_index\n", "print(\"Found {} unique tokens.\".format(len(word_index)))\n", "# pad sequences\n", "X = pad_sequences(X, maxlen=sequencelength)\n", "\n", "embedding_matrix = None\n", "if (word2vec != None):\n", "    # out of vocabulary words > use this to do text analysis\n", "    oov_words = []\n", "    # prepare embedding matrix\n", "    embedding_matrix = np.zeros((num_words+1, 200)) #200 = word2vec dim\n", "    for word, i in word_index.items():\n", "        if i >= num_words:\n", "            continue\n", "        if word in word2vec.vocab:\n", "            # embedd from word2vec\n", "            embedding_matrix[i] = word2vec.word_vec(word)\n", "        else:\n", "            # add to out of vocabulary\n", "            oov_words.append(word)\n", "    print('Preparing embedding matrix done. out-of-vocabulary rate (OOV): {} ({})'.format(len(oov_words)/float(len(word_index)),len(oov_words)))\n", "    "]}, {"metadata": {"_cell_guid": "9823cf62-450e-4676-a91a-ea01a95f74dd", "_uuid": "89984b763e3ebe61ec5fad74cfe8349110a0b803"}, "cell_type": "markdown", "source": ["### CNN model\n", "Data class distribution is uneven, so we will use class weights for correction.  \n", "On local env you should train longer and with smaller lr. Model should reach ~0.8 log loss."]}, {"metadata": {"_cell_guid": "e0f84946-bdb9-415b-b336-66b4e39720ab", "_uuid": "e2c59c0ede0cb81b9de53ebbc10eceab8ec73b02"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import keras\n", "from sklearn.utils import class_weight\n", "\n", "embed_dim = 200 #same as word2vec dim\n", "\n", "model_filename = 'model'\n", "\n", "# prepare Y values\n", "Y = train['Class'].values-1\n", "# get weights for unevenly distributed dataset \n", "class_weight = class_weight.compute_class_weight('balanced', np.unique(Y), Y)\n", "# one hot\n", "Y = keras.utils.to_categorical(Y)\n", "# batch size increase on local env\n", "batch_size = 20\n", "# epochs increase on local env\n", "epochs = 3\n", "# Model saving callback\n", "ckpt_callback = keras.callbacks.ModelCheckpoint(model_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n", "\n", "# input layer\n", "input1 = keras.layers.Input(shape=(sequencelength,))\n", "# embedding layer\n", "if (embedding_matrix == None):\n", "    # word2vec was not loaded. use fallback method\n", "    embedding = keras.layers.Embedding(num_words+1, embed_dim, trainable=True)(input1)\n", "else:\n", "    # word2vec was loaded, load weights and set to untrainable\n", "    embedding = keras.layers.Embedding(num_words+1, embed_dim, weights=[embedding_matrix], trainable=False)(input1)\n", " \n", "# conv layers\n", "convs = []\n", "filter_sizes = [2,3,4]\n", "for fsz in filter_sizes:\n", "    l_conv = keras.layers.Conv1D(filters=100,kernel_size=fsz,activation='relu')(embedding)\n", "    l_pool = keras.layers.MaxPooling1D(sequencelength-100+1)(l_conv)\n", "    l_pool = keras.layers.Flatten()(l_pool)\n", "    convs.append(l_pool)\n", "# merge conv layers\n", "l_merge = keras.layers.concatenate(convs, axis=1)\n", "# drop out regulation\n", "l_out = keras.layers.Dropout(0.5)(l_merge)\n", "# output layer\n", "output = keras.layers.Dense(units=9, activation='softmax')(l_out)\n", "# model\n", "model = keras.models.Model(input1, output)\n", "# compile model\n", "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\n", "# train model\n", "model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, class_weight=class_weight, callbacks=[ckpt_callback])"]}, {"metadata": {"_cell_guid": "dbe62309-dec4-4514-8d48-7ef6d0d523f2", "_uuid": "919e027a05a87a2c9e294c495213ab8e3800817e"}, "cell_type": "markdown", "source": ["### Prepare test data"]}, {"metadata": {"_cell_guid": "a8ffdd57-4eaa-413b-8787-1f2b6dba3e51", "_uuid": "cf1d41547a8a944a8aea5de29de1fa5050e9b335"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# load test dataset\n", "test = pd.read_csv('../input/stage2_test_variants.csv')\n", "# load test text dataset\n", "test_txt_ = pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "# merge text & variants\n", "test = pd.merge(test, test_txt_, how='left', on='ID')\n", "# clean up\n", "del test_txt_"]}, {"metadata": {"_cell_guid": "b487c26f-d4ad-42bc-908d-4881c5ea53aa", "_uuid": "3bf07b49849229fc76fc4a432f614b49d171b804"}, "cell_type": "markdown", "source": ["### Expand dataset by seperation of sentences in batches\n", "Same as we did with training data."]}, {"metadata": {"_cell_guid": "3b350995-f156-46a4-882b-c0a4fe6ad0cf", "_uuid": "f78f8bf0aaac90b458fd20da657a07d5ed02ea08"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print('Expand records to sentences.')\n", "# temp dict for new train set\n", "tmpdf_ = {'Text': [], 'ID': [], 'Gene': [], 'Variation': []}\n", "for index, row in test.iterrows():\n", "    # get sentences nltk\n", "    sent_tokenize_list = nltk.sent_tokenize(row['Text'])\n", "    # truncate sentences to last maxnumberofsentences (most important informations are at the end of text)\n", "    if (len(sent_tokenize_list) > maxnumberofsentences):\n", "        sent_tokenize_list = sent_tokenize_list[len(sent_tokenize_list)-maxnumberofsentences:]\n", "    # split sentences to batch\n", "    sent_chunk = list(chunks(sent_tokenize_list, splitbysentences))\n", "    for chunk in sent_chunk:\n", "        # join sentences in text\n", "        tmpdf_['Text'].append(\" \".join(chunk))\n", "        # assign ID\n", "        tmpdf_['ID'].append(row['ID'])\n", "        # assign Gene\n", "        tmpdf_['Gene'].append(row['Gene'])\n", "        # assign Variation\n", "        tmpdf_['Variation'].append(row['Variation'])\n", "# create new train set from temp dict\n", "origtestlen = len(test)\n", "test = pd.DataFrame(tmpdf_)\n", "# clean up\n", "del tmpdf_\n", "# display head\n", "display(test.head())\n", "# display \n", "print('expanded from {} to {}'.format(origtestlen,len(test)))"]}, {"metadata": {"_cell_guid": "bdd99716-e8ff-4439-b19d-99516e4cb5d8", "_uuid": "bcebcdf6c2b76c118d860953f5e6fff688fc7eb7"}, "cell_type": "markdown", "source": ["### Predict on test set and save to submission file"]}, {"metadata": {"_cell_guid": "4e7845f3-0a02-4d88-8057-e3d944e76e4b", "_uuid": "549759160265e33ea699b0f25f96c17e306496a6"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# load best model\n", "model = keras.models.load_model(model_filename)\n", "# get sequences\n", "Xtest = tokenizer.texts_to_sequences(test['Text'])\n", "# pad sequences\n", "Xtest = pad_sequences(Xtest, maxlen=sequencelength)\n", "# predict\n", "probas = model.predict(Xtest, verbose=1)\n", "# prepare data for submission\n", "submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])\n", "# insert IDs\n", "submission_df.insert(loc=0, column='ID', value=test['ID'].values)\n", "# average grouped data\n", "submission_df = submission_df.groupby(['ID'], as_index=False).mean()\n", "# save to csv\n", "submission_df.to_csv('submission.csv', index=False)\n", "# debug\n", "print(\"\\n----------------------\\n\")\n", "print(\"Done\")"]}]}