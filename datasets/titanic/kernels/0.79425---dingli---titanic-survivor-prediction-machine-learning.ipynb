{"cells":[{"metadata":{"_cell_guid":"16d0a6a7-a435-4d33-a73b-0322717c94c5","_uuid":"f3d6ea2696ed4407e64fa570602522eebcff2109"},"cell_type":"markdown","source":"**Introduction**\n\nIn this kernal for the project [Titanic: Machine Learning from Disaster](https://www.kaggle.com/competitions), I cleaned and explored the passenger data first, then compared some tipycal ML models to predict Titanic survivors,  and turned the hyper parameters of decision tree model to overcome the overfitting. \n\nKaggle is a great place to learn ML, I learned a lot from the following kernals:\n* [Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions)\n* [A Data Science Framework](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n\n"},{"metadata":{"_cell_guid":"a3323799-c582-4d57-9ba5-1dca2329a823","_uuid":"4359e5b5c15c07472d027305ea7a0d2bad86d1a1"},"cell_type":"markdown","source":"**Import Packages**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# data process\nimport numpy as np \nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz \n%matplotlib inline\n\n# modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble\nfrom xgboost import XGBClassifier\n\n# system\nimport os\nimport sys\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Python version\nprint(\"Python version: {}\". format(sys.version))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"404d0205-059d-4e20-8f11-0c69dbb21566","_uuid":"be5c00663f9aa886ebf514026d24657e79bb985c"},"cell_type":"markdown","source":"**Data Loading & Initial Check**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\nprint(os.listdir(\"../input\"))\n\n# read data into pandas' data frame\ntrain_raw = pd.read_csv('../input/train.csv')\ntest_raw = pd.read_csv('../input/test.csv')\n\n# make a deep copy to keep original data\ntrain_df = train_raw.copy(deep = True)\ntest_df = test_raw.copy(deep = True)\ncombine = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e90e9385-f551-4ce4-882a-876b0117f10e","_uuid":"2923c9ff4276c399ebafdd7f501a44bf1cb2da7c","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# training data sample\ntrain_df.head()\n\n# test data sample\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60ba5d7a-8df3-407f-acdf-5645070cca01","_uuid":"f9d0a55ea30816ca23830416e0280312159fceec","collapsed":true,"trusted":true},"cell_type":"code","source":"# training data info\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d4a37ca-38cc-4ad1-8968-ff8ce691396b","_uuid":"9caa0ea5cdf556f68dbae42bc0a12c9c3a3eace9","collapsed":true,"trusted":true},"cell_type":"code","source":"# test data info\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5df8443-a33b-4974-8d4b-5f8be5bdb7e3","_uuid":"17bacff907fce108729294cce9538e7f055228ac"},"cell_type":"markdown","source":"**Data Cleaning and Completion**"},{"metadata":{"_cell_guid":"4b065c8f-b688-4476-a0d0-084203a820a3","_uuid":"dce4894616c51036e1b0266927297a8752502fa1","collapsed":true,"trusted":true},"cell_type":"code","source":"# check missing value\nprint('Train columns with null values:\\n', train_df.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72ec82a6-a428-4307-8fa9-8179fd6be1e2","_uuid":"22c46416a8d6ae16d1b3c5f40760d2c3a1d0d49d","collapsed":true,"trusted":true},"cell_type":"code","source":"# complete missing values in train and test dataset\nfor dataset in combine:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"401308c5-8867-41ec-8d87-1043d2acd0f4","_uuid":"6156f1907e78241291aee6792da944f8d03b4a51","collapsed":true,"trusted":true},"cell_type":"code","source":"# delete columns not informative\ndrop_column = ['Cabin', 'Ticket']\nfor dataset in combine:  \n    dataset.drop(drop_column, axis=1, inplace=True)\n               ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9413c8c1-1942-43c8-b8a9-a66144af70e3","_uuid":"08939113328d3e0feb52cfd689eec44a3f5dcd86"},"cell_type":"markdown","source":"**Data Transform**"},{"metadata":{"_cell_guid":"0605a3af-e1c4-4f04-89d7-92520610ea34","_uuid":"2ea1e18f24cd810e757ccf953725cba52a67b522","collapsed":true,"trusted":true},"cell_type":"code","source":"# create features for modeling\nfor dataset in combine:    \n\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8191de5-95a0-4575-9041-cdddd3999f43","_uuid":"0ddab9c695c641116c1bdfb30f815fe1bec69ba2","collapsed":true,"trusted":true},"cell_type":"code","source":"# rename the less frequent Titile to 'Misc'\n#pd.crosstab(train_df['Title'], train_df['Sex'])\nmain_titles = (train_df['Title'].append(test_df['Title']).value_counts() >10)\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].apply(lambda x: x if main_titles.loc[x] == True else 'Misc')\ntrain_df['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d8258ea-ad3d-4065-868c-17feb1c0f879","_uuid":"e782679968116e0722e9ccca7500e26444b0a135","collapsed":true,"trusted":true},"cell_type":"code","source":"# convert objects to category\nlabel = LabelEncoder()\nfor dataset in combine:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22774cb9-cedd-4d1a-b3fa-a96dc686218c","_uuid":"6a38f068fc91ce8bff2ce9a2774da114ebc176c3","collapsed":true},"cell_type":"markdown","source":"**Data Exploration**"},{"metadata":{"_cell_guid":"b1cab734-ddea-48d1-896b-07e07f206355","_uuid":"5e07747906c8494731bd9bfa82c554001f70ad62","collapsed":true,"trusted":true},"cell_type":"code","source":"# check surviving rate for dimensions\nTarget = ['Survived']\ncolumns_to_check = ['Sex','Pclass', 'Embarked', 'Title', 'FamilySize', 'IsAlone']\nfor x in columns_to_check:\n    print('Survival Correlation by:', x)\n    #print(train_df[[x, Target[0]]].groupby(x, as_index=False).mean())\n    print(train_df[[x, Target[0]]].groupby(x, as_index=False).agg(['count', 'mean']))\n    print('-'*10, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc05d2a6-2944-47c1-8972-12fa32d858c2","_uuid":"7c40f365c81a22acc119f2e747513fd1ed4151ff","collapsed":true,"trusted":true},"cell_type":"code","source":"#graph individual features by survival\nsns.set_context(\"paper\", font_scale=1.6) \nfig, saxis = plt.subplots(2, 3,figsize=(10,8))\nfig.tight_layout()\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=train_df, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train_df, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=train_df, ax = saxis[0,2])\n\nax1=sns.pointplot(x = 'FareBin', y = 'Survived',  data=train_df, ax = saxis[1,0])\nax1.set_xticklabels(ax1.get_xticklabels(), rotation = 30)\nax2 = sns.pointplot(x = 'AgeBin', y = 'Survived',  data=train_df, ax = saxis[1,1])\nax2.set_xticklabels(ax2.get_xticklabels(), rotation = 30)\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=train_df, ax = saxis[1,2])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3aed536f-800c-46a4-b71e-a2009ce87c13","_uuid":"25928dcaa1e3598d438d4ae73c9ea4dc9b84aeb5","collapsed":true,"trusted":true},"cell_type":"code","source":"#more side-by-side comparisons\nfig, (maxis1, maxis2, maxis3, maxis4) = plt.subplots(1, 4, figsize=(16,8))\nfig.tight_layout() # increase space between plots\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does fair factor with sex & survival compare\nsns.pointplot(x=\"FareBin\", y=\"Survived\", hue=\"Sex\", data=train_df,\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)\nmaxis2.set_xticklabels(maxis2.get_xticklabels(), rotation = 30)\n\n#how does age factor with sex & survival compare\nsns.pointplot(x=\"AgeBin\", y=\"Survived\", hue=\"Sex\", data=train_df,\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis3)\nmaxis3.set_xticklabels(maxis3.get_xticklabels(), rotation = 30)\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=train_df,\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis4)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42d51a7d-307e-4eea-91cb-05e80e7724b5","_uuid":"36074a1b95d62f6a35f56263dfc8937a773e3496"},"cell_type":"markdown","source":"**Modeling and Prediction**"},{"metadata":{"_cell_guid":"e1c185a6-6146-430f-bff9-8d65816d2f7d","_uuid":"e6dbc0c47871bb4b94af56419c866f65d00b46d3","collapsed":true,"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e93ccbff-2754-4f08-9280-fa15fc24ef0b","_uuid":"671d4da2e5a1fa8d0b1d43232db832ec56e286ce","collapsed":true,"trusted":true},"cell_type":"code","source":"# select features for modeling\ndata_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata_xy_bin = Target + data_x_bin\n\ndata = train_df[data_xy_bin]\nX = train_df[data_x_bin]\ny = train_df[Target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_pred = test_df[data_x_bin]\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"351e281a-4f55-4a40-9d06-ddfb909ccac2","_uuid":"edaffc87b831ab3e7000d83174de731d9dcbd3b7","collapsed":true,"trusted":true},"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(10, 8))\n    sns.set(font_scale=1.5) \n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        #cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    #plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f127175-b5ba-4ef4-8ed1-e46ad8c4f736","_uuid":"2f2ade041bfe502b0ba7bf82b6d86e4437df6913","collapsed":true,"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_train_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_test_log = round(logreg.score(X_test, y_test) * 100, 2)\nprint('logistic regression train accurary: ',acc_train_log)\nprint('logistic regression test accurary: ',acc_test_log)\n\n#coefficients of each factors\ncoeff_df = pd.DataFrame(X_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96688b8e-7a8f-4ecc-b12a-dc4bf2887083","_uuid":"f50270cfbff6cfc6edfba53d720375f37a06b6e3","collapsed":true,"trusted":true},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nacc_train_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_test_svc = round(svc.score(X_test, y_test) * 100, 2)\nprint('Support Vector Machine train accurary: ',acc_train_svc)\nprint('Support Vector Machine test accurary: ',acc_test_svc)\n\n# K-Nearest Neighbors\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nacc_train_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_test_knn = round(knn.score(X_test, y_test) * 100, 2)\nprint('K-Nearest Neighbors train accurary: ',acc_train_knn)\nprint('K-Nearest Neighbors test accurary: ',acc_test_knn)\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nacc_train_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_test_decision_tree = round(decision_tree.score(X_test, y_test) * 100, 2)\nprint('Decision Tree train accurary: ',acc_train_decision_tree)\nprint('Decision Tree test accurary: ',acc_test_decision_tree)\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n#Y_pred = random_forest.predict(X_test)\nacc_train_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_test_random_forest = round(random_forest.score(X_test, y_test) * 100, 2)\nprint('Random Forest train accurary: ',acc_train_random_forest)\nprint('Random Forest test accurary: ',acc_test_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a68fb410-71b5-4ed9-96c1-d7deb2d43336","_uuid":"f62be5dbdc86a5eb27e9145c784cf1d3e94e7e20","collapsed":true},"cell_type":"markdown","source":"**Model Comparison**"},{"metadata":{"_cell_guid":"c02c627c-368d-42e8-b90c-3a6c37a23779","_uuid":"db70694549d3d18baca968ddfad7ab0b8a88a4a0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    #ensemble.AdaBoostClassifier(),\n    #ensemble.BaggingClassifier(),\n    #ensemble.ExtraTreesClassifier(),\n    #ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    #linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    #linear_model.Perceptron(),\n    \n    #Navies Bayes\n    #naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    #svm.NuSVC(probability=True),\n    #svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    #tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    #discriminant_analysis.LinearDiscriminantAnalysis(),\n    #discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = y\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, train_df[data_x_bin], train_df[Target],cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions\n    alg.fit(train_df[data_x_bin], train_df[Target])\n    MLA_predict[MLA_name] = alg.predict(train_df[data_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39eab496-d3cf-46d3-ac7c-beb233976371","_uuid":"f942cc198eada8ea8afde2da7af8c9fb8d578966","collapsed":true},"cell_type":"markdown","source":"**Tune Model with Hyper-Parameters**"},{"metadata":{"_cell_guid":"0cf2128c-daa4-410c-9b2f-6465cbca1ab3","_uuid":"1b79aa09216a5d6b8f61269c29d5f2b305514384","collapsed":true,"trusted":true},"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, train_df[data_x_bin], train_df[Target], cv  = cv_split)\ndtree.fit(train_df[data_x_bin], train_df[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bea461a-4142-4a55-8f87-79445626d577","_uuid":"8a5875fa893dbce45130062b641dd8631bb071a0","collapsed":true,"trusted":true},"cell_type":"code","source":"#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03406197-7558-438c-8bae-2e873ae08484","_uuid":"bcb7a1d14ac7cf9c7500333e5dffbc19738f94b3","collapsed":true,"trusted":true},"cell_type":"code","source":"#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n             }\n\n#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(train_df[data_x_bin], train_df[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n\ndtree = tree.DecisionTreeClassifier(criterion='gini', max_depth=4, random_state = 0)\ndtree.fit(train_df[data_x_bin], train_df[Target])\ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6802cad6-b38c-497d-bef8-9dc105c6bdcd","_uuid":"d974272003ea0db1e485f0b1708cefb2e46f9e70","collapsed":true,"trusted":true},"cell_type":"code","source":"Y_pred = dtree.predict(X_pred)\n\n# get result file\nsubmission_tree = pd.DataFrame({\n        \"PassengerId\": test_df['PassengerId'],\n        \"Survived\": Y_pred\n    })\n\nsubmission_tree.head()\n\n#save the submission file\nsubmission_tree.to_csv('submission_tree_turned.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}