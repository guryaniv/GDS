{"cells":[{"metadata":{"_uuid":"3b744eb3cbd7eb0f8dfd5c090b04feb1034fea87"},"cell_type":"markdown","source":"Optimizing hyperparameters can be a painfully slowexperience even for some of the most experienced data scientist. The most popular choices currently are Random or Grid searches. In my experience i have found that Grid Searches take an inordinant amount of time due to them trying every possible combination of hyperparameters and Random Searches have no garuntee of finding a good let alone great set of hyperparameters. Lately i've stumbled onto using Bayesian Optimization stratergies which are much quicker than full grid searches and also seem to have a higher chance of converging on a great set of hyperparameters."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab3e4ff26b086017fa4ba7f6fbc567ef8532f2fd"},"cell_type":"markdown","source":"Lets prepare the dataset (using code from other kagglers)..."},{"metadata":{"trusted":true,"_uuid":"2ee9217d22ab84fe6073d21dcb99fb91cc3f793e","collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\n\n# borrowed from other kagglers: https://www.kaggle.com/hmendonca/testing-engineered-features-lb-1-42\n# Find and drop duplicate rows\nt = train_df.iloc[:,2:].duplicated(keep=False)\nduplicated_indices = t[t].index.values\nprint(\"Removed {} duplicated rows: {}\".format(len(duplicated_indices), duplicated_indices))\ntrain_df.iat[duplicated_indices[0], 1] = np.expm1(np.log1p(train_df.target.loc[duplicated_indices]).mean()) # keep and update first with log mean\ntrain_df.drop(duplicated_indices[1:], inplace=True) # drop remaining\n\n# Columns to drop because there is no variation in training set\nzero_std_cols = train_df.drop(\"ID\", axis=1).columns[train_df.std() == 0]\ntrain_df.drop(zero_std_cols, axis=1, inplace=True)\nprint(\"Removed {} constant columns\".format(len(zero_std_cols)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f181769663949e80d475c85bd627cd8a2e4f8c"},"cell_type":"markdown","source":"lets log transform the columns..."},{"metadata":{"trusted":true,"_uuid":"2319f998eca3b72608938be29d33d064ba6e62c4","collapsed":true},"cell_type":"code","source":"# Log-transform all column\ntrain_df = train_df.drop(['ID'], axis=1)\ntrain_df = np.log1p(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a7ad1de4b50da14b381b453aa909600fc85eee2"},"cell_type":"markdown","source":"and finally lets produce the data sets..."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"X = train_df.drop(['target'], axis=1)\nY = train_df[['target']]\n\n# we will use only a subset of the train data so this kaggle kerenel will run quicker...\nfrom sklearn.model_selection import train_test_split\nX, _, Y, __ = train_test_split(X, Y, test_size=0.33)\nY = np.array(Y).reshape(len(Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09ec8b90f96b297bb7373d3c40593379a0e51c19"},"cell_type":"markdown","source":"now lets set up our optimisation problem..."},{"metadata":{"trusted":true,"_uuid":"6a3c31e6b5a14b1efd7401e78506071d68750602","collapsed":true},"cell_type":"code","source":"from skopt.space import Integer, Categorical, Real\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\n\n# set up hyperparameter space\nspace = [Integer(16, 256, name='num_leaves'),\n         Integer(8, 256, name='n_estimators'),\n         Categorical(['gbdt', 'dart', 'goss'], name='boosting_type'),\n         Real(0.001, 1.0, name='learning_rate')]\n\nimport lightgbm\nregressor = lightgbm.LGBMRegressor()\n\nfrom sklearn.model_selection import cross_val_score\n\n@use_named_args(space)\ndef objective(**params):\n    regressor.set_params(**params)\n    return -np.mean(cross_val_score(regressor, X, Y, cv=5, n_jobs=1, scoring='neg_mean_absolute_error'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69aee8ba7421dbebdcd0dd9a1a7e33b455e783a5"},"cell_type":"markdown","source":"now, lets run the optimization process"},{"metadata":{"trusted":true,"_uuid":"ccad5b264268026b8b3340cb4ce9575e3e9cb92e","collapsed":true},"cell_type":"code","source":"reg_gp = gp_minimize(objective, space, verbose=True)\n\nprint('best score: {}'.format(reg_gp.fun))\n\nprint('best params:')\nprint('       num_leaves: {}'.format(reg_gp.x[0]))\nprint('     n_estimators: {}'.format(reg_gp.x[1]))\nprint('    boosting_type: {}'.format(reg_gp.x[2]))\nprint('    learning_rate: {}'.format(reg_gp.x[3]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}