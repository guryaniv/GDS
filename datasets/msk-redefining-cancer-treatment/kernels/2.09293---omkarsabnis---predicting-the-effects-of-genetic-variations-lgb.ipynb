{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Predicting the effects of Genetic Variations using Light GBM\nIn this notebook, we will use Light GBM as out algorithm for classifying genetic mutations based on clinical evidence. There are 9 different classes a genetic mutation can be classified based upon.\n**Light GBM - Some basic information:**\n* A decision tree based algorithm, LGBM splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. \n* Advantages:\n    * Faster training speed and higher accuracy.\n    * Compatible with huge datasets.\n    * Better than XGBoost and other boosting algorithms.   \n* Let's start this kernel!   "},{"metadata":{"_uuid":"cb5a516c885fb10174c9d91dc9ec104912d738ff"},"cell_type":"markdown","source":"**(1). Importing all the necessary modules:**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# PREDICTING THE EFFECTS OF GENETIC VARIATIONS USING LGBM\n# BY - OMKAR SABNIS - 29-05-2018\n#Importing library\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import confusion_matrix,mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score,train_test_split\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"27c6b6795463c814929a23ab3764b08e7d804279"},"cell_type":"markdown","source":"**(2). Reading and Visualizing the data:**"},{"metadata":{"trusted":true,"_uuid":"9c827c857e5755f88ccd82e3866f4edff0c91be0","collapsed":true},"cell_type":"code","source":"# READING THE DATASETS\ntrain = pd.read_csv(\"../input/training_variants\")\ntrainx = pd.read_csv('../input/training_text',sep = '\\|\\|', engine= 'python', header=None, \n                     skiprows=1, names=[\"ID\",\"Text\"])\ntrain = pd.merge(train, trainx, how = 'left', on = 'ID').fillna('')\ntrain.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2314ffe4dd86fa5237d2568f12388d8748a0abb","collapsed":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/stage2_test_variants.csv\")\ntestx = pd.read_csv('../input/stage2_test_text.csv',sep = '\\|\\|', engine= 'python', header=None, \n                     skiprows=1, names=[\"ID\",\"Text\"])\ntest = pd.merge(test, testx, how = 'left', on = 'ID').fillna('')\ntest.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ef68065c85ae451b4d4b349378171920ebcc5df5"},"cell_type":"markdown","source":"**(3). Data Exploration:**"},{"metadata":{"trusted":true,"_uuid":"1050615c556d5b5fa8d7005d6f901cfbad1cfc0c","collapsed":true},"cell_type":"code","source":"train.Gene.nunique()\ntrain['Gene'].unique()\n\nk = train.groupby('Gene')['Gene'].count()\n\nplt.figure(figsize=(12,6))\nplt.hist(k, bins=150,log=True)\nplt.xlabel('Number of times Gene appared')\nplt.ylabel('Log of count')\nplt.title('Appearence of gene')\nplt.show()\n\n#count Gene\nfrom collections import Counter\nplt.figure(figsize=(12,10))\nsns.countplot((train['Gene']))\nplt.xticks()\ngenecount = Counter(train['Gene'])\nprint('Genes and their appearence:')\nprint(genecount,'\\n',len(genecount))\n\ntrain.Variation.nunique()\ntrain['Variation'].unique()\n\nk = train.groupby('Variation')['Variation'].count()\nplt.title('Graph of Gene vs Count')\nplt.figure(figsize=(12,6))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"2d43af76563cad8a2a3703170ab1317261767596"},"cell_type":"markdown","source":"**(4). Determining the length of the text:**"},{"metadata":{"trusted":true,"_uuid":"14a790289acff32d5a4189abe8ca18bf12916920","collapsed":true},"cell_type":"code","source":"def textlen(train):\n    k = train['Text'].apply(lambda x: len(str(x).split()))\n    l = train['Text'].apply(lambda x: len(str(x)))\n    return k, l\n\ntrain['Text_no_word'], train['Text_no_char'] = textlen(train)\ntest['Text_no_word'], test['Text_no_char'] = textlen(test)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"5ea4e6c683b227396a516cf9b3c7b0dcef6b2894"},"cell_type":"markdown","source":"**(5). Bag of words and converting the variables into categorical variables:**"},{"metadata":{"trusted":true,"_uuid":"a14e4dd6ce4a72567e26b9aeff86f8e1f60da549","collapsed":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(\n\tmin_df=1, max_features=1600, strip_accents='unicode',lowercase =True,\n\tanalyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n\tsmooth_idf=True, sublinear_tf=True, stop_words = 'english')\nX_train = tfidf.fit_transform(train['Text']).toarray()\nprint(X_train)\nX_test = tfidf.fit_transform(test['Text']).toarray()\n\ndef encoding(df,col):\n    le = LabelEncoder()\n    for i in col:\n        df[i] = le.fit_transform(df[i])\ntrain.columns\ncol = ['Gene', 'Variation', 'Class']\nencoding(train,col)\nencoding(test,['Gene', 'Variation'])\n\nX_train = pd.DataFrame(X_train)\nX_train = X_train.join(train[['Gene', 'Variation', 'Text_no_word','Text_no_char']]) \nX_test = pd.DataFrame(X_test)\nX_test = X_test.join(test[['Gene', 'Variation', 'Text_no_word','Text_no_char']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0775c9fb2eb354252b5b89b9b6934b192bf02335"},"cell_type":"markdown","source":"**(6). Feature Scaling:**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a01ba221613c19772de02d5dfc154b371c52a7a0"},"cell_type":"code","source":"# FEATURE SCALING\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\ny_train = train['Class']","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"4a56a18672332ae41ee397b5c32918ca23e33195"},"cell_type":"markdown","source":"**(7). Splitting the Dataset into training and testing set:**"},{"metadata":{"trusted":true,"_uuid":"6907558314d3de5bd238d22406687b57a9e8c937","collapsed":true},"cell_type":"code","source":"xtr,xvl,ytr,yvl = train_test_split(X_train,y_train,test_size=0.3,random_state=10)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"9bc9c2e9d41e5060bc5085e34c301b19f7d200fa"},"cell_type":"markdown","source":"**(8). Modelling - Naive Bayes and Random Forest:**"},{"metadata":{"trusted":true,"_uuid":"e2d192e50e464b5f57804e7457e7f846ff2dcf39","scrolled":true,"collapsed":true},"cell_type":"code","source":"# NAIVE BAYES\nnbc = GaussianNB()\nnbc.fit(xtr,ytr)\ny_nbcP = nbc.predict(xvl)\ny_nbc = nbc.predict_proba(X_test)\nprint(\"Confusion Matrix using Naive Bayes:\")\nprint(confusion_matrix(yvl,y_nbcP))\nprint(\"\\n\")\n\n# RANDOM FOREST\nrfc = RandomForestClassifier(n_estimators=50,max_depth=8,min_samples_split=4)\nrfc.fit(xtr,ytr)\ny_rfcp = rfc.predict(xvl)\ny_rfc=rfc.predict_proba(X_test)\nprint(\"Confusion Matrix using Random Forest:\")\nprint(confusion_matrix(yvl,y_rfcp))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"20c2a302a9acb425c7ce5c8ebc4dc553009d14e9"},"cell_type":"markdown","source":"**(9). Modelling - Light Gradient Boosting Machine(LGBM):**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56cf0596debac43004ea85d16e50f7cee24c2a6e"},"cell_type":"code","source":"def runLgb(Xtr,Xvl,ytr,yvl,test,num_rounds=10,max_depth=10,eta=0.5,subsample=0.8,\n           colsample=0.8,min_child_weight=1,early_stopping_rounds=50,seeds_val=2017):\n    \n    param = {'task': 'train',\n             'boosting_type': 'gbdt',\n             'objective':'multiclass',\n             'num_class':9,\n             'learning_rate':eta,\n             'metric':{'multi_logloss'},\n             'max_depth':max_depth,\n             #'min_child_weight':min_child_weight,\n             'bagging_fraction':subsample,\n             'feature_fraction':colsample,\n             'bagging_seed':seeds_val,\n             'num_iterations': num_rounds, \n             'num_leaves': 95,           \n             'min_data_in_leaf': 60, \n             'lambda_l1': 1.0,\n             'verbose':10,\n             'nthread':-1}\n    lgtrain = lgb.Dataset(Xtr,label=ytr)\n    lgval = lgb.Dataset(Xvl,label=yvl)\n    model = lgb.train(param,lgtrain,num_rounds,valid_sets=lgval,\n                      early_stopping_rounds=early_stopping_rounds,verbose_eval=20)\n    pred_val = model.predict(Xvl,num_iteration = model.best_iteration)\n    pred_test = model.predict(test,num_iteration=model.best_iteration)\n    return pred_test,pred_val,model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd4e99b5a01ea3885d104c2bbbe8baffdd899c8e"},"cell_type":"markdown","source":"**(10). K- Fold Cross Validation of Model:**"},{"metadata":{"trusted":true,"_uuid":"2a88c9f14687ca8d9074fffddf0ad260b0689732","collapsed":true},"cell_type":"code","source":"kf = KFold(n_splits=10,random_state=111,shuffle=True)\ncv_score = []\npred_test_full=0\n\nfor train_index,test_index in kf.split(X_train):\n    Xtr,Xvl = X_train[train_index],X_train[test_index]\n    ytr,yvl = y_train[train_index],y_train[test_index]\n    \n    pred_test,pred_val,model = runLgb(Xtr,Xvl,ytr,yvl,X_test,num_rounds=10,max_depth=3,\n                            eta=0.02,)\n    pred_test_full +=pred_test\npred_test = pred_test_full/10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e56bd7af7a1bfb20c4ef46b398ba10b8333d2b59"},"cell_type":"code","source":"# SUBMISSION OF FILE IN CSV FORMAT:\nsubmit = pd.DataFrame(test.ID)\nsubmit = submit.join(pd.DataFrame(pred_test))\nsubmit.columns = ['ID', 'class1','class2','class3','class4','class5','class6','class7','class8','class9']\nsubmit.to_csv('submission.csv', index=False) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}