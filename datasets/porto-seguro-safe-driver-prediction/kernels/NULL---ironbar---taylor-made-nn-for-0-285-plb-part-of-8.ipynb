{"cells": [{"cell_type": "markdown", "source": ["# Taylor made keras model\n", "I have uploaded the notebook I used for building the best NN model of our ensemble. It is not cleaned but maybe could be helpful."], "metadata": {"nbpresent": {"id": "0257e2ee-fc30-4538-b1ae-e35d63005945"}}}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import os \n", "import glob\n", "import seaborn as sns\n", "from termcolor import colored\n", "import keras\n", "from sklearn.model_selection import KFold\n", "from tqdm import tqdm\n", "import sys\n", "import hyperopt\n", "from hyperopt import fmin, tpe, hp\n", "import time\n", "import cPickle\n", "import scipy.stats as ss\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Input\n", "from keras import regularizers\n", "from keras.models import Model\n", "from keras.layers import Concatenate\n", "\n", "from porto.definitions import TRAIN_SET_PATH, TEST_SET_PATH, DATA_DIR\n", "from porto.dataset import DatasetCleaner, DatasetBuilder\n", "from porto.metrics import gini_normalized\n", "\n", "%matplotlib inline"], "execution_count": null, "outputs": [], "metadata": {"nbpresent": {"id": "4519d3ce-1785-48bd-9c91-c85c6e9f0c64"}}}, {"cell_type": "markdown", "source": ["## Gather the data"], "metadata": {}}, {"cell_type": "code", "source": ["df_train = pd.read_csv(TRAIN_SET_PATH)\n", "df_test = pd.read_csv(TEST_SET_PATH)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true, "nbpresent": {"id": "aea4f3e7-a6b3-43c0-bfed-b76c5b300497"}, "scrolled": true}}, {"cell_type": "markdown", "source": ["Clean the missing values"], "metadata": {}}, {"cell_type": "code", "source": ["dataset_cleaner = DatasetCleaner(min_category_samples=50)\n", "dataset_cleaner.fit(df_train)\n", "dataset_cleaner.transform(df_train)\n", "dataset_cleaner.transform(df_test)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["Get rid of calc features."], "metadata": {}}, {"cell_type": "code", "source": ["unwanted = df_train.columns[df_train.columns.str.startswith('ps_calc_')]\n", "df_train.drop(unwanted, axis=1, inplace=True)\n", "df_test.drop(unwanted, axis=1, inplace=True)\n", "print df_train.shape"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["### One hot encoding\n", "Apply one hot encoding to categorical features."], "metadata": {}}, {"cell_type": "code", "source": ["categorical_columns = df_train.columns[df_train.columns.str.endswith('_cat')]\n", "df_train = pd.concat([pd.get_dummies(df_train, columns=categorical_columns), df_train[categorical_columns]], axis=1)\n", "df_test = pd.concat([pd.get_dummies(df_test, columns=categorical_columns), df_test[categorical_columns]], axis=1)\n", "print df_train.shape, df_test.shape"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["df_train[df_train.columns[df_train.columns.str.startswith(categorical_columns[7])]].head(15)"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["### Custom binary encoding\n", "Apply custom binary encoding to discrete numeric features."], "metadata": {}}, {"cell_type": "code", "source": ["numerical_discrete_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', \n", "                               'ps_reg_01', 'ps_reg_02',\n", "                               'ps_car_15', 'ps_car_11']"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def apply_custom_binary_encoding(column):\n", "    column_values = df_train[column]\n", "    unique_values = sorted(df_train[column].unique())\n", "    for i, value in enumerate(unique_values[0:-1]):\n", "        new_column_name = '%s_cbe%02d' % (column, i)\n", "        df_train[new_column_name] = (df_train[column] > value).astype(np.int)\n", "        df_test[new_column_name] = (df_test[column] > value).astype(np.int)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["for column in numerical_discrete_features:\n", "    apply_custom_binary_encoding(column)\n", "print df_train.shape, df_test.shape"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["df_train[df_train.columns[df_train.columns.str.startswith('ps_ind_01')]].head(15)"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["### Target encoding"], "metadata": {}}, {"cell_type": "code", "source": ["def add_noise(series, noise_level):\n", "    return series * (1 + noise_level * np.random.randn(len(series)))\n", "\n", "\n", "def target_encode(trn_series=None,\n", "                  tst_series=None,\n", "                  target=None,\n", "                  min_samples_leaf=1,\n", "                  smoothing=1,\n", "                  noise_level=0):\n", "    \"\"\"\n", "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n", "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n", "    trn_series : training categorical feature as a pd.Series\n", "    tst_series : test categorical feature as a pd.Series\n", "    target : target data as a pd.Series\n", "    min_samples_leaf (int) : minimum samples to take category average into account\n", "    smoothing (int) : smoothing effect to balance categorical average vs prior\n", "    \"\"\"\n", "    assert len(trn_series) == len(target)\n", "    assert trn_series.name == tst_series.name\n", "    temp = pd.concat([trn_series, target], axis=1)\n", "    # Compute target mean\n", "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n", "    # Compute smoothing\n", "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n", "    # Apply average function to all target data\n", "    prior = target.mean()\n", "    # The bigger the count the less full_avg is taken into account\n", "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n", "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n", "    # Apply averages to trn and tst series\n", "    ft_trn_series = pd.merge(\n", "        trn_series.to_frame(trn_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=trn_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_trn_series.index = trn_series.index\n", "    ft_tst_series = pd.merge(\n", "        tst_series.to_frame(tst_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=tst_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_tst_series.index = tst_series.index\n", "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["target_encoding_columns = list(numerical_discrete_features) + list(categorical_columns)\n", "for f in target_encoding_columns:\n", "    df_train[f + \"_tef\"], df_test[f + \"_tef\"] = target_encode(trn_series=df_train[f],\n", "                                         tst_series=df_test[f],\n", "                                         target=df_train['target'],\n", "                                         min_samples_leaf=200,\n", "                                         smoothing=10,\n", "                                         noise_level=0)\n", "print df_train.shape, df_test.shape"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["df_train.head()"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["### Normalize numerical columns"], "metadata": {}}, {"cell_type": "code", "source": ["numerical_columns = [column for column in df_train.columns if not 'bin' in column and not 'cat' in column]\n", "numerical_columns = [column for column in numerical_columns if not 'cbe' in column and not 'tef' in column]\n", "numerical_columns = [column for column in numerical_columns if column.startswith('ps_')]\n", "tfe_columns = [column for column in df_train.columns if 'tef' in column]\n", "\n", "normalize_columns = numerical_columns + tfe_columns"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["for column in normalize_columns:\n", "    mean_value = df_train[column].mean()\n", "    std_value = df_train[column].std()\n", "    df_train[column] = (df_train[column] - mean_value)/std_value\n", "    df_test[column] = (df_test[column] - mean_value)/std_value\n", "    \n", "    max_value = np.maximum(df_train[column].max(), -df_train[column].min())\n", "    MAX_VALUE_ALLOWED = 5.\n", "    if max_value > MAX_VALUE_ALLOWED:\n", "        scale = MAX_VALUE_ALLOWED/max_value\n", "        df_train[column] *= scale\n", "        df_test[column] *= scale"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["### Organize the columns for feeding the network"], "metadata": {}}, {"cell_type": "markdown", "source": ["Remove categorical columns because they have been encoded and we don't need them any more."], "metadata": {}}, {"cell_type": "code", "source": ["df_train.drop(categorical_columns, axis=1, inplace=True)\n", "df_test.drop(categorical_columns, axis=1, inplace=True)\n", "print df_train.shape, df_test.shape"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["Now we have to divide the features by individual, region and car.\n", "We also have to divide them between categorical and non-categorical. In the categorical columns I will encode the custom binary encoding features. In the non-categorical columns I will include the target encode of categorical columns "], "metadata": {}}, {"cell_type": "code", "source": ["df_train.columns"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["column_dict = {}\n", "for key in ['car', 'ind', 'reg']:\n", "    column_dict[key] = [column for column in df_train.columns if key in column]\n", "    column_dict['%s_categorical' % key] = [column for column in column_dict[key] if '_cbe' in column or 'cat_' in column]\n", "    column_dict['%s_categorical' % key] = [column for column in column_dict['%s_categorical' % key] if 'tef' not in column]\n", "    column_dict[key] = [column for column in column_dict[key] if column not in column_dict['%s_categorical' % key]]\n", "for key in column_dict:\n", "    print key, len(column_dict[key])"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["for key in column_dict:\n", "    print key\n", "    print column_dict[key]\n", "    print"], "execution_count": null, "outputs": [], "metadata": {"scrolled": true}}, {"cell_type": "markdown", "source": ["### Prepare the data for keras"], "metadata": {}}, {"cell_type": "code", "source": ["x = {key: df_train[column_dict[key]].values for key in column_dict}"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["x_test = {key: df_test[column_dict[key]].values for key in column_dict}"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["y = df_train.target.values\n", "ids = df_train.id.values"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["column_dict[key]"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "markdown", "source": ["### Load the test prediction of the best model"], "metadata": {}}, {"cell_type": "code", "source": ["def load_save_dict(filename):\n", "    with open(filename, 'r') as f:\n", "        save_dict = cPickle.load(f)\n", "    return save_dict\n", "keras_save_dict = load_save_dict('/media/guillermo/Data/Kaggle/Porto_Safe_Driver/experiments/keras_log_20_5folds/2017_11_05_07_51_47.pkl')\n", "best_test_pred= keras_save_dict['test_pred'][:, 0]"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["keras_save_dict = load_save_dict('/media/guillermo/Data/Kaggle/Porto_Safe_Driver/experiments/keras_log_20_5folds/2017_11_05_07_51_47.pkl')"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["best_test_pred= keras_save_dict['test_pred'][:, 0]"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["val_pred = keras_save_dict['val_pred'][:, 0]\n", "sampling_probabilities = np.abs(df_train.target.values - val_pred)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["## Function for getting the score"], "metadata": {}}, {"cell_type": "code", "source": ["def save_log(filepath, params, time_stamp, gini_val_list, \n", "             gini_train_list, best_epoch_list, new_gini_val_list, \n", "             gini_test, optimizer_score):\n", "    if not os.path.exists(filepath):\n", "        with open(filepath, 'w') as f:\n", "            f.write('\\t'.join(['timestamp', 'new_gini_val_score', 'gini_test', 'gini_val_mean', \n", "                               'gini_train_mean', 'best_epoch',\n", "                               'gini_val_std', 'gini_train_std','params']) + '\\n')\n", "    with open(filepath, 'a') as f:\n", "        text = time_stamp + '\\t'\n", "        text += '%.4f\\t%.4f\\t%.4f\\t' % (new_gini_val_list[-1], gini_test, optimizer_score)\n", "        text += '%.4f\\t%.4f\\t' % (np.mean(gini_val_list), np.mean(gini_train_list))\n", "        text += '%.1f\\t' % np.mean(best_epoch_list)\n", "        text += '%.4f\\t%.4f\\t%s\\n' % (np.std(gini_val_list), np.std(gini_train_list), params)\n", "        f.write(text)"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def get_keras_model(encoding_conf, layers, dropout_rates, l1=0, l2=0, encoding_activation='relu'):\n", "    # Create the encoding\n", "    encoding_list = []\n", "    input_list = []\n", "    for key in ['reg', 'car', 'ind']:\n", "        categorical_key = '%s_categorical' % key\n", "        input_layer = Input(shape=(x[categorical_key].shape[1],), name=categorical_key)\n", "        input_list.append(input_layer)\n", "        encoding = Dense(int(encoding_conf[categorical_key]), \n", "                         activation=encoding_activation, name='%s_encoding' % categorical_key, \n", "                        kernel_regularizer=regularizers.l1_l2(l1, l2))(input_layer)\n", "        input_layer = Input(shape=(x[key].shape[1],), name=key)\n", "        input_list.append(input_layer)\n", "        encoding_input = Concatenate(axis=1)([input_layer, encoding])\n", "        encoding = Dense(int(encoding_conf[key]), activation=encoding_activation, name='%s_encoding' % key,\n", "                        kernel_regularizer=regularizers.l1_l2(l1, l2))(encoding_input)\n", "        encoding_list.append(encoding)\n", "        \n", "    encoding = Concatenate(axis=1)(encoding_list)\n", "    \n", "    first_layer = True\n", "    for n_units, drop in zip(layers, dropout_rates):\n", "        if first_layer:\n", "            output = Dense(n_units, activation='relu')(encoding)\n", "            first_layer = False\n", "        else:\n", "            output = Dense(n_units, activation='relu')(output)\n", "        if drop > 0:\n", "            output = Dropout(drop)(output)\n", "    # Add the final layer\n", "    output = Dense(1, activation='sigmoid', name='output')(output)\n", "    \n", "    model = Model(inputs=input_list, outputs=output)\n", "    model.compile(loss='binary_crossentropy',  optimizer='RMSprop')\n", "    return model"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def get_upsampled_index(y, train_index, n_upsampling):\n", "    positive_index = train_index[y[train_index] == 1]\n", "    upsampled_index = train_index.tolist() + positive_index.tolist()*(n_upsampling)\n", "    np.random.shuffle(upsampled_index)\n", "    return upsampled_index"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def get_noisy_target(y, prob):\n", "    if prob == 0:\n", "        return y\n", "    noise = np.random.binomial(1, prob, y.shape)\n", "    noisy_target = noise + y\n", "    noisy_target[noisy_target == 2] = 0\n", "    return noisy_target"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def plot_train_evolution(val_score):\n", "    plt.figure(figsize=(12, 6))\n", "    plt.plot(val_score, label='val')\n", "    plt.plot(val_score, 'ro')\n", "    plt.ylabel('validation score')\n", "    plt.xlabel('Number of epochs')\n", "    plt.ylim(ymin=np.max(val_score) - 0.01)\n", "    plt.show()"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["def get_score(params):\n", "    # Get the parameters for the model\n", "    model_layers = [int(params['n_units_per_layer'])]*int(params['n_layers'])\n", "    model_dropout_rates = [params['dropout_rate']]*int(params['n_layers'])    \n", "    \n", "    time_stamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n", "    gini_train_list, gini_val_list, best_epoch_list  = [], [], []\n", "    test_pred_list, val_pred_list, val_id_list = [], [], []\n", "    new_gini_val_list = []\n", "    \n", "    random_state = -1\n", "    sys.stdout.flush()\n", "    for i in tqdm(range(params['max_experiments'])):\n", "        random_state += 1\n", "        \n", "        fold_val_ids = []\n", "        unsorted_val_preds = []\n", "        \n", "        for n_fold in range(params['n_folds']):\n", "            kf = KFold(n_splits=params['n_folds'], shuffle=True, random_state=random_state)\n", "            partitions = [_ for _ in kf.split(y)]\n", "            train_index, val_index = partitions[n_fold]\n", "\n", "            x_train = {key:x[key][train_index] for key in x}\n", "            x_val = {key:x[key][val_index] for key in x}\n", "            upsampled_probs = y[train_index]*params['n_upsampling'] + 1\n", "            train_sample_probs = sampling_probabilities[train_index]\n", "            train_sample_probs *= upsampled_probs\n", "            train_sample_probs /= np.sum(train_sample_probs)\n", "            #print 'train_sample_probs: ', train_sample_probs[0:20]\n", "\n", "            model = get_keras_model(params['encoding_conf'], encoding_activation=params['encoding_activation'],\n", "                                    layers=model_layers, dropout_rates=model_dropout_rates,\n", "                                    l1=params['l1'], l2=params['l2'])\n", "\n", "            model_gini_train_list = []\n", "            model_gini_val_list = []\n", "            best_weights = None\n", "            for epoch in range(params['max_epochs']):\n", "                for _ in range(params['val_period']):\n", "                    epoch_index = np.random.choice(train_index, size=params['epoch_size'],\n", "                                                   p=train_sample_probs, replace=False)\n", "                    x_train_epoch = {key:x[key][epoch_index] for key in x}\n", "                    model.fit(x=x_train_epoch, y=y[epoch_index], epochs=1, \n", "                              batch_size=params['batch_size'], verbose=False)\n", "\n", "                preds_val = model.predict(x=x_val, batch_size=params['batch_size'])\n", "                gini_val = gini_normalized(y[val_index], preds_val)\n", "                model_gini_val_list.append(gini_val)\n", "\n", "                best_epoch = np.argmax(model_gini_val_list)\n", "                if best_epoch == epoch:\n", "                    best_weights = model.get_weights()\n", "                if epoch - best_epoch >= params['patience']:\n", "                    break\n", "\n", "            best_epoch = np.argmax(model_gini_val_list)\n", "            best_epoch_list.append(best_epoch)\n", "            gini_val_list.append(model_gini_val_list[best_epoch])\n", "\n", "            model.set_weights(best_weights)\n", "            preds_test = preds_val = model.predict(x=x_test, batch_size=params['batch_size'])\n", "            test_pred_list.append(preds_test)\n", "\n", "            preds_train = model.predict(x=x_train, batch_size=params['batch_size'])\n", "            gini_train = gini_normalized(y[train_index], preds_train)\n", "            gini_train_list.append(gini_train)\n", "\n", "            preds_val = model.predict(x=x_val, batch_size=params['batch_size'])\n", "            unsorted_val_preds.append(preds_val)\n", "            fold_val_ids.append(ids[val_index])\n", "\n", "            if params['verbose']:\n", "                print colored('Gini val: %.4f\\tGini train: %.4f' % (gini_val_list[-1], gini_train_list[-1]), 'blue')\n", "                plot_train_evolution(model_gini_val_list)\n", "        # Sort the validation predictions\n", "        fold_val_ids = np.concatenate(fold_val_ids)\n", "        unsorted_val_preds = np.concatenate(unsorted_val_preds)\n", "        sorted_index = np.argsort(fold_val_ids)\n", "        sorted_val_preds = unsorted_val_preds[sorted_index]\n", "        val_pred_list.append(sorted_val_preds)\n", "        # Get the gini validation score\n", "        new_gini_val = gini_normalized(y, np.mean(val_pred_list, axis=0))\n", "        new_gini_val_list.append(new_gini_val)\n", "        # Get test score\n", "        test_pred_mean = np.mean(test_pred_list, axis=0)\n", "        gini_test = gini_normalized(best_test_pred, test_pred_mean)\n", "        \n", "        if params['verbose']:\n", "            text = 'Gini val: %.4f\\tGini test: %.4f' % (new_gini_val, gini_test)\n", "            print colored(text, 'blue')\n", "     \n", "    gini_train_score = np.mean(gini_train_list)\n", "    gini_val_score = np.mean(gini_val_list)\n", "    gini_val = new_gini_val_list[-1]\n", "    \n", "    \n", "    print time_stamp\n", "    print colored('params: %s' % params, 'green')\n", "    print colored('Gini val score: %.4f' % gini_val, 'green')\n", "    print colored('Gini test score: %.4f' % gini_test, 'green')\n", "    \n", "    optimizer_score = gini_test - 2.9949206966767021*gini_val - 0.12420528931875374\n", "    \n", "    save_log(params['log_file'], params, time_stamp, gini_val_list, \n", "             gini_train_list, best_epoch_list, new_gini_val_list,\n", "             gini_test, optimizer_score)\n", "    save_dict = {\n", "        'gini_test': gini_test,\n", "        'test_pred': test_pred_mean,\n", "        'val_pred': np.mean(val_pred_list, axis=0),\n", "        'new_gini_val_list': new_gini_val_list,\n", "        'gini_train_list': gini_train_list,\n", "        'gini_val_list': gini_val_list,\n", "        'params': params,\n", "        'time_stamp': time_stamp,\n", "        'best_epoch_list': best_epoch_list,\n", "    }\n", "    dirname = os.path.splitext(os.path.basename(params['log_file']))[0]\n", "    pickle_path = os.path.join(DATA_DIR, 'experiments', dirname, '%s.pkl' % time_stamp)\n", "    if not os.path.exists(os.path.dirname(pickle_path)):\n", "        os.mkdir(os.path.dirname(pickle_path))\n", "    with open(pickle_path, 'w') as f:\n", "        cPickle.dump(save_dict, f)\n", "        \n", "    \n", "    \n", "    return optimizer_score"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["## Speed up"], "metadata": {}}, {"cell_type": "markdown", "source": ["I think there are too much evaluations, let's divide them by 4 and 2"], "metadata": {}}, {"cell_type": "code", "source": ["sampling_probabilities[:] = 1"], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["raise"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["params = {\n", "    'encoding_conf': {\n", "        'reg_categorical': 3.0, \n", "        'car_categorical': 1, \n", "        'ind_categorical': 50.0, \n", "        'ind': 70.0, \n", "        'car': 35.0, \n", "        'reg': 34.0}, \n", "    'n_layers': 1, \n", "    'n_units_per_layer': 90.0, \n", "    'dropout_rate': 0.5, \n", "    'encoding_activation': 'tanh', \n", "    \n", "    'l2': 0.0001, \n", "    'l1': 1e-05, \n", "    \n", "    'batch_size': 2048,\n", "    'val_period': 4,\n", "    'epoch_size': 100000, \n", "    'patience': 12, \n", "    'n_upsampling': 25, \n", "    'n_folds': 5, \n", "    'max_epochs': 1000, \n", "    'max_experiments': 10, \n", "    'verbose': True, \n", "    'log_file': '../logs/keras_v31_5folds.csv'}\n", "get_score(params)"], "execution_count": null, "outputs": [], "metadata": {"scrolled": true}}, {"cell_type": "code", "source": ["params = {\n", "    'encoding_conf': {\n", "        'reg_categorical': 3.0, \n", "        'car_categorical': 1, \n", "        'ind_categorical': 50.0, \n", "        'ind': 70.0, \n", "        'car': 35.0, \n", "        'reg': 34.0}, \n", "    'n_layers': 1, \n", "    'n_units_per_layer': 90.0, \n", "    'dropout_rate': 0.5, \n", "    'encoding_activation': 'tanh', \n", "    \n", "    'l2': 0.0001, \n", "    'l1': 1e-05, \n", "    \n", "    'batch_size': 2048,\n", "    'val_period': 8,\n", "    'epoch_size': 50000, \n", "    'patience': 12, \n", "    'n_upsampling': 25, \n", "    'n_folds': 5, \n", "    'max_epochs': 1000, \n", "    'max_experiments': 10, \n", "    'verbose': True, \n", "    'log_file': '../logs/keras_v31_5folds.csv'}\n", "get_score(params)"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": ["params = {\n", "    'encoding_conf': {\n", "        'reg_categorical': 3.0, \n", "        'car_categorical': 1, \n", "        'ind_categorical': 50.0, \n", "        'ind': 70.0, \n", "        'car': 35.0, \n", "        'reg': 34.0}, \n", "    'n_layers': 1, \n", "    'n_units_per_layer': 90.0, \n", "    'dropout_rate': 0.5, \n", "    'encoding_activation': 'tanh', \n", "    \n", "    'l2': 0.0001, \n", "    'l1': 1e-05, \n", "    \n", "    'batch_size': 2048,\n", "    'val_period': 16,\n", "    'epoch_size': 25000, \n", "    'patience': 12, \n", "    'n_upsampling': 25, \n", "    'n_folds': 5, \n", "    'max_epochs': 1000, \n", "    'max_experiments': 10, \n", "    'verbose': True, \n", "    'log_file': '../logs/keras_v31_5folds.csv'}\n", "get_score(params)"], "execution_count": null, "outputs": [], "metadata": {}}, {"cell_type": "code", "source": [], "execution_count": null, "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"version": 2, "name": "ipython"}, "name": "python", "version": "2.7.13", "pygments_lexer": "ipython2", "nbconvert_exporter": "python"}, "nbpresent": {"themes": {}, "slides": {"1ee3170a-b315-438d-8b07-773dc39d544d": {"id": "1ee3170a-b315-438d-8b07-773dc39d544d", "regions": {}, "prev": null}}}, "kernelspec": {"language": "python", "display_name": "Python 2", "name": "python2"}}}