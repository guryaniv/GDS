{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom collections import Counter\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ab10313b6d94dcb559e9c10f3fdaeac59d4f597"},"cell_type":"code","source":"# lgb_params =  {\n#     'task': 'train',\n#     'boosting_type': 'gbdt',\n#     'objective': 'regression',\n#     'metric': 'rmse',\n#     'num_leaves': 50,\n#     'feature_fraction': 0.7,\n#     'bagging_fraction': 0.7,\n#     'bagging_freq': 4,\n#     'learning_rate': 0.015,\n#     'zero_as_missing':True,\n#     'verbose': 0\n#     }\n\nlgb_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    \"num_leaves\": 180,\n    \"feature_fraction\": 0.50,\n    \"bagging_fraction\": 0.50,\n    'bagging_freq': 4,\n    \"max_depth\": -1,\n    \"reg_alpha\": 0.3,\n    \"reg_lambda\": 0.1,\n    #\"min_split_gain\":0.2,\n    \"min_child_weight\":10,\n    'zero_as_missing':True,\n    'verbose': 0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a8510a4ba685ac0540d039a72424a7c30fe0968","collapsed":true},"cell_type":"code","source":"def setup_dataset():\n    train = pd.read_csv(\"../input/train.csv\")\n    test = pd.read_csv(\"../input/test.csv\")\n    subs = pd.read_csv(\"../input/sample_submission.csv\")\n\n    # preprocess\n    train[\"target\"] = np.log1p(train[\"target\"])\n    subs[\"target\"] = 0.0\n    # Replace 0 with NaN to ignore them.\n    train = prepare(train.replace(0, np.nan),\"train\")\n    test = prepare(test.replace(0, np.nan),\"test\")\n    return train,test,subs\n\ndef prepare(data,case):\n    if case==\"train\":\n        del_col=[\"ID\",\"target\"]\n    elif case==\"test\":\n        del_col=[\"ID\"]\n    data_c = data.copy()\n    data['mean'] = data_c.drop(del_col,axis=1).mean(axis=1)\n    data['std'] = data_c.drop(del_col,axis=1).std(axis=1)\n    data['min'] = data_c.drop(del_col,axis=1).min(axis=1)\n    data['max'] = data_c.drop(del_col,axis=1).max(axis=1)\n    data['number_of_different'] = data_c.drop(del_col,axis=1).nunique(axis=1)               # Number of diferent values in a row.\n    data['non_zero_count'] = data_c.drop(del_col,axis=1).fillna(0).astype(bool).sum(axis=1) # Number of non zero values (e.g. transaction count)\n    \n    #extra\n    d_matrix = data_c.drop(del_col,axis=1).fillna(0).apply(pair_check,axis=1)\n    tmp_result = pd.DataFrame(list(d_matrix),columns=[\"not_pair_cnt\", \"not_pair_sum\", \"not_pair_max\", \"not_pair_min\", \"not_pair_mean\",\"even_pair_cnt\",\"even_pair_sum\", \"even_pair_max\",\"even_pair_min\",\"even_pair_mean\"])\n    for col in tmp_result.columns.tolist():\n        data[col]=tmp_result[col]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2ab6091ddc7c3ebea597c1732164971abcd540","collapsed":true},"cell_type":"code","source":"def pair_check(column_values):\n    v = column_values.values.tolist()\n    not_pair_cnt=0\n    not_pair_sum=0\n    not_pair_max=0\n    not_pair_min=9999999999\n    even_pair_cnt=0\n    even_pair_sum=0\n    even_pair_max=0\n    even_pair_min=0\n    for key,cnt in Counter(v).items():\n        if key ==0:\n            continue\n        if cnt %2==1:\n            not_pair_cnt+=cnt\n            not_pair_sum+=key\n            if not_pair_max<key:\n                not_pair_max=key\n            if not_pair_min>key:\n                not_pair_min=key\n        else:\n            even_pair_cnt+=cnt\n            even_pair_sum+=key\n            if even_pair_max<key:\n                even_pair_max=key\n            if even_pair_min>key:\n                even_pair_min=key\n    if not_pair_cnt==0:\n        not_pair_mean=0\n    else:\n        not_pair_mean = not_pair_sum / not_pair_cnt\n    if even_pair_cnt==0:\n        even_pair_mean=0\n    else:\n        even_pair_mean = even_pair_sum / even_pair_cnt\n    return (not_pair_cnt, not_pair_sum, not_pair_max, not_pair_min, not_pair_mean, even_pair_cnt,even_pair_sum, even_pair_max,even_pair_min, even_pair_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"291ab5dce8af68780fa206ac822f6d49ffd3e66f","collapsed":true},"cell_type":"code","source":"def train_lgb(d_train, d_test, d_subs, epoch, kfold_num=3,\n                delete_cols=[], lgb_params=lgb_params):\n    best_score=0\n    print(\"epoch:{}\".format(epoch))\n    d_train = d_train.drop(delete_cols,axis=1)\n    d_test = d_test.drop(delete_cols,axis=1)\n    X_test  = d_test.drop([\"ID\"],axis=1)\n    \n    # substract ID,target\n    fti_list=np.zeros(d_train.shape[1] - 2)\n    low_fti=[]\n    \n    del d_test\n    gc.collect()\n\n    for i,(train_idx, valid_idx) in enumerate(KFold(n_splits=kfold_num).split(d_train)):\n        # setup dataset\n        X_train = d_train.iloc[train_idx,:].drop([\"ID\",\"target\"],axis=1)\n        X_valid = d_train.iloc[valid_idx,:].drop([\"ID\",\"target\"],axis=1)\n        Y_train = d_train.iloc[train_idx,:][\"target\"]\n        Y_valid = d_train.iloc[valid_idx,:][\"target\"]\n        lgb_train = lgb.Dataset(X_train, label=Y_train)\n        lgb_valid = lgb.Dataset(X_valid, label=Y_valid)\n        # training!!\n        lgb_clf = lgb.train(\n            lgb_params,\n            lgb_train,\n            num_boost_round=10000,\n            valid_sets=[lgb_train, lgb_valid],\n            valid_names=['train','valid'],\n            early_stopping_rounds=50,\n            verbose_eval=50\n        )\n        # get feature importances\n        fti_list += lgb_clf.feature_importance()\n        # set prediction\n        d_subs[\"target\"] += np.expm1(lgb_clf.predict(X_test))\n        best_score += lgb_clf.best_score[\"valid\"]['rmse']\n    d_subs[\"target\"] /= kfold_num # 3\n    best_score /= kfold_num # 3\n    # detect low important columns\n    tee=[]\n    for i, feat in enumerate(X_train.columns.tolist()):\n        ##### delete 0.0 importance columns ######\n        if fti_list[i]==0.0:\n            low_fti.append(feat)\n        else:\n            tee.append([feat,fti_list[i]])\n    print(\"best_score:{:.4f}\".format(best_score))\n    return d_subs, low_fti, best_score,tee","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9a84788bde11da18116e03137fa7a06f5c49ebbd"},"cell_type":"code","source":"def execute_santander():\n    print(\"start\")\n    train,d_test,subs = setup_dataset()\n    delete_cols=[]\n    fti_results=[]\n    score=99999.9\n    for epoch in range(10):\n        d_train = train.copy()\n        d_subs = subs.copy()\n        d_subs, low_fti, best_score,tee = train_lgb(d_train, d_test, d_subs, epoch=epoch,\n                                       kfold_num=3, delete_cols=delete_cols,\n                                       lgb_params=lgb_params)\n        if score < best_score:\n            print(\"score is not improved\")\n            break\n        else:\n            score = best_score\n            delete_cols.extend(low_fti)\n            print(\"delete_cols numbers:\",len(delete_cols))\n            d_subs.to_csv(\"subs_lgb.csv\",index=False)\n            fti_results.append(tee)\n    print(\"finish\")\n    return fti_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5240cf90e76755cde0390226105fe56db1f90921","scrolled":false},"cell_type":"code","source":"fti_results = execute_santander()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f639ef0fb54c73c27407ba8eb8bb5149c4c6b07","scrolled":false,"collapsed":true},"cell_type":"code","source":"# for t in fti_results[0]:\n#     print(t)\ndf = pd.DataFrame(fti_results[0],columns=[\"name\",\"score\"])\ndf.sort_values(by=\"score\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6ce08bd66a197f6e32a317e39c1e3dab5cc3c96"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"06cd0ddf28a48b4f3505dd4e11b9a2c3ebf3f119"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40faa344b68384beb3de22834076f398c1f489c8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}