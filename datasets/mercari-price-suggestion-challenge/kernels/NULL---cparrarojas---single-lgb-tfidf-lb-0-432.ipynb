{"cells":[{"metadata":{"_uuid":"be1592ddb9ac75c5711064eaac62878c84a0fb96"},"cell_type":"markdown","source":"A relatively decent score for my very first competition. I tried doing an ensemble of simple models but couldn't manage to get better results than with the single LGB with a large number of leaves and long training. Definitely something to learn.\n\nThe whole thing takes ~59min, with ~10 of prediction and ~2 of preprocessing.\n\n**Comments:**\n- I join the columns with freely-entered text into one. Treating them separately did not improve the results.\n- Label encoding of brands and categories is done manually as a function of mean price. This worked better for me than an unsorted encoding. Unseen data is handled as missing the brand/category name (perhaps the median would have been better?).\n- Missing values in `shipping`, if any, are treated as if the shipping is paid by the buyer.\n- I made the idiotic mistake of encoding missing values in `item_condition` with a 0 and then applying OHE. There were no missing entries either in the training set or reduced testing set, but if there are any in the full testing set then the kernel will fail because the model was trained with 5 columns per item condition instead of 6.\n- In case there are no missing values for `item_condition` in the full testing set and the kernel survives the time and memory constraints, the finall score may be affected by the fact that I filled missing values of freely-entered text AFTER combining the two fields, so eg. properly named listings with a null description will be blank.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3d49d8ff-bf0f-4039-9f97-69eb590c4562","_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true,"_uuid":"8e5ad1388871da1fbedcae52c8b916fd37c6960e","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport scipy\n\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6de4e09d-b12a-4789-833e-c763c1979645","collapsed":true,"_uuid":"5d8ceaf42069d6ac3cba9f0257f802e7a7d463ba","trusted":false},"cell_type":"code","source":"develop = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96d36a61-a5e8-42ce-9528-154e6ab1d8af","collapsed":true,"_uuid":"f5cd543e0b41ae00cd59c9fe345d80685afbc6f9","trusted":false},"cell_type":"code","source":"add_stop = ['[rm]', 'rm']\nstop_words = ENGLISH_STOP_WORDS.union(add_stop)\n\ntfidf = TfidfVectorizer(stop_words=stop_words, max_features=50000)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f91a083f-3354-49d6-8621-3013b923225c","collapsed":true,"_uuid":"a8370d52c9e10f68fe26f4236e373be256941241","trusted":false},"cell_type":"code","source":"def preprocess(dataset, train=True, brands_by_price=None, cat1_by_price=None, cat2_by_price=None, cat3_by_price=None):\n    \n    dataset.replace('No description yet', '', inplace=True)\n    dataset['text'] = dataset['name'] + ' ' + dataset['item_description']\n    dataset['text'].fillna('', inplace=True)\n    \n    if train:\n        tfidfvec = tfidf.fit_transform(dataset['text'])\n    else:\n        tfidfvec = tfidf.transform(dataset['text'])\n        \n    dataset['brand_name'].fillna('missing_brand', inplace=True)\n    \n    dataset['item_condition_id'].fillna(0, inplace=True)\n\n    dataset['shipping'].fillna(0, inplace=True)\n\n    dataset['category_name'].fillna('missing_cat', inplace=True)\n\n    dataset['main_cat'] = dataset['category_name'].apply(lambda name: name.split('/')[0])\n    dataset['second_cat'] = dataset['category_name'].apply(lambda name: name.split('/')[1] if len(name.split('/')) > 1 else 'missing_cat')\n    dataset['third_cat'] = dataset['category_name'].apply(lambda name: name.split('/')[2] if len(name.split('/')) > 2 else 'missing_cat')\n    \n    if train:\n        brands_by_price = dataset.groupby('brand_name').mean()['price'].sort_values(ascending=False).to_frame()\n        brands_by_price['id'] = brands_by_price.reset_index().index.values\n        n_brands = len(brands_by_price)\n        brand_names = brands_by_price.index.values\n        \n        cat1_by_price = dataset.groupby('main_cat').mean()['price'].sort_values(ascending=False).to_frame()\n        cat1_by_price['id'] = cat1_by_price.reset_index().index.values\n        n_cat1 = len(cat1_by_price)\n        cat1_names = cat1_by_price.index.values\n        \n        cat2_by_price = dataset.groupby('second_cat').mean()['price'].sort_values(ascending=False).to_frame()\n        cat2_by_price['id'] = cat2_by_price.reset_index().index.values\n        n_cat2 = len(cat2_by_price)\n        cat2_names = cat2_by_price.index.values\n        \n        cat3_by_price = dataset.groupby('third_cat').mean()['price'].sort_values(ascending=False).to_frame()\n        cat3_by_price['id'] = cat3_by_price.reset_index().index.values\n        n_cat3 = len(cat3_by_price)\n        cat3_names = cat3_by_price.index.values\n        \n        dataset=dataset.drop(['price'], axis=1)\n    \n    else:\n        n_brands = len(brands_by_price)\n        brand_names = brands_by_price.index.values\n        dataset['brand_name'] = dataset['brand_name'].apply(lambda name: name if name in brand_names else 'missing_brand')\n        \n        n_cat1 = len(cat1_by_price)\n        cat1_names = cat1_by_price.index.values\n        dataset['main_cat'] = dataset['main_cat'].apply(lambda name: name if name in cat1_names else 'missing_cat')\n        \n        n_cat2 = len(cat2_by_price)\n        cat2_names = cat2_by_price.index.values\n        dataset['second_cat'] = dataset['second_cat'].apply(lambda name: name if name in cat2_names else 'missing_cat')\n        \n        n_cat3 = len(cat3_by_price)\n        cat3_names = cat3_by_price.index.values\n        dataset['third_cat'] = dataset['third_cat'].apply(lambda name: name if name in cat3_names else 'missing_cat')\n        \n    \n    brand_data = brands_by_price.loc[dataset['brand_name']]\n    dataset['brand_id'] = brand_data['id'].values/n_brands\n    \n    cat1_data = cat1_by_price.loc[dataset['main_cat']]\n    dataset['cat1_id'] = cat1_data['id'].values/n_cat1\n    \n    cat2_data = cat2_by_price.loc[dataset['second_cat']]\n    dataset['cat2_id'] = cat2_data['id'].values/n_cat2\n    \n    cat3_data = cat3_by_price.loc[dataset['third_cat']]\n    dataset['cat3_id'] = cat3_data['id'].values/n_cat3\n    \n    ohe_data = pd.concat([pd.get_dummies(dataset[col], prefix=col) for col in dataset[['item_condition_id', 'shipping']]], axis=1)\n    \n    dataset = dataset.drop(['item_description', 'name', 'category_name', 'brand_name', 'main_cat', 'second_cat', 'third_cat', 'text', 'item_condition_id', 'shipping'], axis=1)\n    feat_data = scipy.sparse.hstack((dataset.values, ohe_data.values, tfidfvec)).tocsr()\n    \n    del dataset\n    gc.collect()\n    \n    if train:\n        return feat_data, brands_by_price, cat1_by_price, cat2_by_price, cat3_by_price\n    return feat_data","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6a4d5e1a-100d-4056-915b-89ebc3a457be","_uuid":"eb2d8cf9d0af072b24409993e8f773ee1dc8ce62","trusted":false},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.tsv', sep='\\t', index_col='train_id')\n\ntrain_data = train_data[train_data.price >= 3.]\n\ny=np.log1p(train_data.price.values)\n\nfeat_train, brands_by_price, cat1_by_price, cat2_by_price, cat3_by_price = preprocess(train_data)\n\ndel train_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b43711a-7390-4d79-9d86-d1d1f1cf8343","collapsed":true,"_uuid":"742ffbbcee86b9ab2c84c7323a313b87784f7bbf","trusted":false},"cell_type":"code","source":"params = {'num_leaves': 350, 'learning_rate': 0.1, 'feature_fraction': 0.9, 'bagging_fraction': 0.7, 'bagging_freq': 5, 'metric': 'l2_root',  'data_random_seed': 0, 'num_threads': 4, 'max_bin': 64}\n\nif develop:\n    X_train, X_test, y_train, y_test = train_test_split(feat_train, y, test_size=0.1, random_state=0)\n    train_set = lgb.Dataset(X_train, label=y_train)\n    valid_set = lgb.Dataset(X_test, label=y_test, reference=train_set)\n    \n    bst = lgb.train(params, train_set, num_boost_round=900, valid_sets=[train_set, valid_set], early_stopping_rounds=100, verbose_eval=True)\n    \nelse:\n    train_set = lgb.Dataset(feat_train, label=y)\n    \n    bst = lgb.train(params, train_set, num_boost_round=900, valid_sets=[train_set], early_stopping_rounds=100, verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"714cd080-3bae-4e5e-a7a6-d4e2f19a5fa7","collapsed":true,"_uuid":"50ce1c5fa121dcebe8cc0a567f20d3a2cdd8d52c","trusted":false},"cell_type":"code","source":"predictions = pd.DataFrame({'test_id': [], 'price': []})\n\ntest_chunks = pd.read_csv('../input/test.tsv', sep='\\t', index_col='test_id', chunksize=350000)\n\nfor chunk in test_chunks:\n    testId = chunk.index\n    \n    feat_test = preprocess(chunk, False, brands_by_price, cat1_by_price, cat2_by_price, cat3_by_price)\n        \n    preds = pd.DataFrame({'test_id': testId, 'price': np.expm1(bst.predict(feat_test, num_iteration=bst.best_iteration))})\n\n    del feat_test\n    gc.collect()\n\n    predictions = pd.concat([predictions, preds], join=\"inner\")\n\npredictions.test_id = predictions.test_id.astype(int)\npredictions.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}