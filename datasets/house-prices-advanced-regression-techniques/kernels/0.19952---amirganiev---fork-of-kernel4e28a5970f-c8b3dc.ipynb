{"cells":[{"metadata":{"_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199","_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86"},"cell_type":"markdown","source":"*  *PERSONAL PROJECT - MACHINE LEARNING*\n* **My work to predict housing prices in the state of Iowa**.\nEach section is dedicated to practice a certain aspect of machine learning. Each section is named after what will be practiced in that section. All sections are divided into two chapers. CHAPTER ONE - deals with everything essential and basic for machine learning. CHAPTER TWO - tries to get better predictions by using more complex approaches and tools.\nIn the end, the final section of each chapter summerizes all the work completed in that chapter and predicts the final prices.\n"},{"metadata":{"_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","_uuid":"1c728098629e1301643443b1341556a15c089b2b","trusted":true},"cell_type":"code","source":"#CHAPTER 1: MACHINE LEARNING BASICS\n#section 1: import pandas and data\nimport pandas as pd\n\nmain_file_path = '../input/train.csv' # training data for Iowa state information\ndata = pd.read_csv(main_file_path)\n\nprint(data.describe())\n\n\n#section 2: Choosing a column\n#one column\nprint('section 3')\nprint(data.columns)\ndataNeighborhood=data.Neighborhood\nprint(dataNeighborhood.head())\n#two columns\nchosencolumns=['SalePrice','MoSold']\ndatacolumnschosen=data[chosencolumns]\ndatacolumnschosen.describe()\n\n#section 3\n#assigning a prediction target\ny=data.SalePrice\n#Choosing Predictors\ndatapredictors=['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr', 'TotRmsAbvGrd']\nX= data[datapredictors]\n#building a model\nfrom sklearn.tree import DecisionTreeRegressor\n#Dedine model\niowa_model=DecisionTreeRegressor()\n#fit model\niowa_model.fit(X,y)\n\nprint(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(iowa_model.predict(X.head()))\nprint('actual price')\nprint(y.head())\n\n#section 4\n#Model Validation\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\nprint ('Section 5')\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n\n#define model\nfrom sklearn.tree import DecisionTreeRegressor\niowa_model=DecisionTreeRegressor()\n#fit model\niowa_model.fit(train_X, train_y)\n\n#get predicted prices on validation data\nfrom sklearn.metrics import mean_absolute_error\nval_predictions = iowa_model.predict(val_X)\n\nprint (mean_absolute_error(val_y, val_predictions))\n\n\n#Section 5\n#Underfitting, Overfitting and Model Optimization\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    iowamodel=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    iowamodel.fit(predictors_train, targ_train)\n    preds_val = iowamodel.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)\n\n#compare MAE with differing values of max_leaf_nodes\n\nfor max_leaf_nodes in [5, 50,70, 100, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))\n    \n    \n#section 6\n#new model RandomForest (works well withour any parameters needed to be changed)\nprint('Section 7: New Model, Random Forest')\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\niowaforest_model=RandomForestRegressor()\niowaforest_model.fit(train_X, train_y)\niowa_predictions=iowaforest_model.predict(val_X)\nprint(mean_absolute_error(val_y, iowa_predictions))\n\n\n#section 7\n#Summerizes everything done in sections 1-7 and predicts the prices\n#However, there are more sections below that make the code more complex and predictions more accurate\nprint('Section 8: Submission and final file with test')\n#get the right columns\ntrain_y=data.SalePrice\ndatapredictors=['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr', 'TotRmsAbvGrd']\ntrain_X=data[datapredictors]\n#fitting the data\niowafinalmodel=RandomForestRegressor()\niowafinalmodel.fit(train_X, train_y)\n\n#Read the test data\ntest=pd.read_csv('../input/test.csv')\nprint(test.describe())\n#treat the test data in the same way as training data. Pull same columns\ntest_X=test[datapredictors]\n#Use model to make predictions\nfinalpredicted_prices=iowafinalmodel.predict(test_X)\n\n#check the predicted prices\nprint(finalpredicted_prices)\n\n#submit\nmy_submission=pd.DataFrame({'Id': test.Id, 'SalePrice': finalpredicted_prices})\n#you could use any filename. We choose subission here\nmy_submission.to_csv('submission.csv', index=False)\n\n\n#NOW WE WILL USE OTHER TOOLS TO MAKE PREDICTIONS BETTER\n#CHAPTER 2: Attempt to MAke Better Predictions\nprint('Chapter 2: Attempt to Make Better Predictions')\n#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n#Section 1: Dealing with missing values in data\nprint ('Section 1: Dealing With Missing Values in Data')\n#set up the problem and then compare all the options to eliminate missing values and find the best one\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\ndata = pd.read_csv('../input/train.csv') #training data\n#how many missing values?\n#print(data.isnull().sum())\n\ntrain_target = data.SalePrice\ntrain_predictors = data.drop(['SalePrice'], axis=1)\nnumeric_predictors = train_predictors.select_dtypes(exclude=['object']) #excludes not numeric values\ntrain_X, test_X, train_y, test_y = train_test_split(numeric_predictors, train_target, random_state=0) #split data to see error\n\n#fuction that returns MAE (error)\ndef get_mae(train_X, test_X, train_y, test_y):\n    iowaforest_model=RandomForestRegressor()\n    iowaforest_model.fit(train_X, train_y)\n    iowa_predictions=iowaforest_model.predict(test_X)\n    print(mean_absolute_error(test_y, iowa_predictions))\n    \n    \n#THREE OPTIONS TO DEAL WITH MISSING VALUES\n#Option 1: Droping columns with missing values\nprint('Option 1')\n# you can drop columns with data_withou_missing_values=original_data.dropna(axis=1)\ncols_with_missing = [col for col in train_X.columns \n                        if train_X[col].isnull().any()]\nreduced_train_X=train_X.drop(cols_with_missing, axis=1)\nreduced_test_X=test_X.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nget_mae(reduced_train_X, reduced_test_X, train_y, test_y) #find error with this option\n\n#no missing values in training data now! However, we lose useful information too\n\n#Option 2: IMPUTATION\nprint('Option 2')\n#assign not exact value to the missing spot\nfrom sklearn.preprocessing import Imputer\nmy_imputer = Imputer()\nimputed_train_X=pd.DataFrame(my_imputer.fit_transform(train_X))\nimputed_test_X=pd.DataFrame(my_imputer.transform(test_X))\nimputed_train_X.columns=train_X.columns\nimputed_test_X.columns=test_X.columns\nprint(\"Mean Absolute Error from Imputation:\")\nget_mae(imputed_train_X, imputed_test_X, train_y, test_y)\n\n#Section 2: Dealing with categorical data (words instead of numbers)\n#lets see how much of train_X is categorical\ntrain_target = data.SalePrice\ntrain_predictors = data.drop(['SalePrice'], axis=1)\n\ntrain_X, test_X, train_y, test_y = train_test_split(train_predictors, train_target, random_state=0) #split data to see error\n\n#lets one-hot encode the data and that use imputation and compare final values\nnewtrain_X=pd.get_dummies(train_X)\nnewtest_X=pd.get_dummies(test_X)\nhotencodedtrain_X, hotencodedtest_X = newtrain_X.align(newtest_X, join='left', axis=1) #allign two datasets\n\n#now find MAE and answer and compare with the results we has in secion of this chapter\nmy_imputer = Imputer()\nimputed_train_X=pd.DataFrame(my_imputer.fit_transform(hotencodedtrain_X))\nimputed_test_X=pd.DataFrame(my_imputer.transform(hotencodedtest_X))\nimputed_train_X.columns=hotencodedtrain_X.columns\nimputed_test_X.columns=hotencodedtest_X.columns\n                                \nprint(\"Mean Absolute Error from Imputation and Categorical Data:\")\nget_mae(imputed_train_X, imputed_test_X, train_y, test_y)\n\nprint('Section 3: New Model/Calssifier - XGBoost')\n#Section 3: New Better Model/Classifier - XGBoost\n#set up the problem\ntrain_target = data.SalePrice\ntrain_predictors = data.drop(['SalePrice'], axis=1)\nnumeric_predictors = train_predictors.select_dtypes(exclude=['object']) #excludes not numeric values\ntrain_X, test_X, train_y, test_y = train_test_split(numeric_predictors, train_target, random_state=0) #split data to see error\n#get rid of misssing values\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n#now work with the new model\nfrom xgboost import XGBRegressor\nmy_model=XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=2)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n\n#we found that best iteration was 200!\n#predict\npredictions=my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n#compare and assign 200 in the beggining\nmy_model=XGBRegressor(n_estimators=200, learning_rate=0.05, n_jobs=2)\nmy_model.fit(train_X, train_y)\npredictions=my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n\n\n#section 4: Partial Dependence Plot\n#valuable way to extract insights from from the model\n#for patial dependence plots we need to use a new model GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n\ny=data.SalePrice\n#Choosing Predictors\ndatapredictors=['YearBuilt', 'LotArea', 'BedroomAbvGr', 'Id']\nX=data[datapredictors]\n\n\nmy_imputer=Imputer()\nX=my_imputer.fit_transform(X)\nmy_model = GradientBoostingRegressor()\n\nmy_model.fit(X,y)\n#make a plot\nmy_plots = plot_partial_dependence(my_model,       \n                                   features=[0,3], # column numbers of plots we want to show\n                                   X=X,            # raw predictors data.\n                                   feature_names=['YearBuilt', 'LotArea', 'BedroomAbvGr', 'Id'], # labels on graphs # supposed to have the right order\n                                   grid_resolution=20) # number of values to plot on x axis\n\n\n\n#section 5: Introduction of pipelines\n#makes code shorter and faster which leads to fewer mistakes\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor\n# we have already seperated training and test data\n\n#use the pipeline\nmy_pipeline=make_pipeline(Imputer(),GradientBoostingRegressor())\nmy_pipeline.fit(train_X,train_y)\npredictions=my_pipeline.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))\n\n\n#section 6:\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04becaecca84c4b4d7e81563a64ba8c8a605fec4"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}