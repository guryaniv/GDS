{"cells":[{"metadata":{"trusted":false,"_uuid":"c02f92bc2e8e0cd459e718fdd48e759efe46a4eb"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\n\n\n\n\npd.set_option(\"display.max_columns\",100)\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"acda042572d37657b0697af333ca8bdcda9df675"},"cell_type":"code","source":"print('Importing data...')\ndata = {\n    'train': pd.read_csv('../input/application_train.csv'),\n    'test': pd.read_csv('../input/application_test.csv'),\n    'bb': pd.read_csv('../input/bureau_balance.csv'),\n    'b': pd.read_csv('../input/bureau.csv'),\n    'ccb': pd.read_csv('../input/credit_card_balance.csv'),\n    'ip': pd.read_csv('../input/installments_payments.csv'),\n    'POSb': pd.read_csv('../input/POS_CASH_balance.csv'),\n    'previous': pd.read_csv('../input/previous_application.csv')\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"60a22e04222b16adff98e130c599a686c59774a2"},"cell_type":"code","source":"def cat_features(df):\n    cat_f = df.select_dtypes(include = ['object']).apply(lambda x: x.nunique(dropna=False), axis = 0)\n    return cat_f\n\ndef cat_levels(df,cat_info):\n    cat_f = df[cat_info.index]\n    levels = {}\n    for c in cat_f:\n        level = list(df[c].replace(np.nan,'NaN').unique())\n        levels.update({c:level})\n    return pd.DataFrame.from_dict(levels,orient='index').fillna('')\n\ndef bin_num(df):\n    binary_f = []\n    for c in df.columns:\n        if len(df[c].unique())==2 and c not in cat_features(df).index:\n            binary_f.append(c)\n    return binary_f\n\ndef cat_plot(df,cols,r,c,figsize):\n    fig, ax = plt.subplots(r,c,figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        row = i//c\n        col = i%c\n        axa = sns.countplot(x=colname, data=df,ax = ax[row,col])\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n        plt.tight_layout()\n    plt.show()\n\ndef cat_plot_target(df,cols,r,c,figsize):\n    plt.figure(figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        df_plot = df[['TARGET',colname]].dropna().melt(['TARGET'],value_name=colname)\n        df_group = df_plot.groupby(['TARGET',colname],as_index = False).count()\n        sums = df_group.groupby('TARGET',as_index=False).sum()['variable']\n        df_group['sum'] = df_group['TARGET'].apply(lambda x: sums[0] if x==0 else sums[1])\n        df_group['percent'] = df_group['variable']/df_group['sum']\n        plt.subplot(r,c,i+1)\n        axa = sns.barplot(x = colname, y = 'percent', hue = 'TARGET',data = df_group)\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n    plt.tight_layout()\n    plt.show()\n\ndef num_abberrant(df):\n    num_df = df.describe()\n    abb = []\n    for n in num_df.columns:\n        low = num_df[n]['mean']-3*num_df[n]['std']\n        up = num_df[n]['mean']+3*num_df[n]['std']\n        if num_df[n]['min'] < low or num_df[n]['max'] > up:\n            abb.append(n)\n    return num_df[abb]\n\n# all values in normalized data should be in [0,1]\ndef norm_abb(df):\n    norm_abb = []\n    for c in df.columns:\n        if df[c].dropna().between(0,1).all() == False:\n            print('Values of column ' + c + ' not in range [0,1].')\n            norm_abb.append(c)\n    return norm_abb\n\n# all values in 'time only relative to the application' data should be negative\ndef time_abb(df):\n    time_abb = []\n    for c in df.columns:\n        if df[c].dropna().le(0).all() == False:\n            print('Values of column ' + c + ' has value greater than 0.')\n            time_abb.append(c)\n    return time_abb\n\n# all values in 'rounded' data should be integer\ndef round_abb(df):\n    round_abb = []\n    for c in df.columns:\n        if df[c].dropna().dtype != int:\n            print('Values of column ' + c + ' has value greater than 0.')\n            round_abb.append(c)\n    return round_abb\n\ndef sns_distplot(df,cols,r,c):\n    plt.figure(figsize = (24,12))\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        sns.distplot(df[cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()\n\ndef sns_distplot_target(df,cols,r,c,figsize):\n    plt.figure(figsize = figsize)\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        df_plot = df[['TARGET',cols[i]]].melt(['TARGET'],value_name=cols[i])\n        sns.distplot(df_plot[df_plot['TARGET']==0][cols[i]].dropna())\n        sns.distplot(df_plot[df_plot['TARGET']==1][cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()\n\ndef NA_finder(df):\n    NA_f = df.isnull().sum() \n    NA_f = NA_f[NA_f != 0].sort_values(ascending=False)\n    NA_f_percent = NA_f.sort_values(ascending=False)/len(df)*100.0\n    plt.figure(figsize=(20,20))\n    NA_f_percent.plot.bar()\n    plt.title('NA percentage distribution for NA containing features.')\n    plt.ylabel('Percentage (%)')\n    plt.show()\n    return NA_f_percent\n\ndef cat_plot_bureau(df,cols,r,c,figsize):\n    plt.figure(figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        df_plot = df[['CREDIT_ACTIVE',colname]].dropna().melt(['CREDIT_ACTIVE'],value_name=colname)\n        df_group = df_plot.groupby(['CREDIT_ACTIVE',colname],as_index = False).count()\n        df_group['counts'] = df_group['variable']\n        plt.subplot(r,c,i+1)\n        axa = sns.barplot(x = colname, y = 'counts', hue = 'CREDIT_ACTIVE',data = df_group)\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n    plt.tight_layout()\n    plt.show()\n    \ndef sns_distplot_bureau(df,cols,r,c,figsize):\n    plt.figure(figsize = figsize)\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        df_plot = df[['CREDIT_ACTIVE',cols[i]]].melt(['CREDIT_ACTIVE'],value_name=cols[i])\n        sns.distplot(df_plot[df_plot['CREDIT_ACTIVE']=='Active'][cols[i]].dropna())\n        sns.distplot(df_plot[df_plot['CREDIT_ACTIVE']=='Closed'][cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d0a04bbca3e9b8c3b900b361bb1a737dca9a12"},"cell_type":"markdown","source":"# Training and testing dataset"},{"metadata":{"_uuid":"8c25120f703637f13942854bd473b38d31487567"},"cell_type":"markdown","source":"To make good assumption about the features and target value, I will use the training set data for correlation analysis."},{"metadata":{"trusted":false,"_uuid":"b30f396ecf6efb7b9289dc3d3a919b59220e7a36"},"cell_type":"code","source":"train = data['train']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cfe3ba197fffa728c35c89234f101400457d2e1"},"cell_type":"markdown","source":"__1. target distribution__"},{"metadata":{"trusted":false,"_uuid":"64a4f806b63d8d36c3e661e4605015cc28cf3d38"},"cell_type":"code","source":"plt.hist(train['TARGET'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e013e36b99d1102f48c9be6ea6c2a4282fbbb4bd"},"cell_type":"markdown","source":"The label looks highly imbalanced. Use ROC AUC score as performance metric will be helpful for model validation. To perform feature engineering on training and testing data together, I concatened the two datasets."},{"metadata":{"trusted":false,"_uuid":"f2a70bcff1c42fb8391dfe5bbc12226f7914c8ab"},"cell_type":"code","source":"df = pd.concat([data['train'],data['test']])\ntrain_row = data['train'].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3a2fc68b18dbbfee3ac9de99abca77fd6a4d7a9"},"cell_type":"markdown","source":"__2. categorical features__<br/>\nThe nunique function has dropna=True as default, which is not what I want. I want to find the binary features for label encoding, if it contains NAN values, then OneHotEncoder will be more useful here."},{"metadata":{"trusted":false,"_uuid":"c1473b535c7b6abc8c1aed00f822196dad8e46b1"},"cell_type":"code","source":"# summary of categorical features\ncats = cat_features(train)\ncat_levels = cat_levels(df,cats)\ncat_levels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75e80cc3bafd2dd650d92f148fee5e24a2b68c2a"},"cell_type":"markdown","source":"By examing the categorical features, I found some categorical data use 'XNA' instead of np.nan as NA values. So next I will replace 'XNA' as np.nan values in both train and test set. I also found 'WEEKDAY_APPR_PROCESS_START' is a time feature that may be encoded by LabelEncoder, but I still chosed to use OneHotEncoding to avoid possible missing feature characteristics."},{"metadata":{"trusted":false,"_uuid":"676e8ba8e27d5288b7c8b97d34f89f734a126901"},"cell_type":"code","source":"# find 'XNA'\ndf.replace('XNA',np.nan,inplace = True)\ntrain.replace('XNA',np.nan,inplace = True)\n# find binary features\nbin_cats = cats[cats == 2].index\nprint('binary categorical features are: ' + str(bin_cats))\nbin_nums = bin_num(df)\nprint('binary numerical features are: ' + str(bin_nums))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ebfb3a40da91caf5db33b1f8b8f82793d100fa5"},"cell_type":"markdown","source":"To further investigate the categorical features' effect on target, I plotted the percentage distribution on different TARGET conditions for all categorical features."},{"metadata":{"trusted":false,"_uuid":"df10bdac2a11022e659177e93f6399fc2a22c8a0"},"cell_type":"code","source":"cat_plot_target(train,cats.index,4,4,(15,15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cca44f5fae6125980a7b64850c7f261851f25efa"},"cell_type":"markdown","source":"From the above paired plots, I find 'HOUSETYPE_MODE' feature value of 'block of flats' and 'EMERGENCYSTATE_MODE' feature do not affect the TARGET value. So I will clean them from my dataset for modeling.<br/>\n<br/>\n__3. numerical features__<br/>\nI first look at description table and try to group different numerical features."},{"metadata":{"trusted":false,"_uuid":"03dca0049efc5af597a9318d3d43da6407ccedb6"},"cell_type":"code","source":"print(df['DAYS_EMPLOYED'][df['DAYS_EMPLOYED']>0].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c85e6d68cb9b01ac953546fba6f2af0e7c3cc833"},"cell_type":"code","source":"df['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\ntrain['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b8e4061bd8af8d97a9937c83751427401cc0220"},"cell_type":"code","source":"num_f = [f for f in df.columns if (f not in cats.index)]\nnum_f.remove('TARGET')\nnum_f.remove('SK_ID_CURR')\ndist_plot_f = [f for f in num_f if df[f].nunique()>50]\nprint(str(len(dist_plot_f)) + ' numerical features will be compared in the distribution plots.')\nsns_distplot_target(train,dist_plot_f,10,5,(24,24))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dfa2a225597dff981f6efe70ee39a32c5731a793"},"cell_type":"code","source":"bar_plot_f = [f for f in num_f if f not in dist_plot_f]\nprint(str(len(bar_plot_f)) + ' numerical features will be compared in the percentage bar plots.')\ncat_plot_target(train,bar_plot_f,11,5,(24,48))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f21f424d312f8d376ec0c7cd0d8251599e63a378"},"cell_type":"markdown","source":"From the distribution and bar plots, I found the following features have almost the same distribution for both target values and will be cleaned in my dataset for modeling:\n            ['COMMONAREA_MEDI','COMMONAREA_MODE','LANDAREA_MEDI','LANDAREA_MODE','LIVINGAPARTMENTS_MEDI',\n              'LIVINGAPARTMENTS_MODE','LIVINGAREA_MEDI','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MEDI',\n              'NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_MEDI','NONLIVINGAREA_MODE','YEARSBEGINEXPLUATATION_MEDI',\n                'YEARSBEGINEXPLUATATION_MODE','YEARSBUILD_MEDI','YEARSBUILD_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', \n              'AMT_REQ_CREDIT_BUREAU_WEEK','DEF_60_CNT_SOCIAL_CIRCLE','ELEVATORS_MODE',\n              'FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n              'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n              'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', \n              'FLAG_EMAIL', 'FLAG_MOBIL', 'FLOORSMAX_MODE','FLOORSMIN_MODE','OBS_60_CNT_SOCIAL_CIRCLE',\n              'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']"},{"metadata":{"trusted":false,"_uuid":"4f31da853dcb5c7befbab2ec0fb78ccef3594b75"},"cell_type":"code","source":"'EMERGENCYSTATE_MODE' in bar_plot_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aff286e33f2dfae785c34b09197430d21999809e"},"cell_type":"code","source":"drop_f = ['NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_WEEK','FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', 'FLAG_EMAIL', 'FLAG_MOBIL', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']\ndf = df.drop(drop_f,axis=1)\ntrain = train.drop(drop_f,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e246216b2c10a6b068a16885e0a149444a9f3c6"},"cell_type":"markdown","source":"I find for some feature contains a lot of NA values, a good way to impute these data is important since the goal for this project is to help people with few credit history."},{"metadata":{"_uuid":"a42c8fc14881e312e9882c509471ca77c33aa70d"},"cell_type":"markdown","source":"__3. NA values__"},{"metadata":{"trusted":false,"_uuid":"406dc2a1015e66269b601b4a161ef2711698a75b"},"cell_type":"code","source":"NA_f = NA_finder(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e8ed1eee2a78d90d80cd5c77868af129b7a2873"},"cell_type":"raw","source":"######### Summary feature engineering for train/test dataset\n\ndef traintest_processing(df):\n    df.replace('XNA',np.nan,inplace = True)\n    df['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\n    bin_f = bin_finder(df)\n    for b in bin_f:\n        le.fit(df[b])\n        df[b] = le.transform(df[b])\n    def df['EMERGENCYSTATE_MODE']\n    drop_f = ['NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', \n              'AMT_REQ_CREDIT_BUREAU_WEEK','FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n              'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n              'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', \n              'FLAG_EMAIL', 'FLAG_MOBIL', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']\n    df = df.drop(drop_f,axis=1)\n    df_dum = pd.get_dummies(df)\n    ### new features learnt from other kernels\n    df_dum['DAYS_EMPLOYED_PERC'] = df_dum['DAYS_EMPLOYED'] / df_dum['DAYS_BIRTH']\n    df_dum['INCOME_CREDIT_PERC'] = df_dum['AMT_INCOME_TOTAL'] / df_dum['AMT_CREDIT']\n    df_dum['INCOME_PER_PERSON'] = df_dum['AMT_INCOME_TOTAL'] / df_dum['CNT_FAM_MEMBERS']\n    df_dum['ANNUITY_INCOME_PERC'] = df_dum['AMT_ANNUITY'] / df_dum['AMT_INCOME_TOTAL']\n    del df_dum['HOUSETYPE_MODE_block of flats']\n    return df_dum"},{"metadata":{"_uuid":"17da5758fc64f2534b07fb048c18dffa03f331ee"},"cell_type":"markdown","source":"# Bureau balance data"},{"metadata":{"trusted":false,"_uuid":"d42557edfddd361d41fda5a07c300448046dfcca"},"cell_type":"code","source":"bb = data['bb']\nbb.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ffd6e4ceeb1b82502aedfc5e4a2603bcf45604d"},"cell_type":"markdown","source":"The bureau balance data contains one numerical data and one categorical data, and this two data seems to be related. According to the description table, 'MONTHS_BALANCE' == -1 means the freshest balance date."},{"metadata":{"trusted":false,"_uuid":"5c260bf2f26d9cc3e0b650e286f69de7902b3fb4"},"cell_type":"code","source":"bb['STATUS'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1652fc9bf9db72ef295c78b1617d04093bd99dce"},"cell_type":"code","source":"bb_fresh = bb[bb['MONTHS_BALANCE'] == -1]\nsns.countplot(x=\"STATUS\", data=bb_fresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"52d98d6e2abfcaa7f1530a9140e4f651e0a8797c"},"cell_type":"code","source":"# length of balance history is a good feature\n# scoring of previous status is also a good one\ncount = bb[['SK_ID_BUREAU','MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count().rename(columns = {'MONTHS_BALANCE':'HIST_LEN'}).reset_index()\nstatus = bb[bb['MONTHS_BALANCE'] == -1][['SK_ID_BUREAU','STATUS']]\nbb_join = pd.merge(count,status,on='SK_ID_BUREAU')\nbb_dum = pd.get_dummies(bb)\nbb_dum_sum = bb_dum.groupby('SK_ID_BUREAU',as_index=False).sum()\nweights = np.array([1,2,3,4,5])\n# score1 is the weighted sum of DPD status\nbb_dum_sum['BUREAU_SCORE1'] = (bb_dum_sum.iloc[:,2:7]*weights).sum(axis=1)\nbb_score = pd.merge(bb_dum_sum[['SK_ID_BUREAU','BUREAU_SCORE1']], bb_join, on='SK_ID_BUREAU')\n# score2 is the ratio of score1 and history length\nbb_score['BUREAU_SCORE2'] = bb_score['BUREAU_SCORE1']/bb_score['HIST_LEN']\n# score3 is the reward score of not have any DPD\nbb_score['BUREAU_SCORE3'] = bb_score['BUREAU_SCORE2'].apply(lambda x: 1 if x==0 else 0)\nbb_score.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"698554200c09136b911aa395160f1fba449c1eff"},"cell_type":"markdown","source":"# Bureau data"},{"metadata":{"_uuid":"bc4ce14926bb553036f50780f4c7b736b3b72187"},"cell_type":"markdown","source":"__1. Overview__ <br/>\nBureau balance and bureau data are connected tightly, I will merge the two tables together to perform EDA. Before merging, I found like bureau balance data, bureau data has a column showing the status of account, I want to see if they show the same information."},{"metadata":{"trusted":false,"_uuid":"4f85f08c642083b5f620e6036778b41ab59c94dd"},"cell_type":"code","source":"b = data['b']\ncredit_status_check = pd.merge(bb_fresh,b[['SK_ID_BUREAU','CREDIT_ACTIVE']],on = 'SK_ID_BUREAU',how = 'inner')\ncredit_status_check.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4858ab5d5038677244dca70b1b245f043bc52749"},"cell_type":"markdown","source":"So these two columns are actually different information, I cannot combine the two information together."},{"metadata":{"trusted":false,"_uuid":"4b9cb4cd9f3798bfd9797168e72066c3b440f643"},"cell_type":"code","source":"bureau = pd.merge(b,bb_score,on='SK_ID_BUREAU')\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33cfe49624135767f1031ff526cf7a95f5a7884f"},"cell_type":"markdown","source":"__2. Categorical variables__"},{"metadata":{"trusted":false,"_uuid":"4a634afb893d6ff304e93bb0da84a9cedc3bb9c0"},"cell_type":"code","source":"bureau_cats = cat_features(bureau)\nbureau_cats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9c4908daaa4bea45f54c48b7bd71c55d32e1aa5"},"cell_type":"markdown","source":"__3. data split into two sets based on 'CREDIT_ACTIVE' feature__<br/>\nThe 'CREDIT_ACTIVE' is an important feature when we consider historical data. The number of bureau records that are still active or already closed can affect other features as well. So I want to examine if 'CREDIT_ACTIVE' condition can affect a lot of other feature distributions."},{"metadata":{"trusted":false,"_uuid":"da1bdb9453313a3f3d193e98354f91609bbe3bb7"},"cell_type":"code","source":"cat_plot_bureau(bureau,bureau_cats.index.drop('CREDIT_ACTIVE'),1,3,(15,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1701965164081675dbaa74dfab1aae88102da1eb"},"cell_type":"markdown","source":"The categroical features countplots look very different for active and closed accounts. So I think it's necessary to analyze them seperately when apply feature enigneering."},{"metadata":{"trusted":false,"_uuid":"be558de46a4daa07571b89db7879fd0e4de950c8"},"cell_type":"code","source":"bureau_active = bureau[bureau['CREDIT_ACTIVE'] == 'Active'].drop('CREDIT_ACTIVE',axis=1)\nbureau_closed = bureau[bureau['CREDIT_ACTIVE'] == 'Closed'].drop('CREDIT_ACTIVE',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ad5c4a9b2716680eaa78590eecbaa0fd7518b20"},"cell_type":"markdown","source":"__4. Single value column exploration__<br/>\nThe 'DAYS_ENDDATE_FACT' feature only has value in closed bureau credit account (in feature description). I need to find features like this and delete them."},{"metadata":{"trusted":false,"_uuid":"4a478c84cac3731c45261eca60db00b3ed0e05cf"},"cell_type":"code","source":"bureau_active = bureau_active.drop([col for col in bureau_active.columns \n                                    if bureau_active[col].nunique(dropna=False) == 1],axis=1)\nprint(str(bureau.shape[1]-1-bureau_active.shape[1]) + ' columns dropped due to columns contain only 1 value.')\nbureau_closed = bureau_closed.drop([col for col in bureau_closed.columns \n                                    if bureau_closed[col].nunique(dropna=False) == 1],axis=1)\nprint(str(bureau.shape[1]-1-bureau_closed.shape[1]) + ' columns dropped due to columns contain only 1 value.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9fad2ed70f2735167d53e954725065525ee73c3"},"cell_type":"markdown","source":"Unfortunately, the 'DAYS_ENDDATE_FACT' column is not found by nunique method. I have to mannually remove this column. In case of the samething happen, I first looked at the NA value distribution in all variable."},{"metadata":{"trusted":false,"_uuid":"6e964c9930f26f7f1e65ba39df0f8d96e8ae1e0d"},"cell_type":"code","source":"bureau_active.isnull().sum()/len(bureau_active)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f97cdcc369f6c27cb2a145127a1369a914fd44ef"},"cell_type":"code","source":"bureau_closed.isnull().sum()/len(bureau_closed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb6de11ffcce8a91b590e509cb57080717b7ea17"},"cell_type":"markdown","source":"From the above information, I find only 'DAYS_ENDDATE_FACT' should be removed from bureau_active."},{"metadata":{"trusted":false,"_uuid":"32ae77601f4b6842d9354320f5a202b857147cba"},"cell_type":"code","source":"bureau_active.drop('DAYS_ENDDATE_FACT',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d58043dd5eb313f6bf399aade6eb9d2d59f2fe45"},"cell_type":"markdown","source":"__5. 'ground truth' validation__<br/>\nLike when exploring the training and test set data, I will check for some ground truth restrictions."},{"metadata":{"_uuid":"6b32817f3b1207820e0b092315c96d2ec15edae1"},"cell_type":"markdown","source":"The 'DAYS_CREDIT_ENDDATE' feature has 46039 values above 0, the description of this feature was clear about that it was the date decided by the loan company. I will still keep the positive values."},{"metadata":{"_uuid":"0b5ac189ce6a3359f93aeed7da5a70a8f8e89628"},"cell_type":"markdown","source":"__6. Aggregation function exploration__"},{"metadata":{"_uuid":"38a88402d77666792f5ef77e5c43266816a6bd59"},"cell_type":"markdown","source":"Since each SK_ID_CURR has multiple SK_BUREAU_ID, I will need aggregation function for different features. I first examine the characteristics of different features by looking at the description table.<br/>\nTo apply aggregation function, I should first encode the categorical variables."},{"metadata":{"trusted":false,"_uuid":"72e9b5f863458130ecf9ee9a0bda7e4b70493376"},"cell_type":"code","source":"# no binary categorical variable, use one-hot-encoding\nbureau_active_dum = pd.get_dummies(bureau_active)\nbureau_closed_dum = pd.get_dummies(bureau_closed)\nbureau_active_dum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"deee6949af6970af9e30166bbcd38ddfdc0dee16"},"cell_type":"code","source":"bureau_f = bureau.drop(['SK_ID_CURR','SK_ID_BUREAU'],axis=1).columns\nbureau_num_f = [f for f in bureau_f if f not in bureau_cats.index]\nsns_distplot_bureau(bureau,bureau_num_f,3,6,(24,12))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3785201f9a80a1440c67b79145a256d383a4a4f"},"cell_type":"markdown","source":"From the distribution plots, I think all above features should be retained in the dataset. Here are the aggregation functions I decided after observing each feature:"},{"metadata":{"trusted":false,"_uuid":"6a838fee950e76dc8371ca89714b9bad302e9bb4"},"cell_type":"code","source":"bureau_agg = {\n    'SK_ID_BUREAU': ['count'],\n    'DAYS_CREDIT': ['min','max','mean','median','var'],\n    'CREDIT_DAY_OVERDUE': ['min','max'],\n    'DAYS_CREDIT_ENDDATE': ['min','max','mean'],\n    'AMT_CREDIT_MAX_OVERDUE': ['min','max','mean'],\n    'CNT_CREDIT_PROLONG': ['mean','sum'],\n    'AMT_CREDIT_SUM': ['sum','mean'],\n    'AMT_CREDIT_SUM_DEBT': ['sum','mean'],\n    'AMT_CREDIT_SUM_LIMIT': ['max','mean'],\n    'AMT_CREDIT_SUM_OVERDUE': ['sum'],\n    'DAYS_CREDIT_UPDATE': ['min','max'],\n    'AMT_ANNUITY': ['min','max','mean'],\n    'BUREAU_SCORE1': ['mean','min','max'],\n    'HIST_LEN': ['max','min','mean'],\n    'BUREAU_SCORE2': ['mean'],\n    'BUREAU_SCORE3': ['mean','max'],\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a807b937429b1be1a5d9097972caa3dcfbf4d7a1"},"cell_type":"markdown","source":"For the features created by one-hot-encodeing, I think taking the mean is reasonable."},{"metadata":{"trusted":false,"_uuid":"02a9606a57e5e068922503b2047168b765675387"},"cell_type":"code","source":"bureau_active_agg = {}\nbureau_active_agg.update(bureau_agg)\nfor col in bureau_active_dum.columns:\n    if col not in bureau_active_agg.keys():\n        bureau_active_agg.update({col:['mean']})\n\nbureau_closed_agg = {}\nbureau_closed_agg.update(bureau_agg)\nfor col in bureau_closed_dum.columns:\n    if col not in bureau_closed_agg.keys():\n        bureau_closed_agg.update({col:['mean']})\n\ndel bureau_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2ab48a078aaeab712527bff176eef7069b536d3"},"cell_type":"code","source":"bureau_active_grouped = bureau_active_dum.groupby('SK_ID_CURR').agg(bureau_active_agg)\nbureau_active_grouped.columns = pd.Index(['BUREAU_ACTIVE_' + e[0] + '_' + e[1].upper() for e in bureau_active_grouped.columns.tolist()])\nbureau_closed_grouped = bureau_closed_dum.groupby('SK_ID_CURR').agg(bureau_closed_agg)\nbureau_closed_grouped.columns = pd.Index(['BUREAU_CLOSED_' + e[0] + '_' + e[1].upper() for e in bureau_closed_grouped.columns.tolist()])\nbureau_grouped = pd.merge(bureau_active_grouped,bureau_closed_grouped, left_index = True, right_index = True).reset_index()\nbureau_df = pd.merge(train[['SK_ID_CURR','TARGET']],bureau_grouped,on='SK_ID_CURR',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dffddb30fd8365f4ba5c6e6046483872ffdf65ef"},"cell_type":"code","source":"bureau_df.iloc[:,:4].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5b8f811c1b1bc4c96191674d2d977032bf094e08"},"cell_type":"code","source":"plot_cols = bureau_df.columns[2:]\ncat_plot_f = [f for f in plot_cols if bureau_df[f].nunique()<50]\ncat_plot_target(bureau_df,cat_plot_f,11,5,(24,48))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"edcb78966630df3fece8774a72bd9e96af6f0554"},"cell_type":"code","source":"dist_plot_f = [f for f in plot_cols if f not in cat_plot_f]\nsns_distplot_target(bureau_df,dist_plot_f,15,5,(24,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f15c7bbff7648a37d87366cd9823ee20f4713270"},"cell_type":"code","source":"drop_f = ['CREDIT_CURRENCY']\ndrop_dum_f = ['BUREAU_ACTIVE_CREDIT_TYPE_Another type of loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Cash loan (non-earmarked)_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Loan for business development_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Loan for the purchase of equipment_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Mobile operator loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Real estate loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Unknown type of loan_MEAN',\n              'BUREAU_ACTIVE_STATUS_2_MEAN',\n              'BUREAU_ACTIVE_STATUS_3_MEAN',\n              'BUREAU_ACTIVE_STATUS_4_MEAN',\n              'BUREAU_ACTIVE_STATUS_5_MEAN',\n              'BUREAU_CLOSED_CREDIT_DAY_OVERDUE_MIN',\n             'BUREAU_CLOSED_CREDIT_DAY_OVERDUE_MAX',\n             'BUREAU_CLOSED_CNT_CREDIT_PROLONG_MEAN',\n             'BUREAU_CLOSED_CNT_CREDIT_PROLONG_SUM',\n             'BUREAU_CLOSED_AMT_CREDIT_SUM_OVERDUE_SUM',\n             'BUREAU_CLOSED_CREDIT_TYPE_Another type of loan_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Cash loan (non-earmarked)_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for business development_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for the purchase of equipment_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for working capital replenishment_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Real estate loan_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Unknown type of loan_MEAN',\n             'BUREAU_CLOSED_STATUS_1_MEAN',\n             'BUREAU_CLOSED_STATUS_2_MEAN',\n             'BUREAU_CLOSED_STATUS_3_MEAN',\n             'BUREAU_CLOSED_STATUS_4_MEAN',\n             'BUREAU_CLOSED_STATUS_5_MEAN',\n              'BUREAU_CLOSED_DAYS_CREDIT_ENDDATE_MAX'\n             ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2abefcf42ddace609322052961d33432508fa42"},"cell_type":"markdown","source":"# Credit card balance"},{"metadata":{"trusted":false,"_uuid":"5d80c35338dfe41c13cae199bc4b32387adf2cfc"},"cell_type":"code","source":"credit = data['ccb']\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd3337ad49035fc0d47337521ede3e2805bcd000"},"cell_type":"code","source":"credit_cats = cat_features(credit)\ncredit_num_f = credit.iloc[:,2:].drop(credit_cats.index,axis=1).columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"53211df8319913c8d6e054bccdf3817e08168fb2"},"cell_type":"code","source":"sns_distplot(credit,credit_num_f,4,5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4be090deae7cf229eb80cffa900a2a0cf261c37a"},"cell_type":"markdown","source":"Like bureau data, 1 SK_ID_CURR has multiple SK_ID_PREV, aggregation functions need to be assigned. From the above exploration, I have the following aggregation functions:"},{"metadata":{"trusted":false,"_uuid":"f1c26a3380b7926211421dee74140973a9d154b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"932af76aeb9cfe9a4917dad42f8b7c9764d797bd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}