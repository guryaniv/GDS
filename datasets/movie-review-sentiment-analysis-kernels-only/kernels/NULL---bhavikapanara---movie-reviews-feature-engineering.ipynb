{"cells":[{"metadata":{"_uuid":"39b2cdce47327ef27023beaca6224f7be6d2b674"},"cell_type":"markdown","source":"** Introduction: Movie Review Sentiment Analysis**\n\nThis notebook is for those who are new to machine learning and deep learning industry. I want to introduce the basic concept to learn data, understand the data using some statistics and virtualization tools in order to start machine learning instead of jumping to a complicated model. Any suggestion is much appreciated.\n\nSentiment analysis is an example of supervised machine learning task a labelled dataset which containing text documents and their labels is used for a train a classifier."},{"metadata":{"_uuid":"eda33eb93e12f677c91f26117faa257089a20381"},"cell_type":"markdown","source":"**Objective:**\n\nEDA/ feature engineering  for movie review sentiment analysis"},{"metadata":{"_uuid":"da456b581034945ebbe6713f14906e934aa4629d"},"cell_type":"markdown","source":"** DATA**\n\nThe data is a movie review dataset for text classification. The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis. Reviews are classified into 5 following categories.\n\n * Negative\n * Somewhat negative\n * Neutral\n * Somewhat positive\n * Positive."},{"metadata":{"_uuid":"1cb8f4efad51ea06a40e6bbd69dfc8373baa76ed"},"cell_type":"markdown","source":"**Import libraries :**"},{"metadata":{"trusted":true,"_uuid":"2a0e07ce556cb2ec59922d00839888d30cf2101e","_kg_hide-input":true},"cell_type":"code","source":"#basic \n\nimport pandas as pd\nimport numpy as np\npd.set_option('max_colwidth',400)\nimport string\n\n#Graph\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport plotly.tools as tls\n%matplotlib inline\n\n#machine learning\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom scipy.sparse import hstack\n\n\n#Deep Learning\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n\n#NLP\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nfrom nltk.stem import WordNetLemmatizer\nimport textblob\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb1444fd080d1ed97220d1675df7a16be2f456ee"},"cell_type":"markdown","source":"**Read Data**"},{"metadata":{"trusted":true,"_uuid":"66ded9960fd168a9a86d93fe65a0717432b0ea90"},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/test.tsv' , sep=\"\\t\")\nsub =pd.read_csv('../input/sampleSubmission.csv')\n\ntrain.shape , test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf38a0fdaef48c531ee283732d26e33e49198c2"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"955ab7caeae424abdb99991486f497e793bc44bd"},"cell_type":"markdown","source":"Each Sentence has been parsed into many phrases by the parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data. Let's look at sentence number two with it phrases."},{"metadata":{"trusted":true,"_uuid":"189e462ccf6e7cf9cad43fd6e51727c424cd7267"},"cell_type":"code","source":"train.loc[train['SentenceId'] == 2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6381f5d6c76a006e8d8469b006555157ed4a58b4"},"cell_type":"markdown","source":"* Let's check missing value"},{"metadata":{"trusted":true,"_uuid":"159eac111a828c651c5bad659b9a8a0cc169a56a"},"cell_type":"code","source":"def check_missing(df):\n    \n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum()/df.isnull().count()*100).sort_values(ascending = False)\n    missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n    \n\nmissing_data_df = check_missing(train)\nmissing_data_test = check_missing(test)\nprint('Missing data in train set: \\n' , missing_data_df.head())\nprint('\\nMissing data in test set: \\n'  ,missing_data_test.head())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfae67ef16d951741ec149edd3efa16f37c54eb6"},"cell_type":"markdown","source":"No missing data in both train and test set. Go Further and find something interesting in data."},{"metadata":{"_uuid":"fa6dba618e94e3bf517a99db168d6d42dccd7df9"},"cell_type":"markdown","source":"** Class Imbalanced **"},{"metadata":{"trusted":true,"_uuid":"da795e28026582b42e60a61140cf227ae0347946","_kg_hide-input":true},"cell_type":"code","source":"temp = train['Sentiment'].value_counts()\n\ntrace = go.Bar(\n    x = temp.index,\n    y = temp.values,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of Class Label in train dataset\",\n    xaxis=dict(\n        title='Class Label',\n        tickfont=dict(\n            size=10,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Occurance of Class label',\n        titlefont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='Sentiment')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c272095af9675c7f5817f659eed4e4b4002a25c"},"cell_type":"markdown","source":"The reviews are not evenly spread into categories.\nmajority of the reviews are classified as \"neutral\" class.\nOhh it's imbalanced data"},{"metadata":{"_uuid":"02e51b35ffa6832f9bec7ac0ea726da2b61d075a"},"cell_type":"markdown","source":"**Feature Engineering:**\nHere, I have explained Feature Engineering in deeply.\n\nFeature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms works well. "},{"metadata":{"_uuid":"5f2434021cea1323312a6dc50e7d5f4e9421060f"},"cell_type":"markdown","source":"1 )New features are created from text feature.\n* Number of character in Phrase\n* Number of words in Phrase\n* Number of punctuation_count\n* Number of uppercase words \n* Number of stopwords\n* Number of positive words in Phrase\n* Number of negative words in Phrase\n\n2 )Frequency distribution of Part of Speech Tags:\n* Noun Count\n* Verb Count\n* Adjective Count\n* Adverb Count\n* Pronoun Count\n\n3)Sentiment analysis using Textblob library\n* Subjectivity\n* polarity"},{"metadata":{"_uuid":"248225b37ad544ea556239fe0ec301f28c455658"},"cell_type":"markdown","source":"Let's first create features from a text. But, First of all, merge train and test set."},{"metadata":{"trusted":true,"_uuid":"625b1fcac2ef7495a4ceaee31344b666a72ae17b"},"cell_type":"code","source":"length = len(train)\ndf = pd.concat([train, test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a38218a8fb7ac17cecabbcef97866a38c30984"},"cell_type":"code","source":"# generate clean text from Phrase \ndef review_to_words(raw_review): \n    review =raw_review\n    review = re.sub('[^a-zA-Z]', ' ',review)\n    review = review.lower()\n    review = review.split()\n    lemmatizer = WordNetLemmatizer()\n    review = [lemmatizer.lemmatize(w) for w in review if not w in set(stopwords.words('english'))]\n    return (' '.join(review))\n\ndf['Clean_text'] = df['Phrase'].apply(lambda x : review_to_words(x))\n\ndf['Clean_text'].replace('', str('something'), inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b57f348633c7f9d88755572c83e25bfc6d1dc1"},"cell_type":"code","source":"df['char_count'] = df['Phrase'].apply(len)\ndf['word_count'] = df['Phrase'].apply(lambda x: len(x.split()))\ndf['word_density'] = df['char_count'] / (df['word_count']+1)\ndf['punctuation_count'] = df['Phrase'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \ndf['title_word_count'] = df['Phrase'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\ndf['upper_case_word_count'] = df['Phrase'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\ndf[\"stopword_count\"] = df['Phrase'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ca717ed7b5642fd2f926022e151c71fc154dd38"},"cell_type":"markdown","source":"We required to find positive words and negative words in order to create a feature number of positive words and the number of negative words in Phrase."},{"metadata":{"_uuid":"8d72d0ba97a38f0dd9709f2af911483d7a0bf396"},"cell_type":"markdown","source":"Let's consider words of a phrase as positive words which classified into class Positive(label: 4)"},{"metadata":{"trusted":true,"_uuid":"a47b7981dda2197a44bf6a4c255217178903127c"},"cell_type":"code","source":"positive = df['Clean_text'][df['Sentiment']== 4 ]\n\nvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None, max_features = 6877)\n\npos_words = vectorizer.fit_transform(positive)\npos_words = pos_words.toarray()\npos= vectorizer.get_feature_names()\nprint (\"Total number of positive words : \" ,len(pos))\n\ndist = np.sum(pos_words, axis=0)\npostive_new= pd.DataFrame(dist)\npostive_new.columns=['word_count']\npostive_new['word'] = pd.Series(pos, index=postive_new.index)\ntop = postive_new.sort_values(['word_count'] , ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e52cd81ac25fa765deee79872fdf8914cf8214e"},"cell_type":"markdown","source":"Let's consider words of a phrase as negative words which classified into class Negative( class label : 0)"},{"metadata":{"trusted":true,"_uuid":"12685e6c11f5b98634c0e3cdecd8937c4e31db2d"},"cell_type":"code","source":"negative=df['Clean_text'][df['Sentiment']== 0]\n\nneg_vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None,preprocessor = None, max_features = 6891)\n\nneg_words = neg_vectorizer.fit_transform(negative)\nneg_words = neg_words.toarray()\nneg= neg_vectorizer.get_feature_names()\nprint (\"Total number of negative words :\",len(neg))\n\ndist = np.sum(neg_words, axis=0)\nnegative_new= pd.DataFrame(dist)\nnegative_new.columns=['word_count']\nnegative_new['word'] = pd.Series(neg, index=negative_new.index)\ntop_neg = negative_new.sort_values(['word_count'] , ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80df64ed92867cb9eb57de0fde89209feace713d"},"cell_type":"markdown","source":"Now we count the number of positive words and number of negative words in a Phrase.\n\none more feature is : Ratio of positive word count to negative word count"},{"metadata":{"trusted":true,"_uuid":"b186290b496f3676387d3ddb2f7c1c68fbed9a11"},"cell_type":"code","source":"def count_word(x , pos_tag):\n    cnt = 0\n    if pos_tag:\n        for e in x.split():\n            if e in pos:\n                cnt = cnt + 1\n    else:\n        for e in x.split():\n            if e in neg:\n                cnt = cnt + 1\n    return cnt\n    \ndf['pos_cnt'] = df['Clean_text'].apply(lambda x : count_word(x , pos_tag = True))\ndf['neg_cnt'] = df['Clean_text'].apply(lambda x : count_word(x, pos_tag = False))\n\ndf['Ratio'] = df['pos_cnt'] / (df['neg_cnt']+0.0001)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad73ea19e4f127417a17d038d28f1ec1aea1bfc9"},"cell_type":"markdown","source":"Let's create Frequency distribution of Part of Speech Tags features:"},{"metadata":{"trusted":true,"_uuid":"4ac039a10091ff1c6e173f9531418720144f5bc1"},"cell_type":"code","source":"pos_family = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n\n# function to check and get the part of speech tag count of a words in a given sentence\ndef check_pos_tag(x, flag):\n    cnt = 0\n    wiki = textblob.TextBlob(x)\n    for tup in wiki.tags:\n        ppo = list(tup)[1]\n        if ppo in pos_family[flag]:\n            cnt += 1\n\n    return cnt\n\ndf['noun_count'] = df['Phrase'].apply(lambda x: check_pos_tag(x, 'noun'))\ndf['verb_count'] = df['Phrase'].apply(lambda x: check_pos_tag(x, 'verb'))\ndf['adj_count'] = df['Phrase'].apply(lambda x: check_pos_tag(x, 'adj'))\ndf['adv_count'] = df['Phrase'].apply(lambda x: check_pos_tag(x, 'adv'))\ndf['pron_count'] = df['Phrase'].apply(lambda x: check_pos_tag(x, 'pron'))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaeb99c9dd7bf344dc58b6492b70619f96a340d4"},"cell_type":"markdown","source":"Let's create the feature using textblob package. TextBlob is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more. "},{"metadata":{"trusted":true,"_uuid":"02d6f5aef6b448737536c2b2fc4f263f1ef73bae"},"cell_type":"code","source":"def getSentFeat(s , polarity):\n    sent = textblob.TextBlob(s).sentiment\n    if polarity:\n        return sent.polarity\n    else :\n        return sent.subjectivity\n    \ndf['polarity'] = df['Phrase'].apply(lambda x: getSentFeat(x , polarity=True))\ndf['subjectivity'] = df['Phrase'].apply(lambda x: getSentFeat(x , polarity=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227c11fc9ffdc4b9cc0539ae37c97ac157d6e243"},"cell_type":"code","source":"#separate train and test data\ntrain = df[:length]\ntest = df[length:]\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e683a616c8d45cb587bec3e3f6d73b1941dace0"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f10e781020c52a2f328ae4adf93fdf5bd31430aa"},"cell_type":"code","source":"\nplt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.violinplot(y='pos_cnt',x='Sentiment', data=train,split=True)\nplt.xlabel('Class Label', fontsize=12)\nplt.ylabel('# of Positive words ', fontsize=12)\nplt.title(\"Number of Positive word in each review\", fontsize=15)\n\nplt.subplot(122)\nsns.violinplot(y='neg_cnt',x='Sentiment', data=train,split=True)\nplt.xlabel('Class label', fontsize=12)\nplt.ylabel('# of negative words', fontsize=12)\nplt.title(\"Number of Negative words in each review\", fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69d6cc50c4e35c5a02c084f073820968f5bb9d21"},"cell_type":"markdown","source":"A Violin Plot is used to visualise the distribution of the data and its probability density. Let's check correlation with target Sentiment."},{"metadata":{"trusted":true,"_uuid":"0cec42baf23a1ef977e2a8cfb28c8a63f70499e4"},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(15,15))    #correlation between numerical values' maps\nsns.heatmap(train.corr() , annot = True, linewidths = .5, fmt= '.1f', ax=ax , vmin=-1, vmax=1)\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba1ec414262ecb492cd91143105026d7416b55c5"},"cell_type":"markdown","source":"**Baseline Model**"},{"metadata":{"trusted":true,"_uuid":"97e465719927fe903c6b9b7e24c55dab8c1b397e"},"cell_type":"code","source":"# Standardize numeric feature\n\nss = StandardScaler()\nnum_col = [ 'pos_cnt', 'neg_cnt' , 'Ratio','polarity','subjectivity' ,\n            'char_count' , 'word_count' , 'word_density' , 'punctuation_count','title_word_count' ,\n           'upper_case_word_count' ,'stopword_count' ,\n            'adv_count' ,'verb_count','adj_count', 'pron_count' , 'noun_count']\n\nX_num = ss.fit_transform(train[num_col].fillna(-1).clip(0.0001 , 0.99999))\n\ny = train['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17bbb4aa532ba9442fa2b74477b78d2956e9513e"},"cell_type":"code","source":"# vectorization of text data\n\ncount_vect = CountVectorizer(analyzer='word', ngram_range=(1,2))\n\nX_txt = count_vect.fit_transform(train['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a14d38a719a51c67283cd612785e3f6d8acd2ad"},"cell_type":"markdown","source":"Let's train a model on numeric feature and see which feature is work best for model training."},{"metadata":{"trusted":true,"_uuid":"ef3ed12fb9355b07760314d68ebdd2adfe994ac4"},"cell_type":"code","source":"\nX_train, X_val, Y_train, Y_val = train_test_split(X_num, y, test_size=0.10, random_state=1234)\n\nclf = LogisticRegression(C=3)\n\nclf.fit(X_train,Y_train)\nclf.score(X_val,Y_val)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0836366a5d33d6573d3e3100a0d9a3aa4d8bfde3"},"cell_type":"markdown","source":"Observe feature importance"},{"metadata":{"trusted":true,"_uuid":"3eca4dc0a096bd7c65f0c70f9839bae6389e4b3c"},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"Feature importance\",fontsize=20)\ngridspec.GridSpec(3,2)\nplt.subplots_adjust(hspace=0.4)\nplt.subplot2grid((3,2),(0,0))\nsns.barplot(num_col,clf.coef_[0],color=color[0])\nplt.title(\"class : Negative\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(0,1))\nsns.barplot(num_col,clf.coef_[1] , color=color[1])\nplt.title(\"class : Somewhat negative\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(1,0))\nsns.barplot(num_col,clf.coef_[2],color=color[2])\nplt.title(\"class : Neutral\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(1,1))\nsns.barplot(num_col,clf.coef_[3],color=color[3])\nplt.title(\"class : Somewhat positive\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,0))\nsns.barplot(num_col,clf.coef_[4],color=color[4])\nplt.title(\"class : Positive\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547ba50e6e23ebf71c181b955f0c91d77da8ff03"},"cell_type":"markdown","source":"Positive word count, negative word count, the ratio of positive word count to negative word count , polarity and subjectivity feature sound great for the model."},{"metadata":{"_uuid":"ba50fda425e574b68bf92f10fa179d6955defc5c"},"cell_type":"markdown","source":"Let's train model on both feature text and numeric feature."},{"metadata":{"trusted":true,"_uuid":"ed7b44b58cd69e2974cfc112041e07e3d442f513"},"cell_type":"code","source":"x = hstack((X_num,X_txt)).tocsr()\n\nX_train, X_val, Y_train, Y_val = train_test_split(x, y, test_size=0.10, random_state=1234)\n\nclf1 = LogisticRegression(C = 3)\n\nclf1.fit(X_train,Y_train)\nclf1.score(X_val,Y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aadf2be29eb79cf897329699e4666201a69300e2"},"cell_type":"markdown","source":"**Deep learning Baseline Model**"},{"metadata":{"trusted":true,"_uuid":"8dd94a227a87c3b1ee0ac957f5f88d649047b40e"},"cell_type":"code","source":"#pre-processing of data for keras model\n\ntrain_DL = train.drop(['PhraseId' , 'SentenceId' , 'Sentiment'] , axis =1)\ny = train['Sentiment']\n\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))\n\nX_train, X_val, Y_train, Y_val = train_test_split(train_DL, y_ohe, test_size=0.10, random_state=1234)\n\ntk = Tokenizer(lower = True, filters='', num_words= 15000)\ntk.fit_on_texts(train_DL['Phrase'])\n\ntrain_tokenized = tk.texts_to_sequences(X_train['Phrase'])\nvalid_tokenized = tk.texts_to_sequences(X_val['Phrase'])\n\nmax_len = 80\nX_train_txt = pad_sequences(train_tokenized, maxlen = max_len)\nX_valid_txt = pad_sequences(valid_tokenized, maxlen = max_len)\n\nX_num_train = ss.transform(X_train[num_col].fillna(-1).clip(0.0001 , 0.99999))\nX_num_valid = ss.transform(X_val[num_col].fillna(-1).clip(0.0001 , 0.99999))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ee23553e68b489fcd9dfda0d1558e4dd1922f08"},"cell_type":"code","source":"inp = Input(shape = (max_len,))\ninput_num = Input((len(num_col), ))\n\nx = Embedding(15000 , 100 ,mask_zero=True)(inp)\nx = LSTM(128, dropout=0.4, recurrent_dropout=0.4,return_sequences=True)(x)\nx = LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False)(x)\n\nx_num = Dense(64, activation=\"relu\")(input_num)   \nX_num = Dropout(0.2)(x_num)\nX_num = Dense(32, activation = \"relu\")(X_num)\n\nxx = concatenate([x_num, x])\nxx = BatchNormalization()(xx)\nxx = Dropout(0.1)(Dense(20, activation='relu') (xx))\n\noutp = Dense(5, activation = \"softmax\")(xx)\n\nmodel = Model(inputs = [inp,input_num], outputs = outp)\n\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit([X_train_txt,X_num_train], Y_train, validation_data=([X_valid_txt,X_num_valid], Y_val),\n         epochs=6, batch_size=128, verbose=1)\n\naccuracy = model.evaluate([X_valid_txt,X_num_valid], Y_val )[1]\naccuracy\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"777d33325ab72c6c48defe7a4a701bfabeba2731"},"cell_type":"markdown","source":"**Next steps:**\n\n* Add Embedding  features vector\n* hyperparameter Tuning\n* Stacking \n\nTo be continued. Please stay tuned!"},{"metadata":{"trusted":false,"_uuid":"0572050a4b9ace11a1e0c8bd80a47fd6ded7228c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}