{"cells":[{"metadata":{"trusted":true,"_uuid":"1ff174931af1e4c26e808ab542c8d0a299d08c09"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint('Importing data...')\ndata = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ntest = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\nprev = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\nburo = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\nburo_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\ncredit_card  = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\nPOS_CASH  = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\npayments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\nlgbm_submission = pd.read_csv('../input/home-credit-default-risk/sample_submission.csv')\n\n#Separate target variable\ny = data['TARGET']\ndel data['TARGET']\n\n#Feature engineering\n#data['loan_to_income'] = data.AMT_ANNUITY/data.AMT_INCOME_TOTAL\n#test['loan_to_income'] = test.AMT_ANNUITY/test.AMT_INCOME_TOTAL\n\n#One-hot encoding of categorical features in data and test sets\ncategorical_features = [col for col in data.columns if data[col].dtype == 'object']\n\none_hot_df = pd.concat([data,test])\none_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n\ndata = one_hot_df.iloc[:data.shape[0],:]\ntest = one_hot_df.iloc[data.shape[0]:,]\n\n#Pre-processing buro_balance\nprint('Pre-processing buro_balance...')\nburo_grouped_size = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size()\nburo_grouped_max = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].max()\nburo_grouped_min = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min()\n\nburo_counts = buro_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\nburo_counts_unstacked = buro_counts.unstack('STATUS')\nburo_counts_unstacked.columns = ['STATUS_0', 'STATUS_1','STATUS_2','STATUS_3','STATUS_4','STATUS_5','STATUS_C','STATUS_X',]\nburo_counts_unstacked['MONTHS_COUNT'] = buro_grouped_size\nburo_counts_unstacked['MONTHS_MIN'] = buro_grouped_min\nburo_counts_unstacked['MONTHS_MAX'] = buro_grouped_max\n\nburo = buro.join(buro_counts_unstacked, how='left', on='SK_ID_BUREAU')\n\n#Pre-processing previous_application\nprint('Pre-processing previous_application...')\n#One-hot encoding of categorical features in previous application data set\nprev_cat_features = [pcol for pcol in prev.columns if prev[pcol].dtype == 'object']\nprev = pd.get_dummies(prev, columns=prev_cat_features)\navg_prev = prev.groupby('SK_ID_CURR').mean()\ncnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\navg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\ndel avg_prev['SK_ID_PREV']\n\n#Pre-processing buro\nprint('Pre-processing buro...')\n#One-hot encoding of categorical features in buro data set\nburo_cat_features = [bcol for bcol in buro.columns if buro[bcol].dtype == 'object']\nburo = pd.get_dummies(buro, columns=buro_cat_features)\navg_buro = buro.groupby('SK_ID_CURR').mean()\navg_buro['buro_count'] = buro[['SK_ID_BUREAU', 'SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\ndel avg_buro['SK_ID_BUREAU']\n\n#Pre-processing POS_CASH\nprint('Pre-processing POS_CASH...')\nle = LabelEncoder()\nPOS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\nnunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\nnunique_status2 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\nPOS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\nPOS_CASH['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\nPOS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n\n#Pre-processing credit_card\nprint('Pre-processing credit_card...')\ncredit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\nnunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\nnunique_status2 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\ncredit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\ncredit_card['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\ncredit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n\n#Pre-processing payments\nprint('Pre-processing payments...')\navg_payments = payments.groupby('SK_ID_CURR').mean()\navg_payments2 = payments.groupby('SK_ID_CURR').max()\navg_payments3 = payments.groupby('SK_ID_CURR').min()\ndel avg_payments['SK_ID_PREV']\n\n#Join data bases\nprint('Joining databases...')\ndata = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\n\n#Remove features with many missing values\nprint('Removing features with more than 80% missing...')\n#test = test[test.columns[data.isnull().mean() < 0.85]]\n#data = data[data.columns[data.isnull().mean() < 0.85]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71cb6a2bc692e7dc940ce445740e5bf067cc4e16"},"cell_type":"code","source":"test = test[test.columns[data.isnull().mean() < 0.85]]\ndata = data[data.columns[data.isnull().mean() < 0.85]]\ndata.drop(columns=['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n            'FLAG_DOCUMENT_21'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39bac7ac55b2843f952111194508968bad6597f"},"cell_type":"code","source":"test.drop(columns=['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n            'FLAG_DOCUMENT_21'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fe25018b69513696fa11a9f75d548e95010ffa2"},"cell_type":"code","source":"# Create an anomalous flag column\ndata['DAYS_EMPLOYED_ANOM'] = data[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ndata['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# Create an anomalous flag column\ntest['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ntest['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f655026a52241ad6f63bd8260420edb648790bd9"},"cell_type":"code","source":"data['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\ntest['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4e0d122ea3e52ea785db93289594302a7602e12"},"cell_type":"code","source":"data['DAYS_CREDIT_ENDDATE'][data['DAYS_CREDIT_ENDDATE'] < -40000] = np.nan\ntest['DAYS_CREDIT_ENDDATE'][test['DAYS_CREDIT_ENDDATE'] < -40000] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3274de923956ad3fda74e983f885dacef6f9013"},"cell_type":"code","source":"data['DAYS_CREDIT_UPDATE'][data['DAYS_CREDIT_UPDATE'] < -40000] = np.nan\ndata['DAYS_ENDDATE_FACT'][data['DAYS_ENDDATE_FACT'] < -40000] = np.nan\n\ntest['DAYS_CREDIT_UPDATE'][test['DAYS_CREDIT_UPDATE'] < -40000] = np.nan\ntest['DAYS_ENDDATE_FACT'][test['DAYS_ENDDATE_FACT'] < -40000] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59428e163983f3ee93b56357c105b32c0a799d69","scrolled":true},"cell_type":"code","source":"data['AMT_DRAWINGS_ATM_CURRENT'][data['AMT_DRAWINGS_ATM_CURRENT'] < 0] = np.nan\ndata['AMT_DRAWINGS_CURRENT'][data['AMT_DRAWINGS_CURRENT'] < 0] = np.nan\n\ntest['AMT_DRAWINGS_ATM_CURRENT'][test['AMT_DRAWINGS_ATM_CURRENT'] < 0] = np.nan\ntest['AMT_DRAWINGS_CURRENT'][test['AMT_DRAWINGS_CURRENT'] < 0] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e757d42434a84f5ea6f0db4504d0dec3ed77b09"},"cell_type":"code","source":"# Align train and test\n\ntrain_labels = y\n\n# Align the training and testing data, keep only columns present in both dataframes\ndata, test = data.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\ndata['TARGET'] = y\n\nprint('Training Features shape: ', data.shape)\nprint('Testing Features shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f551a724dc078d9f896ad928d064dd034b62f6a"},"cell_type":"code","source":"test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\ntest[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7badfd2cfe1d10c58987c50eb9073d6fc7a7793"},"cell_type":"code","source":"#data['EXT_SOURCE_1_y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"736bdd44bf134095f687e2e3b91196d56cc71c53"},"cell_type":"code","source":"#Polynomial Features\n\n# Make a new dataframe for polynomial features\npoly_features = data[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61ee5da3f1dbcb10867ff7c585745f3808e570e5"},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b450d23bba070a887b0a4ed3bbb0d9f0e29499"},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"153207b73e2b7977487c84cc9260323e30f3561a"},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(20))\nprint(poly_corrs.tail(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b871b3d3c3779694cc5a91e2871f1843f50ea10"},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = data['SK_ID_CURR']\napp_train_poly = data.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = test['SK_ID_CURR']\napp_test_poly = test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caa0f8362cb926895b0fdfcbaf4e10142bdd59aa"},"cell_type":"code","source":"app_train_poly.head()\n\napp_train_poly=app_train_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})\n#app_train_poly=app_train_poly.drop('1',inplace=True)\n\napp_test_poly=app_test_poly.rename(columns={'EXT_SOURCE_1_x':'EXT_SOURCE_1','EXT_SOURCE_2_x':'EXT_SOURCE_2','EXT_SOURCE_3_x':'EXT_SOURCE_3','DAYS_BIRTH_x':'DAYS_BIRTH'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a310f5f00e12cf5caa074727275b1fa2a9bb8351"},"cell_type":"code","source":"app_test_poly.drop('1',axis=1,inplace=True)\napp_train_poly.drop('1',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb37d66185421105ef0d95ddd62de144ac273d6"},"cell_type":"code","source":"#app_train_poly['NAME_TYPE_SUITE_Spouse, partner_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3385d341e3db4d3cd7c8f1d822a1eaa9f962a4d9"},"cell_type":"code","source":"# check and remove constant columns\ncolsToRemove = []\nfor col in app_train_poly.columns:\n    if col != 'SK_ID_CURR' and col != 'TARGET':\n        if app_train_poly[col].std() == 0: \n            colsToRemove.append(col)\n        \n# remove constant columns in the training set\napp_train_poly.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\napp_test_poly.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6db4452ac6ec7ff57e558e9e84809b019d84e06"},"cell_type":"code","source":"#app_train_domain['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68e3df2199a9739da046815e73b6ab5e74014d41"},"cell_type":"code","source":"# create temp DF\n#data1 = pd.read_csv('../input/application_train.csv')\n#test1 = pd.read_csv('../input/application_test.csv')\n\n#app_train_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n#app_test_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)\n\n#app_train_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)\n#app_test_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e2e7d47f1cfb74e80fe0645cdf63f845ccfa24a"},"cell_type":"code","source":"#app_train_poly['AMT_CREDIT'] = data1['AMT_CREDIT']\n#app_test_poly['AMT_CREDIT'] = test1['AMT_CREDIT']\n#app_train_poly['AMT_GOODS_PRICE'] = data1['AMT_GOODS_PRICE']\n#app_test_poly['AMT_GOODS_PRICE'] = test1['AMT_GOODS_PRICE']\napp_train_poly=app_train_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})\napp_test_poly=app_test_poly.rename(columns={'AMT_CREDIT_x':'AMT_CREDIT','AMT_GOODS_PRICE_x':'AMT_GOODS_PRICE'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83924f6574e86c8afbc187448b21a96fd5b84957"},"cell_type":"code","source":"#app_train_poly['AMT_ANNUITY_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b77e86a1f80af0623a4bdc6572b1e7d64f868cb"},"cell_type":"code","source":"app_train_domain = app_train_poly.copy()\napp_test_domain = app_test_poly.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY_x'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY_x'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\napp_train_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_ANNUITY_x']\napp_train_domain['NEW_CREDIT_TO_GOODS_RATIO'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_GOODS_PRICE']\napp_train_domain['NEW_EXT_SOURCES_MEAN'] = app_train_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\napp_train_domain['NEW_EMPLOY_TO_BIRTH_RATIO'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4505cd992232d3158315214fedfd525c4c9977e9"},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY_x'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY_x'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']\n\napp_test_domain['NEW_CREDIT_TO_ANNUITY_RATIO'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_ANNUITY_x']\napp_test_domain['NEW_CREDIT_TO_GOODS_RATIO'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_GOODS_PRICE']\napp_test_domain['NEW_EXT_SOURCES_MEAN'] = app_test_domain[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\napp_test_domain['NEW_EMPLOY_TO_BIRTH_RATIO'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ea0b3d3ee19c8adfb307fd13f53e02f018be762"},"cell_type":"code","source":"app_train_domain['TARGET'] = poly_target\nprint('Training data with polynomial features shape: ', app_train_domain.shape)\nprint('Testing data with polynomial features shape:  ', app_test_domain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"336e903a7a4c4be26bc3cc4943cd11c243970c02"},"cell_type":"code","source":"#app_train_domain = app_train_domain[app_train_domain.columns[app_train_domain.isnull().mean() < 0.80]]\n#app_test_domain = app_test_domain[app_test_domain.columns[app_test_domain.isnull().mean() < 0.80]]\n#print('Training data with polynomial features shape: ', app_train_domain.shape)\n#print('Testing data with polynomial features shape:  ', app_test_domain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6d064e084efb6373abd92c4961fab4552b02d71"},"cell_type":"code","source":"#app_train_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)\n#app_test_domain = app_test_domain.drop('AMT_ANNUITY',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f293952bbffde2a8b4e6ad4524cf9de08eb8e6dd"},"cell_type":"code","source":"#app_train_domain['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4621c5cdf5ed89f3a3806f689e93c64bd1d4fae"},"cell_type":"code","source":"app_train_domain['TARGET'] = poly_target\nprint('Training data with polynomial features shape: ', app_train_domain.shape)\nprint('Testing data with polynomial features shape:  ', app_test_domain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b50311403d93e99586819404a60997fd815db0cf"},"cell_type":"code","source":"#app_train_domain = app_train_domain.reindex(\n #   np.random.permutation(app_train_domain.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70d8c21e78526c445819b3f78647ab3bcd0615de"},"cell_type":"code","source":"#app_train_domain=app_train_domain.drop('TARGET',axis=1)\n#app_train_domain = np.log1p(app_train_domain)\n#app_test_domain=np.log1p(app_test_domain)\n#print('Training data with polynomial features shape: ', app_train_domain.shape)\n#print('Testing data with polynomial features shape:  ', app_test_domain.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff0a677dfbe95fbb7aca60d9138de9e18d9b2fae"},"cell_type":"code","source":"app_train_domain = app_train_domain.drop('SK_ID_PREV_x',axis=1)\napp_test_domain = app_test_domain.drop('SK_ID_PREV_x',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65c1b1bfe293b7a3aebf4144b95990bf9fbbbf4d"},"cell_type":"code","source":"app_train_domain = app_train_domain.drop('SK_ID_PREV_y',axis=1)\napp_test_domain = app_test_domain.drop('SK_ID_PREV_y',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5b517cbd8769326456ce8936659088c2445859b"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds =5 ):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, nthread=4,objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.015, \n                                   reg_alpha = 0.041545473, reg_lambda = 0.1, \n                                   n_jobs = -1, random_state = 50,num_leaves=32,colsample_bytree=.9497036,subsample=.8715623,\n                                  max_depth=5,min_split_gain=.0222415,min_child_weight=39.3259775,max_bin=200,num_boost_round=3000,min_data_in_leaf=100,bagging_fraction=0.5,bagging_freq=10)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1c8b9a78defac7e48f7188ce2b0dc851c1b5788","scrolled":false},"cell_type":"code","source":"submission, fi, metrics = model(app_train_domain, app_test_domain)\nprint('Baseline metrics')\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf1f0015f8b8eb24747a59b13c5f980f450a8da0"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (20, 20))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:50]))), \n            df['importance_normalized'].head(50), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:50]))))\n    ax.set_yticklabels(df['feature'].head(50))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2d7f50ba5347ab337d851f3a5a5cfe765e098f","scrolled":false},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ac3c002d26190d507f2634a11feb98d0569d6f6"},"cell_type":"code","source":"submission.to_csv('second_sub.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7d65820dfa0782f770ff075410320167718de27"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}