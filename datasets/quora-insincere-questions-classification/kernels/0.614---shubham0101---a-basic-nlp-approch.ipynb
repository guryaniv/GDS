{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport nltk\nimport os\nimport gc\nfrom keras.preprocessing import sequence,text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense,Dropout,Embedding,LSTM, CuDNNGRU, Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,GlobalMaxPool1D,SpatialDropout1D,Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ntest = pd.read_csv(\"../input/test.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport string\npunctuations = string.punctuation\nstopword = stopwords.words(\"english\")\ndef clean(text):\n    \n    lower_text = text.lower()\n    \n    text = \"\".join(w for w in lower_text if w not in punctuations)\n    \n    words = text.split()\n    words = [w for w in words if w not in stopword]\n    res = \" \".join(words)\n    return res\nclean(\"this is a test!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c1dc7b390ddd78041e8b4554da2ca2690899d01"},"cell_type":"code","source":"train['cleaned'] = train['question_text'].apply(clean)\ntest['cleaned'] = test['question_text'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70d28571e82881a350f195bbc921109a4e059f3a"},"cell_type":"code","source":"target = to_categorical(train['target']) \n#target = train['label']\nx_train, x_val, y_train, y_val  = train_test_split(train['cleaned'], target, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b15d346fb88a94df9770cacce4b610f0e9049247"},"cell_type":"code","source":"words = ' '.join(x_train)\nwords = nltk.word_tokenize(words)\ndist = nltk.FreqDist(words)\nnum_unique_words = len(dist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbbc1f3571a84ab36c645cf55f3287e84502cb5b"},"cell_type":"code","source":"r_len = []\nfor w in x_train:\n    word=nltk.word_tokenize(w)\n    l=len(word)\n    r_len.append(l)\nmax_len = np.max(r_len)\nmax_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c53b7596578df4be7417c6f57319f3b2208099c"},"cell_type":"code","source":"max_features = num_unique_words\nmax_words = max_len\nbatch_size = 128\nembed_dim = 300\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726e25b9ae55a7ccd61b629dd37b3a4e8e6469fb"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_val = tokenizer.texts_to_sequences(x_val)\nx_test = tokenizer.texts_to_sequences(test['cleaned'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b3b1d2d9685187dd81213badc39aac41efa69bb"},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=max_words)\nx_val = sequence.pad_sequences(x_val, maxlen=max_words)\nx_test = sequence.pad_sequences(x_test, maxlen=max_words)\nprint(x_train.shape,x_val.shape,x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad093e1e5759a24cde2e221ee503c828cc2b8d65"},"cell_type":"code","source":"from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nros = RandomUnderSampler(random_state=777)\nX_ROS, y_ROS = ros.fit_sample(x_train, y_train)\n#x_train = X_ROS\n#y_train = y_ROS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afa42354dc8ea5a580a1e063dbfaf05d629fa4e"},"cell_type":"code","source":"EMBEDDING_FILE =open(\"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\", encoding=\"utf8\") \n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in EMBEDDING_FILE)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a542152b2030af58e359b93eab67ddeb9fa7bd74","scrolled":true},"cell_type":"code","source":"inp = Input(shape=(max_words,))\nx = Embedding(max_features, embed_dim, weights=[embedding_matrix])(inp)\nx = Bidirectional(GRU(64,return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(2, activation=\"softmax\")(x)\nmodel_2 = Model(inputs=inp, outputs=x)\nmodel_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46229806e073db1a159f66908ddcec615bf321df"},"cell_type":"code","source":"model_2.fit(x_train, y_train, batch_size=512, epochs=1, validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13dadced659ed1d9f9a558bd1e5315fd77f0119a"},"cell_type":"code","source":"pred=np.round(np.clip(model_2.predict(x_val), 0, 1))\nprint(f1_score(y_val, pred, average = None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d42dfb205c5e14f2d4824d627f068dbe63fa4c4"},"cell_type":"code","source":"pred_2=np.round(np.clip(model_2.predict(x_test), 0, 1)).astype(int)\npred_2 = pd.DataFrame(pred_2)\npred_2 = pred_2.idxmax(axis=1)\nsubmission = pd.DataFrame({'qid':test['qid'], 'prediction':pred_2})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6053b55e974682001f99d36ef56f1b615a477a5e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1ecfa8e24ef4bcb902c05eb1e99169d91182343"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}