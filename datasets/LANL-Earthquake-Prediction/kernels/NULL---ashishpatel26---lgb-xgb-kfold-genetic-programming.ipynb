{"cells":[{"metadata":{"_uuid":"1bc99ff52d68285950e6b52535b03351449f5eef"},"cell_type":"markdown","source":"---\n\n<h1><span style=\"color:red;\"><strong>LANL Earthquake Prediction</strong></span></h1>\n\n---\n\n<h3><span style=\"color:Blue;\"><strong><i>Can you predict upcoming laboratory earthquakes?</i></strong></span></h3>\n\n---\n\n> ## **_Objective_**\n> In this competition, you will address when the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data.\n> ## **_Solution thought by me_**\n> _In this kernel, I tried to apply lightgbm with initial parameter and kfold validation and also try to apply xgboost and other ensemble model and stacking is also applied._\n\n---\n> ## **_Outline_**\n* [**1.Load library**](#1.Load-library)\n* [**2.Read Data**](#2.Read-Data)\n* [**3.Feature Engineering**](#3.Feature-Engineering)\n* [**4.Data transformation**](#4.Data-transformation)\n* [**5.Test Data**](#5.Test-Data)\n* [**6.Model Training**](#6.Model-Training)\n    * [**1. Lightgbm**](#1.-Lightgbm)\n    * [**2. XGboost**](#2.-XGboost)\n* [**7.Stacking**](#7.Stacking)\n> * [**8.Final Prediction**](#8.Final-Prediction)\n---"},{"metadata":{"_uuid":"8d519229ccbe835bd270f584bfddd280271efa41"},"cell_type":"markdown","source":"## **1.Load library**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import BayesianRidge\nimport multiprocessing as mp\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac832eb60aa3be659710024160d992360896913d"},"cell_type":"markdown","source":"## **2.Read Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0107b91790b46fb42d0093a1aa8a22f61ccc905"},"cell_type":"code","source":"def add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a699dfd9209105d9f1357e85d776e60c9a476b2"},"cell_type":"markdown","source":"### **GeneticAlgorithm**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0430aa8e4b575795b1805d2b6a17530df10dd24c"},"cell_type":"code","source":"class GeneticAlgorithm:\n\n    def __init__(self,X,Y,Algorithm,Niter=100,keep_fraction=0.5,mutation_rate=\"auto\",nfeatures=\"auto\",test_size=0.3,njobs=1):\n\n        '''\n        A simple genetic algorithm designed to work with sklearn objects.\n        The object of the algorithm is to select the optimal combination of columns of input datafame X that produce the\n        best prediction of Y given the algorithm object provided.\n        To do this, the algorithm starts by randomly selecting different \n        combinations of columns. These become the first 'generation' of individuals\n        For each combination, the sklearn algorithm is trained in the associated columns and tested. The test score \n        becomes the 'fitness' of that combination/individual\n        Individuals are then selected for 'breeding'. Those with the highest fitness scores have the highest chances\n        of doing so. When two individuals 'breed'. Two children are produced by a simple crossover of their column IDs\n        The child generation is then combined with the 'best' of the parent generation via the 'keep_fraction' argument\n        The algorithm proceeds by testing the fitness of each generation. The result is an 'optimal' combination of \n        features that correspond to the best fitness score. \n        This should work with any supervised sklearn object\n        Inputs:\n        X - dataframe containing the predictors only\n        Y - datafame or series containing the target only\n        Algorithm - sklearn classifier or regression object, such as an RandomForestClassifier\n        Niter - number of iterations of the genetic algorithm\n        keep_fraction - the proportion of fittest parents to keep in each new generation\n        mutation_rate - the probability of mutation in each child\n        nfeatures - the maximum number of features that an output model can have\n        test_size - test_size in the train_test_split that occurs in model fitness evaluation \n        Main outputs:\n        self.fitness_evolution - list of the best fitness value from each generation\n        self.best_individual_evolution - list of arrays of the best individuals in each generation\n        self.feature_selection - dataframe corresponding to the selected features \n        self.best_fitness - fitness score correspondng to self.feature_selection\n        self.best_individual - individual corresponding to self.feature_selection\n        '''\n\n\n        self.dataset = X\n        self.response = Y\n        self.algorithm = Algorithm #needs to be a sklearn object\n        self.Niter = Niter #number of iterations \n        self.parent_keep = keep_fraction\n        self.test_size = test_size\n        self.nprocs = int(njobs)\n\n        if self.nprocs > mp.cpu_count():\n\n            raise ValueError(\"Entered number of processes > CPU count!\")\n\n\n        self.feature_columns = self.dataset.columns\n\n        if nfeatures == 'auto':\n            self.nfeatures = len(self.feature_columns)\n        else:\n            self.nfeatures = nfeatures\n\n        self.P = 2*int(np.ceil(self.nfeatures*1.5/2)) #number of individuals in a given generation\n\n        if mutation_rate == 'auto':\n\n            self.mutation_rate = 1.0/(self.P*np.sqrt(self.nfeatures))\n        else:\n            self.mutation_rate = mutation_rate\n\n        self.fitness_evolution = []\n        self.best_individual_evolution = []\n\n        #These three things are typically the most desired output\n        self.feature_selection = None\n        self.best_fitness = None\n        self.best_individual = None\n\n\n    def fitness(self,generation):\n\n        '''\n        Assess the fitness of a generation of individuals\n        This is the part that takes a long time because it must train a supervised ML algorithm on all individuals in a generation\n        '''\n\n        def determine_fitness(subgeneration,output,pos):\n\n            fitness_array = np.zeros(np.shape(subgeneration)[0])\n\n            for i in range(np.shape(subgeneration)[0]):\n            \n                individual = subgeneration[i,:]\n                \n                #Subset the columns based on this individual\n                X_individual = self.dataset[[self.dataset.columns[j] for j in range(len(individual)) if individual[j] == 1]]\n                \n                #Split into train-test datasets\n                X_train, X_test, y_train, y_test = train_test_split(X_individual,self.response,test_size=self.test_size)\n                \n                #Fit the classifier\n                self.algorithm.fit(X_train,y_train)\n                \n                #Report fitness score (score in the testing dataset)\n                fitness = self.algorithm.score(X_test,y_test)\n                \n                #append to fitness array\n                fitness_array[i] = fitness\n\n            output.put((pos,fitness_array))\n\n\n        process_output = mp.Queue()\n        subarrays = np.array_split(generation,self.nprocs)\n        processes = [mp.Process(target=determine_fitness,args=(subarrays[i],process_output,i)) for i in range(self.nprocs)]\n\n        for p in processes:\n            p.start()\n\n        for p in processes:\n            p.join()\n\n        results = [process_output.get() for p in processes]\n        results.sort()\n        rlist = []\n        for element in results:\n            r = element[1]\n            for j in range(len(r)):\n                rlist.append(r[j])\n        rlist = np.array(rlist)\n   \n        return rlist\n\n    def make_new_generation(self,old_generation,old_fitness_array):\n        \n        '''\n        Make a new generation of individuals\n        '''\n        \n        generation_size = len(old_fitness_array)\n            \n        #Vector describing the probability of reporduction of each individual in a generation\n        prob_weights = 2*np.argsort(old_fitness_array/(generation_size*(generation_size+1)))[::-1]\n        \n        prob_reproduction = prob_weights/np.sum(prob_weights)\n        \n        #Make vector of indices to choose\n        a = np.arange(generation_size)\n        \n        children = np.zeros([2*generation_size,np.shape(old_generation)[1]])\n        \n        for i in range(generation_size):\n            parent_index_pair = np.random.choice(a,size=2,replace=False,p=prob_reproduction)\n            \n            parent1 = old_generation[parent_index_pair[0]]\n            parent2 = old_generation[parent_index_pair[1]]\n            \n            #Do cross over and apply mutation to generate two children for each parent pair\n            child1 = parent1.copy()\n            child2 = parent2.copy()\n            \n            #Generate locations of genetic information to swap\n            pos = np.random.choice(len(parent1),size=int(len(parent1)/2),replace=False)\n            child1[pos] = parent2[pos]\n            child2[pos] = parent1[pos]\n            \n            #Generate mutation vector\n            mutate1 = np.random.binomial(1,self.mutation_rate,len(parent1))\n            mutate2 = np.random.binomial(1,self.mutation_rate,len(parent1))\n            \n            #Generate children and fill child array\n            child1 = (child1+mutate1 >= 1).astype(int)\n            child2 = (child2+mutate2 >= 1).astype(int)\n            \n            children[i,:] = child1\n            children[-(i+1),:] = child2\n            \n        #shuffle and return only the same number of children as there were parents \n        np.random.shuffle(children)\n        \n        new_generation = children[0:generation_size,:]\n        \n        #replace some fraction of the children with the fittest parents, if desired\n        \n        nparents_to_keep = int(self.parent_keep*generation_size)\n        \n        if nparents_to_keep > 0:\n            parents_keep = np.argsort(old_fitness_array)[::-1][:nparents_to_keep]\n\n            for i in range(len(parents_keep)):\n                new_generation[i,:] = old_generation[parents_keep[i],:]\n\n        np.random.shuffle(new_generation)\n        \n        \n        return new_generation \n\n    def fit(self):\n\n        '''\n        Run the genetic algorithm to obtain the optimal features for this problems\n        This part takes a long time and could be parallelized\n        '''\n\n        #Make the first generation \n        old_generation = np.zeros([self.P,self.nfeatures])\n        for i in range(self.P):\n            old_generation[i,:] = np.random.binomial(1,0.5,self.nfeatures)\n\n        old_fitness_array = self.fitness(old_generation)\n\n        self.best_fitness = np.max(old_fitness_array)\n        self.best_individual = old_generation[np.argmax(old_fitness_array),:]\n\n        self.fitness_evolution.append(self.best_fitness)\n        self.best_individual_evolution.append(self.best_individual)\n\n        for n in range(1,self.Niter):\n\n            print(\"GeneticAlgorithm: Testing generation %i\" %n)\n\n            #Make new generation\n            new_generation = self.make_new_generation(old_generation,old_fitness_array)\n            #Get fitness of new generation\n            new_fitness_array = self.fitness(new_generation)\n\n            #Locate and extract the best individual and its score\n            self.best_fitness = np.max(new_fitness_array)\n            self.best_individual = new_generation[np.argmax(new_fitness_array),:]\n            self.fitness_evolution.append(self.best_fitness)\n            self.best_individual_evolution.append(self.best_individual)\n\n            old_fitness_array = new_fitness_array\n            old_generation = new_generation\n\n        #Get the features associated with the 'winning' individual\n\n        self.feature_selection = self.dataset[[self.dataset.columns[j] for j in range(len(self.best_individual)) if self.best_individual[j] == 1]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e37e5f6d74968b3d96ce7ade89fac4cf0df587b"},"cell_type":"markdown","source":"## **3.Feature Engineering**"},{"metadata":{"trusted":true,"_uuid":"e04936d09afde6091a302337f4947f8116c568a5"},"cell_type":"code","source":"rows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min','q95','q99', 'q05','q01',\n                                'abs_max', 'abs_mean', 'abs_std', 'trend', 'abs_trend'])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.mean()\n    X_train.loc[segment, 'std'] = x.std()\n    X_train.loc[segment, 'max'] = x.max()\n    X_train.loc[segment, 'min'] = x.min()\n    X_train.loc[segment, 'q95'] = np.quantile(x,0.95)\n    X_train.loc[segment, 'q99'] = np.quantile(x,0.99)\n    X_train.loc[segment, 'q05'] = np.quantile(x,0.05)\n    X_train.loc[segment, 'q01'] = np.quantile(x,0.01)\n    \n    X_train.loc[segment, 'abs_max'] = np.abs(x).max()\n    X_train.loc[segment, 'abs_mean'] = np.abs(x).mean()\n    X_train.loc[segment, 'abs_std'] = np.abs(x).std()\n    X_train.loc[segment, 'trend'] = add_trend_feature(x)\n    X_train.loc[segment, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n    \nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e7b32d237b4fd402e72ee691d3f36e8b06ed4cd"},"cell_type":"markdown","source":"## **4.Data transformation**"},{"metadata":{"trusted":true,"_uuid":"f3bb36700ff5b557f559029a42cff0e9f9cba557"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26bf2e5c080e51900681a2c8bfdc4a78325ac66a"},"cell_type":"markdown","source":"## **5.Test Data**"},{"metadata":{"trusted":true,"_uuid":"523e73c6f68c40e5694dbb796cb0da5ccf117ab4"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\nfor seg_id in tqdm(X_test.index):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n    X_test.loc[seg_id, 'q95'] = np.quantile(x,0.95)\n    X_test.loc[seg_id, 'q99'] = np.quantile(x,0.99)\n    X_test.loc[seg_id, 'q05'] = np.quantile(x,0.05)\n    X_test.loc[seg_id, 'q01'] = np.quantile(x,0.01)\n    \n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_mean'] = np.abs(x).mean()\n    X_test.loc[seg_id, 'abs_std'] = np.abs(x).std()\n    X_test.loc[seg_id, 'trend'] = add_trend_feature(x)\n    X_test.loc[seg_id, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7b3abef5702bb48e852d6c5231c5cfddff7eb4"},"cell_type":"code","source":"X_train_scaled = pd.DataFrame(X_train_scaled,columns=X_train.columns)\nX_train_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09325f7b837e9b34568fa3e68c9204f9456cd2e5"},"cell_type":"code","source":"X_test_scaled = pd.DataFrame(X_test_scaled,columns=X_test.columns)\nX_test_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4e4d8d1f06a9d7ad8f3b2c2a9ae1d9a7f032677"},"cell_type":"markdown","source":"## **6.Model Training**\n---\n## **1. Lightgbm**"},{"metadata":{"trusted":true,"_uuid":"0b091a4082ec1061069c19a245d08c9378851f1d"},"cell_type":"code","source":"import time\nimport lightgbm as lgb\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 32, \n#          'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"min_child_samples\": 20,\n#          \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"nthread\": 4,\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a380c449b0a45db979085c3fa0fbb4f69ab4769e"},"cell_type":"code","source":"features = X_train_scaled.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"812f605a50bd59c289cc94441cbfcec4183afd41"},"cell_type":"code","source":"# dataset = lgb.Dataset(X_train_scaled.values, y_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37a7044bf984e2e27a76463dc8f61afdd7662125"},"cell_type":"code","source":"lgbm = lgb.LGBMRegressor(param)\nGA = GeneticAlgorithm(X_train_scaled,y_train,lgbm,njobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4596b42b9647838372a1cff4f0058d5972143db"},"cell_type":"code","source":"GA.fit()\nprint(GA.best_fitness)\nprint(GA.best_individual)\nX_subset = GA.feature_selection\nprint(X_subset.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5ed1d1c1fedafe49976eee6f18dba91cf0ffc969"},"cell_type":"code","source":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(X_train_scaled))\npredictions = np.zeros(len(X_test_scaled))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled.values, y_train.values)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data = lgb.Dataset(X_train_scaled.iloc[trn_idx][features], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train_scaled.iloc[val_idx][features], label=y_train.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n    oof[val_idx] = clf.predict(X_train_scaled.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(X_test_scaled[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, y_train)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8f90b78f9ddc400cb336e52c72a6f4bc15e8c99"},"cell_type":"code","source":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,16))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84916e0bcf15a7bbc8ce622bf3def97ac62672ab"},"cell_type":"markdown","source":"## **2. XGboost**"},{"metadata":{"trusted":true,"_uuid":"85f8070cb4a9f7599bfe68679923933e8469c65c"},"cell_type":"code","source":"%%time\nimport xgboost as xgb\n\nxgb_params = {'eta': 0.001, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, 'alpha':0.1,\n          'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'random_state':folds}\n\n\nfolds = KFold(n_splits=5, random_state=4520)\noof_xgb = np.zeros(len(X_train_scaled))\npredictions_xgb = np.zeros(len(X_test_scaled))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled.values, y_train.values)):\n    print(\"fold n°{}\".format(fold_ + 1))\n    trn_data = xgb.DMatrix(data=X_train_scaled.iloc[trn_idx][features], label=y_train.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=X_train_scaled.iloc[val_idx][features], label=y_train.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"-\" * 10 + \"Xgboost \" + str(fold_) + \"-\" * 10)\n    num_round = 11000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(X_train_scaled.iloc[val_idx][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(X_test_scaled[features]), ntree_limit=xgb_model.best_ntree_limit+50) / folds.n_splits\n    \nnp.save('oof_xgb', oof_xgb)\nnp.save('predictions_xgb', predictions_xgb)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_xgb, y_train)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fa69f7acab27e2b6190e47fb2ab9880ed5bf8c85"},"cell_type":"code","source":"# %%time\n# from catboost import CatBoostRegressor\n# folds = KFold(n_splits=5, random_state=4520)\n# oof_cat = np.zeros(len(X_train_scaled))\n# predictions_cat = np.zeros(len(X_test_scaled))\n\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_scaled.values, y_train.values)):\n#     print(\"fold n°{}\".format(fold_ + 1))\n#     trn_data, trn_y = X_train_scaled.iloc[trn_idx][features], y_train.iloc[trn_idx]\n#     val_data, val_y = X_train_scaled.iloc[val_idx][features], y_train.iloc[val_idx]\n#     print(\"-\" * 10 + \"Catboost \" + str(fold_) + \"-\" * 10)\n#     cb_model = CatBoostRegressor(iterations=8000, learning_rate=0.01, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), use_best_model=True, verbose=True,)\n    \n#     oof_cat[val_idx] = cb_model.predict(val_data)\n#     predictions_cat += cb_model.predict(X_test_scaled[features]) / folds.n_splits\n    \n# np.save('oof_cat', oof_cat)\n# np.save('predictions_cat', predictions_cat)\n# np.sqrt(mean_squared_error(y_train.values, oof_cat))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2765dbed40f1f34bf9507b8567473bc793cad263"},"cell_type":"markdown","source":"## **7.Stacking**"},{"metadata":{"trusted":true,"_uuid":"4d590837dfedc76f5b2448b19009147f7f5c5793"},"cell_type":"code","source":"train_stack = np.vstack([oof, oof_xgb]).transpose()\ntest_stack = np.vstack([predictions,predictions_xgb]).transpose()\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof_stack = np.zeros(train_stack.shape[0])\npredictions_stack = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y_train)):\n    print(\"fold n°{}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], y_train.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], y_train.iloc[val_idx].values\n\n    print(\"-\" * 10 + \"Ridge Regression\" + str(fold_) + \"-\" * 10)\n#     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n    clf = BayesianRidge()\n    clf.fit(trn_data, trn_y)\n    \n    oof_stack[val_idx] = clf.predict(val_data)\n    predictions_stack += clf.predict(test_stack) / 5\n\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, y_train)**0.5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a5b1849731681cb075e90537daebdcb775e1ed4"},"cell_type":"markdown","source":"## **8.Final Prediction**"},{"metadata":{"trusted":true,"_uuid":"dd46c9e2c3ec68f45900cc49f95aee82459137a1"},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv')\nsample_submission['time_to_failure'] = predictions_stack\nsample_submission.to_csv('Bayesian_Ridge_Stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2f170d0b38206487afc2b8337dd2f1038924483"},"cell_type":"code","source":"sample_submission.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}