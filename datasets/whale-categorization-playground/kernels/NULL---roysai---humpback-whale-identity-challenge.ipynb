{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport glob\nimport sys\nimport random\nimport tqdm as tqdm\n\nimport matplotlib.pyplot as plt\n\nfrom skimage.io import imread, imshow\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\n\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Input, BatchNormalization, merge, GlobalMaxPooling2D, Lambda\nfrom keras.layers.core import Flatten, Dense, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.applications.vgg19 import VGG19\n\nimport tensorflow as tf\n\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n\nfrom scipy.stats import norm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e9411e7fa24e64160e7e8e208575c2a32c924cd","collapsed":true},"cell_type":"code","source":"#Going through the images one by one, we found some of the images that are not good for training, due to bad quality, photo taken too far away, bad image angle or simply too noisy. these images are listed below.\nbad_image=['0ae69f1e.jpg', '0b1e39ff.jpg', '0d614410.jpg', '0fb4c4dd.jpg', '01c11eaf.jpg', '1ae163da.jpg', '2a2ecd4b.jpg', '2bc459eb.jpg', '2c824757.jpg', '2f1b1a58.jpg', '3e793658.jpg', '3f919786.jpg', '4d818204.jpg',\n           '05d402e7.jpg', 'obde3564.jpg', '5efe6139.jpg', '06ffe77b.jpg', '6b9f5632.jpg', '6bc32ac5.jpg', '7a399627.jpg', '7aadcef5.jpg', '7cc11b57.jpg', '7d008e04.jpg', '7eacaef2.jpg', '7f048f21.jpg', '7f7702dc.jpg', \n           '8aafd575.jpg', '8b615df8.jpg', '8bd96828.jpg', '8eb500b3.jpg', '9e6ff81f.jpg', '19b2000c.jpg', '29ab7864.jpg', '35e0706e.jpg', '53ccd15c.jpg', '54aeef4a.jpg', '55cb38a4.jpg', '56fafc52.jpg', '69d946a0.jpg', \n           '68ffd7bf.jpg', '73cfb77f.jpg', '342ea7cf.jpg', '383eb5a3.jpg', '734d181a.jpg', '761b8f76.jpg', '806cf583.jpg', '851a3114.jpg', '932d8f1e.jpg', '993f0479.jpg', '1323a889.jpg', '3401bafe.jpg', '14944e45.jpg', \n           '20594d9d.jpg', '28405ee2.jpg', '34333c52.jpg', '43189ff2.jpg', '91885aec.jpg', '300806a1.jpg', '666282d2.jpg', '1990152d.jpg', '42025982.jpg', '95226283.jpg', 'a3e9070d.jpg', 'a3844c28.jpg', 'a2095252.jpg', \n           'ade8176b.jpg', 'ae2f76dc.jpg', 'b1cfda8a.jpg', 'b006cec6.jpg', 'b6e4f09a.jpg', 'b985ae1e.jpg', 'b9315c19.jpg', 'bc5bf694.jpg', 'c8d44ff3.jpg', 'c97c3ae6.jpg', 'c482d51b.jpg', 'c912ac66.jpg', 'c6854c76.jpg', \n           'cc8eb5e2.jpg', 'ced4a25c.jpg', 'cf756424.jpg', 'd14f0126.jpg', 'd79047b6.jpg', 'd781262d.jpg', 'd1502267.jpg', 'dc0e29fe.jpg', 'dc79e75f.jpg', 'de5e2ea7.jpg', 'de8d631f.jpg', 'e0b00a14.jpg', 'e6ce415f.jpg', \n           'e9bd2e9c.jpg', 'e30d9525.jpg', 'e53d2b96.jpg', 'eafadfb3.jpg', 'ee897d4a.jpg', 'f00f98fb.jpg', 'f1b24b92.jpg', 'f4063698.jpg', 'fb2271f1.jpg']\n\nprint (f'Total of {len(bad_image)} images are not suitable for training')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#Define train and test path\nTRAIN_PATH=\"../input/whale-categorization-playground/train/\"\nTEST_PATH=\"../input/whale-categorization-playground/test/\"\n\n#read train file summary, minus bad files\ndf=pd.read_csv('../input/whale-categorization-playground/train.csv')\ndf=df[~df['Image'].isin(bad_image)]\n\n#set 20% of image for validation purpose\nnp.random.seed(100)\ndf['train']=np.random.randint(0,10, len(df))>1\n\nprint (f'Total train files = {len(df)}')\n\n#group the train files by ID\nID_freq=df['Id'].value_counts()\nprint (f'Total unique ID= {len(ID_freq)}')\n\nID_count_by_freq=ID_freq.value_counts()\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(ID_count_by_freq)+1), list(ID_count_by_freq))\nplt.xticks(np.arange(1, len(ID_count_by_freq)+1), list(ID_freq.unique()[::-1]))\nplt.xlabel('number of images')\nplt.ylabel('count')\nplt.show()\n\nprint (ID_freq.head())\n#find out how many percent of train file consist of new whale\npercent_new_whale= ID_freq.loc['new_whale']/len(df)\nprint (f'\\n{percent_new_whale*100}% of images are new whale')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc09ab0c186eb507bdcd9baff2e2a2cfb2bde5f"},"cell_type":"markdown","source":"Majority of ID have only one, two or three images.\nabout 10% of images (after removing bad images) are new whale."},{"metadata":{"trusted":true,"_uuid":"ad7787f9da2146e7b967db9b7cd653cd5eadfade","collapsed":true},"cell_type":"code","source":"\"Uncomment to train on train files only\"\n#df_train=df[df['train']==True]\n\"Uncomment to train on all files\"\ndf_train=df.copy()\n\nID_freq=df_train['Id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ead5bd5c8c20ee9e29d56120cced251bdd80214d"},"cell_type":"markdown","source":"\nI also created a simple neural network that localize the whale fluke from the image. This model return upper left and bottom right corner of the bounding box for the fluke. We use the outcome to crop the image before feeding them for training.\n\nCredit to @lisa needs braces for providing training set. (https://www.kaggle.com/c/whale-categorization-playground/discussion/57108)\n\n"},{"metadata":{"trusted":true,"_uuid":"e14b10cec9ea29eeaf8c3221db55f18efb597352","collapsed":true},"cell_type":"code","source":"train_crop_table=pd.read_csv('../input/crop-coordinate/train_crop_coordinate.csv', index_col=0)\ntest_crop_table=pd.read_csv('../input/crop-coordinate/test_crop_coordinate.csv', index_col=0)\n\n\"\"\"\nIn this table:\n\"image\"                     -- image name\n\"ori height\"                -- original height of the image\n\"ori width                  -- original width of the image\n\"ystart, xstart\"            -- the upper left corner of bounding box of the image in 224x224 size\n\"yend, xend\"                -- the bottom right corner of bounding box of the image in 224x224 size\n\"new ystart, new xstart\"    -- the upper left corner of bounding box of the image in original size\n\"new yend, new xend\"        -- the bottom right corner of bounding box of the image in original size\n\"\"\"\n\ntrain_crop_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"850e675779ec3a2e8d4d3eb3c77cc610ac9a81b8","collapsed":true},"cell_type":"code","source":"IMG_SHAPE=(224, 224, 3)\nOUTPUT_DIMENSION=800\n#define base model for croping and triplet loss\ndef get_base_model():\n    \"\"\" load vgg19 model with no top, add final output as dense layer with dimension OUTPUT_DIMENSION\n    \"\"\"\n    \n    base_model=VGG19(include_top=False, weights=None)\n    #base_model.load_weights('../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', by_name=True)\n    base_model.load_weights('../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', by_name=True)\n    \n    x=base_model.output\n    x=GlobalMaxPooling2D()(x)\n    x=Dropout(0.3)(x)\n    dense_1=Dense(OUTPUT_DIMENSION)(x)\n    normalized=Lambda(lambda x: K.l2_normalize(x, axis=1))(dense_1)\n    base_model=Model(base_model.input, normalized)\n    return base_model\n\ndef get_triplet_model(lr=0.001):\n    input1=Input(IMG_SHAPE)\n    input2=Input(IMG_SHAPE)\n    input3=Input(IMG_SHAPE)\n    \n    output1=base_model(input1)\n    output2=base_model(input2)\n    output3=base_model(input3)\n    \n    loss=merge([output1, output2, output3], mode=triplet_loss, output_shape=(1,))\n    \n    model=Model(inputs=[input1, input2, input3], outputs=loss)\n    model.compile(loss=identity_loss, optimizer=Adam(lr))\n    return model\n\ndef triplet_loss(X, alpha=0.2):\n    pos, neg, anchor=X\n    pos_dist=tf.reduce_sum(tf.square(tf.subtract(anchor, pos)), axis=-1)\n    neg_dist=tf.reduce_sum(tf.square(tf.subtract(anchor, neg)), axis=-1)\n    basic_loss=tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n    \n    loss=tf.reduce_mean(tf.maximum(basic_loss, 0))\n    return loss\n\ndef identity_loss(y_true, y_pred):\n    return K.mean(y_pred-0*y_true)\n\ndef check_id(image):\n    ID=df['Id'][df['Image']==image].iloc[0]\n    return ID\n\ndef crop_image(images, train=True):\n    \"\"\"take in images, crop according to train/test crop table, resize to IMG_SHAPE\n    \n    Arguments:\n    images-- list consist of image name to be cropped\n    train-- bool, True if the images are from train, False if test\n    \n    return:\n    output_array-- array with axis-0 as number of images, axis-1 to 3 as cropped array of the input images\n    \"\"\"\n    output_array=np.zeros((len(images), IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2]))\n    for n, image in enumerate (images):\n        if train:\n            path=TRAIN_PATH+image\n            table=train_crop_table.copy()\n        else:\n            path=TEST_PATH+image\n            table=test_crop_table.copy()\n            \n        ystart=table['new ystart'][table['image']==image].values\n        xstart=table['new xstart'][table['image']==image].values\n        yend=table['new yend'][table['image']==image].values\n        xend=table['new xend'][table['image']==image].values\n        \n        img_array=imread(path)\n        height, width=img_array.shape[0], img_array.shape[1]\n        \n        #provide 10 pixel of margin, so that we are sure that entire flute will be cropped.\n        ystart=int(max(ystart-10, 0))\n        xstart=int(max(xstart-10, 0))\n        yend=int(min(yend+10, height))\n        xend=int(min(xend+10, width))\n        \n        #if image is greyscale, convert it to rgb with all channel = greyscale channel\n        if img_array.ndim==3:\n            img_array=img_array[ystart:yend, xstart:xend, :]\n            img_array=resize(img_array, IMG_SHAPE)\n        else:\n            img_array=img_array[ystart:yend, xstart:xend]\n            img_array=resize(img_array, (IMG_SHAPE[0], IMG_SHAPE[1]))\n            img_array=np.expand_dims(img_array, axis=-1)\n            img_array[:,:,:]=img_array\n        output_array[n]=img_array\n    return output_array\n\ndef generate_ID_map(unique_ID):\n    \"\"\" \n    generate a dictionary to map ID in unique_ID to its corresponding images, distances and indices among the images of the same ID\n    \"\"\"\n    global ID_map\n    \n    for ID in unique_ID:\n        img_lst=list(df_train['Image'][df_train['Id']==ID])\n        img_array=crop_image(img_lst)\n        encoding=base_model.predict(img_array)\n        nbrs=NearestNeighbors(n_neighbors=len(img_lst), algorithm= 'ball_tree').fit(encoding)\n        dist, indices=nbrs.kneighbors(encoding)\n        centroid=find_centroid(encoding)\n        \n        ID_map[ID]={'images':img_lst,\n                    'distances': dist, \n                    'indices': indices,\n                    'centroid': centroid}\n    \n    return ID_map\n\ndef find_centroid(encoding):\n    return np.mean(encoding, axis=0)\n\ndef generate_global(img_array):\n    global global_dist, global_idx\n    img_encoding=base_model.predict(img_array)\n    nbrs=NearestNeighbors(n_neighbors=anchor_encoding.shape[0], algorithm= 'ball_tree').fit(anchor_encoding)\n    global_dist, global_idx=nbrs.kneighbors(anchor_encoding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1955fd0816432b3b857397116f5b7fcdf9b76332","collapsed":true},"cell_type":"code","source":"base_model=get_base_model()\n\ninput1=Input(IMG_SHAPE)\ninput2=Input(IMG_SHAPE)\ninput3=Input(IMG_SHAPE)\n\noutput1=base_model(input1)\noutput2=base_model(input2)\noutput3=base_model(input3)\n\nloss=merge([output1, output2, output3], mode=triplet_loss, output_shape=(1,))\n\ntriplet_model=Model(inputs=[input1, input2, input3], outputs=loss)\ntriplet_model.compile(loss=identity_loss, optimizer=Adam(10**-5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9ae2ea52d1f74c15469a2f737009d4fadf2c84","scrolled":true,"collapsed":true},"cell_type":"code","source":"#Select all ID which has at least 2 images, at most 100 (to eliminate \"new_whale\") for training\nID_SL=list(ID_freq[(ID_freq>=2) & (ID_freq<100)].index)\nprint ('Generating ID_map...')\nID_map={}\ngenerate_ID_map(ID_SL)\nprint (\"ID_map generated\")\nprint (f'\\ntotal number of IDs to train= {len(ID_SL)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e45f02b7b4624d5790d6eeb14461aaf4203ae48","collapsed":true},"cell_type":"code","source":"\"Skip this segment if not loading pretrained weight\"\nbase_model.load_weights('../input/128-min2-max4-crop-e2/min2_max4_crop_e2_128.h5', by_name=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"799c11919c3728657d2cbb6f2d213908363edc0c","collapsed":true},"cell_type":"code","source":"\"Skip this segment if not training.\"\n\nclass data_generator:\n    def __init__(self, ID_map, ID_lst, batch_size, num_img):\n        \n        self.ID_lst=ID_lst\n        self.batch_size=batch_size\n        self.num_img=num_img\n        \n    \n    def find_neighbors(self):\n        self.centroid_array=np.zeros((len(ID_map.keys()), OUTPUT_DIMENSION))\n        \n        for n, ID in enumerate(ID_map.keys()):\n            self.centroid_array[n]=ID_map[ID]['centroid']\n                    \n        centroid_neighbors=NearestNeighbors(n_neighbors=len(self.ID_lst), algorithm='ball_tree').fit(self.centroid_array)\n        _, self.nearest_neighbors=centroid_neighbors.kneighbors(self.centroid_array)\n        \n        return self.centroid_array\n      \n    def generate_positive(self, img_shortlist):\n        self.pos_sample_lst=[]\n        for n, image in enumerate(img_shortlist):\n            ID=check_id(image)\n            img_lst=ID_map[ID]['images']\n            idx=img_lst.index(image)\n            furthest_pos_idx=ID_map[ID]['indices'][idx, len(img_lst)-1]\n            furthest_pos_img=ID_map[ID]['images'][furthest_pos_idx]\n            self.pos_sample_lst.append(furthest_pos_img)\n        return self.pos_sample_lst\n    \n    def generate_negative(self, img_shortlist, diff=1):\n        self.neg_sample_lst=[]\n        for n, image in enumerate(img_shortlist):\n            ID=check_id(image)\n            idx=self.ID_lst.index(ID)\n            #print (self.nearest_neighbors.shape)\n            nearest_centroid=self.ID_lst[self.nearest_neighbors[idx, diff]]#diff= 1 to number of ID group, one being the hardest\n            nearest_nbrs=random.choice(ID_map[nearest_centroid]['images'])\n            self.neg_sample_lst.append(nearest_nbrs)\n            #print (self.nearest_neighbors)\n        return self.neg_sample_lst\n    \n    def generate_training_ID_SL(self):\n        random.shuffle(self.ID_lst)\n        self.img_shortlist=[]\n        for ID in ID_SL:\n            images=ID_map[ID]['images']\n            self.img_shortlist.extend(images)                \n\n    def generate_training_triplet(self):\n        while True:\n            self.generate_training_ID_SL()\n            m=len(self.img_shortlist)\n            batches=int(m/self.batch_size)\n            indexes=np.arange(batches)\n            ID_map=generate_ID_map(ID_SL)\n            self.find_neighbors()\n            \n            for i in indexes:\n                if i==indexes[-1]:\n                    anchor_lst=self.img_shortlist[i*self.batch_size:]\n                       \n                else:\n                    anchor_lst=self.img_shortlist[i*self.batch_size:(i+1)*self.batch_size]\n                \n                \n                \n                pos_lst=self.generate_positive(anchor_lst)\n                neg_lst=self.generate_negative(anchor_lst)\n                \n                anchor_img=crop_image(anchor_lst)\n                pos_img=crop_image(pos_lst)\n                neg_img=crop_image(neg_lst)\n                \n                yield([pos_img, neg_img, anchor_img], np.zeros(anchor_img.shape[0]))\n                \ntraining_images=[]\nID_SL=list(ID_SL)\nprint (f'Total number of IDs to be trained is {len(ID_SL)}')\nnum_img=1500\nbatch_size=16\nsteps_per_epoch=int(num_img/batch_size)\ntraining_data=data_generator(ID_map, ID_SL, batch_size, num_img)\ntriplet_model.fit_generator(training_data.generate_training_triplet(), steps_per_epoch=steps_per_epoch, epochs=10, workers=2)\nbase_model.save_weights('4th_run 10 epochs')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f46e972e34393885b3ef5a83f8f5597bf57f570"},"cell_type":"markdown","source":"Training for 10 epochs using all training data takes about 1.5 hours.\n\nSome modification to reduce training that have yet to be tested out:\n    1. Use smaller but deeper model as base model. Something like resnet\n    2.Further reduce output dimension. High output dimension leads to slow KNN analysis.\n\nTo identify the whale based on the encoding, the following steps are taken:\n1. Regenerate ID_map, to include all IDs that were not used in training, but not the 'new_whale' ID.\n2. Calculate the distance_to_centroid of each image to its' ID centroid. find the mean and std of the distance_to_centroid among the images with same ID.\n3. Calculate the average of mean and std of distance_to_centroid for ID that has > 3 images. This average will then be used as the mean and std for ID with <=3 images.\n4. generate encoding of test images.\n5. create nearest neighbors model with all train encoding. Then find the 30 nearest neighbor of the test encoding.\n6. from the 30 nearest neighbors, find out their ID and append to ID list.\n7. on each ID, find out their centroid, and the distance of test encoding to the centroid. Generate z-score and probability based on the distance of test encoding to centroid, mean and std distance calculated in step 2.\n8. Add 'new_whale' into the ID list, assigning the probability as 0.1 (percent of new_whale in training images)\n9. Choose the top 5 ID based on probability\n10. generate submission file.\n"},{"metadata":{"trusted":true,"_uuid":"8bb5f4a05d09fb41a7213e0d2352009c5fc85d8f","collapsed":true},"cell_type":"code","source":"#step 1 to 3\ntrain_unique_ID=ID_freq.index[1:] #list all ID in training set except 'new_whale'\nID_map={}\ntrain_img_lst=df_train['Image'][df_train['Id']!='new_whale'].tolist()\nfor ID in train_unique_ID:\n    img_lst=list(df_train['Image'][df_train['Id']==ID])\n    img_array=crop_image(img_lst)\n    train_encoding=base_model.predict(img_array)\n    nbrs=NearestNeighbors(n_neighbors=len(img_lst), algorithm= 'ball_tree').fit(train_encoding)\n    dist, indices=nbrs.kneighbors(train_encoding)\n    centroid=find_centroid(train_encoding)\n\n    if len(img_lst)<=3:\n        #assign mean and std with average of mean and std from ID>=5\n        mean_dist=0.45 \n        std_dist=0.085\n    else:\n        individual_distance=np.sqrt(np.power(train_encoding-centroid, 2).sum(axis=1))\n        mean_dist=np.mean(individual_distance)\n        std_dist=np.std(individual_distance)\n\n    ID_map[ID]={'images':img_lst,\n                'distances': dist, \n                'indices': indices,\n                'centroid': centroid,\n                'mean dist': mean_dist,\n                'std dist': std_dist}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45c83b4c1274c10227f8f64ebccbc32e17c8aa65"},"cell_type":"markdown","source":"In view of limited RAM, we will load and store train and test encoding batch by batch."},{"metadata":{"trusted":true,"_uuid":"0049da44de0755152522be8041a597fe2f2d9dcf","collapsed":true},"cell_type":"code","source":"#step 4\ndef encoding_image_list(lst, coordinate_table, train=True):\n    encoding=[]\n    for fname, array in predict_data_generator(lst, coordinate_table, batch_size=16, train=train):\n        encoding.extend(base_model.predict(array))\n    return encoding\n\ndef predict_data_generator(lst, coordinate_table, batch_size=16, train=True):\n        m=len(lst)\n        batches=int(m/batch_size)\n        indexes=list(range(batches))\n        for i in indexes:\n            if i==indexes[-1]:\n                fname=lst[i*batch_size:]\n            else:\n                fname=lst[i*batch_size:(i+1)*batch_size] \n            img_array=crop_image(fname, train=train)\n            if i%50==0:\n                print (i, 'of', indexes[-1], 'loaded')\n            yield fname, img_array\n        raise StopIteration()\n\ndef save_train_encoding():\n    print ('loading train encoding list...')\n    train_encoding_lst=encoding_image_list(train_img_lst, train_crop_table)\n    np.save('train_encoding_lst.npy', train_encoding_lst) \n    print ('loading train encoding list completed')\n    return train_encoding_lst\ntrain_encoding_lst=save_train_encoding()\n\n#load all test file name into list\ntest_file_path=glob.glob('../input/whale-categorization-playground/test/*.jpg')\ntest_file_path=test_file_path\ntest_img_lst=[]\nfor file in test_file_path:\n    test_img_lst.append(file.split('/')[-1])\n    \ndef save_test_encoding():\n    print ('loading test encoding list...')\n    test_encoding_lst=encoding_image_list(test_img_lst, test_crop_table, train=False)\n    np.save('test_encoding_lst.npy', test_encoding_lst)\n    print ('loading test encoding list completed')\n    return test_encoding_lst\ntest_encoding_lst=save_test_encoding()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"118cde07575ad32c3d58f7c172df8bfe70e8bafd","collapsed":true},"cell_type":"code","source":"#step 5\nn_neighbors=30\nnbrs=NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(train_encoding_lst)\n\nprint ('fitting neighbor model...')\ndist, idx=nbrs.kneighbors(test_encoding_lst)\nprint ('fitting neighbor model completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a2e526cbd75c04336a322f0249701cb34345d01","collapsed":true},"cell_type":"code","source":"#step 6-9\ndef choose_top5(test_encoding_lst, NW_cutoff=0.1):\n    top5=[]\n    m=len(test_encoding_lst)\n    for i in range(m):\n        if i%500==0:\n            print ('analyzing ', i , 'of ', m)\n        ID_lst=[]\n        cfm_lst=[]\n\n        for j in idx[i]:\n            ID=check_id(train_img_lst[j])\n            if ID not in ID_lst:\n                ID_lst.append(ID)\n                centroid=ID_map[ID]['centroid']\n                mean_dist=ID_map[ID]['mean dist']\n                std_dist=ID_map[ID]['std dist']\n\n                encoding_dist_to_centroid=np.linalg.norm(test_encoding_lst[i]-centroid)\n                cfm=1-norm.cdf(encoding_dist_to_centroid, mean_dist, std_dist)\n                cfm_lst.append(cfm)\n        \n        if 'new_whale' not in ID_lst:\n            ID_lst.append('new_whale')\n            cfm_lst.append(NW_cutoff) ## NW_cutoff as fraction of train files are new whale.\n        \n        top_5_cfm=np.flip(np.argsort(cfm_lst.copy()), axis=0)\n        ID_lst=np.array(ID_lst)\n        top5.append(list(ID_lst[top_5_cfm[:5]]))\n        \n    return top5\nprint ('predicting top 5 choices')\ntop5=choose_top5(test_encoding_lst)\nprint ('predicting top 5 completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ec5eedf9b9031249d3395c74b54a454167909326"},"cell_type":"code","source":"#step 10\nnew_top5=[]\nfor row in top5:\n    new_row=''\n    for col in row:\n        if col==row[0]:\n            new_row=new_row+''+col\n        else:\n            new_row=new_row+' '+col\n    new_top5.append(new_row)\n        \n\nprint ('writing output file')\nfilename='min2_maxall_128.txt'\nf=open (filename, 'w+')\nf.write('Image,Id\\n')\n\nfor i in range (len(test_img_lst)):\n    data=test_img_lst[i]+','+ new_top5[i]\n    f.write(data+'\\n')\nf.close\nprint ('writing complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0250a6b48a9a7d9a92a64c4755841843c1a64666"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}