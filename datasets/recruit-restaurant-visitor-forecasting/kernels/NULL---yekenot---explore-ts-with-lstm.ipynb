{"cells": [{"source": ["In this notebook I'll try the approach, which discovered in one tutorial about multivariate time series forecasting using LSTM.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "8bd41f5e-8931-42ef-a440-072078a72531", "_uuid": "b86dc252887403d87ab2d04cf965801ffc41a3b1"}}, {"source": ["import pandas as pd\n", "import numpy as np\n", "np.random.seed(10)\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.metrics import mean_squared_error\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense, LSTM"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "725ec533-357d-45cb-ba91-6af034696543", "_uuid": "e2058e19bb6a2b4a8bfdb0740a1b371092b0b476"}, "execution_count": 1}, {"source": ["## **Data Aggregation**\n", "\n", "\n", "Features from **the1owl**'s kernel https://www.kaggle.com/the1owl/surprise-me"], "cell_type": "markdown", "metadata": {"_cell_guid": "5f9b6c6e-bc43-411e-a9d8-0ed4cb746074", "_uuid": "604b6bd57b7c254f7014abb079667f5a350f2694"}}, {"source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "cac9e094-3b2f-471e-8bb2-5a6967bb3790", "_uuid": "98e8cdbccba65e193d36603a77d483e00480e949", "collapsed": true}, "execution_count": 2}, {"source": ["for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n", "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n", "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date    \n", "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "09486f7b-8b1f-47a3-b2b8-17fefc08ae0f", "_uuid": "623d2a29ababd1a2ff8ade01bcec2af814d4f33b", "collapsed": true}, "execution_count": 3}, {"source": ["data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "2eefb47d-8081-4741-8662-6b52235c1f92", "_uuid": "e499470cea771207c9cea5543bf3ae8d5b69343e"}, "execution_count": 4}, {"source": ["unique_stores = data['tes']['air_store_id'].unique()\n", "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n", "\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n", "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "31b66338-3871-4183-b342-292e64430f4f", "_uuid": "50f7dab23a845ef129a48bf31ba5d66bd9327c60", "collapsed": true}, "execution_count": 5}, {"source": ["stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n", "lbl = LabelEncoder()\n", "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) "], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "38da6714-9617-481a-a133-560319af5a6f", "_uuid": "338259bf538ed869f0b9c7c7e2bc56d496980cfb", "collapsed": true}, "execution_count": 6}, {"source": ["train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \n", "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow'])\n", "\n", "for df in ['ar','hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n", "    \n", "train = train.fillna(-1)\n", "test = test.fillna(-1)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b40d2db7-d960-4b43-ae5c-5939e68befc4", "_uuid": "2010505626053fbae1b852e34c32050cab9599b1", "collapsed": true}, "execution_count": 7}, {"source": ["def RMSLE(y, pred):\n", "    return mean_squared_error(y, pred)**0.5"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "6b5bb4d2-197e-4a1e-bd5b-37334c45137d", "_uuid": "d57e38ca429843983116df1943e4468b192231bf", "collapsed": true}, "execution_count": 8}, {"source": ["train.head()"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a2194f79-07c5-4184-8396-098ccea92a98", "scrolled": true, "_uuid": "b3d392b9506abc14db46706fe84a86c589e0344f"}, "execution_count": 9}, {"source": ["# **Part 1** \n", "## **Visitors as a feature to fit LSTM**\n", "\n", "Functions from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras"], "cell_type": "markdown", "metadata": {"_cell_guid": "7f9178b2-e59f-492b-b56f-98ef997dbd09", "_uuid": "1a8a720ac5590e1097eb362cb8d0c0f77a201fa8"}}, {"source": ["### *Normalize feature*"], "cell_type": "markdown", "metadata": {"_cell_guid": "4318487b-3c93-47eb-90cd-111fd903392d", "_uuid": "e32336ef9ff1a8f4c8fd69e016113325bd24437f"}}, {"source": ["train = train.sort_values('visit_date')\n", "values = np.log1p(train['visitors'].values).reshape(-1,1)\n", "values = values.astype('float32')\n", "\n", "scaler = MinMaxScaler(feature_range=(0, 1))\n", "scaled = scaler.fit_transform(values)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "efa3fe0a-771e-4d4b-9e57-80d888b1789d", "_uuid": "62cddb6514b7153a906a804c40b66da627592525", "collapsed": true}, "execution_count": 10}, {"source": ["### *Split into train and test sets*"], "cell_type": "markdown", "metadata": {"_cell_guid": "14290506-1f5e-4fd6-bb25-06ff50a09146", "_uuid": "ef66d00ed041acb0f254ae4fa0256274b005d6c3"}}, {"source": ["train_size = int(len(scaled) * 0.7)\n", "test_size = len(scaled) - train_size\n", "\n", "V_train, V_test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n", "print(len(V_train), len(V_test))"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "51cc721e-6f40-4cdc-b03c-27abcb5ce5c7", "_uuid": "e0593a65c332c528b40f9faf5c75d72e48e5421d"}, "execution_count": 11}, {"source": ["### *Convert an array of values into a dataset matrix*"], "cell_type": "markdown", "metadata": {"_cell_guid": "01b4dea0-5c79-4145-8d69-dc5ca71cb342", "_uuid": "5ea9b627b68aa452bee41448dbcaa9b98d255507"}}, {"source": ["def create_dataset(dataset, look_back=1):\n", "    dataX, dataY = [], []\n", "    for i in range(len(dataset) - look_back):\n", "        a = dataset[i:(i + look_back), 0]\n", "        dataX.append(a)\n", "        dataY.append(dataset[i + look_back, 0])\n", "    print(len(dataY))\n", "    return np.array(dataX), np.array(dataY)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c46462f1-6818-4d04-8f39-50b382990393", "_uuid": "b1d03dd3506b5c48149f5aed24caa92d06d16465", "collapsed": true}, "execution_count": 12}, {"source": ["### *Create dataset with look back*"], "cell_type": "markdown", "metadata": {"_cell_guid": "4178114a-7e11-4302-885f-873781cf681b", "_uuid": "85ad747825bf1ccf3ec2e87a4f4f76e0b29da18f"}}, {"source": ["look_back = 1\n", "trainX, trainY = create_dataset(V_train, look_back)\n", "testX, testY = create_dataset(V_test, look_back)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "74438c7a-45ea-4ef0-9973-e2d4f7935192", "_uuid": "a068488ea3bef9affc7d1052f05bdb719a6383a5"}, "execution_count": 13}, {"source": ["### *Reshape X for model training*"], "cell_type": "markdown", "metadata": {"_cell_guid": "1cb7592e-fe3d-468d-91d7-9622ff3b9e26", "_uuid": "a150742a7687f1a4f70df6b0c45e2c7094037a9c"}}, {"source": ["trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n", "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "7a2e23fa-a184-4f09-be5e-07364b4f902e", "_uuid": "c53740ea7194f31e45ca96ac464d79c18a9e9209", "collapsed": true}, "execution_count": 14}, {"source": [" ### *Train LSTM with 3 epochs*"], "cell_type": "markdown", "metadata": {"_cell_guid": "79e6cd0e-2a38-46c6-aa8c-3506fbe177ee", "_uuid": "9a5ce2f4b30c1ad5992bd51ad997471682507a91"}}, {"source": ["model = Sequential()\n", "model.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2])))\n", "model.add(Dense(1))\n", "model.compile(loss='mse', optimizer='adam')\n", "history = model.fit(trainX, trainY, epochs=3, batch_size=100,\n", "                            validation_data=(testX, testY), verbose=1, shuffle=False) "], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "68511997-bb73-4b3b-96fc-91db08025f4c", "_uuid": "f2063c1c8bc8f73d97840e437010d7c8f1760eef"}, "execution_count": 15}, {"source": ["### *Make prediction and apply invert scaling*"], "cell_type": "markdown", "metadata": {"_cell_guid": "1e135933-aefb-482f-991c-fae28736125e", "_uuid": "7b9a6cd42e0d82b105dd1244a06588a899014d3f"}}, {"source": ["yhat = model.predict(testX)\n", "\n", "yhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))\n", "testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "6bac9566-3ce3-45cd-9de3-680220c418b6", "_uuid": "0e66c5594b0f96320d29ed724d32e052d953c059", "collapsed": true}, "execution_count": 16}, {"source": ["### *RMSLE*"], "cell_type": "markdown", "metadata": {"_cell_guid": "3d2cdd33-4cb1-4776-b5cc-9b8ab4829401", "_uuid": "ef8133a54f36a3e21370c1e636a94918aaa9513b"}}, {"source": ["rmsle = RMSLE(testY_inverse, yhat_inverse)\n", "print('Test RMSLE: %.3f' % rmsle)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "4d9d99f7-8f52-4518-9e39-f9306759e466", "_uuid": "d7a0a497c8430e36cc97a075df0312ea8660a27c"}, "execution_count": 17}, {"source": ["# **Part 2**\n", "## **Multivariate Forecast**\n", "\n", "Functions from https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras"], "cell_type": "markdown", "metadata": {"_cell_guid": "addf84e4-7235-4ca6-b8a9-f631ade22fc1", "_uuid": "00ef42aef7ecc896c47423ad1190116fc0345f13"}}, {"source": ["### *Using all features for model training*"], "cell_type": "markdown", "metadata": {"_cell_guid": "38878ce6-98da-406d-ab3d-c5bee4e1d238", "_uuid": "c48e420633e0bf91cde66d03ac57eafcf229a783"}}, {"source": ["train = train.sort_values('visit_date')\n", "target_train = np.log1p(train['visitors'].values)\n", "\n", "col = [c for c in train if c not in ['id', 'air_store_id', 'visitors']]\n", "\n", "train = train[col]\n", "train.set_index('visit_date', inplace=True)\n", "\n", "train.head()"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "6d7d4fc4-3526-47f2-9069-2783e7c43ff8", "_uuid": "f8264be6aa6785f04a87a4ca8155281c840c7bdd"}, "execution_count": 18}, {"source": ["### *Function to convert series to supervised learning*"], "cell_type": "markdown", "metadata": {"_cell_guid": "7b4fa080-c750-43fd-ab31-2dafb71ef0ed", "_uuid": "22a7e3a4b35aba652f9e500675707d4561e4aa83"}}, {"source": ["def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n", "    \"\"\"\n", "    Frame a time series as a supervised learning dataset.\n", "    Arguments:\n", "        data: Sequence of observations as a list or NumPy array.\n", "        n_in: Number of lag observations as input (X).\n", "        n_out: Number of observations as output (y).\n", "        dropnan: Boolean whether or not to drop rows with NaN values.\n", "    Returns:\n", "        Pandas DataFrame of series framed for supervised learning.\n", "    \"\"\"\n", "    n_vars = 1 if type(data) is list else data.shape[1]\n", "    df = pd.DataFrame(data)\n", "    cols, names = list(), list()\n", "    # Input sequence (t-n, ... t-1)\n", "    for i in range(n_in, 0, -1):\n", "        cols.append(df.shift(i))\n", "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # Forecast sequence (t, t+1, ... t+n)\n", "    for i in range(0, n_out):\n", "        cols.append(df.shift(-i))\n", "        if i == 0:\n", "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n", "        else:\n", "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # Put it all together\n", "    agg = pd.concat(cols, axis=1)\n", "    agg.columns = names\n", "    # Drop rows with NaN values\n", "    if dropnan:\n", "        agg.dropna(inplace=True)\n", "    return agg"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "1ebfdd4c-24ad-4280-bbe7-63234d71bfc7", "_uuid": "1690ee4ad571a4978b358daa6f9cbbb4be41fcd9", "collapsed": true}, "execution_count": 19}, {"source": ["### *Normalize features*"], "cell_type": "markdown", "metadata": {"_cell_guid": "64f79c05-b4aa-46a1-9efd-69db23447f48", "_uuid": "dfb94265d258324c6cd183826da54afbfa89171d"}}, {"source": ["train['visitors'] = target_train\n", "values = train.values\n", "values = values.astype('float32')\n", "\n", "scaler = MinMaxScaler(feature_range=(0, 1))\n", "scaled = scaler.fit_transform(values)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c4e4f56e-9f43-4ca2-88a8-28f7f22ec7ed", "_uuid": "2b2b4c58989fd03bade2c8bd227602a1c8d7a5da", "collapsed": true}, "execution_count": 20}, {"source": ["### *Frame as supervised learning*"], "cell_type": "markdown", "metadata": {"_cell_guid": "a71d31a6-b169-4a2b-9b50-849bf3c34f24", "_uuid": "3ca96959d0de395f6df36b4113e9f9762c0fa4d6"}}, {"source": ["reframed = series_to_supervised(scaled, 1, 1)\n", "reframed.head()"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a0c1f374-dad5-42fe-a33f-874b951e934e", "_uuid": "e49faf36f97ff0bd128a1fa9fb2549b3d454b42b"}, "execution_count": 21}, {"source": ["### *Drop unncessary columns*"], "cell_type": "markdown", "metadata": {"_cell_guid": "4527e2c0-c138-4d8a-a446-4927973ed61d", "_uuid": "23891f7fb6d0b1362b463c945ade07c6d70c2403"}}, {"source": ["reframed.drop(reframed.columns[[i for i in range(17,33)]], axis=1, inplace=True)\n", "reframed.head()"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "283d2ccc-bb49-4cba-aa6f-8cc4eb220f91", "scrolled": false, "_uuid": "a98b0b425675269218b81e9a27215996a0678b43"}, "execution_count": 22}, {"source": ["### *Split into train and test sets*"], "cell_type": "markdown", "metadata": {"_cell_guid": "01e41387-5d43-4bc3-8399-07888fa69acd", "_uuid": "9fdd0ba18aa6c252248ea216ee339988669515f4"}}, {"source": ["values = reframed.values\n", "n_train_days = int(len(values) * 0.7)\n", "train = values[:n_train_days, :]\n", "test = values[n_train_days:, :]\n", "# Split into input and outputs\n", "train_X, train_y = train[:, :-1], train[:, -1]\n", "test_X, test_y = test[:, :-1], test[:, -1]\n", "# Reshape input to be 3D [samples, timesteps, features]\n", "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n", "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n", "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b79a3851-b0c5-4a0c-acaa-9436ed229439", "_uuid": "c9dea01c0946587f54f6fdd0d2049bed41f0a907"}, "execution_count": 23}, {"source": [" ### *Train LSTM with 3 epochs*"], "cell_type": "markdown", "metadata": {"_cell_guid": "9b37acff-e975-4c11-914a-9b8b090747ca", "_uuid": "ba44e1fad5384246cc42831740c9e50b39322856"}}, {"source": ["multi_model = Sequential()\n", "multi_model.add(LSTM(4, input_shape=(train_X.shape[1], train_X.shape[2])))\n", "multi_model.add(Dense(1))\n", "multi_model.compile(loss='mse', optimizer='adam')\n", "multi_history = multi_model.fit(train_X, train_y, epochs=3,\n", "                                batch_size=100, validation_data=(test_X, test_y),\n", "                                verbose=1, shuffle=False)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "67593e30-d79a-469a-b373-387cd5bafe39", "_uuid": "ab4349824bd3859cfb232e6e46de90fad4b42f2e"}, "execution_count": 24}, {"source": ["### *Make prediction*"], "cell_type": "markdown", "metadata": {"_cell_guid": "ff2f075c-b0b9-4c3f-9587-83dd613d6e3f", "_uuid": "ba033e07a4b21b9b89f493dc23924d4999afe150"}}, {"source": ["yhat = multi_model.predict(test_X)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a106cb5a-a304-4939-9642-8924cba32273", "_uuid": "9bc02f954b5a61e508067f3cf9604d68182dde49", "collapsed": true}, "execution_count": 25}, {"source": ["### *Apply invert scaling*"], "cell_type": "markdown", "metadata": {"_cell_guid": "f3a9ab8f-1fff-4b70-876e-ec83a0dca450", "_uuid": "0a6ed5c118a4a723152ec0674d8687567f697cf7"}}, {"source": ["test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n", "# Invert scaling for forecast\n", "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n", "inv_yhat = scaler.inverse_transform(inv_yhat)\n", "inv_yhat = inv_yhat[:,0]\n", "# Invert scaling for actual\n", "test_y = test_y.reshape((len(test_y), 1))\n", "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n", "inv_y = scaler.inverse_transform(inv_y)\n", "inv_y = inv_y[:,0]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "abd97ae2-7c9c-48b2-ac5b-46a1ad3f1c10", "_uuid": "279fb937c1d1462df8e3be8bea5aa4d6a8fb6017"}, "execution_count": 26}, {"source": ["### *RMSLE*"], "cell_type": "markdown", "metadata": {"_cell_guid": "8a67ef42-bc43-4453-bf5a-75bddcec54a0", "_uuid": "8c39ade35c980876f0aa094916a524571bc974cb"}}, {"source": ["rmsle = RMSLE(inv_y, inv_yhat)\n", "print('Test RMSLE: %.3f' % rmsle)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "2e87a7ba-1b8d-496e-a5ee-cc64d25f8187", "_uuid": "7781a4c953d7159899c1d2a1a77e9d55c7c1b8b3"}, "execution_count": 27}, {"source": ["Slight improve, not enough, however, to beat benchmarks. I would add that the LSTM may not be suited for autoregression type problems (at least with such set of features, window and LSTM-configuration) and that maybe better off exploring an MLP with a large window. But i think it's good demo how to fit neural network to a multivariate time series forecasting problem. \n", "Specifically:\n", "\n", "* How to transform a raw dataset into something we can use for time series forecasting.\n", "* How to prepare data and fit an LSTM for a multivariate time series forecasting problem.\n", "* How to make a forecast and rescale the result back into the original units."], "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1}