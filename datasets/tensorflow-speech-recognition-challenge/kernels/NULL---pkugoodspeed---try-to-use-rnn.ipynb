{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["# input, output and command line tools\n", "import os\n", "from os.path import isdir, join\n", "import pandas as pd\n", "\n", "# math and data handler\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.decomposition import PCA\n", "\n", "# audio file i/o\n", "from scipy.fftpack import fft\n", "from scipy import signal\n", "from scipy.io import wavfile\n", "\n", "# Visualization\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from IPython.display import display\n", "\n", "\n", "mpl.rc('font', family = 'serif', size = 17)\n", "mpl.rcParams['xtick.major.size'] = 5\n", "mpl.rcParams['xtick.minor.size'] = 2\n", "mpl.rcParams['ytick.major.size'] = 5\n", "mpl.rcParams['ytick.minor.size'] = 2\n", "\n", "# Shuffle data\n", "from sklearn.utils import shuffle\n", "\n", "# Keras\n", "from keras import backend as K\n", "from keras.datasets import mnist\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation\n", "from keras.layers import Flatten, Conv2D, MaxPooling2D, GRU\n", "from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n", "from keras.utils import np_utils, plot_model\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.layers.advanced_activations import LeakyReLU, PReLU\n", "from keras.callbacks import LearningRateScheduler\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.layers.recurrent import SimpleRNN, LSTM #Actually in this test, SimpleRNN works much better\n", "from keras.layers.embeddings import Embedding"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["hyper_pwr = 0.5\n", "hyper_train_ratio = 0.9\n", "hyper_n = 25\n", "hyper_m = 15\n", "hyper_NR = 208\n", "hyper_NC = 112\n", "hyper_delta = 0.3\n", "hyper_dropout0 = 0.2\n", "hyper_dropout1 = 0.4\n", "hyper_dropout2 = 0.6\n", "hyper_dropout3 = 0.6\n", "hyper_dropout4 = 0.4\n", "hyper_dropout5 = 0.7\n", "\n", "TAGET_LABELS = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["## Function for loading the audio data, return a dataFrame\n", "MAX_SIZE = 16000\n", "def load_audio_data(path, ltoi):\n", "    '''\n", "    path: audio file path\n", "    return: pd.DataFrame\n", "    '''\n", "    x = []\n", "    y = []\n", "    for i, folder in enumerate(os.listdir(path)):\n", "        for filename in os.listdir(path + '/' + folder):\n", "            if filename == 'README.md':\n", "                continue\n", "            rate, sample = wavfile.read(data_dir + '/' + folder + '/' + filename)\n", "            assert(rate == MAX_SIZE)\n", "            if folder == '_background_noise_':\n", "                length = len(sample)\n", "                for j in range(int(length/rate)):\n", "                    x.append(np.array(sample[j*rate: (j+1)*rate]))\n", "                    y.append(ltoi['silence'])\n", "            else:\n", "                x.append(np.array(sample))\n", "                label = folder\n", "                if folder not in TAGET_LABELS:\n", "                    label = 'unknown'\n", "                y.append(ltoi[label])\n", "    x = np.array(pad_sequences(x, maxlen=MAX_SIZE))\n", "    y = np.array(y)\n", "    df = pd.DataFrame()\n", "    df['x'] = list(x)\n", "    df['y'] = list(y)\n", "    return df"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["data_dir = '../input/train/audio'\n", "os.listdir('{0}/_background_noise_'.format(data_dir))"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["## Loading raw data Frame\n", "print(\"LOADING RAW DATA!\")\n", "label2idx = {}\n", "idmap = {}\n", "for i,lab in enumerate(TAGET_LABELS):\n", "    label2idx[lab] = i\n", "    idmap[i] = lab\n", "raw_df = load_audio_data(data_dir, label2idx)\n", "print(label2idx)\n", "print(idmap)\n", "print(raw_df.x.as_matrix().shape)\n", "print(raw_df.y.as_matrix().shape)"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Split train, test sets, and also return label_map\n", "def train_test_split(df, train_ratio = 0.2, test_ratio = 0.1):\n", "    '''\n", "    return train_sets + test_sets + label_map, which maps from y to label name\n", "    '''\n", "    test_x = []\n", "    test_y = []\n", "    train_x = []\n", "    train_y = []\n", "    for i in set(df.y.tolist()):\n", "        tmp_df = df[df.y == i]\n", "        tmp_df = shuffle(tmp_df)\n", "        tmp_n = int(len(tmp_df)*train_ratio)\n", "        tmp_m = int(len(tmp_df)*test_ratio)\n", "        train_x += tmp_df.x.tolist()[: tmp_n]\n", "        test_x += tmp_df.x.tolist()[tmp_n: tmp_n + tmp_m]\n", "        train_y += tmp_df.y.tolist()[: tmp_n]\n", "        test_y += tmp_df.y.tolist()[tmp_n: tmp_n + tmp_m]\n", "    return np.array(train_x), np.array(train_y), np.array(test_x), np.array(test_y)"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["## Parsing the data Frame into train and test sets\n", "print(\"SPLITTING DATA INTO TRAIN AND TEST SETS!\")\n", "tr_x, tr_y, ts_x, ts_y = train_test_split(raw_df, 0.3, 0.1)\n", "print(tr_x.shape)\n", "print(tr_y.shape)\n", "print(ts_x.shape)\n", "print(ts_y.shape)\n", "del raw_df"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["BASE_LVL = min(tr_x.min(), ts_x.min())\n", "print(BASE_LVL)\n", "tr_x = tr_x - BASE_LVL\n", "ts_x = ts_x - BASE_LVL\n", "UPPER_X = max(tr_x.max(), ts_x.max()) + 1\n", "print(UPPER_X)\n", "print(tr_x[0])"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["print(tr_x[0])\n", "print(tr_x.max())\n", "print(ts_x.max())\n", "print(tr_x.min())\n", "print(ts_x.min())"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["# Function to compute class weights\n", "def comp_cls_wts(y, pwr = 0.5):\n", "    '''\n", "    Used to compute class weights\n", "    '''\n", "    dic = {}\n", "    for x in set(y):\n", "        dic[x] = len(y)**pwr/list(y).count(x)**pwr\n", "    return dic"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["cls_wts = comp_cls_wts(tr_y)\n", "print(cls_wts)"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["NUM_CLS = len(TAGET_LABELS)\n", "tr_y = np_utils.to_categorical(tr_y, num_classes=NUM_CLS)\n", "ts_y = np_utils.to_categorical(ts_y, num_classes=NUM_CLS)"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["model = Sequential()\n", "model.add(Embedding(UPPER_X, 128, input_length=MAX_SIZE))\n", "model.add(SimpleRNN(512))\n", "model.add(Dense(64, activation='relu'))\n", "model.add(Dense(NUM_CLS, activation='softmax'))\n", "model.summary()"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["### Compile the model\n", "optimizer = SGD()\n", "metrics = ['accuracy']\n", "loss = 'categorical_crossentropy'\n", "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["res = model.fit(tr_x, tr_y, batch_size = 64,epochs = 15, validation_data = (ts_x, ts_y),\n", "                class_weight = cls_wts)"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": [], "outputs": []}]}