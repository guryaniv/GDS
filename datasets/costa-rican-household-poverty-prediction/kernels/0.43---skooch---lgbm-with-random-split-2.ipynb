{"cells":[{"metadata":{"_uuid":"0479e25c001874fda15f875d76f8663885caa138"},"cell_type":"markdown","source":"# Do feature engineering to improve LightGBM prediction\n\n**Edits by Eric Scuccimarra** - This is a fork of  https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro with a few changes:\n - Some additional features have been added.\n - Some features which were previously dropped have been retained.\n - In order to train on the entire training I randomly split the data during the training and use that split for the early stopping. It's not quite a k-fold split because it's random each time, but it's better than the alternative. This also means that the validation results are not accurate.\n \nSome additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score\n\nThis kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. \n\nSeveral key points:\n- **This kernel runs training on the heads of housholds only** (after extracting aggregates over households). This follows the announced scoring startegy: *Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.* (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score\n- **It seems to be very important to balance class frequencies.** Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set `class_weight='balanced'` in the LightGBM model constructor in sklearn API.\n- **This kernel uses macro F1 score to early stopping in training**. This is done to align with the scoring strategy.\n- Categoricals are turned into numbers with proper mapping instead of blind label encoding. \n- **OHE if reversed into label encoding, as it is easier to digest for a tree model.** This trick would be harmful for non-tree models, so be careful.\n- **idhogar is NOT used in training**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n- **There are aggregations done within households and new features are hand-crafted**. Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n- **A voting classifier is used to average over several LightGBM models**\n\nThe main goal is to do feature engineering"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4e08a17549fd247619178c96c3ade2519e9773"},"cell_type":"markdown","source":"The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https://www.kaggle.com/mlisovyi/categorical-variables-in-the-data\n    and the insight from https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n    \ndef feature_importance(forest, X_train):\n    ranked_list = []\n    \n    importances = forest.feature_importances_\n\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(X_train.shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n        ranked_list.append(X_train.columns[indices[f]])\n    \n    return ranked_list    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1785c8ca3467a7e95d007a2c5f36e39919fc0910"},"cell_type":"markdown","source":"**There is also feature engineering magic happening here:**"},{"metadata":{"_kg_hide-input":true,"_uuid":"9c9f13e54fc2af9f938b895959631e1aeb3b08a2","trusted":false,"collapsed":true},"cell_type":"code","source":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    \n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n\n            # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"a7df32a07c9157ee02ff9688cdc70c69e7571aae","trusted":false,"collapsed":true},"cell_type":"code","source":"def convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab84429fc9893c82e33b8319161c190b4104e9f"},"cell_type":"markdown","source":"# Read in the data and clean it up"},{"metadata":{"_uuid":"e6f696a1677230c565532f141a02852e7c69b2e1","trusted":false,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntest_ids = test.Id","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd1f66cbdbfa4741d19a8b1f53793b967d62d281","trusted":false,"collapsed":true},"cell_type":"code","source":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n\n    # do feature engineering and drop useless columns\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# change education to a number instead of a string, combine the two fields and drop the originals\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 1\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\n\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 1\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\n\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\ntest['edjefe'] = test['edjefe'].astype(\"int\")\n\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# some rows indicate both that the household does and does not have a toilet, if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b564a6552f0521581af1ee38d6040ef7ae5d2fb5","trusted":false,"collapsed":true},"cell_type":"code","source":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2695108e103c9c61088fc4c100d01bc8c0f4138c","trusted":false,"collapsed":true},"cell_type":"code","source":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e90e68abd266c9db808dbf336308cef7175886bd"},"cell_type":"markdown","source":"# Geo aggregates"},{"metadata":{"_uuid":"0ffc288b3829ffdb1dabf8f2e7fe518f2f520480","trusted":false,"collapsed":true},"cell_type":"code","source":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n    #print(pd.get_dummies(train[cols_2_ohe], \n    #                                   columns=cols_2_ohe).head())\n    #print(tmp_df.head())\n    #print(tmp_df.groupby(['lugar_LE','idhogar']).mean().head())\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    #print(gb.T)\n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc96d527090d9b0723049c5d763c97be5145b8c7","trusted":false,"collapsed":true},"cell_type":"code","source":"# we'll init the feature to 0, then count the people over 18 in the household and use the max of that\ntrain['num_over_18'] = 0\ntrain['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain['num_over_18'] = train['num_over_18'].fillna(0)\n\ntest['num_over_18'] = 0\ntest['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest['num_over_18'] = test['num_over_18'].fillna(0)\n\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    df['dependency_yes'] = df['dependency'].apply(lambda x: 1 if x == 'yes' else 0)\n    df['dependency_no'] = df['dependency'].apply(lambda x: 1 if x == 'no' else 0)\n    \n    \nextract_features(train)    \nextract_features(test)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a6ed97e10f021b42a19c7228dfc1a52ce9de2c60"},"cell_type":"code","source":"needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'female', ]\n\ninstlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n\nneedless_cols.extend(instlevel_cols)\n\ntrain = train.drop(needless_cols, axis=1)\ntest = test.drop(needless_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e1ccce811e7a1d76282fcb8a13edf92672f834"},"cell_type":"markdown","source":"# Model fitting with HyperParameter optimisation\n\nWe will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"da37e677a32477006e8468b954e23d595c82eced"},"cell_type":"code","source":"def split_data(train, target, households, test_percentage=0.20, seed=None):\n    # pull out and drop the target variable\n    y = target\n\n    np.random.seed(seed=seed)\n\n    train2 = train.copy()\n\n    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n\n    cv_idx = np.isin(households, cv_hhs)\n\n    X_test = train2[cv_idx]\n    y_test = y[cv_idx]\n\n    X_train = train2[~cv_idx]\n    y_train = y[~cv_idx]\n    \n#     X_train.drop(extra_drop_features, axis=1, inplace=True)\n#     X_test.drop(extra_drop_features, axis=1, inplace=True)\n    \n    return X_train, y_train, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e14a9619ca6516b225b55bf65d6a9e423d6b5fc7"},"cell_type":"code","source":"X = train.query('parentesco1==1')\n# X = train.copy()\n\n# pull out and drop the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)\n\nnp.random.seed(seed=1977)\n\ntrain2 = X.copy()\n\ntrain_hhs = train2.idhogar\n\nhouseholds = train2.idhogar.unique()\ncv_hhs = np.random.choice(households, size=int(len(households) * 0.1), replace=False)\n\ncv_idx = np.isin(train2.idhogar, cv_hhs)\n\nX_test = train2[cv_idx]\ny_test = y[cv_idx]\n\nX_train = train2[~cv_idx]\ny_train = y[~cv_idx]\n\n# train on entire dataset\nX_train = train2\ny_train = y\n\ntrain_households = X_train.idhogar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d9bcfbfcfc0b5b7e3aaeb2b0c8495bd92fdf51a3"},"cell_type":"code","source":"extra_drop_features = ['geo_manual_elec_LE_2',\n 'agg18_estadocivil5_COUNT',\n 'agg18_estadocivil7_COUNT',\n 'geo_pared_LE_5',\n 'agg18_estadocivil6_COUNT',\n 'geo_pared_LE_4',\n 'agg18_parentesco10_COUNT',\n 'num_over_18',\n 'agg18_estadocivil1_MEAN',\n 'agg18_estadocivil4_COUNT',\n 'r4t3_to_tamhog',\n 'agg18_estadocivil3_COUNT',\n 'geo_pared_LE_3',\n 'geo_pared_LE_6',\n 'geo_pared_LE_0',\n 'geo_pared_LE_7',\n 'agg18_parentesco7_COUNT',\n 'agg18_parentesco10_MEAN',\n 'agg18_parentesco8_MEAN',\n 'parentesco1',\n 'rez_esc',\n 'dependency_yes',\n 'hacdor',\n 'hacapo',\n 'fe_people_weird_stat',\n 'geo_hogar_mayor',\n 'geo_eviv_LE_0',\n 'geo_epared_LE_0',\n 'geo_epared_LE_2',\n 'geo_elimbasu_LE_4',\n 'geo_energcocinar_LE_0',\n 'agg18_parentesco9_COUNT',\n 'agg18_parentesco8_COUNT',\n 'agg18_parentesco11_COUNT',\n 'parentesco_LE',\n 'agg18_parentesco6_MEAN',\n 'agg18_parentesco6_COUNT',\n 'agg18_parentesco5_COUNT',\n 'agg18_parentesco4_MEAN',\n 'agg18_parentesco4_COUNT',\n 'geo_energcocinar_LE_1',\n 'agg18_parentesco3_COUNT',\n 'geo_energcocinar_LE_2',\n 'agg18_parentesco2_COUNT',\n 'agg18_parentesco1_COUNT',\n 'agg18_parentesco12_MEAN',\n 'agg18_parentesco12_COUNT',\n 'dependency_no'] + ['geo_hogar_nin', 'geo_manual_elec_LE_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"67a0f3b7d019517ed9a40b41df318284a33a5ce1"},"cell_type":"code","source":"lgb_drop_cols = extra_drop_features + [\"idhogar\"]\ntry:\n    X_train.drop(lgb_drop_cols, axis=1, inplace=True)\n    X_test.drop(lgb_drop_cols, axis=1, inplace=True)\nexcept:\n    print(\"Error dropping columns\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dc384aeb44db2454978df78fdbb84b2b1ff3ced"},"cell_type":"markdown","source":"# Fit a voting classifier\nDefine a derived VotingClassifier class to be able to pass `fit_params` for early stopping. Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate.\n\nThe parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"629cf88dd0596a98d58487495e5b096636e2586a"},"cell_type":"code","source":"opt_parameters = {'colsample_bytree': 0.93, 'min_child_samples': 56, 'num_leaves': 19, 'subsample': 0.84}\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nfit_params={\"early_stopping_rounds\":300,\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n            'eval_names': ['train', 'valid'],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['verbose'] = 200\nfit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"101a2fd45bbbe6c7fb351513803550d4edeef2b3","trusted":false,"collapsed":true},"cell_type":"code","source":"def _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    \n    X_train, y_train, X_test, y_test = split_data(X, y, households=train_households, seed=None)\n    \n    fit_params[\"eval_set\"] = [(X_train,y_train), (X_test,y_test)]\n    \n    \"\"\"Private function used to fit an estimator within a job.\"\"\"\n    if sample_weight is not None:\n        estimator.fit(X_train, y_train, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    return estimator\n\nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b909c2aa273651b7bb57c69b939760f14f38f7","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"clfs = []\nfor i in range(10):\n    clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=314+i, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced')\n    \n    clf.set_params(**opt_parameters)\n    clfs.append(('lgbm{}'.format(i), clf))\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\ndel clfs\n\n#Train the final model with learning rate decay\n_ = vc.fit(X_train, y_train, **fit_params)\n\nclf_final = vc.estimators_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"427d1bb3c69eebc73c6b490cc0fbb2c825b2236d"},"cell_type":"code","source":"global_score = f1_score(y_test, clf_final.predict(X_test), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_test, vc.predict(X_test), average='macro')\nvc.voting = 'hard'\nglobal_score_hard = f1_score(y_test, vc.predict(X_test), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"241d2973497b8eb2e5214f5c8ddb76432576ba35","trusted":false,"collapsed":true},"cell_type":"code","source":"global_score = f1_score(y_test, clf_final.predict(X_test), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_test, vc.predict(X_test), average='macro')\nvc.voting = 'hard'\nglobal_score_hard = f1_score(y_test, vc.predict(X_test), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"71dae26394955fcf940feea1c9ba1a5bcf134ce9"},"cell_type":"code","source":"ranked_features = feature_importance(clf_final, X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5664db878261ee78c484aa640e0898bff89876bc"},"cell_type":"code","source":"extra_drop_features = ranked_features[116:]\nextra_drop_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78749eec7f69bcc8c587278a2c1a43ac8b5832e3"},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"_uuid":"32bfd69fe130005cb88865399c460ac00c7b1574","trusted":false,"collapsed":true},"cell_type":"code","source":"y_subm = pd.DataFrame()\ny_subm['Id'] = test_ids","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"718054937aa849c23ca7c65483f8a52de6f6fd49","trusted":false,"collapsed":true},"cell_type":"code","source":"y_subm['Target'] = clf_final.predict(test[X_train.columns]) + 1\n\nvc.voting = 'soft'\ny_subm_soft = y_subm.copy(deep=True)\ny_subm_soft['Target'] = vc.predict(test[X_train.columns]) + 1\n\nvc.voting = 'hard'\ny_subm_hard = y_subm.copy(deep=True)\ny_subm_hard['Target'] = vc.predict(test[X_train.columns]) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8258f28127f235e427a25f6824566a23df61b8af","trusted":false,"collapsed":true},"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()\n\nsub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_soft = 'submission_soft_LGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_hard = 'submission_hard_LGB_{:.4f}_{}.csv'.format(global_score_hard, str(now.strftime('%Y-%m-%d-%H-%M')))\n\ny_subm_soft.to_csv(sub_file_soft, index=False)\ny_subm_hard.to_csv(sub_file_hard, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c6d7f945dec95a777b4221c5fe217c3eea24100","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}