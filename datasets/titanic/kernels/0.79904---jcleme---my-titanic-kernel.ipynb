{"cells":[{"metadata":{"_uuid":"778f20cc44fe7088d3e7c25551ef08068f574e2b"},"cell_type":"markdown","source":"# Introduction\n\nLike most people, the Titanic Challenge is my first Kaggle competition and my first machine learning project outside of econometrics courses. In this project I walk through the basics of dealing with a binary classification problem. I can't thank stackpoverflow and various data science blogs enough for learning how to tune hyperparameters using grid and random search. I can't remember all of the specific sites, but I have the basics down now and can't wait to apply it to a new project when I have the time."},{"metadata":{"_uuid":"d674e6ef564ae8a2809c71db55a68c058e38c78b"},"cell_type":"markdown","source":"# Setting the Environment (loading up the basics)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n%matplotlib inline\nimport seaborn as sns # data visualization\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Importing the data"},{"metadata":{"trusted":true,"_uuid":"a4ff94129415e459020d37988ad9d7d8e3da25f0"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d14845aeb096dec1a546126cdee840423639f4c"},"cell_type":"markdown","source":"# Data Exploration\n\nThis section looks at the shape of the data (how many observations and variables) and looks at basic desciptions of the values for each variable."},{"metadata":{"trusted":true,"_uuid":"c646e0a7b85607a08ed20377dbb20a30735ef8ec"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c61d59ac80b278b349b3eafa136c45bd4629de"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c75d9ed54c76d31a7bdabefe478eda3186f8a8"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10cc41a488b88b94ec8bad2c76601cbdbbca4a31"},"cell_type":"code","source":"train.describe(include = ['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ddda9e394a6ddc9974a83ab300969edd5653558"},"cell_type":"code","source":"train.info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e7f27b82f084888aa0be4b58071da7ee4c88f9c"},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a1830fb61f45afd0c377bbe4033c00c17e7467e"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9ab608b1ad73699779a1c32de22429a2d57539a"},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fa7132e255d6cacb8a7d5544f578a53ed100f65"},"cell_type":"code","source":"test.describe(include = ['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c240b3df26d313ecf14814bc5e1500c76d7582"},"cell_type":"code","source":"test.info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b89bd316a44e9bfadc90ad2fa5943f5b7bcc73c1"},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55f7546a13c226e310624fb165860e6a36223595"},"cell_type":"markdown","source":"# Finding Relationships between Independent Variables and Survival"},{"metadata":{"_uuid":"e4160a5e4f4a17ed7fa5b1038851c6a1a13d56d7"},"cell_type":"markdown","source":"Quick note, PassengerId and Name are indentifiers and should not affect survival rating."},{"metadata":{"trusted":true,"_uuid":"7006289df063d03373fec7483764f6639cbb3aab"},"cell_type":"code","source":"survived = train[train.Survived == 1]\nnot_survived = train[train.Survived == 0]\nprint('Survived: %i (%.1f%%)' %(len(survived), float(100*len(survived)/(len(survived)+len(not_survived)))))\nprint('Did not Survive: %i (%.1f%%)' %(len(not_survived), float(100*len(not_survived)/(len(survived)+len(not_survived)))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0922ad7a9757d09580da9ac8e869d7a232b1b6da"},"cell_type":"markdown","source":"## Pclass vs. Survival"},{"metadata":{"trusted":true,"_uuid":"1ec850c8fcc4d57107c14efd07029159ffe1c06c"},"cell_type":"code","source":"print(train.Pclass.value_counts())\n# Gives the number of people in each class\n\nprint(train.groupby('Pclass').Survived.value_counts())\n# Counts the number of people in each class who survived and did not survive\n\nprint(train[['Pclass', 'Survived']].groupby('Pclass', as_index = False).mean())\n# Counts the proportion of people in each class that survived (1 = Survived and 0 = Did Not Survive, so mean = proportion who survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e029435d09e38d760c2c4f6dee8839bea486ea8d"},"cell_type":"code","source":"#train.groupby('Pclass').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'Pclass', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a1fb121c053f925fd057d5ebeb4d5a87129dd4"},"cell_type":"markdown","source":"The results suggest that people in 1st class were most likely to survive, followed by those in 2nd class. Those in 3rd class were the least likely to survive.\n\nPerhaps this is a result of 1st class being the closest to the deck, followed by 2nd class, while 3rd class is near the bottom of the ship."},{"metadata":{"_uuid":"bda4ce58a5ea2ca28a3052c33a719ed1ed5c444e"},"cell_type":"markdown","source":"## Sex vs. Survival"},{"metadata":{"trusted":true,"_uuid":"89b3fbba2d065c8d13df367f947a95b6f6da8f9a"},"cell_type":"code","source":"print(train.Sex.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('Sex').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('Sex').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"687c8345b2117e791895af42d75433d9bf24cbfc"},"cell_type":"code","source":"#train.groupby('Sex').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'Sex', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68579a8a50ce4ec1b1ddef61be134a43acbc0b94"},"cell_type":"markdown","source":"Females are far more likely than males to have survived the Titanic. I hypothesize this is due to the \"women and children\" first policy."},{"metadata":{"_uuid":"79d91d175264188aae700f4a0ca44e82534e83e7"},"cell_type":"markdown","source":"## Age vs. Survival"},{"metadata":{"trusted":true,"_uuid":"6ef2ecf3bb8cade78688044788a08cda2ab70f9a"},"cell_type":"code","source":"train.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"126d9ecbed1262ac370eeb482968f4c4ce70cad7"},"cell_type":"code","source":"age = train.Age.dropna()\nsns.distplot(age, bins = 25, kde = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3776e4ddbe3efdc2b231564cdb079ef6e6fa3fa4"},"cell_type":"markdown","source":"The passengers on the Titanic tended to be younger adults in their 20s and 30s. Among the children, the age skews to very young."},{"metadata":{"trusted":true,"_uuid":"744aa3a3f67fb485b4c01ed264917389633619e6"},"cell_type":"code","source":"train['AgeBand'] = np.where(train.Age <= 16, 1, \n                            np.where((train.Age > 16) & (train.Age <= 32), 2, \n                                     np.where((train.Age > 32) & (train.Age <= 48), 3, \n                                              np.where((train.Age > 48) & (train.Age <= 64), 4, \n                                                      np.where((train.Age > 64) & (train.Age <= 80), 5, False)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55ff0778b8c4c91b0a563337f0edb85d8d03af99"},"cell_type":"code","source":"print(train.AgeBand.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('AgeBand').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('AgeBand').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d93375e254c386aaee3c16b3a07fbfb62c50b51f"},"cell_type":"code","source":"train.groupby('AgeBand').Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4a12e296a3edf5038f9e6904de499efd36e570b"},"cell_type":"code","source":"#train.groupby('AgeBand').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'AgeBand', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a948ed770ab3575bff9f3466b8a3db78cff29e63"},"cell_type":"markdown","source":"People under 16 were the most likely group to survive, supporting the \"women and children\" hypothesis. The AgeBand variable is used only for this analysis and will not be used in the final model."},{"metadata":{"trusted":true,"_uuid":"293f45a36331f56e5ca3b95df051a52414d2c23e"},"cell_type":"markdown","source":"## Sibsp vs. Survival"},{"metadata":{"trusted":true,"_uuid":"1b9be94a238551f5dcd20209d4cd5f594d1312df"},"cell_type":"code","source":"train.SibSp.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bc4959b48ee01ebaf24cc35df45ce56a38910f4"},"cell_type":"markdown","source":"Most people came without a spouse or sibling."},{"metadata":{"trusted":true,"_uuid":"2fbb8bbac9152292bb33e89c4f1ca2af8c132171"},"cell_type":"code","source":"print(train.SibSp.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('SibSp').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('SibSp').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd7def7f75bdbec2ec4d5f385de5c97ce10cd9a"},"cell_type":"markdown","source":"People with 2 or fewer siblings or spouses on the Titanic (assuming a big chunk of the people with 1 SibSp are spouses) were more likely to survive than those with many people. I hypothesize this is due to it being difficult to round up a big group in a crisis. But it does appear better to have 1 or 2 siblings and or spouse with you than to be alone."},{"metadata":{"trusted":true,"_uuid":"41f4fc24326d3fc1a72918b69428bf078c26f997"},"cell_type":"code","source":"#train.groupby('SibSp').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'SibSp', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8383a0dc3328e1a831a80660f059e9413958b232"},"cell_type":"markdown","source":"## Parch vs. Survival"},{"metadata":{"trusted":true,"_uuid":"8a3fa4deb131524dc6e1d7ce4fbfa24192fa289e"},"cell_type":"code","source":"train.Parch.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b4841b0d4c67db6c54299a0eca5c77472ba71ac"},"cell_type":"markdown","source":"Most people came without their parents and children."},{"metadata":{"trusted":true,"_uuid":"d64bdd32bc6312937a3155b4c172adfdd76e8876"},"cell_type":"code","source":"print(train.Parch.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('Parch').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('Parch').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70fcc6b713bb0a50d086fd7648d749a47a35f97a"},"cell_type":"markdown","source":"Having 3 or more parents or children with you is associated with high survival rates, but some number combinations have small sample sizes so there is more variation in those groups."},{"metadata":{"trusted":true,"_uuid":"d1b07a5afd04d7e7740bd1a9611e26f40c73345e"},"cell_type":"code","source":"#train.groupby('Parch').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'Parch', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f63d5baa77d68fa64e9077512eb585edbfa4f1c"},"cell_type":"markdown","source":"## Embarked Point vs. Survival"},{"metadata":{"trusted":true,"_uuid":"407961476ea0788bee7a9c9c69c48627955cccf1"},"cell_type":"code","source":"train.Embarked.describe(include = ['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d027a83d177b099d92e5179bd8ca96c357804b37"},"cell_type":"code","source":"print(train.Embarked.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('Embarked').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('Embarked').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdba371aa51e818e172b906f78a040e3e7a964ea"},"cell_type":"markdown","source":"Those who embarked at South Hampton and those who embarked at Cherbourg (more variation here) were the most likely to survive."},{"metadata":{"trusted":true,"_uuid":"a93a77d5686314a1d867bd88ece78fbc42cbc402"},"cell_type":"code","source":"#train.groupby('Embarked').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'Embarked', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d0c6c247e5dde1b7dfdc6b223c0480df88db54f"},"cell_type":"markdown","source":"## FamilySize vs. Survival"},{"metadata":{"trusted":true,"_uuid":"987b593c64d3c819c0a37cf461e17c4b10afd260"},"cell_type":"code","source":"train['FamilySize'] = train.SibSp + train.Parch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99bdcb85e4adaa571cd898e667d5731c71fdb856"},"cell_type":"code","source":"train.FamilySize.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4bec1c5233b1e071cee107c05b40518d9e38c84"},"cell_type":"markdown","source":"The plurarlity of people on the Titanic were alone, at least in this sample."},{"metadata":{"trusted":true,"_uuid":"462185249289288824e771792766f40b04463b42"},"cell_type":"code","source":"print(train.FamilySize.value_counts())\n# Displays the number of each sex\n\nprint(train.groupby('FamilySize').Survived.value_counts())\n# Displays the number of each sex that survived and did not survive\n\nprint(train.groupby('FamilySize').Survived.mean())\n# Displays the proportion of each sex that survived or did not survive","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe68aa7a2a4997433c879cc589aab6aaf8aae9ba"},"cell_type":"markdown","source":"Having 3 or fewer family members is associated with higher likelihood of survival, and being alone is more dangerous than having another person or 2 people with you."},{"metadata":{"trusted":true,"_uuid":"90d29f232ce9c67c0c68a2e065871eeac4c286a3"},"cell_type":"code","source":"#train.groupby('FamilySize').Survived.mean().plot(kind = 'bar')\nsns.barplot(x = 'FamilySize', y = 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3732ee1debee0b3b47e89e849498914a27e8d480"},"cell_type":"markdown","source":"## Fare"},{"metadata":{"trusted":true,"_uuid":"fe144de537952afb000029c8ddcd020953b03f7a"},"cell_type":"code","source":"train.Fare.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"228e6d4f3da75cc3a04fc29a67a21a96e4e6dd79"},"cell_type":"code","source":"sns.distplot(train.Fare, bins = 15, kde = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36ce4999cbc2c575b5951995ed6fd8f708c85953"},"cell_type":"markdown","source":"I took the natural log of it to try and make the data less spread out. You can see above that the mass of the distribution is on the left side, but the mean is a litte off the y-axis like a log-normal distribution."},{"metadata":{"trusted":true,"_uuid":"2170d2508fc7f1f3719432b93104adccc0795e17"},"cell_type":"code","source":"train['logFare'] = np.where(train.Fare != 0, np.log(train.Fare), train.Fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07fa3e174da086038c169859cb4433d0b3be5146"},"cell_type":"code","source":"sns.distplot(train.logFare, bins = 15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9340681446198a087d46db4dbb322b51154ecc5d"},"cell_type":"markdown","source":"# Creating a Model"},{"metadata":{"_uuid":"abc9bbc45fb93ebed870c3ace033343969ac367d"},"cell_type":"markdown","source":"## Data Pre-Processing"},{"metadata":{"_uuid":"c86d42576a7f22b478c849c74ff99d97e8b92dbd"},"cell_type":"markdown","source":"I reload and process the data here so I can skip to this section when I take breaks and come back to working on it."},{"metadata":{"trusted":true,"_uuid":"a169299b175de4bd5283e13bc18fef52156f3b1e"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n#Loads the data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#Creates the FamilySize and logFate variables\ntrain['FamilySize'] = train.SibSp + train.Parch\ntrain['logFare'] = np.where(train.Fare != 0, np.log(train.Fare), train.Fare)\ntest['FamilySize'] = test.SibSp + test.Parch\ntest['logFare'] = np.where(test.Fare != 0, np.log(test.Fare), test.Fare)\n\n#Puts the features that should have no effect on survival in a list\ncols_to_drop = ['Name', 'Ticket', 'Cabin', 'PassengerId']\n\n#Drops the aforementioned features\ntrain = train.drop(cols_to_drop, axis=1)\nX_test = test.drop(cols_to_drop, axis=1)\n\n#Creates boolean variables for categorical features\ntrain_data = pd.get_dummies(train)\nX_test = pd.get_dummies(X_test)\n\n#Creates the training feature matrix and the training target vector\nX_train = train_data.drop('Survived', axis=1)\ny_train = train_data.Survived\n\n#Replaces missing values with averages\nmy_imputer = SimpleImputer()\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cfb64c03142007174f4c099a1d3d2270976efef"},"cell_type":"markdown","source":"## Importing Model Modules"},{"metadata":{"trusted":true,"_uuid":"51e1a07d0aa4b7ab68188dac72bc5a32bf35dcc0"},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n#Splits the training data up for use to score model accuracy and model selection.\ntrain_X, test_X, train_y, test_y = train_test_split(X_train, y_train, train_size = 0.7, test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4270ec1324d728654cc364dba405bbc9f3fde2c"},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"_uuid":"07fdf0db3d1718646c14fa7c62312bce093d258e"},"cell_type":"markdown","source":"Below the hyperparameters are selected."},{"metadata":{"trusted":true,"_uuid":"a7af1104dce71a237667fef0e3aad29a7f12f9c9"},"cell_type":"code","source":"#Defines a function to return the best parameters for the SVC model\ndef svc_param_selection(X, y, nfolds):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(svm.SVC(kernel='linear'), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\nsvc_param_selection(X_train, y_train, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1574b6ff74f0b785952c95f722d9df7d734946e"},"cell_type":"markdown","source":"Used an article (https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a0) to learn how to choose the hyperparameters for the SVC mocel. They are C = 0.01 and gamma = 0.001\n\nBelow the model is tested for accuracy using cross-validation with 10 folds."},{"metadata":{"trusted":true,"_uuid":"673c634c682412ae9870e96bbbab5239d31c7b47"},"cell_type":"code","source":"my_svc_model = svm.SVC(C = 0.01, kernel ='linear', gamma = 0.001)\nmy_svc_model.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_svc_model, X_train, y_train, cv=kfold)\nprint(\"SVM Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8929c497e07d8ab57e0902e4e0a9a51de06dcfe"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_uuid":"bac9f383d0bfbceeb924d450cec33880f55afdaf"},"cell_type":"markdown","source":"Below I use random search instead of grid search to do the hyperparameter tuning because a grid search would take forever."},{"metadata":{"trusted":true,"_uuid":"2f65493e09eb86bcece5ee1b88052cb61e288e77"},"cell_type":"code","source":"# This is the number of trees starting at 200 and going to 2000 in increments of 10\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Sets a maximum number of levels in each tree to avoid overfitting\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Sets a minimum number of observations to allow a node to make a split\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\nmy_rf_model = RandomForestClassifier()\nmy_rf_model = RandomizedSearchCV(estimator = my_rf_model, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=7, n_jobs = -1)\nmy_rf_model.fit(X_train, y_train)\nmy_rf_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab63900c2581a6ae1a58501d8254d04ad238254"},"cell_type":"code","source":"my_forest_model = RandomForestClassifier(n_estimators = 200,\n                                         min_samples_split = 5,\n                                         min_samples_leaf = 4,\n                                         max_features = 'auto',\n                                         max_depth = 80,\n                                         bootstrap = True)\nmy_forest_model.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_forest_model, X_train, y_train, cv=kfold)\nprint(\"Random Forest Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"438e82052622d8242bcd510d60fd90e2c3192b01"},"cell_type":"markdown","source":"### K-Nearest Neighbors Classifier"},{"metadata":{"_uuid":"034f30c93042a4dcffa3103c17b7fcf474399d75"},"cell_type":"markdown","source":"Below the hyperparameters are selected."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c31994f2bf612c2bf032fd799f7a0d64e77faf95"},"cell_type":"code","source":"#Creates a list of possible ks from 1 to 30\nk_range = list(range(1, 31))\n#Creates a list of 2 possible weighting options\nweight_options = ['uniform', 'distance']\n#Creates a dictionary containing the k_range and weight_options that is the parameter grid\nparam_grid = {'n_neighbors': k_range, 'weights': weight_options}\n\nmy_knn_model = KNeighborsClassifier(algorithm = 'brute')\nclf = GridSearchCV(my_knn_model, param_grid, cv=5)\nclf.fit(X_train, y_train)\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"779be37c676e158a2d54c30722eed0b213a54628"},"cell_type":"markdown","source":"Below the model is tested for accuracy using cross-validation with 10 folds."},{"metadata":{"trusted":true,"_uuid":"6077703219c6687969ae7efa124cb7dd9c444f0c"},"cell_type":"code","source":"my_knn_model = KNeighborsClassifier(n_neighbors = 13, weights = 'distance')\nmy_knn_model.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_knn_model, X_train, y_train, cv=kfold)\nprint(\"Knn Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db647bceefa6c8023f1784ba715faacb049f3400"},"cell_type":"markdown","source":"### Gaussian Naive Bayes Classifier"},{"metadata":{"trusted":true,"_uuid":"ee403e9986dc12ce116b13c3c5ae5cb9467d80b9"},"cell_type":"code","source":"my_gnb_model = GaussianNB()\nmy_gnb_model.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_gnb_model, X_train, y_train, cv=kfold)\nprint(\"GNB Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9b0f4d629dcf297d9205c4c3a007a197d35f4b5"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"adb8f8092a8b2da1fab1dab36f51308ae76fc63f"},"cell_type":"code","source":"my_logit_model = LogisticRegression(solver = 'liblinear')\nmy_logit_model.fit(X_train, y_train)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_logit_model, X_train, y_train, cv=kfold)\nprint(\"Logistic Regression Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12424919d36af5572dc650734ae3faaed09d408f"},"cell_type":"markdown","source":"### XTREME GRADIENT BOOSTED TREE"},{"metadata":{"_uuid":"a729b248969791d98a14383fae8410a015370f97"},"cell_type":"markdown","source":"A grid search is used to find the best learning rate, max depth and min child weight."},{"metadata":{"trusted":true,"_uuid":"f301710e2690a9d72ac77d40fc090b392e883e9c"},"cell_type":"code","source":"my_xgb_model = XGBClassifier()\n\nparameters = {'nthread':[4],\n              'objective':['binary:logistic'],\n              'learning_rate': [0.03, 0.04, 0.05, 0.06, 0.07, 0.08], \n              'max_depth': [5, 6, 7, 8],\n              'min_child_weight': [9, 10, 11, 12, 13],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [10], #kept small so the grid search doesn't take too long\n              'missing':[-999],\n              'seed': [7]}\n\nclf = GridSearchCV(my_xgb_model, parameters, n_jobs=5, \n                   cv=StratifiedKFold(n_splits=5), \n                   scoring='roc_auc',\n                   verbose=2, refit=True)\n\nclf.fit(X_train, y_train)\n\nprint(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4e5233b65890bfc8313ea4c511e2203e6de44c"},"cell_type":"markdown","source":"By a little experimentation, the best number of estimators appears to be 125."},{"metadata":{"trusted":true,"_uuid":"5899598b287bd8aa6978b4fd2bff64d0875dd285"},"cell_type":"code","source":"my_xgb_model = XGBClassifier(colsample_bytree = 0.7, \n                             learning_rate = 0.07, \n                             max_depth = 5, \n                             min_child_weight = 9, \n                             missing = -999, \n                             n_estimators = 125, \n                             nthread = 4, \n                             objective = 'binary:logistic', \n                             seed = 7, \n                             silent = 1, \n                             subsample = 0.8)\nmy_xgb_model.fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(test_X, test_y)], verbose = False)\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(my_xgb_model, X_train, y_train, cv=kfold)\nprint(\"XGBTree Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51c031ed6cb9a12e462ed9e1f69d7ff9047cc4c5"},"cell_type":"markdown","source":"## Final Model (Going with a Gradient Boosted Tree, but it is a close call between it and the Random Forest. The Gradient Boosted Tree has a smaller standard deviation in accuracy score, which is why I made this decision in favor of using XGBoost.)"},{"metadata":{"_uuid":"cb7ec38aa2c82c20f617e6e365501ee41aaa9bc7"},"cell_type":"markdown","source":"I reload everything so I can skip down here and mess around without needing to find and run select cells before running this one."},{"metadata":{"trusted":true,"_uuid":"0954b482377aa862ac7722b95a62bd47ca954075"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain['FamilySize'] = train.SibSp + train.Parch\ntrain['logFare'] = np.where(train.Fare != 0, np.log(train.Fare), train.Fare)\n\ncols_to_drop = ['Name', 'Ticket', 'Cabin', 'PassengerId']\n\ntrain.Pclass = train.Pclass.astype(str)\ntrain = train.drop(cols_to_drop, axis=1)\ntest.Pclass = test.Pclass.astype(str)\nX_test = test.drop(cols_to_drop, axis=1).copy()\n\nX_test['FamilySize'] = X_test.SibSp + X_test.Parch\nX_test['logFare'] = np.where(X_test.Fare != 0, np.log(X_test.Fare), X_test.Fare)\n\ntrain_data = pd.get_dummies(train)\nX_test = pd.get_dummies(X_test)\n\nX_train = train_data.drop('Survived', axis=1)\ny_train = train_data.Survived\n\nmy_imputer = SimpleImputer()\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.fit_transform(X_test)\n\ntrain_X, test_X, train_y, test_y = train_test_split(X_train, y_train, train_size = 0.7, test_size = 0.25, random_state = 0)\n\nmy_xgb_model = XGBClassifier(colsample_bytree = 0.7, \n                             learning_rate = 0.07, \n                             max_depth = 5, \n                             min_child_weight = 9, \n                             missing = -999, \n                             n_estimators = 125, \n                             nthread = 4, \n                             objective = 'binary:logistic', \n                             seed = 1337, \n                             silent = 1, \n                             subsample = 0.8)\nmy_xgb_model.fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(test_X, test_y)], verbose = False)\n\nmy_predictions = my_xgb_model.predict(X_test)\n\njcleme_submission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": my_predictions})\n\njcleme_submission.to_csv('jcleme_xgb_submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}