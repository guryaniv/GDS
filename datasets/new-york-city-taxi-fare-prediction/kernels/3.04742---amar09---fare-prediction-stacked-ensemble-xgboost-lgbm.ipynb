{"cells":[{"metadata":{"_uuid":"ea5141c720ad7d3d44b32cbf59a25fc9412ab933"},"cell_type":"markdown","source":"Here, we are going to perform data cleaning, feature engineering, data visualization for which we will prefer seaborn and will train Random Forest, XGBoost & LGBM models then will compare and apply GridSearchCV parameter selection on the better one to check if it does any more improvement and at last will ensemble best predictions.\n\nThanks to kagglers, I am going through many splendid kernels and learning new techniques and approaches and yes this is my first Kaggle competition submission, will be glad to have your suggestions :)"},{"metadata":{"_uuid":"47ece6d40c9a10e12114e2dd80c7793880cfcef4"},"cell_type":"markdown","source":"**Import required libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # for plot visualization\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom scipy.stats import skew\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ccba1224322720e923fe7dc3efcadcac393308"},"cell_type":"code","source":"plt.figure(figsize=(8, 5), dpi=80)\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ac26968d2b53b3f5fdbdf8898580e2124e84182"},"cell_type":"markdown","source":"**Read test and train dataset**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test_dataset = pd.read_csv('../input/test.csv')\ntrain_dataset = pd.read_csv('../input/train.csv', nrows=2_000_000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f20e0790e1ac397aebdd02dabba499543105964b"},"cell_type":"markdown","source":"Let's check first and last 5 records of train_dataset."},{"metadata":{"trusted":true,"_uuid":"01bc985ec8c17ffbbc6d3f7d7eebe6e712c309af"},"cell_type":"code","source":"train_dataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e5750f9b1a1786ff3292c6889d66b482ab05b31"},"cell_type":"code","source":"train_dataset.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf26223612153cd2b09ac564b66246cbe448752b"},"cell_type":"code","source":"train_dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea6b5b4928d063912025f68a2450e86a2e02c12"},"cell_type":"markdown","source":"<h2>Handle dataset memory consumption</h2>"},{"metadata":{"trusted":true,"_uuid":"fec0147240865bdb9e3845304aa75c5aa8e6ac13"},"cell_type":"code","source":"# lets check current memory usage status\ntrain_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"630756967d2a0fc51e119d76bbc68bb702f4754d"},"cell_type":"markdown","source":"So, here we have 3 type of feature values - float64, int64 and object. Let's check how much memory they acquire."},{"metadata":{"trusted":true,"_uuid":"fe1db45745b558b63a66541a5eb1656e263af0c1"},"cell_type":"code","source":"for dtype in ['float','int','object']:\n    selected_dtype = train_dataset.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a733a977e052204efccc67aa65ac780424fd0d51"},"cell_type":"markdown","source":"So object consumes most of the space. We have already observed that we have two object columns, 'pickup_datetime' and 'key', which is nothing but the same pickup_datetime value, So let's drop key column"},{"metadata":{"trusted":true,"_uuid":"5e9224fe4d6bc58ac530a810e861b769ed8c9225"},"cell_type":"code","source":"train_dataset.drop(labels='key', axis=1, inplace=True)\ntest_dataset.drop(labels='key', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fec36ac3088d20bfd3dbefa41428186bd31b9fc4"},"cell_type":"code","source":"# Let's again check the memory usage.\ntrain_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64920cae1ee31ea7d83da4d230d73cb1067d6625"},"cell_type":"markdown","source":"Size is considerably reduced now. Later we will change the type of pickup_datetime as well."},{"metadata":{"_uuid":"46ba40cd31b7388c3af6e2cb92477e24d76b1ebf"},"cell_type":"markdown","source":"One more thing we can do - change the type of numeric values to subtypes that consumers less memory. For Eg : passenger_count's type is **int64**, which takes 8 bytes and we know that maximum one digint (non-negative) is going to get stored here for which uint is enough."},{"metadata":{"trusted":true,"_uuid":"6a2b3e591a83e240f0f7eea15bb0a2ce127e3889"},"cell_type":"code","source":"train_dataset.passenger_count = train_dataset.passenger_count.astype(dtype = 'uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21e583c5fc047a4ee3d4861dfa506f757d609cbb"},"cell_type":"code","source":"train_dataset.pickup_longitude = train_dataset.pickup_longitude.astype(dtype = 'float32')\ntrain_dataset.pickup_latitude = train_dataset.pickup_latitude.astype(dtype = 'float32')\ntrain_dataset.dropoff_longitude = train_dataset.dropoff_longitude.astype(dtype = 'float32')\ntrain_dataset.dropoff_latitude = train_dataset.dropoff_latitude.astype(dtype = 'float32')\ntrain_dataset.fare_amount = train_dataset.fare_amount.astype(dtype = 'float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"028c8691cb87e39bfc12e8b804fe13342cfaa5ea"},"cell_type":"code","source":"# let's again check the memory_usage report\ntrain_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76942c9670681255d19f91d046ab3402616d7aaf"},"cell_type":"markdown","source":"Great! we have reduced the size of dataset more that 50%."},{"metadata":{"_uuid":"7ef9af9494f99bcbcb774d4d1380b7778172ca2a"},"cell_type":"markdown","source":"<h2>Check for anomalies in dataset</h2>"},{"metadata":{"_uuid":"ac1f397f08796f418ed43e14c71fb6281a66d48c"},"cell_type":"markdown","source":"**Let's check for 'null' in feature values**"},{"metadata":{"trusted":true,"_uuid":"35d702d55be19160f93828812d39f9274b4770e2"},"cell_type":"code","source":"train_dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d031afb9880b3bb6c535b329efb1b9f557fa62c"},"cell_type":"code","source":"print(f'Row count before drop-null operation - {train_dataset.shape[0]}')\ntrain_dataset.dropna(inplace = True)\nprint(f'Row count after drop-null operation - {train_dataset.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ff92767fcb3f135534df2f17e568b671d28c7c6"},"cell_type":"markdown","source":"**Let's change pickup_datetime type to datetime**"},{"metadata":{"trusted":true,"_uuid":"3ab309ea3142cab6e082e6d157e425e2995c0c49"},"cell_type":"code","source":"train_dataset['pickup_datetime'] = pd.to_datetime(arg=train_dataset['pickup_datetime'], infer_datetime_format=True)\ntest_dataset['pickup_datetime'] = pd.to_datetime(arg=test_dataset['pickup_datetime'], infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcea7046c76a1ad7e2e653eda1cdeae566dd043d"},"cell_type":"code","source":"train_dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"504fe79502a51b096e230f53bbc618e641496620"},"cell_type":"markdown","source":"Great! now it's type is changed to the appropriate one and we can generate many more important features from this like day, month, year, weekday, hour etc. Let's do that."},{"metadata":{"trusted":true,"_uuid":"37b318d71f0f1488f3fab10d074374a2767b1a64"},"cell_type":"code","source":"def add_new_date_time_features(dataset):\n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    dataset['day_of_week'] = dataset.pickup_datetime.dt.dayofweek\n    \n    return dataset\n\ntrain_dataset = add_new_date_time_features(train_dataset)\ntest_dataset = add_new_date_time_features(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f58912963f415feb76e29ebc2a81ee1a10465b"},"cell_type":"markdown","source":"**Let's check for outliers**"},{"metadata":{"trusted":true,"_uuid":"f0e48a0c692945786b1e4374f584ba5fc5cea74a"},"cell_type":"code","source":"train_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb891f7dc1a6292c68d552dafbaffaa77faee77d"},"cell_type":"markdown","source":"Co-ordinate values are varying too much. We can take test_dataset coordinate min and max limit to drop the outliers."},{"metadata":{"trusted":true,"_uuid":"2653a1cee0451a6c11e2925222d10c0107256442"},"cell_type":"code","source":"print(f'Rows before removing coordinate outliers - {train_dataset.shape[0]}')\n\ntrain_dataset = train_dataset[train_dataset.pickup_longitude.between(test_dataset.pickup_longitude.min(), test_dataset.pickup_longitude.max())]\ntrain_dataset = train_dataset[train_dataset.pickup_latitude.between(test_dataset.pickup_latitude.min(), test_dataset.pickup_latitude.max())]\ntrain_dataset = train_dataset[train_dataset.dropoff_longitude.between(test_dataset.dropoff_longitude.min(), test_dataset.dropoff_longitude.max())]\ntrain_dataset = train_dataset[train_dataset.dropoff_latitude.between(test_dataset.dropoff_latitude.min(), test_dataset.dropoff_latitude.max())]\n\nprint(f'Rows after removing coordinate outliers - {train_dataset.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0edced14a149bfb807d6e4b72715f4c38669733e"},"cell_type":"code","source":"train_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e035b312dcb5cfbc64a9d19fe1c3d4da261c4b36"},"cell_type":"markdown","source":"Seems fine now. We have also noted that minimum fare_amount is in negative which is not possible, at the same time we can also notice that maximum fare_amount is 500, which is not very resonable. So lets count rows where fare_amount is not between 0 and 350."},{"metadata":{"trusted":true,"_uuid":"f667f9f6237fdb3ac679638353cda7dd279f6fa9"},"cell_type":"code","source":" train_dataset.fare_amount[(train_dataset.fare_amount <= 0) | (train_dataset.fare_amount >= 350)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f62ad38e66c29617555389bb3c7358d576bdb51"},"cell_type":"code","source":"# Let's eliminate these rows\nprint(f'Row count before elimination - {train_dataset.shape[0]}')\ntrain_dataset = train_dataset[train_dataset.fare_amount.between(0, 350, inclusive=False)]\nprint(f'Row count after elimination - {train_dataset.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38e30df0e43172d127bc91e9a00bab210d600ba1"},"cell_type":"markdown","source":"Similarly, passanger_count also contains some outliers, idealy it should now contain values less than 1 or greater than 7"},{"metadata":{"trusted":true,"_uuid":"11df89fcef607b8c7c55cd599ab545f9ed0327af"},"cell_type":"code","source":"train_dataset.passenger_count[(train_dataset.passenger_count < 1) | (train_dataset.passenger_count > 8)].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e20203283db70004f5927795848020eff1e18484"},"cell_type":"code","source":"# Let's eliminate these rows\nprint(f'Row count before elimination - {train_dataset.shape[0]}')\ntrain_dataset = train_dataset[train_dataset.passenger_count.between(0, 8, inclusive=False)]\nprint(f'Row count after elimination - {train_dataset.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9dd819cc5ba81d826a01b0366aa955c37eb87a7"},"cell_type":"markdown","source":"**Calculate 'distance' between co-ordinates**"},{"metadata":{"_uuid":"e0e5c9063187cb440dc1c8c7bf78229e99123c0b"},"cell_type":"markdown","source":"Let's use pickup and dropoff co-ordinate features to add a new feature **distance**, later on it can be used as one of the significant predictor."},{"metadata":{"trusted":true,"_uuid":"45a2c29b432b2e843353195d49d26d1d42103713"},"cell_type":"code","source":"def degree_to_radion(degree):\n    return degree*(np.pi/180)\n\ndef calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n    \n    from_lat = degree_to_radion(pickup_latitude)\n    from_long = degree_to_radion(pickup_longitude)\n    to_lat = degree_to_radion(dropoff_latitude)\n    to_long = degree_to_radion(dropoff_longitude)\n    \n    radius = 6371.01\n    \n    lat_diff = to_lat - from_lat\n    long_diff = to_long - from_long\n\n    a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    \n    return radius * c\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92dd5db42269cee0442957bf297e2403ebba138"},"cell_type":"code","source":"train_dataset['distance'] = calculate_distance(train_dataset.pickup_latitude, train_dataset.pickup_longitude, train_dataset.dropoff_latitude, train_dataset.dropoff_longitude)\ntest_dataset['distance'] = calculate_distance(test_dataset.pickup_latitude, test_dataset.pickup_longitude, test_dataset.dropoff_latitude, test_dataset.dropoff_longitude)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a77eeb43a79faaa3facee7ea4dc4bb93d9ac9d9"},"cell_type":"markdown","source":"Let's check the train_dataset sorted by distance values."},{"metadata":{"trusted":true,"_uuid":"bdfad7481c1097e8a51ffe26a6f5a71e6b1eae70"},"cell_type":"code","source":"train_dataset.sort_values(by='distance')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9156c2d485e686cd143260d3580d344305ac830"},"cell_type":"markdown","source":"It appears that we have many 0 values in distance, let's check how many such rows are there"},{"metadata":{"trusted":true,"_uuid":"62d0b0d9b04988be4849f9557878a0418b7ffb05"},"cell_type":"code","source":"train_dataset.distance[(train_dataset.distance == 0)].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e29b9c55b7a113bc7bea73ec7b28f787d13c091"},"cell_type":"markdown","source":"Well, there are too many.  In above table If you check dropoff and pickup coordinates for same rows, then can notice that latitudes and longitudes are same but fare_amount is non-zero, which can be because of round trips, So it won't be a prudent choice to drop them.\n\nInstead we can remove records that have distance 0 with unequal pickup & dropoff coordinates"},{"metadata":{"trusted":true,"_uuid":"52e48e06fd2e48c6422195fca810380e2c473cca"},"cell_type":"code","source":"train_dataset[(train_dataset.pickup_latitude != train_dataset.dropoff_latitude) &\n              (train_dataset.pickup_longitude != train_dataset.dropoff_latitude) &\n              (train_dataset.distance == 0)].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24fac7bbbb4edc3f7084b18f8ddbec6322917499"},"cell_type":"markdown","source":"This is good that we don't have any such row. Lets move forward.\n\nWe should also consider adding few more features which can be distances from popular New York airports:\n\n1. John F. Kennedy International Airport (JFK)\n2. Newark Liberty International Airport (EWR)\n3. LaGuardia Airport (LGA)\n\nI didn't considered this initially but after going through some kernels got that it can be an important feature which can make a reasonable difference in model predictions. So lets proceed and add them."},{"metadata":{"trusted":true,"_uuid":"73b41136916088b4b0f8cfc894ab0d7845ea5e5b"},"cell_type":"code","source":"def add_distances_from_airport(dataset):\n    #coordinates of all these airports\n    jfk_coords = (40.639722, -73.778889)\n    ewr_coords = (40.6925, -74.168611)\n    lga_coords = (40.77725, -73.872611)\n\n    dataset['pickup_jfk_distance'] = calculate_distance(jfk_coords[0], jfk_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_jfk_distance'] = calculate_distance(jfk_coords[0], jfk_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_ewr_distance'] = calculate_distance(ewr_coords[0], ewr_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_ewr_distance'] = calculate_distance(ewr_coords[0], ewr_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_lga_distance'] = calculate_distance(lga_coords[0], lga_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_lga_distance'] = calculate_distance(lga_coords[0], lga_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    return dataset\n\n\ntrain_dataset = add_distances_from_airport(train_dataset)\ntest_dataset = add_distances_from_airport(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c432e63513014a496c48310a5a3e16c495013a3"},"cell_type":"markdown","source":"<h2>Data Visualizations</h2>"},{"metadata":{"_uuid":"96f162d61ad2906d2860c26ddbb476325ae4b66c"},"cell_type":"markdown","source":"**Unvariate distribution of fare_amount**"},{"metadata":{"trusted":true,"_uuid":"196504820a0d98007b7934fb6a22ce0dc6c2b9b7"},"cell_type":"code","source":"sns.distplot(a=train_dataset.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac316d790755341533c0472a39adfc38cba1350"},"cell_type":"markdown","source":"**Total distribution of 'fare_amount' and the 'distance'**"},{"metadata":{"trusted":true,"_uuid":"0c8d4f2b2333e116a043b9ed9518fb111d09c961"},"cell_type":"code","source":"sns.jointplot(x='distance', y='fare_amount', data=train_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"601cd1aa13e7d2cbb38154c49d2739b9ac69f4bb"},"cell_type":"markdown","source":"If you see the segment on bottom-right part, you will notice many points that bit uncommon, since their distance is too much but fare is not that much, I guess it might be because of some offer/discount was going on for long journies, not sure though. So let's plot more detailed plot including some other features as well."},{"metadata":{"trusted":true,"_uuid":"71b2a8cff0d4219a1ea3305be2cef33b0f834189"},"cell_type":"code","source":"g = sns.FacetGrid(train_dataset, col=\"year\", hue=\"passenger_count\")\ng.map(plt.scatter, \"distance\", \"fare_amount\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91e82e8dc944535d8239f0e71e0e8c2faf480142"},"cell_type":"markdown","source":"Interesting, here atleas one thing is very clear that these journies happend only in 2009 and 2010. Let's find out some more information about this segment"},{"metadata":{"trusted":true,"_uuid":"c926fa4c24bd672f7c28b1caed6199494b3d4232"},"cell_type":"code","source":"train_dataset[(train_dataset.distance>90) & (train_dataset.fare_amount<70)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cd6ece582f738df21f571b4b1e39ef7522245e5"},"cell_type":"markdown","source":"All I could find here is many similiar pickup coordinates (41.366138, -73.137390). There might be some better alternative got available after 2010 for larger journey So people must have started preferring that, again not sure ;)"},{"metadata":{"_uuid":"688f8824422b85dce07e091362a962d5918cbc53"},"cell_type":"markdown","source":"**Total journies on each weekdays (0 is for Monday)**"},{"metadata":{"trusted":true,"_uuid":"24fbd7efb973b933138d9256a1152cf68b24d587"},"cell_type":"code","source":"sns.countplot(x='day_of_week', data=train_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48e9572af67e48e67223b3de9b5f0108671ac25"},"cell_type":"markdown","source":"Means on Monday and Sunday people tends to travel less."},{"metadata":{"_uuid":"a627e70c05895a3698be4179deaabcacf39deb5b"},"cell_type":"markdown","source":"**Weekdays and Months impact on fare_amount**"},{"metadata":{"trusted":true,"_uuid":"9b4000e79c0e4274078eda004f1d7ae0c87549fd"},"cell_type":"code","source":"tc = train_dataset.pivot_table(index='day_of_week', columns='month', values='fare_amount')\nsns.heatmap(data = tc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75155386b08d28436a0d490f58ecc322dd61a878"},"cell_type":"markdown","source":"If you have observed in unvariate distribution of 'fare_amount', It is currently right skewed"},{"metadata":{"trusted":true,"_uuid":"51802c0c1dbdf5cbb758c9f2aeb76c61f0027367"},"cell_type":"code","source":"train_dataset['fare_amount'].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b435616a851deaf3102483e532e5f9bc3dbff1c"},"cell_type":"markdown","source":"We can certainly fix this by using log transformation"},{"metadata":{"trusted":true,"_uuid":"e4b71b73f60940d7efd6f9250cdf63e824a8b4ea"},"cell_type":"code","source":"train_dataset['fare_amount'] = np.log1p(train_dataset['fare_amount'])\nsns.distplot(train_dataset['fare_amount'], color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fefbaa3f43907512dcb9f7c443303ce8b9685f6b"},"cell_type":"markdown","source":"<h2>Apply Regression Algorithms</h2>"},{"metadata":{"_uuid":"9155b9a84624e335010c18ed4200fde85668d898"},"cell_type":"markdown","source":"Here we are going to try few best regression models like **Random Forest Regression**, **XGBoost** and **LGBM** models along with gridSearchCV parameter selection and will prefer best two for stacked ensemble technique.\n\nFor cross checking the performance of these models, we are spliting train dataset from out test_dataset itself, since it contains the 'fare_amount' column.\n\nLet's select the predictors from features, since we should not consider all features for training our model. "},{"metadata":{"trusted":true,"_uuid":"51578c99fd4aaf3daebf4acc7b0e55d9a23e0aef"},"cell_type":"code","source":"selected_predictors = [\n    'pickup_longitude', \n    'pickup_latitude', \n    'dropoff_longitude', \n    'dropoff_latitude',\n    'pickup_jfk_distance',\n    'dropof_jfk_distance',\n    'pickup_ewr_distance',\n    'dropof_ewr_distance',\n    'pickup_lga_distance',\n    'dropof_lga_distance',\n    'hour',\n    'month',\n    'year',\n    'distance'\n]\n\nX = train_dataset.loc[:, selected_predictors].values\ny = train_dataset.iloc[:, 0].values\nX_test_dataset = test_dataset.loc[:, selected_predictors].values\n\n# Since test_dataset is too large, So we are going to keep only 5% of the dataset in test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3781111b9c0553d92e858d397698f9671ed942f3"},"cell_type":"markdown","source":"<h3>Random Forest Regression</h3>"},{"metadata":{"trusted":true,"_uuid":"6cc4dc5237d89643f199e894c5e6901205e1993e"},"cell_type":"code","source":"rand_forest_regressor = RandomForestRegressor()\nrand_forest_regressor.fit(X_train, y_train)\n\ny_rand_forest_predict = rand_forest_regressor.predict(X_test)\nrandom_forest_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_rand_forest_predict)))\nprint(f' Random Forest Mean Squared Error - {random_forest_model_error}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6caa51aa3e3936e25d9edabbd4e818e1fa25cc86"},"cell_type":"markdown","source":"<h3>XGBoost Regression</h3>"},{"metadata":{"_uuid":"16eba7e765b2b66f5da8096577ec481b83a844c5"},"cell_type":"markdown","source":"**Parameter Estimation using GridSearchCV**"},{"metadata":{"_uuid":"7a72bbf55ac422b07a253db4d4ad5605950e0fb3"},"cell_type":"markdown","source":"Lets use GridSearchCV for best parameter selection in XGBoost. We are going to provide three parameter sets *learning_rate*, *max_depth* & *n_estimators*, so that we can re-train this models with new best fit parameters."},{"metadata":{"trusted":true,"_uuid":"56d69b0d86e8d6e0200f2f8bf32f5f5ca642ef02"},"cell_type":"code","source":"# parameters = {\n#                 'learning_rate': [0.07, 0.1, 0.3],\n#                 'max_depth': [3, 5, 7],\n#                 'n_estimators': [200, 400, 500]\n#             }\n\n# XGB_hyper_params = GridSearchCV(estimator=XGB_regressor, param_grid=parameters, n_jobs=-1, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b0657572d71515125db61cdc86713b7161d0af7"},"cell_type":"markdown","source":"We are using only first 50,000 records here, otherwise it will take too much time to complete execution."},{"metadata":{"trusted":true,"_uuid":"1ce754da1f8cac8707685695a9c4efacb76a870a"},"cell_type":"code","source":"# XGB_hyper_params.fit(X_train[:50_000], y_train[:50_000])\n# # find out the best hyper parameters\n# XGB_hyper_params.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca630b624f6b8a3fdd0510f9a92cdff8fa5238bb"},"cell_type":"markdown","source":"(Commented out the above code, since it takes lots of time while commiting)\n\nNow we have best fit parameter values for XGBoost model, So lets go ahead and train this model again."},{"metadata":{"trusted":true,"_uuid":"8ce35f8a216b2528a9ebba818a701b668e888e5a","scrolled":true},"cell_type":"code","source":"XGB_model = XGBRegressor(learning_rate=0.3, max_depth=6, n_estimators=500)\nXGB_model.fit(X_train, y_train)\ny_XGB_predict = XGB_model.predict(X_test)\n\nXGB_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_XGB_predict)))\n\nprint(f'XGBoost Mean Squared Error - {XGB_model_error}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cca979e2d2daaf3c018418598af3ab28f07159cc"},"cell_type":"code","source":"# let's plot feature_importance again and check if there is any difference or not.\nsns.barplot(y=list(train_dataset.loc[:, selected_predictors].columns), x=list(XGB_model.feature_importances_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c95fdf57dfb87760303127143070e9cf8dc5b1d7"},"cell_type":"markdown","source":"<h3>LightGBM</h3>"},{"metadata":{"trusted":true,"_uuid":"304a974fa59bcf00470560c53fba2e0771fb795b"},"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=35, n_estimators=300)\n\nlgb_model.fit(X_train, y_train)\ny_LGB_predict = lgb_model.predict(X_test)\n\nLGB_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_LGB_predict)))\n\nprint(f'LGBM Mean Squared Error - {LGB_model_error}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e968be0676030a7972210c6f93b32fa8f4f45314"},"cell_type":"markdown","source":"<h3>Stacked Ensemble</h3>"},{"metadata":{"_uuid":"aa4785b2407375e90f268e93a3d7d18540302c45"},"cell_type":"markdown","source":"Here we are going to taked weighted average (as per mean squared error score) of XGBoost and LGBM."},{"metadata":{"trusted":true,"_uuid":"12579c0d40e8237843260ce0ddb98f84576dcbcd"},"cell_type":"code","source":"# ensembled prediction over splitted test data\nensembled_prediction = (0.5*np.expm1(y_XGB_predict))+(0.5*np.expm1(y_LGB_predict))\nensembled_prediction_error = sqrt(mean_squared_error(np.expm1(y_test), ensembled_prediction))\n\nprint(f'Ensembled Mean Squared Error - {ensembled_prediction_error}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56452d91210ddd9ade638d633c534062bedb8ce7"},"cell_type":"markdown","source":"Better, now we should make prediction using stacked XGBoost and LGBM models."},{"metadata":{"_uuid":"424bc7c9fbfef57c25609273012510464a88c2de"},"cell_type":"markdown","source":"<h2>Fare Prediction on test_dataset and final submission</h2>"},{"metadata":{"trusted":true,"_uuid":"742eca02ba2dfe08b04aed306c1cd56f0b94ad8e"},"cell_type":"code","source":"# making prediction using test_dataset predictors\ny_XGB_predict = np.expm1(XGB_model.predict(X_test_dataset))\n\n# submitting our predictions\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = y_XGB_predict\nsubmission.to_csv('xgb_submission.csv', index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb14cad478fe7a7cc56e856f3ed9dde281101eda"},"cell_type":"code","source":"# making prediction using test_dataset predictors\ny_LGB_predict = np.expm1(lgb_model.predict(X_test_dataset))\n\n# submitting our predictions\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['fare_amount'] = y_LGB_predict\nsubmission.to_csv('lgbm_submission.csv', index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca3d1cae78d9d01142293f0ffd7e1cb07dc4e05a"},"cell_type":"code","source":"# making prediction using test_dataset predictors\n# y_rand_forest_predict = np.expm1(rand_forest_regressor.predict(X_test_dataset))\n\n# # submitting our predictions\n# submission = pd.read_csv('../input/sample_submission.csv')\n# submission['fare_amount'] = y_rand_forest_predict\n# submission.to_csv('random_forest_submission.csv', index=False)\n# submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"202dbd1bd7a2e9602df7b4a6d21c5c59ef18f0ff"},"cell_type":"code","source":"# submitting our predictions\nensembled_prediction = (0.5*y_XGB_predict)+(0.5*y_LGB_predict)\nsubmission.to_csv('ensembled_submission.csv', index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01dc0ce9560473884eabffefa27938050b99c83a"},"cell_type":"markdown","source":"Looking good.\n\nWill do few more improvements later and update ;)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}