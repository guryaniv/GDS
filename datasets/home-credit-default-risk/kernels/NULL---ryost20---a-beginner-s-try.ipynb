{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**First Real Competition**\nThis is my first real competition that I will be entering and working on. I've worked on the Titanic and the Housing competitions in the past. I'm new both to programming and data science, but I figure the best way to learn is to just do it. I'm not expecting to win, but being in a competition with a firm deadline will provide more impetus to me to learn. I'd like to give a shout out to Will Koerhsen for his kernel: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction. I modified his code and have been using it to learn on my own here.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"fa17f5ad4bad48d4991d99eebd0b5e9db25d019c"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nimport warnings\nfrom sklearn.decomposition import PCA\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bb28d80f45f4564c68fd08bf299a74748bd9917"},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/application_train.csv\")\ntest_data = pd.read_csv(\"../input/application_test.csv\")\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a8a64fb39f26e482e87c1efb1c162686f2701f9"},"cell_type":"code","source":"print('Training data shape: ', train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60934364cae8288f911e202c1b0d5eb3cb353623"},"cell_type":"code","source":"train_data['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267166fa7909763cb6035f68af879f6e71e3efa8"},"cell_type":"code","source":"train_data['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc326fd7db223f90d47b148e0413796cf2270464"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9aa7ad4c306610507251e55139ed1d69ed2a299"},"cell_type":"code","source":"missing_values_table(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06fd0a715c37e1eab7d203914a477bbb9e33083a"},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in train_data:\n    if train_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(train_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(train_data[col])\n            # Transform both training and testing data\n            train_data[col] = le.transform(train_data[col])\n            test_data[col] = le.transform(test_data[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa59f61220cf528c2d530fd6305fbec12bf80197"},"cell_type":"code","source":"# one-hot encoding of categorical variables\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\nprint('Training Features shape: ', train_data.shape)\nprint('Testing Features shape: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23928301fa209c816a055f05b5fd2260ba9bbc17"},"cell_type":"code","source":"train_labels = train_data['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ntrain_data, test_data = train_data.align(test_data, join = 'inner', axis = 1)\n\n# Add the target back in\ntrain_data['TARGET'] = train_labels\n\nprint('Training Features shape: ', train_data.shape)\nprint('Testing Features shape: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3870a2a1b1ec6ee0ce12459bcedfc3fcb13d179"},"cell_type":"markdown","source":"In the Gentle Introduction, at this point, Will went back to his EDA without any dimensionality reduction. What follows is my attempt at dimensionality reduction before proceeding."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"47fbaf67d0cfec71afc24156f4715a28ef2aae26"},"cell_type":"code","source":"# Make an instance of the Model\npca = PCA(.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6398b53c0403d50f9ca2c20308f37002417ca4b"},"cell_type":"code","source":"pca.fit(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1397ad26b23a21fdf8a142137d8c5093c0e2ddb1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}