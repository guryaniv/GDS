{"cells":[{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e08e2ae14e9d54e55d149191a034f94a7f493138"},"cell_type":"code","source":"# -------------------------------------------------------START-----------------------------------------\n\n\"\"\"It's my first nlp pratice my code is mainly based on use datacamp nlp module, \nkirill eremenko machine learning mooc section on nlp\nand the great notebook of Bojan Tuguz \"logistic regression with word and char n-grams.\nthanks to them, i'm open to criticism in purpose to improve myself\"\"\"\n\nimport numpy as np\nimport pandas as pd \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import HashingVectorizer\nimport re \nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport nltk\n#nltk.download('stopwords')\n  \n\n# --------------------------------------------------------------------------------------------\n\ntrain = pd.read_csv('../input/train.csv')[:50000]\ntest = pd.read_csv('../input/test.csv')[:50000]\n\n#train.head()\n\n\ntrain['comment_text'] = train['comment_text'].fillna('__nocomment__')\ntest['comment_text']  = test['comment_text'].fillna('__nocomment__')\n\n#seems those line are too greedy for kaggle so i use 10k rows\n\ntrain_text = train[\"comment_text\"]\ntest_text = test[\"comment_text\"]\n\n#test.shape\n#train.shape\n\n\n\n\n#--------------------------------------------------------------------------\n\n\nps = PorterStemmer()\n\n#word \n\ntrain_corpus = []\nfor i in range(0, 50000):\n    data = re.sub(\"[^a-zA-Z]\", ' ', train_text[i]).lower().split()\n    data = [ps.stem(word) for word in data if not word in set(stopwords.words(\"english\"))]\n    data = ' '.join(data)\n    train_corpus.append(data)\n\n\n\ntest_corpus = []\n\nfor i in range(0, 50000):\n    data2 = re.sub(\"[^a-zA-Z]\", ' ', test_text[i]).lower().split()\n    data2 = [ps.stem(word) for word in data2 if not word in set(stopwords.words(\"english\"))]\n    data2 = ' '.join(data2)\n    test_corpus.append(data2)\n\n\n#-------------------------------------------------- char/word --------------------\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    ngram_range=(1, 2),\n    max_features=8000,\n    norm = \"l1\") \nword_vectorizer.fit(train_corpus)\n\nword_vectorizer2 = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    ngram_range=(1, 2),\n    max_features=8000,\n    norm = \"l1\")\n\nword_vectorizer2.fit(test_corpus)\n\ntrain_word_features = word_vectorizer.transform(train_corpus)\ntest_word_features = word_vectorizer2.transform(test_corpus)\n\n#~~~~~~~~\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    ngram_range=(4, 6),\n    max_features=25000, \n    norm = \"l1\")\nchar_vectorizer.fit(train_corpus)\n\nchar_vectorizer2 = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(4, 6),\n    max_features=25000, \n    norm=\"l1\")\n\nchar_vectorizer2.fit(test_corpus)\n\ntrain_char_features = char_vectorizer.transform(train_corpus)\ntest_char_features = char_vectorizer2.transform(test_corpus)\n\n#----------------------------------------------------\n\ntrain_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n#-----------------------------------------------\n\n#submission template took from Dr.fuzzy notebook\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nScores = []\n\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\n\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = MultinomialNB()\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=5, scoring='roc_auc'))\n    Scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n    \n#submission.to_csv('submission.csv', index=False)\n\n# ------------------------------------------------- End -------------------------------------------","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}