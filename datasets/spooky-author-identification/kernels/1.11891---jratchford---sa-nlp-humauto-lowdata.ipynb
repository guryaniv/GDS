{"nbformat_minor": 1, "cells": [{"outputs": [], "source": ["# Imports\n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "import nltk.data\n", "import nltk\n", "import os\n", "from collections import OrderedDict\n", "from subprocess import check_call\n", "from shutil import copyfile\n", "from sklearn.metrics import log_loss\n", "import matplotlib.pyplot as plt\n", "import mpld3\n", "import seaborn as sns\n", "from collections import Counter\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn import ensemble, metrics, model_selection, naive_bayes\n", "from sklearn.preprocessing import LabelEncoder\n", "import xgboost as xgb\n", "from tqdm import tqdm\n", "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n", "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n", "from keras.preprocessing import sequence, text\n", "from keras.callbacks import EarlyStopping\n", "from nltk import word_tokenize\n", "from keras.layers.merge import concatenate\n", "from keras.utils import np_utils\n", "from keras.models import Sequential\n", "from keras.layers.recurrent import LSTM, GRU\n", "from keras.layers.core import Dense, Activation, Dropout\n", "from keras.layers.embeddings import Embedding\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras import initializers\n", "from keras import backend as K\n", "from sklearn.linear_model import SGDClassifier as sgd\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.callbacks import EarlyStopping\n", "from time import time"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "81522729384df9780b21e0f76768bd5e402eab31", "_cell_guid": "ca1abe0f-ea70-40f6-8bf4-1790a577626b", "collapsed": true}}, {"source": ["Add small test samples here:"], "cell_type": "markdown", "metadata": {"_uuid": "5c69c8e253c3ffb5d287c895e21f4004c9b2fc6c", "_cell_guid": "034b43e9-eec3-4962-8b00-21d7dd7d906d"}}, {"outputs": [], "source": ["start = time()\n", "# Read data\n", "print('Extract...',round(time()-start,0))\n", "train = \"../input/train.csv\" #change this to correct training csv\n", "test = \"../input/test.csv\" #change this to correct test csv\n", "X_train_ = pd.read_csv( train, header=0,delimiter=\",\" )\n", "X_train=X_train_.sample(frac=0.3, random_state=12345)\n", "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n", "\n", "authors = ['EAP','MWS','HPL']\n", "Y_train = LabelEncoder().fit_transform(X_train['author'])"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c2753ac1526af4d716ecb359c186d0ed54d6f61a", "_cell_guid": "431ad233-ef87-48bc-a12e-c1481adaeb42", "scrolled": true, "collapsed": true}}, {"outputs": [], "source": ["# Clean data\n", "def clean(X_train,X_test):\n", "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n", "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n", "    return X_train,X_test\n", "X_train,X_test = clean(X_train,X_test)\n", "print('Leaning words...',round(time()-start,0))\n", "\n", "auth_wds = {'EAP':0,'HPL':0,'MWS':0}\n", "\n", "wd = {}\n", "for i, row in X_train.iterrows():\n", "    for a in row['words']:\n", "        if len(a) > 1:\n", "            try: \n", "                wd[a][row['author']] = wd[a][row['author']] + 1\n", "                auth_wds[row['author']] = auth_wds[row['author']] + 1\n", "            except:\n", "                c_eap = 0\n", "                c_hpl = 0\n", "                c_mws = 0\n", "                try: c_eap = wd[a]['EAP'] \n", "                except: pass\n", "                try: c_hpl = wd[a]['HPL'] \n", "                except: pass\n", "                try: c_mws = wd[a]['EAP'] \n", "                except: pass   \n", "                wd[a] = {'EAP':c_eap,'HPL':c_hpl,'MWS':c_mws}\n", "                wd[a][row['author']] = wd[a][row['author']] + 1\n", "                auth_wds[row['author']] = auth_wds[row['author']] + 1\n", "                \n", "def remove_key(dictionary,key):\n", "    r = dict(dictionary)\n", "    del r[key]\n", "    return r        \n", "\n", "for key in list(wd.keys()):\n", "    pass\n", "    if wd[key]['EAP'] + wd[key]['HPL'] + wd[key]['MWS'] < 100: \n", "        wd = remove_key(wd,key)\n", "        \n", "c_eap = 0\n", "c_hpl = 0\n", "c_mws = 0    \n", "for key in list(wd.keys()): \n", "    pass\n", "    if not any([(wd[key]['EAP']/auth_wds['EAP'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2,\n", "          (wd[key]['EAP']/auth_wds['EAP'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2,\n", "          (wd[key]['HPL']/auth_wds['HPL'])/((wd[key]['EAP']+1)/auth_wds['EAP'])>2.2,\n", "          (wd[key]['HPL']/auth_wds['HPL'])/((wd[key]['MWS']+1)/auth_wds['MWS'])>2.2,\n", "          (wd[key]['MWS']/auth_wds['MWS'])/((wd[key]['EAP']+1)/auth_wds['EAP'])>2.2,\n", "         ( wd[key]['MWS']/auth_wds['MWS'])/((wd[key]['HPL']+1)/auth_wds['HPL'])>2.2]):\n", "        wd = remove_key(wd,key)\n", "\n", "col_wds = {}\n", "for key in list(wd.keys()): \n", "    col_wds[key]=0\n", "\n", "rows = []\n", "for words in X_train['words']:\n", "    line_wds = dict(col_wds)\n", "    for word in words:\n", "        try: line_wds[word] = line_wds[word] + 1\n", "        except: pass\n", "    row = [line_wds[key] for key in list(line_wds.keys())]\n", "    rows.append(row)\n", "pd_df = pd.DataFrame(rows)\n", "    \n", "for column in pd_df: \n", "    pass\n", "    X_train['wd_'+str(column)] = pd_df[column]\n", "\n", "rows = []\n", "for words in X_test['words']:\n", "    line_wds = dict(col_wds)\n", "    for word in words:\n", "        try: line_wds[word] = line_wds[word] + 1\n", "        except: pass\n", "    row = [line_wds[key] for key in list(line_wds.keys())]\n", "    rows.append(row)\n", "pd_df = pd.DataFrame(rows)\n", "    \n", "for column in pd_df: \n", "    pass\n", "    X_test['wd_'+str(column)] = pd_df[column]\n", "\n", "auth_wds = {'EAP':0,'HPL':0,'MWS':0}\n", "\n", "wd = {}\n", "for i, row in X_train.iterrows():\n", "    for a in row['words']:\n", "        if len(a) > 1:\n", "            try: \n", "                wd[a][row['author']] = wd[a][row['author']] + 1\n", "                auth_wds[row['author']] = auth_wds[row['author']] + 1\n", "            except:\n", "                c_eap = 0\n", "                c_hpl = 0\n", "                c_mws = 0\n", "                try: c_eap = wd[a]['EAP'] \n", "                except: pass\n", "                try: c_hpl = wd[a]['HPL'] \n", "                except: pass\n", "                try: c_mws = wd[a]['EAP'] \n", "                except: pass   \n", "                wd[a] = {'EAP':c_eap,'HPL':c_hpl,'MWS':c_mws}\n", "                wd[a][row['author']] = wd[a][row['author']] + 1\n", "                auth_wds[row['author']] = auth_wds[row['author']] + 1\n", "                \n", "def remove_key(dictionary,key):\n", "    r = dict(dictionary)\n", "    del r[key]\n", "    return r        \n", "\n", "for key in list(wd.keys()):\n", "    pass\n", "    if wd[key]['EAP'] + wd[key]['HPL'] + wd[key]['MWS'] < 5: \n", "        wd = remove_key(wd,key)\n", "        \n", "   \n", "e = auth_wds['EAP']\n", "h = auth_wds['HPL']\n", "m = auth_wds['MWS']\n", "eap_wds = []\n", "hpl_wds = []\n", "mws_wds = []\n", "for key in list(wd.keys()): \n", "    pass\n", "    if (wd[key]['EAP']/e)>((wd[key]['HPL']/h)+(wd[key]['MWS'])/m):\n", "        eap_wds.append(key)\n", "    elif (wd[key]['HPL']/e)>((wd[key]['EAP']/e)+(wd[key]['MWS'])/m):\n", "        hpl_wds.append(key)\n", "    elif (wd[key]['MWS']/e)>((wd[key]['HPL']/h)+(wd[key]['EAP'])/e):\n", "        mws_wds.append(key)\n", "c_wd_rows_eap = []\n", "c_wd_rows_hpl = []\n", "c_wd_rows_mws = []\n", "dup_wds = []\n", "for row in X_train['words']:\n", "    if len(row) == len(set(row)): dup_wds.append(0)\n", "    else: dup_wds.append((len(row)-len(set(row)))/len(row)*10)\n", "    for word in row:\n", "        c_eap = 0\n", "        c_hpl = 0\n", "        c_mws = 0 \n", "        if word in eap_wds: c_eap+=1\n", "        elif word in hpl_wds: c_hpl+=1\n", "        elif word in mws_wds: c_mws+=1\n", "    c_wd_rows_eap.append(c_eap)\n", "    c_wd_rows_hpl.append(c_hpl)\n", "    c_wd_rows_mws.append(c_mws)\n", "X_train['c_wd_eap'] = c_wd_rows_eap  \n", "X_train['c_wd_hpl'] = c_wd_rows_hpl  \n", "X_train['c_wd_mws'] = c_wd_rows_mws \n", "X_train['dup_wds'] = dup_wds\n", "c_wd_rows_eap = []\n", "c_wd_rows_hpl = []\n", "c_wd_rows_mws = []\n", "dup_wds = []\n", "for row in X_test['words']:\n", "    if len(row) == len(set(row)): dup_wds.append(0)\n", "    else: dup_wds.append((len(row)-len(set(row)))/len(row)*10)\n", "    for word in row:\n", "        c_eap = 0\n", "        c_hpl = 0\n", "        c_mws = 0 \n", "        if word in eap_wds: c_eap+=1\n", "        elif word in hpl_wds: c_hpl+=1\n", "        elif word in mws_wds: c_mws+=1\n", "    c_wd_rows_eap.append(c_eap)\n", "    c_wd_rows_hpl.append(c_hpl)\n", "    c_wd_rows_mws.append(c_mws)\n", "X_test['c_wd_eap'] = c_wd_rows_eap  \n", "X_test['c_wd_hpl'] = c_wd_rows_hpl  \n", "X_test['c_wd_mws'] = c_wd_rows_mws \n", "X_test['dup_wds'] = dup_wds\n", "\n", "print('Characters...',round(time()-start,0))\n", "all_char = set([i for i in str(X_train['text'])])\n", "for char in all_char:\n", "    X_train['punc_'+char] = [(sum([1  for nchar in sentence if nchar == char])/len(sentence)) for sentence in X_train['text']]\n", "    X_test['punc_'+char] = [(sum([1  for nchar in sentence if nchar == char])/len(sentence)) for sentence in X_test['text']]"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "785a93df26ba4790fe278d9413e3c57edeb2e928", "_cell_guid": "d93cd623-b12a-4431-9d8e-49b53f29e6b5", "collapsed": true}}, {"outputs": [], "source": ["from gensim.parsing.preprocessing import STOPWORDS\n", "\n", "# Feature Engineering\n", "# Stop Words\n", "print('Other columns...',round(time()-start,0))\n", "_dist_train = [x for x in X_train['words']]\n", "X_train['stop_word'] = [len([word for word in sentence if word in STOPWORDS])*100.0/len(sentence) for sentence in _dist_train]\n", "\n", "_dist_test = [x for x in X_test['words']]\n", "X_test['stop_word'] = [len([word for word in sentence if word in STOPWORDS])*100.0/len(sentence) for sentence in _dist_test]  \n", "\n", "## Number of words in the text ##\n", "X_train[\"num_words\"] = X_train[\"text\"].apply(lambda x: len(str(x).split()))\n", "X_test[\"num_words\"] = X_test[\"text\"].apply(lambda x: len(str(x).split()))\n", "\n", "## Number of unique words in the text ##\n", "X_train[\"num_unique_words\"] = X_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "X_test[\"num_unique_words\"] = X_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "\n", "## Number of characters in the text ##\n", "X_train[\"num_chars\"] = X_train[\"text\"].apply(lambda x: len(str(x)))\n", "X_test[\"num_chars\"] = X_test[\"text\"].apply(lambda x: len(str(x)))\n", "\n", "## Average length of the words in the text ##\n", "X_train[\"mean_word_len\"] = X_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "X_test[\"mean_word_len\"] = X_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "\n", "print('TFIDF...',round(time()-start,0))\n", "### Fit transform the count vectorizer ###\n", "tfidf_vec = CountVectorizer(stop_words=STOPWORDS, ngram_range=(1,3))\n", "tfidf_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n", "\n", "# Feature Engineering\n", "# count - words - nb\n", "def countWords(X_train,X_test):\n", "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n", "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_count = count_vec.transform(X_train['text'].values.tolist())\n", "    test_count = count_vec.transform(X_test['text'].values.tolist())\n", "    return train_count,test_count\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do_count_MNB(X_train,X_test,Y_train):\n", "    train_count,test_count=countWords(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test /5.\n", "    return pred_train,pred_full_test\n", "\n", "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n", "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n", "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n", "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n", "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"count_words_nb_mws\"] = pred_test[:,2]\n", "\n", "\n", "# Feature Engineering\n", "# tfidf - chars - nb\n", "def tfidfWords(X_train,X_test):\n", "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n", "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n", "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n", "    return train_tfidf,test_tfidf\n", "    \n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do(X_train,X_test,Y_train):\n", "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([X_train.shape[0], 3])\n", "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=88)\n", "    for dev_index, val_index in kf.split(X_train):\n", "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"Mean cv score : \", np.mean(cv_scores))\n", "    pred_full_test = pred_full_test /5.\n", "    return pred_train,pred_full_test\n", "pred_train,pred_test = do(X_train,X_test,Y_train)\n", "\n", "\n", "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n", "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n", "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n", "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n", "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n", "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]\n", "print('SpaCy...',round(time()-start,0))"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "5972266e8f574f158e75ad176c285640dd5fa54b", "_cell_guid": "2c5487a2-077f-4393-a158-0603a3ce6dba", "collapsed": true}}, {"outputs": [], "source": ["# incorporate spacy vectors\n", "import spacy\n", "import en_core_web_lg\n", "import string\n", "nlp = en_core_web_lg.load()\n", "\n", "#Clean text before feeding it to spaCy\n", "punctuations = string.punctuation\n", "\n", "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n", "def cleanup_text(docs):\n", "    texts = []\n", "    for doc in docs:\n", "        doc = nlp(doc)\n", "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n", "        tokens = [tok for tok in tokens if tok not in STOPWORDS and tok not in punctuations]\n", "        tokens = ' '.join(tokens)\n", "        texts.append(tokens)\n", "    return pd.Series(texts)"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "89df3b57be13a7bf7e88e10631562886a15c0934", "_cell_guid": "bb76cb4d-6cee-4bdb-bfda-9d4f77d635b7", "collapsed": true}}, {"outputs": [], "source": ["spacy_cleaned_train = cleanup_text(X_train['text'])\n", "spacy_cleaned_test =  cleanup_text(X_test['text'])"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "9af6b2a7d47cd30597dbc070635ef996cce6cd1c", "_cell_guid": "c1bed23f-eba8-44c3-9c60-156803a4dc88", "collapsed": true}}, {"outputs": [], "source": ["print('Spacy Vectors...',round(time()-start,0))\n", "train_vec = [doc.vector for doc in nlp.pipe(list(spacy_cleaned_train), batch_size=500, n_threads=4)]\n", "train_vec = np.array(train_vec)\n", "\n", "X_train[['spacy_vec_'+str(i) for i in range(300)]] = pd.DataFrame(train_vec.tolist())"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8b50fc72e4ff883d7cf9cbcb0fb681357af9385f", "_cell_guid": "d932d701-0834-4fbb-b9c3-efa3d217aba9", "collapsed": true}}, {"outputs": [], "source": ["test_vec = [doc.vector for doc in nlp.pipe(spacy_cleaned_train, batch_size=500, n_threads=4)]\n", "test_vec = np.array(train_vec)\n", "\n", "X_test[['spacy_vec_'+str(i) for i in range(300)]] = pd.DataFrame(test_vec.tolist())"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c8d99f4aaf8a47a46e38108738c228509389eccf", "_cell_guid": "477ca04e-cc5d-49a4-b38a-a2451e318b1a", "collapsed": true}}, {"outputs": [], "source": ["print('Gensim Train...',round(time()-start,0))\n", "import gensim\n", "from gensim.models import doc2vec\n", "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n", "\n", "#Gensim doc2vec\n", "corpus_train = [z.split() for z in spacy_cleaned_train]\n", "corpus_test = [z.split() for z in spacy_cleaned_test]\n", "corpus_all = corpus_train + corpus_test\n", "\n", "\n", "def labelizeReviews(reviews, label_type):\n", "    labelized = []\n", "    for i,v in enumerate(reviews):\n", "        label = '%s_%s'%(label_type, i)\n", "        labelized.append(LabeledSentence(v, [label]))\n", "    return labelized\n", "\n", "\n", "\n", "X_train_lab = labelizeReviews(corpus_train, 'Train')\n", "X_test_lab = labelizeReviews(corpus_test, 'Test')\n", "All_lab = labelizeReviews(corpus_all, 'All')\n", "\n", "\n", "model = doc2vec.Doc2Vec(All_lab, min_count=1, window=10, size=300, workers=6)\n", "\n", "\n", "def getVecs(model, corpus, size, vecs_type):\n", "    vecs = np.zeros((len(corpus), size))\n", "    for i in range(0, len(corpus)):\n", "        vecs[i] = model.docvecs[i]\n", "    return vecs\n", "\n", "print('Gensim Vectors...',round(time()-start,0))\n", "gen_train_vecs = getVecs(model, X_train_lab, 300, 'Train')\n", "gen_test_vecs = getVecs(model, X_test_lab, 300, 'Train')\n", "\n", "\n", "X_train[['d2v_vec_'+str(i) for i in range(300)]] = pd.DataFrame(gen_train_vecs.tolist())\n", "X_test[['d2v_vec_'+str(i) for i in range(300)]] = pd.DataFrame(gen_test_vecs.tolist())"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7a316c5c56f88b62f7d9d26200127df684c22962", "_cell_guid": "931cf529-0424-416f-be1f-c24a146d65bd", "collapsed": true}}, {"outputs": [], "source": ["#SVD\n", "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n", "full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n", "\n", "n_comp = 20\n", "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n", "svd_obj.fit(full_tfidf)\n", "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n", "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n", "\n", "X_train[['svd_char_'+str(i) for i in range(20)]] = pd.DataFrame(train_svd)\n", "X_test[['svd_char_'+str(i) for i in range(20)]] = pd.DataFrame(test_svd)"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d3d23bf5d84df7195f540fa5c0ee1c7276aedee0", "_cell_guid": "78f6caaf-7258-4508-88be-d2e20ffd8a9d", "collapsed": true}}, {"outputs": [], "source": ["splits = 4"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "a0007692ec5522f31426cda0fd19fb26c3e17de9", "_cell_guid": "e4c9ab3a-16ec-4a97-b7d6-96dd463c412a", "collapsed": true}}, {"outputs": [], "source": ["# Using Neural Networks and Facebook's Fasttext\n", "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n", "\n", "# NN\n", "def doAddNN(X_train,X_test,pred_train,pred_test):\n", "    X_train[\"nn_eap\"] = pred_train[:,0]\n", "    X_train[\"nn_hpl\"] = pred_train[:,1]\n", "    X_train[\"nn_mws\"] = pred_train[:,2]\n", "    X_test[\"nn_eap\"] = pred_test[:,0]\n", "    X_test[\"nn_hpl\"] = pred_test[:,1]\n", "    X_test[\"nn_mws\"] = pred_test[:,2]\n", "    return X_train,X_test\n", "\n", "def initNN(nb_words_cnt,max_len):\n", "    model = Sequential()\n", "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n", "    model.add(Dropout(0.3))\n", "    model.add(Conv1D(64,\n", "                     5,\n", "                     padding='valid',\n", "                     activation='relu'))\n", "    model.add(Dropout(0.3))\n", "    model.add(MaxPooling1D())\n", "    model.add(Flatten())\n", "    model.add(Dense(800, activation='relu'))\n", "    model.add(Dropout(0.5))\n", "    model.add(Dense(3, activation='softmax'))\n", "\n", "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n", "    return model\n", "\n", "def doNN(X_train,X_test,Y_train):\n", "    max_len = 70\n", "    nb_words = 10000\n", "    \n", "    print('Processing text dataset')\n", "    texts_1 = []\n", "    for text in X_train['text']:\n", "        texts_1.append(text)\n", "\n", "    print('Found %s texts.' % len(texts_1))\n", "    test_texts_1 = []\n", "    for text in X_test['text']:\n", "        test_texts_1.append(text)\n", "    print('Found %s texts.' % len(test_texts_1))\n", "    \n", "    tokenizer = Tokenizer(num_words=nb_words)\n", "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n", "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n", "    word_index = tokenizer.word_index\n", "    print('Found %s unique tokens.' % len(word_index))\n", "\n", "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n", "\n", "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n", "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n", "    del test_sequences_1\n", "    del sequences_1\n", "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n", "\n", "    # we need to binarize the labels for the neural net\n", "    ytrain_enc = np_utils.to_categorical(Y_train)\n", "    \n", "    kf = model_selection.KFold(n_splits=splits, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n", "    for dev_index, val_index in kf.split(xtrain_pad):\n", "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n", "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n", "        model = initNN(nb_words_cnt,max_len)\n", "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,\n", "                  validation_data=(val_X, val_y),callbacks=[earlyStopping])\n", "        pred_val_y = model.predict(val_X)\n", "        pred_test_y = model.predict(xtest_pad)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "    return doAddNN(X_train,X_test,pred_train,pred_full_test/splits)\n", "\n", "# Fast Text\n", "\n", "def doAddFastText(X_train,X_test,pred_train,pred_test):\n", "    X_train[\"ff_eap\"] = pred_train[:,0]\n", "    X_train[\"ff_hpl\"] = pred_train[:,1]\n", "    X_train[\"ff_mws\"] = pred_train[:,2]\n", "    X_test[\"ff_eap\"] = pred_test[:,0]\n", "    X_test[\"ff_hpl\"] = pred_test[:,1]\n", "    X_test[\"ff_mws\"] = pred_test[:,2]\n", "    return X_train,X_test\n", "\n", "\n", "def initFastText(embedding_dims,input_dim):\n", "    model = Sequential()\n", "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n", "    model.add(GlobalAveragePooling1D())\n", "    model.add(Dense(3, activation='softmax'))\n", "\n", "    model.compile(loss='categorical_crossentropy',\n", "                  optimizer='adam',\n", "                  metrics=['accuracy'])\n", "    return model\n", "\n", "def preprocessFastText(text_docs):\n", "    text_docs = text_docs.replace(\"' \", \" ' \")\n", "    signs = set(',.:;\"?!')\n", "    prods = set(text_docs) & signs\n", "    if not prods:\n", "        return text_docs\n", "\n", "    for sign in prods:\n", "        text_docs = text_docs.replace(sign, ' {} '.format(sign) )\n", "    return text_docs\n", "\n", "def create_docs(df, n_gram_max=2):\n", "    def add_ngram(q, n_gram_max):\n", "            ngrams = []\n", "            for n in range(2, n_gram_max+1):\n", "                for w_index in range(len(q)-n+1):\n", "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n", "            return q + ngrams\n", "    docs = []\n", "    for doc in df.text:\n", "        doc = preprocessFastText(doc).split()\n", "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n", "    return docs\n", "\n", "def doFastText(X_train,X_test,Y_train):\n", "    min_count = 2\n", "\n", "    docs = create_docs(X_train)\n", "    tokenizer = Tokenizer(lower=False, filters='')\n", "    tokenizer.fit_on_texts(docs)\n", "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n", "\n", "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n", "    tokenizer.fit_on_texts(docs)\n", "    docs = tokenizer.texts_to_sequences(docs)\n", "\n", "    maxlen = 300\n", "\n", "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n", "    input_dim = np.max(docs) + 1\n", "    embedding_dims = 20\n", "\n", "    # we need to binarize the labels for the neural net\n", "    ytrain_enc = np_utils.to_categorical(Y_train)\n", "\n", "    docs_test = create_docs(X_test)\n", "    docs_test = tokenizer.texts_to_sequences(docs_test)\n", "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n", "    xtrain_pad = docs \n", "    kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=2017)\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n", "    for dev_index, val_index in kf.split(xtrain_pad):\n", "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n", "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n", "        model = initFastText(embedding_dims,input_dim)\n", "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=28, verbose=1,\n", "                  validation_data=(val_X, val_y),callbacks=[earlyStopping])\n", "        pred_val_y = model.predict(val_X)\n", "        pred_test_y = model.predict(docs_test)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/3)\n", "print('Other cool methods..',round(time()-start,0))\n", "X_train,X_test = doFastText(X_train,X_test,Y_train)\n", "X_train,X_test = doNN(X_train,X_test,Y_train)"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "790b2f4906be95944867ff5167f450415c6a8f13", "_cell_guid": "3abe3cfb-97fe-4d04-a1f3-04f5d2b7423b", "scrolled": true, "collapsed": true}}, {"outputs": [], "source": ["# Final Model\n", "# XGBoost\n", "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n", "    param = {}\n", "    param['objective'] = 'multi:softprob'\n", "    param['eta'] = 0.1\n", "    param['max_depth'] = 3\n", "    param['silent'] = 1\n", "    param['num_class'] = 3\n", "    param['eval_metric'] = \"mlogloss\"\n", "    param['min_child_weight'] = child\n", "    param['subsample'] = 0.8\n", "    param['colsample_bytree'] = colsample\n", "    param['seed'] = seed_val\n", "    num_rounds = 2000\n", "\n", "    plst = list(param.items())\n", "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n", "\n", "    if test_y is not None:\n", "        xgtest = xgb.DMatrix(test_X, label=test_y)\n", "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n", "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=30, verbose_eval=20)\n", "    else:\n", "        xgtest = xgb.DMatrix(test_X)\n", "        model = xgb.train(plst, xgtrain, num_rounds)\n", "\n", "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n", "    if test_X2 is not None:\n", "        xgtest2 = xgb.DMatrix(test_X2)\n", "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def do(X_train,X_test,Y_train):\n", "    drop_columns=[\"id\",\"text\",\"words\"]\n", "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n", "    x_test = X_test.drop(drop_columns,axis=1)\n", "    y_train = Y_train\n", "    \n", "    kf = model_selection.KFold(n_splits=4, shuffle=True, random_state=2017)\n", "    cv_scores = []\n", "    pred_full_test = 0\n", "    pred_train = np.zeros([x_train.shape[0], 3])\n", "    for dev_index, val_index in kf.split(x_train):\n", "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n", "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n", "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n", "        pred_full_test = pred_full_test + pred_test_y\n", "        pred_train[val_index,:] = pred_val_y\n", "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    print(\"cv scores : \", cv_scores)\n", "    return pred_full_test/4\n", "result = do(X_train,X_test,Y_train)\n", "\n", "result = pd.DataFrame(list(result),columns=['EAP','HPL','MWS'])\n", "result['id'] = X_test['id']\n", "result.to_csv('output_easier_process_version.csv',index=False)\n", "print('Time to completion: ',round(time()-start,0))"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "561397401f6978bb3ba70076a3fbe7e0a77e62a7", "_cell_guid": "94e93d29-b79d-408b-be06-b8493494af8e", "collapsed": true}}, {"outputs": [], "source": ["len(result)"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e9d49f6465438a7a35284550594776713322872a", "_cell_guid": "1540b851-fcbe-4a17-98f0-96a8ed8cec3c", "collapsed": true}}, {"outputs": [], "source": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d238a896afb7745609b1d03e1cd2cfad494c3097", "_cell_guid": "a91fa843-2770-4baf-932f-7729ca77977e", "collapsed": true}}, {"outputs": [], "source": ["result.to_csv('test.csv')"], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e50756eb56f1ed07b794b63a278a1502a76bebe2", "_cell_guid": "d7fce055-a72e-409f-b0c7-38111508b0be", "collapsed": true}}, {"outputs": [], "source": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e80771db4f85e3a602c00ccabcd57cccdfed19d3", "_cell_guid": "e0e35bca-6d13-4e87-9ef0-10de6ca48ed0", "collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}