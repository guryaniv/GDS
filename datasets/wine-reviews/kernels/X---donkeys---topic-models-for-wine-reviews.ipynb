{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# This notebook experiments with building topic models for the reviews - can we find some useful topics, assign reviews to these topics and use those to classify the reviews somehow?"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# For example, does some topic discuss specific types of wine?"},{"metadata":{"_uuid":"dcaa281cff5d02e2ab07193cd90aadf979c1fcd1"},"cell_type":"markdown","source":"Using Gensim for topic modelling, NLTK for some basic features. Something like Spacy and Mallet with Gensim would also be interesting to investigate, but not necessary for this exercise."},{"metadata":{"trusted":true,"_uuid":"c37973b640542dc9d5ce67d0379ab54ab25a7672"},"cell_type":"code","source":"import pandas as pd\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b3458be30a52c6e5319bbc063230c4bdd82e0cf"},"cell_type":"markdown","source":"There are several datasets in this dataset, main variants being a 150k review set and a 130k review set. Since the 130k set was described (or so I understood) as having duplicates removed, and additional information added, I used that."},{"metadata":{"_uuid":"0431395f69b7211c27ae69b0ec63958688b67ae3"},"cell_type":"markdown","source":"Running some of these analysis, I got some best matching documents, which actually had all duplicate information. So had to drop duplicates as well in any case. However, the added information about the reviewer turned out to be very useful, as we will see in the end of this notebook."},{"metadata":{"trusted":true,"_uuid":"9e56f607362959d0749f8b5304bcdb91f9e4a56e"},"cell_type":"code","source":"df = pd.read_csv('../input/winemag-data-130k-v2.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb00510c9b657763d313f2fe44d06f0f54e0dff9"},"cell_type":"code","source":"df = df.drop_duplicates('description') \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"555a5004f1749f4cefe8dddd42098a75e8e84f47"},"cell_type":"markdown","source":"So that dropped about 10k rows with duplicate descriptions. \nThe reviews are in this \"description\" column, so pick that up."},{"metadata":{"trusted":true,"_uuid":"85578adce947288eda8d34718f3d44fa3937f30d"},"cell_type":"code","source":" descriptions = df['description']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb09525861069f084c1e49cabb80baf22126fc94"},"cell_type":"markdown","source":"\nCollect stopwords for removal. This is the NLTK english stopwords, Python listed punctuation characters, and a set of custom token I found were still floating around in the results after all the NLTK stopwords and punctuations were removed. The last ones seem to be possibly some artefacts of how the stemming (or lemmatization/word tokenization) is done."},{"metadata":{"trusted":true,"_uuid":"7030f9f15690319ae00dba165930263aa7318741"},"cell_type":"code","source":"from string import punctuation\nstop_words = set(stopwords.words('english')) \nstop_words = stop_words.union(set(punctuation)) \nstop_words.update([\"\\'s\", \"n't\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56d301fe7f28d6c46c25a4f3fe6cd60921d222e4"},"cell_type":"markdown","source":"To better analyze the remaining words, I lemmatize them. Stemming is another option, but I prefer to lemmatize it, so I can actually look at the resulting topics/features and understand what they are. In some languages, the lemma might also have less overlap. Of course, POS tagging could also be useful but let's see how this works for now."},{"metadata":{"trusted":true,"_uuid":"fd1c797541abfaab452c6da1cdd085933c5e21fb"},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ntexts = [[lemmatizer.lemmatize(word) for word in word_tokenize(description.lower()) if word not in stop_words] for description in descriptions]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2cb4007401b69cd0e1e042c51727918e31d45b"},"cell_type":"code","source":"print(texts[4])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91c0f39cf0268e8519c87511ca92446e528d85b1"},"cell_type":"markdown","source":"Here I set up bi-gram and tri-gram identification for gensim. Most approaches I found seem to just want to make everything a bi-gram (so bi-gram for every word pair there is, or maybe I just misunderstand). I don't see that as useful, however, so this takes the most often co-occurring ones only."},{"metadata":{"_uuid":"7d7c1fb9367183318b0f7cd9485bbc134f0ff7f1"},"cell_type":"markdown","source":"For more discussion and formula on the threshold value: [StackOverflow](https://stackoverflow.com/questions/35716121/how-to-extract-phrases-from-corpus-using-gensim), [Radim](https://radimrehurek.com/gensim/models/phrases.html)\n\nSeems a bit complicated, but 100 was giving me good results so I used that for threshold."},{"metadata":{"trusted":true,"_uuid":"029ffa3a41135f71ca48bca5cd1ba5ebc34eb88a"},"cell_type":"code","source":"import gensim\n#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\ntrigram = gensim.models.Phrases(bigram[texts], threshold=100)\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram) \ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0781b345c7fa584983ef442977cacd1f972009f2"},"cell_type":"markdown","source":"So that trained a bi-gram and tri-gram analyzer/generator for me. Now to see how it handles the description at index 4 as I printed out above:"},{"metadata":{"trusted":true,"_uuid":"3ab43ed0bb97fc87d02fd532784131c3b5fbf406"},"cell_type":"code","source":" print(trigram_mod[bigram_mod[texts[4]]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"754a086a6791d2a3f958fad590085184afbe5027"},"cell_type":"markdown","source":"So it has identified \"winter stew\" in that text to be one word (a bi-gram). Don't really know what that is, but Google does give lots dishes for it, so seems good.\n\nNow lets replace all the identified bi-gram and tri-gram word-pairs and triplets in the text with the bi-gram and tri-gram representations (i.e., make them single features for algorithms)."},{"metadata":{"trusted":true,"_uuid":"1950f39c49e35f9bd6ab32dd9d65f1874c1ecf21"},"cell_type":"code","source":"texts = [trigram_mod[bigram_mod[text]] for text in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6e8ad89f4806b707a20ac4b512c0adbf93ef06d"},"cell_type":"code","source":" #id to word mapping for gensim\nid2word = corpora.Dictionary(texts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6fe79a21dcd192a7d42563a5440ccf82ae5f01"},"cell_type":"markdown","source":"Initial goal for me was to build a binary classifier. That would make this a two-topic affair. Let's see how this goes."},{"metadata":{"trusted":true,"_uuid":"50fb5d6a429c4ceffedf5f48526e080832b5ffd0"},"cell_type":"code","source":"from gensim.models import LdaModel\n\ncorpus = [id2word.doc2bow(text) for text in texts] \ntest_lda = LdaModel(corpus,num_topics=2, id2word=id2word) \nsentence = 'i like red wine with steak'\nsentence2 = [word for word in sentence.lower().split()] \ntest_lda[id2word.doc2bow(sentence2)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"046b49a8ae449bf416cecb14dd079039cf65e53d"},"cell_type":"markdown","source":"So the above trained the Gensim LDA model for two topics, and used it to classify a given sentence in regards to those two topics. In a real system, the classified evaluated sentence (variable \"sentence\" above) should be stop-word removed and lemmatized but in this case the words are pretty much there already (lemma form). Good enough for this experiment.\n\nThe result shows the sentence ranked as 11.8% in topic 1 and 88.2% in topic 2. And what are those topics?"},{"metadata":{"trusted":true,"_uuid":"f8b278d11745656318a5bc0e686d15c1fe6fff38"},"cell_type":"code","source":" test_lda.print_topics(num_words=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4299575166b9b0ad1d54bbde3a405d3b877bd7fa"},"cell_type":"markdown","source":"Above are the 20 top words for each of these two topics. Typically I try to look at these words to figure out what the topics might be about. These two topics do not seem very cohesive, so cannot say. Maybe a wine specialist could see something there.\n\nGensim has a measure called topic coherence, which gives a measure of how good it thinks the topics are. So let's try that.\n\nTo try this with different topic counts, I borrowed some code from the internets (as usual), maybe it was here: [Datascienceplus](https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/)\n\nWherever it was, thanks! :)\n"},{"metadata":{"trusted":true,"_uuid":"8d5fe9388a1bb4dd2619f33f45f1fb6c0d53b675"},"cell_type":"code","source":"from gensim.models import CoherenceModel\ndef compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3): \n    \"\"\"\n    Compute c_v coherence for various number of topics\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respect \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word) \n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abe3c4752b65af0ee5e7d71520e0eb5cf2e0b3f7"},"cell_type":"code","source":"# Can take a long time to run.\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, limit=40, start=2, step=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a80d3f05bc2de8a7553938901a2933a38ec25bb"},"cell_type":"code","source":"coherence_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d60f10cf2131fe5ac9a84e8387f1fdad5e133497"},"cell_type":"code","source":"import matplotlib.pyplot as plt \n%matplotlib inline\n# Show graph\nlimit=40; start=2; step=6;\nx = range(start, limit, step)\nplt.plot(x, coherence_values) \nplt.xlabel(\"Num Topics\") \nplt.ylabel(\"Coherence score\") \nplt.legend((\"coherence_values\"), loc='best') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeb07f7d348abdcafc1024d83639e0069069d703"},"cell_type":"markdown","source":"This indicates that the 2 topics I started with was a pretty poor choice. So no binary classification with this it seems. Around 10-15 this seems to spike, so with a quick eyeballing of the chart, I will try with 14 topics. I actually ran this with some slightly different configurations and in all cases the beginning of the chart was about the same, and the rest of it went down after about 20 topics. So 14 it is for now.\n\nFirst pick the topic model:"},{"metadata":{"trusted":true,"_uuid":"fd43c4f129204928fd969f99e1d486ab4f9328ff"},"cell_type":"code","source":"print(coherence_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30df50baeb6b6a3f07e81ce8ecf32fdbaf9083a5"},"cell_type":"code","source":"print(coherence_values[2]) \ntest_lda = model_list[2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c21d09c1c403ebedea8c2250527d4759fcdd17ed"},"cell_type":"markdown","source":"Top words for this model:"},{"metadata":{"trusted":true,"_uuid":"26d01ccfedeb043c81de69ee969d8f6067195780"},"cell_type":"code","source":" test_lda.print_topics(num_words=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ea29ce8e2f1d5d3bdb3a4856849f1ff63d68d3f"},"cell_type":"markdown","source":"There are some more intuitive ones here. For example, one seems to be around wine age, vintage wines, and so on. Another seems to be quite a lot about different wine types. The topic numbers seem to possibly change a bit across kernel runs, so cannot put a number here as I cannot predict what will be the topic number in the final Kaggle commit/run, after which this is uneditable. Probably there is some configuration of randomness I should set to exactly deterministic results. Since the topics generally seem the same across runs (just minor differences), I did not bother this time. Sorry. \n\nIn any case, for the topics and their sensibility with regards to wines, someone with more wine knowledge would likely be able to say something deeper about those. And how to iterate from these with more stopwords, etc.\n\nTo investigate a bit deeper myself, I take the top documents for each of these topics. This means the documents that the LDA model classifies to most heavily belonging to that specific topic:"},{"metadata":{"trusted":true,"_uuid":"74dbc9141f2786b3832d4c07c3b947aa1c5450a4"},"cell_type":"code","source":"import heapq \n\ntop_docs = {} \nn_topics = 14\n#first create placeholder lists for top 3 docs in each topic \nfor t in range(0, n_topics):\n    doc_list = [(-1,-1),(-1,-1),(-1,-1)] \n    heapq.heapify(doc_list)\n    top_docs[t] = doc_list\n#count variable in following is practically doc_id since the index is from 0 with increments of 1\ncount = 0\nfor doc in corpus:\n    if count % 10000 == 0:\n        #this is just to see it progresses, as it sometimes seems slow \n        print(count)\n    topics = test_lda[doc] \n    for topic_prob in topics:\n        topic_n = topic_prob[0]\n        topic_p = topic_prob[1]\n        top_list = top_docs[topic_n]\n        #count is document id, heapq sorts by first item in tuple\n        heapq.heappushpop(top_list, (topic_p, count))\n        #above pushes new item, pops lowest item. so pop itself if lowest..\n    count += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be64bf637852218dac75158616a338eb74618548"},"cell_type":"code","source":"print(top_docs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fe7f31b4fbbed77099b7478deabd2e4fb5782de"},"cell_type":"markdown","source":"For example, topic 3 (or this was the number in my run..) has 3 documents that are classified as 97% in that topic. So by looking at those documents, maybe we can get an idea of what the topic is about?"},{"metadata":{"trusted":true,"_uuid":"21d980cc12611a59d3b70f419d04cd3cc85004ce"},"cell_type":"code","source":"#topic 3:\ndoc_ids = [23682, 35855, 79546] \ntemp_df = df.iloc[doc_ids, :] \ntemp_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47dcbf51d756b9ed6ffea73be9738b7be83079ef"},"cell_type":"markdown","source":"That is one reviewer, who is trying a lot of Italian wines in this topic (at least when I printed this in my run..). Lets take a bit clearer look at the top docs for all topics:"},{"metadata":{"trusted":true,"_uuid":"dee103b2aa2ccde59cd56700e422c73cfafdb687"},"cell_type":"code","source":"top_sorted = {}\nfor topic_id in top_docs:\n    heap = top_docs[topic_id]\n    sorted_topics = [heapq.heappop(heap) for _ in range(len(heap))] \n    print(str(topic_id)+\": \"+str(sorted_topics)) \n    top_sorted[topic_id] = sorted_topics\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc203eddad3f41936eb5470109720330825d124f"},"cell_type":"markdown","source":"Topic 2 was also all just under 97% when I ran this:"},{"metadata":{"trusted":true,"_uuid":"4207dabc2d5488ba7faa0ea8fb603dfcb503b2e1"},"cell_type":"code","source":"#topic 2:\ndoc_ids = [58978, 1195, 82788] \ntemp_df = df.iloc[doc_ids, :] \ntemp_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d71e65070a9164df208f7fb1cc3f0cca9548976c"},"cell_type":"markdown","source":"Again one reviewer, but this time with some Argentinian and one Spanish wine. So lets just look at all the topics then:\n\n(This Kaggle environment seems to cut the size of the results box, and I cannot find how to expand it, so scroll in the below results  results to see all the topics)"},{"metadata":{"trusted":true,"_uuid":"4cfc65e2187925b453992a43019402db53206d8e"},"cell_type":"code","source":"from IPython.display import display\n\nfor topic_id in top_sorted:\n    print(\"Topic:\"+str(topic_id))\n    top_docs = top_sorted[topic_id]\n    doc_ids = [doc_tuple[1] for doc_tuple in top_sorted[topic_id]] \n    doc_weights = [doc_tuple[0] for doc_tuple in top_sorted[topic_id]] \n    temp_df = df.iloc[doc_ids, :]\n    temp_df[\"topic_weight\"] = doc_weights \n    display(temp_df.head())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ee717e372dc7df0e8b8409b4c35a613905a2a1"},"cell_type":"markdown","source":"\nSo here is the part why I said in the beginning it is nice to have the reviewer information available. The topics seem to actually map to a reviewer. Especially for the ones where the document is highly ranked as part of a specific topic.\n\nI was hoping to find topics revealing something interesting about the wines, what best describes some types of wine, and so on. This seems to have identified something about the reviewers writing style, or perhaps there would be something deeper in there to be found..?"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}