{"nbformat": 4, "nbformat_minor": 0, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.0"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"cell_type": "markdown", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "109fdb8d27ed5297d00fb52087ec80acfa509137", "_cell_guid": "7c504d99-eaf2-45b8-9b8d-867d53c93180", "collapsed": false}, "source": "Creative features"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "b837687630d82c79d3cb7b2e683622120d1babc5", "_cell_guid": "524120e2-a7c3-4633-9ac6-0f4319a4485d", "collapsed": false}, "source": "\"\"\"\nfor both:\ntook the data cleaning code from the forums\n\nfor split and joint:\nfill nans with 99\nadd some features\nfill the infs and nans of new features\nsplit train data by product type and train different models for them and get predictions\ncompare the relative importance of the features in each of the models\nform new interaction features with product type and train a joint model then use this to fill for \npredictions with missing product type.\n\nfor interact and randomize:\nBecause previous performed poorly, I do a randomizedlasso on combined df with the interaction features\nthen try to predict based on the transformed one\n\nfor split interact and average:\nI average the predictions for the previous two because they both get low LB scores\n\n\"\"\""}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "97a66749d5291efc511dc616cdc81cce63b9a10a", "_cell_guid": "9edbc92f-ea25-4a04-a7ec-40f86e865e2d", "collapsed": false}, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "cc1598abab42ad88fa7ebe5b8cc935cb08c1f8c1", "_cell_guid": "6b36bfaa-8181-4666-a95b-f9f5b604048c", "collapsed": false}, "source": "df = pd.read_csv(\"../input/train.csv\")\ndf_x = df.drop(labels=[\"price_doc\"], axis=1)\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain = df_x\n#data cleaning\nbad_index = train[train.life_sq > train.full_sq].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ntest.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\nbad_index = test[test.life_sq > test.full_sq].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.life_sq < 5].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = test[test.life_sq < 5].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.full_sq < 5].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[test.full_sq < 5].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train[train.kitch_sq >= train.life_sq].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[test.kitch_sq >= test.life_sq].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train[train.life_sq > 300].index\ntrain.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = test[test.life_sq > 200].index\ntest.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain.product_type.value_counts(normalize= True)\ntest.product_type.value_counts(normalize= True)\nbad_index = train[train.build_year < 1500].index\ntrain.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = test[test.build_year < 1500].index\ntest.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train[train.num_room == 0].index \ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = test[test.num_room == 0].index \ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\ntrain.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train[train.floor == 0].index\ntrain.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train[train.max_floor == 0].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.max_floor == 0].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train[train.floor > train.max_floor].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.floor > test.max_floor].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\ntrain.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain.ix[bad_index, \"floor\"] = np.NaN\ntrain.material.value_counts()\ntest.material.value_counts()\ntrain.state.value_counts()\nbad_index = train[train.state == 33].index\ntrain.ix[bad_index, \"state\"] = np.NaN\ntest.state.value_counts()\n\n\ndf_x = train \ncombined = pd.concat([df_x,test], ignore_index=True, axis=0)\nobj_col = combined.select_dtypes(include=[object]).columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfor name in obj_col:\n    if name != \"timestamp\" and name != \"product_type\":\n        print(name)\n        encoder = LabelEncoder()\n        combined[name] = encoder.fit_transform(combined[name].fillna(value=99).values)\n        \n#investment is 1 while occupier 0        \ncombined[\"product_type\"] = combined.product_type.map({\"Investment\":1, \n                                                      \"OwnerOccupier\":0, np.nan:99}).values\n\nfilled= combined.groupby(by=\"sub_area\").fillna(99)\ncombined = filled.merge(combined[[\"id\", \"sub_area\"]], on=\"id\")\n\n#add ratio and age vars\ncombined[\"age\"] = pd.to_datetime(combined[\"timestamp\"]).dt.year - combined.build_year\ncombined[\"ratio\"] = (combined[\"full_sq\"]/combined[\"life_sq\"]).fillna(value=99)\ncombined = combined.replace([-np.inf, np.inf], 99)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9697cc5230934f71e11fe7b00b04457f578e86ce", "_cell_guid": "0ae165a1-75b4-4b65-a3d1-19f018089eae", "collapsed": false}, "source": "#look at some stats for the different product types\nimport scipy.stats as stats\nprod = combined[[\"product_type\"]][:30471]\nprod[\"price_doc\"]=df.price_doc\nprod.pivot_table(index=[\"product_type\"], values=[\"price_doc\"], aggfunc=np.median)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "6899017fd94fae040e7c0f64f3c9043bd657020d", "_cell_guid": "4fe34135-7c69-488a-83b7-96585be4411c", "collapsed": false}, "source": "# add month, monthyear, ratio of green to industry, ofice count over leisure, \n#muslim to christ, ratio of close to far big chuches, mean distance to the art attractions\n#increase in buildings from old_times, ratio of children to youngens, and ratio of young male to female \ncombined[\"timestamp\"] = pd.to_datetime(combined.timestamp)\ncombined = combined.assign(month=combined.timestamp.dt.month)\ncombined = combined.assign(month_year=combined.timestamp.dt.year.astype(str) + combined.month.astype(str))\\\n           .assign(green_industry=combined[\"green_part_3000\"]/combined[\"prom_part_3000\"])\\\n           .assign(work_or_play=combined[\"office_count_1500\"]/(combined[\"sport_count_1500\"]+combined[\"leisure_count_1500\"]))\\\n           .assign(islam_or_christ=combined[\"mosque_count_500\"]/ combined[\"church_count_500\"])\\\n           .assign(church_appeal=combined[\"big_church_count_500\"]/combined[\"big_church_count_1500\"])\\\n           .assign(mean_km_art=combined[[\"museum_km\", \"exhibition_km\", \"catering_km\", \"theater_km\", \"park_km\"]].mean(axis=1))\\\n           .assign(new_to_old_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1971-1995\"])\\\n           .assign(new_to_older_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1946-1970\"])\\\n        .assign(young_to_old=combined[\"0_13_all\"]/combined[\"16_29_all\"])\\\n        .assign(male_to_femal_young=combined[\"young_male\"]/combined[\"young_female\"])\n        \ncombined[\"month_year\"] = combined.month_year.astype(int)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "9f68434d671a690fbc987dc07b0f8a1479e82d1d", "_cell_guid": "3288c050-b350-43a6-9033-a6725964d5d8", "collapsed": false}, "source": "counts = combined.groupby(\"month_year\")[\"id\"].count()\ndf4 = pd.DataFrame(counts).reset_index()\ndf4.columns = [\"month_year\", \"sales_count\"]\ncombined = combined.merge(df4, on=\"month_year\")"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "0abc3d08346140df77d1df71cdae5d82a2a16a40", "_cell_guid": "9d777718-e792-4273-a160-7ac8f4aaec90", "collapsed": false}, "source": "for i, col in enumerate(combined.columns.tolist()):\n    try:\n        if col != \"timestamp\" or col != \"month_year\" or col != \"month\":\n            combined.iloc[np.where(combined[col].isnull().values)[0], i]=0\n            combined.iloc[np.where(np.isinf(combined[col].values))[0], i] = 99\n    except TypeError:\n        continue"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "63b85b65693021640f49fb35658e6ff88aa1c561", "_cell_guid": "8c852069-df74-401b-8071-fe47c3bbb5f2", "collapsed": false}, "source": "\nno_type = combined.loc[combined[\"product_type\"]==99, \"id\"].values #all are from test\ntrain_0_id = combined[:30471].query(\"product_type == 0\")[\"id\"].values\ntrain_1_id = combined[:30471].query(\"product_type == 1\")[\"id\"].values\ntest_0_id = combined[30471:].query(\"product_type == 0\")[\"id\"].values\ntest_1_id = combined[30471:].query(\"product_type == 1\")[\"id\"].values\ntest_all_id = combined[30471:].id.values"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "d3f10b21ecdeeaa0daabe97d73ec9d8313c38381", "_cell_guid": "633ff235-46ec-4f4a-a09b-87d713dbfbc2", "collapsed": false}, "source": "#split train test samples for each product type\ncombined = combined.drop(labels=[\"id\", \"timestamp\"], axis=1, errors=\"ignore\")\nx_train_all = combined[:30471].copy()\nx_test_all = combined[30471:]\nx_train_all.loc[:, \"price_doc\"] =  df.price_doc.values\n\nx_train_0 = x_train_all.query(\"product_type == 0\")\nprice_0 = x_train_0.price_doc.values\n\nx_train_1 = x_train_all.query(\"product_type == 1\")\nprice_1 = x_train_1.price_doc.values\n\nx_test_0 = x_test_all.query(\"product_type == 0\")\nx_test_1 = x_test_all.query(\"product_type == 1\")"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "f78279d86abe4da6375979caa2543c55254a5d36", "_cell_guid": "96bc39b9-ac0a-4a11-91eb-d01932922ebe", "collapsed": false}, "source": "from sklearn.ensemble import GradientBoostingRegressor\nreg_0 = GradientBoostingRegressor(max_depth=2, min_samples_split=10, min_samples_leaf=8)\nreg_1 = GradientBoostingRegressor(max_depth=2, min_samples_split=10, min_samples_leaf=8)\n\nreg_0.fit(X=x_train_0.drop(labels=[\"price_doc\"], axis=1).values, y=price_0)\nreg_1.fit(X=x_train_1.drop(labels=[\"price_doc\"], axis=1).values, y=price_1)\n\n#ridge regression for whole sample\nfrom sklearn.linear_model import RidgeCV\nreg_all = RidgeCV(alphas=(0.6,0.7,0.8,0.9,1))\nreg_all.fit(X=x_train_all.drop(labels=[\"price_doc\"], axis=1).values, y=df.price_doc.values)\n\n\n#predictions\npredictions_0 = reg_0.predict(X=x_test_0)\npredictions_1 = reg_1.predict(X=x_test_1)\npredictions_all = reg_all.predict(X=x_test_all.values)\n\n#put into Series\ns0 = pd.Series(data=predictions_0, index=test_0_id, name=\"price_doc\")\ns1 = pd.Series(data=predictions_1, index=test_1_id, name=\"price_doc\")\npred_01 = s0.append(s1)\n\n\ns3 = pd.Series(data=predictions_all, index=test_all_id, name=\"price_doc\")\ns4 = s3.loc[no_type]\npred_all = pred_01.append(s4)\nprint(pred_all.head(5))\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntr_pred_0 = reg_0.predict(X=x_train_0.drop(labels=[\"price_doc\"], axis=1).values)\ntr_pred_1 = reg_1.predict(x_train_1.drop(labels=[\"price_doc\"], axis=1).values)\nplt.hist([tr_pred_0, tr_pred_1], bins=60, label=[\"0_type\", \"1_type\"], histtype=\"stepfilled\", alpha=0.5)\nplt.title(\"Histogram of 1 and 0 type predictions\")\nplt.legend()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "0aae6d6536f64dd366c8feb3e6d1618feb9f22e7", "_cell_guid": "113f52ab-6fa5-49f9-9125-3754512e92ca", "collapsed": false}, "source": "tr_pred_all = reg_all.predict(X=x_train_all.drop(labels=[\"price_doc\"], axis=1).values)\ndic = {\"pred_price\": np.insert(arr=tr_pred_1, values=tr_pred_0, obj=0), \n       \"id\":np.insert(arr=train_1_id, values=train_0_id, obj=0), \n       \"month_year\":np.insert(arr=x_train_1[\"month_year\"].values, values=x_train_0[\"month_year\"].values, obj=0)}\ndf1 = pd.DataFrame(data=dic)\ndic2 = {\"pred_price_all\": tr_pred_all, \"id\": df.id.values, \"month_year\":x_train_all[\"month_year\"].values}\ndf2 = pd.DataFrame(data=dic2)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "fac66c1cb1cf0812e078e00480b9f3bf450ee2d4", "_cell_guid": "47fb1a28-df0e-46fc-91a1-2306ce3a0f6d", "collapsed": false}, "source": "mean1 = df1.groupby(by=\"month_year\").mean().sort_index()\nmean2 = df2.groupby(by=\"month_year\").mean().sort_index()\nmean_actual = x_train_all.groupby(\"month_year\")[\"price_doc\"].mean().sort_index()\nfig, ax = plt.subplots(1)\nax.plot(range(mean1.index.shape[0]), mean1.pred_price.values, \"-\", \n       range(mean1.index.shape[0]), mean_actual.values, \"--\")\nax.legend([\"split_predictions\", \"actual\"])\n"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "97400fba305314d6230c82a3480debd7b74f201a", "_cell_guid": "db5068a6-4d40-4e97-bc9e-6aa709f3fab6", "collapsed": false}, "source": "print(reg_0.score(X=x_train_0.drop(labels=[\"price_doc\"], axis=1).values,y=x_train_0.price_doc.values))\nreg_1.score(X=x_train_1.drop(labels=[\"price_doc\"], axis=1).values,y=x_train_1.price_doc.values)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "564b323d70056b63e5e99b5544183f272f646cb4", "_cell_guid": "1099b4b2-2a81-4304-bf91-0d281e31c9eb", "collapsed": false}, "source": "f_imp = pd.DataFrame(index=x_train_0.drop(labels=[\"price_doc\"], axis=1).columns.values, \n          data={\"imp_0\":reg_0.feature_importances_, \n               \"imp_1\":reg_1.feature_importances_})"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "cddd55b3ec0ccc9a64809aff2f94a2662be71d93", "_cell_guid": "893694b5-cadb-4e03-9d50-a9c00b1ff93c", "collapsed": false}, "source": "plt.rcParams[\"figure.figsize\"]=30,10\nf_imp[f_imp[\"imp_1\"]>0].sort_values(\"imp_1\").plot(kind=\"bar\")\n\n\n    \n    \n#unimportant = f_imp[f_imp[\"imp_1\"]==0].index[:100]\n#combined2 = combined[[x for x in combined.columns if x not in unimportant]]"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "97cfca45f16584bbeaf60db087cad9ea49dd0870", "_cell_guid": "65a0f56d-a43f-46e3-9c2f-f60e2602dc80", "collapsed": false}, "source": "#some additional_vars from the difference in importance\n#this will be interactions btn prod type and features\ninteract = [\"full_sq\", \"build_year\", \"life_sq\", \"floor\", \"thermal_power_plant_km\", \"ttk_km\", \n          \"mean_km_art\", \"metro_min_avto\", \"university_km\", \"prom_part_2000\"]\npt = combined.product_type.values\nfor name in interact:\n    combined[name+\"*pt\"] = combined[name] * pt\n    \n\n#different direction starts here\ntrain_interact = combined[:30471]\ntest_interact = combined[30471:]\nlog_price = np.log(df.price_doc.values)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "b1c41bc64287f632ba75c8c8ab3b4c0745f6e598", "_cell_guid": "97a71e97-26f1-4fcb-abd6-8906c4a75322", "collapsed": false}, "source": "from sklearn.linear_model import RandomizedLasso\nrl = RandomizedLasso()\nrl.fit(X=train_interact.values, y=log_price)\n\n\nrl_train = rl.transform(train_interact.values)\nrl_test = rl.transform(test_interact.values)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "19e3e18e57acb25f08d8f5e70cad27978e9efe87", "_cell_guid": "601a4a65-fb5a-4635-91ab-a60a54468ce4", "collapsed": false}, "source": "from sklearn.ensemble import GradientBoostingRegressor\nreg_interact = GradientBoostingRegressor(max_depth=2, min_samples_split=10, min_samples_leaf=8)\nreg_interact.fit(X=rl_train, y=log_price)\npred_interact = reg_interact.predict(X=rl_test)\npred_series = pd.Series(data=np.exp(pred_interact), index=test.id.values, name=\"price_doc\")"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8bd3ab4e24b13e8b0c5aa8ac803e35886c914329", "collapsed": false}, "source": "reg_interact.score(X=rl_train, y=log_price)"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "e8d1ddc40fa978bb10d76b76298ffde2452a6a6a", "collapsed": false}, "source": "pred_series.head()"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "8c3e0a13d7772a28a96c4ab2b16be60bba836f76", "collapsed": false}, "source": "pred_all = pred_all.sort_index()\naveraged = (pred_all+pred_series)/2"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "bb4eb0d27eaf52e3d218652ebfda5568a94fa1ae", "collapsed": false}, "source": "averaged.to_csv(\"interact and randomize.csv\", header=True, index_label=\"id\")"}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "044deaac5342199d9fbad550a3891c7d95016d7b", "_cell_guid": "6dc8a82f-5625-4585-a584-9550a7125626", "collapsed": false}, "source": "\"The end\""}, {"cell_type": "code", "execution_count": null, "outputs": [], "metadata": {"_execution_state": "idle", "_uuid": "58ad88d57ce4961421d04bcd1262b538c55e025d", "collapsed": false}, "source": ""}]}