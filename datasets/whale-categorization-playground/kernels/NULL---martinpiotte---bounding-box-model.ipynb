{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Bounding Box Model\nThis notebook explains how to generage a bounding box model.<br/>\nWhile many of the whale pictures in the dataset are already cropped tight around the whale fluke, in some images the whale fluke occupies only a small area of the picture. Zooming in the relevant part of the picture provides greater accuracy to a classification model. To automate the process, this notebook explains how to construct a convolutional neural network (CNN) capable of estimating the whale bounding box.<br/>\nUsing this model, whale pictures can be cropped automatically to a more uniform appearance. This facilitates training of classification models, and improves the test accuracy.<br/>\nTraining of the bounding box model is performed over a dataset of 1200 bounding boxes for pictures selected from the Humpback Whale Identification Challenge training set. 1000 pictures are used for training, while 200 are reserved for validation."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dce51ea560d9b30bd2dc0c72ae97804f006f6a60"},"cell_type":"code","source":"# Suppress annoying stderr output when importing keras.\nimport sys\nold_stderr = sys.stderr\nsys.stderr = open('/dev/null', 'w')\nimport keras\nsys.stderr = old_stderr","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Read the cropping dataset\nOnce decoded, the variable *data* is a list of tuples. Each tuple contains the picture filename and a list of coordinates."},{"metadata":{"trusted":true,"_uuid":"41d55d57aa4404de5e00e4170b4d2b81511ebfae"},"cell_type":"code","source":"with open('../input/humpback-whale-identification-fluke-location/cropping.txt', 'rt') as f: data = f.read().split('\\n')[:-1]\ndata = [line.split(',') for line in data]\ndata = [(p,[(int(coord[i]),int(coord[i+1])) for i in range(0,len(coord),2)]) for p,*coord in data]\ndata[0] # Show an example: (picture-name, [coordinates])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b094e4935bfa0ff788a588554508769911e6de29"},"cell_type":"markdown","source":"The coordinates represent points on the fluke edge. The extremum values can be used to construct a bounding box."},{"metadata":{"trusted":true,"_uuid":"cf0668be8370405a366864491b40043429e96b71"},"cell_type":"code","source":"from PIL import Image as pil_image\nfrom PIL.ImageDraw import Draw\nfrom os.path import isfile\n\ndef expand_path(p):\n    if isfile('../input/whale-categorization-playground/train/' + p): return '../input/whale-categorization-playground/train/' + p\n    if isfile('../input/whale-categorization-playground/test/' + p): return '../input/whale-categorization-playground/test/' + p\n    return p\n\ndef read_raw_image(p):\n    return pil_image.open(expand_path(p))\n\ndef draw_dot(draw, x, y):\n    draw.ellipse(((x-5,y-5),(x+5,y+5)), fill='red', outline='red')\n\ndef draw_dots(draw, coordinates):\n    for x,y in coordinates: draw_dot(draw, x, y)\n\ndef bounding_rectangle(list):\n    x0, y0 = list[0]\n    x1, y1 = x0, y0\n    for x,y in list[1:]:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n    return x0,y0,x1,y1\n\nfilename,coordinates = data[0]\nbox = bounding_rectangle(coordinates)\nimg = read_raw_image(filename)\ndraw = Draw(img)\ndraw_dots(draw, coordinates)\ndraw.rectangle(box, outline='red')\nimg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d558ea44539391ea789c845b774399792a90d8fb"},"cell_type":"markdown","source":"# Image preprocessing code\nImages are preprocessed by:\n1. Converting to black&white;\n1. Compressing horizontally by a factor of 2.15 (the mean aspect ratio);\n1. Apply a random image transformation (only for training)\n1. Resizing to 128x128;\n1. Normalizing to zero mean and unit variance.\n\nThese operation are performed by the following code that is later invoked when preparing the corpus."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d911ee27cdae1e2d0838119e621068a8cc24c4d"},"cell_type":"code","source":"# Define useful constants\nimg_shape  = (128,128,1)\nanisotropy = 2.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cbbf2f5525f59cca4d4d8214856180a1349319cc"},"cell_type":"code","source":"import random\nimport numpy as np\nfrom scipy.ndimage import affine_transform\nfrom keras.preprocessing.image import img_to_array\n\n# Read an image as black&white numpy array\ndef read_array(p):\n    img = read_raw_image(p).convert('L')\n    return img_to_array(img)\n\ndef build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation        = np.deg2rad(rotation)\n    shear           = np.deg2rad(shear)\n    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n\n# Compute the coordinate transformation required to center the pictures, padding as required.\ndef center_transform(affine, input_shape):\n    hi, wi = float(input_shape[0]), float(input_shape[1])\n    ho, wo = float(img_shape[0]), float(img_shape[1])\n    top, left, bottom, right = 0, 0, hi, wi\n    if wi/hi/anisotropy < wo/ho: # input image too narrow, extend width\n        w     = hi*wo/ho*anisotropy\n        left  = (wi-w)/2\n        right = left + w\n    else: # input image too wide, extend height\n        h      = wi*ho/wo/anisotropy\n        top    = (hi-h)/2\n        bottom = top + h\n    center_matrix   = np.array([[1, 0, -ho/2], [0, 1, -wo/2], [0, 0, 1]])\n    scale_matrix    = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n    decenter_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n\n# Apply an affine transformation to an image represented as a numpy array.\ndef transform_img(x, affine):\n    matrix   = affine[:2,:2]\n    offset   = affine[:2,2]\n    x        = np.moveaxis(x, -1, 0)\n    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n                                 mode='constant', cval=np.average(channel)) for channel in x]\n    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n\n# Read an image for validation, i.e. without data augmentation.\ndef read_for_validation(p):\n    x  = read_array(p)\n    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    t  = center_transform(t, x.shape)\n    x  = transform_img(x, t)\n    x -= np.mean(x, keepdims=True)\n    x /= np.std(x, keepdims=True) + K.epsilon()\n    return x,t \n\n# Read an image for training, i.e. including a random affine transformation\ndef read_for_training(p):\n    x  = read_array(p)\n    t  = build_transform(\n            random.uniform(-5, 5),\n            random.uniform(-5, 5),\n            random.uniform(0.9, 1.0),\n            random.uniform(0.9, 1.0),\n            random.uniform(-0.05*img_shape[0], 0.05*img_shape[0]),\n            random.uniform(-0.05*img_shape[1], 0.05*img_shape[1]))\n    t  = center_transform(t, x.shape)\n    x  = transform_img(x, t)\n    x -= np.mean(x, keepdims=True)\n    x /= np.std(x, keepdims=True) + K.epsilon()\n    return x,t   \n\n# Transform corrdinates according to the provided affine transformation\ndef coord_transform(list, trans):\n    result = []\n    for x,y in list:\n        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n        result.append((x,y))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df8c54acaeb6bd4cba6dee323fe40b627142616c"},"cell_type":"markdown","source":"# Prepare the corpus\nSplit the corpus between training and validation data. Duplicate the training data 16 times to make reasonable size training epochs."},{"metadata":{"trusted":true,"_uuid":"639b3dbcc5174010b9de4971bc98c34d3a10ebeb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(data, test_size=200, random_state=1)\ntrain += train\ntrain += train\ntrain += train\ntrain += train\nlen(train),len(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"238f91cf49c438e766c4bf4bf7abf1e02da9d690"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras import backend as K\nfrom keras.preprocessing.image import array_to_img\nfrom numpy.linalg import inv as mat_inv\n\ndef show_whale(imgs, per_row=5):\n    n         = len(imgs)\n    rows      = (n + per_row - 1)//per_row\n    cols      = min(per_row, n)\n    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n    for ax in axes.flatten(): ax.axis('off')\n    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n\nval_a = np.zeros((len(val),)+img_shape,dtype=K.floatx()) # Preprocess validation images \nval_b = np.zeros((len(val),4),dtype=K.floatx()) # Preprocess bounding boxes\nfor i,(p,coords) in enumerate(tqdm_notebook(val)):\n    img,trans      = read_for_validation(p)\n    coords         = coord_transform(coords, mat_inv(trans))\n    x0,y0,x1,y1    = bounding_rectangle(coords)\n    val_a[i,:,:,:] = img\n    val_b[i,0]     = x0\n    val_b[i,1]     = y0\n    val_b[i,2]     = x1\n    val_b[i,3]     = y1\n\nidx  = 1\nimg  = array_to_img(val_a[idx])\nimg  = img.convert('RGB')\ndraw = Draw(img)\ndraw.rectangle(val_b[idx], outline='red')\nshow_whale([read_raw_image(val[idx][0]), img], per_row=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"13a67cf59860954a341a1a75a00a79eb52f8f45d"},"cell_type":"markdown","source":"The image on the left side is the original image. The image on the right side is converted to B&W, compressed horizontally, padded and resized to 128x128.<br.>\nThe right side image is annotated with the transformed bounding box."},{"metadata":{"_uuid":"e84a7aa4f1bf8035f61f3be3a7da18ffb9e94ea6"},"cell_type":"markdown","source":"The following class extends the Sequence class from keras to generate input image data augmentation on the fly. Each image is processed through a random affine transformation. The tagged boundary points are also transformed to adjust the dimension of the bounding box."},{"metadata":{"trusted":true,"_uuid":"50243ecf6d4f5b81fcf50a020b27c3a383bda8e0"},"cell_type":"code","source":"from keras.utils import Sequence\n\nclass TrainingData(Sequence):\n    def __init__(self, batch_size=32):\n        super(TrainingData, self).__init__()\n        self.batch_size = batch_size\n    def __getitem__(self, index):\n        start = self.batch_size*index;\n        end   = min(len(train), start + self.batch_size)\n        size  = end - start\n        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n        b     = np.zeros((size,4), dtype=K.floatx())\n        for i,(p,coords) in enumerate(train[start:end]):\n            img,trans   = read_for_training(p)\n            coords      = coord_transform(coords, mat_inv(trans))\n            x0,y0,x1,y1 = bounding_rectangle(coords)\n            a[i,:,:,:]  = img\n            b[i,0]      = x0\n            b[i,1]      = y0\n            b[i,2]      = x1\n            b[i,3]      = y1\n        return a,b\n    def __len__(self):\n        return (len(train) + self.batch_size - 1)//self.batch_size\n\nrandom.seed(1)\na, b = TrainingData(batch_size=5)[1]\nimg  = array_to_img(a[0])\nimg  = img.convert('RGB')\ndraw = Draw(img)\ndraw.rectangle(b[0], outline='red')\nshow_whale([read_raw_image(train[0][0]), img], per_row=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6ecdd4b6ddaae3cefc4a02d82ce1a589d585b0f"},"cell_type":"markdown","source":"The image on the left side is the original image. The image on the right side is converted to B&W, compressed horizontally, randomly transformed, padded and resized to 128x128.<br.>\nThe right side image is annotated with the transformed bounding box."},{"metadata":{"_uuid":"7a87ca0b18396d1860462120a84398fa072de356"},"cell_type":"markdown","source":"# Keras Model\nThe following code fragment shows the bounding box model construction."},{"metadata":{"trusted":true,"_uuid":"90ca34d01e28107b862b389e23bd8df2a7907d35"},"cell_type":"code","source":"from keras.engine.topology import Input\nfrom keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\nfrom keras.models import Model\n\ndef build_model(with_dropout=True):\n    kwargs     = {'activation':'relu', 'padding':'same'}\n    conv_drop  = 0.2\n    dense_drop = 0.5\n    inp        = Input(shape=img_shape)\n\n    x = inp\n\n    x = Conv2D(64, (9, 9), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = Conv2D(64, (3, 3), **kwargs)(x)\n    x = BatchNormalization()(x)\n    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n\n    h = MaxPooling2D(pool_size=(1, int(x.shape[2])))(x)\n    h = Flatten()(h)\n    if with_dropout: h = Dropout(dense_drop)(h)\n    h = Dense(16, activation='relu')(h)\n\n    v = MaxPooling2D(pool_size=(int(x.shape[1]), 1))(x)\n    v = Flatten()(v)\n    if with_dropout: v = Dropout(dense_drop)(v)\n    v = Dense(16, activation='relu')(v)\n\n    x = Concatenate()([h,v])\n    if with_dropout: x = Dropout(0.5)(x)\n    x = Dense(4, activation='linear')(x)\n    return Model(inp,x)\n\nmodel = build_model(with_dropout=True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d1ba2edcfb42d9f94291c99d9539e36243b52c0"},"cell_type":"markdown","source":"Here are a few thoughts aboud the model:\n\n * The basic idea is mostly inspired from the VGG model, with a stack of 3x3 convolutions separated by pooling layers. Here max pooling is replaced by a 2x2 convolution with stride 2. It seemed more logical, as max pooling appears to lose some location information. In practice in makes little difference.\n * At the end, max pooling is used on rows and columns separately. For the fluke height, we don't care if it occurs on the left or right. Similarly for the width, we don't care if it occurs at the top or the bottom. Both sets are concatenated, but clearly one subset is aimed at finding left and right, and the other top and bottom.\n * A few changes from VGG include a larger kernel for the first convolution, batch normalization and dropout. "},{"metadata":{"_uuid":"2a0a7760683822fe319e43e922ba9d580271452d"},"cell_type":"markdown","source":"The model is trained using data augmentation (random affine transformations) as generated by the TrainingData class defined above.<br/>\nDecreasing learning rate and early stopping is used when no significant progress is made.<br/>\nThe model is trained three times. The weights are preserved between runs, but the learning rate is reset to the original value. This essentially cycles the learning rate between low and high values, which can be advantageous according to **Smith, L.N.**,  \"*Cyclical Learning Rates for Training Neural Networks*\", [arXiv:1506.01186](https://arxiv.org/abs/1506.01186)."},{"metadata":{"trusted":true,"_uuid":"a355b4c2f53422dfc30ac389000954d08a37736e","collapsed":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\nfor num in range(1, 4):\n    model_name = 'cropping-%01d.h5' % num\n    print(model_name)\n    model.compile(Adam(lr=0.032), loss='mean_squared_error')\n    model.fit_generator(\n        TrainingData(), epochs=50, max_queue_size=12, workers=4, verbose=1,\n        validation_data=(val_a, val_b),\n        callbacks=[\n            EarlyStopping(monitor='val_loss', patience=9, min_delta=0.1, verbose=1),\n            ReduceLROnPlateau(monitor='val_loss', patience=3, min_delta=0.1, factor=0.25, min_lr=0.002, verbose=1),\n            ModelCheckpoint(model_name, save_best_only=True, save_weights_only=True),\n        ])\n    model.load_weights(model_name)\n    model.evaluate(val_a, val_b, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c93f5f3bceedc765f674ef197dbf3a312450d314"},"cell_type":"markdown","source":"Select the best of the three attempts."},{"metadata":{"trusted":true,"_uuid":"e89ffd1ace4057291242ea82b0c9f91dfbde8b09","collapsed":true},"cell_type":"code","source":"model.load_weights('cropping-1.h5')\nloss1 = model.evaluate(val_a, val_b, verbose=0)\nmodel.load_weights('cropping-2.h5')\nloss2 = model.evaluate(val_a, val_b, verbose=0)\nmodel.load_weights('cropping-3.h5')\nloss3 = model.evaluate(val_a, val_b, verbose=0)\nmodel_name = 'cropping-1.h5'\nif loss2 <= loss1 and loss2 < loss3: model_name = 'cropping-2.h5'\nif loss3 <= loss1 and loss3 <= loss2: model_name = 'cropping-3.h5'\nmodel.load_weights(model_name)\nloss1, loss2, loss3, model_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4ea09792a118595a6eef23e6d3e75029f539ff3a"},"cell_type":"markdown","source":"# Variance normalization\nUsing batch normalization after dropout has a small problem. During training, dropout zeros some outputs, but scales up the remaining ones to maintain the output average. However, the output **variance** is not preserved. The variance is larger during training than it is during inference.<br/>\nBatch normalization also behaves differently during training and inference. During training batches are normalized, but at the same time a running average of batch mean and variance is computed. This running average is used as a sample estimate during inference. It should be immediately obvious that the running average value is not a good approximation of the sample variance during inference, because dropout behavior changes the variance.  See **Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang**, \"*Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift*\", [arXiv:1801.05134](https://arxiv.org/abs/1801.05134).<br/>\nOne of the proposed solutions is to recompute the batch normalization running average without dropout, while freezing other layers. The resulting accuracy is expected to be slightly better."},{"metadata":{"trusted":true,"_uuid":"f2327ef9a1a9a60f7e7216880a48b9bcfa278e9e","collapsed":true},"cell_type":"code","source":"model2 = build_model(with_dropout=False)\nmodel2.load_weights(model_name)\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28bfca6f2ea093672fd94209503c1f623f97936a","collapsed":true},"cell_type":"code","source":"model2.compile(Adam(lr=0.002), loss='mean_squared_error')\nmodel2.evaluate(val_a, val_b, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef74e0a191cf2811ceb8ae8c2c54f380692d0769","collapsed":true},"cell_type":"code","source":"# Recompute the mean and variance running average without dropout\nfor layer in model2.layers:\n    if not isinstance(layer, BatchNormalization):\n        layer.trainable = False\nmodel2.compile(Adam(lr=0.002), loss='mean_squared_error')\nmodel2.fit_generator(TrainingData(), epochs=1, max_queue_size=12, workers=6, verbose=1, validation_data=(val_a, val_b))\nfor layer in model2.layers:\n    if not isinstance(layer, BatchNormalization):\n        layer.trainable = True\nmodel2.compile(Adam(lr=0.002), loss='mean_squared_error')\nmodel2.save('cropping.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9983ecd5228b4d7a1c66305894e4ba78541cfa9d","collapsed":true},"cell_type":"code","source":"model2.evaluate(val_a, val_b, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c6331910e492b55a8c642651e9d0332964c1063"},"cell_type":"markdown","source":"# Explore the results\nThe model is not trained over the validation set, so it represents a fair assessment of the bounding box model accuracy.<br/>\nThe following figure shows the transformed whale images, the reference bounding boxes in red and the computed bounding boxes in yellow for all images from the bounding box validation set (200)."},{"metadata":{"trusted":true,"_uuid":"b808f5da974a9f50a581be7ef6ba98b79e1b9d54","collapsed":true},"cell_type":"code","source":"images = []\nfor i,(p,coords) in enumerate(val[:25]):\n    a         = val_a[i:i+1]\n    rect1     = val_b[i]\n    rect2     = model2.predict(a).squeeze()\n    img       = array_to_img(a[0]).convert('RGB')\n    draw      = Draw(img)\n    draw.rectangle(rect1, outline='red')\n    draw.rectangle(rect2, outline='yellow')\n    images.append(img)\nshow_whale(images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bea4597d97cee09553b296f7f0e2866bfecbd149"},"cell_type":"markdown","source":"# Generate best bounding boxes"},{"metadata":{"trusted":true,"_uuid":"8b39e595a5ebb5cdbf7b4b46e12d7573e2bf25e8","collapsed":true},"cell_type":"code","source":"from pandas import read_csv\n\ntagged = [p for _,p,_ in read_csv('../input/whale-categorization-playground/train.csv').to_records()]\nsubmit = [p for _,p,_ in read_csv('../input/whale-categorization-playground/sample_submission.csv').to_records()]\njoin = tagged + submit\nlen(join)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b16b5f37e18482c177c166db2081ade9335994e","collapsed":true},"cell_type":"code","source":"# If the picture is part of the bounding box dataset, use the golden value.\np2bb = {}\nfor i,(p,coords) in enumerate(data): p2bb[p] = bounding_rectangle(coords)\nlen(p2bb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d620288ef35cb74571b6425d247f81f8d81d8e22","collapsed":true},"cell_type":"code","source":"# For other pictures, evaluate the model.\np2bb = {}\nfor p in tqdm_notebook(join):\n    if p not in p2bb:\n        img,trans         = read_for_validation(p)\n        a                 = np.expand_dims(img, axis=0)\n        x0, y0, x1, y1    = model2.predict(a).squeeze()\n        (u0, v0),(u1, v1) = coord_transform([(x0,y0),(x1,y1)], trans)\n        p2bb[p]           = (u0, v0, u1, v1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbec7374070a47ba5019b10ea19ee9ff2c698a38","collapsed":true},"cell_type":"code","source":"import pickle\n\nwith open('bounding-box.pickle', 'wb') as f: pickle.dump(p2bb, f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ebfca659382623a38066784782f99a96c9f9171"},"cell_type":"markdown","source":"# Show some examples"},{"metadata":{"trusted":true,"_uuid":"99abb236b371d3fe948c7765e1bb1317ffaca867","collapsed":true},"cell_type":"code","source":"samples = []\nfor p in tagged[:25]:\n    img         = read_raw_image(p).convert('RGB')\n    draw        = Draw(img)\n    x0,y0,x1,y1 = p2bb[p]\n    draw.line([(x0, y0),(x0,y1),(x1,y1),(x1,y0),(x0,y0)], fill='yellow', width=6)\n    samples.append(img)\nshow_whale(samples)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d971dd745d01e16626c6c8a984a0090ce64829b"},"cell_type":"markdown","source":"# Generated files"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"624b296aa0f31deaf7ae9fe57e6dbcbe3ead232d"},"cell_type":"code","source":"import os\n\nos.remove('cropping-1.h5')\nos.remove('cropping-2.h5')\nos.remove('cropping-3.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3cdd795d335a94cd2f92ba5cbb0b3759e392276","collapsed":true},"cell_type":"code","source":"!ls *.pickle *.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"483bea830e61c5aafc8233cf3c26b17cd8f59dc5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}