{"cells":[{"metadata":{"_uuid":"14b334fe7ca8164488c7c93a90ce3237d36a063a"},"cell_type":"markdown","source":"# Data Wrangling for [Recruit Restaurant Visitor Forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting)\n---\nHey everyone! It's my first kernel on Kaggle and I appreciate any and all feedbacks  on this work.\nThis document breaks down the data wrangling tasks into the following steps:\n\n* [1. Load Datasets](#1.-Load-Datasets): imports Python libraries and loads data files into DataFrames.\n\n\n* [2. Visual Inspection](#2.-Visual-Inspection): examines the shape, column names and data types used by DataFrames.\n    \n    \n* [3. Combine Datasets](#3.-Combine-Datasets): joins DataFrames into a single entity.    \n\n\n* [4. Cleaning Data](#4.-Cleaning-Data): analyses the amount of work that needs to be done and performs the steps necessary to make the data tidy and clean.    \n\n\n* [5. Preliminary Visual Exploratory Data Analysis](#5.-Preliminary-Visual-Exploratory-Data-Analysis): renders a chart to visualize the amount of reservations and visitations made along the years."},{"metadata":{"_uuid":"28f51f1198c2cf1675be88cdaec934ab22ab31ed"},"cell_type":"markdown","source":"---\n## 1. Load Datasets"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d1737d932e7ad5676a57c75e3bbaa1d545913da4"},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n%matplotlib inline\n\nair_reserve_df = pd.read_csv('../input/air_reserve.csv')\nair_store_df = pd.read_csv('../input/air_store_info.csv')\nair_visit_df = pd.read_csv('../input/air_visit_data.csv')\n\nhpg_reserve_df = pd.read_csv('../input/hpg_reserve.csv')\nhpg_store_df = pd.read_csv('../input/hpg_store_info.csv')\n\ndate_info_df = pd.read_csv('../input/date_info.csv')\nstore_ids_df = pd.read_csv('../input/store_id_relation.csv')","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"4a5f73646451215ef3dc6e872c3ac9e2081e47d8"},"cell_type":"markdown","source":"---\n## 2. Visual Inspection\nA brief examination of the dataframes involved helps to get a better sense of the data structures that are going to be manipulated."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"77f733d9ead05e05c711c7049d9dd282c67f09b1"},"cell_type":"code","source":"def visual_inspect(df, size=3):\n    print('>>> Shape =', df.shape)\n    display(df.head(size))    \n    print('>>> Types\\n', df.dtypes)\n    #print('\\n>>> Info')\n    #df.info()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0cdcbd8d9407d320fb476dd1415dde868383417f"},"cell_type":"code","source":"visual_inspect(air_reserve_df)","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e9a724ac5e63f92a1d9b84762a796e3ca657d2c4"},"cell_type":"code","source":"visual_inspect(air_store_df)","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6691a07262b585560ede6c7039037f0db25deaf1"},"cell_type":"code","source":"visual_inspect(air_visit_df)","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1b768446ab0ed6aa352942c5a6386ab28a5b7be2"},"cell_type":"code","source":"visual_inspect(hpg_reserve_df)","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c6a984f696a1ef1d45714319d94a5c5a41210fd5"},"cell_type":"code","source":"visual_inspect(hpg_store_df)","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"354ea780807a2690a69cc355c1c19e8abc461155"},"cell_type":"code","source":"visual_inspect(store_ids_df)","execution_count":8,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"19a1db089be39f2a9c6c069222d97145c21dec27"},"cell_type":"code","source":"visual_inspect(date_info_df)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"3cacb44332c23e32341b4c061b3d5f29e62457c8"},"cell_type":"markdown","source":"---\n## 3. Combine Datasets\nThis section starts by joining `air_visit_df` with `air_reserve_df` on key `air_store_id`. The resulting DataFrame, `df`, is subsequentely merged with `air_store_df` thus containing all relevant data from the AIR system.\n\nNext, data from `hpg_reserve_df` is merged with `store_ids_df` into a new DataFrame, `hpg_df`. Then, `hpg_store_df` data is also added to this DataFrame and the merging of all HPG data is done.\n\nFinally, after `df` and `hpg_df` are merged together, holyday information is added from `date_info_df`."},{"metadata":{"_uuid":"cedbb79d5d64e7486ff58bfa44df70bb954dc20f"},"cell_type":"markdown","source":"### 3.1  `df = air_visit_df + air_reserve_df`"},{"metadata":{"_uuid":"32a3077e5a51431eb03b0c90bb120d01dac0cf21"},"cell_type":"markdown","source":"Prepare the data for merge:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a6fa1498f7e2b012dadcda203cee610db892b070"},"cell_type":"code","source":"# Add a visit_date column for air_reserve\nair_reserve_df['visit_datetime'] = pd.to_datetime(air_reserve_df['visit_datetime'])\nair_reserve_df['visit_date'] = air_reserve_df['visit_datetime'].apply(lambda x: x.strftime('%Y-%m-%d'))\n\n# Rename columns so they match during merge\nair_reserve_df.rename(columns={'reserve_visitors': 'visitors'}, inplace=True)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"06b58eb01d474dd16b45fa8ebb8882f6a6c43c4d"},"cell_type":"markdown","source":"Looking at the [documentation](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/data) on **air_visit_data.csv** and **air_reserve.csv** got me thinking that data from `air_reserve_df` brings extra info on visitation. For this merge, we treat it as something to be added to `air_visit_df`, hence the `outer` method:"},{"metadata":{"trusted":false,"_uuid":"f130750aac2b88609468dad13c86a5e94af06432"},"cell_type":"code","source":"print('>>> air_visit_df   shape=', air_visit_df.shape) \n# display(air_visit_df.head(2))\nprint('>>> air_reserve_df shape=', air_reserve_df.shape) \n# display(air_reserve_df.head(2))\n\nprint('>>> Merging...\\n')\ndf = pd.merge(air_visit_df, air_reserve_df, on=[\"air_store_id\", \"visit_date\", \"visitors\"], how=\"outer\")\n\nprint('>>> df             shape=', df.shape) \ndisplay(df.head())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"35f3428d8cf92316252651a8b1437c1b8d9644d2"},"cell_type":"markdown","source":"### 3.2 `df = df + air_store_df`"},{"metadata":{"trusted":false,"_uuid":"544c70242932822debde87164e08bf6cdc875ad3"},"cell_type":"code","source":"print('>>> df             shape=', df.shape) \n# display(df.head(2))\nprint('>>> air_store_df   shape=', air_store_df.shape) \n# display(air_store_df.head(2))\n\nprint('>>> Merging...\\n')\ndf = pd.merge(df, air_store_df, on=\"air_store_id\", how=\"left\")\n\nprint('>>> df             shape=', df.shape)\ndisplay(df.head(3))","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"b74c1aba70b9d271bae56b8b805e0339bce28f57"},"cell_type":"markdown","source":"### 3.3 `hpg_df = hpg_reserve_df + store_ids_df`"},{"metadata":{"trusted":false,"_uuid":"386173c42a6b23e03b77df4b4470f946f6a9ea72"},"cell_type":"code","source":"# convert visit_datetime from string to datetime object to add a new column containing only the Y-M-D\nhpg_reserve_df['visit_datetime'] = pd.to_datetime(hpg_reserve_df['visit_datetime'])\nhpg_reserve_df['visit_date'] = hpg_reserve_df['visit_datetime'].apply(lambda x: x.strftime('%Y-%m-%d'))\n\nprint('>>> hpg_reserve_df shape=', hpg_reserve_df.shape)\ndisplay(hpg_reserve_df.head(1))\nprint('\\n>>> store_ids_df   shape=', store_ids_df.shape)\ndisplay(store_ids_df.head(1))\n\n# replace the HPG code in hpg_merged_df for its equivalent AIR code\nprint('>>> Merging...\\n')\nhpg_df = pd.merge(hpg_reserve_df, store_ids_df, on='hpg_store_id', how='left')\n\nprint('\\n>>> hpg_df         shape=', hpg_df.shape)\ndisplay(hpg_df.head(3))\n\n# DEBUG: print rows that have valid air_store_id and hpg_store_id\nvalid_ids_df = hpg_df[hpg_df.air_store_id.notnull() & hpg_df.hpg_store_id.notnull()]\nvalid_ids_percent = len(valid_ids_df.index) / len(hpg_df.index) * 100\nprint('>>> hpg_df rows with valid HPG & AIR ids: ',  round(valid_ids_percent, 1), '%')","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"4acc2698225185de41bb9704fdf69c04fa1c8434"},"cell_type":"markdown","source":"### 3.4 `hpg_df = hpg_df + hpg_store_df`"},{"metadata":{"trusted":false,"_uuid":"92dfb63714d5ff09d33ee7053c149beaf2a5c821"},"cell_type":"code","source":"hpg_df = pd.merge(hpg_df, hpg_store_df, on=\"hpg_store_id\", how=\"left\")\n\nprint('>>> hpg_df         shape=', hpg_df.shape)\ndisplay(hpg_df.head(3))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"dd5cfa68651a86658fd195e2eb0ffb6f531ca956"},"cell_type":"markdown","source":"### 3.5 `df = df + hpg_df`"},{"metadata":{"trusted":false,"_uuid":"16e2a3359ecb5307546c01faabf114ac7b7e4741"},"cell_type":"code","source":"# rename columns in both dataframes so they blend in nicely during merge\nhpg_df.rename(columns={'reserve_visitors': 'visitors'}, inplace=True)\nhpg_df.rename(columns={'hpg_genre_name': 'genre_name'}, inplace=True)\nhpg_df.rename(columns={'hpg_area_name' : 'area_name'}, inplace=True)\ndf.rename(columns={'air_genre_name': 'genre_name'}, inplace=True)\ndf.rename(columns={'air_area_name' : 'area_name'}, inplace=True)\n\nprint('>>> df             shape=', df.shape)\n# display(df.head(2))\nprint('>>> hpg_df         shape=', df.shape)\n# display(hpg_df.head(2))\n\nprint('\\n>>> Merging...\\n')\ndf = pd.concat([df, hpg_df], axis=0).reset_index(drop=True)\n\nprint('>>> df             shape=', df.shape)\ndisplay(df.head(3))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"5c0badb1afc67a388bc86e1806bd41ca0951cf57"},"cell_type":"markdown","source":"### 3.6 `df = df + date_info_df`"},{"metadata":{"trusted":false,"_uuid":"8dd2b37324983a6a2695346163a7218ab0bdb686"},"cell_type":"code","source":"print('>>> date_info_df   shape=', date_info_df.shape)\n# display(date_info_df.head(2))\nprint('>>> df             shape=', df.shape)\n# display(df.tail(3))\n\n# convert column visit_datetime from string to datetime object\ndf['visit_datetime'] = pd.to_datetime(df['visit_datetime'])\n\n# combine both datasets to add columns 'day_of_week' and 'holiday_flg' to merged_df\nprint('\\n>>> Merging...\\n')\ndf = pd.merge(df, date_info_df, left_on='visit_date', right_on='calendar_date')\ndel df['calendar_date']\n\nprint('>>> df             shape=', df.shape)\ndisplay(df.tail(3))","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"43322955dd1a28f53291e8e6df42d114332f4ecc"},"cell_type":"markdown","source":"---\n## 4. Cleaning Data\nThe cleaning procedure starts by checking if there are rows full of *NaN* values. If there are, this would indicate a serious problem during the merging procedures. Later, IDs are checked to see if they have valid data. Then, column data types are evaluated and adjusted. The amount of work that needs to be done to clean *NaN* values is also quantified in plot. Finally, a sequence of steps are executed to clean the remaining data."},{"metadata":{"_uuid":"6719a7f5be3c2d3ad5690a2a7ba36ff76022e8cf"},"cell_type":"markdown","source":"### 4.1 Checking for *NaN* rows"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3b4847664132a28e600a3f047d489fbf3363652d"},"cell_type":"code","source":"def check_nan_rows(df):\n    ## Store all rows with NaN values\n    nan_df = df[pd.isnull(df).any(axis=1)]\n    print('>>> Number of rows that have at least ONE NaN value:', len(nan_df.index))\n    #display(nan_df.head())\n\n    ## List only rows that have NaN values (in all columns)\n    nan_rows_df = nan_df[nan_df.isnull().all(1)]      \n    nan_rows_count = len(nan_rows_df.index)\n    print('>>> Number of rows completely filled with NaN values:', nan_rows_count)    \n    #display(nan_rows_df.head())\n\n    if (nan_rows_count != 0):\n        print('>>> Oopsie! Found', nan_rows_count, 'NaN rows:')\n        display(nan_rows_df.head())        ","execution_count":17,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fa8f3dcb4e6af2f7ccad1911e4a21432d5242664"},"cell_type":"code","source":"check_nan_rows(df)  ","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"a861cefc2759ffcd546c28600fdc2302cb325f07"},"cell_type":"markdown","source":"### 4.2 Checking for valid IDs"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"67cbe4e093f1adee86fc5b6f4b4727692617416b"},"cell_type":"code","source":"def check_IDs(df):\n    # Count how many IDs are NaNs\n    print('>>> NaN count on air_store_id:', df.air_store_id.isnull().sum())\n    print('>>> NaN count on hpg_store_id:', df.hpg_store_id.isnull().sum())\n\n    # Count how many rows don't have a valid ID (i.e. air_store_id and hpg_store_id are both NaNs)\n    null_ids_df = df[df.air_store_id.isnull() & df.hpg_store_id.isnull()]\n    null_ids_count = len(null_ids_df.index)\n    print('>>> Number of rows without any IDs:', null_ids_count)\n    \n    if (null_ids_count != 0):\n        display(null_ids_df.head())\n\n    # Check how many IDs don't have any data at all (i.e. all columns are NaNs)\n    max_nan_per_row = len(df.columns) - 1    \n    df['NaN_count'] = df.isnull().sum(axis=1)    \n    invalid_ids_df = df[df['NaN_count'] == max_nan_per_row]    \n    \n    invalid_ids_count = len(invalid_ids_df.index)\n    print('>>> Number of rows with IDs but NO DATA at all connected to them:', invalid_ids_count)\n    \n    if (invalid_ids_count != 0):\n        print('>>> Found', invalid_ids_count, ' rows with IDs that have NO DATA at all')\n        display(invalid_ids_df.head())\n        \n    del df['NaN_count']","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7267b225ef681247ec29e14fd45145678883268d"},"cell_type":"code","source":"check_IDs(df)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"01bb52744118bff5b3c1435beef0055a2cb1cf37"},"cell_type":"markdown","source":"### 4.3 Checking data types"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cc87f1bebacc712541adf1a742925d4971fdc4c7"},"cell_type":"code","source":"def get_data_types(df):    \n    col_names = df.columns.tolist()\n    data_types_df = pd.DataFrame(columns=col_names)    \n    \n    row = []\n    for col in col_names:        \n        row.append(str(df[col].dtype))\n\n    data_types_df.loc[0] = row\n    return data_types_df","execution_count":21,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9d599589c313eee08f3aa3b603330f3da9437957"},"cell_type":"code","source":"dtypes_df = get_data_types(df)\ndisplay(dtypes_df.head())","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"a9ee23392a6c4eb5ae305a953bd111deda005ef4"},"cell_type":"markdown","source":"Most column data types makes sense except `reserve_datetime` and `visit_date`, which should be converted to *datetime* objects:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7bc5a054cf57c6776c97a3d9e5a357edf549daf8"},"cell_type":"code","source":"df['reserve_datetime'] = pd.to_datetime(df['reserve_datetime'])\ndf['visit_date'] = pd.to_datetime(df['visit_date'])","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"6528f956df5368f2e47286c133799c2f0f84c380"},"cell_type":"markdown","source":"**The final data types are:**"},{"metadata":{"trusted":false,"_uuid":"aec045558bb2cb2c1cca74a75f74003b99608110"},"cell_type":"code","source":"dtypes_df = get_data_types(df)\ndisplay(dtypes_df.head())","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"402c830914c185ccf29186443a5dd8add4f35d1a"},"cell_type":"markdown","source":"### 4.4 Checking for corrupt data (negative values)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fa6118c90e845fea6166154ad20612afa046dcc4"},"cell_type":"code","source":"# Checks numeric columns for negative numbers\ndef check_numeric(df):     \n    neg_count_df = pd.DataFrame(columns=['Negative values count'])\n    \n    #print('>>> Number of negative values found in numeric columns:')\n    num_col_list = list(df.select_dtypes(include=['int64', 'float64']).columns)    \n    \n    total_neg = 0\n    for col_name in num_col_list:\n        neg_count = df[df[col_name] < 0].shape[0]  # extract number of rows        \n        #print('\\t*' + col_name+ '* = ' + str(neg_count))\n        neg_count_df.loc[col_name] = neg_count\n        total_neg += neg_count\n    \n    return neg_count_df, total_neg","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba5870033ee8c14946f39f813c57435d4d220d00"},"cell_type":"code","source":"neg_count_df, total_neg_count = check_numeric(df)\ndisplay(neg_count_df)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"2ace8e64f5e9ca210153b6ed3fa32a5c6be3b0f4"},"cell_type":"markdown","source":"### 4.5 Checking for invalid data (*NaN*)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e3db48c84037ca61b5305fffb3cea5a2d8fcf6cc"},"cell_type":"code","source":"# Checks columns separately for NaN values\ndef check_invalid(df): \n    nan_count_df = pd.DataFrame(columns=['NaN values count'])\n    \n    #print('>>> Number of NaN values found in each column:')\n    col_list = list(df.columns.tolist())    \n    \n    total_nan = 0\n    for col_name in col_list:\n        nan_count = df[col_name].isnull().sum()        \n        #print('\\t' + col_name+ ' = ' + str(nan_count))\n        nan_count_df.loc[col_name] = nan_count        \n        total_nan += nan_count\n    \n    return nan_count_df, total_nan","execution_count":27,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5338f3a6de6a69b150552c37e686f0023cdb09e2"},"cell_type":"code","source":"# count how many NaNs are there\nnan_count_df, total_nan_count = check_invalid(df)\ndisplay(nan_count_df)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"d4f9c87ca79f50199f3fb6fe056f369f09d8196f"},"cell_type":"markdown","source":"### 4.6 Missing Data Visualization (*NaN*)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bee23b050d24f7cf18ad37c7e3ab19f803d28254"},"cell_type":"code","source":"# plots a pie chart to visualize data corruption\ndef plot_corruption(total_values, total_neg_count, total_nan_count):\n    neg_fraction = total_neg_count / total_values * 100\n    nan_fraction = total_nan_count / total_values * 100\n    ok_fraction  =  (total_values - (neg_fraction + nan_fraction)) / total_values * 100\n\n    # Data to plot\n    labels = 'Negative values', 'NaN values', 'Good values'\n    sizes = [neg_fraction, nan_fraction, ok_fraction]\n    colors = ['orange', 'lightcoral', 'yellowgreen']\n    explode = (0.2, 0.2, 0)  # explode 1st and 2nd slice\n\n    # Plot\n    plt.subplots(figsize=(9,4))\n    patches, texts = plt.pie(sizes, explode=explode, labels=labels, colors=colors, shadow=True, startangle=330)\n    plt.legend(patches, labels, loc=\"best\")\n    plt.axis('equal')\n    plt.show()\n    \n    print('>>>', round(nan_fraction, 1), '% of the values are NaNs.')\n    print('>>> There are', total_values, 'cells in the DataFrame. Shape =', df.shape)","execution_count":29,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ab4357af4af4431a802263907bf8365332c7973"},"cell_type":"code","source":"total_values = df.shape[0] * df.shape[1]\nplot_corruption(total_values, total_neg_count, total_nan_count)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"b878e444b804dfb38b7699d2cba581bc0d91bbd4"},"cell_type":"markdown","source":"### 4.7 Dealing with *NaN* values\n\n#### 4.7.1 *NaN* on *string* columns\nOn these columns, cells that contain *NaN* values are replaced by empty literals:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"47bb8c7dcee13f76975c171478e27070fa371129"},"cell_type":"code","source":"df['area_name'].fillna(value='', inplace=True)\ndf['genre_name'].fillna(value='', inplace=True)","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"c98384ee909d89d477ff1d3b84e7611f072f1a97"},"cell_type":"markdown","source":"#### 4.7.2 *NaN* on float columns `air_store_id` and `hpg_store_id`\nFirst, populate missing AIR and HPG ids from the lookup table `store_ids_df`:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ca9ff9ad9dcb8ae6999b6eb251c1d6a8fdb6b96e"},"cell_type":"code","source":"df.fillna(pd.concat([ df.hpg_store_id.map(store_ids_df.set_index('hpg_store_id').air_store_id),\n                      df.air_store_id.map(store_ids_df.set_index('air_store_id').hpg_store_id),\n                    ], axis=1, keys=['air_store_id', 'hpg_store_id']), inplace=True)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"c770b3b9372e78977ac5583f326b521bc659da17"},"cell_type":"markdown","source":"The remaining *NaNs* on these columns are replaced by empty strings:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"85ad8727a27ec61854cddbc027aa842268d23703"},"cell_type":"code","source":"df['air_store_id'].fillna(value='', inplace=True)\ndf['hpg_store_id'].fillna(value='', inplace=True)","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"729d7fa0b899d08eb4eb20d0fdee6b6e82a1cf44"},"cell_type":"markdown","source":"#### 4.7.3 *NaN* on *numeric* columns\nAll *NaN* values are replaced by `-1`."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"542b2c330ecb3b5ecf57056c8954adcb36e2b9d1"},"cell_type":"code","source":"df['latitude'].fillna(value=-1, inplace=True)\ndf['longitude'].fillna(value=-1, inplace=True)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"5e171e1563027f733560389e2b710c8322174675"},"cell_type":"markdown","source":"#### 4.7.4 *NaN* on *datetime* columns\nUse sentinel value -9999 for missing dates:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c65edd7cab941d5fc2423cbbea83e15334ec98ce"},"cell_type":"code","source":"df['reserve_datetime'].fillna(value=-9999, inplace=True)\ndf['visit_datetime'].fillna(value=-9999, inplace=True)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"0bdd89abf2b3e8c3c611391cb5d6ede5f7f28794"},"cell_type":"markdown","source":"#### 4.7.5 Check for missing data (again)\nAfter all the clean up, no *NaNs* should be found."},{"metadata":{"trusted":false,"_uuid":"93b61ff184b6094e6729fbe703e23a8cc2d3fd1b"},"cell_type":"code","source":"nan_count_df, total_nan_count = check_invalid(df)\ndisplay(nan_count_df)","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"de22b40801311f3565505c1d3d24dc4fa60b39f3"},"cell_type":"markdown","source":"## 5. Preliminary Visual Exploratory Data Analysis\nPlot the number of reservations (per week) in contrast with the number of visitations (per week)."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f8de4593bad1c237d934a6e4c131dea7afc52912"},"cell_type":"code","source":"def plot_reservation_vs_visitors(df):\n    # set a datetime index so resample() works properly\n    df = df.set_index('visit_date')\n    reservations_made_df = df.resample('W').apply({'visitors':'count'})\n    visitors_df = df.resample('W').apply({'visitors':'sum'})\n\n    activity_df = pd.concat([reservations_made_df, visitors_df], join='inner', axis=1)\n    activity_df.columns = ['reservations_made', 'visitors']\n\n    fig, ax = plt.subplots(figsize=(15,5))\n\n    # There's a bug in Pandas' plot() that handles data differently than Matplotlib's plot()\n    # What this means is that if you use Pandas' plot() you cannot change xticks values later.\n    #ax = activity_df.plot(kind='area', ax=ax, stacked=True, title='Reservations made and number of visitors (per week)')\n\n    # alternative method: use matplotlib's plot_date() and configure xticks at will\n    plt.plot_date(activity_df.index.to_pydatetime(), activity_df, fmt='-')\n    plt.title('Number of reservations made VS visitors (per week)')    \n    ax = plt.gca()\n    ax.xaxis.grid(True, which=\"major\")\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b\\n%y'))\n\n    d = activity_df.index.to_pydatetime()\n    h_v = plt.fill_between(d, activity_df['visitors'], activity_df['reservations_made'], \n                           activity_df['visitors'] >= activity_df['reservations_made'],\n                           facecolor='orange', alpha=0.5, interpolate=True)\n    h_r = plt.fill_between(d, 0, activity_df['reservations_made'], \n                           facecolor='blue', alpha=0.5, interpolate=True)\n    \n    plt.legend(handles=[h_v, h_r], labels=[\"Visitors\", \"Reservations\"], loc='upper left')\n\n    ax.set_ylabel('Number of people')\n    ax.margins(0.005, 0) # set margins to avoid \"whitespace\" while showing the first x-tick\n    plt.show()","execution_count":38,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a8177e381b6db00e2cfe47f79724f59a13e8c26"},"cell_type":"code","source":"plot_reservation_vs_visitors(df)","execution_count":39,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8471c2a7596598b7c37ab59fe0d40310636f501c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}