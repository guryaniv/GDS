{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# handle imports\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# define initialisers\n#------------------------------------------------------------------------------\n\n# plain initialisation\ndef random_normal(shape, scale=1., mean=0.):\n    return np.random.normal(mean, scale, shape)\n\ndef random_uniform(shape, scale=1.):\n    return np.random.uniform(-scale, scale, shape)\n\ndef init_zeros(shape):\n    return np.zeros(shape)\n\ndef init_ones(shape):\n    return np.ones(shape)\n\ndef init_constant(shape, constant):\n    return np.full(shape, constant)\n\n\n# lecun initialisations\ndef lecun_normal(shape):\n    sd = np.sqrt(3./shape[0])\n    return np.random.normal(0., sd, shape)\n\ndef lecun_uniform(shape):\n    limit = np.sqrt(3./shape[0])\n    return np.random.uniform(-limit, limit, shape)\n\n\n# he initialisations\ndef he_normal(shape):\n    sd = np.sqrt(2./shape[0])\n    return np.random.normal(0., sd, shape)\n\ndef he_uniform(shape):\n    limit = np.sqrt(2./shape[0])\n    return np.random.uniform(-limit, limit, shape)\n\n\n# glorot initialisations\ndef glorot_normal(shape):\n    sd = np.sqrt(2./np.sum(shape))\n    return np.random.normal(0., sd, shape)\n\ndef glorot_uniform(shape):\n    limit = np.sqrt(2./np.sum(shape))\n    return np.random.uniform(-limit, limit, shape)\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a38a81b7168f246a94fc3784bc1034508dc8a037"},"cell_type":"code","source":"# define activations\n#------------------------------------------------------------------------------\n\n'''\nActivations\n-----------\n\nthis module houses all the activation functio classes. the classes themselves\ncan be used for calculate the activation function or the gradient of the \nactivation function.\n\n'''\n\n\nclass Activation(object):\n    \n    def __init__(self):    \n        pass\n    \n    def forward(self, x):\n        '''runs the activation function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `False`. This will compute the activation function.\n        \n        Parameters\n        ----------\n        x : np.array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function has been applied.\n        '''\n        pass\n    \n    def backward(self, x):\n        '''runs the derivative of the activation function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `True`. This will compute the derivative activation function.\n        \n        Parameters\n        ----------\n        x : np.array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the derivative of the activation function \n            has been applied.\n        '''\n        pass\n    \n    def __call__(self, x, deriv=False):\n        '''runs the activation function (or its derivative)\n        \n        this class is directly callable, each layer should be passed an \n        instance of one of these activation functions.\n        \n        Parameters\n        ----------\n        x : np.array\n            the array to apply the activation function to\n        deriv : boolean\n            controls whether the activation function or its derivative will be \n            ran when called, eg. during forward propagation set to `False` and\n            during back propagation set to `True`\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function or its derivative \n            has been applied.\n        '''\n        if deriv:\n            return self.backward(x)\n        return self.forward(x)\n\n\nclass Linear(Activation):\n    \n    def __init__(self, m=1.0, c=0.):\n        self.m = m\n        self.c = c\n    \n    def forward(self, x):\n        r'''returns the applied linear function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `False`. This will compute the activation function.\n        \n        .. math:: \n            f(x) = mx + c\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function has been applied.\n        '''\n        return (m * x) + c\n    \n    def backward(self, x):\n        r'''runs the derivative of the activation function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `True`. This will compute the derivative activation function.\n        \n        .. math:: \n            f'(x) = m\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the derivative of the activation function \n            has been applied.\n        '''        \n        return np.ones(x.shape) * m\n\n\nclass ReLU(Activation):\n    \n    def __init__(self, stable=True):\n        self.stable = stable\n    \n    def forward(self, x):\n        r'''returns the applied ReLU function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `False`. This will compute the activation function.\n        \n        The ReLU function is a modern activation fucntion which will return the\n        linear function when (`x`) :math:`x>0` and will return `0` when \n        :math:`x<=>0`\n        \n        .. math:: \n            f(x) =\n              \\begin{cases}\n                x    & \\quad \\text{if } x>0\\\\\n                0    & \\quad \\text{if } x<=0\n              \\end{cases}\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function has been applied.\n        '''\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return x * (x > 0)\n    \n    def backward(self, x):\n        r'''runs the derivative of the activation function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `True`. This will compute the derivative activation function.\n        \n        Due to the fact that the ReLU function will return `x` when :math:`x>0` \n        and `0` when :math:`x<=0`, the gradient is simply `1.0` when \n        :math:`x>0` and `0` when :math:`x<=0`\n        \n        .. math:: \n            f'(x) =\n              \\begin{cases}\n                1.0  & \\quad \\text{if } x>0\\\\\n                0    & \\quad \\text{if } x<=0\n              \\end{cases}\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the derivative of the activation function \n            has been applied.\n        '''\n        return 1. * (x > 0)\n\n\nclass LeakyReLU(Activation):\n    \n    def __init__(self, stable=True, alpha=0.5):\n        self.stable = stable\n        self.alpha = alpha\n    \n    def forward(self, x):\n        r'''returns the applied ReLU function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `False`. This will compute the activation function.\n        \n        The Leaky ReLU function is an improved version of the ReLU function. It\n        is more robust against the dying neuron problem of the ReLU function \n        because it doesnt return a `0` for all values where :math:`x<=0`. \n        Instead it returns a linear function with a very small gradient \n        :math:\\alpha. Therefor it can be described as the inear function when \n        ()`x`) :math:`x>0` and will return :math:`\\alpha x` when :math:`x<=0`\n        \n        .. math:: \n            f(x) =\n              \\begin{cases}\n                x & \\quad \\text{if } x>0\\\\\n                \\alpha x  & \\quad \\text{if } x<=0\n              \\end{cases}\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function has been applied.\n        '''        \n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return x * (x > 0) + x * self.alpha * (x <= 0)\n    \n    def backward(self, x):\n        r'''runs the derivative of the activation function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `True`. This will compute the derivative activation function.\n        \n        Due to the fact that the ReLU function will return `x` when :math:`x>0` \n        and `0` when :math:`x<=>0`, the gradient is simply `1.0` when \n        :math:`x>0` and `0` when :math:`x<=>0`\n        \n        .. math:: \n            f'(x) =\n              \\begin{cases}\n                1.0  & \\quad \\text{if } x>0\\\\\n                \\alpha & \\quad \\text{if } x<=0\n              \\end{cases}\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the derivative of the activation function \n            has been applied.\n        '''        \n        return 1. * (x > 0) + self.alpha * (x <= 0)\n\nclass SchlerpReLU(Activation):\n    r'''\n    .. math::\n        \n        \\alpha_+ &= \\text{pos_alpha}\\\\\n        \\alpha_- &= \\text{neg_alpha}\\\\\n        xstep_+ &= arctanh(\\sqrt{1-\\alpha_+})\\\\\n        xstep_- &= -arctanh(\\sqrt{1-\\alpha_-})\\\\\n        ystep_+ &= tanh(xstep_+) - \\alpha_+ * xstep_+\\\\\n        ystep_- &= tanh(xstep_-) - \\alpha_- * xstep_-\n    \n    '''\n    def __init__(self, stable=True, m=1.0, c=0.0,\n                 alpha_pos=0.1, alpha_neg=0.01, \n                 xstep_pos=1.0, xstep_neg=-1.0):\n        self.stable = stable\n        self.m = m\n        self.c = c\n        self.alpha_pos = alpha_pos\n        self.alpha_neg = alpha_neg\n        self.xstep_pos = xstep_pos\n        self.xstep_neg = xstep_neg\n        self.ystep_pos = (self.m*self.xstep_pos)+self.c - (self.xstep_pos * self.alpha_pos)\n        self.ystep_neg = (self.m*self.xstep_neg)+self.c - (self.xstep_neg * self.alpha_neg)\n    \n    def forward(self, x):\n        r'''returns the applied ReLU function\n        \n        this method is used when the calss is called with the `deriv` parameter\n        set to `False`. This will compute the activation function.\n        \n        The schlerp ReLU function is an experimental version of the ReLU and\n        tanh functions. It can be explained simply as the tanh function, with \n        added linear functions to either end that stop the tanh function from \n        saturating completely since there is no asymptote at `y=-1` and `y=1`.\n        \n        .. math::\n            \n            f(x) =\n              \\begin{cases}\n                \\alpha x  & \\quad \\text{if } x<=xstep_-\\\\\n                x         & \\quad \\text{if } xstep_-<=x<=xstep_+\\\\\n                \\alpha x  & \\quad \\text{if } x<=xstep_+\n              \\end{cases}\n        \n        Parameters\n        ----------\n        x : array\n            the array to apply the activation function to\n        \n        Returns\n        -------\n        np.array\n            the output array after the activation function has been applied.\n        '''\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return ((x * self.alpha_pos) + self.ystep_pos) * (x > self.xstep_pos) + \\\n               ((self.m*x)+self.c) * (np.logical_and(self.xstep_neg <= x, self.c <= self.xstep_pos)) + \\\n               ((x * self.alpha_neg) + self.ystep_neg) * (x < self.xstep_neg)\n    \n    def backward(self, x):\n     \n        return self.alpha_pos * (x > self.xstep_pos) + \\\n               self.m * (np.logical_and(self.xstep_neg <= x, self.c <= self.xstep_pos)) + \\\n               self.alpha_neg * (x < self.xstep_neg)\n\n\nclass SchlerpTanh(Activation):\n    r'''\n    .. math::\n        \n        \\alpha_+ &= \\text{pos_alpha}\\\\\n        \\alpha_- &= \\text{neg_alpha}\\\\\n        xstep_+ &= arctanh(\\sqrt{1-\\alpha_+})\\\\\n        xstep_- &= -arctanh(\\sqrt{1-\\alpha_-})\\\\\n        ystep_+ &= tanh(xstep_+) - \\alpha_+ * xstep_+\\\\\n        ystep_- &= tanh(xstep_-) - \\alpha_- * xstep_-\n    \n    '''\n    def __init__(self, stable=True, pos_alpha=0.1, neg_alpha=0.01):\n        self.stable = stable\n        self.pos_alpha = pos_alpha\n        self.neg_alpha = neg_alpha\n        self.pos_x_step = np.arctanh(np.sqrt(1-self.pos_alpha))\n        self.neg_x_step = -np.arctanh(np.sqrt(1-self.neg_alpha))\n        self.pos_y_step = np.tanh(self.pos_x_step) - \\\n                          (self.pos_x_step * self.pos_alpha)\n        self.neg_y_step = np.tanh(self.neg_x_step) - \\\n                          (self.neg_x_step * self.neg_alpha)\n    \n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return ((x * self.pos_alpha) + self.pos_y_step) * (x > self.pos_x_step) + \\\n               np.tanh(x) * (np.logical_and(self.neg_x_step <= x, x <= self.pos_x_step)) + \\\n               ((x * self.neg_alpha) + self.neg_y_step) * (x < self.neg_x_step)\n    \n    def backward(self, x):\n        return self.pos_alpha * (x > self.pos_x_step) + \\\n               (1.0 - np.square(np.tanh(x))) * (np.logical_and(self.neg_x_step <= x, x <= self.pos_x_step)) + \\\n               self.neg_alpha * (x < self.neg_x_step)\n\n\nclass ELU(Activation):\n    \n    def __init__(self, stable=True, alpha=1.0):\n        self.stable = stable\n        self.alpha = alpha\n    \n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return x * (x > 0) + self.alpha * (np.exp(x)-1) * (x <= 0)\n    \n    def backward(self, x):\n        return 1. * (x > 0) + self.alpha * (np.exp(x)-1) * (x <= 0)\n\n\nclass Sigmoid(Activation):\n    \n    def __init__(self, stable=True):\n        self.stable = stable\n    \n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return 1 / (1 + np.exp(-x))\n    \n    def backward(self, x):\n        return np.exp(-x) / np.square(1+np.exp(-x))\n\n\nclass Tanh(Activation):\n    \n    def __init__(self, stable=True):\n        self.stable = stable\n    \n    def forward(self, x):\n        if self.stable:\n            x = np.clip(x, -700, 700)\n        return np.tanh(x)\n    \n    def backward(self, x):\n        return 1.0 - np.square(np.tanh(x))\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ca506ad756a170fcadef554a774427dc4d1c8baa"},"cell_type":"code","source":"# layer functions\n#------------------------------------------------------------------------------\n'''\nhelper functions to create layer definitions\n\na layer definition is just a tuple that contains:\n  * input size (number of neurons of previous layer)\n  * output size (number of neurons)\n  * activation function (just pass in the function, eg. relu not relu() )\n'''\n\n\ndef create_input(num_features):\n    '''\n    creates an input layer\n\n    this function will return a tuple of (None, num_features, None). This is \n    interpreted in aspecial way by the NeuralNetwork class as long as it is \n    the first layer in the layer_defs argument. \n\n    Basically, it implies how many features the input data will have, and \n    creates a layer definition that can be consumed by the \n    `create_hidden_layer` and `create_output_layer` functions!\n\n    Parameters\n    ----------\n    num_features : int\n        the number of features in the dataset\n\n    Returns\n    -------\n    tuple\n        a tuple consisting of (None, num_features, None)\n    '''\n    return {'in': None, 'out': num_features, 'act': None}\n\n\ndef create_hidden(previous_layer, neurons, activation):\n    '''creates a hidden layer\n\n    this function will return a tuple of (prev_neurons, neurons, activation)\n    where `prev_neurons` is the number of neurons in the previous layer, neurons \n    is the number of neurons in this layer.\n\n    Parameters\n    ----------\n    previous_layer : tuple\n        the layer definition of the previous layer\n    neurons : int\n        the number of neurons in this layer\n    activation : function\n        the activation function to be used in this layer\n\n    Returns\n    -------\n    tuple\n        a tuple consisting of (prev_neurons, neurons, activation)\n    '''\n    return {'in': previous_layer['out'], 'out': neurons, 'act': activation}\n\ndef create_output(previous_layer, classes, activation):\n    '''creates an out layer\n\n    this function will return a tuple of (prev_neurons, classes, activation)\n    where `prev_neurons` is the number of neurons in the previous layer, \n    classes is the number of output classes/neurons.\n\n    this function is essentially just the `create_hidden_layer` function with \n    the neurons parameter changed to classes.\n\n    Parameters\n    ----------\n    previous_layer : tuple\n        the layer definition of the previous layer\n    classes : int\n        the number of neurons in this layer (output classes)\n    activation : function\n        the activation function to be used in this layer\n\n    Returns\n    -------\n    tuple\n        a tuple consisting of (prev_neurons, classes, activation)\n    '''    \n    return create_hidden(previous_layer, classes, activation)\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2206ee8186e8a9c95a319114cd32b7768e62b2e0"},"cell_type":"code","source":"# optimizers\n#------------------------------------------------------------------------------\ndef sgd(w, z, act, prev_delta, bias, alpha):\n    '''applies SGD/BGD and returns the modified values'''\n    cur_delta = backward_layer(w, z, act, prev_delta)\n    w += alpha * z.T.dot(prev_delta)\n    bias += -alpha * np.sum(prev_delta, axis=0, keepdims=True)\n    return cur_delta, w, bias\n\ndef momentum(w, z, act, prev_delta, bias, v, alpha, mu):\n    '''applies SGD/BGD with momentum and returns the modified values'''\n    cur_delta = backward_layer(w, z, act, prev_delta)\n    v = mu * v + z.T.dot(prev_delta)\n    w += alpha * v\n    bias += -alpha * np.sum(prev_delta, axis=0, keepdims=True)\n    return cur_delta, w, bias, v\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c2c5264afa44e9739c030b7f3404278d5ef7e7b"},"cell_type":"code","source":"# neural network definition\n#------------------------------------------------------------------------------\n\ndef forward_layer(z, w, b, act):\n    '''Does the forward pass between layers'''\n    return act(z.dot(w)+b)\n\n\ndef backward_layer(w, z, act, prev_delta):\n    '''does the backwards pass between layers'''\n    w_error = prev_delta.dot(w.T)\n    return w_error * act(z, deriv=True)\n\n\nclass NeuralNetwork(object):\n    def __init__(self, layer_defs):\n        self.inputs = layer_defs[0]['out'] # get number of inputs\n        self.outputs = layer_defs[-1]['out']\n        self.layer_count = len(layer_defs) - 1\n        self.momentum_enable = False\n        self.layer_defs = layer_defs\n\n    def add_momentum(self, mu=0.9):\n        '''add momentum to network (needs to be called before initialise)'''\n        self.momentum_mu = mu\n        self.momentum_enable = True\n        self.velocities = {}\n    \n    def initialise(self, weight_init=he_uniform, bias_init=init_zeros):\n        '''initlise all the variables that will be used in the network'''\n        self.weights = {}\n        self.biass = {}\n        self.activations = {}\n        self.layers = {}\n        for i in range(self.layer_count):\n            _in = self.layer_defs[i+1]['in']\n            _out = self.layer_defs[i+1]['out']\n            self.weights[i] = weight_init((_in, _out))\n            self.biass[i] = bias_init((1, _out))\n            self.activations[i] = self.layer_defs[i+1]['act']\n            if self.momentum_enable:\n                self.velocities[i] = np.zeros(_out)\n    \n    def fit(self, X, Y, batch_size=32, alpha=0.001, epochs=100, info_freq=10):\n        loss_array = []\n        best_loss = np.inf\n        for j in range(epochs):\n            X, Y = shuffle(X, Y)\n            x_batches = split_to_minibatch(X, batch_size)\n            y_batches = split_to_minibatch(Y, batch_size)\n            epoch_loss = 0\n            for x, y in zip(x_batches, y_batches):\n                # set up layers\n                prev_layer = self.layers[0] = x\n                for i in range(self.layer_count):\n                    current_layer = forward_layer(prev_layer, \n                                                  self.weights[i],\n                                                  self.biass[i], \n                                                  self.activations[i])\n                    self.layers[i+1] = current_layer\n                    prev_layer = current_layer\n                last_layer = current_layer\n    \n                # calculate errors\n                error = y - last_layer\n                nonlin = self.activations[self.layer_count - 1]\n                delta = error * nonlin(last_layer, deriv=True)\n    \n                epoch_loss += np.square(error).mean()\n    \n                prev_delta = delta\n                for i in reversed(range(self.layer_count)):\n                    if self.momentum_enable:\n                        (prev_delta,\n                        self.weights[i],\n                        self.biass[i],\n                        self.velocities[i]) = momentum(self.weights[i],\n                                                       self.layers[i],\n                                                       self.activations[i], \n                                                       prev_delta, \n                                                       self.biass[i],\n                                                       self.velocities[i],\n                                                       alpha, \n                                                       self.momentum_mu)\n                    else:\n                        (prev_delta,\n                        self.weights[i],\n                        self.biass[i]) = sgd(self.weights[i],\n                                             self.layers[i], \n                                             self.activations[i], \n                                             prev_delta, \n                                             self.biass[i],\n                                             alpha)\n            loss_array.append(epoch_loss)\n            \n            if epoch_loss < best_loss:\n                self.best_weights = copy.deepcopy(self.weights)\n                self.best_biass = copy.deepcopy(self.biass)\n            \n            if (j % (epochs/info_freq)) == 0:             \n                print(\"loop: {}\".format(j))\n                print(\"Guess (rounded): \")\n                print(np.round(last_layer[0], 3))\n                print(\"Actual: \")\n                print(np.round(y[0], 3))\n        \n        return loss_array    \n\n    def evaluate(self, x, y):\n        # set up layers\n        prev_layer = self.layers[0] = x\n        for i in range(self.layer_count):\n            current_layer = forward_layer(prev_layer, \n                                          self.best_weights[i],\n                                          self.best_biass[i], \n                                          self.activations[i])\n            self.layers[i+1] = current_layer\n            prev_layer = current_layer\n        last_layer = current_layer\n        error = np.square(y) - np.square(last_layer)\n        total_error = error.mean()\n        return total_error\n    \n    def predict(self, X, batch_size=64):\n        preds = []\n        x_batches = split_to_minibatch(X, batch_size)\n        for x in x_batches:\n            # set up layers\n            prev_layer = self.layers[0] = x\n            for i in range(self.layer_count):\n                current_layer = forward_layer(prev_layer, \n                                              self.best_weights[i],\n                                              self.best_biass[i], \n                                              self.activations[i])\n                self.layers[i+1] = current_layer\n                prev_layer = current_layer\n            last_layer = current_layer\n            for pred in last_layer:\n                preds.append(pred)\n        return preds\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4eca7d5bca742f612132bd99ffcbec5fdd884d8"},"cell_type":"code","source":"# plotting functions\n#------------------------------------------------------------------------------\n\ndef graph_loss(loss):\n    y = loss\n    x = [x for x in range(len(loss))]\n    min_epoch, min_loss = min(enumerate(loss), key=lambda x: x[1])\n    plt.xlabel = 'Epochs'\n    plt.ylabel = 'error'\n    plt.plot(x, y, 'b-', label='Training loss')\n    plt.plot(min_epoch, min_loss, 'rx', mew=2, ms=20, label='minimum loss')\n    plt.legend()\n    plt.show()\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60f86f223237cc6b594d5ef0c5c322594e7f7dfc"},"cell_type":"code","source":"# data functions\n#------------------------------------------------------------------------------\n\ndef one_hot(x, classes, zero_based=True):\n    '''returns onehot encoded vector for each item in x'''\n    ret = []\n    for value in x:\n        temp = [0. for _ in range(classes)]\n        if zero_based:\n            temp[int(value)] = 1.\n        else:\n            temp[int(value)-1] = 1.\n        ret.append(temp)\n    return np.array(ret)\n\n\ndef split_to_minibatch(z, batch_size=32):\n    z_length = len(z)\n    current_batch_size = batch_size\n    batch_split_points = []\n    while current_batch_size < z_length:\n        batch_split_points.append(current_batch_size)\n        current_batch_size += batch_size\n    return np.split(z, batch_split_points)\n\n#------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d1b7298996b5703b52c52fab28f739c6b38d697","collapsed":true},"cell_type":"code","source":"# set up dataset\nnumber_classes = 7\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n# lets take a look...\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6227f0edeab9f18e4bbcaf10812972ee607b3a08"},"cell_type":"code","source":"# create train datasets\nX_train = train_df.drop(['Id', 'Cover_Type'], axis=1)\nY_train = train_df[['Cover_Type']].values\nY_train = Y_train.reshape(len(Y_train))\n\n# create test dataset and ID's\nX_test = test_df.drop(['Id'], axis=1)\nID_test = test_df['Id'].values\nID_test = ID_test.reshape(len(ID_test))\n\n# concatenate both together for feature engineering and normalisation\nX_all = pd.concat([X_train, X_test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"15a789e0b155874d96e1bb0f855be8b6e5f56149"},"cell_type":"code","source":"# mean hillshade\ndef mean_hillshade(df):\n    df['mean_hillshade'] = (df['Hillshade_9am'] + df['Hillshade_Noon'] + df['Hillshade_3pm']) / 3\n    return df\n\n# calculate the distance to hydrology using pythagoras theorem\ndef distance_to_hydrology(df):\n    df['distance_to_hydrology'] = np.sqrt(np.power(df['Horizontal_Distance_To_Hydrology'], 2) + \\\n                                          np.power(df['Vertical_Distance_To_Hydrology'], 2))\n    return df\n\n# calculate diagnial distance down to sea level?\ndef diag_to_sealevl(df):\n    df['diag_to_sealevel'] = np.divide(df['Elevation'], np.cos(180-df['Slope']))\n    return df\n\n# calculate mean distance to features\ndef mean_dist_to_feature(df):\n    df['mean_dist_to_feature'] = (df['Horizontal_Distance_To_Hydrology'] + \\\n                                  df['Horizontal_Distance_To_Roadways'] + \\\n                                  df['Horizontal_Distance_To_Fire_Points']) / 3\n    return df\n\nX_all = mean_hillshade(X_all)\nX_all = distance_to_hydrology(X_all)\nX_all = diag_to_sealevl(X_all)\nX_all = mean_dist_to_feature(X_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc5c54c7a0cbc75e5c7212c58a32f033a390b6ad"},"cell_type":"code","source":"# normalise dataset\ndef normalise_df(df):\n    df_mean = df.mean()\n    df_std = df.std()    \n    df_norm = (df - df_mean) / (df_std)\n    return df_norm, df_mean, df_std\n\n# define columsn to normalise\ncols_non_onehot = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                'Horizontal_Distance_To_Fire_Points', 'mean_hillshade',\n                'distance_to_hydrology', 'diag_to_sealevel', 'mean_dist_to_feature']\n\nX_all_norm, df_mean, df_std = normalise_df(X_all[cols_non_onehot])\n\n# replace columns with normalised versions\nX_all = X_all.drop(cols_non_onehot, axis=1)\nX_all = pd.concat([X_all_norm, X_all], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1384d8daefc3c806205e0c8f02bfe848b9a058ee"},"cell_type":"code","source":"# split back into test and train sets\nX_train = np.array(X_all[:len(X_train)])\nX_test = np.array(X_all[len(X_train):])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6140e64bed515a006239d20305ffaf27bf9048fa","collapsed":true},"cell_type":"code","source":"Y_train = one_hot(list(Y_train), number_classes, zero_based=False)\nXt, Xv, Yt, Yv = train_test_split(X_train, Y_train, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a003f330c4272c64a7cbaa45161d38123787f6e5"},"cell_type":"code","source":"layer_defs = []\n\n# make the input layer\ninput_layer = create_input(len(Xt[0]))\n\nlayer_defs.append(input_layer)\n\nprev_layer = input_layer\n\n# make the hidden layers\nnum_hidden = 2\nnum_neurons = 128\nfor i in range(num_hidden):\n    cur_layer = create_hidden(prev_layer, num_neurons, SchlerpTanh())\n    layer_defs.append(cur_layer)\n    prev_layer = cur_layer\n\nnum_hidden = 2\nnum_neurons = 64\nfor i in range(num_hidden):\n    cur_layer = create_hidden(prev_layer, num_neurons, SchlerpTanh())\n    layer_defs.append(cur_layer)\n    prev_layer = cur_layer\n\nnum_hidden = 2\nnum_neurons = 32\nfor i in range(num_hidden):\n    cur_layer = create_hidden(prev_layer, num_neurons, SchlerpTanh())\n    layer_defs.append(cur_layer)\n    prev_layer = cur_layer\n    \n# add output layer\noutput_layer = create_output(prev_layer, len(Yt[0]), Sigmoid())\nlayer_defs.append(output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7e5469ee0e673684340c629af280c8bafceda433"},"cell_type":"code","source":"# create the neural network\nnn = NeuralNetwork(layer_defs)\n\n# add momentum\nnn.add_momentum()\n\n# initialise the variables\nnn.initialise(weight_init=he_normal, bias_init=init_zeros)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8797bc9dee794b4e689b6bdfb33468caa66908ae","scrolled":false,"collapsed":true},"cell_type":"code","source":"train_error = nn.fit(Xt, Yt, batch_size=64, epochs=500, info_freq=10, alpha=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9feb448fde3f8fd1f55fc0c6f11f29b7c8d34863","scrolled":false,"collapsed":true},"cell_type":"code","source":"graph_loss(train_error)\nmin_epoch, min_loss = min(enumerate(train_error), key=lambda loss: loss[1])\nprint('min loss: {}, was acheived at {} epochs'.format(min_loss, min_epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5554e0f5289ed0a7db0034d21c82e34a26234769","collapsed":true},"cell_type":"code","source":"test_error = nn.evaluate(Xv, Yv)\nprint(test_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c84762b23ff76674fdfda2da53e168063a459f","collapsed":true},"cell_type":"code","source":"y_pred = nn.predict(X_test, batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08c570cf4fdcb04ecd1fa661bd6ecc38e3964d5b","collapsed":true},"cell_type":"code","source":"y_pred = np.argmax(y_pred, axis=1) + 1\ny_pred = y_pred.astype(int)\n\nprint('max prediction class: {}'.format(np.max(y_pred)))\nprint('min prediction class: {}'.format(np.min(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf162a3a5ce7b12ce616e595dee22291ad53a0e","collapsed":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = ID_test\nsub['Cover_Type'] = y_pred\nsub.to_csv('my_submission.csv', index=False)\nprint('good luck!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"17b890e541ed525cbdf89a26c466a9f403acc765"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}