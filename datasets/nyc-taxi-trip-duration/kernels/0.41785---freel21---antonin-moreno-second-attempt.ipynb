{"cells":[{"metadata":{"_uuid":"18f49596c38d08af5affc69f31336cfd530e30ce"},"cell_type":"markdown","source":"# This kernel is the result of a second chance at beating this competition.\n# My previous score was at 0.73031, which is not catastrophic but not very good either.\n# My goal is now to pass at least under 0.5\n# Please, machine learning god, make it happen \\o/"},{"metadata":{"_uuid":"dbc1cc50bb9a72230e3b168ac73a0876bc74cf78"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"eb215bbb77762d7d2d8e9073fae3506d4350eb38"},"cell_type":"markdown","source":"# 1 - Importing the magic stuff"},{"metadata":{"trusted":true,"_uuid":"b810fedd153f8d18b10ba514e3e0099b6e31377f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#from sklearn.ensemble import RandomForestRegressor\n#from sklearn.model_selection import cross_val_score, train_test_split\n#from sklearn.metrics import r2_score, mean_squared_error as MSE\n#from sklearn.linear_model import SGDRegressor, LinearRegression\n\nimport lightgbm as lgb\n\n#all the previous algorithms from sklearn did a terrible job at predicting the trip duration on this dataset.\n#I tried something new first with gbm, next with lightgbm which seemed to be faster and even more accurate\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5b0c824cf9cdc288a217449dab44c08f78bd72e"},"cell_type":"markdown","source":"# 2 - Data exploration"},{"metadata":{"trusted":true,"_uuid":"ba35d683c5f57fe48b59a8dec72408dcf6e9d059"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaa364bd567cc0ad93013f1c9cccab90f6148b15"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d7dc705f5135da2d192c1eb3ea7133cabb6208"},"cell_type":"code","source":"df_train.size\n\n# Holy Pepperoni, that's big","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb34c652c5015f6a7e7d2be62d0d8df5f0614a54"},"cell_type":"code","source":"print(df_train.columns)\n\n# in order to get an idea of the futur features to take in count for the prediction\n# ... and because I love to print columns anyway, hope you don't mind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"851a4c59a310fb840411c380f60547294d5df6fb"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f08a68b0af12d03abfbb021ac081e51147c9a0c6"},"cell_type":"markdown","source":"# 3 - Further exploration"},{"metadata":{"trusted":true,"_uuid":"1fbdbb4141f420cb768709d8df3328a6b51dc46a"},"cell_type":"code","source":"df_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2e2ad4bc4a88ebf7de74da1158195807c2e476"},"cell_type":"code","source":"df_train.isnull().sum()\n\n# a data with no NaN value, what is this sorcery","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7abd13acc985324495233ba9ce4bfa905705214"},"cell_type":"code","source":"# just to be sure\n\ndf_train.duplicated().sum()\n\n# sweet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5315ab0ec738786e9d0cf9f462c0a53d19b0be7f"},"cell_type":"markdown","source":"# 4 - Data cleaning"},{"metadata":{"trusted":true,"_uuid":"9c23c3ffcaee8c39a575d71d2686116fb5e0e329"},"cell_type":"code","source":"# First things first, we need to get rid of outliers in the trip duration feature\n\nplt.subplots(figsize=(20,10))\nplt.title(\"Top Outliers repartition in the trip duration feature\")\ndf_train.boxplot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9845616c3ec656238165dc7216642039a517650"},"cell_type":"code","source":"plt.subplots(figsize=(20,10))\nplt.title(\"Top Outliers repartition in the trip duration feature - zoomed\")\ndf_train.loc[df_train.trip_duration<5000,\"trip_duration\"].hist(bins=120);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"727ec36c8c872d5b99d900c617074ae1b71f17ad"},"cell_type":"code","source":"# 1 minute of silence for the people who forgot to turn off the taxi counter\n# We have to get rid of these values in order to make correct predictions\n\ndf_train = df_train[(df_train['trip_duration'] > 60) & (df_train['trip_duration'] < 4600)]\ndf_train['trip_duration'] = np.log(df_train['trip_duration'].values)\n\n# The \"top\" outliers are the easiest to deal with. It gets more complicated with the \"bottom\" outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"472f4d01878d26d2b091380fd7da194c8853fe52"},"cell_type":"code","source":"# Beware mortal, from this point of the kernel, you will find few lines of code put under commentary \n# because I made some wild test before getting a satisfactory result on the predictions\n\n# I did not erase the code not giving useless information, not working correctly or even at all \n# so you can see by yourself what went wrong in the first steps. Sorry for the mess !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d53a5cd882936df35b7891c8fc24e558927e245d"},"cell_type":"code","source":"# we will consider any trip duration below 5 minutes as outliers\n# haters gonna hate, I know, who would take the taxi for a trip duration less than 5 minutes anyway\n\n#df_train = df_train.loc[(df_train['trip_duration'] > 300) & (df_train['trip_duration'] < 300000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c405a9bd8478603e421d639b4d274d218004ac54"},"cell_type":"code","source":"# Now we also need to get rid of outliers in the geographical place data section where people are picked up\n\n#df_train.plot.scatter(x='pickup_longitude',y='pickup_latitude');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fdff4e10073601d40a77793703393d395f6eb3"},"cell_type":"code","source":"#df_train = df_train.loc[(df_train['pickup_longitude']> -90) & (df_train['pickup_latitude']< 46)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9892bad7f6eff5aa42fb6638603d3e328030f7f"},"cell_type":"code","source":"# Same goes with the place people are dropped off\n\n#df_train.plot.scatter(x='dropoff_longitude',y='dropoff_latitude');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfc6abac8e7b0a0930ca207c6928a5a3d60188b"},"cell_type":"code","source":"#df_train = df_train.loc[(df_train['dropoff_longitude']< -70) & (df_train['dropoff_latitude']> 35)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7b3c216d6c0996103284d7b4fa3cf3f5d90671"},"cell_type":"markdown","source":"# 5 - Features creation, selection and extraction"},{"metadata":{"trusted":true,"_uuid":"06603d58ff436a75c01b6dc288eb1885a106878b"},"cell_type":"code","source":"# I tried to create those two features but it turned out to be useless in the end, mostly because the output information was not precise enough\n\n#df_train['Time'] = df_train['pickup_datetime'].apply(lambda x: int(x.split()[1][0:2]))\n\n#df_train['Distance'] = np.sqrt((df_train['pickup_latitude']-df_train['dropoff_latitude'])**2 + (df_train['pickup_longitude']-df_train['dropoff_longitude'])**2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9424d82adff2dab08979630193d0a3b37f8bbb65"},"cell_type":"code","source":"# In prevision of the prediction model, we are going to create a new feature : DateTime\n# The DateTime module gives classes to manipulate times and dates.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438924483034873539c7be80c7e459667d29aea0"},"cell_type":"code","source":"from datetime import datetime\n\ndf_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'])\ndf_train['dropoff_datetime'] = pd.to_datetime(df_train['dropoff_datetime'])\ndf_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59ac0b2b036b8b2ea46cef79451a7b63de08eef1"},"cell_type":"code","source":"# In order to use at best this feature, we will add 4 mores columns in the dataset, both Train and Test :\n# Month, Day, Hour, Minute","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33ab38dd287194c0556335ecfad345ed67b82f66"},"cell_type":"code","source":"df_train['month'] = df_train.pickup_datetime.dt.month\ndf_train['day'] = df_train.pickup_datetime.dt.dayofweek\ndf_train['hour'] = df_train.pickup_datetime.dt.hour\ndf_train['minute'] = df_train.pickup_datetime.dt.minute\n\ndf_test['month'] = df_test.pickup_datetime.dt.month\ndf_test['day'] = df_test.pickup_datetime.dt.dayofweek\ndf_test['hour'] = df_test.pickup_datetime.dt.hour\ndf_test['minute'] = df_test.pickup_datetime.dt.minute","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91c296c94cde1093613a21f630cd641ae0fa4407"},"cell_type":"code","source":"# Now we will add some mathematics in the cauldron to get the distance of any trip by using the pickup & dropoff lattitude & longitude data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e54b86c29033beea9007d5f01f3b4795cd60a61"},"cell_type":"code","source":"df_train['d_latitude'] = df_train['pickup_latitude'] - df_train['dropoff_latitude']\ndf_train['d_longitude'] = df_train['pickup_longitude'] - df_train['dropoff_longitude']\n\ndf_test['d_latitude'] = df_test['pickup_latitude'] - df_test['dropoff_latitude']\ndf_test['d_longitude'] = df_test['pickup_longitude'] - df_test['dropoff_longitude']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df58b69607fff3fe16b7e2d7c61c7951c49264f"},"cell_type":"code","source":"# And now, for the final result :","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc65e5a925aba21132b81fdd625fe325cd3ce113"},"cell_type":"code","source":"df_train['distance'] = np.sqrt(np.square(df_train['d_longitude']) + np.square(df_train['d_latitude']))\ndf_test['distance'] = np.sqrt(np.square(df_test['d_longitude']) + np.square(df_test['d_latitude']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4030e7d435bda78e9aa93e03565f64115ffac57c"},"cell_type":"code","source":"df_train.shape, df_test.shape\n\n# Good boy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcbdf60502221b956733a77148a8178715b65233"},"cell_type":"code","source":"# Next, we are going to use every feature and column we added in the dataset to prepare the training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"114c0256ffadc74b638937bc0a80d01965ae9278"},"cell_type":"code","source":"#y = df_train[\"trip_duration\"] # <-- target\n#X = df_train[[\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"Time\",\"Distance\",\"minute\",\"hour\",\"day\",\"month\"]] # <-- features\n\nFeatures = [\"vendor_id\",\"passenger_count\",\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"distance\",\"month\",\"hour\",\"day\"]\nTarget = \"trip_duration\"\n\n#X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\n#X_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n\nX_train = df_train[Features]\ny_train = df_train[Target]\n\nlgb_train = lgb.Dataset(X_train,y_train)\n\nlgb_params = {'learning_rate': 0.1,\n                'max_depth': 25,\n                'num_leaves': 1000, \n                'objective': 'regression',\n                'feature_fraction': 0.9,\n                'bagging_fraction': 0.5,\n                'max_bin': 1000\n             }   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec9671436eeaf24521628a0643f2eafd3dea7cd"},"cell_type":"code","source":"#m1 = RandomForestRegressor()\n#m1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe9cf6bf5ecae358d5f9734160c2acfb52a3c4bd"},"cell_type":"markdown","source":"# 6 - Training"},{"metadata":{"trusted":true,"_uuid":"60ff2d738c259d4b9d7c2f78123391c3090391db"},"cell_type":"code","source":"#model_lgb = lgb.train(lgb_params,lgb_train)\n#model_lgb = lgb.train(lgb_params,lgb_train,num_boost_round=1000)\n\nmodel_lgb = lgb.train(lgb_params,lgb_train,num_boost_round=500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1c509dd8f1ba1672f891a337d2b69a5e4f38a7c"},"cell_type":"markdown","source":"# 7 - Validation"},{"metadata":{"trusted":true,"_uuid":"cf435d942a9011af2794dd586b4af84cd6bf4045"},"cell_type":"code","source":"#m1_scores = cross_val_score(m1, X, y, cv=5, scoring =\"neg_mean_squared_log_error\")\n#m1_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3126b90095405d6d1e986ae96991ea06519ecbce"},"cell_type":"code","source":"#for i in range(len(m1_scores)):\n#    m1_scores[i] = np.sqrt(abs(m1_scores[i]))\n#print(m1_scores)\n#print(np.mean(m1_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee9bbb0a3a8cea047ef7b58d3a0510c2582fa642"},"cell_type":"code","source":"#df_test['Time'] = df_test['pickup_datetime'].apply(lambda x: int(x.split()[1][0:2]))\n\n#df_test['Distance'] = np.sqrt((df_test['pickup_latitude']-df_test['dropoff_latitude'])**2 + (df_test['pickup_longitude']-df_test['dropoff_longitude'])**2) \n#df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc1db391567adc59e68c3061591d43a3286afdae"},"cell_type":"code","source":"#X_test = df_test[[\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"Time\",\"Distance\"]]\n#predicted_duration = m1.predict(X_test)\n#print(predicted_duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a31bb854e24cc90c21e36ea42ab9f344b03a23d"},"cell_type":"code","source":"X_prediction = df_test[Features]\nprediction = np.exp(model_lgb.predict(X_prediction))\nprint(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa99b32dc0cf134a12bed8ec66653688643fd73f"},"cell_type":"code","source":"My_Submission = pd.DataFrame({'id': df_test.id, 'trip_duration': prediction})\nprint(My_Submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b887b8a691579276a064f72f6bf8318f975128b6"},"cell_type":"code","source":"My_Submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"817236d71f7f29ac80352cb9210894952fe5569e"},"cell_type":"markdown","source":"### Thanks for the time you spent reading this Kernel. I'm completely new to machine learning and this is my first attempt (revisited to get a better score) on a Kaggle dataset. I found out that my previous prediction was bad mostly because I was too harsh with the outliers management, removing too many values from the dataset. By being more flexible and trying out new algorithms, I managed to go from 0.73031 to 0.41785. \n### If you have any suggestion on how I can improve myself, please feel free to share it ! "},{"metadata":{"trusted":true,"_uuid":"130617d2272574fd1859296474ec05ea816637b9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}