{"cells":[{"metadata":{"_uuid":"df15f2710b802375860de230a96c855b760b0f02"},"cell_type":"markdown","source":"# Imports \nI'm going to use typical data science stack: numpy, pandas, sklearn, matplotlib."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad8a5e44a1d616bb430e5d395be6477b2fda2ee2"},"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdf64bf53e64457b6b0acaa03a41a4b61e23d104"},"cell_type":"markdown","source":"\n# Read in Data\n\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01e6c1962c529bbd72e90177c9a7cc275b71fde"},"cell_type":"markdown","source":"As we can see our training dataset have total 122 features combinations of Ids, categorical variable and measures. It also includes TARGET variable which we want to predict."},{"metadata":{"trusted":true,"_uuid":"e00e7b95372b21d441c945a522ffeea1ce30b6fa"},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"270e4c2fb20bce07bb43ecf770f4fe9ce665e691"},"cell_type":"markdown","source":"The test set is considerably smaller and lacks a TARGET column."},{"metadata":{"_uuid":"ad291c56320926118ff0d4684155b01fd9d17679"},"cell_type":"markdown","source":"\n# Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.\n"},{"metadata":{"_uuid":"369752dd7306a6043d431353296e35cfb48220fd"},"cell_type":"markdown","source":"\n# Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.\n"},{"metadata":{"trusted":true,"_uuid":"c1bdce83d6f7cabb548f9a489510e7ce52e4aaa5"},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b96803c8fd455f2e44de551e1b730ce3dca51da9"},"cell_type":"code","source":"app_train['TARGET'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06aa8b880d2c8753b8d93eb70844e82d561742a7"},"cell_type":"markdown","source":"From above plot, we see this is an [imbalanced class problem](http://chioka.in/class-imbalance-problem/). There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance."},{"metadata":{"_uuid":"eeeeeea47af6fbb373b7dcded2d7c642e8f01d33"},"cell_type":"markdown","source":"\n# Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column.\n"},{"metadata":{"trusted":true,"_uuid":"ea9b5f6ebf8eb1d769402e274768b80a87a84356"},"cell_type":"code","source":"app_train.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48e7716c43bf76a0d3509df2f987f30a7b057c70"},"cell_type":"markdown","source":"As we can see this gives us no clear picture about missing values that which colomns have how many missing  and what percentage of tatal values in that columns. Let's write a customised missing value table function"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38c59241795710b0a168752df2befc74ce5b5f8f"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" rows.\\n\"\n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fd4ac648432e34bb388de917ec432ff26b1a605"},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(41)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"891f0ec22df3cad9ce426b3f00c83f239ecb9ac1"},"cell_type":"markdown","source":"There is a general approcah where we drop coloumns which have more than 50% missing values. So above table you know which columns you can drop if you decided to drop it.\nWhen it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase). Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now."},{"metadata":{"trusted":true,"_uuid":"117e461350b0b688b568e47fe70d528bdc165a39"},"cell_type":"code","source":"sns.heatmap(app_train.isnull(), cbar=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6cf3d52912ee467b8e10e7cd9bc0769fc45c08c"},"cell_type":"markdown","source":"\n# Column Types\n\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features. .\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d9b80eaffbf9e0887dfff15ad1e8ef91ee45265d"},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11c7be1a1c9b1f30198834f43aac564799f89fd5"},"cell_type":"markdown","source":"Let's now look at the number of unique entries in each of the object (categorical) columns."},{"metadata":{"trusted":true,"_uuid":"64cd6361ddf5ba00e9930fb682c15ba7e0c3e77b"},"cell_type":"code","source":"# Number of unique classes that is level of categorical variable in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a42f7ca90db31f37b36c5923ed40345b840f4a5f"},"cell_type":"markdown","source":"Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables because most machine learning models deals with only numerical input."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}