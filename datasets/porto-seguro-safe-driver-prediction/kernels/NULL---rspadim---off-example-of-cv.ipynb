{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.1"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "6f5dcc7969de4094da628be88b7f6f96ca140ab2", "_cell_guid": "a8d2dbe0-b71b-476b-9924-70f4ce7da59c"}, "source": [" example to Daniel Moller - https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40881\n", " ideas and comments are wellcode"], "cell_type": "markdown"}, {"metadata": {"_uuid": "0fa6422e7dcc650e00503e8ed25d10e7769cff5b", "_cell_guid": "589124d3-0d37-43c4-936c-cdb084f832db"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["import numpy as np  # numeric library\n", "import pandas as pd # read file, and use dataframes (like database tables, but in python)\n", "# read train file (train have features (X vectors) and target (Y vectors)), you use it to fit models\n", "train  =pd.read_csv(\"../input/train.csv\")\n", "print(train.columns)   # show columns at train pandas.DataFrame\n", "print(np.shape(train)) # show DataFrame shape (~= rows, features)\n", "\n", "#this variable will contain an 'array' of all feature columns, i will use it to train model\n", "col = train.columns.drop(['id','target'])\n"]}, {"metadata": {"collapsed": true, "_uuid": "792d572b4a6bb9728ff604ed056382ff27f17197", "_cell_guid": "77c72854-8e89-439d-90c0-cfe73ac594d6"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["#in this competition we use GINI metric as 'evaluation function'\n", "# there're 2 metrics, error functions - used to fit models (normally they are differentiable)\n", "#                and evaluation functions - used to select / check what's the useful metric information\n", "\n", "# i will show an XGBoost model classifier, it's an gradient boosted , it will use logloss to train, in this case\n", "# it will use logloss as error function, and evaluation function is just to check if model is doing a good work\n", "# XGBoost have others methods to crossvalidate it, \n", "# but i will show an 'generic' estimator CV scoring idea using scikit learn lib\n", "\n", "\n", "\n", "\n", "\n", "\n", "#let's implement the evaluation function:\n", "from sklearn.metrics import make_scorer #function to create a scorer from an metric function\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return 'gini', gini_score\n", "# i will create an SCORER to use with SCIKIT LEARN library\n", "gini_scorer = make_scorer(gini_normalized, greater_is_better = True)"]}, {"metadata": {"_uuid": "00180ef769ce7a9c7a43f05cd71ae934cad6b643", "_cell_guid": "7d91134f-e5d0-4939-9a2f-a8082e08167d"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["from xgboost import XGBClassifier\n", "from sklearn.model_selection import validation_curve,StratifiedShuffleSplit\n", "#let's create a model (xgb_model variable) using some 'default' parameters\n", "xgb_model = XGBClassifier(\n", "    n_jobs=-1,\n", "    objective='binary:logistic',\n", "    learning_rate=1,\n", "    max_depth=2,\n", "    silent=False,\n", "    subsample=1,\n", "    colsample_bytree=1,\n", "    n_estimators=100,\n", "    random_state=1\n", ")\n", "\n", "#i will naive reduce dataset size cause validation_curve is a bit cpu intensive (that's not ok, just an example)\n", "# you should check if you can do it, and how, with your model+dataset or not\n", "some_sample_data_to_test=train.sample(n=10000)\n", "\n", "# let's do some fitting, using different parameters and plot what's the crossvalidation score:\n", "param_range=[2,4,6]\n", "train_scores, test_scores = validation_curve( #http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html\n", "    xgb_model,       #model that we will fit many many times (crossvalidate) with different parameters\n", "    some_sample_data_to_test[col],      #features\n", "    some_sample_data_to_test['target'], #target variable \n", "    param_name=\"max_depth\",       #parameter that we will change\n", "    param_range=param_range,      #values that we will change\n", "    cv=StratifiedShuffleSplit(5,random_state=1,test_size=.1),  # CV SPLIT STRATEGY, here we select how to cut data and validation_curve function will execute crossvalidation scoring\n", "    scoring=gini_scorer,#\"neg_log_loss\",    #score function\n", "    n_jobs=-1)\n", "\n", "# get scorer values\n", "train_scores_mean = np.mean(train_scores, axis=1)\n", "train_scores_std  = np.std(train_scores, axis=1)\n", "test_scores_mean  = np.mean(test_scores, axis=1)\n", "test_scores_std   = np.std(test_scores, axis=1)\n", "\n", "#plot some chart\n", "import matplotlib.pyplot as plt\n", "plt.title(\"Validation Curve\")\n", "plt.xlabel(\"Param\")\n", "plt.ylabel(\"Score\")\n", "lw = 2\n", "plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw)\n", "plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2,color=\"darkorange\", lw=lw)\n", "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",color=\"navy\", lw=lw)\n", "plt.fill_between(param_range, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.2,color=\"navy\", lw=lw)\n", "plt.legend(loc=\"best\")\n", "plt.show()\n", "\n", "#more explanation about bias-variance: https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n", "#http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py\n", "#T. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning\u201d, Springer, 2009.\n", "\n", "#check that training score is high (in-sample data) - the model overfit in sample\n", "#check that test score is lower (out-of-sample data) - the model don't generalize to the out-of-sample \n", "#                                                      data (bad estimator ~ wrong hyperparameters or wrong model, or poor dataset ~ poor features or intratable problem)\n", "#\n", "#the crossvalidation idea is test model with diferent train-test data (folds / cuts), \n", "#and have a 'good' crossvalidation score (good bias/variance), good crossvalidation ~= good generalization ~= good models"]}, {"metadata": {"_uuid": "352cd4fbe85b598a0abfc4ddb571591463bf8409", "_cell_guid": "417fc011-c399-4be1-8989-41f039544d5c"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# you can use cross_val_score too, it just train-test and return scores, no validation curve\n", "\n", "from sklearn.model_selection import cross_val_score #http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score\n", "\n", "\n", "xgb_model = XGBClassifier(\n", "    n_jobs=-1,\n", "    objective='binary:logistic',\n", "    learning_rate=1,\n", "    max_depth=2,\n", "    silent=False,\n", "    subsample=1,\n", "    colsample_bytree=1,\n", "    n_estimators=100,\n", "    random_state=1\n", ")\n", "\n", "some_sample_data_to_test=train.sample(n=10000)\n", "\n", "print(\n", "    'score for each kfold, using cross_val_score function:',\n", "    cross_val_score(xgb_model, \n", "                          X=some_sample_data_to_test[col], \n", "                          y=some_sample_data_to_test['target'],\n", "                          scoring=gini_scorer,\n", "                          cv=StratifiedShuffleSplit(5,random_state=1,test_size=.1)\n", "     ) ) "]}, {"metadata": {"_uuid": "699410187f99dbd70ac265daae49a02d92ff0d8b", "_cell_guid": "0d057931-7dff-4e44-9362-dfdaee3d1ccb"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# what StratifiedShuffleSplit do?\n", "cv=StratifiedShuffleSplit(5,random_state=1,test_size=.1)\n", "\n", "#some_sample_data_to_test=train.sample(n=10000).copy()   #copy dataset \n", "#some_sample_data_to_test.reset_index(drop=True,inplace=True) #reset index\n", "\n", "X=some_sample_data_to_test[col]\n", "y=some_sample_data_to_test['target']\n", "print('splits using CV functions->',cv.get_n_splits(X,y))\n", "\n", "#it create an generator object that return (X,y) slices to train-test\n", "i=0\n", "for train_index, test_index in cv.split(X,y):\n", "    if(False): #change to true to check index selected at each fold\n", "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n", "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] #cut data - X\n", "    y_train, y_test = y.iloc[train_index], y.iloc[test_index] #cut data - Y\n", "    \n", "    #fit your model\n", "    xgb_model.fit(X_train,y_train)\n", "    #score it (that's what cross_val_score do)\n", "    print('cv split',i,' score=',gini_normalized(y_test,xgb_model.predict_proba(X_test)[:,1]))\n", "    i+=1\n", "    \n", "#if your model have a good bias/variance tradeoff in crossvalidation, you can fit you model \n", "# with all data:\n", "\n", "xgb_model.fit(some_sample_data_to_test[col],some_sample_data_to_test['target'])\n", "\n", "#and predict any other new data\n", "predict  =pd.read_csv(\"../input/test.csv\") #reading test data\n", "\n", "xgb_model.predict_proba(predict[col])[:,1]  #[:,1] select only positive (true) probabilities"]}, {"metadata": {"_uuid": "746591283b810f0725404fd0cdfca27561e99454", "_cell_guid": "9df21392-f3e7-481c-8f21-b79baef0529a"}, "source": ["good luck!"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "e0d852d72d44e0139a98470fe24949259979d5eb", "_cell_guid": "a9a1f5e0-b275-46f7-ab51-a2c6248853a1"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": []}]}