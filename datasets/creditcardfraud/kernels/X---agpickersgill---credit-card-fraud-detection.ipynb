{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "910b1179-99f9-4a48-a346-d390f0e55fd6", "_uuid": "b6b8d49203fec9b9d00b10687e591b33b692c417"}, "source": ["**Credit Card Fraud Detection**\n", "\n", "Outcome: Pending...\n", "\n", "Edit: (28th Sep 2017) - I have received feedback that I was only testing on a small subset of the overall data. I will be rewriting this notebook to take this into account. I expect my precision to drop dramatically, but my overall SVC score to trend up (which demonstrates why you should never trust this score in isolation).\n", "\n", "I have tangled with the Credit Card Fraud Detection dataset before with little success. In past attempts my accuracy was equivalent to a coin toss and my precision was a crapshoot. The challenges of the dataset are extensive - a vastly skewed dataset, inscrutible feature labels and haphazardly applied algorithms. \n", "\n", "If you have not seen this dataset before it is 280,000 records of credit card transactions. I suspect that the data has been heavily obsfuscated (for obvious reasons) before it was submitted to the site and so little intuitive knowledge can be gleaned from the features.  The feature labels are hidden, replaced with simple titles of V1 through to V28, with two named features (Amount and Time) and a Class label of 1 (meaning a fraudulent transaction) or 0 (meaning a legitimate transaction). The vast majority of the data is legitimate, with < 500 instances of fraud scattered amongst the numbers. \n", "\n", "In previous attempts I tried to grapple with the data by throwing algorithms at it -  a random forest here, a logistic regression there, with little thought given to the type of problem I was facing let alone the structure of the data presented.\n", "\n", "This time, however, I decided to approach this with a little more structure. I reviewed the data structure for simple correlations and plotted these patterns against the postive and negative examples to determine what patterns, if any, indicated fraudulent data."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b985faba-5c8f-4491-bdcd-f45301cb9d8b", "_uuid": "ea55a84f590055712b29250553bfecfe74127633"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "\n", "\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import pandas as pd \n", "import numpy as np \n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.utils import shuffle\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.svm import SVC\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "\n", "# Load the transactions from the credit card fraud file\n", "transactions = pd.read_csv('../input/creditcard.csv') \n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "246a62f2-6b6a-4618-b954-43b24ae0c14c", "_uuid": "dd64b7d4191ecfe9977103f877fa1e53060737c5"}, "source": ["I've loaded the dataset but I want to read it. Let's look at the actual data.\n", "\n", "The data represents a lot of credit card transactions. 280,000+. Nearly 500 of these are fraudulent. Yeah, less than 20% of 1% of the transactions are fraudulent.\n", "\n", "First, let's perform a correlation on the legitimate transactions to see if there are any commonalities to how this data appears which might not be represented in the illegitimate dataset."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "86a59084-bda2-4823-9698-78efd2da0696", "_uuid": "6135fada1806ab2d1c03cea3bbb26098af030f3d"}, "source": ["# Produce a correlation heat map of the negative class (Legitimate transactions)\n", "sample = transactions[transactions['Class']==0]\n", "normcorr=  sample.corr()\n", "sns.heatmap(normcorr, cbar = True,  square = True, annot=False, fmt= '.2f',annot_kws={'size': 15},\n", "           cmap= 'coolwarm')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "00d73d4f-8e7b-4c2a-b8ac-5fa4b0211a38", "_uuid": "e440c083261e1402cec184a38e8a0458f2057d7d"}, "source": ["In my last iteration of this notebook I made fun of the blandness of this heat map.  This is unfair. I said it had less taste than frozen tofu. Again, I am better than this. This is not the nearly blank-square of the perfectly distributed dataset - this is the blank face of the psycopath. This is the smooth-faced criminal who stares at you impassively just before he shivs you in the stomach. There are hints, that little chequerboard of negative correlation in the V1-V17 square for instance, which suggest something might lurk beneath, but its pure evil is swept aside by a smooth facade.\n", "\n", "Let's look at the positive dataset - the fraudulent transactions."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "e354aa0e-fe51-4116-8212-9723f1e1a715", "_uuid": "c83e806dca47b856534ecafe5fffc7d65794632f"}, "source": ["# Produce a correlation heat map of the fraudulent transactions.\n", "\n", "fraud = transactions[transactions['Class']==1]\n", "\n", "fraudcorr = fraud.corr()\n", "sns.heatmap(fraudcorr, cbar = True,  square = True, annot=False, fmt= '.2f',annot_kws={'size': 15},\n", "           cmap= 'coolwarm')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d3cc0bbd-795a-4779-afe8-4f684d0fcc55", "_uuid": "64f9d2ba7a8af7f201ed21991dd1b780dad68210"}, "source": ["This! This is the cocky face of the little bugger who swipes your credit card and spends it on soda and pixie-sticks. You can see in this heat-map the pock-marked visage between V1 and V18 that the previous graph only hinted at. It sneers at you. \n", "\n", "I'm feeling a sudden swirl of something resembling... hope.\n", "\n", "I can see a few areas of high heat or burning cold on this map. Of particular note is the V1 through V3 square, and the red and blue lines between V9 and V18. \n", "\n", "It should be important to note at this point that the number of positive features is so small compared to the negative features that even strong correlations such as this might be washed away by the sheer size of the negatively labled data. \n", "\n", "To assuage my fears I shall map some of these correlations out and see how the distrubution fairs. Let's start with my favourites - V9 and V10."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "555cc5d8-8585-4175-84c3-c6f87eab22ee", "_uuid": "dc51d5eddc2b551f540717ba659abb91da575a54"}, "source": ["print('V9 - V10')\n", "plt.scatter(fraud['V9'], fraud['V10'],s=1, color='r')\n", "plt.scatter(sample['V9'], sample['V10'], s=1, color='g')\n", "plt.show()\n", "plt.clf()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "eea011c4-2567-46ee-8258-79d33702d676", "_uuid": "c2529bbb21e54597b55837459c6b83dafd0495ec"}, "source": ["A nice clear distribution. The positive results (red points) sweep up from the lower left upwards at a nice angle while the green dots (the negative results) clump in the centre. A few more of these clean delineations and we might be onto something.\n", "\n", "Let's try V16 and V17."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "4b9235f5-1d6f-4a16-a66c-c4fad6f68356", "_uuid": "2d4f8222b2458fa2d80fda573cdabf4beaf7a605"}, "source": ["print('V16-V17')\n", "plt.scatter(sample['V16'], sample['V17'], s=1, color = 'g')\n", "plt.scatter(fraud['V16'], fraud['V17'], s=1, color = 'r')\n", "plt.show()\n", "plt.clf()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b2725ee2-eea4-4d03-9ca7-c4d7eef81520", "_uuid": "fa74403df5f46b143116ef8204f997e3d7ba4dcf"}, "source": ["Again, a nice correlation and a nice clump, but this time the positive results and the negative results share some graph-space. Let's move on to V17 and V18.\n"]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f535bbfa-241b-494d-99de-5fb8b7fd14fa", "_uuid": "c564f495de6940e6f07d13bfd2dadc493d85bcb2"}, "source": ["print('V17 - V18')\n", "plt.scatter(sample['V18'], sample['V17'], s=1, color = 'g')\n", "plt.scatter(fraud['V18'], fraud['V17'], s=1, color = 'r')\n", "plt.show()\n", "plt.clf()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "60e801cd-9ea9-42f5-89cb-6f7fb427f889", "_uuid": "f0109cc299a291fb85a3a3c763ce40f62db8dd0f"}, "source": ["And again. We might be in luck here... Let's try some of the lower features now. V1 and V3 had a nice, strong correlation."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "ccd4e997-3d51-4ef1-a602-862253c021a3", "_uuid": "b3dbfa2dc7b6ae3eadcd50f38f2c251d3bff8be9"}, "source": ["print('V1 - V3')\n", "plt.scatter(sample['V1'], sample['V3'], s=1, color = 'g')\n", "plt.scatter(fraud['V1'], fraud['V3'], s=1, color = 'r')\n", "plt.show()\n", "plt.clf()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "c7ca67cd-482e-4bed-84e7-3bf8595f2789", "_uuid": "a0ea53a38bcb7c6fe74d72c0c140cedf9c853be2"}, "source": ["I... uh.... Ergh. While the correlation did not show strongly on the negative data, you can almost imagine the sound of the data being smeared across the surface. A slow, intentional smear. Not too much, just enough to overwhelm the near straight-line correlation of the positive results. \n", "\n", "Hopefully we will have a better result with V1 and V2."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "03b5520a-88f0-4d4e-a597-e7f01c723b4d", "_uuid": "d050bd2587e111704a9df705367fcabf57be4efa"}, "source": ["print('V1 - V2')\n", "plt.scatter(sample['V1'], sample['V2'], s=1, color = 'g')\n", "plt.scatter(fraud['V1'], fraud['V2'], s=1, color = 'r')\n", "plt.show()\n", "plt.clf()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "36e76b39-4580-43fe-b826-a0b294fee6b6", "_uuid": "966c3c9e98734c0e50d7dfdacff8b11b2e84ff38"}, "source": ["No. This seems much the same, only at a higher speed, as if the smearer realised we were hot on his tail and just hurled the data at the graph as he scarpered out the door. \n", "\n", "I don't think I'll include the smears. \n", "\n", "Anyway - I have a small number of apparently useful data features, enough at least for a rough SVC. You know, just to see if I'm on the right path."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "a35635e1-f98e-4dac-b38e-cbc4d4f650dd", "_uuid": "52c3dbaf4d6348e6c2c259c18226dbaa9fd81f37"}, "source": ["transactions = transactions[['Class', 'V9', 'V10', 'V16', 'V17', 'V18','Amount']]\n", "\n", "\n", "sample = transactions[transactions['Class']==0]\n", "fraud = transactions[transactions['Class'] == 1]\n", "\n", "# need a very small but random sample of the legitimate data since it is massively over represented.\n", "ignore_me, sample = train_test_split(sample, test_size = 0.01)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0b498b86-cd5b-4676-8a5e-26b61be0a125", "_uuid": "6a09cd0c4c76f77a6404e20e53813d9141355c99"}, "source": ["I have used the train_test_split randomness to extract a 1% chunk of the negative data to overcome some of the skewage.\n", "\n", "Now that I have both sample (negative data) and fraud (positive data) I need to concatenate them back together so I can break them apart into a training and test set."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "bb186fa6-f12c-42fd-bafb-53c4ed656a89", "_uuid": "cbcda05a416caf132be71c496e0c3ed6d1de2ed5"}, "source": ["import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "sample = pd.concat([sample, fraud])\n", "\n", "# Break into train and test units.\n", "train, test = train_test_split(sample, test_size = 0.3)\n", "\n", "trainy = train['Class']\n", "testy = test['Class']\n", "train.drop('Class', 1, inplace = True)\n", "test.drop('Class', 1, inplace = True)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0014594b-61ac-47be-a18b-c2b13fe2462b", "_uuid": "31d8cc31dd8081e39353a4cc9f5e6c68061d5eac"}, "source": ["You know how your programming lecturer tells you to never **NEVER** turn off all warnings when compiling? Yeah, don't do what I did. "]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "def929a5-b673-4bfa-8d5f-b120837e81a7", "_uuid": "69efc1f50348a789a568f15adc84944b7ad08a22"}, "source": ["scaler = StandardScaler()\n", "scaler.fit(train)\n", "train = scaler.transform(train)\n", "test = scaler.transform(test)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ad6597fc-29bf-4791-88bb-cb2d55a6f23e", "_uuid": "07a751462f3ebadf3f696c8e61dd016bf4c22ee1"}, "source": ["Scaling because that nice Mr Ng told me to. Also because of smooth, easy gradient descent. Not that it really matters here."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "6a10cf71-900c-4076-999a-645df0e07bff", "_uuid": "d0e3d14a053ffe65d6275e49d84666265cc3f288"}, "source": ["clf = SVC()\n", "clf.fit(train, trainy)\n", "outcome = list(clf.predict(test))\n", "testy = list(testy)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a58a6bf6-521f-441a-bb50-47393b868eaa", "_uuid": "6468d7bffa45f2ca1791522baba3a35d74bb760a"}, "source": ["I've trained my reduced set and thrown the test set at the algorithm. Now I just need to change these ones and zeros into some kind of score."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "7b87be98-4ae1-468f-a6de-144867c436a3", "_uuid": "d4a639cb7443af05ea2f97d8dfe21835efd73443"}, "source": ["count = 0\n", "falsepos = 0\n", "truepos = 0\n", "falseneg = 0\n", "trueneg = 0\n", "\n", "\n", "for i in range (1,len(testy)):\n", "    if (outcome[i]==1):\n", "        if (testy[i] == 1):\n", "            truepos = truepos + 1\n", "        else:\n", "            falsepos = falsepos + 1\n", "    else:\n", "        if (testy[i] == 0):\n", "            trueneg = trueneg + 1\n", "        else:\n", "            falseneg = falseneg  +1\n", "    count = count + 1\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "37e78710-ecb7-4964-b0ba-57856c6cf638", "_uuid": "b5114d9d83426a949935d963cb24c383aa59820b"}, "source": ["The Precision score is high (meaning those items we predicted were fraudulent tended to be fraudulent 98% of the time) compared to my previous attempts. The Recall is not so high, meaning some attempts of fraud slipped under the radar. The F1 is a respectable 90%. Good, but not great.\n", "\n", "Overall, I am happy with the success of this approach and will likely return to see if I can improve.\n", "\n", "I hope you enjoyed reading this."]}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "0a0fdd47-d2a6-4f16-a768-23f5032d8177", "_uuid": "2af37893842b1d35562a9d602aa5f51f9a0f7299"}, "source": ["\n", "precision = truepos / (truepos + falsepos)\n", "recall = truepos / (truepos + falseneg)\n", "F1 = 2*((precision * recall ) / (precision + recall))\n", "\n", "print(\"Precision = \" + str(precision))\n", "print(\"Recall = \" + str(recall))\n", "print(\"F1 = \" + str(F1))\n"]}], "metadata": {"language_info": {"mimetype": "text/x-python", "version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}