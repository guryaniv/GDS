{"cells":[{"metadata":{"_uuid":"a1cf7386268232e0b0b5861163930289c1e48e4c"},"cell_type":"markdown","source":"This kernel is based on https://www.kaggle.com/shujian/ml-model-example-with-train-test.  \nThanks S.L. for your excellent work!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\nimport nltk\nfrom sklearn import *\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test_stage_1.tsv\", delimiter=\"\\t\").rename(columns={\"A\": \"A_Noun\", \"B\": \"B_Noun\"})\nsub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\ntest.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bdd35cff00ff1148603dbb80955bfe20db6ba17"},"cell_type":"code","source":"gh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\ngh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\ntrain = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"169d69ab4bb6e84e95567473724335c57d01596c"},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true,"_uuid":"6547d7db6bdc571acbbdc9c29e499d627c0cfbd6"},"cell_type":"code","source":"def name_replace(s, r1, r2):\n    s = str(s).replace(r1,r2)\n    for r3 in r1.split(\" \"):\n        s = str(s).replace(r3,r2)\n    return s\n\ndef get_features(df):\n    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'subjectone'), axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'subjecttwo'), axis=1)\n    \n    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n    return(df)\n\ntrain = get_features(train)\ntest = get_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79cb7bfeb0073f51c6696bfe9633e005849c952b"},"cell_type":"code","source":"%%time\ndef get_nlp_features(s, w):\n    doc = nlp(str(s))\n    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\n\ntrain['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntrain['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\ntest['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntest['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1942214e43fdc189e2e34bdf19f7b0905fe038c9"},"cell_type":"code","source":"train = train.rename(columns={\"A-coref\": \"A\", \"B-coref\": \"B\"})\ntrain[\"A\"] = train[\"A\"].astype(int)\ntrain[\"B\"] = train[\"B\"].astype(int)\ntrain[\"NEITHER\"] = 1.0 - (train[\"A\"] + train[\"B\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa866228e5526cb2a180283da1ae6f60ab53c516"},"cell_type":"code","source":"col = [\"Pronoun-offset\", \"A-offset\", \"B-offset\", \"section_min\", \"Pronoun-offset2\", \"A-offset2\", \"B-offset2\", \"section_max\", \"A-poss\", \"B-poss\", \"A-dist\", \"B-dist\"]\nx1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[[\"A\", \"B\", \"NEITHER\"]], test_size=0.2, random_state=1)\nx1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53b6dfc97b2af37b0954baf38baf69fca89745a0"},"cell_type":"markdown","source":"# Model Training & Prediction"},{"metadata":{"trusted":true,"_uuid":"658efeb99b3054cd35f6023f730346467b651e89"},"cell_type":"code","source":"# set hyper parameters\nlgb_params = {\"learning_rate\": 0.01,\n              \"num_leaves\": 16,\n              \"min_data_in_leaf\": 20,\n              \"boosting\": \"gbdt\",\n              \"num_iterations\": 120,\n              \"bagging_fraction\": 0.6,\n              \"feature_fraction\": 1.0,\n              \"seed\": 42,\n              \"num_threads\": -1\n              }\n\"\"\"\nxgb_params = {\"eta\": 0.05,\n              \"max_depth\": 2,\n              \"n_estimators\": 120,\n              \"objective\": \"binary:logistic\",\n              \"eval_metric\": \"logloss\",\n              \"booster\": \"gbtree\",\n              \"subsample\": 0.6,\n              \"colsample_bytree\": 0.6,\n              \"seed\": 42,\n              \"n_jobs\": -1\n             }\n\"\"\"\n\n#model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth=7, n_estimators=1000, random_state=33))\n#model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**xgb_params))\nmodel = multiclass.OneVsRestClassifier(lgb.LGBMClassifier(**lgb_params))\n\n# 5 fold CV\nfolds = 5\nkf = KFold(n_splits=folds, shuffle=False, random_state=11)\ntrn = train[col].fillna(-1)\nval = train[[\"A\", \"B\", \"NEITHER\"]]\nscores = []\ni = 0\n\nfor train_index, test_index in kf.split(train):\n    x1, x2 = trn.iloc[train_index], trn.iloc[test_index]\n    y1, y2 = val.iloc[train_index], val.iloc[test_index]\n\n    model.fit(x1, y1)\n    score = metrics.log_loss(y2, model.predict_proba(x2))\n    print(str(i+1), \"log-loss:\", score)\n    scores.append(score)\n    i += 1\n\nprint(\"CV Score(log-loss):\", np.mean(scores))\n\n\nmodel.fit(train[col].fillna(-1), train[[\"A\", \"B\", \"NEITHER\"]])\nresults = model.predict_proba(test[col])\ntest[\"A\"] = results[:,0]\ntest[\"B\"] = results[:,1]\ntest[\"NEITHER\"] = results[:,2]\ntest[[\"ID\", \"A\", \"B\", \"NEITHER\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dabbbbf156c19fd310cef65cdbe5ebb8029a299"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}