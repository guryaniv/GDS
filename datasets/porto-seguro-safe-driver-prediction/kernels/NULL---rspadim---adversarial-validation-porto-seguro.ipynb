{"cells": [{"source": ["FROM https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms\n", "\n", "motivied by @the1owl =) thanks @the1owl\n", "\n", "\n", "---\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["[FROM OTHER CONTEST]\n", "\n", "Like probably everyone else in this contest, I've been scratching my head: what to do about validation? The usual approaches don't work (obviously regular cross validation is a bad idea due to the time dimension [NOT IN PORTO SEGURO CONTEST], the score on last year of data is nowhere near the LB). Solution? Adversarial validation, inspired by FastML:\n", "\n", "[Adversarial validation][1]\n", "\n", "The general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0 - train, 1-test) and evaluating a binary classification task.\n", "\n", "\n", "  [1]: http://fastml.com/adversarial-validation-part-two/"], "metadata": {"_uuid": "1d734049717370542a597437f417e0437221c609", "_cell_guid": "b1511075-2040-1acf-08f8-db4ab6bed4e1"}, "cell_type": "markdown"}, {"source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import StratifiedKFold\n", "import xgboost as xgb\n", "from sklearn.metrics import roc_auc_score\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "outputs": [], "metadata": {"_uuid": "4863a6e30358fbb65fe3ef2d6a4045dc1ae034a6", "_cell_guid": "bcbf5e1a-6442-47ba-5435-7053db612903"}, "cell_type": "code", "execution_count": 8}, {"source": ["# We start by loading the training / test data and combining them with minimal preprocessing necessary\n", "xtrain = pd.read_csv('../input/train.csv')\n", "xtrain.drop(['id', 'target'], axis = 1, inplace = True)\n", "xtest = pd.read_csv('../input/test.csv')\n", "xtest.drop(['id'], axis = 1, inplace = True)\n", "\n", "# add identifier and combine\n", "xtrain['istrain'] = 1\n", "xtest['istrain'] = 0\n", "xdat = pd.concat([xtrain, xtest], axis = 0)\n", "\n", "# convert non-numerical columns to integers\n", "df_numeric = xdat.select_dtypes(exclude=['object'])\n", "df_obj = xdat.select_dtypes(include=['object']).copy()\n", "    \n", "for c in df_obj:\n", "    df_obj[c] = pd.factorize(df_obj[c])[0]\n", "    \n", "xdat = pd.concat([df_numeric, df_obj], axis=1)\n", "y = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)"], "outputs": [], "metadata": {"_uuid": "45aad642ad15eebf87376343bd6421aa5bbe5156", "_cell_guid": "3f4ffc92-a776-6715-b7f3-925f1547d4fc"}, "cell_type": "code", "execution_count": 9}, {"source": ["[FROM OTHER CONTEST]\n", "\n", "Define a split and the model (xgboost, what else :-)"], "metadata": {"_uuid": "ea6f7afc0e25ce0ebafe785f568e5c1a72b6f026", "_cell_guid": "d0d09367-f024-ae7e-82ce-e75d53780274"}, "cell_type": "markdown"}, {"source": ["skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 44) # why stratified k fold?\n", "xgb_params = { # is this parameters ok?\n", "        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n", "        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n", "        'silent': 1, 'n_estimators':100, 'gamma':1,\n", "        'min_child_weight':4, 'n_jobs':-1\n", "        }   \n", "clf = xgb.XGBClassifier(**xgb_params, seed = 10)     "], "outputs": [], "metadata": {"collapsed": true, "_uuid": "e7bdea3e061b85af7eba75d5a5e0e407ba2d8285", "_cell_guid": "891a4cdc-d085-b67f-45e7-dcdc3ca0f8b2"}, "cell_type": "code", "execution_count": 10}, {"source": ["[FROM OTHER CONTEST]\n", "\n", "Calculate the AUC for each fold"], "metadata": {"_uuid": "6ffde62c36939f3f7c3c6db97ba8257fede6b19c", "_cell_guid": "e103b1ab-479c-777d-ee19-a9f3a572a7c4"}, "cell_type": "markdown"}, {"source": ["for train_index, test_index in skf.split(xdat, y):\n", "        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n", "        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n", "        print(x0.shape)\n", "        clf.fit(x0, y0, eval_set=[(x1, y1)],\n", "               eval_metric='logloss', verbose=False,early_stopping_rounds=10) # it takes ~ 80 rounds to fit\n", "                \n", "        prval = clf.predict_proba(x1)[:,1]\n", "        print(roc_auc_score(y1,prval))"], "outputs": [], "metadata": {"_uuid": "c460000353cffc78554d772be84707b5e3e48be2", "_cell_guid": "223a095a-712f-fa90-707a-ea34fa091fc0"}, "cell_type": "code", "execution_count": null}, {"source": ["[FROM OTHER CONTEST]\n", "\n", "As we can see, the separation is almost perfect - which strongly suggests that the train / test rows are very easy to distinguish even for an xgboost\n", "\n", "---\n", "\n", "maybe not... 0.499538408032 (not tunned parameters) of log loss vs 0.992604547897 from  Sberbank Russian Housing Market"], "metadata": {"_uuid": "7aa9c2629e7f3b59b4d5ea01c50ebe9a15f35e65", "_cell_guid": "394561c3-a9d5-d3ad-31a0-801a8e7e2c42"}, "cell_type": "markdown"}, {"source": [], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}, {"source": [], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null}], "nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "name": "python", "version": "3.6.0", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}, "_is_fork": false, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "_change_revision": 0}, "nbformat": 4}