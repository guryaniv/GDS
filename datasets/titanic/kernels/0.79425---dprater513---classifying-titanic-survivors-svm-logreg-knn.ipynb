{"cells":[{"metadata":{"_uuid":"3b02a48390de37a19dcedf0903ccd184dfa2a342"},"cell_type":"markdown","source":"The purpose of this kernel is to reinforce my understanding of using the logistic regression, k nearest neighbors and support vector machine to make predictions on a binary response. In addition to that, I'd like to get practice of completing an end to end machine learning project. This kernel contains the following sections:\n1. Data Cleaning\n2. Exploratory Data Analysis\n3. Feature Engineering\n4. Model training and selection using a training and validation data set.\n5. Submitting predictions."},{"metadata":{"_cell_guid":"8cd8adb0-b91e-4ca8-a4f3-d1b23b9d8d12","_uuid":"93155b9f42636438fc3dbb785cc6281d0c44ae38","trusted":true},"cell_type":"code","source":"#Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # import seaborn\nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79526e36-b78f-4d5f-9b00-c53b03d3cc35","_uuid":"d86cdec1dab838d7f963acd528c1ee827840d3d9"},"cell_type":"markdown","source":"# 1. Data Cleaning"},{"metadata":{"_cell_guid":"95b21bcc-3272-482e-90c2-0b337e9facf6","_uuid":"38c4b523fb9ba610a647cb4ade97c59df1ac64da","trusted":true},"cell_type":"code","source":"titanic_train = pd.read_csv('../input/train.csv')\ntitanic_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a5d9830b-742f-4c26-a362-9f3eb3eb79db","_uuid":"905de21a4c696957ba666ad3789b740e5b8ea1d2","trusted":true},"cell_type":"code","source":"titanic_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"efde6699-f5cf-4d4f-bb9d-6ca775b78ae3","_uuid":"9703590d1ec31416bdf3062d5d42d4011cc895cd"},"cell_type":"markdown","source":"## Some observations about the head of the data frame\n* Looks like there are 11 features and one response variable \"survived\".\n* PassengerID could represent the index of the data frame\n* Pclass looks like it could be a categorical variable\n* Cabin appears to have some missing data."},{"metadata":{"_cell_guid":"24bee767-1c68-40d0-a2ec-1b1a8c248af2","_uuid":"940ba5fdf466f3ba07ff5061be6525de9fdf3c8a","trusted":true},"cell_type":"code","source":"titanic_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1068578a-918c-4ce3-8ff9-fc8851303e9c","_uuid":"376d556b037993a7f1dd3a4f6da197dac6971291"},"cell_type":"markdown","source":"Looks like the following columns have some missing values:\n\n* Age\n* Cabin - which aligns with the previous observation\n* Embarked"},{"metadata":{"trusted":true,"_uuid":"9228d6f67f954a9698b3aa8ee38de746e184519f"},"cell_type":"code","source":"titanic_test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab99abb06bc6bcd7687fc1d9090fbf64a6ea11c5"},"cell_type":"markdown","source":"The test data set had missing values for the following columns\n* Age\n* Cabin - which aligns with the previous observation\n* Fare"},{"metadata":{"_cell_guid":"995be0c2-974b-4746-b085-cf93aab2bc3b","_uuid":"23ed04f4684bc1e5483b273255d3510cb0a364a8","trusted":true},"cell_type":"code","source":"titanic_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70cb3832-f9db-4a16-acd9-e15121d9a938","_uuid":"19edde69029f9eb5a40c7b8740aeaa6a9c9ffe90","trusted":true},"cell_type":"code","source":"titanic_train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95c31e16-ed94-400d-b5f8-8270d4666448","_uuid":"f1a5dc71e130f35ba05f187bdc407f5cf6d302e5"},"cell_type":"markdown","source":"A couple things here:\n* No repeated names\n* 3 different values for embarked"},{"metadata":{"_cell_guid":"94979a1d-0e33-4d76-ac87-6b0f21bd04fd","_uuid":"be2571823dc474b7b1b973b841f3fee3fabf8a78"},"cell_type":"markdown","source":"# Data Cleaning\nIn order to clean this dataset, I'd like to make sure that each column is free from NaN values and is of the correct type. As noted previously, the age, embarked, and cabin columns are all missing values."},{"metadata":{"_uuid":"5ca15e0e7c4be13173b6fb46812dff1c27ab28bb"},"cell_type":"markdown","source":"Let's take a look at the age column"},{"metadata":{"_uuid":"77fe94641bab8c551bb8ec825508e47affec3a74","trusted":true},"cell_type":"code","source":"print(\"Age broken down by P-class\")\ntitanic_train.groupby('Pclass').mean()[['Age']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be6e3f82e2a4317d09752d48784f7c5388481740"},"cell_type":"markdown","source":"I'm going to impute the age column based on the average age per passenger determined by the Pclass column for both the training and testing data sets because both of these columns contain missing data and less than 25% of the column is missing data. "},{"metadata":{"_uuid":"3af62976480a5b6e47e5b28e9121fc3616ef7995","trusted":true},"cell_type":"code","source":"titanic_train.loc[titanic_train.Age.isnull(), 'Age'] = titanic_train.groupby('Pclass')['Age'].transform('mean')\ntitanic_test.loc[titanic_test.Age.isnull(), 'Age'] = titanic_test.groupby('Pclass')['Age'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bca0533a724ffa5908e37cc95edd513a66dc9b4"},"cell_type":"markdown","source":"Check out rows 5 and 17 to ensure age of ~25 got inputed for age in row 5 and ~29 was inputted for age in row 17. Looks good, and checking .info() method there are no missing values for age column."},{"metadata":{"_uuid":"d6e5576f19e0a783395438b148d8026deeb6bf51","trusted":true},"cell_type":"code","source":"titanic_train.iloc[[5, 17]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63d526ea5ed73571cb355839c1fd4ee1811592f8"},"cell_type":"markdown","source":"Due to the large number of missing entires for the cabin column in both the training and testing dataset, I'm going to drop it from both."},{"metadata":{"_uuid":"b7643cff68ddac00ed0bda4a0ede0b91ab81c84e","trusted":true},"cell_type":"code","source":"titanic_train = titanic_train.drop('Cabin', axis=1)\ntitanic_test = titanic_test.drop('Cabin', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec7150db1512a85ed5d3359220224b571e16be2"},"cell_type":"markdown","source":"Also because Embarked is only missing two entries from the training dataset and fare is only missing one entry from the test dataset I'm just going to impute these values with the mode and median value for each column respectively."},{"metadata":{"_uuid":"bd483cba4b12d9122a8f596646fdd3b814d56216","trusted":true},"cell_type":"code","source":"titanic_train['Embarked'].fillna(titanic_train['Embarked'].mode()[0], inplace=True)\ntitanic_test['Fare'].fillna(titanic_test['Fare'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31a82027a1a4c68c5adce38f6927190caf10a4b0"},"cell_type":"markdown","source":"Ensure all columns have no null values"},{"metadata":{"trusted":true,"_uuid":"951a71f469be776bfb2709e544e3fc21ba37e553"},"cell_type":"code","source":"print('Training Data Null Values')\nprint(titanic_train.isnull().sum())\nprint(\"-\" * 30)\nprint('Test Data Null Values')\nprint(titanic_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cced8f744b8faecf1fde2c0c7335e731cdc2516"},"cell_type":"markdown","source":"Looks like all columns are cleaned"},{"metadata":{"_uuid":"c5797616d1379e57e083f3d829746873021ed0f1"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"48f59aa89fef7b03f7e5b47957e9a980f3431b18","trusted":true},"cell_type":"code","source":"titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"924d49fd04ab94dd9498c59925e9605d4c1ab0d1"},"cell_type":"markdown","source":"Because the goal is to predict the Survived column I want to take a look at the class balance in that column"},{"metadata":{"trusted":true,"_uuid":"895346b794d126cafaf031cc1f87d81a2cff6789"},"cell_type":"code","source":"sns.countplot(x='Survived', data=titanic_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc6734937348897f4fbac2e18f37218367bf95fe"},"cell_type":"markdown","source":"There is a class imbalance meaning that more people did not survive the titanic than did survive it in our training dataset."},{"metadata":{"_uuid":"e24396eb712370001854b88bf8dbf15acb3a10f4"},"cell_type":"markdown","source":"Want to look at how the price of tickets bought varied by the age of the people on board."},{"metadata":{"trusted":true,"_uuid":"ee7d96d6175f98fd62f2fa528c84a9b9b4032591"},"cell_type":"code","source":"sns.boxplot(x = 'Survived', y = 'Fare', data = titanic_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d17a5796fde2fa63f9536d53d04ffdb9967f366f"},"cell_type":"code","source":"titanic_train.groupby('Survived').mean()[['Fare']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b875f23ba5242804cff44461a4a6c95beb3af92"},"cell_type":"markdown","source":"Looks like the median ticket price is larger for those who survived. Average ticket price is much higher but is likely due to the outlier. Want to investigate this outlier. Look below and see that three individuals purchased tickets at a fare of $512. Money must have not been a problem for these folks!"},{"metadata":{"trusted":true,"_uuid":"ef2e4f716f99b82b41342c7995fa295b2612693d"},"cell_type":"code","source":"titanic_train.loc[titanic_train['Fare'] > 500, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02d51d44477642a4537ea2c5fbb0b112c759a513"},"cell_type":"code","source":"titanic_no_500s = titanic_train.loc[titanic_train['Fare'] < 500, :]\nsns.boxplot(x = 'Survived', y = 'Fare', data = titanic_no_500s, palette = 'RdBu_r')\ntitanic_no_500s.groupby('Survived').mean()[['Fare']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7384beb009f10e51817efef4183103e959bd6548"},"cell_type":"markdown","source":"With the fare's of 500+ removed, the boxplots are more readable. The mean and median are definitely higher for those who survived and will include as a feaure for model training."},{"metadata":{"_uuid":"51cadaea9ff9be650469d803363c787d2400087a"},"cell_type":"markdown","source":"Now I want to take a look at the effect of male vs female passengers"},{"metadata":{"trusted":true,"_uuid":"7cdb50eecbe26b41c742686cc5677364f52e2846"},"cell_type":"code","source":"sns.countplot(x = 'Sex', data = titanic_train, hue = 'Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b4d0fc170ae3220f2fcb60e20a9d5c74574a357"},"cell_type":"markdown","source":"Looking at this chart more male a larger proportion of male passengers didn't survive when compared to female. Will consider this as an important feature for model training and building."},{"metadata":{"_uuid":"366649cd8a1d4b085bf6cc686a2c583f314bbcfe"},"cell_type":"markdown","source":"Let's take a look at the Age column"},{"metadata":{"trusted":true,"_uuid":"e4aa1c2c6ec512172fb128188ef46b305465f5a5"},"cell_type":"code","source":"hist = sns.distplot(titanic_train['Age'], color='b', bins=30, kde=False)\nhist.set(xlim=(0, 100), title = \"Distribution of Passenger Age's\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42eac3466d26a9d0df3613da0418cdf09e3f0e7d"},"cell_type":"code","source":"titanic_train.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89f95fddecd219a33af3f2beea2039d23bfcb3b6"},"cell_type":"code","source":"age_box = sns.boxplot(y = 'Age', x = 'Survived',data = titanic_train, palette='coolwarm')\nage_box.set(title='Boxplot of Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"139f80c95238c3ae956ed028e2f8cbf0a2d6486e"},"cell_type":"markdown","source":"Based on the description and histogram our passengers are roughly normally distributed with a mean of 29 and median of 26 years of age respectively. Looking at the boxplots of ages of passengers who did and didn't survive the distributions look relatively similar. Based on this I'm debating including the age column in model training."},{"metadata":{"_uuid":"4b58100f70bb98579cc169cb5a83f4146482152e"},"cell_type":"markdown","source":"Embarked Column"},{"metadata":{"trusted":true,"_uuid":"f12ff9401f0f9ba3b2fb971feaa6b5cc0b2017a8"},"cell_type":"code","source":"titanic_train.groupby(['Embarked']).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16893b33ee8a63e201f80c32d8bf61bfa793cd08"},"cell_type":"code","source":"sns.countplot(x = 'Embarked', hue = 'Survived', data=titanic_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6b23566cd7cbf9534c62904abceeb66f4a15686"},"cell_type":"markdown","source":"Looks like people who boarded from S were more likely to not survive than those who didn't board at S"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"28239d3dd4d719af1ccd73f089e47f2917054ee0"},"cell_type":"markdown","source":"PClass column"},{"metadata":{"trusted":true,"_uuid":"10e17a086c997d9eccb66b056fcb732975171b10"},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass', data=titanic_train, palette = 'rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c53a123911ae1914ae5e0e2193e3b296d5d0b32"},"cell_type":"markdown","source":"Looks like a majority of those who didn't survive were in the 3rd P-class. Would definitely be worth including as a feature in the model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ec94c2c0dabde78b441448483652cff3b8726dd"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"_uuid":"4d584f38ed73b333f1dc5119d5781fbb079c8c20"},"cell_type":"markdown","source":"First step is to make copies of each dataframe"},{"metadata":{"trusted":true,"_uuid":"d0b94ac6bc74be85651aff9e635058f66b0089d2"},"cell_type":"code","source":"#Make copies of both dataframes.\ntraindf = titanic_train.copy()\ntestdf = titanic_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76459e322a1b2177bd1a45091040d73514c02832"},"cell_type":"markdown","source":"Next I'm going to put the copied dataframes into a list so I can perform the same actions to both dataframes."},{"metadata":{"trusted":true,"_uuid":"84ade1efc328bf6557ad51d1de376e3aa587c85c"},"cell_type":"code","source":"#Create list of both data frames to apply similar functions to.\nall_data = [traindf, testdf]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"038a5842cc01d74290e7eb545f85eaf563950e49"},"cell_type":"markdown","source":"### Drop Name and Ticket Columns"},{"metadata":{"trusted":true,"_uuid":"39a5f92f84a5ab978324f325735772ec84fe2b2d"},"cell_type":"code","source":"#Drop name and ticket columns\nfor dat in all_data:\n    dat.drop(['Name', 'Ticket'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aac624eef93e61a479bd0be53fa0a0178187140"},"cell_type":"markdown","source":"### Bin Fare Column\nNext I'm going to bin the fare column based on the summary statistics for that column"},{"metadata":{"trusted":true,"_uuid":"f070f631713e5aab4472d4ec35d5995c85958c4d"},"cell_type":"code","source":"traindf.describe()['Fare']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7f0e7b7e3f6a2c5f5957876ed18027809f645db"},"cell_type":"markdown","source":"Looks like some good cutoff points will be 0, 8, 15, 31, and 515 to include the max fare value of 512."},{"metadata":{"trusted":true,"_uuid":"e504410fd4423c34df880413dd19b0e0795caf56"},"cell_type":"code","source":"#Perform operation on both frames\nfor dat in all_data:\n    \n    #Create bins to separate fares\n    bins = (0, 8, 15, 31, 515)\n\n    #Assign group names to bins\n    group_names = ['Fare_Group_1', 'Fare_Group_2', 'Fare_Group_3', 'Fare_Group_4']\n\n    #Bin the Fare column based on bins\n    categories = pd.cut(dat.Fare, bins, labels=group_names)\n    \n    #Assign bins to column\n    dat['Fare'] = categories\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4105d8ddeca8d8e744506fe9fd851c45c066f0ba"},"cell_type":"markdown","source":"### Bin Age Column"},{"metadata":{"trusted":true,"_uuid":"6b983a5e17b154bb8d0ddbdea7c80b5a7df4c55a"},"cell_type":"code","source":"traindf.describe()['Age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02eae3c0619299a95ea0bf83befa3fa0236e3c9"},"cell_type":"markdown","source":"Am going to try binning by every 15 years."},{"metadata":{"trusted":true,"_uuid":"4e64c1e02e77cf9a91c980e379b693b2326e131b"},"cell_type":"code","source":"#Perform operation on both frames\nfor dat in all_data:\n    \n    #Create bins to separate fares\n    bins = (0, 15, 30, 45, 60, 75, 90)\n\n    #Assign group names to bins\n    group_names = ['Child', 'Young Adult', 'Adult', 'Experienced', 'Senior', 'Elderly']\n\n    #Bin the Fare column based on bins\n    categories = pd.cut(dat.Age, bins, labels=group_names)\n    \n    #Assign bins to column\n    dat['Age'] = categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ad0425805b5315e8e6bb9fe13bd327d5e736a2"},"cell_type":"code","source":"traindf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f19066b9a434fa570dd4b71faddd70c00e7689b"},"cell_type":"markdown","source":"### Create Family Size Feature. SibSp + Parch"},{"metadata":{"trusted":true,"_uuid":"dabbdceca650e00fc9910eb5cb15a9c8edde8d82"},"cell_type":"code","source":"for dat in all_data:\n    dat['Fam_Size'] = dat['SibSp'] + dat['Parch']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ef5a09c22212976d4dcf96c3a580273f4a2c5e"},"cell_type":"markdown","source":"### Use one hot encoding to code categorical variables."},{"metadata":{"trusted":true,"_uuid":"9197d83d92ba1e049f5e525f6fcb265380186211"},"cell_type":"code","source":"traindf = pd.get_dummies(traindf)\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d3c7034fbfd36365deaf99598c91307a39ee018"},"cell_type":"code","source":"testdf = pd.get_dummies(testdf)\ntestdf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c485dd9ce8cbc09c59183ccae3fac9dfa561ead"},"cell_type":"markdown","source":"# Machine Learning\nIn order to predict whether a passenger survived the titainc or not, a classification machine learning algorithm will be needed. I've decided for this kernel to try the following methods:\n* Logistic Regression\n* Support Vector Machine\n* K Nearest Neighbors\n\nThe steps I'm going to take to find the best model are outlined below\n1. Split data into training, validation, and test sets\n2. Train and fit each model to training data\n3. Test each model on validation data\n4. Pick model with highest prediction accuracy on validation set.\n5. Use model from step 4 on test dataset."},{"metadata":{"trusted":true,"_uuid":"7863507e71b198dca1676381f3dd966e43764d26"},"cell_type":"code","source":"#Import libraries\nfrom sklearn.metrics import confusion_matrix #confusion matrix\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest Classifier\nfrom sklearn.svm import SVC #Support Vector Machine\nfrom sklearn.preprocessing import StandardScaler #For scaling data\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.model_selection import train_test_split #Split data into training and validation sets.\nfrom sklearn.metrics import accuracy_score  #Accuracy Score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b08c06dd6989dfe11566bb0da4cac84fac1757"},"cell_type":"markdown","source":"### 1. Split data into training and validation sets\nBecause we already have the test dataset provided to us, all we need to do is split the training dataset into a training and validation set."},{"metadata":{"trusted":true,"_uuid":"aaaa64b1cf57001d429b1f5e141d876bfd72f3ee"},"cell_type":"code","source":"#Split data into training and validation set\nX = traindf.drop(columns=['PassengerId', 'Survived'], axis=1)\ny = traindf['Survived']\n\n#Note they are labeled as test sets but I'm treating them as validation data sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb46cb6fdbea4e980f85093d4feea37205bc5417"},"cell_type":"markdown","source":"### 2. Train and fit each model to train, test on validaiton data.\nI will do this for each model listed above. The dataframe below will hold the validation results."},{"metadata":{"trusted":true,"_uuid":"b0afea8d1239544aa3ed8650deef9daa5e3aa173"},"cell_type":"code","source":"results = pd.DataFrame(columns=['Validation'], index=['Logistic Regression', 'Support Vector Machine', 'KNN', 'Random Forest'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"529a5a7791705b7477727614c9fbdcbbcb94f3a2"},"cell_type":"markdown","source":"### Logistic Regression\nFirst create function to train, fit, and test logistic regression model on validation data"},{"metadata":{"trusted":true,"_uuid":"5044626ced6b20c16755eb5dfb39eaf0796b100b"},"cell_type":"code","source":"def log_reg(X_train, X_test, y_train, y_test):\n    #Create logmodel object\n    logmodel = LogisticRegression(C=.01)\n\n    #fit logistic regression model\n    logmodel.fit(X_train, y_train)\n\n    #Make predictions on validation data\n    predictions = logmodel.predict(X_test)\n    \n    #Print Statistics\n    print(accuracy_score(y_test, predictions))\n    \n    #Return predictions\n    return accuracy_score(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b052234215c7d37d8ccc192fe50f0c984eba0b5"},"cell_type":"code","source":"#Get prediction accuracy for model.\nLR_preds = log_reg(X_train, X_test, y_train, y_test)\n\n#Add to dataframe.\nresults.loc['Logistic Regression', 'Validation'] = LR_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"676c9c7b03f77b779a1c9a5779543806e2fcedc3"},"cell_type":"markdown","source":"### Support Vector Machine\nFirst create function to train, fit, and test support vector machine model on. For SVM we will need to scale the input features.\n"},{"metadata":{"trusted":true,"_uuid":"6cde6ba2d1a9f8e9f06e07d8d3466792b9ba7d4a"},"cell_type":"code","source":"def svm(X_train, X_test, y_train, y_test):\n    \n    #Scale data\n    #scaler = StandardScaler()\n    #scaler.fit(X_train)\n    #X_train = scaler.transform(X_train)\n    #X_test = scaler.transform(X_test)\n    \n    #Create list of c values to try\n    c_vals = list(range(1, 100))\n    \n    #Accuracy list\n    accuracy = [0 for i in range(99)]\n    \n    #Loop through c_values\n    for i, c in enumerate(c_vals):\n        #Create support vector machine object\n        svc_model = SVC(C=c)\n        \n        #fit support vector machine model\n        svc_model.fit(X_train, y_train)\n        \n        #Make predictions\n        predictions = svc_model.predict(X_test)\n        \n        #add accuracy score to accuracy list\n        accuracy[i] = accuracy_score(y_test, predictions)\n    \n    print(\"Best C Value:\", c_vals[accuracy.index(max(accuracy))])\n    print(accuracy)\n    print(\"Prediction Accuracy: \", max(accuracy))\n    \n    return max(accuracy)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa7b5a8d68bdaec64f3f53d7fe3158d347981216"},"cell_type":"code","source":"#Get support vector machine results\nsvm_preds = svm(X_train, X_test, y_train, y_test)\n\n#Add to dataframe.\nresults.loc['Support Vector Machine', 'Validation'] = svm_preds\nresults.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5437b7def2fc029d83b22a030d13bc02615b2ce3"},"cell_type":"markdown","source":"### K Nearest Neighbors\nFirst create function to train, fit, and test K Nearest Neighbors model on. For KNN we will need to scale the input features."},{"metadata":{"trusted":true,"_uuid":"ba8436d65347623f5d0a78172d4ec3892789521f"},"cell_type":"code","source":"def knn(X_train, X_test, y_train, y_test):\n    \n    #Scale data\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    #Create list of c values to try\n    ks = [i + 1 for i in range(20)]\n    \n    #Accuracy list\n    accuracy = [0 for i in range(20)]\n    \n    #Loop through c_values\n    for i, k in enumerate(ks):\n        #Create support vector machine object\n        knn = KNeighborsClassifier(n_neighbors = k)\n        \n        #fit support vector machine model\n        knn.fit(X_train, y_train)\n        \n        #Make predictions\n        predictions = knn.predict(X_test)\n        \n        #add accuracy score to accuracy list\n        accuracy[i] = accuracy_score(y_test, predictions)\n    \n    print(ks)\n    print(accuracy)\n    print(\"Best k Value:\", ks[accuracy.index(max(accuracy))])\n    \n    print(\"Prediction Accuracy: \", max(accuracy))\n    \n    return max(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be077bc6633ded8dbb61b4657c46d5d293007fb"},"cell_type":"code","source":"knn_preds = knn(X_train, X_test, y_train, y_test)\nresults.loc['KNN', 'Validation'] = knn_preds\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5324b443949d7636f3ffe4d30689e0ae27a8a49d"},"cell_type":"markdown","source":"![](http://)Use SVM with C = 1 to make predictions on testing data."},{"metadata":{"trusted":true,"_uuid":"1b00d7b90a44487fbc7ae3cec6b5df1cf9bcf7bb"},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8afef09ca8e001726d77b35b78a6f47f8b4c9395"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X)\ntest_feats = testdf.drop('PassengerId', axis=1)\nX = scaler.transform(X)\ntest_feats = scaler.transform(test_feats)\npca = PCA(n_components = 4)\npca.fit(X)\nx_train_pca = pca.transform(X)\nx_test_pca = pca.transform(test_feats)\nsvc_model = SVC(C = 1)\nsvc_model.fit(x_train_pca, y)\nsvm_predictions = svc_model.predict(x_test_pca)\noutput = pd.DataFrame({ 'PassengerId' : testdf['PassengerId'], 'Survived': svm_predictions })\noutput.to_csv('titanic-predictions-svm-pca.csv', index=False)\noutput\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a9f49e22654318f8d1744b80d9e86c1047634d"},"cell_type":"code","source":"#svc_model = SVC(C = 1)\n#svc_model.fit(X, y)\n#svm_predictions = svc_model.predict(test_feats)\n#output = pd.DataFrame({ 'PassengerId' : testdf['PassengerId'], 'Survived': svm_predictions })\n#output.to_csv('titanic-predictions-svm.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d52f8cf4076e3e134f17729d32f8dc3aa98ad4dc"},"cell_type":"markdown","source":"Conclusion: This model resulted in 78.947% accuracy which ranks in the top 1/2 of submissions on the kaggle leaderboard. As this was intended to be a \nsimple notebook to reinforce learning concepts I'm pretty happy with this result. As I continue to improve my feature engineering skills and understand the workings of more advanced machine learning models I will update this kernel to try and improve upon the body of work that is here.\n\nIf you  made it this far, thanks for reading! Any feedback is appreciated :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}