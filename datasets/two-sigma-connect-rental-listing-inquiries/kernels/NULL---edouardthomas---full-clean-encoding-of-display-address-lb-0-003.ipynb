{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f8ddae87-ddae-4230-cd2f-527ce0fcf3e1"
      },
      "source": [
        "**OBJECTIVE**\n",
        "The objective is to encode the address feature better than one-hot, ie. with less columns but without information loss.\n",
        "The objective is to get a sparse matrix whose each column corresponds to a street name, an avenue name, or more generally any element of interest.\n",
        "For this purpose we create a dictionnary with all important places (Broadway, 5th street,...), and important keywords (ex : rd, st, blvd...). To build this dictionnary we begin by cleaning the address field and we select the expressions that appear the most.\n",
        "If an address contains an element of the dictionnary it gets a one in the corresponding column.\n",
        "Therefore, this encoding of the address field is not stricty one-hot, as there can be more than 1 one per row. It is cleaner than one-hot encoding (less columns) and gives the results a nice boost :)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e6895ec0-05bb-31f1-7e68-5c59390dedd9"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "from scipy.sparse import csr_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72c73fd0-d424-2e9d-fe27-28bf352c30bd"
      },
      "source": [
        "Now time for cleaning the addresses :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f9bc3053-7e90-afba-ae1d-0a5976e22e80"
      },
      "outputs": [],
      "source": [
        "train = pd.read_json(\"../input/train.json\")\n",
        "test = pd.read_json(\"../input/test.json\")\n",
        "data=pd.concat([train.drop(\"interest_level\",1),test],0)\n",
        "\n",
        "#To select the rows corresponding to the train and the test set\n",
        "trainSel=np.array([True]*len(train)+[False]*len(test))\n",
        "testSel=np.array([False]*len(train)+[True]*len(test))\n",
        "\n",
        "data=data.reset_index(drop=True)\n",
        "\n",
        "#We need the field of interest\n",
        "disadr=data.display_address    \n",
        "\n",
        "#What does this feature look like ? \n",
        "disadr.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d3b67af8-0b24-4889-d37c-6259754048b5"
      },
      "outputs": [],
      "source": [
        "def cleanAdr(adress):\n",
        "    adress=adress.lower()\n",
        "    adress=adress.strip()\n",
        "    adress=re.sub('\\.','',adress)\n",
        "    adress=re.sub(',','',adress)\n",
        "    adress=re.sub('\\'','',adress)\n",
        "    adress=re.sub('(?P<num>[0-9]+)(th|st|rd|nd)','\\g<1>',adress) \n",
        "    adress=re.sub('(?P<num>[0-9]+) (th|nd)','\\g<1>',adress) \n",
        "    adress=re.sub('3 rd (?P<num>(st|rd|ave))','3 \\g<1>',adress) \n",
        "    adress=re.sub('^[0-9]+-[0-9]+ ','',adress)\n",
        "    adress=re.sub('street','st',adress) \n",
        "    adress=re.sub(' av$',' ave',adress)\n",
        "    adress=re.sub('avenue','ave',adress)\n",
        "    adress=re.sub('place','pl',adress)\n",
        "    adress=re.sub('boulevard','blvd',adress)\n",
        "    adress=re.sub(' lane',' ln',adress)\n",
        "    adress=re.sub(' road',' rd',adress)\n",
        "    adress=re.sub(' parkway',' pkwy',adress)\n",
        "    adress=re.sub(' square',' sq',adress)\n",
        "    adress=re.sub(' drive',' dr',adress)\n",
        "    adress=re.sub(' park',' pk',adress)\n",
        "    adress=re.sub('east','e',adress) \n",
        "    adress=re.sub('west','w',adress) \n",
        "    adress=re.sub('north','n',adress)\n",
        "    adress=re.sub('south ','s',adress)\n",
        "    adress=re.sub(' +',' ',adress)\n",
        "    #special case of the abcx avenues\n",
        "    adress=re.sub('(?P<num>(a|b|c|x)) ave','ave \\g<1>',adress) \n",
        "    #Pb with streets that contain pk or st in their name (ex : pk avenue)\n",
        "    adress=re.sub('.*pk ave','pk ave',adress)\n",
        "    adress=re.sub('.*pk pl','pk pl',adress)\n",
        "    adress=re.sub('.*st marks','st marks',adress)\n",
        "    adress=re.sub('.*(?P<num>ave (x|a|b))','\\g<1>',adress)\n",
        "    #famous places without their specification (avenue,street,park...)\n",
        "    #We choose the most common\n",
        "    adress=re.sub('ave of the [a-z]+','6 ave',adress)\n",
        "    adress=re.sub('^([0-9]+ )?madison$','madison ave',adress)\n",
        "    adress=re.sub('^([0-9]+ )?thompson$','thompson st',adress)\n",
        "    adress=re.sub('^([0-9]+ )?columbus$','columbus ave',adress)\n",
        "    adress=re.sub('^([0-9]+ )?sullivan$','sullivan st',adress)\n",
        "    adress=re.sub('^([0-9]+ )?john$','john st',adress)\n",
        "    adress=re.sub('^([0-9]+ )?putnam$','putnam ave',adress)\n",
        "    adress=re.sub('^([0-9]+ )?union$','union sq',adress)\n",
        "    adress=re.sub('^([0-9]+ )?st marks$','st marks pl',adress)\n",
        "    adress=re.sub('^([0-9]+ )?saint nicholas$','saint nicholas ave',adress)\n",
        "    #streets without the \"st\"\n",
        "    adress=re.sub('(?P<num>(e|w) [0-9]+)$','\\g<1> st',adress)\n",
        "    adress=re.sub('^(?P<num>[0-9][0-9])$','\\g<1> st',adress)\n",
        "    #The \"and\" without specifications\n",
        "    adress=re.sub('(?P<num>[0-9]+) (and|&) (?P<n>([a-z]+|[0-9]+))','\\g<1> st and \\g<2> ave',adress)    \n",
        "    adress=adress.strip()\n",
        "    return adress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0d894ee-3a43-5744-4374-c3db6bd17b06"
      },
      "outputs": [],
      "source": [
        "#We apply this cleaning to the address column\n",
        "disadr=disadr.apply(cleanAdr)\n",
        "\n",
        "#The addresses of the training set, for the dictionnaries\n",
        "disadrTr=disadr[trainSel].reset_index(drop=True)\n",
        "\n",
        "#What does the cleaned feature look like ?\n",
        "disadr.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "07d4b5ff-5815-166b-bc47-a25d077b59e3"
      },
      "source": [
        "Now that we have the cleaned address we can build the dictionnary :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff582277-3b1d-26a4-131d-3a46e0091d1d"
      },
      "outputs": [],
      "source": [
        "def createDico(regex,serieofad,withdrawuniques):\n",
        "    sel=serieofad.apply(lambda x:True if re.search(regex,x) else False)\n",
        "    d=serieofad[sel]\n",
        "    d=d.apply(lambda x:re.search(regex,x).group(0))\n",
        "    dico=d.value_counts()\n",
        "    if withdrawuniques:\n",
        "        dico=dico[dico>2]\n",
        "    dico=list(dico.index)\n",
        "    return dico\n",
        "\n",
        "#dictionnary of the streets\n",
        "regex=re.compile('([0-9]+|[a-z]+) st')\n",
        "dicost=createDico(regex,disadr,True)\n",
        "dicostTr=createDico(regex,disadrTr,False)\n",
        "\n",
        "#dictionnary of the avenues\n",
        "regex=re.compile('(([0-9]+|[a-z]+) ave|ave (a|b|c|x))')\n",
        "dicoave=createDico(regex,disadr,True)\n",
        "dicoaveTr=createDico(regex,disadrTr,False)\n",
        "\n",
        "#dictionnary of the other stuffs\n",
        "regex=re.compile('([0-9]+|[a-z]+)( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)')\n",
        "dicorest=createDico(regex,disadr,True)\n",
        "dicorestTr=createDico(regex,disadrTr,False)\n",
        "\n",
        "\n",
        "\n",
        "#is the word only in the testing set?\n",
        "stsel1=list(map(lambda x:1 if x in dicostTr else 0,dicost))\n",
        "avesel1=list(map(lambda x:1 if x in dicoaveTr else 0,dicoave))\n",
        "restsel1=list(map(lambda x:1 if x in dicorestTr else 0,dicorest))\n",
        "\n",
        "\n",
        "#We select the elements outside the intersections\n",
        "dicost2=list(map(lambda x:re.sub(' st','',x),dicost))\n",
        "dicoave2=list(map(lambda x:re.sub(' ave','',x),dicoave))\n",
        "dicorest2=list(map(lambda x:re.sub('( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)','',x),dicorest))\n",
        "stsel2=list(map(lambda x:0 if x in dicoave2+dicorest2 else 1,dicost2))\n",
        "avesel2=list(map(lambda x:0 if x in dicost2+dicorest2 else 1,dicoave2))\n",
        "restsel2=list(map(lambda x:0 if x in dicoave2+dicost2 else 1,dicorest2))\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(dicost)):\n",
        "    if stsel2[i]==1:\n",
        "        dicost[i]=re.sub(' st','',dicost[i])\n",
        "for i in range(len(dicoave)):\n",
        "    if avesel2[i]==1:\n",
        "        dicoave[i]=re.sub(' ave','',dicoave[i])\n",
        "for i in range(len(dicorest)):\n",
        "    if restsel2[i]==1:\n",
        "        dicorest[i]=re.sub('( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)','',dicorest[i])\n",
        "\n",
        "\n",
        "\n",
        "#Now we keep in the dictionnaries only the words present in the training set\n",
        "dicost=[obj for ind,obj in enumerate(dicost) if stsel1[ind]==1]\n",
        "dicoave=[obj for ind,obj in enumerate(dicoave) if avesel1[ind]==1]\n",
        "dicorest=[obj for ind,obj in enumerate(dicorest) if restsel1[ind]==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "636909a6-5169-5c9c-ce2a-0d9203de7974"
      },
      "source": [
        "We can add special elements to the dictionnary :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4827ccee-8553-31b6-7456-ef3bd2fa4718"
      },
      "outputs": [],
      "source": [
        "#Additional dictionnaries - you can add as many words as you want\n",
        "dicosp=[\"central pk\",\"flatiron\",\"broadway\"]\n",
        "dicodis=[\"financial district\",\"hells kitchen\",\"midtown\",\"upper\",\"soho\",\"murray hill\",\"village\",\n",
        "         \"chelsea\",\"tribeca\",\"harlem\",\"lower\",\"astoria\",\"kips bay\",\"turtle bay\",\"williamsburg\"]\n",
        "diconewfeatures=[\"st\",\"ave\",\"blvd\",\"pkwy\",\"rd\",\"terrace\",\"e\",\"w\"] \n",
        "\n",
        "#The final dictionnary !! -> dico\n",
        "dico=dicost+dicoave+dicorest+dicosp+dicodis+diconewfeatures\n",
        "dico=list(set(dico))\n",
        "\n",
        "#Control of the dictionnary (we want to withdraw abnormal words)\n",
        "l=pd.Series(list(map(len,dico)))\n",
        "ind=l[l==1].index  #OK\n",
        "del dico[410]\n",
        "\n",
        "#An extract of the dictionnary\n",
        "dico[165:175]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "edb04f09-e575-8f66-41de-26930eff7073"
      },
      "source": [
        "For each address we want a list of all the dictionnary words it contains :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1970c4a0-452d-c9b8-2599-90015e7eaced"
      },
      "outputs": [],
      "source": [
        "#We check the presence of each expression of the dictionnary in each address\n",
        "def searchDico(x,dico):\n",
        "    out=[]\n",
        "    for i in range(len(dico)):\n",
        "        if \" \"+dico[i]+\" \" in x:   #spaces are here to avoid '3 st' being found in '33 st'\n",
        "            out.append(i)\n",
        "    return out\n",
        "\n",
        "\n",
        "disadr=disadr.apply(lambda x:' '+x+' ') \n",
        "streets=disadr.apply(lambda x:searchDico(x,dico)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "942aece1-43a6-1849-880c-fd65cc6a5320"
      },
      "source": [
        "We can finally build the sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a4f834b-a52a-83d1-620f-a5d8f8f952e2"
      },
      "outputs": [],
      "source": [
        "#Direct building of the sparse matrix - much more efficient than building a DF and convert to sparse :)\n",
        "nbof1=sum(streets.apply(len))\n",
        "l=list(streets)\n",
        "columns=[e for liste in l for e in liste]\n",
        "ind=list(streets.apply(len))\n",
        "ind=list(np.cumsum(ind))\n",
        "ind=[0]+ind\n",
        "display_address_sparse=csr_matrix(([1]*nbof1,columns,ind))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b1deb11-49b9-ffa5-46b4-5713d577aa19"
      },
      "outputs": [],
      "source": [
        "#Final check : small cleaning of the dictionnary\n",
        "\n",
        "das=display_address_sparse.toarray()\n",
        "das=pd.DataFrame(das)\n",
        "s=das.apply(sum)\n",
        "badwords=s[s==0].index\n",
        "\n",
        "dico=list(np.delete(np.array(dico),badwords))  \n",
        "display_address_sparse=display_address_sparse[:,np.nonzero(display_address_sparse.sum(axis=0))[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "757bb595-2ed2-c9c1-26f8-74a26514fffc"
      },
      "outputs": [],
      "source": [
        "display_address_sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "62946a80-c49a-00a7-f36e-c9236a70f838"
      },
      "source": [
        "Appendix : a good feature, the presence of the street number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8d4c31e-ebff-fdee-904f-666207ef5624"
      },
      "outputs": [],
      "source": [
        "r=r\"^([0-9]+) \"\n",
        "disadr2=disadr.apply(lambda x:re.sub('[0-9]+ (st|ave|rd|blvd)','',x))\n",
        "sel=disadr2.apply(lambda x:True if re.search(r,x.strip()) else False)\n",
        "street_number=pd.Series(np.zeros(len(data)))\n",
        "street_number[sel]=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7100d72e-663e-68ba-6e0d-051101396fe2"
      },
      "source": [
        "A big part of the dictionary is built automatically, but you can add or remove expressions and see the effects :)\n",
        "Personally, as there are still many columns, I used this matrix to reinforce my stacking only at the second level and it gave me a cool boost !\n",
        "It was a great competition and I hope you enjoyed this kernel, :)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}