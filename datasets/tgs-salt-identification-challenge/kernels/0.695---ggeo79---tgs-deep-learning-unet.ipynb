{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport shutil\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom skimage import transform\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, BatchNormalization, SeparableConv2D, UpSampling2D, Cropping2D, ZeroPadding2D\nfrom keras.layers.core import Dropout\nfrom keras.models import Model, load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import np_utils, Sequence\nfrom keras.optimizers import Adam, SGD, RMSprop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76f429ff18f9b12cae7ceb01f56a4eb0a9fca767"},"cell_type":"code","source":"TRAIN_PATH = '../input/train/'\nTEST_PATH = '../input/test/'\nIMAGE_WIDTH_ORIG = 101\nIMAGE_HEIGHT_ORIG = 101\nIMAGE_WIDTH_TARGET = 128\nIMAGE_HEIGHT_TARGET = 128\nIMG_CHANNELS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03de669ec0141aef6dfc7441f5b3795b1e85dcef"},"cell_type":"code","source":"# Load train/depth ids\ntrain_data = pd.read_csv('../input/train.csv', index_col = 'id', usecols=[0])\ndepths_data = pd.read_csv('../input/depths.csv', index_col = 'id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8db065761e3040379c89f5845b3b43c73775f34"},"cell_type":"code","source":"train_data = train_data.join(depths_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bbbeec6ee9f63c5e4146bc5374d471daf75a386"},"cell_type":"code","source":"# Read images/masks\ntrain_data['images'] = [np.array(image.load_img(TRAIN_PATH + 'images/{}.png'.format(idx), color_mode='grayscale')).astype(np.float32) for idx in train_data.index]\ntrain_data['masks'] = [np.array(image.load_img(TRAIN_PATH + 'masks/{}.png'.format(idx), color_mode='grayscale')).astype(np.float32) for idx in train_data.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd870dc0f5a9058ed363a0b36d014f3647c9a602"},"cell_type":"code","source":"# Rescale pixel values from range [0-255] to range [0-1] (neural networks prefer smaller values)\ntrain_data['images'] /= 255\ntrain_data['masks'] /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29c34d7380631e3152fe5b92f35af286ae08b556"},"cell_type":"code","source":"# Compute salt coverage\n# Every salt pixel has value 1 and every non salt has the value 0.So, we sum in  order to obtain the\n# number of salt pixels and then we divide by the total number of pixels in the image\ntrain_data = train_data.assign(coverage = train_data.masks.map(np.sum) / (IMAGE_WIDTH_ORIG * IMAGE_HEIGHT_ORIG))\n# create salt coverage classes\ndef coverage_to_class(inp):\n    for i in range(0, 11):\n        if 10 * inp <= i:\n            return i\n        \ntrain_data = train_data.assign(coverage_class = train_data.coverage.map(coverage_to_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a6fd9d5e0ac882c7f54840a13fb565f9c86d4e"},"cell_type":"code","source":"# check coverage vs classes\nplt.scatter(train_data.coverage, train_data.coverage_class)\nplt.xlabel('Coverage')\nplt.ylabel('Coverage class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92baf1d73e85b1d617844706c9681caee96d5462"},"cell_type":"code","source":"def data():\n    # Resize and resample\n    train_data_images_sampling = np.array([resize(r, (IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET), mode='constant', preserve_range=True) for r in train_data.images])\n    train_data_images_sampling = train_data_images_sampling.reshape(-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1)\n    train_data_masks_sampling = np.array([resize(r, (IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET), mode='constant', preserve_range=True) for r in train_data.masks])\n    train_data_masks_sampling = train_data_masks_sampling.reshape(-1, IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1)\n    \n    # split data into train and validation sets using the salt coverage as a stratification criterion\n    # in order to have a more homogenous split \n    # note, that X_train, X_val, y_train, y_val will have the target size\n    ids_train, ids_val, X_train, X_val, y_train, y_val , coverage_train, coverage_test, depths_train, depths_test = train_test_split(\n                        train_data.index.values,\n                        train_data_images_sampling, \n                        train_data_masks_sampling, \n                        train_data.coverage.values,\n                        train_data.z.values,\n                        test_size=0.2, stratify=train_data.coverage_class,random_state=1340)\n\n    return ids_train, ids_val, X_train, X_val, y_train, y_val , coverage_train, coverage_test, depths_train, depths_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97d64fe14fee2743f06f2788109c368af3003a3d"},"cell_type":"code","source":"# test data\ntest_data = depths_data.loc[~depths_data.index.isin(train_data.index)]\nX_test = [np.array(image.load_img(TEST_PATH + 'images/{}.png'.format(idx), color_mode='grayscale')).astype(np.float32) / 255 for idx in test_data.index]\nX_test = np.array([resize(r, (IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET), mode='constant', preserve_range=True) for r in X_test]).reshape(-1, IMAGE_HEIGHT_TARGET,IMAGE_WIDTH_TARGET, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e145f80bb77527509ea0bdffaedcf3b1dd9e07a"},"cell_type":"code","source":"def get_crop_shape(target, refer):\n    # width, the 3rd dimension\n    cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n    assert (cw >= 0)\n    if cw % 2 != 0:\n        cw1, cw2 = int(cw/2), int(cw/2) + 1\n    else:\n        cw1, cw2 = int(cw/2), int(cw/2)\n    # height, the 2nd dimension\n    ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n    assert (ch >= 0)\n    if ch % 2 != 0:\n        ch1, ch2 = int(ch/2), int(ch/2) + 1\n    else:\n        ch1, ch2 = int(ch/2), int(ch/2)\n\n    return (ch1, ch2), (cw1, cw2)\n\n# Create model\ndef build_UNet_model(X_train, y_train, X_val, y_val):\n    \n    concat_axis=3\n    \n    input_layer = Input((IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET, 1))      \n    \n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n\n    up_conv5 = UpSampling2D(size=(2, 2))(conv5)\n    ch, cw = get_crop_shape(conv4, up_conv5)\n    crop_conv4 = Cropping2D(cropping=(ch,cw))(conv4)\n    up6 = concatenate([up_conv5, crop_conv4], axis=concat_axis)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up_conv6 = UpSampling2D(size=(2, 2))(conv6)\n    ch, cw = get_crop_shape(conv3, up_conv6)\n    crop_conv3 = Cropping2D(cropping=(ch,cw))(conv3)\n    up7 = concatenate([up_conv6, crop_conv3], axis=concat_axis) \n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up_conv7 = UpSampling2D(size=(2, 2))(conv7)\n    ch, cw = get_crop_shape(conv2, up_conv7)\n    crop_conv2 = Cropping2D(cropping=(ch,cw))(conv2)\n    up8 = concatenate([up_conv7, crop_conv2], axis=concat_axis)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up_conv8 = UpSampling2D(size=(2, 2))(conv8)\n    ch, cw = get_crop_shape(conv1, up_conv8)\n    crop_conv1 = Cropping2D(cropping=(ch,cw))(conv1)\n    up9 = concatenate([up_conv8, crop_conv1], axis=concat_axis)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    ch, cw = get_crop_shape(input_layer, conv9)\n    conv9 = ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\n       \n    output_layer = Conv2D(1, (1, 1), padding='same', activation='sigmoid')(conv9)\n\n    early_stopping = EarlyStopping(patience=10, verbose=1)\n    model_checkpoint = ModelCheckpoint('./tgs_b_32_relu_optim_adam_UNET.model', verbose=1, save_best_only=True)\n    reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.0001, verbose=1)\n    \n    model = Model(input_layer, output_layer)\n    adam = Adam(lr=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    history = model.fit(X_train,\n                        y_train,\n                        validation_data=[X_val, y_val],\n                        batch_size=32,\n                        epochs = 50,\n                        callbacks=[model_checkpoint, early_stopping])\n    \n    return history,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c79749bbb1f296bb4fcc0fa2a6190a253f41ad14"},"cell_type":"code","source":"ids_train, ids_val, X_train, X_val, y_train, y_val , coverage_train, coverage_test, depths_train, depths_test = data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5168636a4048a867e92634fe767f5127062d277"},"cell_type":"code","source":"#flip\nX_train = np.append(X_train, [np.fliplr(f) for f in X_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(f) for f in y_train], axis=0)\n# rotate\nX_train = np.append(X_train, [transform.rotate(r, angle=45, mode='reflect') for r in X_train], axis=0)\ny_train = np.append(y_train, [transform.rotate(r, angle=45, mode='reflect') for r in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6e4b4f1286bf2b7d22fade5ff147d3fbbcf664e"},"cell_type":"code","source":"history, model = build_UNet_model(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97d9ca1197edb267834c9166aa113f1f79f8ae5a"},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(20,4))\nax_loss.plot(history.epoch, history.history['loss'], label='Train loss')\nax_loss.plot(history.epoch, history.history['val_loss'], label='Val loss')\nax_loss.legend()\nax_acc.plot(history.epoch, history.history['acc'], label = 'Train acc')\nax_acc.plot(history.epoch, history.history['val_acc'], label = 'Val acc')\nax_acc.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c16d56d5859483201dbcb82bccd6633979a2c7"},"cell_type":"code","source":"# Load the best saved model\nmodel = load_model('./tgs_b_32_relu_optim_adam_UNET.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f4b799c1375fa2193f57a1ee0afca17543bee71"},"cell_type":"code","source":"# predict on validation and test data\npred_val = model.predict(X_val)\npred_test = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc5281c6a3aee8427777bfa2109c885597dd4bfa"},"cell_type":"code","source":"# convert result to 0 or 1\npred_val_int = (pred_val > 0.5).astype(np.uint8)\npred_test_int = (pred_test > 0.5).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9ab3a919ca18bf38b1f670c6089cb12c2be28a8"},"cell_type":"code","source":"# Squeeze one dimension to be able to plot\nX_train_squeeze = X_train.squeeze()\ny_train_squeeze = y_train.squeeze()\npred_val_squeeze = pred_val.squeeze()\nX_val_squeeze = X_val.squeeze()\ny_val_squeeze = y_val.squeeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2490449ead84afcd93fe5939def5040adfeaa1b3"},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4059796cc367296dc429264a283bbcca14360d1"},"cell_type":"code","source":"max_images = 6\n\nfig, axes = plt.subplots(3, max_images, figsize=(25, 14))\n\nfor i in  range(max_images):   \n    idx = np.random.randint(0, len(X_val))\n   \n    axes[0][i].imshow(X_val_squeeze[idx], cmap='Greys')\n    axes[1][i].imshow(y_val_squeeze[idx], cmap='Greens')\n    axes[2][i].imshow(pred_val_squeeze[idx], cmap='YlOrRd') \n\nplt.suptitle('Top: Validation images - Middle: Validation masks  - Bottom: Predicted images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60e9b7f181e084614bce27c6acb9d4a3a09bf9e6"},"cell_type":"code","source":"# From kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37d3ebe99e2ee1eb62ba37b05885c5757a5024b7"},"cell_type":"code","source":"# metrics\n# use the original masks for the metric \ny_valid_orig = np.array([train_data.loc[idx].masks for idx in ids_val])\n# resize to the original size in order to compare\npred_val = pred_val.reshape(-1,IMAGE_HEIGHT_TARGET, IMAGE_WIDTH_TARGET)\npred_val = np.array([resize(r, (IMAGE_HEIGHT_ORIG, IMAGE_WIDTH_ORIG), mode='constant', preserve_range=True) for r in pred_val])  \nthresholds = np.linspace(0.1, 1, 40)\niou = np.array([iou_metric_batch(y_valid_orig, np.int32(pred_val > threshold)) for threshold in thresholds])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cc6402d0520b1b04cc856b7dbb8560e186999da"},"cell_type":"code","source":"plt.plot(thresholds, iou)\nplt.xlabel('threshold')\nplt.ylabel('IoU')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3a7e489432d8bb403d96bc4ab3f7991884446f3"},"cell_type":"code","source":"best_threshold = thresholds[np.argmax(iou)]\nbest_threshold, max(iou)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2ffbc7bfde50b9a81a973cfdc9b94131b02e20d"},"cell_type":"code","source":"# From https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe21047509b70b181617a35060bd983d4143b93"},"cell_type":"code","source":"tmp = [resize((pred_test[i] > best_threshold), (IMAGE_HEIGHT_ORIG, IMAGE_WIDTH_ORIG), mode='constant', preserve_range=True) for i in range(len(test_data.index.values))]\npred_dict = {idx: RLenc(np.round(tmp[i])) for i, idx in enumerate(test_data.index.values)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e19b0d921925436d6856c89521cd5b1cf860f22f"},"cell_type":"code","source":"submissions = pd.DataFrame.from_dict(pred_dict, orient='index')\nsubmissions.index.names = ['id']\nsubmissions.columns = ['rle_mask']\nsubmissions.to_csv('./submission_tgs_b_32_relu_optim_adam_UNET.model.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}