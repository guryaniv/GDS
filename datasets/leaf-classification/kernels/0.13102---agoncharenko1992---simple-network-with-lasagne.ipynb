{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4784ef21-6cc1-9ddb-b566-ee4bb882a8b4"
      },
      "source": [
        "Simple network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "efb9a051-24d0-e355-f466-15764c7391f9"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import lasagne\n",
        "import time\n",
        "import pickle\n",
        "from lasagne.layers import DenseLayer, NonlinearityLayer, DropoutLayer\n",
        "from lasagne.nonlinearities import softmax\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cbdd97c-313b-9b86-f59c-321e03612e2d"
      },
      "outputs": [],
      "source": [
        "def train_data_load():\n",
        "    Xy_train_df = pd.read_csv(\"../input/train.csv\")\n",
        "    Xy_train_df['species'] = Xy_train_df.species.astype('category')\n",
        "    cat_names = list(Xy_train_df.species.cat.categories)\n",
        "    cat_code = range(len(Xy_train_df.species.cat.categories))\n",
        "    species_dict = dict(zip(cat_code, cat_names))\n",
        "    Xy_train_df['species'] = Xy_train_df.species.cat.codes\n",
        "    y_train_df = Xy_train_df.species\n",
        "    Xy_train_df.drop(labels='species', axis=1, inplace=True)\n",
        "    X_train_df = Xy_train_df\n",
        "    del Xy_train_df\n",
        "    X = X_train_df.drop(labels='id', axis=1, inplace=False).values\n",
        "    y = y_train_df.values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X, y)\n",
        "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler, species_dict \n",
        "def gen_minibatches(X, y, batch_size, shuffle=False):\n",
        "    ex_count = X.shape[0]\n",
        "    assert ex_count==y.shape[0], \"Training data sizes don't match\"\n",
        "    if shuffle:\n",
        "        ids = np.random.permutation(ex_count)\n",
        "    else:\n",
        "        ids = np.arange(ex_count)\n",
        "    for start_idx in range(0, ex_count - batch_size + 1, batch_size):\n",
        "        ii = ids[start_idx:start_idx + batch_size]\n",
        "        yield X[ii], y[ii]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3465c1f-aeb6-bfc5-6e81-caf0037059f6"
      },
      "outputs": [],
      "source": [
        "input_var = T.matrix('inputs', dtype=theano.config.floatX)\n",
        "def model_construction(input_var, num_units_list, features_count, classes_count):\n",
        "    network = lasagne.layers.InputLayer(shape=(None, features_count), input_var=input_var)\n",
        "    for idx, curr_layer_num_units in enumerate(num_units_list):\n",
        "        network = DenseLayer(network, num_units=curr_layer_num_units, \n",
        "                   W=lasagne.init.HeNormal(gain='relu'), b=lasagne.init.Constant(0.), \n",
        "                   nonlinearity=lasagne.nonlinearities.rectify, name='layer' + str(idx))\n",
        "        network = DropoutLayer(network, p=0.6, name='dropout' + str(idx))\n",
        "    network = DenseLayer(network, num_units=classes_count, nonlinearity=None, name='last_layer')\n",
        "    network = NonlinearityLayer(network, softmax, name='probs')    \n",
        "    return network    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a0b2e65-3bfb-573d-b57c-4cca18457579"
      },
      "outputs": [],
      "source": [
        "save_path = 'super_model'\n",
        "def train(X_train, y_train, X_val, y_val, X_test, y_test, network, num_epochs=500, \n",
        "          learning_rate=0.001, learning_rate_decay=0.95, \n",
        "          momentum=0.9, momentum_decay=0.95, \n",
        "          decay_after_epochs=10, regu=0.002, batch_size=64, updates='adam'):\n",
        "    \n",
        "    target_var = T.ivector('target')\n",
        "    prediction = lasagne.layers.get_output(network)\n",
        "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
        "    cross_entr_loss = loss.mean()\n",
        "    regu_loss = regu * lasagne.regularization.regularize_network_params(\n",
        "        network, lasagne.regularization.l2)\n",
        "    loss = cross_entr_loss + regu_loss\n",
        "    print(updates)\n",
        "    print(\"initial learning_rate=%f, \\\n",
        "decay_value %f per %d epoch, L2_reg coeff value=%f, batch_size=%d\" \n",
        "          % (learning_rate, learning_rate_decay, decay_after_epochs, regu, batch_size))\n",
        "    train_acc = T.mean(T.eq(T.argmax(prediction, axis=1), target_var), dtype=theano.config.floatX)\n",
        "    learning_rate_var = theano.shared(np.float32(learning_rate))\n",
        "    momentum_var = theano.shared(np.float32(momentum))\n",
        "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
        "    if updates=='nesterov':\n",
        "        updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=learning_rate_var,\n",
        "                                                    momentum=momentum_var)\n",
        "    else:\n",
        "        updates = lasagne.updates.adam(loss, params)\n",
        "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
        "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
        "                                                            target_var)\n",
        "    test_loss = test_loss.mean()\n",
        "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
        "                      dtype=theano.config.floatX)\n",
        "    train_fn = theano.function([input_var, target_var], [cross_entr_loss, regu_loss], updates=updates)\n",
        "    train_acc_fn = theano.function([input_var, target_var], train_acc)\n",
        "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
        "    print(\"Training...\")\n",
        "    best_val_acc = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    test_acc_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_err = train_batches = cross_loss = weights_sums = 0\n",
        "        start_time = time.time()\n",
        "        for X_batch, y_batch in gen_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
        "            cross_err, weights = train_fn(X_batch, y_batch)\n",
        "            train_err += (cross_err + weights)\n",
        "            cross_loss += cross_err\n",
        "            weights_sums += weights\n",
        "            train_batches += 1\n",
        "            loss_history.append(cross_err + weights)\n",
        "        # training accuracy\n",
        "        n_acc = len(y_val)\n",
        "        trval_err = trval_acc = trval_batches = 0\n",
        "        for X_batch, y_batch in gen_minibatches(X_train[:n_acc], y_train[:n_acc], \n",
        "                                                batch_size, shuffle=False):\n",
        "            err, acc = val_fn(X_batch, y_batch)\n",
        "            trval_err += err\n",
        "            trval_acc += acc\n",
        "            trval_batches += 1\n",
        "        trval_acc /= trval_batches\n",
        "        train_acc_history.append(trval_acc)\n",
        "        # validation accuracy\n",
        "        val_err = val_acc = val_batches = 0\n",
        "        for X_batch, y_batch in gen_minibatches(X_val, y_val, batch_size//2, shuffle=False):\n",
        "            err, acc = val_fn(X_batch, y_batch)\n",
        "            val_err += err\n",
        "            val_acc += acc\n",
        "            val_batches += 1\n",
        "        val_acc /= val_batches\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        test_err = test_acc = test_batches = 0\n",
        "        for X_batch, y_batch in gen_minibatches(X_test, y_test, batch_size//2, shuffle=False):\n",
        "            err, acc = val_fn(X_batch, y_batch)\n",
        "            test_err += err\n",
        "            test_acc += acc\n",
        "            test_batches += 1\n",
        "        test_acc /= test_batches\n",
        "        test_acc_history.append(test_acc)\n",
        "        \n",
        "        # keep track of the best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            # make a copy of the model\n",
        "            best_val_acc = val_acc\n",
        "            best_model = lasagne.layers.get_all_param_values(network)\n",
        "        if epoch % 50 == 0:\n",
        "            print('epoch %d / %d in %.1fs: loss %f, cross_loss %f, weights_loss %f, train: %.3f, val %.3f, test %.3f, lr %e mom %e'\n",
        "                  % (epoch + 1, num_epochs, time.time() - start_time,\n",
        "                     train_err / train_batches, cross_loss / train_batches,\n",
        "                     weights_sums / train_batches, trval_acc, val_acc, test_acc, \n",
        "                     learning_rate_var.get_value(), momentum_var.get_value()))\n",
        "        # decay learning rate\n",
        "        if (epoch + 1) % decay_after_epochs == 0:\n",
        "            learning_rate_var.set_value(\n",
        "                np.float32(learning_rate_var.get_value() * learning_rate_decay))\n",
        "            momentum = (1.0 - (1.0 - momentum_var.get_value()) * momentum_decay) \\\n",
        "                       .clip(max=0.9999)\n",
        "            momentum_var.set_value(np.float32(momentum))\n",
        "        # save model snapshots\n",
        "        if save_path and (epoch + 1) % 10 == 0:\n",
        "            model = lasagne.layers.get_all_param_values(network)\n",
        "            path = '%s.pickle' % (save_path)\n",
        "            with open(path, 'wb') as f:\n",
        "                pickle.dump({'model': model}, f, -1)\n",
        "    return network          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf51a774-a8ee-f886-d5f1-a7718d436950"
      },
      "outputs": [],
      "source": [
        "def predict_proba(network, Xtest_vals, save_path):\n",
        "    if save_path is not None:\n",
        "        path = '%s.pickle' % (save_path)\n",
        "        with open(path, 'rb') as f:\n",
        "            data_new = pickle.load(f)\n",
        "            print(len(data_new['model']))\n",
        "            lasagne.layers.set_all_param_values(network, data_new['model'])\n",
        "    proba_tensor = lasagne.layers.get_output(network, Xtest_vals, deterministic=True)\n",
        "    proba_vals = proba_tensor.eval()\n",
        "    return proba_vals\n",
        "\n",
        "def main():\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, scaler, species_dict = train_data_load()\n",
        "    network = model_construction(input_var=input_var, num_units_list=[128, 64], \n",
        "                                 features_count=X_train.shape[1], classes_count=99)\n",
        "    network = train(X_train, y_train, X_val, y_val, X_test, y_test, network, num_epochs=700, regu=0.002,\n",
        "                   batch_size=32)\n",
        "    X_test = pd.read_csv(\"../input/test.csv\")\n",
        "    Xtest_vals = X_test.drop(labels='id', axis=1, inplace=False).values\n",
        "    Xtest_vals = scaler.transform(Xtest_vals)\n",
        "    print(Xtest_vals.shape)\n",
        "    probs = predict_proba(network, Xtest_vals, save_path)\n",
        "    print(probs)\n",
        "    cols = list(species_dict.values())\n",
        "    predicted = pd.DataFrame(probs, columns=cols)\n",
        "    predicted = pd.concat([X_test.id, predicted], axis=1)\n",
        "    predicted.to_csv(\"sample_submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b96455b4-ec00-c67a-0e4f-5f4e8e2ec7f9"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "188a65a3-86c7-75e9-85a0-982c4984f877"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}