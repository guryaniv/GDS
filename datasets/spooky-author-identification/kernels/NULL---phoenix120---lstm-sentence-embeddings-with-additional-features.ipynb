{"metadata": {"language_info": {"version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "ea8a94f4-0d60-4fff-960c-ea45a9aa5495", "_uuid": "fbd285f3ca449c813592b2df2c41f27c0e5295db"}, "source": ["Model: Bidrectional LSTM over 2 types of word embeddings (from chars using RNN, and from one-hot [as in the paper](https://arxiv.org/abs/1604.05529)). Additional featrues from [\"Simple Feature Engg Notebook - Spooky Author\"](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) and sentence embedding from LSTM output are used as input for Dense layer. \n", "\n", "LB: around 0.38"]}, {"cell_type": "code", "metadata": {"_cell_guid": "ab3f0d14-8342-4d6a-aa08-0e4887d87740", "scrolled": true, "_uuid": "2d6e105e3d31be0d71e9ba09d41c35be6194ec5e"}, "execution_count": null, "source": ["import csv\n", "import re\n", "import os\n", "import pickle\n", "import copy\n", "import string\n", "from collections import Counter\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import nltk\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import log_loss\n", "\n", "from keras.models import Model, load_model\n", "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n", "from keras.preprocessing import sequence\n", "from keras.layers import LSTM, Bidirectional, Dropout, Dense, Input, Embedding, BatchNormalization, TimeDistributed\n", "from keras.layers.merge import concatenate\n", "\n", "RANDOM_SEED = 43\n", "np.random.seed(RANDOM_SEED)\n", "\n", "# nltk.download('punkt')\n", "# nltk.download('stopwords')"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "4163875e-49cf-44ae-ae9e-1b1d01a3279f", "scrolled": true, "_uuid": "3fe90819d67e477197f48b279b9ab21b08d69f4c"}, "execution_count": null, "source": ["# Reading dataset\n", "DATA_TRAIN = \"../input/train.csv\"\n", "DATA_TEST = \"../input/test.csv\"\n", "\n", "train = pd.read_csv(DATA_TRAIN, delimiter=',', quotechar='\"')\n", "test = pd.read_csv(DATA_TEST, delimiter=',', quotechar='\"')\n", "\n", "author_to_index = {\n", "    \"EAP\": 0, \n", "    \"HPL\": 1, \n", "    \"MWS\": 2\n", "}\n", "train[\"author\"] = train[\"author\"].map(author_to_index)\n", "\n", "print(\"Train samples: {}\".format(train.shape[0]))\n", "print(train.head())\n", "print()\n", "print(\"Test samples: {}\".format(test.shape[0]))\n", "print(test.head())"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "69c0f743-9b81-4cff-9117-b1c708fb8cf4", "_uuid": "e23285c17af74534121035cca87f1cdb9810d000"}, "execution_count": null, "source": ["# Tokenizer - nltk.word_tokenize (punkt module of NLTK)\n", "tokenize = nltk.word_tokenize\n", "print(train[\"text\"][0])\n", "print(tokenize(train[\"text\"][0]))"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "c34c7098-0ce8-4cc2-b5e8-9a8c3f9061e3", "_uuid": "1891c08363fadade8360571294618d491f169417", "collapsed": true}, "execution_count": null, "source": ["class Vocabulary(object):\n", "    def __init__(self, dump_filename):\n", "        self.dump_filename = dump_filename\n", "        self.word_to_index = {}\n", "        self.index_to_word = []\n", "        self.counter = Counter()\n", "        self.reset()\n", "\n", "        if os.path.isfile(self.dump_filename):\n", "            self.load()\n", "\n", "    def save(self):\n", "        with open(self.dump_filename, \"wb\") as f:\n", "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n", "\n", "    def load(self):\n", "        with open(self.dump_filename, \"rb\") as f:\n", "            vocab = pickle.load(f)\n", "            self.__dict__.update(vocab.__dict__)\n", "\n", "    def add_word(self, word):\n", "        self.counter[word] += 1\n", "        if self.word_to_index.get(word) is None:\n", "            self.index_to_word.append(word)\n", "            index = len(self.index_to_word) -1\n", "            self.word_to_index[word] = index\n", "            return index\n", "        return self.word_to_index[word]\n", "\n", "    def get_word_index(self, word):\n", "        if self.word_to_index.get(word) is not None:\n", "            return self.word_to_index[word]\n", "        return len(self.word_to_index)\n", "\n", "    def get_word(self, index):\n", "        return self.index_to_word[index]\n", "\n", "    def size(self):\n", "        return len(self.index_to_word)\n", "    \n", "    def reset(self):\n", "        self.word_to_index = {}\n", "        self.index_to_word = []\n", "        self.counter = Counter()\n", "        self.word_to_index[\"NotAWord\"] = 0\n", "        self.index_to_word.append(\"NotAWord\")\n", "        self.counter[\"NotAWord\"] = 1\n", "    \n", "    def shrink(self, num):\n", "        pairs = self.counter.most_common(num)\n", "        self.reset()\n", "        for word, count in pairs:\n", "            self.add_word(word)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "a8bdfed6-5b1a-414d-ac2f-d396c5ab6352", "_uuid": "ca58e4317fb8f1cf8570ea856c11d8b37f689554", "collapsed": true}, "execution_count": null, "source": ["def bow(train_texts, test_texts, tokenizer=nltk.word_tokenize, preprocessor=None,\n", "        use_tfidf=False, max_features=None, bow_ngrams=(1, 1), analyzer='word'):\n", "    train = copy.deepcopy(train_texts)\n", "    test = copy.deepcopy(test_texts)\n", "\n", "    if use_tfidf:\n", "        vectorizer = TfidfVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n", "                                     preprocessor=preprocessor, max_features=max_features)\n", "    else:\n", "        vectorizer = CountVectorizer(analyzer=analyzer, ngram_range=bow_ngrams, tokenizer=tokenizer,\n", "                                     preprocessor=preprocessor, max_features=max_features)\n", "    data = train + test\n", "    data = vectorizer.fit_transform(data)\n", "    train_data = data[:len(train)]\n", "    test_data = data[len(train):]\n", "    return train_data, test_data\n", "\n", "def run_bow_nb(train_sentences, train_answers, test_sentences):\n", "    train_data, test_data = bow(train_sentences, test_sentences)\n", "    nb = MultinomialNB()\n", "    clf = GridSearchCV(estimator=nb, \n", "                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n", "                       scoring=\"neg_log_loss\", cv=5)\n", "    clf.fit(train_data, train_answers)\n", "    print(\"CV: {}\".format(clf.best_score_))\n", "    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n", "\n", "def run_boc_nb(train_sentences, train_answers, test_sentences):\n", "    train_data, test_data = bow(train_sentences, test_sentences, tokenizer=None, use_tfidf=True, analyzer='char')\n", "    nb = MultinomialNB()\n", "    clf = GridSearchCV(estimator=nb, \n", "                       param_grid={\"alpha\": [0.1, 0.3, 0.6, 0.9, 1.0]}, \n", "                       scoring=\"neg_log_loss\", cv=5)\n", "    clf.fit(train_data, train_answers)\n", "    print(\"CV: {}\".format(clf.best_score_))\n", "    return  clf.predict_proba(train_data), clf.predict_proba(test_data)\n", "\n", "def collect_additional_features(train, test):\n", "    train_df = train.copy()\n", "    test_df = test.copy()\n", "    eng_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n", "    \n", "    train_df[\"words\"] =  train_df[\"text\"].apply(lambda text: text.split())\n", "    test_df[\"words\"] = test_df[\"text\"].apply(lambda text: text.split())\n", "    \n", "    train_df[\"num_words\"] = train_df[\"words\"].apply(lambda words: len(words))\n", "    test_df[\"num_words\"] = test_df[\"words\"].apply(lambda words: len(words))\n", "    \n", "    train_df[\"num_unique_words\"] = train_df[\"words\"].apply(lambda words: len(set(words)))\n", "    test_df[\"num_unique_words\"] = test_df[\"words\"].apply(lambda words: len(set(words)))\n", "    \n", "    train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda text: len(text))\n", "    test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda text: len(text))\n", "    \n", "    train_df[\"num_stopwords\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w in eng_stopwords]))\n", "    test_df[\"num_stopwords\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w in eng_stopwords]))\n", "    \n", "    train_df[\"num_punctuations\"] = train_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n", "    test_df[\"num_punctuations\"] =test_df['text'].apply(lambda text: len([c for c in text if c in string.punctuation]))\n", "    \n", "    train_df[\"num_words_upper\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n", "    test_df[\"num_words_upper\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.isupper()]))\n", "    \n", "    train_df[\"num_words_title\"] = train_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n", "    test_df[\"num_words_title\"] = test_df[\"words\"].apply(lambda words: len([w for w in words if w.istitle()]))\n", "    \n", "    train_df[\"mean_word_len\"] = train_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n", "    test_df[\"mean_word_len\"] = test_df[\"words\"].apply(lambda words: np.mean([len(w) for w in words]))\n", "    \n", "    pred_train, pred_test = run_bow_nb(train_df[\"text\"].tolist(), train_df[\"author\"].tolist(), test_df[\"text\"].tolist())\n", "    train_df[\"nb_count_eap\"] = pred_train[:,0]\n", "    train_df[\"nb_count_hpl\"] = pred_train[:,1]\n", "    train_df[\"nb_count_mws\"] = pred_train[:,2]\n", "    test_df[\"nb_count_eap\"] = pred_test[:,0]\n", "    test_df[\"nb_count_hpl\"] = pred_test[:,1]\n", "    test_df[\"nb_count_mws\"] = pred_test[:,2]\n", "    \n", "    pred_train, pred_test = run_boc_nb(train_df[\"text\"].tolist(), train_df[\"author\"].tolist(), test_df[\"text\"].tolist())\n", "    train_df[\"nb_count_chars_eap\"] = pred_train[:,0]\n", "    train_df[\"nb_count_chars_hpl\"] = pred_train[:,1]\n", "    train_df[\"nb_count_chars_mws\"] = pred_train[:,2]\n", "    test_df[\"nb_count_chars_eap\"] = pred_test[:,0]\n", "    test_df[\"nb_count_chars_hpl\"] = pred_test[:,1]\n", "    test_df[\"nb_count_chars_mws\"] = pred_test[:,2]\n", "    \n", "    train_df.drop([\"text\", \"id\", \"words\"], axis=1, inplace=True)\n", "    test_df.drop([\"text\", \"id\", \"words\"], axis=1, inplace=True)\n", "    if \"author\" in train_df.columns:\n", "        train_df.drop([\"author\"], axis=1, inplace=True)\n", "    if \"author\" in test_df.columns:\n", "        test_df.drop([\"author\"], axis=1, inplace=True)\n", "    \n", "    scaler = MinMaxScaler()\n", "    train_df = scaler.fit_transform(train_df)\n", "    test_df = scaler.transform(test_df)\n", "    return train_df, test_df"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "8c97a4cf-60a4-4e88-8c8d-7124cac8ecb8", "_uuid": "28f3fe21c76a688fbcd828ad193d1ad2decbd425"}, "execution_count": null, "source": ["VOCAB_PATH = \"vocab.pickle\"\n", "\n", "def prepare_vocabulary(vocab_path, train, test, shrink_border=None):\n", "    vocabulary = Vocabulary(vocab_path)\n", "    if vocabulary.size() <= 1:\n", "        for sentence in train['text'].tolist():\n", "            for word in tokenize(sentence):\n", "                vocabulary.add_word(word)\n", "        print(\"Train vocabulary size: {}\".format(vocabulary.size()))\n", "        for sentence in test['text'].tolist():\n", "            for word in tokenize(sentence):\n", "                vocabulary.add_word(word) \n", "        print(\"Train+test vocabulary size: {}\".format(vocabulary.size()))\n", "        vocabulary.save()\n", "\n", "    print(\"Vocabulary size: {}\".format(vocabulary.size()))\n", "    if shrink_border is not None:\n", "        vocabulary.shrink(shrink_border)\n", "        print(\"Vocabulary size after shrink: {}\".format(vocabulary.size()))\n", "    return vocabulary\n", "\n", "vocabulary = prepare_vocabulary(VOCAB_PATH, train, test)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "df033a1b-f903-4be3-ad17-4b8207dcf440", "_uuid": "7fb13ac4887dddc73ee37960add081aea76d914f"}, "execution_count": null, "source": ["CHAR_SET = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-'\\\"\"\n", "\n", "def get_samples(sentences, vocabulary, word_max_count, max_word_len):\n", "    n = len(sentences)\n", "    word_matrix = np.zeros((n, word_max_count), dtype='int')\n", "    char_matrix = np.zeros((n, word_max_count, max_word_len), dtype=np.int)\n", "    for i, sentence in enumerate(sentences):\n", "        words = tokenize(sentence)[:word_max_count]\n", "        word_matrix[i, -len(words):] = [vocabulary.get_word_index(word) for word in words]\n", "        char_vectors = []\n", "        for word in words:\n", "            char_indices = np.zeros(max_word_len)\n", "            word_char_indices = [CHAR_SET.index(ch) if ch in CHAR_SET else len(CHAR_SET) for ch in word]\n", "            char_indices[-min(len(word), max_word_len):] = word_char_indices[:max_word_len]\n", "            char_vectors.append(char_indices)\n", "        char_matrix[i, -len(words):] = char_vectors\n", "    return word_matrix, char_matrix\n", "\n", "def get_train_val_test_sets(x, y, x_test, vocabulary, word_max_count=80, max_word_len=30, val_part=0.1):\n", "    word_matrix, char_matrix = get_samples(x[\"text\"].tolist(), vocabulary, word_max_count, max_word_len)\n", "\n", "    n = x.shape[0]\n", "    np.random.seed(RANDOM_SEED)\n", "    perm = np.random.permutation(n)\n", "    idx_train = perm[:int(n*(1-val_part))]\n", "    idx_val = perm[int(n*(1-val_part)):]\n", "\n", "    additional_features_matrix_train, additional_features_matrix_val = \\\n", "        collect_additional_features(x.iloc[idx_train], x.iloc[idx_val])\n", "\n", "    word_matrix_train = word_matrix[idx_train]\n", "    char_matrix_train = char_matrix[idx_train]\n", "    y_train = np.array(y, dtype='int32')[idx_train]\n", "\n", "    word_matrix_val = word_matrix[idx_val]\n", "    char_matrix_val = char_matrix[idx_val]\n", "    y_val = np.array(y, dtype='int32')[idx_val]\n", "\n", "    word_matrix_test, char_matrix_test = get_samples(x_test[\"text\"].tolist(), \n", "                                                     vocabulary, word_max_count, max_word_len)\n", "    _, additional_features_matrix_test = collect_additional_features(x.iloc[idx_train], x_test)\n", "\n", "    return (word_matrix_train, char_matrix_train, additional_features_matrix_train, y_train), \\\n", "        (word_matrix_val, char_matrix_val, additional_features_matrix_val, y_val), \\\n", "        (word_matrix_test, char_matrix_test, additional_features_matrix_test)\n", "        \n", "data_train, data_val, data_test = get_train_val_test_sets(train, train['author'].tolist(), test, vocabulary)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "df01fc1d-df71-4c84-9539-224e7eb1ec79", "_uuid": "562e6bdb8a73a01f74941953ee9c97f69dbb4898", "collapsed": true}, "execution_count": null, "source": ["class SpookyRNN:\n", "    def __init__(self, rnn_units=64, dense_units=32, dropout=0.4, batch_size=256, \n", "                 embeddings_dimensions=150, char_embeddings_dimension=20, max_word_len=30,\n", "                 char_lstm_output_dim=64):\n", "        self.batch_size = batch_size\n", "        self.dropout = dropout\n", "        self.rnn_units = rnn_units\n", "        self.dense_units = dense_units\n", "        self.embeddings_dimensions = embeddings_dimensions\n", "        self.char_embeddings_dimension = char_embeddings_dimension\n", "        self.char_lstm_output_dim =char_lstm_output_dim\n", "        self.max_word_len = max_word_len\n", "\n", "        self.model = None\n", "\n", "    def build(self, n_additional_features, vocabulary_size):\n", "        word_index_input = Input(shape=(None,), dtype=\"int32\", name=\"word_index_input\")\n", "        word_embeddings = Embedding(vocabulary_size + 1, \n", "                                    self.embeddings_dimensions, name=\"word_embeddings\")(word_index_input)\n", "        \n", "        char_input = Input(shape=(None, self.max_word_len), dtype=\"int32\", name=\"char_input\")\n", "        char_embeddings = Embedding(len(CHAR_SET) + 1, \n", "                                    self.char_embeddings_dimension, name='char_embeddings')(char_input)\n", "        word_from_char_embeddings = TimeDistributed(Bidirectional(\n", "            LSTM(self.char_lstm_output_dim // 2, dropout=self.dropout, \n", "                 recurrent_dropout=self.dropout, name='CharLSTM')))(char_embeddings)\n", "        \n", "        additional_features_input = Input(shape=(n_additional_features, ), dtype='float32', name='add_input')\n", "        \n", "        lstm_input = concatenate([word_embeddings, word_from_char_embeddings], name=\"lstm_input\")\n", "        lstm_layer = Bidirectional(LSTM(self.rnn_units // 2, dropout=self.dropout, \n", "                                        recurrent_dropout=self.dropout))(lstm_input)\n", "        \n", "        layer = concatenate([lstm_layer, additional_features_input], name=\"dense_input\")\n", "        dense = Dense(self.dense_units, activation='relu')(layer)\n", "        dense = Dropout(self.dropout)(dense)\n", "        \n", "        predictions = Dense(3, activation='softmax')(dense)\n", "        model = Model(inputs=[word_index_input, char_input, additional_features_input], outputs=predictions)\n", "        \n", "        model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n", "                                                                                          \n", "        print(model.summary())\n", "        self.model = model\n", "\n", "    def train(self, data_train, data_val, model_filename, enable_checkpoints=True):\n", "        word_matrix_train, char_matrix_train, additional_features_matrix_train, y_train = data_train\n", "        word_matrix_val, char_matrix_val, additional_features_matrix_val, y_val = data_val\n", "        \n", "        print(\"Train example:\")\n", "        print(word_matrix_train[0])\n", "        print(char_matrix_train[0])\n", "        print(additional_features_matrix_train[0])\n", "        print(y_train[0])\n", "        \n", "        # Callback to prevent overfitting.\n", "        callbacks = [EarlyStopping(monitor='val_loss', patience=0)]\n", "\n", "        # Callback to save best only model.\n", "        if enable_checkpoints:\n", "            callbacks.append(ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True))\n", "\n", "        self.model.fit([word_matrix_train, char_matrix_train, additional_features_matrix_train], y_train, \n", "                       validation_data=([word_matrix_val, char_matrix_val, additional_features_matrix_val], y_val),\n", "                       epochs=50,\n", "                       batch_size=self.batch_size,\n", "                       shuffle=True, \n", "                       callbacks=callbacks,\n", "                       verbose=1)\n", "\n", "    def load(self, filename: str) -> None:\n", "        self.model = load_model(filename)\n", "        print(self.model.summary())\n", "\n", "    def predict(self, data_test, answer_filename):\n", "        word_matrix, char_matrix, additional_features_matrix = data_test\n", "        \n", "        print(\"Test example: \")\n", "        print(word_matrix[0])\n", "        print(char_matrix[0])\n", "        print(additional_features_matrix[0])\n", "        preds = self.model.predict([word_matrix, char_matrix, additional_features_matrix], \n", "                                   batch_size=self.batch_size, verbose=1)\n", "        index_to_author = { 0: \"EAP\", 1: \"HPL\", 2: \"MWS\" }\n", "        submission = pd.DataFrame({\"id\": test[\"id\"], index_to_author[0]: preds[:, 0], \n", "                                   index_to_author[1]: preds[:, 1], index_to_author[2]: preds[:, 2]})\n", "        submission.to_csv(answer_filename, index=False)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "9cad4050-7aba-4503-a740-3b0cf5254519", "_uuid": "99600905d1ce4af7f926263e530a2a499b31e5b4", "collapsed": true}, "execution_count": null, "source": ["MODEL_FILENAME = \"model.h5\""], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "5452978d-e96b-4eaf-b5fc-acd359ef42c0", "_uuid": "b832f812966f89f5acf57b014fcb18771fa5d212"}, "execution_count": null, "source": ["rnn = SpookyRNN()\n", "rnn.build(data_train[2].shape[1], vocabulary.size())\n", "rnn.train(data_train, data_val, MODEL_FILENAME)"], "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "36ba6b33-754e-4c61-b927-0430adebd867", "_uuid": "af0d5634ee909cc325b1b709bb84c56db6a44533"}, "execution_count": null, "source": ["rnn = SpookyRNN()\n", "rnn.load(MODEL_FILENAME)\n", "rnn.predict(data_test, 'answer.csv')"], "outputs": []}], "nbformat": 4}