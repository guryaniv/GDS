{"cells":[{"metadata":{"_uuid":"47e3a5da4e3808461c7086572d2ea857e43f3056","_cell_guid":"72c3e36d-040a-42dc-9aa1-3e2b29bf40f3"},"cell_type":"markdown","source":"**Contents**\n* Loading files and Data Preprocessing\n* Baseline Artificial Neural Network\n* Evaluate Baseline Model\n* CNN, Data Augmentation, & Preprocessing\n* Construct CNN\n* Evaluate CNN Model\n* References"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nnp.random.seed(0)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Visualization library\nimport matplotlib.pyplot as plt\n# Model for spliting data to training and test set\nfrom sklearn.model_selection import train_test_split\n# The Sequential is used to define the machine learning model in keras\nfrom keras.models import Sequential\n# The layers that we might need to construct the neural network\n# Conv2D and MaxPool2D are used for CNN\n# Dropout are used to fight with overfitting\n# Flatten is used to transfer the dimension of data so that the data can be\n# passed into fully connected network after extracting features from CNN\nfrom keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\nfrom keras import optimizers\nimport keras.backend as K\n# this model is used to preprocess the label data. It is a multiclass classification\n# problem. We need to use transfer the label like 4,6,1 to something like \n# [0,0,0,1,0,0,0,...], [0,0,0,0,0,1,0,...]\nfrom keras.utils.np_utils import to_categorical\n\n# We might use data augmentation, this model can help us generate augmentation data\nfrom keras.preprocessing.image import ImageDataGenerator\n# Adjusting the learning rate during the training process to enhance the convergence\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"9bf3d7eb8ee3802dd21467425819bc92559a586c","_cell_guid":"e8dea81b-15a6-4179-9562-c662853cd6bb"},"cell_type":"markdown","source":"**Loading data from the csv file**"},{"metadata":{"_uuid":"4a974ec57299c392742b6af3aa31390fa8ab7193","_cell_guid":"b3bf7597-36e2-4456-8bc7-c926ddbad502","collapsed":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"7bcfa7167b96a764d4d711d98d9cbb5490f56600","_cell_guid":"e061aa78-6823-4ad5-b73a-9c7b5008d1e8"},"cell_type":"markdown","source":"**Preprocessing Training Data**\nIn this step I only use the data from train.csv. By spliting the data from train.csv to train, validation, and test sets, the model can be evaluated before fitting to the test set that is used to submit the result.\n\n**1. Get train and label data**\n* Extract label column from train.csv\n* Obtain the remainder columns as data of train set\n\n**2. Rescale data to range between 1 and 0 (Neural Networks work well for the data in this range) by divided 255 (the image is grey-scaled so that the pixel-value is an integer between 0 and 255).**\n* Convert data type to 'float32' with numpy\n* Divide all data in train from train.csv by 255\n* Divide all data in test.csv by 255\n\n**3.Split data to train, validation, and test set**\n* Extract a portion of data as validation set with sample()\n* Randomly split data to train and test sets with train_test_split from sklearn\n\n**4. Manipulate label data with vectorization**\n* Convert label data to vector\n* 10 vectors will be generated becasue it is a multiclass classification problem with 10 classes\n"},{"metadata":{"_uuid":"bad8768db0f323c806c821b68ac4777e8fca3b43","_cell_guid":"1f03b5f2-345b-4709-94ed-edcbba63dc8f","trusted":true},"cell_type":"code","source":"# Create an index list for convenience in spliting data\nindex = [i for i in range(len(train))]\nindex = pd.Series(index)\ntrain['index'] = index\ntrain.head(10)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"8d77180cabd07e6a4ac03f8845dfc3d275841f8f","_cell_guid":"5cb6309a-aac1-4341-ad6c-e83e3e96307b","trusted":true},"cell_type":"code","source":"# Check original data size in train\nprint('Original Size: ' + str(train.shape))\n# Get a portion of data as validation set\nval = train.sample(int(len(train)*0.1))\n# Check the size in validation set\nprint('Validation Size: ' + str(val.shape))\n# Concat originzal dataframe and validataion dataframe, drop the same columns so that the data which is \n# used as validation would be chosen as data in any of train or test set\nX = train.drop(val['index'])\n# Check the size in train set\nprint('Train Size before split: ' + str(X.shape))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"ae2b2f41a6cdb077413d56e9070b2f044deb59d2","_cell_guid":"b2656ed5-6da2-4b23-83d4-90f5eb918ac2","collapsed":true,"trusted":false},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6624e1d8e1b9c874ef263f042506662569ca0ab1","_cell_guid":"184bea51-5136-428e-9a60-29a45b9b384f"},"cell_type":"markdown","source":"**Currently, we obtain a validation set with 20% of data from original dataset, and a training set which does not contain any data in validatiaon set**"},{"metadata":{"_uuid":"ebf1277473437a877ef0ba34d0e7c46a99898f79","_cell_guid":"dcdf57fa-7fa8-4f0f-ad41-cfb11f75152a"},"cell_type":"markdown","source":"Now, preprocess the data in training set"},{"metadata":{"_uuid":"c1b4cf5a8003a67af1105a2b93b0d4f0c06937af","_cell_guid":"871f1567-7a91-4edd-8f27-86ec9c292d7b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get training data through slicing to get rid of label data and index data\ntrain_afsplit = X.iloc[:,1:-1]\n# Get label data\ny = X['label']\n# Convert label data to vector\ny_cat = to_categorical(y)\n\n# Change its type to float for scaling purpose\ntrain_afsplit = train_afsplit.astype('float32')\ntest = test.astype('float32')\n\n# Rescale the data\ntrain_afsplit /= 255.0\ntest /= 255.0\n\n# Split data to train, validation, and test set\nX_train, X_test, y_train, y_test = train_test_split(train_afsplit, y_cat, test_size = 0.2)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"3ed3b162d4362dfb0447e87dad660792a0e15bc2","_cell_guid":"d7c2fde5-67f0-4e83-bb83-47f51c1a26ff","trusted":true},"cell_type":"code","source":"print('Train: ' + str(X_train.shape))\nprint('y train: ' + str(X_test.shape))\nprint('Test: ' + str(y_train.shape))\nprint('y test: ' + str(y_test.shape))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"181eab820553868027776cb6f41043082641589b","_cell_guid":"9868634b-7262-4303-9ac7-456872128829"},"cell_type":"markdown","source":"Now, preprocess the data in validation set"},{"metadata":{"_uuid":"2da0976e28ddf8536b2dd7a88ac165efd1ef0562","_cell_guid":"206c72a1-6061-4174-b9e5-6613c8428a98","trusted":true},"cell_type":"code","source":"val_train = val.iloc[:, 1:-1]\nval_cat = to_categorical(val['label'])\n\nval_train = val_train.astype('float32')\n\nval_train /= 255.0\n\nprint('val train: ' + str(val_train.shape))\nprint('val cat: ' + str(val_cat.shape))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"bf5e0de8d1fa23bc1d968e340ebee790c7d6b156","_cell_guid":"56bf65f8-fedd-4945-a581-a4cdd9500e4d","trusted":false,"collapsed":true},"cell_type":"code","source":"print(type(val_train))\nprint(type(X_train))\nprint(type(X_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f927757ab091c4354fcc2761d87714f45f30fd2","_cell_guid":"e3deccd6-d146-4d83-8cb1-b1c1c7525a64","collapsed":true},"cell_type":"markdown","source":"**Constructing a simple Artificical Neural Network (ANN) as baseline model**\n* In this model, I decided to add 3 hiden layers with 512, 256, and 128 units seperately (the larger the architecture the larger probability of overfitting)\n* The data is passed into the model as a long vector with length equal to 28 * 28 = 784 => the input shape of the model"},{"metadata":{"_uuid":"9a26c98a1265e988b7e8785d20315476c3c990f2","_cell_guid":"b2f912a9-db6b-444f-b18a-fd60d26dba79","trusted":true},"cell_type":"code","source":"# Defien the input dimension for the model\n# Access the length of the vector with shape[1], the number of training examples can\n# be access with shape[0]\ndim = X_train.shape[1]\n\n# save memory\nK.clear_session()\n# Defien the model\nANN = Sequential()\n# Add a layer with 512 units to the model\n# relu just like max(x, 0) here x is the input. If x was lower than 0,\n# relu will map it to 0. If x was larger than 0, relu will keep it unchange\nANN.add(Dense(512, input_dim = dim, activation = 'relu'))\n# Add other three layers\n# In Keras, we just need to set the input_dim for the first layer. For the \n# remainder layers, keras will automatically figure out its input dimension\nANN.add(Dense(512, activation = 'relu'))\nANN.add(Dense(256, activation = 'relu'))\nANN.add(Dense(128, activation = 'relu'))\n# Add the last layer, and set the units to 10 becasue we have 10 classes\n# Use softmax for multiclass classification\nANN.add(Dense(10, activation = 'softmax'))\n\n# compile is used to construct the model that we just defined\n# Use categorical_crossentropy loss for multiclass classification\nANN.compile(loss = 'categorical_crossentropy',\n              optimizer = 'rmsprop',\n              metrics = ['accuracy'])\nANN.summary()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"675c942485276063006afce0b85b3b90c266a53b","_cell_guid":"7bc1a3dd-101e-4988-a309-7f0eba77bd24","collapsed":true},"cell_type":"markdown","source":"**Fit the model to the data and evaluate its performance**"},{"metadata":{"_uuid":"25c06a223ff8a796d2bd84fcfd21896985d8534b","_cell_guid":"6fcc0861-4e8f-42ca-a4a8-3bf9c3b7c65f","trusted":true},"cell_type":"code","source":"# define the batch size for the model => how many data will be input to the model at once\nbz = 128\n\nANN_pred = ANN.fit(X_train,\n                   y_train,\n                   batch_size = bz,\n                   epochs = 15,\n                   verbose = 2,\n                   validation_data = (val_train, val_cat))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"c5abbf62dfe62d7b5d3e2ad984e0ced720c631da","_cell_guid":"d86c54d3-5869-4593-a544-3a8383572e21"},"cell_type":"markdown","source":"**By using the ANN with no data augmentation, the accuracy in the validation set is around 97%, while the accuracy in training set is around 99.6%.**\n* Plotting the changes of training accuracy and validation accuracy to see whether the model can still be improved"},{"metadata":{"_uuid":"aa60aa163eccbd4ef869d7eead08c3dd441a4bd4","_cell_guid":"ee985af2-d4aa-4586-9c92-d1994a09dd57","trusted":true},"cell_type":"code","source":"plt.plot(ANN_pred.history['acc'])\nplt.plot(ANN_pred.history['val_acc'])\nplt.legend(['training accuracy', 'validation accuracy'])\nplt.title('Accuracy')\nplt.xlabel('Epochs')","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"0f5fc6fe1fd6037624089808a24b2d6901a76874","_cell_guid":"bbde8ad2-0991-42b3-9d75-efa0b2a7bec1"},"cell_type":"markdown","source":"After plotting the changes in accuracy, the accuracy in the training set seems like that still can be improved a little bit by running with more epochs. However, there is a big gap between the training accuracy and validation accuracy. It means that our model might be **overfitting**."},{"metadata":{"_uuid":"5e914532c6d36de2ac5071786a4e1a16b6ad04fc","_cell_guid":"370d64ba-9d3d-489b-9188-7eda37e77f88"},"cell_type":"markdown","source":"**Evaluate ANN model in test set splitted from training data**"},{"metadata":{"_uuid":"49353812ef4e2d5f13b3e341cea1e160696b9b4a","_cell_guid":"5b96bc25-6afd-443c-ab83-3aa2dd16b791","trusted":true},"cell_type":"code","source":"# Get probability return as results\nANN_test_pred = ANN.predict_proba(X_test)\n# Get the maximum probability in the predicted results => the probability belongs to a specific class\nANN_test_pred = ANN_test_pred.argmax(axis = 1)\nANN.evaluate(X_test, y_test)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"8ed62d0c35c181bab42885fbe20af67ae591298c","_cell_guid":"6005692a-25cf-40b5-9396-e8d1d000b8ca"},"cell_type":"markdown","source":"**The evaluation results shows the accuracy in test set is around 97%, and it can be the baseline for the prediction**"},{"metadata":{"_uuid":"e5964157cf9af20dab1406165bd68873f13d7518","_cell_guid":"36570746-cbe0-4e43-a3fa-58351f6eb1f7","collapsed":true},"cell_type":"markdown","source":"**CNN with Data Augmentation**\n\n**Construct a CNN model with Data Augmentation to see whether the result can be improved because the ANN model might almost reach its capacity based on the accuracy plot**\n* Before passing data to the CNN, the data require to be transformed to tensor\n* We need to reshape the vector to (28, 28, 1) => it means the size of image is 28 * 28 with 1 color channel beacause it is grey-scale image"},{"metadata":{"_uuid":"35b82f645d4ffb3b096255913833b9165fe5a65d","_cell_guid":"fca87f32-0179-443b-a799-4df0c16cdd5f","trusted":true},"cell_type":"code","source":"print(type(val_train))\nprint(type(X_train))\nprint(type(X_test))","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"8702358a378ac3d66d1f7f51ffdcd86d4b420619","_cell_guid":"045ccb65-2986-4b60-ab9b-ac3c82dc54c9","trusted":true},"cell_type":"code","source":"X_train = X_train.values\nX_test = X_test.values\n\n# -1 means let reshape function decides the dimension by itself\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\n# for validation data\nval_train = val_train.values\nval_train = val_train.reshape(-1,28,28,1)\n\n# for test data\ntest = test.values\ntest = test.reshape(-1,28,28,1)\nprint('X_train: ' + str(X_train.shape))\nprint('X_test: ' + str(X_test.shape))\nprint('val_train: ' + str(val_train.shape))\nprint('test: ' + str(test.shape))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"413bcbf7a078db83230bf259614606cc23ef1d6b","_cell_guid":"c0c250a1-d77b-4da8-a30a-aa01423e87b6","collapsed":true},"cell_type":"markdown","source":"**Using ImageDataGenerator to complete the Data Augmentation**\n* It will be helpful to increase our training examples and to fight with overfitting"},{"metadata":{"_uuid":"f42fb02a359aa879935596e902621a25cb7a14a2","_cell_guid":"b1595719-75be-4dd6-af75-9f1e28c44b93","collapsed":true,"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    # set smaller range for this to prevent misclassification like 6 and 9, 2 and 5\n    rotation_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    fill_mode='nearest')","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"1f2292392a1ab9b48d385dd435e145b9ef285163","_cell_guid":"643bd48b-0d3c-4455-b13e-4ff464bb20bb"},"cell_type":"markdown","source":"Fit the generator to the training data"},{"metadata":{"_uuid":"ef6ad290743a80222d55ccd14fb0956da8362991","_cell_guid":"c4c1ef38-51d2-4d14-a795-d005fc9e4943","collapsed":true,"trusted":true},"cell_type":"code","source":"datagen.fit(X_train)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"f73bca58cbaf7a99579fb9e6705f93180a295947","_cell_guid":"11100b22-b12d-4fbc-870b-79163cccf631","trusted":true},"cell_type":"code","source":"# save memory\nK.clear_session()\n\n# Adjusting the learning rate during the process to enhance the convergency\nLR_Adjust = ReduceLROnPlateau(monitor = 'val_loss',\n                              patience = 2,\n                              verbose = 1,\n                              factor = 0.3,\n                              min_lr = 0)\n\noptimizer = optimizers.rmsprop()\n\nCNN = Sequential()\n\nCNN.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\nCNN.add(Dropout(0.1))\nCNN.add(MaxPool2D(pool_size = (2,2)))\nCNN.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nCNN.add(Dropout(0.1))\nCNN.add(MaxPool2D(pool_size = (2,2)))\n\nCNN.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nCNN.add(Dropout(0.1))\n\nCNN.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nCNN.add(MaxPool2D(pool_size = (2,2)))\nCNN.add(Dropout(0.05))\n\nCNN.add(Flatten())\nCNN.add(Dense(512, activation = 'relu'))\nCNN.add(Dense(256, activation = 'relu'))\nCNN.add(Dense(128, activation = 'relu'))\nCNN.add(Dense(10, activation = 'softmax'))\n\nCNN.compile(loss = 'categorical_crossentropy',\n            optimizer = optimizer,\n            metrics = ['accuracy'])\n\n\nCNN.summary()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"db284bd43df97c84ed53d2283eeb6f3369946d2a","_cell_guid":"8c85f660-5a26-4e1d-b521-2d9fa50e653f","trusted":true,"scrolled":true},"cell_type":"code","source":"bz = 128\n\nCNN_pred = CNN.fit_generator(datagen.flow(X_train,y_train, batch_size= bz),\n                             epochs = 30, \n                             validation_data = (val_train,val_cat),\n                             verbose = 2, \n                             steps_per_epoch=X_train.shape[0] // bz,\n                             callbacks=[LR_Adjust])","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"057d3074a58a951a93e2dab17e2dfc92c95aba92","_cell_guid":"90b3f4fc-cd3d-4f2c-8820-876fbe55e6a4","trusted":true},"cell_type":"code","source":"plt.plot(CNN_pred.history['acc'])\nplt.plot(CNN_pred.history['val_acc'])\nplt.legend(['training accuracy', 'validation accuracy'])\nplt.title('Accuracy')\nplt.xlabel('Epochs')","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"d41b04db2f3305d1ce81286b90609bea68fc8d25","_cell_guid":"55a7666b-1542-4320-af7b-afd34e282bb7","collapsed":true},"cell_type":"markdown","source":"**By using CNN and Data Augmentation, the training accuracy and validataion accuracy are very approching to each other, and it means that the model is comparatively generalized**\n* After running for 30 epochs, the trainning accuracy is around 99.5% and the validation accuracy is around 99.3%\n\n**The loss in the validation set is almost not changed; therefore, the model might already converge, and the accuracy might reach the max capacity of this model**"},{"metadata":{"_uuid":"d2f4e68bd0ef762fa3fd4c27617cfeb443ffa8ca","_cell_guid":"3562ad41-43e3-43fa-85e8-85de1eb957f7","collapsed":true},"cell_type":"markdown","source":"**Evaluate the CNN model**"},{"metadata":{"_uuid":"f43d6e3575a56aaa16b266d9d0fc42d6adad2aaf","_cell_guid":"439093c7-1393-4ec2-ae58-89fb7feecab3","trusted":true},"cell_type":"code","source":"# Get probability return as result\nCNN_test_pred = CNN.predict_proba(X_test)\n# Get the maximum probability in the predicted results => the probability belongs to a specific class\nCNN_test_pred = CNN_test_pred.argmax(axis = 1)\nCNN.evaluate(X_test, y_test)","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"3d5091657502b02a07202bca8e89b375ba088c2b","_cell_guid":"9ee15ac3-af52-4eb9-99cf-7357ca2e05de"},"cell_type":"markdown","source":"**The accuracy in the test set is 99.3%, which is 2% higher than the baseline**"},{"metadata":{"_uuid":"9bfb5fbf98a767c298f6c6b21a3a68ef47b6b62f","_cell_guid":"358fb8a8-af18-4444-8203-9bbda5ffdef8","collapsed":true,"trusted":true},"cell_type":"code","source":"test_pred = CNN.predict_proba(test)\n\n# select the indix with the maximum probability\ntest_pred = np.argmax(test_pred,axis = 1)\n\ntest_pred = pd.Series(test_pred,name=\"Label\")\n\nsub_index = [i for i in range(1, 28000+1)]\n\nsub_index = pd.Series(sub_index, name='ImageId')\n\ntest_pred = pd.concat([sub_index, test_pred],axis = 1)\n\ntest_pred.to_csv(\"submission.csv\",index=False)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"0e9823cc3819299ce4692df2067d356d8181e2d5"},"cell_type":"markdown","source":"**Analyze Error Cases**\n* Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"bc2c666a3187a1062fd3b26f6c3a880909f59ea6"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, normalize = False,\n                         title = 'Confusion matrix', cmap = plt.cm.Blues):\n    plt.imshow(cm,interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    # first parameter is how to arrange the x labels\n    # second parameter is the actual label, here we set to classes 1 - 9\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks,classes)\n    \n    if normalize:\n        cm = cm.astype('float32') / cm.sum(axis = 1)[:, np.newaxis]\n        \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        # display text in the box to show how many error cases\n        plt.text(j,i, cm[i,j],\n                horizontalalignment = 'center',\n                color = 'white' if cm[i,j] > thresh else 'black')\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbf56a3756a11960e07670c976da6a3fae8f89ba"},"cell_type":"code","source":"val_pred = CNN.predict_proba(val_train)\nval_pred_class = np.argmax(val_pred, axis = 1)\nval_true = np.argmax(val_cat, axis=1)\ncm = confusion_matrix(val_true, val_pred_class)\nplot_confusion_matrix(cm,classes = range(10))","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"9e613d0713930e64dd10c4363f094406a9766a5a"},"cell_type":"markdown","source":"**Based on the confusion matrix, 5 cases of 7 being predicted as 2**"},{"metadata":{"_uuid":"0697fe3b0c51af4d7b5ce00edb1054b0c0f8c376"},"cell_type":"markdown","source":"**Visualize the error cases**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8a022f10f1fdd9fc0194c72700921761fd6a671f"},"cell_type":"code","source":"errors = (val_pred_class - val_true != 0)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f96d805013d4ee8456f2b4ebfdf4d0289c65450e"},"cell_type":"code","source":"# errors is an array of boolean value, where True represents the errors\n# val_pred_class[errors] will return the values that are true, the error\n# label\nval_pred_class_errors = val_pred_class[errors]\n# return the error images to val_pred_errors\nval_pred_errors = val_pred[errors]","execution_count":47,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d79cc553e05f2791d5b47b4a02f1ec495f8f8c10"},"cell_type":"code","source":"val_true_errors = val_true[errors]\nval_train_errors = val_train[errors]","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a992bc0a8a071f964550a5c199446485bcfe7d6a"},"cell_type":"code","source":"def show_errors(error_index, img_errors, pred_errors, obs_errors):\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = error_index[n]\n            ax[row, col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row, col].set_title(f'Predicted label:{pred_errors[error]}\\n True label:{obs_errors[error]}')\n            n += 1\n    ","execution_count":53,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ced6100293f671f24fdecf043d5b59ba700388fb"},"cell_type":"code","source":"val_pred_errors_prob = np.max(val_pred_errors, axis = 1)\ntrue_prob_errors = np.diagonal(np.take(val_pred_errors, val_true_errors, axis = 1))\ndelta_pred_true_errors = val_pred_errors_prob - true_prob_errors\nsorted_delta_errors = np.argsort(delta_pred_true_errors)\n\nmost_important_errors = sorted_delta_errors[-6:]\n\nshow_errors(most_important_errors, val_train_errors, val_pred_class_errors, val_true_errors)","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"4132483de04428d72e524da03bbed49e167125ad","_cell_guid":"1df7dc54-e9d7-4fe3-831c-11db62d575b6","collapsed":true},"cell_type":"markdown","source":"**References:**\n\n[https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6/log](http://)\n\n[http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py](http://)"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}