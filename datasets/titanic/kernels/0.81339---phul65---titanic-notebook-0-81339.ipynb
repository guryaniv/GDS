{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import LabelEncoder, normalize\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f00827d9ef43dfa8d1665bff640591d91361fde","collapsed":true},"cell_type":"code","source":"ROOT_DIR = \"../input/\"\nSUBINT_DIR = \"./\"","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"print (\"Lecture des fichiers train.csv\")\ntrain = pd.read_csv(ROOT_DIR+\"train.csv\",sep=',')\nprint(train.shape)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be65d33b0a42054ddfb1cd74fb9395025a1201ec","collapsed":true},"cell_type":"code","source":"print (\"Lecture des fichiers test.csv\")\ntest = pd.read_csv(ROOT_DIR+\"test.csv\",sep=',')\nprint(test.shape)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"123dc4e19a9d288d5375b78798cb27fa5af2e577","collapsed":true},"cell_type":"code","source":"print (\"Lecture des fichiers gender_submission.csv\")\nsubmit = pd.read_csv(ROOT_DIR+\"gender_submission.csv\",sep=',')\nprint(submit.shape)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efada5b9accb21ec71f2cafa1c754cdbec0aafd1","collapsed":true},"cell_type":"code","source":"print(\"Concatenation train + test\")\ntrain['X'] = 'X'\ntest['X'] = 'Y'\nbig = pd.concat([train,test])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"365d9f268e427d0199a4e51b05bcb1d06a55e324","collapsed":true},"cell_type":"code","source":"# Nom exemple : Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n# XName : Hirvonen\n# TName : Mrs.\nbig['XName'] = big['Name'].apply(lambda x: str(x)[0:str(x).find(',')] if str(x).find(',') != -1 else x)\nbig['TName'] = big['Name'].apply(lambda x: str(x)[str(x).find(',')+2:str(x).find('. ')+1:] if str(x).find('. ') != -1 else x)\n\nbig['XCabin'] = big['Cabin'].apply(lambda x: 'U' if (x is np.nan or x != x) else str(x)[0])\n\nbig['LTick'] = big['Ticket'].apply(lambda x: str(x)[0:str(x).find(' ')] if str(x).find(' ') != -1 else ' ')\nK = big.groupby(['Ticket']).groups\n#display(K)\nfor name,group in K.items():\n    if len(group) > 1:\n        CN = list(set([ str(x)[0] for x in big['Cabin'].iloc[group] ]) - set(['n']))\n        if (len(CN) == 0):\n            big['XCabin'].iloc[group] = 'U'\n        else:\n            big['XCabin'].iloc[group] = CN[0]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a144821004ddb5aca71eee22a46e5aede79afdc1"},"cell_type":"code","source":"big['XFam'] = big['SibSp'] + big['Parch'] + 1\nbig['XFam'] = np.log1p((big['XFam'] - big['XFam'].mean()) / big['XFam'].std())","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"df7db0c7eb32d7fc8f09ce1725b4b76b05744eda"},"cell_type":"markdown","source":"**From Konstantin - Kernel : Titanic [0.82] - [0.83]**\n\n*     **Adding Family_Survival**\n\n    This feature is from S.Xu's kernel, he groups families and people with the same tickets togerher and researches the info. \n    I've cleaned the code a bit but it still does the same, I left it as is. For comments see the original kernel."},{"metadata":{"trusted":true,"_uuid":"b566fec962e22a3c1abbaeb8c1abc924eb2fc7f2","collapsed":true},"cell_type":"code","source":"big['Last_Name'] = big['Name'].apply(lambda x: str.split(x, \",\")[0])\nbig['Fare'].fillna(big['Fare'].mean(), inplace=True)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\nbig['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in big[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                        'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                big.loc[big['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                big.loc[big['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", big.loc[big['Family_Survival']!=0.5].shape[0])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1f4d6eeef84c1dc542248161452dbd63b3b4912","collapsed":true},"cell_type":"code","source":"for _, grp_df in big.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    big.loc[big['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    big.loc[big['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \"+str(big[big['Family_Survival']!=0.5].shape[0]))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c7bff61b8be2be8d3e14559e9823f5823915a02a"},"cell_type":"code","source":"del big['Ticket'], big['Cabin'], big['Name'], big['XName'], big['Last_Name']","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0575895001cc500dbb98d139f5b6c7738519cf5f","collapsed":true},"cell_type":"code","source":"print(big['TName'].value_counts())\n\nbig['XWho'] = big['TName']\n\nfor i in [ 'Master.', 'Sir.', 'Don.', 'Lady.', 'Dona.', 'the Countess.', 'Mme.' ]:\n    big['XWho'][big['TName'] == i] = \"High.\"\n    \nfor i in [ 'Col.', 'Major.', 'Capt.' ]:\n    big['XWho'][big['TName'] == i] = \"Mil.\"\n    \nfor i in [ 'Mr.', 'Dr.', 'Rev.' ]:\n    big['XWho'][big['TName'] == i] = \"Mr.\"\n    \nfor i in [ 'Mrs.', 'Ms.', 'Mlle.', 'Miss.' ]:\n    big['XWho'][big['TName'] == i] = \"Miss.\"\n\nbig['XWho'][~big['TName'].isin([ 'Sir.', 'Don.', 'Lady.', 'Dona.', 'the Countess.', 'Col.', \n                                'Major.', 'Capt.', 'Mr.', 'Master.', 'Dr.', 'Rev.', 'Mrs.', \n                                'Ms.', 'Mlle.', 'Mme.', 'Miss.' ])] = \"Oth.\"\n            \nprint(big['XWho'].value_counts())","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57beee3d7c34ce055206ea9e50cda97650467da7","collapsed":true},"cell_type":"code","source":"for col in [ 'Sex', 'Pclass', 'XWho', 'Embarked', 'LTick', 'XCabin', 'TName' ]:\n    dummy = pd.get_dummies(big[col],prefix=str(col),prefix_sep=\"__\")\n    big = pd.concat([big, dummy], axis=1)\n    big.drop(col, inplace=True, axis=1) \n    \nfor col in [ 'XFam' ]:\n    lbl = LabelEncoder()\n    lbl.fit(list(big[col].values))\n    big[col] = lbl.transform(list(big[col].values))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c479978e0ae0a5efc49f686677ac987928ad8db1","collapsed":true},"cell_type":"code","source":"CNULL = big.isnull().sum()\nprint(CNULL[CNULL != 0])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f72e01defa18f980440b5f12e8db540530108925"},"cell_type":"code","source":"big['Fare'] = big['Fare'].fillna(big['Fare'].mean())\nbig['Age'] = big['Age'].fillna(big['Age'].mean())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e783d3ecccf93a0a024c2dd7935057b7deb32a13","collapsed":true},"cell_type":"code","source":"train = big[big['X'] == 'X']\ntest = big[big['X'] == 'Y']\n\ndel test['Survived']\ndel train['X'], test['X']\n\nprint(train.shape)\nprint(test.shape)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31455acab1ce92c5a2f5adbaf0b9ab17e566feff","collapsed":true},"cell_type":"code","source":"train['Age'] = (train['Age'] - train['Age'].mean()) / train['Age'].std()\ntest['Age'] = (test['Age'] - test['Age'].mean()) / test['Age'].std()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6da912d65d15d5a9a97cc987c4ddf65d12fe264d","collapsed":true},"cell_type":"code","source":"train['Fare'] = np.log1p((train['Fare'] - train['Fare'].mean()) / train['Fare'].std())\ntest['Fare'] = np.log1p((test['Fare'] - test['Fare'].mean()) / test['Fare'].std())","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d0863e6ebe0fe6c92e00aadcb031862e28a98d4","collapsed":true},"cell_type":"code","source":"# Add colonnes statistiques\ntrain['c_mean'] = pd.Series(train.mean(axis=1), index=train.index)\nc_mean_max = train['c_mean'].max()\nc_mean_min = train['c_mean'].min()\nc_mean_scaled = (train.c_mean-c_mean_min) / c_mean_max\ntrain['c_mean_s'] = pd.Series(c_mean_scaled, index=train.index)\ndel train['c_mean']\n\ntrain['c_std'] = pd.Series(train.std(axis=1), index=train.index)\nc_std_max = train['c_std'].max()\nc_std_min = train['c_std'].min()\nc_std_scaled = (train.c_std-c_std_min) / c_std_max\ntrain['c_std_s'] = np.log1p(pd.Series(c_std_scaled, index=train.index))\ndel train['c_std']\n\ntest['c_mean'] = pd.Series(test.mean(axis=1), index=test.index)\nc_mean_max = test['c_mean'].max()\nc_mean_min = test['c_mean'].min()\nc_mean_scaled = (test.c_mean-c_mean_min) / c_mean_max\ntest['c_mean_s'] = np.log1p(pd.Series(c_mean_scaled, index=test.index))\ndel test['c_mean']\n\ntest['c_std'] = pd.Series(test.std(axis=1), index=test.index)\nc_std_max = test['c_std'].max()\nc_std_min = test['c_std'].min()\nc_std_scaled = (test.c_std-c_std_min) / c_std_max\ntest['c_std_s'] = pd.Series(c_std_scaled, index=test.index)\ndel test['c_std']\n\nprint(train.shape, test.shape)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111141f7c45a78e6a011f20501922a4dc361be37","collapsed":true},"cell_type":"code","source":"print(train.shape)\ntrain.drop_duplicates(inplace=True)\nprint(train.shape)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5327545ddffc1184c1fd917453786b178c45096c","collapsed":true},"cell_type":"code","source":"import time\nimport datetime\n\nprint (\" <*> Debut\")\n\nkDate = time.strftime('%d%m%y_%H%M%S',time.localtime())\n\nstart = time.time()\n\ny_train = train['Survived'].values.astype(np.float64)\nx_train = train.drop(['PassengerId', 'Survived'], axis=1).values.astype(np.float64)\nx_test  = test.drop(['PassengerId'], axis=1).values.astype(np.float64)\n\nprint('Shape train: {}\\nShape test: {}\\nShape Y: {}'.format(x_train.shape, x_test.shape, y_train.shape))\n\nNSplit = 5\nSliceTrain = 0.75\nSliceTest  = 0.25\nmodels = []\nNIter = 0\nTScore = 0\n\nprint(\"Entrainement\")\nrs = StratifiedShuffleSplit(n_splits=NSplit, random_state=99, test_size=SliceTest) \nfor train_index, test_index in rs.split(x_train, y_train):\n    \n    X_train = x_train[train_index]\n    Y_train = y_train[train_index]\n    X_valid = x_train[test_index]\n    Y_valid = y_train[test_index]\n\n    rfc_params = {}\n    rfc_params['n_estimators'] = 200  \n    rfc_params['learning_rate'] = 0.015\n    rfc_params['max_depth'] = 250   \n    rfc_params['max_features'] = \"auto\"\n    rfc_params['min_samples_split'] = 0.7\n    rfc_params['min_samples_leaf'] = 0.01    \n    rfc_params['random_state'] = 0\n    rfc_params['verbose'] = 0   \n    \n    sum_score = 0\n    score     = 0\n    \n    clf = GradientBoostingClassifier(**rfc_params)\n    clf.fit(X_train, Y_train)\n    models.append(clf)\n\n    score = clf.score(X_valid, Y_valid)\n    print (\" <*> Entrainement \",NIter,\" avec \", SliceTrain, \" pour train et \",SliceTest,\" pour test - Score : \", score)\n    TScore += score\n    NIter += 1\n    \nTScore /= NSplit\n\nprint(\" <*> ---------------- Resultats CV ------------------ \")\nprint(\" <*> params : \",rfc_params)\nprint(\" <*> Score Moyenne training : \", TScore)\n    \nprint(\"Verification avec le train\")\nscore = 0\nSCLOG = 0\nNIter = 0\nfor clf in models:\n\n    PTrain = clf.predict(x_train)\n    score  = clf.score(x_train, y_train)\n    SCLOG += score\n    NIter += 1\n\nSCLOG /= NSplit\nprint(\" <*> Score Moyenne Train    : \", SCLOG)\n       \nprint(\"Predictions\")\nNIter = 0\nctb_pred1 = []\nfor clf in models:\n\n    PTest = clf.predict(x_test)\n    ctb_pred1.append(PTest)\n\n    NIter += 1\n\nPTest = [0] * len(ctb_pred1[0])\nfor i in range(NSplit):\n    PTest += ctb_pred1[i]\nPTest /= NSplit\n\nprint( pd.DataFrame(PTest).head() )        \n    \nend = time.time()\nprint (\" <*> Duree : \",end - start)\n    \nprint (\" <*> Fin\")","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d96ef3e2b02789208671a85092c1218e630c186d"},"cell_type":"code","source":"# Submit resultats\nprint( \" Mise a jour des colonnes submit\" )\nsubmit['Survived'] = np.clip(PTest, 0, 1).astype(int) \nlocaltime = time.localtime(time.time())\nWDate = str(localtime.tm_mday).rjust(2, '0')+str(localtime.tm_mon).rjust(2, '0')+str(localtime.tm_year)\n\nSUBFIC = SUBINT_DIR+\"Titanic_GBR_\"+str(kDate)+\".csv\"\nprint (\" <*> Ecriture deb CSV/7z : \", SUBFIC)\nsubmit.to_csv(SUBFIC, index=False) \nprint (\" <*> Ecriture fin CSV/7z : \", SUBFIC)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}