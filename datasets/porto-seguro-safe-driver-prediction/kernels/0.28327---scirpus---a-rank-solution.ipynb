{"nbformat_minor": 1, "cells": [{"source": ["import numpy as np \n", "import pandas as pd\n", "from xgboost import XGBClassifier\n", "from sklearn.model_selection import KFold,StratifiedKFold\n", "from numba import jit"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "8e5984dd969fbfbf26a0238e4c056cc1ed3cf5a4", "_cell_guid": "d024112a-0bca-4d7f-996e-f2ffd741ed27"}, "execution_count": 1}, {"source": ["@jit\n", "def eval_gini(y_true, y_prob):\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    preds -= preds.min()\n", "    preds / preds.max()\n", "    gini_score = -eval_gini(labels, preds)\n", "    return [('gini', gini_score)]\n", "\n", "\n", "def add_noise(series, noise_level):\n", "    return series * (1 + noise_level * np.random.randn(len(series)))\n", "\n", "\n", "def target_encode(trn_series=None,    # Revised to encode validation series\n", "                  val_series=None,\n", "                  tst_series=None,\n", "                  target=None,\n", "                  min_samples_leaf=1,\n", "                  smoothing=1,\n", "                  noise_level=0):\n", "    \"\"\"\n", "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n", "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n", "    trn_series : training categorical feature as a pd.Series\n", "    tst_series : test categorical feature as a pd.Series\n", "    target : target data as a pd.Series\n", "    min_samples_leaf (int) : minimum samples to take category average into account\n", "    smoothing (int) : smoothing effect to balance categorical average vs prior\n", "    \"\"\"\n", "    assert len(trn_series) == len(target)\n", "    assert trn_series.name == tst_series.name\n", "    temp = pd.concat([trn_series, target], axis=1)\n", "    # Compute target mean\n", "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n", "    # Compute smoothing\n", "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n", "    # Apply average function to all target data\n", "    prior = target.mean()\n", "    # The bigger the count the less full_avg is taken into account\n", "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n", "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n", "    # Apply averages to trn and tst series\n", "    ft_trn_series = pd.merge(\n", "        trn_series.to_frame(trn_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=trn_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_trn_series.index = trn_series.index\n", "    ft_val_series = pd.merge(\n", "        val_series.to_frame(val_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=val_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_val_series.index = val_series.index\n", "    ft_tst_series = pd.merge(\n", "        tst_series.to_frame(tst_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=tst_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_tst_series.index = tst_series.index\n", "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "196986d94aa749057f8613105abe7b9bc0428e9f", "_cell_guid": "ba1003f0-f8c8-4796-8867-d15366c4af86"}, "execution_count": 6}, {"source": ["strdirectory = '../input/'\n", "train = pd.read_csv(strdirectory+'train.csv')\n", "test = pd.read_csv(strdirectory+'test.csv')\n", "\n", "\n", "test.insert(1,'target',0)\n", "print(train.shape)\n", "print(test.shape)\n", "\n", "x = pd.concat([train,test])\n", "x = x.reset_index(drop=True)\n", "unwanted = x.columns[x.columns.str.startswith('ps_calc_')]\n", "x.drop(unwanted,inplace=True,axis=1)\n", "\n", "x.loc[:,'ps_reg_03'] = pd.cut(x['ps_reg_03'], 50,labels=False)\n", "x.loc[:,'ps_car_12'] = pd.cut(x['ps_car_12'], 50,labels=False)\n", "x.loc[:,'ps_car_13'] = pd.cut(x['ps_car_13'], 50,labels=False)\n", "x.loc[:,'ps_car_14'] =  pd.cut(x['ps_car_14'], 50,labels=False)\n", "x.loc[:,'ps_car_15'] =  pd.cut(x['ps_car_15'], 50,labels=False)\n", "\n", "test = x.iloc[train.shape[0]:].copy()\n", "train = x.iloc[:train.shape[0]].copy()\n"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "bdefba5b97a34fe8260d1a5ed1582a2636663fa3", "_cell_guid": "d94b25f0-d0b7-4869-9771-c7a66d684db8"}, "execution_count": 5}, {"source": ["features = train.columns[2:]\n", "ranktestpreds = np.zeros(test.shape[0])\n", "kf = KFold(n_splits=5,shuffle=True,random_state=2017)\n", "for i, (train_index, test_index) in enumerate(kf.split(list(train.index))):\n", "    print('Fold: ',i)\n", "    myfeatures = list(features[:])\n", "    blindtrain = train.iloc[test_index].copy()\n", "    vistrain = train.iloc[train_index].copy()\n", "    mytest = test.copy()\n", "    for column in features:\n", "        vis, blind, tst = target_encode(trn_series=vistrain[column],\n", "                                        val_series=blindtrain[column],\n", "                                        tst_series=mytest[column],\n", "                                        target=vistrain.target,\n", "                                        min_samples_leaf=200,\n", "                                        smoothing=10,\n", "                                        noise_level=0)\n", "        vistrain['te_' + column] = vis\n", "        blindtrain['te_' + column] = blind\n", "        mytest['te_' + column] = tst\n", "        myfeatures = myfeatures + list(['te_' + column])\n", "        \n", "    clf = XGBClassifier(n_estimators=2000,\n", "                        objective=\"rank:pairwise\",\n", "                        learning_rate = 0.04,\n", "                        max_depth = 5,\n", "                        min_child_weight = 9,\n", "                        subsample = 0.8,\n", "                        colsample_bytree = 0.8,\n", "                        reg_alpha = 10.4,\n", "                        reg_lambda = 0.59,\n", "                        seed = 2017,\n", "                        nthread = 8,\n", "                        silent = 1)\n", "    \n", "    eval_set=[(blindtrain[myfeatures],blindtrain.target)]\n", "    model = clf.fit(vistrain[myfeatures], vistrain.target, \n", "                    eval_set=eval_set,\n", "                    eval_metric=gini_xgb,\n", "                    early_stopping_rounds=70,\n", "                    verbose=False)\n", "    \n", "    print( \"  Best N trees = \", model.best_ntree_limit )\n", "    print( \"  Best gini = \", model.best_score )\n", "    trainpreds = model.predict_proba(blindtrain[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n", "    print( \"  Best gini = \", eval_gini(blindtrain.target,trainpreds))\n", "    ranktestpreds += model.predict_proba(mytest[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n", "ranktestpreds /= 5\n", "ranktestpreds -= ranktestpreds.min()\n", "ranktestpreds /= ranktestpreds.max()\n"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "7ec757f0872d097a9d06ab9d2c029f9a3f31b1c6", "_cell_guid": "b8592c33-e90c-4cbe-bd20-120868269193"}, "execution_count": 10}, {"source": ["xgbtestpreds = np.zeros(test.shape[0])\n", "kf = KFold(n_splits=5,shuffle=True,random_state=2017)\n", "for i, (train_index, test_index) in enumerate(kf.split(list(train.index))):\n", "    print('Fold: ',i)\n", "    myfeatures = list(features[:])\n", "    blindtrain = train.iloc[test_index].copy()\n", "    vistrain = train.iloc[train_index].copy()\n", "    mytest = test.copy()\n", "    for column in features:\n", "        vis, blind, tst = target_encode(trn_series=vistrain[column],\n", "                                        val_series=blindtrain[column],\n", "                                        tst_series=mytest[column],\n", "                                        target=vistrain.target,\n", "                                        min_samples_leaf=200,\n", "                                        smoothing=10,\n", "                                        noise_level=0)\n", "        vistrain['te_' + column] = vis\n", "        blindtrain['te_' + column] = blind\n", "        mytest['te_' + column] = tst\n", "        myfeatures = myfeatures + list(['te_' + column])\n", "        \n", "    clf = XGBClassifier(n_estimators=2000,\n", "                        learning_rate = 0.04,\n", "                        max_depth = 5,\n", "                        min_child_weight = 9,\n", "                        subsample = 0.8,\n", "                        colsample_bytree = 0.8,\n", "                        reg_alpha = 10.4,\n", "                        reg_lambda = 0.59,\n", "                        seed = 2017,\n", "                        nthread = 8,\n", "                        silent = 1)\n", "    \n", "    eval_set=[(blindtrain[myfeatures],blindtrain.target)]\n", "    model = clf.fit(vistrain[myfeatures], vistrain.target, \n", "                    eval_set=eval_set,\n", "                    eval_metric=gini_xgb,\n", "                    early_stopping_rounds=70,\n", "                    verbose=False)\n", "    \n", "    print( \"  Best N trees = \", model.best_ntree_limit )\n", "    print( \"  Best gini = \", model.best_score )\n", "    trainpreds = model.predict_proba(blindtrain[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n", "    print( \"  Best gini = \", eval_gini(blindtrain.target,trainpreds))\n", "    xgbtestpreds += model.predict_proba(mytest[myfeatures],ntree_limit=model.best_ntree_limit)[:,1]\n", "xgbtestpreds /= 5\n", "xgbtestpreds -= xgbtestpreds.min()\n", "xgbtestpreds /= xgbtestpreds.max()"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["rankdata = pd.DataFrame()\n", "rankdata['xgbnormal'] = xgbtestpreds\n", "rankdata['xgbrank'] = ranktestpreds"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null}, {"source": ["sub = pd.read_csv('../input/sample_submission.csv')\n", "sub.target = (rankdata.xgbnormal.rank()+rankdata.xgbrank.rank())\n", "sub.target -= sub.target.min()\n", "sub.target /= sub.target.max()\n", "sub.to_csv('xgbsubmission.csv', index = False)"], "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4}