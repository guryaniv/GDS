{"cells":[{"metadata":{"trusted":true,"_uuid":"d042cbf143302a2f66f16bc3f43372c441c39e9f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_union\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom itertools import product\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import ComplementNB\nfrom scipy.sparse import csr_matrix\nfrom imblearn.ensemble import EasyEnsembleClassifier\n\nfrom scipy.sparse import hstack\nimport regex as re\nimport re as r\nimport regex, string\nimport time\nstart = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90377912c59ac55db3fee2cf33e4dccc2cd63b92"},"cell_type":"code","source":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('../input/train.csv').fillna(' ')\ntest = pd.read_csv('../input/test.csv').fillna(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72393a8f060cc4508de0b6945e6377f4661faa5e"},"cell_type":"code","source":"repl = {\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" frown \",\n    \":(\": \" frown \",\n    \":s\": \" frown \",\n    \":-s\": \" frown \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\n\nkeys = [i for i in repl.keys()]\n\nnew_train_data = []\nnew_test_data = []\nltr = train[\"comment_text\"].tolist()\nlte = test[\"comment_text\"].tolist()\nfor i in ltr:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            continue\n        if j in keys:\n            # print(\"inn\")\n            j = repl[j]\n        xx += j + \" \"\n    new_train_data.append(xx)\nfor i in lte:\n    arr = str(i).split()\n    xx = \"\"\n    for j in arr:\n        j = str(j).lower()\n        if j[:4] == 'http' or j[:3] == 'www':\n            continue\n        if j in keys:\n            # print(\"inn\")\n            j = repl[j]\n        xx += j + \" \"\n    new_test_data.append(xx)\ntrain[\"new_comment_text\"] = new_train_data\ntest[\"new_comment_text\"] = new_test_data\n\ntrate = train[\"new_comment_text\"].tolist()\ntete = test[\"new_comment_text\"].tolist()\nfor i, c in enumerate(trate):\n    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\nfor i, c in enumerate(tete):\n    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\ntrain[\"comment_text\"] = trate\ntest[\"comment_text\"] = tete\ndel trate, tete\ntrain.drop([\"new_comment_text\"], axis=1, inplace=True)\ntest.drop([\"new_comment_text\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b971408f20e3b32ca94df99f46d34f67cbd15f20"},"cell_type":"code","source":"train_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41f230258f81c3fa5f687acf27e1be2cd23ef379","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def prepare_for_char_n_gram(text):\n    \"\"\" Simple text clean up process\"\"\"\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower().encode('utf-8'))\n    # 2. Drop \\n and  \\t\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    \n    exclude = r.compile(b'[%s]' % re.escape(bytes(string.punctuation.encode('utf-8'))))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n\n    clean = r.sub(b\"\\d+\", b\" \", clean)\n    return str(clean).encode('utf-8')\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))\ndef get_indicators_and_clean_comments(df):\n    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\nfor df in [train, test]:\n   get_indicators_and_clean_comments(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ce8d560d5ac31ffa6d7c67b701e739de8cd6c12"},"cell_type":"code","source":"train_text = train['clean_comment']\ntest_text = test['clean_comment']\nall_text = pd.concat([train_text, test_text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f280d658a89bc2e34f65dc6497537168d6d7220b"},"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n        sublinear_tf=True,\n        strip_accents='unicode',\n        tokenizer=lambda x: regex.findall(r'[^\\p{P}\\W]+', x),\n        analyzer='word',\n        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b\\\\w{,1}',\n        min_df=5,\n        norm='l2',\n        ngram_range=(1, 1),\n        max_features=31000)\n\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    token_pattern=None,\n    min_df=5,\n    norm='l2',\n    ngram_range=(2, 4),\n    max_features=23000)\n\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"088b18d8a22e93e5b9f7dd0c3dc95203dd8305ef"},"cell_type":"code","source":"train_features = hstack([train_word_features, train_char_features]).tocsr()\ndel train_word_features\ntest_features = hstack([test_word_features, test_char_features]).tocsr()\ndel test_word_features\ndel all_text\ndel test_text\ndel train_text\nprint(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e5cc9965090e36a87dcbf6153582678881ae9a2"},"cell_type":"code","source":"def pr(y_i, y):\n    p = train_features[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"333e98320b49d2208a5a9af489f4dee9f384a161"},"cell_type":"code","source":"scores = []\nsubmission = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    y = train_target.values\n    r = np.log(pr(1,y) / pr(0,y))\n    x_nb = train_features.multiply(r)\n    l = EasyEnsembleClassifier(base_estimator=LogisticRegression(C=2, solver='sag', max_iter=500))\n    n = EasyEnsembleClassifier(base_estimator=SGDClassifier(alpha=.0002, max_iter=180, penalty=\"l2\", loss='modified_huber'))\n    o = LogisticRegression(C=2, dual=True, max_iter=500)\n    p = RandomForestClassifier(criterion='gini',\n            max_depth=100, max_features=1000, max_leaf_nodes=None, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=60)  \n    m = VotingClassifier(estimators=[ ('lr', l), ('sgd', n),('lr1',o),('rdf',p)], voting='soft', weights=[0.94,1.35,0.58,0.9])\n    m.fit(x_nb, y)\n    \"\"\"For cross validation scores please uncomment the following lines of code\"\"\"\n    \n#     cv_score = np.mean(cross_val_score(\n#         m, x_nb, train_target, cv=5, scoring='roc_auc'))\n#     scores.append(cv_score)\n#     print('CV score for class {} is {}'.format(class_name, cv_score))\n# print('Total CV score is {}'.format(np.mean(scores)))\n    submission[class_name] = m.predict_proba(test_features.multiply(r))[:, 1]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8657d49aec411014a1ce57c582cb5f1d4ef0d3aa"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nend=time.time()\nprint(end-start)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}