{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e78b05e7-6704-8793-b749-57897e7182e1"
      },
      "source": [
        "##About XGBoost\n",
        "XGBoost is an advanced implementation of the gradient boosting algorithm. \n",
        "\n",
        "XGBoost includes regularisation to avoid overfitting and uses parallel computing to improve performance.\n",
        "\n",
        "Here I am using it as a standalone entity without any Exploratory Data Analysis - this is because it has an in-built routine to handle missing values. We supply a different value and pass it as a parameter then Xgboost will try different things as it encounters missing values on each node and learn which path to take for missing values in the future. \n",
        "\n",
        "XGBoost will make splits on nodes up to the max_depth parameter specified, then it will prune the tree backwards and remove splits beyond which there is no positive gain\n",
        "\n",
        "XGBoost has built-in cross-validation at each iteration of the boosting process. \n",
        "\n",
        "As a comparison please see my other notebook with EDA and random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a29fd27d-48f9-f1db-57b6-048e164d8d5c"
      },
      "source": [
        "## Beginning of routine\n",
        "We start by importing the various libraries we are going to use.\n",
        "We just need four in this example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59baa4a0-d90f-6c37-3b65-548bd0cfcae2"
      },
      "outputs": [],
      "source": [
        "import numpy as np # mathematical library including linear algebra\n",
        "import pandas as pd #data processing and CSV file input / output\n",
        "from sklearn import model_selection, preprocessing # sklearn is the machine learning library\n",
        "import xgboost as xgb # this is the extreme gradient boosting library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b9b60b37-a2ab-d3d3-5033-a9ed07d66ee3"
      },
      "source": [
        "##Read in data\n",
        "Now we read in the training and test data. We also read in the macro economic variables. We are using Pandas \"read_csv\" function for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "453ab9a5-c6ed-ffb5-7557-59cdda0589b5"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "id_test = test.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e19107bc-8b80-b97d-be5b-69633af3e4a3"
      },
      "source": [
        "## Set our response variables and perform any data modifications\n",
        "We set y_train to be the price_doc variable - our required prediction\n",
        "We then drop id, timestamp and price_doc from the training set to use in the prediction\n",
        "\n",
        "To be consistent we also drop id and timestamp from our test data set.\n",
        "\n",
        "Normally we would do both of these together by combining our train and test sets for data wrangling but in this instance this affects performance severely. \n",
        "\n",
        "The modification to the training price_doc reflects movement in house prices \n",
        "between the times in the training set versus the test set - we try to have them consistent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4143c3ec-79e0-2d3b-87e0-5594b06e57b7"
      },
      "outputs": [],
      "source": [
        "y_train = train[\"price_doc\"] * .969 + 10\n",
        "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
        "x_test = test.drop([\"id\", \"timestamp\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "972831f6-323b-cb31-eaa3-f4bded3d4970"
      },
      "source": [
        "##Fitting the model\n",
        "We run through each column in the training set and give it a label. We do this using the preprocessing.LabelEncoder() function from the sklearn library.\n",
        "This function takes a list of values and transforms non-numerical labels to numerical values. We require our labels to have numerical values for use in most algorithms and in partcular, the XGBoost algorithm.\n",
        "\n",
        "We then repeat the process for the test set - again we would normally do this on a combined test / train for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d67d70d7-cc8a-7053-c8b9-04299b74423a"
      },
      "outputs": [],
      "source": [
        "for c in x_train.columns:\n",
        "    if x_train[c].dtype == 'object':\n",
        "        lbl = preprocessing.LabelEncoder() # set an instance of the label encoder\n",
        "        lbl.fit(list(x_train[c].values)) # fit it to the values of the training set column headers\n",
        "        x_train[c] = lbl.transform(list(x_train[c].values)) # Have them transformed to encoded labels\n",
        "        \n",
        "for c in x_test.columns:\n",
        "    if x_test[c].dtype == 'object':\n",
        "        lbl = preprocessing.LabelEncoder() # set an instance of the label encoder\n",
        "        lbl.fit(list(x_test[c].values)) # fit it to the values of the test set column headers\n",
        "        x_test[c] = lbl.transform(list(x_test[c].values)) # Have them transformed to encoded labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "358f782a-cfa9-01cf-06ae-92ff33dbf5e2"
      },
      "source": [
        "#Set the parameters for xgboost as follows:\n",
        "\n",
        "##Booster parameters \n",
        "These parameters are used to optimise the algorithm in terms of both accuracy and performance.\n",
        "\n",
        "**eta: 0.05**  - the default value for this parameter is 0.3. This is similar to the learning rate (alpha) in gradient descent. \n",
        "Makes the model more robust by shrinking the weights on each step. Typical final values range from 0.01-0.2\n",
        "\n",
        "**max_depth: 5** - the default here is 6. It sets the maximum depth of a tree and is used to control over-fitting as higher depth allows the model to learn relations very specific to a particular sample. We tune it using cross-validation. Typical values range from 3-10\n",
        "\n",
        "**subsample: 0.7** - the default here is 1. It denotes the fraction of obeservations to be randomly samples for each tree. Lower values make the algorithm conservative and prevent overfitting but too small and we may get under-fitting. Typical values range from 0-1\n",
        "\n",
        "**colsample_bytree: 0.7** - the default here is 1. It denotes the fraction of columns to be randomly samples for each tree. Typical values range from 0.5-1\n",
        "\n",
        "##Learning Task Parameters\n",
        "These parameters are used to define the optimisation metric to be calculated at each step.\n",
        "\n",
        "**'eval_metric': 'rmse'** sets our evaluation metric to root mean squared error\n",
        "    This  evaluation metric used to score submissions in this competition is the log root mean squared error, however this option is not available to us within xgboost so this is the closest match.\n",
        "\n",
        "## General parameters\n",
        "**booster** - left at default by not setting it, which means we are using a tree-based model. It can also be set to use linear models.\n",
        "\n",
        "**silent: 1** - this defaults to 0 and is a binary switch. When set to 0 running messages will be printed which may help to understand the model. It can be set to 1 to suppress running messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "376534b5-54cd-ae75-0316-9ba476146980"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 5,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'silent': 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2f625037-f77c-9add-db35-30f4f4d30f9e"
      },
      "source": [
        "##Import the train and test sets to XGBoost and create a cross-validation set\n",
        "Format the train and test sets we modified above for use in xgboost \n",
        "(Dmatrix is the format required by the xgboost library)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "04c52bd1-428e-698f-0104-4be7cc7d62d6"
      },
      "outputs": [],
      "source": [
        "dtrain = xgb.DMatrix(x_train, y_train)\n",
        "dtest = xgb.DMatrix(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "74214476-3c46-d816-0292-b3c62cdff8ba"
      },
      "source": [
        "Create a cross-validation set and define the early stopping criteria.\n",
        "The num_boost_round parameter sets the number of iterations of the algorithm. \n",
        "Here it is set to just 200 to speed up the run but in practice \n",
        "we should set it to something like 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c3da6d1-0875-acac-4156-9dbc228a7a1c"
      },
      "outputs": [],
      "source": [
        "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=200, early_stopping_rounds=20,\n",
        "    verbose_eval=50, show_stdv=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d07d8ea7-ac57-f971-3b14-216b7f9e4e3e"
      },
      "source": [
        "##Train the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e23b720-ebfe-5ecf-e58f-9a5119dfd2be"
      },
      "outputs": [],
      "source": [
        "num_boost_rounds = len(cv_output)\n",
        "model = xgb.train(dict(xgb_params), dtrain, num_boost_round= num_boost_rounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "76bc2ff8-f524-d937-2f4b-4ea02bdaa5f9"
      },
      "source": [
        "##Now we can make a prediction of house prices in our test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "733fe57d-a793-c47a-f987-d688d343855c"
      },
      "outputs": [],
      "source": [
        "y_predict = model.predict(dtest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "74d744a8-ba02-111c-aaf8-4e61e24f19c4"
      },
      "source": [
        "##Store our predictions for submission\n",
        "We need to submit our results in a prescribed format. Two columns containing the id and the price.\n",
        "First format the output and then write the formatted data to csv for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a71e14b3-c842-153a-7b2b-696f0482e9f8"
      },
      "outputs": [],
      "source": [
        "output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
        "\n",
        "output.to_csv('xgbSub.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}