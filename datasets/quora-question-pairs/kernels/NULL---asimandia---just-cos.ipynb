{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95acb98a-3f03-7217-3920-dec3c054e839"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer   # TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import hstack, vstack\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import normalize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "09ac6a6c-2c83-f4c2-92cb-66a018fdffad"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4168ea6a-5930-b27e-b0da-3ae3eb13eb45"
      },
      "outputs": [],
      "source": [
        "# \u0418\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\n",
        "data = pd.read_csv('../input/train.csv')\n",
        "# \u0418\u0441\u0445\u043e\u0434\u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\n",
        "data_test_b = pd.read_csv('../input/test.csv')\n",
        "stop = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f05a0330-37d8-33b7-e970-ede6201abe27"
      },
      "outputs": [],
      "source": [
        "data.question1=data.question1.fillna('').astype(str)\n",
        "data.question2=data.question2.fillna('').astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "502a4620-010c-47a3-18f6-254d47cbb84d"
      },
      "outputs": [],
      "source": [
        "def texts_vectorization(vectorizer, texts_1, texts_2):\n",
        "    vectorizer.fit(pd.concat([texts_1.fillna(''), texts_2.fillna('')]))\n",
        "    vect_1 = vectorizer.transform(texts_1.fillna(''))\n",
        "    vect_2 = vectorizer.transform(texts_2.fillna(''))\n",
        "    return vect_1, vect_2\n",
        "def stopwords_go_away(Question):\n",
        "    global stop\n",
        "    if type(Question)==str:\n",
        "        for mark in [',', '.', '!', ':','&', '?', '-', '[', ']', '{', '}', '(', ')', '/', '^', '\"',\"'\"]:\n",
        "            Question=Question.replace(mark,' ')\n",
        "        words=[i for i in Question.lower().split() if i not in stop]\n",
        "        return ' '.join(words)\n",
        "data['question1stop']= data['question1'].apply(stopwords_go_away)\n",
        "data['question2stop']= data['question2'].apply(stopwords_go_away)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "baf95a32-5dc5-4abe-06c5-39e91e655038"
      },
      "outputs": [],
      "source": [
        "def doublewords_go_away(Question):\n",
        "    if type(Question)==str:\n",
        "        words=list(set(Question.lower().split()))\n",
        "        return ' '.join(words)\n",
        "data['question1wd']= data['question1'].apply(doublewords_go_away)\n",
        "data['question2wd']= data['question2'].apply(doublewords_go_away)\n",
        "data['question1min']= data['question1wd'].apply(doublewords_go_away)\n",
        "data['question2min']= data['question2wd'].apply(doublewords_go_away)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d06016e4-a46d-fb42-e634-c371ef9a2639"
      },
      "outputs": [],
      "source": [
        "vect = CountVectorizer(lowercase=True, analyzer='word')\n",
        "bag_product, bag_search = texts_vectorization(vect, data.question1, data.question2)\n",
        "data['cosine_similarity_queries'] = paired_cosine_distances(bag_product, bag_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f71bc1b-d99e-c598-ec63-49e30f8f80c5"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "def score_on_features(data, features):\n",
        "    for clf in [LogisticRegression()]:\n",
        "        X = data[features]\n",
        "        res = cross_val_score(clf, X, y=data.is_duplicate, scoring='neg_log_loss', cv=10)\n",
        "        print (res.mean(), res.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ad05199-11f8-b700-88eb-7249c538d610"
      },
      "outputs": [],
      "source": [
        "features_1 = ['cosine_similarity_queries']\n",
        "score_on_features(data, features_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "949df277-b023-f4ae-931f-e86e40f063f4"
      },
      "outputs": [],
      "source": [
        "tsvd = TruncatedSVD(n_components=10)\n",
        "vert_queiries = vstack([bag_product, bag_search])\n",
        "vert_svd = tsvd.fit_transform(vert_queiries)\n",
        "product_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\n",
        "features_2 = []\n",
        "for num_comp in range(product_vert_svd.shape[1]):\n",
        "    f_name = 'comp_svd_diff_{}'.format(num_comp)\n",
        "    features_2.append(f_name)\n",
        "    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29ccbac6-2f89-022e-4bac-e37b91e7c208"
      },
      "outputs": [],
      "source": [
        "tsvd = TruncatedSVD(n_components=10)\n",
        "side_queries = hstack([bag_product, bag_search])\n",
        "svd_features = tsvd.fit_transform(side_queries)\n",
        "features_3 = []\n",
        "for num_comp in range(svd_features.shape[1]):\n",
        "    f_name = 'svd_paired_feat_{}'.format(num_comp)\n",
        "    features_3.append(f_name)\n",
        "    data[f_name] = svd_features[:, num_comp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a3f2e09-a02f-38f8-8b85-c90fd5d9eddb"
      },
      "outputs": [],
      "source": [
        "data['len1'] = data['question1'].fillna(0).str.len().astype(np.float32)\n",
        "data['len2'] = data['question2'].fillna(0).str.len().astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7017747-1672-67db-5b05-61591b79e767"
      },
      "outputs": [],
      "source": [
        "data['abs_diff_len1_len2'] = np.abs(data['len1'] - data['len2'])\n",
        "features_4=['abs_diff_len1_len2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40d0535b-54d3-9095-a1a1-789766b4025b"
      },
      "outputs": [],
      "source": [
        "data.abs_diff_len1_len2=data.abs_diff_len1_len2.fillna(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "658f8049-2efe-7d6d-580d-1bbe308a722a"
      },
      "outputs": [],
      "source": [
        "score_on_features(data, features_1+features_2+features_3+features_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c37b5d0-3688-8e4b-6786-0f513d5aec49"
      },
      "outputs": [],
      "source": [
        "tsvd = TruncatedSVD(n_components=15)\n",
        "bag_product.shape, bag_search.shape\n",
        "vert_queiries = vstack([bag_product, bag_search])\n",
        "vert_queiries.shape\n",
        "vert_svd = tsvd.fit_transform(vert_queiries)\n",
        "product_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\n",
        "product_vert_svd.shape, product_vert_svd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5489941-eb4a-f095-fabb-44fe9641ed56"
      },
      "outputs": [],
      "source": [
        "features_2 = []\n",
        "for num_comp in range(product_vert_svd.shape[1]):\n",
        "    f_name = 'comp_svd_diff_{}'.format(num_comp)\n",
        "    features_2.append(f_name)\n",
        "    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2efd966c-80b7-18f5-87cc-99046c4e887b"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f95ec3d9-28c7-4f6d-7e79-d8bdc4b6f482"
      },
      "outputs": [],
      "source": [
        "tsvd = TruncatedSVD(n_components=10)\n",
        "side_queries = hstack([bag_product, bag_search])\n",
        "svd_features = tsvd.fit_transform(side_queries)\n",
        "features_3 = []\n",
        "for num_comp in range(svd_features.shape[1]):\n",
        "    f_name = 'svd_paired_feat_{}'.format(num_comp)\n",
        "    features_3.append(f_name)\n",
        "    data[f_name] = svd_features[:, num_comp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d76a7fd-6d18-e3b8-c7d2-af057d91f4d4"
      },
      "outputs": [],
      "source": [
        "score_on_features(data, features_1+features_2+features_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f59fbac4-c6bd-57f2-ee35-68dfafb860be"
      },
      "outputs": [],
      "source": [
        "data['len1'] = data['question1'].str.len().astype(np.float32)\n",
        "data['len2'] = data['question2'].str.len().astype(np.float32)\n",
        "data['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0c014c6-8ebc-a5eb-3563-7cb2505d2575"
      },
      "outputs": [],
      "source": [
        "data['len1'] = data['question1'].str.len().astype(np.float32)\n",
        "data['len2'] = data['question2'].str.len().astype(np.float32)\n",
        "data['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))\n",
        "data['len1min'] = data['question1min'].str.len().astype(np.float32)\n",
        "data['len2min'] = data['question2min'].str.len().astype(np.float32)\n",
        "data['abs_diff_len1_len2min'] = (np.abs(data['len1min'] - data['len2min']))\n",
        "features_4=['abs_diff_len1_len2','abs_diff_len1_len2min' ]\n",
        "score_on_features(data, features_1+features_2+features_3+features_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4c6def4-86b6-074b-b8b7-14b3e6a377ec"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_similarity_score\n",
        "def jaccard(X, Y):\n",
        "    res=np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]):\n",
        "        X1=set(X[i].split(' '))\n",
        "        Y1=set(Y[i].split(' '))\n",
        "        if len((X1 | Y1) - (X1 &Y1))==0:\n",
        "            res[i]=1.0\n",
        "        else:\n",
        "            res[i]=1.0*len(X1 &Y1)/len((X1 | Y1)) #- (X1 &Y1)\n",
        "    return res\n",
        "data['jaccard']=jaccard(data.question1.values, data.question2.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bceb30ac-b8e9-7291-bd06-e729b12ef12a"
      },
      "outputs": [],
      "source": [
        "features_5=['jaccard' ]\n",
        "score_on_features(data, features_1+features_2+features_3+features_4+features_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb716e7f-4443-e2a3-9fa1-67f4c87dfe0a"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f659a00-0c5b-f399-512e-0f262e888b57"
      },
      "outputs": [],
      "source": [
        "X=data.drop('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b92d7b0-64c2-0c00-99c2-e20eecf14b6f"
      },
      "outputs": [],
      "source": [
        "model=LogisticRegression().fit(, data['is_duplicate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "190d4ce2-c477-30db-2f81-13f4b0a83058"
      },
      "outputs": [],
      "source": [
        "data=data_test_b\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1505776a-4816-33bf-bd58-c13e89127e6c"
      },
      "outputs": [],
      "source": [
        "def texts_vectorization(vectorizer, texts_1, texts_2):\n",
        "    vectorizer.fit(pd.concat([texts_1.fillna(''), texts_2.fillna('')]))\n",
        "    vect_1 = vectorizer.transform(texts_1.fillna(''))\n",
        "    vect_2 = vectorizer.transform(texts_2.fillna(''))\n",
        "    return vect_1, vect_2\n",
        "def stopwords_go_away(Question):\n",
        "    global stop\n",
        "    if type(Question)==str:\n",
        "        for mark in [',', '.', '!', ':','&', '?', '-', '[', ']', '{', '}', '(', ')', '/', '^', '\"',\"'\"]:\n",
        "            Question=Question.replace(mark,' ')\n",
        "        words=[i for i in Question.lower().split() if i not in stop]\n",
        "        return ' '.join(words)\n",
        "data.question1= data['question1'].apply(stopwords_go_away)\n",
        "data.question2= data['question2'].apply(stopwords_go_away)\n",
        "def doublewords_go_away(Question):\n",
        "    if type(Question)==str:\n",
        "        words=list(set(Question.lower().split()))\n",
        "        return ' '.join(words)\n",
        "data.question1= data['question1'].apply(doublewords_go_away)\n",
        "data.question2= data['question2'].apply(doublewords_go_away)\n",
        "vect = CountVectorizer(lowercase=True, analyzer='word')\n",
        "bag_product, bag_search = texts_vectorization(vect, data.question1, data.question2)\n",
        "data['is_duplicate'] = paired_cosine_distances(bag_product, bag_search)\n",
        "data['len1'] = data['question1'].str.len().astype(np.float32)\n",
        "data['len2'] = data['question2'].str.len().astype(np.float32)\n",
        "data['abs_diff_len1_len2'] = (np.abs(data['len1'] - data['len2']))\n",
        "data['len1min'] = data['question1min'].str.len().astype(np.float32)\n",
        "data['len2min'] = data['question2min'].str.len().astype(np.float32)\n",
        "data['abs_diff_len1_len2min'] = (np.abs(data['len1min'] - data['len2min']))\n",
        "features_4=['abs_diff_len1_len2','abs_diff_len1_len2min' ]\n",
        "data['jaccard']=jaccard(data.question1.values, data.question2.values)\n",
        "features_5=['jaccard' ]\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "vert_queiries = vstack([bag_product, bag_search])\n",
        "vert_svd = tsvd.fit_transform(vert_queiries)\n",
        "product_vert_svd, search_vert_svd = vert_svd[:bag_product.shape[0], :], vert_svd[bag_product.shape[0]:, :]\n",
        "features_2 = []\n",
        "for num_comp in range(product_vert_svd.shape[1]):\n",
        "    f_name = 'comp_svd_diff_{}'.format(num_comp)\n",
        "    features_2.append(f_name)\n",
        "    data[f_name] = np.abs(product_vert_svd[:, num_comp] - search_vert_svd[:, num_comp])\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "side_queries = hstack([bag_product, bag_search])\n",
        "svd_features = tsvd.fit_transform(side_queries)\n",
        "features_3 = []\n",
        "for num_comp in range(svd_features.shape[1]):\n",
        "    f_name = 'svd_paired_feat_{}'.format(num_comp)\n",
        "    features_3.append(f_name)\n",
        "    data[f_name] = svd_features[:, num_comp]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c6a9539-7866-87f0-afc2-9c86ff180b1b"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa5ea6ba-8f9f-a3bb-99c2-5f6c2caffc49"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad38dd93-0e10-45e1-6b9c-48edeca222f9"
      },
      "outputs": [],
      "source": [
        "data['is_duplicate']=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f287820-d9a2-f0e7-16f0-3e2c04be6657"
      },
      "outputs": [],
      "source": [
        "X=data.fillna(0).drop([''])\n",
        "clf= LogisticRegression()\n",
        "data.is_duplicate=model.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "082c35e8-2e71-128f-a2c0-9b1c3a93f843"
      },
      "outputs": [],
      "source": [
        "data.index=data.test_id\n",
        "data.drop(['question1', 'question2', 'test_id'], axis=1).to_csv('submition1')"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}