{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval \nfrom hyperopt.pyll.base import scope","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f62ff72891faad7b626f601fd9ed31d5ebe15a9d","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain['log_target'] = np.log1p(train.target)\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"010d350a36f2f823931166a68865d1d93348959e","collapsed":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b87083b883315b20cf9dc7cd1bf509f289d32f54"},"cell_type":"markdown","source":"All the given features are numeric and no categorical. All the features are anonymized.                 \nThere are approx 5000 features present and so feature selection becomes a very important exercise.            "},{"metadata":{"trusted":true,"_uuid":"236f45e224180546529073edc02e7a26696f9aaf","collapsed":true},"cell_type":"code","source":"X_cols = [col for col in train.columns if col not in ['ID','target','log_target']]\nprint(len(X_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63bd04cb342f5507383ff47349653be3e31c5e59","collapsed":true},"cell_type":"code","source":"# Converting features into log transformation\nfeats_to_convert = []\nfor col in X_cols:\n    diff = train[col].max() - train[col].min()\n    if diff>1000:\n        feats_to_convert.append(col)\nprint(len(feats_to_convert))\n\ntrain[feats_to_convert] = np.log1p(train[feats_to_convert].values)\ntest[feats_to_convert] = np.log1p(test[feats_to_convert].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f177b23c17fbff765d600b253c7c899a279fb34b"},"cell_type":"markdown","source":"Lasso for Feature Selection          \n**TODO** Finding best alpha value by Grid-Search based tuning or Hyperopt"},{"metadata":{"trusted":true,"_uuid":"1237a169019bb6ab16536b22e710086737790735","collapsed":true},"cell_type":"code","source":"lasso_mod = Lasso(alpha=0.05, max_iter=1000, fit_intercept=True, normalize=False, random_state=42)\nlasso_mod.fit(X=train[X_cols].values, y=train.log_target.values)\nimp_feats_indexes = np.nonzero(lasso_mod.coef_)[0]\nprint(imp_feats_indexes)\nimp_feats = np.array(X_cols)[imp_feats_indexes]\nprint('Number of important features selected by lasso:', len(imp_feats))\nprint('Important features are:', imp_feats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c27a92d06735bee50cd6dadc628badcf2352d4c"},"cell_type":"markdown","source":"Using HyperOpt for Hyper-Parameter Selection"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afd630a9b6ab32a61f6d9a25242313b3bf3fcdb9"},"cell_type":"code","source":"def model_metrics(y_test, preds, scores):\n    scores['rmsle'].append(np.sqrt(mean_squared_error(y_test, preds)))\n    #scores['mae'].append(mean_absolute_error(y_test, preds))\n    return scores\n\ndef get_space(clf_choice):\n    if clf_choice=='LGB':\n        lgb_space ={'num_leaves': scope.int(hp.quniform('num_leaves', 50, 200, 1)),\n                    'learning_rate': 0.1, #hp.uniform('learning_rate', 0.02, 0.05),\n                    'max_bin': scope.int(hp.quniform('max_bin', 300, 500, 1)),\n                    'num_boost_round': scope.int(hp.quniform('num_boost_round', 100, 2000, 1)),\n                    'max_depth': scope.int(hp.quniform('max_depth', 3, 10, 1)),\n                    'min_child_samples': scope.int(hp.quniform('min_child_samples', 1, 100, 1)), \n                    'subsample': hp.uniform('subsample', 0.5, 1.0), \n                    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0)}\n        return lgb_space\n    \ndef lgb_train(space, X_train, y_train, X_test):\n    lgb_params ={'task':'train', 'boosting_type':'gbdt', 'objective':'regression', 'metric': {'rmse'},\n                 'num_leaves': space['num_leaves'], 'learning_rate': space['learning_rate'], 'max_bin': space['max_bin'], \n                 'max_depth': space['max_depth'], 'min_child_samples':space['min_child_samples'], 'subsample': space['subsample'],\n                 'colsample_bytree': space['colsample_bytree'], 'nthread':4, 'verbose': 0}\n    lgbtrain = lgb.Dataset(X_train, label=y_train)\n    lgbtrain.construct()\n    lgb_model = lgb.train(lgb_params, lgbtrain, num_boost_round=space['num_boost_round'])\n    preds = lgb_model.predict(X_test, num_iteration=space['num_boost_round'])\n    return lgb_model, preds\n\ndef hyperopt_param_tuning(space, kf, clf_choice, trainX, trainY, max_evals):\n    \n    def objective(space):\n        print('Space:', space)\n        scores = {'rmsle':[]} #, 'mae':[]}\n        for train_index, test_index in kf.split(trainX):\n            X_train, X_test, y_train, y_test = trainX[train_index], trainX[test_index], trainY[train_index], trainY[test_index]\n            lgb_model, preds = lgb_train(space, X_train, y_train, X_test)\n            scores = model_metrics(y_test, preds, scores)\n            print('scores', scores)\n        loss = np.array(scores['rmsle']).mean()\n        print('RMSLE:', loss, '\\n\\n')\n        return{'loss':loss, 'status': STATUS_OK, 'scores':scores}\n    \n    trials = Trials()\n    # Run the hyperparameter search\n    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n    # Get the values of the optimal parameters\n    best_params = space_eval(space, best)\n    return best_params, trials\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0bf7325eb0be38e78fccad2907a69d808a36d35","collapsed":true},"cell_type":"code","source":"trainX = train[imp_feats].values\ntrainY = train['log_target'].values\n# 5-fold CV\nkf = KFold(n_splits=10, random_state=42)    \nlgb_space = get_space('LGB')\n# Hyperparameter tuning\nlgb_best_params, trials = hyperopt_param_tuning(lgb_space, kf, 'LGB', trainX, trainY, 100)\nprint('Best LGB params for our task:', lgb_best_params)\n#print(trials)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f409b0ec013793ea4d62b4e77bf0e3dc98aed4f","collapsed":true},"cell_type":"code","source":"# Train on whole data using best params that we got from HyperOpt and lower learning_rate     \ntestX = test[imp_feats].values\n#lgb_best_params['learning_rate'] = 0.001\n#lgb_best_params['num_boost_round'] = lgb_best_params['num_boost_round'] * 80 \nlgb_model, preds = lgb_train(lgb_best_params, trainX, trainY, testX)\nsub = test[['ID']]\nsub['target'] = np.expm1(preds)\nprint(sub.head(), '\\n')\nprint(sub.describe())\nsub.to_csv('first_model_prediction.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d67ff620300e8e8ab4dad407c4f9c45fd22c0b55"},"cell_type":"markdown","source":"**TODO**          \n* More Feature Selection method like Boruta or Correaltion based FS.         \n* Create feature interactions of some important features.                "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7e95454b8bf6c388d8a48b0f5ae263f7e37fae70"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}