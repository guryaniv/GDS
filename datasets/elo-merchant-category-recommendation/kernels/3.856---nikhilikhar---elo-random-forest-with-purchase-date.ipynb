{"cells":[{"metadata":{"_uuid":"a418fd0fb8aa0082eb1733a2e651a2b0e98ea458"},"cell_type":"markdown","source":"https://www.kaggle.com/guntherthepenguin/fastai-based-mixed-input-model-lb-3-8\nhttps://www.kaggle.com/delayedkarma/let-s-add-some-features-for-the-national-holidays"},{"metadata":{"trusted":true,"_uuid":"272d430596a11b125241b7253c8db09755052576","scrolled":true},"cell_type":"code","source":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa88c265b67eda9860fb6427f145d76603e6177e"},"cell_type":"code","source":"from IPython.lib.deepreload import reload as dreload\nimport PIL, os, numpy as np, math, collections, threading, json, bcolz, random, scipy, cv2\nimport pandas as pd, pickle, sys, itertools, string, sys, re, datetime, time, shutil, copy\nimport seaborn as sns, matplotlib\nimport IPython, graphviz, sklearn_pandas, sklearn, warnings, pdb\nimport contextlib\nfrom abc import abstractmethod\nfrom glob import glob, iglob\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom itertools import chain\nfrom functools import partial\nfrom collections import Iterable, Counter, OrderedDict\nfrom isoweek import Week\nfrom pandas_summary import DataFrameSummary\nfrom IPython.lib.display import FileLink\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom sklearn import metrics, ensemble, preprocessing\nfrom operator import itemgetter, attrgetter\nfrom pathlib import Path\nfrom distutils.version import LooseVersion\n\nfrom matplotlib import pyplot as plt, rcParams, animation\nfrom ipywidgets import interact, interactive, fixed, widgets\nmatplotlib.rc('animation', html='html5')\nnp.set_printoptions(precision=5, linewidth=110, suppress=True)\n\n\n\nfrom ipykernel.kernelapp import IPKernelApp\ndef in_notebook(): return IPKernelApp.initialized()\n\ndef in_ipynb():\n    try:\n        cls = get_ipython().__class__.__name__\n        return cls == 'ZMQInteractiveShell'\n    except NameError:\n        return False\n\nimport tqdm as tq\nfrom tqdm import tqdm_notebook, tnrange\n\ndef clear_tqdm():\n    inst = getattr(tq.tqdm, '_instances', None)\n    if not inst: return\n    try:\n        for i in range(len(inst)): inst.pop().close()\n    except Exception:\n        pass\n\nif in_notebook():\n    def tqdm(*args, **kwargs):\n        clear_tqdm()\n        return tq.tqdm(*args, file=sys.stdout, **kwargs)\n    def trange(*args, **kwargs):\n        clear_tqdm()\n        return tq.trange(*args, file=sys.stdout, **kwargs)\nelse:\n    from tqdm import tqdm, trange\n    tnrange=trange\n    tqdm_notebook=tqdm\n\n\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nfrom sklearn.ensemble import forest\nfrom sklearn.tree import export_graphviz\n\n\ndef set_plot_sizes(sml, med, big):\n    plt.rc('font', size=sml)          # controls default text sizes\n    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=sml)    # legend fontsize\n    plt.rc('figure', titlesize=big)  # fontsize of the figure title\n\ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\"\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\n\ndef combine_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,\n              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):\n    years = np.asarray(years) - 1970\n    months = np.asarray(months) - 1\n    days = np.asarray(days) - 1\n    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',\n             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')\n    vals = (years, months, days, weeks, hours, minutes, seconds,\n            milliseconds, microseconds, nanoseconds)\n    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)\n               if v is not None)\n\ndef get_sample(df,n):\n    \"\"\"\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\nimport workalendar\nfrom workalendar.america import Brazil\ncal = Brazil()\nfrom datetime import timedelta\nholidays_delta = {}\ndef get_working_days_delta(begin, end):\n    '''\n    Get working days between two dates\n    '''\n    if begin in holidays_delta:\n        return holidays_delta[begin]\n    \n    try:\n        \n        sign = 1\n        if begin > end:\n            begin, end = end, begin\n            sign = -1\n        days = 0\n        temp_day = begin\n        while temp_day <= end:\n            if cal.is_working_day(temp_day):\n                days += 1\n            temp_day = temp_day + timedelta(days=1)\n        delta = days * sign\n        holidays_delta[begin] = delta\n    except Exception:\n        holidays_delta[begin] = 0\n    return holidays_delta[begin] \n\ndef add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\t\n    \"\"\"\n    \"\"\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n    try:\n        df[targ_pre + 'Holiday_Elapsed'] = fld.apply(\n            lambda x: get_working_days_delta(x.date(),cal.holidays(int(x.year))[0][0])\n        )\n    except Exception:\n        df[targ_pre + 'Holiday_Elapsed'] = 0\n    \n    if drop: df.drop(fldname, axis=1, inplace=True)\n\ndef is_date(x): return np.issubdtype(x.dtype, np.datetime64)\n\ndef train_cats(df):\n    \"\"\"\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):\n    \"\"\"\n    \"\"\"\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n\ndef fix_missing(df, col, name, na_dict):\n    \"\"\" \n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef numericalize(df, col, name, max_n_cat):\n    \"\"\" \n    \"\"\"\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1\n\ndef scale_vars(df, mapper):\n    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    \"\"\" \n    \"\"\"\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n\ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n\ndef get_nn_mappers(df, cat_vars, contin_vars):\n    # Replace nulls with 0 for continuous, \"\" for categorical.\n    for v in contin_vars: df[v] = df[v].fillna(df[v].max()+100,)\n    for v in cat_vars: df[v].fillna('#NA#', inplace=True)\n\n    # list of tuples, containing variable and instance of a transformer for that variable\n    # for categoricals, use LabelEncoder to map to integers. For continuous, standardize\n    cat_maps = [(o, LabelEncoder()) for o in cat_vars]\n    contin_maps = [([o], StandardScaler()) for o in contin_vars]\n    return DataFrameMapper(cat_maps).fit(df), DataFrameMapper(contin_maps).fit(df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path='../input/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00c46ef3366002091c18c9084ef4f8b0aeaaa7b3"},"cell_type":"code","source":"new_transactions = pd.read_csv(f'{path}new_merchant_transactions.csv', parse_dates=['purchase_date'])\nnew_transactions.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d3551386a1529cd9cd8d025836a3454e3e2db16"},"cell_type":"code","source":"new_transactions['authorized_flag'] = new_transactions['authorized_flag']=='Y'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"196b41759d670cca2d766bfb09f83f104efbb4d2"},"cell_type":"code","source":"add_datepart(new_transactions, 'purchase_date') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fe4fa54e102a7db424388d34fc448e1aba34c2b6"},"cell_type":"code","source":"new_transactions.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a524cd24bf3fc1c50cf58c3972f1b212fa13642"},"cell_type":"code","source":"def aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max'],\n#         'purchase_date': ['max','min'],\n        'purchase_Year':['min', 'max'],\n        'purchase_Month':['min', 'max'],\n        'purchase_Week':['min', 'max'],\n        'purchase_Day':['min', 'max'],\n        'purchase_Dayofweek':['min', 'max'],\n        'purchase_Dayofyear':['min', 'max'],\n        'purchase_Is_month_end':['min', 'max'],\n        'purchase_Is_month_start':['min', 'max'],\n        'purchase_Is_quarter_end':['min', 'max'],\n        'purchase_Is_quarter_start':['min', 'max'],\n        'purchase_Is_year_end':['min', 'max'],\n        'purchase_Is_year_start':['min', 'max'],\n        'purchase_Elapsed':['min', 'max'],\n        'purchase_Holiday_Elapsed':['nunique'],\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n#     print(agg_new_trans.columns)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    print(agg_new_trans.columns)\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_transactions)\nnew_trans.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8909a87dd366afb1218b7541cd351a735a367af6","scrolled":true},"cell_type":"code","source":"new_trans.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a524cd24bf3fc1c50cf58c3972f1b212fa13642","scrolled":true},"cell_type":"code","source":"del new_transactions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d98e1277cad1d8d7cbf4fbce88d7567010cc56b3"},"cell_type":"code","source":"historical_transactions = pd.read_csv('../input/historical_transactions.csv',parse_dates=['purchase_date'])\nhistorical_transactions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11f02fc8752e241850e0564d46d9901cc1458632"},"cell_type":"code","source":"historical_transactions['authorized_flag'] = historical_transactions['authorized_flag']=='Y'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a9f9be6be3e80d7518b2ceee84f53c006eedb62"},"cell_type":"code","source":"add_datepart(historical_transactions, 'purchase_date') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f5d75bc3d47dab9d1f30bc261e1193ff670f8cf"},"cell_type":"code","source":"def aggregate_historical_transactions(history):\n    \n#     history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n#                                       astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n#         'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max'],\n#         'purchase_date': ['max','min'],\n        'purchase_Year':['min', 'max'],\n        'purchase_Month':['min', 'max'],\n        'purchase_Week':['min', 'max'],\n        'purchase_Day':['min', 'max'],\n        'purchase_Dayofweek':['min', 'max'],\n        'purchase_Dayofyear':['min', 'max'],\n        'purchase_Is_month_end':['min', 'max'],\n        'purchase_Is_month_start':['min', 'max'],\n        'purchase_Is_quarter_end':['min', 'max'],\n        'purchase_Is_quarter_start':['min', 'max'],\n        'purchase_Is_year_end':['min', 'max'],\n        'purchase_Is_year_start':['min', 'max'],\n        'purchase_Elapsed':['min', 'max'],\n        'purchase_Holiday_Elapsed':['nunique'],\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(historical_transactions)\nhistory.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f5d75bc3d47dab9d1f30bc261e1193ff670f8cf"},"cell_type":"code","source":"del historical_transactions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cde0aaa9db4eb265f626510e7415e78d051c962"},"cell_type":"code","source":"def read_data(input_file):\n    df = pd.read_csv(input_file)\n    add_datepart(df,'first_active_month')\n    return df\ntrain = read_data('../input/train.csv')\ntest = read_data('../input/test.csv')\n\ntarget='target'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"863b5a324fc6944f95508d1ce900ea6037e7fbc3"},"cell_type":"code","source":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22167f5a0795b1c7aa51e9e972421956f84bdca8"},"cell_type":"code","source":"train=train.set_index('card_id')\ntest=test.set_index('card_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ef717143098a119bbee22e5d92fd48e1d17ad6"},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05ebac39e6e97e81c1218820c1f80c447b306817"},"cell_type":"code","source":"cat_vars = [col  for col in train.columns if('feature' in col or 'first_active_month' in col)]\ncontin_vars = [col  for col in train.columns if ('feature' not in col and 'first_active_month' not in col) and target not in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e0051d1c6ca5604d2dc0a505a9fae627a37750a"},"cell_type":"code","source":"train = train[cat_vars+contin_vars+[target]].copy()\nn = len(train); n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2f4d90fbfc83bd63b3c6d45c9f8053d9d26bccf"},"cell_type":"code","source":"test[target] = 0\ntest = test[cat_vars+contin_vars+[target]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85a1ce6f2b320728f7d5990c62226856cf6a5497"},"cell_type":"code","source":"for v in cat_vars: train[v] = train[v].astype('category').cat.as_ordered()\napply_cats(test, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a505d3780ecc69b71fea1b05847f301fe57d18ce"},"cell_type":"code","source":"for v in contin_vars:\n    train[v] = train[v].fillna(0).astype('float32')\n    test[v] = test[v].fillna(0).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b301b16938b07d50a9de17d2fd64f35c54f2cae5"},"cell_type":"code","source":"df_trn, y, nas, mapper = proc_df(train, target, do_scale=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1dda2395f340cb0a88c340599f12a949c5a76f6"},"cell_type":"code","source":"n=len(df_trn);n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d605c88c6f03c3c0c771654e870acac4e17e4160"},"cell_type":"code","source":"df_test, _, nas, mapper = proc_df(test, target, do_scale=True, \n                                  mapper=mapper, na_dict=nas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ab5a1a3005e539cc0516557accc9c141eedd746"},"cell_type":"code","source":"def split_vals(a,n): \n    return a[:n].copy(), a[n:].copy()\n\n\n\nn_valid = int(len(df_trn) * .25)\nn_trn = len(df_trn)-n_valid\n# raw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4fe610aa2ffdcae19b9d5abfa4f528c871c05b4"},"cell_type":"code","source":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65f054b239cb729348764e699e7fe5a44f8a2a41"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41e18cd899eedf77e78205a3c16a50196a817eca"},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b1e857246fac5cfa2a3fbc3c40f1019d061ffec"},"cell_type":"code","source":"draw_tree(m.estimators_[0], df_trn, precision=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ed4b3ef5f165c8b750b644a6f196ed04614cb55"},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=1, bootstrap=False, n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"411489e1e9ffdae5ef10eaa308bdebb0df49b427"},"cell_type":"code","source":"m = RandomForestRegressor(n_estimators=150, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ea041f673ab0bb00561dd905a4094dc4f035edd"},"cell_type":"code","source":"prediction = m.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b46cee4fb6e85bce1062cd9dd597fdb9079df03b"},"cell_type":"code","source":"from datetime import datetime\nids = df_test.index.values\nvals = prediction\nsub = pd.DataFrame({'card_id': ids, 'target': vals})\nsub.to_csv(\"submission-elo-%s.csv\"%datetime.now().date(), index = False, header = True)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b41fae30f618637013b4b67882837cac8252f5a"},"cell_type":"code","source":"np.max(sub), np.min(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45f8da079f85b869bf8701ab97851b1e9718aeb1"},"cell_type":"code","source":"from IPython.display import FileLink, FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11f328e2a6950ee59b118df2b2ac2e53f765c2ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9bda5ef6a5d762fe08380e7f0b70f93563b514b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}