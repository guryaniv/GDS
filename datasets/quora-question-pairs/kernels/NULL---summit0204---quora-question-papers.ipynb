{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\").fillna(\"\")\ndf_test = pd.read_csv(\"../input/test.csv\")\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c8d13dd6bcd66f015b43ffb422f57c25ba00f9d"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d071a91adfcf12ffbf187e034212f5f1f184eb"},"cell_type":"code","source":"df_train['is_duplicate'].value_counts()\ndf_train.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a4ba3f164fe3fbadb005f3c3ddb4989ab0f920f"},"cell_type":"code","source":"df_train['q1len']=df_train['question1'].str.len()\ndf_train['q2len']=df_train['question2'].str.len()\ndf_train['q1_n_words'] = df_train['question1'].apply(lambda row: len(row.split(\" \")))\ndf_train['q2_n_words'] = df_train['question2'].apply(lambda row: len(row.split(\" \")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210307516e61511a018b60bd2ff129752b626978"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"328045633548a41ff13e564fca0cb271acbd9d09"},"cell_type":"code","source":"def normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n\ndf_train['word_share'] = df_train.apply(normalized_word_share, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"801550b94d0874038c56916d3fc136f905343d65"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af7d4ae04d948b9650cd698ece2ba1cc8710ddcb"},"cell_type":"markdown","source":"Lets create TF_IDF using sklearn's TfidfVectorizer to compute weight"},{"metadata":{"trusted":true,"_uuid":"eb9f73ebd77d0efa9336ce2b5b089c35f5d69dbd"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"557b8395785febd390eb39fbc5749ab4ea674da9"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# define tfidf vectorizer \ntfidf = TfidfVectorizer(analyzer = 'word',\n                        stop_words = 'english',\n                        lowercase = True,\n                        max_features = 300,\n                        norm = 'l1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2e9f24177dbdb812615cae013e04e7a0c424515"},"cell_type":"code","source":"BagOfWords = pd.concat([df_train.question1, df_train.question2], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb444de01e442beedf3d2faf062b073640fe5d55"},"cell_type":"code","source":"tfidf.fit(BagOfWords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d835c6e99c12cbfd199aa0a1d5011c46c357d17"},"cell_type":"code","source":"df_train['q1_tfidf'] = tfidf.transform(df_train.question1)\ndf_train['q2_tfidf'] = tfidf.transform(df_train.question2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e6112a2be7cf796dfe37fa4e5c9dda3936d4be1"},"cell_type":"code","source":"df_train['q1_tfidf'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65087aafc84322b726fd61ed02d98ffd1e69e7b6"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler().fit(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\n\nX = scaler.transform(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\ny = df_train['is_duplicate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9cb1bb356a155187882f1110b3d3d718544ed30"},"cell_type":"code","source":"clf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd5d1f0d0be176552bfd01a384aec9049309ac45"},"cell_type":"code","source":"print(cv.best_params_)\nprint(cv.best_estimator_.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe47f6489919d534fc1b28eada0cc91242123e44"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nretrained = cv.best_estimator_.fit(X, y)\ny_pred = retrained.predict(X)\nconfusion_matrix = confusion_matrix(df_train['is_duplicate'],y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da201b8f7585f7006763c34d5bb3c0bb62473703"},"cell_type":"code","source":"retrained.score(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"554b4b00661cc569334e682e2f0cca500e86e47d"},"cell_type":"code","source":"ques = pd.concat([df_train[['question1', 'question2']], \\\n        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\nques.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa06582407b61999c16f071659da0f6b4ac922e"},"cell_type":"code","source":"ques.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bcbd8187d547119806210c8cdba254909701f1a"},"cell_type":"code","source":"from collections import defaultdict\nq_dict= defaultdict(set)\nfor i in range(ques.shape[0]):\n    q_dict[ques.question1[i]].add(ques.question2[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e22bc4ead1eefeca884be686e3ef1b196f01706"},"cell_type":"code","source":"def q1_freq(row):\n    return(len(q_dict[row['question1']]))\n    \ndef q2_freq(row):\n    return(len(q_dict[row['question2']]))\n    \ndef q1_q2_intersect(row):\n    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n\ndf_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\ndf_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\ndf_train['q2_freq'] =df_train.apply(q2_freq, axis=1, raw=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6733c656f0e10e8a75970872b912df6bd6fa8382"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a323232c562f13caf07a2705dd793acbdae6c1c4"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler().fit(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share','q1_q2_intersect','q1_freq','q2_freq']])\n\nX = scaler.transform(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share','q1_q2_intersect','q1_freq','q2_freq']])\ny = df_train['is_duplicate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"573090f6d1e415ebd555b981de7b6d1337c3e5d6"},"cell_type":"code","source":"clf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10c0cddae942921fea5d28cedefe066a3e49a0ab"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nretrained = cv.best_estimator_.fit(X, y)\ny_pred = retrained.predict(X)\nconfusion_matrix = confusion_matrix(df_train['is_duplicate'],y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f46dc3e5a0f4d4aceb075514cd40dd7484c16379"},"cell_type":"code","source":"retrained.score(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38fa2dab2e13bf689c61101196a17ea304d21618"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}