{"nbformat": 4, "metadata": {"language_info": {"version": "3.6.3", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["**In this notebook, three models (NB-SVM, LSTM, LR) are trained. The final submission are the weighted average of the results of three models.**"], "metadata": {"_cell_guid": "214f8950-536b-4dd6-82d7-7aaaa86263c7", "_uuid": "108f96aa9bd403e5e980222493c255f4f64c115c"}}, {"execution_count": null, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "dd9cf18c-3a50-4f22-96f6-999f7d66d37e", "_uuid": "23de149637d198065ed3e2842195aa779c6dead2"}}, {"cell_type": "markdown", "source": ["Now let's have a look at the three data files to get a sense what they look like.\n"], "metadata": {"_cell_guid": "6b9d54be-45e1-4335-a383-c9e4060748f7", "_uuid": "62e6a26c7536d50b61f74f4dbde876e3aa237221"}}, {"execution_count": null, "cell_type": "code", "source": ["sample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n", "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n", "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n", "train.head(10)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "9dc72003-92b3-4975-8036-204f2e5f7677", "_uuid": "f7b086c527b49c883d120418b7c49d1b3ba11755"}}, {"cell_type": "markdown", "source": ["Have a look at the lengths of the comments. I replace empty comments with \"unknown\" to avoid errors."], "metadata": {"_cell_guid": "920b06c5-5acc-4db1-8f3e-fcef8704a7f3", "_uuid": "1ea8696779713ab9a2750a7e39ac36cd3422bf1c"}}, {"execution_count": null, "cell_type": "code", "source": ["lens = train.comment_text.str.len()\n", "lens.hist()\n", "sorted(lens.tolist())[:10]\n", "COMMENT = 'comment_text'\n", "train[COMMENT].fillna(\"unknown\", inplace=True)\n", "test[COMMENT].fillna(\"unknown\", inplace=True)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "bc54eb52-e200-455c-a6be-4a0be7813b76", "_uuid": "8d85a0a27045fe48bf9dd85cc175dd1e397170b7"}}, {"execution_count": null, "cell_type": "code", "source": ["import re, string\n", "re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n", "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "5b85c35f-3910-49bb-9d79-cf67f2df68d1", "_uuid": "89c99351b07c5c3f2adb2d4c4e0e6d7d53006dc7"}}, {"execution_count": null, "cell_type": "code", "source": ["n = train.shape[0]\n", "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n", "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n", "               smooth_idf=1, sublinear_tf=1 )\n", "trn_term_doc = vec.fit_transform(train[\"comment_text\"])\n", "test_term_doc = vec.transform(test[\"comment_text\"])"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "2f8c6fc2-a632-4245-9ab7-9da78da7dfb3", "_uuid": "bee54c73c3ab278bd56cbed12e2e02b0c867ff09"}}, {"execution_count": null, "cell_type": "code", "source": ["trn_term_doc, test_term_doc"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "6c48c71f-fad2-4b00-93cf-1f4dd712d245", "_uuid": "1799b3d013fb0f285b06f0ec88eb777f627febed"}}, {"execution_count": null, "cell_type": "code", "source": ["def pr(y_i, y):\n", "    p = x[y==y_i].sum(0)\n", "    return (p+1) / ((y==y_i).sum()+1)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "4c0411e4-bfdf-46a4-b837-352e0cb7e1e3", "_uuid": "85ddae523b4642b810576164a47dda2659dfbab3"}}, {"execution_count": null, "cell_type": "code", "source": ["x = trn_term_doc\n", "test_x = test_term_doc"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "9e463252-12df-4d50-bb7c-fd2fbce48bd9", "_uuid": "0e28f1809c993cf5ddcc032a1b70c9a163b0fd3a"}}, {"execution_count": null, "cell_type": "code", "source": ["def get_mdl(y):\n", "    y = y.values\n", "    r = np.log(pr(1,y) / pr(0,y))\n", "    m = LogisticRegression(C=4, dual=True)\n", "    x_nb = x.multiply(r)\n", "    return m.fit(x_nb, y), r"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "2bac6c17-5926-4505-9997-7ec7a1c4f6b6", "_uuid": "2fb9d9555a63dca864c8b2a245e6bfc334b832da"}}, {"execution_count": null, "cell_type": "code", "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "\n", "preds = np.zeros((len(test), len(label_cols)))\n", "\n", "for i, j in enumerate(label_cols):\n", "    print('fit', j)\n", "    m,r = get_mdl(train[j])\n", "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "44296916-1779-4616-88a3-b0f31c66c681", "_uuid": "43e06d3b632544775cae3365537ca156d5cd7ccc"}}, {"execution_count": null, "cell_type": "code", "source": ["subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n", "submid = pd.DataFrame({'id': subm[\"id\"]})\n", "submission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n", "submission.to_csv('submission_NBSVM.csv', index=False)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "9b01a6d1-c546-4ae1-9cca-b56b43d8c29e", "_uuid": "c143112445a7d37dc3df12eb9b51a845089a579c"}}, {"cell_type": "markdown", "source": ["Now try LSTM"], "metadata": {"_cell_guid": "2ede9442-dd03-4b32-8972-b2a7181eaf9c", "_uuid": "e4c6e73311e885a90e10d718c74df332cf04dcb7"}}, {"execution_count": null, "cell_type": "code", "source": ["import sys, os, re, csv, codecs, numpy as np, pandas as pd\n", "\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n", "from keras.layers import Bidirectional, GlobalMaxPool1D\n", "from keras.models import Model\n", "from keras import initializers, regularizers, constraints, optimizers, layers"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "108feabc-4dde-4018-9ac5-714f2ab55665", "_uuid": "f15e82c8037716343e8eeb309beff54481959b16"}}, {"execution_count": null, "cell_type": "code", "source": ["path = '../input/'\n", "comp = 'jigsaw-toxic-comment-classification-challenge/'\n", "EMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\n", "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n", "TEST_DATA_FILE=f'{path}{comp}test.csv'"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "f068a911-fd0e-4e62-93c1-1db8e6bb2306", "_uuid": "0d4002ca89deabca80f39652962d3135c0acf292"}}, {"execution_count": null, "cell_type": "code", "source": ["embed_size = 50 # how big is each word vector\n", "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n", "maxlen = 100 # max number of words in a comment to use"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "dd64834b-4d98-4864-aa11-eeed0af60276", "_uuid": "40b27e99a5e9a026d5f5ac13501952517384602a"}}, {"execution_count": null, "cell_type": "code", "source": ["train = pd.read_csv(TRAIN_DATA_FILE)\n", "test = pd.read_csv(TEST_DATA_FILE)\n", "\n", "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "y = train[list_classes].values\n", "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "13bc01b0-3665-4d42-9edf-ab8354f656cf", "_uuid": "c300ff1357da8eadc619becfd951855d550f28e3"}}, {"execution_count": null, "cell_type": "code", "source": ["tokenizer = Tokenizer(num_words=max_features)\n", "tokenizer.fit_on_texts(list(list_sentences_train))\n", "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n", "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n", "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n", "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "3ae6503d-bd0d-4572-8443-fc46f55a1555", "_uuid": "5d6af43bf8138c05480ac21bb25241f8cb679a0d"}}, {"execution_count": null, "cell_type": "code", "source": ["def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n", "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n", "all_embs = np.stack(embeddings_index.values())\n", "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n", "emb_mean,emb_std"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "f9b956a5-c942-40cc-849a-6d751e877e37", "_uuid": "6c6a7eba73e2492651f27e7f8890f9ee640e0dbb"}}, {"execution_count": null, "cell_type": "code", "source": ["word_index = tokenizer.word_index\n", "nb_words = min(max_features, len(word_index))\n", "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n", "for word, i in word_index.items():\n", "    if i >= max_features: continue\n", "    embedding_vector = embeddings_index.get(word)\n", "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "818695aa-a94e-4e83-8fe9-b1f7d4db0da1", "_uuid": "272b8a4017458ba9988c3afa59d60fe315a27673"}}, {"execution_count": null, "cell_type": "code", "source": ["inp = Input(shape=(maxlen,))\n", "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n", "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n", "x = GlobalMaxPool1D()(x)\n", "x = Dense(50, activation=\"relu\")(x)\n", "x = Dropout(0.1)(x)\n", "x = Dense(6, activation=\"sigmoid\")(x)\n", "model = Model(inputs=inp, outputs=x)\n", "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "14232c10-1d25-4336-8742-289de5fbffc9", "_uuid": "b507634f01ee6832ae3c7c8e2e63dc5a84ade472"}}, {"execution_count": null, "cell_type": "code", "source": ["model.fit(X_t, y, batch_size=32, epochs=2) # validation_split=0.1);"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "d1733d1f-1f85-47c1-bcb6-c517891a9771", "_uuid": "506ecd84dabc843a45a3dc5f4470289feb5841d5"}}, {"execution_count": null, "cell_type": "code", "source": ["y_test = model.predict([X_te], batch_size=1024, verbose=1)\n", "submission2 = pd.read_csv(f'{path}{comp}sample_submission.csv')\n", "submission2[list_classes] = y_test\n", "submission2.to_csv('submission_LSTM.csv', index=False)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "5565e070-5ca7-4ece-8e4c-9b0110f918b9", "_uuid": "4c839a288da2602f7f0857bfb697249cd07db41f"}}, {"cell_type": "markdown", "source": ["Now try linear regression model. The data used are from https://www.kaggle.com/eoveson/convai-datasets-baseline-models"], "metadata": {"_cell_guid": "1c19821b-2ca0-4e9f-88c6-33de30876aec", "_uuid": "f80a6207abc5532641d7761cb56cfff457caa660"}}, {"execution_count": null, "cell_type": "code", "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.pipeline import Pipeline\n", "from scipy import sparse\n", "from subprocess import check_output\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "f8adc8f4-3af4-4ce5-a1df-afa259784285", "_uuid": "94aa709499ba681ca3681224721305428c039f12"}}, {"execution_count": null, "cell_type": "code", "source": ["train = pd.read_csv('../input/convai-datasets-baseline-models/train_with_convai.csv')\n", "test = pd.read_csv('../input/convai-datasets-baseline-models/test_with_convai.csv')"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "6e4b8e35-0a17-4d46-8c66-78b8296fe8bd", "_uuid": "a4ab142130bc34322ab79fb61b9e49b78febf40b"}}, {"execution_count": null, "cell_type": "code", "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "p_res = submission2.copy()\n", "p_res[label_cols] = (submission[label_cols] + submission2[label_cols]) / 2\n", "p_res.to_csv('submission.csv', index=False)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "5bc1de73-0a6a-4709-a114-d36e3cf33251", "_uuid": "7ba3829fd9b1f832da7cb3f4e8e5ba473f8eaf4d"}}, {"execution_count": null, "cell_type": "code", "source": ["feats_to_concat = ['comment_text', 'toxic_level', 'attack', 'aggression']\n", "# combining test and train\n", "alldata = pd.concat([train[feats_to_concat], test[feats_to_concat]], axis=0)\n", "alldata.comment_text.fillna('unknown', inplace=True)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "47f5af13-2dce-4f59-bb18-54145b0b2381", "_uuid": "d5263e981979581fb942fb566340f756ce6d0074"}}, {"execution_count": null, "cell_type": "code", "source": ["vect_words = TfidfVectorizer(max_features=50000, analyzer='word', ngram_range=(1, 1))\n", "vect_chars = TfidfVectorizer(max_features=20000, analyzer='char', ngram_range=(1, 3))\n", "all_words = vect_words.fit_transform(alldata.comment_text)\n", "all_chars = vect_chars.fit_transform(alldata.comment_text)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "74a824c8-412e-442a-9261-f3ddf66b624d", "_uuid": "101f5cc31fe65c531f4ebdc9aaaeadc2cd58b625"}}, {"execution_count": null, "cell_type": "code", "source": ["train_new = train\n", "test_new = test\n", "\n", "train_words = all_words[:len(train_new)]\n", "test_words = all_words[len(train_new):]\n", "\n", "train_chars = all_chars[:len(train_new)]\n", "test_chars = all_chars[len(train_new):]"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "4f12cfc0-57b0-48ab-8988-734e2e9167e0", "_uuid": "61bf4bbf9293280a086308ad1d730840fd7888f1"}}, {"execution_count": null, "cell_type": "code", "source": ["feats = ['toxic_level', 'attack']\n", "# make sparse matrix with needed data for train and test\n", "train_feats = sparse.hstack([train_words, train_chars, alldata[feats][:len(train_new)]])\n", "test_feats = sparse.hstack([test_words, test_chars, alldata[feats][len(train_new):]])"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "9cc59e76-b9de-4044-832c-1913264ab0b1", "_uuid": "6ffa9b8087c84746d4e89fd59fe403821193547c"}}, {"execution_count": null, "cell_type": "code", "source": ["col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "\n", "only_col = ['toxic']\n", "\n", "preds = np.zeros((test_new.shape[0], len(col)))\n", "\n", "for i, j in enumerate(col):\n", "    print('===Fit '+j)\n", "    \n", "    model = LogisticRegression(C=4.0, solver='sag')\n", "    print('Fitting model')\n", "    model.fit(train_feats, train_new[j])\n", "      \n", "    print('Predicting on test')\n", "    preds[:,i] = model.predict_proba(test_feats)[:,1]"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "e8e94d9a-42b0-4a45-8dc5-96c3928066e2", "_uuid": "7076d260d2c25ef180b639d791063c66680c4b45"}}, {"execution_count": null, "cell_type": "code", "source": ["subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n", "\n", "submid = pd.DataFrame({'id': subm[\"id\"]})\n", "submission3 = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\n", "submission3.to_csv('submission_LR.csv', index=False)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "67195fdb-b58b-4f26-9f28-caf8c1f5189e", "_uuid": "926479cddd3bf5b7c6bac5064d48c915d335e588"}}, {"execution_count": null, "cell_type": "code", "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "p_res = submission4.copy()\n", "p_res[label_cols] = (2* submission[label_cols] + 3 * submission2[label_cols] + 4 * submission3[label_cols]) / 9\n", "p_res.to_csv('submission.csv', index=False)"], "outputs": [], "metadata": {"collapsed": true, "_cell_guid": "862db44b-5f41-4339-8968-cd075fd5396f", "_uuid": "a10435806d731484cc1be9b8a31cb74fd8e59e7e"}}]}