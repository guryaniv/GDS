{"cells": [{"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "1f87e893-12d3-41b4-a415-e7a5d1d93704", "_uuid": "6bd4a3d11fdfe20a4a3901d9d66054a758f2b726", "collapsed": true}, "source": ["# importing basic dependencies\n", "import matplotlib.pyplot as plt # for seeing the images\n", "%matplotlib inline\n", "import cv2 # for image processing\n", "import glob # for file handling\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from subprocess import check_output # to get the files in currect folder\n", "from keras.utils import to_categorical # to convert to one-hot encodings\n", "import tqdm # progress bar\n", "from collections import Counter # for getting breed data\n", "\n", "# Importing ML Dependencies\n", "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n", "from keras.layers import Dropout, Flatten, Dense\n", "from keras.models import Sequential\n", "\n", "'''# Importing ML Dependencies --> Using InceptionV3 as base model\n", "from keras.applications.inception_v3 import InceptionV3 # using this model\n", "from keras.preprocessing import image # preprocessing the images\n", "from keras.models import Model # custom model\n", "from keras.layers import Dense, GlobalAveragePooling2D # layers\n", "from keras import backend as K # backend\n", "from keras.optimizers import SGD # during second compilation, for smoother learning'''\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "files = check_output([\"ls\", \"../input\"]).decode(\"utf8\")\n", "print(files)\n", "# Any results you write to the current directory are saved as output."], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "0e91888e-a007-45c9-bbbe-72dca380c633", "_uuid": "6d043f6a328713f06b1f6718237802129673a7cb", "collapsed": true}, "source": ["# loading images path --> train\n", "images_train_path = '../input/train/*.jpg'\n", "images_train_paths = glob.glob(images_train_path)\n", "print(images_train_paths[0])\n", "\n", "# laoding images path --> test\n", "images_test_path = '../input/test/*.jpg'\n", "images_test_paths = glob.glob(images_test_path)\n", "print(images_test_paths[0])"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a6c82e2f-4953-48d8-9ef1-4cb4455092e5", "_uuid": "8c400bd77adacc13975cc8a0a697b4c87672b47c", "collapsed": true}, "source": ["# taking the labels for the images\n", "labels = pd.read_csv('../input/labels.csv')\n", "print(labels.head())"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a2874fd6-9a5c-427c-9f22-f19b0266002a", "scrolled": true, "_uuid": "b5680e536cc8b425c5e80a302908b9c037944d6f", "collapsed": true}, "source": ["# taking the labels and converting to one hot\n", "breeds = sorted(list(set(labels['breed'].values)))\n", "# making a dictionary of breeds which will be used for one-hot encoding\n", "b2id = dict((b,i) for i,b in enumerate(breeds))\n", "# converting labeled breeds to numbers\n", "breed_vector = [b2id[i] for i in labels['breed'].values]\n", "# converting to one-hot encoding\n", "data_y = to_categorical(breed_vector)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c2fdfcf8-fd66-4595-8175-8d107e25d76f", "_uuid": "2a52f0f43579b4bb14bcadec7e8351f750969b87", "collapsed": true}, "source": ["print('[*]Total images:', len(images_test_paths) + len(images_train_paths))\n", "print('[*]Total training images:', len(images_train_paths))\n", "print('[*]Total test images:', len(images_test_paths))\n", "print('[*]Total breeds:',len(breeds))\n", "print('[*]data_y.shape:', data_y.shape)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "edb4f409-5a65-401b-8b5f-1252f8203d59", "_uuid": "a5359212e1b109c3478261d7625e47a15c126020", "collapsed": true}, "source": ["print(data_y[0])"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "2677e854-3843-48a1-9b2e-122cd4af3416", "_uuid": "836b1e8d4ab72728c07e02322b77a2761b68b44a", "collapsed": true}, "source": ["# understanding the distribution of breeds\n", "breed_dict = Counter(labels['breed'].values)\n", "# getting top 5 breeds\n", "breed_numbers = [i for i in breed_dict.values()]\n", "breed_names = [b for b in breed_dict.keys()]"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "f2c8eb6c-c6b5-43ff-9ef2-312ece613340", "_uuid": "4c5ae2496921a5ae60221f9d9dd65dbd6981d9d0"}, "source": ["## Looking at a Sample image"]}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "5881433c-b807-4faa-9e77-a3ecb01e8ae9", "_uuid": "de41ee0aa52d661b53e12d96a1855d81832daf51", "collapsed": true}, "source": ["# taking a sample image\n", "img1 = cv2.imread(images_train_paths[120])\n", "plt.imshow(img1)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "e3a7ab80-9006-465e-b202-a17697b3bb15", "_uuid": "bb8d6373709f4194ce77c998ec5c1eb8ec444bc6", "collapsed": true}, "source": ["img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n", "plt.imshow(img1)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "d862d8ac-b6c0-40c0-b29f-08941965611c", "_uuid": "1d1b455f310d2fe768a739314397fec16164fd52", "collapsed": true}, "source": ["# Resizing an image to a sqaure\n", "img1 = cv2.resize(img1, (224, 224))\n", "plt.imshow(img1)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "810a172b-f549-4b52-96ea-224edaf6af46", "_uuid": "8ca3c58a116edc5cde6058b7c9731cbdf82860b0", "collapsed": true}, "source": ["# converting to the image to array which will be understood by the model\n", "print(img1.shape)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e8aaa67c-d413-40c2-a0a0-d9714cc59de3", "_uuid": "ca8fe5bf03b22a7348de56d3f933ca2aaccfcc41"}, "source": ["## Loading the images"]}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a48ba518-a03a-4b1c-909c-8470fda477b8", "_uuid": "736a2865cd281caf0542a2220151de5ced4b1d98", "collapsed": true}, "source": ["print('[!]Getting training images:')\n", "total_images_train = np.zeros((len(images_train_paths), 224, 224, 3))\n", "for i in tqdm.tqdm(range(len(images_train_paths))):\n", "    image = cv2.imread(images_train_paths[i]) # reading the image\n", "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to proper colour channel\n", "    image = cv2.resize(image, (224,224)) # resizing to feed into model\n", "    total_images_train[i] = image # adding to the total data"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f2de7b8b-9616-4010-8b77-f0ccdfce9915", "_uuid": "a9f0d404b00d9c81db93514728b79c28214c828d", "collapsed": true}, "source": ["print('[!]Getting testing images:')\n", "total_images_test = np.zeros((len(images_test_paths), 224, 224, 3))\n", "for i in tqdm.tqdm(range(len(images_test_paths))):\n", "    image = cv2.imread(images_test_paths[i]) # reading the image\n", "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to proper colour channel\n", "    image = cv2.resize(image, (224,224)) # resizing to feed into model\n", "    total_images_test[i] = image # adding to the total data"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f7d0367b-ea02-4efc-ac0c-b759655245f0", "_uuid": "96e4b0989604b9df1d767495c33f623fbf59e8bb", "collapsed": true}, "source": ["total_images_train = np.array(total_images_train)\n", "# total_images_test = np.array(total_images_test)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "1a8d6dfb-b0f0-4b6c-b895-1f75a56208f5", "_uuid": "9ca279f9cc19c56639cd78c2d1761b0101527ff7", "collapsed": true}, "source": ["print('[*]Traning set shape:', total_images_train.shape)\n", "# print('[*]Testing set shape:', total_images_test.shape)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0b242360-fb8e-48a3-9a85-d82c147cb0ea", "_uuid": "5a8e24ae8d8a09587c7df99d08d329d3a7a07272"}, "source": ["## Making Custom Classifier"]}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c2a76bcc-b9d2-4994-945c-8173bde1a7aa", "_uuid": "db01e396de5d08493c3f72691488532e38496c65", "collapsed": true}, "source": ["model = Sequential()\n", "model.add(Conv2D(filters = 16, kernel_size = 2, padding = 'same',\n", "                 activation = 'relu', input_shape = (224, 224, 3)))\n", "model.add(MaxPooling2D(pool_size= 2))\n", "model.add(Conv2D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\n", "model.add(MaxPooling2D(pool_size= 2))\n", "model.add(Conv2D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu'))\n", "model.add(MaxPooling2D(pool_size= 2))\n", "model.add(Dropout(0.3))\n", "model.add(Flatten())\n", "model.add(Dense(1024, activation = 'relu'))\n", "model.add(Dropout(0.4))\n", "model.add(Dense(len(breeds), activation = 'softmax'))\n", "\n", "print(model.summary())"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "8fcaed8b-b65e-478f-8b1c-61065f4e15b0", "_uuid": "56245ccc95181d44a749ec87b9eaf654a11984f9", "collapsed": true}, "source": ["# Compiling the model\n", "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "9214dd65-6b2e-4297-bf55-60b2512d445f", "_uuid": "4fa36059a38686b13c82c42b12728ee0bbdd1939", "collapsed": true}, "source": ["model.fit(total_images_train, data_y,  epochs = 10, batch_size = 64)"], "execution_count": null}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "nbformat": 4, "nbformat_minor": 1}