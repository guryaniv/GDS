{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.datasets import load_files       \nfrom keras.utils import np_utils\nimport PIL\nfrom PIL import ImageFile  \nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sklearn\n\nimport keras\nfrom keras.utils import np_utils\nfrom keras.preprocessing import image   \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"727b174358841b424b63337ecb3c675ac64ed369"},"cell_type":"code","source":"train_imgs = \"../input/train\"\ntest_imgs = \"../input/test\"\n\nresize = 224","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!ls \"../input/train\" -1A | wc -l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6f7c1479f872d374f67ed76610743c33947b403"},"cell_type":"code","source":"!ls \"../input/test\" -1A | wc -l","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a3e00ebb8fb5d249cb127be003630d4127d3e2"},"cell_type":"markdown","source":"We have got 25361 train images  and 7960 test of whales' tails"},{"metadata":{"trusted":true,"_uuid":"8a1c08cfa46712178711aebd6be6d2c31bc52973"},"cell_type":"code","source":"# Funtion to create a list of images' names\ndef dataset_list_load (image_dir):\n    lstFilesJGP = []\n    for dirName, subdirList, fileList in os.walk(image_dir):\n        for filename in fileList:\n            if \".jpg\" in filename.lower():  # check whether the file's JPEG\n                lstFilesJGP.append(os.path.join(dirName,filename))\n    return lstFilesJGP\n\n# Funtion to create a list of images' names\ndef dataset_list_load_name (image_dir):\n    lstFilesJGP = []\n    for dirName, subdirList, fileList in os.walk(image_dir):\n        for filename in fileList:\n            if \".jpg\" in filename.lower():  # check whether the file's JPEG\n                lstFilesJGP.append(filename)\n    return lstFilesJGP","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f5e17d42200b044704c8b95a7d914b632c8f23a","scrolled":true},"cell_type":"code","source":"test_files = dataset_list_load_name(test_imgs)\n\nprint('There are %d test whale images.'% len(test_files))\nprint(test_files[1:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"704224d967f142a1f736a334aaa8427a59ff5732"},"cell_type":"code","source":"from keras.preprocessing import image                  \nfrom tqdm import tqdm\nimport PIL\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897f20b2977b8867b87a6c08b20d98787162fb33"},"cell_type":"code","source":"from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_image_list).astype('float32')/255\ntest_tensors = paths_to_tensor(test_image_list).astype('float32')/255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8b8c9b4caf7abe01d577ca948778cd499feef10"},"cell_type":"code","source":"whale_labels = pd.read_csv('../input/train.csv')\nwhale_labels.info()\nwhale_labels.head(10)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5e6ad866f598fac28c46639b9de183f9654c00c"},"cell_type":"code","source":"whale_labels['Id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a465a661ddc97f0ef5acc278d0dedf7b650903e"},"cell_type":"code","source":"whale_labels = whale_labels.loc[whale_labels['Id'] != 'new_whale']\nwhale_labels.info()\nwhale_labels.head(10)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fe5b4e3eade6511dc60a75c9c2a0f13f05adae2"},"cell_type":"code","source":"whale_labels['Id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc793bbade4ec6770ef581637edd06ec65b77861"},"cell_type":"code","source":"num_classes = len(whale_labels['Id'].unique())\nprint(num_classes)\n\nIds_enum = {cat: k for k,cat in enumerate(whale_labels.Id.unique())}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98e25d6f9910451bcbca677eaa4d01e59a8c682d"},"cell_type":"code","source":"from keras.preprocessing import image                  \nfrom tqdm import tqdm\nimport PIL\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True      \n\nresize = 224\n     \nim_arrays = []\nlabels = []\nfs = {} ##dictionary with original size of each photo \nfor index, row in tqdm(whale_labels.iterrows()): \n    # CV2 using\n    im = cv2.imread(os.path.join(train_imgs,row['Image']),0)\n    norm_image = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    new_image = cv2.resize(norm_image,(resize,resize))\n    new_image = np.reshape(new_image,[resize,resize,1])\n    im_arrays.append(new_image)\n    fs[row['Image']] = norm_image.shape\n    \n    # PIL Using\n    #new_image = image.load_img(os.path.join(train_imgs,row['Image']), target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    #x = image.img_to_array(new_image)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    #np.expand_dims(x, axis=0)\n    #im_arrays.append(x)\n    \n    labels.append(Ids_enum [row['Id']])\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ffbbf29a329df9b65bfd7128a1ee73658998bda"},"cell_type":"code","source":"#We rescale the images by dividing every pixel in every image by 255.\n#train_tensors = np.vstack(im_arrays).astype('float32')/255 # PIL\ntrain_tensors = np.array(im_arrays).astype('float32')/255 # CV2\n\ntrain_labels = np.array(labels)\ntrain_targets = np_utils.to_categorical(train_labels, num_classes)\n\nprint(type(train_tensors), train_tensors.size, train_tensors.shape)\nprint(type(train_labels), train_labels.size, train_labels.shape)\nprint(type(train_targets), train_targets.size, train_targets.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bbba65cfb41349a8d60dcd8af3a63b72a8958e1"},"cell_type":"markdown","source":"**Let's  plot a random image**"},{"metadata":{"trusted":true,"_uuid":"4f5e0b4f92eac774108d4c93c9229bb6a2bad74b"},"cell_type":"code","source":"import random\n\nwhale_choose = whale_labels.sample()\nprint(whale_choose)\nfile_name = whale_choose['Image'].values[0]\nwhale_id = whale_choose['Id'].values[0] \nimg_tail = mpimg.imread(os.path.join(\"../input/train\",file_name))\n\nprint(file_name)\nprint(whale_id)\nprint(img_tail.shape)\nimgplot = plt.imshow(img_tail)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"910ea05e989994570539cacc9cf02caa7219b18f"},"cell_type":"markdown","source":"**Simple Model Architecture**"},{"metadata":{"trusted":true,"_uuid":"8fc4b680d030a95dd3026cd2ffddb05e53ba5c67"},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\n### TODO: Define your architecture.\nmodel.add(Conv2D(filters = 16, kernel_size = 2, padding = 'same', activation='relu', \n                                         input_shape = train_tensors.shape[1:]))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Conv2D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(Conv2D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = 2))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(num_classes, activation = 'softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98326d9b0b1d4e841a97db6f4086a52227e2a2d0"},"cell_type":"markdown","source":"**Compile the Model**"},{"metadata":{"trusted":true,"_uuid":"616425e249fd9e618e6529ad0a81a1a9754bfa30"},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e7b287301fa856e5ed05604bfa987f28ba88bdc"},"cell_type":"markdown","source":"**Create Train/Validation sets **"},{"metadata":{"trusted":true,"_uuid":"6e92f8e2bdef3b042b185013ddb40ba3e0841f5c"},"cell_type":"code","source":"split_percentage = 0.1\n\nX_train_tens = train_tensors[int(train_tensors.shape[0]*split_percentage):, :, :, :]\nX_train_targets = train_targets[int(train_tensors.shape[0]*split_percentage):, :]\n\nY_valid_tens = train_tensors[: int(train_tensors.shape[0]*split_percentage), :, :, :]\nY_valid_targets = train_targets[: int(train_tensors.shape[0]*split_percentage), :]\n\nprint(type(X_train_tens), X_train_tens.size, X_train_tens.shape)\nprint(type(X_train_targets), X_train_targets.size, X_train_targets.shape)\n\nprint(type(Y_valid_tens), Y_valid_tens.size, Y_valid_tens.shape)\nprint(type(Y_valid_targets), Y_valid_targets.size, Y_valid_targets.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7a9ee8e5c166366860f404353f2ae76fe140816"},"cell_type":"markdown","source":"**(IMPLEMENTATION) Train the Model**"},{"metadata":{"trusted":true,"_uuid":"025773e8b30c70b44631884f771c99f78124726e"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint  \n\nepochs = 20\n\ncheckpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5',\n                               verbose=1, save_best_only= True)\n\nmodel.fit(X_train_tens, X_train_targets, \n          validation_data = (Y_valid_tens, Y_valid_targets),\n          epochs=epochs, batch_size=50, callbacks=[checkpointer], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6761a08cd9f86ea888f112704a8fdaf2bd532752"},"cell_type":"code","source":"# See that file with coefficients is created\nimport os\nprint(os.listdir())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42bbf5679a20a7514debcba389db3b1428106455"},"cell_type":"markdown","source":"**Load the Model with the Best Validation Loss**"},{"metadata":{"trusted":true,"_uuid":"1dcbc6d256df28ad9dde0bbc1c3a6bac54bccac6"},"cell_type":"code","source":"model.load_weights('weights.best.from_scratch.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34d07bf986c57cc2fd7b3ae939b6af28b902bbc5"},"cell_type":"markdown","source":"**Get Test Tensors**"},{"metadata":{"trusted":true,"_uuid":"60d53be2994f704ad93dc6b289c94262c2888eeb"},"cell_type":"code","source":"from keras.preprocessing import image                  \nfrom tqdm import tqdm\nimport PIL\nfrom PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True          \n\nresize = 224\n     \ntest_arrays = []\n\nfor index in tqdm(test_files[:]): \n    # CV2 using    \n    im = cv2.imread(os.path.join(test_imgs,index),0)\n    norm_image = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    test_new_image = cv2.resize(norm_image,(resize,resize))\n    test_new_image = np.reshape(test_new_image,[resize,resize,1])\n    test_arrays.append(test_new_image)\ntest_tensors = np.array(test_arrays).astype('float32')/255 # CV2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a52d0f8288337631dcf94403a09aa429406f2b49"},"cell_type":"markdown","source":"**Test the Model**"},{"metadata":{"trusted":true,"_uuid":"04e5e16e0f080bc755fa2875558ca5fe4dc7176b"},"cell_type":"code","source":"# get index of predicted whale tails for each image in test set\nwhale_tail_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24a26e159932f578865278955354f9ef10f6a3b2"},"cell_type":"code","source":"Num = 55\nprint(whale_tail_predictions[:Num])\nprint(whale_labels.Id.unique()[1])\nprint(whale_labels.Id.unique()[65])\nprint(whale_labels.Id.unique()[165])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9588bb12626b520105470777477b5a24e83a4ce5"},"cell_type":"markdown","source":"**Create Submission File**"},{"metadata":{"trusted":true,"_uuid":"9bc44c739b0122d5ebeaba35d2406d21011a932d"},"cell_type":"code","source":"# Make predictions on test images and write a submission file            \nfile_path = 'submission_simple_model.csv'\nwith open(file_path , 'w') as file:\n    file.write(\"Image,Id\\n\")\n    for list_index in tqdm(enumerate(test_files)):\n        sub_str = \"\"\n        sub_str += test_files[list_index[0]]\n        sub_str += \",\"\n        sub_str += whale_labels.Id.unique()[whale_tail_predictions[list_index[0]]] \n        file.write(sub_str+\"\\n\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}