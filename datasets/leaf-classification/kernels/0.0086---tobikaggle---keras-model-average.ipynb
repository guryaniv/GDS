{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "57a257b8-f5f9-ddc8-2c84-0968cbd90315"
      },
      "source": [
        "Average several keras models to obtain better scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1a3c8fd-06df-aeb3-1519-5fceda7fa470"
      },
      "outputs": [],
      "source": [
        "## Measure execution time, becaus Kaggle cloud fluctuates  \n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "## Importing standard libraries\n",
        "%pylab inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Importing sklearn libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "## Keras Libraries for Neural Networks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Merge\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "## Read data from the CSV file\n",
        "data = pd.read_csv('../input/train.csv')\n",
        "parent_data = data.copy()    ## Always a good idea to keep a copy of original data\n",
        "ID = data.pop('id')\n",
        "\n",
        "data.shape\n",
        "data.describe()\n",
        "\n",
        "## Since the labels are textual, so we encode them categorically\n",
        "y = data.pop('species')\n",
        "y = LabelEncoder().fit(y).transform(y)\n",
        "print(y.shape)\n",
        "\n",
        "## Most of the learning algorithms are prone to feature scaling\n",
        "## Standardising the data to give zero mean =)\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.MinMaxScaler().fit(data).transform(data)\n",
        "X = StandardScaler().fit(data).transform(data)\n",
        "## normalizing does not help here; l1 and l2 allowed\n",
        "## X = preprocessing.normalize(data, norm='l1')\n",
        "print(X.shape)\n",
        "X\n",
        "\n",
        "## We will be working with categorical crossentropy function\n",
        "## It is required to further convert the labels into \"one-hot\" representation\n",
        "y_cat = to_categorical(y)\n",
        "print(y_cat.shape)\n",
        "\n",
        "## retain class balances \n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1,random_state=12345)\n",
        "train_index, val_index = next(iter(sss.split(X, y)))\n",
        "x_train, x_val = X[train_index], X[val_index]\n",
        "y_train, y_val = y_cat[train_index], y_cat[val_index]\n",
        "print(\"x_train dim: \",x_train.shape)\n",
        "print(\"x_val dim:   \",x_val.shape)\n",
        "print()\n",
        "\n",
        "## --------------------------------------------------\n",
        "## Developing a layered model for Neural Networks No/1/\n",
        "## Input dimensions should be equal to the number of features\n",
        "## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(600,input_dim=192,  init='uniform', activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(600, activation='sigmoid'))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(99, activation='softmax'))\n",
        "\n",
        "## Error is measured as categorical crossentropy or multiclass logloss\n",
        "## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n",
        "model1.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n",
        "\n",
        "## Fitting the model on the whole training data\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=300)\n",
        "history = model1.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n",
        "                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n",
        "                    \n",
        "## we need to consider the loss for final submission to leaderboard\n",
        "print('val_acc: ',max(history.history['val_acc']))\n",
        "print('val_loss: ',min(history.history['val_loss']))\n",
        "print('train_acc: ',max(history.history['acc']))\n",
        "print('train_loss: ',min(history.history['loss']))\n",
        "print(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n",
        "\n",
        "## summarize history for loss\n",
        "## Plotting the loss with the number of iterations\n",
        "plt.semilogy(history.history['loss'])\n",
        "plt.semilogy(history.history['val_loss'])\n",
        "plt.title('model1 loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Plotting the error with the number of iterations\n",
        "## With each iteration the error reduces smoothly\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model1 accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## --------------------------------------------------\n",
        "## Developing a layered model for Neural Networks No/2/\n",
        "## Input dimensions should be equal to the number of features\n",
        "## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(1024,input_dim=192,  init='glorot_normal', activation='relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(512, activation='sigmoid'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(99, activation='softmax'))\n",
        "\n",
        "## Error is measured as categorical crossentropy or multiclass logloss\n",
        "## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n",
        "model2.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n",
        "\n",
        "## Fitting the model on the whole training data\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=300)\n",
        "history = model2.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n",
        "                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n",
        "                    \n",
        "## we need to consider the loss for final submission to leaderboard\n",
        "print('val_acc: ',max(history.history['val_acc']))\n",
        "print('val_loss: ',min(history.history['val_loss']))\n",
        "print('train_acc: ',max(history.history['acc']))\n",
        "print('train_loss: ',min(history.history['loss']))\n",
        "print(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n",
        "\n",
        "## summarize history for loss\n",
        "## Plotting the loss with the number of iterations\n",
        "plt.semilogy(history.history['loss'])\n",
        "plt.semilogy(history.history['val_loss'])\n",
        "plt.title('model2 loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Plotting the error with the number of iterations\n",
        "## With each iteration the error reduces smoothly\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model2 accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## --------------------------------------------------\n",
        "## Developing a layered model for Neural Networks No/3/\n",
        "## Input dimensions should be equal to the number of features\n",
        "## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(600,input_dim=192,  init='uniform', activation='relu'))\n",
        "model3.add(Dropout(0.3))\n",
        "model3.add(Dense(600, activation='sigmoid'))\n",
        "model3.add(Dropout(0.3))\n",
        "model3.add(Dense(99, activation='softmax'))\n",
        "\n",
        "## Error is measured as categorical crossentropy or multiclass logloss\n",
        "## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n",
        "model3.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n",
        "\n",
        "## Fitting the model on the whole training data\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=300)\n",
        "history = model3.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n",
        "                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n",
        "                    \n",
        "## we need to consider the loss for final submission to leaderboard\n",
        "print('val_acc: ',max(history.history['val_acc']))\n",
        "print('val_loss: ',min(history.history['val_loss']))\n",
        "print('train_acc: ',max(history.history['acc']))\n",
        "print('train_loss: ',min(history.history['loss']))\n",
        "print(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n",
        "\n",
        "## summarize history for loss\n",
        "## Plotting the loss with the number of iterations\n",
        "plt.semilogy(history.history['loss'])\n",
        "plt.semilogy(history.history['val_loss'])\n",
        "plt.title('model3 loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Plotting the error with the number of iterations\n",
        "## With each iteration the error reduces smoothly\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model3 accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## --------------------------------------------------\n",
        "## Developing a layered model for Neural Networks No/4/\n",
        "## Input dimensions should be equal to the number of features\n",
        "## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n",
        "model4 = Sequential()\n",
        "model4.add(Dense(768,input_dim=192,  init='uniform', activation='relu'))\n",
        "model4.add(Dropout(0.3))\n",
        "model3.add(Dense(384, activation='sigmoid'))\n",
        "model3.add(Dropout(0.3))\n",
        "model4.add(Dense(99, activation='softmax'))\n",
        "\n",
        "## Error is measured as categorical crossentropy or multiclass logloss\n",
        "## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n",
        "model4.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n",
        "\n",
        "## Fitting the model on the whole training data\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=600)\n",
        "history = model4.fit(x_train, y_train,batch_size=192,nb_epoch=2500 ,verbose=0,\n",
        "                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n",
        "                    \n",
        "## we need to consider the loss for final submission to leaderboard\n",
        "print('val_acc: ',max(history.history['val_acc']))\n",
        "print('val_loss: ',min(history.history['val_loss']))\n",
        "print('train_acc: ',max(history.history['acc']))\n",
        "print('train_loss: ',min(history.history['loss']))\n",
        "print(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))\n",
        "\n",
        "## summarize history for loss\n",
        "## Plotting the loss with the number of iterations\n",
        "plt.semilogy(history.history['loss'])\n",
        "plt.semilogy(history.history['val_loss'])\n",
        "plt.title('model4 loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "## Plotting the error with the number of iterations\n",
        "## With each iteration the error reduces smoothly\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model4 accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#----------------------------------------------\n",
        "#----------------------------------------------\n",
        "## read test file\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "index = test.pop('id')\n",
        "\n",
        "## we need to perform the same transformations from the training set to the test set\n",
        "test = preprocessing.MinMaxScaler().fit(test).transform(test)\n",
        "test = StandardScaler().fit(test).transform(test)\n",
        "yPred1 = model1.predict_proba(test)\n",
        "yPred2 = model2.predict_proba(test)\n",
        "yPred3 = model3.predict_proba(test)\n",
        "yPred4 = model4.predict_proba(test)\n",
        "\n",
        "## average all models\n",
        "yPred = (yPred1 + yPred2 + yPred3 + yPred4 ) / 4.0\n",
        "\n",
        "## Converting the test predictions in a dataframe as depicted by sample submission\n",
        "yPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))\n",
        "\n",
        "# show data frame\n",
        "yPred\n",
        "\n",
        "## write submission to file\n",
        "fp = open('submission_nn_kernel.csv','w')\n",
        "fp.write(yPred.to_csv())\n",
        "\n",
        "## print run time\n",
        "end = time.time()\n",
        "print()\n",
        "print(round((end-start),2), \"seconds\")"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}