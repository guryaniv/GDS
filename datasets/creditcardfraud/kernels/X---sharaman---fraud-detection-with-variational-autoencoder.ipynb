{"cells":[{"metadata":{"_uuid":"8fd18ebe311ab216ff8d9446a8f6b5010a9e1fba"},"cell_type":"markdown","source":"This kernel trains a Variational Autoencoder in Keras with Gaussian input and output."},{"metadata":{"_uuid":"bfd324e0f39675b85e2f14673d29262764c1aac0"},"cell_type":"markdown","source":"Import Keras and other necessary libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing, metrics\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Lambda, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"b36a8d510f6130f8b00c64f6ae7fba9f3427f81f"},"cell_type":"markdown","source":"Read data and set train/validation/test split:"},{"metadata":{"_uuid":"a36f5f17a310779236b09a94ed67fadcb1313c92","_cell_guid":"daf551eb-a910-4d3d-82d1-2c1ceffbe221","collapsed":true,"trusted":true},"cell_type":"code","source":"#read the dataset\ncsv = pd.read_csv(\"../input/creditcard.csv\")\n\ntest_split = 0.3 #portion of data used for testing\nval_split = 0.2 #portion of training data used for validation","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"5c56caafd3b4b8493ca73778a24d6b945d8515c5"},"cell_type":"markdown","source":"Prepare training, validation and test datasets:"},{"metadata":{"_uuid":"b4d90f3e596bd62ce207eac9276290a240f8bdf0","_cell_guid":"01cf0cc2-5932-4916-bf3c-ffcfbc9c114c","collapsed":true,"trusted":true},"cell_type":"code","source":"data = csv.astype('float32').copy()\ndata.sort_values('Time', inplace=True)\n\nfirst_test = int(data.shape[0] * (1 - test_split))\nfirst_val = int(first_test * (1 - val_split))\n\ntrain = data.iloc[:first_val]\nval = data.iloc[first_val:first_test]\ntest = data.iloc[first_test:]\n\nx_train_df, x_val_df, x_test_df = train.iloc[:, 1:-2], val.iloc[:, 1:-2], test.iloc[:, 1:-2]\ny_train_df, y_val_df, y_test_df = train.iloc[:, -1], val.iloc[:, -1], test.iloc[:, -1]\n\nx_train, x_val, x_test = x_train_df.values, x_val_df.values, x_test_df.values\ny_train, y_val, y_test = y_train_df.values, y_val_df.values, y_test_df.values\n\nscaler = preprocessing.StandardScaler()\n\nx_train, x_val, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_val), scaler.fit_transform(x_test)\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"4ea7827f403325785f0a726e078d13e6c9a526f5"},"cell_type":"markdown","source":"Build the model and print summary:"},{"metadata":{"_uuid":"14baa17af3a2f125d3f760534571af214f51634b","_cell_guid":"770eb2e9-769d-4b91-b45f-50649491511e","trusted":true},"cell_type":"code","source":"hidden_size = 16 #size of the hidden layer in encoder and decoder\nlatent_dim = 2 #number of latent variables to learn\n\ninput_dim = x_train.shape[1]\n\nx = Input(shape=(input_dim,))\nt = BatchNormalization()(x)\nt = Dense(hidden_size, activation='tanh' , name='encoder_hidden')(t)\nt = BatchNormalization()(t)\n\nz_mean = Dense(latent_dim, name='z_mean')(t)\nz_log_var = Dense(latent_dim, name='z_log_var')(t)\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=1.)\n    return z_mean + K.exp(z_log_var / 2) * epsilon\n\nz = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])\n#t = BatchNormalization()(z)\n\nt = Dense(hidden_size, activation='tanh', name='decoder_hidden')(z)\n#t = BatchNormalization()(t)\n\ndecoded_mean = Dense(input_dim, activation=None, name='decoded_mean')(t)\n\nvae = Model(x, decoded_mean)\n\ndef rec_loss(y_true, y_pred):\n    return K.sum(K.square(y_true - y_pred), axis=-1)\n\ndef kl_loss(y_true, y_pred):\n    return - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n\ndef vae_loss(x, decoded_mean):\n    rec_loss = K.sum(K.square(x - decoded_mean), axis=-1)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return K.mean((rec_loss + kl_loss) / 2)\n\nvae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss, metrics=[rec_loss, kl_loss])\nvae.summary()\n","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"986cf6f9649034e015727b6845a141f03af2bdb3"},"cell_type":"markdown","source":"Train the model with early stopping and a kind of learning rate schedule for the specified number of epochs with the specified batch size:"},{"metadata":{"trusted":true,"_uuid":"2bbe25a6961e252294913a036c11ad68aafcf90f"},"cell_type":"code","source":"n_epochs = 30\nbatch_size = 128\n\nearly_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001\nreduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001\n\ncallbacks = [early_stopping, reduce_lr]\n\n#collect training data in history object\nhistory = vae.fit(x_train, x_train, \n                  validation_data=(x_val, x_val), \n                  batch_size=batch_size, epochs=n_epochs, \n                  callbacks=callbacks)\n","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"7ff8ac9002fc5035470e92de7a5fd8fd775b9f22"},"cell_type":"markdown","source":"Plotting training and validation loss:"},{"metadata":{"_uuid":"242200ed138247c77abc7debaa04e12ba978f1cb","_cell_guid":"488e0909-8376-4e10-9126-073c45c0422a","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14, 6))\nax = fig.gca()\nax.plot(history.history['rec_loss']);\nax.plot(history.history['val_rec_loss']);\n","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"84dce722240914836f49d38bec4db13e18e132ad"},"cell_type":"markdown","source":"Plotting the learned latent representations for normal and anomalous instances in the specified dataset (training/validation/test):"},{"metadata":{"_uuid":"1b584c09b0881faf4bb5f04b8b1e68e27fd22aa6","_cell_guid":"da255d38-33ab-47d9-820d-1698a0e57de9","trusted":true},"cell_type":"code","source":"x_data = x_train\ny_data = y_train\n\nencoder = Model(x, z_mean)\n\nwith_labels = np.concatenate([x_data, np.reshape(y_data, (-1, 1))], axis=1) #concatenate x and y to be able to filter by class\n\nnormal = with_labels[np.where(with_labels[:, -1] == 0)] #filter normal instances\nanomalous = with_labels[np.where(with_labels[:, -1] == 1)] #filter anomalous instances\n\nnormal_encoded = encoder.predict(normal[:, :-1], batch_size=128)\nanomalous_encoded = encoder.predict(anomalous[:, :-1], batch_size=128)\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.gca()\n\n#semicolon at the end hides unnecessary output\nax.scatter(normal_encoded[:, 0], normal_encoded[:, 1]);\nax.scatter(anomalous_encoded[:, 0], anomalous_encoded[:, 1]);\n","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"8d7f755b11f863f43f20d665619362a57728cad0"},"cell_type":"markdown","source":"Apply simple Linear Discriminant Analysis to learned features (the latent representations):"},{"metadata":{"_uuid":"13ea7fe9d3a974beebebd87093c1fb1712b71d27","_cell_guid":"8f9658ca-6f31-4046-9c4f-272632e946e0","trusted":true},"cell_type":"code","source":"with_labels_encoded = encoder.predict(with_labels[:, :-1], batch_size=128)\n\nX = with_labels_encoded\ny = with_labels[:, -1]\n\nclf = LinearDiscriminantAnalysis()\nclf.fit(X, y)\n\npred = clf.predict(X)\n\nprint(\"AUC(ROC): \" + str(metrics.roc_auc_score(y, pred)))\nprint(\"Precision: \" + str(metrics.precision_score(y, pred)))\nprint(\"Recall: \" + str(metrics.recall_score(y, pred)))\nprint(\"F1 score: \" + str(metrics.f1_score(y, pred)))\n\ntn, fp, fn, tp = metrics.confusion_matrix(y, pred).ravel()\n\nprint(\"False positives: \" + str(fp))\nprint(\"True positives: \" + str(tp))\nprint(\"False negatives: \" + str(fn))\nprint(\"True negateives: \" + str(tn))","execution_count":82,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}