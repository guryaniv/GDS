{"cells":[{"metadata":{"_uuid":"0ad0077dec3d90d719bd4b114e1c3492c4e8106b"},"cell_type":"markdown","source":"The purpose of this Kernel is to create a pickle file output which contains a dictionary of the sorted training data to be distributed for processing by parallel Kernels in order to maximize training time. File names are imported, paired with their corresponding training labels, sorted into train, test, and validation dictionaries, and then pickled for use in each of the individual training Kernels so that individual kernel runs can occur overnight within the Kernel uptime limit (6 hours).\n\nAfter each individual model trains in its own kernel, the weights are saved, and combined into an ensemble which is then trained on a small densely connected network. The final multi input network is then used to make predictions on the training data. This approach cuts down on the training time required by parallelizing the training of each individual neural network. 6 kernels may be run simultaneously by a single user, meaning that this can have a maximum time save of up to 30 hours."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Custom imports\nimport pickle\n\nfrom pathlib import Path\nfrom subprocess import check_output\n\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc281e56fabbc5196f5f19d262e6fdd7083e6983"},"cell_type":"code","source":"# folder names are labels. File names are not necessarily unique without considering the label connected to the filename,\n# therefore full paths including sorting folders are required.\n\nfolders = os.listdir(\"../input/tensorflow-speech-recognition-challenge/train/audio\")\nprint(folders)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f98d8d9270a95829caf357fcfff4ac6c0bffe14"},"cell_type":"markdown","source":"The testing and validation lists are predetermined for this contest. We will be required to sort the files into one of three categories depending on their presence in one of these lists.\n\nThe contest also does not utilize all of the labels on the training data, instead there are 10 known labels and one label consisting of 20 different words which are all considered \"unknown\". A dictionary is declared to sort all of the unknown words into a single label.\n\nThere are also longer recordings of \"silence\" which consists of background noise which is to be ignored. These files are not in 1 second recordings and will need to be cut into 16000 sample wav items and binned as \"silence\" for training."},{"metadata":{"trusted":true,"_uuid":"02e6696f93ad92e506972d6508cbf93b21452f52"},"cell_type":"code","source":"# Open test / validation lists\ntest_list = open(\"../input/tensorflow-speech-recognition-challenge/train/testing_list.txt\", \"r\").readlines()\nvalidation_list = open(\"../input/tensorflow-speech-recognition-challenge/train/validation_list.txt\", \"r\").readlines()\n\n# The contest does not inlude all labels, most are classified as \"unknown\"\ncontest_dict = {'yes': 0,\n                'no': 1,\n                'up': 2,\n                'down': 3,\n                'left': 4,\n                'right': 5,\n                'on': 6,\n                'off': 7,\n                'stop': 8,\n                'go': 9,\n                'unknown': 10,\n                'silence': 11\n               }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a53d01616e5a42428b32b2de2d174d158ad2fbe9"},"cell_type":"code","source":"train_audio_path = '../input/tensorflow-speech-recognition-challenge/train/audio'\n\ntrain_labels = os.listdir(train_audio_path)\nprint(f'Number of labels: {len(train_labels)}')\n\nwavs = []\nlabels = []\n\n# create a list of all the wav files and their labels which is NOT background noise\nfor label in train_labels:\n    if label == '_background_noise_':\n        continue\n    files = os.listdir(train_audio_path + '/' + label)\n    for f in files:\n        if not f.endswith('wav'):\n            continue\n        wavs.append(f)\n        labels.append(label)\n\n# append on a list of generated background noise from the silence kernel to be included in training data\nfiles = os.listdir('../input/silence')\nfor f in files:\n    if not f.endswith('wav'):\n        continue\n    wavs.append(f)\n    labels.append('silence')\n\nx_train = []\nx_val = []\nx_test = []\ny_train = []\ny_val = []\ny_test = []\n\n# sort by comparing path to list, anything not found on the lists will be used as training data\nfor i in range(len(wavs)):\n    if any(labels[i] + '/' + wavs[i] in s for s in test_list):\n        x_test.append(wavs[i])\n        y_test.append(labels[i])\n    elif any(labels[i] + '/' + wavs[i] in s for s in validation_list):\n        x_val.append(wavs[i])\n        y_val.append(labels[i])\n    else:\n        x_train.append(wavs[i])\n        y_train.append(labels[i])\n\n# format as full file path, this will be useful when using a generator to train\nx_train = [\"{}/{}\".format(y_train,x_train) for x_train, y_train in zip(x_train, y_train)]\nx_val = [\"{}/{}\".format(y_val,x_val) for x_val, y_val in zip(x_val, y_val)]\nx_test = [\"{}/{}\".format(y_test,x_test) for x_test, y_test in zip(x_test, y_test)]\n\n# overwrite labels which are not present in the contest dictionary with the string 'unknown'\nfor i in range(len(y_train)):\n    if not(y_train[i] in contest_dict):\n        y_train[i] = 'unknown'\n\nfor i in range(len(y_val)):\n    if not(y_val[i] in contest_dict):\n        y_val[i] = 'unknown'\n\nfor i in range(len(y_test)):\n    if not(y_test[i] in contest_dict):\n        y_test[i] = 'unknown'\n\ntrain_sequences = []\ntest_sequences = []\n\n# create a list of numeric identifiers for use with NN when feeding dictionaries\nfor i in range(len(y_train)):\n    train_sequences.append(contest_dict[y_train[i]])\n\nfor i in range(len(y_val)):\n    train_sequences.append(contest_dict[y_val[i]])\n\nfor i in range(len(y_test)):\n    test_sequences.append(contest_dict[y_test[i]])\n\nlabel_list = x_train + x_val\n\n# create label dictionaries\nlabels = dict(zip(label_list, train_sequences))\ntest_labels = dict(zip(x_test, test_sequences))\n\n# create test, train, and validation dictionaries for training and final evaluation\ntest_dict = {'test': x_test}\n\npartition = {'train': x_train,\n             'validation': x_val}\n\n# pickle the results\nwith open('SavedTestDict.pickle', 'wb') as handle:\n    pickle.dump(test_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('SavedPartition.pickle', 'wb') as handle:\n    pickle.dump(partition, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('SavedLabels.pickle', 'wb') as handle:\n    pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nwith open('SavedTestLabels.pickle', 'wb') as handle:\n    pickle.dump(test_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21930ca4b433a6c1c4728b45f6cf90893d7e5174"},"cell_type":"markdown","source":"Our Partitions of training and validation data, training and validation labels, test data, and test labels are now available for loading into other kernels."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}