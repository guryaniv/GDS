{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3d2aab97-5d4b-9736-b2dd-b66ef429189e"
      },
      "source": [
        "# Two-Sigma Financial Modelling Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f2664d86-b2aa-c472-16f6-f3bc8d0fa593"
      },
      "source": [
        "## 0. Problem setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dbb024b8-ef23-d830-b32e-96e911e11e12"
      },
      "source": [
        "In this competition the task is the prediction of the economic outcomes of a portfolio of financial instruments managed by Two Sigma Investments. The dataset contains anonymized features pertaining to a time-varying value for a financial instrument, identified by the variable *id*. Time is represented by the *timestamp* feature and the variable to predict is *y*. This is a **time series forecasting problem**.\n",
        "\n",
        "The competition uses the Kernels environment and the competition data API:\n",
        " - the API is designed to prevent accessing data beyond the timestamp for which we are predicting and informs about which *ids* require predictions at which timestamps\n",
        " - the API also provides a \"reward\" for each timestamp, in the form of an average R value over the predicted values for the previous day. It's possible to use this reward to do reinforcement-style learning\n",
        " - the code should expect and handle missing values\n",
        "\n",
        "The training is partitioned in such a way that:\n",
        " - the first half (split by time) is provided as a training set at the start of a run\n",
        " - the seconf half is streamed through the API, as though it is a holdout set\n",
        "\n",
        "The submissions are evaluated on the **R value** between the predicted and actual values. The R value is similar to the *$R^2$* value, also called the **coefficient of determination**. *$R^2$* can be calculated as:\n",
        "\n",
        "$$$\n",
        "R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\mu)^2}.\n",
        "$$$\n",
        "\n",
        "To calculate R, we then use:\n",
        "\n",
        "$$\n",
        "R = sign\\left( R^2 \\right) \\sqrt{\\left|R^2\\right|},\n",
        "$$\n",
        "\n",
        "where *yy* is the actual value, *\u03bc* is the mean of the actual values, and *\u0177* is the predicted value. Negative R values are clipped at -1, i.e. the score you see will be max(\u22121,R)max(\u22121,R)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3b2728cf-b9e0-f018-922d-e0f2480cd068"
      },
      "outputs": [],
      "source": [
        "# Pandas / Numpy for data loading and preparation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn as sk\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "\n",
        "# Charts / graphs\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import division\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "from matplotlib.colors import LogNorm\n",
        "import matplotlib.pylab as pylab\n",
        "import seaborn as sns\n",
        "\n",
        "# General purpose classes / utilities\n",
        "from copy import deepcopy\n",
        "import json\n",
        "import timeit\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "from ast import literal_eval\n",
        "import csv\n",
        "import ast\n",
        "import itertools\n",
        "\n",
        "# NLTK\n",
        "import nltk.corpus\n",
        "from nltk import SnowballStemmer\n",
        "\n",
        "# Scipy\n",
        "from scipy.stats import skew, boxcox\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Geopy for geographic clustering\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Keras framework for neural network training\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.models import model_from_json\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# XGBoost framework\n",
        "import xgboost as xgb\n",
        "\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "388c9376-acc8-1158-6f42-9d6904305076"
      },
      "source": [
        "# 1. Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3e899085-eef8-8e9d-a2bd-94268187bcd1"
      },
      "outputs": [],
      "source": [
        "# Location of files and basic identifiers\n",
        "ID = 'id'\n",
        "TIMESTAMP = 'timestamp'\n",
        "TARGET = 'y'\n",
        "SEED = 0\n",
        "DATA_DIR = \"../input\"\n",
        "TRAIN = \"{0}/train.h5\".format(DATA_DIR)\n",
        "TEST = \"{0}/train.h5.csv\".format(DATA_DIR)\n",
        "SUBMISSION = \"{0}/sample_submission.csv\".format(DATA_DIR)\n",
        "\n",
        "# read data\n",
        "with pd.HDFStore(TRAIN, \"r\") as train:\n",
        "    df = train.get(\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3ecc82b4-7715-eacc-74e2-fc5df352ad39"
      },
      "source": [
        "## 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a3f086b-eac6-fd8c-5b75-bd80758b6510"
      },
      "source": [
        "### 2.1 Basic stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c25d1c3-da0d-4fa0-0fd4-11612ba14607"
      },
      "outputs": [],
      "source": [
        "# print all rows and columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# basic stats\n",
        "print('The training set contains:')\n",
        "print('{} records'.format(df.shape[0]))\n",
        "print('{} features'.format(df.shape[1]))\n",
        "\n",
        "# check colums types and values\n",
        "features = list(df.columns)\n",
        "derived_features = [x for x in features if x.find('derived') != -1]\n",
        "fundamental_features = [x for x in features if x.find('fundamental') != -1]\n",
        "technical_features = [x for x in features if x.find('technical') != -1]\n",
        "\n",
        "print ('\\nFeature FAMILIES:')\n",
        "print ('- {} derived features'.format(len(derived_features)))\n",
        "print ('- {} fundamental features'.format(len(fundamental_features)))\n",
        "print ('- {} technical features'.format(len(technical_features)))\n",
        "print('\\nFeature TYPES:')\n",
        "print('{}'.format(df.dtypes.value_counts()))\n",
        "\n",
        "print('\\n{} distinct time series each with an average of {} datapoints over a total time span of {} periods'.format(df.id.nunique(), \n",
        "                                                                             int(np.round(df.shape[0]/df.id.nunique(),0)),\n",
        "                                                                             len(df.timestamp.unique())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ceadad9-e001-88f5-53ae-26acf2834581"
      },
      "source": [
        "So we have a market portfolio of 1424 securities of variable composition as some of them are evidently acquired / sold during the observation period. We can define the market portfolio of all the assets together and calculate the market return over the observation period:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3ff6e4c-c80d-cd59-d3f1-6a419ac30f39"
      },
      "outputs": [],
      "source": [
        "market = df[['timestamp', 'y']].groupby('timestamp').agg([np.mean, np.std, len]).reset_index()\n",
        "t      = market['timestamp']\n",
        "y_mean = np.array(market['y']['mean'])\n",
        "y_std  = np.array(market['y']['std'])\n",
        "n      = np.array(market['y']['len'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa1785b8-6b89-9bb5-dbfc-9e6ec6d839f5"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(t, y_std, '-')\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('std of y')\n",
        "plt.title('Portfolio VOLATILITY over time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "196a98ee-a279-7aef-1a18-da1ebb093266"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(t, y_mean, '-')\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('mean of y')\n",
        "plt.title('Portfolio AVERAGE VALUE over time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f21c794c-b1ca-ba12-e5a4-57470c45b8c9"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(t, y_std, '-')\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('std of y')\n",
        "plt.title('Portfolio VOLATILITY over time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "288fb8c7-6ca4-7cc9-db29-f6267df3bef1"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(t, n, '-')\n",
        "plt.xlabel('timestamp')\n",
        "plt.ylabel('portfolio size')\n",
        "plt.title('Portfolio ASSET COUNT over time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6358883a-e9aa-58a0-2fe4-7145741edf1c"
      },
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a15fdadb-700e-b56d-6394-68919f218bb6"
      },
      "outputs": [],
      "source": [
        "import kagglegym\n",
        "env = kagglegym.make()\n",
        "observation = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf7d8078-8459-2cec-1868-87fa34525184"
      },
      "outputs": [],
      "source": [
        "# Get the train dataframe\n",
        "train = observation.train\n",
        "mean_values = train.mean(axis=0)\n",
        "# median_values = train.median(axis=0)\n",
        "train.fillna(mean_values, inplace=True)\n",
        "\n",
        "\n",
        "cols_to_use = ['technical_30', 'technical_20', 'fundamental_11']\n",
        "# Observed with histograns:\n",
        "low_y_cut = -0.086093\n",
        "high_y_cut = 0.093497\n",
        "\n",
        "y_is_above_cut = (train.y > high_y_cut)\n",
        "y_is_below_cut = (train.y < low_y_cut)\n",
        "y_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d736ceb6-07e5-16b3-c39d-8dc164d5b5df"
      },
      "source": [
        "## 3. Model 1: Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2044916f-8179-7b9f-da61-2d1ce487f180"
      },
      "outputs": [],
      "source": [
        "def get_weighted_y(series):\n",
        "    id, y = series[\"id\"], series[\"y\"]\n",
        "    # return 0.95 * y + 0.05 * ymean_dict[id] if id in ymean_dict else y\n",
        "    return 0.95 * y + 0.05 * ymedian_dict[id] if id in ymedian_dict else y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "05fbc908-1484-a06a-3dbf-444ba4e1214b"
      },
      "outputs": [],
      "source": [
        "model = Ridge()\n",
        "model.fit(np.array(train.loc[y_is_within_cut, cols_to_use].values), train.loc[y_is_within_cut, TARGET])\n",
        "\n",
        "# ymean_dict = dict(train.groupby([\"id\"])[\"y\"].mean())\n",
        "ymedian_dict = dict(train.groupby([\"id\"])[\"y\"].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fe91f5f-d2b5-ebaa-d273-d61da65907ee"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    \n",
        "    # make prediction\n",
        "    observation.features.fillna(mean_values, inplace=True)\n",
        "    test_x = np.array(observation.features[cols_to_use].values)\n",
        "    observation.target.y = model.predict(test_x).clip(low_y_cut, high_y_cut)\n",
        "    # weighted y using average value\n",
        "    observation.target.y = observation.target.apply(get_weighted_y, axis = 1)\n",
        "    \n",
        "    # Execute step\n",
        "    target = observation.target\n",
        "    timestamp = observation.features[\"timestamp\"][0]\n",
        "    if timestamp % 100 == 0:\n",
        "        print(\"Timestamp #{}\".format(timestamp))\n",
        "\n",
        "    observation, reward, done, info = env.step(target)\n",
        "    if done:        \n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "263224eb-9542-2cc1-c78f-316db305964b"
      },
      "outputs": [],
      "source": [
        "print(info)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}