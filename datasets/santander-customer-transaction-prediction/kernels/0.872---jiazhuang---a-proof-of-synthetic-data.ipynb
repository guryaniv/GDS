{"cells":[{"metadata":{"_uuid":"6c22cd36989e2516ae04e5a9b7271fb2317bfa05"},"cell_type":"markdown","source":"This kernel is inspired by [@Branden Murray](https://www.kaggle.com/brandenkmurray)'s thoughts about [Any explanation why shuffling augmentation works?](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/84847). I quote his hypothesis:\n\n>Well, one explanation could be that it's how they generated the dataset in the first place. For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together.\n\nI'm going to generate positive(target==1) and negative(target==0) samples for each feature, then combine the 200 simulated features together, I can get a synthetic data. With this simulated data, I can test Brander Murray's hypothesis.  \n\nIf his hypothesis is true, then we can calculate the probability of positive sample(i.e. P(target=1|features)) using traditional Probability theory."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport lightgbm as lgb\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col=0)\ntest = pd.read_csv('../input/test.csv', index_col=0)\n\ntarget = train.target.values\ntrain.drop('target', axis=1, inplace=True)\ntrain.shape, target.shape, test.shape, ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ec24c8a5f02b6a6b52c4a0cfc5d0e4b87dca343"},"cell_type":"markdown","source":"### Calculate the mean/sd of postive and negative samples for each feature"},{"metadata":{"trusted":true,"_uuid":"4dd7a6c1e22cb73cc858e15df859af15aa887d4e"},"cell_type":"code","source":"pos_idx = (target == 1)\nneg_idx = (target == 0)\nstats = []\nfor col in train.columns:\n    stats.append([\n        train.loc[pos_idx, col].mean(),\n        train.loc[pos_idx, col].std(),\n        train.loc[neg_idx, col].mean(),\n        train.loc[neg_idx, col].std()\n    ])\n    \nstats_df = pd.DataFrame(stats, columns=['pos_mean', 'pos_sd', 'neg_mean', 'neg_sd'])\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df40b43dd1093f1a343ee0d24e46751fad17c1d7"},"cell_type":"markdown","source":"### Synthetic data using normal distribution with train's mean/sd"},{"metadata":{"trusted":true,"_uuid":"bfb5de8cc47dd4f3119434b26f27dc8410cb0fe5"},"cell_type":"code","source":"npos = pos_idx.sum()\nnneg = neg_idx.sum()\n\nsim_feats = []\nfor pos_mean, pos_sd, neg_mean, neg_sd in stats:\n    pos_feat = np.random.normal(loc=pos_mean, scale=pos_sd, size=npos)\n    neg_feat = np.random.normal(loc=neg_mean, scale=neg_sd, size=nneg)\n    sim_feats.append(np.hstack([pos_feat, neg_feat]))\n    \nsim_feats = np.column_stack(sim_feats)\nsim_target = np.hstack([np.ones(npos), np.zeros(nneg)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80dac04f855798bb0ba225d195a157474c0c0b42"},"cell_type":"code","source":"sim_feats.shape, sim_target.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1120fd25a1a81462564a6c207076a79a9a3601ae"},"cell_type":"markdown","source":"### Test the synthetic data"},{"metadata":{"trusted":true,"_uuid":"796ad7fe2bd30f9a75eb799380b2cc6413918efd"},"cell_type":"code","source":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2cc5423b030e2552d99cce44f99583d3c532a936"},"cell_type":"code","source":"trn_data = lgb.Dataset(sim_feats, sim_target)\ncv = lgb.cv(param, trn_data, 100000, shuffle=True, early_stopping_rounds=600, verbose_eval=600)\nprint(cv['auc-mean'][-1], len(cv['auc-mean']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ed25a9db120d64cc7e8d9c6cf860c814d22ec15"},"cell_type":"markdown","source":"We can achieve 0.885 just using train data's mean and sd, this is not bad! Maybe the data is generated using this way!"},{"metadata":{"_uuid":"718d970f266bf02a3ad6974098b70a9dcfc6b05d"},"cell_type":"markdown","source":"### Calculate probability use hypothetical test\n\nIf each feature is generated by sample positive samples and negtive samples, then we can use hypothetical test to distinguish them. The positive samples and negative samples of each feature are slightly different. Let's take `var_0` and `var_1` as an example. "},{"metadata":{"trusted":true,"_uuid":"1a4a308c7f5921e03c70e4b193c8d0af68a2acaa"},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\n# var_0\nplt.subplot(2, 2, 1)\nsns.distplot(train.loc[pos_idx, 'var_0'], hist=False, label='pos', color='blue')\nsns.distplot(train.loc[neg_idx, 'var_0'], hist=False, label='neg', color='orange')\nplt.vlines(x=[stats_df.loc[0, 'pos_mean'], stats_df.loc[0, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\nplt.xlabel('var_0')\nplt.title('Real data')\nplt.legend()\nplt.subplot(2, 2, 2)\nsns.distplot(sim_feats[pos_idx, 0], hist=False, label='pos', color='blue')\nsns.distplot(sim_feats[neg_idx, 0], hist=False, label='neg', color='orange')\nplt.vlines(x=[stats_df.loc[0, 'pos_mean'], stats_df.loc[0, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\nplt.title('Simulated data')\nplt.legend()\nplt.xlabel('var_0')\n\n# var_1\nplt.subplot(2, 2, 3)\nsns.distplot(train.loc[pos_idx, 'var_1'], hist=False, label='pos', color='blue')\nsns.distplot(train.loc[neg_idx, 'var_1'], hist=False, label='neg', color='orange')\nplt.vlines(x=[stats_df.loc[1, 'pos_mean'], stats_df.loc[1, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\nplt.xlabel('var_1')\nplt.legend()\nplt.subplot(2, 2, 4)\nsns.distplot(sim_feats[pos_idx, 1], hist=False, label='pos', color='blue')\nsns.distplot(sim_feats[neg_idx, 1], hist=False, label='neg', color='orange')\nplt.vlines(x=[stats_df.loc[1, 'pos_mean'], stats_df.loc[1, 'neg_mean']], ymin=0, ymax=0.15, colors=['blue', 'orange'])\nplt.legend()\nplt.xlabel('var_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d5fbad2ea6d050b923004c8827f08efce5f62e"},"cell_type":"markdown","source":"Let's use Z-test:\n- Null hypothesis: a sample is negative(target == 0)\n- Alternative hypothesis: a sample is not negative(target == 1)\n\nIf we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the smaller the pvalue, the more likely a sample is positive."},{"metadata":{"trusted":true,"_uuid":"f46b73e8a0e315152d56e95f53839e14a3e96c76"},"cell_type":"code","source":"zval1 = (train.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\nzval1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceeead2412109212f653e77e19d7c693e3262067"},"cell_type":"code","source":"pval1 = (1 - norm.cdf(np.abs(zval1))) * 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31f8412a261c1fe0c8bd8b5c6a19ecf07c48bcb1"},"cell_type":"code","source":"pval1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"713d8f2e9aeb8883da84e0556988264c12df0ced"},"cell_type":"markdown","source":"Since we have 200 feats, we get 200 pvalue for each sample, we can multiply them together."},{"metadata":{"trusted":true,"_uuid":"f5a8234c42ce1f42c3d912506fdbbc37c2141fde"},"cell_type":"code","source":"prob1 = pval1.prod(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab209f269dd312ca902d5ca9d4917410df8de24"},"cell_type":"markdown","source":"The smaller the prob1, the more likely a sample is positive. let's see the performance."},{"metadata":{"trusted":true,"_uuid":"4752c92556892466bbdf4c68566d42786086e2aa"},"cell_type":"code","source":"roc_auc_score(target, 1/prob1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40a6bc46f0a36b33384f8739480922a6057f177b"},"cell_type":"markdown","source":"If we test whether a sample is positive, we can get another hypothetical test:\n- Null hypothesis: a sample is positive(target == 1)\n- Alternative hypothesis: a sample is not positive(target == 0)\n\nIf we get a small pvalue(< 0.05), we reject the null hypothesis, i.e. the bigger the pvalue, the more likely a sample is positive."},{"metadata":{"trusted":true,"_uuid":"c4b82cd5fd6a52b224762f3345f4b93c4105f39d"},"cell_type":"code","source":"zval2 = (train.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\npval2 = (1 - norm.cdf(np.abs(zval2))) * 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ece3e71eb91724d0b597c59334dd9c1f6e31181"},"cell_type":"code","source":"prob2 = pval2.prod(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32aa392757e4b082ee6ab623451ce87222f44114"},"cell_type":"markdown","source":"Combine the two prob together:"},{"metadata":{"trusted":true,"_uuid":"1546e0c868c8d898ef9e5382f9f70a7a327b698c"},"cell_type":"code","source":"roc_auc_score(target, prob2 / prob1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2543026bc91c69dbd01a42b3ed8f92ed242d1449"},"cell_type":"markdown","source":"**We can get 0.874 just using Probability theory, It's quite good I think.**"},{"metadata":{"_uuid":"af270d931748f6278bd4f17dd4872bfd0aa5d1e5"},"cell_type":"markdown","source":" ### Use this mothed to predict test.csv"},{"metadata":{"trusted":true,"_uuid":"bc363987a92088476a3992793a8a6637884fe9f2"},"cell_type":"code","source":"te_zval1 = (test.values - stats_df.neg_mean.values) / stats_df.neg_sd.values\nte_pval1 = (1 - norm.cdf(np.abs(te_zval1))) * 2\nte_prob1 = te_pval1.prod(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c2eddc85582f97006c06eecef999fca2ec2dc4b"},"cell_type":"code","source":"te_zval2 = (test.values - stats_df.pos_mean.values) / stats_df.pos_sd.values\nte_pval2 = (1 - norm.cdf(np.abs(te_zval2))) * 2\nte_prob2 = te_pval2.prod(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae2cb11c7158f2fef1a8b3d12e6618bd771b83e"},"cell_type":"code","source":"pred = te_prob2 / te_prob1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c597d26f9f80d24ea97ed7a6e9645aed45e7a1b"},"cell_type":"code","source":"pd.DataFrame({\n    'ID_code': test.index,\n    'target': pred\n}).to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5be8528c3089850d5ff46a356c53f8968f9c649"},"cell_type":"markdown","source":"### Conclusion\n\nBranden Murray's hypothesis **For each feature they had a distribution for target==0 and a distribution for target==1 and they randomly sampled from each and then put it together** is a wonderful explanation of shuffling also works and weak interaction between features.\n\nWe can even use tranditional Probability theory to calculate the P(target==1) value to achive 0.874 local cv. But this model is still too naive, the feature is not normal distribution(I try normality test, none of the 200 features passed), and the positive samples and negative samples is not variance homogeneity(2/3 of the features failed variance homogeneity test).\n\nHope this kernal can help, thanks!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}