{"cells": [{"source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import sys\n", "from importlib import reload"], "outputs": [], "metadata": {"_uuid": "2a358ed7b06c5203dae31862c9dfdc53a8757c59", "_cell_guid": "8c56838a-8657-4e31-8a9a-8e0361f8cd51", "collapsed": true}, "cell_type": "code", "execution_count": 1}, {"source": ["data_dir = '../input/'\n", "train = pd.read_csv(data_dir + 'train.csv')\n", "test = pd.read_csv(data_dir + 'test.csv')"], "outputs": [], "metadata": {"_uuid": "153d58545b25f1cfd060d18896ac700a37225a6b", "_cell_guid": "ecc35233-cb10-4f57-95ca-c6bbafcad787", "collapsed": true}, "cell_type": "code", "execution_count": 2}, {"source": ["train.head()"], "outputs": [], "metadata": {"_uuid": "e88f03511d37ddfb1fe25080d10491b05ef2859a", "_cell_guid": "3be78db7-1ef8-4e54-b096-96bc723ee116"}, "cell_type": "code", "execution_count": 3}, {"source": ["**Lets do some preprocessing and convert sentences into words**"], "metadata": {"_uuid": "061d9db5a9823cffb2cbe80b565a8c5bc85c0d60", "_cell_guid": "53381987-e375-4446-aa04-b477e86d5ee5"}, "cell_type": "markdown"}, {"source": ["import re\n", "all_words = set([])\n", "train_words = []\n", "test_words = []\n", "train_words_len = []\n", "test_words_len = []\n", "regex = re.compile('[ ]')\n", "\n", "def preprocess_sentence(sentence):\n", "    #sentence = sentence.lower()\n", "    punctuations = [',', '.', '\"', '\\'', '?', ':', ';']\n", "    for p in punctuations:\n", "        sentence = sentence.replace(p, \" \" + p + \" \")\n", "    sentence = re.sub( '\\s+', ' ', sentence ).strip()\n", "    return sentence\n", "\n", "for sentence in train['text']:\n", "    sentence = preprocess_sentence(sentence)\n", "    words = regex.split(sentence)\n", "    train_words.append(words)\n", "    train_words_len.append(len(words))\n", "    for w in words:\n", "        all_words.add(w)\n", "        \n", "for sentence in test['text']:\n", "    sentence = preprocess_sentence(sentence)\n", "    words = regex.split(sentence)\n", "    test_words.append(words)\n", "    test_words_len.append(len(words))\n", "    for w in words:\n", "        all_words.add(w)\n"], "outputs": [], "metadata": {"_uuid": "3027dd16d8abbd596da27b20c8488cc869ea4ac4", "_cell_guid": "5235ecdc-9d38-42fb-8fa1-a8678726dd4a", "collapsed": true}, "cell_type": "code", "execution_count": 4}, {"source": ["train['word_len'] = train_words_len\n", "test['word_len'] = test_words_len"], "outputs": [], "metadata": {"_uuid": "be08de015ed572d804d091c684437900afbdb726", "_cell_guid": "9107a3fb-6985-45e5-a038-9ca6376efdfd", "collapsed": true}, "cell_type": "code", "execution_count": 5}, {"source": ["Lets plot words length to get an idea about how longs sentences are"], "metadata": {"_uuid": "0211372c5982df0e2e648fb1937889fd3d540189", "_cell_guid": "b06205af-13ec-402a-ac37-96f24bba8ebc"}, "cell_type": "markdown"}, {"source": ["plt.plot(train['word_len'])\n", "plt.ylabel('text lengths')\n", "plt.show()"], "outputs": [], "metadata": {"_uuid": "4af33a1166da28eeaf4dc6527a76ea1d7cb99141", "_cell_guid": "9ad26e43-38ea-47ee-8367-0c068f8de03d"}, "cell_type": "code", "execution_count": 6}, {"source": ["train['word_len'].describe()"], "outputs": [], "metadata": {"_uuid": "3c170ba43f0bcbcbfb32cc6af0291fe01071f6c8", "_cell_guid": "814d8757-e8b7-40de-8920-5ee72bfe1faa"}, "cell_type": "code", "execution_count": 7}, {"source": ["taking 150 as lenght. Sentences with less then 150 words will be padded with zero and sentences with more then 150 words will be  truncated."], "metadata": {"_uuid": "fd20550984c52fc8d435a118beb1b9b6fe47ac4c", "_cell_guid": "724b5c3d-b63f-4416-ac6c-9a87bba2c51c"}, "cell_type": "markdown"}, {"source": ["feature_len = 150"], "outputs": [], "metadata": {"_uuid": "e4885512ce8adf7a94f59ec863211bf5dec3f54f", "_cell_guid": "37758b90-8f4e-4487-ada3-ed7678e750f6", "collapsed": true}, "cell_type": "code", "execution_count": 8}, {"source": ["vocab_size = len(all_words) + 1"], "outputs": [], "metadata": {"_uuid": "51b7cf408b9afd406f06df1d3f8b53ff31ccb74a", "_cell_guid": "49325946-b377-4304-bdbe-cc031dbb0d22", "collapsed": true}, "cell_type": "code", "execution_count": 9}, {"source": ["word2ids = {}\n", "for index, word in enumerate(all_words):\n", "    word2ids[word] = index + 1"], "outputs": [], "metadata": {"_uuid": "f759671a50174e490d9e61accd565a27fc930e55", "_cell_guid": "70094d96-93bb-4099-80d6-c91428687389", "collapsed": true}, "cell_type": "code", "execution_count": 10}, {"source": ["**Lets create training data**"], "metadata": {"_uuid": "e29871bdea6e1a4a2721964eec5562c80d0932fc", "_cell_guid": "7359d7d5-1d48-422a-b82f-6dc54a8b4792"}, "cell_type": "markdown"}, {"source": ["x_train = []\n", "x_test = []\n", "y_train = []\n", "for index, row in train.iterrows():\n", "    x = np.zeros(feature_len)\n", "    for ix, word in enumerate(train_words[index]):\n", "        if (ix >= feature_len):\n", "            break\n", "        x[ix] = word2ids[word]\n", "    x_train.append(x)\n", "    y = [0, 0, 0]\n", "    if row['author']=='EAP':\n", "        y[0] = 1\n", "    elif row['author']=='HPL':\n", "        y[1] = 1\n", "    else:\n", "        y[2] = 1\n", "    y_train.append(y)\n", "\n", "for index, row in test.iterrows():\n", "    x = np.zeros(feature_len)\n", "    for ix, word in enumerate(test_words[index]):\n", "        if (ix >= feature_len):\n", "            break\n", "        x[ix] = word2ids[word]\n", "    x_test.append(x)\n"], "outputs": [], "metadata": {"_uuid": "bfa337cac58c5caddccc3b41c85d17d7ac7d3091", "_cell_guid": "b572f5b1-93d5-4dc8-8187-e44a35b3680b", "collapsed": true}, "cell_type": "code", "execution_count": 11}, {"source": ["#validation set\n", "msk = np.random.rand(len(x_train)) < 0.95\n", "x_train = np.array(x_train)\n", "y_train = np.array(y_train)\n", "x_valid = x_train[~msk]\n", "y_valid = y_train[~msk]\n", "x_train = x_train[msk]\n", "y_train = y_train[msk]"], "outputs": [], "metadata": {"_uuid": "8ef2a936f8c8972e8505036a3e97bd2f76370cfb", "_cell_guid": "5b5e69a2-6e12-4296-aa4f-fb42325c1873", "collapsed": true}, "cell_type": "code", "execution_count": 12}, {"source": ["### Lets create simple CNN with embeddings using keras"], "metadata": {"_uuid": "3b02d0b144856e05f0673a410b0b90a6236efc33", "_cell_guid": "c4c30ba4-f598-42b4-9fd7-a78e812d6485"}, "cell_type": "markdown"}, {"source": ["import keras\n", "from keras.layers import *\n", "from keras.layers.embeddings import Embedding\n", "from keras import regularizers\n", "from keras.models import Model\n", "from keras.optimizers import SGD, RMSprop, Adam\n", "from keras.models import Sequential\n", "\n", "from keras.layers import Dense, Activation"], "outputs": [], "metadata": {"_uuid": "50985cea5ce312299d9c4213ae55c530359da438", "_cell_guid": "5a009b16-dd94-46b4-9d4d-f025a7212fce"}, "cell_type": "code", "execution_count": 13}, {"source": ["import gensim\n", "from gensim.models import *"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": 14}, {"source": ["n_fact = 30"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": 15}, {"source": ["all_sentences = train_words + test_words\n", "word2vec = Word2Vec(all_sentences, size=n_fact, window=7, min_count=2, workers=4, iter=20)"], "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": 16}, {"source": ["word2vec.wv.similar_by_word('The')"], "outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 17}, {"source": ["from numpy.random import random, normal\n", "\n", "emb = np.zeros((vocab_size, n_fact))\n", "for word,idx in word2ids.items():\n", "    print(word)\n", "    if word in word2vec.wv:\n", "        emb[idx] = word2vec.wv[word]\n", "    else:\n", "        print(\"word is not present:\" + str(word))\n", "        emb[idx] = normal(scale=0.6, size=(n_fact,))"], "outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 18}, {"source": ["model = Sequential([\n", "    Embedding(vocab_size, n_fact, input_length=feature_len, weights=[emb], trainable=True),\n", "    Dropout(0.5),\n", "    Conv1D(30, 7, border_mode='same', activation='relu'),\n", "    Dropout(0.5),\n", "    Flatten(),\n", "    BatchNormalization(),\n", "    Dense(40, activation='relu'),\n", "    Dropout(0.6),\n", "    Dense(3, activation='softmax')])"], "outputs": [], "metadata": {"_uuid": "19f43a411f4d4347d2b347e84541c8e24cea9d62", "_cell_guid": "6efaf09e-1174-4b86-b214-adcb6a2b64a7"}, "cell_type": "code", "execution_count": 19}, {"source": ["model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n", "model.summary()"], "outputs": [], "metadata": {"_uuid": "c7f3116d7e9f47eb4506855bd2feabd1b19b1869", "_cell_guid": "640ade03-6827-42aa-bde8-a00f5606bfb6"}, "cell_type": "code", "execution_count": 20}, {"source": ["model.fit(np.array(x_train), y_train, validation_data=(np.array(x_valid),y_valid), nb_epoch=3, batch_size=64)"], "outputs": [], "metadata": {"_uuid": "53f7473ca16c9df7c5904bb9dea20883b9a26e2c", "_cell_guid": "d6d849c5-da81-4692-996a-08481c2197da"}, "cell_type": "code", "execution_count": 21}, {"source": ["That's amazing, In just 3 epochs we got ~ 81 %  acuracy and validation los of ~ .48. Lets run few more."], "metadata": {"_uuid": "30fb97a804ea0912c472c46b79ed874363d9c9ab", "_cell_guid": "f0f8bece-4843-4387-b1fa-8568f588d44b"}, "cell_type": "markdown"}, {"source": ["model.fit(np.array(x_train), y_train, validation_data=(np.array(x_valid),y_valid), nb_epoch=3, batch_size=64)"], "outputs": [], "metadata": {"_uuid": "da3907cfeffd1623cde1a718c044405989b4fc03", "_cell_guid": "32debf09-d947-428f-b66c-c62b5ed5f52b"}, "cell_type": "code", "execution_count": 22}, {"source": ["We are seeing that model is already overfitting. Let predit test data now."], "metadata": {"_uuid": "5096c418771180079ea2981e5a535a9edd5d75f9", "_cell_guid": "4a52a481-ee7c-474c-8eed-39f694db0687"}, "cell_type": "markdown"}, {"source": ["pred = model.predict(np.array(x_test))"], "outputs": [], "metadata": {"_uuid": "01129fe4a6a9d53d300bbbe3fd46f2b513a2544c", "_cell_guid": "b96c3a0b-5e19-41e8-b708-0faa13b6d646", "collapsed": true}, "cell_type": "code", "execution_count": 23}, {"source": ["Lets create submission file"], "metadata": {"_uuid": "9e1b65c397bcc23e24b5ba00b9863b7cbe7aecf8", "_cell_guid": "f54f447b-a497-4030-81fc-963c3c01aa4f"}, "cell_type": "markdown"}, {"source": ["submission  = pd.DataFrame()\n", "submission ['id'] = test['id']\n", "submission ['EAP'] = pred[:,0]\n", "submission ['HPL'] = pred[:,1]\n", "submission ['MWS'] = pred[:,2]\n", "submission.to_csv('sub.csv', index=False)"], "outputs": [], "metadata": {"_uuid": "c6703c0476400767d0c59ef0de19b334ba4f8f37", "_cell_guid": "5e035c35-f898-4214-9e38-a0bc727bd997", "collapsed": true}, "cell_type": "code", "execution_count": 24}, {"source": ["!ls -l"], "outputs": [], "metadata": {"_uuid": "7229f4bb374427e86e614aaf69386b77bc830dec", "_cell_guid": "86d0853f-71bf-4aad-ac63-074b47ccfb4d"}, "cell_type": "code", "execution_count": 25}, {"source": ["I am new to python and data science and this is my first model so If I have made some basic mistake. Please let me know. :)"], "metadata": {"_uuid": "7aea44b8ad66b37b2cd14510cf64d5552d24ef24", "_cell_guid": "466a0ec0-9630-4462-b498-70bcd0ec8e52", "collapsed": true}, "cell_type": "markdown"}, {"source": [], "outputs": [], "metadata": {"_uuid": "c6c55e74ecb35c40017c79649bf6bf23afb41c93", "_cell_guid": "6c41954d-bd9b-440f-9e36-05948aea013d", "collapsed": true}, "cell_type": "code", "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "version": "3.6.3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}