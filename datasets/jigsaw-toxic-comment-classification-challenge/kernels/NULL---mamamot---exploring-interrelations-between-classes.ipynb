{"nbformat": 4, "cells": [{"source": ["# Exploring Interrelations between Classes\n", "We can assume that given classes toxic and severe_toxic, having one class implies having the other. What about other target variables - does one variable affect another and can we use it to improve the final score? This notebook is an attempt to glimpse into the decision-making process of the annotators and also in the nature of the data itself. We will calculate one standard and one not-so-standard statistic and visualize them using plotly heatmaps."], "cell_type": "markdown", "metadata": {}}, {"source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n", "import plotly.graph_objs as go\n", "import plotly.figure_factory as ff\n", "init_notebook_mode(connected=True)"], "cell_type": "code", "metadata": {"_uuid": "17d5dbef0f8bf58110e49633a82eebfd13c5d270", "_cell_guid": "5160c093-8d61-4c91-986f-c323da566f74"}, "outputs": [], "execution_count": 24}, {"source": ["First, we load the dataset and separate text data from the target variables."], "cell_type": "markdown", "metadata": {}}, {"source": ["train = pd.read_csv('../input/train.csv')\n", "train.index = train['id']\n", "x_train = train['comment_text']\n", "y_train = train.iloc[:, 2:]"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "execution_count": 2}, {"source": ["Then, let's calculate Spearman correlation between the target variables."], "cell_type": "markdown", "metadata": {}}, {"source": ["correlation_matrix = y_train.corr()"], "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": ["..and plot it using a heatmap."], "cell_type": "markdown", "metadata": {}}, {"source": ["heatmap = go.Heatmap(\n", "    z=np.flip(correlation_matrix.values, axis=1),  # try it without flipping - looks unusual\n", "    x=y_train.columns[::-1],\n", "    y=y_train.columns,\n", "    showscale=False,\n", "    colorscale=\"viridis\"\n", ")\n", "\n", "layout = go.Layout(\n", "    title=\"Correlation between target variables\",\n", "    showlegend=False,\n", "    width=700, height=700,\n", "    autosize=False,\n", "    margin=go.Margin(l=100, r=100, b=100, t=100, pad=4)\n", ")\n", "\n", "fig = go.Figure(data=[heatmap], layout=layout)\n", "iplot(fig, filename='hmap')"], "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 36}, {"source": ["We see that, for instance, obscene and insult really correlate with each other, wich is kind of logical - most insults are obscene. But this doesn't give us the full picture. What is really important is, according to annotators, do we always assign \"toxic\" for \"sever_toxic\"? And what about \"obscene\"?\n", "\n", "Let's calculate a following matrix: given classes *i* and *j*, what proportion of objects belonging to class *i* also belong to class *j*?"], "cell_type": "markdown", "metadata": {}}, {"source": ["dim = y_train.shape[1]\n", "cooccurence_matrix = np.zeros((dim, dim))\n", "\n", "for i in range(dim):\n", "    for j in range(dim):\n", "        res = sum(y_train.iloc[:, i] & y_train.iloc[:, j]) / sum(y_train.iloc[:, i])\n", "        cooccurence_matrix[i, j] = res"], "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "execution_count": 54}, {"source": ["And another heatmap:"], "cell_type": "markdown", "metadata": {}}, {"source": ["heatmap = go.Heatmap(\n", "    z=np.flip(cooccurence_matrix, axis=1),\n", "    x=y_train.columns[::-1],\n", "    y=y_train.columns,\n", "    showscale=False,\n", "    colorscale=\"viridis\"\n", ")\n", "\n", "layout = go.Layout(\n", "    title=\"Coocurence of target variables\",\n", "    showlegend=False,\n", "    width=700, height=700,\n", "    autosize=False,\n", "    margin=go.Margin(l=100, r=100, b=100, t=100),\n", "    yaxis=dict(\n", "        title='If...'\n", "    ),\n", "    xaxis=dict(\n", "        title='then...'\n", "    )\n", ")\n", "\n", "fig = go.Figure(data=[heatmap], layout=layout)\n", "iplot(fig, filename='hmap')"], "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 55}, {"source": ["What this means is *if we have severe_toxic, we have toxic 100% of the time* (hover over the plot to see the figures). This means it is a good sanity check to see if your sever_toxic comments also have toxic class. Also threats are toxic in 95% and obscenities are toxic in 94% of the cases.\n", "\n", "If a comment is an insult, it is obscene in 78% of the cases, which is, as already said, quite logical. More interestingly, 86% of sever_toxic comments also contains insults, as well as 82% of identity_hate instances. But if a comment is obscene, it is a threat only 3% of the time, but if it is a threat, it more probably than not contains an insult (65%)."], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3"}}, "nbformat_minor": 1}