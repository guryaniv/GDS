{"cells": [{"outputs": [], "execution_count": null, "source": ["# Some concepts borrowed from other kernels (will give credit later)\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import warnings\n", "from collections import Counter\n", "from sklearn.feature_selection import mutual_info_classif\n", "warnings.filterwarnings('ignore')"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train = pd.read_csv(\"train.csv\")\n", "#train.head()"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train_copy = train\n", "train_copy = train_copy.replace(-1, np.NaN)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["import missingno as msno\n", "# Nullity or missing values by columns\n", "msno.matrix(df=train_copy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))\n", "\n", "#ps_reg_03, ps_car_03_cat and ps_car_05_cat has many missing values, hence we need to be carefull while doing NA. For the time being we will just do na, but later try different things"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["data = [go.Bar(\n", "            x = train[\"target\"].value_counts().index.values,\n", "            y = train[\"target\"].value_counts().values,\n", "            text='Distribution of target variable'\n", "    )]\n", "\n", "layout = go.Layout(\n", "    title='Target variable distribution'\n", ")\n", "\n", "fig = go.Figure(data=data, layout=layout)\n", "\n", "py.iplot(fig, filename='basic-bar')\n", "\n", "#Skewed target, Hence F1 score is more important than accuracy"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#Remove colums if contains all NULL (none so here)\n", "train = train.dropna(axis=1, how='all')"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["cols = train.columns.tolist()\n", "print cols"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["target = train.target\n", "train.drop('target', axis=1, inplace=True)\n", "train.drop('id', axis=1, inplace=True)\n", "train.dtypes\n", "\n", "#float64 are continuous variable , int64 are either binary or categorical"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["cols = train.columns.tolist()\n", "print len(cols)\n", "print(train.skew())\n", "# May require transformaion of the skewed variables"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train.describe()\n", "# Different stastical figures"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn import preprocessing\n", "def draw_histograms(df, variables, n_rows, n_cols):\n", "    fig=plt.figure(figsize=(60,80))\n", "    for i, var_name in enumerate(variables):\n", "        ax=fig.add_subplot(n_rows,n_cols,i+1)\n", "        df[var_name].hist(bins=10,ax=ax)\n", "        ax.set_title(var_name)\n", "    fig.tight_layout()  # Improves appearance a bit.\n", "    plt.show()\n", "\n", "le = preprocessing.LabelEncoder()\n", "for x in range(len(cols)):\n", "    typ = train[cols[x]].dtype\n", "    if typ == 'int64':\n", "        train[cols[x]] = train[cols[x]].fillna(value=0)\n", "    elif typ == 'float64':\n", "        train[cols[x]] = train[cols[x]].fillna(value=0.0)\n", "    elif typ == 'object':\n", "        pass\n", "        train[cols[x]] = train[cols[x]].fillna(value=0)\n", "draw_histograms(train, train.columns, 8, 8)\n", "\n", "#looking at the histogram of the input variable, some type of normalization of feature scaling may be required for some variables"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train.plot.box(return_type='axes', figsize=(90,70))\n", "#Box plot of all varibales https://www.wellbeingatschool.org.nz/information-sheet/understanding-and-interpreting-box-plots"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["Counter(train.dtypes.values)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train_float = train.select_dtypes(include=['float64'])\n", "train_int = train.select_dtypes(include=['int64'])"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["colormap = plt.cm.inferno\n", "plt.figure(figsize=(16,12))\n", "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "sns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n", "\n", "#pearson correlation of continuous varibales shows some strong correlation between variables, we may have to drop some correlated varibales or have to transform them to new variables"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["cat_features = [a for a in train.columns if a.endswith('cat')]\n", "print cat_features"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["import scipy \n", "from scipy.stats import spearmanr\n", "from pylab import rcParams"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train_cat = train[cat_features]\n", "train_cat.head()"], "metadata": {}, "cell_type": "code"}, {"source": [], "metadata": {}, "cell_type": "raw"}, {"outputs": [], "execution_count": null, "source": ["bin_features = [a for a in train.columns if a.endswith('bin')]\n", "print bin_features"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["bin_train = train[bin_features]\n", "bin_train.head()"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#rcParams['figure.figsize'] = 5, 4\n", "#sns.set_style(\"whitegrid\")"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#sns.pairplot(train_cat)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from scipy.stats import chi2_contingency\n", "for x in range(len(cat_features)):\n", "    for y in range((x+1), len(cat_features)):\n", "        col1 = train_cat[cat_features[x]]\n", "        col2 = train_cat[cat_features[y]]\n", "        table = pd.crosstab(col1,col2)\n", "        chi2, p, dof, expected = chi2_contingency(table.values)\n", "        print cat_features[x], cat_features[y], ':'\n", "        print 'Chi-square statistics: %0.3f p_value: %0.3f' % (chi2, p)\n", "        \n", "#chisquare test show heavy correlation between categorical vribales, not sure about it"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#from scipy.stats import chi2_contingency\n", "for x in range(len(bin_features)):\n", "    for y in range((x+1), len(bin_features)):\n", "        col1 = bin_train[bin_features[x]]\n", "        col2 = bin_train[bin_features[y]]\n", "        table = pd.crosstab(col1,col2)\n", "        chi2, p, dof, expected = chi2_contingency(table.values)\n", "        print bin_features[x], bin_features[y], ':'\n", "        print 'Chi-square statistics: %0.3f p_value: %0.3f' % (chi2, p)\n", "        \n", "#chisquare test of binary variables"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#sns.pairplot(bin_train)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn.feature_selection import VarianceThreshold\n", "train_bin_copy = bin_train\n", "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n", "sel.fit_transform(train_bin_copy)\n", "\n", "print train_bin_copy.columns\n", "print bin_train.columns"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn.feature_selection import VarianceThreshold\n", "train_cat_copy = train_cat\n", "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n", "sel.fit_transform(train_cat_copy)\n", "\n", "print train_cat_copy.columns\n", "print train_cat.columns"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn.feature_selection import SelectKBest\n", "from sklearn.feature_selection import chi2\n", "\n", "print bin_train.head()\n", "X_new = SelectKBest(chi2, k=5).fit_transform(bin_train, target)\n", "X_new.shape\n", "\n", "#SelectKbest does univariate feature selection , can ue chi2, ANOVA etc. http://scikit-learn.org/stable/modules/feature_selection.html\n", "# K is number of most important features"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["print X_new"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#Tree based feature selection, I would first do the hyperparameter tuning usig GBM and then use SelectFromModel to choose the best features\n", "#  have used the the link to do hyperparameter tuning\n", "\n", "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n", "from sklearn import cross_validation, metrics   #Additional scklearn functions\n", "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n", "\n"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#I ran following gridsearch method to come up with best parameters, hence commenting them\n", "\n", "#param_test1 = {'n_estimators':range(20,81,10)}\n", "#gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n", "#param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch1.fit(train, target)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n", "#gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n", "#param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch2.fit(train,target)\n", "#gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test3 = {'min_samples_leaf':range(30,71,10)}\n", "#gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7,min_samples_split=600,max_features='sqrt', subsample=0.8, random_state=10), \n", "#param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch3.fit(train,target)\n", "#gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test4 = {'max_features':range(30,46,2)}\n", "#gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_split=600, min_samples_leaf=50, subsample=0.8, random_state=10),\n", "#param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch4.fit(train,target)\n", "#gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n", "#gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7,min_samples_split=600, min_samples_leaf=50, random_state=10,max_features=31),\n", "#param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch5.fit(train,target)\n", "#gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["import xgboost as xgb\n", "from xgboost.sklearn import XGBClassifier\n", "from sklearn import cross_validation, metrics   #Additional scklearn functions\n", "from sklearn.grid_search import GridSearchCV   #Perforing grid search"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test1 = {\n", "# 'max_depth':range(3,10,2),\n", "# 'min_child_weight':range(1,6,2)\n", "#}\n", "#gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n", "# min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=50, scale_pos_weight=1, seed=27), \n", "# param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch1.fit(train,target)\n", "#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test2 = {\n", "# 'max_depth':[5,6,7],\n", "# 'min_child_weight':[1,3,5]\n", "#}\n", "#gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n", "# min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=70, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test2, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n", "#gsearch2.fit(train,target)\n", "#gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test2b = {\n", "# 'min_child_weight':[6,8,10,12]\n", "#}\n", "#gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n", "# min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch2b.fit(train,target)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test3 = {\n", "# 'gamma':[i/10.0 for i in range(0,5)]\n", "#}\n", "#gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n", "# min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch3.fit(train,target)\n", "#gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test4 = {\n", "# 'subsample':[i/10.0 for i in range(6,10)],\n", "# 'colsample_bytree':[i/10.0 for i in range(6,10)]\n", "#}\n", "#gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n", "# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch4.fit(train,target)\n", "#gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test5 = {\n", "# 'subsample':[i/100.0 for i in range(75,90,5)],\n", "# 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n", "#}\n", "#gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n", "# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch5.fit(train,target)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#param_test7 = {\n", "# 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n", "#}\n", "#gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n", "# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n", "# objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n", "# param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n", "#gsearch7.fit(train,target)\n", "#gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from matplotlib.pylab import rcParams\n", "rcParams['figure.figsize'] = 12, 4\n", "\n", "def modelfit(alg, dtrain, targer,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n", "    \n", "    if useTrainCV:\n", "        xgb_param = alg.get_xgb_params()\n", "        xgtrain = xgb.DMatrix(dtrain.values, label=target.values)\n", "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n", "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n", "        alg.set_params(n_estimators=cvresult.shape[0])\n", "    \n", "    #Fit the algorithm on the data\n", "    alg.fit(dtrain, target,eval_metric='auc')\n", "        \n", "    #Predict training set:\n", "    dtrain_predictions = alg.predict(dtrain)\n", "    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n", "        \n", "    #Print model report:\n", "    print \"\\nModel Report\"\n", "    print \"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions)\n", "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob)\n", "                    \n", "    feat_imp = pd.Series(alg.feature_importances_, dtrain.columns).sort_values(ascending=False)\n", "    feat_imp.plot(kind='bar', title='Feature Importances')\n", "    plt.ylabel('Feature Importance Score')\n", "\n", "xgb4 = XGBClassifier(\n", " learning_rate =0.01,\n", " n_estimators=5000,\n", " max_depth=5,\n", " min_child_weight=6,\n", " gamma=0,\n", " subsample=0.8,\n", " colsample_bytree=0.8,\n", " reg_alpha=0,\n", " objective= 'binary:logistic',\n", " nthread=80,\n", " scale_pos_weight=1,\n", " seed=27)\n", "modelfit(xgb4, train, target)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn.feature_selection import SelectFromModel\n", "\n", "clf = XGBClassifier(\n", " learning_rate =0.01,\n", " n_estimators=5000,\n", " max_depth=5,\n", " min_child_weight=6,\n", " gamma=0,\n", " subsample=0.8,\n", " colsample_bytree=0.8,\n", " reg_alpha=0,\n", " objective= 'binary:logistic',\n", " nthread=80,\n", " scale_pos_weight=1,\n", " seed=27)\n", "clf = clf.fit(train, target)\n", "clf.feature_importances_  \n", "\n", "model = SelectFromModel(clf, prefit=True)\n", "X_new = model.transform(train)\n", "X_new.shape \n", "\n", "#This is choosing 23 features out of 50 features"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from matplotlib.pylab import rcParams\n", "rcParams['figure.figsize'] = 12, 4\n", "\n", "\n", "def modelfit(alg, dtrain, target, performCV=True, printFeatureImportance=True, cv_folds=5):\n", "    #Fit the algorithm on the data\n", "    alg.fit(dtrain, target)\n", "        \n", "    #Predict training set:\n", "    dtrain_predictions = alg.predict(dtrain)\n", "    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n", "    \n", "    #Perform cross-validation:\n", "    if performCV:\n", "        cv_score = cross_validation.cross_val_score(alg, dtrain, target, cv=cv_folds, scoring='roc_auc')\n", "    \n", "    #Print model report:\n", "    print \"\\nModel Report\"\n", "    print \"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions)\n", "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob)\n", "    \n", "    if performCV:\n", "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n", "        \n", "    #Print Feature Importance:\n", "    if printFeatureImportance:\n", "        feat_imp = pd.Series(alg.feature_importances_, dtrain.columns).sort_values(ascending=False)\n", "        feat_imp.plot(kind='bar', title='Feature Importances')\n", "        plt.ylabel('Feature Importance Score')\n", "        \n", "\n", "gbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_split=600,min_samples_leaf=50, subsample=0.80, random_state=10, max_features=31)\n", "modelfit(gbm_tuned_2, train, target)"], "metadata": {}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["from sklearn.feature_selection import SelectFromModel\n", "\n", "clf = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_split=600,min_samples_leaf=50, subsample=0.80, random_state=10, max_features=31)\n", "clf = clf.fit(train, target)\n", "clf.feature_importances_  \n", "\n", "model = SelectFromModel(clf, prefit=True)\n", "X_new = model.transform(train)\n", "X_new.shape "], "metadata": {"collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": [], "metadata": {"collapsed": true}, "cell_type": "code"}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python2", "display_name": "Python 2", "language": "python"}, "language_info": {"name": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.12"}}}