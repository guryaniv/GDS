{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1a1b68b5-f272-d015-4355-e080a09a9888"
      },
      "source": [
        "**Overview** This notebook provides basic data exploration and wrangling. Prediction model is based on gradient boosting decision tree (from scikit-learn) with additional consideration of abnormal peaks in target variable, regression is tuned with randomized search cross validation.  At the end of notebook is unfinished attempt to apply adversarial validation. \n",
        "\n",
        "Based on kernels:\n",
        "\n",
        "[Simple data exploration](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-sberbank) \n",
        "\n",
        "[Adversarial validation](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b901964-0ff8-07c0-e77c-041553083c81"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.metrics import  precision_score,recall_score,average_precision_score,roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\n",
        "from pylab import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = 10, 10\n",
        "color = sns.color_palette()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "00c4fd34-2696-73ed-e5f8-eb1951533012"
      },
      "source": [
        "### 1.Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b579fd5-fa46-16ca-7567-ebf6da77be22"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "id_test = test_df.id\n",
        "\n",
        "print('train_df shape:',train_df.shape)\n",
        "print('test_df shape:',test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb7b7782-3a4d-a259-7834-6867311b4c6e"
      },
      "outputs": [],
      "source": [
        "dtype_df = train_df.dtypes.reset_index()\n",
        "dtype_df.columns = [\"Count\", \"Column Type\"]\n",
        "print('Variables data type:')\n",
        "dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a0b4c5d-00e1-b597-c430-739d4d01f171"
      },
      "source": [
        "### Data quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "edcb865f-6273-25f4-1b79-65a8b6e4105d"
      },
      "outputs": [],
      "source": [
        "train_df.describe().round(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a345d76c-ae5d-8b68-ae6c-99d68f7b0c97"
      },
      "source": [
        "build_year column contain outlier greater by few orders of magnitude than rest of data, probably second half of number (another year) belong to next row of data.\n",
        "Also maximal value in state is out of scale (1-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee8a8724-9e34-5ad0-e22c-942a9f05c16a"
      },
      "outputs": [],
      "source": [
        "print(train_df.loc[train_df['build_year'] == 20052009].id)\n",
        "print(train_df.loc[train_df['state'] == 33].id)\n",
        "print('build_year:',train_df.ix[10090].build_year)\n",
        "print('state:',train_df.ix[10090].state)\n",
        "\n",
        "train_df.loc[train_df['id'] == 10092, 'build_year'] = 2007\n",
        "train_df.loc[train_df['id'] == 10092, 'state'] = 3\n",
        "train_df.loc[train_df['id'] == 10093, 'build_year'] = 2009"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "729b636c-f52d-c094-7de3-0cd9f424b0fd"
      },
      "outputs": [],
      "source": [
        "train_df.describe().round(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "41d46be6-be6c-defb-4622-0aa00219d487"
      },
      "source": [
        "### Missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1baecda-382d-482b-782a-aa7f2c5b4f1d"
      },
      "outputs": [],
      "source": [
        "train_na = (train_df.isnull().sum() / len(train_df)) * 100\n",
        "train_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\n",
        "sns.barplot(y=train_na.index, x=train_na,color=color[0])\n",
        "plt.xlabel('% missing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "73528b4c-b012-0eeb-6cea-f85f3af10905"
      },
      "source": [
        "Encoding categorical data to numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71b6e2e4-fe89-751c-969e-4c205f5bb42d"
      },
      "outputs": [],
      "source": [
        "for f in train_df.columns:\n",
        "    if train_df[f].dtype=='object':\n",
        "        lbl = LabelEncoder()\n",
        "        lbl.fit(list(train_df[f].values)) \n",
        "        train_df[f] = lbl.transform(list(train_df[f].values))\n",
        "        \n",
        "for c in test_df.columns:\n",
        "    if test_df[c].dtype == 'object':\n",
        "        lbl = LabelEncoder()\n",
        "        lbl.fit(list(test_df[c].values)) \n",
        "        test_df[c] = lbl.transform(list(test_df[c].values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ba96b990-57be-a4b7-4b2a-61b3e120e952"
      },
      "source": [
        "Variables like life_sq or kitch_sq are important in prediction (see below Features importance), and because they are linked with full_sq it is better to fill missing values with ratio of full_sq than median or mean.\n",
        "\n",
        "Rest of missing values is filled with median of respected feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e8947de4-df6c-beee-fc26-be22a1e318b8"
      },
      "outputs": [],
      "source": [
        "kitch_ratio = train_df['full_sq']/train_df['kitch_sq']\n",
        "train_df['kitch_sq']=train_df['kitch_sq'].fillna(train_df['full_sq'] /kitch_ratio.median())\n",
        "test_df['kitch_sq']=test_df['kitch_sq'].fillna(test_df['full_sq'] /kitch_ratio.median())\n",
        "\n",
        "lifesq_ratio = train_df['full_sq']/train_df['life_sq']\n",
        "train_df['life_sq']=train_df['life_sq'].fillna(train_df['full_sq'] /lifesq_ratio.median())\n",
        "test_df['life_sq']=test_df['life_sq'].fillna(test_df['full_sq'] /lifesq_ratio.median())\n",
        "\n",
        "train_df=train_df.fillna(train_df.median(),inplace=True)\n",
        "test_df=test_df.fillna(test_df.median(),inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f95a9141-a29d-05f9-4504-2c7a5fd1be50"
      },
      "source": [
        "### Exploration of target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35e5edcf-e859-c17f-c19f-df9904c19296"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train_df.price_doc.values, kde=None)\n",
        "plt.xlabel('price')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b54b73ec-bf89-633c-70ee-ba0d1eb25973"
      },
      "source": [
        "Our target variable is spread across few orders of magnitude, it's more suitable to work with log10 of this value.\n",
        "Also it's reasonable to 'coarse grind' our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1f442408-115e-3ac5-60ed-9338243e88b0"
      },
      "outputs": [],
      "source": [
        "ulimit = np.percentile(train_df.price_doc.values, 99)\n",
        "llimit = np.percentile(train_df.price_doc.values, 1)\n",
        "train_df.loc[train_df['price_doc'] >ulimit, 'price_doc'] = ulimit\n",
        "train_df.loc[train_df['price_doc'] <llimit, 'price_doc'] = llimit\n",
        "\n",
        "sns.distplot(np.log(train_df.price_doc.values),  bins=50,kde=None)\n",
        "plt.xlabel('price')\n",
        "\n",
        "train_df['price_doc_log'] = np.log1p(train_df['price_doc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "04485af3-e399-fcca-2c62-1a746780bb27"
      },
      "source": [
        "Log of our data have distribution close to normal, with exception to two abnormal peaks on left side.  Lets mark this values for further 'investigation'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da2a9559-1d43-5b6c-5589-4810194485e4"
      },
      "outputs": [],
      "source": [
        "print(train_df['price_doc'].value_counts().head(10))\n",
        "\n",
        "train_df['label_value'] = 0\n",
        "train_df.loc[train_df['price_doc'] == 1000000, 'label_value'] = 1\n",
        "train_df.loc[train_df['price_doc'] == 2000000, 'label_value'] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "60bc6669-98ae-a214-3582-6c7e5765104b"
      },
      "source": [
        "### 2.Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9068ff9f-d032-429e-0e82-9e6ad9cfdf53"
      },
      "outputs": [],
      "source": [
        "data_X = train_df.drop([\"id\",\"timestamp\",\"price_doc\",\"price_doc_log\",'label_value'],axis=1)\n",
        "data_y = train_df['price_doc_log']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "906cd41e-6863-5446-2520-bbba2bb43a58"
      },
      "source": [
        "Gradient boosted tree was tuned with randomized cross validation. Because it's time consuming to compute, results are listed below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93a1821f-a1bc-9461-c9a2-e8fcd13980fc"
      },
      "outputs": [],
      "source": [
        "# GBmodel = GradientBoostingRegressor()\n",
        "# param_dist = {\"learning_rate\": np.linspace(0.05, 0.15,5),\n",
        "#               \"max_depth\": range(3, 5),\n",
        "#               \"min_samples_leaf\": range(3, 5)}\n",
        "\n",
        "# rand = RandomizedSearchCV(GBmodel, param_dist, cv=7,n_iter=10, random_state=5)\n",
        "# rand.fit(data_X,data_y)\n",
        "# rand.grid_scores_\n",
        "\n",
        "# print(rand.best_score_)\n",
        "# print(rand.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "48b25cff-3526-5d4d-5749-2345df24b133"
      },
      "source": [
        "best score: 0.38834623226916964 \n",
        "\n",
        "best parameters: {'min_samples_leaf': 4, 'learning_rate': 0.1, 'max_depth': 4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf2637d5-455d-a910-b973-b46b9b3d7684"
      },
      "outputs": [],
      "source": [
        "GBmodel = GradientBoostingRegressor(min_samples_leaf= 4, learning_rate= 0.1, max_depth= 4)\n",
        "GBmodel.fit(data_X,data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "692b05be-f62c-fb63-7370-329c6e9d6d27"
      },
      "outputs": [],
      "source": [
        "sns.distplot(GBmodel.predict(data_X),kde=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "10c2517f-a330-9a4b-3003-3b78285f7344"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ce5ddb5-c701-3dca-6a0d-499b1f6da6fb"
      },
      "source": [
        "Because our regression model doesn't predict briefly mentioned peaks let's create classification model for them. After classification respected values will be assigned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1df0bfcf-c534-3e2c-7c82-b8e26423ec0e"
      },
      "outputs": [],
      "source": [
        "clfdata_X = train_df.drop(['id','timestamp','label_value','price_doc_log','price_doc'],axis=1)\n",
        "clfdata_y = train_df['label_value']\n",
        "\n",
        "clfX_train, clfX_test, clfY_train, clfY_test = train_test_split(clfdata_X, clfdata_y, test_size=0.30,random_state=31)\n",
        "\n",
        "GBclf= GradientBoostingClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e97dc158-47dc-1f46-0730-2384a3e35397"
      },
      "outputs": [],
      "source": [
        "GBclf.fit(clfX_train,clfY_train)\n",
        "GBclf.score(clfX_test,clfY_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "26156c05-4c69-be2d-6440-d09c240dfb15"
      },
      "outputs": [],
      "source": [
        "print(precision_score(GBclf.predict(clfX_test),clfY_test.values,average='macro'))\n",
        "print(recall_score(GBclf.predict(clfX_test),clfY_test.values,average='macro'))\n",
        "\n",
        "print(precision_score(GBclf.predict(clfX_test),clfY_test.values,average='micro'))\n",
        "print(recall_score(GBclf.predict(clfX_test),clfY_test.values,average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d709ae18-47a0-2a48-383b-e40ee3488666"
      },
      "outputs": [],
      "source": [
        "pred = GBmodel.predict(data_X)\n",
        "lab = GBclf.predict(clfdata_X)\n",
        "pred_Y = pd.DataFrame({'pred': np.expm1(pred), 'label':lab})\n",
        "\n",
        "\n",
        "\n",
        "pred_Y.loc[pred_Y['label'] == 1, 'pred'] = 1000000\n",
        "pred_Y.loc[pred_Y['label'] == 2, 'pred'] = 2000000\n",
        "sns.distplot(np.log(pred_Y.pred),kde=None)\n",
        "sns.distplot(train_df.price_doc_log.values,kde=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ffa71cdb-3ba8-2d51-6984-b3cfb190bcec"
      },
      "source": [
        "Unfortunately prediction performance is rather awful, model needs further development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "078dfec0-eb74-d335-9123-006a185fabc3"
      },
      "source": [
        "## Features importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16f949b1-5599-df90-65fd-a09cfcb32559"
      },
      "outputs": [],
      "source": [
        "importances = GBmodel.feature_importances_\n",
        "importances_by_trees=[tree[0].feature_importances_ for tree in GBmodel.estimators_]\n",
        "std = np.std(importances_by_trees,axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "\n",
        "sns.barplot(importances[indices][:20],data_X.columns[indices[:20]].values)\n",
        "plt.title(\"Feature importances - regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ff3bff6-cb43-cb2b-94c8-8b357326994b"
      },
      "outputs": [],
      "source": [
        "clf_importances = GBclf.feature_importances_\n",
        "clf_importances_by_trees=[tree[0].feature_importances_ for tree in GBclf.estimators_]\n",
        "clf_std = np.std(clf_importances_by_trees,axis=0)\n",
        "clf_indices = np.argsort(clf_importances)[::-1]\n",
        "\n",
        "\n",
        "sns.barplot(clf_importances[clf_indices][:20],clfdata_X.columns[clf_indices[:20]].values)\n",
        "plt.title(\"Feature importances - classification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6c6b9041-a0ae-73b2-06dd-69dd62db7ca9"
      },
      "source": [
        "### 3.Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80da5f87-c777-834f-fac9-bef099bd7bf0"
      },
      "outputs": [],
      "source": [
        "predict = GBmodel.predict(test_df.drop([\"id\", \"timestamp\"],axis=1))\n",
        "label = GBclf.predict(test_df.drop(['id','timestamp'],axis=1))\n",
        "output = pd.DataFrame({'id': id_test, 'price_doc': np.expm1(predict), 'label':label})\n",
        "\n",
        "\n",
        "\n",
        "output.loc[output['label'] == 1, 'price_doc'] = 1000000\n",
        "output.loc[output['label'] == 2, 'price_doc'] = 2000000\n",
        "output = output.drop(['label'],axis=1)\n",
        "output.to_csv('output.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "57898282-e02e-6c62-cf43-601d24210f57"
      },
      "source": [
        "### Adversarial validation (unfinished)\n",
        "\n",
        "\n",
        "According to [this kernel](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) our test data is easy distinguished from training data. In this case it is better to validate our model with set familiar to test data. Below is unfinished attempt in implementing this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7405906-8e76-1d11-7b2a-1dd4c9506498"
      },
      "outputs": [],
      "source": [
        "train_dfadv = train_df.drop([\"timestamp\",\"price_doc\",\"price_doc_log\"],axis=1)\n",
        "test_dfadv = test_df\n",
        "train_dfadv['istrain'] = 1\n",
        "test_dfadv['istrain'] = 0\n",
        "whole_df = pd.concat([train_dfadv, test_dfadv], axis = 0)\n",
        "whole_df = whole_df.fillna(whole_df.median())\n",
        "valY = whole_df['istrain']\n",
        "valX = whole_df.drop(['istrain',\"id\", \"timestamp\"],axis=1)\n",
        "\n",
        "X_vtrain, X_vtest, y_vtrain, y_vtest = train_test_split(valX.values, valY.values, test_size=0.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "646b74dd-83d2-1c35-9153-533b301cbb96"
      },
      "outputs": [],
      "source": [
        "GBclf= GradientBoostingClassifier()\n",
        "GBclf.fit(X_vtrain,y_vtrain)\n",
        "vpred_y = GBclf.predict(X_vtest)\n",
        "roc_auc_score(vpred_y,y_vtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c581f134-641f-9f5b-2f65-72b3bacf3fdb"
      },
      "outputs": [],
      "source": [
        "importances = GBclf.feature_importances_\n",
        "importances_by_trees=[tree[0].feature_importances_ for tree in GBclf.estimators_]\n",
        "std = np.std(importances_by_trees,axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "\n",
        "sns.barplot(importances[indices][:20],valX.columns[indices[:20]].values)\n",
        "plt.title(\"Feature importances\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e615d41c-789f-73dc-ad53-2a5dca71770c"
      },
      "outputs": [],
      "source": [
        "X=train_df.drop([\"id\", \"timestamp\", \"price_doc\",\"price_doc_log\"], axis=1)\n",
        "y=train_df.price_doc_log.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1ea58b9-105f-10d2-b06c-06dc99f22f8c"
      },
      "outputs": [],
      "source": [
        "val_prob = GBclf.predict_proba(X)\n",
        "adversarial_set = train_df\n",
        "adversarial_set['prob'] = val_prob.T[1]\n",
        "\n",
        "adversarial_set=adversarial_set.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
        "\n",
        "adversarial_set_length =int(adversarial_set.shape[0]*0.20)\n",
        "adversarial_set = adversarial_set.sort_values(by='prob')\n",
        "validation_set = adversarial_set[:adversarial_set_length] \n",
        "train_set = adversarial_set[adversarial_set_length:]\n",
        "\n",
        "trainY  =train_set['price_doc_log'].values\n",
        "trainX = train_set.drop(['price_doc_log','prob'],axis=1).values\n",
        "\n",
        "validationY  =validation_set['price_doc_log'].values\n",
        "validationX = validation_set.drop(['price_doc_log','prob'],axis=1).values"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}