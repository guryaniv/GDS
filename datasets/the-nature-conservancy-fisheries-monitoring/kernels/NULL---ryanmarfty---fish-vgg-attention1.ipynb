{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n!mkdir ~/.keras\n!mkdir ~/.keras/models\n!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n!cp ../input/keras-pretrained-models/resnet50* ~/.keras/models/","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"train_dir = '../input/the-nature-conservancy-fisheries-monitoring/train'","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"2a4e09ff-6dc4-4069-ab97-7a38de8d43a7","_uuid":"7bd435790716a4f4ecb340b145aa17160bc7e526","scrolled":false,"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True, height_shift_range=0.15, width_shift_range = 0.15, rotation_range = 5, shear_range = 0.01, fill_mode = 'nearest', zoom_range=0.2)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"37227cad-4adc-40a6-99fd-fee90ecea692","_uuid":"f779a5c863dc6c6e31570ae9448b9bad1dc9e095","trusted":true},"cell_type":"code","source":"train_gen = datagen.flow_from_directory(train_dir, target_size=(224, 224),batch_size=32)\nval_gen = datagen.flow_from_directory(train_dir, target_size=(224, 224),batch_size=32)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"2608b74f-29a2-4ba0-b90a-9606a6c88b4c","_uuid":"348f08c8e1c4cda53ae2b1835832bf904d6422b8","trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\nfrom keras.models import Model\nin_lay = Input((224,224,3))\nbase_pretrained_model = VGG16(input_shape =(224,224,3), include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\npt_features = base_pretrained_model(in_lay)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"9d8a072a-eb14-455c-a46c-238f2d9846ae","_uuid":"39495f66e9b7fae0caf793fea20309a0e049399a","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization\nbn_features = BatchNormalization()(pt_features)\n# here we do an attention mechanism to turn pixels in the GAP on an off\nattn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)\nattn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\nattn_layer = Conv2D(1, kernel_size = (1,1),  padding = 'valid', activation = 'sigmoid')(attn_layer)\n# fan it out to all of the channels\nup_c2_w = np.ones((1, 1, 1, pt_depth))\nup_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', activation = 'linear', use_bias = False, weights = [up_c2_w])\nup_c2.trainable = False\nattn_layer = up_c2(attn_layer) ","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"8db92b27-c206-4015-9242-727e2bf8871b","_uuid":"564ee47d638fd5c275f92a59194089c6b7ae8b6f","collapsed":true,"trusted":true},"cell_type":"code","source":"mask_features = multiply([attn_layer, bn_features])\ngap_features = GlobalAveragePooling2D()(mask_features)\ngap_mask = GlobalAveragePooling2D()(attn_layer)\n# to account for missing values from the attention model\ngap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\ngap_dr = Dropout(0.5)(gap)\ndr_steps = Dropout(0.5)(Dense(128, activation = 'elu')(gap_dr))\nout_layer = Dense(8, activation = 'softmax')(dr_steps)\ntb_model = Model(inputs = [in_lay], outputs = [out_layer])\ntb_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"28666998-8a83-4b19-9694-434ad128dc4f","_uuid":"ae0eea34ced206cc0335ef82f367d2ad3ceac667","trusted":true,"scrolled":true},"cell_type":"code","source":"tb_model.summary()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"5dbf79d5-230d-41a1-a5dd-58ec30234aeb","_uuid":"3b1c8dc2829a699fe4d7b584bff3ac1ddfa89517","trusted":true},"cell_type":"code","source":"tb_model.fit_generator(train_gen, validation_data=val_gen,steps_per_epoch =50,epochs = 8,validation_steps=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f11ed33-f2b1-41a1-bc67-7f201b0620ce","_uuid":"dbfba8be837209f2aac0aaa5ef137a321f78a256","trusted":true},"cell_type":"code","source":"tb_model.fit_generator(train_gen, validation_data=val_gen,steps_per_epoch =50,epochs = 8,validation_steps=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e632b9c-5df7-41a0-95a3-bbbf25e8f410","_uuid":"15b748e41af3c37c5f843c41e00a6868ec2acafc","collapsed":true,"trusted":true},"cell_type":"code","source":"tb_model.save('fish_round1.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}