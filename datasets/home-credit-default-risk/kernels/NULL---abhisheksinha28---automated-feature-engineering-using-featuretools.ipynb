{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2df9213bdd7e5490e1791a318b6e07c610a11e9"},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\napp_test = pd.read_csv('../input/application_test.csv').sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]\n\nbureau_balance = pd.read_csv('../input/bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index(drop = True).loc[:1000, :]\ncash = pd.read_csv('../input/POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ncredit = pd.read_csv('../input/credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\nprevious = pd.read_csv('../input/previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]\ninstallments = pd.read_csv('../input/installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index(drop = True).loc[:1000, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05dbd78e5a1b2d8fee19897e324d203e0f9cb25f"},"cell_type":"code","source":"bureau = pd.read_csv('../input/bureau.csv').sort_values('SK_ID_BUREAU').reset_index(drop = True).loc[:1000, :]\napp_train['set'] = 'train'\napp_test['set'] = 'test'\napp_test['TARGET'] = np.nan\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcff6a0ecd7b2f7fab63d3ce3bd3cf97e03f003d"},"cell_type":"code","source":"data = app_train.append(app_test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af6c6c32ae70006aed62d32c1fc1662d9eb23825"},"cell_type":"code","source":"import featuretools as ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9e0fe62c72207c836d5d828da61dc77abf37097"},"cell_type":"code","source":"es = ft.EntitySet(id ='clients')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2f94b703db488416a1fdb2c6374e4f4bf28c9f5"},"cell_type":"code","source":"# Entities with a unique index\nes = es.entity_from_dataframe(entity_id = 'app', dataframe = data, index = 'SK_ID_CURR')\n\nes = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n\nes = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n\n# Entities that do not have a unique index\nes = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n                              make_index = True, index = 'bureaubalance_index')\n\nes = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n                              make_index = True, index = 'cash_index')\n\nes = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n                              make_index = True, index = 'installments_index')\n\nes = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n                              make_index = True, index = 'credit_index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2037ea7bd1226f90647fede221e109599a89f441"},"cell_type":"code","source":"print('Parent: app, Parent Variable: SK_ID_CURR\\n\\n', data.iloc[:, 111:115].head())\nprint('\\nChild: bureau, Child Variable: SK_ID_CURR\\n\\n', bureau.iloc[10:30, :4].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"246fddc0c689f90709682b48846b08ffa31523a9"},"cell_type":"code","source":"r_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af2806a72d3e5653140354f324d3192f1c230b65"},"cell_type":"code","source":"es = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])\n# Print out the EntitySet\nes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a0b7abf4468df89dbafac2272db6416007468d"},"cell_type":"code","source":"default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\ndefault_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"numwords\", \"characters\"]\n\n# DFS with specified primitives\nfeature_names = ft.dfs(entityset = es, target_entity = 'app',\n                       trans_primitives = default_trans_primitives,\n                       agg_primitives=default_agg_primitives, \n                       max_depth = 2, features_only=True)\n\nprint('%d Total Features' % len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7298cdc0cca7d91681cdd140272ffd0a15412636"},"cell_type":"code","source":"features_set, feature_names = ft.dfs(entityset = es, target_entity='app', trans_primitives=default_trans_primitives, agg_primitives= default_agg_primitives,\n                                    max_depth=2, features_only = False, verbose= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad3a4c9772b93018feb8dad3dd65b7358b44faa"},"cell_type":"code","source":"feature_matrix_spec, feature_names_spec = ft.dfs(entityset = es, target_entity = 'app',  \n                                                 agg_primitives = ['sum', 'count', 'min', 'max', 'mean', 'mode'], \n                                                 max_depth = 2, features_only = False, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17b7d1fe9a85199fcaf9bf7f2326f4db081c0bdb"},"cell_type":"code","source":"features_set = features_set.reindex(index=data['SK_ID_CURR'])\nfeatures_set = features_set.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a2bd4b41094063f5e3bc496ee76a758c61aad9a"},"cell_type":"code","source":"feature_matrix_spec = feature_matrix_spec.reindex(index=data['SK_ID_CURR'])\nfeature_matrix_spec = feature_matrix_spec.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c5827a35eac34af190ea8cf56550439cedc7b9c"},"cell_type":"code","source":"feature_matrix_spec.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5517fd09dee4d4f216233a24ee32b5d4cd0e7947"},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7d7fe0623899d727c006524ae65c6f67750ccb4"},"cell_type":"code","source":"features_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59c580b8e96dcb7057e671e70432cc7184a0c695"},"cell_type":"code","source":"correlation = features_set.corr()['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6602b7ef2216e41aa7b889fe86dc5c977d0350"},"cell_type":"code","source":"train, test = features_set[features_set['TARGET'].notnull()].copy(), features_set[features_set['TARGET'].isnull()].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8db143894e0c5b7936b27df3641b0628eabd84a1"},"cell_type":"code","source":"train_labels = np.array(train.pop('TARGET')).reshape((-1, ))\n\ntest_ids = list(test.pop('SK_ID_CURR'))\ntest = test.drop(columns = ['TARGET'])\ntrain = train.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e5b7be5d1b78c2de74bd7d3ef6a60539959fa15"},"cell_type":"code","source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\n# Match the columns in the dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ca7e7256be9914920d0eba71e0236d4a05a981"},"cell_type":"code","source":"import gc \ngc.enable()\ndel app_train,app_test,bureau,cash,credit,bureau_balance,previous,installments\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4c62641c4765e2f70863673a9e4d6316b2b1f14"},"cell_type":"code","source":"cols_with_id = [x for x in train.columns if 'SK_ID_CURR' in x]\ncols_with_bureau_id = [x for x in train.columns if 'SK_ID_BUREAU' in x]\ncols_with_previous_id = [x for x in train.columns if 'SK_ID_PREV' in x]\nprint('There are %d columns that contain SK_ID_CURR' % len(cols_with_id))\nprint('There are %d columns that contain SK_ID_BUREAU' % len(cols_with_bureau_id))\nprint('There are %d columns that contain SK_ID_PREV' % len(cols_with_previous_id))\n\ntrain = train.drop(columns = cols_with_id)\ntest = test.drop(columns = cols_with_id)\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"519467aac69f67db284ffa701ac71b5219ff6942"},"cell_type":"code","source":"threshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5814056b1e602bff602c3c794a6450e258e77d90"},"cell_type":"code","source":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfaf1c208d304e66ec5f4667580eb0f3bb282f96"},"cell_type":"code","source":"to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd1a66b00f99757e19b7ea1970e599bd497b4e52"},"cell_type":"code","source":"train = train.drop(columns = to_drop)\ntest = test.drop(columns = to_drop)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ada9914238463232ea75a98fcca02a65ab2085d6"},"cell_type":"code","source":"train_missing = (train.isnull().sum() / len(train)).sort_values(ascending = False)\ntrain_missing.head()\ntest_missing = (test.isnull().sum() / len(test)).sort_values(ascending = False)\ntest_missing.head()\ntrain_missing = train_missing.index[train_missing > 0.75]\ntest_missing = test_missing.index[test_missing > 0.75]\n\nall_missing = list(set(set(train_missing) | set(test_missing)))\nprint('There are %d columns with more than 75%% missing values' % len(all_missing))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a209f7436ac7a36e122e6bde8caec476c2fd01"},"cell_type":"code","source":"train = pd.get_dummies(train.drop(columns = all_missing))\ntest = pd.get_dummies(test.drop(columns = all_missing))\n\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\nprint('Training set full shape: ', train.shape)\nprint('Testing set full shape: ' , test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cac0d6b588c3282c157849a83c85081dafa36553"},"cell_type":"code","source":"import lightgbm as lgb\nfeature_importances = np.zeros(train.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86cad6f2236be3bb46de56ee305abe4fe9627a94"},"cell_type":"code","source":"for i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b26ca19c50855fc32b45218133433e8790a22428"},"cell_type":"code","source":"feature_importances = feature_importances / 2\nfeature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf4d8bc50cc8f7a407af84a23adf3b92a63f1d3"},"cell_type":"code","source":"zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38a5f4b262313909161870e2f00b93e4f517f8b7"},"cell_type":"code","source":"train = train.drop(columns = zero_features)\ntest = test.drop(columns = zero_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb434767a6aa7858e07a7e793ba82996877223aa"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5670126125d5d49f205c4e19473a69c62828cfeb"},"cell_type":"code","source":"def plot_feature_importances(df, n = 15, threshold = None):\n    \"\"\"\n    Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be \"feature\" and \"importance\"\n    \n    n : int, default = 15\n        Number of most important features to plot\n    \n    threshold : float, default = None\n        Threshold for cumulative importance plot. If not provided, no plot is made\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    Note\n    --------\n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n    \n    \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'blue', edgecolor = 'k', figsize = (12, 8),\n                            legend = False)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'Top {n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.2, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, 100 * threshold))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd9aced789755fdf10d745635c8a3514d6361761"},"cell_type":"code","source":"plot_feature_importances(feature_importances, n=15,threshold = 0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8a51497ae78a1d9cea76611149a75c157ca3d15"},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae502e17944ba0ef92b4bcd3b6bd0cfcfea443b1"},"cell_type":"code","source":"train_ids = data['SK_ID_CURR']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d9b721da1f8bfeab5ddaa54c297494a8429281"},"cell_type":"code","source":"train['TARGET'] = train_labels\ntrain['SK_ID_CURR'] = train_ids[:1001]\ntest['SK_ID_CURR'] = test_ids\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e9e97a49bedd69dc00d67abbea3c131a106deb8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"007c5e88860185d16e9d41757a8f6d35cbdf8f41"},"cell_type":"code","source":"submission, feature_importances, metrics = model(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da5bab2117ea5dbb3e3594881cc764db7beae3e"},"cell_type":"code","source":"submission.to_csv('selected_features_submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}