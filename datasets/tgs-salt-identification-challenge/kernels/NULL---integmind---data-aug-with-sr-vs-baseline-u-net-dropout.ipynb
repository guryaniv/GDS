{"cells":[{"metadata":{"_uuid":"fa525d982fdb7e518de47cd8ec52f6ee868d312b"},"cell_type":"markdown","source":"# Changelog\n- V2: \n - Uploaded train/test images with Super Resolution which where produced on my own work-station. (See details on this ohase below)\n - Define Double size U-net to acommodate with the new image size: 202x202 (instead of 101x101)\n - Add blur image feature that categorieses image bluriness\n - Use same L/R data augmentation method as in baseline network. \n \n-V8:  Cancel second phase training, fix test phase bug\n\n-V9: Fix file path of the test dataset\n\n-V10: Fix bug in submission phase,\n\n-V11: Yet another bug fix in submission phase,\n\n-V12: Reduce augmented data to avoid memory overflow.\n\n-V13 : One more bug fix\n\n-V15 : Try out with less data\n\n-V16 : Disabled all data mirroring"},{"metadata":{"_uuid":"ada861a85e9549dca27667692da408c5fdccbaa5"},"cell_type":"markdown","source":"# About\nUsing the well written code of Peter, I wanted to see if using super resolution method that converts the training/testing images to x2 resolution can bring better results to the segmentation task and thus can be considered as a data augmentation method. The baseline comparison for improvement will be in relation to the original U-net (The Forked parent kernel). To accommodate with the higher resolution the new model network includes an additional stage so that it can handle double size image: 256x256 (instead of 128x128) \nIn addition an image blurriness estimator is added in order to optimize the training process. It is assumed that the sharper images are then they would yield better super resolution images. The training data uses the same L/R data augmentation method as in baseline network. \n\nThe super resolution data-set of images (SR images) were generated using the following network:\n\"Zero-Shot\" Super-Resolution using Deep Internal Learning (ZSSR)'\nOfficial implementation for paper by: Assaf Shocher, Nadav Cohen, Michal Irani\nhttps://github.com/assafshocher/ZSSR\n\nand was uploaded. to the local input folder. The code includes less graphic samples and illustrations (with you my apologies). \nSee results in the Conclusions section\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\nfrom tqdm import tqdm_notebook\nfrom pathlib import Path\n#from os import walk\n#import zipfile","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"trusted":true,"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_super = 202\nimg_size_target = 128\nBlur = 20\n\ndef upsampleSR(img):\n    #if img_size_super == img_size_target*2:\n    #    return img\n    return resize(img, (img_size_target*2, img_size_target*2), mode='constant', preserve_range=True)\n\ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\npath_trainSuper = '../input/tsg-salt-superres-training-data/trainsuper/'\npath_testSuper = '../input/tsg-salt-superres-training-data/testsuper/'\n\n#print(check_output([\"ls\", \"../input/tsg-salt-superres-training-data/testsuper/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"trusted":true,"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(path_trainSuper + \"trainSuper/images/{}_zssr_X2.00X2.00.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86620c6a070571895f4f36ec050a25803915ed74"},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(path_trainSuper + \"trainSuper/masks/{}_zssr_X2.00X2.00.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1137f0a009f10b5f69e4dade5f689e744e9ce1d6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 10 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"trusted":true,"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_super, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b13d1ecc7004832e8e042d034922796263054b7"},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 10):\n        if int(val * 10) <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5e66ff4809ea2f9a679b7ddbda5028dc324137a"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670"},"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2391c568019151b098a002937516bb77a506f403"},"cell_type":"markdown","source":"# New Image Blur Classification\n An image bluriness estimator is added using a laplacian estimator (from open-cv library)"},{"metadata":{"trusted":true,"_uuid":"6ae7b7011b7de3caed58f9ca3939df15ffa319ad"},"cell_type":"code","source":"def loadCalcImageBlur(img_path, tindex):\n  f = []\n  imBlurDict = {}\n  for fl in tindex:\n     if not Path(img_path+ fl+\".png\").exists():\n       #print ( img_path+fl)\n       continue\n     img = cv2.imread(img_path+fl+\".png\", cv2.COLOR_BGR2GRAY)\n     a =np.asarray(img)\n     bl = np.max(cv2.convertScaleAbs(cv2.Laplacian(img,3)))\n     #print (fl, \"blur:\", bl)\n     imBlurDict[fl] = bl\n  return imBlurDict\n\nd_blur =loadCalcImageBlur('../input/tgs-salt-identification-challenge/train/images/',train_df.index)\ntrain_df[\"id\"]=train_df.index\n#print ( train_df[\"id\"].map(d_blur))\ntrain_df[\"blur\"]= train_df[\"id\"].map(d_blur)\nblur_df = train_df[\"blur\"]\n\nprint (\"Total train images:\",len(train_df))\nprint (\"Total test images:\",len(test_df))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"},"cell_type":"markdown","source":"# Filter out blurry images"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1a6bc85ee458f72c0917edf77895d5abc5eaf3ee"},"cell_type":"code","source":"if Blur >0:\n  train_df = train_df[train_df[\"blur\"] >= Blur]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00655e32f93f96ebd90dbe94e35ee052f52217cd"},"cell_type":"markdown","source":"# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."},{"metadata":{"trusted":true,"_uuid":"2d3c3157512d11e71ac74ce51a937b85bedfe1d1"},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.tolist()).reshape(-1, img_size_super, img_size_super, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_super, img_size_super, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2f1ab00f03e71e6d7f9b2214408b5a9779fc235"},"cell_type":"code","source":"x_train_sr = np.array(train_df.images.loc[ids_train].map(upsampleSR).tolist()).reshape(-1, img_size_target*2, img_size_target*2, 1)\ny_train_sr = np.array(train_df.masks.loc[ids_train].map(upsampleSR).tolist()).reshape(-1, img_size_target*2, img_size_target*2, 1)\nprint (\"Shape upsample:\",x_train_sr.shape) \n\nx_valid_sr = np.array(train_df.images.loc[ids_valid].map(upsampleSR).tolist()).reshape(-1, img_size_target*2, img_size_target*2, 1)\ny_valid_sr = np.array(train_df.masks.loc[ids_valid].map(upsampleSR).tolist()).reshape(-1, img_size_target*2, img_size_target*2, 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"trusted":true,"_uuid":"a517622135321d17e4aaad749def999205da358c"},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    # 256 -> 128\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    # 16-> 32\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 32 -> 64\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 64 -> 128\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    #128 -> 256    \n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size_target*2, img_size_target*2, 1))\noutput_layer = build_model(input_layer, 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa78bd7c607e1f0e0235e4b2f82056c0361dac5"},"cell_type":"code","source":"model = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b"},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c007157c2fd3d7dadcaeee2a6376351852d1e565"},"cell_type":"markdown","source":"# Data augmentation\nUse data augmentation with only part of the mirrord images due to lack of memory"},{"metadata":{"trusted":true,"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"},"cell_type":"code","source":"'''\nltr = len(x_train_sr)\nx_train_sr = np.append(x_train_sr, [np.fliplr(x) for x in x_train_sr[:ltr-2800]], axis=0)\ny_train_sr = np.append(y_train_sr, [np.fliplr(x) for x in y_train_sr[:ltr-2800]], axis=0)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7040f72549212dd4f71c13dfbd8bf013481ea369"},"cell_type":"code","source":"print(len(x_train_sr))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":false},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras_SR.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.14, patience=4, min_lr=0.000001, verbose=1)\n\nepochs = 100\nbatch_size = 24 #32\n\nhistory = model.fit(x_train_sr, y_train_sr,\n                    validation_data=[x_valid_sr, y_valid_sr], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a85337421d13b334bc91f013b8f5caaa8c8974c"},"cell_type":"markdown","source":"## Second Phase Training (TBD) \nSecond phase training with mirrored data due to lack of memory."},{"metadata":{"trusted":true,"_uuid":"797417e794f32065ee40212cf9caccb46cf0f946"},"cell_type":"code","source":"'''\nx_train_sr = np.array(list(map(np.fliplr, x_train_sr)))\ny_train_sr = np.array(list(map(np.fliplr, y_train_sr)))\n#y_train_sr = np.array([np.fliplr(x) for x in y_train_sr])\nprint(len(x_train_sr))\nmodel = load_model(\"./keras_SR.model\")\nepochs = 30\nbatch_size = 24 #32\nhistory = model.fit(x_train_sr, y_train_sr,\n                    validation_data=[x_valid_sr, y_valid_sr], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])\n'''                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","collapsed":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7"},"cell_type":"code","source":"model = load_model(\"./keras_SR.model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"trusted":true,"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","collapsed":true},"cell_type":"code","source":"preds_valid_sr = model.predict(x_valid_sr).reshape(-1, img_size_target*2, img_size_target*2)\n#preds_valid = np.array([downsample(x) for x in preds_valid])\n#y_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4"},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"trusted":true,"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_sr, np.int32(preds_valid_sr > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc","collapsed":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ced29761f2d1760245112a30a7abd4783b373dd","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332a614c0ae837c115ec6563f355753ffbb8cd83"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"trusted":true,"_uuid":"72128add82c6853441671fde67e7e66601a01787","collapsed":true},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"502d1126b2d9715f234bf0516f87491c2b983864"},"cell_type":"markdown","source":"## Dividing prediction task to batchs to reduce memory load\nClean up training data to save memory and start predictions on test images, doing so in batches."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3ecb152b492c7126d12c5ef2c701eec8ea3d86f1","collapsed":true},"cell_type":"code","source":"\n\nx_test = []\npred_idx = []\npred_dict = {}\nlenl = len(test_df)\nprint (lenl)\nfor cnt, (idx, row) in enumerate(test_df.iterrows()):\n  #print(cnt, idx, row.values)\n # print(cnt, test_df.loc[row])\n  #arr_test = upsampleSR(np.array(load_img(zf_testSuper.open(\"testSuper/images/{}_zssr_X2.00X2.00.png\".format(idx)), grayscale=True))) / 255 #.reshape(-1, img_size_target*2, img_size_target*2, 1)\n  arr_test =  upsampleSR(np.array(load_img(path_testSuper + \"testSuper/images/{}_zssr_X2.00X2.00.png\".format(idx), grayscale=True))) / 255\n  x_test.append(arr_test)\n  pred_idx.append(idx)\n  if (cnt % 3000 == 0)  :\n    print(cnt, idx, row.values)\n    #x_test_a = np.array(x_test).reshape(-1, img_size_target*2, img_size_target*2, 1)\n    preds_test = model.predict(np.array(x_test).reshape(-1, img_size_target*2, img_size_target*2, 1))\n    pred_dict.update({idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(pred_idx))})\n    pred_idx =[]\n    x_test =[]\n \nif (len(pred_idx)>0):\n    preds_test = model.predict(np.array(x_test).reshape(-1, img_size_target*2, img_size_target*2, 1))\n    pred_dict.update({idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(pred_idx))})    \n\nprint (len(pred_dict))    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4243166f91c4bcb4da00208f4f53dd912dbb429f"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"},"cell_type":"markdown","source":"# Conclusions\n\nA big problem that was encountered is the overloading of memory that is caused by the SR images and bigger NN. I could not run it through in the kaggle environment in a single batch. In an offline workstation that I used the training is performed in 2 phases - first one using the sharper images and the second shorter one using all images.\nThe results are not very conclusive but the SR method does attain about 1.5% precent improvment in the private score and 0.3% precent improvment in the public score.\n\nThese are the results :\n\nOriginal net : Private: 0.737057 Public:0.710566\n\nBig SR net :  Private 0.751683 Public: 0.713566\n\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}