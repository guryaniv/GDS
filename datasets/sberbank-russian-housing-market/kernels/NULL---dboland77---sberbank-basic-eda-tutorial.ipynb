{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b14b46fd-cc88-faac-03a7-a754d355617f"
      },
      "source": [
        "##Exploratory Data Analysis - Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a0a3578-ac13-7635-3aa9-be877f06d662"
      },
      "source": [
        "Firstly lets import the libraries that we will need for our data analysis. \n",
        "\n",
        " - *Numpy* - Advanced mathematical functions and linear algebra\n",
        " - *Pandas* - Data analytics and easy CSV input / output\n",
        " - *Matplotlib* - Basic plotting functionality\n",
        " - *Seaborn* - \"Snazzier\" plots - automatically updates matplotlib plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1f5091b-8052-f155-09d6-07d1b1e2b674"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "pd.set_option('display.max_columns', 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "452b80c1-d934-bcaf-6fd1-a2a3ca5204e3"
      },
      "source": [
        "##Read in the the train, test and macro data\n",
        "\n",
        "The pandas package that we read in above, comes with a function (pandas.read_csv) which reads in a csv file and turns it into and pandas dataframe.  pandas comes with functions for reading in many types of data, json, excel, sas and other.  See http://pandas.pydata.org/pandas-docs/version/0.20/io.html for further details. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5dfe1f3c-8faa-bc15-7828-55e5722efde9"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n",
        "test_df = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n",
        "macro_df = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c4b98c6c-e021-6a24-c4f8-94d59135459a"
      },
      "source": [
        "### Look at the head and tail of the data\n",
        "\n",
        "Lets start by looking at the first and last few records of each data set to get a feel for what they look like.  If you are used to R,  using 'head' and 'tail' to inspect the first few or last few records of a data frame will be familiar to you.  In R, they are functions and you use them as such e.g. head(train_df).  In pandas, they are methods and you use them by typing them after the object you wish to apply them to.  For example if your object is 'df_', you would type df_.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "988f2645-9381-76ef-6b4f-e47d879aa0e2"
      },
      "outputs": [],
      "source": [
        "# Fill in the line of code below so that it applies the head() method to train_df\n",
        "# The default is to show the first 5 records.  \n",
        "# Type a number in the brackets (e.g. head(10)) to override the default\n",
        "# What number does the row numbering start from?\n",
        "# Are there any missing values (NaN is the symbol for Not A Number - and represents missing values)?\n",
        "\n",
        "train_df. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "524274d7-3c1d-a67e-ed59-87c04df324eb"
      },
      "outputs": [],
      "source": [
        "train_df.tail() # this will show us the last five records in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "31487bf8-b5a9-672f-b119-9080bab24920"
      },
      "source": [
        "Already we are getting a feel for the data - note that NaN is short for Not a Number and represents missing data.\n",
        "\n",
        "Lets continue with a look at the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "269240ac-94a6-2243-293e-7bb230a6b65d"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "17bc537f-5fde-0c1d-121e-0c2cf8d28c0f"
      },
      "source": [
        "And finally lets have a quick look at the macro data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fbd22fe9-d087-91ef-ebd8-dfc2185a38e2"
      },
      "outputs": [],
      "source": [
        "macro_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6f234a32-eeed-52fd-6e9b-cb2c66d63a66"
      },
      "source": [
        "### It is also useful to have an idea of the number of rows and columns of each dataset\n",
        "\n",
        "The method 'shape' will give you the number of rows and columns of a pandas dataframe.  Apply the shape method to test_df, train_df, macro_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9b4fe2c-426a-d792-1394-0123f85c724e"
      },
      "outputs": [],
      "source": [
        "# To apply the method shape to a data frame called df_, type df_.shape. \n",
        "# Fill in the missing shape method below after the period.\n",
        "# Enclosing this in print() ensures that the result is printed.\n",
        "# Do the train and test data have the same number of columns?  If not, why not?\n",
        "print(train_df.)\n",
        "print(test_df.)\n",
        "print(macro_df.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "558c0d3b-d905-8bda-85a1-f9abe81bb260"
      },
      "source": [
        "### Next we take a look at the datatypes contained in the training set\n",
        "The method 'dtypes' will give you the datatypes in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c25bfd15-f1d6-23f1-2699-2006876c0481"
      },
      "outputs": [],
      "source": [
        "# In the line below, fill in the 'dtypes' method after the period\n",
        "# What type are most of the data fields?\n",
        "data_types = train_df.\n",
        "print(data_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79cd48d8-a100-2f68-7779-be35ca1c3c5b"
      },
      "source": [
        "Run the code below to see how many of each type of data there are.  (This code uses a very popular approach called split-apply-combine.  In R this is achieved with packages such as dplyr.  In Python this is part of the pandas package.  We don't have time to discuss this further.  See http://pandas.pydata.org/pandas-docs/stable/groupby.html for further details.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cb9416f-3af3-542a-7a0e-0d047f48f553"
      },
      "outputs": [],
      "source": [
        "# Find the type of each column and store the results in a data frame\n",
        "df_dataTypes = train_df.dtypes.reset_index()\n",
        "\n",
        "# Rename the columns for convencience (note the columns method being used)\n",
        "df_dataTypes.columns = [\"count\",\"dtype\"]\n",
        "\n",
        "# So far we have one line per feature\n",
        "df_dataTypes.head(10)\n",
        "\n",
        "# In the next code chunk we use split-apply-combine to summarise it.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69f3646d-37b9-7011-3c98-d3fa4c2e6155"
      },
      "outputs": [],
      "source": [
        "# Now use split-apply-combine to get the result\n",
        "# First we choose the data we are going to summarise, in this case df_dataTypes[[\"count\",\"dtype\"]]\n",
        "# Then we split it into groups of dtypes with .groupby(by = \"dtype\")\n",
        "# Finally we apply the 'length' function (len) and combine with the aggregate function\n",
        "\n",
        "df_dataTypes[[\"count\",\"dtype\"]].groupby(by = \"dtype\").aggregate(len).reset_index()\n",
        "\n",
        "# Are most of the columns numeric, strings (i.e. 'object') or dates?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ae77af5f-5d2f-032a-f697-b346a6f4d0ae"
      },
      "source": [
        "The code below uses the seabourne package to create a bar chart of the number of each datatype.  For those used to ggplot in R, you will find the seabourne output sometimes looks surprisingly familiar.  We don't have time to review the code below in detail, but there are many introductory tutorials online (for example https://elitedatascience.com/python-seaborn-tutorial).  \n",
        "\n",
        "seabourne is typically imported as 'sns'.  We have done the same which is why you will see it referred to as 'sns' below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77da27f1-130f-ea5d-50cf-12627ffcf725"
      },
      "outputs": [],
      "source": [
        "# Run the code below to produce a bar chart showing the count of each data type.\n",
        "\n",
        "# Count of different datatypes\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.countplot(df_dataTypes['dtype'])\n",
        "plt.xlabel('dtype', fontsize=24)\n",
        "plt.ylabel('count', fontsize=24)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "26ac2532-5fae-f137-7c5b-a5b19e70967c"
      },
      "source": [
        "### What is the distribution of house prices?\n",
        "\n",
        "For now lets focus on the training set - and in particular the variable we are asked to predict - the house price.\n",
        "\n",
        "This is represented by the **price_doc** variable.\n",
        "\n",
        "We start with a dot plot to check for outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a61dd563-98d9-9323-b4bc-bbde270b86da"
      },
      "outputs": [],
      "source": [
        "# Run the code below to create a dot plot of all the prices, in increasing order\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(range(train_df.shape[0]), np.sort(train_df.price_doc.values), s = 2)\n",
        "plt.xlabel('index', fontsize=24)\n",
        "plt.ylabel('price', fontsize=24)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "96a7374f-1ec1-6eb8-508a-b659aa97fbcf"
      },
      "source": [
        "Looks skew - with most values being less that 20m but with the rest of the values reaching up to 100m.  We can look at the distribution using the seabourne package..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7af1693-2ee7-ffdc-37f5-6e6ff1373dc2"
      },
      "outputs": [],
      "source": [
        "# Run the code below to create a histogram of the distrubtion of prices.\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(train_df['price_doc'], kde=False, bins=50)\n",
        "plt.xlabel('price (RUB 00m)', fontsize = 24)\n",
        "plt.xticks(fontsize = 16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "26197756-a51d-8ccd-37c3-dfa46bc43ad3"
      },
      "source": [
        "We can see the data is positively skewed and the range in large.\n",
        "\n",
        "It looks as if a log-transform may suit this data. \n",
        "\n",
        "**Lets plot log of the target variable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c29e99fd-bf5d-dbd2-1dff-a855fa26d8e0"
      },
      "outputs": [],
      "source": [
        "# Run the code below to view the distribution of the (natural) log of the prices.\n",
        "# How would the skew of the price data influence how you would normally go about modelling this data?\n",
        "# How should the skew influence how you create predictions from the point of view \n",
        "# of this competition?\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(np.log(train_df['price_doc']),kde=False,bins=50)\n",
        "plt.xlabel('log of price', fontsize = 24)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0a2874eb-0a05-da33-a14a-eeb7db17ccb0"
      },
      "source": [
        "### How have house prices changed over time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "356c739d-c9e6-888c-1215-2093466a4269"
      },
      "outputs": [],
      "source": [
        "# For convenience create a new feature which is the year and month of the sale\n",
        "train_df['year'] = train_df['timestamp'].dt.year\n",
        "train_df['month'] = train_df['timestamp'].dt.month\n",
        "train_df['yearmonth'] = train_df['timestamp'].dt.strftime(\"%Y%m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb668297-787a-56e1-0ad1-4d5f7f138930"
      },
      "outputs": [],
      "source": [
        "# Now use split-apply-combine to find the median price of sale in each month\n",
        "\n",
        "# Remember how we used split-apply-combine above?\n",
        "# df_dataTypes[[\"count\",\"dtype\"]].groupby(by = \"dtype\").aggregate(len).reset_index()\n",
        "\n",
        "# Let us do the same here.  We will store the result in df_grouped, so we start with\n",
        "# df_grouped = \n",
        "# We will summarise: train_df[['yearmonth', 'price_doc']]\n",
        "# We will groupby 'yearmonth'\n",
        "# We will apply the function np.median\n",
        "# Fill in the code below\n",
        "df_grouped = (train_df[['yearmonth', 'price_doc']]\n",
        "              .groupby(by = )\n",
        "              .aggregate()\n",
        "              .reset_index())\n",
        "\n",
        "df_grouped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a09c4528-b73b-dc89-c87b-12cd8276c645"
      },
      "outputs": [],
      "source": [
        "# Now run the following code chunk to plot the trend in median price\n",
        "fig, ax = plt.subplots(figsize = [12,8])\n",
        "sns.pointplot(df_grouped['yearmonth'].values, df_grouped['price_doc'].values)\n",
        "\n",
        "\n",
        "x_ticks_labels = df_grouped['yearmonth'].values\n",
        "x_ticks = 4 * np.arange(0,12)\n",
        "plt.xticks(x_ticks, x_ticks_labels[x_ticks], rotation = 45)\n",
        "\n",
        "plt.xlabel('year - month', fontsize=24)\n",
        "plt.ylabel('median price', fontsize=24)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e387d785-42c0-a41e-c800-7b6af0fc37c5"
      },
      "source": [
        "### Correlations of Internal Characteristics in the house\n",
        "\n",
        "From the data dictionary we can see that there are seven features which describe each property:\n",
        "\n",
        "'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'num_room', 'kitch_sq'\n",
        "\n",
        "The other features describe the reason for the purchase, the neighbourhood and the \"Raion\" (post code).\n",
        "\n",
        "We take a quick look at how the basic features of the property are correlated with each other and with the price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4dd1846e-cd07-d6c1-3078-a2f5ca8b0013"
      },
      "outputs": [],
      "source": [
        "# Run the code below to create a heat map of the correlations.\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "internal_characteristics=['full_sq', 'life_sq', 'floor', 'max_floor', 'material',\n",
        "                          'num_room', 'kitch_sq','price_doc']\n",
        "heatmap_data=train_df[internal_characteristics].corr()\n",
        "sns.heatmap(heatmap_data, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4c9a3f7-5020-d0a9-691f-3ca76321fcfa"
      },
      "source": [
        "We note a very high correlation (0.7) between full_sq (the area of the home) and num_room (the number of rooms in the property).\n",
        "\n",
        "Also there are high correlations of 0.34 and 0.48 respectively between the house price (price_doc) and full_sq and num_room.\n",
        "\n",
        "These two are definitely key features for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c90b939c-ec3a-047b-b5f0-2a5d13016de0"
      },
      "source": [
        "###Lets examine the number of houses built each year\n",
        "\n",
        "**Grouping the data by build year will quickly identify any outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d26b878e-142f-c9a3-23eb-b82bf01cb089"
      },
      "outputs": [],
      "source": [
        "# Find the frequency of property by build year.\n",
        "grouped_data_count = train_df.groupby('build_year')['id'].aggregate('count').reset_index()\n",
        "grouped_data_count.columns=['build_year','count']\n",
        "print(grouped_data_count.head())\n",
        "print(grouped_data_count.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85e541aa-31d6-8899-c59d-cbf04ea064cb"
      },
      "outputs": [],
      "source": [
        "# Print the tail of grouped_data_count, can you see any outliers?\n",
        "\n",
        "print()\n",
        "\n",
        "# Now print the head - are there any other outliers?\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4cfc8348-17dc-f4ee-38e3-f3099a21998a"
      },
      "source": [
        "###This is highlighting some data quality issues \n",
        "\n",
        "We have a build year of 20052009, clearly a mistake in the data.\n",
        "\n",
        "We also have build year of 4965 - likely to have been meant to be 1965.\n",
        "\n",
        "Lets fix both of these by just removing these outliers. \n",
        "\n",
        "We can set an upper limit on build year of 2019 (some houses may be bought off plan).\n",
        "\n",
        "There are various kernels which deal with data quality issues (for example https://www.kaggle.com/keremt/very-extensive-cleaning-by-sberbank-discussions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "96da3636-927c-734c-4f3b-400c3ee2c9f6"
      },
      "source": [
        "###Finally we can plot the missing data and percentages to get an indication of what values are missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "558fe718-1f30-4af1-e7ee-87597d899886"
      },
      "outputs": [],
      "source": [
        "train_missing = train_df.isnull().sum()/len(train_df)\n",
        "train_missing = train_missing.drop(train_missing[train_missing == 0].index).sort_values(ascending = False).reset_index()\n",
        "train_missing.columns = ['column name','missing percentage']\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "sns.barplot(train_missing['column name'], train_missing['missing percentage'], palette = 'coolwarm')\n",
        "plt.xticks(rotation = 'vertical')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "daaf1be9-dda0-e900-f601-049409b19944"
      },
      "source": [
        "Of 292 columns, 51 have missing values. The percentage of values missing ranges from 0.1% in metro_min_walk to 47.4% in hospital_beds_raion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c99a4c5-c7c8-642d-3324-3cd3f9ee369a"
      },
      "source": [
        "##Ok so now we have a better grasp of the data we can start to think about how we might model it. "
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}