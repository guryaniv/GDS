{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"cell_type":"markdown","source":"Hello gens,\n\nI've tried to process images with croping and normalizing aspect ratio.\n\nThere are two idea for the task\n1. Find object boundary and crop by a window with the most amount of boundary\n2. Train 'whale tail' or 'background' by CNN with keras and zoom image by its probability.\n\nEven though the images are already cropped as we can easily find the object (whale tail), these have indifferent information. So, I'd like to scrape off.\nIn addition, the aspect ratio of cropped area should be normalized for inputting to Keras model.\n"},{"metadata":{"_uuid":"660f39de928c881e2e31706080cbbdca226e1290"},"cell_type":"markdown","source":"**1. Cropping with an amount of boundary**"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"#Import required library\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport os\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"10f59c4289113f1a94ba494751a0e2fad28b0a5a","_cell_guid":"f3c4c7b4-5c73-4996-8cff-98d6ebe6e289","trusted":false},"cell_type":"code","source":"# loading the table\ntrain_df = pd.read_csv('../input/train.csv')\nIds = train_df['Id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"718980bd9dc47ada11aba436fb888dae0105d858","_cell_guid":"7df07c63-bcee-49c2-992b-f21ce333f191"},"cell_type":"markdown","source":"Thanks for the data owner, the images are already cropped and directed.\nBut, the sizes and aspect ratio are not normalized, nor cropping area."},{"metadata":{"collapsed":true,"_uuid":"a585f5739b9ce4880aeb4aceb51cc863db4a553d","_cell_guid":"8c26d1f0-01e6-4c0e-b63a-1c9c1b99234e","trusted":false},"cell_type":"code","source":"# Thanks Lex Toumbourou!!\nINPUT_DIR = '../input'\ndef plot_images_for_filenames(filenames, labels, rows=4):\n    imgs = [plt.imread(f'{INPUT_DIR}/train/{filename}') for filename in filenames]\n    return plot_images(imgs, labels, rows)\n\ndef plot_images(imgs, labels, rows=4):\n    figure = plt.figure(figsize=(13,8))\n    cols = len(imgs) // rows + 1\n    \n    for i in range(len(imgs)):\n        subplot = figure.add_subplot(rows, cols, i + 1)\n        subplot.axis('Off')\n        if labels:\n            subplot.set_title(labels[i], fontsize=16)\n        plt.imshow(imgs[i],cmap='gray')\n\nrand_rows = train_df.sample(frac=1.)[:5]\nimgs = list(rand_rows['Image'])\nlabels = list(rand_rows['Id'])\n\nplot_images_for_filenames(imgs, labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ece4263712b4d69a51893c06f3bb4b05230d4eb1","_cell_guid":"05d4c19e-eff9-475b-8823-c2c61637f642"},"cell_type":"markdown","source":"Then, I've tried to normalize the Images and found a method for recognizing edges with OpenCV.\n\nUsing OpenCV, I show the process for the normalizing as below;\n1. Convert to gray scale array\n2. Find edges by Canny method with OpenCV\n3. Count edges in moving rectangle which has fixed aspect ratio\n4. A rectangle which have maximum count of edges is considered as the normalized cropping"},{"metadata":{"_uuid":"ca7d98827ee6c26282ef71c7c0dfb0a5314a461d","_cell_guid":"f77347c0-bac0-4964-9a27-adee16174030"},"cell_type":"markdown","source":"1. Convert to gray scale array\n\nSize of 00aa021c is (497, 746), almost 1:1.5"},{"metadata":{"collapsed":true,"_uuid":"b2059112cde6f248f3dee74ba7a9a62e521cec85","_cell_guid":"dfcb11ef-aedb-468b-8c0f-7e21287862ba","trusted":false},"cell_type":"code","source":"PATH ='../input/train/00aa021c.jpg'\nimg = cv2.imread(PATH,0) \nprint(img.shape) #checking image-array shape. Here, aspect ratio is 497:746, it's almost 1:1.5\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc005adfe76ab52540b03459bcff181e35ff071","_cell_guid":"78841830-fca7-46bb-9dda-67658888cf78"},"cell_type":"markdown","source":"2. Find edges by Canny method with OpenCV\n\nParameter shall be tuned more. Here, below's is moderate"},{"metadata":{"collapsed":true,"_uuid":"0bfa25b39ff6da303274f1ffd3f6fdfaabc1299f","_cell_guid":"3b67d6f1-2244-4906-8eda-dbfbbcb9f1a8","trusted":false},"cell_type":"code","source":"canny_edges = cv2.Canny(img,300,300)\nplt.imshow(canny_edges)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07e2fa3d6c639842bf2b7d63457f750559327cce","_cell_guid":"0072c93f-ad32-4352-9a92-07881e12f3e4"},"cell_type":"markdown","source":"3. Count edges in moving rectangle which has fixed aspect ratio\n4. A rectangle which have maximum count of edges is considered as the normalized cropping\n\nHere is critical idea for cropping the images. \n\nBut I like simple way.\n\nPrepare a rectangle which has 1:2 aspect and count how much edge points are in the rectangle which has the same horizontal length as original image and its vertical length is a half of horizontal length"},{"metadata":{"collapsed":true,"_uuid":"075b056c0b07b006f6eb2b499a63a48f163113f0","_cell_guid":"db699290-e62c-4072-8a84-08c574c75102","trusted":false},"cell_type":"code","source":"plt.plot( [0, 746], [100,100], 'w--', lw=2 )\nplt.plot( [0, 0], [100,100+746/2], 'w--', lw=2 )\nplt.plot( [746, 746], [100,100+746/2], 'w--', lw=2 )\nplt.plot( [0, 746], [100+746/2,100+746/2], 'w--', lw=2 )\nplt.imshow(canny_edges)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6bd01915c588dfa1af9d5cf8960f0ca8778e392f","_cell_guid":"b3c764b7-7655-4a69-a91f-8a9c879d02d8","trusted":false},"cell_type":"code","source":"v = img.shape[0] # vertial pixels\nh = img.shape[1] # horizontal pixels \n\nver = int(h/2)\ncnt = []\nfor i in range(canny_edges.shape[0]-ver):\n    cnt.append(canny_edges[i:i+ver,:].sum()/255) # moving rectangle \n\ncnt_arr = np.array(cnt)\ni = cnt_arr.argmax()\n\nImg_cropped = Image.fromarray(np.uint8(img[i:i+ver,:]))\nImg_cropped","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9055f1f6cb8e95c435f2cb98ef91f3592a3db942","_cell_guid":"270873fc-51bd-46a5-b2f9-75f8c2e17ac2"},"cell_type":"markdown","source":"some of image has less vertical length than half of horizontal length.\n\nConsidering that, the function is generated as below."},{"metadata":{"collapsed":true,"_uuid":"8f927bce99ff7b89b014478c48c6e8e84868f0df","_cell_guid":"64ef527e-99e0-4138-a8ee-83d4c804d484","trusted":false},"cell_type":"code","source":"def crop(PATH):\n    \n    h_factor = 2 #here is for 1:\"2\"\n    \n    img = cv2.imread(PATH,0)\n    \n    v = img.shape[0] # vertial pixels\n    h = img.shape[1] # horizontal pixels \n\n    #find edges with Canny algorism\n    canny_edges = cv2.Canny(img,300,300)\n    \n    if v < h/h_factor:\n        fill_length = int(abs(h/h_factor-v)*0.5)#np.random.rand()) # for upper filling\n    \n        fill = np.zeros(fill_length* h).reshape(fill_length, h) # black rectangle for upper filling\n\n        canny_edges = np.r_[fill,canny_edges,fill] # fill with black rectangle\n        img = np.r_[fill,img,fill] # fill with black rectangle\n    \n    ver = int(h/h_factor)\n    cnt = []\n    for i in range(canny_edges.shape[0]-ver+2):\n        cnt.append(canny_edges[i:i+ver,:].sum()/255)\n\n    cnt_arr = np.array(cnt)\n    i = cnt_arr.argmax()\n    return Image.fromarray(np.uint8(img[i:i+ver,:]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"871e7725826149bfd656d65112f359446dd0b35d","_cell_guid":"02db9d81-31b1-4a2e-92e4-ec3a77c84bc0"},"cell_type":"markdown","source":"**2. Cropping by keras model**\n\nMy next idea is to generate model which tells an image is tail or not, and use the model to crop images\nOf course, all images show whale tail, but some of them show small tail and vast background landscape.\nIf the model can tell a probability of showing whale tail, that helps cropping images to less background landscape, in other words, like zooming.\n\nI'd like to prepare two kinds of images, which are tail images and dividend images.\nDividend images are generated from original images as quadrant, which are labeled 'Not tail'\nThen, it's ready to push data into keras model, 'Tail Images' and 'Not Tail Images'\n\nThe keras model can be called 'Tail-Or-Not Model'"},{"metadata":{"collapsed":true,"_uuid":"75102db087ee824b8a10d06ec95ba0863af756a9","_cell_guid":"ace14769-ed66-4134-a5b0-d133d0f89abe","trusted":false},"cell_type":"code","source":"#handling data & images\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\n\n#keras\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, Activation, Dropout, MaxPooling2D, Flatten\nfrom keras.utils import np_utils\n\n#keras needs normalized size, here, former (100) is width, later (50) is height of image for opencv (cv2)\nSIZE = (100, 50) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b558aac5c18389f1c8f8ed3bcb3b2cd05b01b2"},"cell_type":"markdown","source":"Functions are defined.\n\nIt's easy to pick up image by just choosing index"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3db8d3d29df0d7c6c4c00a113cd5fd18e11cc7b4"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\n\ndef image_path(i):\n    NAME = train_df['Image'][i]\n    DIR_PATH = '../input/train/'\n    return DIR_PATH + NAME\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fcc62e59a0fb5e574594164d11ff0f9e4e95f9d"},"cell_type":"markdown","source":"This function gets quadrant of images (upper/lower & left/right, respectively)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60d3306fd0301fcd7dbf68f89ee797fd5b0466de"},"cell_type":"code","source":"def quad_div(path):\n    img = cv2.imread(path)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    width = img_gray.shape[1]\n    height = img_gray.shape[0]\n    half_width = int(width / 2)\n    half_height = int(height / 2)\n    \n    upper_left = img_gray[:half_height, :half_width]\n    upper_right = img_gray[:half_height, half_width:]\n    lower_left = img_gray[half_height:, :half_width]\n    lower_right = img_gray[half_height:, half_width:]\n    \n    return cv2.resize(upper_left, SIZE), cv2.resize(upper_right, SIZE), cv2.resize(lower_left, SIZE), cv2.resize(lower_right, SIZE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"600bd7b8ef8410f966d3fe9a6561d387a98fc377"},"cell_type":"markdown","source":"An original image is shown below"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b104abe3bd071af908b70d9f3d2234129538571d"},"cell_type":"code","source":"PATH = image_path(1)\nimg = cv2.imread(PATH)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7fe480be0593821908e0bf19bd5c666efe27a27"},"cell_type":"markdown","source":"Quadrant images are shown below. I consider it's difficult to recoginize it's whale tail with only one quadrant"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fb24fe0590cdd821ce549f03a73e933f3e0b089"},"cell_type":"code","source":"upper_left, upper_right, lower_left, lower_right = quad_div(PATH)\n\nplt.subplot(221)\nplt.imshow(upper_left)\nplt.axis(\"off\")\n\nplt.subplot(222)\nplt.imshow(upper_right)\nplt.axis(\"off\")\n\nplt.subplot(223)\nplt.imshow(lower_left)\nplt.axis(\"off\")\n\nplt.subplot(224)\nplt.imshow(lower_right)\nplt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f224301be095f9b8c1aecf9d7e0a24ffadf6e0eb"},"cell_type":"markdown","source":"One of quadrant is enugh as a not-tail (lanscape) sample. Then, input is PATH, outputs are resized original and one of quadrant."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6798f6f40128a2d52a6cf12dba0f2f3c76549e4a"},"cell_type":"code","source":"def div_arg(path):\n    imgs = []\n    labels = []\n    img = cv2.imread(path)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img_resized = cv2.resize(img_gray, SIZE)\n    div_list = quad_div(path)\n    imgs.append(img_resized)\n    imgs.append(div_list[np.random.randint(3)]) # pick up only one quadrant image\n    for i in ([1,0],[0,1]): # labeled as one-hot method\n        labels.append(i)\n        \n    return np.array(imgs), np.array(labels) # out put is original (but resized) image and one quadrant image","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17092e0692e7bb234c746c7681e6c67200180de7"},"cell_type":"markdown","source":"Prepare training data for Tail-Or-Not Model .\n1,000 images may enough for this training. But, you can try several kinds of training amount."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d581b38d6b673a868edcc5a8b90018de5ed8c34d"},"cell_type":"code","source":"N = 10 \n\nPATH = image_path(N)\nimgs, labels = div_arg(PATH)\n\nfor n in range(N-1):\n    PATH = image_path(n)\n    imgs_add, labels_add = div_arg(PATH)\n    imgs = np.r_[imgs, imgs_add]\n    labels = np.r_[labels, labels_add]\n\nX = imgs.reshape([imgs.shape[0], imgs.shape[1], imgs.shape[2], 1]) #fit dimension for keras model\nY = labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4528a613bda8faea4a58f910450dd1186a60af0d"},"cell_type":"markdown","source":"CNN model is prepared as keras model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a55c646e0c7ca735c6d74edd02ad88babd90ccce"},"cell_type":"code","source":"num_fil = 16\nnum_classes = 2\n\nmodel = Sequential()\nmodel.add(Convolution2D(num_fil, (3,3), border_mode='valid', input_shape=(SIZE[1],SIZE[0],1)))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid'))\nmodel.add(Activation('relu'))\n\nmodel.add(Convolution2D(num_fil, (3,3), border_mode='valid'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid'))\nmodel.add(Activation('relu'))\n\nmodel.add(Convolution2D(num_fil, (3,3), border_mode='valid'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid'))\nmodel.add(Activation('relu'))\n\nmodel.add(Convolution2D(num_fil, (3,3), border_mode='valid'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), border_mode='valid'))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.8))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer='adadelta', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"645a892a32d6b7768a82e2f08296fdf36d8126c1"},"cell_type":"markdown","source":"Unfortunately, its convergence is not so good, sometime. It needs many epochs."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0ec782231a642f7cd014f5502db81aa8b691297"},"cell_type":"code","source":"batch_size =32\nepochs = 5\n\nN_train = int(len(X) * 0.8)\n\nmodel.fit(X[:N_train], Y[:N_train], batch_size=batch_size, epochs=epochs, validation_data=(X[N_train:], Y[N_train:]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20540c2640a52041bbd182ea9ebe31656e741544"},"cell_type":"markdown","source":"Below is example for fitting & cropping image"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"473378e3e22fc7c4ad8b5b121f399524f31b6346"},"cell_type":"code","source":"PATH = image_path(1)\nimg = cv2.imread(PATH)\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4df90fc3ccfa9a58723a70953ac9c7173245c75f"},"cell_type":"markdown","source":"Below is trying to several size of cropping (or zooming), but image center is fixed."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"69040b6242b76c4447d796d32e90cf7554cbf8c1"},"cell_type":"code","source":"width = img_gray.shape[1]\nheight = img_gray.shape[0]\ninc = int(min(width, height) / 2 / 11) - 1\n\nimgs_list = []\nfor num in range(10):\n    dum = cv2.resize(img_gray[(inc*num):(height-inc*num), (inc*num):(width-inc*num)], SIZE)\n    imgs_list.append(dum)\nimgs = np.array(imgs_list)\n\n# to fit dimension for keras model\ntest = imgs.reshape([imgs.shape[0], imgs.shape[1], imgs.shape[2], 1])\n\n# prediction (probability) can be used to get optimized cropping size\npred = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0da302ca117f43ec944dd01a7056b78859f8cdad"},"cell_type":"markdown","source":"For the image, argmax can be 4~6"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"83bc22c71305aec7cc2c01432bd4e5e217d37d92"},"cell_type":"code","source":"#i =np.argmax(pred[:,0])\ni = 5\nprint(i)\nplt.imshow(imgs[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0988ec15ba8a032d6eef9a2df10262114fa111dc"},"cell_type":"markdown","source":"Summarizing above, a function is defined.  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5d6fdddedd067742cb41c2172563a31e0ae49a1f"},"cell_type":"code","source":"SIZE_model = (300, 150)\n\ndef generate_normalized_tail(path):\n    img = cv2.imread(path)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    width = img_gray.shape[1]\n    height = img_gray.shape[0]\n    inc = int(min(width, height) / 2 / 11) - 1\n\n    imgs_list = []\n    for num in range(10):\n        dum = cv2.resize(img_gray[(inc*num):(height-inc*num), (inc*num):(width-inc*num)], SIZE)\n        imgs_list.append(dum)\n    imgs = np.array(imgs_list)\n    \n    test = imgs.reshape([imgs.shape[0], imgs.shape[1], imgs.shape[2], 1])\n    pred = model.predict(test)\n    \n    num =np.argmax(pred[:,0])\n    dum = cv2.resize(img_gray[(inc*num):(height-inc*num), (inc*num):(width-inc*num)], SIZE_model)\n    return dum.reshape(1,SIZE_model[1],SIZE_model[0])","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}