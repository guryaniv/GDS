{"nbformat": 4, "nbformat_minor": 1, "cells": [{"source": ["Sample of code below."], "cell_type": "markdown", "metadata": {"_cell_guid": "e81ee64d-e474-4662-9036-ce23df615199", "_uuid": "b6269c0e8f417f82daf093dda8fa0da6d2c57d86"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "\n", "main_file_path = '../input/train.csv'\n", "data_train = pd.read_csv(main_file_path)\n", "# if i want to see columns\n", "print(data_train.columns)\n"], "metadata": {"_cell_guid": "86b26423-563a-4fa1-a595-89e25ff93089", "_uuid": "1c728098629e1301643443b1341556a15c089b2b"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["data_train\n", "#LotShape,SaleType,SaleCondition,"], "metadata": {"_cell_guid": "e561906e-61cf-42c2-be83-eb45834cef87", "_uuid": "30f15439a79184c348ff0c1460ae31e23f7abde4"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["col_interest =['ScreenPorch','MoSold','LotShape','SaleType','SaleCondition']\n", "sa= data_train[col_interest]\n", "sa.describe()"], "metadata": {"_cell_guid": "ea3acc2f-fdd7-4e91-9904-102f28473949", "_uuid": "8d12eb4dfb8b0db9039e5e1b5f5a03538218eb90"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["#what we actually want to predict\n", "y= data_train.SalePrice\n"], "metadata": {"_cell_guid": "1e10566b-5b6f-49c7-8689-75915682f90e", "collapsed": true, "_uuid": "b61e89c99ba48d164eef56ba05742ff362195b2f"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["predicators =['YearBuilt','YrSold','TotalBsmtSF','LotShape','SaleType','SaleCondition']\n", "one_hot_encoded_training_predictors = pd.get_dummies(predicators)\n", "# we are using pd.get_dummies for encoding .\n", "one_hot_encoded_training_predictors\n", "#predicators.dtypes.sample(10)"], "metadata": {"_cell_guid": "66ef5824-5667-4ee3-8b75-bb740d0e06ad", "_uuid": "0e7299f8e84e964e18500000c1bd2259d36d0aa7"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["X = data_train[predicators]\n", "X\n"], "metadata": {"_cell_guid": "4ee2a9fb-7e19-4140-8c7b-e17d64c64c03", "_uuid": "9913f6c05ab11ff616f25cce84b62158b1f9ce23"}}, {"source": ["****Sklearn is the most popular library for modelling the types of data in dataframe.\n", "\n", "Define: What type of model? Decision tree? Naive bayes? KNN? **\n", "\n", "Fit :Capture patterns from provided data..This is heart of modelling.\n", "\n", "Predict: do it.\n", "\n", "Evaluate: Determine how accurate the model is.**\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "3d0c3682-598e-4446-acc4-846ec0e3c298", "collapsed": true, "_uuid": "6a13aa73f9b40f44ec9bf5676e2ed3859374d8bf"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["from sklearn.tree import DecisionTreeRegressor\n", "# Define the model\n", "housing_model = DecisionTreeRegressor()\n", "\n", "#Fit model\n", "housing_model.fit(X,y)\n", "\n"], "metadata": {"_cell_guid": "15c227b0-8182-452a-a536-32f6a1c5b575", "_uuid": "e06a53492095bc540399e29001a99979ccb6ead0"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["#Lets make prediction on he houses in the training dataset.\n", "print(\" making predictions for the following 5 houses:\")\n", "print(X.head())\n", "print (\"The prediction are\")\n", "print(housing_model.predict(X.head()))"], "metadata": {"_cell_guid": "f70abebe-7f5d-4954-9c6b-7756fbd77abc", "collapsed": true, "_uuid": "2b1de09d0d3262ad1e4f1e4df32526701ae26011"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# here we use MAE( Mean Absolute error)\n", "from sklearn.metrics import mean_absolute_error\n", "predicted_Home_prices = housing_model.predict(X)\n", "mean_absolute_error(y,predicted_Home_prices)"], "metadata": {"_cell_guid": "783f4c4a-01cd-455e-b766-80059daefbc5", "collapsed": true, "_uuid": "722591331d1c3fab328f03ff243a9528ecf3211f"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# use the data that was not used to build the model. This is the validation data.\n", "# scikit learn library has a function train_test_split data into two pieces.\n", "from sklearn.model_selection import train_test_split\n", "#split data into tarining and validation data, for both predicators and target\n", "# use random_state is used to provide the same split every time we run this script.\n", "train_X,val_X,train_y,val_y = train_test_split(X,y,random_state=0)\n", "#Define Model\n", "housing_model = DecisionTreeRegressor()\n", "#Fit model\n", "housing_model.fit(train_X,train_y)\n", "\n", "#get predicted prices on validation data\n", "val_predictions =housing_model.predict(val_X)\n", "print(mean_absolute_error(val_y,val_predictions))\n", "\n", "\n"], "metadata": {"_cell_guid": "8534c6ed-6ec4-4b1f-8946-346a062933fb", "collapsed": true, "_uuid": "ed0cce3c058fc94ffcd24db4d58571766cbe799a"}}, {"source": ["**Experimenting with different models**\n", "Overfitting: Where a model matches the training data almost perfectly, but does poorly in validation and other new data.\n", "Underfitting: When model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is underfitting."], "cell_type": "markdown", "metadata": {"_cell_guid": "f3320697-810c-4e9d-b119-949ccbb9361f", "_uuid": "e2171e98fd061f93d980b1f28fad67d5a7a24a84"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# we want to compare the Mean absolute error(MAE) from different values for max_leaf_nodes.\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.tree import DecisionTreeRegressor\n", "\n", "def get_mae(max_leaf_nodes,predictors_train,predictors_val,targ_train,targ_val):\n", " # Define the model  \n", "    model=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n", "# fit the model. fit the data.\n", "    model.fit(predictors_train,targ_train)\n", "# predict the values    \n", "    preds_val =model.predict(predictors_val)\n", "# calculate Mean absolute error\n", "    mae =mean_absolute_error(targ_val,preds_val)\n", "    return(mae)\n"], "metadata": {"_cell_guid": "b41eb372-2742-494c-8a62-13cea0cf5016", "collapsed": true, "_uuid": "0d55d7e36f5b621b3d91d2ace0aa2112f46346d3"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# comapre the MAE with different values of max_leaf_nodes\n", "for max_leaf_nodes in [5,50,500,5000]:\n", "    my_mae = get_mae(max_leaf_nodes,train_X,val_X,train_y,val_y)\n", "    print(\"Max leaf nodes :%d  \\t\\t  Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))\n", "# learn to format string\n", "          #http://www.diveintopython.net/native_data_types/formatting_strings.html\n", "          # Trying to concatenate a string with a non-string raises an exception so we\n", "          #have to use % to convert it int into string.\n", "          \n"], "metadata": {"_cell_guid": "bb84e8df-d60d-499e-bf21-987f963a7d37", "collapsed": true, "_uuid": "d1193f4fc654ea136309b10aa48a86518a6ebb83"}}, {"source": ["**Learning more sophistiated algorithms know as \"Random Forest\"** because Decision tree is not a sophisticated model. Generally we do not get validation data in model training"], "cell_type": "markdown", "metadata": {"_cell_guid": "5bf5f244-8ecb-4587-81e2-97b4e8210daa", "_uuid": "781d5b7bb85c1e41e7ba6c1361836d10c2ae0ca9"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.metrics import mean_absolute_error\n", "#define the model\n", "forest_model =RandomForestRegressor()\n", "# fit the model with the data. it is the training data\n", "forest_model.fit(train_X,train_y)\n", "#predict \n", "predict_vals = forest_model.predict(val_X)\n", "# calculate the Mean absolute error\n", "print(mean_absolute_error(val_y,predict_vals))\n"], "metadata": {"_cell_guid": "abf7aa5e-1aeb-469b-9003-4ffa5b07b5f2", "collapsed": true, "_uuid": "a5e464ece736ed2a760ec94bf78f94c9b16a9a89"}}, {"source": ["It is a improvment from 42k to 34k."], "cell_type": "markdown", "metadata": {"_cell_guid": "8a3ef370-816e-49e9-ad76-adefa3a41bda", "collapsed": true, "_uuid": "9e098ecb627cef6b61553fc290c66942a6c0b9fd"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# We are learning to make submission. here we have a separate file known as test.\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "#reading the training file\n", "train = pd.read_csv('../input/train.csv')\n", "\n", "# pull data into target and predictors.\n", "train_y= train.SalePrice\n", "predictor_cols =['LotArea','OverallQual','YearBuilt','TotRmsAbvGrd']\n", "# Create training predictors data\n", "train_X =train[predictor_cols]\n", "# define model\n", "my_model = RandomForestRegressor()\n", "# fit the model\n", "my_model.fit(train_X,train_y)\n"], "metadata": {"_cell_guid": "cfe2146b-e118-42ce-9b2c-8b30339f2edf", "collapsed": true, "_uuid": "f4c84b475ee1f82e49ddd3bbb11309e9284a3573"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["#Read the test file\n", "test = pd.read_csv('../input/test.csv')\n", "# test is like train data except no predictor value.That is what is our task\n", "test_X=test[predictor_cols]\n", "#now please use the model that was created in previous step to predict the outcome\n", "predict_prices =my_model.predict(test_X)\n", "print(predict_prices)\n", "# difference between print(predict_prices) and predict_prices is [....]  and ([....])\n", "# it means immutable that is tuple and mutable that is List.\n", "#predict_prices"], "metadata": {"_cell_guid": "7524d195-1486-40e8-807a-d25e36a83a0f", "collapsed": true, "_uuid": "01443d9d558ce4f18df3cae315a36e5494f5769b"}}, {"source": ["# remember we cannot have Mean absolute error here because we dont have a predicted values.\n", "\n", "\n", "**PREPARE SUBMISSION FILE**"], "cell_type": "markdown", "metadata": {"_cell_guid": "fbeab97f-b94d-4dac-88db-6b6343958d51", "_uuid": "44865bfbcb281d170143c83c6bc37c167363d296"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predict_prices})\n", "# it is a dictionary {}, a key value pair\n", "my_submission.to_csv('submission.csv',index=False)"], "metadata": {"_cell_guid": "891b1725-d156-4b79-b474-46b1607c37c6", "collapsed": true, "_uuid": "c7622486a1a048d532f486425fd88209208119a6"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# handling missing values\n", "#Option one is to drop column with missing values.\n"], "metadata": {"_cell_guid": "d523969c-da7b-4429-9d93-d8a4ecacecc9", "collapsed": true, "_uuid": "243d84a11426fdd8065ed646434daceed910952c"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "train_data =pd.read_csv('../input/train.csv')\n", "test_data =pd.read_csv('../input/test.csv')"], "metadata": {"_cell_guid": "c74c1d14-0fc7-4134-ba89-db62d611f91a", "collapsed": true, "_uuid": "82d057748330cdcf6439843f1771e04b9e95e2ca"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# we will not include houses in training dataset where houses are missing.\n", "train_data.dropna(axis=0,subset=['SalePrice'],inplace=True)\n", "target =train_data.SalePrice\n", "# we will drop the missingvalues columns which actually is not the right approach, we \n", "#could have have used the imputed method .\n", "cols_with_missing =[col for col in train_data.columns\n", "                               if train_data[col].isnull().any()]\n", "# drop these columns and also drop Id and SalePrice\n", "candidate_train_predictors = train_data.drop(['Id','SalePrice'] + cols_with_missing,axis=1)\n", "candidate_test_predictors = test_data.drop(['Id']+ cols_with_missing,axis=1)\n", "\n", "# cardinality means the number of unique values in a column\n", "#We use it as our only way to select categorical columns here.Ha ha ha. We select the low cardinality \n", "#column as a indicator of categorical values.\n", "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n", "                              candidate_train_predictors[cname].nunique() <10 and \n", "                              candidate_train_predictors[cname].dtype ==\"object\"]                         \n", "numeric_cols =[cname for cname in candidate_train_predictors.columns if\n", "                     candidate_train_predictors[cname].dtype in ['int64','float64']]\n", "my_cols = low_cardinality_cols + numeric_cols\n", "train_predictors =candidate_train_predictors[my_cols]\n", "test_predictors = candidate_test_predictors[my_cols]\n", "\n", "train_predictors.dtypes.sample(10)\n", "\n"], "metadata": {"_cell_guid": "062edd98-eb37-47b1-bd3a-d8a6e5a9ed49", "collapsed": true, "_uuid": "5103e812995beaa8f8adf7086edc2b3ed976e8f3"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["one_hot_encoded_training_predictors =pd.get_dummies(train_predictors)\n", "one_hot_encoded_training_predictors"], "metadata": {"_cell_guid": "1a385b0e-d266-4a55-95d9-fef5a1df3ff3", "collapsed": true, "_uuid": "e270bfb4c037e46275cba2f974bf77cdc12594c1"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["from sklearn.model_selection import cross_val_score\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "def get_mae(X,y):\n", "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n", "    return -1 * cross_val_score(RandomForestRegressor(50), X,y, scoring='neg_mean_absolute_error').mean()\n", "\n", "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n", "\n", "mae_without_categoricals =get_mae(one_hot_encoded_training_predictors,target)\n", "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors,target)\n", "print('Mean Absolute Error when Dropping Categoricals:' +str(int(mae_without_categoricals)))\n", "print('Mean Absolute Error with One-Hot Encoding:' +str(int(mae_one_hot_encoded)))\n"], "metadata": {"_cell_guid": "e6a8f179-7023-4316-8738-631b08fa1bcc", "collapsed": true, "_uuid": "4aee36cb4b33ebc1140b8235084d8f76d2d73433"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# the challenge is if training and test gets misalinged, results will be nonsense.\n", "# use align command \n", "one_hot_encoded_training_predictors =pd.get_dummies(train_predictors)\n", "one_hot_encoded_test_predictors =pd.get_dummies(test_predictors)\n", "final_train,final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, \n", "                                                                   join='left',axis=1)\n", "final_train\n", "final_test"], "metadata": {"_cell_guid": "6d7ba5c4-c75f-471e-9670-7fc886834062", "collapsed": true, "_uuid": "73f0ffe982b8d339038b0cd47f7e4bf2fb0802e1"}}, {"source": ["#Gradient Boosting with XGBoost\n", "XGBoost is the leading model for working with standard tabular data (it is the type of data stored in Pandas DataFrames, and not data like images and videos)"], "cell_type": "markdown", "metadata": {"_cell_guid": "88d1c2e7-9c8b-4c93-a857-4d6d805a25a0", "_uuid": "8b213482ad319c771f0738cfe2bcf7ff94d968ec"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import Imputer\n", "\n", "data_train =pd.read_csv('../input/train.csv')\n", "data_train.dropna(axis=0,subset=['SalePrice'],inplace=True)\n", "y=data_train.SalePrice\n", "X=data_train.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n", "# we are converting it to matrix\n", "train_X,test_X,train_Y,test_y=train_test_split(X.as_matrix(),y.as_matrix(),test_size=0.25)\n", "#label  = X[:,32]\n", "#weights = np.ones(len(labels))\n", "#dtrain = xgb.DMatrix(X, label = labels, weight = weights)\n", "# using imputer function of sklearn\n", "my_imputer = Imputer()\n", "# fit the model\n", "train_X =my_imputer.fit_transform(train_X)\n", "test_X =my_imputer.transform(test_X)\n", "    "], "metadata": {"_cell_guid": "8306862f-a475-4f61-9f37-fa1056d5f881", "collapsed": true, "_uuid": "2f68ec70e7987cc148e2a274ab36af80e0b36c9e"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["from xgboost import XGBRegressor\n", "#Define the model\n", "my_model =XGBRegressor()\n", "my_model.fit(train_X,train_Y,verbose=False)"], "metadata": {"_cell_guid": "4ff31130-78d5-4871-8151-4b5d641bd8d0", "collapsed": true, "_uuid": "b538de480940a48846c48529dcdf1a38028296ff"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# make predictions\n", "predictions= my_model.predict(test_X)\n", "from sklearn.metrics import mean_absolute_error\n", "print(\"Mean absolute error :\" + str(mean_absolute_error(predictions,test_y)))\n", "#train_X,test_X,train_Y,test_y"], "metadata": {"_cell_guid": "dccf2756-20c6-4ca6-93ae-126090a19778", "collapsed": true, "_uuid": "c4b6d5c4fc839b57d255acc66018a963910f7d73"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["#n_estimators and early_stopping_rounds\n", "my_model = XGBRegressor(n_estimators=1000,learning_rate=0.5)\n", "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n", "             eval_set=[(test_X, test_y)], verbose=False)\n"], "metadata": {"_cell_guid": "f8be028a-82c9-47bf-8987-4084f600ca22", "collapsed": true, "_uuid": "7243d3543059abb4afb777413c82d8e4a344d7fe"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["data_train.columns"], "metadata": {"_cell_guid": "9e7b9410-1344-42df-86e5-75eb93535991", "collapsed": true, "_uuid": "047833b8550d35cfff1d6d41b94cfff19d95ed52"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["def get_some_data():\n", "    cols_to_use =['LotArea','TotalBsmtSF','TotRmsAbvGrd']\n", "    data =pd.read_csv('../input/train.csv')\n", "    Y=data.SalePrice\n", "    x=data[cols_to_use]\n", "    my_imputer =Imputer()\n", "    imputed_X =my_imputer.fit_transform(X)\n", "    return imputed_X,y\n", "\n", "#partisl dependence to figure out complex patterns and dependences\n", "from sklearn.ensemble.partial_dependence import partial_dependence,plot_partial_dependence\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "#from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n", "# get_some_data is defined in hidden cell above\n", "X,y = get_some_data()\n", "#define\n", "my_model =GradientBoostingRegressor()\n", "\n", "#fit the model\n", "my_model.fit(X,y)\n", "# Here we make the plot\n", "my_plots = plot_partial_dependence(my_model,features=[1,2], #column number of plots we want to show\n", "                                  X=X, #raw predictor data\n", "                                  feature_names=['LotArea','TotalBsmtSF','TotRmsAbvGrd'],# labels on graphs\n", "                                  grid_resolution =10)# number of values to plot on X axis\n"], "metadata": {"_cell_guid": "49c4a482-efa9-4f58-8ca1-ba4a902b07e1", "collapsed": true, "_uuid": "04e2681887f5970c2578e81b4716ca41cca77a50"}}, {"source": ["# What are Pipelines?\n", "Pipelines bind preprocessing and modelling, so that whole bundle can be used as if it were a single step\n", "1)Cleaner Code: no need to take care of training and validation data at each step of processing.\n", "2)the challenge is, it is tough to transition a model from prototype to something deployable at scale. \n", "3)helps in cross validation"], "cell_type": "markdown", "metadata": {"_cell_guid": "3c6fbbc2-9b6c-459e-9e58-d9ad39f9c05a", "_uuid": "cc69eb7fb292228be22d8f2ad0784ad2e46906c7"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# let load the data and set predicators and divide it into train and test\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "\n", "#Read data\n", "data= pd.read_csv('../input/train.csv')\n", "cols_to_use =['BedroomAbvGr','LotArea','PoolArea','TotRmsAbvGrd','YrSold']\n", "X= data[cols_to_use]\n", "y=data.SalePrice\n", "train_X,test_X,train_y,test_y = train_test_split(X,y)\n", "#Modelling proces that will fill Imputer at missing places,\n", "#than folowed by RandomForestRegressor to make predictions\n", "# will bundle Imputer and RandomForestRegressor\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocesing import Imputer\n", "# pipelines binds pre processing and modeling\n", "my_pipeline =make_pipeline(Imputer(),RandomForestRegressor())\n", "#now predict as a fued whole\n", "#fit\n", "my_pipeline.fit(train_X,train_y)\n", "#predictions\n", "predictions =my_pipeline.predict(test_X)\n", "\n", "# Sciket learn are of two categories. Transformers and models\n", "# Transformers are for pre-processing and models are for predictions\n", "# generally after fitting a transformer we apply transform command,After fitting a model we apply predict\n", "#command\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n"], "metadata": {"_cell_guid": "ddaf9e2c-bdf6-44b9-ab38-6a1dbe7f87e3", "collapsed": true, "_uuid": "9c9c1e3132b810a147d895e0bad90005ffdbc438"}}, {"source": ["**CROSS VALIDATION**\n", "\n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "64ed3357-87c2-4a9e-ac1d-50bb0c5d8518", "collapsed": true, "_uuid": "3a5b6ce7eb906c231400e9cc02cfa7a695545969"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "data_train=pd.read_csv('../input/train.csv')\n", "cols_to_use =['LotArea','OverallQual','YearBuilt','TotRmsAbvGrd']\n", "X =data_train[cols_to_use]\n", "y= data_train.SalePrice\n", "\n", "#NOTE: IT IS EASY TO DO CROSS VALIDATON IF PIPELINE IS BEING USED\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.pipeline import make_pipeline\n", "from sklearn.preprocessing import Imputer\n", "my_pipeline = make_pipeline(Imputer(),RandomForestRegressor())\n", "\n", "#lets do the cross validation\n", "\n", "from sklearn.model_selection import cross_val_score\n", "scores = cross_val_score(my_pipeline,X,y,scoring='neg_mean_absolute_error')\n", "print(scores)\n", "# take the average of the mean \n", "print('Mean Absolute Error %2f'%(-1 * scores.mean()))"], "metadata": {"_cell_guid": "4a0d6968-6ca7-40cd-8b55-5cf6788c4596", "collapsed": true, "_uuid": "00723e66f77ba7833ae0f5d75d721351452bd7d6"}}, {"source": ["What is Data Leakage?\n", "Leakage causes a model to look acccurate until you start making decisions with the model.there are two main types of leakages:\n", "**Leaky Predictors** and **Leaky validation Strategies.**\n", "\n", "\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "93f6f7e4-ac35-487a-b549-838f60ba59dc", "collapsed": true, "_uuid": "109fdc52e41a0d2a022a084d892b2491763c7ddc"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["#Leaky Predictors\n", "#This occurs when your predictors include data that will not be available at the time you make predictions\n", "# Leaky Validation Strategy\n", "#A much different type of leak occurs when you aren't careful distinguishing training data from validation data.\n", "#Your model will get very good validation scores, giving you great confidence in it,\n", "#but perform poorly when you deploy it to make decisions.\n", "# The biggest challenge is that leaky predictors have HIGH CORRELATION WITH THE TARGET.\n", "\n"], "metadata": {"_cell_guid": "1fd2d23a-396f-41b6-959b-546fe5dd92a9", "collapsed": true, "_uuid": "48ee1fd7932bf653ba1b63f8274f8f7f5d44761d"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": [], "metadata": {"_cell_guid": "2c165c7a-4913-4399-ad48-7cfeaea70a03", "collapsed": true, "_uuid": "58dc1146650dd637ae254c56133b80cb35226555"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": [], "metadata": {"_cell_guid": "22065bab-502a-4c73-abfc-49be93b15fe7", "collapsed": true, "_uuid": "c8c28e4b8ed0bd589c1046446d301b0562bfa284"}}], "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}}