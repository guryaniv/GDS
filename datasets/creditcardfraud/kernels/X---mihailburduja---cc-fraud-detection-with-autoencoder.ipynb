{"cells": [{"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:31.72593Z", "start_time": "2017-07-17T20:05:29.336576Z"}, "_execution_state": "idle", "_cell_guid": "258a6cdb-8369-4f08-9246-a9f4043e6074", "_uuid": "dc92972c97319524c8d57299dbd46c7adc534925"}, "outputs": [], "execution_count": null, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf # Neural network\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# from subprocess import check_output\n# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:32.282411Z", "start_time": "2017-07-17T20:05:31.727435Z"}, "_execution_state": "idle", "_cell_guid": "e4285c4d-93c4-401a-8e21-cf6d67570728", "_uuid": "611ad3319b0278aeac026c4f22d9f9ca4af8e235"}, "outputs": [], "execution_count": null, "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nprint(plt.style.available)"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:32.312992Z", "start_time": "2017-07-17T20:05:32.299966Z"}, "_execution_state": "idle", "_cell_guid": "1414aa8d-5b6a-4ece-81c4-35809f7c8851", "_uuid": "c0c2dbea78677d7ee6f12a9147bba7f65de6e5fc"}, "outputs": [], "execution_count": null, "source": ["plt.style.use('ggplot')"]}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:33.834574Z", "start_time": "2017-07-17T20:05:32.316004Z"}, "_execution_state": "idle", "_cell_guid": "02d2c625-4297-4d98-92d9-fb7f2e218c37", "_uuid": "16e594ef6e003350b0be5067a944d93b9565b18b"}, "outputs": [], "execution_count": null, "source": "plt.scatter(df[df.Class == 0].Time, df[df.Class == 0].Amount)"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:33.95837Z", "start_time": "2017-07-17T20:05:33.837045Z"}, "_execution_state": "idle", "_cell_guid": "bbce8577-4bfc-4454-890c-2087b61daecd", "_uuid": "4fe5ff0064e47069e36b9bae7bcc1d07f4fefca6"}, "outputs": [], "execution_count": null, "source": "plt.scatter(df[df.Class == 1].Time, df[df.Class == 1].Amount, c=df[df.Class == 1].Class)"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:05:34.057649Z", "start_time": "2017-07-17T20:05:33.960409Z"}, "_execution_state": "idle", "_cell_guid": "acb99673-67da-4df5-8e06-84f9fb1309b8", "_uuid": "ba9345bcd5e21bd74ef7d61f4b4675f4d6810af9"}, "outputs": [], "execution_count": null, "source": "data = df.values\n\nnormals = df[df.Class == 0].values[:, 1:30]\nfrauds = df[df.Class == 1].values[:, 1:30]\n\ntraining = normals[:-16384]\nvalidation = normals[-16384:]"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:11:22.326702Z", "start_time": "2017-07-17T20:11:22.124164Z"}, "_execution_state": "idle", "_cell_guid": "d90386d2-90c8-4d20-a7a4-f4d1f82faea6", "_uuid": "eb9a1c36128814349939b08055ca4ba78ba9ac52"}, "outputs": [], "execution_count": null, "source": "input_size = 29\nhidden_size = 128\n\n# Graph definition\ngraph = tf.Graph()\nwith graph.as_default():\n    features = tf.placeholder(tf.float32, shape=(None, input_size), name='features')\n    is_training = tf.placeholder(tf.bool, name='is_training')\n\n    with tf.name_scope('hidden_layer'):\n        fcw = tf.Variable(tf.truncated_normal([input_size, hidden_size],\n                                              dtype=tf.float32,\n                                              stddev=1e-1), name='weights')\n        fcb = tf.Variable(tf.constant(1.0, shape=[hidden_size], dtype=tf.float32), name='biases')\n        logits = tf.nn.bias_add(tf.matmul(features, fcw), fcb)\n        # logits = tf.layers.batch_normalization(logits, training=is_training)\n        logits = tf.nn.relu(logits)\n\n    '''\n    with tf.name_scope('secret_layer'):\n        fcw = tf.Variable(tf.truncated_normal([hidden_size, hidden_size],\n                                              dtype=tf.float32,\n                                              stddev=1e-1), name='weights')\n        fcb = tf.Variable(tf.constant(1.0, shape=[hidden_size], dtype=tf.float32), name='biases')\n        logits = tf.nn.bias_add(tf.matmul(logits, fcw), fcb)\n        # logits = tf.layers.batch_normalization(logits, training=is_training)\n        logits = tf.nn.relu(logits)\n    '''\n\n    with tf.name_scope('output_layer'):\n        fcw = tf.Variable(tf.truncated_normal([hidden_size, input_size],\n                                              dtype=tf.float32,\n                                              stddev=1e-1), name='weights')\n        fcb = tf.Variable(tf.constant(1.0, shape=[input_size], dtype=tf.float32), name='biases')\n        logits = tf.nn.bias_add(tf.matmul(logits, fcw), fcb)\n\n    # Define loss and optimizer\n    batch_losses = tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(features, logits), 2), 1))\n    loss = tf.reduce_mean(batch_losses)\n    optimize = tf.train.AdamOptimizer(0.0001).minimize(loss=loss)"}, {"cell_type": "code", "metadata": {"trusted": false, "ExecuteTime": {"end_time": "2017-07-17T20:12:42.176568Z", "start_time": "2017-07-17T20:12:37.939804Z"}, "_execution_state": "idle", "_cell_guid": "b3e7154b-9d3e-4ce5-80dc-d8bc54998537", "_uuid": "78198ee9102f09a66c40036704d4833671964570"}, "outputs": [], "execution_count": null, "source": " batch_size = 1024\n\n# Run graph\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    losses = []\n    \n    # Training\n    for epoch in range(24):\n        epoch_loss = 0.\n        for i in range(0, len(training), batch_size):\n            start_idx = i\n            end_idx = min(i + batch_size, len(normals))\n\n            batch_loss, _ = sess.run([loss, optimize], feed_dict={\n                features: training[start_idx: end_idx],\n                is_training: True\n            })\n            # print('Loss at', start_idx, batch_loss)\n            losses.append(batch_loss)\n            epoch_loss += batch_loss\n        # print('Mean Epoch Loss', (epoch_loss / (len(normals) / batch_size)))\n        \n    # print('Mean Loss', np.mean(losses))\n    \n    # Evaluation\n    batch_loss, valid_predictions = sess.run([loss, batch_losses], feed_dict={\n        features: validation,\n        is_training: False\n    })\n    print('Non-fraudulent transactions loss', batch_loss)\n    batch_loss, fraud_predictions = sess.run([loss, batch_losses], feed_dict={\n        features: frauds,\n        is_training: False\n    })\n    print('Fraudulent transactions Loss', batch_loss)\n    \n    print('Valid transactions:', np.shape(valid_predictions)[0])\n    print('Fraudulent transactions:', np.shape(fraud_predictions)[0])\n    \n    # Calculation\n    true_positives = 0\n    false_positives = 0\n    true_negatives = 0\n    false_negatives = 0\n    \n    # threshold = 2. # 24 epochs, 2 hidden layers, no batch normalization - 0.87, 0.82, 0.83\n    threshold = 2 # 24 epochs, 1x128 hidden layers - 0.89, 0.81, 0.84\n    for x in valid_predictions:\n        if x > threshold:\n            false_positives += 1\n        else:\n            true_negatives += 1\n\n\n    for x in fraud_predictions:\n        if x > threshold:\n            true_positives += 1\n        else:\n            false_negatives += 1\n\n    print('True fraudulent transactions: {}'.format(true_positives))\n    print('False fraudulent transactions: {}'.format(false_positives))\n    print('True valid transactions: {}'.format(true_negatives))\n    print('False valid transactions: {}'.format(false_negatives))\n\n    precision = true_positives * 1.0 / (true_positives + false_positives)\n    recall = true_positives * 1.0 / (true_positives + false_negatives)\n    f1_score = 2.0 * (precision * recall) / (precision + recall)\n    print('Precision: {:.2}'.format(precision))\n    print('Recall: {:.2}'.format(recall))\n    print('F1-Score: {:.2}'.format(f1_score))\n\n    plt.figure(figsize=(12, 7))\n    plt.plot(losses)\n    plt.show()\n    \n    bins = np.linspace(0, 1000, 100)\n    plt.figure(figsize=(12, 7))\n    plt.hist(valid_predictions[:492], bins=bins, alpha=0.5, label='valids', color='green')\n    plt.hist(fraud_predictions, bins=bins, alpha=0.5, label='frauds', color='red')\n    plt.show()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "anaconda-cloud": {}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.1"}}, "nbformat": 4, "nbformat_minor": 1}