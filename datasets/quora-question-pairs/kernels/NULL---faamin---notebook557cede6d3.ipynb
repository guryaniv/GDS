{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ded48a8c-937a-f15a-3ff6-c66cf93231bd"
      },
      "source": [
        "Quora Question Pair Kernl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca8035f1-3738-0b46-dc46-a0df8cc2fd37"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import linear_model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "matplotlib.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b5803a0-c8de-4ff8-1f48-b2a1e0638f41"
      },
      "outputs": [],
      "source": [
        "trainDF = pd.read_csv('../input/train.csv')\n",
        "trainDF = trainDF.dropna(how=\"any\").reset_index(drop=True)\n",
        "\n",
        "trainDF.ix[:7,3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b101dc9-87ba-d2c4-186e-f76d4ddc73ca"
      },
      "outputs": [],
      "source": [
        "#%% create dictionary and extract BOW features from questions\n",
        "\n",
        "featureExtractionStartTime = time.time()\n",
        "\n",
        "maxNumFeatures = 300\n",
        "\n",
        "# bag of letter sequences (chars)\n",
        "BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=1000, max_features=maxNumFeatures, \n",
        "                                      analyzer='char', ngram_range=(1,2), \n",
        "                                      binary=True, lowercase=True)\n",
        "# bag of words\n",
        "#BagOfWordsExtractor = CountVectorizer(max_df=0.999, min_df=10, max_features=maxNumFeatures, \n",
        "#                                      analyzer='word', ngram_range=(1,6), stop_words='english', \n",
        "#                                      binary=True, lowercase=True)\n",
        "\n",
        "BagOfWordsExtractor.fit(pd.concat((trainDF.ix[:,'question1'],trainDF.ix[:,'question2'])).unique())\n",
        "\n",
        "trainQuestion1_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question1'])\n",
        "trainQuestion2_BOW_rep = BagOfWordsExtractor.transform(trainDF.ix[:,'question2'])\n",
        "lables = np.array(trainDF.ix[:,'is_duplicate'])\n",
        "\n",
        "featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\n",
        "print(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f70ae54-72c7-9807-cb3f-909a9d142e87"
      },
      "outputs": [],
      "source": [
        "#%% prefrom cross validation\n",
        "\n",
        "crossValidationStartTime = time.time()\n",
        "\n",
        "numCVSplits = 8\n",
        "numSplitsToBreakAfter = 2\n",
        "\n",
        "X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int)\n",
        "#X = -(trainQuestion1_BOW_rep != trainQuestion2_BOW_rep).astype(int) + \\\n",
        "#      trainQuestion1_BOW_rep.multiply(trainQuestion2_BOW_rep)\n",
        "y = lables\n",
        "\n",
        "logisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "logRegAccuracy = []\n",
        "logRegLogLoss = []\n",
        "logRegAUC = []\n",
        "\n",
        "print('---------------------------------------------')\n",
        "stratifiedCV = model_selection.StratifiedKFold(n_splits=numCVSplits, random_state=2)\n",
        "for k, (trainInds, validInds) in enumerate(stratifiedCV.split(X, y)):\n",
        "    foldTrainingStartTime = time.time()\n",
        "\n",
        "    X_train_cv = X[trainInds,:]\n",
        "    X_valid_cv = X[validInds,:]\n",
        "\n",
        "    y_train_cv = y[trainInds]\n",
        "    y_valid_cv = y[validInds]\n",
        "\n",
        "    logisticRegressor.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "    y_train_hat =  logisticRegressor.predict_proba(X_train_cv)[:,1]\n",
        "    y_valid_hat =  logisticRegressor.predict_proba(X_valid_cv)[:,1]\n",
        "\n",
        "    logRegAccuracy.append(accuracy_score(y_valid_cv, y_valid_hat > 0.5))\n",
        "    logRegLogLoss.append(log_loss(y_valid_cv, y_valid_hat))\n",
        "    logRegAUC.append(roc_auc_score(y_valid_cv, y_valid_hat))\n",
        "    \n",
        "    foldTrainingDurationInMinutes = (time.time()-foldTrainingStartTime)/60.0\n",
        "    print('fold %d took %.2f minutes: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (k+1,\n",
        "             foldTrainingDurationInMinutes, logRegAccuracy[-1],logRegLogLoss[-1],logRegAUC[-1]))\n",
        "\n",
        "    if (k+1) >= numSplitsToBreakAfter:\n",
        "        break\n",
        "\n",
        "\n",
        "crossValidationDurationInMinutes = (time.time()-crossValidationStartTime)/60.0\n",
        "\n",
        "print('---------------------------------------------')\n",
        "print('cross validation took %.2f minutes' % (crossValidationDurationInMinutes))\n",
        "print('mean CV: accuracy = %.3f, log loss = %.4f, AUC = %.3f' % (np.array(logRegAccuracy).mean(),\n",
        "                                                                 np.array(logRegLogLoss).mean(),\n",
        "                                                                 np.array(logRegAUC).mean()))\n",
        "print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6004a132-9cb3-8485-907b-eb49aaae8ae6"
      },
      "outputs": [],
      "source": [
        "#%% show prediction distribution and \"feature importance\"\n",
        "\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
        "\n",
        "plt.figure(); \n",
        "sns.kdeplot(y_valid_hat[y_valid_cv==0], shade=True, color=\"b\", bw=0.01)\n",
        "sns.kdeplot(y_valid_hat[y_valid_cv==1], shade=True, color=\"g\", bw=0.01)\n",
        "plt.legend(['non duplicate','duplicate'],fontsize=24)\n",
        "plt.title('Validation Accuracy = %.3f, Log Loss = %.4f, AUC = %.3f' %(logRegAccuracy[-1],\n",
        "                                                                      logRegLogLoss[-1],\n",
        "                                                                      logRegAUC[-1]))\n",
        "plt.xlabel('Prediction'); plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)\n",
        "\n",
        "\n",
        "numFeaturesToShow = 30\n",
        "\n",
        "sortedCoeffients = np.sort(logisticRegressor.coef_)[0]\n",
        "featureNames = BagOfWordsExtractor.get_feature_names()\n",
        "sortedFeatureNames = [featureNames[x] for x in list(np.argsort(logisticRegressor.coef_)[0])]\n",
        "\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['figure.figsize'] = (10,12)\n",
        "\n",
        "plt.figure()\n",
        "plt.suptitle('Feature Importance',fontsize=24)\n",
        "ax = plt.subplot(1,2,1); plt.title('top non duplicate predictors'); \n",
        "plt.xlabel('minus logistic regression coefficient')\n",
        "ax.barh(range(numFeaturesToShow), -sortedCoeffients[:numFeaturesToShow][::-1], align='center'); \n",
        "plt.ylim(-1,numFeaturesToShow); ax.set_yticks(range(numFeaturesToShow)); \n",
        "ax.set_yticklabels(sortedFeatureNames[:numFeaturesToShow][::-1],fontsize=20)\n",
        "\n",
        "ax = plt.subplot(1,2,2); plt.title('top duplicate predictors'); \n",
        "plt.xlabel('logistic regression coefficient')\n",
        "ax.barh(range(numFeaturesToShow), sortedCoeffients[-numFeaturesToShow:], align='center'); \n",
        "plt.ylim(-1,numFeaturesToShow); ax.set_yticks(range(numFeaturesToShow)); \n",
        "ax.set_yticklabels(sortedFeatureNames[-numFeaturesToShow:],fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "085d6edd-3d70-83a0-0b06-40f26750d663"
      },
      "outputs": [],
      "source": [
        "#%% train on full training data\n",
        "\n",
        "trainingStartTime = time.time()\n",
        "\n",
        "logisticRegressor = linear_model.LogisticRegression(C=0.1, solver='sag', \n",
        "                                                    class_weight={1: 0.46, 0: 1.32})\n",
        "logisticRegressor.fit(X, y)\n",
        "\n",
        "trainingDurationInMinutes = (time.time()-trainingStartTime)/60.0\n",
        "print('full training took %.2f minutes' % (trainingDurationInMinutes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98a61188-00b1-d3e3-76e9-d646d4a55595"
      },
      "outputs": [],
      "source": [
        "#%% load test data, extract features and make predictions\n",
        "\n",
        "testPredictionStartTime = time.time()\n",
        "\n",
        "testDF = pd.read_csv('../input/test.csv')\n",
        "testDF.ix[testDF['question1'].isnull(),['question1','question2']] = 'random empty question'\n",
        "testDF.ix[testDF['question2'].isnull(),['question1','question2']] = 'random empty question'\n",
        "\n",
        "testQuestion1_BOW_rep = BagOfWordsExtractor.transform(testDF.ix[:,'question1'])\n",
        "testQuestion2_BOW_rep = BagOfWordsExtractor.transform(testDF.ix[:,'question2'])\n",
        "\n",
        "X_test = -(testQuestion1_BOW_rep != testQuestion2_BOW_rep).astype(int)\n",
        "#X_test = -(testQuestion1_BOW_rep != testQuestion2_BOW_rep).astype(int) + \\\n",
        "#           testQuestion1_BOW_rep.multiply(testQuestion2_BOW_rep)\n",
        "\n",
        "#testPredictions = logisticRegressor.predict_proba(X_test)[:,1]\n",
        "\n",
        "# quick fix to avoid memory errors\n",
        "seperators= [750000,1500000]\n",
        "testPredictions1 = logisticRegressor.predict_proba(X_test[:seperators[0],:])[:,1]\n",
        "testPredictions2 = logisticRegressor.predict_proba(X_test[seperators[0]:seperators[1],:])[:,1]\n",
        "testPredictions3 = logisticRegressor.predict_proba(X_test[seperators[1]:,:])[:,1]\n",
        "testPredictions = np.hstack((testPredictions1,testPredictions2,testPredictions3))\n",
        "\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
        "\n",
        "plt.figure(); \n",
        "plt.subplot(2,1,1); sns.kdeplot(y_valid_hat, shade=True, color=\"b\", bw=0.01); \n",
        "plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)\n",
        "plt.title('mean valid prediction = ' + str(np.mean(y_valid_hat)))\n",
        "plt.subplot(2,1,2); sns.kdeplot(testPredictions, shade=True, color=\"b\", bw=0.01);\n",
        "plt.xlabel('Prediction'); plt.ylabel('Probability Density'); plt.xlim(-0.01,1.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "703e7a43-c3d5-ef3d-292c-268a3061619a"
      },
      "outputs": [],
      "source": [
        "Bag = CountVectorizer(max_df=0.999, min_df=50, max_features=300, \n",
        "                                      analyzer='char', ngram_range=(1,2), \n",
        "                                      binary=True, lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "258f9ee9-8164-4699-4865-f7e1954c662a"
      },
      "outputs": [],
      "source": [
        "Bag.fit(pd.concat((train.question1,train.question2)).unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8746cadc-48e7-2ae3-54bb-043a9349d031"
      },
      "outputs": [],
      "source": [
        "question1 = Bag.transform(train['question1'])\n",
        "question2 = Bag.transform(train['question2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fffb19c-e08f-6937-ddc6-643b6eecc42b"
      },
      "outputs": [],
      "source": [
        "question1 = question1.toarray()\n",
        "question2 = question2.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40bee776-ce63-1c7f-e30c-10869389877e"
      },
      "outputs": [],
      "source": [
        "question1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7926589-8886-077d-e8df-adf2b483a371"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}