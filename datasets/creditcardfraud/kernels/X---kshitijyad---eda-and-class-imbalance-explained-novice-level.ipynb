{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "version": "3.6.1", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python"}}, "nbformat_minor": 2, "nbformat": 4, "cells": [{"source": "from sklearn.preprocessing import binarize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix,auc,roc_auc_score,recall_score,classification_report,precision_recall_curve, roc_curve\nfrom subprocess import check_output\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "afb9bf7f1f4a9c1d7c1869bf095a1e3d4f3d0b75", "_cell_guid": "97ec8303-ea09-4467-99f3-818e461b3018"}, "execution_count": null, "cell_type": "code"}, {"source": "data=pd.read_csv(\"../input/creditcard.csv\")\ndata.head()", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "58646e5af8a350fa18e1666cb1e33177c2be37fd", "_cell_guid": "453cc5aa-e931-40fc-b611-412861cd479a"}, "execution_count": null, "cell_type": "code"}, {"source": "# Data Preprocessing and EDA :\n\nLets start by finding out if there is any correlation in the variables in the dataset", "outputs": [], "metadata": {"_uuid": "f74926919b50333b2887c0cf2738717d5a6c5994", "_cell_guid": "4b165977-6960-41b6-ae8a-7b1f2b8e0b9a"}, "execution_count": null, "cell_type": "markdown"}, {"source": "plt.rcParams['figure.figsize']=(10,10)\nsns.heatmap(data.corr())\nsns.plt.show()", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "e23f699f020b193bfb4ac4e73cf440d20d61897d", "_cell_guid": "3f2a8f60-eaec-40fd-abe7-3f1259b434a1"}, "execution_count": null, "cell_type": "code"}, {"source": "Looking above it is safe to say that the data is uncorrelated and hence we can moveforward with our analysis. Lets check for missing values and do some Exploratory Data Analysis before we move forward.\n\nLooking for missing values in the Data - ", "outputs": [], "metadata": {"_uuid": "ab89cf27d837b9f4f5392cd26dd08bd2f37b6c02", "_cell_guid": "d1ffe806-fe6a-4412-bf50-2db7525a1643"}, "execution_count": null, "cell_type": "markdown"}, {"source": "data.isnull().any().sum()", "outputs": [], "metadata": {"trusted": false, "_uuid": "ab35f32e0ac5afca6bbeebeeaa6ac4aac39c71c0", "_cell_guid": "ddf21534-3e18-4363-a071-0163846557c6"}, "execution_count": null, "cell_type": "code"}, {"source": "No missing values found!\nLets see the **'Class'** in the data", "outputs": [], "metadata": {"_uuid": "b8daf8e54fcf272828e17dbfcebd9a7202799a5b", "_cell_guid": "3b139cf2-dac9-4ccc-a466-ffb065666ae3"}, "execution_count": null, "cell_type": "markdown"}, {"source": "sns.countplot(data['Class'])\nsns.plt.show()\nprint('Percent of fraud transaction: ',len(data[data['Class']==1])/len(data['Class'])*100,\"%\")", "outputs": [], "metadata": {"trusted": false, "_uuid": "2e59dcfe134c19a8be45988a01e23c54d895aa91", "_cell_guid": "b5a7315b-4729-4de8-ae8a-7b3196605281"}, "execution_count": null, "cell_type": "code"}, {"source": "Seeing the above plot we can say that the data is Highly imbalanced meaning that the ratio of normal transactions to fraud transactions is is very high or that fraud transactions are very very few. This will make our data highly [imbalanced](http://www.chioka.in/class-imbalance-problem/).\n\nLets take a look at how **Amount** looks like for normal and then for fraud transaction-", "outputs": [], "metadata": {"_uuid": "f1cfa8bb9c4f69dbbbb1393f80bb38d1423a5746", "_cell_guid": "4a417be0-2e13-4fbe-9ac0-e2ee9d731156"}, "execution_count": null, "cell_type": "markdown"}, {"source": "sns.distplot(data.Amount)\nsns.plt.show()\nsns.distplot(data[data.Class==1].Amount)\nsns.plt.show()\n", "outputs": [], "metadata": {"trusted": false, "_uuid": "4d13f6b2e8f8e42fe956734bb269b343288f2af5", "_cell_guid": "b0db2b02-ff0a-4623-b979-3aced8f4bb84"}, "execution_count": null, "cell_type": "code"}, {"source": "We can see from above that the maximum fraudulent transactions happen for small amount and next to negligiable for higher transaction. **But is the mean of transaction for fradulent transaction amount higher or lower than the mean of normal transaction?**\nFor this lets perform a Hypothesis test and get the result through statistics.\n\n        H0: Mean of Fradulent transaction = Mean of Normal Transaction\n        H1: Mean of Fradulent transaction != Mean of Normal Transaction\n        \nLets calculate 'Z', '\u03bc' and S.E -", "outputs": [], "metadata": {"_uuid": "41ec0ad93866098c0888935eebc79dfac4aa649e", "_cell_guid": "0ff66c19-5fea-43ef-a34b-171f5c6c9d97"}, "execution_count": null, "cell_type": "markdown"}, {"source": "population = data[data.Class == 0].Amount\nsample = data[data.Class == 1].Amount\nsampleMean = sample.mean()\npopulationStd = population.std()\npopulationMean = population.mean()\nz_score = (sampleMean - populationMean) / (populationStd / sample.size ** 0.5)\nz_score", "outputs": [], "metadata": {"trusted": false, "_uuid": "65e6340aa59a64fa0e99138d5b9ad7cc5756cdd7", "_cell_guid": "45803f8c-f95a-461e-b786-bc5f466e59aa"}, "execution_count": null, "cell_type": "code"}, {"source": "Since Z score is more than **2.576** we can say we **reject** the Null Hypothesis and hence the mean of amount of fradulent transaction is more than the normal transaction. This is a very good insight as this can help us see the characterstics of a fradulent transaction. \n\nLets prepare to run a Logistic Regression on the data. We will start by dropping unneccesary columns and splitting it into Train and Test.\n\nAlso since the data is imbalanced, we will undersample the data so as to process it and use it to fit a model. ", "outputs": [], "metadata": {"_uuid": "d073b7dbc26c5eac561e84721224e90b7348d6bf", "_cell_guid": "d3f51b7a-9e67-4a69-8d9e-c47b51d0b761"}, "execution_count": null, "cell_type": "markdown"}, {"source": "train= data.drop(['Time'], axis=1)\n\nX= train.drop('Class',axis=1)\ny= train['Class']", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "d1026a7dcd87fa14dd966a5856fb53e396ef6ef4", "_cell_guid": "24fc24d1-c8b1-42dd-9563-df43c483d359"}, "execution_count": null, "cell_type": "code"}, {"source": "Lets assign random variables to the undersample class and then break the data set into Test and Train and Test Undersample and Train Undersample.\n#### Lets 1st try to run a model without tackling the problem of imbalanced class and see what precision we get.", "outputs": [], "metadata": {"_uuid": "dcfeef04e2a36b9811267c84acac50e3463abd53", "_cell_guid": "aa13e7bc-18b8-4993-a729-586b751f378a"}, "execution_count": null, "cell_type": "markdown"}, {"source": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "b3acebef50fca400802b003e2011ee23780c6246", "_cell_guid": "ffaf7adb-3541-4c76-993e-233e1fb07a3d", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": "Defining a function to plot the confusion matrix. [Source](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)", "outputs": [], "metadata": {"_uuid": "af16bd3b9bf46061d1b06c6135c58ca8bc6ce826", "_cell_guid": "534d56a0-4904-420a-a505-e3f1afe2d20c"}, "execution_count": null, "cell_type": "markdown"}, {"source": "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=15)\n    plt.yticks(tick_marks, classes, rotation=15)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    \n        #print(cm)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "9e7650748c46a35b8ccb5bc21562bafc8267be7a", "_cell_guid": "055770f3-93cb-400b-a8c0-3192acf1b691"}, "execution_count": null, "cell_type": "code"}, {"source": "class_set = [0, 1]\nlr = LogisticRegression()\nlr.fit(X_train, y_train.values.ravel())\ny_pred = lr.predict(X_test)\ncnf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\nnp.set_printoptions(precision=2)\nprint (\"Confusion matrix undersampled\")\nplt.rcParams['figure.figsize']=(4,4)\nplot_confusion_matrix(cm=cnf_matrix, classes=class_set)\nplt.show()\nprint('cr:', classification_report(y_test,y_pred))", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "6bdcffdae35c41886091c7792211d61792437bf2", "_cell_guid": "9f1115ad-78bd-4cc8-8543-c038fcbefc4a"}, "execution_count": null, "cell_type": "code"}, {"source": "y_predprob = lr.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test,y_predprob)\nroc_auc = auc(fpr,tpr)\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC curve for fraud classifier')\nplt.grid(True)\nplt.show()\nroc_auc_score(y_test, y_predprob)", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "8236b361a4ad34653ae2c2a86a125f68eaaa0bdc", "_cell_guid": "d8dd42df-a40c-4f73-a8b1-c04607d49206"}, "execution_count": null, "cell_type": "code"}, {"source": "We can see that the precision we got is somewhat about 0.88 . Lets under Sample the data and see if there any improvement to the precision or not ?\n#### Using the Undersampled Test and Train data and running the model.\n\nPreparing for Undersampling - ", "outputs": [], "metadata": {"_uuid": "690b0ce8e37e90c14429fc3621595cd5ed2821f6", "_cell_guid": "3ca47cad-9e26-4dc2-ad29-f9c32184a04d"}, "execution_count": null, "cell_type": "markdown"}, {"source": "fraud_count = len(train[train.Class == 1])\n\nfraud_indices = train[train.Class == 1].index\nnormal_indices = train[train.Class == 0].index\n\nr_normal_indices = np.random.choice(normal_indices, fraud_count, replace = False)\n\nundersample_indices = np.concatenate([fraud_indices,r_normal_indices])\nundersample_train = train.iloc[undersample_indices,:]\n\nX_undersample = undersample_train.drop('Class',axis=1)\ny_undersample = undersample_train['Class']\n\nX_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X_undersample,y_undersample,test_size = 0.3,random_state = 0)", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "c2df887cfd8bf749e14b8abb69655453aac89108", "_cell_guid": "805534f9-7c7b-4158-9fa0-4b0309fbb3f4", "collapsed": true}, "execution_count": null, "cell_type": "code"}, {"source": "class_set = [0, 1]\nlr_und = LogisticRegression()\nlr_und.fit(X_train_u, y_train_u.values.ravel())\ny_pred_u = lr_und.predict(X_test_u)\ncnf_matrix_und = confusion_matrix(y_true=y_test_u, y_pred=y_pred_u)\nnp.set_printoptions(precision=2)\nprint (\"Confusion matrix undersampled\")\nplt.rcParams['figure.figsize']=(4,4)\nplot_confusion_matrix(cm=cnf_matrix_und, classes=class_set)\nplt.show()\nprint('cr:', classification_report(y_test_u,y_pred_u))", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "624c6e945425b46b200a4aa5711e3c8083c00ed6", "_cell_guid": "74324713-80db-49c9-b474-775361432ee3"}, "execution_count": null, "cell_type": "code"}, {"source": "y_predprob_u = lr.predict_proba(X_test_u)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test_u,y_predprob_u)\nroc_auc = auc(fpr,tpr)\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC curve for fraud classifier')\nplt.grid(True)\nplt.show()\nroc_auc_score(y_test_u, y_predprob_u)", "outputs": [], "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "1d1702fa6e8b14a97a5777e14f5a46653628c292", "_cell_guid": "f45305b0-9202-4453-a5f9-6e4caf825dbb"}, "execution_count": null, "cell_type": "code"}, {"cell_type": "markdown", "outputs": [], "metadata": {"_uuid": "30d68e2a5387024845e60480baf1dd8a8db4db92", "_cell_guid": "743efca1-50e0-4621-8925-5c5700504df0"}, "source": "We can see the difference between un-sampled and under sampled data. The precision rises from **0.88 to 0.94** which clearly demonstrates importance of under sampling the data when there is a class imbalance! We can improve our model even further by using a lot of further more techniques such as using cross validation, threshold calculation etc.", "execution_count": null}]}