{"cells":[{"metadata":{"_uuid":"35f9d7c8a019cb2bdad4dffd8ebdc33d54847d6d"},"cell_type":"markdown","source":"Thanks PetFinder for this great competition! \n\nUPDATE 3: feature selection in progress\n\nUPDATE 2: adding sentiment data is in progress.\n\nUPDATE: NLP features based on the description were included but subsequently removed because the n-gram features of the description actually reduced the LB score. It sees the descriptions in the test set are quite different from the descriptions in the training data, or I have a bug in the code.\n\nI put together a machine learning pipeline based on train.csv (tabular data and the description). My focus is on extracting actionable insights from the model, so I use XGBoost with SHAP values to aid interpretability. If you are not familiar with SHAP values, I highly recommend you read [this paper](https://arxiv.org/abs/1802.03888v2). SHAP values provide global and local additive feature importances, and they are excellent at explaining tree-based methods with a negligible cost in computational time. \n\nThe model gives an LB score of 0.33, and it can most accurately predict which animals will not be adopted within 100 days with an accuracy of 68%. I take a closer look at this category to derive actionable insights based on extrinsic factors.\n\nThings that could reduce the time until adoption are (in order of importance):\n1. Only one animal per profile. The likelihood of not getting adopted within 100 days positively correlates with the number of pets represented in the profile.\n2. Determine the sterilization status. The likelihood of not getting adopted within 100 days is high if an animal has an unknown sterilization status.\n3. Add photos. The likelihood of not getting adopted within 100 days is high if there are no photos of the animal.\n4. Add at least a short description. No description increases the likelihood of no adoption within 100 days.\n5. Lower fees. There is a weak positive correlation between the fee and the likelihood of no adoption within 100 days.\n\nI illustrate at the end of the kernel how SHAP values can be used to assess how a change in the animal's profile will likely influence the adoption time.\n\nLet's walk through the kernel to see how I arrived to these conclusions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\nimport xgboost\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.cluster import hierarchy\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport matplotlib.pyplot as plt\nimport itertools\nimport json\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n\nplt.rcParams.update({'font.size': 14})\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b86c0c2d1400db69952ca36a8375b7ecaf364b33"},"cell_type":"code","source":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef conf_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y_pred,y):\n    \"\"\"\n    !!!modified to be an XGBoost custom metric!!!\n    \n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    # convert predicted probabilities to hard predictions\n    rater_a = np.argmax(y_pred,axis=1)\n    # convert from XGB's DMatrix\n    rater_b = y.get_label()\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = conf_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n    return 'qwk', -1e0*(1.0 - numerator / denominator)\n\ndef qwk(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = conf_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# an animal class to collect and organize the data. It takes the animals ID as input and\n# whether the animal is in train or test. It returns labels (.get_label() method) and \n# features (.tabular_features() method).\n\ndf_breed = pd.read_csv('../input/breed_labels.csv')\nbreed_names = list(df_breed['BreedName'])\nbreed_IDs = list(df_breed['BreedID'])\n\ndf_color = pd.read_csv('../input/color_labels.csv')\ncolor_names = list(df_color['ColorName'])\ncolor_IDs = list(df_color['ColorID'])\n\ndf_state = pd.read_csv('../input/state_labels.csv')\nstate_names = list(df_state['StateName'])\nstate_IDs = list(df_state['StateID'])\n\nclass animal(object):\n    \n    def __init__(self,ID,in_train=True):\n        self.ID = ID\n        self.in_train = in_train\n    \n    def get_label(self):\n        if self.in_train:\n            data = self.collect_data()\n            return data['AdoptionSpeed'].values[0]\n        else:\n            print('test data without labels')\n            raise ValueError\n            \n    def collect_data(self):\n        if self.in_train:\n            data = df_train.loc[df_train['PetID'] == self.ID]\n        else:\n            data = df_test.loc[df_test['PetID'] == self.ID]\n        return data\n    \n    def tabular_features(self):\n        # collect features\n        data = self.collect_data()\n        \n        features = []\n        feature_names = []\n        \n        # features that either have only two categories (plus missing) or \n        # the values are ranked, so it's ok not to one-hot encode\n        features.append(data[\"Type\"].values[0])\n        feature_names.append('Type')\n\n        if pd.isnull(data['Name'].values[0]):\n            features.append(0)\n        else:\n            features.append(1)\n        feature_names.append('Has name')\n        \n        features.append(data['Age'].values[0])\n        feature_names.append('Age')\n        \n        if data['MaturitySize'].values[0] == 0:\n            features.append(np.nan)\n        else:\n            features.append(data['MaturitySize'].values[0])\n        feature_names.append('MaturitySize')\n\n        if data['FurLength'].values[0] == 0:\n            features.append(np.nan)\n        else: \n            features.append(data['FurLength'].values[0])\n        feature_names.append('FurLength')\n        \n        if data['Vaccinated'].values[0] == 3:\n            features.append(np.nan)\n        else: \n            features.append(data['Vaccinated'].values[0])\n        feature_names.append('Vaccinated')\n        \n        if data['Dewormed'].values[0] == 3:\n            features.append(np.nan)\n        else: \n            features.append(data['Dewormed'].values[0])\n        feature_names.append('Dewormed')\n        \n        if data['Sterilized'].values[0] == 3:\n            features.append(np.nan)\n        else: \n            features.append(data['Sterilized'].values[0])\n        feature_names.append('Sterilized')\n\n        if data['Health'].values[0] == 0:\n            features.append(np.nan)\n        else: \n            features.append(data['Health'].values[0])\n        feature_names.append('Health')\n        \n        features.append(data['Quantity'].values[0])\n        feature_names.append('Quantity')\n        \n        features.append(data['Fee'].values[0])\n        feature_names.append('Fee')\n        \n        features.append(data['VideoAmt'].values[0])\n        feature_names.append('VideoAmt')\n        \n        features.append(data['PhotoAmt'].values[0])\n        feature_names.append('PhotoAmt')\n        \n        # features that need to be one-hot encoded\n        # gender, breed, color, state\n        if data['Gender'].values[0] == 1:\n            features.append(1)\n        else:\n            features.append(0)\n        feature_names.append('Male')\n        if data['Gender'].values[0] == 2:\n            features.append(1)\n        else:\n            features.append(0)\n        feature_names.append('Female')\n        if data['Gender'].values[0] == 3:\n            features.append(1)\n        else:\n            features.append(0)\n        feature_names.append('Multiple')\n        \n        breed = np.zeros(len(breed_IDs))\n        if data['Breed1'].values[0] > 0:\n            breed[breed_IDs.index(data['Breed1'].values[0])] = 1\n        if data['Breed2'].values[0] > 0:\n            breed[breed_IDs.index(data['Breed2'].values[0])] = 1\n        features.extend(breed)\n        feature_names.extend(breed_names)\n        \n        color = np.zeros(len(color_IDs))\n        if data['Color1'].values[0] > 0:\n            color[color_IDs.index(data['Color1'].values[0])] = 1\n        if data['Color2'].values[0] > 0:\n            color[color_IDs.index(data['Color2'].values[0])] = 1\n        if data['Color3'].values[0] > 0:\n            color[color_IDs.index(data['Color3'].values[0])] = 1\n        features.extend(color)\n        feature_names.extend(color_names)\n        \n        state = np.zeros(len(state_IDs))\n        state[state_IDs.index(data['State'].values[0])] = 1\n        features.extend(state)\n        feature_names.extend(state_names)\n        \n        # features related to the description\n        desc = self.description()\n        \n        if desc == 'no_description':\n            features.append(0)\n        else:\n            features.append(len(desc))\n        feature_names.append('NrCharsInDesc')\n\n        if desc == 'no_description':\n            features.append(0)\n        else:\n            features.append(len(desc.split()))\n        feature_names.append('NrWordsInDesc')\n        \n        # add sentiment features\n        sent_features, sent_ftr_names = self.sentiment()\n        features.extend(sent_features)\n        feature_names.extend(sent_ftr_names)\n        \n        if len(features) != len(feature_names):\n            print('the number of features must be the same as the number of feature names')\n            raise ValueError\n        \n        return features,feature_names\n    \n    def description(self):\n        data = self.collect_data()\n        if isinstance(data['Description'].values[0], str):\n            return data['Description'].values[0]\n        else:\n            return 'no_description'\n    \n    def sentiment(self):\n        try:\n            if self.in_train:\n                with open('../input/train_sentiment/'+str(self.ID)+'.json') as f:\n                    data = json.load(f)\n            else:\n                with open('../input/test_sentiment/'+str(self.ID)+'.json') as f:\n                    data = json.load(f)\n            sentence_level = []\n            for i in data['sentences']:\n                sentence_level.append([i['sentiment']['score'],i['sentiment']['magnitude']])\n            sentence_level = np.array(sentence_level).T\n            features = []\n            feature_names = []\n            features.append(data['documentSentiment']['score'])\n            feature_names.append('overall score')\n            features.append(data['documentSentiment']['magnitude'])\n            feature_names.append('overall magnitude')\n            features.append(np.min(sentence_level[0]))\n            feature_names.append('min sentence score')\n            features.append(np.max(sentence_level[0]))\n            feature_names.append('max sentence score')\n            features.append(np.std(sentence_level[0]))\n            feature_names.append('stdev sentence score')\n            features.append(np.min(sentence_level[1]))\n            feature_names.append('min sentence magnitude')\n            features.append(np.max(sentence_level[1]))\n            feature_names.append('max sentence magnitude')\n            features.append(np.std(sentence_level[1]))\n            feature_names.append('stdev sentence magnitude')\n        except FileNotFoundError:\n            features = []\n            feature_names = []\n            features.append(-1)\n            feature_names.append('overall score')\n            features.append(-1)\n            feature_names.append('overall magnitude')\n            features.append(-1)\n            feature_names.append('min sentence score')\n            features.append(-1)\n            feature_names.append('max sentence score')\n            features.append(-1)\n            feature_names.append('stdev sentence score')\n            features.append(-1)\n            feature_names.append('min sentence magnitude')\n            features.append(-1)\n            feature_names.append('max sentence magnitude')\n            features.append(-1)\n            feature_names.append('stdev sentence magnitude')\n                    \n        return features,feature_names\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"105f3e83bdd7eeba5e3271f4e973a475f5b60fc9"},"cell_type":"code","source":"# collect features and labels\ndf_train = pd.read_csv('../input/train/train.csv')\ndf_test = pd.read_csv('../input/test/test.csv')\ntrain_IDs = list(df_train['PetID'])\ntest_IDs = list(df_test['PetID'])\n\n__, feature_names = animal(train_IDs[0]).tabular_features()\nfeature_names_tab = np.array(feature_names)\n\nclass_names = ['same day','1st week','1st month','2nd and 3rd m.','>100 days']\n\nY = np.zeros(len(train_IDs))\nX_tab = np.zeros([len(train_IDs),len(feature_names_tab)])\n# collect training data of tabular features\nfor i in range(len(train_IDs)):\n    ftrs, __ = animal(train_IDs[i]).tabular_features()\n    X_tab[i] = ftrs\n    Y[i] = animal(train_IDs[i]).get_label()\nprint(np.shape(X_tab))\nprint(np.unique(Y,return_counts=True))\n\n# collect test data\nX_test_tab = np.zeros([len(test_IDs),len(feature_names_tab)])\nfor i in range(len(test_IDs)):\n    ftrs, __ = animal(test_IDs[i],in_train=False).tabular_features()\n    X_test_tab[i] = ftrs\n\nprint(np.shape(X_test_tab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315eb492385984b0c40b6509baa0510ccfae4589"},"cell_type":"code","source":"# n-gram features actually reduced the LB score from 0.33 to 0.28-0.3 I'm not sure why.\n# If you spot a bug in the cell, please let me know.\n\n# # collect the descriptions, n-grams will be calculated in the ML pipeline\n# def stemming_tokenizer(str_input):\n#     words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n#     words = [PorterStemmer().stem(word) for word in words]\n#     return words\n\n# corpus_train = []\n# for i in range(len(train_IDs)):\n#     corpus_train.append(animal(train_IDs[i]).description())\n# corpus_train = np.array(corpus_train)\n# print(len(corpus_train))\n\n# corpus_test = []\n# for i in range(len(test_IDs)):\n#     corpus_test.append(animal(test_IDs[i],in_train=False).description())\n# corpus_test = np.array(corpus_test)\n# print(len(corpus_test))\n\n# # 1- to 3-grams are collected and only those ngrams are kept that appear in at least 100 documents\n# vectorizer = CountVectorizer(ngram_range=(1,3),min_df=500,tokenizer=stemming_tokenizer)\n# X_text = vectorizer.fit_transform(corpus_train)\n# feature_names_text = vectorizer.get_feature_names()\n# X_test_text = vectorizer.transform(corpus_test)\n\n# print(np.shape(X_text))\n# print(np.shape(X_test_text))\n# print(len(feature_names_text))\n# print(feature_names_text)\n# # concatenate the arrays\n# X = np.concatenate((X_tab,X_text.toarray()),axis=1)\n# X_test = np.concatenate((X_test_tab,X_test_text.toarray()),axis=1)\n# feature_names = np.concatenate((feature_names_tab,feature_names_text))\n\nX = np.copy(X_tab)\nX_test = np.copy(X_test_tab)\nfeature_names = np.copy(feature_names_tab)\n\nprint(np.shape(X))\nprint(np.shape(X_test))\nprint(len(feature_names))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad619bd2796a387c6cceec088d627107e7123f1d"},"cell_type":"code","source":"# compare the train and test distributions\n# do some feature selection and remove uninformative features\n# if you want info and plots of each figure printed, set detailed_output to True \ndetailed_output = False\nto_keep = []\nfor i in range(len(feature_names)):\n    if len(np.unique(X[:,i])) < 3:\n        # categorical features, no figure\n        if detailed_output:\n            print(feature_names[i])\n        values_train, dist_train = np.unique(X[:,i],return_counts=True)\n        values_test, dist_test = np.unique(X_test[:,i],return_counts=True)\n        # normalize the counts\n        dist_train = 1e0*dist_train/np.sum(dist_train)\n        dist_test = 1e0*dist_test/np.sum(dist_test)\n        unique_values = np.unique(np.concatenate((values_train,values_test)))\n        for v in unique_values:\n            indx1 = np.where(values_train == v)[0]\n            indx2 = np.where(values_test == v)[0]\n            if detailed_output:\n                if (len(indx1) == 1) and (len(indx2) == 1):\n                    print('   ',v,np.around(dist_train[indx1[0]],4),np.around(dist_test[indx2[0]],4))\n                else:\n                    if len(indx1) == 0:\n                        print('   ',v,0e0,np.around(dist_test[indx2[0]],4))                    \n                    elif len(indx2) == 0:\n                        print('   ',v,np.around(dist_train[indx1[0]],4),0e0)\n        if (np.min(np.concatenate((dist_train,dist_test))) > 0.01)&\\\n           (np.min(np.concatenate((dist_train,dist_test))) < 1e0):\n            to_keep.append(i)\n    else:\n        to_keep.append(i)\n        if detailed_output:\n            # categorical features with more than two categories and continuous features\n            # prepare a figure to compare distributions\n            if len(np.unique(X[:,i])) > 5:\n                bins = 100\n            else:\n                bins = np.arange(np.min(np.unique(X[:,i])),len(np.unique(X[:,i]))+1)-0.5\n            n, bins, patches = plt.hist(X[:,i],bins = bins,alpha=0.5,color='b',density=True,label='train')\n            plt.hist(X_test[:,i],bins = bins,alpha=0.5,color='r',density=True,label='test')\n            if feature_names[i] == 'Fee':\n                plt.xlim([0,500])\n            if feature_names[i] == 'Age':\n                plt.xlim([0,50])\n            plt.xlabel('feature value')\n            plt.ylabel('pdf')\n            plt.legend()\n            plt.title(feature_names[i])\n            plt.show()\n\n        \nX = X[:,to_keep]\nX_test = X_test[:,to_keep]\nfeature_names = feature_names[to_keep]\nprint(np.shape(X))\nprint(np.shape(X_test))\nprint(len(feature_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6871e7ffb63c81c54d0725e11f4860b047b9b91e"},"cell_type":"code","source":"# the ML pipeline\n\n# number of runs to average. a higher number reduces the internal randomness of the model\nnr_runs = 3\n# number of folds\nnr_folds = 5\n\ndef train_pred(X_tab,Y,X_test_tab):\n    rskf = RepeatedStratifiedKFold(n_splits=nr_folds, n_repeats=nr_runs,\n        random_state=0)\n    SHAP = np.zeros([len(Y),5,len(feature_names)+1])\n    SHAP_test = np.zeros([np.shape(X_test)[0],5,len(feature_names)+1])\n    # the SHAP interaction features give a memory error in the kernel :(\n    #SHAP_int = np.zeros([len(Y),5,len(feature_names)+1,len(feature_names)+1])\n    Y_test = np.zeros([np.shape(X_test_tab)[0],5])\n    Y_CV_pred = np.zeros([np.shape(X_tab)[0],5])\n    models = []\n    i = 0\n    for train_index, CV_index in rskf.split(X_tab, Y):\n        print(i)\n        xgb = xgboost.XGBClassifier(learning_rate=0.1,subsample=0.66, colsample_bytree=0.9,\\\n                                    random_state=0,objective='multi:softprob',num_class=5,\\\n                                    n_estimators=10000)\n        X_train, X_CV = X[train_index], X[CV_index]\n        Y_train, Y_CV = Y[train_index], Y[CV_index]\n        eval_set = [(X_CV, Y_CV)]\n        xgb.fit(X_train,Y_train,early_stopping_rounds=50,eval_set = eval_set,\\\n                verbose=False,eval_metric=quadratic_weighted_kappa)\n        # collect shap values and predictions for the CV set\n        Y_CV_pred[CV_index] = Y_CV_pred[CV_index] + xgb.predict_proba(X_CV,ntree_limit=xgb.best_ntree_limit)\n        SHAP[CV_index] = SHAP[CV_index] + xgb.get_booster().predict(xgboost.DMatrix(X_CV),pred_contribs=True,ntree_limit=xgb.best_ntree_limit)\n        #SHAP_int[CV_index] = SHAP_int[CV_index] + xgb.get_booster().predict(xgboost.DMatrix(X_CV),pred_interactions=True,ntree_limit=xgb.best_ntree_limit)\n        # predict test\n        Y_test = Y_test + xgb.predict_proba(X_test,ntree_limit=xgb.best_ntree_limit)\n        SHAP_test = SHAP_test + xgb.get_booster().predict(xgboost.DMatrix(X_test),pred_contribs=True,ntree_limit=xgb.best_ntree_limit)\n        # save the model\n        models.append(xgb)\n        i = i + 1\n        \n    Y_test = Y_test / (nr_runs*nr_folds)\n    SHAP_test = SHAP_test / (nr_runs*nr_folds)\n    Y_CV_pred = Y_CV_pred / nr_runs\n    SHAP = SHAP / (nr_runs)\n\n    return Y_test,SHAP_test,Y_CV_pred,SHAP,models\n    \n    \nY_test, SHAP_test, Y_CV_pred, SHAP, models = train_pred(X_tab,Y,X_test_tab)    \n\nY_CV = np.argmax(Y_CV_pred,axis=1)\nprint('local CV score: ',qwk(Y,Y_CV))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f944a85c3324483223df98b2b13fba7869bd1032"},"cell_type":"code","source":"# prepare the submission file\nsubmission = pd.DataFrame({'PetID': test_IDs, 'AdoptionSpeed': np.argmax(Y_test,axis=1)})\nprint(submission.head())\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b939049393a0f539c4ccd2b00eb5e00f215493"},"cell_type":"code","source":"# confusion matrix\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n        #print('Confusion matrix, without normalization')\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap,vmin=0.0)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\nY_CV = np.argmax(Y_CV_pred,axis=1)\n    \n# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y, Y_CV)\nnp.set_printoptions(precision=2)\n\n# Plot confusion matrix\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False,\n                      title='Confusion matrix')\nplt.show()\n\n# Plot normalized confusion matrix\nplt.figure(figsize=(8,6))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalize confusion matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ec8969f0d43ce79761673f5b811fd8b78cd2298"},"cell_type":"markdown","source":"The model fails to predict same day adoptions and it also predicts 2nd and 3rd months adoptions poorly. No adoptions within 100 days are easiest to predict with an accuracy of 68% although there are a large number of false positives in this class. Roughly 3000 animals are predicted to be in '>100 days' eventhough they were adopted sooner."},{"metadata":{"trusted":true,"_uuid":"304986b9f1d7c9f9031aaac4610f01aa7749787c"},"cell_type":"code","source":"# study the SHAP values and extract actionable insights\ndef plot_SHAP(indcs = 'all',title=''):\n    if indcs == 'all':\n        indcs = np.arange(len(Y))\n        \n    # global feature importances\n    sorted_indcs = np.argsort(np.mean(np.abs(SHAP_final[indcs,:-1]),axis=0))[::-1]\n\n    print(feature_names[sorted_indcs[:10]])\n    \n    plt.figure(figsize=(8,6))\n\n    for i in range(10):\n        plt.scatter(SHAP_final[indcs,sorted_indcs[i]],np.zeros(len(Y[indcs]))+(10-1-i)+np.random.normal(0e0,0.08,size=len(Y[indcs])),\\\n                    s=10,c=np.ravel(X[indcs,sorted_indcs[i]]),cmap='plasma')\n    plt.axvline(0e0,linestyle='dotted',color='k')\n    plt.yticks(range(10)[::-1],feature_names[sorted_indcs[:10]])\n    plt.xlabel(\"feature's contribution\")\n    plt.colorbar(ticks=[],label='feature value (from low to high)')\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n    \n    return\n\ndef plot_ftr_vs_shap(ftr_name,class_indx='all'):\n    indx_ftr = np.where(feature_names == ftr_name)[0][0]\n    if class_indx=='all':\n        indcs = np.arange(len(Y_CV))\n    else:\n        indcs = np.where(Y_CV == class_indx)[0]\n    plt.scatter(np.ravel(X[indcs,indx_ftr]),SHAP_final[indcs,indx_ftr])\n    plt.xlabel(ftr_name)\n    plt.ylabel('SHAP value')\n    plt.show()\n    return\n\n# collect the shap values of the most likely class\nSHAP_final = np.zeros([len(Y),len(feature_names)+1])\nfor i in range(len(Y)):\n    SHAP_final[i] = SHAP[i,Y_CV[i]]\n\nplot_SHAP(title='all')\n\nplot_SHAP(indcs = np.where(Y_CV == 4)[0],title='predicted class: '+class_names[4])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d7110b4682ad92423acf2eaaa4496270fc84b92"},"cell_type":"markdown","source":"The SHAP summary plot for animals predicted to be in '>100 days'. If you are not familiar with this type of figure, please check out [this paper](https://arxiv.org/abs/1802.03888v2) especially Fig. 8. The y axis lists the top 10 most predictive features, the x axis shows how much a feature contributes to the prediction (the animal will likely be in '>100 days' for positive values). One animal in the CV set gets one point in every row and the color of the point illustrates whether the feature value is low or high (see colorbar on the right side).\n\nOld, mixed breed animals with an unknown sterilized status are likely not adopted within 100 days. The state of Selangor shows up for some reason I can't explain. Important extrinsic features are the 'Quantity' - number of animals represented in the profile, 'Sterilized' - whether the animal is sterilized, 'PhotoAmt' - number of photos uploaded, 'NrCharsInDesc' - number of characters in the description, and 'Fee' - the adoption fee (I assume in $). Let's take a closer look at these four."},{"metadata":{"trusted":true,"_uuid":"61df0d8b9c01309741cc893a4d46eecd748a06d3"},"cell_type":"code","source":"# closer look at quantity, photoamt, and fee vs. shap value\nplot_ftr_vs_shap('Quantity',4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34f9a4a69448083700d2390a3cc007ffea2f301c"},"cell_type":"markdown","source":"The more animals are represented in a profile, the more likely they will not be adopted within 100 days."},{"metadata":{"trusted":true,"_uuid":"2e6afa13e94d2497dda59012ec95ab396d843fde"},"cell_type":"code","source":"plot_ftr_vs_shap('Sterilized',4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8d1abe9f77d221de9de45862f74eee1576aba35"},"cell_type":"markdown","source":"1 - sterilized, 2 - not sterilized, 3 - unknown\n\nThis figure is counterintuitive. The SHAP value is large if the sterilization status is unknown, which makes sense. However, the SHAP value is positive if the animal is sterilized meaning the feature value of 1 contributes positively to the probability of being in the '>100 days' class. I'd expect that a sterilized animal is adopted sooner than an unsterilized animal, but that's not what the figure shows."},{"metadata":{"trusted":true,"_uuid":"f43772f832201e8d31655145b9c7dcbf85eb85f5"},"cell_type":"code","source":"plot_ftr_vs_shap('PhotoAmt',4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ef21cd09fbb6f4282c3323bd78227dda4eb2e06"},"cell_type":"markdown","source":"If an animal has no photo associated with it, the model predicts no adoption within 100 days. "},{"metadata":{"trusted":true,"_uuid":"8da9dee0ae5730facb0e80d845bf2c932b4b70fd"},"cell_type":"code","source":"plot_ftr_vs_shap('NrCharsInDesc',4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afcc6c07e07bec7d16bc55b33c75e3ef0e394a0f"},"cell_type":"markdown","source":"A short or no description seems weakly correlated with with a long adoption time."},{"metadata":{"trusted":true,"_uuid":"089fd3550028c41a1e2abfcf30aa80ebf1b3e078"},"cell_type":"code","source":"plot_ftr_vs_shap('Fee',4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c28acefcb59086378a3abb8768e6ee6f370ecd30"},"cell_type":"markdown","source":"If the fee is larger than $100, adoption might not happen within 100 days. Please note that the SHAP value on the y axis only goes up to 0.5, but it is around or above 1 for 'Quantity' and 'PhotoAmt'."},{"metadata":{"_uuid":"63790eaca12f9da06f401b6dd27c3fba18e45ee1"},"cell_type":"markdown","source":"So far I used SHAP values to assess global feature importance, but SHAP values can also be used to calculate local feature importances. Given an animal's profile, we can 1) figure out which features drive the prediction 2) change extrinsic properties and measure how the change influences the adoption time.\n\nLet's take a look at the animal with the highest predicted probability in the '>100 days' class in the test set, and check what happens if we change its profile."},{"metadata":{"trusted":true,"_uuid":"21a5dae3433a374a2d7085bbae711691beb8f5c9"},"cell_type":"code","source":"indx = np.argmax(Y_test[:,4])\nprint('animal ID:',test_IDs[indx])\nprint('class probabilities: ',Y_test[indx])\nftrs, __ = animal(test_IDs[indx],in_train=False).tabular_features()\nftrs = np.array(ftrs)\nsorted_ftrs = np.argsort(np.abs(SHAP_test[indx,4,:-1]))[::-1]\nfor f in sorted_ftrs[:5]:\n    print(feature_names[f],':',ftrs[f], \", feature's contribution to the prediction: \",np.around(SHAP_test[indx,4,f],2))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dfd139acb0714da63341e640f625ad55ff4dd0d"},"cell_type":"markdown","source":"The model predicts that this animal will not be adopted within 100 days with an 86% probability. The reason is that the animal has no photo on their profile, they are 2 years old, mixed breed, the description is only 30 characters long, and the sterilization status is unknown. I consider the age, breed, and the location to be intrinsic properties. \n\nLet's check what happens if we add photos to the profile and if we change the sterilization status of the animal."},{"metadata":{"trusted":true,"_uuid":"1d257f688daaecb9a207ac7f59a04fa244b187bb"},"cell_type":"code","source":"ftrs_mod = np.copy(ftrs)\nprob_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[0]] = i\n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_class4[i] = probs[4]\nprint(probs)\n\nprob_desc_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[0]] = i\n    ftrs_mod[sorted_ftrs[3]] = 1000\n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_desc_class4[i] = probs[4]\nprint(probs)\n\nplt.plot(range(0,31),prob_class4,label='short description')\nplt.plot(range(0,31),prob_desc_class4,label='long description')\nplt.xlabel('PhotoAmt')\nplt.ylabel(\"probability, '>100 days'\")\nplt.title('Sterilization status unknown')\nplt.legend()\nplt.ylim([0.25,0.85])\nplt.grid()\nplt.tight_layout()\nplt.show()\n\nftrs_mod = np.copy(ftrs)\nprob_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[4]] = 1    \n    ftrs_mod[sorted_ftrs[0]] = i\n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_class4[i] = probs[4]\nprint(probs)\nprob_desc_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[4]] = 1    \n    ftrs_mod[sorted_ftrs[0]] = i\n    ftrs_mod[sorted_ftrs[3]] = 1000\n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_desc_class4[i] = probs[4]\nprint(probs)\n\nplt.plot(range(0,31),prob_class4,label='short description')\nplt.plot(range(0,31),prob_desc_class4,label='long description')\nplt.xlabel('PhotoAmt')\nplt.ylabel(\"probability, '>100 days'\")\nplt.title('Sterilized')\nplt.legend()\nplt.ylim([0.25,0.85])\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n\nftrs_mod = np.copy(ftrs)\nprob_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[0]] = i\n    ftrs_mod[sorted_ftrs[4]] = 2    \n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_class4[i] = probs[4]\nprint(probs)\nprob_desc_class4 = np.zeros(31)\nfor i in range(0,31):\n    ftrs_mod[sorted_ftrs[4]] = 2    \n    ftrs_mod[sorted_ftrs[0]] = i\n    ftrs_mod[sorted_ftrs[3]] = 1000\n    probs = np.zeros(5)\n    for xgb in models:\n        probs = probs + xgb.predict_proba(ftrs_mod[np.newaxis,:],validate_features=False)[0]\n    probs = probs / (nr_runs*nr_folds)\n    prob_desc_class4[i] = probs[4]\nprint(probs)\n\nplt.plot(range(0,31),prob_class4,label='short description')\nplt.plot(range(0,31),prob_desc_class4,label='long description')\nplt.xlabel('PhotoAmt')\nplt.ylabel(\"probability, '>100 days'\")\nplt.title('Not sterilized')\nplt.legend()\nplt.ylim([0.25,0.85])\nplt.grid()\nplt.tight_layout()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d94cc98bd11f92c1acfe6320db597087f5aae81b"},"cell_type":"markdown","source":"Adding ~10 photos can reduce the probability of no adoption within 100 days from 86% to ~55% with a short description and to ~40% with a long description (1000 characters). If we knew that the animal is not sterilized, the probability can be pushed down to 30% with a long description, and class '>100 days' is not the most likely class anymore. It's still suspicious though why the model predicts lower probabilities to not sterilized animals."},{"metadata":{"_uuid":"8fac1680111a771f5564ed6df3b237f35529007b"},"cell_type":"markdown","source":"This is it so far. I'll check out the descriptions and the images later on. Additional (interpretable) features will hopefully improve the predictive power of the model and it will allow us to extract more actionable insights.\n\nFinally, here is a pic of my rescue pup, Waiola. :) She was six years old when I adopted her and she spent more than 100 days in the shelter and at fosters. I hope we can find homes to many animals in need as a result of this competition.\n\n![Waiola](https://www.dropbox.com/s/e7kg8qb2zup99yn/waiola.png?dl=0)\n"},{"metadata":{"trusted":true,"_uuid":"c0d110467adc42dff262e86fcdd38a2f91036534"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}