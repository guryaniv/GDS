{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Dropout, Conv1D, Flatten, MaxPooling1D, Concatenate, BatchNormalization, GlobalAveragePooling1D, LeakyReLU\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom sklearn.model_selection import KFold \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import Callback\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE\nimport lightgbm as lgb\nimport math\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport pandas as pd \nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport matplotlib\nfrom sklearn.naive_bayes import GaussianNB\nnp.random.seed(203)\nimport math\nfrom scipy.stats import ks_2samp, normaltest\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Since there's already so many kernels that try out different parameters in lgbm models, I thougth I'd try out a different type of model and see how high an AUCROC I could get. It turns out (for me atleast) Convolutional 1 Dimensional Networks achieve a higher accuracy than standard Neural Nets. "},{"metadata":{"trusted":true,"_uuid":"b4980f87bbe47e4aa6f9ef44e85a9753c6833952"},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e5ad0a7f436197ca1f3f96f73aa3d6314c58fca"},"cell_type":"markdown","source":"We need to scale and reshape the data fot the Neural Network."},{"metadata":{"trusted":true,"_uuid":"f8f23e216c622768b3a149e58a38dfe21cc662b2"},"cell_type":"code","source":"scaler = StandardScaler()\nX = df_train.drop([\"target\", \"ID_code\"], axis=1).values\nX = scaler.fit_transform(X)\nX = X.reshape(200000, 200, 1)\ntest = df_test.drop([\"ID_code\"], axis=1).values\ntest = scaler.transform(test)\ntest = test.reshape(200000, 200, 1)\ny = df_train[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46e0bf232ef5b5cc1e95c96edeaff723204921f4"},"cell_type":"markdown","source":"The following code is necessary for viewing AUCROC during training (Credits to: Tom on https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras)"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"f0fdd7fa6a092c2d0b3096ba579cb1613bf2cb05"},"cell_type":"code","source":"class roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc933d735fa1ccdbee2f8163dd3b055f910adc09"},"cell_type":"markdown","source":"It turns out that model performs a lot better with BatchNormalization and High Dropout. "},{"metadata":{"trusted":true,"_uuid":"4c6feba91f98740f9caffe9db9a0ed8293a20af9"},"cell_type":"code","source":"def init_model():\n\n    input1 = Input(shape = (200, 1))\n    a = Conv1D(22, 2, activation=\"relu\", kernel_initializer=\"uniform\")(input1)\n    a = BatchNormalization()(a)\n    a = Flatten()(a)\n    a = Dropout(0.6)(a)\n    a = Dense(50, activation = \"relu\", kernel_initializer=\"uniform\")(a)\n    a = Dropout(0.6)(a)\n    output = Dense(1, activation = \"sigmoid\", kernel_initializer=\"uniform\")(a)\n    model = Model(input1, output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06662797e318bee4961771311472674f09e8c0a0"},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=12, shuffle=True, random_state=4590)\noof = np.zeros(len(df_train))\ntest_predictions = np.zeros(len(df_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    X_train, X_test = X[trn_idx], X[val_idx]\n    y_train, y_test = y[trn_idx], y[val_idx]\n    \n    model = init_model()\n    \n    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=512, shuffle=True, callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_test, y_test))])\n    oof[val_idx] = model.predict(X_test).reshape(X_test.shape[0],)\n    test_predictions+= model.predict(test).reshape(test.shape[0],)/12\nroc_auc_score(y, oof)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8caa465ccc854b0de099bbbef7234d92962590ca"},"cell_type":"markdown","source":"The code can still be optimized by increasing the number of epochs, adding early stopping and saving the models best weights. In this way, the model doesn't end on a bad epoch. \n\nIf you found this code useful, feel free to leave an upvote :)"},{"metadata":{"trusted":true,"_uuid":"b29ebe2dbbbb85a0044150b4f2d35c3f3e9c07fc"},"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"ID_code\"] = df_test[\"ID_code\"]\nsub[\"target\"] = test_predictions\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}