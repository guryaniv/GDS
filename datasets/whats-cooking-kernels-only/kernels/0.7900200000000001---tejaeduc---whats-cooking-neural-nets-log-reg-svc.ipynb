{"cells":[{"metadata":{"_uuid":"f1d24df70cb075d090d717588426748a43a5779c"},"cell_type":"markdown","source":"# What's cooking kernel !"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3c31bb2a8f9c6d21fa68ed051586d2a417ec9183"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"709a3dbef47bd1e5a7a63447172b615eda1c94b0"},"cell_type":"markdown","source":"Load the dataset"},{"metadata":{"trusted":true,"_uuid":"59c34979fe8026b415d6a8f2b4632e5f3e61ca64","collapsed":true},"cell_type":"code","source":"df = pd.read_json(\"../input/train.json\")\ntestset = pd.read_json(\"../input/test.json\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1c259e960d7533217faf0386524782f0810dfb2","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95a160f394c136c3a0cbf0f18e275a978b67501e","collapsed":true},"cell_type":"code","source":"testset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bb4f2dfbd6f296d877025cbfbfc734b719967f5"},"cell_type":"markdown","source":"Check for any null values."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c6015884924464739b58e818bc74002f6344a205"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"60176aee7953845dbcf811132cf87d5e073340ba"},"cell_type":"code","source":"testset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c692900703e5df3a6c0c55ecd086de457cb609ec"},"cell_type":"markdown","source":"Check different types of cuisines"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c44840bf1e5a95cf715a1032736c0768c32bee8a"},"cell_type":"code","source":"df.cuisine.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d565da04a7b3977f2bfc73ae4f1f02bcc01275bd"},"cell_type":"markdown","source":"# Text Data processing"},{"metadata":{"_uuid":"cd42d525137f9f9be57b8b54232750a58f1ac673"},"cell_type":"markdown","source":"Convert the ingredients to string."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e03c52f6d11d31f1915a8418b7c937bdec340d93"},"cell_type":"code","source":"df.ingredients = df.ingredients.astype('str')\ntestset.ingredients = testset.ingredients.astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cd269ed362db4080bee7e45ee7ded2cff8d9611e"},"cell_type":"code","source":"df.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4407743ca6aad81884342538d154b48702a710e9"},"cell_type":"code","source":"testset.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1300219a7827bd0208ed55c7188dca198ad2843"},"cell_type":"markdown","source":"Lets remove those unnecessary symbols, which might be problem when tokenizing and lemmatizing"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9fc0c828240968fea5ade8670d7e2b7c9c7d453b"},"cell_type":"code","source":"df.ingredients = df.ingredients.str.replace(\"[\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"]\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"'\",\" \")\ndf.ingredients = df.ingredients.str.replace(\",\",\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ec2f3efd173648d0ea06b79f34044af30f57be2e"},"cell_type":"code","source":"testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\",\",\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"76fa343016d0ae7d4fd7363530af6f1739b8c4b0"},"cell_type":"code","source":"df.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"883a9351d374a882ccad88defeb07ca109cb2537"},"cell_type":"code","source":"testset.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c25f50a3975f5a242d4297cc48e03ef1d0721e4e"},"cell_type":"markdown","source":"Convert everything to lower ( I think they are already in lower case, but to be on safe side)."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c3becd6f80ab112009694a734851309cfb4e3866"},"cell_type":"code","source":"df.ingredients = df.ingredients.str.lower()\ntestset.ingredients = testset.ingredients.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"076c3aaeb786f931f45305c9505c9e8c49082937"},"cell_type":"markdown","source":"Lets TOKENIZE the data now. (the processing of splitting into individual words)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9582614d00c51eae6fd2baacccfae84a87f9f38b"},"cell_type":"code","source":"df.ingredients = df.ingredients.apply(lambda x: word_tokenize(x))\ntestset.ingredients = testset.ingredients.apply(lambda x: word_tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"913a28d3e27d2f7758d2dae207c1dc17ae7cc43e"},"cell_type":"markdown","source":"Lets LEMMATIZE the data now (Since i believe that dataset might have different representation of same words, like the olives and olive, tomatoes and tomato, which represent the same word)"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"75b7d78a54b5ab1320cf8fb56219f8b3e628ce07"},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"82df6e08b1169a9963447a1f598be589621dd340"},"cell_type":"code","source":"def lemmat(wor):\n    l = []\n    for i in wor:\n        l.append(lemmatizer.lemmatize(i))\n    return l","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6e08c62522e7ce4dc515b9f36541e8ca5fcbd0b5"},"cell_type":"code","source":"df.ingredients = df.ingredients.apply(lemmat)\ntestset.ingredients = testset.ingredients.apply(lemmat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"59b41cdfbb67c772104042df50bf17af36f81a40"},"cell_type":"code","source":"df.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a82107fa9003ac5063618957b8ee8e8e7c7e030f"},"cell_type":"code","source":"testset.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"904f488a24c648c2852feaae21e66b3445a63454"},"cell_type":"markdown","source":"Observe that olives converted to olive, tomatoes to tomato etc, many words are now in their root form."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8285f9193e18d7a1fbeedb4759b95fc62461f523"},"cell_type":"code","source":"type(df.ingredients[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c36f754495d2118bc1706869801164f79ae08a16"},"cell_type":"markdown","source":"Lemmatization converted it back to list, so change to str again and remove the unncessary words."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"eca831818aa81cb782e7e162073eecca44ca3471"},"cell_type":"code","source":"df.ingredients = df.ingredients.astype('str')\ndf.ingredients = df.ingredients.str.replace(\"[\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"]\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"'\",\" \")\ndf.ingredients = df.ingredients.str.replace(\",\",\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b3f7ab7b6c3306b2bb83041248f951cbff7cd9f4"},"cell_type":"code","source":"testset.ingredients = testset.ingredients.astype('str')\ntestset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\",\",\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"75dabff753820dd12706ea923901ca299fbbe15c"},"cell_type":"code","source":"type(df.ingredients[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2b8e97dff9fc2ac52cd21ce879de339f52718c6e"},"cell_type":"code","source":"df.ingredients[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5534b588a7dfe17c08c0ab421be2727ef90cbc39"},"cell_type":"markdown","source":"Now our data looks good for vectorization."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4afeada4047b7fae0d85720c221d1098bc54957a"},"cell_type":"code","source":"#vect = HashingVectorizer ()\nvect = TfidfVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29131ddecf565afbd42537b488208a7fc865c5b2","collapsed":true},"cell_type":"code","source":"features = vect.fit_transform(df.ingredients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7454bee374cbe52091f6a2477cfb894b890859f6"},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57a900ee78184da32d15980e33746ea839443149"},"cell_type":"markdown","source":"So, now our features has 2826 features, which are created by the process of vectorization."},{"metadata":{"_uuid":"11b08afb9a96457745dad33d309183d38d336d1f"},"cell_type":"markdown","source":"Lets visualize some random features."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"11a51b14d92df7a3513b8e062a4b3a7d39e6f8d0"},"cell_type":"code","source":"#vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3418a4277b2800063be8d2b642354cfd4e49be2a"},"cell_type":"markdown","source":"Lets vectorize our testset as well, we only tranform it with already fitted model"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"eb1d5ba7ce0611277bcc72686b27384c8e38c4dd"},"cell_type":"code","source":"testfeatures = vect.transform(testset.ingredients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fd0a7c2b89870e60a38203766f2851968d12ffb1"},"cell_type":"code","source":"testfeatures","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f5c1b1faf885ef9f3a1a1d18fb17cde0d11ef73"},"cell_type":"markdown","source":"Lets create our labels now, which is obviously cuisine column. Lets labelencode it so that they convert to numerical lables, which usually might give better prediction results. Not a necessary step tho"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"43fcd564bc31e3ce8e6db4f0fd71e180dae3eccf"},"cell_type":"code","source":"encoder = LabelEncoder()\nlabels = encoder.fit_transform(df.cuisine)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e7500de35a9826d984699caade51ba0272497cb"},"cell_type":"markdown","source":"Lets split the dataset into training and testing parts"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5ad070284777780c3c099abd9fc8d884f120e31e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0875929cb3eb2f2e5842d85068b2f90db0dc08"},"cell_type":"markdown","source":"Check the shapes, to make sure."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"25a76d577b715de822c7ea6f6f85767e4f12d83c"},"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b90798b99b6a932ea5cbe9e1fc7a740b0a48cc4"},"cell_type":"markdown","source":"# Data Modeling"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8010e4b8fc161096c85a76e6f19cb3a168537aad"},"cell_type":"code","source":"#logreg = LogisticRegression(C=10,solver='lbfgs', multi_class='multinomial',max_iter=400)\n#logreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0ce63a9cb76f4d9c8ce02dca90cd6a75b1786ca7"},"cell_type":"code","source":"#print(\"Logistic Regression accuracy\",logreg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a8e472e9acd36c0d622c337878c3860c8807a03c"},"cell_type":"code","source":"#logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"684c2b225f1c72b7d41c54206a6628b482a2a0eb"},"cell_type":"code","source":"#from sklearn import linear_model\n#sgd = linear_model.SGDClassifier()\n#sgd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5d481e658bafc7b35b61ac3ae82b1a14db0e24a0"},"cell_type":"code","source":"#print(\"SGD classifier accuracy\",sgd.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1eda2df143742d2a0bddbfc75141beada74af266"},"cell_type":"code","source":"#from sklearn.svm import LinearSVC\n#linearsvm = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n#linearsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b6c07905956a662bec9eee2ba92300329272ded6"},"cell_type":"code","source":"#print(\"Linear SVM accuracy\", linearsvm.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a5591fbe2418d3825e8f8a3f5edea465b7cc29d"},"cell_type":"markdown","source":"Now, lets try our luck with neural networks."},{"metadata":{"_uuid":"461bf718c2ed93b842158c8fca006a9b6a7ff9db"},"cell_type":"markdown","source":"# NEURAL NETWORK'S"},{"metadata":{"_uuid":"1d10ee4878832cfcce69a0aa06ef33516132c691"},"cell_type":"markdown","source":"I have tried both Keras and tensorflow (Of course the backend is same), but Keras code looks simpler and clear."},{"metadata":{"_uuid":"f02e48e7c0d52c9965805a350d38eb21857a77fb"},"cell_type":"markdown","source":"For Neural Networks we need to have the dense array's as inputs and preferably one hot encoding for lables.\nSo, lets create lables."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b8d704f7b19bd15eddccf0f5c8ff39f3aa7dd41a"},"cell_type":"code","source":"#labelsNN = df.cuisine","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aed0376b52e735b8229e864566385a0f42a4da96"},"cell_type":"markdown","source":"Convert it to one hot formatting, there are many ways to do, i prefer to do this way."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"af68be8c7d763921ee890c46f02d399503f618c6"},"cell_type":"code","source":"#labelsNN = pd.get_dummies(labelsNN)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57779eb6339eb9219e530c07e5834798bb6a4062"},"cell_type":"markdown","source":"Convert it to arrays, you can do by values method or np.array() both are same"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"34189847fc90d11390943a8d14ad7c54cea28c9f"},"cell_type":"code","source":"#labelsNN = labelsNN.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c7779791b511334f505c1eaf272cbf0af7533df"},"cell_type":"markdown","source":"Here's how the one hot encoding looks like."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bb4570a193df9470dd0635d6112ffb315a1de0ea"},"cell_type":"code","source":"#labelsNN[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7a49c053c856c02712fd50af4e181d69d593191"},"cell_type":"markdown","source":"Our labels are ready, now we need the features, we have already created the features above but it was sparse matrix, which neural network doesnt like, so convert to dense arrays."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"05abb41c285cd361c038ccbfed8e9fa0ecb1c89b"},"cell_type":"code","source":"#from scipy.sparse import csr_matrix\n#sparse_dataset = csr_matrix(features)\n#featuresNN = sparse_dataset.todense()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f0239585d0d629a9d40d800e824038dbf2ab169"},"cell_type":"markdown","source":"Here's how the features look like."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f89b37e373768c106ff1ff397c9d0a33d4ebfee1"},"cell_type":"code","source":"#featuresNN[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8254a47631ecdf9174dfc7d5becc45558503946c"},"cell_type":"markdown","source":"Split the dataset."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1768912f1dfacefc033fb98a7db9e20ffabb41a4"},"cell_type":"code","source":"#X_trainNN, X_testNN, y_trainNN, y_testNN = train_test_split(featuresNN, labelsNN, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ed0ab609cae6a623c12809192c9bad8489d7781f"},"cell_type":"code","source":"#print(X_trainNN.shape, X_testNN.shape, y_trainNN.shape, y_testNN.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55e1813ddb1f374fe13fc8d7f4e5675509d4c8e7"},"cell_type":"code","source":"#numfeat = X_trainNN.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd3c01b2b2e1288a6ff89bee81c6ac3668d8be42"},"cell_type":"markdown","source":"# KERAS"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"faa7d7250cdb206687fdca616977860559c9b6eb"},"cell_type":"code","source":"#import keras\n#from keras.layers import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e0d2920e5693d34ffba506090d0f25175ffe3b"},"cell_type":"markdown","source":"A sequential NN with 300,500 and 400 nodes in first,second and third layers resp."},{"metadata":{"_uuid":"9bb7ac6c7f947518fecc96d33f3836bc2917e599"},"cell_type":"markdown","source":"The loss is categorical cross entropy and the optimizer is adam with default learning rate.\nWe can tweak a lot of parameters like the no of nodes, epochs, batchsize etc to improve accuracy."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8273f42ce28cc6f249636f2d0a59afbf23f3aeb5"},"cell_type":"code","source":"#model = keras.models.Sequential()\n#model.add(Dense(300,input_dim = numfeat,activation = 'relu'))\n#model.add(Dense(500,activation = 'relu'))\n#model.add(Dense(400,activation = 'relu'))\n#model.add(Dense(20,activation='softmax'))\n#model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\n#model.fit(X_trainNN,y_trainNN,epochs=50,shuffle=True, verbose =2,batch_size=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4ea8e72dd5ee7115e7775bf06f38db6c48af7482"},"cell_type":"code","source":"#print(\"Accuracy with KERAS\" ,model.evaluate(X_testNN,y_testNN)[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"006877a5b5e723ca03c7b2e17fe67043dc08ca9f"},"cell_type":"markdown","source":"I have trained with KERAS on my pc for few times and achieved max accuracy of 0.81."},{"metadata":{"_uuid":"753d88a940945cf5ba69e887b7ad297f0782463d"},"cell_type":"markdown","source":"Now, we have achieved almost similar accuracies in all the above models, I dont prefer NN's on this data as it is computationally very expensive."},{"metadata":{"_uuid":"d9dd21e86efd2acb23e95d45985759fb84e5a27d"},"cell_type":"markdown","source":"# PREDICTION"},{"metadata":{"_uuid":"71e090a1f81e95c56c839a8968d38a93bd0f3e5d"},"cell_type":"markdown","source":"I prefer just using the logisticRegression or linearsvm for predictions, but linearSVC also has almost same results.\nI'm not predict using Keras or Tensorflow, since it needs an extra two steps to convert the labels, which I dont want to waste my time on."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9d36b42ca535444d72b2b5efa216a517ec9d57f0"},"cell_type":"code","source":"#linearsvmfinal = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n#linearsvmfinal.fit(features,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c385836c610ffae4a75452088038fcf8410be9a4"},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ed76c8849a42195bcdbd1119138d962033af59b9"},"cell_type":"code","source":"gbm = lgb.LGBMClassifier(objective=\"mutliclass\",n_estimators=10000,num_leaves=512)\ngbm.fit(X_train,y_train,verbose = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a61696222c3135a6a9234b603ab6d56bfd1fd24f"},"cell_type":"code","source":"pred = gbm.predict(testfeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c68aa165029751a01e1cad0688342716344bd3de"},"cell_type":"code","source":"#pred = linearsvmfinal.predict(testfeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c0415433c80bd3e59723eb14f2d6e6ef8849fef1"},"cell_type":"code","source":"predconv = encoder.inverse_transform(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"358cd2342158e6f21c60c0d47973d21e417cd837"},"cell_type":"code","source":"sub = pd.DataFrame({'id':testset.id,'cuisine':predconv})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"45df55d7c9b833cb7d2155f3c95700fa8ec36d28"},"cell_type":"code","source":"output = sub[['id','cuisine']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"46e64414aa942f23466356fc43bace1c6c210d9f"},"cell_type":"code","source":"output.to_csv(\"outputfile.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23eafed82bb2f7facabab37b05025a4f6c9e618c"},"cell_type":"markdown","source":"# END"},{"metadata":{"_uuid":"f2c5cb613b0538ae0fec76f43262f174f0943735"},"cell_type":"markdown","source":"# NOTES:\n1) You can achieve better accuracy by ensembling the model, i will update this very soon.\n2) Neural Network has even scored an accuracy of 0.81 but the computation is very time taking.\n3) I have not used my time on visualizing the dataset.(which is not needed for this submission).\n4) Please comment for any questions, doubts or suggestions.\n\n THANK YOU\n \n# please UPVOTE, if you like."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5ea3c97fc25cb2238164d2bd47c45f6f5d338e2f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}