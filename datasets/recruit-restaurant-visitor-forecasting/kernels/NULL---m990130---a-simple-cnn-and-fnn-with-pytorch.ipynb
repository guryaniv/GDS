{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py", "name": "python"}}, "cells": [{"metadata": {"_cell_guid": "ff63fa26-83a1-435a-964e-1714ca175650", "_uuid": "74796c4b7f0d9d7f7d1a8f91d88f7224933c3f8f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "from sklearn import preprocessing\n", "from datetime import datetime\n"], "outputs": []}, {"metadata": {"_cell_guid": "63a2bbdc-304a-4b38-a28b-753861bdc7a8", "_uuid": "762958452a80cec06fffeb8143514c3092dca066"}, "source": ["# preprocessing with:\n", "https://www.kaggle.com/tflana/simple-keras-feed-fwd-nn/code  \n", "\n", "**Don't run all cells together. Run it cell by cell. Contains 2 Networks, CNN and FNN(linear), uncomment to switch to another mode.**  \n", "**Watch out those .cuda() will throw error if you don't have cuda**"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "f2605c7f-50aa-44fd-9480-0943a3bb2bf9", "_uuid": "790699c5b7e3f69f4fd202611f1264132941b83d", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# load the data as DataFrame\n", "\n", "data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "\n", "# take the common part of hpg reservation, id relation with key hpg_store_id\n", "\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n", "\n", "# convert the time info into datetime format, add diff_time col\n", "# group the data with id then visit time, then rename them\n", "# done with the DataFrame 'ar' and 'hr'\n", "\n", "\n", "for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime']).dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n", "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n", "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n", "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n", "\n", "# divide visit_date info into 4 cols \n", "    \n", "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "# apply the same convention on data['tes']\n", "\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n", "\n", "\n", "# stores is the aggregate info of 'tra'\n", "# rename\n", "# merge the new DataFrame with air_reserve on key air_id\n", "\n", "stores = data['tra'].groupby(['air_store_id','dow']).agg({'visitors' : [np.min,np.mean,np.median,np.max,np.size]}).reset_index()\n", "stores.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n", "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n", "\n", "# label the styles and the areas \n", "\n", "# NEW FEATURES FROM Georgii Vyshnia\n", "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n", "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n", "lbl = preprocessing.LabelEncoder()\n", "for i in range(4):\n", "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "    stores['air_area_name' +str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n", "\n", "# merge information above into stores as 2 new cols   \n", "    \n", "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n", "\n", "# same work(time &labels) with hol\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "\n", "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n", "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n", "\n", "for df in ['ar','hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n", "\n", "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n", "\n", "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n", "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n", "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n", "\n", "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n", "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n", "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n", "\n", "# NEW FEATURES FROM JMBULL\n", "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n", "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n", "train['var_max_long'] = train['longitude'].max() - train['longitude']\n", "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n", "test['var_max_long'] = test['longitude'].max() - test['longitude']\n", "\n", "# NEW FEATURES FROM Georgii Vyshnia\n", "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n", "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n", "\n", "lbl = preprocessing.LabelEncoder()\n", "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n", "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n", "\n", "test_index = test['id']\n", "#test_index_idx = test.index\n", "train['visitors'] = np.log1p(train['visitors'].values)\n", "\n", "##### preprocess steps to feed into the network #################\n", "#################################################################\n", "\n", "train['cat'] = 'train'\n", "test['cat'] = 'test'\n", "    \n", "hot_enc_cols_cat = ['air_genre_name0','air_genre_name1','air_genre_name2','air_genre_name3',\n", "                 'air_area_name0','air_area_name1','air_area_name2','air_area_name3',\n", "                 'air_genre_name','air_area_name','day_of_week','dow','year','month']\n", "\n", "full_df = pd.concat((train,test), axis=0, ignore_index=False)\n", "    \n", "df_index = full_df.index\n", "    \n", "full_df = pd.get_dummies(full_df, columns=hot_enc_cols_cat)\n", "\n", "scale_cols = ['lon_plus_lat','var_max_long','var_max_lat','date_int','total_reserv_dt_diff_mean','total_reserv_mean',\n", "             'total_reserv_sum','rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rs2_y','rv2_y','latitude','longitude',\n", "             'count_observations','max_visitors','median_visitors','min_visitors','holiday_flg','rv1_y',\n", "              'mean_visitors','air_store_id2','date_int','var_max_long']\n", "\n", "full_df = full_df.fillna(0)\n", "\n", "from scipy.special import erfinv\n", "\n", "def rank_gauss(x):\n", "    # x is numpy vector\n", "    N = x.shape[0]\n", "    temp = x.argsort()\n", "    rank_x = temp.argsort() / N\n", "    rank_x -= rank_x.mean()\n", "    rank_x *= 2 # rank_x.max(), rank_x.min() should be in (-1, 1)\n", "    efi_x = erfinv(rank_x) # np.sqrt(2)*erfinv(rank_x)\n", "    efi_x -= efi_x.mean()\n", "    return efi_x\n", "\n", "for coln in scale_cols:\n", "    full_df[coln] = rank_gauss(np.array(full_df[coln]))\n", "\n", "full_df.index = df_index    \n", "        \n", "train = full_df[full_df['cat']=='train']\n", "test = full_df[full_df['cat']=='test']    \n", "    \n", "drop_cols = ['cat','id', 'air_store_id', 'visit_date','visitors']\n", "\n", "targets = train['visitors']\n", "train = train.drop(train[drop_cols],axis=1)\n", "test = test.drop(test[drop_cols],axis=1)\n", "\n", "print('Pre-processing done!')\n", "\n", "print('train',train.shape)\n", "print('test',test.shape)\n", "print(targets.shape)"], "outputs": []}, {"metadata": {"_cell_guid": "698b8507-013f-4858-8097-498554d3543b", "_uuid": "d9fb29f28502023d646783fa953b3cc5f714e720"}, "source": ["**Split the train data into tain{X,y} and valid{X,y}. Test here is used to store the submition**"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6a57d5de-4902-49cb-8be9-e69d5bc707ff", "_uuid": "4e5daa0e0b8d78355c9b10e21390c93d08a92548", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["from sklearn.model_selection import train_test_split\n", "train, valid, y_train, y_valid = train_test_split(train, targets, test_size=0.3, random_state=2018)\n", "\n", "print(train.shape)\n", "print(valid.shape)\n", "print(y_train.shape)\n", "print(y_valid.shape)\n", "print('test shape',test.shape)\n", "print('\\n-------------------\\n')"], "outputs": []}, {"metadata": {"_cell_guid": "35c8eeb3-c109-4fa9-8cb1-4d1aa6ae0ff3", "_uuid": "7020c3d0a3e2d2396bdeb2125686a67a6424ee7b"}, "source": ["**Set up Models**"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "004433be-c515-40c4-ab3e-12443e93cafb", "_uuid": "8c241fd126ae283e97db9442dc621e7551a4ee63", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["import torch \n", "import torch.nn as nn\n", "import torchvision.transforms as transforms\n", "from torch.autograd import Variable\n", "import torch.nn.functional as F\n", "\n", "# Hyper-Parameters\n", "\n", "EPOCH = 20\n", "BATCH_SIZE = 200\n", "L_R = 0.001\n", "CUDA = torch.cuda.is_available() # if not run, set here to False manually"], "outputs": []}, {"metadata": {"_cell_guid": "2c42d473-3959-4fec-9d71-d48cd03a093c", "_uuid": "7dfc9cd6c94de9b3ef258870e180520d550362df", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# expand the train and valid data to fit the expected Tensor\n", "\n", "train_np = np.expand_dims(train.values.astype(np.float32), axis=1)\n", "print(train_np.shape)\n", "valid_np = np.expand_dims(valid.values.astype(np.float32), axis=1)\n", "print(valid_np.shape)\n", "\n", "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_np),\n", "                                               torch.from_numpy(y_train.values))\n", "\n", "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n", "                                           batch_size = BATCH_SIZE,\n", "                                           shuffle = True)\n", "\n", "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(valid_np),\n", "                                               torch.from_numpy(y_valid.values))\n", "\n", "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n", "                                           batch_size =500,\n", "                                           shuffle = False)\n", "\n", "print(type(train_dataset),type(test_dataset))\n", "\n", "# every batch: BATCH_SIZE * 274(featrues) data and BATCH_SIZE*1 labels"], "outputs": []}, {"metadata": {"_cell_guid": "61f218fb-11a4-4f8e-b0e3-60355e78b5f4", "_uuid": "4624dacb1a484a4f21a97c4a8bc336a961ecc551", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# a toy cnn, works worse than fnn below....\n", "\n", "class CNN(nn.Module):\n", "    def __init__(self):\n", "        super(CNN, self).__init__()\n", "        self.layer1 = nn.Sequential(\n", "            nn.Conv1d(1, 16, kernel_size=18),\n", "            nn.BatchNorm1d(16),\n", "            nn.ReLU(),\n", "            nn.Dropout(.2),\n", "            nn.MaxPool1d(2))\n", "        self.layer2 = nn.Sequential(\n", "            nn.Conv1d(16, 32, kernel_size=8),\n", "            nn.BatchNorm1d(32),           \n", "            nn.ReLU(),\n", "            nn.Dropout(.2),\n", "            nn.MaxPool1d(2))\n", "        self.fc = nn.Linear(32*60, 32*10)\n", "        self.fc2 = nn.Linear(32*10, 1)\n", "\n", "#input size (BATCH_SIZE * 1 *274)--(conv1d(1, 16, 18))-->(BATCH_SIZE * 16 * 274-18)--(maxpool1d(2))-->(BATCH_SIZE * 16 * (274-18)/2=128)\n", "\n", "       \n", "        \n", "    def forward(self, x):\n", "        #print(x.shape)\n", "        out = self.layer1(x)\n", "        #print(out.shape)\n", "        out = self.layer2(out)\n", "        #print(out.shape)\n", "        out = out.view(-1, 32*60)\n", "        out = self.fc(out)\n", "        out = self.fc2(out)\n", "        #print(out.shape)\n", "        return out\n"], "outputs": []}, {"metadata": {"_cell_guid": "16a5566e-2854-4a1e-9f47-df6898e5f50d", "_uuid": "f4cd91485b186fae011dced8ac27ded58321da1b", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["class Net(nn.Module):\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.layer = nn.Sequential(\n", "            nn.Linear(274, 128),\n", "            nn.ReLU(inplace=True),\n", "            nn.Dropout(.3),\n", "            nn.Linear(128, 64),\n", "            nn.ReLU(inplace=True),\n", "            nn.Dropout(.2),\n", "            nn.Linear(64, 32),\n", "            nn.ReLU(inplace=True),\n", "            nn.Dropout(.2),\n", "            nn.Linear(32, 1)\n", "        )           \n", "        \n", "        \n", "    def forward(self, x):\n", "        return self.layer(x)\n"], "outputs": []}, {"metadata": {"_cell_guid": "db912a59-cf1f-4a90-a1e0-afe272e10633", "_uuid": "8575ab5d6c8c75504b445055b2cc0e07034cb5a3", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["#model = CNN()\n", "model = Net()\n", "\n", "if CUDA:\n", "    model.cuda()\n", "\n", "# -----------------------------empty the cache of GPU-----------------------------\n", "torch.cuda.empty_cache()\n", "# Optimizer\n", "#criterion = nn.MSELoss()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=L_R)"], "outputs": []}, {"metadata": {"_cell_guid": "d55de711-9421-40a0-b0cf-3e253889f984", "_uuid": "0c46f2597d36ff44e5dcb27d28eb6f806a1cfcd2", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def train_step(epoch):\n", "    model.train()\n", "    for batch_idx, (x, y) in enumerate(train_loader):        \n", "        if CUDA:\n", "            x, y = x.cuda(), y.cuda()\n", "        x, y = Variable(x), Variable(y).float()\n", "        #print(x.shape)\n", "        optimizer.zero_grad()\n", "        outputs = model(x)\n", "        loss = F.mse_loss(outputs, y)\n", "        loss.backward()\n", "        optimizer.step()\n", "        \n", "        if batch_idx % 256 == 0:\n", "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n", "                epoch, batch_idx * len(x), len(train_loader.dataset),\n", "                100. * batch_idx / len(train_loader), loss.data[0]))\n", "\n", "            \n", "# ToDo: plot the result of test_loss\n", "\n", "def test_step():\n", "    model.eval()\n", "    test_loss = 0\n", "    correct = 0\n", "    for x, y in test_loader:\n", "        if CUDA:\n", "            x, y = x.cuda(), y.cuda()\n", "        x, y = Variable(x, volatile=True), Variable(y).float()\n", "        outputs = model(x)\n", "        \n", "        # Problem F.mse_loss use size_average=True so the value is wrong in early version\n", "        \n", "        test_loss += F.mse_loss(outputs, y, size_average=False).data[0] # sum up the batch loss \n", "    test_loss /= len(test_loader.dataset)\n", "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n", "    loss_p.append(test_loss)"], "outputs": []}, {"metadata": {"_cell_guid": "5637ab73-2487-426d-aa00-e2010893abe7", "_uuid": "54ea462ea1eadf04d4076f85aa2d537795306558", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# run training\n", "\n", "loss_p = []\n", "\n", "for epoch in range(EPOCH):\n", "    train_step(epoch)\n", "    test_step()"], "outputs": []}, {"metadata": {"_cell_guid": "247f0321-410f-43b3-820f-1fa35a09285b", "_uuid": "3771ec81fc774fc7a2efa2954ac83fb2054f176e", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# result loader\n", "res_np = np.expand_dims(test.values.astype(np.float32), axis=1)\n", "\n", "res_tensor = torch.from_numpy(res_np)\n", "\n", "res_loader = torch.utils.data.DataLoader(dataset=res_tensor,\n", "                                           batch_size =BATCH_SIZE,\n", "                                           shuffle = False)"], "outputs": []}, {"metadata": {"_cell_guid": "a3b4749c-2c7a-4043-a2b5-e1ff3fa52e5f", "_uuid": "8a1f2818248f813ff18f18c5a7f7c7a3c5b7b633", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["results = []\n", "\n", "for x in res_loader:\n", "    x = Variable(x).cuda()\n", "    outputs = model(x)\n", "    results.append(outputs.cpu().data.numpy())\n", "\n", "# turn results into np.array and flattend them then get the real predict with np.exp\n", "results_n = np.array(results)\n", "flattened = np.exp(np.array([val for sublist in results_n for val in sublist]).reshape(-1))\n", "\n", "# transform it into DataFrame to save of visualize\n", "nn_df = pd.DataFrame(flattened,columns=['visitors'],index=test_index)\n", "print(nn_df.head())\n", "nn_df.to_csv('submit_nn.csv')"], "outputs": []}], "nbformat": 4}