{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["\n", "\n", "Version 24: Added Nikunj's features and retuned<br>\n", "Version 25: Added more Nikunj features and retuned again. <br>\n", "Version 26: Deleted some of Nikunj features and retuned again.<br>\n", "Version 27: Remove Niknuj features and go to tuning that was optimal without them, as baseline<br>\n", "Version 28: Same as version 27 but after having tested some Nikunj features individually<br>\n", "Version 29: Add 2 best Nikunj features (zip_count, city_count)<br>\n", "Version 30: Add 3rd feature (GarPoolAC), and some cleanup<br>\n", "Version 32: Retune: colsample .7 -> .8<br>\n", "Version 33: Retune: lambda=10, subsample=.55<br>\n", "Version 34: Revert subsample=.5<br>\n", "Version 35: Fine tune: lambda=9<br>\n", "Version 36: Revert: colsample .7<br>\n", "Version 37: Cleanup<br>\n", "Version 38: Make boosting rounds and stopping rounds inversely proportional to learning rate<br>\n", "Version 40: Add city_mean and zip_mean features<br>\n", "Version 41: Fix comments (Previously mis-stated logerror as \"sale price\" in feature descriptions)<br>\n", "Version 42: Fix bug in city_mean definition<br>\n", "Version 43: Get rid of city_mean<br>\n", "Version 44: Retune: alpha=0.5<br>\n", "Version 45: fine tune: lambda=9.5<br>\n", "Version 46: Roll back to version 39 model, because zip_mean had a data leak, and the corrected version doesn't help<br>\n", "Version 47: Add additional aggregation features, including by neighborhood<br>\n", "Verison 48: Put test set features in the correct order<br>\n", "Version 49: Retune: lambda=5, colsample=.55<br>\n", "Version 50: Retune: alpha=.65, colsample=.50<br>\n", "Version 51: Retune: max_depth=7<br>\n", "Version 52: Make it optional to generate submission file when running full notebook<br>\n", "Version 53. Option to do validation only<br>\n", "Version 54. Starting to clean up the code<br>\n", "Version 55. Option to fit final model to full training set<br>\n", "Version 56. Optimize fudge factor<br>\n", "Version 57. Allow change to validation set cutoff date<br>\n", "Version 59. Try September 15 as validation cutoff<br>\n", "Version 62. Allow final fit on 2017 (no correction for data leak)<br>\n", "Version 68. Add seasonal features<br>\n", " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Turns out the seasonal features make the fudge factor largely irrelevant,<br>\n", " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but that's partly because I chose the basedate to fit the fudge factors.)<br>\n", " Version 71. Make separate predictions for 2017 using 2017 properties data<br>\n", " Version 72. Run with FIT_2017_TRAIN_SET = False<br>\n", " Version 73. Remove outliers from 2017 data and set FIT_2017_TRAIN_SET = True<br>\n", " Version 74. Set FIT_2017_TRAIN_SET = False again<br>\n", "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Removing outliers helps, but 2017 data still generate bad 2016 predictions.)<br>\n", "  Version 76. Allow fitting combined training set<br>"], "metadata": {"_uuid": "5897f4e67c6020a4ce7113a58e8b29a3913fd948", "_cell_guid": "b4f23ac9-4229-4c1d-ad9b-80e64ce2c5a9"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["MAKE_SUBMISSION = True          # Generate output file.\n", "CV_ONLY = False                 # Do validation only; do not generate predicitons.\n", "FIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\n", "FIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\n", "FIT_COMBINED_TRAIN_SET = True   # Fit combined 2016-2017 training set\n", "USE_SEASONAL_FEATURES = True\n", "VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n", "LEARNING_RATE = 0.007           # shrinkage rate for boosting roudns\n", "ROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\n", "OPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\n", "FUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction"], "metadata": {"_uuid": "47787be314e3cc68e7071ae827aef9710fc8341a", "_cell_guid": "f16ac0de-de1c-4b65-a85e-a3227fbe0c47", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import mean_absolute_error\n", "import datetime as dt\n", "from datetime import datetime\n", "import gc\n", "import patsy\n", "import statsmodels.api as sm\n", "import statsmodels.formula.api as smf\n", "from statsmodels.regression.quantile_regression import QuantReg"], "metadata": {"_uuid": "a6d445499b3664bf165ab992f309a417471707f4", "_cell_guid": "1c1a4c8e-9379-47b8-aa2e-3df8e5d22918", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["properties16 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\n", "properties17 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n", "\n", "# Number of properties in the zip\n", "zip_count = properties16['regionidzip'].value_counts().to_dict()\n", "# Number of properties in the city\n", "city_count = properties16['regionidcity'].value_counts().to_dict()\n", "# Median year of construction by neighborhood\n", "medyear = properties16.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n", "# Mean square feet by neighborhood\n", "meanarea = properties16.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n", "# Neighborhood latitude and longitude\n", "medlat = properties16.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n", "medlong = properties16.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "for c in properties16.columns:\n", "    properties16[c]=properties16[c].fillna(-1)\n", "    if properties16[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties16[c].values))\n", "        properties16[c] = lbl.transform(list(properties16[c].values))"], "metadata": {"_uuid": "43ea1cf09b04ca53bb99c95d428b4624572ba75a", "_cell_guid": "a6cc2bdd-9d38-4f66-8903-3ce019f7109a", "collapsed": true, "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df = train.merge(properties16, how='left', on='parcelid')\n", "select_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE\n", "if USE_SEASONAL_FEATURES:\n", "    basedate = pd.to_datetime('2015-11-15').toordinal()\n"], "metadata": {"_uuid": "dfe82baffa246f7ebe77d472de49ee6a6d43a419", "_cell_guid": "b5752c23-b3d7-4403-ab0b-2813d88e14f5", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["del train\n", "gc.collect()"], "metadata": {"_uuid": "95c7a84848e30fa37a75d319b9e7e04deaff9ef9", "_cell_guid": "101437b6-c54e-4452-9c81-a824818d651a"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Inputs to features that depend on target variable\n", "# (Ideally these should be recalculated, and the dependent features recalculated,\n", "#  when fitting to the full training set.  But I haven't implemented that yet.)\n", "\n", "# Standard deviation of target value for properties in the city/zip/neighborhood\n", "citystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "zipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "hoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()"], "metadata": {"_uuid": "9722eb834d3cf02e943b8c85bf99e7a7515173e4", "_cell_guid": "d9460c6d-bf39-4bfb-a584-f3aafcabc8fa", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["def calculate_features(df):\n", "    # Nikunj's features\n", "    # Number of properties in the zip\n", "    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n", "    # Number of properties in the city\n", "    df['N-city_count'] = df['regionidcity'].map(city_count)\n", "    # Does property have a garage, pool or hot tub and AC?\n", "    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n", "                         (df['pooltypeid10']>0) & \\\n", "                         (df['airconditioningtypeid']!=5))*1 \n", "\n", "    # More features\n", "    # Mean square feet of neighborhood properties\n", "    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n", "    # Median year of construction of neighborhood properties\n", "    df['med_year'] = df['regionidneighborhood'].map(medyear)\n", "    # Neighborhood latitude and longitude\n", "    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n", "    df['med_long'] = df['regionidneighborhood'].map(medlong)\n", "\n", "    df['zip_std'] = df['regionidzip'].map(zipstd)\n", "    df['city_std'] = df['regionidcity'].map(citystd)\n", "    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n", "    \n", "    if USE_SEASONAL_FEATURES:\n", "        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n", "                             (2*np.pi/365.25) ).apply(np.cos)\n", "        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n", "                             (2*np.pi/365.25) ).apply(np.sin)\n"], "metadata": {"_uuid": "4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77", "_cell_guid": "058d3299-005e-4458-a8cd-cd3c4d854190", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n", "            'buildingqualitytypeid', 'regionidcity']\n", "droptrain = ['parcelid', 'logerror', 'transactiondate']\n", "droptest = ['ParcelId']"], "metadata": {"_uuid": "d816890fe0edcab13c415144e76ecd12938ab17e", "_cell_guid": "e389a360-b1dc-4257-b3c3-fea69026863d", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["calculate_features(train_df)\n", "\n", "x_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\n", "y_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n", "\n", "print('Shape full training set: {}'.format(train_df.shape))\n", "print('Dropped vars: {}'.format(len(dropvars+droptrain)))\n", "print('Shape valid X: {}'.format(x_valid.shape))\n", "print('Shape valid y: {}'.format(y_valid.shape))\n", "\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "print('\\nFull training set after removing outliers, before dropping vars:')     \n", "print('Shape training set: {}\\n'.format(train_df.shape))\n", "\n", "if FIT_FULL_TRAIN_SET:\n", "    full_train = train_df.copy()\n", "\n", "train_df=train_df[~select_qtr4]\n", "x_train=train_df.drop(dropvars+droptrain, axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "n_train = x_train.shape[0]\n", "print('Training subset after removing outliers:')     \n", "print('Shape train X: {}'.format(x_train.shape))\n", "print('Shape train y: {}'.format(y_train.shape))\n", "\n", "if FIT_FULL_TRAIN_SET:\n", "    x_full = full_train.drop(dropvars+droptrain, axis=1)\n", "    y_full = full_train[\"logerror\"].values.astype(np.float32)\n", "    n_full = x_full.shape[0]\n", "    print('\\nFull trainng set:')     \n", "    print('Shape train X: {}'.format(x_train.shape))\n", "    print('Shape train y: {}'.format(y_train.shape))"], "metadata": {"_uuid": "233443f9b4c2af79e91d235fdc4f660ff1d630bc", "_cell_guid": "720a950e-5392-4571-8fd6-e849a9c6967e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["if not CV_ONLY:\n", "    # Generate test set data\n", "    \n", "    sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n", "    \n", "    # Process properties for 2016\n", "    test_df = pd.merge( sample_submission[['ParcelId']], \n", "                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n", "                        how = 'left', on = 'ParcelId' )\n", "    if USE_SEASONAL_FEATURES:\n", "        test_df['transactiondate'] = '2016-10-31'\n", "        droptest += ['transactiondate']\n", "    calculate_features(test_df)\n", "    x_test = test_df.drop(dropvars+droptest, axis=1)\n", "    print('Shape test: {}'.format(x_test.shape))\n", "\n", "    # Process properties for 2017\n", "    for c in properties17.columns:\n", "        properties17[c]=properties17[c].fillna(-1)\n", "        if properties17[c].dtype == 'object':\n", "            lbl = LabelEncoder()\n", "            lbl.fit(list(properties17[c].values))\n", "            properties17[c] = lbl.transform(list(properties17[c].values))\n", "    zip_count = properties17['regionidzip'].value_counts().to_dict()\n", "    city_count = properties17['regionidcity'].value_counts().to_dict()\n", "    medyear = properties17.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n", "    meanarea = properties17.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n", "    medlat = properties17.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n", "    medlong = properties17.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n", "\n", "    test_df = pd.merge( sample_submission[['ParcelId']], \n", "                        properties17.rename(columns = {'parcelid': 'ParcelId'}), \n", "                        how = 'left', on = 'ParcelId' )\n", "    if USE_SEASONAL_FEATURES:\n", "        test_df['transactiondate'] = '2017-10-31'\n", "    calculate_features(test_df)\n", "    x_test17 = test_df.drop(dropvars+droptest, axis=1)\n", "\n", "    del test_df"], "metadata": {"_uuid": "f3a9ea8db9161705b9e0d61a82736b44c7dfcb01", "_cell_guid": "d0c5819c-0873-4b49-a8a7-5295632ccbb9", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["del train_df\n", "del select_qtr4\n", "gc.collect()"], "metadata": {"_uuid": "c6d653da30197972f12f1a7a0ea4d0a36f28c554", "_cell_guid": "03fb6b2f-5166-4646-8445-304765a404ad", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["xgb_params = {  # best as of 2017-09-28 13:20 UTC\n", "    'eta': LEARNING_RATE,\n", "    'max_depth': 7, \n", "    'subsample': 0.6,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 5.0,\n", "    'alpha': 0.65,\n", "    'colsample_bytree': 0.5,\n", "    'base_score': y_mean,'taxdelinquencyyear'\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dvalid_x = xgb.DMatrix(x_valid)\n", "dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n", "if not CV_ONLY:\n", "    dtest = xgb.DMatrix(x_test)\n", "    dtest17 = xgb.DMatrix(x_test17)\n", "    del x_test"], "metadata": {"_uuid": "c8c1043d72790dc926ae76f398487e8313caeed1", "_cell_guid": "de59798a-1b8a-4d0e-a92c-846577f4406b", "collapsed": true, "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["del x_train\n", "gc.collect()"], "metadata": {"_uuid": "dd0776733ec3d3ac960b9ab9ac6e518a99e012d9", "_cell_guid": "45e61d05-da7f-4841-811a-3a95c86a3306", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\n", "early_stopping_rounds = round( num_boost_rounds / 20 )\n", "print('Boosting rounds: {}'.format(num_boost_rounds))\n", "print('Early stoping rounds: {}'.format(early_stopping_rounds))"], "metadata": {"_uuid": "fe6761fd44aaf7a5c063897c9ca3b90b957b7108", "_cell_guid": "de8e97e6-d681-465b-a68a-6178e7593e49", "scrolled": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n", "model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n", "                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n", "                  verbose_eval=10)"], "metadata": {"_uuid": "09c99f75bcfb64225f390eee5e5dbf4f47f4103b", "_cell_guid": "95d8ac93-f2ff-4ce3-9cb6-8044d73f8600", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n", "print( \"XGBoost validation set predictions:\" )\n", "print( pd.DataFrame(valid_pred).head() )\n", "print(\"\\nMean absolute validation error:\")\n", "mean_absolute_error(y_valid, valid_pred)"], "metadata": {"_uuid": "02a37067bd8ba72b050d6634e9994b234bdf6389", "_cell_guid": "c241b1e2-d2c6-4936-a6ee-282a4e90ca27", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["if OPTIMIZE_FUDGE_FACTOR:\n", "    mod = QuantReg(y_valid, valid_pred)\n", "    res = mod.fit(q=.5)\n", "    print(\"\\nLAD Fit for Fudge Factor:\")\n", "    print(res.summary())\n", "\n", "    fudge = res.params[0]\n", "    print(\"Optimized fudge factor:\", fudge)\n", "    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n", "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n", "\n", "    fudge **= FUDGE_FACTOR_SCALEDOWN\n", "    print(\"Scaled down fudge factor:\", fudge)\n", "    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n", "    print(mean_absolute_error(y_valid, fudge*valid_pred))\n", "else:\n", "    fudge=1.0"], "metadata": {"_uuid": "dbd7ae96da81ab900d6eee1c186b2b56f17e4fc8", "_cell_guid": "7dd798fb-9160-4a48-9b52-9bc16f582e0f", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["if FIT_FULL_TRAIN_SET and not CV_ONLY:\n", "    if FIT_COMBINED_TRAIN_SET:\n", "        # Merge 2016 and 2017 data sets\n", "        train16 = pd.read_csv('../input/train_2016_v2.csv')\n", "        train17 = pd.read_csv('../input/train_2017.csv')\n", "        train16 = pd.merge(train16, properties16, how = 'left', on = 'parcelid')\n", "        train17 = pd.merge(train17, properties17, how = 'left', on = 'parcelid')\n", "        train17[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n", "        train_df = pd.concat([train16, train17], axis = 0)\n", "        # Generate features\n", "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n", "        calculate_features(train_df)\n", "        # Remove outliers\n", "        train_df=train_df[ train_df.logerror > -0.4 ]\n", "        train_df=train_df[ train_df.logerror < 0.419 ]\n", "        # Create final training data sets\n", "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n", "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n", "        n_full = x_full.shape[0]     \n", "    elif FIT_2017_TRAIN_SET:\n", "        train = pd.read_csv('../input/train_2017.csv')\n", "        train_df = train.merge(properties17, how='left', on='parcelid')\n", "        # Generate features\n", "        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n", "        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n", "        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n", "        calculate_features(train_df)\n", "        # Remove outliers\n", "        train_df=train_df[ train_df.logerror > -0.4 ]\n", "        train_df=train_df[ train_df.logerror < 0.419 ]\n", "        # Create final training data sets\n", "        x_full = train_df.drop(dropvars+droptrain, axis=1)\n", "        y_full = train_df[\"logerror\"].values.astype(np.float32)\n", "        n_full = x_full.shape[0]     \n", "    dtrain = xgb.DMatrix(x_full, y_full)\n", "    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)\n", "    full_model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds, \n", "                           evals=[(dtrain,'train')], verbose_eval=10)"], "metadata": {"_uuid": "4f9649857441bbc57beaeb9f5e7a91234c03f4c7", "_cell_guid": "6d782218-3b72-431c-8f5e-ef3a2723bbfb"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["del properties16\n", "del properties17\n", "gc.collect()"], "metadata": {"_uuid": "a39b9309cbaa1588b0646a079c4bfea929990cc1", "_cell_guid": "74ad3d77-75eb-447c-9524-21c225507879"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["if not CV_ONLY:\n", "    if FIT_FULL_TRAIN_SET:\n", "        pred = fudge*full_model.predict(dtest)\n", "        pred17 = fudge*full_model.predict(dtest17)\n", "    else:\n", "        pred = fudge*model.predict(dtest, ntree_limit=model.best_ntree_limit)\n", "        pred17 = fudge*model.predict(dtest17, ntree_limit=model.best_ntree_limit)\n", "        \n", "    print( \"XGBoost test set predictions for 2016:\" )\n", "    print( pd.DataFrame(pred).head() )\n", "    print( \"XGBoost test set predictions for 2017:\" )\n", "    print( pd.DataFrame(pred17).head() )    "], "metadata": {"_uuid": "a710f272857376365c6a02929197a06be72ae28b", "_cell_guid": "c8f7bb43-015e-4c2c-a1bd-afa92219bd87", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["if MAKE_SUBMISSION and not CV_ONLY:\n", "   y_pred=[]\n", "   y_pred17=[]\n", "\n", "   for i,predict in enumerate(pred):\n", "       y_pred.append(str(round(predict,4)))\n", "   for i,predict in enumerate(pred17):\n", "       y_pred17.append(str(round(predict,4)))\n", "   y_pred=np.array(y_pred)\n", "   y_pred17=np.array(y_pred17)\n", "\n", "   output = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n", "           '201610': y_pred, '201611': y_pred, '201612': y_pred,\n", "           '201710': y_pred17, '201711': y_pred17, '201712': y_pred17})\n", "   # set col 'ParceID' to first col\n", "   cols = output.columns.tolist()\n", "   cols = cols[-1:] + cols[:-1]\n", "   output = output[cols]\n", "\n", "   output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)"], "metadata": {"_uuid": "b85616fbde70ef9d6fe220ef12947fb5d2dd3df6", "_cell_guid": "8d793a38-6df2-45e7-a725-cc2161ed3cee", "_execution_state": "idle"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["print(\"Mean absolute validation error without fudge factor: \", )\n", "print( mean_absolute_error(y_valid, valid_pred) )\n", "if OPTIMIZE_FUDGE_FACTOR:\n", "    print(\"Mean absolute validation error with fudge factor:\")\n", "    print( mean_absolute_error(y_valid, fudge*valid_pred) )"], "metadata": {"_uuid": "c5ab53465012b552d5b08be82864f972d02946e4", "_cell_guid": "50d1d19d-26e8-4227-aa77-7d452d6c7d36", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": [], "metadata": {"_uuid": "b26353143ba91c98ba970f5051bc9656b39c9bb4", "_cell_guid": "b66c3b07-5941-44ab-b2f2-58fc46071055", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": [], "metadata": {"_uuid": "9944aa5d56350e51a0b151af9cf38c7b464dcf0b", "_cell_guid": "e5b97a3f-4428-487c-abd4-9507d7850260", "collapsed": true}}], "metadata": {"language_info": {"file_extension": ".py", "version": "3.6.1", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}