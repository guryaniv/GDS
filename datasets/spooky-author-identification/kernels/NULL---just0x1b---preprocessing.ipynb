{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "mimetype": "text/x-python", "name": "python"}}, "cells": [{"metadata": {"_cell_guid": "555bc9f8-187d-4b5b-9866-dfac8cf572c5", "collapsed": true, "_uuid": "0d6476f1f9825b6d5fd7ba738473445726e8a44b"}, "outputs": [], "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import wordpunct_tokenize\n", "\n", "import csv\n", "\n", "from gensim.models import word2vec\n", "import gensim.models\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "4e6e58c2-f435-4eb6-949f-948e6c4ee44e", "collapsed": true, "_uuid": "905c7154d57c457d7c4e7d221038d016a0d38cd2"}, "outputs": [], "execution_count": null, "source": ["# Read input data\n", "train_data = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\n", "test_data  = pd.read_csv(\"../input/spooky-author-identification/test.csv\")\n", "\n", "train_data.describe()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "2b943e6c-4e55-4e20-a8f3-a33bfd7197ee", "collapsed": true, "_uuid": "b4c8ac392b51a4628bd214531aa19494e26b5390"}, "outputs": [], "execution_count": null, "source": ["\n", "def preprocess_text(text, remove_list):   \n", "    '''\n", "    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n", "    print(tokens)\n", "    good_words = [w for w, wtype in tokens if wtype not in remove_list]\n", "    print(good_words)\n", "    '''\n", "    \n", "    def clean_word(word):\n", "        word = word.lower()\n", "        if (len(word) > 1 and word[0] == '\\''):\n", "            return word[1:]\n", "        return word\n", "            \n", "    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n", "    tokens = [(clean_word(word), pos) for (word,pos) in tokens]\n", "    return [(word, pos) for (word,pos) in tokens if word not in remove_list]\n", "\n", "def preprocess_corpus(corpus):\n", "    \n", "    stop_words = set(stopwords.words('english'))\n", "    stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n", "    \n", "    return [preprocess_text(text, stop_words) for text in corpus]\n", "\n", "preprocessed_train_corpus = preprocess_corpus(train_data['text'])\n", "preprocessed_test_corpus  = preprocess_corpus(test_data['text'])\n", "\n", "print(preprocessed_train_corpus[0])"], "cell_type": "code"}, {"metadata": {"_cell_guid": "918fa752-ae8e-4011-aaab-c5be8df8935d", "collapsed": true, "_uuid": "7becb21ca2b45acc1524c459179105341e508b40"}, "outputs": [], "execution_count": null, "source": ["# GLOVE\n", "def extract_vocabulary(corpus, vocabulary = set()):\n", "    for sentence_chunk in corpus:\n", "        for word,tag in sentence_chunk:\n", "            vocabulary.add(word)\n", "    return vocabulary\n", "            \n", "def extract_glove_mapping(vocabulary):   \n", "    glove_table_path = '../input/glove.42b.300d/glove.42B.300d.txt'\n", "    glove_table = pd.read_table(glove_table_path, sep=' ', index_col=0, header = None, quoting = csv.QUOTE_NONE)\n", "    \n", "    extracted_table = glove_table.loc[list(vocabulary)]\n", "    \n", "    del glove_table\n", "    return extracted_table\n", "\n", "vocabulary = extract_vocabulary(preprocessed_train_corpus)\n", "vocabulary = extract_vocabulary(preprocessed_test_corpus, vocabulary)\n", "glove_table = extract_glove_mapping(vocabulary)\n", "glove_table.to_csv('glove_table.csv')"], "cell_type": "code"}, {"metadata": {"_cell_guid": "1be33a31-b117-436f-9184-768ef9da168a", "collapsed": true, "_uuid": "059f75f92478fafa7f13f67fb9fb32b35c28b7eb", "scrolled": true}, "outputs": [], "execution_count": null, "source": ["def glove_fill_missing(glove_table):\n", "    # Find words with missing glove vecs\n", "    missing_data = glove_table[glove_table.isnull().any(axis=1)]\n", "    missing_words = list(missing_data.index)\n", "\n", "    # Train Word2Vec from whole data\n", "    sentences = []\n", "    for sentence_chunk in preprocessed_train_corpus:\n", "        sentences.append([word for (word, tag) in sentence_chunk])\n", "\n", "    for sentence_chunk in preprocessed_test_corpus:\n", "        sentences.append([word for (word, tag) in sentence_chunk])\n", "\n", "    model = word2vec.Word2Vec(sentences,size=100, min_count=1)\n", "\n", "    word2vec_model = model.wv\n", "    del model\n", "    \n", "    # \n", "    def generate_glove_missing(missing_word,k = 5):\n", "        similar_words = word2vec_model.similar_by_word(missing_word, topn=k)\n", "        similar_words = [similar_word for (similar_word, similarity) in similar_words]\n", "        glove_vec = np.nanmean(glove_table.loc[similar_words].as_matrix(),axis=0)\n", "        return glove_vec\n", "    \n", "    for missing_word in missing_words:\n", "        glove_vec = generate_glove_missing(missing_word)\n", "        glove_table.loc[missing_word] = glove_vec\n", "        \n", "    return glove_table\n", "    \n", "glove_table =  glove_fill_missing(glove_table)"], "cell_type": "code"}, {"metadata": {"_cell_guid": "7f6578d8-2a14-474c-be1b-86d07f14b249", "collapsed": true, "_uuid": "b05144519374ed51e6fe4066ea786dddafc947f5"}, "outputs": [], "execution_count": null, "source": ["glove_table.to_csv('filled_glove_table.csv')\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "99724f58-e534-45e0-9be0-f9bf43a60965", "collapsed": true, "_uuid": "9a73b9d2dddbf409c93d8572dbbe910142c1d9e8"}, "outputs": [], "execution_count": null, "source": ["np.save(\"train_corp.npy\", np.array(preprocessed_train_corpus))\n", "np.save(\"test_corp.npy\", np.array(preprocessed_test_corpus))"], "cell_type": "code"}, {"metadata": {"_cell_guid": "89189d4c-395c-4450-bafe-b38e3818ff16", "collapsed": true, "_uuid": "3aa7c74b43370d7425a0d14f995c123abe98b48b"}, "outputs": [], "execution_count": null, "source": [], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 1}