{"cells":[{"metadata":{"_uuid":"296ea7474fcebf7a3e4c8d12ff66e775b4d63c8c"},"cell_type":"markdown","source":"# Deeply Titanic\n\n## An alternate approach\n\nIn this is my second titanic kernel I am going to concentrate on building a deep learning model. In my first attemp I used sklearn and managed to get a score of 0.81.  The first couple of stages are going to be relatively short and sweet, as  i don't intend to repeat the data exploration or go through the rationals for cleaning i did in my first kernel in as much detail (Your just going to have to trust me that this is all good or look at the explanation at https://www.kaggle.com/davidcoxon/titanic-practice-by-davidcoxon/notebook. )"},{"metadata":{"_uuid":"6e92c6d747fef242facb616d31ef794dbf954d49"},"cell_type":"markdown","source":"# Stage 1 : Get Data, Clean Data and Build Features"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# for handling data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.cross_validation import KFold\nfrom sklearn import preprocessing\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.\n\n# import data\ndf_train=pd.read_csv('../input/train.csv',sep=',')\ndf_test=pd.read_csv('../input/test.csv',sep=',')\ndf_data = df_train.append(df_test) # The entire data: train + test.\n\n# exporting the submission\nPassengerId = df_test['PassengerId']\nSubmission=pd.DataFrame()\nSubmission['PassengerId'] = df_test['PassengerId']\nprint('Components imported')","execution_count":61,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## The Missing Data"},{"metadata":{"trusted":true,"_uuid":"f137d2b191eb0b3390fff0223562eea6c4abccc9"},"cell_type":"code","source":"#check for any other unusable values\nprint(pd.isnull(df_data).sum())","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"53a3e46d15d567f5877038b97ed4a8d410988b73"},"cell_type":"markdown","source":"## Statistical Overview of the data"},{"metadata":{"trusted":true,"_uuid":"152a60928041bf86e904e07c3c8ad945e9c7600d"},"cell_type":"code","source":"# Get a statistical overview of the training data\ndf_train.describe()","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"867d154bc1f4ee4d49a27d5d5fd02d884eaaeca4"},"cell_type":"code","source":"# Get a statistical overview of the training data\ndf_test.describe()","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"ab9ade1e08331122037e1a3f349467a1373f3172"},"cell_type":"markdown","source":"## Extract Title data\n\nPassengers titles are included in their name data and provide a really nice way to categorize passengers because they let us about both passenger gender and approximate age. Because we have names for almost all passenger we can use title data to help estimate missing data.  "},{"metadata":{"trusted":true,"_uuid":"57359d1cd03d60cf8c6e4bd2b9149a68bb29e683"},"cell_type":"code","source":"# Get title\ndf_data[\"Title\"] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#Unify common titles. \ndf_data[\"Title\"] = df_data[\"Title\"].replace('Mlle', 'Miss')\ndf_data[\"Title\"] = df_data[\"Title\"].replace('Master', 'Master')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Jonkheer','Don'],'Mr')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Capt','Major', 'Col','Rev','Dr'], 'Millitary')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Honor')\n\n# Age in df_train and df_test:\ndf_train[\"Title\"] = df_data['Title'][:891]\ndf_test[\"Title\"] = df_data['Title'][891:]\n\n# convert Title categories to Columns\ntitledummies=pd.get_dummies(df_train[['Title']], prefix_sep='_') #Title\ndf_train = pd.concat([df_train, titledummies], axis=1) \nttitledummies=pd.get_dummies(df_test[['Title']], prefix_sep='_') #Title\ndf_test = pd.concat([df_test, ttitledummies], axis=1) \nprint('Title Feature created')","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"b5c6209c879561a3f7388f0c011df053add03c36"},"cell_type":"markdown","source":"   ## Estimate missing Embarkation Data"},{"metadata":{"trusted":true,"_uuid":"ad91ea0219e8d4f74ceb315409916fac1578d03e"},"cell_type":"code","source":"#Fill the na values in Fare\ndf_data[\"Embarked\"]=df_data[\"Embarked\"].fillna('S')\ndf_train[\"Embarked\"] = df_data['Embarked'][:891]\ndf_test[\"Embarked\"] = df_data['Embarked'][891:]\nprint('Missing Embarkations Added')","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"519ec482e53b13a7df253a9a717b9c79afd2d151"},"cell_type":"markdown","source":"## Embarked Feature"},{"metadata":{"trusted":true,"_uuid":"8478b9150af2e13151386d41ea71a6522eb5cae2"},"cell_type":"code","source":"# convert Embarked categories to Columns\ndummies=pd.get_dummies(df_train[[\"Embarked\"]], prefix_sep='_') #Embarked\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"Embarked\"]], prefix_sep='_') #Embarked\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Embarked Feature created\")","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"6a89853f6d58cc12830501fa76f434e2bfbcc312"},"cell_type":"markdown","source":"## Estimate missing Fare Data"},{"metadata":{"trusted":true,"_uuid":"736f08f052c8c695b13b0f1171ff2e6108569d31"},"cell_type":"code","source":"# Fill the na values in Fare based on average fare\ndf_data[\"Fare\"]=df_data[\"Fare\"].fillna(np.median(df_data[\"Fare\"]))\ndf_train[\"Fare\"] = df_data[\"Fare\"][:891]\ndf_test[\"Fare\"] = df_data[\"Fare\"][891:]\nprint('Estimate missing Fare')","execution_count":68,"outputs":[]},{"metadata":{"_uuid":"3ab5ba2c89b7235039f66b3c5ca7075220c226de"},"cell_type":"markdown","source":"## Fare Feature"},{"metadata":{"trusted":true,"_uuid":"e5883d726229c6bd1cd6e917b0682b5683f93d20"},"cell_type":"code","source":"Pclass = [1,2,3]\nfor aclass in Pclass:\n    fare_to_impute = df_data.groupby('Pclass')['Fare'].median()[aclass]\n    df_data.loc[(df_data['Fare'].isnull()) & (df_data['Pclass'] == aclass), 'Fare'] = fare_to_impute\n        \ndf_train[\"Fare\"] = df_data[\"Fare\"][:891]\ndf_test[\"Fare\"] = df_data[\"Fare\"][891:]        \n\n#map Fare values into groups of numerical values\ndf_train[\"FareBand\"] = pd.qcut(df_train['Fare'], 4, labels = [1, 2, 3, 4]).astype('category')\ndf_test[\"FareBand\"] = pd.qcut(df_test['Fare'], 4, labels = [1, 2, 3, 4]).astype('category')\n\n# convert FareBand categories to Columns\ndummies=pd.get_dummies(df_train[[\"FareBand\"]], prefix_sep='_') #Embarked\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"FareBand\"]], prefix_sep='_') #Embarked\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Fareband categories created\")","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"2fcee831714d4bc81e97127b1f93b2579064a8cf"},"cell_type":"markdown","source":"## Estimate missing Age Data"},{"metadata":{"trusted":true,"_uuid":"8864dee738e862df4afa4bc74bb7aa013795cc8e"},"cell_type":"code","source":"titles = ['Master', 'Miss', 'Mr', 'Mrs', 'Millitary','Honor']\nfor title in titles:\n    age_to_impute = df_data.groupby('Title')['Age'].median()[title]\n    df_data.loc[(df_data['Age'].isnull()) & (df_data['Title'] == title), 'Age'] = age_to_impute\n# Age in df_train and df_test:\ndf_train[\"Age\"] = df_data['Age'][:891]\ndf_test[\"Age\"] = df_data['Age'][891:]\nprint('Missing Ages Estimated')","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"ac0e30c0c0d3a3a0447e24d3a6b5411076652231"},"cell_type":"markdown","source":"## Create Pclass Categories"},{"metadata":{"trusted":true,"_uuid":"0028b167cd19c225ce3466e436c0ce498541c274"},"cell_type":"code","source":"df_train[\"Pclass\"]=df_train[\"Pclass\"].astype('category')\ndf_test[\"Pclass\"]=df_test[\"Pclass\"].astype('category')\n# convert Pclass categories to Columns\ndummies=pd.get_dummies(df_train[[\"Pclass\"]], prefix_sep='_') #Embarked\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"Pclass\"]], prefix_sep='_') #Embarked\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Pclass Feature created\")","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"33a87961b728fb1783092a8973ad6ba64a51c0cb"},"cell_type":"markdown","source":"## Create Age Band Categories"},{"metadata":{"trusted":true,"_uuid":"825bc7a7b00303ef0bb16edd7b1cafb125a9b7a4"},"cell_type":"code","source":"# sort Age into band categories\nbins = [0,12,24,45,60,np.inf]\nlabels = ['Child', 'Young Adult', 'Adult','Older Adult','Senior']\ndf_train[\"AgeGroup\"] = pd.cut(df_train[\"Age\"], bins, labels = labels)\ndf_test[\"AgeGroup\"] = pd.cut(df_test[\"Age\"], bins, labels = labels)\nprint('Age Feature created')\n\n# convert AgeGroup categories to Columns\ndummies=pd.get_dummies(df_train[[\"AgeGroup\"]], prefix_sep='_') #Embarked\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"AgeGroup\"]], prefix_sep='_') #Embarked\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"AgeGroup categories created\")","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"3985e68b767d590f403fe05c32167b27b3d3e0d2"},"cell_type":"markdown","source":"## Gender Categories"},{"metadata":{"trusted":true,"_uuid":"7c100c1cd64a554db6483d121aac9d9de158c712"},"cell_type":"code","source":"# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Sex']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \ntestdummies=pd.get_dummies(df_test[['Sex']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test, testdummies], axis=1)\nprint('Gender Categories created')","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"d69887067e8431eef71c5e985e86053fe3f0d264"},"cell_type":"markdown","source":"## Lone Travellers Feature"},{"metadata":{"trusted":true,"_uuid":"40308e3ff05dfdb82735f3d040e7f58ecd32115b"},"cell_type":"code","source":"# People with parents or siblings\ndf_data[\"Alone\"] = np.where(df_data['SibSp'] + df_data['Parch'] + 1 == 1, 1,0) # People travelling alone\n# Age in df_train and df_test:\ndf_train[\"Alone\"] = df_data['Alone'][:891]\ndf_test[\"Alone\"] = df_data['Alone'][891:]\nprint('Lone Traveller feature created')","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"e9ddf3171c422abe7f00ef850a15d33370374151"},"cell_type":"markdown","source":"## Family Size Feature"},{"metadata":{"trusted":true,"_uuid":"07ca3c07611a7108adad30055cb62c22b3c4189c"},"cell_type":"code","source":"# get last name\ndf_data[\"Last_Name\"] = df_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n# Set survival value\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_data[\"Family_Survival\"] = DEFAULT_SURVIVAL_VALUE\n\n# Find Family groups by Fare\nfor grp, grp_df in df_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      df_data.loc[df_data['Family_Survival']!=0.5].shape[0])\n\n# Find Family groups by Ticket\nfor _, grp_df in df_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(df_data[df_data['Family_Survival']!=0.5].shape[0]))\n\n# Family_Survival in df_train and df_test:\ndf_train[\"Family_Survival\"] = df_data['Family_Survival'][:891]\ndf_test[\"Family_Survival\"] = df_data['Family_Survival'][891:]","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"3eea8f8693b2bf3c12c48a26494dbd2c3ea57d36"},"cell_type":"markdown","source":"## Cabin feature"},{"metadata":{"trusted":true,"_uuid":"3d6116b025c0e325cef18b6c35bdec5699a6884f"},"cell_type":"code","source":"# check if cabin inf exists\ndf_data[\"HadCabin\"] = (df_data[\"Cabin\"].notnull().astype('int'))\n# split Embanked into df_train and df_test:\ndf_train[\"HadCabin\"] = df_data[\"HadCabin\"][:891]\ndf_test[\"HadCabin\"] = df_data[\"HadCabin\"][891:]\nprint('HasCabin feature created')","execution_count":76,"outputs":[]},{"metadata":{"_uuid":"eece20f5e7cbabb7579f90bb06ad773ce73bd4a5"},"cell_type":"markdown","source":"## Deck feature"},{"metadata":{"trusted":true,"_uuid":"68f7032fe8bb3555ab67931aec5b7723d8beed3a"},"cell_type":"code","source":"#Map and Create Deck feature for training\ndf_data[\"Deck\"] = df_data.Cabin.str.extract('([A-Za-z])', expand=False)\ndeck_mapping = {\"0\":0,\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5}\ndf_data['Deck'] = df_data['Deck'].map(deck_mapping)\ndf_data[\"Deck\"] = df_data[\"Deck\"].fillna(\"0\")\ndf_data[\"Deck\"]=df_data[\"Deck\"].astype('int')\n\ndf_train[\"Deck\"] = df_data['Deck'][:891]\ndf_test[\"Deck\"] = df_data['Deck'][891:]\nprint('Deck feature created')\n\n# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Deck']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[['Deck']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test,dummies], axis=1)\nprint('Deck Categories created')","execution_count":77,"outputs":[]},{"metadata":{"_uuid":"4717d33751c9ed703d7a68507b7cc208d7138d0d"},"cell_type":"markdown","source":"## Create SibSp Categories"},{"metadata":{"trusted":true,"_uuid":"65fd9ba3a3d651afbd721731ff4c344a80ffc317"},"cell_type":"code","source":"# convert SibSp categories to Columns\n(df_train['SibSp'])=(df_train['SibSp']).astype('category')\ndummies=pd.get_dummies(df_train[['SibSp']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \n(df_test['SibSp'])=(df_test['SibSp']).astype('category')\ndummies=pd.get_dummies(df_test[['SibSp']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test,dummies], axis=1)\nprint('Sibsp Categories created')","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"dc74f1624e82169a4bf0d3039cea43c0d426c4e3"},"cell_type":"markdown","source":"## Create Parch Categories"},{"metadata":{"trusted":true,"_uuid":"0a869bd98c81035c24b0ea62d9abb345f9dd3aa2"},"cell_type":"code","source":"# convert SibSp categories to Columns\n(df_train['Parch'])=(df_train['Parch']).astype('category')\ndummies=pd.get_dummies(df_train[['Parch']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \n(df_test['Parch'])=(df_test['Parch']).astype('category')\ndummies=pd.get_dummies(df_test[['Parch']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test,dummies], axis=1)\nprint('Parch Categories created')","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"e3417c8fc3a19dfd0fe895b8b0a76649df484634"},"cell_type":"markdown","source":"## Drop Unneeded Columns"},{"metadata":{"trusted":true,"_uuid":"53292eed0a85abeb2f7cae956f9de85eaa2e5a66"},"cell_type":"code","source":"df_data=df_data.drop(['Cabin','Embarked','Title','Age','Sex','Name','Ticket','Deck','Fare'], axis=1)\ndf_train=df_train.drop(['Cabin','Embarked','Title','Age','Sex','Name','Ticket','AgeGroup','Deck','Pclass','Fare','FareBand','SibSp','Parch','Parch_7','Parch_8','Parch_9'], axis=1)\ndf_test=df_test.drop(['Cabin','Embarked','Title','Age','Sex','Name','Ticket','AgeGroup','Deck','Pclass','Fare','FareBand','SibSp','Parch','Parch_7','Parch_8','Parch_9'], axis=1)\nprint('None Numeric Columns droped')","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"e001df0fc10131db9c017280a5b2fbe938045312"},"cell_type":"markdown","source":"# Final Missing Data"},{"metadata":{"trusted":true,"_uuid":"d27c7bf220e52e9c8aaf1243f3030a8b61efe02c"},"cell_type":"code","source":"#check for any other unusable values\nprint(pd.isnull(df_train).sum())","execution_count":82,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62332f994542975cead7d5d74c395a9bc3a56b67"},"cell_type":"code","source":"#check for any other unusable values\nprint(pd.isnull(df_test).sum())","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"dc9314e883891047c6f3136db43b6c17cc73ebce"},"cell_type":"markdown","source":"# Statistical Overview on final Features"},{"metadata":{"trusted":true,"_uuid":"0cc3690bc6d69dfb6a67b6104e336b3642082b52"},"cell_type":"code","source":"df_train.head()","execution_count":84,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"729c1a1288f453cca4ebafbc29792a5d72d59fbe"},"cell_type":"code","source":"df_train.describe()","execution_count":85,"outputs":[]},{"metadata":{"_uuid":"964479419ecab2fee380afd69b87cd73c1c7894e"},"cell_type":"markdown","source":"# Stage 2 : Build Simple Supervised Learning Model"},{"metadata":{"_uuid":"98d21520f22efaa1bb719df1046bba2c4399fbe6"},"cell_type":"markdown","source":"## Split data"},{"metadata":{"trusted":true,"_uuid":"8c411a216fbe33ad2c0e5a50034016a20e8e738f"},"cell_type":"code","source":"df_test.columns","execution_count":86,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fadfa07c7777f07200f627d8b07c2e23ace74f9"},"cell_type":"code","source":"# define columns to be used\nNUMERIC_COLUMNS=['Alone','Family Size','Sex','Pclass','Fare','FareBand','Age','TitleCat','Embarked'] #72\nORIGINAL_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #83\nREVISED_NUMERIC_COLUMNS=['Title_Master', 'Title_Millitary',\n       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Embarked_C', 'Embarked_Q',\n       'Embarked_S', 'FareBand_1', 'FareBand_2', 'FareBand_3', 'FareBand_4',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'AgeGroup_Child',\n       'AgeGroup_Young Adult', 'AgeGroup_Adult', 'AgeGroup_Older Adult',\n       'AgeGroup_Senior', 'Sex_female', 'Sex_male', 'Alone', 'Family_Survival',\n       'HadCabin','SibSp_0', 'SibSp_1', 'SibSp_2',\n       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n       'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', ]\n\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n\n# create test and training data\ny=df_train['Survived']\nX=data_to_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\nprint('Model Split')","execution_count":87,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8999d22970fa4d7a171dbdadfd1f37b2ed5863"},"cell_type":"code","source":"print(df_test.shape)\nprint(X.shape)\n#df_training=df_test.drop(['AgeGroup_Baby'], axis=1)\n#df_test=df_test.drop(['AgeGroup_Baby'], axis=1)","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"316b105090011e2040288e38addccd8ac641289c"},"cell_type":"markdown","source":"## Create basic SVC model"},{"metadata":{"trusted":true,"_uuid":"bbc0976fccd308922048b3192c854e86befa2f92"},"cell_type":"code","source":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nacc_clf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_clf)","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"e7bf4d789ba3e4da41928b15d0dde02d239a8f31"},"cell_type":"markdown","source":"## Submit prediction"},{"metadata":{"trusted":true,"_uuid":"84735b3a8c74827c04516bc65a50e6d4f5aa4585"},"cell_type":"code","source":"test = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nSubmission['Survived']=clf.predict(test)\nSubmission.set_index('PassengerId', inplace=True)\n# write data frame to csv file\nSubmission.to_csv('baselinemodel01.csv',sep=',')\nprint('Submission Created')","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"1b23e93cd25284e2f6fdbe2169d1f87d4166f8b2"},"cell_type":"markdown","source":"# Stage 3 : Build Hyper Tuner Supervised Learning Model"},{"metadata":{"_uuid":"85b6e2c1884501a957988bb9816dba5e86b4e62c"},"cell_type":"markdown","source":"## Prepare the model"},{"metadata":{"trusted":true,"_uuid":"ce207ab7e11885dd61b37da4de07929f13309c02"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nREVISED_NUMERIC_COLUMNS=['Title_Master', 'Title_Millitary',\n       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Embarked_C', 'Embarked_Q',\n       'Embarked_S', 'FareBand_1', 'FareBand_2', 'FareBand_3', 'FareBand_4',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'AgeGroup_Child',\n       'AgeGroup_Young Adult', 'AgeGroup_Adult', 'AgeGroup_Older Adult',\n       'AgeGroup_Senior', 'Sex_female', 'Sex_male', 'Alone', 'Family_Survival',\n       'HadCabin','SibSp_0', 'SibSp_1', 'SibSp_2',\n       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n       'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', ]# create test and training data\npredictors = df_train.drop(['Survived', 'PassengerId'], axis=1)\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX=data_to_train\ny = df_train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(data_to_train, y, test_size = 0.3,random_state=21, stratify=y)\nprint('Data Split')","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"6ab1df6d47699de7bb863d3bee49448d3748bbc8"},"cell_type":"markdown","source":"## Tune Model"},{"metadata":{"trusted":true,"_uuid":"16a6a9b1476c4c59f131fb8ff667e292adf16beb"},"cell_type":"code","source":"# DecisionTree with RandomizedSearch\n\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": np.arange(1, 6),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 6),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=30)\n\n# Fit it to the data\ntree_cv.fit(X,y)\ny_pred = tree_cv.predict(x_val)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\nacc_tree_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_tree_cv)","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"bde04e60342c5f7c63a12b3c8e3c28cda7492f74"},"cell_type":"markdown","source":"## Create Hyper tuned model"},{"metadata":{"trusted":true,"_uuid":"a5834d9d7f78d8929493526755572bdf2a109731"},"cell_type":"code","source":"# Select columns\ntest = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n# select classifier\ntree = DecisionTreeClassifier(max_depth=5,max_features=7,min_samples_leaf=1,criterion=\"entropy\")\n\n# train model\ntree.fit(X,y)\n# make predictions\nSubmission['Survived']=tree.predict(test)\nprint(Submission.head(5))","execution_count":94,"outputs":[]},{"metadata":{"_uuid":"1475ff1a6a94da354e83509b517c31b371d635e7"},"cell_type":"markdown","source":"## Submit Hyper Tuned Baseline Model "},{"metadata":{"trusted":true,"_uuid":"da63ff3c0b934d0ba7faf04820d7995e865dbb9c"},"cell_type":"code","source":"#Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('Tunedtree1submission.csv',sep=',')\nprint(\"Submission Submitted\")","execution_count":95,"outputs":[]},{"metadata":{"_uuid":"9f45a694eeb0fa03187916d803a1a1121962df09"},"cell_type":"markdown","source":"# Stage 4: Build Deep Learning Model"},{"metadata":{"_uuid":"fc0a9476ad2a9619864cba2f3110a4a486655bab"},"cell_type":"markdown","source":"## Split Data"},{"metadata":{"trusted":true,"_uuid":"d2cd0ced2c47254675d4621512394dfe733d7f8e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nREVISED_NUMERIC_COLUMNS=['Title_Master', 'Title_Millitary',\n       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Embarked_C', 'Embarked_Q',\n       'Embarked_S', 'FareBand_1', 'FareBand_2', 'FareBand_3', 'FareBand_4',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'AgeGroup_Child',\n       'AgeGroup_Young Adult', 'AgeGroup_Adult', 'AgeGroup_Older Adult',\n       'AgeGroup_Senior', 'Sex_female', 'Sex_male', 'Alone', 'Family_Survival',\n       'HadCabin','SibSp_0', 'SibSp_1', 'SibSp_2',\n       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n       'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', ]\n# create test and training data\npredictors = df_train.drop(['Survived', 'PassengerId'], axis=1)\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\ndata_to_predict=df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\ny=df_train['Survived']\nX=data_to_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\nprint('Data split')","execution_count":96,"outputs":[]},{"metadata":{"_uuid":"9d9f5554395d6ee37bb607ccd1e473bcda5a362a"},"cell_type":"markdown","source":"## Specify Architecture"},{"metadata":{"trusted":true,"_uuid":"6b2edb3e15586728fb9fab03313236a926ad7491"},"cell_type":"code","source":"# Import necessary modules\n\nfrom __future__ import print_function\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.utils import to_categorical\nprint('Modules imported')","execution_count":97,"outputs":[]},{"metadata":{"_uuid":"c07761c14099637f49a3286c065ed593eb59fedf"},"cell_type":"markdown","source":"### Check Data"},{"metadata":{"trusted":true,"_uuid":"8da017d910861ddeeeb58d0f7c9aaebe9b0dc758"},"cell_type":"code","source":"print('Training Data shape')\nprint(df_train.shape)\nprint(df_train.shape)\nprint('Test Data shape')\nprint(df_test.shape)\n\nprint(df_train.head())\nprint(df_test.head())","execution_count":98,"outputs":[]},{"metadata":{"_uuid":"8b80150ebbbb781586e1b80ef415642914eb5187"},"cell_type":"markdown","source":"# Build Keras Model"},{"metadata":{"trusted":true,"_uuid":"1c5db6fb6cef342d8f1996da062d3481997c2d53"},"cell_type":"code","source":"# create model\nmodel = Sequential()\nmodel.add(Dense(units=56, input_dim=X.shape[1], activation='selu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=27, activation='selu')) \nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1, activation='tanh'))\n\n# choose loss function and optimizing method\nmodel.compile(loss='mse', optimizer='sgd')\n\nprint('Keras Model Created')","execution_count":99,"outputs":[]},{"metadata":{"_uuid":"ac44114d898b508925ff4f8a2cb6de8e3846ca19"},"cell_type":"markdown","source":"## Fit Model"},{"metadata":{"trusted":true,"_uuid":"3c63f5c1d5f9f212f2e5181ea0d2bc518758c1b5","scrolled":true},"cell_type":"code","source":"model.fit(X.values, y.values, epochs=500, verbose=0)\nprint('Keras model fitted')","execution_count":100,"outputs":[]},{"metadata":{"_uuid":"81d22298e61d379b9d2af1f3b9ab5720eaaaaee6"},"cell_type":"markdown","source":"## Make Predictions"},{"metadata":{"trusted":true,"_uuid":"b6a2c57218460c0a52e1fe33f6df849389faa9fd"},"cell_type":"code","source":"print(df_test.columns)\nprint(X.columns)\n","execution_count":103,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae36998f5720029447e69a90334610325af47ff","scrolled":false},"cell_type":"code","source":"df_test=df_test.set_index('PassengerId')\np_survived = model.predict_classes(df_test)\nprint('Prediction Completed')","execution_count":104,"outputs":[]},{"metadata":{"_uuid":"adc1192d440b4d565409a7d0c73ef679b772c8dd"},"cell_type":"markdown","source":"## Add predictions to submission"},{"metadata":{"trusted":true,"_uuid":"cecb391f0f15e1531c6720e0a531e95a7c3cceec"},"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['PassengerId'] = df_test.index\nsubmission['Survived'] = p_survived\nprint('predictions added to submission')","execution_count":105,"outputs":[]},{"metadata":{"_uuid":"8b6a5f87a96819a37c619c0f51e27f30608d7aba"},"cell_type":"markdown","source":"### Check Submission"},{"metadata":{"trusted":true,"_uuid":"020d56d9175eda4156a8ebfe26564789ece25708"},"cell_type":"code","source":"print(submission.shape)\nprint(submission.head(10))","execution_count":106,"outputs":[]},{"metadata":{"_uuid":"c32eac800c2582fb70aea12c184c88de8ed2c984"},"cell_type":"markdown","source":"## Submit entry"},{"metadata":{"_uuid":"cf901bc389fbdcc3bca47fef7514909252ecf55f","trusted":true},"cell_type":"code","source":"submission.to_csv('DeepLearning03.csv', index=False)\nprint('csv created')","execution_count":107,"outputs":[]},{"metadata":{"_uuid":"5b7f20511180da3960180a4294395f433c4b245b"},"cell_type":"markdown","source":"********# Stage 5: Optimizing Deep Learning Model"},{"metadata":{"trusted":true,"_uuid":"5ee297565c6a8993bb9a8b055f0997b369d32e1c"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import StratifiedKFold\nseed=70\n\n# define 10-fold cross validation test harness\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncvscores = []\nfor train, test in kfold.split(X, y):\n    # create model\n    model = Sequential()\n    model.add(Dense(54, input_dim=X.shape[1], activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(54, activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(54, activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(1, activation='tanh'))\n    # Compile model\n    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n    # Fit the model\n    model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n    # evaluate the model\n    scores = model.evaluate(X_test, y_test, verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\nprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))","execution_count":116,"outputs":[]},{"metadata":{"_uuid":"d711c1560cecefd7357e68d49b0d40a7cf82ab14"},"cell_type":"markdown","source":"## activation\n\nrelu = 85.45\n\nsigmoid = 83.40\n\nsoftmax = 38\n\ntanh = 85.11\n\n## optimizers\n\nAdadelta= 84.85\n\nAdagrad= 85.52\n\nAdam=85.07\n\nAdamax = 85.19\n\nNadam= 84.51\n\nRMSprop= 85.04\n\nSGD= 85.07\n\n## loss\n\nbinary_crossentropy = 85.52\n\nmse = 85.11"},{"metadata":{"_uuid":"d1d6171c285222bba5c6eaffa076d34a44b3f17c"},"cell_type":"markdown","source":"## Make Predictions"},{"metadata":{"trusted":true,"_uuid":"361f38c31a2e5001f3e1ca9741e23c6d27ba5a04"},"cell_type":"code","source":"#df_test=df_test.set_index('PassengerId')\np_survived = model.predict_classes(df_test)\nprint('Prediction Completed')","execution_count":117,"outputs":[]},{"metadata":{"_uuid":"9c63843f877da7853cd95dac3763a930a8cf5619"},"cell_type":"markdown","source":"## Add prediction to submission"},{"metadata":{"trusted":true,"_uuid":"f4456131b3055c815b8fdeff0daa07225e187485"},"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['PassengerId'] = df_test.index\nsubmission['Survived'] = p_survived\nprint('predictions added to submission')","execution_count":118,"outputs":[]},{"metadata":{"_uuid":"9097d9f878bae6f030771c6664f27c963bf728ab"},"cell_type":"markdown","source":"## Check Submissions"},{"metadata":{"trusted":true,"_uuid":"aaf48400c09508fb65f08638e2d75d6dc6185265"},"cell_type":"code","source":"print(submission.shape)\nprint(submission.head(10))","execution_count":119,"outputs":[]},{"metadata":{"_uuid":"93af80f5b9a7cd0a056de73c105db9746d638b9e"},"cell_type":"markdown","source":"## Submit Predictions"},{"metadata":{"trusted":true,"_uuid":"ea929b7c7ef24501c2994bc6dbcf3c412b35f0de"},"cell_type":"code","source":"submission.to_csv('OptimisedDeepLearning04.csv', index=False)\nprint('csv created')","execution_count":120,"outputs":[]},{"metadata":{"_uuid":"388df8f919762360a0f276667293d69d492b2522"},"cell_type":"markdown","source":"# Summary\n\nI started this notebook with a set of engineered features and estimated missing values that I knew to be robust, having reliably scored over 0.80 with them in a previous entry. My first move was to convert these categorical features to columns that could be more readily  used for deep learning using keras. To ensure that these newly converted columns produced the same sort of results at the original Categorical dataset, in stage 2 and 3 I will produce a simple linear model and a hyper tuned model using the dataset that I will use with the Deep learning model in Stage 4.\n\nAs suspected the sklearn models archived a score of over 0.80 based on sparce dataset and scales data.\n\nThe first run of the deep learning model achived a score of 0.77."},{"metadata":{"_uuid":"d99363349ef9a3c5096e0f386ce27adafe31d3ae"},"cell_type":"markdown","source":"# Sources\n\nThis Notebook is largely based on my first Machine Learning Kernel, you can find the code at https://www.kaggle.com/davidcoxon/titanic-practice-by-davidcoxon \n\n## The feature engineering is based on the following kernels:\n\n### Anisotropic\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook\n\n### bisaria\nhttps://www.kaggle.com/bisaria/titanic-lasso-ridge-implementation/code\n\n### CalebCastleberry\nhttps://www.kaggle.com/ccastleberry/titanic-cabin-features\n\n### Henrique Mello\nhttps://www.kaggle.com/hrmello/introduction-to-data-exploration-using-seaborn/notebook\n\n### Nadin Tamer\nhttps://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner/notebook\n\n### Omar El Gabry\nhttps://www.kaggle.com/omarelgabry/a-journey-through-titanic?scriptVersionId=447802/notebook\n\n### Oscar Takeshita\nhttps://www.kaggle.com/pliptor/divide-and-conquer-0-82296/code\n\n### Sina\nhttps://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=566580\n\n### S.Xu\nhttps://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever\n\n## The deep Learning modeling is based on the following kernels:\n\n### Alan Wong\nhttps://www.kaggle.com/alan1229/titanic-neural-network-using-keras\n\n### CStahl\nhttps://www.kaggle.com/cstahl12/titanic-with-keras\n\n## Other Sources\n\nhttps://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\nhttps://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"18a5ec54578d460d49bcaa62a329f98212df0e4a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}