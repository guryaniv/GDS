{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import warnings\n", "from collections import Counter\n", "from sklearn.feature_selection import mutual_info_classif\n", "from sklearn.externals import joblib\n", "warnings.filterwarnings('ignore')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ab35bed0-6a03-42e2-9c01-fd5f9a0fea67", "_uuid": "d730821b143b165cd00109f50bc717075943aa60"}}, {"execution_count": null, "source": ["train = pd.read_csv('../input/train.csv')\n", "train.head()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fcfa2dd6-b2d0-4cc8-8131-78a791fef035", "_uuid": "260a2cacb954913fd95d31d375bb23d306b1aaa7"}}, {"execution_count": null, "source": ["round(0)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "c0f1843f966d33d02b8809c6558777fdb2551c8b", "_cell_guid": "4fa09871-ee72-4bef-a4b3-1e60f411ef31"}}, {"execution_count": null, "source": ["test = pd.read_csv('../input/test.csv')\n", "test.tail()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "045beb2c-09a5-4118-970f-9cc6d823ce0d", "_uuid": "b491a5e445d0ab4c57ed76e6ce2c3663ee8b0ec3"}}, {"execution_count": null, "source": ["print(train.shape, test.shape)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ed7c0f25-852e-49f2-897e-f9b5b7702f70", "_uuid": "2261adec335cadbe8abaa8d5ba98d00bf68f036a"}}, {"metadata": {"_cell_guid": "f05a6b73-87da-4524-8606-960c500d5349", "_uuid": "d688d466d2f0b37f4a5653ab3f9ccc8d7becd3fd"}, "source": ["## Construction of metadata"], "cell_type": "markdown"}, {"execution_count": null, "source": ["data = []\n", "for col in train.columns:\n", "    # Defining the role\n", "    if col == 'target' or col == 'id':\n", "        role = col\n", "    else:\n", "        role = 'input'\n", "         \n", "    # Defining the level\n", "    if 'bin' in col or col == 'target':\n", "        level = 'binary'\n", "    elif 'cat' in col or col == 'id':\n", "        level = 'nominal'\n", "    elif train[col].dtype == float:\n", "        level = 'interval'\n", "    elif train[col].dtype == int:\n", "        level = 'ordinal'\n", "        \n", "    # Initialize keep to True for all variables except for id\n", "    keep = True\n", "    if col == 'id':\n", "        keep = False\n", "    \n", "    # Defining the data type \n", "    dtype = train[col].dtype\n", "    \n", "    # Creating a Dict that contains all the metadata for the variable\n", "    col_dict = {'col_name': col, 'role': role, 'level': level, 'keep': keep, 'dtype': dtype}\n", "    data.append(col_dict)\n", "    \n", "meta = pd.DataFrame(data, columns=['col_name', 'role', 'level', 'keep', 'dtype'])\n", "meta.set_index('col_name', inplace=True)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e64a1c5f-8aae-44ca-97cd-733808ec0cbf", "_uuid": "3412a9847f57233da9bacdbc89cd89d21ab9b8b6", "collapsed": true}}, {"execution_count": null, "source": ["bin_cols = meta[(meta.level=='binary') & meta.keep].index\n", "train[bin_cols].describe()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0228a322-55e4-455c-a658-cbdcd712f4cc", "_uuid": "5ac0580f79a56117859dab7847ddbfadb2690d59", "collapsed": true}}, {"metadata": {"_cell_guid": "76804373-8d11-4add-8c68-49645be9fdae", "_uuid": "490feacc1dc052e9157570e02b798cfd8261e8c2"}, "source": ["## Balance training data distribution"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from sklearn.utils import shuffle\n", "# desired_apriori=0.10\n", "\n", "# # Get the indices per target value\n", "# idx_0 = train[train.target == 0].index\n", "# idx_1 = train[train.target == 1].index\n", "\n", "# # Get original number of records per target value\n", "# nb_0 = len(train.loc[idx_0])\n", "# nb_1 = len(train.loc[idx_1])\n", "\n", "# undersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\n", "# undersampling_nb_0 = int(nb_0 * undersampling_rate)\n", "# print('Number of training samples with target == 0 after undersampling', undersampling_nb_0)\n", "\n", "# undersampled_idx = shuffle(idx_0, random_state=77, n_samples=undersampling_nb_0)\n", "# idx_list = list(undersampled_idx) + list(idx_1)\n", "# balanced_train = train.loc[idx_list].reset_index(drop=True)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "3527485f-bac7-464b-abac-3e55ee241ebe", "_uuid": "66362c23b2112d14bc806a34a90ec8b4d7786e80", "collapsed": true}}, {"metadata": {"_cell_guid": "54739a03-45d1-4d32-bc48-68163543aaa7", "_uuid": "e81d6f87eb3b1d6174cb050f2e15943da01d441d"}, "source": ["## 1. Data quality checks"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b8905d9b-6300-4553-bb53-f6b0a2f401aa", "_uuid": "6f16d1891d9bcc92216e04b75a247f9774a67802"}, "source": ["### Null or missing values check"], "cell_type": "markdown"}, {"execution_count": null, "source": ["train.isnull().any().any()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "66e7b8e6-eec4-4a26-ada7-6761824774c8", "_uuid": "648468a6fcd4b0359bd49228eb760854777f8c0c", "collapsed": true}}, {"execution_count": null, "source": ["ms_cols = []\n", "train_copy = train\n", "train_copy = train_copy.replace(-1, np.NaN)\n", "for col in train_copy.columns:\n", "    ms_nb = train_copy[col].isnull().sum()\n", "    if ms_nb > 0:\n", "        ms_cols.append(col)\n", "        print('Column {} has {} records ({:.2%}) with missing values'.format(col, ms_nb, ms_nb/train_copy.shape[0]))"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "68497924-128b-4ceb-9104-bf8ba5e3a710", "_uuid": "d3898e6b49c8fc67c98dc53f2447800dd6d0f44d", "collapsed": true}}, {"execution_count": null, "source": ["# import missingno as msno\n", "# msno.matrix(df=train_copy.iloc[:, 2:40], figsize=(20, 14),\n", "#             color=(0.42, 0.1, 0.05))"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1b54d19b-92a9-4043-a159-94edb81f9c98", "_uuid": "b029c5781a3d5ef318dba6aaef31d2750e78b440", "collapsed": true}}, {"metadata": {"_cell_guid": "792bd630-91bb-48a7-9b67-8f425df202a8", "_uuid": "2806ab716abf40b2fdb2c91c79b452cc143cab00"}, "source": ["### Dropout columns with too many missing values and imputing"], "cell_type": "markdown"}, {"execution_count": null, "source": ["from sklearn.preprocessing import Imputer\n", "\n", "drop_cols = ['ps_car_03_cat', 'ps_car_05_cat']\n", "real_train = train.drop(drop_cols, axis=1)\n", "real_test = test.drop(drop_cols, axis=1)\n", "meta.loc[drop_cols, 'keep'] = False\n", "\n", "mean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\n", "mode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n", "\n", "# Imputing training data\n", "real_train['ps_reg_03'] = mean_imp.fit_transform(X=real_train[['ps_reg_03']]).ravel()\n", "real_train['ps_car_11'] = mode_imp.fit_transform(X=real_train[['ps_car_11']]).ravel()\n", "real_train['ps_car_12'] = mode_imp.fit_transform(X=real_train[['ps_car_12']]).ravel()\n", "real_train['ps_car_14'] = mean_imp.fit_transform(X=real_train[['ps_car_14']]).ravel()\n", "\n", "# Imputing test data\n", "real_test['ps_reg_03'] = mean_imp.fit_transform(X=real_test[['ps_reg_03']]).ravel()\n", "real_test['ps_car_11'] = mode_imp.fit_transform(X=real_test[['ps_car_11']]).ravel()\n", "real_test['ps_car_12'] = mode_imp.fit_transform(X=real_test[['ps_car_12']]).ravel()\n", "real_test['ps_car_14'] = mean_imp.fit_transform(X=real_test[['ps_car_14']]).ravel()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "39e5272e-b5a7-42c9-89e8-8cc8599cd5e0", "_uuid": "7955264e845e63cad7742ddfb9b9588d2ad7b4ed", "collapsed": true}}, {"metadata": {"_cell_guid": "3071c022-232f-44e3-80de-2e16f046ea30", "_uuid": "786914b55e6410d50229340522a9f50d57af2465"}, "source": ["We didn't impute the missing values in the categorical columns, instead, we kept it as a seperate category. As later on we can see that customers with a missing value in these variables appear to have a much higher possibility to file an insurance claim (a good takeaway for the future data preproccessing method)"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# Replace missing values with 999\n", "# real_train1 = real_train.replace(-1, 999)\n", "# real_test1 = real_test.replace(-1, 999)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "87d99eb5-847d-41ff-907c-0e39cd113a86", "_uuid": "123e2f24afb7de87ef63a86bf208b35517c7e9b9", "collapsed": true}}, {"metadata": {"_cell_guid": "dcd2601b-4634-4168-9819-2429b99a9783", "_uuid": "b4a2f2a221039e30d3377cce888f77a5a82ce90e"}, "source": ["## Check the cardinality of categorical columns"], "cell_type": "markdown"}, {"execution_count": null, "source": ["cat_cols = meta[(meta.level=='nominal') & meta.keep].index\n", "\n", "for col in cat_cols:\n", "    distinct_values = real_train[col].value_counts().shape[0]\n", "    print('Categorical column {} has {} distinct values'.format(col, distinct_values))"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "45cc4f21-8dfc-49dc-ba1d-153325e7e816", "_uuid": "5bdcadf5a913fb29d1e5d7c71c37f74971641dde", "collapsed": true}}, {"metadata": {"_cell_guid": "9668648c-f966-4318-8580-21f3edcc67cc", "_uuid": "5bf86161ec4c905f814038f5717f46d7f78eb8f5"}, "source": ["## Handling column \"ps_car_11_cat\" specifically as it has too many cardinality"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# Script by https://www.kaggle.com/ogrellier\n", "# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n", "def add_noise(series, noise_level):\n", "    return series * (1 + noise_level * np.random.randn(len(series)))\n", "\n", "def target_encode(trn_series=None, \n", "                  tst_series=None, \n", "                  target=None, \n", "                  min_samples_leaf=1, \n", "                  smoothing=1,\n", "                  noise_level=0):\n", "    \"\"\"\n", "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n", "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n", "    trn_series : training categorical feature as a pd.Series\n", "    tst_series : test categorical feature as a pd.Series\n", "    target : target data as a pd.Series\n", "    min_samples_leaf (int) : minimum samples to take category average into account\n", "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n", "    \"\"\" \n", "    assert len(trn_series) == len(target)\n", "    assert trn_series.name == tst_series.name\n", "    temp = pd.concat([trn_series, target], axis=1)\n", "    # Compute target mean \n", "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n", "    # Compute smoothing\n", "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n", "    # Apply average function to all target data\n", "    prior = target.mean()\n", "    # The bigger the count the less full_avg is taken into account\n", "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n", "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n", "    # Apply averages to trn and tst series\n", "    ft_trn_series = pd.merge(\n", "        trn_series.to_frame(trn_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=trn_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_trn_series.index = trn_series.index \n", "    ft_tst_series = pd.merge(\n", "        tst_series.to_frame(tst_series.name),\n", "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n", "        on=tst_series.name,\n", "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n", "    # pd.merge does not keep the index so restore it\n", "    ft_tst_series.index = tst_series.index\n", "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "617b7167-2efc-4f07-9643-b0e213f0ded4", "_uuid": "12583fb629f3e5491a9a936391fcf22985562a6a", "collapsed": true}}, {"execution_count": null, "source": ["train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n", "                             test[\"ps_car_11_cat\"], \n", "                             target=train.target, \n", "                             min_samples_leaf=100,\n", "                             smoothing=10,\n", "                             noise_level=0.01)\n", "    \n", "real_train['ps_car_11_cat_te'] = train_encoded\n", "real_train.drop('ps_car_11_cat', axis=1, inplace=True)\n", "meta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\n", "real_test['ps_car_11_cat_te'] = test_encoded\n", "real_test.drop('ps_car_11_cat', axis=1, inplace=True)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fe6b5881-199f-4843-a4e5-bd6f4685de32", "_uuid": "82dc8a18e2fd710c34acd01bd80c5039c4a3bcdc", "collapsed": true}}, {"metadata": {"_cell_guid": "4bb19cad-7124-4860-9a71-cf48d654c017", "_uuid": "50f5adf5562e6468d26d2ea783b8fdf389bb3f7a"}, "source": ["## Dropout calculated columns "], "cell_type": "markdown"}, {"execution_count": null, "source": ["# calc_cols = [col for col in train.columns if 'calc' in col]\n", "\n", "# ###=========== A data exploration of calc features from armamut ==============###\n", "# # Script: https://www.kaggle.com/armamut/ps-calc-15-bin-ps-calc-20-bin\n", "\n", "# # Columns -> binary decoded.\n", "\n", "# tmp  =real_train['ps_calc_15_bin'] * 32 + real_train['ps_calc_16_bin'] * 16 + real_train['ps_calc_17_bin'] * 8\n", "# tmp += real_train['ps_calc_18_bin'] * 4 + real_train['ps_calc_19_bin'] * 2 + real_train['ps_calc_20_bin'] * 1\n", "\n", "# tmp2 = [5, 22, 9, 32, 13, 38, 20, 47, 2, 19, 8, 30, 10, 35, 17, 45, 1,\n", "#         15, 4, 24, 7, 29, 14, 40, 0, 12, 3, 21, 6, 26, 11, 36, 27, 52,\n", "#         37, 57, 42, 60, 51, 63, 23, 49, 34, 56, 39, 59, 48, 62, 18, 46,\n", "#         28, 53, 33, 55, 44, 61, 16, 43, 25, 50, 31, 54, 41, 58]\n", "# tmp2 = pd.Series(tmp2)\n", "\n", "# real_train['ps_calc_15_16_17_18_19_20'] = tmp.map(tmp2)\n", "# real_test['ps_calc_15_16_17_18_19_20'] = tmp.map(tmp2)\n", "# # You may now drop the others peacefully.\n", "# # real_train_nocalc = real_train.drop(['ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin',\n", "# #               'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin'], axis=1, inplace=False)\n", "# ###============================================================================###\n", "\n", "# real_train_nocalc = real_train.drop(calc_cols, axis=1)\n", "# real_test_nocalc = real_test.drop(calc_cols, axis=1)\n", "# real_train_nocalc.shape"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "25e4573e-d866-4444-8012-fcbe0e8b4c78", "_uuid": "a18617fbe21bdaa8751adaa77bb93ca675353d2c", "collapsed": true}}, {"execution_count": null, "source": ["calc_cols = [col for col in train.columns if 'calc' in col]\n", "\n", "real_train_nocalc = real_train.drop(calc_cols, axis=1)\n", "real_test_nocalc = real_test.drop(calc_cols, axis=1)\n", "real_train_nocalc.shape"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9ea02492-b18a-4493-a016-fd30095f5d1c", "_uuid": "d2c7d114b267814f27e4b21d2b092d0a4b1cc17e", "collapsed": true}}, {"metadata": {"_cell_guid": "edbb0044-a8f0-411d-90f4-9a8b0903a415", "_uuid": "ce1c0328625140ebc7064484c188ce20c36c212a"}, "source": ["### Target variable inspection"], "cell_type": "markdown"}, {"execution_count": null, "source": ["def plotTargetDistribution(dataset):\n", "    data = [go.Bar(\n", "                x = dataset['target'].value_counts().index.values,\n", "                y = dataset['target'].value_counts().values,\n", "                text = 'Distribution of target variable')]\n", "    layout = go.Layout(\n", "                title = 'Distribution of target variable')\n", "    fig = go.Figure(data=data, layout=layout)\n", "    py.iplot(fig, filename='basic-bar')\n", "plotTargetDistribution(real_train)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ddd1a52a-5634-4610-8559-80d6bbb4cb5a", "_uuid": "3b7ec2e5bf5b37ebbdad834ed0fb5acd1a634881", "collapsed": true}}, {"metadata": {"_cell_guid": "f8d49c51-4891-4166-98b8-5f41e0c807d8", "_uuid": "9e1f2686bceabbdc9d39d82a10eb87b9dd638d68"}, "source": ["### Typedata check"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# Counter(real_train.dtypes.values)\n", "# train_float = real_train.select_dtypes(include=['float64'])\n", "# train_int = real_train.select_dtypes(include=['int64'])"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "db7c6450-2bf9-411b-8a0d-975c87aa1754", "_uuid": "c3beade78af75d2786efafb672ac8cb74634b2c6", "collapsed": true}}, {"metadata": {"_cell_guid": "7d1a3a53-0031-495b-9df1-363425d6c7af", "_uuid": "d97bb78ea21d06a4b98ae870658fb2316a122d5f"}, "source": ["## 2. Correlation plots"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "188e0563-11f4-47e6-b8be-362cbdfc1e95", "_uuid": "5fb2ebb3460cc8e39d4b8920ab6b2c4994acd921"}, "source": ["### Correlation of float features"], "cell_type": "markdown"}, {"execution_count": null, "source": ["float_int_cols = meta[((meta.level == 'interval') | (meta.level == 'ordinal')) & meta.keep].index\n", "float_int_train = real_train[float_int_cols]\n", "\n", "colormap = plt.cm.cubehelix_r\n", "plt.figure(figsize=(16,12))\n", "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "sns.heatmap(float_int_train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "bd9fdebe-72d8-421d-91b8-a67f21d4a8e1", "_uuid": "fb4820c9fd1f831cd16e9f57803a69f79ae4e537", "collapsed": true}}, {"metadata": {"_cell_guid": "0f156d1f-4a94-4a5b-b36e-cf8d643857a0", "_uuid": "2fb8a8d4af4b04e0770b14103ba3bb4467bc253d"}, "source": ["Column pairs with high correlation:\n", "1. ps_reg_01 and ps_reg_02: 0.47\n", "2. ps_reg_02 and ps_reg_03: 0.7\n", "3. ps_car_12 and ps_car_13: 0.67\n", "4. ps_car_12 and ps_car_14: 0.58\n", "5. ps_car_13 and ps_car_14: 0.44\n", "6. ps_car_13 and ps_car_15: 0.52\n", "\n", "Maybe we need to do some domensionality reduction by using PCA..."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "46ee0444-63eb-45e4-a88f-9cf4a034488a", "_uuid": "2464587d6836e88b5ff5de220f17fd5682a8a83a"}, "source": ["### Dropout float features with high correlation to another column"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "585182bb-0ac4-4686-8087-7c58c2a10ae0", "_uuid": "45c0ccf75164e33a05d5c20a34c00c197f6a41e4"}, "source": ["Within the two groups of highest correlation features, there is one column \"ps_car_13\" shared by both, so we will throw that column directly as to preserve as much information"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# realTrain = realTrain.drop('ps_car_13', axis=1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "85a6f0d4-1bf0-4b5d-9fd0-d5ede1b90a1a", "_uuid": "6067d338f9b64671ba86a438e5d77529146c45e4", "collapsed": true}}, {"metadata": {"_cell_guid": "98d83d12-2b3e-462f-a309-9fc83fab92e0", "_uuid": "e909d848621c745c0a364f20291a3f1c74abd9cc"}, "source": ["## 3. Binary features inspection"], "cell_type": "markdown"}, {"execution_count": null, "source": ["bin_cols = meta[(meta.level == 'binary') & (meta.role != 'target') & meta.keep].index\n", "ones_list = []\n", "zeros_list = []\n", "for col in bin_cols:\n", "    zeros_nb = (real_train[col] == 0).sum()\n", "    ones_nb = real_train.shape[0] - zeros_nb\n", "    ones_list.append(ones_nb)\n", "    zeros_list.append(zeros_nb)\n", "    print('Binary column {} has {} records ({:.2%}) with value zero'.format(col, zeros_nb, zeros_nb/real_train.shape[0]))"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "855f74f7-0b1e-4737-93d8-8713a8ecd2ab", "_uuid": "d7b0976b37b1ef747cab080caee7b42b36b4b9e8", "collapsed": true}}, {"execution_count": null, "source": ["trace0 = go.Bar(x=bin_cols, y=zeros_list, name='Zeros count')\n", "trace1 = go.Bar(x=bin_cols, y=ones_list, name='Ones count')\n", "\n", "data = [trace0, trace1]\n", "layout = go.Layout(barmode='stack', title='Count of zeros and ones')\n", "\n", "fig = go.Figure(data=data, layout=layout)\n", "py.iplot(fig, filename='stacked-bar')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "34c0f02b-c549-4c45-84a1-8204d5fab534", "_uuid": "4ccef2064afaf22381a0e3e734e99e60353eb67a", "scrolled": false, "collapsed": true}}, {"metadata": {"_cell_guid": "d07bd035-e7c6-4179-be85-96ec714d4ea4", "_uuid": "e6b67b3f32dbda58595eca4484c3976b77d08f74"}, "source": ["### Dropout binary features dominated by zeros"], "cell_type": "markdown"}, {"execution_count": null, "source": ["imbalanced_cols = ['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_13_bin']\n", "real_train = real_train.drop(imbalanced_cols, axis=1)\n", "real_test = real_test.drop(imbalanced_cols, axis=1)\n", "\n", "real_train_nocalc = real_train_nocalc.drop(imbalanced_cols, axis=1)\n", "real_test_nocalc = real_test_nocalc.drop(imbalanced_cols, axis=1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5809bd01-883d-463f-9c29-d0d408ec6fc8", "_uuid": "eb412b18489591937cc66aea2c83129ff0c339eb", "collapsed": true}}, {"execution_count": null, "source": ["# plotTargetDistribution(train_data)\n", "# plotTargetDistribution(val_data)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "29fe315f-eb58-4a99-b76c-aaf3e9c23a9c", "_uuid": "3a70474ccf5c0376b1272fcbf86b88e747ef7dbc", "collapsed": true}}, {"metadata": {"_cell_guid": "ad8b7253-20e6-4db2-be8d-8e1d9d4e7510", "_uuid": "d9199fc562b2efc86aeaf33f025fc53b62458d76"}, "source": ["## Feature engineering"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b528732a-12e8-490d-b349-e0bae3f1f98f", "_uuid": "4ff7d0c8608e486538d37301228e43a9ea0192d8"}, "source": ["### Create dummy variables"], "cell_type": "markdown"}, {"execution_count": null, "source": ["cat_cols = meta[(meta.level=='nominal') & meta.keep].index\n", "# cat_cols = cat_cols.drop('ps_car_11_cat')\n", "\n", "print('Nb of columns in train data before dummification: {}'.format(real_train.shape[1]))\n", "real_train = pd.get_dummies(data=real_train, columns=cat_cols, drop_first=True)\n", "print('Nb of columns in train data before dummification: {}'.format(real_train.shape[1]))\n", "\n", "print('Nb of columns in test data before dummification: {}'.format(real_test.shape[1]))\n", "real_test = pd.get_dummies(data=real_test, columns=cat_cols, drop_first=True)\n", "print('Nb of columns in test data after dummification: {}'.format(real_test.shape[1]))\n", "\n", "real_train_nocalc = pd.get_dummies(data=real_train_nocalc, columns=cat_cols, drop_first=True)\n", "real_test_nocalc = pd.get_dummies(data=real_test_nocalc, columns=cat_cols, drop_first=True)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1342afd7-9099-4092-8199-a34eba2220ce", "_uuid": "700124d1ece349d4c2a2df7a930715b835ed8009", "collapsed": true}}, {"metadata": {"_cell_guid": "f949412d-3109-4829-a7e4-02e477e5a5d5", "_uuid": "3a6c2063aaae14d7a78889859b4c0badb454d5a0"}, "source": ["### Create interaction variables"], "cell_type": "markdown"}, {"execution_count": null, "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "float_cols = meta[(meta.level=='interval') & meta.keep].index\n", "\n", "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n", "interactions = pd.DataFrame(data=poly.fit_transform(real_train[float_cols]), columns=poly.get_feature_names(float_cols))\n", "interactions.drop(float_cols, axis=1, inplace=True)\n", "interacted_train = pd.concat(objs=[real_train, interactions], axis=1)\n", "interacted_test = pd.concat(objs=[real_test, interactions], axis=1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "23777eab-f010-40e5-b0ab-29cbfe724332", "_uuid": "0b8eb8780c7f42d8a835579bd13979a95339d108", "collapsed": true}}, {"metadata": {"_cell_guid": "bc8c7c8d-10ae-4512-9324-300431c7b663", "_uuid": "4bbf8d290483c00a100e0282ff6eeccc7f806e4a"}, "source": ["## 4. Learning models and predictions"], "cell_type": "markdown"}, {"execution_count": null, "source": ["from sklearn.model_selection import train_test_split\n", "train_data, val_data = train_test_split(real_train, train_size=0.9, random_state=77)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "7fe5aa5c-ec18-4690-8149-365e4bcb7f56", "_uuid": "00fcf7c2d97ae8e4bec61d74ea57587f59760a9b", "collapsed": true}}, {"execution_count": null, "source": ["# _, compressed_train = train_test_split(real_train_nocalc, train_size=0.95, random_state=777)\n", "# train_data, val_data = train_test_split(compressed_train, train_size=0.8, random_state=777)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "ccde133e-a6a3-4207-8bd4-bb72c7d472e9", "_uuid": "bc06e520d9ba3744364ae22af1ec6f7014959c68", "collapsed": true}}, {"metadata": {"_cell_guid": "cc8b72be-09ef-4503-9a07-b96a2d09371e", "_uuid": "9bd392323e3c00a0d8614be8b8100d155eef7bef", "collapsed": true}, "source": ["### Feature importance via random forest"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from sklearn.ensemble import RandomForestClassifier\n", "# rf = RandomForestClassifier(n_estimators=300, min_samples_leaf=4,\n", "#                             n_jobs=-1, random_state=77, max_features=1, class_weight={0:1, 1:700})\n", "# rf.fit(X=train_data.drop(['id', 'target'], axis=1), y=train_data['target'])\n", "# features = train_data.drop(['id', 'target'], axis=1).columns.values"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5e11967f-0ebb-4c2b-b969-9d1850e75ec0", "_uuid": "2e7a368f1536b0f3c6c7fc0c48d2afca89a13eff", "collapsed": true}}, {"execution_count": null, "source": ["# predVal = rf.predict(X=val_data.drop(['id', 'target'], axis=1))"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9e46647a-40fb-49ae-a5f2-3ea844c9c65d", "_uuid": "4183b20077236e730b42fb3144345964cdd1c625", "collapsed": true}}, {"metadata": {"_cell_guid": "e6e89b42-f735-4d13-bbd8-1422d65b3854", "_uuid": "4cd0c7e76bf863337cba42aae5588e671cdac6c6"}, "source": ["### Visualisation of features importances"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), reverse=False)))\n", "# trace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale='Viridis'), \n", "#                name='Random Forest feature importance', orientation='h')\n", "# layout = dict(title='Barplot of reature importance', width=900, height=2000, \n", "#              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\n", "# fig = go.Figure(data=[trace])\n", "# fig['layout'].update(layout)\n", "# py.iplot(figure_or_data=fig, filename='Barplots')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "38ecfca8-2ab8-488a-b6fc-99cebd29969b", "_uuid": "ea87f2916558a06235b297209f30535a2e903964", "collapsed": true}}, {"metadata": {"_cell_guid": "c6be4b65-f043-4ee8-aa16-ae2e0c3ad4ed", "_uuid": "876ae56ffdcaf7942495833b672c1a32b63d19f2"}, "source": ["### Select features with a given threshold feature importance with SelectFromModel method"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from sklearn.feature_selection import SelectFromModel\n", "# sfm = SelectFromModel(estimator=rf, threshold=0.001, prefit=True)\n", "# sfm.transform(X=train_data.drop(['id', 'target'], axis=1))\n", "\n", "# train_data = train_data.iloc[:, sfm.get_support(indices=True)]\n", "# val_data = val_data.iloc[:, sfm.get_support(indices=True)]"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "10c3ebc6-4f70-4e31-b424-104bf369580d", "_uuid": "ebb84bc85eee0bc24189297051ad46bcc8fbc343", "collapsed": true}}, {"metadata": {"_cell_guid": "ecbb7826-46f8-4c24-98fb-0f1c4477e6fd", "_uuid": "49effd71e3e85cb1546f01c0de02f7984c760e2f"}, "source": ["### Training a neural network"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from sklearn.neural_network import MLPClassifier\n", "# nnet = MLPClassifier(hidden_layer_sizes=(7, 7, 7), max_iter=250, batch_size=700, \n", "#                      random_state=777, verbose=True, tol=1e-7)\n", "# nnet.fit(X=train_data.drop(['id', 'target'], axis=1), y=train_data[['target']])"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "799e8d4a-5070-44fd-a773-fd4e7886c4a6", "_uuid": "7a94e126fc76830c746252563e55df9ac7254cc7", "collapsed": true}}, {"metadata": {"_cell_guid": "17d87afc-9699-453f-a0e4-b0c95f23e718", "_uuid": "548d0c8fda95b34551353943bbaa6f9aa7fb59d0"}, "source": ["### Introduce imblearn package to hopefully resolve skewed data problem"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from imblearn.ensemble import BalanceCascade\n", "# bc = BalanceCascade(random_state=7)\n", "# X_resampled, y_resampled = bc.fit_sample(realTrain.drop(['id', 'target'], axis=1), \n", "#                                          realTrain[['target']])"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "315eba3e-74de-43b8-9413-f140f56b317d", "_uuid": "136aa1ff7d5f52a9343affb3ed52c34f27b399c9", "collapsed": true}}, {"metadata": {"_cell_guid": "593193e5-4ab3-4c55-a138-8d7e7fd522a4", "_uuid": "6b0cdf38a1f07b5f3897672c94d60c1dbe8831ad"}, "source": ["## Compute gini coefficient"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "def gini_xgb(preds, d_train):\n", "    targets = d_train.get_label()\n", "    gini_score = gini_normalized(targets, preds)\n", "    return [('gini', gini_score)]\n", "    "], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a56c9f34-c714-42b0-80f7-e1d833e1263c", "_uuid": "6f9c8c7af09ce879cf11ec804914624782dcca5d", "collapsed": true}}, {"metadata": {"_cell_guid": "64d90e12-6e92-474e-ad8a-62c4aad3b6a2", "_uuid": "779adfc6cb7bf9c0d14bca7e4c418bf34160bed1", "collapsed": true}, "source": ["\n", "## XGBoost"], "cell_type": "markdown"}, {"execution_count": null, "source": ["import xgboost as xgb\n", "d_train = xgb.DMatrix(real_train.drop(['id', 'target'], axis=1), real_train[['target']])\n", "# d_val = xgb.DMatrix(val_data.drop(['id', 'target'], axis=1), val_data[['target']])\n", "d_test = xgb.DMatrix(real_test.drop(['id'], axis=1))\n", "\n", "n_splits = 5\n", "n_estimators = 7\n", "folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=77) \n", "\n", "params = {\n", "        'objective': 'binary:logistic', \n", "        'eta': 0.015,\n", "        'eval_metric': 'auc', \n", "        'max_depth': 6, \n", "        'min_child_weight': 10,\n", "        'gamma': 1, \n", "        'reg_lambda': 0.3, \n", "        'reg_alpha': 0.07, \n", "        'subsample': 0.8,\n", "        'colsample_bytree': 0.8,\n", "        'silent': False, \n", "        }\n", "\n", "# watchlist = [(d_train, 'train'), (d_val, 'valid')]\n", "\n", "xgbCV = xgboost.cv(params=params, dtrain=d_train, stratified=True, num_boost_round=10000, feval=gini_xgb,\n", "                   early_stopping_rounds=100, maximize=True, verbose_eval=5, nfold=3)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e45ff0d0-3f16-4984-869f-0406130db0cc", "_uuid": "8490ca7244881ea60b91a5b8af907d38359e5009", "collapsed": true}}, {"execution_count": null, "source": ["from xgboost import XGBClassifier\n", "from sklearn.model_selection import StratifiedKFold\n", "import gc\n", "from numba import jit\n", "from sklearn.preprocessing import LabelEncoder\n", "import time \n", "\n", "@jit\n", "def eval_gini(y_true, y_prob):\n", "    \"\"\"\n", "    Original author CPMP : https://www.kaggle.com/cpmpml\n", "    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n", "    \"\"\"\n", "    y_true = np.asarray(y_true)\n", "    y_true = y_true[np.argsort(y_prob)]\n", "    ntrue = 0\n", "    gini = 0\n", "    delta = 0\n", "    n = len(y_true)\n", "    for i in range(n-1, -1, -1):\n", "        y_i = y_true[i]\n", "        ntrue += y_i\n", "        gini += y_i * delta\n", "        delta += 1 - y_i\n", "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n", "    return gini\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = eval_gini(labels, preds)\n", "    return [('gini', gini_score)]\n", "\n", "trn_df = real_train\n", "sub_df = real_test\n", "\n", "target = trn_df.target\n", "del trn_df[\"target\"]\n", "\n", "n_splits = 5\n", "n_estimators = 7\n", "folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=77) \n", "imp_df = np.zeros((len(trn_df.columns), n_splits))\n", "xgb_evals = np.zeros((n_estimators, n_splits))\n", "oof = np.empty(len(trn_df))\n", "sub_preds = np.zeros(len(sub_df))\n", "increase = True\n", "np.random.seed(0)\n", "\n", "print('Start fitting: ')\n", "for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n", "    trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n", "    val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n", "\n", "    clf = XGBClassifier(n_estimators=n_estimators,\n", "                        max_depth=4,\n", "                        objective=\"binary:logistic\",\n", "                        learning_rate=0.1, \n", "                        subsample=.8, \n", "                        colsample_bytree=.8,\n", "                        gamma=1,\n", "                        reg_alpha=0,\n", "                        reg_lambda=1,\n", "                      #  min_child_weight=10, \n", "                        nthread=2)\n", "    # Upsample during cross validation to avoid having the same samples\n", "    # in both train and validation sets\n", "    # Validation set is not up-sampled to monitor overfitting\n", "    if increase:\n", "        # Get positive examples\n", "        pos = pd.Series(trn_tgt == 1)\n", "        # Add positive examples\n", "        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n", "        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n", "        # Shuffle data\n", "        idx = np.arange(len(trn_dat))\n", "        np.random.shuffle(idx)\n", "        trn_dat = trn_dat.iloc[idx]\n", "        trn_tgt = trn_tgt.iloc[idx]\n", "    \n", "    print('{}th fold'.format(fold_))\n", "    clf.fit(trn_dat, trn_tgt, \n", "            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n", "            eval_metric=gini_xgb,\n", "            early_stopping_rounds=None,\n", "            verbose=False)\n", "            \n", "    imp_df[:, fold_] = clf.feature_importances_\n", "    oof[val_idx] = clf.predict_proba(val_dat)[:, 1]\n", "    \n", "    # Find best round for validation set\n", "    xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n", "    best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n", "    \n", "    # Display results\n", "    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\" \n", "          % (fold_ + 1, \n", "             eval_gini(val_tgt, oof[val_idx]),\n", "             n_estimators,\n", "             xgb_evals[best_round, fold_],\n", "             best_round))\n", "             \n", "    # Update submission\n", "    sub_preds += clf.predict_proba(sub_df)[:, 1] / n_splits \n", "          \n", "print(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n", "\n", "# Compute mean score and std\n", "mean_eval = np.mean(xgb_evals, axis=1)\n", "std_eval = np.std(xgb_evals, axis=1)\n", "best_round = np.argsort(mean_eval)[::-1][0]\n", "\n", "print(\"Best mean score : %.6f + %.6f @%4d\"\n", "      % (mean_eval[best_round], std_eval[best_round], best_round))\n", "    \n", "importances = imp_df.mean(axis=1)\n", "for i, imp in enumerate(importances):\n", "    print(\"%-20s : %10.4f\" % (trn_df.columns[i], imp))\n", "    \n", "sub_df[\"target\"] = sub_preds\n", "\n", "sub_df[[\"target\"]].to_csv(\"submission_20fold.csv\", index=True, float_format=\"%.9f\")"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "55e1f14a-5c61-49b1-8100-bc4e38fabb62", "_uuid": "eb30249859f28708bf849b648d3338adb6ac5318", "collapsed": true}}, {"execution_count": null, "source": ["# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n", "# from xgboost import XGBClassifier\n", "\n", "# X_train = real_train.drop(['id', 'target'], axis=1)\n", "# y_train = real_train['target'].values\n", "# # A parameter grid for XGBoost\n", "# params = {\n", "#         'objective': ['binary:logistic'], \n", "#         'min_child_weight': [10],\n", "#         'gamma': [1], \n", "#         'reg_lambda': [0.3], \n", "#         'subsample': [0.8],\n", "#         'colsample_bytree': [0.8],\n", "#         'max_depth': [6], \n", "#         'learning_rate': [0.015],\n", "#         'n_estimators': [700],\n", "#         'silent': [True], \n", "#        # 'nthread': [1], \n", "#         }\n", "# folds = 4\n", "\n", "# xgbc = XGBClassifier()\n", "# # xgbc.get_params().keys() # get the name of all parameters\n", "# skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=77)\n", "# grid_search = GridSearchCV(param_grid=params, estimator=xgbc, verbose=45, \n", "#                            cv=skf.split(X_train, y_train), n_jobs=4, scoring='roc_auc')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "c756eb91-e4a4-4b75-a38f-9fa151681938", "_uuid": "66051b49155b2e58711228d5cd678b7d26bd9548", "collapsed": true}}, {"execution_count": null, "source": ["# grid_search.fit(X_train, y_train)\n", "# print('Best estimator: ', grid_search.best_estimator_)\n", "# print('Best score: ', grid_search.best_score_ * 2 - 1)\n", "# print('Best parameters: ', grid_search.best_params_)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8214ac71-d1fc-4da6-a6f5-471fb66d6db2", "_uuid": "e0b4e5cccd9ee1197441c1c6b421814be7f35c93", "scrolled": true, "collapsed": true}}, {"execution_count": null, "source": ["import xgboost as xgb\n", "d_train = xgb.DMatrix(train_data.drop(['id', 'target'], axis=1), train_data[['target']])\n", "d_val = xgb.DMatrix(val_data.drop(['id', 'target'], axis=1), val_data[['target']])\n", "d_test = xgb.DMatrix(real_test.drop(['id'], axis=1))\n", "\n", "# xgboost parameters\n", "# params = {}\n", "# params['objective'] = 'binary:logistic'\n", "# params['eta'] = 0.04\n", "# # params['max_delta_step'] = 8\n", "# params['eval_metric'] ='auc'\n", "# params['silent'] = True\n", "# params['max_depth'] = 6\n", "# params['subsample'] = 0.9\n", "# params['colsample_bytree'] = 0.9\n", "\n", "params = {\n", "        'objective': 'binary:logistic', \n", "        'eta': 0.015,\n", "        'eval_metric': 'auc', \n", "        'max_depth': 6, \n", "        'min_child_weight': 10,\n", "        'gamma': 1, \n", "        'reg_lambda': 0.3, \n", "        'reg_alpha': 0.07, \n", "        'subsample': 0.8,\n", "        'colsample_bytree': 0.8,\n", "        'silent': True, \n", "        'n_estimators': 400,\n", "        }\n", "\n", "watchlist = [(d_train, 'train'), (d_val, 'valid')]\n", "\n", "mdl_xgb = xgb.train(params, d_train, num_boost_round=10000, evals=watchlist, early_stopping_rounds=100, \n", "                    feval=gini_xgb, maximize=True, verbose_eval=10)\n", "\n", "# Save model to disk as soon as learning process finishes in case the kernel dies\n", "filename = 'xgb_model.joblib.pkl'\n", "_ = joblib.dump(mdl_xgb, filename, compress=9)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "3aa566b7-7cfd-4f57-8e80-1212c50ea66c", "_uuid": "1ddfe19061818a57030b0ab528ece50f39e08e67", "scrolled": true, "collapsed": true}}, {"execution_count": null, "source": [], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "885f20c1-7abe-4ddf-97a7-1d03513f8783", "_uuid": "8f77dc09dcac4e6b71a2f8d62193acdf60945683", "scrolled": true, "collapsed": true}}, {"execution_count": null, "source": ["# Prediction on test set\n", "test_pred = mdl_xgb.predict(data=d_test)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d6f7272c-72f4-4fd2-84f0-afa729cc780d", "_uuid": "3b6aa932462cea3cd582d8521fe3ba9856aa4782", "scrolled": true, "collapsed": true}}, {"execution_count": null, "source": ["# val_pred = mdl_xgb.predict(data=d_val)\n", "# gini_xgb(d_train=d_val, preds=val_pred)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "9e45321b-29f1-4659-af5f-8ba839e20eaf", "_uuid": "4bd9c76974bfd8063751d293ca92fffe7699f298", "collapsed": true}}, {"execution_count": null, "source": ["# Save prediction results to csv\n", "submissions = pd.DataFrame()\n", "submissions['id'] = real_test.iloc[:, 0]\n", "submissions['target'] = test_pred\n", "submissions.to_csv('xgb_model.csv', index=False)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b8f56ee8-d7da-4bce-92a5-223c0f5a8aec", "_uuid": "a84049942fed93715f07050f77c6fd0280875a4d", "collapsed": true}}, {"metadata": {"_cell_guid": "e7183a99-4688-4eca-bf66-adea22379c33", "_uuid": "76134c9b21d120f4aad4b7068c9c7973da687934"}, "source": ["### Evaluation the model on validation data"], "cell_type": "markdown"}, {"execution_count": null, "source": ["# from sklearn.metrics import confusion_matrix\n", "# predVal = nnet.predict(X=train_data.drop(['id', 'target'], axis=1))\n", "# conf_mat = confusion_matrix(y_pred=predVal, y_true=train_data.target).transpose()\n", "# print(conf_mat)\n", "# precision = conf_mat[0, 0] / (conf_mat[0, 0] + conf_mat[0, 1])\n", "# recall = conf_mat[0, 0] / (conf_mat[0, 0] + conf_mat[1, 0])\n", "# f1 = 2 * precision * recall / (precision + recall)\n", "# f1"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "bf5dbaa8-6851-4265-a7ac-95831b22f59b", "_uuid": "d986d0d6f30a5d8fb9cc37a0a5cb2c4919e07963", "collapsed": true}}, {"metadata": {"_cell_guid": "3276cae8-998e-4781-b883-4d5251b01ad2", "_uuid": "5ab40ea59a9b0df69ec50597914517a11e0ce9c5", "collapsed": true}, "source": ["### Predictions on test data"], "cell_type": "markdown"}, {"execution_count": null, "source": [], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4a7869b3-64cf-4512-a26c-b4f57cc3422b", "_uuid": "52dcf3d6fe99851c4bf97212bc6a76c46d60a870", "collapsed": true}}]}