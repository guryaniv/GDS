{"cells":[{"metadata":{"trusted":true,"_uuid":"473d493b2b3e54bf04bc8ad004874d54573001a7"},"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.dict_vectorizer import DictVectorizer\n\nfrom sklearn.metrics.classification import classification_report, accuracy_score, confusion_matrix\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold, RandomizedSearchCV\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nINPUT_DIR = '../input'\n\nN_FOLDS = 5\nN_ITER = 50\nSEED = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18d35c4ba5bc92b7018f22161e42774e1f8b5d63"},"cell_type":"code","source":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n\ndef min_max_scale(train_data, test_data, cat_cols):\n    \n    data = pd.concat([train_data, test_data])\n    \n    # numeric attributes\n    num_data = data.drop(cat_cols, axis=1)\n    \n    # fit scaler on all data\n    scaler = MinMaxScaler().fit(num_data)\n    \n    # transform all data with scaler\n    train_data = scaler.transform(train_data.drop(cat_cols, axis=1))\n    test_data = scaler.transform(test_data.drop(cat_cols, axis=1))\n    \n    # scale to <0,1>\n    num_train_data = pd.DataFrame(train_data)\n    num_test_data = pd.DataFrame(test_data)\n\n    # fill nan with mean column values\n    num_train_data.fillna(data.mean(), inplace=True)\n    num_test_data.fillna(data.mean(), inplace=True)\n\n    return num_train_data, num_test_data\n\n\ndef cat_vectorize(train_data, test_data, num_cols):\n    # categorical attributes\n    cat_train_data = train_data.drop(num_cols, axis=1)\n    cat_test_data = test_data.drop(num_cols, axis=1)\n\n    cat_train_data.fillna('NA', inplace=True)\n    cat_test_data.fillna('NA', inplace=True)\n\n    cat_train_data_values = cat_train_data.T.to_dict().values()\n    cat_test_data_values = cat_test_data.T.to_dict().values()\n\n    # vectorize (encode as one hot)\n    vectorizer = DictVectorizer(sparse=False)\n    vec_train_data = vectorizer.fit_transform(cat_train_data_values)\n    vec_test_data = vectorizer.transform(cat_test_data_values)\n\n    return vec_train_data, vec_test_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0d8420ae87d16181dc66f6ee2888ad55d14c6b3"},"cell_type":"code","source":"\"\"\" -------------------------------------- Data loading ---------------------------------------- \"\"\"\n\n# load dataframes\ndf_train = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\n\ndf_full = [df_train, df_test]\n\nprint(df_train.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726d585ec5b43f8edc85adf8dde053be72bcd01d"},"cell_type":"code","source":"\"\"\" -------------------------------------- Feature Engineering ---------------------------------------- \"\"\"\n\nfor dataset in df_full:\n    dataset['Last_Name'] = dataset['Name'].apply(lambda x: str.split(x, \",\")[0])\n    \n    dataset['Name_length'] = dataset['Name'].apply(len)\n\n    # Feature that tells whether a passenger had a cabin on the Titanic\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n    # Create new feature FamilySize as a combination of SibSp and Parch\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n    # Create new feature IsAlone from FamilySize\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\n    # Remove all NULLS in the Embarked column\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\n    # Remove all NULLS in the Fare column and create a new feature CategoricalFare\n    dataset['Fare'] = dataset['Fare'].fillna(df_train['Fare'].median())\n\n#     df_train['CategoricalFare'] = pd.qcut(df_train['Fare'], 4)\n\n    # Create a New feature CategoricalAge\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n\n#     df_train['CategoricalAge'] = pd.cut(df_train['Age'], 5)\n\n    # Create a new feature Title, containing the titles of passenger names\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\n    # Group all non-common titles into one single grouping \"Rare\"\n    dataset['Title'] = dataset['Title'].replace(\n        ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map({'female': 0, 'male': 1}).astype(int)\n\n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n\n    # Mapping Fare\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n    # Mapping Age\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4\n\n    dataset = dataset.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp'], axis = 1)\n    \nprint(df_train.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaa97e415af398d8b76e58601d69fe8f805ebb91"},"cell_type":"code","source":"\"\"\" -------------------------------------- Feature preparation ---------------------------------------- \"\"\"\n\nlabel_column = 'Survived'\n\n# get all column names\ncols = list(df_train.columns.values)\n\n# numeric columns\nnum_cols = [e for e in df_train.select_dtypes(include=[np.number]).columns.tolist() if e != label_column]\n\n# categorical columns\ncat_cols = [e for e in cols if e not in num_cols and e != label_column]\n\nprint(num_cols, cat_cols)\n\nx_train, y_train = df_train.drop(label_column, axis=1), df_train[label_column]\nx_test = df_test\n\n# scale everything to [0, 1]\nx_num_train, x_num_test = min_max_scale(x_train, x_test, cat_cols)\n\n# vectorize categorical columns\nvec_x_cat_train, vec_x_cat_test = cat_vectorize(x_train, x_test, num_cols)\n\n# build the feature vector\nx_train = np.hstack((x_num_train, vec_x_cat_train))\nx_test = np.hstack((x_num_test, vec_x_cat_test))\n\n# labels or target attribute\ny_train = y_train.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12fcd4039a43033099fb02bf5a629642d8abe087"},"cell_type":"code","source":"\"\"\" -------------------------------------- Correlation report ---------------------------------------- \"\"\"\n\n# plt.figure(figsize=(14, 12))\n# plt.title('Pearson Correlation of Features', y=1.05, size=15)\n# sns.heatmap(pd.DataFrame(x_train).astype(float).corr(), linewidths=0.1, vmax=1.0, \n#             square=True, cmap=plt.cm.RdBu, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51163a466131dd9d9ee414f88f803219117505af"},"cell_type":"code","source":"\"\"\" -------------------------------------- Cross validation ---------------------------------------- \"\"\"\n\n# split the data into train and test\n# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90d0738acf8fab51a4bc327459177e66e0ec271f"},"cell_type":"code","source":"\"\"\" ---------------------------------- Grid params initialization ----------------------------------- \"\"\"\n\nMODELS = {\n#     'lr': {\n#         'model': LogisticRegression,\n#         'params': {\n#             'fit_intercept': [True, False],\n#             'multi_class': ['ovr'],\n#             'penalty': ['l2'],\n#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n#             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5]\n#         }\n#     },\n#     'lrcv': {\n#         'model': LogisticRegressionCV,\n#         'params': {\n#             'Cs': [1, 2, 4, 8, 16, 32],\n#             'fit_intercept': [True, False],\n#             'refit': [True, False],\n#             'multi_class': ['ovr'],\n#             'penalty': ['l2'],\n#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n#             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n#             'cv': [cv]\n#         },\n#         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'refit': True, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': False, 'cv': 4, 'Cs': 2},\n#         'best_score': 0.8428731762065096\n#     },\n    'svc': {\n        'model': SVC,\n        'params': {\n            'C': [0.1, 0.5, 1., 2., 4.],\n            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'gamma': ['auto', 'scale'],\n            'degree': range(5),\n            'shrinking': [True, False],\n            'probability': [True, False],\n            'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],\n        },\n        'best_params': {'tol': 0.5, 'shrinking': False, 'probability': True, 'kernel': 'rbf', 'gamma': 'scale', 'degree': 2, 'C': 4.0},\n        'best_score': 0.8496071829405163\n    },\n#     'dt': {\n#         'model': DecisionTreeClassifier,\n#         'params': {\n#             'criterion': ['gini', 'entropy'],\n#             'max_depth': range(6, 10),\n#             'max_features': ['auto', 'sqrt', 'log2', None],\n#             'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n#             'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n#         }\n#     },\n    'rf': {\n        'model': RandomForestClassifier,\n        'params': {\n            'n_estimators': range(10, 251, 20),\n            'max_features': ['auto', 'sqrt', 'log2', None],\n            'max_depth': range(5, 20),\n            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n            'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n            'bootstrap': [True, False], # Method of selecting samples for training each tree,\n            'random_state': [SEED],\n            'n_jobs': [-1]\n        },\n        'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 8, 'bootstrap': True},\n        'best_score': 0.8204264870931538\n    },\n    'ada': {\n        'model': AdaBoostClassifier,\n        'params': {\n            'n_estimators': range(10, 251, 20),\n            'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n            'algorithm': ['SAMME', 'SAMME.R'],\n            'random_state': [SEED],\n        },\n        'best_params': {'random_state': 32, 'n_estimators': 230, 'learning_rate': 0.05, 'algorithm': 'SAMME'},\n        'best_score': 0.8148148148148148\n    },\n    'et': {\n        'model': ExtraTreesClassifier,\n        'params': {\n            'n_estimators': range(10, 251, 20),\n            'max_features': ['auto', 'sqrt', 'log2', None],\n            'max_depth': range(5, 20),\n            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n            'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n            'bootstrap': [True, False], # Method of selecting samples for training each tree,\n            'random_state': [SEED],\n            'n_jobs': [-1]\n        },\n        'best_params': {'random_state': 32, 'n_jobs': -1, 'n_estimators': 150, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 11, 'bootstrap': False},\n        'best_score': 0.8237934904601572\n    },\n    'gb': {\n        'model': GradientBoostingClassifier,\n        'params': {\n            'n_estimators': range(10, 251, 20),\n            'max_depth': range(5, 20),\n            'loss': ['deviance', 'exponential'],\n            'learning_rate': [.01, .05, .1, .2, .5, 1, 2],                      \n            'subsample': [.25, .5, .8, 1.],\n            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n            'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n            'random_state': [SEED],\n#             'n_jobs': [-1]\n        },\n        'best_params': {'subsample': 0.25, 'random_state': 32, 'n_estimators': 130, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 19, 'loss': 'exponential', 'learning_rate': 0.1},\n        'best_score': 0.8204264870931538\n    }\n#     'xgb': {\n#         'model': XGBClassifier,\n#         'params': {\n#             'n_estimators': range(8, 20),\n#             'max_depth': range(5, 20),\n#             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n#             'colsample_bytree': [.6, .7, .8, .9, 1]\n#         }\n#     }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b74179cbe7effc051957c8a3a6543b8d548a0126"},"cell_type":"code","source":"\"\"\" -------------------------------------- Best Linear Regression search ---------------------------------------- \"\"\"\n\nfor k, model in MODELS.items():\n    \n    if 'best_score' in model:\n        # Initialize with best parameters & fit to data\n        print(f'Fitting {k}...')\n        model['best_estimator'] = model['model'](**model['best_params']).fit(x_train, y_train)\n        \n        scores = cross_val_score(model['best_estimator'], x_train, y_train, cv=N_FOLDS)\n        score = sum(scores) / len(scores)\n        diff = score - model['best_score']\n        if diff > 0:\n            print(f'Accuracy of model {k}: {score} (BIGGER for {diff})')\n            \n        elif diff < 0:\n            print(f'Accuracy of model {k}: {score} (SMALLER for {-diff})')\n        else:\n            print(f'Accuracy of model {k}: {score} (SAME)')\n    else:\n        # Perform random search\n        searcher = RandomizedSearchCV(param_distributions=model['params'],\n                                      estimator=model['model'](), scoring=\"accuracy\",\n                                      verbose=1, n_iter=N_ITER, cv=N_FOLDS)\n        # Fit to data\n        print(f'Fitting {k}...')    \n        searcher.fit(x_train, y_train)\n\n        # Print the best parameters and best accuracy\n        print(f'Best parameters found for {k}: {searcher.best_params_}')\n        print(f'Best accuracy found {k}: {searcher.best_score_}')\n\n        model['best_estimator'] = searcher.best_estimator_\n        model['best_params'] = searcher.best_params_\n        model['best_score'] = searcher.best_score_\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60037af207e2382137bde8e0cbb72014fda9cd40"},"cell_type":"code","source":"\"\"\" ---------------------------------- Preparing 2nd level features ------------------------------------ \"\"\"\n\nn_train = len(x_train)\nn_test = len(x_test)\n\nk_folds = KFold(n_splits=N_FOLDS, random_state=SEED)\n\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((n_train,))\n    oof_test = np.zeros((n_test,))\n    oof_test_skf = np.empty((N_FOLDS, n_test))\n\n    for i, (train_index, test_index) in enumerate(k_folds.split()):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n\n# Create our OOF train and test predictions. These base results will be used as new features\n# et_oof_train, et_oof_test = get_oof(MODEL['et']['best_estimator'], x_train, y_train, x_test) # Extra Trees\n# rf_oof_train, rf_oof_test = get_oof(MODEL['rf']['best_estimator'], x_train, y_train, x_test) # Random Forest\n# ada_oof_train, ada_oof_test = get_oof(MODEL['ada']['best_estimator'], x_train, y_train, x_test) # AdaBoost \n# gb_oof_train, gb_oof_test = get_oof(MODEL['gb']['best_estimator'], x_train, y_train, x_test) # Gradient Boost\n# svc_oof_train, svc_oof_test = get_oof(MODEL['svc']['best_estimator'], x_train, y_train, x_test) # Support Vector Classifier\n\net_oof_train, et_oof_test = MODELS['et']['best_estimator'].predict(x_train), MODELS['et']['best_estimator'].predict(x_test)\nrf_oof_train, rf_oof_test = MODELS['rf']['best_estimator'].predict(x_train), MODELS['rf']['best_estimator'].predict(x_test)\nada_oof_train, ada_oof_test = MODELS['ada']['best_estimator'].predict(x_train), MODELS['ada']['best_estimator'].predict(x_test)\ngb_oof_train, gb_oof_test = MODELS['gb']['best_estimator'].predict(x_train), MODELS['gb']['best_estimator'].predict(x_test)\nsvc_oof_train, svc_oof_test = MODELS['svc']['best_estimator'].predict(x_train), MODELS['svc']['best_estimator'].predict(x_test)\n\nX_train = np.vstack(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train)).T\nX_test = np.vstack(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test)).T\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49b9a391f104263ebb5becb9cbf329a2c3eaa251"},"cell_type":"code","source":"print(len(X_train), len(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8f171b83222c9d83b5b852e53a69a90259f571"},"cell_type":"code","source":"\"\"\" ----------------------------------- Fitting XGBoost classifier ------------------------------------- \"\"\"\n\nxgb_params = {\n    'n_estimators': range(20, 501, 20),\n    'max_depth': range(4, 21, 4),\n    'learning_rate': [.01, .05, .1, .2, .5, 1, 2],\n    'colsample_bytree': [.6, .7, .8, .9, 1]\n}\nxgb = XGBClassifier(**{'n_estimators': 140, 'max_depth': 4, 'learning_rate': 0.2, 'colsample_bytree': 0.6})\n\n# Perform random search\n# searcher = RandomizedSearchCV(param_distributions=xgb_params,\n#                               estimator=XGBClassifier(), scoring=\"accuracy\",\n#                               verbose=1, n_iter=N_ITER, cv=N_FOLDS)\n# Fit to data\nprint(f'Fitting {k}...')    \nxgb.fit(X_train, y_train)\n\npred = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"694982f5e35c52c17fdb1175bcc2e0cb3f77c5b0"},"cell_type":"code","source":"# pred = MODELS[max(MODELS, key=lambda k: MODELS[k]['best_score'])]['best_estimator'].predict(x_test)\nsubmission = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': pred})\nsubmission.to_csv('gender_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6874ce3751c2d62a196c19e6250a7ec9c16f73a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}