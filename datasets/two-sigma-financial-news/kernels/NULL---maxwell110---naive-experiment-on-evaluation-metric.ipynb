{"cells":[{"metadata":{"_uuid":"775e4306f1aeee8afb329ba6e79776e3ba1a3814"},"cell_type":"markdown","source":"<font color=lightseagreen size=6><b> Naive Experiment on evaluation metric </b></font>\n<br>\n<font color=lightseagreen size=5><b> 2 sigma : Using News to Predict Stock Movements </b></font>"},{"metadata":{"_uuid":"1c3b9adebf8be7d5def6629740685ee6afbb9cda"},"cell_type":"markdown","source":"![hedge](https://cdn-ak.f.st-hatena.com/images/fotolife/g/greenwind120170/20181002/20181002214157.jpg)"},{"metadata":{"_uuid":"bbef8ffedee45ae92fd2bdfcffad6d967be332dc"},"cell_type":"markdown","source":"<font size=4>\nIn this competition, we evaluate our models with following metric.  \n</font>  \n  \n$$x_t = \\Sigma_{i} \\; \\hat{y}_{ti} r_{ti} u_{ti}$$  \n  \n$$\\mathrm{score} = \\frac{\\bar{x_t}}{\\sigma(x_t)}$$  \n  \n$r_{ti}$ is the 10-day market-adjusted leading return for day t for instrument i,  \nso this metric is a kind of [Information Ratio](https://www.investopedia.com/terms/i/informationratio.asp) , I think.  \n  \nI did a naive experiment on this metric and got some insights about validation strategy in this competition.  "},{"metadata":{"_uuid":"a503ef21ba24e707d6ae3859d29ab405c61d57af"},"cell_type":"markdown","source":"# Load packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(font_scale=1)\n\nimport warnings\nimport missingno as msno\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\npd.options.mode.chained_assignment = None\n# dir(pd.options.display)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nplt.style.use('ggplot')\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28b998b1e125ea6b5a830b87b6cf874020603799"},"cell_type":"markdown","source":"# make 2sigma kaggle env"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"env = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb36a79501ad05ceff5c4fa6d8be84d10cbd063d"},"cell_type":"markdown","source":"We will concentrate on market data.   \nLet's delete news data."},{"metadata":{"trusted":true,"_uuid":"7f4d470db0531bbb4ed6228c6a8d8b69cddc5b84"},"cell_type":"code","source":"(market_train, news_train) = env.get_training_data()\ndel news_train\ngc.enable()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d33bb35c5ebef7d5a365fd0da9cb39ad1d982d3d"},"cell_type":"markdown","source":"In @jannesklaas awesome [kernel](https://www.kaggle.com/jannesklaas/lb-0-63-xgboost-baseline),   \n> Stocks can only go up or down, if the stock is not going up, it must go down (at least a little bit). So if we know our model confidence in the stock going up, then our new confidence is:\n> $$\\hat{y}=up-(1-up)=2*up-1$$\n> \n> We are left with a \"simple\" binary classification problem, for which there are a number of good tool, here we use XGBoost, but pick your poison.  \n  \nI followed his setting.  \nAddition to it, I assumed that we managed to get the perfect model.  \nIt means that when stocks go up, our prediction is always 1, vice versa."},{"metadata":{"trusted":true,"_uuid":"360c0f4614349b27a11f11bcb4b4aa3402909b34"},"cell_type":"code","source":"ret = market_train.returnsOpenNextMktres10\nuniv = market_train.universe\nlabel = (ret > 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"940ba8db4a6055c7d721dd5d25a78ef552398180"},"cell_type":"markdown","source":"Define the function to calculate evalution metric.  \nThis function has `window` arg to limit the range of time.  \nWithin this function, we calculate the perfect confidence value.  "},{"metadata":{"trusted":true,"_uuid":"e0456c8e218a07421bc6198974bab070b19097c3"},"cell_type":"code","source":"def ir(label, window):\n    global market_train, ret, univ\n    time_idx = market_train.time.factorize()[0]\n    # (label * 2 - 1) : perfect confidence value\n    x_t = (label * 2 - 1) * ret * univ\n    x_t_sum = x_t.groupby(time_idx).sum()\n    x_t_sum = x_t_sum[window:]\n    score = x_t_sum.mean() / x_t_sum.std()\n    return score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6858ced7d02a7fd6bd75124c07ef496c41d987e4"},"cell_type":"markdown","source":"Move by 10 operational days ( ~ 252days / year ), calculate scores.  "},{"metadata":{"trusted":true,"_uuid":"f991ef57eabc9eeeaa19b68e5d93b250536d7199"},"cell_type":"code","source":"ir_l = [ir(label, t) for t in range(0, market_train.time.nunique(), 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa06815ed5966a8914a350ef101a8492343ccfb"},"cell_type":"code","source":"trace = go.Scatter(\n    x = np.arange(0, market_train.time.nunique(), 10),\n    y = ir_l,\n    mode = 'lines+markers',\n    marker = dict(\n        size = 4,\n        color = 'lightblue'\n    ),\n    line = dict(\n        width = 1\n    )\n)\ndata = [trace]\nlayout = go.Layout(dict(\n    title = 'Eval Metric trend',\n    xaxis = dict(title = 'operational days passed ( window start point )'),\n    yaxis = dict(title = 'Evaluation metric'),\n    height = 400,\n    width = 750\n))\npy.iplot(dict(data=data, layout=layout), filename='IR trend')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"984df750933f7f7e1cc1933aebf151f252f5bb05"},"cell_type":"markdown","source":"From above pitcure,   \nit is obvious that in the early stage of this data period ( 2007 ~ 2008 ), score is too low **due to its high volatility** .  \nBelow picture shows the standard deviation of `returnsOpenPrevRaw1` with time.  "},{"metadata":{"trusted":true,"_uuid":"97932f3ddf9fb50b6e6e045d626616531abfcf6d"},"cell_type":"code","source":"op = ['mean', 'std']\ndf = market_train[['time', 'returnsOpenPrevRaw1']].groupby('time').agg({\n    'returnsOpenPrevRaw1' : op,\n}).reset_index()\ndf.columns = ['time'] + [o + '_returnsOpenPrevRaw1' for o in op]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"804d5a6d6185af9eed085cc4f8223e14ec9fa8d0"},"cell_type":"code","source":"trace = go.Scatter(\n    x = df.time,\n    y = df.std_returnsOpenPrevRaw1,\n    mode = 'lines+markers',\n    marker = dict(\n        size = 4,\n        color = 'pink'\n    ),\n    line = dict(\n        width = 1\n    )\n)\ndata = [trace]\nlayout = go.Layout(dict(\n    title = 'std of returnsOpenPrevRaw1',\n    xaxis = dict(title = 'date'),\n    yaxis = dict(title = 'std of returnsOpenPrevRaw1'),\n    height = 400,\n    width = 750\n))\npy.iplot(dict(data=data, layout=layout), filename='.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e41c8b3bdd403f1340a438e58988062e3e06d36"},"cell_type":"markdown","source":"<font size=4 color=deeppink>\nMy conclusion is,  \n</font>\n<br>\n- Including data within 2007 ~ 2008 to bulid or evaluate model will not be good due to its high volatility.  \n<br>\n- The current financial market is similar to 2009 ~ 2017 rather than 2007 ~ 2008.  \n<br>\n- The possibility of the shock like Lehman or Pariba occured will be very low with the current situation.  \n    ( some rigid law like Basel III and Solvency II will protect the world to some extent. )"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}