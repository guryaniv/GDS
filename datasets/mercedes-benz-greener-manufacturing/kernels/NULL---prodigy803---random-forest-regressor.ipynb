{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# I have taken a lot of code from https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-mercedes,\n# Since this is my first submission for kaggle, please be patient with and allow me to improve by suggesting \n# improvements to my kernel or general understanding of ML Concepts.\n\nimport os\nfrom subprocess import check_output\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#FOR BUILDING THE MODEL\nimport pylab\nimport scipy.stats as stats\nfrom sklearn import preprocessing\n\nimport xgboost as xgb\nfrom sklearn import ensemble\n%matplotlib inline\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")\nprint(train_data.shape)\nprint(train_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a1fccd05eac2f62fb634b9bd4a8c8626eb582ea"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ad392b89f54962131b480dbd03b3988910b53b8"},"cell_type":"markdown","source":"Splitting the Data into Target and predictor variables"},{"metadata":{"trusted":true,"_uuid":"ccba1f3115a92b3f64eb0ce00b2733fe24dcd174"},"cell_type":"code","source":"train_data_target = train_data.iloc[:,0:2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6375b5aa44c9e9ddde2429aaace8c6b9297c96f"},"cell_type":"markdown","source":"## We will check if the target has any outliers\n- For checking outliers, I have decided to go for a Boxplot, which will give me a general understanding of what the data-set is about"},{"metadata":{"trusted":true,"_uuid":"64cbb11b288d430afd60dbf9800c516c2277d03a"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxplot(train_data_target.loc[:,'y'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42f8f7c9ea1f3da153b311c2f7e78995f48cdf4a"},"cell_type":"markdown","source":"**As you can see, there are quite some outliers.**\n- To Remove these outliers, I have decided to use the z-score method with threshold of three"},{"metadata":{"trusted":true,"_uuid":"c225e245e544ca6399b46429add563a2943ad849"},"cell_type":"code","source":"train_data_target['z'] = np.abs(stats.zscore(train_data_target.loc[:,'y']))\nthreshold=3\n\nID_OF_OUTLIERS = train_data_target[train_data_target['z']>3].ID","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"795db460f7bff0efc86ffffb79c059e8dfd23eaf"},"cell_type":"markdown","source":"Once I have all the ID's of the outliers, I will remove them from the training data-set"},{"metadata":{"trusted":true,"_uuid":"8ca36cee3b069dcde0d22e1df2e4128270be95ee"},"cell_type":"code","source":"train_data_target = train_data_target[~train_data_target['ID'].isin(list(ID_OF_OUTLIERS))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e6c026904b62512eba1abbf4bba42d425ba8ef5"},"cell_type":"code","source":"train_data_predictors = train_data[~train_data['ID'].isin(list(ID_OF_OUTLIERS))].iloc[:,2:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e6c36f63f38030c7ba2c01ce982bea4340c62c"},"cell_type":"markdown","source":"## Now again lets see the distribution of the target var"},{"metadata":{"trusted":true,"_uuid":"0dfce2f8e9e0eb44d915d9da802c7fee65eb1cda"},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.boxplot(train_data_target.loc[:,'y'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"610bf2d53f18a4bdb5777854625ef84479546780"},"cell_type":"markdown","source":"The Distribution is a lot better"},{"metadata":{"trusted":true,"_uuid":"a87da0bca4994e32e6e629840f5e10880be68c05"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.scatter(range(train_data_target.shape[0]), np.sort(train_data_target.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3c979955f04e7e7810a84a6af1fa5707964f131"},"cell_type":"markdown","source":"Overall the scatter plot looks good\n\n### The Next that we should do is to check the unique values of all the columns. If atall there are columns with only one value then we should exclude them.\n\n- First check for all the unique values:\n- Then drop the ones in which there is oly one value\n- Then check for feature importance using XGBOOST AND RANDOM FOReST Regressor\n- USE RANDOM FOREST REGRESSOR TO PREDICT THE OUTPUT\n"},{"metadata":{"trusted":true,"_uuid":"27600b250e075c91651ae452051939406720693f"},"cell_type":"code","source":"# This portion has been taken from https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-mercedes\n\nunique_values_dict = {}\nfor col in train_data_predictors.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_data_predictors[col].unique()).tolist())\n        tlist = unique_values_dict.get(unique_value, [])\n        tlist.append(col)\n        unique_values_dict[unique_value] = tlist[:]\nfor unique_val, columns in unique_values_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"--------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"705865be2ea49a902418979549ee64b612d4086d"},"cell_type":"code","source":"#Dropping the column who dont have a lot of importance\n\ntrain_data_predictors_v2 = train_data_predictors.drop(columns = unique_values_dict['[0]'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70ebe8dadad94f976373ba5fd8e63db3955617c5"},"cell_type":"code","source":"# We will first need to do one hot encoding before we run the XGBOOST MODEL\n\nfor f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n  lbl = preprocessing.LabelEncoder()\n\n  lbl.fit(list(train_data_predictors_v2[f].values)) \n\n  train_data_predictors_v2[f] = lbl.transform(list(train_data_predictors_v2[f].values))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e26bcce55d11a3f3fa4f02db88b28233379a4a97"},"cell_type":"markdown","source":"# Lets do some feature selection using XGBoost and Random Forest."},{"metadata":{"trusted":true,"_uuid":"cf0d0448634df60784678779ec2a3f4106eb162c"},"cell_type":"code","source":"train_y = train_data_target.y.values\ntrain_X = train_data_predictors_v2\n\n# Thanks to anokas for this #\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100, feval=xgb_r2_score, maximize=True)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eabd4e2bca1ad896b9e315b743995013c80dd28e"},"cell_type":"markdown","source":"### Feature Selection Using Random Forest>"},{"metadata":{"trusted":true,"_uuid":"42c49dcdb0195eb62ffdeb90cd443c06a5e1cee3"},"cell_type":"code","source":"\nmodel_RFR = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel_RFR.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\n## plot the importances ##\nimportances = model_RFR.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model_RFR.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a036537e8a4b3f0a933acd42367d17ab1600ca2"},"cell_type":"markdown","source":"As inferred by SRK, the features selected by Random Forest and XGBoost are quite different.\nSince I am more familiar with Random Forest, I will go ahead and predict the values using Random Forest Regressor"},{"metadata":{"trusted":true,"_uuid":"83ebea1e3c1303b5c3eaf3572a872dfd12aa277a"},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73d8517082902cbfe82b3a2b782d7f0d2e82f0ff"},"cell_type":"markdown","source":"Dropping the non-important columns"},{"metadata":{"trusted":true,"_uuid":"c17ab7dbae5dc102b2019fe268e62ea0cbc16530"},"cell_type":"code","source":"test_df_v2 = test_df.drop(columns = unique_values_dict['[0]'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"329ee3d22dfe71d033c609d5b330ba1afae59739"},"cell_type":"markdown","source":"One Hot encoding the categorical vars in the test data"},{"metadata":{"trusted":true,"_uuid":"62de1930da3bf4e59112cdbb32b9d87ad389c8f4"},"cell_type":"code","source":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n  lbl = preprocessing.LabelEncoder()\n\n  lbl.fit(list(test_df_v2[f].values)) \n\n  test_df_v2[f] = lbl.transform(list(test_df_v2[f].values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90e40faa7b95b84505cd2f0f06d66204b39bdf01"},"cell_type":"markdown","source":"Finally predicting the variables using Random Forest"},{"metadata":{"trusted":true,"_uuid":"8f65943c45bb33e5f89f695fd8c78a7277eb01de"},"cell_type":"code","source":"predictions = model_RFR.predict(test_df_v2.iloc[:,1:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e9d108d049f58519c3981a1588e372727a928e9"},"cell_type":"markdown","source":"Preparing the submission file"},{"metadata":{"trusted":true,"_uuid":"4eaeec6505fb91aaf562c97f4bdbcde286b1dd59"},"cell_type":"code","source":"my_submission = pd.DataFrame({'ID':test_df_v2['ID'],'y':predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc33253fed1df107331b7e68fdd73e59c0436e43"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}