{"cells":[{"metadata":{"_uuid":"08c0c8baf241a57ba91214df7f43329f106fe0de"},"cell_type":"markdown","source":"<h2>1. About this notebook</h2>\n\nIn this notebook I try a few different models to predict the time to failure during earthquake simulations. I'm using some new features like trend and absolute values with others from public kernels (e.g. quantiles and rolling means).\n\nUpdate 03/02: Fixed erros at lgbm; add feature importance and visualizations.\n\nFor more details about LANL competition you can check my [previous kernel](https://www.kaggle.com/jsaguiar/seismic-data-exploration)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n# Sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nimport lightgbm as lgb\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nsns.set()\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data_type = {'acoustic_data': np.int16, 'time_to_failure': np.float64}\ntrain = pd.read_csv('../input/train.csv', dtype=data_type)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22a9edd2ef3911b7de989cd22781b871416ecdd1"},"cell_type":"markdown","source":"<h2>2. Feature Engineering</h2>\n\nSimple trend feature: fit a linear regression and return the coefficient"},{"metadata":{"trusted":true,"_uuid":"e915924b13a10bb09a64e2c0d5a81e0a04905ec3"},"cell_type":"code","source":"def add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab25676aa4a93cb6213347bfd3dcd15d9c767fe7"},"cell_type":"markdown","source":"Group the training data in chunks of 150,000 examples and extract the following features:\n\n* Aggregations: min, max, mean and std\n* Absolute features: max, mean and std\n* Quantile features\n* Trend features\n* Rolling features\n* Ratios"},{"metadata":{"trusted":true,"_uuid":"1772c3a5aa5dbe3d672031eb64e6bac596206588"},"cell_type":"code","source":"rows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64)\n\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data']   # pd series\n    y = seg['time_to_failure'].values[-1]  # single value\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.values.mean()\n    X_train.loc[segment, 'std'] = x.values.std()\n    X_train.loc[segment, 'max'] = x.values.max()\n    X_train.loc[segment, 'min'] = x.values.min()\n    X_train.loc[segment, 'q90'] = np.quantile(x.values, 0.90)\n    X_train.loc[segment, 'q95'] = np.quantile(x.values, 0.95)\n    X_train.loc[segment, 'q99'] = np.quantile(x.values, 0.99)\n    X_train.loc[segment, 'q05'] = np.quantile(x.values, 0.05)\n    X_train.loc[segment, 'q10'] = np.quantile(x.values, 0.10)\n    X_train.loc[segment, 'q01'] = np.quantile(x.values, 0.01)\n    \n    X_train.loc[segment, 'abs_max'] = np.abs(x.values).max()\n    X_train.loc[segment, 'abs_mean'] = np.abs(x.values).mean()\n    X_train.loc[segment, 'abs_std'] = np.abs(x.values).std()\n    X_train.loc[segment, 'trend'] = add_trend_feature(x.values)\n    X_train.loc[segment, 'abs_trend'] = add_trend_feature(x.values, abs_values=True)\n    \n    # New features - rolling features\n    for w in [10, 50, 100, 1000]:\n        x_roll_std = x.rolling(w).std().dropna().values\n        x_roll_mean = x.rolling(w).mean().dropna().values\n        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n        \n        X_train.loc[segment, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n        X_train.loc[segment, 'std_roll_std_' + str(w)] = x_roll_std.std()\n        X_train.loc[segment, 'max_roll_std_' + str(w)] = x_roll_std.max()\n        X_train.loc[segment, 'min_roll_std_' + str(w)] = x_roll_std.min()\n        X_train.loc[segment, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n        X_train.loc[segment, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n        X_train.loc[segment, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n        X_train.loc[segment, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n        X_train.loc[segment, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n        \n        X_train.loc[segment, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n        X_train.loc[segment, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n        X_train.loc[segment, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n        X_train.loc[segment, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n        X_train.loc[segment, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n        X_train.loc[segment, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n        X_train.loc[segment, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n        X_train.loc[segment, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n        \n        X_train.loc[segment, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n        X_train.loc[segment, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n        X_train.loc[segment, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n        X_train.loc[segment, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n        X_train.loc[segment, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n        X_train.loc[segment, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n        X_train.loc[segment, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n        X_train.loc[segment, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff26be4db60a010ddf7e81554456bbd19e068f96"},"cell_type":"code","source":"print(\"Train shape:\", X_train.shape)\nX_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79675d6eca47c3039e729c937b1498eea8a54b60"},"cell_type":"markdown","source":"Scale features and helper functions:"},{"metadata":{"trusted":true,"_uuid":"c2f28f7269af8bd5ba9fe1bec70da271bc93d3fe","_kg_hide-input":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\ntarget = y_train.values.flatten()\nnum_folds = 5\n\ndef grid_search_cv(estimator, grid, features, target):\n    \"\"\"Return the best hyperparameters combination in grid.\"\"\"\n    t0 = time.time()\n    reg = GridSearchCV(estimator, grid, cv=num_folds, scoring='neg_mean_absolute_error')\n    reg.fit(features, target)\n    \n    t0 = time.time() - t0\n    print(\"Best CV score: {:.4f}, time: {:.1f}s\".format(-reg.best_score_, t0))\n    print(reg.best_params_)\n    return reg.best_params_\n\ndef make_predictions(estimator, features, target, test=None, plot=True, lgb=False):\n    \"\"\"Train the estimator and make predictions for oof and test data.\"\"\"\n    folds = KFold(num_folds, shuffle=True, random_state=2019)\n    oof_predictions = np.zeros(features.shape[0])\n    if test is not None:\n        sub_predictions = np.zeros(test.shape[0])\n    for (train_index, valid_index) in folds.split(features, target):\n        \n        if lgb:\n            estimator.fit(features[train_index], target[train_index],\n                          early_stopping_rounds=100, verbose=False,\n                          eval_set=[(features[train_index], target[train_index]),\n                                    (features[valid_index], target[valid_index])])\n        else:\n            estimator.fit(features[train_index], target[train_index])\n        oof_predictions[valid_index] = estimator.predict(features[valid_index]).flatten()\n        if test is not None:\n            sub_predictions += estimator.predict(test).flatten() / num_folds\n    \n    # Plot out-of-fold predictions vs actual values\n    if plot:\n        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n        ax1, ax2 = axis\n        ax1.set_xlabel('actual')\n        ax1.set_ylabel('predicted')\n        ax2.set_xlabel('train index')\n        ax2.set_ylabel('time to failure')\n        ax1.scatter(target, oof_predictions, color='brown')\n        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n        ax2.plot(target, color='blue', label='y_train')\n        ax2.plot(oof_predictions, color='orange')\n    if test is not None:\n        return oof_predictions, sub_predictions\n    else:\n        return oof_predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5255831db3f42ce8c31fcb18363ed7920f2d410"},"cell_type":"markdown","source":"<h2>3. Models</h2>\n\nLet's try a few different models and submit the one with the best validation score. The predicted values in the following plots are using a out-of-fold scheme.\n\n<h3>Ridge Regression</h3>\n\nThe first model will be a linear regression with L2 regularization.\n\n"},{"metadata":{"trusted":true,"_uuid":"38dd94339081930175c7ca60ed983fa802bd0e0a"},"cell_type":"code","source":"grid = [{'alpha': np.concatenate([np.linspace(0.001, 1, 100), np.linspace(1, 200, 1000)])}]\nparams = grid_search_cv(Ridge(), grid, X_train_scaled, target)\nridge_oof = make_predictions(Ridge(**params), X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37fc4cff531998c5bfae84772795e8c01e607fbe"},"cell_type":"markdown","source":"There are some huge negative values when using a linear model. We can try to change negative values for zeros:"},{"metadata":{"trusted":true,"_uuid":"97bbc24d95919791d48766abe1f23f11eea78597"},"cell_type":"code","source":"ridge_oof[ridge_oof < 0] = 0\nprint(\"Mean error: {:.4f}\".format(mean_absolute_error(target, ridge_oof)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9f936d7866fea46f5d83401b0e1882291c4d63"},"cell_type":"markdown","source":"<h3>Kernel Ridge</h3>\n\nThis model combines regularized linear regression with a given kernel (radial basis in this case)."},{"metadata":{"trusted":true,"_uuid":"f0b7e94dda285927518b226b716c2966e9def90f"},"cell_type":"code","source":"grid = [{'gamma': np.linspace(1e-8, 0.1, 10), 'alpha': [0.0005, 0.001, 0.02, 0.08, 0.1]}]\nparams = grid_search_cv(KernelRidge(kernel='rbf'), grid, X_train_scaled, target)\nkr_oof = make_predictions(KernelRidge(kernel='rbf', **params), X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7738cdc47cc603b645b35aa8d931985888fa6d77"},"cell_type":"markdown","source":"<h3>SVM</h3>\nSupport vector machine with radial basis function kernel."},{"metadata":{"trusted":true,"_uuid":"9bb9c43c3a5de20552fbb2a6c7d91e81ddb55e1e"},"cell_type":"code","source":"grid = [{'epsilon': np.linspace(0.01, 0.5, 10), 'C': np.linspace(0.01, 2, 10)}]\nparams = grid_search_cv(SVR(kernel='rbf', gamma='scale'), grid, X_train_scaled, target)\nsvm_oof = make_predictions(SVR(kernel='rbf', gamma='scale', **params), X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cc3b80df2183655437dfad7607ecd45acfd02f7"},"cell_type":"markdown","source":"<h3>Random Forests</h3>\n\nThis regressor fits many decision trees with different subsets of the original data and average the predictions between them."},{"metadata":{"trusted":true,"_uuid":"f293eae9eac71f20a299b44270010eaab1f94a60"},"cell_type":"code","source":"grid = [{\n    'max_depth': [8, 10, 12],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    #'min_samples_leaf': [2, 4, 12],\n    #'min_samples_split': [2, 6, 12],\n}]\nparams = grid_search_cv(RandomForestRegressor(criterion='mae', n_estimators=50),\n                        grid, X_train_scaled, target)\nrf_oof = make_predictions(RandomForestRegressor(criterion='mae', n_estimators=200, **params),\n                          X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6331c5911feab57e25b49f32c28669d74f938d00"},"cell_type":"markdown","source":"<h3>Extremely Randomized Trees</h3>"},{"metadata":{"trusted":true,"_uuid":"2ed92af2667163451c8190ed87d54342edc1d96c"},"cell_type":"code","source":"params = grid_search_cv(ExtraTreesRegressor(criterion='mae', n_estimators=50),\n                        grid, X_train_scaled, target)\nex_oof = make_predictions(ExtraTreesRegressor(criterion='mae', n_estimators=200, **params),\n                          X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44aa21f6b7a41bcb47aef88d83c7892614700dc4"},"cell_type":"markdown","source":"<h3>Ada Boost</h3>\n\nAdaBoost begins by fitting a base estimator on the original dataset and then fits additional copies on the same dataset. At each iteration (estimator), the weights of instances are adjusted according to the error of the last prediction. It's similar to the next model, but gradient boosting fits additional estimator copies on the current error and not on the original dataset."},{"metadata":{"trusted":true,"_uuid":"146acb052cc6bf6bb448b2244b52ec8f92036ab2"},"cell_type":"code","source":"grid = [{'learning_rate': np.linspace(0.01, 0.1, 10)}]\n#base = DecisionTreeRegressor(max_depth=5)\nbase = Ridge(alpha=10)\nparams = grid_search_cv(AdaBoostRegressor(base_estimator=base, n_estimators=100),\n                        grid, X_train_scaled, target)\nada_oof = make_predictions(AdaBoostRegressor(base_estimator=base, n_estimators=100, **params),\n                           X_train_scaled, target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fc9dc56cc1220a8a07f69ab52dad35af7800572"},"cell_type":"markdown","source":"<h3>Gradient Boosting</h3>\n\nThe last model is a gradient boosting decision tree. It's not possible to use GridSearchCV with early stopping (lightgbm), so I am using a custom function for random search."},{"metadata":{"trusted":true,"_uuid":"e7e09e34f28daac2d4e7dca1f58f9882ef4f3e8a"},"cell_type":"code","source":"fixed_params = {\n    'objective': 'regression_l1',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'random_seed': 19,\n    'n_estimators': 20000,\n}\n\nparam_grid = {\n    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n    'num_leaves': list(range(8, 92, 4)),\n    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n    'min_data_in_leaf': [10, 20, 40, 60, 100],\n    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n}\n\nbest_score = 999\ndataset = lgb.Dataset(X_train, label=y_train)  # no need to scale features\n\nfor i in range(500):\n    params = {k: random.choice(v) for k, v in param_grid.items()}\n    params.update(fixed_params)\n    result = lgb.cv(params, dataset, nfold=5, early_stopping_rounds=100,\n                    stratified=False)\n    \n    if result['l1-mean'][-1] < best_score:\n        best_score = result['l1-mean'][-1]\n        best_params = params\n        best_nrounds = len(result['l1-mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08fc682c7ebff2403000a97cf23ea30cda98a65d"},"cell_type":"code","source":"print(\"Best mean score: {:.4f}, num rounds: {}\".format(best_score, best_nrounds))\nprint(best_params)\ngb_oof = make_predictions(lgb.LGBMRegressor(**best_params), X_train.values, target, lgb=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b837edfd77f9eff4b45f452495186d6a2cc11ad"},"cell_type":"markdown","source":"Now let's have a look at the <b>feature importance</b>:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"36c5fa96d7d353285006f7aef366d36eedc41be1"},"cell_type":"code","source":"def plot_feature_importance(features, target, columns):\n    folds = KFold(num_folds, shuffle=True, random_state=2019)\n    importance_frame = pd.DataFrame()\n    for (train_index, valid_index) in folds.split(features, target):\n        reg = lgb.LGBMRegressor(**best_params)\n        reg.fit(features[train_index], target[train_index],\n                early_stopping_rounds=100, verbose=False,\n                eval_set=[(features[train_index], target[train_index]),\n                          (features[valid_index], target[valid_index])])\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = columns\n        fold_importance[\"gain\"] = reg.booster_.feature_importance(importance_type='gain')\n        #fold_importance[\"split\"] = reg.booster_.feature_importance(importance_type='split')\n        importance_frame = pd.concat([importance_frame, fold_importance], axis=0)\n        \n    mean_importance = importance_frame.groupby('feature').mean().reset_index()\n    mean_importance.sort_values(by='gain', ascending=True, inplace=True)\n    trace = go.Bar(y=mean_importance.feature, x=mean_importance.gain,\n                   orientation='h', marker=dict(color='rgb(49,130,189)'))\n\n    layout = go.Layout(\n        title='Feature importance', height=1200, width=800,\n        showlegend=False,\n        xaxis=dict(\n            title='Importance by gain',\n            titlefont=dict(size=14, color='rgb(107, 107, 107)'),\n            domain=[0.25, 1]\n        ),\n    )\n\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \nplot_feature_importance(X_train.values, target, X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93cf93c87f894606c8c9f76770c5bbda9b01a8f7"},"cell_type":"markdown","source":"<h2>4. Blending</h2>\n\nWork in progress...."},{"metadata":{"_uuid":"7bf1e7fc30de8ba4e4a609af578f562418f5ed4b"},"cell_type":"markdown","source":"<h2>5. Test data and submission</h2>"},{"metadata":{"trusted":true,"_uuid":"74e6d87f69f6c26d5d84595b11e36a184862bcfb"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e9c6c4a4f5184eda4c16d0c9c8a0ebd46f11abe"},"cell_type":"code","source":"for seg_id in X_test.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data']  # pd series\n    \n    X_test.loc[seg_id, 'ave'] = x.values.mean()\n    X_test.loc[seg_id, 'std'] = x.values.std()\n    X_test.loc[seg_id, 'max'] = x.values.max()\n    X_test.loc[seg_id, 'min'] = x.values.min()\n    X_test.loc[seg_id, 'q90'] = np.quantile(x.values, 0.90)\n    X_test.loc[seg_id, 'q95'] = np.quantile(x.values, 0.95)\n    X_test.loc[seg_id, 'q99'] = np.quantile(x.values, 0.99)\n    X_test.loc[seg_id, 'q05'] = np.quantile(x.values, 0.05)\n    X_test.loc[seg_id, 'q10'] = np.quantile(x.values, 0.10)\n    X_test.loc[seg_id, 'q01'] = np.quantile(x.values, 0.01)\n    \n    X_test.loc[seg_id, 'abs_max'] = np.abs(x.values).max()\n    X_test.loc[seg_id, 'abs_mean'] = np.abs(x.values).mean()\n    X_test.loc[seg_id, 'abs_std'] = np.abs(x.values).std()\n    X_test.loc[seg_id, 'trend'] = add_trend_feature(x.values)\n    X_test.loc[seg_id, 'abs_trend'] = add_trend_feature(x.values, abs_values=True)\n    \n    # New features - rolling features\n    for w in [10, 50, 100, 1000]:\n        x_roll_std = x.rolling(w).std().dropna().values\n        x_roll_mean = x.rolling(w).mean().dropna().values\n        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n        \n        X_test.loc[seg_id, 'ave_roll_std_' + str(w)] = x_roll_std.mean()\n        X_test.loc[seg_id, 'std_roll_std_' + str(w)] = x_roll_std.std()\n        X_test.loc[seg_id, 'max_roll_std_' + str(w)] = x_roll_std.max()\n        X_test.loc[seg_id, 'min_roll_std_' + str(w)] = x_roll_std.min()\n        X_test.loc[seg_id, 'q01_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.01)\n        X_test.loc[seg_id, 'q05_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.05)\n        X_test.loc[seg_id, 'q10_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.10)\n        X_test.loc[seg_id, 'q95_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.95)\n        X_test.loc[seg_id, 'q99_roll_std_' + str(w)] = np.quantile(x_roll_std, 0.99)\n        \n        X_test.loc[seg_id, 'ave_roll_mean_' + str(w)] = x_roll_mean.mean()\n        X_test.loc[seg_id, 'std_roll_mean_' + str(w)] = x_roll_mean.std()\n        X_test.loc[seg_id, 'max_roll_mean_' + str(w)] = x_roll_mean.max()\n        X_test.loc[seg_id, 'min_roll_mean_' + str(w)] = x_roll_mean.min()\n        X_test.loc[seg_id, 'q01_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.01)\n        X_test.loc[seg_id, 'q05_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.05)\n        X_test.loc[seg_id, 'q95_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.95)\n        X_test.loc[seg_id, 'q99_roll_mean_' + str(w)] = np.quantile(x_roll_mean, 0.99)\n        \n        X_test.loc[seg_id, 'ave_roll_abs_mean_' + str(w)] = x_roll_abs_mean.mean()\n        X_test.loc[seg_id, 'std_roll_abs_mean_' + str(w)] = x_roll_abs_mean.std()\n        X_test.loc[seg_id, 'max_roll_abs_mean_' + str(w)] = x_roll_abs_mean.max()\n        X_test.loc[seg_id, 'min_roll_abs_mean_' + str(w)] = x_roll_abs_mean.min()\n        X_test.loc[seg_id, 'q01_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.01)\n        X_test.loc[seg_id, 'q05_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n        X_test.loc[seg_id, 'q95_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n        X_test.loc[seg_id, 'q99_roll_abs_mean_' + str(w)] = np.quantile(x_roll_abs_mean, 0.99)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d58bae69cf6a4c47a0df669239191a309b700d09"},"cell_type":"markdown","source":"Using predictions from the model with the highest validation score (gradient boosting):"},{"metadata":{"trusted":true,"_uuid":"eca824c317d8aa9f14aba2ed04f6d08e783c1827"},"cell_type":"code","source":"gb_oof, gb_sub = make_predictions(lgb.LGBMRegressor(**best_params),\n                                  X_train.values, target, X_test,\n                                  plot=False, lgb=True)\nsubmission['time_to_failure'] = gb_sub\nsubmission.to_csv('submission_gb.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58cceb12ceffca113c6f2df435b0b00411210fd5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}