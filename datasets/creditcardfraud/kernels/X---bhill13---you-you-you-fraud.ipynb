{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0c293f45-8af2-9fde-8c64-7f1f0f26cbb9"
      },
      "source": [
        "How does one exactly detect credit card fraud? Let's examine a couple of ways in which we can. Two methods I can think of immediately are:\n",
        "\n",
        "* Change point detection (i.e. looking at statistically significant changes in credit card expenditures)\n",
        "* Supervised learning, creating a classifier (logistic regression, decision trees, etc) to predict whether or not a transaction will be fraudulent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a6527d8-a725-5aed-8a6e-5a32a704913b"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4b3825ca-a1c4-303b-cd35-cf1db1022144"
      },
      "source": [
        "Let's load in and get to know the data a bit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8cdc619e-d244-e371-c3b6-4d4a83721321"
      },
      "outputs": [],
      "source": [
        "# load in data\n",
        "credit = pd.read_csv('../input/creditcard.csv')\n",
        "\n",
        "# useful printouts\n",
        "print (credit.head(10))\n",
        "print (\"============================\")\n",
        "print (credit.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8281e23-1f9c-b956-a2b5-dfa402c5239b"
      },
      "source": [
        "The thing most people would be most immediately interested in are the amounts. Let's look at cumulative transaction amounts across time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "747e4ab2-50ec-b9ac-45c7-bb2a998ad4f6"
      },
      "outputs": [],
      "source": [
        "# split data set into fraudulent and non-fraudulent sets \n",
        "fraud = credit[credit['Class'] == 1]\n",
        "good = credit[credit['Class'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af3b425e-8b4b-94e8-56d7-bc70b86be2d5"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "\n",
        "fig, ([ax1, ax2, ax3]) = plt.subplots(3, 1, sharex=True)\n",
        "\n",
        "ax1.plot(fraud['Time'], np.cumsum(fraud['Amount']))\n",
        "ax1.set_title('Cumulative Fraudulent Transaction Amount')\n",
        "ax1.set_xlabel('Time')\n",
        "\n",
        "ax2.plot(good['Time'], np.cumsum(good['Amount']))\n",
        "ax2.set_title('Cumulative Non-Fraudulent Transaction Amount')\n",
        "\n",
        "ax3.plot(credit['Time'], np.cumsum(credit['Amount']))\n",
        "ax3.set_title('Cumulative Transaction Amount')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8926f754-6f1a-653d-3da9-985f3c410ec1"
      },
      "source": [
        "They have the same basic trend, but the jumps in the graph of the fraudulent transactions are more profound (only due to the fact there are only 490 such observations). The non-fraudulent transactions as well as transactions for the entire set look about the same. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5aaaff20-7b6d-9278-7d46-44b6cc19be2a"
      },
      "outputs": [],
      "source": [
        "# grab ratio of fraudulent transactions to good transactions\n",
        "percentage = len(fraud)/float(len(good))\n",
        "# undersample by this percentage, set seed to 1 for reproducibility\n",
        "np.random.seed(1)\n",
        "# subsample of good transactions will be some percentage of all the good transactions\n",
        "good_trans = good.take(np.random.permutation(len(good))[:round(percentage*len(good))])\n",
        "# fraudulent transactions are fraudulent\n",
        "fraud_trans = fraud\n",
        "# combine into a new frame, resetting the index\n",
        "cred_data = pd.concat([good_trans, fraud_trans], ignore_index= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "292a5853-73ed-bb13-e061-01890cdeb6c1"
      },
      "outputs": [],
      "source": [
        "target = 'Class'\n",
        "features = cred_data.columns[1:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd32abdd-3860-c3d5-12da-5fe9237ae43b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_and_val, test = train_test_split(cred_data, test_size = 0.1)\n",
        "train, val = train_test_split(train_and_val, test_size = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e428901e-b78d-226b-544a-cd5bf9991886"
      },
      "source": [
        "# Train a logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c56bedd5-d385-335f-5a46-abe2b02f853d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logit = LogisticRegression()\n",
        "logit.fit(train[features], train[target]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad1e4b5a-988a-39bd-a604-952533379ac2"
      },
      "outputs": [],
      "source": [
        "# import tools for evaluating performance of classifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "420a4283-c05f-1b40-bdf6-06c6420ee464"
      },
      "outputs": [],
      "source": [
        "prediction = logit.predict(test[features])\n",
        "actual = test[target]\n",
        "# accuracy \n",
        "acc = accuracy_score(actual, prediction)\n",
        "# precision\n",
        "prec = precision_score(actual, prediction)\n",
        "# recall\n",
        "rec = recall_score(actual, prediction)\n",
        "# F1 score\n",
        "f1 = f1_score(actual, prediction)\n",
        "\n",
        "print (\"The accuracy is: %0.2f.\" %acc)\n",
        "print (\"The precision is: %0.2f.\" %prec)\n",
        "print (\"The recall is: %0.2f.\" %rec)\n",
        "print (\"The F1 score is: %0.2f.\" %f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "67a51199-c4da-44b5-afb0-26689d420836"
      },
      "outputs": [],
      "source": [
        "def plot_f1_scores(train_data, validation_data, features, target, reg_params):\n",
        "    f1_scores = []\n",
        "    for c in reg_params:\n",
        "        logit = LogisticRegression(C = c, penalty = 'l2')\n",
        "        logit.fit(train_data[features], train_data[target])\n",
        "        predicts = logit.predict(validation_data[features])\n",
        "        f1 = f1_score(validation_data[target], predicts)\n",
        "        f1_scores.append(f1)\n",
        "        \n",
        "    plt.plot(reg_params, f1_scores)\n",
        "    plt.xlabel('Regularization Parameter')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4074d373-56f8-f5e1-054c-2573eb0a9f79"
      },
      "outputs": [],
      "source": [
        "plot_f1_scores(train, val, features, target, np.arange(1,11))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8ed1e8b-1a00-5feb-08fb-7faeb04a2c3d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}