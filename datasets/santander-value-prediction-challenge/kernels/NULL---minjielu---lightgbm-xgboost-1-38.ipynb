{"cells":[{"metadata":{"trusted":true,"_uuid":"5feb395d9665b115ab27fdd2d7880af5b2a99fc0"},"cell_type":"code","source":"# Load python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, learning_curve, validation_curve, GridSearchCV\nfrom skopt import BayesSearchCV\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\nimport sys\nsys.path.append('/Users/minjielu/anaconda3/envs/python/lib/python3.5/site-packages')\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew, kurtosis\nfrom matplotlib import pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"379e20fffcc9cd6d1df9cbb66944e915ac802dd3"},"cell_type":"code","source":"%%time\n# Load data\ndata = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3da16b85f7786026ea2f59698a1445a16d9755ad"},"cell_type":"code","source":"# take a look at train and test data\nprint('train data size: {}*{}'.format(data.shape[0],data.shape[1]))\nprint('test data size: {}*{}'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5e12e4d1ac2c88cf805149bff082206360d4beb"},"cell_type":"code","source":"data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d4734ca7c9f572ed4cd4ad0ec2bce188c157f78"},"cell_type":"code","source":"test.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8116dda7d50dc0040e3771a51f2793dd6c20e64"},"cell_type":"markdown","source":"## 1. Feature engineering"},{"metadata":{"trusted":true,"_uuid":"4514bf1d1839d51b6a37f1f9f887e8f7188a3f3d"},"cell_type":"code","source":"# Take out magic features discovered by olivier\nmagic_features = ['f190486d6', 'c47340d97', 'eeb9cd3aa', '66ace2992', 'e176a204a',\n        '491b9ee45', '1db387535', 'c5a231d81', '0572565c2', '024c577b9',\n        '15ace8c9f', '23310aa6f', '9fd594eec', '58e2e02e6', '91f701ba2',\n        'adb64ff71', '2ec5b290f', '703885424', '26fc93eb7', '6619d81fc',\n        '0ff32eb98', '70feb1494', '58e056e12', '1931ccfdd', '1702b5bf0',\n        '58232a6fb', '963a49cdc', 'fc99f9426', '241f0f867', '5c6487af1',\n        '62e59a501', 'f74e8f13d', 'fb49e4212', '190db8488', '324921c7b',\n        'b43a7cfd5', '9306da53f', 'd6bb78916', 'fb0f5dbfe', '6eef030c1']\n\nfeatures = [f for f in data.columns if f not in ['target', 'ID']]\nmagic_features_loc = [features.index(x) for x in magic_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad14322ab46745c535ecb461cc5434763fff1ceb"},"cell_type":"code","source":"%%time\n# Statistic features and magic features are used as inputs for machine learning algortihms\nfeatures = [f for f in data.columns if f not in ['target', 'ID']]\n\ndef to_hist_func(row):\n    count = row[row != 0].shape[0]\n    hist = []\n    hist.extend(row[magic_features_loc]) # Add Santander 46 magic features.\n    # Replace 0 values with null, this procedure seems to improve the performance of regressions.\n    # hist[hist == 0] = np.nan\n    # When statistic features are calculated, zero values are removed.\n    row = row[row != 0]\n    hist.append(np.min(row)) # Add the minimum\n    # hist.append(np.percentile(row,10)) # Add percentiles\n    # hist.append(np.percentile(row,20))\n    hist.append(np.percentile(row,25))\n    # hist.append(np.percentile(row,30))\n    # hist.append(np.percentile(row,40))\n    hist.append(np.percentile(row,50))\n    # hist.append(np.percentile(row,60))\n    # hist.append(np.percentile(row,70))\n    hist.append(np.percentile(row,75))\n    # hist.append(np.percentile(row,80))\n    # hist.append(np.percentile(row,90))\n    hist.append(np.max(row)) # Add the maximum\n    # hist.append(np.mean(row)) # Add the mean\n    # hist.append(np.median(row)) # Add the median\n    # hist.append(np.sum(row)) # Add sum\n    # Add fine histogram.\n    # for x in np.arange(8,17,0.2):\n        # hist.append(row[(row < x+1) & (row >= x)].shape[0])\n    # Add coarse histogram.\n    # for x in np.arange(8,17,1):\n        # hist.append(row[(row < x+2) & (row >= x)].shape[0])\n    # hist.append(row[(row < 23) & (row >= 20)].shape[0])\n    hist.append(count)  # Add the number of nonzero features\n    hist.append(skew(row)) # Add the skewness\n    hist.append(kurtosis(row)) # Add the kurtosis\n    '''\n    # One observation is that there are lots of repeated values\n    # Therefore, statistic features are also extracted after these repeated values are removed\n    row_unique = np.unique(row)\n    hist.append(np.min(row_unique))\n    hist.append(np.percentile(row_unique,10))\n    hist.append(np.percentile(row_unique,20))\n    hist.append(np.percentile(row_unique,25))\n    hist.append(np.percentile(row_unique,30))\n    hist.append(np.percentile(row_unique,40))\n    hist.append(np.percentile(row_unique,50))\n    hist.append(np.percentile(row_unique,60))\n    hist.append(np.percentile(row_unique,70))\n    hist.append(np.percentile(row_unique,75))\n    hist.append(np.percentile(row_unique,80))\n    hist.append(np.percentile(row_unique,90))\n    hist.append(np.max(row_unique))\n    for x in np.arange(8,17,0.2):\n        hist.append(row_unique[(row_unique < x+1) & (row_unique >= x)].shape[0])\n    for x in np.arange(8,17,1):\n        hist.append(row_unique[(row_unique < x+2) & (row_unique >= x)].shape[0])\n    hist.append(row_unique[(row_unique < 23) & (row_unique >= 20)].shape[0])\n    hist.append(len(row_unique)) # Add the number of unique values.\n    hist.append(skew(row_unique))\n    hist.append(kurtosis(row_unique))\n    '''\n    pdrow = pd.Series(row)\n    # Add the three most frequent values. If there is not enough unique values, zeroes or nans are used instead\n    unique_values = pdrow.value_counts()\n    hist.append(unique_values.index[0])\n    if unique_values.shape[0] == 1:\n        hist.extend([0,0]) \n        # hist.extend([np.nan,np.nan])\n        return hist\n    hist.append(unique_values.index[1])\n    if unique_values.shape[0] == 2:\n        hist.extend([0])\n        # hist.extend([np.nan])\n        return hist\n    hist.append(unique_values.index[2])\n    return hist\n\n\n# Generate statistic features for train data\nhist_data = np.apply_along_axis(\n    func1d=to_hist_func, \n    axis=1, \n    arr=(np.log1p(data[features])).astype(float)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da8d892c52a08b7f0cc264669c421d5cb53be5d2"},"cell_type":"code","source":"%%time\n# Generate statistic features for test data\nhist_test = np.apply_along_axis(\n    func1d=to_hist_func, \n    axis=1, \n    arr=(np.log1p(test[features])).astype(float))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e833e4556cf3c2151661e72ef6430b582cb068cb"},"cell_type":"markdown","source":"## 3. Plot learning curves"},{"metadata":{"trusted":true,"_uuid":"c0f8a56a3eff9a0645f1fa7bcdc54aadfe606474"},"cell_type":"code","source":"# Define a score function that returns mean squared log error, since the built-in mean_squared_log_error of sklearn somehow doesn't work\ndef my_own_score(ground_truth,predictions):\n    return (mean_squared_error(ground_truth,predictions) ** .5)\n    \nscore = make_scorer(my_own_score,greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64cd0c2b7641b34fdd633093d4a3a4e14f90e40a"},"cell_type":"code","source":"def plot_learning_curve(regressor, title, x, y, score):\n    train_sizes, train_scores, test_scores = learning_curve(regressor, x, y, scoring=score, train_sizes = np.linspace(0.1,1.0,7), cv = 5)\n    plt.figure()\n    plt.title(title)\n    plt.xlabel('Number of samples')\n    plt.ylabel('score')\n    plt.grid()\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab8e6ad97e8a55482321a16a9ab83df4393df0df"},"cell_type":"code","source":"# Define regressors using the best parameters found by grid search or Bayesian search\nmyET = ExtraTreesRegressor(n_estimators=1000, max_features=.8, max_depth=10, min_samples_leaf=5, random_state=3, n_jobs=-1)\nmyKNN = KNeighborsRegressor(n_neighbors=20, weights='distance', algorithm='kd_tree', n_jobs=-1)\nmyDT = DecisionTreeRegressor(criterion='friedman_mse', splitter='random', min_samples_leaf=15)\nmyRF = RandomForestRegressor(n_estimators=2000, max_features=.3, max_depth=10, min_samples_leaf=5,random_state=3,n_jobs=-1)\nmyADBoost = AdaBoostRegressor(n_estimators=100, learning_rate=0.05, loss='linear', random_state=3)\n# mylgb = lgb.LGBMRegressor(num_leaves=58,subsample=.4,colsample_bytree=.4,max_depth=10,learning_rate=0.05,objective='regression',random_state=3,boosting_type='gbdt',seed=3,min_child_weight=np.power(10,-0.1477),reg_lambda=np.power(10,1.7570),reg_alpha=np.power(10,-2.2887),min_split_gain=np.power(10,-2.5988))\n# For data with leak items added\n# mylgb = lgb.LGBMRegressor(objective='regression', random_state=3, learning_rate=0.0353, max_bin=386, max_depth=19, min_child_samples=19,min_child_weight=4, min_split_gain=0.464, n_estimators=124, num_leaves=51, reg_alpha=0.000246, reg_lambda=0.0001696, subsample_freq=4)\n# myxgb = xgb.XGBRegressor(objective='reg:linear',booster='gbtree',seed=3,colsample_bylevel=0.477,colsample_bytree=0.1,gamma=0.119,learning_rate=0.0602,max_depth=9,min_child_weight=100,n_estimators=150,reg_lambda=1e-9,subsample=1.0)\n# For data with train leak removed\n# mylgb = lgb.LGBMRegressor(objective='regression', random_state=3, learning_rate=0.0237, max_bin=100, max_depth=50, min_child_samples=7,min_child_weight=2, min_split_gain=0.001, n_estimators=150, num_leaves=10, reg_alpha=1e-9, reg_lambda=3.12e-8, subsample_freq=0)\n# myxgb = xgb.XGBRegressor(objective='reg:linear',booster='gbtree',seed=3,colsample_bylevel=0.117,colsample_bytree=0.524,gamma=0.00885,learning_rate=0.147,max_depth=6,min_child_weight=76,n_estimators=86,reg_lambda=37.56,subsample=0.948)\nmylgb = lgb.LGBMRegressor(num_leaves=60,subsample=.4,colsample_bytree=.6,max_depth=2,learning_rate=0.1,objective='regression',random_state=3,boosting_type='gbdt',seed=3,min_child_weight=np.power(10,-0.1477),reg_lambda=np.power(10,1.7570),reg_alpha=np.power(10,-2.2887),min_split_gain=np.power(10,-2.5988))\nmyxgb = xgb.XGBRegressor(objective='reg:linear',booster='gbtree',seed=3,colsample_bylevel=0.44,colsample_bytree=0.53,gamma=1.98e-3,learning_rate=0.0355,max_depth=44,min_child_weight=79,n_estimators=144,reg_lambda=0.0355,subsample=1.0)\n\n# Plot learning curves for all regressors\nplot_learning_curve(myET,'ExtraTrees regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(myKNN,'KNN regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(myDT,'DecisionTree regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(myRF,'RandomForest regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(myADBoost,'AdaBoost regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(mylgb,'LightGBM regressor learning curve',hist_data,np.log1p(data['target']),score)\nplot_learning_curve(myxgb,'XGBoost regressor learning curve',hist_data,np.log1p(data['target']),score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"829f971eabb85d408766338c5ad74958f7645828"},"cell_type":"markdown","source":"## 4. Stacking"},{"metadata":{"trusted":true,"_uuid":"efb7a579c34c3bdd843f5de3ccd2286127e41382"},"cell_type":"code","source":"# Generate meta features using selected regressors\ndef generate_meta_features(regressor,x,y,z):\n    folds = KFold(n_splits=5,shuffle=True,random_state=1)\n    oof_preds = np.zeros(x.shape[0])\n    test_preds = np.zeros(z.shape[0])\n    \n    for n_fold, (trn_, val_) in enumerate(folds.split(x)):\n        regressor.fit(x[trn_],y[trn_])\n        oof_preds[val_] = regressor.predict(hist_data[val_])\n        test_preds += regressor.predict(z) / folds.n_splits\n        \n    return oof_preds,test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b54486221b254dc08514eab65f8b934e6764e9b6"},"cell_type":"code","source":"ET_meta,ET_test_meta = generate_meta_features(myET,hist_data,np.log1p(data['target']),hist_test)\nKNN_meta,KNN_test_meta = generate_meta_features(myKNN,hist_data,np.log1p(data['target']),hist_test)\nDT_meta,DT_test_meta = generate_meta_features(myDT,hist_data,np.log1p(data['target']),hist_test)\nRF_meta,RF_test_meta = generate_meta_features(myRF,hist_data,np.log1p(data['target']),hist_test)\nADBoost_meta,ADBoost_test_meta = generate_meta_features(myADBoost,hist_data,np.log1p(data['target']),hist_test)\nLGB_meta,LGB_test_meta = generate_meta_features(mylgb,hist_data,np.log1p(data['target']),hist_test)\nXGB_meta,XGB_test_meta = generate_meta_features(myxgb,hist_data,np.log1p(data['target']),hist_test)\nET_meta = pd.Series(ET_meta,name='ET_meta')\nKNN_meta = pd.Series(KNN_meta,name='KNN_meta')\nDT_meta = pd.Series(DT_meta,name='DT_meta')\nRF_meta = pd.Series(RF_meta,name='RF_meta')\nADBoost_meta = pd.Series(ADBoost_meta,name='ADBoost_meta')\nLGB_meta = pd.Series(LGB_meta,name='LGB_meta')\nXGB_meta = pd.Series(XGB_meta,name='XGB_meta')\nET_test_meta = pd.Series(ET_test_meta,name='ET_meta')\nKNN_test_meta = pd.Series(KNN_test_meta,name='KNN_meta')\nDT_test_meta = pd.Series(DT_test_meta,name='DT_meta')\nRF_test_meta = pd.Series(RF_test_meta,name='RF_meta')\nADBoost_test_meta = pd.Series(ADBoost_test_meta,name='ADBoost_meta')\nLGB_test_meta = pd.Series(LGB_test_meta,name='LGB_meta')\nXGB_test_meta = pd.Series(XGB_test_meta,name='XGB_meta')\n\ntrain_meta=pd.concat([ET_meta,KNN_meta,DT_meta,RF_meta,ADBoost_meta,LGB_meta,XGB_meta],axis=1) # Meta features for train data\ntest_meta=pd.concat([ET_test_meta,KNN_test_meta,DT_test_meta,RF_test_meta,ADBoost_test_meta,LGB_test_meta,XGB_test_meta],axis=1) # Meta features for test data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87307b21226d34df17d7d24cd85bc6440ac5311"},"cell_type":"code","source":"for column in train_meta.columns:\n    print('Cross validation score for ' + column + ': {}'.format(mean_squared_error(np.log1p(data['target']),train_meta[column]) ** .5))\ng = sns.heatmap(test_meta[[\"ET_meta\",\"KNN_meta\",\"DT_meta\",\"RF_meta\",\"ADBoost_meta\",'LGB_meta','XGB_meta']].corr(),cmap=\"BrBG\",annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcc8066c2307926caec00f025d402c4ba8ebd5ee"},"cell_type":"markdown","source":"Ensembling models sometimes provides better result than using them individually. In my case, ExtraTrees, RandomForest, LightGBM, and XGBoost have very good cross validation scores on train set. ExtraTrees and RandomForest actually overfits on train set so they provide worse scores on test set. Ensembling LightGBM and XGBoost gives me the best public leaderboard score 1.38."},{"metadata":{"trusted":true,"_uuid":"793e45f08cc750cf6aab67e45f732756afef0c30"},"cell_type":"code","source":"'''\nfor i in np.arange(0,1,0.05):\n    oof_preds = LGB_meta*i+XGB_meta*(1-i)\n    print(str(i)+':'+str(mean_squared_error(oof_preds,np.log1p(data['target'])) ** .5)\n'''\n    \n# sub_preds = (RF_test_meta+LGB_test_meta)/2\n# sub_preds = (ET_test_meta+LGB_test_meta)/2\n# Generate the final result using the average prediction from LightGBM and XGBoost regressor\nsub_preds = (LGB_test_meta+XGB_test_meta)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c2a40ef1481e600f26537ce703a083afec722ca"},"cell_type":"code","source":"# Observe correlation between XGBoost and LightGBM result\ng = sns.regplot(x='LGB_meta',y='XGB_meta',data=test_meta,fit_reg=False)\n_ = g.set_title('XGBoost result versus LightGBM result')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a139c096c34fc103e972cd1304eb35de2efc0f21"},"cell_type":"markdown","source":"Clearly, for a fixed lightGBM prediction, XGBoost prediction has an uncertainty around 1, and vice versa. This may indicate the limit of prediction. This uncertainty can be real fluctuation of a customer's investment."},{"metadata":{"trusted":true,"_uuid":"441e2e837748ff19583695a399996997d0df303f"},"cell_type":"code","source":"#customerid = data['ID']\n#result = pd.Series(gbm.predict(test_x),name='target')\n#result = pd.Series(oof_preds,name='target')\n#result = pd.concat([customerid,result],axis=1)\n#min_value = train_y.min()\n#result.loc[result['target'] < min_value,'target'] = min_value\n#result.to_csv('Santander_train_2.csv',index=False)\n\n\ntest['target'] = np.expm1(sub_preds)\ntest[['ID', 'target']].to_csv('lgb_xgb.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}