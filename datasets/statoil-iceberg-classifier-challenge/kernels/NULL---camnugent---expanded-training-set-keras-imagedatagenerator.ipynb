{"nbformat_minor": 1, "metadata": {"language_info": {"version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "cells": [{"metadata": {"_uuid": "4b9b80f1652295f7bb55a03be8d812c46cb2c53a", "_cell_guid": "5f4fb7b4-99cf-447c-841b-5600e086b7ab"}, "source": ["# Creation of additional data using Keras ImageDataGenerator\n", "\n", "There have been several great Kernels created so far that use Keras to develop convolutional neural networks to classify an image as containing an iceberg or not. I have been using a Keras made cnn, and my work right now focuses on 1. Tweaking the nn setup to optimize predictive ability. 2. Creating more data on which to train the nn.\n", "\n", "In this kernel I discuss how I'm going about accomplishing goal number 2. This is an easy job using Keras, but the task at hand has a little catch. Keras ImageDataGenerator() is designed to take 1, 3, or 4 channels of data... but here we have two input channels. In order to solve this problem, I first add an additional dummy channel to the input data, and then when the data is generated I pull the dummy channel off and go about my nn training.\n", "\n", "\n", "The first section here I draw from 'Exploring the Icebergs with skimage and Keras' by Kevin Mader for the read-in and train/test split. (You can pair the new data you make with that Kernel's cnn for some pretty good results!)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "87438aa8dac2eb7b56ee7db8b8ad55dc5f1ba3f7", "collapsed": true, "_cell_guid": "f66a8a5f-f29a-4969-b3cf-f132c8f400ec"}, "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "\n", "from sklearn.model_selection import train_test_split\n", "from keras.utils.np_utils import to_categorical\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "41cf74583b75ef8304c53f872a60320726e3fdff", "collapsed": true, "_cell_guid": "18ba49dc-1471-4ebd-b981-908f551bd16f"}, "source": ["# load function from: https://www.kaggle.com/kmader/exploring-the-icebergs-with-skimage-and-keras\n", "# b/c I didn't want to reinvent the wheel\n", "def load_and_format(in_path):\n", "\t\"\"\" take the input data in .json format and return a df with the data and an np.array for the pictures \"\"\"\n", "\tout_df = pd.read_json(in_path)\n", "\tout_images = out_df.apply(lambda c_row: [np.stack([c_row['band_1'],c_row['band_2']], -1).reshape((75,75,2))],1)\n", "\tout_images = np.stack(out_images).squeeze()\n", "\treturn out_df, out_images\n", "\n", "\n", "train_df, train_images = load_and_format('../input/train.json')\n", "\n", "test_df, test_images = load_and_format('../input/test.json')"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "78c5608f9e356bee316925f516b6fcad25bb17b1", "_cell_guid": "98ed27f8-65df-4529-b608-22d066ebd7f6"}, "source": ["Note here I am splitting off the validation set before I create the additional training instances, this is a conservative measure (i.e. you could make your new data off of all the training instances provided by moving the train/test split further down the notebook... but be wary of overfit!)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "1b2a8819f1d056559d15db62db4a1ed63319ccd1", "collapsed": true, "_cell_guid": "1d3cecd1-83d1-458c-a83d-85bdab7e753d"}, "source": ["\n", "#also from https://www.kaggle.com/kmader/exploring-the-icebergs-with-skimage-and-keras\n", "X_train, X_test, y_train, y_test = train_test_split(train_images,\n", "\t\t                                            to_categorical(train_df['is_iceberg']),\n", "                                                    random_state = 42,\n", "                                                    test_size = 0.5\n", "                                                   )\n", "print('Train', X_train.shape, y_train.shape)\n", "print('Validation', X_test.shape, y_test.shape)\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "d0c6f5f737a3e7ec13f6f419b6b3affb3f3172ab", "_cell_guid": "9aa89ce5-6674-4ae4-8070-d6b8e3ed9b46"}, "source": ["Below is a dummy channel of all zeros (in the same size as the two true channels)."], "cell_type": "markdown"}, {"metadata": {"_uuid": "72bfcb7c104ff0ce2b944e3064654f6db39a7d5b", "collapsed": true, "_cell_guid": "df055329-7132-47bd-bdeb-1f9c5d547256"}, "source": ["dummy_dat = np.zeros((802,75,75,1), dtype=np.float32)\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "fe68641d24a3b365ae05e2087845c99fc41dfbda", "_cell_guid": "8b645d40-e194-4b32-9194-80e13a055ba6"}, "source": ["This dummy channel is simply concatenated along the fourth axis... upvote if you struggle with visualizing 4 dimensions as much as I do!"], "cell_type": "markdown"}, {"metadata": {"_uuid": "39377e785830b9d7fca73579840d92373b2f3e62", "collapsed": true, "_cell_guid": "6c4bee03-14af-498b-86c3-1c51d5d869ff"}, "source": ["fudge_X_train = np.concatenate((X_train, dummy_dat), axis = 3)"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8d06837d65fabbe59b907c45433dcfbd8ab97cdb", "_cell_guid": "81329ccf-0c13-4496-86b6-fc1030226cad"}, "source": ["Below we initiate the ImageDataGenerator. The params I use can be tweaked to your desire."], "cell_type": "markdown"}, {"metadata": {"_uuid": "b87446001fb1864b8e31edcd21f00470751f95d4", "collapsed": true, "_cell_guid": "32e32aef-f432-4a80-b5ec-67f04636bac3"}, "source": ["from keras.preprocessing.image import ImageDataGenerator\n", "\n", "datagen = ImageDataGenerator(\n", "        rotation_range=40,\n", "        width_shift_range=0.2,\n", "        height_shift_range=0.2,\n", "        rescale=1./255,\n", "        shear_range=0.2,\n", "        zoom_range=0.2,\n", "        horizontal_flip=True,\n", "        fill_mode='nearest')\n", "\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "ae8db8b9294d865ffb33bf5b26aee091cbb2132b", "_cell_guid": "cc35955b-7f4b-42b7-aa98-a99df4283f85"}, "source": ["The data (with the dummy channel) is then fit to the generator.\n", "\n", "I initialize the output arrays as the 'real' inputs (which we append our generated data to)."], "cell_type": "markdown"}, {"metadata": {"_uuid": "a6dc77a381b820ce8dc5c2a496e4cf0150130d2f", "collapsed": true, "_cell_guid": "bfe6205c-6847-4046-a126-f9d48df4936c"}, "source": ["datagen.fit(fudge_X_train)\n", "\n", "x_batches = fudge_X_train\n", "y_batches = y_train"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "bad13eef3150da929b709ac7caafbc23957910b4", "_cell_guid": "7ad8f4ed-26aa-4909-8039-55be469ad0eb"}, "source": ["Here we loop through using the .flow() function to generate batches of modified training data.\n", "The kernel has a batch size of 5 and an # of epochs of 5, these can both be increased to make more data!\n", "\n", "Note the break statement to leave the generator... otherwise it loops forever!"], "cell_type": "markdown"}, {"metadata": {"_uuid": "206b37771ec702a97939ca0fd375b8541f98e827", "collapsed": true, "_cell_guid": "bcdfac41-5655-4007-a7e0-935f92374586"}, "source": ["\n", "epochs = 5\n", "\n", "for e in range(epochs):\n", "\tprint('Epoch', e)\n", "\tbatches = 0\n", "\tper_batch = 5\n", "\tfor x_batch, y_batch in datagen.flow(fudge_X_train, y_train, batch_size=per_batch):\n", "\t\tx_batches = np.concatenate((x_batches, x_batch), axis = 0)\n", "\t\ty_batches = np.concatenate((y_batches, y_batch), axis = 0)\n", "\t\tbatches += 1\n", "\t\tif batches >= len(fudge_X_train) / per_batch:\n", "\t\t\t# we need to break the loop by hand because\n", "\t\t\t# the generator loops indefinitely\n", "\t\t\tbreak\n", "\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4e49b3f10b20414be9e286dedec76b768f7ed2e4", "_cell_guid": "5cc4591d-7816-4604-8bd4-b32eea79262a"}, "source": ["Next we drop the dummy channel and can go about training our cnn with an expaned training data set!"], "cell_type": "markdown"}, {"metadata": {"_uuid": "3bcbdd6bda881909e650c9d2db948eddc6e864a8", "collapsed": true, "_cell_guid": "9426b072-1d20-4596-93c3-c4344b7daac7"}, "source": ["x_train_new = x_batches[:,:,:,:2]\n", "x_train_new.shape"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "cb19654afaea8a0af6e9d10db7836290157b5931", "collapsed": true, "_cell_guid": "81fac3e1-be99-41b0-b56e-002e02d656bd"}, "source": ["y_batches.shape"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e2899acf1f56948d4ff82da8096c478428f2b899", "_cell_guid": "7f2939bc-b59c-4282-b5bd-9f7f7c4b2bdf"}, "source": ["Here is one of the original images:"], "cell_type": "markdown"}, {"metadata": {"_uuid": "40e56935617be4cc38d2282cbae6ba3bbe0d3554", "collapsed": true, "_cell_guid": "6cc98b3a-0931-441f-b2e0-49b1f99657c1"}, "source": ["import matplotlib.pyplot as plt\n", "\n", "plt.imshow(x_train_new[500,:,:,0])"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "524585f2fc3cb4f273943ac1169bf90c7dee6788", "_cell_guid": "fecbc60f-6bbe-4f5d-b63e-5cf05a83e12d"}, "source": ["Do you think that is an iceberg or a boat?\n", "\n", "How about one of the ones we have generated? Fake iceberg or fake boat?"], "cell_type": "markdown"}, {"metadata": {"_uuid": "bb48aabd6e9bc00c7ae424b9fb26a90ce255ba59", "collapsed": true, "_cell_guid": "fd9bce5a-428e-4a8a-b206-708929bf1232"}, "source": ["\n", "plt.imshow(x_train_new[-32,:,:,0])\n", "plt.show()\n"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "ed12dbc386dab58385e063829ef49710d8d04965", "_cell_guid": "71cba08d-4c1a-47a6-af5a-bc0d7eb62ea0"}, "source": ["I am currently working to rejig the cnn params to classify more effictively when presented with these additional data\n", "\n", "I hope you can take this an use it to improve the functionality of your own models by beefing up your training set... good luck to everyone! Take this larger training data set and pipe it into your best cnn!\n"], "cell_type": "markdown"}], "nbformat": 4}