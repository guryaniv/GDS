{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "255068aa-a456-8ae4-b3f8-7ea17affd3f5"
      },
      "source": [
        "#Hypothesis Testing on State Data\n",
        "This is a script I created based on [dmi3kno's analysis of cat112][1].\n",
        "To run this code, you would have to download the Census data from:\n",
        "[https://www.census.gov/popest/data/state/asrh/2015/index.html][2], then copy the data to state_populations.csv, which you could then import in the script.\n",
        "\n",
        "I just manually added the data though to simplify it for anyone wanting to check it out.\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/dmi3kno/allstate-claims-severity/all-the-allstate-states-eda/notebook\n",
        "  [2]: https://www.census.gov/popest/data/state/asrh/2015/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "beb9c0f2-f46d-5cc6-6c10-b90e1c5225f7"
      },
      "source": [
        "#Take 3\n",
        "\n",
        "** Updated **: After unsuccessfully providing statistical significance, I finally realized that the sorting alphabetically was the key which I did not realize when reading the script originally.\n",
        "\n",
        "I initially misunderstood and thought that everything was sorted by count of each state in train_test and mapped those values to sorted populations of the states. Anyway, onward I go...\n",
        "\n",
        "This gives clear statistical significance to the hypothesis. I apologize @dmi3kno and retract my retraction ;) I don't use R (another on my list of things to work on... currently trying to wrap my head around time series to fully grasp your other transformations), so I didn't quite realize that you were taking A-Y,AA-AY,BA and mapping to alphabetical state abbreviation until my prior test was questioned (always welcome). When I realized that, I think I found that state name ordering is the key here. I could be wrong because the code was not on the notebook, but it looked by the plot on the bottom of the notebook that it was ordered by alphabetical abbreviation.\n",
        "\n",
        "Here is what I tried in this third (and hopefully final) iteration:\n",
        "\n",
        "I ordered the states both by name first and by abbreviation second and used a permutation test to see if a random permutation of populations could compete. I assumed that it was random and checked if other random shuffling could produce the same chi^2 result over lots of different iterations. The p-values represent the percentage of how often the chi^2 result was lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea105710-1a66-8ead-ccb9-257bd844ea16"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import sys\n",
        "from scipy.stats import chisquare\n",
        "import matplotlib as mpl\n",
        "if sys.version_info[0] < 3:\n",
        "    from StringIO import StringIO\n",
        "else:\n",
        "    from io import StringIO\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "DATA_TRAIN_PATH = '../input/train.csv'\n",
        "DATA_TEST_PATH = '../input/test.csv'\n",
        "#POP_PATH = '../input/state_populations.csv'\n",
        "POPULATION_DATA = StringIO('state_full,state,population\\nAlabama,AL,'\\\n",
        "+'4779736\\nAlaska,AK,710231\\nArizona,AZ,6392017\\nArkansas,AR,'\\\n",
        "+'2915918\\nCalifornia,CA,37253956\\nColorado,CO,5029196\\nConnecticut,CT,'\\\n",
        "+'3574097\\nDelaware,DE,897934\\nDistrict of Columbia,DC,601723\\nFlorida,FL,'\\\n",
        "+'18801310\\nGeorgia,GA,9687653\\nHawaii,HI,1360301\\nIdaho,ID,'\\\n",
        "+'1567582\\nIllinois,IL,12830632\\nIndiana,IN,6483802\\nIowa,IA,'\\\n",
        "+'3046355\\nKansas,KS,2853118\\nKentucky,KY,4339367\\nLouisiana,LA,'\\\n",
        "+'4533372\\nMaine,ME,1328361\\nMaryland,MD,5773552\\nMassachusetts,MA,'\\\n",
        "+'6547629\\nMichigan,MI,9883640\\nMinnesota,MN,5303925\\nMississippi,MS,'\\\n",
        "+'2967297\\nMissouri,MO,5988927\\nMontana,MT,989415\\nNebraska,NE,'\\\n",
        "+'1826341\\nNevada,NV,2700551\\nNew Hampshire,NH,1316470\\nNew Jersey,NJ,'\\\n",
        "+'8791894\\nNew Mexico,NM,2059179\\nNew York,NY,19378102\\nNorth Carolina,NC'\\\n",
        "+',9535483\\nNorth Dakota,ND,672591\\nOhio,OH,11536504\\nOklahoma,OK,'\\\n",
        "+'3751351\\nOregon,OR,3831074\\nPennsylvania,PA,12702379\\nRhode Island,RI,'\\\n",
        "+'1052567\\nSouth Carolina,SC,4625364\\nSouth Dakota,SD,814180\\nTennessee,TN,'\\\n",
        "+'6346105\\nTexas,TX,25145561\\nUtah,UT,2763885\\nVermont,VT,625741\\nVirginia,VA,'\\\n",
        "+'8001024\\nWashington,WA,6724540\\nWest Virginia,WV,1852994\\nWisconsin,WI,'\\\n",
        "+'5686986\\nWyoming,WY,563626\\n')\n",
        "\n",
        "train = pd.read_csv(DATA_TRAIN_PATH, dtype={'id': np.int32})\n",
        "train['logloss'] = np.log(train['loss'])\n",
        "test = pd.read_csv(DATA_TEST_PATH, dtype={'id': np.int32})\n",
        "pop = pd.read_csv(POPULATION_DATA)\n",
        "\n",
        "translation = list(string.ascii_uppercase)[:-1]\n",
        "for elem_i in translation[:2]:\n",
        "    for elem_j in translation[:25]:\n",
        "        translation.append(elem_i + elem_j)\n",
        "swap_dict_to_num = {'cat112': dict(zip(translation[:51], np.arange(51)))}\n",
        "swap_dict_to_state = {'cat112': dict(zip(np.arange(51), pop.state))}\n",
        "\n",
        "train_test = pd.concat((train.drop(['loss','logloss'], axis=1), test))\\\n",
        "               .reset_index(drop=True).replace(swap_dict_to_num)\\\n",
        "               .replace(swap_dict_to_state)\n",
        "\n",
        "pop_train_test = train_test.groupby(by='cat112')\n",
        "counts = pd.DataFrame(pop_train_test.count()['id']).reset_index()\n",
        "counts.columns = ['state', 'counts']\n",
        "pop = pop.merge(counts, how='left', on='state')\n",
        "pop.index = pop.state\n",
        "pop.drop(['state'], axis=1, inplace=True)\n",
        "\n",
        "### Calculate expected values based on Census Data 2015\n",
        "pop['expected'] = pop.population / float(pop.population.sum()) * pop.counts.sum()\n",
        "\n",
        "### Plot comparison\n",
        "mpl.rc(\"figure\", figsize=(8,4))\n",
        "pop[['counts','expected']].plot.bar()\n",
        "\n",
        "### Perform permutation test on whether randomly shuffled population data\n",
        "### can get a better chi-squared score than order of data\n",
        "### P-value represents percentage of times permuted data gets higher chi^2 score\n",
        "### THIS TEST SORTED BY STATE NAME\n",
        "sorted_expected = pop.expected.values\n",
        "sorted_counts = pop.counts.values\n",
        "chi2_sorted, _ = chisquare(sorted_counts, sorted_expected)\n",
        "\n",
        "np.random.seed = 0\n",
        "count = 0\n",
        "n_tests = 100000\n",
        "chi2_vals = np.zeros(n_tests)\n",
        "for i in range(n_tests):\n",
        "    shuffled_counts = np.random.permutation(sorted_counts)\n",
        "    chi2_vals[i], _ = chisquare(f_obs=shuffled_counts, f_exp=sorted_counts)\n",
        "    if chi2_vals[i] < chi2_sorted:\n",
        "        count += 1\n",
        "p_val = count/float(n_tests)\n",
        "print('P-val for ordered by state name: {}'.format(p_val))\n",
        "print('Chi-squared val for ordered by state name: {}'.format(chi2_sorted))\n",
        "\n",
        "\n",
        "### Now perform same test as if ordered by state abbreviation\n",
        "abbr_order = np.argsort(pop.index)\n",
        "pop_2 = pd.DataFrame(pop.values, index=pop.index.values[abbr_order], columns=pop.columns)\n",
        "pop_2.state_full = pop_2.state_full.values[abbr_order]\n",
        "pop_2.population = pop_2.population.values[abbr_order]\n",
        "pop_2.expected = pop_2.expected.values[abbr_order]\n",
        "pop_2[['counts','expected']].plot.bar()\n",
        "\n",
        "sorted_expected_2 = pop_2.expected.values\n",
        "sorted_counts_2 = pop_2.counts.values\n",
        "chi2_sorted_2, _ = chisquare(sorted_counts_2, sorted_expected_2)\n",
        "\n",
        "count = 0\n",
        "chi2_vals_2 = np.zeros(n_tests)\n",
        "for i in range(n_tests):\n",
        "    shuffled_counts_2 = np.random.permutation(sorted_counts_2)\n",
        "    chi2_vals_2[i], _ = chisquare(f_obs=shuffled_counts_2, f_exp=sorted_counts_2)\n",
        "    if chi2_vals_2[i] < chi2_sorted_2:\n",
        "        count += 1\n",
        "p_val = count/float(n_tests)\n",
        "print('P-val for ordered by state abbr: {}'.format(p_val))\n",
        "print('Chi-squared val for ordered by state abbr: {}'.format(chi2_sorted_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e87df690-9df8-0882-9e58-3bc5186e36da"
      },
      "source": [
        "#Significant\n",
        "\n",
        "Even by looking at the graphs, the model of sorting by state name looks a lot closer. The hypothesis is significant after all. While this turned out to be an extended exercise in significance testing, I think that it is clear from both the plots and the lower chi-squared value that the variables are mapped to alphabetical state names and not alphabetical state abbreviations, which might help to wrap our heads around some of the other variables at some point."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}