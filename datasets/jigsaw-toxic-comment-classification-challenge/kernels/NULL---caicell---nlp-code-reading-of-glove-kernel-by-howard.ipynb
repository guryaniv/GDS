{"cells":[{"metadata":{"_cell_guid":"9d2dbdb3-6c74-4f96-9865-2951dfd653ce","_uuid":"bb41ad86b25fecf332927b0c8f55dd710101e33f"},"cell_type":"markdown","source":"# Beginner of Natural Language\nI saw the beatuiful kerenl written by Howard'. It was so readable but I can't correctly catch the menaing of each function since I'm a beginner of beginner of NLP. So that I make this kerenl to interpret the each line.  \n\n** I really thanks for sharing the original code Howard in Kaggle**  \n*If it is unpolite and unlegal way, I delete it in soon*\n\n### Bold Setence is attached by me ###"},{"metadata":{"_cell_guid":"2f9b7a76-8625-443d-811f-8f49781aef81","_uuid":"598f965bc881cfe6605d92903b758778d400fa8b","trusted":true},"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\npath = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"98f2b724-7d97-4da8-8b22-52164463a942","_uuid":"b62d39216c8d00b3e6b78b825212fd190757dff9"},"cell_type":"markdown","source":"Set some basic config parameters & Read in our data and replace missing values:"},{"metadata":{"_cell_guid":"2807a0a5-2220-4af6-92d6-4a7100307de2","_uuid":"d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3","collapsed":true,"trusted":true},"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use\ntrain = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"54a7a34e-6549-45f7-ada2-2173ff2ce5ea","_uuid":"e8810c303980f41dbe0543e1c15d35acbdd8428f"},"cell_type":"markdown","source":"Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b80bf24049bdc14060f7fcb1d0823ec11fea8c35"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"2e4e5ff067de20aa749894aad0d9fd0d1c670800"},"cell_type":"markdown","source":"**What is acted behind the function, tokenizer? I want to see it :)**"},{"metadata":{"trusted":true,"_uuid":"b76b6d57e1c1929e508b6cff24e2abaf9435bef6"},"cell_type":"code","source":"train_list = [['I', 'love', 'you', 'all'], ['I', 'love', 'love', 'favor', 'of', 'you']]\ntokenizer_test = Tokenizer(num_words = 100)\ntokenizer_test.fit_on_texts(train_list)\ntrain_tokenized_list = tokenizer_test.texts_to_sequences(train_list)\nprint(train_tokenized_list)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"d2e7249560ccf627ae15d3c2172c71c4e3c49254"},"cell_type":"markdown","source":"**Did you see the 'love' which appeared three times (the frequentists) indexed as 1 and the I, you appeared two times indexed by 2,3, on the order in the sentence.**"},{"metadata":{"trusted":true,"_uuid":"7702eb2121610423671ffd9e103a7476009a6b32"},"cell_type":"code","source":"print('Train')\nprint('row len :', len(list_tokenized_train))\nprint('max word num :', max([len(row) for row in list_tokenized_train]))\nprint('min word num : ', min([len(row) for row in list_tokenized_train]))\nprint('Test')\nprint('row len :', len(list_tokenized_test))\nprint('max word num :', max([len(row) for row in list_tokenized_test]))\nprint('min word num : ', min([len(row) for row in list_tokenized_test]))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"79afc0e9-b5f0-42a2-9257-a72458e91dbb","_uuid":"c292c2830522bfe59d281ecac19f3a9415c07155","collapsed":true,"trusted":true},"cell_type":"code","source":"X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"9dd0b043cc83061528504baa52f4b6b5255e0e6c"},"cell_type":"markdown","source":"** What happend by pad_sequnces and max_len?**"},{"metadata":{"trusted":true,"_uuid":"07f579162273b6c30ec402961e6a1143e8803fdf"},"cell_type":"code","source":"print('Number of Word: ', len(list_tokenized_train[0]))\nprint(list_tokenized_train[0])\nprint('Number of Element: ',X_t[0].shape[0])\nprint(X_t[0])","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"79e6af108c566620f58b4170df9d3641291da83f"},"cell_type":"markdown","source":"100 - 47 = 53 zero appended to left"},{"metadata":{"trusted":true,"_uuid":"d41fd4a4aed0ab8f3440bf5e7b4f23500b489fdb"},"cell_type":"code","source":"ix_over_100 = -1\nfor i in range(len(list_tokenized_train)):\n    if len(list_tokenized_train[i]) > 100:\n        ix_over_100 = i\n        break\nprint('Number of Word: ', len(list_tokenized_train[ix_over_100]))\nprint(list_tokenized_train[ix_over_100])\nprint('Number of Element: ',X_t[ix_over_100].shape[0])\nprint(X_t[ix_over_100])","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"0d495d1bd942b44988f557195ae3fcdb69ffde03"},"cell_type":"markdown","source":"Cut off the first 14 elements. So what is the first 14 elements? "},{"metadata":{"trusted":true,"_uuid":"155436ad7c69bfe41c5a04896e67da21efb886a3"},"cell_type":"code","source":"train_list = [['I', 'love', 'you', 'all'], ['I', 'am', 'in', 'favor', 'of', 'you']]\ntokenizer_test = Tokenizer(num_words = 100)\ntokenizer_test.fit_on_texts(train_list)\ntrain_tokenized_list = tokenizer_test.texts_to_sequences(train_list)\nprint(train_tokenized_list)\nt_X = pad_sequences(train_tokenized_list, maxlen=2)\nprint(t_X)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"202845e60bfe0c2790a745268087498a457b1a10"},"cell_type":"markdown","source":"**When max_len = 2, the pad_sequences leave the end of two words. So that when 114 len and 100 max_len, cut off the first 14 words :)**"},{"metadata":{"_cell_guid":"f8c4f6a3-3a19-40b1-ad31-6df2690bec8a","_uuid":"e1cb77629e35c2b5b28288b4d6048a86dda04d78"},"cell_type":"markdown","source":"Read the glove word vectors (space delimited strings) into a dictionary from word->vector."},{"metadata":{"_cell_guid":"7d19392b-7750-4a1b-ac30-ed75b8a62d52","_uuid":"e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"b594045197476a624d1aa7b2a65123a936d6700d"},"cell_type":"markdown","source":"**What coefs inserted into the embeddings_index?**"},{"metadata":{"trusted":true,"_uuid":"01c8a5569fc8041846f2f67e56b5bc55e7d5a1df"},"cell_type":"code","source":"for i, (key, val) in enumerate(embeddings_index.items()):\n    print(key, len(val))\n    print('the 5 coefficetions for {}: {}'.format(key, val[:5]))\n    if i == 4: break","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"9ccae70ee75e1ea0e634d676193ee013f1e2ec79"},"cell_type":"markdown","source":"** Embedding File name is 'glove.6B.50d.txt.' It looks like that the glove.XB.Yd.txt, X is number of words, and Y is coefficient dimensions.**"},{"metadata":{"_cell_guid":"7370416a-094a-4dc7-84fa-bdbf469f6579","_uuid":"20cea54904ac1eece20874e9346905a59a604985"},"cell_type":"markdown","source":"Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."},{"metadata":{"trusted":true,"_uuid":"f62ba5773b7de4ce14015593c65d4112ab4de14b"},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"c79fa65af59a979c308fa1a407d0912901366815"},"cell_type":"markdown","source":"**What happend behind np.stack(embeddings_index.values())?**  \nembeddings_index is a dictionary. I try to experiment by a small dictionary"},{"metadata":{"trusted":true,"_uuid":"ff2f4c36adc8e32ee5121850784384c95ec89a6b"},"cell_type":"code","source":"test = {'a':[1,2,3], 'b':[4,5,6]}\nprint(test.values())\nprint(np.stack(test.values()))","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"4d29d827-377d-4d2f-8582-4a92f9569719","_uuid":"96fc33012e7f07a2169a150c61574858d49a561b","collapsed":true,"trusted":false},"cell_type":"markdown","source":"**np.stack helped to make a matrix 60B word x 50 dimensions of coefficents.**"},{"metadata":{"_cell_guid":"62acac54-0495-4a26-ab63-2520d05b3e19","_uuid":"574c91e270add444a7bc8175440274bdd83b7173","collapsed":true,"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"140d80bb409a306bacd61b6e1d6bfac3ac1972e9"},"cell_type":"markdown","source":"**Interpreation**\n- tokenizer.word_index is a function getting an index, as alluded above, which measured by {'the most appearance':1, 'the 2nd appearance':2, ...}\n- nb_words: min(the limitation of the system(tokenzier), the unique number of words) )\n- embedding_matrix: get random coefficients by normal distribution, the size (nb_words X embed_size) where embed_size: neuron numbers(Is it correct? I'm beignner)\n- for func: assign a correct vector mathced with embedding_index of Glove, but a random embedding values if word_index >= max_features == the out of bound, \n\n---"},{"metadata":{"_cell_guid":"f1aeec65-356e-4430-b99d-bb516ec90b09","_uuid":"237345510bd2e664b5c6983a698d80bac2732bc4"},"cell_type":"markdown","source":"Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."},{"metadata":{"_cell_guid":"0d4cb718-7f9a-4eab-acda-8f55b4712439","_uuid":"dc51af0bd046e1eccc29111a8e2d77bdf7c60d28","collapsed":true,"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"c4d1997eb62f0c75931cc2cbc9600b6caef924d9"},"cell_type":"markdown","source":"**Interpretation**  \n- embedding: Initialize the embedding word matrix, weighting to each word\n- Bidirectional: A train model by two direction forward and backward\n- MaxPool: Extract the effective node\n- Dense, activation = 'relu', sigmoid': Get the nonlinear values\n- Dropout: Shrinkage the node size in random\n\n---"},{"metadata":{"_cell_guid":"4a624b55-3720-42bc-ad5a-7cefc76d83f6","_uuid":"e2a0e9ce12e1ff5ea102665e79de23df5caf5802"},"cell_type":"markdown","source":"Now we're ready to fit out model! Use `validation_split` when not submitting."},{"metadata":{"scrolled":false,"_cell_guid":"333626f1-a838-4fea-af99-0c78f1ef5f5c","_uuid":"c1558c6b2802fc632edc4510c074555a590efbd8","collapsed":true,"trusted":true},"cell_type":"code","source":"#model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1);","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"d6fa2ace-aa92-40cf-913f-a8f5d5a4b130","_uuid":"3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"},"cell_type":"markdown","source":"And finally, get predictions for the test set and prepare a submission CSV:"},{"metadata":{"_cell_guid":"28ce30e3-0f21-48e5-af3c-7e5512c9fbdc","_uuid":"e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0","collapsed":true,"trusted":true},"cell_type":"code","source":"#y_test = model.predict([X_te], batch_size=1024, verbose=1)\n#sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n#sample_submission[list_classes] = y_test\n#sample_submission.to_csv('submission.csv', index=False)","execution_count":17,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}