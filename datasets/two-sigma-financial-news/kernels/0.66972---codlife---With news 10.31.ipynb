{
  "cells": [
    {
      "metadata": {
        "_uuid": "512d64e04ad0533dd72ca6bcbca4882a7f289956"
      },
      "cell_type": "markdown",
      "source": "# Market Data Only Baseline\n\nUsing a lot of ideas from NN Baseline Kernel.\nsee. https://www.kaggle.com/christofhenkel/market-data-nn-baseline"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nimport gc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "env = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()\ngc.enable()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e7e45eab0af5570ca16126c8acab76e14fd090d"
      },
      "cell_type": "code",
      "source": "#10:00之后的算成下一天，似乎有不好的影响\n# index = news_train['time'][news_train['time'].dt.hour > 22].index\n# news_train.loc[index,'time']  = news_train.loc[index,'time'].dt.ceil('d')\nnews_train['time'] = news_train['time'].dt.floor('d')\ncols = ['sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize','sentenceCount','wordCount','firstMentionSentence',\n        'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', \n        'volumeCounts12H','volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D','volumeCounts7D']\n\nnews_total = news_train[['time','assetName'] + cols].copy()\ndel news_train\ngc.collect()\nnews_train = news_total\nprint(news_train.columns)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04858b97eb857adcc9f398fbc65535b1cb6340b9"
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(action ='ignore',category = DeprecationWarning)\n\n#直接相乘内存会爆掉，成之后，变成了0.66912，比最好0.66972差了一点，暂时不成\n# for col in cols:\n#     if col != 'relevance':\n#         print(col)\n#         news_train[col] = news_train[col] * news_train['relevance']\n#聚合每一个日期前三天内的新闻数据，影响股价走势\n#之前的版本，直接复制几份，然后和market_train进行join，代价较大\n#直接进行news data的join\ndef get_news_train(raw_data,days = 5):\n    news_last = pd.DataFrame()\n    #衰减系数\n    rate = 1.0\n    for i in range(days):\n        cur_train = raw_data[cols] * rate \n        rate *= 0.7\n        cur_train['time'] = raw_data['time'] + datetime.timedelta(days = i,hours=22)\n        cur_train['key'] = cur_train['time'].astype(str)+ raw_data['assetName'].astype(str)\n        #cur_train的groupby是被迫的操作，处理new_train，6天之内的，内存不足，下面的groupby\n        cur_train = cur_train[['key'] + cols].groupby('key').sum()\n        cur_train['key'] = cur_train.index.values\n        news_last = pd.concat([news_last, cur_train[['key'] + cols]])\n        del cur_train\n        gc.collect()\n        print(\"after concat the shape is:\",news_last.shape)\n        news_last = news_last.groupby('key').sum()\n        news_last['key'] = news_last.index.values\n        print(\"the result shape is:\",news_last.shape)\n       \n    del news_last['key']\n    return news_last\n\nnews_last = get_news_train(news_train)\nprint(news_last.shape)\nprint(news_last.head())\nprint(news_last.dtypes)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f26fb818749b3de15657d8b73cd4bb75b1227e2"
      },
      "cell_type": "code",
      "source": "market_train['key'] = market_train['time'].astype(str) + market_train['assetName'].astype(str)\nmarket_train = market_train.join(news_last,on = 'key',how='left')\nprint(market_train['sentimentNeutral'].isnull().value_counts())\nmarket_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0daf8923ed239a73cb70fc631662cfcea1fd06cb"
      },
      "cell_type": "code",
      "source": "# print(market_train['assetName'].nunique())\n# print(news_train['assetName'].nunique())\n# 通过assetName 判断有market 中有12万个example没在 news中出现,通过时间进行join，交集太少，目前感觉使用\n# assetName比较合适\n# print(market_train['assetName'].isin(news_train['assetName']).value_counts())\n# print(market_train['time'].nunique())\n# print(news_train['time'].nunique())\n# print(news_train['time'].describe())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dfa1843dfece6fccfca91896ef85a332b55e3e6"
      },
      "cell_type": "code",
      "source": "cat_cols = ['assetCode','assetName']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10','sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize',\n            'sentenceCount','wordCount','firstMentionSentence', 'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', \n            'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H','volumeCounts24H', 'volumeCounts3D',\n            'volumeCounts5D','volumeCounts7D']\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e108339134e95473b4a983237d58adb64c3ef64a"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f51d00dc43857b446ae4a24b3718753f09040fd5"
      },
      "cell_type": "markdown",
      "source": "# Handling categorical variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "301a65b834d8614a914883d49be1860550174f06"
      },
      "cell_type": "code",
      "source": "def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\nencoders = [{} for i in range(len(cat_cols))]\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "397360cbacbcb294daf5e0750f471e13ced31978"
      },
      "cell_type": "markdown",
      "source": "# Handling numerical variables"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e6c878e412b3e819e056cee40181b96fbac2f78"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler \nimport matplotlib\n# market_train[num_cols] = market_train[num_cols].fillna(0)\n#异常点过滤\n# print(market_train['close'][market_train['close'] > 1000].count())\n# print(market_train['open'][market_train['open'] > 1000].count())\n# print(market_train['volume'][market_train['volume'] > 1e+08].count())\n\nmarket_train['close'].clip(upper = 1000, inplace = True)\nmarket_train['open'].clip(upper = 1000, inplace = True)\nmarket_train['volume'].clip(upper = 1e+08, inplace = True)\n\n# matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n# prices = pd.DataFrame({\"close\":market_train[\"close\"], \"log(close + 1)\":np.log1p(market_train[\"close\"])})\n# prices.hist(bins = 10)\n# 一定同时进行待预测数据集的对数转换\n# 开盘价，收盘价不太符合正态分布，进行一个对数转换\n# market_train['close'] = np.log1p(market_train['close'])\n# market_train['open'] = np.log1p(market_train['open'])\n\nprint('scaling numerical columns')\nscaler = StandardScaler()\ncol_mean = market_train[num_cols].mean()\nmarket_train[num_cols]=market_train[num_cols].fillna(col_mean)\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n# market_train.describe()\n# market_train[num_cols].isna()\n# market_train['returnsClosePrevMktres1'].isnull().value_counts()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eae09951757845ad5bdf08e004e6477513ed739a"
      },
      "cell_type": "markdown",
      "source": "# Prepare data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb217fc475f418f18720f87a0596aa23eddeb209"
      },
      "cell_type": "code",
      "source": "def get_input(market_train, indices):\n    X = market_train.loc[indices, num_cols]\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\n\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)\nX_train.shape\nprint(X_valid.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ecd18fe6d1a0ecf20678f94d7ba4746a7103b407"
      },
      "cell_type": "markdown",
      "source": "# Train  model using hyperopt to auto hyper_parameters turing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "542f77f1f5675067d810614da9add097266b85a5"
      },
      "cell_type": "code",
      "source": "\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\nfrom sklearn.metrics import mean_squared_error\nalgo = partial(tpe.suggest, n_startup_jobs=10)\n# def auto_turing(args):\n#     #model = XGBClassifier(n_jobs = 4, n_estimators = args['n_estimators'],max_depth=6)\n#     model = lgb.LGBMClassifier(n_estimators=args['n_estimators'])\n#     model.fit(X_train,y_train.astype(int))\n#     confidence_valid = model.predict(X_valid)*2 -1\n#     score = accuracy_score(confidence_valid>0,y_valid)\n#     print(args,score)\n#     return -score\n# space = {\"n_estimators\":hp.choice(\"n_estimators\",range(20,200))}\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=30)\n# print(best)\n\n# 单机xgb程序\n# model = XGBClassifier(n_jobs = 4, n_estimators = 80, max_depth=6, subsample = 0.66,colsample_bytree = 0.66,learning_rate = 0.1)\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\")\n# print(mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# 单机lgb程序,训练比xgb快\n# import lightgbm as lgb\n# model = lgb.LGBMClassifier(num_threads = 4, n_estimators=100, feature_fraction = 0.66, bagging_fraction = 0.66,\n#                            early_stopping_rounds = 10,valid_sets = [X_valid, y_valid.astype(int)],objective = 'binary', metric='binary_logloss')\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\",mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y,args):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"binary_logloss\", \n        \"num_leaves\" : args['num_leaves'],\n        \"min_child_samples\" : args['min_child_samples'],\n        \"learning_rate\" : args['learning_rate'],\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.66,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, args['n_estimators'], valid_sets=[lgval], early_stopping_rounds=50, verbose_eval=100)\n    \n#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    confidence_valid = model.predict(X_valid)*2 -1\n    score = accuracy_score(confidence_valid > 0 , y_valid)\n    print(score)\n    mse = mean_squared_error(confidence_valid > 0, y_valid.astype(float))\n    print(\"MSE\", mse)\n    print(\"args\",args)\n    return model, mse\n\n# def auto_turing(args):\n#     model, mse = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int),args)\n#     return mse\n# space = {\"n_estimators\":hp.choice('n_estimators',range(100,1000)),\n#          \"num_leaves\":hp.choice('num_leaves',range(20,100)),\n#          \"min_child_samples\":hp.choice(\"min_child_samples\",range(20,2000)),\n#          'learning_rate':hp.loguniform('learning_rate',0.01,0.3),\n#          'max_depth': hp.choice('max_depth', range(3,8))\n#         }\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=100)\n# print(best)\nargs = {'learning_rate': 1.0958730495793214, 'max_depth': 7, 'min_child_samples': 301, 'n_estimators': 439, 'num_leaves': 43}\nmodel, _ = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int), args)\n\n# from sklearn.ensemble import RandomForestClassifier\n# distribution of confidence that will be used as submission\n# plt.hist(confidence_valid, bins='auto')\n# plt.title(\"predicted confidence\")\n# plt.show()\n# these are tuned params I found\ngc.collect()\n ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcb2cb97ac4f6b797e9fe9f3cb2682090b1e6850"
      },
      "cell_type": "markdown",
      "source": "Result validation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d8097adcabf5794f04e4ec1f55f2993ef9176cb"
      },
      "cell_type": "code",
      "source": "# calculation of actual metric that is used to calculate final score\nconfidence_valid = model.predict(X_valid)*2 -1\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\nmarket_train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a556452e64bef6bee50b7e281ef65f2923c554a7"
      },
      "cell_type": "markdown",
      "source": "# Prediction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f57d1b1d8cf8bea0594e0d699f1b849566ebad86"
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cea4e659153962bdf2062b0ca10943927549a26"
      },
      "cell_type": "code",
      "source": "n_days = 0\npredicted_confidences = np.array([])\nfrom collections import deque\nnews_pre = deque()\nnews_all = pd.DataFrame()\nBaseMod = 50\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    news_all = pd.concat([news_all,news_obs_df])\n    if n_days >= BaseMod and n_days % BaseMod >= 0 and n_days % BaseMod < 8:\n        news_pre.append(news_obs_df)\n    elif n_days >= BaseMod and n_days % BaseMod == 8:\n        del news_all\n        gc.collect()\n        news_all = pd.DataFrame()\n        for item in news_pre:\n            news_all = pd.concat([news_all,item])\n        news_pre.clear()\n    \n#     index = news_all['time'][news_all['time'].dt.hour > 22].index\n#     news_all.loc[index,'time']  = news_all.loc[index,'time'].dt.ceil('d')\n    news_all['time'] = news_all['time'].dt.floor('d')\n    news_last = pd.DataFrame()\n    \n#     for col in cols:\n#         if col != 'relevance':\n#             print(col)\n#             news_all[col] = news_all[col] * news_all['relevance']\n    #聚合每一个日期前三天内的新闻数据，影响股价走势\n    news_last = get_news_train(news_all)\n\n    market_obs_df['key'] = market_obs_df['time'].astype(str) + market_obs_df['assetName'].astype(str)\n    market_obs_df = market_obs_df.join(news_last,on = 'key',how='left')\n    \n    #异常点过滤\n    market_obs_df['close'].clip(upper = 1000, inplace = True)\n    market_obs_df['open'].clip(upper = 1000, inplace = True)\n    market_obs_df['volume'].clip(upper = 1e+08, inplace = True)\n    \n    # 对数转换\n#     market_obs_df['close'] = np.log1p(market_obs_df['close'])\n#     market_obs_df['open'] = np.log1p(market_obs_df['open'])\n    \n#     col_mean = [num_cols].mean()\n    #归一化\n    market_obs_df[num_cols]=market_obs_df[num_cols].fillna(col_mean)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_test = market_obs_df[num_cols]\n    X_test['assetCode'] = market_obs_df['assetCode'].apply(lambda x: encode(encoders[0], x)).values\n    X_test['assetName'] = market_obs_df['assetName'].apply(lambda x: encode(encoders[1], x)).values\n\n    \n    market_prediction = model.predict(X_test)*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    del news_last\n    gc.collect()\n\nenv.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0b10b1ee21cfa2ff48b7ad2b7351382c03daeaa"
      },
      "cell_type": "code",
      "source": "# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}