{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "metadata": {
        "_cell_guid": "251f8616-5c95-b4dd-f204-1348526fe6c1",
        "_active": false,
        "collapsed": false
      },
      "source": null,
      "execution_count": null,
      "cell_type": "markdown",
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b76f41dc-eac5-89dc-48f1-0ca4fb5a7c71",
        "_active": true,
        "collapsed": false
      },
      "outputs": [],
      "source": "import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport itertools \nimport csv\nimport collections\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\n\n\nsns.set_context(\"paper\")\n%matplotlib inline\n\nRES_DIR = \"../input/\"\n# Load train data (skips the content column)\ndef load_train_data():\n    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n    train_data = []\n    for cat in categories:\n        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags','content'])\n        data['category'] = cat\n        train_data.append(data)\n    \n    return pd.concat(train_data)\ndata = load_train_data()\nprint(data.head())\n\ntest = pd.read_csv(\"../input/test.csv\")\ntest['tags'] = ''\ntest['category'] = 'physics'\ndata=data.append(test)\ndel test\n#data=data.sample(1200)",
      "execution_state": "busy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb34c73c-dcba-aad3-43ae-39c252ee2ae5",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\"\n\n# This could take a while\ndata[\"title\"] = data[\"title\"].map(stripTagsAndUris)\ndata[\"content\"] = data[\"content\"].map(stripTagsAndUris)\nimport string\nprint(data.head())\n\ndef removePunctuation(x):\n    # Lowercasing all words\n    x = x.lower()\n    # Removing non ASCII chars\n    #x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    # Removing (replacing with empty spaces actually) all the punctuations\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n\n# point questionmarks etc\ndata[\"title\"] = data[\"title\"].map(removePunctuation)\ndata[\"content\"] = data[\"content\"].map(removePunctuation)\n#data = clean_dataframe(data)\ndata.to_csv('cleandata.csv')",
      "execution_state": "busy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8bfda00f-6c8d-67ba-b03f-4632d550354b",
        "_active": false
      },
      "outputs": [],
      "source": "from gensim.parsing import PorterStemmer\nglobal_stemmer = PorterStemmer()\n \nclass StemmingHelper(object):\n    \"\"\"\n    Class to aid the stemming process - from word to stemmed form,\n    and vice versa.\n    The 'original' form of a stemmed word will be returned as the\n    form in which its been used the most number of times in the text.\n    \"\"\"\n \n    #This reverse lookup will remember the original forms of the stemmed\n    #words\n    word_lookup = {}\n \n    @classmethod\n    def stem(cls, word):\n        \"\"\"\n        Stems a word and updates the reverse lookup.\n        \"\"\"\n \n        #Stem the word\n        stemmed = global_stemmer.stem(word)\n \n        #Update the word lookup\n        if stemmed not in cls.word_lookup:\n            cls.word_lookup[stemmed] = {}\n        cls.word_lookup[stemmed][word] = (\n            cls.word_lookup[stemmed].get(word, 0) + 1)\n \n        return stemmed\n \n    @classmethod\n    def original_form(cls, word):\n        \"\"\"\n        Returns original form of a word given the stemmed version,\n        as stored in the word lookup.\n        \"\"\"\n \n        if word in cls.word_lookup:\n            return max(cls.word_lookup[word].keys(),\n                       key=lambda x: cls.word_lookup[word][x])\n        else:\n            return word\n        \n#data['title']=data['title'].replace('?',' ?')\ndata.head(5)# Summary about tags"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfbc2d9b-c3e3-8966-b684-cfd3ed19e3da",
        "_active": false
      },
      "outputs": [],
      "source": "from gensim.models import Word2Vec\nfrom six import iteritems\nimport numpy as np\n#import pyemd\n\nwordlist_wv = []\nwordlist_li = []\ntel=0\nfor col in ['title', 'tags','content']:\n    for sentence in data[col].items():\n        if tel/700==round(tel/700):\n            print(sentence[1])\n        tel+=1\n        word_list = str(sentence[1]).split(' ')\n        wordlist_wv.append(word_list)\n        wordlist_li=np.concatenate((wordlist_li, word_list), axis=0)\nwordlist_df=pd.DataFrame(wordlist_li)\nwordlist_df\nwordmodel = Word2Vec(wordlist_wv , size=100, window=5, min_count=0, workers=5,negative=0,hs=1)\nword_vectors = wordmodel.wv\nprint(wordmodel)\n\ntel=0\nfor col in ['title']:\n    for sentence in data[col].items():\n        if tel/250==round(tel/250):\n            print(wordmodel.score([str(sentence[1]).split()]),sentence[1])\n        tel+=1\n\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t#vector=wmb['sex']\n#print(vector)\n#wmb.most_similar(positive=['Berlin', 'Japan'], negative=['Germany'])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "318cc705-c1e7-f82e-2882-0211be7468c2",
        "_active": false
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c403ccb9-f588-a365-eea9-18ee25c3ee6c",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n#Z_ matrix out of train_data content\n\nword=pd.DataFrame([i for i in range(len(wordlist_li))], index=wordlist_li)\nword.columns=['id']\nword.index.name='index'\nword.index.unique()\n\n#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1,vocabulary=worddf.as_matrix())\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1)\nZ_ = tfidf_vectorizer.fit_transform(data['content'])\nZ_names = tfidf_vectorizer.get_feature_names()\nZ_voca=pd.DataFrame.from_dict(tfidf_vectorizer.vocabulary_,orient='index') # dict\nZ_idfv=tfidf_vectorizer.idf_ # array, shape = [n_features], or None\nZ_stop=tfidf_vectorizer.stop_words_ # : set\n# Fuse the  Word vector from Word2Vec with the Z_names form TFIDF\nZ_voca.index.name='index'\n\n#doe de merge\nVt_Z = [wordmodel[index] for index in Z_names if index in word_vectors.vocab]\nVt_voca = [index for index in Z_names if index in word_vectors.vocab]\n#index - nummer verwisselen\nZ_voca_ni=pd.DataFrame(Z_voca.index.values)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf7f37e4-74d3-7754-fe33-5c3effe023a2",
        "_active": false
      },
      "outputs": [],
      "source": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f48a0e9a-1f16-d025-eec3-ec41fd0fcda8",
        "_active": false
      },
      "outputs": [],
      "source": "xyz=2\nprint('Monografie___',data.iloc[xyz].content)\nprint('Tagwords_____',data.iloc[xyz].tags)\n\n\n#Dtemp=pd.DataFrame(xi.todense())\nDtemp=pd.DataFrame(Z_[xyz].todense())\nprint(Dtemp[Dtemp>0].count())\nDtemp_Zv=Dtemp.append(Z_voca_ni.T)\nDtZ=Dtemp_Zv.T\nDtZ.columns=['rf','term']\nDtZ.sort('rf',ascending=[False])\nDtZ_=pd.DataFrame([])\nDtZ_voc=pd.DataFrame([])\nDtZ_=pd.DataFrame([])\nDtZ_voc=pd.DataFrame(['n','term'])\nfor xi in DtZ.index:\n  if DtZ.iloc[xi].rf>0:\n      tempw=DtZ.iloc[xi].term\n      if tempw in word_vectors.vocab:\n          Tempmat=pd.DataFrame( DtZ.iloc[xi].rf*(wordmodel[tempw]) )\n          DtZ_voc= DtZ_voc.append([xi,tempw])\n          DtZ_= DtZ_.append(Tempmat.T)\n          #DtZ_=np.reshape(DtZ_, (int(round(len(DtZ_)/100)),100))\n#print(wordmodel[tempw])\ntemparr=DtZ_voc.as_matrix()\ntemparr=np.reshape(temparr, (len(temparr)/2,-1))  \nDtZ_voc=pd.DataFrame(temparr)\nDtZ_voc.columns=['nr','term']\n#Lis of words in tekst\nListOfWords=DtZ.select(lambda s: DtZ.ix[s].rf>0)\n\nfrom gensim.models import Word2Vec\nfrom six import iteritems\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plotcorr (DVt,txt,YN):\n    Dcorr=DVt.dot(DVt.T)/(abs(DVt).dot(abs(DVt.T)))\n    if YN==1:\n        plt.figure(figsize=(14,12))\n        plt.pcolor(np.array(Dcorr), cmap='plasma')\n        plt.colorbar()\n        plt.yticks(0.5 + np.arange(len(DtZ_voc)), DtZ_voc)\n        plt.xticks(0.5 + np.arange(len(DtZ_voc)), DtZ_voc)\n    superwoord=Dcorr[Dcorr>0.85].sum()\n    lengte=len(superwoord)\n    supdf=pd.DataFrame(superwoord.values,index=range(0,lengte))\n    ddt=DtZ_voc.T.append(supdf.T)\n    dtt=ddt.T\n    \n    resultaat=pd.merge(dtt, ListOfWords, how='left', on=['term', 'term'])\n    resultaat['multi']=resultaat['rf']*resultaat[0]\n    print(resultaat.sort('multi',ascending=0)[:5])\n    prnttmp=''\n    for wn in  ( resultaat.sort('multi',ascending=0)[:5]['term'].values ):\n        prnttmp+=' '+wn\n    print(prnttmp)\n\n    if YN==1:\n        return plt.show()\n    else:\n        return prnttmp\n\n\nplotcorr (DtZ_,DtZ_voc,1)"
    }
  ]
}