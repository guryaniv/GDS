{"cells":[{"metadata":{"_uuid":"e3f7cc8cb242507b2674825b39b04d4e1e5372e4"},"cell_type":"markdown","source":"## General information\n\nIn this competition we have data about small mobile robot driving over different floor surfaces. We need to predict the floor type based on robot's sensor data.\n\nIn this kernel I'll do EDA on the data, try FE and build a variety of models.\n\n![](http://lowryscarpetcare.com/wp-content/uploads/2010/12/hardwood-floor-cleaning-wood-after-lowrys-281x300.jpg)\n\n*work in progress*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nimport json\nimport ast\nimport eli5\nimport shap\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\n\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"56cd4776318f7c75a6156ec9944d48b7b64b9cf2"},"cell_type":"code","source":"# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\nvega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"</script>\",\n)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2876cbb2b804c1b98f731baeb15f7381cabd1a49"},"cell_type":"markdown","source":"## Loading and basic exploring of data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/X_train.csv')\ny = pd.read_csv('../input/y_train.csv')\ntest = pd.read_csv('../input/X_test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"275f763faac42b5958a05fd64e0b73077a61565d"},"cell_type":"markdown","source":"### train and test data\n\nFor train and test we have the following data:\n- ~3800 separate time-series\n- 128 measurements in each time-series with data on robot orientation, angular velocity and linear acceleration\n\nTarget have 1 class per series, so we can aggregate train and test data on series. We have 9 unique classes as a target.\n\nOne more important point: measurements are taken in groups (73 groups in total), so this data could be used in validation. We'll try it later."},{"metadata":{"trusted":true,"_uuid":"210bc1f1d03735c31ed5a55031ee5f5e69509c82"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fe3ac215b7fc7fb1fce6ea24fb37af5d58f4fa2"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c6f92df3d0998a7f2235565c9089960bfa62d56"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17cb8b0faa6203603662fdb6d18881e775ddbd74"},"cell_type":"code","source":"train['series_id'].nunique(), test['series_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8100f6bfe38d04ecb131e5550633a3c6241bd631"},"cell_type":"code","source":"y['surface'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"16e6ce1932a3c66c3134c0deb159437307013768"},"cell_type":"code","source":"y['group_id'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77bf8c043ad66a156d79f7529c321ff4596d7216"},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"_uuid":"4463283e1463ed2faa83f43b6692791b591c028e"},"cell_type":"markdown","source":"### target"},{"metadata":{"trusted":true,"_uuid":"794f5130123eb4f1d7b33b0f39cfcf382fcd5e7e"},"cell_type":"code","source":"y['surface'].value_counts().reset_index().rename(columns={'index': 'target'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c419ec06975ea827a3a2d84fcb64957d32b1bd5"},"cell_type":"code","source":"target_count = y['surface'].value_counts().reset_index().rename(columns={'index': 'target'})\nrender(alt.Chart(target_count).mark_bar().encode(\n    y=alt.Y(\"target:N\", axis=alt.Axis(title='Surface'), sort=list(target_count['target'])),\n    x=alt.X('surface:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'surface']\n).properties(title=\"Counts of target classes\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"284936c3c4b7206dd8950e28672706f2dc23a0fc"},"cell_type":"markdown","source":"We have a serious disbalance, some classes exist only in several series."},{"metadata":{"_uuid":"99799d658d495c1b9bb0dec994203bdc269f65b5"},"cell_type":"markdown","source":"### Orientation - quaternion coordinates\n\nYou could notice that there are 4 coordinates: X, Y, Z, W.\n\nUsually we have X, Y, Z - Euler Angles. But Euler Angles are limited by a phenomenon called \"gimbal lock,\" which prevents them from measuring orientation when the pitch angle approaches +/- 90 degrees. Quaternions provide an alternative measurement technique that does not suffer from gimbal lock. Quaternions are less intuitive than Euler Angles and the math can be a little more complicated.\n\nHere are some articles about it:\n\nhttp://www.chrobotics.com/library/understanding-quaternions\n\nhttp://www.tobynorris.com/work/prog/csharp/quatview/help/orientations_and_quaternions.htm\n\nBasically 3D coordinates are converted to 4D vectors."},{"metadata":{"_uuid":"8be49017e46d48e663256b6ee32b85012cacc1b7"},"cell_type":"markdown","source":"### Feature distribution"},{"metadata":{"trusted":true,"_uuid":"58c86d5f515131d86fcb270b7adf1ac46040f93f"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d60f7638e1bbaed2409349283bea0c2d2f9a0118"},"cell_type":"markdown","source":"Blue values show histograms for train data, green - test data."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3dc70354fbed74c0a4a7cb77f755e7bc55cee6e4"},"cell_type":"code","source":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.hist(train[col], color='blue', bins=100)\n    plt.hist(test[col], color='green', bins=100)\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0a06b80224da7ca4c940556dfb6c37598415986"},"cell_type":"markdown","source":"Velocity and acceleration have normal distribution, orientation features seem to have normalized values (using tanh function).\n\nFeature distributions in train and test are quite similar."},{"metadata":{"_uuid":"ba71ee769a155fb71b5d88059873dd21727131a0"},"cell_type":"markdown","source":"Let's have a look at the values of features in a single time-series"},{"metadata":{"trusted":true,"_uuid":"7830e32252bcbeab0d17bbde7b7ef17b003d2727"},"cell_type":"code","source":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(train.loc[train['series_id'] == 1, col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9fa516dbd98b56b346fae2a89b51bc760dc946"},"cell_type":"markdown","source":"Hm. Don't see any patterns."},{"metadata":{"_uuid":"654bbec41517c591eaad346aacdec031049e49cd"},"cell_type":"markdown","source":"### Feature generation\n\nWe have 128 measurements in each series, so it makes sense to create aggregate features. I create several groups of them:\n\n* Usual aggregations: mean, std, min and max, absolute min and max. Max to min rate;\n* Mean change rate in absolute and relative values - it shows how fast values change;\n* Quantiles - showing extreme values;\n* Trend features - to show whether values decrease or increase;\n* Rolling features - to show mean/std values with windows;\n* Various statistical features from LANL competition;\n\nDescriptions will be done later. I use ideas from my kernel for another competition: https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8125508e6da4b1a144940049704c839921d3e37b"},"cell_type":"code","source":"train_df = train[['series_id']].drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b537eaa807ed785e34d893ae2b7fb0c5731c7c65"},"cell_type":"code","source":"for col in train.columns:\n    if 'orient' in col:\n        scaler = StandardScaler()\n        train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n        test[col] = scaler.transform(test[col].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"896a1eac46a9d1ccb383e257ad788bf59052da35","_kg_hide-input":true},"cell_type":"code","source":"def calc_change_rate(x):\n    change = (np.diff(x) / x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)\n\ndef add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]\n\ndef classic_sta_lta(x, length_sta, length_lta):\n    \n    sta = np.cumsum(x ** 2)\n\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n\n    # Copy for LTA\n    lta = sta.copy()\n\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta /= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta /= length_lta\n\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n\n    return sta / lta\nfor col in tqdm_notebook(train.columns[3:]):\n    train_df[col + '_mean'] = train.groupby(['series_id'])[col].mean()\n    train_df[col + '_std'] = train.groupby(['series_id'])[col].std()\n    train_df[col + '_max'] = train.groupby(['series_id'])[col].max()\n    train_df[col + '_min'] = train.groupby(['series_id'])[col].min()\n    train_df[col + '_max_to_min'] = train_df[col + '_max'] / train_df[col + '_min']\n\n    for i in train_df['series_id']:\n        train_df.loc[i, col + '_mean_change_abs'] = np.mean(np.diff(train.loc[train['series_id'] == i, col]))\n        train_df.loc[i, col + '_mean_change_rate'] = calc_change_rate(train.loc[train['series_id'] == i, col])\n        \n        train_df.loc[i, col + '_q95'] = np.quantile(train.loc[train['series_id'] == i, col], 0.95)\n        train_df.loc[i, col + '_q99'] = np.quantile(train.loc[train['series_id'] == i, col], 0.99)\n        train_df.loc[i, col + '_q05'] = np.quantile(train.loc[train['series_id'] == i, col], 0.05)\n        \n        train_df.loc[i, col + '_abs_min'] = np.abs(train.loc[train['series_id'] == i, col]).min()\n        train_df.loc[i, col + '_abs_max'] = np.abs(train.loc[train['series_id'] == i, col]).max()\n        \n        train_df.loc[i, col + '_trend'] = add_trend_feature(train.loc[train['series_id'] == i, col].values)\n        train_df.loc[i, col + '_abs_trend'] = add_trend_feature(train.loc[train['series_id'] == i, col].values, abs_values=True)\n        train_df.loc[i, col + '_abs_mean'] = np.abs(train.loc[train['series_id'] == i, col]).mean()\n        train_df.loc[i, col + '_abs_std'] = np.abs(train.loc[train['series_id'] == i, col]).std()\n        \n        train_df.loc[i, col + '_mad'] = train.loc[train['series_id'] == i, col].mad()\n        train_df.loc[i, col + '_kurt'] = train.loc[train['series_id'] == i, col].kurtosis()\n        train_df.loc[i, col + '_skew'] = train.loc[train['series_id'] == i, col].skew()\n        train_df.loc[i, col + '_med'] = train.loc[train['series_id'] == i, col].median()\n        \n        train_df.loc[i, col + '_Hilbert_mean'] = np.abs(hilbert(train.loc[train['series_id'] == i, col])).mean()\n        \n        train_df.loc[i, col + '_Hann_window_mean'] = (convolve(train.loc[train['series_id'] == i, col], hann(15), mode='same') / sum(hann(15))).mean()\n        train_df.loc[i, col + '_classic_sta_lta1_mean'] = classic_sta_lta(train.loc[train['series_id'] == i, col], 10, 50).mean()\n\n        train_df.loc[i, col + '_Moving_average_10_mean'] = train.loc[train['series_id'] == i, col].rolling(window=10).mean().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_16_mean'] = train.loc[train['series_id'] == i, col].rolling(window=16).mean().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_10_std'] = train.loc[train['series_id'] == i, col].rolling(window=10).std().mean(skipna=True)\n        train_df.loc[i, col + '_Moving_average_16_std'] = train.loc[train['series_id'] == i, col].rolling(window=16).std().mean(skipna=True)\n        \n        train_df.loc[i, col + 'iqr'] = np.subtract(*np.percentile(train.loc[train['series_id'] == i, col], [75, 25]))\n        train_df.loc[i, col + 'ave10'] = stats.trim_mean(train.loc[train['series_id'] == i, col], 0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f4cb4612ae71e44d597b8fcd54c2ba675062572f"},"cell_type":"code","source":"test_df = sub[['series_id']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b973c87790291967c276d812c88534995c17eda"},"cell_type":"markdown","source":"Feature generation for test data is the same."},{"metadata":{"trusted":true,"_uuid":"ed2b7a62a04cbb9dd1afb2ec848b124fe0931696","_kg_hide-input":true},"cell_type":"code","source":"for col in tqdm_notebook(test.columns[3:]):\n    test_df[col + '_mean'] = test.groupby(['series_id'])[col].mean()\n    test_df[col + '_std'] = test.groupby(['series_id'])[col].std()\n    test_df[col + '_max'] = test.groupby(['series_id'])[col].max()\n    test_df[col + '_min'] = test.groupby(['series_id'])[col].min()\n    test_df[col + '_max_to_min'] = test_df[col + '_max'] / test_df[col + '_min']\n\n    for i in test_df['series_id']:\n        test_df.loc[i, col + '_mean_change_abs'] = np.mean(np.diff(test.loc[test['series_id'] == i, col]))\n        test_df.loc[i, col + '_mean_change_rate'] = calc_change_rate(test.loc[test['series_id'] == i, col])\n        \n        test_df.loc[i, col + '_q95'] = np.quantile(test.loc[test['series_id'] == i, col], 0.95)\n        test_df.loc[i, col + '_q99'] = np.quantile(test.loc[test['series_id'] == i, col], 0.99)\n        test_df.loc[i, col + '_q05'] = np.quantile(test.loc[test['series_id'] == i, col], 0.05)\n        \n        test_df.loc[i, col + '_abs_min'] = np.abs(test.loc[test['series_id'] == i, col]).min()\n        test_df.loc[i, col + '_abs_max'] = np.abs(test.loc[test['series_id'] == i, col]).max()\n        \n        test_df.loc[i, col + '_trend'] = add_trend_feature(test.loc[test['series_id'] == i, col].values)\n        test_df.loc[i, col + '_abs_trend'] = add_trend_feature(test.loc[test['series_id'] == i, col].values, abs_values=True)\n        test_df.loc[i, col + '_abs_mean'] = np.abs(test.loc[test['series_id'] == i, col]).mean()\n        test_df.loc[i, col + '_abs_std'] = np.abs(test.loc[test['series_id'] == i, col]).std()\n        \n        test_df.loc[i, col + '_mad'] = test.loc[test['series_id'] == i, col].mad()\n        test_df.loc[i, col + '_kurt'] = test.loc[test['series_id'] == i, col].kurtosis()\n        test_df.loc[i, col + '_skew'] = test.loc[test['series_id'] == i, col].skew()\n        test_df.loc[i, col + '_med'] = test.loc[test['series_id'] == i, col].median()\n        \n        test_df.loc[i, col + '_Hilbert_mean'] = np.abs(hilbert(test.loc[test['series_id'] == i, col])).mean()\n        \n        test_df.loc[i, col + '_Hann_window_mean'] = (convolve(test.loc[test['series_id'] == i, col], hann(15), mode='same') / sum(hann(15))).mean()\n        test_df.loc[i, col + '_classic_sta_lta1_mean'] = classic_sta_lta(test.loc[test['series_id'] == i, col], 10, 50).mean()\n\n        test_df.loc[i, col + '_Moving_average_10_mean'] = test.loc[test['series_id'] == i, col].rolling(window=10).mean().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_16_mean'] = test.loc[test['series_id'] == i, col].rolling(window=16).mean().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_10_std'] = test.loc[test['series_id'] == i, col].rolling(window=10).std().mean(skipna=True)\n        test_df.loc[i, col + '_Moving_average_16_std'] = test.loc[test['series_id'] == i, col].rolling(window=16).std().mean(skipna=True)\n        \n        test_df.loc[i, col + 'iqr'] = np.subtract(*np.percentile(test.loc[test['series_id'] == i, col], [75, 25]))\n        test_df.loc[i, col + 'ave10'] = stats.trim_mean(test.loc[test['series_id'] == i, col], 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f7fa89bbd7c25e9099532558a972d4f07cb7678"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6383f0953aaf63eb50606fc95829ba8936a0ff36"},"cell_type":"markdown","source":"## Building model"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f9627c032d2c2f041c78155c24c426def7cfae21"},"cell_type":"code","source":"n_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)\n#folds = GroupShuffleSplit(n_splits=n_fold, test_size=0.2, random_state=11)\n#folds = GroupKFold(n_splits=n_fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0460975a645e0caf099f2054026bc6c6fea359a7"},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(y['surface'])\ny['surface'] = le.transform(y['surface'])\n\ntrain_df = train_df.drop(['series_id'], axis=1)\ntest_df = test_df.drop(['series_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1af94d224d9ddd40055f47419883be2d616a6ac"},"cell_type":"markdown","source":"Function to train models:"},{"metadata":{"trusted":true,"_uuid":"5154d9e0a377070214ebd774c0cf08eec4fd8d48","_kg_hide-input":true},"cell_type":"code","source":"def eval_acc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None, groups=y['group_id']):\n\n    oof = np.zeros((len(X), 9))\n    prediction = np.zeros((len(X_test), 9))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, groups)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                    verbose=5000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e7576865735590badf618629ed14554e8c6abde3"},"cell_type":"code","source":"params = {'num_leaves': 123,\n          'min_data_in_leaf': 12,\n          'objective': 'multiclass',\n          'max_depth': 20,\n          'learning_rate': 0.04680350949723872,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8933018355190274,\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': 0.9498109326932401,\n          'reg_lambda': 0.8058490960546196,\n          \"num_class\": 9,\n          'nthread': -1,\n          'min_split_gain': 0.009913227240564853,\n          'subsample': 0.9027358830703129\n         }\n# parameters from my kernel with bayes optimization: https://www.kaggle.com/artgor/bayesian-optimization-for-robots\n\noof_lgb, prediction_lgb, feature_importance = train_model(X=train_df, X_test=test_df, y=y['surface'], params=params, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8569b81e4ce20569d56f0e3e000885716e1de72d"},"cell_type":"code","source":"# I use code from this kernel: https://www.kaggle.com/theoviel/deep-learning-starter\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2925d73a28c11fc2a8665ea940fc79bcf974909d"},"cell_type":"code","source":"plot_confusion_matrix(y['surface'], oof_lgb.argmax(1), le.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19afe0f8fefe590a0699001d4fb02e89d216bbf5"},"cell_type":"markdown","source":"## Model interpretation"},{"metadata":{"_uuid":"3021a096663fe35e4e3873446c354e383541bd45"},"cell_type":"markdown","source":"### ELI5"},{"metadata":{"trusted":true,"_uuid":"0dced9a7aee9b1eb34ef9bc7617805066c63e8e8"},"cell_type":"code","source":"model = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, y['surface'], test_size=0.2, stratify=y['surface'])\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=5000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"44ee792f841592b10bb53551ea96b311be1790d6"},"cell_type":"code","source":"eli5.show_weights(model, targets=[0, 1], feature_names=list(X_train.columns), top=40, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a33eb711b6cd0b58b2b2d7cc886321057aa6356e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"5010bcc45d98bcd5b1171e76b0f7e25a1cf7882a"},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true,"_uuid":"a58955b525224c6c2ef748126350dc9ae5ef220c"},"cell_type":"code","source":"model = SVC(probability=True)\noof_svc, prediction_svc = train_model(X=train_df, X_test=test_df, y=y['surface'], params=None, model_type='sklearn', model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"90d86b4ad503324f822179d1c772ed49e192a9be"},"cell_type":"code","source":"sub['surface'] = le.inverse_transform(prediction_lgb.argmax(1))\nsub.to_csv('lgb_sub.csv', index=False)\nsub['surface'] = le.inverse_transform(prediction_svc.argmax(1))\nsub.to_csv('scv_sub.csv', index=False)\nsub['surface'] = le.inverse_transform((prediction_lgb + prediction_svc).argmax(1))\nsub.to_csv('blend.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}