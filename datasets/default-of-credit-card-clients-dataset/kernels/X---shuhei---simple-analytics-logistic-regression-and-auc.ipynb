{"cells":[{"metadata":{"_uuid":"17a9c4348d4eeff54a61b762168c18436dbe7faf"},"cell_type":"markdown","source":"The data is about the default in finance area. As far as I investigated, on this area, the data are basically imbalance and the analystics and the modeling is based on that fact.  \n\nAbout this data, the explained variable is default.payment.next.month. So, we can imagine the degree of inbalance is not so much.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# load data\ndata_orig = pd.read_csv(\"../input/UCI_Credit_Card.csv\")\ndata_orig.head(10)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(\"data size: \" + str(data_orig.shape))\nprint(\"default size: \" + str(data_orig.ix[data_orig['default.payment.next.month'] == 1,:].shape))","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"f3a434a12cd3bf631b08a48ee5ea6b41fef832f3"},"cell_type":"markdown","source":"ID information is sometimes important. We should not drop without caution. But here, I'll drop.  .  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c8fddaf2570bcfae1379ba1200be2010b6ca04cd"},"cell_type":"code","source":"# omit-target columns\nomit_target_label = ['ID']\n\n# categorical columns\npay_label = ['PAY_'+str(i) for i in range(0,7) if i != 1]\ncategorical_label = ['SEX', 'EDUCATION', 'MARRIAGE']\n\ncategorical_label.extend(pay_label)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acbbba504afc06021a246dab55ab997cd6a5fb0c"},"cell_type":"code","source":"dummied_columns = pd.get_dummies(data_orig[categorical_label].astype('category'))\n\n# drop columns\ndata_orig = data_orig.drop(columns=categorical_label)\ndata_orig = data_orig.drop(columns=omit_target_label)\n\n# merge one-hot-encoded columns\ndata = pd.concat([data_orig, dummied_columns], axis=1, join='outer')","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"708843eb9d6bec450cdabf4a7d48fb28134449f4"},"cell_type":"markdown","source":"To make models, I'll do split the data into train and test ones.  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ee5913dd7fda3903921f74ab91f9e2a21224b1a8"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# explaining and explained\ntarget = data['default.payment.next.month']\ndata = data.drop(columns=['default.payment.next.month'])\n\n# split data into train and test\nx_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.33)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"c481f02b3ad18eb48a5b55fa0e2fb5ccf004f238"},"cell_type":"markdown","source":"I'll do univariate analysis in rough manner.  \nAt first, by using just one variable, I'll make a model and evaluate it by AUC. If this data had the information of TIME, I could split the test data into some groups based on TIME and check AUC per group to see the robustness.  "},{"metadata":{"trusted":true,"_uuid":"7796f7bb7347f351e18351155e78321603510f5b"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nunivar = x_train[['BILL_AMT4']]\n\nlr = LogisticRegression()\nlr.fit(univar, y_train)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e20ce3559f7c5a1058b9d297f4480187ecb45014"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nimport numpy as np\n\npredicted_score = np.array([score[1] for score in lr.predict_proba(x_test[['BILL_AMT4']])])\n\nroc_auc_score(y_test.values, predicted_score)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"f49298403c21b6517d526e4c79888aa6e5d26b75"},"cell_type":"markdown","source":"I'll do same thing to each explaining variables. To think about the conbination of variables, AIC and BIC work well. But here, I don't touch.  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f30342a71d730044fb9f67e752b04186eee9855f"},"cell_type":"code","source":"explaining_labels = x_train.columns\nauc_outcomes = {}\nfor label in explaining_labels:\n    univar = x_train[[label]]\n    \n    lr = LogisticRegression()\n    lr.fit(univar, y_train)\n    \n    predicted_score = np.array([score[1] for score in lr.predict_proba(x_test[[label]])])\n    \n    auc_outcomes[label] = roc_auc_score(y_test.values, predicted_score)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b5d08ca1c71e7a5cab5914f54af5955f9826dc"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nlabel = []\nscore = []\nfor item in sorted(auc_outcomes.items(), key=lambda x: x[1], reverse=True):\n    label.append(item[0])\n    score.append(item[1])\n\n# I wanted to show the bars with decreasing order. But it didn't work here.\nplt.bar(label, score)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"8561b2b97012d9d4edbd59eee5dbbc245d92a368"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"eca8e515adacd0b1644225c64cf5019a3849caba"},"cell_type":"markdown","source":"From the viewpoint of AUC, some models reach 0.6 and the others are around 0.5.  \nNext, with all variables, I'll make Logistic regression model. This time, I don't do standardization.  "},{"metadata":{"trusted":true,"_uuid":"27136fcb8317e19c85a7fae904eed150d47ee2b6"},"cell_type":"code","source":"# using all the explaining variables\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\n\npredicted_score = np.array([score[1] for score in lr.predict_proba(x_test)])\n    \nroc_auc_score(y_test.values, predicted_score)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"4a80eea800e9ff3f73df866c397a38f9fcb68e58"},"cell_type":"markdown","source":"From sklearn, I'll use different evaluation way.  "},{"metadata":{"trusted":true,"_uuid":"e9dc5bbe9b91e7b5f9e9f30f1b34facf4a9e55ef"},"cell_type":"code","source":"from sklearn.metrics import brier_score_loss\n\nbrier_score_loss(y_test.values, predicted_score)","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"7631d807613245f436632b66b4a58932e272999b"},"cell_type":"markdown","source":"To the train and test data, I'll do prediction and evaluate those with AUC and Brier.  "},{"metadata":{"trusted":true,"_uuid":"705f4d3b1608992e3fb69f35a337ab1cf8ccd828"},"cell_type":"code","source":"predicted_score_train = np.array([score[1] for score in lr.predict_proba(x_train)])\npredicted_score_test = np.array([score[1] for score in lr.predict_proba(x_test)])\n\nauc_train = roc_auc_score(y_train.values, predicted_score_train)\nauc_test = roc_auc_score(y_test.values, predicted_score_test)\nbrier_train = brier_score_loss(y_train.values, predicted_score_train)\nbrier_test = brier_score_loss(y_test.values, predicted_score_test)\n\nauc = [auc_train, auc_test]\nbrier = [brier_train, brier_test]\npd.DataFrame({'auc': auc, 'brier': brier}, index=['train', 'test'])","execution_count":48,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}