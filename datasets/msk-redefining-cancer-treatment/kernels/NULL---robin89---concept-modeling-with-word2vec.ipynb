{"cells": [{"source": ["%matplotlib inline\n", "import numpy as np\n", "import pandas as pd \n", "import nltk\n", "import string\n", "import operator\n", "from nltk.stem import WordNetLemmatizer\n", "from sklearn.cluster import KMeans\n", "from sklearn.cluster import AgglomerativeClustering\n", "from sklearn.manifold import TSNE\n", "from gensim.models import word2vec\n", "import matplotlib.pyplot as plt\n", "from wordcloud import WordCloud\n", "from nltk.corpus import wordnet as wn\n", "from nltk.probability import FreqDist\n", "import re"], "metadata": {"collapsed": true, "_cell_guid": "8a814e5d-7099-4e20-93f6-b37fa7041237", "_uuid": "1f908514f7447e8786422a52ec9d07c828444f4c"}, "cell_type": "code", "outputs": [], "execution_count": 36}, {"source": ["This class uses POS Tagging and the RAKE algorithm to compute either a summary of the most important sentences or to extract the keywords for every document. "], "metadata": {"_cell_guid": "76ae240f-d53b-491b-979d-c402ffa6e998", "_uuid": "b0ff80106120a56a832b5cb7bac695604b5c0f0d"}, "cell_type": "markdown"}, {"source": ["class RAKE_tagged():\n", "    '''\n", "    This class applies the RAKE (Rapid Automatic Keyword Extraction) algorithm after filtering and stemming the possible candidates.\n", "    it can be used for any language by passing the taggers, tokenizers and stemmers to the initialisation\n", "    '''\n", "    def __init__(self, no_of_keywords, stopwords='auto', pos =['N'], tokenizer='auto', tagging='auto', stemming='auto'):\n", "        self.pos = pos\n", "        self.nkey = no_of_keywords\n", "        self.stopwords = stopwords\n", "        self.tokenizer = tokenizer\n", "        self.tagging = tagging\n", "        self.lemma = stemming\n", "        self.finaltext = []\n", "\n", "\n", "        pass\n", "    \n", "    def isPunct(self, word):\n", "        return len(word) == 1 and word in string.punctuation\n", "\n", "    def isNumeric(self, word):\n", "        try:\n", "            float(word) if '.' in word else int(word)\n", "            return True\n", "        except ValueError:\n", "            return False\n", "            \n", "    def manage_params(self):\n", "        if type(self.stopwords) == list:\n", "            pass\n", "        elif self.stopwords == \"auto\":\n", "            self.stopwords = set(nltk.corpus.stopwords.words())\n", "        else:\n", "            self.stopwords = []\n", "            \n", "        if self.tokenizer == 'auto':\n", "            self.tokenizer = nltk.word_tokenize\n", "        else:\n", "            pass\n", "        \n", "        if self.tagging == 'auto':\n", "            self.tagging = nltk.pos_tag\n", "        else:\n", "            pass\n", "        \n", "        if self.lemma == 'auto':\n", "            lemmatizer = WordNetLemmatizer()\n", "            self.lemma = lemmatizer.lemmatize\n", "        else:\n", "            pass\n", "\n", "    def candidate_keywords(self, document_sents):\n", "        '''\n", "        Finding the possible candidates for keywords, by removing stopwords and filtering by POS tag\n", "        '''\n", "        phrase_list = []\n", "        all_words = []\n", "\n", "        for sentence in document_sents:\n", "            words = map(lambda x: \"#\" if x in self.stopwords else x, self.tagging(self.tokenizer(sentence.lower())))\n", "            \n", "            # create lemmatized corpus for word2vec (lists-of-words in lists-of-sentences in list-of-corpus)\n", "            sent = []\n", "            for w in words:\n", "                all_words.append(w)\n", "                if w[0] != \"#\" or self.isPunct(w[0]) == False:\n", "                    try:\n", "                        sent.append(self.lemma(w[0], w[1][0].lower()))\n", "                    except:\n", "                        sent.append(self.lemma(w[0]))\n", "            self.finaltext.append(sent)\n", "        \n", "            \n", "        \n", "        # back to the RAKE Algorithm \n", "        phrase = []\n", "        candidates =[]\n", "\n", "        for (word, tag) in all_words:\n", "            if word == \"#\" or self.isPunct(word):\n", "                if len(phrase) > 0:\n", "                    phrase_list.append(phrase)\n", "                    phrase = []\n", "            else:\n", "                if self.pos != None:\n", "                    phrase.append(word)\n", "                    for t in self.pos:\n", "                        if tag.startswith(t):\n", "                            candidates.append(self.lemma(word, t[0].lower()))\n", "                                #print(self.lemma(word, t.lower()))\n", "                        else:\n", "                            pass\n", "                else:\n", "                    phrase.append(word)\n", "                    candidates.append(word)\n", "        return phrase_list, candidates\n", "    \n", "    def calculate_word_scores(self, phrase_list, candidates):\n", "        word_freq = nltk.FreqDist()\n", "        word_degree = nltk.FreqDist()\n", "        for phrase in phrase_list:\n", "          degree = len(list(filter(lambda x: not self.isNumeric(x), phrase))) - 1\n", "          for word in phrase:\n", "            word_freq[word] += 1\n", "            word_degree[word] += degree\n", "        for word in word_freq.keys():\n", "          word_degree[word] = word_degree[word] + word_freq[word]\n", "        word_scores = {}\n", "        for word in word_freq.keys():\n", "            if word in candidates:\n", "                word_scores[word] = word_degree[word] / word_freq[word]\n", "        return word_scores\n", "    \n", "    def calculate_phrase_scores(self, phrase_list, word_scores, candidates):\n", "        phrase_scores = {}\n", "        for phrase in phrase_list:\n", "          phrase_score = 0\n", "          for word in phrase:\n", "              if word in candidates:\n", "                phrase_score += word_scores[word]\n", "          phrase_scores[\" \".join(phrase)] = phrase_score\n", "        return phrase_scores\n", "    \n", "    def fit(self):\n", "        #might be useful for piping\n", "        pass\n", "\n", "    def transform(self, corpus, output_type=\"w\"):\n", "        keyword_results = []\n", "        self.manage_params()\n", "        for document in corpus:\n", "            sentences = nltk.sent_tokenize(document)\n", "            phrase_list, candidate_list = self.candidate_keywords(sentences)\n", "            word_scores = self.calculate_word_scores(phrase_list, candidate_list)\n", "            phrase_scores = self.calculate_phrase_scores(phrase_list, word_scores, candidate_list)\n", "            if output_type == \"s\":\n", "                sorted_scores = sorted(phrase_scores.items(),key=operator.itemgetter(1), reverse=True)\n", "            else:\n", "                sorted_scores = sorted(word_scores.items(),key=operator.itemgetter(1), reverse=True)           \n", "            keyword_results.append([k[0] for k in sorted_scores[0:self.nkey]])\n", "        return keyword_results"], "metadata": {"collapsed": true, "_cell_guid": "882dcf38-47ec-4a67-bb2a-501c7a1d1a5b", "_uuid": "d2c0dd48aa8550c9733aa7ee74cf6f201897e844"}, "cell_type": "code", "outputs": [], "execution_count": 37}, {"source": ["Computing the Vectors for the keywords with word2vec"], "metadata": {"_cell_guid": "7dac7d66-61d6-4953-b2be-82a7235fc342", "_uuid": "1fa655f0d8b5ef68bfccad69bcfa7f2a81961302"}, "cell_type": "markdown"}, {"source": ["class Computing_Vectors():\n", "    \n", "    def __init__(self):\n", "\n", "        pass\n", "    \n", "    def fit(self, corpus):\n", "        self.model = word2vec.Word2Vec(corpus, size=100, window=5, min_count=1, workers=4)\n", "        \n", "    \n", "    def transform(self, keywords):\n", "        self.arrays = []\n", "        \n", "        self.model = self.model.wv\n", "        found = 0\n", "        notfound = 0\n", "        for kwl in keywords:\n", "            for keyw in kwl:\n", "                try:\n", "                    self.arrays.append((self.model[keyw], keyw))\n", "                    found += 1\n", "                except:\n", "                    notfound += 1\n", "                    pass\n", "\n", "        return self.arrays\n", "    \n", "    def visualize(self):\n", "        x_arr =np.array([i[0] for i in self.arrays])\n", "        tsne = TSNE(n_components=2)\n", "        X_tsne = tsne.fit_transform(x_arr)\n", "        \n", "        plt.figure(figsize=(200, 200), dpi=100)\n", "        max_x = np.amax(X_tsne, axis=0)[0]\n", "        max_y = np.amax(X_tsne, axis=0)[1]\n", "        plt.xlim((-max_x,max_x))\n", "        plt.ylim((-max_y,max_y))\n", "\n", "        \n", "        for row_id in range(0, len(self.arrays)):\n", "            target_word = self.arrays[row_id][1]\n", "            x = X_tsne[row_id, 0]\n", "            y = X_tsne[row_id, 1]\n", "            plt.annotate(target_word, (x,y))\n", "\n", "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n", "        plt.savefig(\"word2vec.png\")\n", "        plt.show()\n", " "], "metadata": {"collapsed": true, "_cell_guid": "9a85a356-0d5b-4cfe-9fe1-39654b250933", "_uuid": "1c7847058e7ea6a1370429cf8c8a893081a3e98a"}, "cell_type": "code", "outputs": [], "execution_count": 38}, {"source": ["Clustering the vectors to find concepts"], "metadata": {"_cell_guid": "d3508b4c-f200-4bde-a54f-d2fd163e0c5c", "_uuid": "bbabe45284c43955fcffcffffedaf75f8704aa1f"}, "cell_type": "markdown"}, {"source": ["class Clustering():\n", "    def __init__(self,  w2v, model):\n", "        self.model = model\n", "        self.w2v = w2v\n", "        self.w2vecarrays = np.array([i[0] for i in w2v])\n", "        pass\n", "    \n", "    def fit_transform(self, number_of_clusters):\n", "        cluster = KMeans(n_clusters=number_of_clusters, random_state=0).fit(self.w2vecarrays)\n", "        #cluster = DBSCAN(eps=0.01, min_samples=3).fit(self.w2vecarrays)\n", "        #cluster = SpectralClustering(n_clusters=number_of_clusters, eigen_solver='arpack',affinity=\"nearest_neighbors\")\n", "        #cluster = AgglomerativeClustering(n_clusters=number_of_clusters, linkage='ward')\n", "        self.labels = cluster.labels_\n", "        counts = np.bincount(self.labels[self.labels>=0])\n", "               \n", "        self.concepts = {}\n", "        \n", "        for row_id in range(0, len(self.labels)):\n", "            word = self.w2v[row_id][1]\n", "            label = self.labels[row_id]\n", "            if label in self.concepts:\n", "                self.concepts[label].append(word)\n", "            else:\n", "                self.concepts[label] = [word]\n", "                \n", "        return self.concepts\n", "    \n", "    def visualize(self):\n", "        tsne = TSNE(n_components=2)\n", "        X_tsne = tsne.fit_transform(self.w2vecarrays)\n", "        \n", "        for concept in sorted(self.concepts):\n", "            try:\n", "                txt = \" \".join(self.concepts[concept])\n", "                wordcloud = WordCloud(background_color=\"white\",max_font_size=40, relative_scaling=.5).generate(txt)\n", "                plt.figure()\n", "                plt.imshow(wordcloud)       \n", "                plt.title(concept)\n", "                plt.axis(\"off\")\n", "                plt.show()\n", "            except ValueError:\n", "                print(self.concepts[concept])"], "metadata": {"collapsed": true, "_cell_guid": "c41c6a23-1168-4b34-a420-e18e30848a70", "_uuid": "a129124352d3ae3a4111a0a5164fe5ff6c540396"}, "cell_type": "code", "outputs": [], "execution_count": 39}, {"source": ["Now we put everything together and see the result"], "metadata": {"_cell_guid": "e29e5223-6a98-47db-9c55-0c9365025b12", "_uuid": "6a9fdf723a63a8957c4888d4ff43a2134c8a89f1"}, "cell_type": "markdown"}, {"source": ["First we load the data"], "metadata": {"_cell_guid": "3483b44f-a2a1-426c-ab53-c301d6835d1c", "_uuid": "e5ac193b8cb98bb70bc5ce3145d02896009ac443"}, "cell_type": "markdown"}, {"source": ["\n", "from nltk.corpus import inaugural\n", "\n", "sample = [{'ID': fileid, 'Text': inaugural.raw(fileid)} for fileid in inaugural.fileids()]\n", "df = pd.DataFrame(sample)\n", "\n", "df= pd.read_csv('../input/stage2_test_text.csv', sep='\\|\\|', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n", "#df.head()\n", "\n", "df_txt = df['Text'][:2000]"], "metadata": {"_cell_guid": "a94a2502-668c-47b2-a80b-8b25c41ca681", "_uuid": "2e0beb2612115e058cfa0b29996b1765c570dca2"}, "cell_type": "code", "outputs": [], "execution_count": 40}, {"source": ["In the next step we use two layers of the RAKE algorithm to compute Noun-only-keywords. The first layer computes a summary of  the most important sentences. The second layer computes the keywords. Since the corpus is relatively small, we use a summary of 100 sentences in the first layer and then compute 80 keywords each. "], "metadata": {"_cell_guid": "e3d48a4d-da33-40aa-a5c4-2dae37d69b49", "_uuid": "0fdc0eb16041ee2d45d814cc8870e1ff8411a39a"}, "cell_type": "markdown"}, {"source": ["myRE1 = RAKE_tagged(10, stopwords='auto', pos=[\"N\", \"VBP\", \"R\"])\n", "myRE = RAKE_tagged(10, stopwords='auto', pos=[\"N\"])\n", "summary = myRE1.transform(df_txt, output_type=\"s\")\n", "summaries =[\"; \".join(s) for s in summary]\n", "keywords = myRE.transform(summaries, output_type=\"w\")"], "metadata": {"collapsed": true, "_cell_guid": "b8d60a9a-20a9-41ae-b9ce-3e7a04a58748", "_uuid": "2280b743297176c405a91121f10e26bf5a935d63"}, "cell_type": "code", "outputs": [], "execution_count": 41}, {"source": ["In the next step we use word2vec to compute a vector representation of the keywords. To visualize them, we can perform dimensonality reduction using tsne."], "metadata": {"_cell_guid": "506fec7c-a9a6-4398-a2d0-437a850d57cd", "_uuid": "328dc5cd4d3e6854f3363f46cc1db454c7d570de"}, "cell_type": "markdown"}, {"source": ["CV = Computing_Vectors()\n", "CV.fit(myRE1.finaltext)\n", "arr = CV.transform(keywords)\n", "CV.visualize()"], "metadata": {"_cell_guid": "7ec0644f-5303-43fe-9699-7fd296d91129", "_uuid": "2cc4b5d120cb0bbf012dcaf2dd13a0c60b20456b", "scrolled": true}, "cell_type": "code", "outputs": [], "execution_count": 42}, {"source": ["In this step we cluster the result with KMeans. It would be interesting to try other clustering algorithms for this task. For example DBSCAN. We visualize the output as wordclouds."], "metadata": {"_cell_guid": "5df076e5-5209-4399-b119-b45993298d52", "_uuid": "493cfa282f0ce195a5df1c9b0c910e14a56a8c25"}, "cell_type": "markdown"}, {"source": ["CL = Clustering(arr, CV.model)\n", "concepts = CL.fit_transform(25)\n", "CL.visualize()"], "metadata": {"_cell_guid": "38b4a5e8-d8d8-437f-a090-de3848c960c6", "_uuid": "d34d46f9547ec0a1826e03c3a5912993992f6fae"}, "cell_type": "code", "outputs": [], "execution_count": 43}, {"source": ["Ok so now we can work with those concepts and see what we can do. First,let's write a class to extract some simple relationships between those concepts\n"], "metadata": {"_cell_guid": "0b5b26b2-492e-4fe2-a2eb-6ae0205708bd", "_uuid": "6c7a856316389c9d7f3a0fff35c2e125f18942eb"}, "cell_type": "markdown"}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}