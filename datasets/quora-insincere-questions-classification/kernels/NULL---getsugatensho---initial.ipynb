{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"#### Load the train and test data"},{"metadata":{"trusted":true,"_uuid":"ef66de26bbd376ff249af5728de8cec042f9cf31"},"cell_type":"code","source":"\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b16b31b0e0df51ef5f159ed1adc05a767e02bf8e"},"cell_type":"code","source":"# Count rows with missing (or empty string) values\ntrain_df['question_text'].replace('', np.nan, inplace=True)\ntest_df['question_text'].replace('', np.nan, inplace=True)\nprint(\"Train rows with missing values:\", train_df.shape[0] - train_df.dropna(subset=['question_text']).shape[0])\nprint(\"Test rows with missing values:\", test_df.shape[0] - test_df.dropna(subset=['question_text']).shape[0])\ntrain_df.dropna(subset=['question_text'], inplace=True)\ntest_df.dropna(subset=['question_text'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b5449c0af93e255464187eda2c3c364790349c"},"cell_type":"code","source":"# Most (if not all) of the following cleanup code is from https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n\nimport re\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d67592656bee3c76567dad45b7810a78288af74e"},"cell_type":"code","source":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color', 'centre':'center', 'didnt':'did not', 'doesnt':'does not',\n                'isnt':'is not', 'shouldnt':'should not', 'favourite':'favorite',\n                'travelling':'traveling', 'counselling':'counseling', 'theatre':'theater',\n                'cancelled':'canceled', 'labour':'labor', 'organisation':'organization', 'wwii':'world war 2',\n                'citicise':'criticize', 'instagram': 'social medium', 'whatsapp': 'social medium', 'snapchat': 'social medium',\n                'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n                'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018',\n                'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp',\n                'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d5d03c74cfcd3047bf0072cbfc50c68dc0c4324"},"cell_type":"code","source":"train_questions = train_df[\"question_text\"].values\ntest_questions = test_df[\"question_text\"].values\ntrain_targets = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"022c2e9bc8b8259ddf04bef82588a6035adcd101"},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 50000\nEMBEDDING_DIM = 300 \n\n## Tokenize the sentences                                                                                                                                                                                   \ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(train_questions)\ntrain_questions = tokenizer.texts_to_sequences(train_questions)\ntest_questions = tokenizer.texts_to_sequences(test_questions)\n\n# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n# pad sequences so that we get a N x T matrix\ntrain_X = pad_sequences(train_questions, maxlen=MAX_SEQUENCE_LENGTH)\ntest_X = pad_sequences(test_questions, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of train_X:', train_X.shape)\nprint('Shape of train_X:', test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8d7937dfc66013c5f19f2044e700b77b13518bc"},"cell_type":"code","source":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d54c1f5bcee17d1798081a7585882793a8d5a4"},"cell_type":"code","source":"# load in pre-trained Glove word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt', encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split(\" \")\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f2247e3ff45135e6171a57d600ad594cd8abef8"},"cell_type":"code","source":"# prepare embedding matrix\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n        # words not found in embedding index will be all zeros.\n            embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1647e45b5cf78372ebeacd3c71ede18ddb410946"},"cell_type":"markdown","source":"#### Build and train model"},{"metadata":{"trusted":true,"_uuid":"8dab4ba00b6191517dc63a4a9cc00b9f6623ebda"},"cell_type":"code","source":"VALIDATION_SPLIT = 0.2\nBATCH_SIZE = 512\nEPOCHS = 2\n\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Bidirectional(LSTM(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\n\n\nmodel = Model(input_, x)\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b113d9709f8cfdcadb222cec600f8141155e85d6"},"cell_type":"code","source":"# Train the model\nr = model.fit(train_X, train_targets, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VALIDATION_SPLIT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"717db525a28864036f636ad06367981386fff1b4"},"cell_type":"code","source":"# plot loss and accuracies data\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()\n\n# accuracies\nplt.plot(r.history['acc'], label='acc')\nplt.plot(r.history['val_acc'], label='val_acc')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"932f0fea03820e82f5429be7dd9095003cd0e0cb"},"cell_type":"code","source":"p = model.predict(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dca5c2129b0acaf722b938d3f47baedf5611c982"},"cell_type":"code","source":"# Save submission\ntest_df['prediction'] = (p.flatten() >= 0.5).astype(np.int)\ntest_df.to_csv('sample_submission.csv', columns=['qid', 'prediction'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}