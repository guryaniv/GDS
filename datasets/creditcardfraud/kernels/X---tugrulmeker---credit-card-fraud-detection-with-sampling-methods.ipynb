{"cells":[{"metadata":{"trusted":true,"_uuid":"461d03bc6f36cef271d3b7cc41500bb79dd3ca38"},"cell_type":"code","source":"#The number of fraudulent transactions is 429 out of 284807, only small part of whole dataset belongs to the fraudulent transactions \n#Since dataset has 31 features, it is necessary to find what differences that fraudulent transactions have. \n#There are not any null values in dataset.  \n#Fraudulent instances account for only %0.172 of all transactions. \n#Therefore, the dataset has a strong imbalanced nature and the problem is two-class classification. \n#From the summary results of data in Python, all features between V1 to V28 have the same mean value as zero. \n#It can be interpreted that all anonymn features were normalized with mean 0. \n#As mentioned in Data explanation, these variables are the result of PCA transformations.\n#Therefore, PCA transformation is not applied into the dataset again \n#Feauture selection is carried out by following EDA part results\n#Only ‘Amount’ feature’s mean is different than zero, thus, “Amount” feature may need transformations for equity between variables while developing the machine learning model. \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b5bf18b80ae3e990d716cd2e41ec9fbb05409ab"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns #to visualization\nimport matplotlib.pyplot as plt # to plot the graphs\nimport matplotlib.gridspec as gridspec # to do the grid of plots\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_score ,auc, roc_curve\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, neural_network\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore') #ignore warning messages ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbf5d1028928f983a0141a65b2a2e68bb2cfd0fd"},"cell_type":"code","source":"data = pd.read_csv('../input/creditcard.csv')\ndata.head() #for obtaining first five rows of dataset\ndata.info() #information about dataset\ndata.describe()\ndata[[\"Time\",\"Amount\"]].describe() #important stats from original values\ndata[data.Amount>10000]### there is only seven points after 10.000, thus these values should be excluded from dataset\ndata_new=data[data.Amount<10000]\ndata_new.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7dca1d0d0a3787bc4642c1d8ace3cfcb08c369"},"cell_type":"code","source":"timedelta = pd.to_timedelta(data_new['Time'], unit='s')\ndata_new['Time_min'] = (timedelta.dt.components.minutes).astype(int) #new variable for further analysis\ndata_new['Time_hour'] = (timedelta.dt.components.hours).astype(int)  #new variable for further analysis\n\n\n#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(data_new[data_new['Class'] == 0][\"Time_hour\"],\n             color='g')\nsns.distplot(data_new[data_new['Class'] == 1][\"Time_hour\"],\n             color='r')\nplt.title('Fraud x Normal Transactions by Hours (Red: Fraud; Green:Normal)', fontsize=12)\nplt.xlim([-1,25])\nplt.show()\n\n\n#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(data_new[data_new['Class'] == 0][\"Time_min\"],\n             color='g')\nsns.distplot(data_new[data_new['Class'] == 1][\"Time_min\"],\n             color='r')\nplt.title('Fraud x Normal Transactions by minutes', fontsize=12)\nplt.xlim([-1,61])\nplt.show()\n\n\n#To clearly the data of frauds and no frauds\ndf_fraud = data_new[data_new['Class'] == 1]\ndf_normal = data_new[data_new['Class'] == 0]\n\nprint(\"Fraud transaction statistics\")\nprint(df_fraud[\"Amount\"].describe())\nprint(\"\\nNormal transaction statistics\")\nprint(df_normal[\"Amount\"].describe())\n\n\n#Feature engineering to a better visualization of the values\n\ndata_new['Amount_log'] = np.log(data_new.Amount + 0.01) #logaritmic transformation due to skewness of fraud transaction distributon\nplt.figure(figsize=(14,6))\n#distribution of amount by time variable:\nplt.subplot(121)\nax = sns.boxplot(x =\"Class\",y=\"Amount\",\n                 data=data_new)\nax.set_title(\"Class x Amount\", fontsize=20)\nax.set_xlabel(\"Is Fraud?\", fontsize=16)\nax.set_ylabel(\"Amount(US)\", fontsize = 16)\n\nplt.subplot(122)\nax1 = sns.boxplot(x =\"Class\",y=\"Amount_log\", data=data_new)\nax1.set_title(\"Class x Amount\", fontsize=20)\nax1.set_xlabel(\"Is Fraud?\", fontsize=16)\nax1.set_ylabel(\"Amount(Log)\", fontsize = 16)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.8)\n\nplt.show()\n\n\n#Looking the Amount and time distribuition of FRAUD transactions\nax = sns.lmplot(y=\"Amount\", x=\"Time_min\", fit_reg=False,aspect=1.8,\n                data=data_new, hue='Class')\nplt.title(\"Amounts by Minutes of Frauds and Normal Transactions\",fontsize=8)\nplt.show()\n\n\nax = sns.lmplot(y=\"Amount\", x=\"Time_hour\", fit_reg=False,aspect=1.8,\n                data=data_new, hue='Class')\nplt.title(\"Amounts by Hour of Frauds and Normal Transactions\", fontsize=8)\n\nplt.show()\n\n### distribution of each classes for syntethic variables between V1-V28\n\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(data_new[data_new.iloc[:, 1:29].columns]):\n   ax = plt.subplot(gs[i])\n   sns.distplot(data_new[cn][data_new.Class == 1], bins=50)\n   sns.distplot(data_new[cn][data_new.Class == 0], bins=50)\n   ax.set_xlabel('')\n   ax.set_title('feature: ' + str(cn))\nplt.show()\n\n\n##Corelation matrix for whole data\ncolormap = plt.cm.Greens\nplt.figure(figsize=(14,12))\n\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0,\n            square=True, cmap = colormap, linecolor='white', annot=True)\nplt.show()\n\n\n##\n\n\n#Feauture selection\n\ndata_new2 = data_new[[\"V3\",\"V4\",\"V9\",\"V10\",\"V11\",\"V12\",\"V17\",\"V19\",\"Amount\",\"Class\"]]\n\n#correlation matrix\nf, (ax1, ax2) = plt.subplots(1,2,figsize =( 15, 8))\n\nsns.heatmap(data_new2.query('Class==1').drop(['Class'],1).corr(), vmax = .8, square=True, ax = ax1, cmap = 'YlGnBu')\nax1.set_title('Fraud')\n\nsns.heatmap(data_new2.query('Class==0').drop(['Class'],1).corr(), vmax = .8, square=True, ax = ax2, cmap = 'YlGnBu');\nax2.set_title('Normal')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ed73089681c232fdcbc145f9d03a3636ef49d9"},"cell_type":"code","source":"#Since PCA is already made on dataset, only important variables which are found by EDA are used in machine learning model.\n#Random-under sampling, random-over sampling, SMOTE sampling is used for data-level strategies for imbalanced credit card dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b05463a5710c7386569f74ffdc1d646326e41b5e","scrolled":true},"cell_type":"code","source":"###Machine Learning Model:\n\n###packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_score ,auc, roc_curve\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, neural_network\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\n\n\n\n#####After EDA Part Reduced Dataset& Analysis\n\ndf = pd.read_csv('../input/creditcard.csv')\nprint('The number of normal transactions : ' + str(sum(df.Class == 0)))\nprint('The number of frauds : ' + str(sum(df.Class == 1)))\nprint ('The percentage of fraud of all transactions : ' + str(float(sum(df.Class == 1))/float(len(df.Class))*100.0))\npd.value_counts(df['Class'])\ndf_reduced = df.drop(['V1','V2','V5','V6','V7','V8','V14','V13','V15','V16' ,'V18','V20','V21','V22','V23','V24','V25','V26','V27','V28'], axis =1)\ndf_reduced['Amount_Stand'] = StandardScaler().fit_transform(df_reduced['Amount'].values.reshape(-1, 1)) ###Standart normalizer on Amount\ndf_reduced = df_reduced.drop(['Time', 'Amount'], axis=1)\ndf_reduced.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfcc19a74e191e59304b59f96f0030d62b9fae26"},"cell_type":"code","source":"##special thanks to this amazing work: https://www.kaggle.com/vincentlugat/votingclassifier-f1-score-0-88-data-viz\n\n#####confusion_Matrix\n\n# confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\n####recall-F1 Score-Precision\n\ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp/(tp+fp))*(tp/(tp+fn)))/\n                                                 ((tp/(tp+fp))+(tp/(tp+fn))))))\n\n# precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall_score, precision_score, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall_score, precision_score, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall_score, precision_score, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();\n\n# ROC curve\ndef plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n    plt.xlim([0.0,0.001])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();\n#feature importance plot\ndef plot_feature_importance(model):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (12,12))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1086bd1ea432c9aca0e09cedd1d5e28e39091645"},"cell_type":"code","source":"#####Logistic Regression\n\n#####LogisticRegression with Three Sampling Method \n#SMOTE\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = SMOTE().fit_sample(X, y)  ## Random Oversampler(), SMOTE; Random Undersampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nlogistic_reg = LogisticRegression()\nlogistic_reg.fit(X_train, y_train)\ny_pred = logistic_reg.predict(X_test)\ny_score = logistic_reg.decision_function(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Logistic Regression Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4feb037c9504620422a02fd52674b30497f28ec"},"cell_type":"code","source":"#####Logistic Regression\n\ndf = pd.read_csv('../input/creditcard.csv')\nprint('The number of normal transactions : ' + str(sum(df.Class == 0)))\nprint('The number of frauds : ' + str(sum(df.Class == 1)))\nprint ('The percentage of fraud of all transactions : ' + str(float(sum(df.Class == 1))/float(len(df.Class))*100.0))\npd.value_counts(df['Class'])\ndf_reduced = df.drop(['V1','V2','V5','V6','V7','V8','V14','V13','V15','V16' ,'V18','V20','V21','V22','V23','V24','V25','V26','V27','V28'], axis =1)\ndf_reduced['Amount_Stand'] = StandardScaler().fit_transform(df_reduced['Amount'].values.reshape(-1, 1)) ###Standart normalizer on Amount\ndf_reduced = df_reduced.drop(['Time', 'Amount'], axis=1)\n\n#####LogisticRegression with Three Sampling Method \n#Random.Oversampler\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomOverSampler().fit_sample(X, y)  ## Random Oversampler(), SMOTE; Random Undersampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nlogistic_reg = LogisticRegression()\nlogistic_reg.fit(X_train, y_train)\ny_pred = logistic_reg.predict(X_test)\ny_score = logistic_reg.decision_function(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Logistic Regression Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b14eb07229ec1324bb792706381abd3b1e4c91"},"cell_type":"code","source":"#####Logistic Regression\n\n#####LogisticRegression with Three Sampling Method \n#Random.Undersampler\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomUnderSampler().fit_sample(X, y)  ## Random Oversampler(), SMOTE; Random Undersampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nlogistic_reg = LogisticRegression()\nlogistic_reg.fit(X_train, y_train)\ny_pred = logistic_reg.predict(X_test)\ny_score = logistic_reg.decision_function(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Logistic Regression Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ef157b66d89e6410cb65be76c25baa6c888d70f"},"cell_type":"code","source":"#####random forest with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = SMOTE().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_jobs = -1,\n                                random_state = 42)\n\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\ny_score = random_forest.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Random Forest Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18dda004b0cc5b3b136632b24f02742c9fd82a74"},"cell_type":"code","source":"#####random forest with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomOverSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_jobs = -1,\n                                random_state = 42)\n\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\ny_score = random_forest.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Random Forest Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb27c506ea39dba642cdb3fe08425335124255f7"},"cell_type":"code","source":"#####random forest with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomUnderSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_jobs = -1,\n                                random_state = 42)\n\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\ny_score = random_forest.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Random Forest Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n### Feature importance:\npredictors = ['V3','V4','V9','V10','V11','V12','V17','V19','AmountSTD']\nplot_feature_importance(random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6825eb9fe71525ca1a32b390c9eef4449e810846"},"cell_type":"code","source":"### Decision tree algorithm with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = SMOTE().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\ndecision_tree=tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\ny_score = decision_tree.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Decision Tree Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\npredictors = ['V3','V4','V9','V10','V11','V12','V17','V19','AmountSTD']\nplot_feature_importance(decision_tree)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d58d765c8aa0c420a8071afd592a0a5436f9300d"},"cell_type":"code","source":"### Decision tree algorithm with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomOverSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\ndecision_tree=tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\ny_score = decision_tree.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Decision Tree Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\npredictors = ['V3','V4','V9','V10','V11','V12','V17','V19','AmountSTD']\nplot_feature_importance(decision_tree)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2bd71d54baaeff9f11d749d5707ca3a8ce4bd5c"},"cell_type":"code","source":"### Decision tree algorithm with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomUnderSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\ndecision_tree=tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\ny_score = decision_tree.predict_proba(X_test)[:,1]\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Decision Tree Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\npredictors = ['V3','V4','V9','V10','V11','V12','V17','V19','AmountSTD']\nplot_feature_importance(decision_tree)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"586160e63a2c5d55c54d9305546e4f559eabf6c3"},"cell_type":"code","source":"##Support vector classifier with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = SMOTE().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nsvm.LinearSVC()\nsvmlinear=svm.LinearSVC()\n\nsvmlinear.fit(X_train, y_train)\ny_pred = svmlinear.predict(X_test)\ny_score = svmlinear.decision_function(X_test)\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'SVC Confusion matrix')\nplt.show()\nshow_metrics()\n\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eb70f7cb2577d3fafc3039fca46d06bdb204242"},"cell_type":"code","source":"##Support vector classifier with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomOverSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nsvm.LinearSVC()\nsvmlinear=svm.LinearSVC()\n\nsvmlinear.fit(X_train, y_train)\ny_pred = svmlinear.predict(X_test)\ny_score = svmlinear.decision_function(X_test)\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'SVC Confusion matrix')\nplt.show()\nshow_metrics()\n\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"733cfad56beccdef821dcca0e5ec40499b848cfd"},"cell_type":"code","source":"##Support vector classifier with three sampling methods\n\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomUnderSampler().fit_sample(X, y)\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.2, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\n\nsvm.LinearSVC()\nsvmlinear=svm.LinearSVC()\n\nsvmlinear.fit(X_train, y_train)\ny_pred = svmlinear.predict(X_test)\ny_score = svmlinear.decision_function(X_test)\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'SVC Confusion matrix')\nplt.show()\nshow_metrics()\n\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2df42fe04f8d9287d9041e3cb128cc6f992329b1"},"cell_type":"code","source":"\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = SMOTE().fit_sample(X, y)## RandomUnderSampler, RandomOverSampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nNaive_bayes_model=naive_bayes.GaussianNB()\nNaive_bayes_model.fit(X_train, y_train)\ny_pred = Naive_bayes_model.predict(X_test)\ny_score = Naive_bayes_model.(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Naive Bayes Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afcfde0fce5c8a7a6c20619f7ddee8ab32d0868e"},"cell_type":"code","source":"\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomUnderSampler().fit_sample(X, y)## RandomUnderSampler, RandomOverSampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nNaive_bayes_model=naive_bayes.GaussianNB()\nNaive_bayes_model.fit(X_train, y_train)\ny_pred = Naive_bayes_model.predict(X_test)\ny_score = Naive_bayes_model.(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Naive Bayes Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nprint('ROC AUC Score:',metrics.roc_auc_score(y_test, y_pred))\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12455d35d2f52f53d67579429b6d34f8b83a146"},"cell_type":"code","source":"\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nX_resample, y_resample = RandomOverSampler().fit_sample(X, y)## RandomUnderSampler, RandomOverSampler\nprint ( 'The number of transactions after resampling : ' + str(len(X_resample)))\nprint ('If the number of frauds is equal to the number of normal tansactions? ' + str(sum(y_resample == 0) == sum(y_resample == 1))\n       )\n\n##training size 0.8\nX_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size=0.8, random_state=3)\nX = df_reduced.drop('Class', axis=1)\ny = df_reduced.Class\nNaive_bayes_model=naive_bayes.GaussianNB()\nNaive_bayes_model.fit(X_train, y_train)\ny_pred = Naive_bayes_model.predict(X_test)\ny_score = Naive_bayes_model.(X_test)\n\n# Confusion matrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm,\n                      classes = class_names,\n                      title = 'Naive Bayes Confusion matrix')\nplt.show()\nshow_metrics()\n## ROC_AUC Curve Score\nmetrics.roc_auc_score(y_test, y_pred)\n\n# ROC curve\nfpr, tpr, t = roc_curve(y_test, y_score)\nplot_roc()\n# Precision-recall curve\nprecision_score, recall_score, thresholds = precision_recall_curve(y_test, y_score)\nplot_precision_recall()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d30c92fdbedb4f2fd7614dfb71a698aeebb8b1df"},"cell_type":"code","source":"#### Model Training#####\n\n## Summary for below part: SMOTE sampling with training size 0.8 provides better results.\n\n\n##As mentioned above, in order to deal with imbalanced between classes in data sampling is needed. \n#Total number of instances in dataset are 284807. \n#Only 492 of instances are fraudulent transactions.  \n#As mentioned above, fraudulent instances account for %0.172 of all transactions.  \n#In order to solve the imbalanced issue, sampling methods are used. \n#These methods are SMOTE, random-under sampling, and random-over sampling. \n#Kernels above are repeated agaib by changin the sampling method and  training size.\n#For three sampling methods, the training sizes from 50% to 90% are tested. \n#In random-under sampling, the majority class decreases randomly from 284315 to 492, which is the size of the minority class.  \n#In random-over sampling and SMOTE, the minority class size increases from 492 to 284315. \n#total number of instance in the dataset is 568630 after resampling.\n\n#In the logistic regression model with random-over sampling, train size’s change does not highly affect precision, recall, F1 score and AUC values. \n#However, increases in training size result as increases in accuracy metrics. \n#In SMOTE sampling, train size change slightly increases precision, recall, F1 score and AUC values. \n#In random-under sampling, both accuracy metrics have better values than over-sampling methods. \n#However, ROC Curve graph is similar to the straight line. \n#This shows that in random-under sampling, the logistic regression classifier’s prediction becomes independent from the label, \n#which is not better than a classifier that guesses randomly. \n\n#In the Naive-Bayes model with random-over sampling& SMOTE sampling, \n#train size ‘s change again does not highly affect accuracy metrics’ results. \n#Both over-sampling methods provide worse accuracy results than random-under sampling. \n#However, this is similar to the logistic regression model performance with random-under sampling. \n#The reason is an over-fitted model because of the lost information by under-sampling method. \n\n#In the random forest model with random-over sampling& SMOTE sampling, train size’s change affects accuracy metrics’ result.\n#In random-over sampling, when train size increase from 50% to 80%, precision increases from 0.944 to 1.\n#Also, recall and F1 score increase to 1. Moreover, the AUC value increases from 0.93 to 0.99. \n#For sampling methods, SMOTE provides the best performance in terms of accuracy metrics and AUC values which is 0.99 when train size is 50%. Train size’s changes also do not affect performance in SMOTE sampling.\n\n#In the decision tree algorithm with SMOTE sampling, train size change does not affect accuracy metrics’ result. \n#Also, the AUC value does not improve by changing the train size. Only sampling method affects model’s performance. \n#Random-under sampling method decreases the model’s performance. \n#AUC value is 0.91 in random-under sampling, which is smaller when SMOTE sampling method is used. \n\n#In the support vector classifier algorithm with SMOTE sampling, train size’s change does not affect accuracy metrics’ result. \n#Also, the AUC value does not increase, when train size increases from 50% to 80%. \n#Random-over sampling also provides similar results to SMOTE sampling. \n#Like other algorithms, random under-sampling provides worse results in terms of accuracy metrics and AUC results.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9a04c33f651699ab4616beafb53b63302509c64"},"cell_type":"code","source":"### Parameter tuning-grid search\n\n###Following table is obtained by using kernels above and grid search\n\n\n#In this part, both methodologies and performance with sampling methods’ choices will be compared. \n#As mentioned above, in EDA part, following variables are excluded from data: V1, V2, V5, V6, V7, V8, V13, V14, V15, V16, V17 V18, V20, V21, V22, V23, V24, V25, V26, V27, V28. \n#Moreover, in the EDA part, ‘Time’ feature does not show a significant effect on defining a transaction as if fraudulent or not. Thus, ‘Time’ feature is also excluded from data.\n# Since variables between V1-V28 are all transformed and anonymous variables, another PCA transformation is not applied to the dataset again. As suggested in the ‘About Data’ section, Area Under Curve (AUC) is used as a key performance indicator.\n\n#As the data-level approach solution for imbalanced data, random-over sampling and SMOTE sampling provides similar results which are far better than random-under sampling.\n#Therefore, SMOTE sampling is chosen for further analysis. SMOTE sampling provides slightly better results in logistic regression and random forest.  Since SMOTE sampling & logistic regression, decision-tree and random forest provides better results, grid search is applied to these three algorithms for parameter-tuning.  \n\n#According to the results at Random Forest, Logistic Regression and Decision Tree algorithms provide better results at AUC values. \n#For Precision metric, Random Forest, Logistic regression and SVC provide good results. Support Vector Classifier (SVC) provides a good result at precision however, its Recall score of SVC’s is 0.608 which is not satisfactory as other algorithms.\n\n# Best parameters for logistic regression:\n#C:100, Weight: Balanced, Penalty: L1\n\n# Best parameters for decision tree:\n# Maximum depth:9\n\n# Best parameters for random forest:\n#n_estimators = 500, max_features = 3,min_samples_leaf = 1, min_samples_split = 10,\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f95729ffd1169cf0313fd9a16f7f4cf45dcb99c"},"cell_type":"code","source":"# Further research:\n#In further research, as a data-level approach, cluster based data approach and informed over sampling may be used to investigate the data.  \n#MCC and Kappa can be used for comparing the various algorithms as an evaluation metric. \n#Moreover, ensemble methods such as gradient tree boosting and XG boosting can be used for analyzing data further.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ba9b71858115c6d004de758ac677b5998af0c3"},"cell_type":"code","source":"#In this study the following works are used. Since this is my first EDA and machine learning analysis, sorry for the possible errors:)\n#I have to thank all the people for valueable Kaggle works.\n#https://github.com/ireneliu521/Credit-Card-Fraud_J2D_Project_Python/blob/master/Credit%20Card%20Fraud%20Detection.ipynb\n#https://www.kaggle.com/vincentlugat/votingclassifier-f1-score-0-88-data-viz\n#https://www.kaggle.com/kabure/credit-card-fraud-prediction-rf-smote\n#https://www.kaggle.com/qianchao/smote-with-imbalance-data","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}