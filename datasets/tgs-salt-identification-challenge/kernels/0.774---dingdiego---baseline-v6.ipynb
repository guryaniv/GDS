{"cells":[{"metadata":{"_uuid":"fa525d982fdb7e518de47cd8ec52f6ee868d312b"},"cell_type":"markdown","source":"## Update\n-Add Batch Normalization layer after each conv\n-Add shuffle=True in model.fit() method for a better BN effect (so that we have different batch to normalize in each epoch during the training)\n-You can use crf method (https://www.kaggle.com/meaninglesslives/apply-crf) to improve the result \n## Changelog\n- Changed uncov to uconv, but removed the dropout in the last layer\n- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)\n- Added DICE loss functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"77e05df3218e009eafa91b1afffbb5cdd6535a41"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada861a85e9549dca27667692da408c5fdccbaa5"},"cell_type":"markdown","source":"# About\nSince I am new to learning from image segmentation and kaggle in general I want to share my noteook.\nI saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n\n- As said, the field is new to me so I am open to suggestions.\n- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n- Added stratification by the amount of salt contained in the image.\n- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n- Added dropout to the model which seems to improve performance."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport math\nimport random\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img,ImageDataGenerator \nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom imgaug import augmenters as iaa\nfrom keras.optimizers import SGD\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"trusted":true,"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48","collapsed":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\n\ndef upsample_reflect(img):\n    if img_size_ori == img_size_target:\n        return img\n    return np.pad(img[:,:,0],((13,14),(13,14)),'reflect')\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]\ndef iouMetric(true, pred):\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred) \n    pred = K.cast(K.greater(pred, 0.5), K.floatx())\n\n    intersec = true * pred\n    iou = K.sum(intersec) / (K.sum(true + pred - intersec) + K.epsilon())\n\n    return iou","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a0aa7bb3e1c4bf4cf5a08e62ca6f3ee0d73e575e"},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"482d36c99db804c7cb216d98f56fa3af93b0f427"},"cell_type":"code","source":"def do_resize2(image, mask, H, W):\n    image = cv2.resize(image,dsize=(W,H))\n    mask = cv2.resize(mask,dsize=(W,H))\n    mask  = (mask>0.5).astype(np.float32)\n\n    return image,mask\n#################################################################\n\ndef compute_center_pad(H,W, factor=32):\n\n    if H%factor==0:\n        dy0,dy1=0,0\n    else:\n        dy  = factor - H%factor\n        dy0 = dy//2\n        dy1 = dy - dy0\n\n    if W%factor==0:\n        dx0,dx1=0,0\n    else:\n        dx  = factor - W%factor\n        dx0 = dx//2\n        dx1 = dx - dx0\n\n    return dy0, dy1, dx0, dx1\n\n\ndef do_center_pad_to_factor(image, factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_center_pad(H,W, factor)\n\n    image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n                               #cv2.BORDER_CONSTANT, 0)\n    return image\n\n\ndef do_center_pad_to_factor2(image, mask, factor=32):\n    image = do_center_pad_to_factor(image, factor)\n    mask  = do_center_pad_to_factor(mask, factor)\n    return image, mask\n\n#---\n\ndef do_horizontal_flip(image):\n    #flip left-right\n    image = cv2.flip(image,1)\n    return image\n\ndef do_horizontal_flip2(image,mask):\n    image = do_horizontal_flip(image)\n    mask  = do_horizontal_flip(mask )\n    return image, mask\n\n#---\n\ndef compute_random_pad(H,W, limit=(-4,4), factor=32):\n\n\n    if H%factor==0:\n        dy0,dy1=0,0\n    else:\n        dy  = factor - H%factor\n        dy0 = dy//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dy)\n        dy1 = dy - dy0\n\n    if W%factor==0:\n        dx0,dx1=0,0\n    else:\n        dx  = factor - W%factor\n        dx0 = dx//2 + np.random.randint(limit[0],limit[1]) # np.random.choice(dx)\n        dx1 = dx - dx0\n\n    return dy0, dy1, dx0, dx1\n\n\ndef do_random_pad_to_factor2(image, mask, limit=(-4,4), factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_random_pad(H,W, limit, factor)\n\n    image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n    mask  = cv2.copyMakeBorder(mask,  dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n\n    return image, mask\n\n#----\ndef do_invert_intensity(image):\n    #flip left-right\n    image = np.clip(1-image,0,1)\n    return image\n\n\ndef do_brightness_shift(image, alpha=0.125):\n    image = image + alpha\n    image = np.clip(image, 0, 1)\n    return image\n\n\ndef do_brightness_multiply(image, alpha=1):\n    image = alpha*image\n    image = np.clip(image, 0, 1)\n    return image\n\n\n#https://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/\ndef do_gamma(image, gamma=1.0):\n\n    image = image ** (1.0 / gamma)\n    image = np.clip(image, 0, 1)\n    return image\n\n\ndef do_flip_transpose2(image, mask, type=0):\n    #choose one of the 8 cases\n\n    if type==1: #rotate90\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n\n    if type==2: #rotate180\n        image = cv2.flip(image,-1)\n        mask  = cv2.flip(mask,-1)\n\n\n    if type==3: #rotate270\n        image = image.transpose(1,0)\n        image = cv2.flip(image,0)\n\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,0)\n\n\n    if type==4: #flip left-right\n        image = cv2.flip(image,1)\n        mask  = cv2.flip(mask,1)\n\n\n    if type==5: #flip up-down\n        image = cv2.flip(image,0)\n        mask  = cv2.flip(mask,0)\n\n    if type==6:\n        image = cv2.flip(image,1)\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = cv2.flip(mask,1)\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n    if type==7:\n        image = cv2.flip(image,0)\n        image = image.transpose(1,0)\n        image = cv2.flip(image,1)\n\n        mask = cv2.flip(mask,0)\n        mask = mask.transpose(1,0)\n        mask = cv2.flip(mask,1)\n\n\n    return image, mask\n\n##================================\ndef do_shift_scale_crop( image, mask, x0=0, y0=0, x1=1, y1=1 ):\n    #cv2.BORDER_REFLECT_101\n    #cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    image = image[y0:y1,x0:x1]\n    mask  = mask [y0:y1,x0:x1]\n\n    image = cv2.resize(image,dsize=(width,height))\n    mask  = cv2.resize(mask,dsize=(width,height))\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n\ndef do_random_shift_scale_crop_pad2(image, mask, limit=0.10):\n\n    H, W = image.shape[:2]\n\n    dy = int(H*limit)\n    y0 =   np.random.randint(0,dy)\n    y1 = H-np.random.randint(0,dy)\n\n    dx = int(W*limit)\n    x0 =   np.random.randint(0,dx)\n    x1 = W-np.random.randint(0,dx)\n\n    #y0, y1, x0, x1\n    image, mask = do_shift_scale_crop( image, mask, x0, y0, x1, y1 )\n    return image, mask\n\n#===========================================================================\n\ndef do_shift_scale_rotate2( image, mask, dx=0, dy=0, scale=1, angle=0 ):\n    borderMode=cv2.BORDER_REFLECT_101\n    #cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    sx = scale\n    sy = scale\n    cc = math.cos(angle/180*math.pi)*(sx)\n    ss = math.sin(angle/180*math.pi)*(sy)\n    rotate_matrix = np.array([ [cc,-ss], [ss,cc] ])\n\n    box0 = np.array([ [0,0], [width,0],  [width,height], [0,height], ],np.float32)\n    box1 = box0 - np.array([width/2,height/2])\n    box1 = np.dot(box1,rotate_matrix.T) + np.array([width/2+dx,height/2+dy])\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat  = cv2.getPerspectiveTransform(box0,box1)\n\n    image = cv2.warpPerspective(image, mat, (width,height),flags=cv2.INTER_LINEAR,\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask = cv2.warpPerspective(mask, mat, (width,height),flags=cv2.INTER_NEAREST,#cv2.INTER_LINEAR\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n#https://www.kaggle.com/ori226/data-augmentation-with-elastic-deformations\n#https://github.com/letmaik/lensfunpy/blob/master/lensfunpy/util.py\ndef do_elastic_transform2(image, mask, grid=32, distort=0.2):\n    borderMode=cv2.BORDER_REFLECT_101\n    height, width = image.shape[:2]\n\n    x_step = int(grid)\n    xx = np.zeros(width,np.float32)\n    prev = 0\n    for x in range(0, width, x_step):\n        start = x\n        end   = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step*(1+random.uniform(-distort,distort))\n\n        xx[start:end] = np.linspace(prev,cur,end-start)\n        prev=cur\n\n\n    y_step = int(grid)\n    yy = np.zeros(height,np.float32)\n    prev = 0\n    for y in range(0, height, y_step):\n        start = y\n        end   = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step*(1+random.uniform(-distort,distort))\n\n        yy[start:end] = np.linspace(prev,cur,end-start)\n        prev=cur\n\n    #grid\n    map_x,map_y =  np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n\n    #image = map_coordinates(image, coords, order=1, mode='reflect').reshape(shape)\n    image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=borderMode,borderValue=(0,0,0,))\n\n\n    mask = cv2.remap(mask, map_x, map_y, interpolation=cv2.INTER_NEAREST, borderMode=borderMode,borderValue=(0,0,0,))\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask\n\n\n\n\ndef do_horizontal_shear2( image, mask, dx=0 ):\n    borderMode=cv2.BORDER_REFLECT_101\n    #cv2.BORDER_REFLECT_101  cv2.BORDER_CONSTANT\n\n    height, width = image.shape[:2]\n    dx = int(dx*width)\n\n    box0 = np.array([ [0,0], [width,0],  [width,height], [0,height], ],np.float32)\n    box1 = np.array([ [+dx,0], [width+dx,0],  [width-dx,height], [-dx,height], ],np.float32)\n\n    box0 = box0.astype(np.float32)\n    box1 = box1.astype(np.float32)\n    mat = cv2.getPerspectiveTransform(box0,box1)\n\n    image = cv2.warpPerspective(image, mat, (width,height),flags=cv2.INTER_LINEAR,\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = cv2.warpPerspective(mask, mat, (width,height),flags=cv2.INTER_NEAREST,#cv2.INTER_LINEAR\n                                borderMode=borderMode,borderValue=(0,0,0,))  #cv2.BORDER_CONSTANT, borderValue = (0, 0, 0))  #cv2.BORDER_REFLECT_101\n    mask  = (mask>0.5).astype(np.float32)\n    return image, mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"063afb02ec4ebb45205cf28480dbd10f44da3e5a"},"cell_type":"code","source":"def do_center_pad_to_factor(image, factor=32):\n    H,W = image.shape[:2]\n    dy0, dy1, dx0, dx1 = compute_center_pad(H,W, factor)\n\n    image = cv2.copyMakeBorder(image, dy0, dy1, dx0, dx1, cv2.BORDER_REFLECT_101)\n                               #cv2.BORDER_CONSTANT, 0)\n    return image\n\n\ndef do_center_pad_to_factor2(image, mask, factor=32):\n    image = do_center_pad_to_factor(image, factor)\n    mask  = do_center_pad_to_factor(mask, factor)\n    return image, mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e05536501bed27810d418592f95b432180cac510"},"cell_type":"code","source":"def train_augment(image,mask,index):\n    #cache = Struct(image = image.copy(), mask = mask.copy())\n\n    if np.random.rand() < 0.5:\n         image, mask = do_horizontal_flip2(image, mask)\n         pass\n\n    if np.random.rand() < 0.5:\n        c = np.random.choice(3)\n        if c==0:\n            image, mask = do_random_shift_scale_crop_pad2(image, mask, 0.125)\n        if c==1:\n            image, mask = do_elastic_transform2(image, mask, grid=10,\n                                            distort=np.random.uniform(0,0.1))\n        if c==2:\n            image, mask = do_shift_scale_rotate2( image, mask, dx=0, dy=0, scale=1,\n                                              angle=np.random.uniform(0,10))\n\n    if np.random.rand() < 0.5:\n        c = np.random.choice(3)\n        if c==0:\n            image = do_brightness_shift(image,np.random.uniform(-0.05,+0.05))\n        if c==1:\n            image = do_brightness_multiply(image,np.random.uniform(1-0.05,1+0.05))\n        if c==2:\n            image = do_gamma(image,np.random.uniform(1-0.05,1+0.05))\n        # if c==1:\n        #     image = do_invert_intensity(image)\n\n\n    image, mask = do_center_pad_to_factor2(image, mask, factor=32)\n    return image,mask,index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"trusted":true,"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d","collapsed":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86620c6a070571895f4f36ec050a25803915ed74","collapsed":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1137f0a009f10b5f69e4dade5f689e744e9ce1d6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b13d1ecc7004832e8e042d034922796263054b7"},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5e66ff4809ea2f9a679b7ddbda5028dc324137a","collapsed":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670","collapsed":true},"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2391c568019151b098a002937516bb77a506f403"},"cell_type":"markdown","source":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data."},{"metadata":{"trusted":true,"_uuid":"6ae7b7011b7de3caed58f9ca3939df15ffa319ad","collapsed":true},"cell_type":"code","source":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"},"cell_type":"markdown","source":"# Show some example images"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1a6bc85ee458f72c0917edf77895d5abc5eaf3ee","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00655e32f93f96ebd90dbe94e35ee052f52217cd"},"cell_type":"markdown","source":"# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."},{"metadata":{"trusted":true,"_uuid":"2d3c3157512d11e71ac74ce51a937b85bedfe1d1","collapsed":true},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a23b13d698c205f2c1d8ee1365625ba9e0a91abb","collapsed":true},"cell_type":"code","source":"# def show_tr_val(x_tr,y_tr):\n#     for i in range(10):\n#         print(x_tr[i].shape,y_tr[i].shape)\n#         plt.subplot(1,2,1)\n#         plt.imshow(x_tr[i,:,:,0])\n#         plt.subplot(1,2,2)\n#         plt.imshow(y_tr[i,:,:,0])\n#         plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"050f03cfee686b95b1937f6639afa4d998ad9db7","collapsed":true},"cell_type":"code","source":"# x_tr=[]\n# y_tr=[]\n# x_val=[]\n# y_val=[]\n# for img in x_train:\n#     x_tr.append(upsample_reflect(img))\n# for mask in y_train:\n#     y_tr.append(upsample_reflect(mask))\n# x_train=np.array(x_tr)\n# x_train=np.expand_dims(x_train,axis=3)\n# y_train=np.array(y_tr)\n# y_train=np.expand_dims(y_train,axis=3)\n\n# for img in x_valid:\n#     x_val.append(upsample(img))\n# for mask in y_valid:\n#     y_val.append(upsample(mask))\n# x_valid=np.array(x_val)\n# y_valid=np.array(y_val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43ce54dd4c0d19bbe023cdc501de6e4955a14b4f","collapsed":true},"cell_type":"code","source":"# show_tr_val(x_valid,y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2f1ab00f03e71e6d7f9b2214408b5a9779fc235","collapsed":true},"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1aa78bd7c607e1f0e0235e4b2f82056c0361dac5"},"cell_type":"code","source":"# def conv_block(m, dim, acti, bn, res, do=0):\n# \tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n# \tn = BatchNormalization()(n) if bn else n\n# \tn = Dropout(do)(n) if do else n\n# \tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n# \tn = BatchNormalization()(n) if bn else n\n# \treturn Concatenate()([m, n]) if res else n\n\n# def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n# \tif depth > 0:\n# \t\tn = conv_block(m, dim, acti, bn, res)\n# \t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n# \t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n# \t\tif up:\n# \t\t\tm = UpSampling2D()(m)\n# \t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n# \t\telse:\n# \t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n# \t\tn = Concatenate()([n, m])\n# \t\tm = conv_block(n, dim, acti, bn, res)\n# \telse:\n# \t\tm = conv_block(m, dim, acti, bn, res, do)\n# \treturn m\n\n# def UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n# \t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n# \ti = Input(shape=img_shape)\n# \to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n# \to = Conv2D(out_ch, 1, activation='sigmoid')(o)\n# \treturn Model(inputs=i, outputs=o)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99b5eb3667b59b22e0f53211d544f19fdd2a6fc7"},"cell_type":"code","source":"def ConvBn2D(x,in_dim,out_dim):\n    x=Conv2D()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2382b088c8a6be16490354ebd386120a9ced414d","collapsed":true},"cell_type":"code","source":"model = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b","collapsed":true},"cell_type":"code","source":"sgd = SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\nmodel.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\",iouMetric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28","collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c007157c2fd3d7dadcaeee2a6376351852d1e565"},"cell_type":"markdown","source":"# Data augmentation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"},"cell_type":"code","source":"# x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n# y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7040f72549212dd4f71c13dfbd8bf013481ea369","collapsed":true},"cell_type":"code","source":"# fig, axs = plt.subplots(2, 10, figsize=(15,3))\n# for i in range(10):\n#     axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n#     axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n#     axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n#     axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\n# fig.suptitle(\"Top row: original images, bottom row: augmented images\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"576057909c41a540bcd0d5108e76d401a3179626"},"cell_type":"code","source":"seq = iaa.Sequential([\n    iaa.Fliplr(0.5), # horizontally flip\n#     iaa.OneOf([\n#         iaa.Noop(),\n#         iaa.PiecewiseAffine(scale=(0.05, 0.1), mode='edge', cval=(0)),\n#     ])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"214dbb8d697462d7e5f97c56b5be90e5b013febb"},"cell_type":"code","source":"def gen_flow_for_two_inputs(X, y):\n    genX1 = gen.flow(X,y,  batch_size=batch_size)\n    while True:\n        X=genX1.next()\n        imgs=[]\n        masks=[]\n        for index in range(len(X[0])):\n            img=X[0][index]\n            mask=X[0][index]\n            img,mask,_=train_augment(img,mask,0)\n            imgs.append(img)\n            masks.append(mask)\n        imgs=np.array(imgs)\n        masks=np.array(masks)\n        yield np.expand_dims(imgs,axis=3),np.expand_dims(masks,axis=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"collapsed":true},"cell_type":"code","source":"#early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n#reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1)\ndef step_decay(epoch):\n    if epoch<=60:\n        return 0.01\n    elif epoch<=90:\n        return 0.001\n    else:\n        return 0.0001\n    \nreduce_lr = LearningRateScheduler(step_decay)\nepochs = 120\nbatch_size = 32\ngen = ImageDataGenerator()\ngen_flow = gen_flow_for_two_inputs(x_train, y_train)\nhistory = model.fit_generator(gen_flow,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    steps_per_epoch=len(x_train) / batch_size,\n                    callbacks=[model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","collapsed":true},"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","collapsed":true},"cell_type":"code","source":"model.load_weights(\"./keras.model\",{\"bce_dice_loss\":bce_dice_loss,'iouMetric':iouMetric})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"trusted":true,"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","collapsed":true},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4"},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc","collapsed":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ced29761f2d1760245112a30a7abd4783b373dd","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"},"cell_type":"markdown","source":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."},{"metadata":{"trusted":true,"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332a614c0ae837c115ec6563f355753ffbb8cd83"},"cell_type":"markdown","source":"# Submission\nLoad, predict and submit the test image predictions."},{"metadata":{"trusted":true,"_uuid":"72128add82c6853441671fde67e7e66601a01787","collapsed":true},"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3ecb152b492c7126d12c5ef2c701eec8ea3d86f1","collapsed":true},"cell_type":"code","source":"x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f278d0b87320c117b4ed7c116a991782b82ba5a7","collapsed":true},"cell_type":"code","source":"preds_test = model.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"113f816f9db8b87ca7f6845fe6e61328ab606f41","collapsed":true},"cell_type":"code","source":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4243166f91c4bcb4da00208f4f53dd912dbb429f"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}