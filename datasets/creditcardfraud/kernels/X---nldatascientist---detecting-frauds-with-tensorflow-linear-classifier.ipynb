{"nbformat": 4, "metadata": {"language_info": {"file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.4", "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"metadata": {}, "source": ["# Introduction\n", "\n", "In this notebook we will look at the credit risk fraud data, analyse its content and make a logistic fraud prediction model with *Tensorflow*."], "cell_type": "markdown"}, {"metadata": {"_uuid": "70775862827223e03685b9645e60866fcacc8a8f", "_cell_guid": "65670ccf-44a2-4cba-939b-27f15b621868"}, "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "import seaborn as sns\n", "import matplotlib.gridspec as gridspec\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["# Getting the data and describing it\n", "\n", "First let's fetch the data and describe it."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["df = pd.read_csv(\"../input/creditcard.csv\")\n", "df.describe()"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "execution_count": null, "source": ["fraud_indices = df[df.Class == 1].index\n", "number_records_fraud = len(fraud_indices)\n", "\n", "normal_indices = df[df.Class == 0].index\n", "number_records_normal = len(normal_indices)\n", "\n", "print(\"Normal transactions: \", number_records_normal)\n", "print(\"Fraud transactions: \", number_records_fraud)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["A very skew data set.\n", "\n", "## Missing values\n", "\n", "Are there missing values in the different columns ? A quick plot counting the NULLs per column must provide some insight."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["df.isnull().sum().plot(kind='bar')"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["No missing valules. \n", "## Distributions\n", "Per column of the dataframe- feature - we would like to compare the distributions of the normal and fraud cases. In the hope to find features where the frauds show different values than the non-frauds.\n", "\n", "We use the violin plot as shown in [Feature Selection and Data Visualization](https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization). A great advantage of this plot is the quick overview over all the features. Features that have symmetric shapes, are not very interesting. The frauds show the same values as the non frauds. They can not help to distinguish frauds."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["y = df.Class\n", "x = df.drop(['Class','Time'],axis=1)\n", "x_scaled = (x - x.min()) / (x.max()-x.min()) \n", "\n", "# scaling is necessary to have the same range on the y-axis\n", "\n", "chtdata = pd.concat([y,x_scaled.iloc[:,0:15]],axis=1)\n", "chtdata = pd.melt(chtdata,id_vars=\"Class\",var_name=\"Features\",value_name='Scaled value')\n", "plt.figure(figsize=(20,10))\n", "sns.violinplot(x=\"Features\", y=\"Scaled value\", hue=\"Class\", data=chtdata, split=True, inner=\"quart\")\n", "plt.xticks(rotation=90)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["For feature V13 and V15 the distributions look quite the same. Let's look at the other 14 features."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["chtdata = pd.concat([y,x_scaled.iloc[:,15:]],axis=1)\n", "chtdata = pd.melt(chtdata,id_vars=\"Class\",var_name=\"Features\",value_name='Scaled value')\n", "plt.figure(figsize=(20,10))\n", "sns.violinplot(x=\"Features\", y=\"Scaled value\", hue=\"Class\", data=chtdata, split=True, inner=\"quart\")\n", "plt.xticks(rotation=90)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["Here we see features V20, V22, V24, V25 and V26 are quite symmetric. \n", "\n", "However there are a lot of features that show differences between the frauds and the non-frauds. A model should pick up on those differences.\n", "\n", "## Correlation\n", "We need to verify that the features are not highly correlated. If so we can drop some features, as they are well represented by the others."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["f,ax = plt.subplots(figsize=(18, 18))\n", "sns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["No high correlations found. As described with the dataset, the features are the outcome of a PCA, so this outcome is no surprise. We only see a slight correlation with the amount."], "cell_type": "markdown"}, {"metadata": {}, "source": ["# Tensorflow Linear Classifier\n", "We will try to predict future fraud cases with a linear classifcation model. Let's take a look at the high level *Tensorflow* tools\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "execution_count": null, "source": ["import tensorflow as tf\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import metrics"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["After importing the packages the features are defined. The features are all numeric, so nothing fancy here. In the feature list we leave out the features we identified by the violin plots."], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "execution_count": null, "source": ["nV01 = tf.feature_column.numeric_column('V1')\n", "nV02 = tf.feature_column.numeric_column('V2')\n", "nV03 = tf.feature_column.numeric_column('V3')\n", "nV04 = tf.feature_column.numeric_column('V4')\n", "nV05 = tf.feature_column.numeric_column('V5')\n", "nV06 = tf.feature_column.numeric_column('V6')\n", "nV07 = tf.feature_column.numeric_column('V7')\n", "nV08 = tf.feature_column.numeric_column('V8')\n", "nV09 = tf.feature_column.numeric_column('V9')\n", "nV10 = tf.feature_column.numeric_column('V10')\n", "nV11 = tf.feature_column.numeric_column('V11')\n", "nV12 = tf.feature_column.numeric_column('V12')\n", "nV13 = tf.feature_column.numeric_column('V13')\n", "nV14 = tf.feature_column.numeric_column('V14')\n", "nV15 = tf.feature_column.numeric_column('V15')\n", "nV16 = tf.feature_column.numeric_column('V16')\n", "nV17 = tf.feature_column.numeric_column('V17')\n", "nV18 = tf.feature_column.numeric_column('V18')\n", "nV19 = tf.feature_column.numeric_column('V19')\n", "nV20 = tf.feature_column.numeric_column('V20')\n", "nV21 = tf.feature_column.numeric_column('V21')\n", "nV22 = tf.feature_column.numeric_column('V22')\n", "nV23 = tf.feature_column.numeric_column('V23')\n", "nV24 = tf.feature_column.numeric_column('V24')\n", "nV25 = tf.feature_column.numeric_column('V25')\n", "nV26 = tf.feature_column.numeric_column('V26')\n", "nV27 = tf.feature_column.numeric_column('V27')\n", "nV28 = tf.feature_column.numeric_column('V28')\n", "nV30 = tf.feature_column.numeric_column('Amount')\n", "\n", "features = [nV01 , nV02 , nV03 , nV04 , nV05 , nV06 , nV07 , nV08 , nV09 , nV10 , \n", "             nV11 , nV12 ,\n", "            # nV13 , \n", "            nV14 ,\n", "            # nV15 , \n", "            nV16 , nV17 , nV18 , nV19 , \n", "            # nV20 , \n", "             nV21 , nV22 , nV23 , \n", "#             nV24 , nV25 , nV26 , \n", "             nV27 , nV28 , nV30]"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["## Train and test\n", "A training and test set are made, with a split at 70%."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["trainsize = 0.7\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, train_size=trainsize, random_state=101)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["Now the training can begin. We define a input function specifying the batch size and the number of epochs"], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=100, num_epochs=1000,shuffle=True)\n", " \n", "model = tf.estimator.LinearClassifier(feature_columns=features,n_classes=2)\n", " \n", "model.train(input_fn=input_func,steps=1000)\n", " \n", "#resultaten trainingset\n", "results=model.evaluate(tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=10, num_epochs=1, shuffle=False))"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "execution_count": null, "source": ["print(results)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["Be aware that the high accuracy is caused by the shewness of the data. There are so little frauds, each model that has non-fraud  as outcome will have a high accuracy score.\n", "\n", "Let's evaluate the model for the test set."], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["eval_input_func = tf.estimator.inputs.pandas_input_fn(x=X_test,y=y_test,batch_size=10, num_epochs=1, shuffle=False)\n", "results=model.evaluate(eval_input_func)\n", "print(results)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["## Predictions\n", "With the model we are going to predict frauds based on the test set. These predictions will be compared to the actual test data. A *ROC* plot and it's *Area Under the Curve* (auc) will give us some insight how good the model performs. "], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["pred_input_func= tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size=10, num_epochs=1, shuffle=False)\n", "predictions = model.predict(pred_input_func)\n", "\n", "y_pred= [d['logits'] for d in predictions]\n", "\n", "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n", "roc_auc = metrics.auc(fpr, tpr)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["### ROC\n", "Now the ROC plot showing the balance between the true positive rate (the model predicted the right outcome) and the false positive rate (the model predicted a fraud for a non-fraud transaction).\n"], "cell_type": "markdown"}, {"metadata": {}, "execution_count": null, "source": ["plt.figure(figsize=(10,10))\n", "plt.title('ROC - Tensorflow')\n", "plt.plot(fpr, tpr, 'b',label='Area under curve = %0.2f'% roc_auc)\n", "plt.legend(loc='lower right')\n", "plt.plot([0,1],[0,1],'r--')\n", "plt.ylabel('True Positive Rate')\n", "plt.xlabel('False Positive Rate')\n", "plt.show()"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["The AUC is very good - this linear classification model can be used to predict frauds!"], "cell_type": "markdown"}], "nbformat_minor": 1}