{"cells":[{"metadata":{"_uuid":"bdbf15d6982984cb9243281fdb2ddd3af5a56c9b","_cell_guid":"dd33bd78-1aa5-41cb-9781-72e0ed01bdf5"},"cell_type":"markdown","source":"# OpenMP Performance Sensitivity to the Number of Threads Used\n\n## TL;DR  \n\n*When using Kaggle Kernels, be aware that Kaggle's backend environment reports more CPUs than are available to your Kernel's execution. Some commonly used libraries default values for performance tuning parameters are **significantly sub-optimal** in this environment.  In particular, when using `xgboost` or other `OpenMP` based libraries, we suggest you start your Python kernels with `import os ; os.environ['OMP_NUM_THREADS'] = '4'`, prior to any other import.  Similarly for R, use `library(OpenMPController)` followed by `omp_set_num_threads(4)`.*\n\n## Full Version\n\nAs covered in our [blog](http://blog.kaggle.com/2017/09/21/product-launch-amped-up-kernels-resources-code-tips-hidden-cells/), an individual Kernel executing on Kaggle gets 4 logical CPUs worth of compute.\n\nHowever, currently (and subject to change), a Kaggle Kernel actually runs on a Google Compute Engine (GCE) VM which has 32 logical CPUs.  That is the CPU count you will see when you query the system, e.g. using the `multiprocessing` Python library:"},{"metadata":{"_uuid":"cfeff610d70db53783c4b70d3ce488645e21e94f","_cell_guid":"673cbc58-ed3f-413c-8e1a-7d7cc9b6376d","trusted":false,"collapsed":true},"cell_type":"code","source":"import multiprocessing\nprint(multiprocessing.cpu_count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6962fc389efd11475f35f99932a44cc5bb5b9ff6","_cell_guid":"8ea01b11-d12e-4951-968e-53ef801dd5ea"},"cell_type":"markdown","source":"While the system has 32 CPUs and your Kernel may use all 32 of these within a given Kernel session, the Docker container the Kernel runs in is limited to use only 4 CPUs worth of time over an arbitrary (but short) period of time.\n\n**In summary, while your Kernel only effectively can use 4 CPUs, the system reports the presence of 32 CPUs.**\n\n## Why does this matter?\n\nMany computation libraries allow you to  benefit from parallelism by making use of multiple logical CPUs when these are available to you.  When a CPU-bound computation can be executed in parallel (from the type of algorithm involved and the capabilities of the implementation) and can benefit from it (the inputs are large enough that the implied fixed-cost overhead is largely amortized), you would typically get the best performance (smallest running time) by using as many computation threads as you have logical CPUs available.  Most of these libraries will enable parallelism when these conditions are met, use that most optimal thread count, by default.\n\nLet's take the example of `xgboost`, a popular library that provides gradient boosted decision trees, and in particular in this notebook, its Python interface.\n\nOur `xgboost` installation is built with [GNU OpenMP](https://gcc.gnu.org/onlinedocs/libgomp) (a.k.a. GOMP) support to handle the parallelism aspects.  Let's find out what GOMP's configuration looks like in Kaggle Kernels' runtime:"},{"metadata":{"_uuid":"0c9a69921704179aa89710687b14e9ea1d76b7e2","_cell_guid":"8e7e5fff-d9a3-4b27-ad1e-209ea348d5ad","trusted":false,"collapsed":true},"cell_type":"code","source":"# We can use the OMP_DISPLAY_ENV environment variable to have GOMP output its\n# configuration to stderr. That is done from C++ (libgomp) by operating on the\n# process's stderr file descriptor. Jupyter doesn't properly intercept operations\n# on that file descriptor (it only intercepts the Python space `sys.stderr`).\n# So let's do that here first:\nimport os\nimport sys\n\nstderr_fileno = 2  # sys.stderr.fileno(), if Jupyter hadn't messed with it.\nrpipe, wpipe = os.pipe()\nos.dup2(wpipe, stderr_fileno)\n\nos.environ['OMP_DISPLAY_ENV'] = 'TRUE'\nimport xgboost as xgb\nos.close(stderr_fileno)\nos.close(wpipe)\nprint(os.read(rpipe, 2**10).decode())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4e60222221afbd4b0afc0989d41c5e8c494e595","_cell_guid":"8b32d4f5-7557-4116-b743-5e9ddea88ae7"},"cell_type":"markdown","source":"You can find the meaning of each of these environment variables in [GOMPs documenation](https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html).\n\nA relevant one to discuss though, is `OMP_NUM_THREADS`.  As the output shows, it defaulted to `32`, which is our system's CPU count.  Let's see how this setting fares in a simple training of the Mercari dataset:"},{"metadata":{"_uuid":"ef30ac6c9ebd380778175db9c00c1c7f9de0efa5","_cell_guid":"898807f7-0a01-4419-aa55-353820a4247a","collapsed":true,"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e0d314ddd0040d5e6d8c01edf9990a81c0118e","_cell_guid":"0182bb5a-4f59-469d-938b-b88a0d27c5bc","collapsed":true,"trusted":false},"cell_type":"code","source":"train_dataset = pd.read_csv('../input/train.tsv', sep='\\t')\nignore_columns = (\n    'name', 'item_description', 'brand_name', 'category_name', 'train_id', 'test_id', 'price')\ntrain_columns = [c for c in train_dataset.columns if c not in ignore_columns]\ndtrain = xgb.DMatrix(train_dataset[train_columns], train_dataset['price'])\nparams = {'silent': 1}\nnum_rounds = 100\n\ndef train(params, dtrain, num_rounds):\n    \"\"\"Returns the duration to train a model with given parameters.\"\"\"\n    start_time = time.time()\n    xgb.train(params, dtrain, num_rounds, [(dtrain, 'train')], verbose_eval=False)\n    return time.time() - start_time\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3bd02a3e0758bd932989e7c3d29c4e32f4633d5","_cell_guid":"010f0736-8a0f-4321-9a1e-b01da9c8c2ac","trusted":false,"collapsed":true},"cell_type":"code","source":"print('duration: %.2fs' % train(params, dtrain, num_rounds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b5349774e63cacbf4474a4d5b51131cd739ccc","_cell_guid":"7ea4b21a-b867-4837-acde-89779ee57a3b"},"cell_type":"markdown","source":"Let's see how this looks when we use only 4 threads, which is how many CPUs we actually can use in the Kaggle Kernels environment.\n\n`OMP_NUM_THREADS` can no longer be used to influence the number of threads used at this point, as it is only read once by `xgboost` and/or GOMP.   We can however set the `xgboost` `nthread` parameter to have it reconfigure this at run-time:"},{"metadata":{"_uuid":"ab83835fd14d807f578ec8f315759bcc93a48ebb","_cell_guid":"c9dd0723-d648-4b27-ae89-dec4ce4e1006","trusted":false,"collapsed":true},"cell_type":"code","source":"params['nthread'] = 4\nprint('duration: %.2fs' % train(params, dtrain, num_rounds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7b72e6d6221e6f1d62e7451900249e744669976","_cell_guid":"0c35551f-35a4-41cf-8737-07364c94a4ad"},"cell_type":"markdown","source":"The difference is very significant, and suprisingly [x] so:  While spawning more threads than there are CPUs available isn't helpful and causes multiple threads to be multiplexed on a single CPU, it is unclear why that overhead causes `xgboost` to perform slower by several multiples.\n\n*[x] Not to you?  Please let me know in comments what you see the cause for this might be!*\n\nHere is some more data:\n"},{"metadata":{"_cell_guid":"3cd0391c-9c41-405b-ac08-e8f791386ddb","_uuid":"8c324db63b65cfe6aff1e172f8694d814e8b4b25","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"results = pd.DataFrame()\nfor nthread in (1, 2, 4, 8, 16, 32):\n    params['nthread'] = nthread\n    durs = []\n    for i in range(16):\n        durs.append(train(params, dtrain, num_rounds))\n    results[nthread] = durs\n    print('nthread = %d, durations = %s' % (nthread, ', '.join(['%.2fs' % d for d in durs])))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"540a9cdfc16faf14570850e4e73bf0a0308aaca2","_cell_guid":"50364112-656b-49f0-af19-8edb142cfb4b","trusted":false,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 6))\nplt.errorbar(results.columns, results.mean(), yerr=results.std(), linestyle='-', fmt='o', ecolor='g', capthick=2)\nplt.xlabel('nthread', fontsize=18)\nplt.ylabel('duration (s)', fontsize=18)\nplt.figsize=(12, 6)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f7a3f82-f8ce-48a0-8ee8-e453c63e418d","_uuid":"e2f87ab27973345f74b53dd4698af59ef12394d4","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}