{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nfrom skimage.io import imread\nfrom skimage.io import imshow\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94f3d09feb1192e8638de233c1eaa7e523b1fc17"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train_labels.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0db22884197ead4744d70957b2f63f4174d6b46"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25f7860c811b02f8679787a26d79858943b9881e"},"cell_type":"code","source":"print(\"Number of training smaples -->\" ,len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56a4c74485e9700193efe5610d89a5c6955c733a"},"cell_type":"code","source":"# Function to generate full path of image file\n\ndef train_func_image_file(x):\n    folder = '../input/train/'\n    path = folder + x + '.tif'\n    return path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3c6f7dd9d93142b70560569f67d85313226b6f2"},"cell_type":"code","source":"# Create image path column in frame\n\ntrain['path'] = train['id'].apply(train_func_image_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8f1018b1e88e5d99c7e02d52a9972a0040c1c37"},"cell_type":"code","source":"print(train['path'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d6e0df9d604309382b84d1841c8f27ba4417f57"},"cell_type":"code","source":"# Read image file using skimage imread functionality\n# Loading all training samples might blow off kernel due to limited memory , so taking maximum possible data\n\ntrain['image'] = train['path'][0:215000].map(imread)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e77bc3a9b9651d8dbd5e97b1643be1d5880076c7"},"cell_type":"code","source":"print(imshow(train['image'][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa466977f904349dd223447bb927f454c9fe533"},"cell_type":"code","source":"# Function to crop image , to reduce memory usage but maintaining target area of image 30x30\n\ndef crop(x):\n    return x[24:72, 24:72]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bd85d44a92dcbb0ca83e91d46d88fb3b7817771"},"cell_type":"code","source":"# Create new column for image crop\n\ntrain['image_crop'] = train['image'][0:215000].map(crop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ae54122efdec8c9c4907c1cdf8cbaff5d1d30b"},"cell_type":"code","source":"print(\"Cropped image\" ,imshow(train['image_crop'][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e50e8afe428c46316d3d99eb64994a2beb08a3f6"},"cell_type":"code","source":"print(\"Dimension of image --->\" ,train['image'][0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954c70af6c0d83d8f9bba909dddaf4967c081362"},"cell_type":"code","source":"print(\"Dimension of crop image --->\" ,train['image_crop'][0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2994a9814f59e3a3442c5b94f06407ec15b0279c"},"cell_type":"code","source":"# Drop unwanted columns to release space\ntrain = train.drop(['path'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8608d3b3ffa80c242a227989d142829d3dcd650a"},"cell_type":"code","source":"train = train.drop(['image'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1cfbd7858eb1b8d6a1f5f19b1c2a0a08a8b0391"},"cell_type":"code","source":"# Garbage collector to release memory\n\nimport gc; \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfecc91566982d146b80fde66c66aed7fa2f8e48"},"cell_type":"code","source":"# Create training array for individual image\n\nx_train = np.stack(list(train.image_crop.iloc[0:215000]), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da68f1b5036c87f0eb435664b0703f2d0f04f1d8"},"cell_type":"code","source":"train = train.drop(['image_crop'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b214bf94f8a3b315250dc2c9f0e5c2b941fda1c"},"cell_type":"code","source":"import gc; \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1c610ea7058633f414c21044e7b4b330e25d931"},"cell_type":"code","source":"x_train = x_train.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ceae0c3510522c7044fb8091feddd8042bbbe58"},"cell_type":"code","source":"# Normalise array values\n\nx_train /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f20dc305823dc6d593b153aee2e31e9e8466f8b4"},"cell_type":"code","source":"# 2 classes 0 / 1 for our binary classification\n\nnum_classes = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"513957142110efe03f6be66bf989ae6de053b69f"},"cell_type":"code","source":"# Label is the target variable\n\ny_train = train['label'][0:215000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bb430295e98a6185e9f21869832ef164b28a288"},"cell_type":"code","source":"y_train = keras.utils.to_categorical(y_train, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4b38696b528d12249850eed865327696a0f5aae"},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90c008fc6e49533dcc6bd2f9ca6a1b21c81d8ad3"},"cell_type":"code","source":"import gc; \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a4b165d21d9cf95bfb239b2a2e343cefcac357"},"cell_type":"code","source":"# Neural network variables to be used\n\nimg_rows, img_cols = 48, 48\n\ninput_shape = (img_rows, img_cols, 3)\n\nbatch_size = 128\nepochs = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5834de036f0ca9e569c9775c59cdfc50941b072"},"cell_type":"code","source":"# Neural network with multiple layers\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(num_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddac802cd3367ce21d38854adfc476cc93283ef9"},"cell_type":"code","source":"model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4064b76c1ec2c551caef6d72684622c4ab41a00"},"cell_type":"code","source":"# Train model\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6a428405ca807a861a0e39963e80ccca4cc3fdc"},"cell_type":"code","source":"del x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01104da40e99c1e8f6dc5731a23be897e7e52e1"},"cell_type":"code","source":"import gc; \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ec50d3fcd6782703a66ff8f3e8699b00be67fd"},"cell_type":"code","source":"# Create list of test image files\n\nimage_file = []\nfor file in os.listdir(\"../input/test/\"):\n    image_file.append(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44ba2a7a7899216a0b7e4c5c135bdf1281a84fa3"},"cell_type":"code","source":"# Create test data frame\n\ntest = pd.DataFrame(image_file,columns=['file'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb36d82173429cc4d45c0a18d95418c04dba32bd"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5554f72bf59db8edf5da931037b859ce77a80ab2"},"cell_type":"code","source":"# Function to generate image test file\n\ndef test_func_image_file(x):\n    folder = '../input/test/'\n    path = folder + x\n    return path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52780dcf9a4ce4e9afcc07f105caba9c63f6585c"},"cell_type":"code","source":"test['path'] = test['file'].apply(test_func_image_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a87e40470027413d48f336c478e794f15d36064"},"cell_type":"code","source":"# Test data image processing\n\ntest['image'] = test['path'][0:].map(imread)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f76a938724877f5ccbd784021203c94f2026e8c"},"cell_type":"code","source":"test['image_crop'] = test['image'][0:].map(crop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac098ff6d238599bfcd84a17e1b877bdc07bc37"},"cell_type":"code","source":"test = test.drop(['image'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"429778313b3f07576851df5a3cafb11c97613638"},"cell_type":"code","source":"x_test = np.stack(list(test.image_crop.iloc[0:]), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"312a29cacf04864464d55cf68bee86e438985015"},"cell_type":"code","source":"test = test.drop(['image_crop'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fff6e8c552a0c2e2c0823c814236090333a0b8c"},"cell_type":"code","source":"import gc; \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faf892f4b8c4061bb4561cc8cd4feba7926ac7ef"},"cell_type":"code","source":"x_test = x_test.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f59a59759f866ea1a0af6f92855449c4ef2d1273"},"cell_type":"code","source":"x_test /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f01f3aca4ab2944656c2c99b3999d73a389d81e"},"cell_type":"code","source":"test['id'] = test['file'].apply(lambda x: os.path.splitext(x)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95012c0fea0e16e3b1829443ef6ec21fb361578d"},"cell_type":"code","source":"predictions = model.predict_classes(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8743c19cf19f22919e69c333c9bd925a46f57bb7"},"cell_type":"code","source":"test['label'] = pd.Series(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef0bc9f8ba25a33a97e076e20e4d80b34b421607"},"cell_type":"code","source":"print(\"Cancer Detected - True Positive --> \",len(test['label'][test['label']==1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b31280124523d033a3374e1031e46a1c4f698590"},"cell_type":"code","source":"print(\"NO Cancer Detected - True Negative --> \",len(test['label'][test['label']==0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a19e2b1c60817763e0d65ee241e59224a022cfb6"},"cell_type":"code","source":"test = test.drop(['file','path'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59bda113ae62aec416b7c51d31de55081cdd2068"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a17a0f9eb08525412b31dc83af310b151772040"},"cell_type":"code","source":"test.to_csv(\"submission.csv\", columns = test.columns, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdc2c25fb26ba1a370b5cdf0a114e7e04c590339"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}