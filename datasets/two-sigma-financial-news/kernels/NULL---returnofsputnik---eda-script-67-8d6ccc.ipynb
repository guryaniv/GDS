{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5363c3301fe77aeac413f57975e1759c5a6ae125"},"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\nimport matplotlib.pyplot as plt\nimport random\nfrom datetime import datetime, date\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"079b1d2474f911283fc1b04e552f421af763fdad"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a5f2fbd071df4c2a86b408fd1e8a719757ae4e6"},"cell_type":"code","source":"market_train_df['time'] = market_train_df['time'].dt.date\nmarket_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae10d6a84619658ee14c3ed8c0ed25717e55f0f5"},"cell_type":"code","source":"from multiprocessing import Pool\n\ndef create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n    return df_code.fillna(-1)\n\ndef generate_lag_features(df,n_lag = [3,7,14]):\n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       'returnsOpenNextMktres10', 'universe']\n    \n    assetCodes = df['assetCode'].unique()\n    print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bd4c3ddba90f4b23e324eefabb98272689ac0ce"},"cell_type":"code","source":"# return_features = ['close']\n# new_df = generate_lag_features(market_train_df,n_lag = 5)\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80bb484853e216761cf2e7e08c0e9b093c08505"},"cell_type":"code","source":"return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\nn_lag = [3,7,14]\nnew_df = generate_lag_features(market_train_df,n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b67955bcf92b534e101be7b8b6aa697869fb7db"},"cell_type":"code","source":"print(market_train_df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc53bf017273aa8c59a0f2dabf80c70eeb20108b"},"cell_type":"code","source":"# return_features = ['open']\n# new_df = generate_lag_features(market_train_df,n_lag=[3,7,14])\n# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4681d35d83bf94c0b9e95904124902ef2153e3bc"},"cell_type":"code","source":"def mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\nmarket_train_df = mis_impute(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b9aa2e280d60a66c006af63f78700861ef90698"},"cell_type":"code","source":"def data_prep(market_train):\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    market_train = market_train.dropna(axis=0)\n    return market_train\n\nmarket_train_df = data_prep(market_train_df)\n# # check the shape\nprint(market_train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f947d36b6b6c986fcba9a7ccb4a71fd5e58021d0"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nup = market_train_df['returnsOpenNextMktres10'] >= 0\n\n\nuniverse = market_train_df['universe'].values\nd = market_train_df['time']\n\nfcol = [c for c in market_train_df if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n\nX = market_train_df[fcol].values\nup = up.values\nr = market_train_df.returnsOpenNextMktres10.values\n\n# Scaling of X values\n# It is good to keep these scaling values for later\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)\n\n# Sanity check\nassert X.shape[0] == up.shape[0] == r.shape[0]\n\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nimport time\n\n# X_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = model_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)\n\n\nte = market_train_df['time']>date(2015, 1, 1)\n\ntt = 0\nfor tt,i in enumerate(te.values):\n    if i:\n        idx = tt\n        print(i,tt)\n        break\nprint(idx)\n# for ind_tr, ind_te in tscv.split(X):\n#     print(ind_tr)\nX_train, X_test = X[:idx],X[idx:]\n\nup_train, up_test = up[:idx],up[idx:]\nr_train, r_test = r[:idx],r[idx:]\nu_train,u_test = universe[:idx],universe[idx:]\nd_train,d_test = d[:idx],d[idx:]\n\ntrain_data = lgb.Dataset(X_train, label=up_train.astype(int))\n# train_data = lgb.Dataset(X, label=up.astype(int))\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f9657c977e8556be91af02d04b84afd4fb460f"},"cell_type":"code","source":"# these are tuned params I found\nx_1 = [0.19000424246380565, 2452, 212, 328, 202]\nx_2 = [0.19016805202090095, 2583, 213, 312, 220]\nprint(up_train)\ndef exp_loss(p,y):\n    y = y.get_label()\n#     p = p.get_label()\n    grad = -y*(1.0-1.0/(1.0+np.exp(-y*p)))\n    hess = -(np.exp(y*p)*(y*p-1)-1)/((np.exp(y*p)+1)**2)\n    \n    return grad,hess\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_1[0],\n        'num_leaves': x_1[1],\n        'min_data_in_leaf': x_1[2],\n#         'num_iteration': x_1[3],\n        'num_iteration': 239,\n        'max_bin': x_1[4],\n        'verbose': 1\n    }\n\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n#         'objective': 'regression',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n#         'num_iteration': x_2[3],\n        'num_iteration': 172,\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\ngbm_1 = lgb.train(params_1,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\ngbm_2 = lgb.train(params_2,\n        train_data,\n        num_boost_round=100,\n        valid_sets=test_data,\n        early_stopping_rounds=5,\n#         fobj=exp_loss,\n        )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26d804f9bf25b8d01272b57a1fb84efa2560ec2a"},"cell_type":"code","source":"confidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test))/2\nconfidence_test = confidence_test * 2 - 1 # Convert from [0,1] to [-1,1]\nprint(max(confidence_test),min(confidence_test), len(confidence_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f90489c498f068d2dc0a0bd7e8e8c138899ae0"},"cell_type":"code","source":"x_t_i = confidence_test * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std\nprint(score_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b7847aae49ac0bd13faf9f1aade8115e21b3723"},"cell_type":"code","source":"conf_preds = pd.concat([pd.Series(d_test).reset_index(drop=True), pd.Series(confidence_test).reset_index(drop=True)], axis=1)\nconf_preds.columns = ['time','confidence']\nconf_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e15d5c9d93f59b67cbda9bfbcb075d1eb9ca31f7"},"cell_type":"code","source":"# Get mean and std per day\nmeans = conf_preds.groupby('time').mean().reset_index()\nstds = conf_preds.groupby('time').std().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c1f54d275657b6e8271d666afb32d3bb3e5f1bc"},"cell_type":"code","source":"agg_info = means.merge(stds, how='inner', on = 'time', suffixes=['_mean','_std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f9ed3b0305b271b1288958d011ef6ccb6a3921e"},"cell_type":"code","source":"agg_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1649c86856c9244e5d564c9cc1e5fc514fd0737"},"cell_type":"code","source":"conf_preds_merged = conf_preds.merge(agg_info, on='time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71f438746549beb1c5f369d106bc2f7b45256e9"},"cell_type":"code","source":"conf_preds_merged","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2fb68807b51106b407978dab4af626ac68c73b5"},"cell_type":"code","source":"postscaled_predictions = ((conf_preds_merged['confidence'] - conf_preds_merged['confidence_mean']) / (conf_preds_merged['confidence_std']*8)).clip(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebfba1c3e5adb2fc69e35a29e37c05c510a4578c"},"cell_type":"code","source":"print(max(postscaled_predictions), min(postscaled_predictions), len(postscaled_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a97328a37b8b6b498547e1a6a73ed4ac1f5f0d2"},"cell_type":"code","source":"x_t_i = postscaled_predictions.values * r_test * u_test\ndata = {'day' : d_test, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_test = mean / std\nprint(score_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"615302cab0deda72132940df44c56aae72122927"},"cell_type":"markdown","source":"# Result: Score is 0.4963, so the score went down because the original one was 0.5166"},{"metadata":{"trusted":true,"_uuid":"37beb02d81d67af459d9bcce7501947d9aca70f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}