{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Introduction\n", "\n", "In this kernel, I will perform my first exploration analysis on a dataset containing texts.\n", "\n", "**Note: ** Feedbacks are more than welcome !\n", "\n", "\n", "# Category analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "44ba5223a31429f07c3ea9dec66b97d9583b791d", "collapsed": true, "_cell_guid": "a1a03a0f-4a2c-45f7-a945-8d451ac0d333"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "%matplotlib inline\n", "\n", "# Visualization\n", "import seaborn as sns\n", "from matplotlib import pyplot as plt\n", "\n", "import matplotlib.pyplot as plt\n", "from matplotlib_venn import venn2\n", "from matplotlib_venn import venn3\n", "\n", "from wordcloud import WordCloud\n", "\n", "from collections import Counter\n", "import re\n", "import string\n", "from nltk.corpus import stopwords\n", "stop = stopwords.words('english')\n", "\n", "\n", "sns.set(style=\"white\", context=\"talk\")\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "fd0e71750c16fb05439234cb458dcccdf04eda5a", "_cell_guid": "49dbc2ea-204f-4ef2-b13b-5aaa0962490f"}, "source": ["df_train = pd.read_csv('../input/train.csv')\n", "\n", "\n", "\n", "COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "\n", "\n", "\n", "# Adding 'none' columns if there is no '1' in COLUMNS\n", "df_train['none'] = (df_train[COLUMNS].max(axis=1) == 0).astype(int)\n", "COLUMNS.append('none')\n", "CATEGORIES = COLUMNS.copy()\n", "\n", "print(df_train.shape)\n", "print(df_train.columns.values)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0309a8105d7b2661c9245407d9ac334b8755046d", "_cell_guid": "7168f7e0-f9bc-4bbc-92b4-d8eafadbf134"}, "source": ["df_distribution = df_train[COLUMNS].sum()\\\n", "                            .to_frame()\\\n", "                            .rename(columns={0: 'count'})\\\n", "                            .sort_values('count')\n", "\n", "df_distribution.drop('none').plot.pie(y='count',\n", "                                      title='Label distribution over comments (without \"none\" category)',\n", "                                      figsize=(5, 5))\\\n", "                            .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["df_distribution.sort_values('count', ascending=False)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "5a0a1b320ca742ab565f613809a0fb3205e89857", "_cell_guid": "b0f4b3da-5e02-46a7-b01f-0f29b5d31cfd"}, "source": ["The three major labels are :\n", "1. toxic\n", "2. obscene\n", "3. insult\n", "\n", "Let's take a look at the number of comment for each label combination.\n", "\n", "Here we are looking for combinations that are frequent. Which would indicate a correlation between categories"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c97dcc88998b4174aadef53a0d06eb7bed0d33a7", "_cell_guid": "58786142-0853-41e8-929e-8fde1482d345"}, "source": ["df_comb = df_train.groupby(COLUMNS)\\\n", "                    .size()\\\n", "                    .sort_values(ascending=False)\\\n", "                    .reset_index()\\\n", "                    .rename(columns={0: 'count'})\n", "df_comb.head(n=10)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "dcf6d9a9d98282b588b73f2a44792b44c440ad1c", "_cell_guid": "24263e1e-0537-49ca-8a74-0b0238ff97a1"}, "source": ["We can see several things : \n", "1. As expected, the 'none' label is clearly ahead with 86061 comments\n", "2. 'toxic', which is the first 'real' label, is coming in all combination from rank 1 to 6\n", "3. In this 6 rows, 'obscene' comes 4 times   \n", "4. The number of comments for each combination drops exponentially\n", "\n", "Let's check the correlation matrix :\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["f, ax = plt.subplots(figsize=(9, 6))\n", "f.suptitle('Correlation matrix for categories')\n", "sns.heatmap(df_train[COLUMNS].corr(), annot=True, linewidths=.5, ax=ax)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "01ffa99d80c71541ff7df331c9cc6d593c50c8d9", "collapsed": true, "_cell_guid": "aa0ea34e-4fff-4257-8934-5b20e2a2eb33"}, "source": ["The correlation matrix shows interesting things : \n", "\n", "1. 'toxic' is clearly correlated with 'obscene' and 'insult' (0.68 and 0.65)\n", "2. 'toxic' and 'severe_toxic' are only got a 0.31 correlation factor\n", "3. 'insult' and 'obscene' have a correlation factor of 0.74\n", "\n", "\n", "From my point of view, there are several combinations that are worth digging into :\n", "\n", "1. 'toxic' <-> 'severe_toxic'. The semantic of these two categories seems to show some kind of graduation between them\n", "2. 'toxic' <-> 'insult' and 'toxic' <-> 'obscene'\n", "3. 'insult' <-> 'obscene'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["\n", "t = df_train[(df_train['toxic'] == 1) & (df_train['insult'] == 0) & (df_train['obscene'] == 0)].shape[0]\n", "i = df_train[(df_train['toxic'] == 0) & (df_train['insult'] == 1) & (df_train['obscene'] == 0)].shape[0]\n", "o = df_train[(df_train['toxic'] == 0) & (df_train['insult'] == 0) & (df_train['obscene'] == 1)].shape[0]\n", "\n", "t_i = df_train[(df_train['toxic'] == 1) & (df_train['insult'] == 1) & (df_train['obscene'] == 0)].shape[0]\n", "t_o = df_train[(df_train['toxic'] == 1) & (df_train['insult'] == 0) & (df_train['obscene'] == 1)].shape[0]\n", "i_o = df_train[(df_train['toxic'] == 0) & (df_train['insult'] == 1) & (df_train['obscene'] == 1)].shape[0]\n", "\n", "t_i_o = df_train[(df_train['toxic'] == 1) & (df_train['insult'] == 1) & (df_train['obscene'] == 1)].shape[0]\n", "\n", "\n", "# Make the diagram\n", "plt.figure(figsize=(8, 8))\n", "plt.title(\"Venn diagram for 'toxic', 'insult' and 'obscene'\")\n", "venn3(subsets = (t, i, t_i, o, t_o, i_o, t_i_o), \n", "      set_labels=('toxic', 'insult', 'obscene'))\n", "plt.show()\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["This venn diagram demonstrates the correlations found in the previous visualization. \n", "\n", "1. There is only a small part of 'insult' and 'obscene' that are not also labelled 'toxic'.\n", "2. 3610 comments are labelled with all 3 categories.\n", "\n", "**Note: ** The library used for the Venn diagram does not have a venn4 object, that's why I couldn't display 'severe_toxic' with them.\n", "\n", "Let's take a look at the Venn diagram between 'toxic' and 'severe_toxic'."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0ad55972b8d88a3bf005015f96f902fc5f8f832c", "_cell_guid": "35c49ee8-e6f2-497f-8a0c-471c9ce433ae"}, "source": ["\n", "t = df_train[(df_train['toxic'] == 1) & (df_train['severe_toxic'] == 0)].shape[0]\n", "s = df_train[(df_train['toxic'] == 0) & (df_train['severe_toxic'] == 1)].shape[0]\n", "\n", "t_s = df_train[(df_train['toxic'] == 1) & (df_train['severe_toxic'] == 1)].shape[0]\n", "\n", "\n", "# Make the diagram\n", "plt.figure(figsize=(8, 8))\n", "plt.title(\"Venn diagram for 'toxic' and 'severe_toxic'\")\n", "venn2(subsets = (t, s, t_s), \n", "      set_labels=('toxic', 'severe_toxic'))\n", "plt.show()\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "7e69391c37ec7e21f74c4b3a2a41a71eb323876c", "collapsed": true, "_cell_guid": "abcae53f-03b3-4a48-b5ac-fe8f8467e874"}, "source": ["1. The 'severe_toxic' category is completely contained in 'toxic' which goes in favor of the semantic link between the two category names. \n", "2. The 0.31 correlation factor is explained by the fact that 'severe_toxic' representes a small percentage (11.67%) of 'toxic'. \n", "\n", "Before diving into words, let's analyze the comment structure :\n", "\n", "1. Total length\n", "    * It could indicate the writer implication (either in a good way or 'bad' one)\n", "2. Total number of carriage returns\n", "    * It could indicate some kind of structure in the comment"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["df_train['total_length'] = df_train['comment_text'].str.len()\n", "df_train['new_line'] = df_train['comment_text'].str.count('\\n'* 1)\n", "df_train['new_small_space'] = df_train['comment_text'].str.count('\\n'* 2)\n", "df_train['new_medium_space'] = df_train['comment_text'].str.count('\\n'* 3)\n", "df_train['new_big_space'] = df_train['comment_text'].str.count('\\n'* 4)\n", "\n", "df_train['new_big_space'] = df_train['comment_text'].str.count('\\n'* 4)\n", "df_train['uppercase_words'] = df_train['comment_text'].apply(lambda l: sum(map(str.isupper, list(l))))\n", "df_train['question_mark'] = df_train['comment_text'].str.count('\\?')\n", "df_train['exclamation_mark'] = df_train['comment_text'].str.count('!')\n", "\n", "FEATURES = ['total_length', \n", "            'new_line', \n", "            'new_small_space', \n", "            'new_medium_space', \n", "            'new_big_space', \n", "            'uppercase_words',\n", "            'question_mark',\n", "            'exclamation_mark']\n", "COLUMNS += FEATURES"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["f, ax = plt.subplots(figsize=(20, 20))\n", "f.suptitle('Correlation matrix for categories and features')\n", "sns.heatmap(df_train[COLUMNS].corr(), annot=True, linewidths=.5, ax=ax)\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**Note: ** small, medium and big space features are inclusive, meaning that all 'big_space' are medium and small ones (same for medium_space)\n", "\n", "1. The new correlation matrix with the added features does not show any strong correlations. \n", "2. One thing worth noting is that 'uppercase_words' (which could be assimilated to 'yelling') is slightly more correlated.\n", "3. 'uppercase_words' are correlated with 'exclamation_mark' up to 0.13 which could mean that people express the urge to add as many '!' as possible when they are 'yelling' ;)\n", "\n", "\n", "There is nothing else that comes in mind that I could explore. Please feel free to suggest any idea in comments below.\n", "\n", "# Word analysis\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "source": ["word_counter = {}\n", "\n", "\n", "def clean_text(text):\n", "    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n", "    return ' '.join([word for word in text.split() if word not in (stop)])\n", "\n", "for categ in CATEGORIES:\n", "    d = Counter()\n", "    df_train[df_train[categ] == 1]['comment_text'].apply(lambda t: d.update(clean_text(t).split()))\n", "    word_counter[categ] = pd.DataFrame.from_dict(d, orient='index')\\\n", "                                        .rename(columns={0: 'count'})\\\n", "                                        .sort_values('count', ascending=False)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "source": ["for w in word_counter:\n", "    wc = word_counter[w]\n", "\n", "    wordcloud = WordCloud(\n", "          background_color='black',\n", "          max_words=200,\n", "          max_font_size=100, \n", "          random_state=4561\n", "         ).generate_from_frequencies(wc.to_dict()['count'])\n", "\n", "    fig = plt.figure(figsize=(12, 8))\n", "    plt.title(w)\n", "    plt.imshow(wordcloud)\n", "    plt.axis('off')\n", "\n", "    plt.show()\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The vocabulary used in all categories is quite similar (expect for 'none' of course). Frequencies are varying a bit across (for example 'fuck' and 'suck'.\n", "\n", "\n", "\n", "# Conclusion\n", "\n", "In this kernel, we've found out that the categories we need to predict are overlapping each over. In the basic exploration of words contained in the comments, we can say that the vocabulary is quite similar across all categories except for the 'none' one. While this might be enough to detect unwanted comments, it is clearly not enough to categorised them. \n", "\n"]}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4}