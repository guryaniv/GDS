{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2b59f152-423a-4dd1-31b8-d5e49b249f9a"
      },
      "source": [
        "Introduction\n",
        "====\n",
        "\n",
        "In this notebook I will continue the analysis from my [previous notebook](https://www.kaggle.com/dstuerzer/d/dalpozz/creditcardfraud/optimized-logistic-regression), in which I have discussed the pecularities of highly imbalanced datasets, and tried to optimize a Logistic Regression to predict frauds. Even though the results were quite satisfactory, still better models can be found. Here I will discuss the application of the very popular XGBoost method. It will indeed significantly improve the prediction accuracy. \n",
        "\n",
        "Due to the runtime limitations for Kaggle notebooks, this is only an abridged version, where I have left out the grid search and parameter optimization, as well as high-resollution plots. However, they are an essential part of this notebook, so you might want to have a look at the full version on [GitHub](https://github.com/dstuerzer/Kaggle/blob/master/credit_card_fraud/XGBoost.ipynb), including high-resolution plots. The runtime of the latter notebook was several days.\n",
        "\n",
        "First I load packages and a function to visualize the confusion matrix (you can skip this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36c8f121-6ee0-02f3-9c01-056bdc81127f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "def show_data(cm, print_res = 0):\n",
        "    tp = cm[1,1]\n",
        "    fn = cm[1,0]\n",
        "    fp = cm[0,1]\n",
        "    tn = cm[0,0]\n",
        "    if print_res == 1:\n",
        "        print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n",
        "        print('Recall (TPR) =  {:.3f}'.format(tp/(tp+fn)))\n",
        "        print('Fallout (FPR) = {:.3e}'.format(fp/(fp+tn)))\n",
        "    return tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fcdb0990-c747-c192-9e50-9581a9855955"
      },
      "source": [
        "We read the data, and split it into the training data (X_, y_), which will be used to optimize the parameters and train our final model. This will then be validated on the test set (X_test, y_test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01de5e4e-294d-10d4-bbd4-c5115ab03a3e"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../input/creditcard.csv\")\n",
        "y = np.array(df.Class.tolist())     #classes: 1..fraud, 0..no fraud\n",
        "df = df.drop('Class', 1)\n",
        "X = np.array(df.as_matrix())   # features\n",
        "\n",
        "X_, X_test, y_, y_test = train_test_split(X, y, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa2d976f-e523-9252-4436-1c583f6b2107"
      },
      "source": [
        "From the extensive discussion of the dataset in my [previous notebook](https://www.kaggle.com/dstuerzer/d/dalpozz/creditcardfraud/optimized-logistic-regression) we know that the classes \"Fraud\" and \"Non-Fraud\" are highly imbalanced, and that Precision, Recall and Fallout are meaningful measures for the accuracy of a model.\n",
        "\n",
        "\n",
        "Strategies\n",
        "===\n",
        "\n",
        "Convenient ways to characterize the performance of a classifier here are Fallout and Recall (and the corresponding ROC-curve), and possibly the Precision (check out [Wikipedia](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) for definitions). In order to find good parameters we might try the following approaches:\n",
        "\n",
        "* Maximize the Recall for fixed, sufficiently small Fallout.\n",
        "* Maximize the ROC-AUC. This metric is built in, but might lead to different results than the previous strategy.\n",
        "* Maximize the F1-score, which is the harmonic mean of Precision and Recall. \n",
        "Here, I will compare all three strategies. \n",
        "\n",
        "Before we start, it might be useful to recall our goal. Most importantly, we want to maximize the Recall, which is the probability of actually detecting a fraud. Undetected frauds are what actually creates costs for the bank and/or the customer. However, if we increase the Recall, we also increase the rate of 'false alarms' (i.e., the Fallout), and this can be very annoying, even though it does not directly generate costs. However, if a customer gets a false fraud alert every week, he or she will consider changing to another bank. Keeping false alarms small means keeping the Fallout small, or (almost) equivalently, keeping the Precision high. Hence, we will either:\n",
        "\n",
        "* Find a tradeoff between high Recall and low Fallout, or\n",
        "* simply maximize the F1-score, therefore 'maximizing' Precision and Recall at the same time.\n",
        "\n",
        "Grid Search - Maximizing the F1-Score\n",
        "===\n",
        "\n",
        "That is our first approach. I will only initialize the grid search here, but not execute it, since it would be too time-consuming here (see my GitHub-repo for this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb97c89b-ad8b-a57e-d4a5-a2c0a0b37bfc"
      },
      "outputs": [],
      "source": [
        "cv_params = {'max_depth': [1,2,3,4,5,6], 'min_child_weight': [1,2,3,4]}    # parameters to be tries in the grid search\n",
        "fix_params = {'learning_rate': 0.2, 'n_estimators': 100, 'objective': 'binary:logistic'}   #other parameters, fixed for the moment \n",
        "csv = GridSearchCV(xgb.XGBClassifier(**fix_params), cv_params, scoring = 'f1', cv = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b736e0e5-09cf-0397-986d-46cb341fa780"
      },
      "source": [
        "```\n",
        "csv.fit(X_, y_) \n",
        "```\n",
        "\n",
        "runs the search, and \n",
        "\n",
        "```\n",
        "csv.best_params_\n",
        "```\n",
        "\n",
        "returns the optimal parameteres within the list. We \"see\" that the particular choice of min_child_weight does not have a significant impact on the F1-score. Let's still pick the best results of this search, i.e., max_depth = 5, and min_child_weight = 3, and continue the grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe4228ac-e099-a3a0-ba67-ac491c5d128d"
      },
      "outputs": [],
      "source": [
        "cv_params = {'subsample': [0.8,0.9,1], 'max_delta_step': [0,1,2,4]}\n",
        "fix_params = {'learning_rate': 0.2, 'n_estimators': 100, 'objective': 'binary:logistic', 'max_depth': 5, 'min_child_weight':3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1148556a-e8f9-f151-ac4c-a7b68be5cce5"
      },
      "source": [
        "```\n",
        "csv = GridSearchCV(xgb.XGBClassifier(**fix_params), cv_params, scoring = 'f1', cv = 5) \n",
        "csv.fit(X_, y_)\n",
        "csv.grid_scores_\n",
        "csv.best_params_\n",
        "```\n",
        "\n",
        "We obtain that max_delta_step =1 and subsample = 0.8 are (quasi)-optimal. Finally we search for an optimal leaning rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "369fe796-8d08-f859-f339-1c4f216916ee"
      },
      "outputs": [],
      "source": [
        "cv_params = {'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]}\n",
        "fix_params['max_delta_step'] = 1\n",
        "fix_params['subsample'] = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "88794d7f-7356-1c26-9492-3578fdf8cfbb"
      },
      "source": [
        "```\n",
        "csv = GridSearchCV(xgb.XGBClassifier(**fix_params), cv_params, scoring = 'f1', cv = 5) \n",
        "csv.fit(X_, y_)\n",
        "csv.grid_scores_\n",
        "csv.best_params_\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d833cfb7-c981-6791-48ab-e166e24612cd"
      },
      "source": [
        "Unfortunately, running the same grid search again often yields different 'optimal' parameters. We settle for the rate 0.2, and here are our final parameters (if we wanted, we could rerun the grid searches another time, but I believe that the gain will be minimal compared to the associated computational costs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9ab2b186-3136-24f1-6508-4dbde8076aff"
      },
      "outputs": [],
      "source": [
        "fix_params['learning_rate'] = 0.2\n",
        "params_final =  fix_params\n",
        "print(params_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "01d00cf4-1b4d-a873-1a51-c20fbe1a47be"
      },
      "source": [
        "Now we train our final model on the entire training set, and evaluate it on the still unused testing set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18498b66-edb7-3420-b429-c6cccaff0896"
      },
      "outputs": [],
      "source": [
        "xgdmat_train = xgb.DMatrix(X_, y_)\n",
        "xgdmat_test = xgb.DMatrix(X_test, y_test)\n",
        "xgb_final = xgb.train(params_final, xgdmat_train, num_boost_round = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6b77af45-fc9e-b33e-a01d-84c22c06ce90"
      },
      "outputs": [],
      "source": [
        "y_pred = xgb_final.predict(xgdmat_test)\n",
        "thresh = 0.08\n",
        "y_pred [y_pred > thresh] = 1\n",
        "y_pred [y_pred <= thresh] = 0\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(cm, ['0', '1'], )\n",
        "pr, tpr, fpr = show_data(cm, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0f2b889a-84c6-734c-9c89-43846bef2ece"
      },
      "source": [
        "We can still play around with the probability threshold, such that Precision and Recall are to our liking. As far as we can tell from the evaluation of the final model on the testing set, it seems at least comparable to the Logistic Regression.\n",
        "\n",
        "Grid Search - ROC-AUC\n",
        "===\n",
        "\n",
        "We can repeat exactly the same reasoning, just with scoring = 'roc_auc' in the CV. Indeed, we get similiar near-optimal parameters. I do not show this here.\n",
        "\n",
        "Maximized Recall for given Fallout\n",
        "===\n",
        "\n",
        "Here I will apply a similar strategy to the one described in my previous notebook about the Logistic Regression. I will repeat CV multiple times in order to get averaged, precise ROC-curves and Precision-Recall-curves (PR). If we specify a Fallout or a desired Precision, we can use the curves to find the optimal parameters such that the Recall is then maximized. Since I will average over many curves, this is a very precise approach, but it is bound to be very computation-intensive.\n",
        "\n",
        "Let us specify the XGBoost-parameters we for now assume best, e.g. the ones we have obtained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bdffd76a-17ed-d098-68ca-a66861f798ba"
      },
      "outputs": [],
      "source": [
        "par = params_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "85935925-1747-545e-1a50-06758df734af"
      },
      "source": [
        "The following function splits the training data into a further training and a valuation set, and generates the ROC- and the PR-curve based on the valuation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e346198d-e507-0dce-493d-1789f9b83729"
      },
      "outputs": [],
      "source": [
        "def get_curves(X_, y_, pars):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size = 0.2)\n",
        "    clf = xgb.XGBClassifier(**pars)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_prob = clf.predict_proba(X_val)[:,clf.classes_[1]]\n",
        "    fpr, tpr, thresholds_roc = roc_curve(y_val, y_prob)\n",
        "    prec, rec, thresholds_pr = precision_recall_curve(y_val, y_prob)\n",
        "    return fpr, tpr, prec, rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "33c9bfa9-23d1-a032-fe96-b5a63c9ed804"
      },
      "source": [
        "This function now calls 'get_curves' *N_iter* times and computes averaged ROC- and PR-curves. I have chosen *N_iter = 300*, and obtained very smooth ROC-curves. This is important, since we want to use the curves to find the optimal parameters. Too much noise (i.e. not enough smoothing) will likely obfuscate the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd45db59-44b9-242c-f961-210a1ac25cb9"
      },
      "outputs": [],
      "source": [
        "def gen_curves(X_, y_, pars):\n",
        "    N_iter = 300\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100000)\n",
        "    \n",
        "    mean_prec = 0.0\n",
        "    mean_rec = np.linspace(0, 1, 100000)\n",
        "    \n",
        "    for n in range(N_iter):\n",
        "        fpr, tpr, prec, rec = get_curves(X_, y_, pars)\n",
        "        prec = list(reversed(prec)) #reverse, otherwise the interp doesn not work\n",
        "        rec = list(reversed(rec))\n",
        "        mean_tpr  += np.interp(mean_fpr, fpr, tpr)\n",
        "        mean_prec += np.interp(mean_rec, rec, prec)\n",
        "\n",
        "    mean_tpr /= N_iter\n",
        "    mean_prec /= N_iter\n",
        "    \n",
        "    return mean_fpr, mean_tpr, mean_prec, mean_rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7e068f7f-bc36-fecc-cf5c-3aa5bac4cb7d"
      },
      "source": [
        "And this function finally plots the curves for different parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1860c4b0-ad09-1651-5040-fa87ef86023f"
      },
      "outputs": [],
      "source": [
        "def plot_roc(X_, y_, par, name_par, list_par):\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (18,7));\n",
        "    for l in list_par:\n",
        "        par[name_par] = l\n",
        "        print(par)\n",
        "        mean_fpr, mean_tpr, mean_prec, mean_rec = gen_curves(X_, y_, par)\n",
        "        ax1.plot(mean_fpr, mean_tpr, label = name_par+\" = \"+str(l))\n",
        "        ax2.plot(mean_rec, mean_prec, label = name_par+\" = \"+str(l))\n",
        "    ax1.set_xlim([0, 0.0005])\n",
        "    ax1.set_ylim([0.5, 0.95])\n",
        "    ax1.axvline(2e-4, color='b', linestyle='dashed', linewidth=2)\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.set_xlabel('FPR/Fallout')\n",
        "    ax1.set_ylabel('TPR/Recall')\n",
        "    ax2.set_xlabel('Recall')\n",
        "    ax2.set_ylabel('Precision')\n",
        "    ax1.set_title('ROC')\n",
        "    ax2.set_title('PR')\n",
        "    ax2.legend(loc = \"lower left\")\n",
        "    ax2.set_xlim([0.5, 1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e68c18d5-aed5-c7a3-a025-feaceedd3d28"
      },
      "source": [
        "Let us first vary 'max_depth':\n",
        "\n",
        "```\n",
        "plot_roc(X_, y_, par, 'max_depth', [1,2,3,4,5,7,10,15])\n",
        "```\n",
        "\n",
        "We see that small values of 'max_depth' are not so good (again, please check out [GitHub](https://github.com/dstuerzer/Kaggle/blob/master/credit_card_fraud/XGBoost.ipynb) for high-resolution and high-precision plots), and fix max_depth = 5. We proceed with a grid search for 'learning_rate' (again, on GitHub I use a finer grid):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c789ae8-fc2e-00bd-7137-24452811ceee"
      },
      "outputs": [],
      "source": [
        "par['max_depth'] = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "51cbd90b-e4b8-a7d2-feda-808e12bf1375"
      },
      "source": [
        "```\n",
        "plot_roc(X_, y_, par, 'learning_rate',  [0.05,0.1,0.15,0.2,0.25,0.3])\n",
        "```\n",
        "\n",
        "From the plots here we see that a learning rate around 0.2 seems best. In the grid search for the F1 score above we have observed that it was mostly those two parameters that had an influence on the performance of the model. This suggests we stop here, and set our final parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8aaa1fa2-a513-781f-cf07-90b5ff14f047"
      },
      "outputs": [],
      "source": [
        "par['learning_rate'] = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dd76ff93-199c-879e-d514-be4be48e900c"
      },
      "source": [
        "We now train the final model on the full training set (X_, y_) and apply to the still untouched testing set (X_test, y_test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bca56263-692e-6364-41d8-ec321ac0b7ff"
      },
      "outputs": [],
      "source": [
        "xgdmat_train = xgb.DMatrix(X_, y_)\n",
        "xgdmat_test = xgb.DMatrix(X_test, y_test)\n",
        "xgb_final = xgb.train(par, xgdmat_train, num_boost_round = 100)\n",
        "y_pred = xgb_final.predict(xgdmat_test)\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred)\n",
        "prec, rec, thresholds_pr = precision_recall_curve(y_test, y_pred)\n",
        "\n",
        "mean_fpr = np.linspace(0, 1, 100000)\n",
        "mean_rec = np.linspace(0, 1, 1000)\n",
        "\n",
        "prec = list(reversed(prec)) #reverse, otherwise the interp doesn not work\n",
        "rec = list(reversed(rec))\n",
        "mean_tpr = np.interp(mean_fpr, fpr, tpr)\n",
        "mean_prec = np.interp(mean_rec, rec, prec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "633ce093-2416-48f5-a964-af1c9e3d848f"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize = (18,7));\n",
        "ax1.plot(mean_fpr, mean_tpr);\n",
        "\n",
        "ax2.plot(mean_rec, mean_prec);\n",
        "\n",
        "ax1.set_xlim([0, 0.0005])\n",
        "ax1.set_ylim([0.5, 0.95])\n",
        "ax1.axvline(2e-4, color='b', linestyle='dashed', linewidth=2)\n",
        "ax1.set_xlabel('FPR/Fallout')\n",
        "ax1.set_ylabel('TPR/Recall')\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax1.set_title('ROC')\n",
        "ax2.set_title('PR')\n",
        "ax2.set_xlim([0.5, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "238a2903-acf4-908c-4836-02b331b530da"
      },
      "outputs": [],
      "source": [
        "Above we see the ROC- and the PR-curve for our model on the testing set. The results on the training set indicate that our XGBoost-model performs better than the Logistic Regression (compare to my previous notebook): Especially for the smoothed curves (again, see GitHub) and a fixed Fallout of 2e-4 the (average) Recall is about 5% higher. On the testing set we expect higher fluctuations, but nevertheless, the model performs well there too.\n",
        "\n",
        "The prediction vector *y_pred* contains now probabilities for the classes. By varying the threshold, we obtain the ROC- and PR-curves, and can adapt the sensitivity of the model to our liking (having an eye on the curves, so we know what we can expect)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5a86afe-b8de-7e85-93ea-6864985f2521"
      },
      "outputs": [],
      "source": [
        "y_final = np.copy(y_pred)\n",
        "thresh = 0.08\n",
        "y_final [y_final > thresh] = 1\n",
        "y_final [y_final <= thresh] = 0\n",
        "cm = confusion_matrix(y_test, y_final)\n",
        "plot_confusion_matrix(cm, ['0', '1'], )\n",
        "pr, tpr, fpr = show_data(cm, print_res = 1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9fc8e1fe-748c-65f4-5345-37ae8aaac0f8"
      },
      "source": [
        "Unfortunately, we have to expect a high variation in the testing results, since the testing set only contains very few frauds. The averaged ROC- and PR-curves might still be a better indication on the actual quality of the model.\n",
        "\n",
        "One of the main lessons of this analysis is that XGBoost will (on average) outperform the Logistic Regression, and is more suited to predict frauds. In particular, the ROC-curve is much steeper around the origin, and we can achieve similar Recall rates with only half of the Fallout (i.e. a Fallout of ~80% with a Recall of just 1e-4). Alternatively, we can expect a Precision *and* a Recall both of ~85%. See GitHub for the confirmation of these results."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}