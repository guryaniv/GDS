{"cells":[{"metadata":{"_uuid":"9cf11d0446f8b35daeafc0a085566083929419a8"},"cell_type":"markdown","source":"## LGBM CV Tuning and Seed Diversification\n_By Nick Brooks, June 2018_\n\nInspiration for this kernel is from [@Silogram](https://www.kaggle.com/psilogram)'s [Post](https://www.kaggle.com/c/home-credit-default-risk/discussion/58332)."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"f3ae27d2f6c11d8100dd600dbb9ffaabd55d8c25","collapsed":true},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport random\nrandom.seed(2018)\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\n\n# Specify index/ target name\nid_col = \"ID\"\ntarget_var = \"target\"\n\n# House Keeping Parameters\nDebug = False\nHome = False\nBuild_Results_csv = False # if running for first time\n\nresults = pd.DataFrame(columns = [\"Rounds\",\"Score\",\"STDV\", \"LB\", \"Parameters\"])\nif Build_Results_csv is True & Home is True: results.to_csv(\"results.csv\")\nif Home is True:\n    import os\n    path = r\"D:\\My Computer\\DATA\\Santander\"\n    os.chdir(path)\n    \n    print(\"Data Load Stage\")\n    training = pd.read_csv('train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(100)\n    traindex = training.index\n    testing = pd.read_csv('test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(100)\n    testdex = testing.index\nelse:\n    print(\"Data Load Stage\")\n    training = pd.read_csv('../input/train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(100)\n    traindex = training.index\n    testing = pd.read_csv('../input/test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(100)\n    testdex = testing.index\n\ny = np.log1p(training[target_var])\ntraining.drop(target_var,axis=1, inplace=True)\nprint('Train shape: {} Rows, {} Columns'.format(*training.shape))\nprint('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n\nprint(\"Combine Train and Test\")\ndf = pd.concat([training,testing],axis=0)\ndel training, testing\ngc.collect()\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c07581b2cd588cd571bc7e6a74455bea15fba76","collapsed":true},"cell_type":"code","source":"# Modeling Datasets\ntest_df = df.loc[testdex,:]\nvocab = df.columns\n\n# LGBM Dataset\nlgtrain = lgb.Dataset(df.loc[traindex,vocab],y ,feature_name = \"auto\")\nprint(\"Starting LightGBM. Train shape: {}, Test shape: {}\".format(df.loc[testdex,:].shape,test_df.shape))\nprint(\"Feature Num: \",len(vocab))\ndel df; gc.collect();","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"5a7c25462528493a447219728b03bcfb2cdf41bd"},"cell_type":"markdown","source":"## Modeling Stage"},{"metadata":{"trusted":true,"_uuid":"1c43324b7c5dfc9e31e94f40a32a2c4ed146b84d","collapsed":true},"cell_type":"code","source":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 180,\n    \"feature_fraction\": 0.50,\n    \"bagging_fraction\": 0.50,\n    'bagging_freq': 4,\n    \"max_depth\": -1,\n    \"reg_alpha\": 0.3,\n    \"reg_lambda\": 0.1,\n    #\"min_split_gain\":0.2,\n    \"min_child_weight\":10,\n    'zero_as_missing':True\n                }","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"5a281f230dd6d1f639c771d2b348937d619b422d"},"cell_type":"markdown","source":"## Cross-Validation\n**Hand Tuning:**"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"3e7310ab293e89b282b0e776d75bdbc3b3d1764c","collapsed":true},"cell_type":"code","source":"modelstart= time.time()\n# Find Optimal Parameters / Boosting Rounds\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=False,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)\n\noptimal_rounds = np.argmin(lgb_cv['rmse-mean'])\nbest_cv_score = min(lgb_cv['rmse-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv['rmse-stdv'][optimal_rounds]))\n\nresults = results.append({\"Rounds\": optimal_rounds,\n                          \"Score\": best_cv_score,\n                          \"STDV\": lgb_cv['rmse-stdv'][optimal_rounds],\n                          \"LB\": None,\n                          \"Parameters\": lgbm_params}, ignore_index=True)\nif Home is True:\n    with open('results.csv', 'a') as f:\n        results.to_csv(f, header=False)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbe227cecd9d85db2e23da062996e287d86995aa","collapsed":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 800)\ndisplay(results.sort_values(by=\"Score\",ascending = True))","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"4bc4f5e58b77f0c5b04a498f20ee566ff813622a"},"cell_type":"markdown","source":"**Iterative Tuning:** <br>\nMy current learning rate is 0.005. Say I what to see how well it's neigbors perform.."},{"metadata":{"trusted":true,"_uuid":"f6b13283e701dd3297c49942d75f164e555e160e","scrolled":false,"collapsed":true},"cell_type":"code","source":"learning_rates = [0.012,0.008]\nfor param in learning_rates:\n    print(\"Learning Rate: \", param)\n    modelstart= time.time()\n    lgbm_params[\"learning_rate\"] = param\n    # Find Optimal Parameters / Boosting Rounds\n    lgb_cv = lgb.cv(\n        params = lgbm_params,\n        train_set = lgtrain,\n        num_boost_round=10000,\n        stratified=False,\n        nfold = 5,\n        verbose_eval=200,\n        seed = 23,\n        early_stopping_rounds=75)\n\n    optimal_rounds = np.argmin(lgb_cv['rmse-mean'])\n    best_cv_score = min(lgb_cv['rmse-mean'])\n\n    print(\"Optimal Round: {}\\nOptimal Score: {} + {}\".format(\n        optimal_rounds,best_cv_score,lgb_cv['rmse-stdv'][optimal_rounds]))\n    print(\"###########################################################################################\")\n\n    results = results.append({\"Rounds\": optimal_rounds,\n                              \"Score\": best_cv_score,\n                              \"STDV\": lgb_cv['rmse-stdv'][optimal_rounds],\n                              \"LB\": None,\n                              \"Parameters\": lgbm_params}, ignore_index=True)\n    if Home is True:\n        with open('results.csv', 'a') as f:\n            results.to_csv(f, header=False)\n        # results = pd.read_csv(\"results.csv\")","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0adfba411a52c4c95d1f0067466375e27367937a","collapsed":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 800)\ndisplay(results.sort_values(by=\"Score\",ascending = True))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"efb5bb129aac2718351d80548420f6c352397515"},"cell_type":"markdown","source":"## Final Model and Seed Diversification\nOut-of-Fold Methods are very poplular in public kernels at the moment. This is a alternative that operates in a similar way. Instead of explicitly dividing the data, the randomized seed changes the row and column subsampling choice which also forces the model to do without some data to achieve a more diverse, robust prediction."},{"metadata":{"trusted":true,"_uuid":"efbb975ccab79d7a6ee7b58522561e1cffea4cdd","collapsed":true},"cell_type":"code","source":"# Best Parameters\nfinal_model_params = results.iloc[results[\"Score\"].idxmin(),:][\"Parameters\"]\noptimal_rounds = results.iloc[results[\"Score\"].idxmin(),:][\"Rounds\"]\nprint(\"Parameters for Final Models:\\n\",final_model_params)\nprint(\"Score: {} +/- {}\".format(results.iloc[results[\"Score\"].idxmin(),:][\"Score\"],results.iloc[results[\"Score\"].idxmin(),:][\"STDV\"]))\nprint(\"Rounds: \", optimal_rounds)","execution_count":14,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"edc3c22d262c30208c0605cd0ab37d49f11dd18b","collapsed":true},"cell_type":"code","source":"allmodelstart= time.time()\n# Run Model with different Seeds\nmulti_seed_pred = dict()\nall_feature_importance_df  = pd.DataFrame()\n\nall_seeds = [27,22,300,401]\nfor seeds_x in all_seeds:\n    modelstart= time.time()\n    print(\"Seed: \", seeds_x,)\n    # Go Go Go\n    final_model_params[\"seed\"] = seeds_x\n    lgb_reg = lgb.train(\n        final_model_params,\n        lgtrain,\n        num_boost_round = optimal_rounds + 1,\n        verbose_eval=200)\n\n    # Feature Importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = vocab\n    fold_importance_df[\"importance\"] = lgb_reg.feature_importance()\n    all_feature_importance_df = pd.concat([all_feature_importance_df, fold_importance_df], axis=0)\n\n    multi_seed_pred[seeds_x] =  list(lgb_reg.predict(test_df))\n    print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n    print(\"###########################################################################################\")\n    del lgb_reg\n\ncols = all_feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\nbest_features = all_feature_importance_df.loc[all_feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(8,10))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\nprint(\"All Model Runtime: %0.2f Minutes\"%((time.time() - allmodelstart)/60))\n\n# To DataFrame\nsub_preds = pd.DataFrame.from_dict(multi_seed_pred).replace(0,0.000001)\ndel multi_seed_pred; gc.collect();\n\n# Correlation Plot\nf, ax = plt.subplots(figsize=[8,6])\nsns.heatmap(sub_preds.corr(),\n            annot=True, fmt=\".2f\",cbar_kws={'label': 'Percentage %'},cmap=\"plasma\",ax=ax)\nax.set_title(\"Correlation Plot for Seed Diversified Models\")\nplt.show()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09ba793908334d80e1e470dbb371991846fbec0f","collapsed":true},"cell_type":"code","source":"# Take Mean over Seed prediction\nmean_sub = np.expm1(sub_preds.mean(axis=1).rename(target_var))\nmean_sub.index = testdex\n\n# Submit\nmean_sub.to_csv('mean_sub_ep{}_sc{}.csv'.format(optimal_rounds,round(best_cv_score,5))\n            ,index = True, header=True)\nprint(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))\nmean_sub.head()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b47a3211671ee44a0e2e128730d4bf9587c375bc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}