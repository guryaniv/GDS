{"cells":[{"metadata":{"_cell_guid":"1db60c9d-5f75-47fe-8fbd-6ba312e53c50","_uuid":"75a7e4e28f44ba6cfa95ecd8a89d2ff6025f3394"},"cell_type":"markdown","source":"This notebook is meant to identify and count the number of stellar sea lions in input images using the training data provided. There are 5 classes of sea lions: adult males, subadult males, adult females, juveniles and pups.\nThe submission is evaluated by calculating the RMSE (Root Mean Square Error) from the human labelled ground truth.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"34e07c20-cc8f-4c99-9a64-d5fd856e5416","_uuid":"6d23ba0f827ea3d7674ee0e0535be9ced15785bb"},"cell_type":"markdown","source":"**Explanation of Code :**                                                                                                                                                                     \n**Step 1:** Load the necessary packages\n* numpy : For linear algebra\n* pandas : For data processing, CSV file I/O (e.g. pd.read_csv)\n* glob : It is useful in any situation where your program needs to look for a list of files on the filesystem, with a pattern of file names (eg: f1.jpg,f2.jpg,f3.jpg....etc) .\n* os : provides a way of using operating system dependent functionality.\n* cv2 : OpenCV\n* matplotlib : a Python 2D plotting library\n\n\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c8ef726c-a2ff-4420-965f-7d362c3df499","_uuid":"e75150ec911f39b65005745fee1852be8bdab3a6","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport glob\nimport os\nimport cv2 \nimport matplotlib.pyplot as plt\nimport skimage.feature\n%matplotlib inline\n#With this (inline) backend, the output of plotting commands is displayed inline within frontends \n#like the Jupyter notebook, directly below the code cell that produced it. \n#The resulting plots will then also be stored in the notebook document.\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n#To view the files in the input directory\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0fddbdac-a668-4d51-a377-90f48d1e9260","_uuid":"08a1317273b1e3869479c41163aa7ce502e5cf13"},"cell_type":"markdown","source":"**Step 2 **: Load the environment variables. This is the data we will be working with. It is displayed for convenience.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f954a974-85cf-445f-9404-83810611b0e7","_uuid":"a433b991f2b6e8a90d51f3d726ec819b72b0a5b7","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/Train/train.csv')\ndisplay(train_data.head(5))\nprint('DETAILS OF TRAIN_DATA')\ntrain_data.info()\ntrain_imgs = sorted(glob.glob('../input/Train/*.jpg'), key=lambda name: int(os.path.basename(name)[:-4]))\ntrain_dot_imgs = sorted(glob.glob('../input/TrainDotted/*.jpg'), key=lambda name: int(os.path.basename(name)[:-4]))\nsubmission = pd.read_csv('../input/sample_submission.csv')\nprint('TRAIN_DATA.SHAPE:')\nprint(train_data.shape)\nprint('Number of Train Images: {:d}'.format(len(train_imgs)))\nprint('Number of Dotted-Train Images: {:d}'.format(len(train_dot_imgs)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3c37ac8-08d4-473e-9ebe-8e05f8d9ef58","_uuid":"c139417b1a9c23ff98e5e1d51102053137805ccf","trusted":false,"collapsed":true},"cell_type":"code","source":"# Count of each type\nhist = train_data.sum(axis=0)\nprint(hist)\nsea_lions_types = hist[1:]\nf, ax1 = plt.subplots(1,1,figsize=(5,5))\nsea_lions_types.plot(kind='bar', title='Count of Sea Lion Types (Train)', ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f3b59e9-031d-46c9-96d1-02aeff08a65c","_uuid":"6fe278cddabff9d2791c86916429a4b13550c2fd"},"cell_type":"markdown","source":"Plotting the histogram for one image. (Index no. 5)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1a7acfe1-1c6f-4fbb-b268-f1ea789ecdb4","_uuid":"bc26e92c29b83c3f91762af8e939dbd8833b57a2","trusted":false,"collapsed":true},"cell_type":"code","source":"index = 1\nsl_counts = train_data.iloc[index]\nprint(sl_counts)\nsl_counts = sl_counts[1:]\nplt.figure()\nsl_counts.plot(kind='bar', title='Count of Sea Lion Types index#5')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_cell_guid":"f4580c79-eb96-48e4-842e-f558e5ac9712","_uuid":"1ff988d008f4d6a672e5326a1fa514e9c5144a08","trusted":false,"collapsed":true},"cell_type":"code","source":"print(train_imgs[index])\nimg = cv2.imread(train_imgs[index])\nimg_dot = cv2.imread(train_dot_imgs[index])\n#img_dot = cv2.cvtColor(cv2.imread(train_dot_imgs[index]), cv2.COLOR_BGR2RGB)\n# absolute difference between Train and Train Dotted\nimg_diff = cv2.absdiff(img_dot,img)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d52c933-c906-4aed-b3f3-2a2dd2c1ad8c","_uuid":"2234a9e45937adfb71bc2a1865aa63d668064626"},"cell_type":"markdown","source":"The plan : Do blob detection, mask, extract the blobs around the blobs, ie, the sea lions by some image segmentation techniques, train using the histogram obtained, and hence identify sea lion types? ","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"c35ee6d9-9ebf-4f53-9ba0-e650dbfa9ab5","_uuid":"b51e29edf52f04531d8d57613759bb7175954054","trusted":false},"cell_type":"code","source":"mask_1 = cv2.cvtColor(img_dot, cv2.COLOR_BGR2GRAY)\nmask_1[mask_1 < 20] = 0\nmask_1[mask_1 > 0] = 255\nmask_2 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nmask_2[mask_2 < 20] = 0\nmask_2[mask_2 > 0] = 255\nimage_4 = cv2.bitwise_or(img_diff, img_diff, mask=mask_1)\nimage_5 = cv2.bitwise_or(image_4, image_4, mask=mask_2)\n# convert to grayscale to be accepted by skimage.feature.blob_log\nimage_6 = cv2.cvtColor(image_5, cv2.COLOR_BGR2GRAY)\n# detect blobs\nblobs = skimage.feature.blob_log(image_6, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n# prepare the image to plot the results on\nimage_7 = cv2.cvtColor(image_6, cv2.COLOR_GRAY2BGR)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b5a2175-4e21-4ba4-9f03-e4de23df0340","_uuid":"3c9f3232e6f001c3b9ae19dd8b4b44e55828b2f8"},"cell_type":"markdown","source":"Plotting : Train#5, Train_dotted#5, Train_dotted - Train","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"ca44147b-2b6c-465d-a84d-2e03e5fc638a","_uuid":"1e4305b8afa91892757e9e6165a583ac3ac5f7a4","trusted":false},"cell_type":"code","source":"classes = [\"adult_males\", \"subadult_males\", \"adult_females\", \"juveniles\", \"pups\", \"error\"]\nindices=[0,1,2,3]\ncount_df = pd.DataFrame(index=indices,columns=classes).fillna(0)\nfor blob in blobs:\n        # get the coordinates for each blob\n        y, x, s = blob\n        # get the color of the pixel from Train Dotted in the center of the blob\n        b,g,r = img_dot[int(y)][int(x)][:]\n        \n        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n        if r > 200 and b < 50 and g < 50: # RED\n            count_df[\"adult_males\"][0] += 1\n            cv2.circle(image_7, (int(x),int(y)), 8, (0,0,255), 2)   \n        elif r > 200 and b > 200 and g < 50: # MAGENTA\n            count_df[\"subadult_males\"][0] += 1\n            cv2.circle(image_7, (int(x),int(y)), 8, (250,10,250), 2)            \n        elif r < 100 and b < 100 and 150 < g < 200: # GREEN\n            count_df[\"pups\"][0] += 1\n            cv2.circle(image_7, (int(x),int(y)), 8, (20,180,35), 2) \n        elif r < 100 and  100 < b and g < 100: # BLUE\n            count_df[\"juveniles\"][0] += 1 \n            cv2.circle(image_7, (int(x),int(y)), 8, (180,60,30), 2)\n        elif r < 150 and b < 50 and g < 100:  # BROWN\n            count_df[\"adult_females\"][0] += 1\n            cv2.circle(image_7, (int(x),int(y)), 8, (0,42,84), 2)            \n        else:\n            count_df[\"error\"][0] += 1\n            cv2.circle(image_7, (int(x),int(y)), 8, (255,255,155), 2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bff5e413-b722-453b-bccc-b3bff6a7a729","_uuid":"b166a0eb84d8e06d39ddd37c53ac3137723ddb58","trusted":false,"collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(3,2,figsize=(10,16))\n(ax1, ax2, ax3, ax4, ax5, ax6) = ax.flatten()\nax1.imshow(cv2.cvtColor(img[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\nax1.set_title('Train')\nax2.imshow(cv2.cvtColor(img_dot[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\nax2.set_title('Train Dotted')\nax3.imshow(cv2.cvtColor(img_diff[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\nax3.set_title('Train_Diff = Train Dotted - Train')\nax4.imshow(cv2.cvtColor(image_5[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\nax4.set_title('Mask blackened areas of Train_Diff')\nax5.imshow(image_6[700:1200,2130:2639], cmap='gray')\nax5.set_title('Grayscale for input to blob_log')\nax6.imshow(cv2.cvtColor(image_7[700:1200,2130:2639,:], cv2.COLOR_BGR2RGB))\nax6.set_title('Result')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"01744006-3713-4ca5-988c-30956221e326","_uuid":"593e10f21320bf012592744760e5aebc0ab8abba","trusted":false,"collapsed":true},"cell_type":"code","source":"count_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e0df3ec4-c1cf-4440-a1ad-b13f6c692e38","_uuid":"0a8c2e614e1550f05fa77402766a4ee45ad9b47b"},"cell_type":"markdown","source":"Correct! If you check above, yes, we have , for image #1, 2 adult males, 20 subadult males and 12 juveniles! \nTherefore, we successfully extracted the coordinates of respective classes of sea lion from one image. Now that we have the locations, the next step is to decide how to make use of these coordinates to train the classifier.\nA more general approach is given below:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"65e0a15f-bcd6-4e65-91e0-3d6b58c5c049","_uuid":"833e757634ba91c7454a4ab19706b61c5146b685","trusted":false},"cell_type":"code","source":"#Initialize a dataframe to store coordinates\nclass_names = ['adult_females', 'adult_males', 'juveniles', 'pups', 'subadult_males']\nfile_names = os.listdir(\"../input/Train/\")\nfile_names = sorted(file_names, key=lambda \n                    item: (int(item.partition('.')[0]) if item[0].isdigit() else float('inf'), item)) \n# select a subset of files to run on\nfile_names = file_names[0:1]\n# dataframe to store results in\ncoordinates_df = pd.DataFrame(index=file_names, columns=class_names)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5aac04d7-0ee8-404c-85a3-302e7cebb7c0","_uuid":"74b1511121723636436ac552d143e43b536b6b98"},"cell_type":"markdown","source":"Extract coordinates of sea lions in the training images.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"77eda7e3-ce96-4483-9bf3-146d68d3b8b2","_uuid":"6a8ae040583533a0b2524fcb5472897b477213cd","trusted":false},"cell_type":"code","source":"for filename in file_names:\n    \n    # read the Train and Train Dotted images\n    image_1 = cv2.imread(\"../input/TrainDotted/\" + filename)\n    image_2 = cv2.imread(\"../input/Train/\" + filename)\n    #initializing a 'cut' image, i.e template\n    cut = np.copy(image_2)\n    \n    # absolute difference between Train and Train Dotted\n    image_3 = cv2.absdiff(image_1,image_2)\n    \n    # mask out blackened regions from Train Dotted\n    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n    mask_1[mask_1 < 20] = 0\n    mask_1[mask_1 > 0] = 255\n    \n    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n    mask_2[mask_2 < 20] = 0\n    mask_2[mask_2 > 0] = 255\n    \n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_2) \n    \n    # convert to grayscale to be accepted by skimage.feature.blob_log\n    image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2GRAY)\n    \n    # detect blobs\n    blobs = skimage.feature.blob_log(image_3, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n    \n    adult_males = []\n    subadult_males = []\n    pups = []\n    juveniles = []\n    adult_females = [] \n    \n    image_circles = image_1\n    \n    for blob in blobs:\n        # get the coordinates for each blob\n        y, x, s = blob\n        # get the color of the pixel from Train Dotted in the center of the blob\n        g,b,r = image_1[int(y)][int(x)][:]\n        \n        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n        if r > 200 and g < 50 and b < 50: # RED\n            adult_males.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,0,255), 10) \n        elif r > 200 and g > 200 and b < 50: # MAGENTA\n            subadult_males.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (250,10,250), 10)\n        elif r < 100 and g < 100 and 150 < b < 200: # GREEN\n            pups.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (20,180,35), 10)\n        elif r < 100 and  100 < g and b < 100: # BLUE\n            juveniles.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (180,60,30), 10)\n        elif r < 150 and g < 50 and b < 100:  # BROWN\n            adult_females.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,42,84), 10)  \n        #Cutting a 32x32 frame around each coordinate    \n        cv2.rectangle(cut, (int(x)-112,int(y)-112),(int(x)+112,int(y)+112), 0,-1)\n            \n    coordinates_df[\"adult_males\"][filename] = adult_males\n    coordinates_df[\"subadult_males\"][filename] = subadult_males\n    coordinates_df[\"adult_females\"][filename] = adult_females\n    coordinates_df[\"juveniles\"][filename] = juveniles\n    coordinates_df[\"pups\"][filename] = pups","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95e52be1-9107-413b-bbff-e12c0e45b57e","_uuid":"7a150a69a6abc09cd176f2a1d0e34491f6bcd64e","trusted":false,"collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1,1,figsize=(10,16))\nax.imshow(cv2.cvtColor(image_circles, cv2.COLOR_BGR2RGB))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bdc151c5-3d75-459e-a3cc-44c24564e576","_uuid":"4c603d03c61fee2261701bccad44a4259588ee26"},"cell_type":"markdown","source":"Extract templates of sea lions centered around the coordinates we found.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b522f054-75fe-4d5b-a4be-9d9b6426b103","_uuid":"1d886ceabda8723e9115da3a88f36135e2db8bd9","trusted":false,"collapsed":true},"cell_type":"code","source":"x = []\ny = []\n\nfor filename in file_names:    \n    image = cv2.imread(\"../input/Train/\" + filename) #in each training image,\n    for lion_class in class_names: # for each class,\n        for coordinates in coordinates_df[lion_class][filename]:#create a thumbfile for each coordinate\n            thumb = image[coordinates[1]-32:coordinates[1]+32,coordinates[0]-32:coordinates[0]+32,:]\n            if np.shape(thumb) == (64, 64, 3):\n                x.append(thumb)\n                y.append(lion_class)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49a8f893-1730-439d-a928-06193022726d","_uuid":"4ee97feddd5ef865e45ba89f321414fdada8ac0b"},"cell_type":"markdown","source":"Add negative examples ( though this did not give good results..)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b438bced-db7e-4295-9219-5392360b2a43","_uuid":"fc05073fcc337bbca3fff197ee39566b6cae8d9d","trusted":false,"collapsed":true},"cell_type":"code","source":"for i in range(0,np.shape(cut)[0],224):\n    for j in range(0,np.shape(cut)[1],224):                \n        thumb = cut[i:i+64,j:j+64,:]\n        if np.amin(cv2.cvtColor(thumb, cv2.COLOR_BGR2GRAY)) != 0:\n            if np.shape(thumb) == (64,64,3):\n                x.append(thumb)\n                y.append(\"negative\")     ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"16a40dbc-a8be-4751-8e41-f37fcf023907","_uuid":"9746440937252bdf224216cfbb85cd45a8ce8218","trusted":false},"cell_type":"code","source":"class_names.append(\"negative\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6c8a841d-66fe-4404-86d3-788cd1476dbf","_uuid":"70ffe97af2999235cfb6f0ddb2e7a1501fe80665","trusted":false},"cell_type":"code","source":"x = np.array(x)\ny = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2c7b3fb-2a66-48e4-bc44-b7e8aa6dd480","_uuid":"4d018247053c5c23caebef6bac5692836a8bae52","trusted":false,"collapsed":true},"cell_type":"code","source":"for lion_class in class_names:\n    f, ax = plt.subplots(1,10,figsize=(12,1.5))\n    f.suptitle(lion_class)\n    axes = ax.flatten()\n    j = 0\n    for a in axes:\n        a.set_xticks([])\n        a.set_yticks([])\n        for i in range(j,len(x)):\n            if y[i] == lion_class:\n                j = i+1\n                a.imshow(cv2.cvtColor(x[i], cv2.COLOR_BGR2RGB))\n                break","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c551904b-5871-4686-8ce4-235b5d96c034","_uuid":"69972ccc978c8358a365cd22fd227424829198af"},"cell_type":"markdown","source":"**Training a simple Keras model.**\nWhat is Keras?\nKeras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow.\nIt was developed to make implementing deep learning models as fast and easy as possible for research and development.\nTensorFlow is an open source software library for numerical computation using data-flow graphs. It was originally developed by the Google Brain Team within Google's Machine Intelligence research organization for machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b48f8e08-6a5e-41b2-b782-4f7ab1b3277e","_uuid":"4bd82faac1b135fe9a16d57b4b537db4429d9faa","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, Cropping2D\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41f025fd-8056-46f6-b8c8-64f1267eb0e9","_uuid":"dd9b1ca03e7e225c345841b1feea546fc3eb65e6"},"cell_type":"markdown","source":"**One hot encoding**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"3e7276a4-58f2-41a1-a4f9-6576ded69047","_uuid":"1881a164923ee22f514d18243cf475f8d4e721bc","trusted":false},"cell_type":"code","source":"encoder = LabelBinarizer()\nencoder.fit(y)\ny = encoder.transform(y).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab08a1f9-0004-4740-892a-62167cd39bd6","_uuid":"ea422145d25028603b9df6b53fb142be9ca15085"},"cell_type":"markdown","source":"**Keras model :**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"4cc2a56b-1fcd-4f2c-9e90-d60a2be658f7","_uuid":"9b9587c5f3cafdb99b5fb90b3ddcc45859eec598","trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(64,64,3)))\nmodel.add(Conv2D(32, (5, 5), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128, (5, 5), activation='relu', padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(6, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3314a032-5596-4f8b-b515-dcef8ecbad9a","_uuid":"a5a8726fd63d5e37fce3145d7b0340fefe933158","trusted":false,"collapsed":true},"cell_type":"code","source":"history = model.fit(x, y, epochs=10, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3617d60a-b1f0-4453-9563-d6a5829a7aa2","_uuid":"1243fa30420daffdcfa971375e637392cb3da5e2","trusted":false,"collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e9b7e38-3167-48da-b3b7-25e57ece6bcb","_uuid":"73ec4ffafc8f4a26cb544201eab7e6661555419e"},"cell_type":"markdown","source":"Testing :","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_cell_guid":"a1531fa3-55b1-427d-87d2-d00019d46d30","_uuid":"6bb8e5c043a5a854751c1d90ac89b2aac0fb2fd4","trusted":false},"cell_type":"code","source":"img = cv2.imread(\"../input/Train/\" + filename)\n\nx_test = []\n\nfor i in range(0,np.shape(img)[0],64):\n    for j in range(0,np.shape(img)[1],64):                \n        thumb = img[i:i+64,j:j+64,:]        \n        if np.shape(thumb) == (64,64,3):\n            x_test.append(thumb)\n\nx_test = np.array(x_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e278413c-325b-454a-b768-32165295e62d","_uuid":"4d893dd164472b4f7f3bed8f9903f2f7d8c72f14","trusted":false},"cell_type":"code","source":"y_predicted = model.predict(x_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"359c1758-f54d-4100-a9f6-b8692fcc981b","_uuid":"6b3ec6334c664dd984e5d5172c6c12382484bd44","trusted":false,"collapsed":true},"cell_type":"code","source":"y_predicted = encoder.inverse_transform(y_predicted)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3456c7fd-8864-4569-889c-4fdf8824026b","_uuid":"27999e77654a65906e4e4194aa5ff362d01e5998","trusted":false,"collapsed":true},"cell_type":"code","source":"reference = pd.read_csv('../input/Train/train.csv')\nreference.ix[0:0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60f51590-3c07-42ee-b022-51a2d382f808","_uuid":"83cf5d50952b9cd3a6bc2e91a9e98d0989ef73b9","trusted":false,"collapsed":true},"cell_type":"code","source":"print(Counter(y_predicted).items())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5efe399284fd62a2ed87eaf2ad9c7b19c2a58999"},"cell_type":"markdown","source":"Heading: Summary :\n* The aim of this kernel was to identify different classes of sea lions in a given image.\n* Training data was given in the form of images with a coloured dot on each sea lion to indicate it's position and what class it belongs to.\n* The first step was to extract thumbnail images around each coloured dot. These thumbnails can later be used for training.\n* Step 1 : Import required modules/libraries... numpy, cv2, pandas, glob, os, matplotlib, skikit image.\n* Step 2: Load the environment variables\n* Step 3 : Perform blob detection. For each training image, find absolute difference between train[index] and train_dot[index]. Some areas in the train_dot set were blacked out. So mask that as well. Thus we get blobs (the dots) and their coordinates. For each blob coordinate, extract a 32x32 thumbnail image around it. Store in a numpy array along with the class each thumbnail belongs to.\n* Step 4: The thumbnails train a model using the Keras library. It uses convolutional neural networks which are ideal for  image localization and classification tasks.  \nNote : Blob detection was performed only on a single image. The model was tested on a single image.","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}