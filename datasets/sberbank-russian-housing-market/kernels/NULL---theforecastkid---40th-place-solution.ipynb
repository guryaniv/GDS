{"metadata": {"language_info": {"name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "cells": [{"metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "e618a4f9493fa933f1f80630a6ec4df445c6edaf"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": " Our solution was a stack of Neural-Nets, RandomForest and Linear Models such us:\n RidgeRegression, HuberRegression, RansacRegression. Finally, we used XGB as meta-model.\nWe did extensive feature engineering similar to some solutions already presented. The most \"original\" code that we created was a sample_weight function that penalized samples with more missing values. This function penalizes more the samples that miss the features directly related with the house, such us: full_sq, life_sq, floor, etc.\nThis sample_weight was used in the RandomForest model.\n   \n"}, {"metadata": {"_cell_guid": "88df7bd9-96cb-45e3-8146-606ebd0097e7", "_execution_state": "idle", "_uuid": "a2440d7b1e1e2820c0d256df5ce4bca4b83a50eb", "trusted": false}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "import numpy as np \nimport pandas as pd\n\ntrain = pd.read_csv('../input/train.csv')\n\nnulls = train.isnull().sum()\n\nimportance = train.shape[0]*[None]\nfor i in range(train.shape[0]):\n    nulls = train.loc(i)[i].isnull().sum()\n    imp_nulls = train.loc(i)[i].isnull()[: 12].sum()\n    importance[i] = np.exp((1/(nulls+1)*(1/(imp_nulls+1))))/np.exp(1)\n\n#imp_nulls are the nulls directly related to the house\n#We decide to use the sqrt to reduce the amplitude. The model performed better in LB and local CV\nimportance = np.sqrt(importance)\n"}, {"metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "d0ee613e7ce99020cd7796fff60698ffe23478cc"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Before we fit the Linear Models with the train data, we removed some outliers based on price per square meter. \n"}, {"metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "066e2cb515257128391cdbc0fcbe77b87d13b201"}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "train['price_per_sq'] = np.array(train.price_doc)/np.array(train.full_sq)\ntrain = train[(train.price_per_sq > 60000) & (train.price_per_sq < 500000)]\n#The values 60000 and 500000 were defined based on price_per_sq histogram."}, {"metadata": {"_execution_state": "idle", "collapsed": false, "_uuid": "1c81c76b5c983a910a71953fa8d3dd2aef7adf9b"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "I think this two small ideas were the most interesting part of our solution.\nHope you find it useful."}], "nbformat_minor": 0, "nbformat": 4}