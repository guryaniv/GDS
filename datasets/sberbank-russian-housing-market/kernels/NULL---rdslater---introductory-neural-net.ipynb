{"nbformat": 4, "cells": [{"cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "16920878a5353f63eca307ee5b59f49b1ace9896", "_cell_guid": "e832c671-0681-870b-81df-833282070a91"}, "outputs": [], "source": ["Neural Net Sample.  I have not yet tuned this succesfully (I've tried!), and can only approach 0.32 error on the leaderboard( which is not the same as the error listed below--there is not a strong relation between training/cv error ans submission error).  I chose 5 epochs for demonstration--I have a GPU so can (and have done) do 100 epoch 5 split CVs w/o issue."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "4f0f9df8db83574d6035cf2dfcc6eb0d5c76a51b", "_cell_guid": "2894ff91-ea43-5d35-1ba7-f81eb39b311c"}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "train=pd.read_csv(\"../input/train.csv\")\n", "test=pd.read_csv(\"../input/test.csv\")\n", "n=train.shape[0]\n", "train['data_set']=1\n", "test['data_set']=0\n", "test.price_doc=np.nan\n", "ids=test['id']\n", "train.price_doc=np.log(train.price_doc)\n", "target=train.price_doc\n", "train=train.append(test)\n", "train.drop(['id'],axis=1,inplace=True)\n", "binary=[]\n", "for i in train:\n", "    if train[i].dtypes=='object':\n", "        #print(train[i].value_counts())\n", "        if train[i].value_counts().shape[0]==2:\n", "            binary.append(i)\n", "for i in binary:\n", "    train[i]=pd.factorize(train[i])[0]\n", "train.loc[train['ecology']=='no data','ecology_dat']=0\n", "train.loc[train['ecology']!='no data','ecology_dat']=1\n", "train.loc[train['ecology']=='no data','ecology']=np.nan\n", "train.loc[train['ecology']=='poor','ecology']=1\n", "train.loc[train['ecology']=='satisfactory','ecology']=2\n", "train.loc[train['ecology']=='good','ecology']=3\n", "train.loc[train['ecology']=='excellent','ecology']=4\n", "train.ecology=pd.to_numeric(train.ecology)\n", "train=pd.concat([train,pd.get_dummies(train.sub_area)],axis=1)\n", "\n", "a=train.describe()\n", "for i in a:\n", "    train[i]=train[i].fillna((a.loc['min',i]-a.loc['max',i]*2))\n", "    \n", "train.drop(['timestamp','sub_area'],inplace=True,axis=1)\n", "\n", "from sklearn.preprocessing import MinMaxScaler\n", "scaler = MinMaxScaler()\n", "cols=train.columns.tolist()\n", "\n", "train = pd.DataFrame(scaler.fit_transform(train), columns=cols)\n", "\"\"\"\n", "for i in train:    \n", "    train[i]=train[i]/(train[i].max()-train[i].min())\n", "\"\"\"\n", "test=train[train['data_set']==0]\n", "train=train[train['data_set']==1]\n", "print(test.shape,train.shape,n)\n", "test.drop(['data_set','price_doc',],inplace=True,axis=1)\n", "train.drop(['data_set','price_doc'],inplace=True,axis=1)"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "7e3c308ad601aad4d636d31b03e05036b4b14ba6", "_cell_guid": "ed286c35-b0a7-9633-001c-a7173ac81964"}, "outputs": [], "source": ["Lets get our Neural Net setup.  I used a refernce to pick my hidden layers\n", "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n", "\n", "Nh=Ns/(\u03b1\u2217(Ni+No))\n", "\n", "Ni = number of input neurons.\n", "\n", "No = number of output neurons.\n", "\n", "Ns = number of samples in training data set.\n", "\n", "\u03b1 = an arbitrary scaling factor usually 2-10."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d336cc566ae5d2cf6b44eed52faa3f3257082bd1", "_cell_guid": "ea413fd7-a22b-c35f-a4c8-2b8c81e9e43b"}, "outputs": [], "source": ["from sklearn.cross_validation import KFold\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Activation\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.layers.advanced_activations import PReLU\n", "from keras.optimizers import SGD\n", "from keras import regularizers\n", "from keras.callbacks import EarlyStopping\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "cd59ff776a648382a6b9704471d0d2094d9042cc", "_cell_guid": "dcbaeac2-4c4b-7077-8be5-b04a79b08623"}, "outputs": [], "source": ["You can see if you turn on validation (and on the CV) that the validation loss tends to be less than training loss--this is quite a worry.  I have tuned training rates and dropouts to improve this without much success.  Notice here I am only training on half the data, you may remove the validation split, I wanted to split the data to show the difference in training and validation loss--how the validation loss is typically below training loss (not good!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "48729400316422728ad9299d71b01a217643697f", "_cell_guid": "bbf0355d-d61e-c337-8d68-0f3bae82a0c4"}, "outputs": [], "source": ["    model = Sequential()\n", "    \n", "    model.add(Dense(40, input_dim = train.shape[1], init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())\n", "    model.add(Dense(40, init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())    \n", "    model.add(Dense(20, init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())    \n", "    model.add(Dense(1, init = 'he_normal'))\n", "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n", "    outputs=model.fit(train.as_matrix(),target.as_matrix() , batch_size=32, nb_epoch=5, verbose=1,validation_split=0.5)\n", "    preds=model.predict( test.as_matrix(), batch_size=32, verbose=0)\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e8800a6833d87c0f46024764b07bfdd8b8e4891a", "_cell_guid": "83531e70-4474-f8e7-a8f6-5c59e6459059"}, "outputs": [], "source": ["y=np.reshape(preds,preds.shape[0])\n", "y=np.exp(y)\n", "subs=pd.DataFrame({'id':ids.as_matrix(),'price_doc':y})\n", "subs.to_csv(\"test_smallnn.csv\",index=False)"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "df2f87eb38f78061f80d88df2050ecfe9492a514", "_cell_guid": "6671a3ee-cd81-010c-096d-9027253186b9"}, "outputs": [], "source": ["For CV testing, I have used below.  Includes Early Stopping.  I had dropout when using larger nets, and feel free to tune as needed.  While it is not quite 'Black Box' I have not found a consistent manner to estimate my submission error for this challenge."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7c63a6e75c10c0a6b3030fde31312b4bfa0d5337", "_cell_guid": "4827087c-df71-4471-67cd-ee621d004048"}, "outputs": [], "source": ["\"\"\"\n", "from sklearn.cross_validation import KFold\n", "predict_x=np.zeros(train.shape[0])\n", "kf=KFold(train.shape[0],n_folds=5)\n", "outputs_all=[]\n", "\n", "for train_index, target_index in kf:\n", "    model = Sequential()\n", "    \n", "    model.add(Dense(40, input_dim = train.loc[train_index,].shape[1], init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())\n", "    #model.add(Dropout(0.4))\n", "\n", "    model.add(Dense(40, init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())    \n", "    #model.add(Dropout(0.2))\n", "    model.add(Dense(20, init = 'he_normal'))\n", "    model.add(Activation('relu'))\n", "    model.add(BatchNormalization())    \n", "    #model.add(Dropout(0.2))\n", "\n", "    model.add(Dense(1, init = 'he_normal'))\n", "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n", "    callbacks = [\n", "    EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n", "    \n", "    ]\n", "    outputs=model.fit(train.loc[train_index,].as_matrix(),target[train_index].as_matrix() , batch_size=32, \n", "                      nb_epoch=100, verbose=1,\n", "                      validation_data=[train.loc[target_index,].as_matrix(),target[target_index].as_matrix()],\n", "                      callbacks=callbacks\n", "                     )\n", "    outputs_all.append(outputs)\n", "    preds=model.predict( train.loc[target_index,].as_matrix(), batch_size=32, verbose=0)\n", "    predict_x[target_index]=preds\n", "    \"\"\""]}], "metadata": {"_is_fork": false, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "version": "3.6.0"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "_change_revision": 0}, "nbformat_minor": 0}