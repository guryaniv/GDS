{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7a9c45fc-a4d5-7d81-e665-52e0b570447d"
      },
      "source": [
        "# Import Libraries #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f310b2e5-aad0-deca-d2b2-9017534e17e2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "from scipy.stats import skew\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a7852b95-f9d3-524c-7380-227f577d65ec"
      },
      "source": [
        "#Import Data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0644a46d-7f03-5b79-5133-2f1b08dcb50d"
      },
      "outputs": [],
      "source": [
        "train = '../input/train.csv'\n",
        "test = '../input/test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train)\n",
        "df_test = pd.read_csv(test)\n",
        "\n",
        "print(\"a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dd676753-7afa-7425-c334-efa01f9c0d9f"
      },
      "source": [
        "#Define Median Absolute Deviation Function#\n",
        "\n",
        "Function found in this link: http://stackoverflow.com/a/22357811/5082694"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36b14aa2-cb6b-ee22-822e-b25987a92622"
      },
      "outputs": [],
      "source": [
        "def is_outlier(points, thresh = 3.5):\n",
        "    if len(points.shape) == 1:\n",
        "        points = points[:,None]\n",
        "    median = np.median(points, axis=0)\n",
        "    diff = np.sum((points - median)**2, axis=-1)\n",
        "    diff = np.sqrt(diff)\n",
        "    med_abs_deviation = np.median(diff)\n",
        "\n",
        "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
        "\n",
        "    return modified_z_score > thresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93d67941-46ed-4c88-d962-5104c00424f0"
      },
      "source": [
        "# Remove Skew from SalesPrice data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe73d25a-8c44-f7c1-3690-335f9360dd2b"
      },
      "outputs": [],
      "source": [
        "target = df_train[df_train.columns.values[-1]]\n",
        "target_log = np.log(target)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.distplot(target, bins=50)\n",
        "plt.title('Original Data')\n",
        "plt.xlabel('Sale Price')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(target_log, bins=50)\n",
        "plt.title('Natural Log of Data')\n",
        "plt.xlabel('Natural Log of Sale Price')\n",
        "plt.tight_layout()\n",
        "\n",
        "print(\"b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "611075d7-c509-bd29-5d69-d0ddc08eebf1"
      },
      "source": [
        "# Merge Train and Test to evaluate ranges and missing values #\n",
        "This was done primarily to ensure that Categorical data in the training and testing data sets were consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5356a37-704d-c07b-fc15-cd30b029fc2d"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[df_train.columns.values[:-1]]\n",
        "df = df_train.append(df_test, ignore_index = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "51369f66-1ba0-79de-169b-fdf1dfff957a"
      },
      "source": [
        "#Find all categorical data#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db834360-7793-f4cd-4da7-89d2ea29f510"
      },
      "outputs": [],
      "source": [
        "cats = []\n",
        "for col in df.columns.values:\n",
        "    if df[col].dtype == 'object':\n",
        "        cats.append(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d257aa8f-66db-821e-c86e-0b43d097e28a"
      },
      "source": [
        "# Create separte datasets for Continuous vs Categorical #\n",
        "Creating two data sets allowed me to handle the data in more appropriate ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "94bb44ab-c0be-76e2-8da1-16ddd171b29b"
      },
      "outputs": [],
      "source": [
        "df_cont = df.drop(cats, axis=1)\n",
        "df_cat = df[cats]\n",
        "\n",
        "print(\"c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8e1ec7df-d89d-4d81-6a6a-e8a79231d2b4"
      },
      "source": [
        "# Handle Missing Data for continuous data #\n",
        "\n",
        " - If any column contains more than 50 entries of missing data, drop the column\n",
        " - If any column contains fewer that 50 entries of missing data, replace those missing values with the median for that column\n",
        " - Remove outliers using Median Absolute Deviation\n",
        " - Calculate skewness for each variable and if greater than 0.75 transform it\n",
        " - Apply the sklearn.Normalizer to each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e15b692-770c-8bb1-47ec-159de3fdfb56"
      },
      "outputs": [],
      "source": [
        "for col in df_cont.columns.values:\n",
        "    if np.sum(df_cont[col].isnull()) > 50:\n",
        "        df_cont = df_cont.drop(col, axis = 1)\n",
        "    elif np.sum(df_cont[col].isnull()) > 0:\n",
        "        median = df_cont[col].median()\n",
        "        idx = np.where(df_cont[col].isnull())[0]\n",
        "        df_cont[col].iloc[idx] = median\n",
        "\n",
        "        outliers = np.where(is_outlier(df_cont[col]))\n",
        "        df_cont[col].iloc[outliers] = median\n",
        "        \n",
        "        if skew(df_cont[col]) > 0.75:\n",
        "            df_cont[col] = np.log(df_cont[col])\n",
        "            df_cont[col] = df_cont[col].apply(lambda x: 0 if x == -np.inf else x)\n",
        "        \n",
        "        df_cont[col] = Normalizer().fit_transform(df_cont[col].reshape(1,-1))[0]\n",
        "        \n",
        "print(\"d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9a0edf82-078d-f72f-1809-ffe74fc0758c"
      },
      "source": [
        "# Handle Missing Data for Categorical Data #\n",
        "\n",
        " - If any column contains more than 50 entries of missing data, drop the column\n",
        " - If any column contains fewer that 50 entries of missing data, replace those values with the 'MIA'\n",
        " - Apply the sklearn.LabelEncoder\n",
        " - For each categorical variable determine the number of unique values and for each, create a new column that is binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11ea26ef-680a-8e58-6c3e-f2af9e7889a3"
      },
      "outputs": [],
      "source": [
        "for col in df_cat.columns.values:\n",
        "    if np.sum(df_cat[col].isnull()) > 50:\n",
        "        df_cat = df_cat.drop(col, axis = 1)\n",
        "        continue\n",
        "    elif np.sum(df_cat[col].isnull()) > 0:\n",
        "        df_cat[col] = df_cat[col].fillna('MIA')\n",
        "        \n",
        "    df_cat[col] = LabelEncoder().fit_transform(df_cat[col])\n",
        "    \n",
        "    num_cols = df_cat[col].max()\n",
        "    for i in range(num_cols):\n",
        "        col_name = col + '_' + str(i)\n",
        "        df_cat[col_name] = df_cat[col].apply(lambda x: 1 if x == i else 0)\n",
        "        \n",
        "    df_cat = df_cat.drop(col, axis = 1)\n",
        "    \n",
        "print(\"e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "927398e4-5f02-e0d8-7cc2-c2b10eb9c878"
      },
      "source": [
        "# Merge Numeric and Categorical Datasets and Create Training and Testing Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36b6edfe-8a54-9ab8-2820-e34e455ffddf"
      },
      "outputs": [],
      "source": [
        "df_new = df_cont.join(df_cat)\n",
        "\n",
        "df_train = df_new.iloc[:len(df_train) - 1]\n",
        "df_train = df_train.join(target_log)\n",
        "\n",
        "df_test = df_new.iloc[len(df_train) + 1:]\n",
        "\n",
        "X_train = df_train[df_train.columns.values[1:-1]]\n",
        "y_train = df_train[df_train.columns.values[-1]]\n",
        "\n",
        "X_test = df_test[df_test.columns.values[1:]]\n",
        "\n",
        "print(\"f\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d81a1436-1857-dae4-fd54-ce3b3b2801d6"
      },
      "source": [
        "# Create Estimator and Apply Cross Validation #\n",
        "\n",
        "We can gauge the accuracy of our model by implementing an multi-fold cross validation and outputting the score.  In this case I chose to run 15 iterations and output the score as Root Mean Squared Error.\n",
        "\n",
        "The results range from ~0.11-0.17 with a mean of ~0.14."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96aebe1a-1ace-a7ad-b3f7-3d0871613da0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "scorer = make_scorer(mean_squared_error, False)\n",
        "\n",
        "clf = RandomForestRegressor(n_estimators=530, n_jobs=-1)\n",
        "cv_score = np.sqrt(-cross_val_score(estimator=clf, X=X_train, y=y_train, cv=3, scoring = scorer))\n",
        "\n",
        "print(cv_score)\n",
        "print(\" \")\n",
        "print(cv_score.mean())\n",
        "\n",
        "#plt.figure(figsize=(10,5))\n",
        "#plt.bar(range(len(cv_score)), cv_score)\n",
        "#plt.title('Cross Validation Score')\n",
        "#plt.ylabel('RMSE')\n",
        "#plt.xlabel('Iteration')\n",
        "\n",
        "#plt.plot(range(len(cv_score) + 1), [cv_score.mean()] * (len(cv_score) + 1))\n",
        "#plt.tight_layout()\n",
        "\n",
        "print(\"g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0296b6f-0e95-abc6-1084-e7039d186a46"
      },
      "source": [
        "# Evaluate Feature Significance #\n",
        "\n",
        "Investigating feature importance is a relatively straight forward process:\n",
        "\n",
        " 1. Out feature importance coefficients\n",
        " 2. Map coefficients to their feature name\n",
        " 3. Sort features in descending order\n",
        "\n",
        "Given our choice of model and methods for preprocessing data the most significant features are:\n",
        "\n",
        " 1. OverallQual\n",
        " 2. GrLivArea\n",
        " 3. TotalBsmtSF\n",
        " 4. GarageArea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87300d58-d4b0-0446-4c3c-29e8e4ff8d4e"
      },
      "outputs": [],
      "source": [
        "# Fit model with training data\n",
        "#clf.fit(X_train, y_train)\n",
        "\n",
        "# Output feature importance coefficients, map them to their feature name, and sort values\n",
        "#coef = pd.Series(clf.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "#plt.figure(figsize=(10, 5))\n",
        "#coef.head(25).plot(kind='bar')\n",
        "#plt.title('Feature Significance')\n",
        "#plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0c83c64c-95ed-9ca2-a795-c7748d653fb7"
      },
      "source": [
        "# Visualize Predicted vs. Actual Sales Price #\n",
        "\n",
        "In order to visualize our predicted values vs our actual values we need to split our data into training and testing data sets.  This can easily be accomplished using sklearn's **train_test_split** module.\n",
        "\n",
        "We will train the model using a random sampling of our data set and then compare visually against the actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "989d36e8-cb4b-d5b6-6011-a06a93049151"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train)\n",
        "clf = RandomForestRegressor(n_estimators=530, n_jobs=-1)\n",
        "\n",
        "clf.fit(X_train1, y_train1)\n",
        "y_pred = clf.predict(X_test1)\n",
        "\n",
        "test_preds = clf.predict(X_test)\n",
        "\n",
        "#print(cv_score.mean())\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(y_test1, y_pred, s=20)\n",
        "plt.title('Predicted vs. Actual')\n",
        "plt.xlabel('Actual Sale Price')\n",
        "plt.ylabel('Predicted Sale Price')\n",
        "\n",
        "plt.plot([min(y_test1), max(y_test1)], [min(y_test1), max(y_test1)])\n",
        "plt.tight_layout()\n",
        "\n",
        "print(\"h\")\n",
        "\n",
        "a = np.array(len(y_pred))\n",
        "w = np.array(len(y_pred))\n",
        "#for i in range(len(y_pred)):\n",
        "   # a[i] = y_test1[i]\n",
        " #   w[i] = abs(y_test1[i] - y_pred[i])\n",
        "w = abs(y_test1 - y_pred)\n",
        "\n",
        "plt.hist(y_test1, bins = 50, weights = w)  # plt.hist passes it's arguments to np.histogram\n",
        "plt.title(\"error distribution\")\n",
        "plt.show()\n",
        "\n",
        "submission = pd.DataFrame()\n",
        "submission['Id'] = df_test['Id']\n",
        "submission[\"SalePrice\"] = test_preds\n",
        "submission.to_csv(\"lasso_by_Sarthak.csv\", index=False, header = True)\n",
        "\n",
        "print(\"j\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b0d67ae-9cc3-6b8f-9745-367700b8ef84"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}