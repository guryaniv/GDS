{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gc\nimport os\nimport re\nos.environ['OMP_NUM_THREADS'] = '2'\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', -1)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GroupShuffleSplit\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing import text, sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78578eab64a477d0a5ad6b1c917ae154868a44df"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\",usecols=['question_text','target'])\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df, val_df = train_test_split(train_df, test_size=0.001, random_state = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eaf3fc1e7d31b41a4a94523cf8b2fb9c526042b"},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cac5122ea89be2d501e0e2cf5f1b62e99e3e86d3"},"cell_type":"code","source":"val_df['target'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a16003f33ff26675d1a9c5f4feafc5357c7b087"},"cell_type":"code","source":"%%time\nregex = re.compile('[^a-zA-Z0-9\\$]')\ndef clear_digit(m):\n    return m.group(1) + m.group(3)\ndef clear_dot_name(m):\n    return m.group(1) + m.group(3) + \" \"\ndef sep_digit_leter(m):\n    return m.group(1) + \" \" + m.group(2)\n\n\ndef preprocessing(X):\n#First parameter is the replacement, second parameter is your input string\n    #\n    X = [x.replace('’', '\\'') for x in X]\n    X = [x.replace('$', ' $ ') for x in X]\n    \n    X = [x.replace('\\'s ',' sssssssss ') for x in X]\n    \n    X = [x.replace('i\\'m ','i am ') for x in X]\n    X = [x.replace('I\\'m ','I am ') for x in X]\n    \n    \n    X = [x.replace('\\'re ',' are ') for x in X]\n    X = [x.replace('\\'ve ',' have ') for x in X]  \n    X = [x.replace('won\\'t ','will not ') for x in X]\n    X = [x.replace('n\\'t ',' not ') for x in X]\n    X = [x.replace('\\'ll ',' will ') for x in X]\n    X = [x.replace('\\'d ',' ddddddddd ') for x in X]\n    #X = [x.replace('U.S.', ' USA ') for x in X]\n    \n    #X = [x.replace('B.S.', ' BS ') for x in X]\n    #X = [x.replace('M.S.', ' MS ') for x in X]\n    X = [x.replace('e.g.', ' ') for x in X]\n    #X = [x.lower() for x in X]\n    \n    \n    X = [re.sub('\\[math\\].*?math\\]', ' equation ', x) for x in X]\n    #X = [re.sub('\\(.*?\\)', ' ', x) for x in X]\n    X = [x.replace('B.Tech',  ' BS ') for x in X]\n    X = [x.replace('M.Tech',  ' MS ') for x in X]\n    X = [x.replace('Mr. ',  ' Mr ') for x in X]\n    X = [x.replace('Mrs. ',  ' Mrs ') for x in X]\n    X = [x.replace('Ms. ',  ' Ms ') for x in X]\n    X = [re.sub(\"(http|Http|www\\.).*?( |$)\", ' link ', x) for x in X]\n    X = [re.sub(\"([0-9])(,)([0-9])\", clear_digit, x) for x in X]\n    X = [re.sub(\"([0-9])([a-z])\", sep_digit_leter, x) for x in X]\n    X = [re.sub(\"([a-z])([0-9])\", sep_digit_leter, x) for x in X]\n    X = [re.sub(\"([A-Z])(\\.)([A-Z]{0,1})([a-z]{0,1})(\\.{0,1})\", clear_digit, x) for x in X]\n    \n    \n    X = [re.sub('\\.+',' aaaaaaaaa ',x) for x in X]\n    X = [re.sub(',+',' bbbbbbbbb ',x) for x in X]\n    X = [x.replace('?',  ' ccccccccc ') for x in X]\n    X = [x.replace('!',  ' vvvvvvvvv ') for x in X]\n    \n    \n    X = [regex.sub(' ', x) for x in X]\n    \n    X = [x.replace(' US ', ' USA ') for x in X]\n    \n    X = [x.lower() for x in X]\n    for i in '0123456789':\n        X = [x.replace(i, '#') for x in X]\n    \n    X = [x.replace(' aaaaaaaaa ', ' . ') for x in X]\n    X = [x.replace(' bbbbbbbbb ',' , ') for x in X]\n    X = [x.replace(' ccccccccc ',' ? ') for x in X]\n    X = [x.replace(' vvvvvvvvv ',' ! ') for x in X]\n    X = [x.replace(' sssssssss ',' \\'s ') for x in X]\n    X = [x.replace(' ddddddddd ',' \\'d ') for x in X]\n    \n    X = [x.replace('quorans', 'quora') for x in X]\n    X = [x.replace('quoran', 'quora') for x in X]\n    X = [x.replace('qoura', 'quora') for x in X]\n    X = [x.replace('cryptocurrencies', 'bitcoin') for x in X]\n    X = [x.replace('redmi', 'phone') for x in X]\n    X = [x.replace('oneplus', 'phone') for x in X]\n    X = [x.replace('lenovo','laptop') for x in X]\n    return X\n\n#train_df['question_text'].iloc[37859:37860] ['question_text'] = \"What are Loy Machedo's thoughts on evil spirit?\"\n\ntrain_df['transform'] = preprocessing(train_df['question_text'])\nval_df['transform'] = preprocessing(val_df['question_text'])\ntest_df['transform'] = preprocessing(test_df['question_text'])\n\n#Out: 'abdE'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e23864b5af79a943d5d081093a0da6d23e9e1f15"},"cell_type":"code","source":"#train_df['transform'] = train_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split() ]).lower() )\n#test_df['transform'] = test_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split()]).lower() )\n#val_df['transform'] = val_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split() ]).lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b326f67d6f1ab028c37699cbb2acccc5b88dd53"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e48a32417b8042266b07d8f7f7ca29f185648a"},"cell_type":"code","source":"X_train = train_df['transform'].values\nX_val = val_df['transform'].values\ny_train = train_df['target'].values\ny_val = val_df['target'].values\nX_test = test_df['transform'].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dbc9be6f1ed3e522118b9cb0c3d0820484d8b0e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deb61e6af84236669980e7b31b4258166f33129f"},"cell_type":"code","source":"%%time\nmaxlen = 50\n\ntokenizer = text.Tokenizer(filters='\\t\\n')\ntokenizer.fit_on_texts(list(X_train))\n\n# tokenizer = text.Tokenizer(filters='\\t\\n',oov_token=set(list(oov[0].values)))\n# tokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post',truncating='post')\nx_val = sequence.pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07b77bbf3fc8b5b18462a05d476e369f3a9fb432"},"cell_type":"code","source":"word_index = tokenizer.word_index\nembedding_matrix1 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\nembedding_matrix2 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\n#embedding_matrix3 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\nlen(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92ffbf2ef35d2dc5863ee27c61eadd1869ca5440"},"cell_type":"code","source":"# embdedding setup\n# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n#embeddings_index = {}\nf = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split(\" \")\n    if (len(values) < 200):\n        print(\"a\")\n    word = values[0]\n    if word not in  word_index:\n        continue\n    embedding_matrix1[word_index[word]] = np.asarray(values[1:], dtype='float32')\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"026d2cc756fc2fb517c7e6978cd90e8b2706b4b2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed767fad81efcbdbae23e0a5ca0e095f8de5eefa"},"cell_type":"code","source":"f = open('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt', encoding=\"utf8\", errors='ignore')\nfor line in tqdm(f):\n    values = line.split(\" \")\n    word = values[0]\n    if word in  word_index:\n        embedding_matrix2[word_index[word]] = np.asarray(values[1:], dtype='float32')\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e56b4ce49b4743372ef2c75703225582a2b9d81d"},"cell_type":"code","source":"#tmp = embedding_matrix1.sum(axis=1)\ntmp = pd.DataFrame(list(tokenizer.word_index.items()))\ntmp[2] = (embedding_matrix1.sum(axis=1)==0)[1:]\ntmp[3] = (embedding_matrix2.sum(axis=1)==0)[1:]\n#tmp[4] = (embedding_matrix3.sum(axis=1)==0)[1:]\n#a = tmp[tmp[2]][0][:1000].values\n#a1 = tmp[tmp[2]][0][:1000].index\n\ntmp = tmp[tmp[2] | tmp[3]][:4000]\nname = tmp[0].values\nindexes = tmp[1].values\nembedding_matrix1[indexes] = 0\nembedding_matrix2[indexes] = 0\n#embedding_matrix3[indexes] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90f0fe5389b709cebfeecedc790faf6a6b27fe87"},"cell_type":"code","source":"%%time\ntoken_name = {}\ncnt = 1\nfor i in list(tokenizer.word_index.keys()):\n    if i in name:\n        token_name[i] = cnt\n        cnt += 1\n    else:\n        token_name[i] = 0\n\ntokenizer.word_index = token_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee2f6f331a8784887fbd076dea27a05625bebb55"},"cell_type":"code","source":"X_train = train_df['transform'].values\nX_val = val_df['transform'].values\ny_train = train_df['target'].values\ny_val = val_df['target'].values\nX_test = test_df['transform'].values\n\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nx_train_name = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post',truncating='post')\nx_val_name = sequence.pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\nx_test_name = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde11e1c2c43c4dc64df9ed9e2e3ea50168ae150"},"cell_type":"code","source":"train_df['token'] = list(x_train)\ntrain_df['token_name'] = list(x_train_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50b43b17310126b5067c970c8a044b9e65828403"},"cell_type":"code","source":"mask_zeros = np.ones((name.shape[0] + 1, 600))\nmask_zeros[0] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"798c303ec834fb530a60a1e590cfbd9a86f93fde"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.models import Model, load_model\nfrom keras.layers import CuDNNGRU, CuDNNLSTM, Dense, Bidirectional, Input, SpatialDropout1D,Embedding, \\\n        BatchNormalization, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Conv1D, Multiply, Add","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d8e00f54ef80ebafda2f48b8c8c7718cb184aba"},"cell_type":"code","source":"from keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\n\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7105d08058d0d99aab6ae91ff7ffa43a339da49a"},"cell_type":"code","source":"class GlobalMinPooling1D(Layer):\n    \n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        return K.min(x, axis=1)\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[2])\n\n#     def compute_output_shape(self, input_shape):\n#         #return input_shape[0], input_shape[-1]\n#         return input_shape[0],  self.features_dim\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ab3cb0007d45f9a4288ccd0e5a008a3bf0a3553"},"cell_type":"code","source":"class GlobalSumPooling1D(Layer):\n    \n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        return K.sum(x, axis=1)\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[2])\n\n#     def compute_output_shape(self, input_shape):\n#         #return input_shape[0], input_shape[-1]\n#         return input_shape[0],  self.features_dim\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e06dd1f2a0aeca63620ddf95ef8d5da2670e0b74"},"cell_type":"code","source":"from keras.layers import Dropout\nfrom keras.initializers import he_uniform\ndef build_model(input_layer, input_layer_name,  embedding_matrix):\n    x1 = Embedding(embedding_matrix.shape[0], 600, weights=[embedding_matrix], trainable= False)(input_layer)\n    x2 = Embedding(name.shape[0] + 1, 600,  trainable= True)(input_layer_name)\n    x3 = Embedding(name.shape[0] + 1, 600,  weights=[mask_zeros], trainable= False)(input_layer_name)\n    #x = SpatialDropout1D(0.2)(x)\n    x = Multiply()([x2, x3])\n    x = Add()([x1, x])\n    x = Bidirectional(CuDNNLSTM(128, kernel_initializer=he_uniform(seed=0), return_sequences=True))(x)\n    x = SpatialDropout1D(0.2)(x)\n    y = Bidirectional(CuDNNGRU(128,kernel_initializer=he_uniform(seed=0), return_sequences=True))(x)\n    a = GlobalAveragePooling1D()(y)\n    b = GlobalMaxPooling1D()(y)\n    c = GlobalMinPooling1D()(y)\n    #t = GlobalMaxPooling1D()(x)\n    #d = Attention(30)(x)\n    #e = Attention(30)(y)\n    x = concatenate([a, b])\n    x = Dense(32, activation=\"relu\",kernel_initializer=he_uniform(seed=0))(x)\n    x = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=0))(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52db456d253ffb3eb366e3fe74d790d542b88667"},"cell_type":"code","source":"def threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)]):\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04ff7707433d17da5025a94b0677127397cd6c6d"},"cell_type":"code","source":"\n\ntrain_df.reset_index(drop = True, inplace = True)\nval_df.reset_index(drop= True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d516d03c2204ef0d8ce547f83ef2ea1e9a65ec2"},"cell_type":"code","source":"positive = train_df[train_df['target'] == 1][['token','token_name','target']]\nnegative = train_df[train_df['target'] == 0][['token','token_name','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e06dd1f2a0aeca63620ddf95ef8d5da2670e0b74","scrolled":true},"cell_type":"code","source":"for C in range(4):\n    if (C == 0):\n        embedding_matrix = np.concatenate([embedding_matrix1, embedding_matrix2], axis=1)\n    elif (C == 1):\n        embedding_matrix = np.concatenate([embedding_matrix1, embedding_matrix2], axis=1)\n    elif (C == 2):\n        embedding_matrix = np.concatenate([embedding_matrix2, embedding_matrix1], axis=1)\n    else:\n        embedding_matrix = np.concatenate([ embedding_matrix2, embedding_matrix1], axis=1)\n\n    neg1, neg2 = train_test_split(negative, test_size = 0.5, random_state = C*100)\n    df1, df2 = pd.concat([neg1,positive], ignore_index=True), pd.concat([neg2,positive], ignore_index=True)\n    input_layer = Input((50,), name=\"i1\")\n    input_layer_name = Input((50, ), name = \"i2\")\n    output_layer = build_model(input_layer, input_layer_name, embedding_matrix)\n    model = Model([input_layer, input_layer_name], output_layer)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\n\n    model.fit({\"i1\": np.stack(df1['token'].values), \"i2\": np.stack(df1['token_name'].values)}, \n        df1['target'], batch_size=128, verbose=2,shuffle=True,\\\n              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n    \n\n    \n    model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=256, verbose=2,shuffle=True,\\\n              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n    #model.fit(x_train, train_df['target'], batch_size=1024, verbose=1,shuffle=True,\\\n    #          epochs=1, validation_data=(x_val, val_df['target']), callbacks=callbacks_list)\n    #val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n    #test_df[C] = model.predict(x_test, batch_size=512).flatten()\n    #best_search = threshold_search(val_df['target'].values, val_df[C].values)\n    #print(best_search)\n    \n    #print(model.layers[2].get_weights()[0])\n    model.layers[2].trainable = False\n    #model.layers[7].trainable = False\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n#     model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=512, verbose=2,shuffle=True,\\\n#               epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n#     #model.fit(x_train, train_df['target'], batch_size=1024, verbose=1,shuffle=True,\\\n#     #          epochs=1, validation_data=(x_val, val_df['target']), callbacks=callbacks_list)\n#     val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n#     #test_df[C] = model.predict(x_test, batch_size=512).flatten()\n#     best_search = threshold_search(val_df['target'].values, val_df[C].values)\n#     print(best_search)\n    \n    model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=512, verbose=2,shuffle=True,\\\n              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n   \n    #print(model.layers[2].get_weights()[0])\n    #val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n    test_df[C] = model.predict({\"i1\": x_test, \"i2\": x_test_name}, batch_size=1024).flatten()\n    #best_search = threshold_search(val_df['target'].values, val_df[C].values)\n    #print(best_search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d940e61963942fd4718cc3f87cb64b77b805eb56"},"cell_type":"code","source":"#val_df['preds'] = (val_df[0] + val_df[1] + val_df[2] + val_df[3])/4\ntest_df['preds'] = (test_df[0] + test_df[1] + test_df[2] + test_df[3])/4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e6ed54def110c881f401c6f8a844752baedcfbd"},"cell_type":"code","source":"y_te = (test_df['preds'] > 0.35).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}