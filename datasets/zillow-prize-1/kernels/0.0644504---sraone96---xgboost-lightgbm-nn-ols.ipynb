{"cells": [{"metadata": {"collapsed": true, "_cell_guid": "95e56fb1-1226-48cb-ae4b-c4cb44c3f37b", "_uuid": "b22a1bfde673d92d11e29297cbfea55c8b7ea996"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "import lightgbm as lgb\n", "import gc\n", "from sklearn.linear_model import LinearRegression\n", "import random\n", "import datetime as dt\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Dropout, BatchNormalization\n", "from keras.layers.advanced_activations import PReLU\n", "from keras.layers.noise import GaussianDropout\n", "from keras.optimizers import Adam\n", "from keras.wrappers.scikit_learn import KerasRegressor\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.preprocessing import Imputer\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "3a9237e3-750c-4902-a415-0a8b9ed2de86", "_uuid": "b38f6d5011fdaba9aee98da9affb96dd9e330911"}, "cell_type": "markdown", "source": ["##### Parameters\n", "FUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n", "\n", "XGB_WEIGHT = 0.6200\n", "BASELINE_WEIGHT = 0.0100\n", "OLS_WEIGHT = 0.0620\n", "NN_WEIGHT = 0.0800\n", "\n", "XGB1_WEIGHT = 0.8540  # Weight of first in combination of two XGB models\n", "\n", "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg"]}, {"metadata": {"_cell_guid": "65e2d239-48c1-40e1-96fa-b025010a6f9b", "_uuid": "75252f52d39e4d388a4d2cea5db3b70aa7d0c07f"}, "source": ["prox=pd.read_csv('../input/properties_2016.csv')\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f0306fca-4f37-4b4d-b129-abcc4801df8c", "_uuid": "623f72155e33c10a78f46f43396bb6b093795bdc"}, "source": ["for c in prox.columns:\n", " #   print (c)\n", "    if(c=='airconditioningtypeid' or c=='taxdelinquencyflag' or c=='poolcnt' or c=='regionidcity' or c=='fireplacecnt' or c=='propertyzoningdesc' or c=='propertycountylandusecode'):\n", "        prox[c]=prox[c].fillna(-1)\n", "    else:\n", "        prox[c]=prox[c].fillna(prox[c].median())\n", "#print (cols)\n", "prox.head()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b58eecba-adee-483b-a378-7ee98cbea9ad", "_uuid": "0539afda008fa96b573dca18ce57bb630e550a76"}, "source": ["lbl = LabelEncoder()\n", "for c in prox.columns:\n", "    prox[c]=prox[c].fillna(-1)\n", "    if prox[c].dtype == 'object':\n", "        lbl.fit(list(prox[c].values))\n", "        prox[c] = lbl.transform(list(prox[c].values))\n", "prox.head()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "39670566-0739-466b-8778-41b70016cd90", "_uuid": "41e6d4e272b213359476f3c619a956501a8cfb69"}, "source": ["#life of property\n", "prox['N-life'] = 2018 - prox['yearbuilt']\n", "#error in calculation of the finished living area of home\n", "prox['N-LivingAreaError'] = prox['calculatedfinishedsquarefeet']/prox['finishedsquarefeet12']\n", "\n", "#proportion of living area\n", "prox['N-LivingAreaProp'] = prox['calculatedfinishedsquarefeet']/prox['lotsizesquarefeet']\n", "prox['N-LivingAreaProp2'] = prox['finishedsquarefeet12']/prox['finishedsquarefeet15']\n", "\n", "#Amout of extra space\n", "prox['N-ExtraSpace'] = prox['lotsizesquarefeet'] - prox['calculatedfinishedsquarefeet'] \n", "prox['N-ExtraSpace-2'] = prox['finishedsquarefeet15'] - prox['finishedsquarefeet12'] \n", "\n", "#Total number of rooms\n", "prox['N-TotalRooms'] = prox['bathroomcnt']*prox['bedroomcnt']\n", "\n", "#Average room size\n", "prox['N-AvRoomSize'] = prox['calculatedfinishedsquarefeet']/prox['roomcnt'] \n", "\n", "# Number of Extra rooms\n", "prox['N-ExtraRooms'] = prox['roomcnt'] - prox['N-TotalRooms'] \n", "\n", "#Ratio of the built structure value to land area\n", "prox['N-ValueProp'] = prox['structuretaxvaluedollarcnt']/prox['landtaxvaluedollarcnt']\n", "\n", "#Does property have a garage, pool or hot tub and AC?\n", "prox['N-GarPoolAC'] = ((prox['garagecarcnt']>0) & (prox['pooltypeid10']>0) & (prox['airconditioningtypeid']!=5))*1 \n", "\n", "prox[\"N-location\"] = prox[\"latitude\"] + prox[\"longitude\"]\n", "prox[\"N-location-2\"] = prox[\"latitude\"]*prox[\"longitude\"]\n", "prox[\"N-location-2round\"] = prox[\"N-location-2\"].round(-4)\n", "\n", "prox[\"N-latitude-round\"] = prox[\"latitude\"].round(-4)\n", "prox[\"N-longitude-round\"] = prox[\"longitude\"].round(-4)\n", "\n", "prox.head()\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "31a20d30-c448-49cb-a355-d8f01e0bc199", "_uuid": "1c704aa3b2ced6d167a93cdbb2d64f00021912d6"}, "source": ["#Ratio of tax of property over parcel\n", "prox['N-ValueRatio'] = prox['taxvaluedollarcnt']/prox['taxamount']\n", "\n", "#TotalTaxScore\n", "prox['N-TaxScore'] = prox['taxvaluedollarcnt']*prox['taxamount']\n", "\n", "#polnomials of tax delinquency year\n", "prox[\"N-taxdelinquencyyear-2\"] = prox[\"taxdelinquencyyear\"] ** 2\n", "prox[\"N-taxdelinquencyyear-3\"] = prox[\"taxdelinquencyyear\"] ** 3\n", "\n", "#Length of time since unpaid taxes\n", "prox['N-life'] = 2018 - prox['taxdelinquencyyear']\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "283ff3fc-7b95-43b6-8fdd-8dce2d750689", "_uuid": "5af950c1606ccfc0e5b464fa9f76c768c79a3712"}, "source": ["#Number of prox in the zip\n", "zip_count = prox['regionidzip'].value_counts().to_dict()\n", "prox['N-zip_count'] = prox['regionidzip'].map(zip_count)\n", "\n", "#Number of prox in the city\n", "city_count = prox['regionidcity'].value_counts().to_dict()\n", "prox['N-city_count'] = prox['regionidcity'].map(city_count)\n", "\n", "#Number of prox in the city\n", "region_count = prox['regionidcounty'].value_counts().to_dict()\n", "prox['N-county_count'] = prox['regionidcounty'].map(city_count)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "e096f4c7-7deb-4834-8e2a-cb5021691b27", "_uuid": "b535d9b9bc30a27e3c7cdd4646d6f088a73c49e0"}, "source": ["#Indicator whether it has AC or not\n", "prox['N-ACInd'] = (prox['airconditioningtypeid']!=5)*1\n", "\n", "#Indicator whether it has Heating or not \n", "prox['N-HeatInd'] = (prox['heatingorsystemtypeid']!=13)*1\n", "\n", "#There's 25 different property uses - let's compress them down to 4 categories\n", "prox['N-PropType'] = prox.propertylandusetypeid.replace({31 : \"Mixed\", 46 : \"Other\", 47 : \"Mixed\", 246 : \"Mixed\", 247 : \"Mixed\", 248 : \"Mixed\", 260 : \"Home\", 261 : \"Home\", 262 : \"Home\", 263 : \"Home\", 264 : \"Home\", 265 : \"Home\", 266 : \"Home\", 267 : \"Home\", 268 : \"Home\", 269 : \"Not Built\", 270 : \"Home\", 271 : \"Home\", 273 : \"Home\", 274 : \"Other\", 275 : \"Home\", 276 : \"Home\", 279 : \"Home\", 290 : \"Not Built\", 291 : \"Not Built\" })"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "6a7d6339-4b06-4944-81af-ef6e9b2aa6e0", "_uuid": "89d435d02e46901dceceaa6d9963a11d285ab5ae"}, "source": ["#polnomials of the variable\n", "prox[\"N-structuretaxvaluedollarcnt-2\"] = prox[\"structuretaxvaluedollarcnt\"] ** 2\n", "prox[\"N-structuretaxvaluedollarcnt-3\"] = prox[\"structuretaxvaluedollarcnt\"] ** 3\n", "\n", "#Average structuretaxvaluedollarcnt by city\n", "group = prox.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n", "prox['N-Avg-structuretaxvaluedollarcnt'] = prox['regionidcity'].map(group)\n", "\n", "#Deviation away from average\n", "prox['N-Dev-structuretaxvaluedollarcnt'] = abs((prox['structuretaxvaluedollarcnt'] - prox['N-Avg-structuretaxvaluedollarcnt']))/prox['N-Avg-structuretaxvaluedollarcnt']\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e4d14a88-3aee-47a1-b959-ac6cc8cac16c", "_uuid": "0cc51d9afedb1664b35395bd60bf2dd6edfb4004"}, "source": ["print( \"\\nReading data from disk ...\")\n", "prop = prox\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "674356e1-e133-430c-afbd-e03a4bc14f58", "_uuid": "aade565c835aa7835a6186c81a8fda51a74a5b72"}, "source": ["#LightGBM\n", "print( \"\\nProcessing data for LightGBM ...\" )\n", "for c, dtype in zip(prop.columns, prop.dtypes):\t\n", "    if dtype == np.float64:\n", "        prop[c] = prop[c].astype(np.float32)\n", "\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "df_train.fillna(df_train.median(),inplace = True)\n", "\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n", "#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)\n", "\n", "\n", "train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "\n", "del df_train; gc.collect()\n", "\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "17df07ae-b3a8-4df2-af6a-99def686d2bc", "_uuid": "a19c8980415a738503d84e9d471911d8d4ae66cb"}, "source": ["\n", "params = {}\n", "params['max_bin'] = 10\n", "params['learning_rate'] = 0.0021 # shrinkage_rate\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'l1'          # or 'mae'\n", "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n", "params['bagging_fraction'] = 0.85 # sub_row\n", "params['bagging_freq'] = 40\n", "params['num_leaves'] = 512        # num_leaf\n", "params['min_data'] = 500         # min_data_in_leaf\n", "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n", "params['verbose'] = 0\n", "params['feature_fraction_seed'] = 2\n", "params['bagging_seed'] = 3\n", "\n", "np.random.seed(0)\n", "random.seed(0)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4aca313f-27a7-4387-8493-1baa3bb3d5a8", "_uuid": "2abe4335c4fdaba59627565f38242e4dba4c2a1b"}, "source": ["print(\"\\nFitting LightGBM model ...\")\n", "clf = lgb.train(params, d_train, 430)\n", "\n", "del d_train; gc.collect()\n", "del x_train; gc.collect()\n", "\n", "print(\"\\nPrepare for LightGBM prediction ...\")\n", "print(\"   Read sample file ...\")\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", "print(\"   ...\")\n", "sample['parcelid'] = sample['ParcelId']\n", "print(\"   Merge with property data ...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "print(\"   ...\")\n", "del sample, prop; gc.collect()\n", "print(\"   ...\")\n", "#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\n", "x_test = df_test[train_columns]\n", "print(\"   ...\")\n", "del df_test; gc.collect()\n", "print(\"   Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "print(\"   ...\")\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "\n", "print(\"\\nStart LightGBM prediction ...\")\n", "p_test = clf.predict(x_test)\n", "\n", "del x_test; gc.collect()\n", "\n", "print( \"\\nUnadjusted LightGBM predictions:\" )\n", "print( pd.DataFrame(p_test).head() )\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "87f859ba-3ee0-451a-bee7-921de04a23bf", "_uuid": "9e7940335d5c11bb3198a1d75f9a332a1825bdab"}, "source": ["#XGBOOST\n", "\n", "print( \"\\nRe-reading properties file ...\")\n", "properties = prox\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "fef69f74-b4dc-4908-aba5-fbcc6ea99c3e", "_uuid": "471279bd1df7c281e66cd32aa21c5ae4c6a6cd7b"}, "source": ["print( \"\\nProcessing data for XGBoost ...\")\n", "for c in properties.columns:\n", "    properties[c]=properties[c].fillna(-1)\n", "    if properties[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties[c].values))\n", "        properties[c] = lbl.transform(list(properties[c].values))\n", "\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "x_test = properties.drop(['parcelid'], axis=1)\n", "# shape        \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "27ad2556-3dc9-4bad-be70-3f5e707f3a33", "_uuid": "a583ba74ec83c0daa566408febfeea2f84ceec7b"}, "source": ["#outliers\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "\n", "print('After removing outliers:')     \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "fe31ddd8-6c9d-485b-ad89-3daee661a9fd", "_uuid": "35f4bca42413296074ce6f7fcdf98f631d3e7937"}, "source": ["print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.037,\n", "    'max_depth': 5,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 0.8,   \n", "    'alpha': 0.4, \n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dtest = xgb.DMatrix(x_test)\n", "\n", "num_boost_rounds = 250\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "# train model\n", "print( \"\\nTraining XGBoost ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost ...\")\n", "xgb_pred1 = model.predict(dtest)\n", "\n", "print( \"\\nFirst XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred1).head() )\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "ddf80d0b-0524-4efa-9f92-2907633df64d", "_uuid": "fe6b4be138978a7db450be56dd0a4187955fe52d"}, "source": ["print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.033,\n", "    'max_depth': 6,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "num_boost_rounds = 150\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "print( \"\\nTraining XGBoost again ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost again ...\")\n", "xgb_pred2 = model.predict(dtest)\n", "\n", "print( \"\\nSecond XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred2).head() )\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "f5e7293c-cf73-4ec8-9a7f-3ab3553679a8", "_uuid": "d3f373938ce6c8d93ab53c4d53d357a94af04007"}, "source": ["xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n", "#xgb_pred = xgb_pred1\n", "\n", "print( \"\\nCombined XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred).head() )"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "9b6e8f5f-54df-4383-a1ab-16b27017a7e3", "_uuid": "a76cfc80fd53516782cb27f2549ad8421f8268f7"}, "source": ["del train_df\n", "del x_train\n", "del x_test\n", "del properties\n", "del dtest\n", "del dtrain\n", "del xgb_pred1\n", "del xgb_pred2 \n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "affd4eaf-b9c9-4921-b0cb-42eed006bd47", "_uuid": "75a6c78c40f1e3a1f78e61186cf7b2d1b1d0d50a"}, "source": ["#neural nets\n", "\n", "print( \"\\n\\nProcessing data for Neural Network ...\")\n", "print('\\nLoading train, prop and sample data...')\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "prop = prox\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", " \n", "print('Fitting Label Encoder on properties...')\n", "for c in prop.columns:\n", "    prop[c]=prop[c].fillna(-1)\n", "    if prop[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(prop[c].values))\n", "        prop[c] = lbl.transform(list(prop[c].values))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "ee8c74f5-8f12-465f-91e4-b7aa15ce34a0", "_uuid": "c408f57bc333f53585db2b0a18ce8fbe947ed08b"}, "source": ["print('Creating training set...')\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "\n", "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n", "df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n", "df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n", "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n", "df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n", "\n", "print('Filling NA/NaN values...' )\n", "df_train.fillna(-1.0)\n", "\n", "print('Creating x_train and y_train from df_train...' )\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\n", "y_train = df_train[\"logerror\"]\n", "\n", "y_mean = np.mean(y_train)\n", "print(x_train.shape, y_train.shape)\n", "train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "d2e30e0e-8795-4cd6-a380-780d698e55ba", "_uuid": "32b9a336ffe5d82b1802fcd4709248d61d26b7db"}, "source": ["print('Creating df_test...')\n", "sample['parcelid'] = sample['ParcelId']\n", "\n", "print(\"Merging Sample with property data...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "\n", "df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\n", "df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n", "df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n", "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n", "df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \n", "x_test = df_test[train_columns]\n", "\n", "print('Shape of x_test:', x_test.shape)\n", "print(\"Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "  "], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "6ce4a6b1-9b0c-47f7-9843-e3cca3b11447", "_uuid": "699ddefbcd8a6eb055d43c3449d17c0501d26a95"}, "source": ["print(\"\\nPreprocessing neural network data...\")\n", "imputer= Imputer()\n", "imputer.fit(x_train.iloc[:, :])\n", "x_train = imputer.transform(x_train.iloc[:, :])\n", "imputer.fit(x_test.iloc[:, :])\n", "x_test = imputer.transform(x_test.iloc[:, :])\n", "\n", "sc = StandardScaler()\n", "x_train = sc.fit_transform(x_train)\n", "x_test = sc.transform(x_test)\n", "\n", "len_x=int(x_train.shape[1])\n", "print(\"len_x is:\",len_x)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "e6fac4cd-0e5a-4758-a758-a7a66a401450", "_uuid": "7504b9d59449fb9d00996da4f9880046fe580f7b"}, "source": ["# Neural Network\n", "print(\"\\nSetting up neural network model...\")\n", "nn = Sequential()\n", "nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\n", "nn.add(PReLU())\n", "nn.add(Dropout(.4))\n", "nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n", "nn.add(PReLU())\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.6))\n", "nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n", "nn.add(PReLU())\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.5))\n", "nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n", "nn.add(PReLU())\n", "nn.add(BatchNormalization())\n", "nn.add(Dropout(.6))\n", "nn.add(Dense(1, kernel_initializer='normal'))\n", "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "70d83c1f-624e-4ce4-a9c7-2af4dbfbfc98", "_uuid": "301e771bfecb0d3389e657a258568b5e49e7ea65"}, "source": ["print(\"\\nFitting neural network model...\")\n", "nn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 52, verbose=2)\n", "\n", "print(\"\\nPredicting with neural network model...\")\n", "#print(\"x_test.shape:\",x_test.shape)\n", "y_pred_ann = nn.predict(x_test)\n", "\n", "print( \"\\nPreparing results for write...\" )\n", "nn_pred = y_pred_ann.flatten()\n", "print( \"Type of nn_pred is \", type(nn_pred) )\n", "print( \"Shape of nn_pred is \", nn_pred.shape )\n", "\n", "print( \"\\nNeural Network predictions:\" )\n", "print( pd.DataFrame(nn_pred).head() )"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "fe63c72a-e73a-4aba-9a03-10e105ba7eaa", "_uuid": "3d62aa6ce8e06011a04b51f006f018875633ff3d"}, "source": ["del train\n", "del prop\n", "del sample\n", "del x_train\n", "del x_test\n", "del df_train\n", "del df_test\n", "del y_pred_ann\n", "gc.collect()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "dabd6555-fed9-4293-92fd-ad9f5c023f48", "_uuid": "d05a30dc5a5c5ecd3302ce0d69027fd8e44c8e53"}, "source": ["#OLS\n", "\n", "np.random.seed(17)\n", "random.seed(17)\n", "\n", "print( \"\\n\\nProcessing data for OLS ...\")\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "properties = pd.read_csv('../input/properties_2016.csv')\n", "submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "print(len(train),len(properties),len(submission))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "10b02cf8-c626-44ea-b92d-96ba56352d88", "_uuid": "207ae184f0b4e79771841d357cb1b89b5137e909"}, "source": ["def get_features(df):\n", "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n", "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n", "    df['transactiondate'] = df['transactiondate'].dt.quarter\n", "    df = df.fillna(-1.0)\n", "    return df\n", "\n", "def MAE(y, ypred):\n", "    #logerror=log(Zestimate)\u2212log(SalePrice)\n", "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "2f7de0da-bbb8-4e77-bf83-0ade9c004a8b", "_uuid": "65fc9894bd97f563e46f0145e3eb0a9a7c624ef5"}, "source": ["train = pd.merge(train, properties, how='left', on='parcelid')\n", "y = train['logerror'].values\n", "test = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\n", "properties = [] #memory\n", "\n", "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n", "col = [c for c in train.columns if c not in exc]\n", "\n", "train = get_features(train[col])\n", "test['transactiondate'] = '2016-01-01' #should use the most common training date\n", "test = get_features(test[col])"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "b559282b-0ac9-4b26-823c-0589602af5f4", "_uuid": "c43eda8ff8a6e68238c477f4e20e8c9478cea6db"}, "source": ["print(\"\\nFitting OLS...\")\n", "reg = LinearRegression(n_jobs=-1)\n", "reg.fit(train, y); print('fit...')\n", "print(MAE(y, reg.predict(train)))\n", "train = [];  y = [] #memory\n", "\n", "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n", "test_columns = ['201610','201611','201612','201710','201711','201712']\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "8507e2de-f1a7-4f6c-af98-0a66cbcb92c8", "_uuid": "e25a8cee64c04059ade97c6217c409e428221f94"}, "source": ["#combining all predictions\n", "\n", "print( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\n", "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \n", "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n", "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n", "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n", "nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n", "pred0 = 0\n", "pred0 += xgb_weight0*xgb_pred\n", "pred0 += baseline_weight0*BASELINE_PRED\n", "pred0 += lgb_weight0*p_test\n", "pred0 += nn_weight0*nn_pred\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "b565accf-2ab9-4980-990f-dc821b53d644", "_uuid": "1eb7dd82579da617fa45ba5aceae8d6f230511b5"}, "source": ["print( \"\\nCombined XGB/LGB/NN/baseline predictions:\" )\n", "print( pd.DataFrame(pred0).head() )\n", "\n", "print( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\n", "for i in range(len(test_dates)):\n", "    test['transactiondate'] = test_dates[i]\n", "    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n", "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n", "    print('predict...', i)\n", "\n", "print( \"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\" )\n", "print( submission.head() )"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "6e754544-b0f2-407d-a617-25df099e3905", "_uuid": "f320611dede2df5c90e02bf7e8cf833da21946ed"}, "source": ["##### WRITE THE RESULTS\n", "\n", "from datetime import datetime\n", "\n", "print( \"\\nWriting results to disk ...\" )\n", "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "\n", "print( \"\\nFinished ...\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true}, "source": [], "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}