{"cells":[{"metadata":{"_uuid":"c6519145aee020496c2234c3f7f5ab0bbeace338"},"cell_type":"markdown","source":"# Santander Value Prediction Challenge  \n\n## _**Objective: To predict the value of transactions for each potential customers**_\n\nIt seems to be a regression problem, where we have to predict a continuos variable.\n"},{"metadata":{"_uuid":"597a1d3180f174f2fe901d5c69f21d223aeee02d"},"cell_type":"markdown","source":"## _Let's import the required modules_"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6daa591bfacdecf64aad2c48fa0f46f8e58da66"},"cell_type":"code","source":"#base modules\nimport numpy as np\nimport pandas as pd\n\n#visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n#Scipy\nimport scipy\n\n#scikit-learn\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n#LightGbm\nimport lightgbm\n\n#Model validation\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics.scorer import make_scorer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e4f270866f7807134c3a5b62ed78051942b1c69"},"cell_type":"code","source":"from IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e5a934dba5c472c982a0bfc0d5ec9fc02f288bc"},"cell_type":"markdown","source":"## _Let's load the dataset_"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2fc3a140c0df3b0c8a47a11b1d77209a6b5e3f28"},"cell_type":"code","source":"# Let's import the dataset\ntrain_data  = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fba7001423487a28f70b554671379409d3107c6f"},"cell_type":"markdown","source":"## _Let's get started with understanding the data, it's time for some exploratory data analysis(EDA)_\n_Let's first see how the data looks like_"},{"metadata":{"trusted":true,"_uuid":"14815189f09e5d4380b2e75634412a84f23fe84a","collapsed":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a474965d6fc6cf09c2053c1fd9fb53fb1815339","collapsed":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c6951a77dcb2d4c5d3b21c677838db6bed0d03dc"},"cell_type":"code","source":"train_id = train_data['ID']\ntrain_target = train_data['target']\ntest_id = test_data['ID']\ndel train_data['ID']\ndel train_data['target']\ndel test_data['ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4880948a6e26857edfef3f62bd41a2d2491eb2f9","collapsed":true},"cell_type":"code","source":"print(\"The number of columns in train dataset are %i\" % len(train_data.columns))\nprint(\"The number of rows in train dataset are %i\" % len(train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f06d1d7868e3492a5171999576a84ce5f8c0146","collapsed":true},"cell_type":"code","source":"print(\"The number of columns in test dataset are %i\" % len(test_data.columns))\nprint(\"The number of rows in test dataset are %i\" % len(test_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"035ac2d1590c278d174fe5afc7e728c721ce824d"},"cell_type":"markdown","source":"> Note: \n* _The data is masked, we do not know what the columns really mean_\n* _The number of columns in train dataset are greater than the number of rows_\n* _The test data is almost 10 times larger than train dataset_ \n"},{"metadata":{"_uuid":"084994087e32fd6e0138428139aaa2d881260ba4"},"cell_type":"markdown","source":"## _Let's understand the distribution of the target variable_"},{"metadata":{"trusted":true,"_uuid":"a6e2e5df96cc3ff296188d2ad68ef56da5699c93","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.distplot(train_target)\nplt.xlabel(\"Target\",fontsize=14)\nplt.title(\"Histogram-KDE plot of target variable\",fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23acddd4790add8a7deaa7eaadd0042872215a23"},"cell_type":"markdown","source":"_The target variable is highly skewed towards positively. So, let's apply some transformation to make it normally distributed._"},{"metadata":{"trusted":true,"_uuid":"3f0be546548ba45d59f2c8a7a27e38ee71a03fbb","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.distplot(np.log(train_target))\nplt.xlabel(\"Target\",fontsize=14)\nplt.title(\"Histogram-KDE of log transformation of the target varible\",fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60eeac9e1b0071ebef564cd06f4afed56e1c30b7"},"cell_type":"markdown","source":"_This looks much better than the actual target variable, but let's try out box cox transformation also._"},{"metadata":{"trusted":true,"_uuid":"4460d9f6c973ba152f86be70feff37edc70259d3","collapsed":true},"cell_type":"code","source":"box_cox_trans = scipy.stats.boxcox(train_target.values,lmbda=0.1)\nbox_cox_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be9c78cb7aeb1a6e042865a64e61b54fef933c5a","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.distplot(box_cox_trans)\nplt.xlabel(\"Target\",fontsize=14)\nplt.title(\"Histogram-KDE plot of box-cox transformation of the target variable\",fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f29b4bccfa6b0cfb7cee84ea469a7559ef8a13f"},"cell_type":"markdown","source":"_This looks pretty much normally distributed and better._"},{"metadata":{"_uuid":"b401760f3e351a589b8ef3ffe6047e93abb77684"},"cell_type":"markdown","source":"## _Let's check if there are any columns with constant values through out_"},{"metadata":{"trusted":true,"_uuid":"49a2a56fbf01f1b834a1c7ad5cbf36cbf1693b40","collapsed":true},"cell_type":"code","source":"train_data.nunique()[train_data.nunique(axis=0)==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0663f189c75ea177b7a6d6fab18f45dc6680852"},"cell_type":"code","source":"constant_column_names = train_data.columns[train_data.nunique(axis=0)==1].tolist() #Saving the redundant column names ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70b0f55c59f48c85dec8aeb2e370b07ab4b67778"},"cell_type":"markdown","source":"_There are 256 variables that has constant values. Therefore it can be dropped._"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d41acfa6cf3dd82953e6627ef15bbbc3bb040c71"},"cell_type":"markdown","source":"## _Let's now check for missing values_"},{"metadata":{"trusted":true,"_uuid":"f6dce3ba9eb25d56be89d26188bafca73a5b3976","collapsed":true},"cell_type":"code","source":"train_data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89ea18be30c1f7d1cf0b70bd7cec1e252a98e15b","collapsed":true},"cell_type":"markdown","source":" _There are'nt any missing value in the train data_"},{"metadata":{"_uuid":"88b8c10b05073a23e79478a603f90094768a5ad9"},"cell_type":"markdown","source":" "},{"metadata":{"_uuid":"aa4aa2a88598f356bcdfd4c52d88c9458a329b2d"},"cell_type":"markdown","source":"## _Let's now understand the datatype of the variables_"},{"metadata":{"trusted":true,"_uuid":"3ff26bcad74eb8934acb563d27148fe045e54520","collapsed":true},"cell_type":"code","source":"combined_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55aef3f445bfb76cf6ac229a5e8331a612d53dd2"},"cell_type":"markdown","source":"## _Let's drop the columns with constant values_"},{"metadata":{"trusted":true,"_uuid":"3f970ef640b97af5ee3957a4fb1a065e18946e92","collapsed":true},"cell_type":"code","source":"train_data = train_data.drop(columns = constant_column_names,axis=1)\ntest_data = test_data.drop(columns = constant_column_names,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6b1b2841bbaae6201f69c327083d7b55018e27a"},"cell_type":"markdown","source":"## _Let's now use use PCA for dimensionality reduction the dimensions_"},{"metadata":{"trusted":true,"_uuid":"06484edec8b11f25948396bc4c0cebc2aac060d2","collapsed":true},"cell_type":"code","source":"combined_data = pd.concat([train_data,test_data],axis=0)\nprint(combined_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"85a963044a70f0b4348e7817191868ba97b3eae0"},"cell_type":"code","source":"# combined_data = scale(combined_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bb6605eb5d68d2e7f701547b3a6548545415eaa","collapsed":true},"cell_type":"code","source":"pca = PCA(n_components=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d0a3a250f4e26376d671693ecd2cb0dfd308cb","collapsed":true},"cell_type":"code","source":"pca.fit_transform(combined_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd2633e176f66288b3824263a88a5eeabd915c2","collapsed":true},"cell_type":"code","source":"cumsum_variance = np.cumsum(np.round(pca.explained_variance_ratio_,decimals=4)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3900fd782264a0f62392c6a0788985a709fb018","collapsed":true},"cell_type":"code","source":"plt.plot(cumsum_variance)\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative percentage of explained variance\")\nplt.title(\"Plot of explained variance in percentage\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f16624d87773a1cb81d4356819c8cdaa47a34ca"},"cell_type":"markdown","source":" _Looks like the dimensionality reduction using PCA is not that useful, so going to run my baseline model on the actual data with out dimensionality reduction._"},{"metadata":{"_uuid":"dea72a3a14e5fd0eaa4af88ac7c51f5e2035c601"},"cell_type":"markdown","source":"## _Let's now try truncated SVD for reducing the dimensions_"},{"metadata":{"trusted":true,"_uuid":"fbf44df7849e336028880be6803f6a17a0979dba","collapsed":true},"cell_type":"code","source":"tsvd = TruncatedSVD(n_components=2000)\ntsvd.fit_transform(combined_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f9b8ec10c0e069a3132977e9bd126a4af6774294"},"cell_type":"code","source":"cumsum_variance = np.cumsum(np.round(tsvd.explained_variance_ratio_,decimals=4)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b339785623d2ecff6271e39b75aa134d3236def","collapsed":true},"cell_type":"code","source":"cumsum_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d814b9b4832a85ffb73fcc3226091fcc1fc61c2d","collapsed":true},"cell_type":"code","source":"plt.plot(cumsum_variance)\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative percentage of explained variance\")\nplt.title(\"Plot of explained variance in percentage-tsvd\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3be851d35a100689f085196f3b9c42369e78a0d0"},"cell_type":"markdown","source":"The t-svd performs slightly better than PCA in dimensionality reduction."},{"metadata":{"_uuid":"c9cf1519c57e362a2a036d7523f0a894fdcc27bf"},"cell_type":"markdown","source":"## _Let me now transform some of the variables as categorical based on their frequecies of occurence_"},{"metadata":{"trusted":true,"_uuid":"80f99aad4e6a68a0b4d49df892bf83f35efd4722","collapsed":true},"cell_type":"code","source":"column_name = []\ndistinct_values = []\nfor col in combined_data.columns:\n#     if combined_data[col].dtype == 'int64':\n    column_name.append(col)\n    distinct_values.append(combined_data[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee9873261fd1ffdcd14986885a9c47d7e22dec88","collapsed":true},"cell_type":"code","source":"plt.plot(sorted(distinct_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96fa5520131a2ad65dc8f93c05484442974d9a11","collapsed":true},"cell_type":"code","source":"count=0\nfor col in combined_data.columns:\n    if combined_data[col].nunique()<=500:\n        combined_data[col]=combined_data[col].astype('category')\n#         count=count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f5ffd597d9db19c7fa35a09c4be1c6e8005579","collapsed":true},"cell_type":"code","source":"train_data = combined_data[:len(train_data)]\ntest_data = combined_data[len(train_data):]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381d3c7f958b0a69bec4d868b1303fc79663f527"},"cell_type":"markdown","source":"## _Baseline model building using Lightgbm_"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b9c117f2e0a53cd8ec0339c42631c9c128c60676"},"cell_type":"code","source":"# Let's now define the root mean squared logarthmic error\ndef rmsle(y_pred,y_act):\n    y_pred = scipy.special.inv_boxcox(y_pred,0.1)\n    y_act = scipy.special.inv_boxcox(y_act,0.1)\n    return np.sqrt(np.mean(np.square(np.log(y_pred+1)-np.log(y_act+1))))\n\nscorer = make_scorer(rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5977e71dc134347d4262ccf27f740fbda69faf3d"},"cell_type":"code","source":"model = lightgbm.LGBMRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"853ef8d69127244c06b2f1c39db21010d9285642","collapsed":true},"cell_type":"code","source":"rmsle_scores = cross_validate(model,train_data,box_cox_trans,scoring=scorer,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d34d4b15c6244e1826f12409ef6c609a07649408","collapsed":true},"cell_type":"code","source":"rmsle_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"683b61931d3231d84b5ab8ecb9fa165b931a41cd","collapsed":true},"cell_type":"code","source":"model.fit(train_data,box_cox_trans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38258723c04408b1e9f9e926e823c43409619285","collapsed":true},"cell_type":"code","source":"test_pred = model.predict(test_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"028e66aca3295f9f1b454259f9d2370ec3d60a9d","collapsed":true},"cell_type":"code","source":"test_pred = scipy.special.inv_boxcox(test_pred,0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"35a573b6fa4b41a91b714bcac07948911eb045d7"},"cell_type":"code","source":"dat = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b5ea756ae7292ecd05692855fe167c7d8da2478","collapsed":true},"cell_type":"code","source":"dat[\"ID\"] = test_id\ndat['target'] = test_pred\ndat.to_csv(\"first_sub_2.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ef0ba393d192551d25498c8d1cda309a1e8fe87"},"cell_type":"markdown","source":"This kernel is all about understanding the data and building a baseline model, there are many things which needs to be fine tuned and a lot of work to be done in preprocessing. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}