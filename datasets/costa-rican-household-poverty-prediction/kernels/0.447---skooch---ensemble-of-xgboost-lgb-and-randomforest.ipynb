{"cells":[{"metadata":{"_uuid":"0479e25c001874fda15f875d76f8663885caa138"},"cell_type":"markdown","source":"# Ensemble of Voting Classifiers\n\n**Edits by Eric Antoine Scuccimarra** - This is a fork of  https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyo, with a few changes:\n - The LightGBM models have been replaced with XGBoost and the code has been updated accordingly.\n - Some additional features have been added.\n - Some features which were previously dropped have been retained.\n - Some of the code has been reorganized.\n - Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than a k-fold split in this case.\n\nSome features were taken from the following kernels: \n - https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\n - https://www.kaggle.com/gaxxxx/exploratory-data-analysis-lightgbm\n \n**Notes from Original Kernel (edited by EAS):**\n\nThis kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. \n\nSeveral key points:\n- **This kernel runs training on the heads of housholds only** (after extracting aggregates over households). This follows the announced scoring startegy: *Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.* (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score\n- **It seems to be very important to balance class frequencies.** Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set `class_weight='balanced'` in the LightGBM model constructor in sklearn API.\n- **This kernel uses macro F1 score to early stopping in training**. This is done to align with the scoring strategy.\n- Categoricals are turned into numbers with proper mapping instead of blind label encoding. \n- **OHE if reversed into label encoding, as it is easier to digest for a tree model.** This trick would be harmful for non-tree models, so be careful.\n- **idhogar is NOT used in training**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n- **There are aggregations done within households and new features are hand-crafted**. Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n- **A voting classifier is used to average over several LightGBM models**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport lightgbm as lgb\nfrom scipy import stats\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.utils import class_weight\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4e08a17549fd247619178c96c3ade2519e9773"},"cell_type":"markdown","source":"The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# this only transforms the idhogar field, the other things this function used to do are done elsewhere\ndef encode_data(df):\n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n\n# plot feature importance for sklearn decision trees    \ndef feature_importance(forest, X_train, display_results=True):\n    ranked_list = []\n    zero_features = []\n    \n    importances = forest.feature_importances_\n\n    indices = np.argsort(importances)[::-1]\n    \n    if display_results:\n        # Print the feature ranking\n        print(\"Feature ranking:\")\n\n    for f in range(X_train.shape[1]):\n        if display_results:\n            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n        \n        ranked_list.append(X_train.columns[indices[f]])\n        \n        if importances[indices[f]] == 0.0:\n            zero_features.append(X_train.columns[indices[f]])\n            \n    return ranked_list, zero_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1785c8ca3467a7e95d007a2c5f36e39919fc0910"},"cell_type":"markdown","source":"**There is also feature engineering magic happening here:**"},{"metadata":{"_kg_hide-input":true,"_uuid":"9c9f13e54fc2af9f938b895959631e1aeb3b08a2","trusted":false,"collapsed":true},"cell_type":"code","source":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    \n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"a7df32a07c9157ee02ff9688cdc70c69e7571aae","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert one hot encoded fields to label encoding\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab84429fc9893c82e33b8319161c190b4104e9f"},"cell_type":"markdown","source":"# Read in the data and clean it up"},{"metadata":{"_uuid":"e6f696a1677230c565532f141a02852e7c69b2e1","trusted":false,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntest_ids = test.Id","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd1f66cbdbfa4741d19a8b1f53793b967d62d281","trusted":false,"collapsed":true},"cell_type":"code","source":"def process_df(df_):\n    # encode the idhogar\n    encode_data(df_)\n    \n    # create aggregate features\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaf8a52a939c7d5bc548cbe4ecc1458caae60e7d"},"cell_type":"markdown","source":"Clean up some missing data and convert objects to numeric."},{"metadata":{"_uuid":"65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"# some dependencies are Na, fill those with the square root of the square\ntrain['dependency'] = np.sqrt(train['SQBdependency'])\ntest['dependency'] = np.sqrt(test['SQBdependency'])\n\n# fill \"no\"s for education with 0s\ntrain.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\ntrain.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\ntest.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\ntest.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n\n# if education is \"yes\" and person is head of household, fill with escolari\ntrain.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\ntrain.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n\ntest.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\ntest.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n\n# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\ntrain.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\ntrain.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n\ntest.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\ntest.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n\n# convert to int for our models\ntrain['edjefe'] = train['edjefe'].astype(\"int\")\ntrain['edjefa'] = train['edjefa'].astype(\"int\")\ntest['edjefe'] = test['edjefe'].astype(\"int\")\ntest['edjefa'] = test['edjefa'].astype(\"int\")\n\n# create feature with max education of either head of household\ntrain['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\ntest['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n\n# fill some nas\ntrain['v2a1']=train['v2a1'].fillna(0)\ntest['v2a1']=test['v2a1'].fillna(0)\n\ntest['v18q1']=test['v18q1'].fillna(0)\ntrain['v18q1']=train['v18q1'].fillna(0)\n\ntrain['rez_esc']=train['rez_esc'].fillna(0)\ntest['rez_esc']=test['rez_esc'].fillna(0)\n\ntrain.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\ntrain.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\ntest.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\ntest.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n\n# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n# if there is no water we'll assume they do not\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\ntrain.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\ntest.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"16bff920a23b848a3de30741ae5a124be1f5a10a"},"cell_type":"code","source":"train['roof_waste_material'] = np.nan\ntest['roof_waste_material'] = np.nan\ntrain['electricity_other'] = np.nan\ntest['electricity_other'] = np.nan\n\ndef fill_roof_exception(x):\n    if (x['techozinc'] == 0) and (x['techoentrepiso'] == 0) and (x['techocane'] == 0) and (x['techootro'] == 0):\n        return 1\n    else:\n        return 0\n    \ndef fill_no_electricity(x):\n    if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n        return 1\n    else:\n        return 0\n\ntrain['roof_waste_material'] = train.apply(lambda x : fill_roof_exception(x),axis=1)\ntest['roof_waste_material'] = test.apply(lambda x : fill_roof_exception(x),axis=1)\ntrain['electricity_other'] = train.apply(lambda x : fill_no_electricity(x),axis=1)\ntest['electricity_other'] = test.apply(lambda x : fill_no_electricity(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b564a6552f0521581af1ee38d6040ef7ae5d2fb5","trusted":false,"collapsed":true},"cell_type":"code","source":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2695108e103c9c61088fc4c100d01bc8c0f4138c","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert the one hot fields into label encoded\ntrain, test = train_test_apply_func(train, test, convert_OHE2LE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e90e68abd266c9db808dbf336308cef7175886bd"},"cell_type":"markdown","source":"# Geo aggregates"},{"metadata":{"_uuid":"0ffc288b3829ffdb1dabf8f2e7fe518f2f520480","trusted":false,"collapsed":true},"cell_type":"code","source":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n# add some aggregates by geography\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc96d527090d9b0723049c5d763c97be5145b8c7","trusted":false,"collapsed":true},"cell_type":"code","source":"# add the number of people over 18 in each household\ntrain['num_over_18'] = 0\ntrain['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\ntrain['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain['num_over_18'] = train['num_over_18'].fillna(0)\n\ntest['num_over_18'] = 0\ntest['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\ntest['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntest['num_over_18'] = test['num_over_18'].fillna(0)\n\n# add some extra features, these were taken from another kernel\ndef extract_features(df):\n    df['adult'] = df['hogar_adul'] - df['hogar_mayor']\n    df['dependency_count'] = df['hogar_nin'] + df['hogar_mayor']\n    df['dependency'] = df['dependency_count'] / df['adult']\n    df['dependency'] = df['dependency'].replace({np.inf: 0})\n    df['child_percent'] = df['hogar_nin']/df['hogar_total']\n    df['elder_percent'] = df['hogar_mayor']/df['hogar_total']\n    df['adult_percent'] = df['hogar_adul']/df['hogar_total']\n    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n    df['r4h1_percent_in_male'] = df['r4h1'] / df['r4h3']\n    df['r4m1_percent_in_female'] = df['r4m1'] / df['r4m3']\n    df['r4h1_percent_in_total'] = df['r4h1'] / df['hhsize']\n    df['r4m1_percent_in_total'] = df['r4m1'] / df['hhsize']\n    df['r4t1_percent_in_total'] = df['r4t1'] / df['hhsize']\n    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n    df['rent_per_adult'] = df['v2a1']/df['hogar_adul']\n    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n    df['tablet_per_person_household'] = df['v18q1']/df['hhsize']\n    df['phone_per_person_household'] = df['qmobilephone']/df['hhsize']\n    df['rez_esc_escolari'] = df['rez_esc']/df['escolari']\n    df['rez_esc_r4t1'] = df['rez_esc']/df['r4t1']\n    df['rez_esc_r4t2'] = df['rez_esc']/df['r4t2']\n    df['rez_esc_r4t3'] = df['rez_esc']/df['r4t3']\n    df['rez_esc_age'] = df['rez_esc']/df['age']\n    \n    df['escolari_age'] = df['escolari']/df['age']\n    df['escolari_age_mean'] = df.groupby(\"idhogar\")[\"escolari_age\"].transform(\"mean\")\n    df['age_sum'] = df.groupby(\"idhogar\")[\"age\"].transform(\"sum\")\n    df['age_std'] = df.groupby(\"idhogar\")[\"age\"].transform(\"std\")\n    df['escolari_age_std'] = df.groupby(\"idhogar\")[\"escolari_age\"].transform(\"std\")\n    df['escolari_age_max'] = df.groupby(\"idhogar\")[\"escolari_age\"].transform(\"max\")\n    df['escolari_age_min'] = df.groupby(\"idhogar\")[\"escolari_age\"].transform(\"min\")\n    df['escolari_std'] = df.groupby(\"idhogar\")[\"escolari\"].transform(\"min\")\n    # fill nas\n    df['escolari_age'] = df['escolari_age'].fillna(0)\n    df['escolari_age_std'] = df['escolari_age_std'].fillna(-1)\n    df['age_std'] = df['age_std'].fillna(-1)\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    \nextract_features(train)    \nextract_features(test)   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ecf0572f706579d738ebcf7ecdc44415e6a069be"},"cell_type":"code","source":"train.columns[train.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6ed97e10f021b42a19c7228dfc1a52ce9de2c60","trusted":false,"collapsed":true},"cell_type":"code","source":"# drop duplicated columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'female', ]\n\ninstlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n\nneedless_cols.extend(instlevel_cols)\n\ntrain = train.drop(needless_cols, axis=1)\ntest = test.drop(needless_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e1ccce811e7a1d76282fcb8a13edf92672f834"},"cell_type":"markdown","source":"## Split the data\n\nWe split the data by household to avoid leakage, since rows belonging to the same household usually have the same target. Since we filter the data to only include heads of household this isn't technically necessary, but it provides an easy way to use the entire training data set if we want to do that.\n\nNote that after splitting the data we overwrite the train data with the entire data set so we can train on all of the data. The split_data function does the same thing without overwriting the data, and is used within the training loop to (hopefully) approximate a K-Fold split. "},{"metadata":{"_uuid":"da37e677a32477006e8468b954e23d595c82eced","trusted":false,"collapsed":true},"cell_type":"code","source":"def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n    # uncomment for extra randomness\n#     np.random.seed(seed=seed)\n    \n    train2 = train.copy()\n    \n    # pick some random households to use for the test data\n    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n    \n    # select households which are in the random selection\n    cv_idx = np.isin(households, cv_hhs)\n    X_test = train2[cv_idx]\n    y_test = y[cv_idx]\n\n    X_train = train2[~cv_idx]\n    y_train = y[~cv_idx]\n    \n    if sample_weight is not None:\n        y_train_weights = sample_weight[~cv_idx]\n        return X_train, y_train, X_test, y_test, y_train_weights\n    \n    return X_train, y_train, X_test, y_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e14a9619ca6516b225b55bf65d6a9e423d6b5fc7","trusted":false,"collapsed":true},"cell_type":"code","source":"X = train.query('parentesco1==1')\n# X = train.copy()\n\n# pull out and drop the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)\n\nnp.random.seed(seed=None)\n\ntrain2 = X.copy()\n\ntrain_hhs = train2.idhogar\n\nhouseholds = train2.idhogar.unique()\ncv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n\ncv_idx = np.isin(train2.idhogar, cv_hhs)\n\nX_test = train2[cv_idx]\ny_test = y[cv_idx]\n\nX_train = train2[~cv_idx]\ny_train = y[~cv_idx]\n\n# train on entire dataset\nX_train = train2\ny_train = y\n\ntrain_households = X_train.idhogar","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"871cd5b46da553f6062cef605acc761cddc2a8c0","trusted":false,"collapsed":true},"cell_type":"code","source":"# figure out the class weights for training with unbalanced classes\ny_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dc384aeb44db2454978df78fdbb84b2b1ff3ced"},"cell_type":"markdown","source":"# Fit a voting classifier\nDefine a derived VotingClassifier class to be able to pass `fit_params` for early stopping. Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate.\n\nThe parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"},{"metadata":{"_uuid":"d9bcfbfcfc0b5b7e3aaeb2b0c8495bd92fdf51a3","trusted":false,"collapsed":true},"cell_type":"code","source":"# drop some features which aren't used by the LGBM or have very low importance\nextra_drop_features = ['agg18_estadocivil1_MEAN',\n 'agg18_estadocivil4_COUNT',\n 'agg18_estadocivil5_COUNT',\n 'agg18_estadocivil6_COUNT',\n 'agg18_estadocivil7_COUNT',\n 'agg18_parentesco10_COUNT',\n 'agg18_parentesco11_COUNT',\n 'agg18_parentesco12_COUNT',\n 'agg18_parentesco1_COUNT',\n 'agg18_parentesco2_COUNT',\n 'agg18_parentesco3_COUNT',\n 'agg18_parentesco4_COUNT',\n 'agg18_parentesco5_COUNT',\n 'agg18_parentesco6_COUNT',\n 'agg18_parentesco7_COUNT',\n 'agg18_parentesco8_COUNT',\n 'agg18_parentesco9_COUNT',\n 'electricity_other',\n 'geo_elimbasu_LE_4',\n 'geo_energcocinar_LE_0',\n 'geo_energcocinar_LE_1',\n 'geo_energcocinar_LE_2',\n 'geo_epared_LE_0',\n 'geo_epared_LE_2',\n 'geo_hogar_mayor',\n 'geo_manual_elec_LE_2',\n 'geo_pared_LE_0',\n 'geo_pared_LE_3',\n 'geo_pared_LE_4',\n 'geo_pared_LE_5',\n 'geo_pared_LE_6',\n 'num_over_18',\n 'parentesco_LE',\n#  'rez_esc',\n#  'rez_esc_age',\n 'rez_esc_r4t2',\n 'rez_esc_r4t3']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67a0f3b7d019517ed9a40b41df318284a33a5ce1","trusted":false,"collapsed":true},"cell_type":"code","source":"xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8b3c999be32c944dcbde7acff1c085424561161f"},"cell_type":"code","source":"opt_parameters_1 = {'max_depth':35, 'learning_rate':0.125, 'silent':1, \"n_estimators\": 325, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.4 }\nopt_parameters_2 = {'max_depth':35, 'learning_rate':0.14, 'silent':1, \"n_estimators\": 275, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.25, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\nopt_parameters_3 = {'max_depth':35, 'learning_rate':0.15, 'silent':1, \"n_estimators\": 300, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.94, 'colsample_bytree': 0.9, 'reg_lambda': 0.375 }\nopt_parameters_4 = {'max_depth':35, 'learning_rate':0.13, 'silent':1, \"n_estimators\": 350, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.88, 'reg_lambda': 0.325 }\n\nxgb_param_list = [opt_parameters_1,  opt_parameters_2,  opt_parameters_3, opt_parameters_4]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"629cf88dd0596a98d58487495e5b096636e2586a","trusted":false,"collapsed":true},"cell_type":"code","source":"# since XGB minimizes the metric we need to subtract the F1 score from 1\ndef evaluate_macroF1_xgb(predictions, truth):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.argmax(axis=1)\n    truth = truth.get_label()\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', 1-f1) \n\nfit_params={\"early_stopping_rounds\":150,\n            \"eval_metric\" : evaluate_macroF1_xgb, \n            \"eval_set\" : [(X_test,y_test)],\n            'verbose': False,\n           }\n\nfit_params['verbose'] = 50","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"101a2fd45bbbe6c7fb351513803550d4edeef2b3","trusted":false,"collapsed":true},"cell_type":"code","source":"np.random.seed(100)\n\ndef _parallel_fit_estimator(estimator1, X, y, drop_cols=[], use_cv=True, sample_weight=None, threshold=True, **fit_params):\n    estimator = clone(estimator1)\n    \n    # randomly split the data so we have a test set for early stopping\n    if use_cv:\n        if sample_weight is not None:\n            X_train, y_train, X_cv, y_cv, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n        else:\n            X_train, y_train, X_cv, y_cv = split_data(X, y, None, households=train_households)\n    else:\n        X_train = X\n        y_train = y\n        X_cv = X\n        y_cv = y\n        \n    # update the fit params with our new split\n    if isinstance(estimator1, xgb.XGBClassifier):\n        fit_params[\"eval_set\"] = [(X_cv,y_cv)]\n    else:\n        fit_params[\"eval_set\"] = [(X_train, y_train), (X_cv,y_cv)]\n        \n    # fit the estimator\n    if sample_weight is not None:\n        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n            estimator.fit(X_train, y_train)\n        else:\n            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n    else:\n        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n            estimator.fit(X_train, y_train)\n        else:\n            _ = estimator.fit(X_train, y_train, **fit_params)\n    \n    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n        best_cv_round = np.argmax(estimator.evals_result_['valid']['macroF1'])\n        best_cv = np.max(estimator.evals_result_['valid']['macroF1'])\n        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n    else:\n        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n        best_cv = f1_score(y_cv, estimator.predict(X_cv), average=\"macro\")\n        print(\"Train F1:\", best_train)\n        print(\"Test F1:\", best_cv)\n        \n    # reject some estimators based on their performance on train and test sets\n    if threshold:\n        # if the valid score is very high we'll allow a little more leeway with the train scores\n        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n            return estimator\n\n        # else recurse until we get a better one\n        else:\n            print(\"Unacceptable!!! Trying again...\")\n            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n    \n    else:\n        return estimator\n    \nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, drop_cols=[], split_data=True, sample_weight=None, threshold=True, **fit_params):\n        \n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y, drop_cols=drop_cols, use_cv=split_data,\n                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b909c2aa273651b7bb57c69b939760f14f38f7","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"clfs = []\nfor i in range(5):\n    clf = xgb.XGBClassifier(random_state=217+i, n_jobs=4, **xgb_param_list[i % len(xgb_param_list)])\n    \n    clfs.append(('xgb{}'.format(i), clf))\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\ndel(clfs)\n\n#Train the final model with learning rate decay\n_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, drop_cols=[], threshold=False, **fit_params)\n\nclf_final = vc.estimators_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a6c59e29b54c186c23e2347d7f691501bcb27c78"},"cell_type":"code","source":"# l2 used features\nglobal_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'soft'\nglobal_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\nvc.voting = 'hard'\nglobal_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a single XGBM Classifier: {:.4f}'.format(global_score))\nprint('Validation score of a VotingClassifier on XGBMs with soft voting: {:.4f}'.format(global_score_soft))\nprint('Validation score of a VotingClassifier on XGBMs with hard voting: {:.4f}'.format(global_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b5c068fb785f65a670b810deb9db974ba768f1d4"},"cell_type":"code","source":"_ = feature_importance(vc.estimators_[0], X_train.drop(xgb_drop_cols, axis=1), display_results=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"527b58d849cc1282909f15596cd5441ef4cac93d","trusted":false,"collapsed":true},"cell_type":"code","source":"# see which features are not used by ANY models\nuseless_features = []\ndrop_features = set()\ncounter = 0\nfor est in vc.estimators_:\n    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n    useless_features.append(unused_features)\n    if counter == 0:\n        drop_features = set(unused_features)\n    else:\n        drop_features = drop_features.intersection(set(unused_features))\n    counter += 1\n    \ndrop_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0b91e2e5a118048e27da47d7720996f8a682d54"},"cell_type":"markdown","source":"### LGB"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c3c3bf0dad2cc5d7529e5a155b80edc140c93ca1"},"cell_type":"code","source":"extra_drop_features = ['agg18_estadocivil1_MEAN',\n 'agg18_estadocivil4_COUNT',\n 'agg18_estadocivil5_COUNT',\n 'agg18_estadocivil6_COUNT',\n 'agg18_estadocivil7_COUNT',\n 'agg18_parentesco10_COUNT',\n 'agg18_parentesco10_MEAN',\n 'agg18_parentesco11_COUNT',\n 'agg18_parentesco11_MEAN',\n 'agg18_parentesco12_COUNT',\n 'agg18_parentesco12_MEAN',\n 'agg18_parentesco1_COUNT',\n 'agg18_parentesco2_COUNT',\n 'agg18_parentesco3_COUNT',\n 'agg18_parentesco4_COUNT',\n 'agg18_parentesco4_MEAN',\n 'agg18_parentesco5_COUNT',\n 'agg18_parentesco6_COUNT',\n 'agg18_parentesco6_MEAN',\n 'agg18_parentesco7_COUNT',\n 'agg18_parentesco8_COUNT',\n 'agg18_parentesco8_MEAN',\n 'agg18_parentesco9_COUNT',]\n\nlgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f61234be3f60a791b3e9f8dfe0ba98d030318921"},"cell_type":"code","source":"lgb_parameters_1 = {'max_depth': -1, 'learning_rate': 0.125, 'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 16, 'subsample': 0.94, 'reg_lambda': 0.425, \"n_estimators\": 5000}\nlgb_parameters_2 = {'max_depth': 50, 'learning_rate': 0.11, 'colsample_bytree': 0.95, 'min_child_samples': 90, 'num_leaves': 25, 'subsample': 0.94, 'reg_lambda': 0.34, \"n_estimators\": 3000}\nlgb_parameters_3 = {'max_depth': 35, 'learning_rate': 0.12, 'colsample_bytree': 0.88, 'min_child_samples': 56, 'num_leaves': 20, 'subsample': 0.95, 'reg_lambda': 0.375, \"n_estimators\": 4000}\nlgb_parameters_4 = {'max_depth': 20, 'learning_rate': 0.13, 'colsample_bytree': 0.89, 'min_child_samples': 90, 'num_leaves': 14, 'subsample': 0.96, 'reg_lambda': 0.39, \"n_estimators\": 2500}\nlgb_parameters_5 = {'max_depth': 25, 'learning_rate': 0.14, 'colsample_bytree': 0.93, 'min_child_samples': 70, 'num_leaves': 17, 'subsample': 0.95, 'reg_lambda': 0.35, \"n_estimators\": 4500 }\n\nlgb_param_list = [lgb_parameters_1, lgb_parameters_2, lgb_parameters_3, lgb_parameters_4, lgb_parameters_5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8598d0fba28d1769a87b8f4561d9c74e6e1b2cf8"},"cell_type":"code","source":"opt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 16, 'subsample': 0.94, 'reg_lambda': 0.5, }\nopt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 25, 'subsample': 0.94, 'reg_lambda': 0.5, }\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nfit_params={\"early_stopping_rounds\":150,\n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n            'eval_names': ['train', 'valid'],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['verbose'] = 200\nfit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"0a2b92ee13cad5c8621fbc5011b49c67340f2c20"},"cell_type":"code","source":"clfs = []\nfor i in range(15):\n    clf = lgb.LGBMClassifier(objective='multiclass',\n                             random_state=314+i, silent=True, metric='None', \n                             n_jobs=4, class_weight='balanced', **lgb_param_list[i % len(lgb_param_list)])\n    \n    param_set = i % len(lgb_param_list)\n    clf.set_params(**lgb_param_list[param_set])\n    clfs.append(('lgbm{}'.format(i), clf))\n    \nvc3 = VotingClassifierLGBM(clfs, voting='soft')\ndel clfs\n\n#Train the final model with learning rate decay\n_ = vc3.fit(X_train.drop(lgb_drop_cols, axis=1), y_train, threshold=True, **fit_params)\n\nclf_final = vc3.estimators_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1052700cdddb32a3a4d2592dceb1a04a6c9e8657"},"cell_type":"code","source":"# l1 used features\nglobal_lgb_score = f1_score(y_test, clf_final.predict(X_test.drop(lgb_drop_cols, axis=1)), average='macro')\nvc3.voting = 'soft'\nglobal_lgb_score_soft = f1_score(y_test, vc3.predict(X_test.drop(lgb_drop_cols, axis=1)), average='macro')\nvc3.voting = 'hard'\nglobal_lgb_score_hard = f1_score(y_test, vc3.predict(X_test.drop(lgb_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a single LGBM Classifier: {:.4f}'.format(global_lgb_score))\nprint('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_lgb_score_soft))\nprint('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_lgb_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"57e9fa1724df3d947830f9fe704b8b20e57ba2d0"},"cell_type":"code","source":"# see which features are not used by ANY models\nuseless_features = []\ndrop_features = set()\ncounter = 0\nfor est in vc3.estimators_:\n    ranked_features, unused_features = feature_importance(est, X_train.drop(lgb_drop_cols, axis=1), display_results=False)\n    useless_features.append(unused_features)\n    if counter == 0:\n        drop_features = set(unused_features)\n    else:\n        drop_features = drop_features.intersection(set(unused_features))\n    counter += 1\n    \ndrop_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bfa630281d5326d2cce91d2b89f9e5f41d5f92d8"},"cell_type":"code","source":"_ = feature_importance(vc3.estimators_[0], X_train.drop(lgb_drop_cols, axis=1), display_results=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e78eb892f60a52240aee2b15beaa9f9dfd59994"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_uuid":"8a98ed5c6da8cd601a4ff2842c4440c16dcca8b8","trusted":false,"collapsed":true},"cell_type":"code","source":"et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN',\n       'fe_rent_per_person', 'fe_rent_per_room', 'fe_tablet_adult_density',\n       'fe_tablet_density', 'r4h1_percent_in_male', 'r4m1_percent_in_female',\n       'rent_per_adult', 'rez_esc_escolari', 'rez_esc_r4t1', 'rez_esc_age']# + ['parentesco_LE', 'rez_esc', 'rez_esc_r4t2', 'rez_esc_r4t3']\n\net_drop_cols.extend([\"idhogar\", \"parentesco1\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9126842ab246cdd59a7dc69300ed944f89dc6b26"},"cell_type":"code","source":"rf_params_1 = {\"max_depth\": None, \"min_impurity_decrease\": 1e-3, \"min_samples_leaf\": 2, \"min_samples_split\": 3, \"n_estimators\":750 }\nrf_params_2 = {\"max_depth\": 25, \"min_impurity_decrease\": 1e-3, \"min_samples_leaf\": 2, \"min_samples_split\": 3, \"n_estimators\":700 }\nrf_params_3 = {\"max_depth\": 31, \"min_impurity_decrease\": 1e-4, \"min_samples_leaf\": 3, \"min_samples_split\": 2, \"n_estimators\":850 }\nrf_params_4 = {\"max_depth\": 32, \"min_impurity_decrease\": 1e-3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"n_estimators\":950 }\nrf_params_5 = {\"max_depth\": 29, \"min_impurity_decrease\": 1e-3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"n_estimators\":800 }\n\nrf_param_list = [rf_params_1, rf_params_2, rf_params_3, rf_params_4, rf_params_5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa255a424ccbd3839015c82777963dc6b79d8cde","trusted":false,"collapsed":true},"cell_type":"code","source":"# do the same thing for some extra trees classifiers\nets = []    \nfor i in range(10):\n    rf_params = rf_param_list[i % len(rf_param_list)]\n    rf = RandomForestClassifier(random_state=217+i, n_jobs=4, verbose=0, class_weight=\"balanced\", **rf_params)\n    ets.append(('rf{}'.format(i), rf))   \n\nvc2 = VotingClassifier(ets, voting='soft')    \n_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0516decfa6fbc515a5f722e35a8608f16a0f6ab3"},"cell_type":"code","source":"# all used non-null features + greater depth\nvc2.voting = 'soft'\nglobal_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\nvc2.voting = 'hard'\nglobal_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a VotingClassifier on RF with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\nprint('Validation score of a VotingClassifier on RF with hard voting strategy: {:.4f}'.format(global_rf_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6c19041bdeff286ddf54c4874790fce115e32823"},"cell_type":"code","source":"# all used non-null features\nvc2.voting = 'soft'\nglobal_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\nvc2.voting = 'hard'\nglobal_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a VotingClassifier on RF with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\nprint('Validation score of a VotingClassifier on RF with hard voting strategy: {:.4f}'.format(global_rf_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6e833efcb795d93cc60dca8972b1e8cdec209115"},"cell_type":"code","source":"# all non-null test features\nvc2.voting = 'soft'\nglobal_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\nvc2.voting = 'hard'\nglobal_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n\nprint('Validation score of a VotingClassifier on RF with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\nprint('Validation score of a VotingClassifier on RF with hard voting strategy: {:.4f}'.format(global_rf_score_hard))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a85b976aeda6675152ec31c2576fef487716ff3","trusted":false,"collapsed":true},"cell_type":"code","source":"# see which features are not used by ANY models\nuseless_features = []\ndrop_features = set()\ncounter = 0\nfor est in vc2.estimators_:\n    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n    useless_features.append(unused_features)\n    if counter == 0:\n        drop_features = set(unused_features)\n    else:\n        drop_features = drop_features.intersection(set(unused_features))\n    counter += 1\n    \ndrop_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ef7b8f31b830d160fbe15d2b4de6c4e2eb85e0"},"cell_type":"markdown","source":"### Combine The Voters"},{"metadata":{"_uuid":"33e3f7da04fc8d6099b94885bb6a6e259e56a919","trusted":false,"collapsed":true},"cell_type":"code","source":"def combine_voters(data, weights=[0.3, 0.3, 0.4], voting=\"soft\"):\n    # do soft voting with both classifiers\n\n    vc.voting=\"soft\"\n    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n    vc2.voting=\"soft\"\n    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n    vc3.voting=\"soft\"\n    vc3_probs = vc3.predict_proba(data.drop(lgb_drop_cols, axis=1))\n\n    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1]) + (vc3_probs * weights[2])\n    predictions = np.argmax(final_vote, axis=1)\n    \n    if voting==\"hard\":\n        vc.voting=\"hard\"\n        vc1_preds = vc.predict(data.drop(xgb_drop_cols, axis=1))\n        vc2.voting=\"hard\"\n        vc2_preds = vc2.predict(data.drop(et_drop_cols, axis=1))\n        vc3.voting=\"hard\"\n        vc3_preds = vc3.predict(data.drop(lgb_drop_cols, axis=1))\n        \n        # cstack the predictions, add the soft votes in, and get the mode\n        predictions = np.array(stats.mode(np.column_stack([vc1_preds, vc2_preds, vc3_preds, predictions]), axis=1)[0])[:,0]\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cd61449e198aca7bef88120dda16b736f1fca7b","trusted":false,"collapsed":true},"cell_type":"code","source":"# equal weight\ncombo_preds = combine_voters(X_test, weights=[0.33, 0.34, 0.33])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0bb21e2a3b0167e74e73b2dd96ddc82bcd94f926"},"cell_type":"code","source":"# hard voting\ncombo_preds = combine_voters(X_test, weights=[0.4, 0.2, 0.4], voting=\"hard\")\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"324c4cf008dc0dadd6c74f2bf4ed0a77e0314c2f","trusted":false,"collapsed":true},"cell_type":"code","source":"# xgb weighted higher\ncombo_preds = combine_voters(X_test, weights=[0.4, 0.3, 0.3])\nglobal_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"070fc3eeb109d98a71dfe9f35b217ae8646935ae","trusted":false,"collapsed":true},"cell_type":"code","source":"# rf weighted higher\ncombo_preds = combine_voters(X_test, weights=[0.3, 0.4, 0.3])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1e83aa02b46facafb31ee437aadc45622265c54b"},"cell_type":"code","source":"# rf weighted higher\ncombo_preds = combine_voters(X_test, weights=[0.4, 0.5, 0.2])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2c3ed31dfa35394e7550d78edf1052f6e0eb6bac"},"cell_type":"code","source":"# lgb weighted higher\ncombo_preds = combine_voters(X_test, weights=[0.3, 0.3, 0.4])\nglobal_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\nglobal_combo_score_soft","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78749eec7f69bcc8c587278a2c1a43ac8b5832e3"},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"_uuid":"32bfd69fe130005cb88865399c460ac00c7b1574","trusted":false,"collapsed":true},"cell_type":"code","source":"y_subm = pd.DataFrame()\ny_subm['Id'] = test_ids","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"718054937aa849c23ca7c65483f8a52de6f6fd49","trusted":false,"collapsed":true},"cell_type":"code","source":"vc.voting = 'soft'\ny_subm_xgb = y_subm.copy(deep=True)\ny_subm_xgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n\nvc2.voting = 'soft'\ny_subm_rf = y_subm.copy(deep=True)\ny_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n\nvc3.voting = 'soft'\ny_subm_lgb = y_subm.copy(deep=True)\ny_subm_lgb['Target'] = vc3.predict(test.drop(lgb_drop_cols, axis=1)) + 1\n\ny_subm_ens = y_subm.copy(deep=True)\ny_subm_ens['Target'] = combine_voters(test) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8258f28127f235e427a25f6824566a23df61b8af","trusted":false,"collapsed":true},"cell_type":"code","source":"from datetime import datetime\nnow = datetime.now()\n\nsub_file_xgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_lgb = 'submission_soft_LGB_{:.4f}_{}.csv'.format(global_lgb_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\nsub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n\ny_subm_xgb.to_csv(sub_file_xgb, index=False)\ny_subm_lgb.to_csv(sub_file_lgb, index=False)\ny_subm_rf.to_csv(sub_file_rf, index=False)\ny_subm_ens.to_csv(sub_file_ens, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c6d7f945dec95a777b4221c5fe217c3eea24100","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}