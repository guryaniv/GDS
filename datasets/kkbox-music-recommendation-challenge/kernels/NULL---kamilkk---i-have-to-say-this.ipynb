{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["# 1. Introduction\n", "This will be the longest EDA you've ever seen...\n", "\n", "Let's load some libraries and the data."], "metadata": {}, "cell_type": "markdown"}, {"source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')"], "metadata": {"_cell_guid": "96eb57fb-a4c9-480d-9c89-94f0a7d54656", "_uuid": "f6d03c045dbe1a758efba726e8d515ad4cea871e"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# 2. Adding song year to our datasets"], "metadata": {}, "cell_type": "markdown"}, {"source": ["songs_extra = pd.read_csv('../input/song_extra_info.csv')\n", "\n", "def isrc_to_year(isrc):\n", "    if type(isrc) == str:\n", "        if int(isrc[5:7]) > 17:\n", "            return 1900 + int(isrc[5:7])\n", "        else:\n", "            return 2000 + int(isrc[5:7])\n", "    else:\n", "        return np.nan\n", "        \n", "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n", "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n", "\n", "train = train.merge(songs_extra, on = 'song_id', how = 'left')\n", "test = test.merge(songs_extra, on = 'song_id', how = 'left')"], "metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# 3. Let's count what fraction of songs was released in 2017\n", "We will use rolling mean with a window of 50000 for this purpose."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train['2017_songs_frac'] = (train['song_year'] == 2017).rolling(window = 50000, center = True).mean()\n", "test['2017_songs_frac'] = (test['song_year'] == 2017).rolling(window = 50000, center = True).mean()"], "metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# 4. Let's plot it against train and test index values"], "metadata": {}, "cell_type": "markdown"}, {"source": ["plt.figure()\n", "plt.plot(train.index.values, train['2017_songs_frac'], '-',\n", "        train.shape[0] + test.index.values, test['2017_songs_frac'], '-');"], "metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# 5. Yes! Data is chronologically ordered!\n", "I think everyone should be aware of this, maybe even organizers should confirm this. It does help to establish a pretty good (as for the time series problem) validation set - you just leave last 2.5 mln (length of the test data) rows of the training data for the validation. It helped me to get 0.69 score without even taking a time series approach to this problem."], "metadata": {}, "cell_type": "markdown"}]}