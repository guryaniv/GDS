{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Pipeline is a powerful tool to standardise your operations and chain then in a sequence, make unions and finetune parameters. In this example we will:\n* create a simple pipeline of default sklearn estimators/transformers\n* create our own estimator/transformer\n* create a pipeline which will process features in a different way and then join them horizontally\n* finetune some parameters"},{"metadata":{"collapsed":true,"_uuid":"fe97cb2c16e7cba66c8c34e87b34368de5897e75","_cell_guid":"3adf2a49-3ed9-4b69-afde-de4881aaa096","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import sparse\n\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\nx = df['comment_text'].values[:5000]\ny = df['toxic'].values[:5000]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e066a6f8ef02480d45d7f9bed4df215d4ef50caf","_cell_guid":"29546f9f-2688-4664-956f-73f7963cc7ab","trusted":false},"cell_type":"code","source":"# default params\nscoring='roc_auc'\ncv=3\nn_jobs=-1\nmax_features = 2500","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b523f647558305f3291f54c472b42bda04679ce4","_cell_guid":"a4fda56f-40a5-4b09-8a86-1593d28c12ef"},"cell_type":"markdown","source":"Simple pipelines of default sklearn TfidfVectorizer to prepare features and Logistic Reegression to make predictions. "},{"metadata":{"_uuid":"2b053fe5405199e12e9e83a37a8136d511f4deaa","_cell_guid":"fd6a6785-75d6-41f3-b2da-19cdfcbcba5f","trusted":false,"collapsed":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=max_features)\nlr = LogisticRegression()\np = Pipeline([\n    ('tfidf', tfidf),\n    ('lr', lr)\n])\n\ncross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8b9e8588cdf43541eeace1040be1d8290743e86","_cell_guid":"fad2f328-700f-4252-ad18-e6426d5f6bd9"},"cell_type":"markdown","source":"Lets create or own Estimator to reproduce Jeremy`s notebook in pipelines. This estimator is created with sklearn BaseEstimator class and needs to have fit and transform methods. First Pipeline callss fit methods to learn your dataset and then calls transform to apply knowledge and does some transformations."},{"metadata":{"collapsed":true,"_uuid":"f3e312525331193e2607cd4949c777899fbc95a8","_cell_guid":"6d13a924-89e4-4d54-b65f-798389fb9117","trusted":false},"cell_type":"code","source":"class NBFeaturer(BaseEstimator):\n    def __init__(self, alpha):\n        self.alpha = alpha\n    \n    def preprocess_x(self, x, r):\n        return x.multiply(r)\n    \n    def pr(self, x, y_i, y):\n        p = x[y==y_i].sum(0)\n        return (p+self.alpha) / ((y==y_i).sum()+self.alpha)\n\n    def fit(self, x, y=None):\n        self._r = sparse.csr_matrix(np.log(self.pr(x,1,y) / self.pr(x,0,y)))\n        return self\n    \n    def transform(self, x):\n        x_nb = self.preprocess_x(x, self._r)\n        return x_nb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79425633291ff8adba286fc423f5511f8dcaa30","_cell_guid":"4f854959-d846-4aff-bb17-326ec45afbb8","trusted":false,"collapsed":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(max_features=max_features)\nlr = LogisticRegression()\nnb = NBFeaturer(1)\np = Pipeline([\n    ('tfidf', tfidf),\n    ('nb', nb),\n    ('lr', lr)\n])\n\ncross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7fd499f68f9ae2c325b26b093add73cec6e568e","_cell_guid":"9aecfcc7-aafe-46ac-aeac-f7b4741e3285"},"cell_type":"markdown","source":"Lets add one more custom Estimator to our pipeline, called Lemmatizer"},{"metadata":{"collapsed":true,"_uuid":"7b2b18feec6f72069c638c577be76967ef72de56","_cell_guid":"79f83339-5bf4-44cf-a09a-1577f2033cf2","trusted":false},"cell_type":"code","source":"class Lemmatizer(BaseEstimator):\n    def __init__(self):\n        self.l = WordNetLemmatizer()\n        \n    def fit(self, x, y=None):\n        return self\n    \n    def transform(self, x):\n        x = map(lambda r:  ' '.join([self.l.lemmatize(i.lower()) for i in r.split()]), x)\n        x = np.array(list(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f8a3e84c330c42e9fa542a74fc0c85f58bed5a5","_cell_guid":"41fcfc3b-8c4c-483c-9891-2445c0a1f0cd","trusted":false,"collapsed":true},"cell_type":"code","source":"lm = Lemmatizer()\ntfidf = TfidfVectorizer(max_features=max_features)\nlr = LogisticRegression()\nnb = NBFeaturer(1)\np = Pipeline([\n    ('lm', lm),\n    ('tfidf', tfidf),\n    ('nb', nb),\n    ('lr', lr)\n])\n\ncross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de7ba14d6a4dc97f40d5b3c0e4862b2529666e9a","_cell_guid":"9fbe8d22-a843-459d-9b65-c90af53e68b3"},"cell_type":"markdown","source":"Pipelines also allow you to process different features in a different way and then concat the result. FeatureUnion halps us with this. Lets create additional tfidf vectorizer for chars and join its results with words vectorizer."},{"metadata":{"_uuid":"2e85df4ba89fec86a4db4fd1a699421c4b208ed2","_cell_guid":"47ab51ae-9a7a-4b1e-b01d-bfd5ea20eb62","trusted":false,"collapsed":true},"cell_type":"code","source":"max_features = 2500\nlm = Lemmatizer()\ntfidf_w = TfidfVectorizer(max_features=max_features, analyzer='word')\ntfidf_c = TfidfVectorizer(max_features=max_features, analyzer='char')\nlr = LogisticRegression()\nnb = NBFeaturer(1)\np = Pipeline([\n    ('lm', lm),\n    ('wc_tfidfs', \n         FeatureUnion([\n            ('tfidf_w', tfidf_w), \n            ('tfidf_c', tfidf_c), \n         ])\n    ),\n    ('nb', nb),\n    ('lr', lr)\n])\n\ncross_val_score(estimator=p, X=x, y=y, scoring=scoring, cv=cv, n_jobs=n_jobs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39007f3683aad3616f3815e56276acddb94e63db","_cell_guid":"ad916d6a-811c-4e1c-be13-03b8c5eb2ab5"},"cell_type":"markdown","source":"Who does not like finetuning? Lets make it simple with pipelines  and GridSearchCV/RandomizedSearchCV. "},{"metadata":{"_uuid":"89dfc5aa716aa9c0b0ae6983fd8e49495e84433c","_cell_guid":"d66d2f18-f725-4ddf-9d78-94ffa5602bd4","trusted":false,"collapsed":true},"cell_type":"code","source":"param_grid = [{\n    'wc_tfidfs__tfidf_w__max_features': [2500], \n    'wc_tfidfs__tfidf_c__stop_words': [2500, 5000],\n    'lr__C': [3.],\n}]\n\ngrid = GridSearchCV(p, cv=cv, n_jobs=n_jobs, param_grid=param_grid, scoring=scoring, \n                            return_train_score=False, verbose=1)\ngrid.fit(x, y)\ngrid.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab36753d9d4d4fce580b418182cfd8da4fdb02f8","_cell_guid":"81ef5394-a195-401e-9b75-ee329e0d7f68","trusted":false,"collapsed":true},"cell_type":"code","source":"param_grid = [{\n    'wc_tfidfs__tfidf_w__max_features': [2500, 5000, 10000], \n    'wc_tfidfs__tfidf_c__stop_words': [2500, 5000, 10000],\n    'lr__C': [1., 3., 4.],\n}]\n\ngrid = RandomizedSearchCV(p, cv=cv, n_jobs=n_jobs, param_distributions=param_grid[0], n_iter=1, \n                          scoring=scoring, return_train_score=False, verbose=1)\ngrid.fit(x, y)\ngrid.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef7c893fafab6c10d4d9fee86c6f947cd03ebf47","_cell_guid":"47e2def8-42b5-4741-baed-41a52e5fc864"},"cell_type":"markdown","source":"Useful links:\n* http://scikit-learn.org/stable/modules/pipeline.html#pipeline\n* https://github.com/scikit-learn-contrib/project-template"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}