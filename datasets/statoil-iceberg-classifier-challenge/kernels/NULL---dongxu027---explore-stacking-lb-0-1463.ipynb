{"cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "518b490c58f98a59d25d8d5d8c39dca0673564e1", "collapsed": true, "_cell_guid": "28a96b13-fb54-43af-aeb9-ccb40526ec1f"}, "source": ["import os\n", "import numpy as np \n", "import pandas as pd \n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "outputs": []}, {"metadata": {"_uuid": "9db24b56f89561784a4118c9faf4aca6cfdaea3a", "_cell_guid": "b4d21b76-54f0-42db-a60f-5cba96b239bd"}, "cell_type": "markdown", "source": ["First thing first@\n", "# Credits to the following awesome authors and kernels\n", "\n", "Author: QuantScientist    \n", "File: sub_200_ens_densenet.csv     \n", "Link: https://www.kaggle.com/solomonk/pytorch-cnn-densenet-ensemble-lb-0-1538     \n", "\n", "\n", "Author: wvadim     \n", "File: sub_TF_keras.csv     \n", "Link: https://www.kaggle.com/wvadim/keras-tf-lb-0-18     \n", "\n", "\n", "Author: Ed Miller    \n", "File: sub_fcn.csv    \n", "Link: https://www.kaggle.com/bluevalhalla/fully-convolutional-network-lb-0-193     \n", "\n", "\n", "Author: Chia-Ta Tsai    \n", "File: sub_blend009.csv    \n", "Link: https://www.kaggle.com/cttsai/ensembling-gbms-lb-203    \n", "\n", "\n", "Author: DeveshMaheshwari    \n", "File: sub_keras_beginner.csv    \n", "Link: https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d       \n", "\n", "### Without their truly dedicated efforts, this notebook will not be possible.     "]}, {"metadata": {"_uuid": "aae1d396e2fad44a15fcb8e970c5affa8fb6ddc6", "_cell_guid": "1840df41-7524-4305-93d9-423a2a6fe5bd"}, "cell_type": "markdown", "source": ["# Data Load"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1fa9dd2434873dc5bc918b102a3a69649d0f35c7", "collapsed": true, "_cell_guid": "c978c838-8a87-45ef-a118-b6bd565bdb2e"}, "source": ["sub_path = \"../input/statoil-iceberg-submissions\"\n", "all_files = os.listdir(sub_path)\n", "\n", "# Read and concatenate submissions\n", "outs = [pd.read_csv(os.path.join(sub_path, f), index_col=0) for f in all_files]\n", "concat_sub = pd.concat(outs, axis=1)\n", "cols = list(map(lambda x: \"is_iceberg_\" + str(x), range(len(concat_sub.columns))))\n", "concat_sub.columns = cols\n", "concat_sub.reset_index(inplace=True)\n", "concat_sub.head()\n"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "37b39229e34b71032d8d2371c6b731ed452b8eb2", "collapsed": true, "_cell_guid": "a193cac9-e544-4749-8622-2c0a7d882193"}, "source": ["# check correlation\n", "concat_sub.corr()"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "07cdbb447c11008557c1773e464d08f798d8b2b2", "collapsed": true, "_cell_guid": "c7be9be8-b999-42d9-a0cc-6bf2a035b25b"}, "source": ["# get the data fields ready for stacking\n", "concat_sub['is_iceberg_max'] = concat_sub.iloc[:, 1:6].max(axis=1)\n", "concat_sub['is_iceberg_min'] = concat_sub.iloc[:, 1:6].min(axis=1)\n", "concat_sub['is_iceberg_mean'] = concat_sub.iloc[:, 1:6].mean(axis=1)\n", "concat_sub['is_iceberg_median'] = concat_sub.iloc[:, 1:6].median(axis=1)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d412b01db3dd513eef9158bfaaebe96617e79d5b", "collapsed": true, "_cell_guid": "2b115d55-48f4-43c6-9b22-dbd8cd5a28a8"}, "source": ["# set up cutoff threshold for lower and upper bounds, easy to twist \n", "cutoff_lo = 0.8\n", "cutoff_hi = 0.2"], "outputs": []}, {"metadata": {"_uuid": "698574b4531ce5ec1c59d5afaf451392169af5e0", "_cell_guid": "d0493bc1-1d92-4bc7-aa84-107fcb6d7324"}, "cell_type": "markdown", "source": ["# Mean Stacking"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a6642e2884dacc8c5095413e6a0945c6791ee1b5", "collapsed": true, "_cell_guid": "f1169e63-b019-4a39-87b5-8fa1d0488f09"}, "source": ["concat_sub['is_iceberg'] = concat_sub['is_iceberg_mean']\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_mean.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "a24eecfd51f4f6665d751f3f1c126e46a44426bd", "_cell_guid": "64a7aeea-ac14-4ae8-ac62-0c58b13c1f9e"}, "cell_type": "markdown", "source": ["**LB 0.1698** , decent first try - still some gap comparing with our top-line model performance in stack."]}, {"metadata": {"_uuid": "d213635b179fc8d07a6985d257c8c3e0007e0f7a", "_cell_guid": "103f2414-04a8-40bd-8ffc-4e77e510e023"}, "cell_type": "markdown", "source": ["# Median Stacking"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "76fc5734615b45bf6234df1f450c9a24ca518834", "collapsed": true, "_cell_guid": "9d5af2c0-1cc5-4adc-9153-d20d19c69bd6"}, "source": ["concat_sub['is_iceberg'] = concat_sub['is_iceberg_median']\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_median.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "a1a1754ba1c9ca956da71920dcc5bf2f0ee78172", "_cell_guid": "3da1db01-1922-4d34-ae02-1d3acfa59fca"}, "cell_type": "markdown", "source": ["**LB 0.1575**, very close with our top-line model performance, but we want to see some improvement at least."]}, {"metadata": {"_uuid": "caa0cb178c1f4921cb7c5b6552bfe4e0fb91475e", "_cell_guid": "fba2a588-19a4-41fd-a495-af6a3a551777"}, "cell_type": "markdown", "source": ["# PushOut + Median Stacking \n", "\n", "Pushout strategy is a bit agressive given what it does..."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e6038b30485244cf144ede75fb3ecab55afa3f84", "collapsed": true, "_cell_guid": "f8646a39-f2cc-483a-912b-46af12b5de64"}, "source": ["concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), 1, \n", "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n", "                                             0, concat_sub['is_iceberg_median']))\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_pushout_median.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "ed7b3420cf44929c977970605c9c231714926e0e", "_cell_guid": "f016ad2f-ddc9-4182-a288-32f4dcb466d3"}, "cell_type": "markdown", "source": ["**LB 0.1940**, not very impressive results given the base models in the pipeline..."]}, {"metadata": {"_uuid": "9244a9d9ddce162fa7ddd7d32e271097b0b405df", "_cell_guid": "26fa22e0-4f00-455b-8328-3e3cdf34adfb"}, "cell_type": "markdown", "source": ["# MinMax + Mean Stacking\n", "\n", "MinMax seems more gentle and it outperforms the previous one given its peformance score."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4ffd0c65d59a03ee04d5452b853efac717f3f3f7", "collapsed": true, "_cell_guid": "7634aaa4-2466-45b1-afbf-32dedb8691a4"}, "source": ["concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n", "                                    concat_sub['is_iceberg_max'], \n", "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n", "                                             concat_sub['is_iceberg_min'], \n", "                                             concat_sub['is_iceberg_mean']))\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_mean.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "c4da0e0b2530daf4fdcb362980ad9fbfce5b3476", "_cell_guid": "de9e7a52-2a3e-440a-9056-93ed4d9f87f9"}, "cell_type": "markdown", "source": ["**LB 0.1622**, need to stack with Median to see the results."]}, {"metadata": {"_uuid": "95b5ccf95505b498dede6966f2a899c376d855dd", "_cell_guid": "61831c59-a583-45d6-9bba-1102d1f80862"}, "cell_type": "markdown", "source": ["# MinMax + Median Stacking "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d4b7e91489d86452dc6d13340109e07da40f9fa9", "collapsed": true, "_cell_guid": "8aa75397-da75-4d93-916e-d88f464e13fe"}, "source": ["concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n", "                                    concat_sub['is_iceberg_max'], \n", "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n", "                                             concat_sub['is_iceberg_min'], \n", "                                             concat_sub['is_iceberg_median']))\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_median.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "1abbdcd40aca61b375ee4dd0e3e90b814cecceba", "_cell_guid": "b9d1df47-6c0b-4bca-9357-ceaf61746171"}, "cell_type": "markdown", "source": ["**LB 0.1488** - **Great!** This is an improvement to our top-line model performance (LB 0.1538). But can we do better?"]}, {"metadata": {"_uuid": "935499734e508b98e7d694606bd2851a4b3cbce5", "_cell_guid": "307ebff0-d690-4dbf-8496-3fc8340bcc60"}, "cell_type": "markdown", "source": ["# MinMax + BestBase Stacking"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8e3f2ac12368eac5fc2b69e9e899873dd33adef8", "collapsed": true, "_cell_guid": "59f60095-f1ff-4c26-8d51-e0111e80f3ba"}, "source": ["# load the model with best base performance\n", "sub_base = pd.read_csv('../input/statoil-iceberg-submissions/sub_200_ens_densenet.csv')"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2ac62e85fbafd2a9095e71d576633a65694787d8", "collapsed": true, "_cell_guid": "2758287f-0e47-4dc6-ab11-004ce8812f8f"}, "source": ["concat_sub['is_iceberg_base'] = sub_base['is_iceberg']\n", "concat_sub['is_iceberg'] = np.where(np.all(concat_sub.iloc[:,1:6] > cutoff_lo, axis=1), \n", "                                    concat_sub['is_iceberg_max'], \n", "                                    np.where(np.all(concat_sub.iloc[:,1:6] < cutoff_hi, axis=1),\n", "                                             concat_sub['is_iceberg_min'], \n", "                                             concat_sub['is_iceberg_base']))\n", "concat_sub[['id', 'is_iceberg']].to_csv('stack_minmax_bestbase.csv', \n", "                                        index=False, float_format='%.6f')"], "outputs": []}, {"metadata": {"_uuid": "f4a0c7572916fd1ace3a3b0b7ea9fd9564c96d4d", "_cell_guid": "c493ff8f-99bd-4b20-8e68-80b4f85682c9"}, "cell_type": "markdown", "source": ["**LB 0.1463** - **Yes!** This is a decent score given none of the models in our ensemble pipeline has achieved thus better. I am sure there are more twisted ways to boost the score further, so will keep updating or just leave to more Kagglers to discover!"]}, {"metadata": {"_uuid": "3b1a6a2c393b1858847a895dbfc92f4bd71b719a", "_cell_guid": "a6e915c2-719e-41be-8760-e05271058ac7"}, "cell_type": "markdown", "source": ["\n", "### P.S. As I wrote along this work, deeply I think, building strong & roboust model is always the key component, stacking only comes last with the promise to surprise, sometimes, in an unpleasant direction@ \n", "\n", "\n"]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3"}}}