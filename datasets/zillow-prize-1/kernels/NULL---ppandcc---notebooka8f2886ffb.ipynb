{"cells": [{"cell_type": "markdown", "source": ["## This is my first time learning kaggle. Thank you!"], "metadata": {"_uuid": "dcf74290ba2e39ec6923d213dbfd7cc18dbd91be", "_cell_guid": "b2783aac-ccca-46b3-8f71-6176b30c5d52"}}, {"source": ["# ! _*_ coding:utf-8 _*_\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline"], "cell_type": "code", "metadata": {"_uuid": "f21e71e3a4b314f8e09a25585fec26f595feac05", "_cell_guid": "87752291-1df5-4c4f-abe1-eddcefdee7f0", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["from subprocess import check_output\n", "print(check_output([\"ls\",\"../input\"]).decode(\"utf8\"))"], "cell_type": "code", "metadata": {"_uuid": "b0695bc009d5dbf93fad335e0b6f3ceb818c9ea0", "_cell_guid": "b4ea3432-eed4-44bd-bc34-96b5237c9fa6"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_csv(\"../input/train_2016_v2.csv\",parse_dates=[\"transactiondate\"])\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "4a417f5b75a63384f8c94a2b35f32c156e8cacc7", "_cell_guid": "3e93818d-2819-4ad2-806d-5fdb8ab12acc"}, "execution_count": null, "outputs": []}, {"source": ["plt.scatter(range(train_df.shape[0]),np.sort(train_df.logerror))\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "81b362d31fc905204292f25f50ec9460d0e50c68", "_cell_guid": "5641e9b4-757f-4b13-a407-0622b8486053"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_csv(\"../input/properties_2016.csv\")\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "36b079c6024040c7bf3cc86cf31e6915652bc605", "_cell_guid": "84446fc1-1938-4a08-8195-d12243399c94"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_csv(\"../input/properties_2017.csv\")\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "a62255657588b53c4dcfcf047edcdfcd7f48fd61", "_cell_guid": "2c320bc9-565b-416c-af8e-0c71b729bc05"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_csv(\"../input/sample_submission.csv\")\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "04a7d006f7c71b3d9782b2f923423ea75d384c47", "_cell_guid": "65ee6e90-56fb-456a-9719-280e8a37f146"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_csv(\"../input/train_2017.csv\")\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "a1a6277de4584ce8aee5f0bc8c476596cdd083b1", "_cell_guid": "9ba5d0f6-3413-486d-bde8-e9148b4f663d"}, "execution_count": null, "outputs": []}, {"source": ["train_df = pd.read_excel(\"../input/zillow_data_dictionary.xlsx\")\n", "print(train_df.shape)\n", "train_df.head()"], "cell_type": "code", "metadata": {"_uuid": "ab796c1feffeb6e692205aaf62eb004dcef527b9", "_cell_guid": "9e3ba4f6-e1eb-4cce-8265-ee8c18e939fb"}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "0657c8d8cd95cc67c1dcef90e46ecfaeccdc22e3", "_cell_guid": "8c1734b8-c714-4f35-a4c4-8d9b50c470f1", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "a3e11ee7bd6ba2d9909ec31c859a2f322c224878", "_cell_guid": "6ddf525c-a5b0-4aa7-8659-febf6291ae7c", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "import gc\n", "print(\"Loading data ...\")"], "cell_type": "code", "metadata": {"_uuid": "d6a6e6c76e6cce27e8bf609730dd328c89f5319f", "_cell_guid": "ea00be2d-63fb-4f02-b5ea-ba9f1d33943f"}, "execution_count": null, "outputs": []}, {"source": ["\"\"\"\n", "properties_2016.csv\n", "properties_2017.csv\n", "sample_submission.csv\n", "train_2016_v2.csv\n", "train_2017.csv\n", "zillow_data_dictionary.xlsx\n", "\"\"\"\n", "\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "prop = pd.read_csv(\"../input/properties_2016.csv\")\n", "sample = pd.read_csv(\"../input/sample_submission.csv\")\n", "print(\"Binding to float32\")\n", "for c,dtype in zip(prop.columns,prop.dtypes):\n", "    if dtype == np.float64:\n", "        prop[c] = prop[c].astype(np.float32)"], "cell_type": "code", "metadata": {"_uuid": "25ea1f856a56b8b09f33ebfaa22cb95dad7c327f", "_cell_guid": "7852f099-0bf6-4f05-9b14-ecad5f5c1ac3"}, "execution_count": null, "outputs": []}, {"source": ["print(\"Creating training set ...\")\n", "df_train = train.merge(prop,how=\"left\",on=\"parcelid\")\n", "x_train = df_train.drop(['parcelid','logerror','transactiondate','propertyzoningdesc',\n", "                        'propertycountylandusecode'],axis=1)\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape,y_train.shape)"], "cell_type": "code", "metadata": {"_uuid": "71b99d7190be75cb95734f5632c709d3fc99fd95", "_cell_guid": "576327ce-3e16-4541-82be-52e775945387"}, "execution_count": null, "outputs": []}, {"source": ["train_columns = x_train.columns\n", "for c in x_train.dtypes[x_train.dtypes==object].index.values:\n", "    x_train[c]=(x_train[c]==True)\n", "del df_train;gc.collect()\n", "split = 80000\n", "x_train,y_train,x_valid,y_valid = x_trian[:split],y_train[:split],x_train[split:],y_train[split:]"], "cell_type": "code", "metadata": {"_uuid": "7d7903e1e190ee25e97d4fe3bf0804fb3afeced2", "_cell_guid": "72047699-d3ea-48e5-928d-0da52de8f2f3"}, "execution_count": null, "outputs": []}, {"source": ["print('Nuilding DMatrix ...')\n", "d_train = xgb.DMatrix(x_train, label=y_train)\n", "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n", "del x_train,x_valid;gc.collect()"], "cell_type": "code", "metadata": {"_uuid": "4acf930b1e02f639414dec37ad389454df6eabb5", "_cell_guid": "96a5a4bb-09bb-4393-ac73-0a505863bc5f", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "8622173a737a8b5399d84615a2b66aea17ea3307", "_cell_guid": "208325c4-96c3-443f-bb69-1de4d084c90c", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "95bb736f5f2c5166835290fa73bed62e731f4329", "_cell_guid": "16c3daee-6c98-417d-b9c4-18233854768c", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["# Parameters\n", "XGB_WEIGHT = 0.6500\n", "BASELINE_WEIGHT = 0.0056\n", "BASELINE_PRED = 0.0115\n", "import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "import lightgbm as lgb\n", "import gc\n", "##### READ IN RAW DATA\n", "print( \"\\nReading data from disk ...\")\n", "prop = pd.read_csv('../input/properties_2016.csv')\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "##### PROCESS DATA FOR LIGHTGBM\n", "print( \"\\nProcessing data for LightGBM ...\" )\n", "for c, dtype in zip(prop.columns, prop.dtypes):\t\n", "    if dtype == np.float64:\t\t\n", "        prop[c] = prop[c].astype(np.float32)\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "df_train.fillna(df_train.median(),inplace = True)\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)\n", "train_columns = x_train.columns\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "del df_train; gc.collect()\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)\n", "##### RUN LIGHTGBM\n", "params = {}\n", "params['max_bin'] = 10\n", "params['learning_rate'] = 0.0021 # shrinkage_rate\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'l1'          # or 'mae'\n", "params['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\n", "params['bagging_fraction'] = 0.85 # sub_row\n", "params['bagging_freq'] = 40\n", "params['num_leaves'] = 512        # num_leaf\n", "params['min_data'] = 500         # min_data_in_leaf\n", "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n", "params['verbose'] = 0\n", "print(\"\\nFitting LightGBM model ...\")\n", "clf = lgb.train(params, d_train, 430)\n", "del d_train; gc.collect()\n", "del x_train; gc.collect()\n", "print(\"\\nPrepare for LightGBM prediction ...\")\n", "print(\"   Read sample file ...\")\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", "print(\"   ...\")\n", "sample['parcelid'] = sample['ParcelId']\n", "print(\"   Merge with property data ...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "print(\"   ...\")\n", "del sample, prop; gc.collect()\n", "print(\"   ...\")\n", "x_test = df_test[train_columns]\n", "print(\"   ...\")\n", "del df_test; gc.collect()\n", "print(\"   Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "print(\"   ...\")\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "print(\"\\nStart LightGBM prediction ...\")\n", "# num_threads > 1 will predict very slow in kernal\n", "clf.reset_parameter({\"num_threads\":1})\n", "p_test = clf.predict(x_test)\n", "del x_test; gc.collect()\n", "print( \"\\nUnadjusted LightGBM predictions:\" )\n", "print( pd.DataFrame(p_test).head() )\n", "##### RE-READ PROPERTIES FILE\n", "##### (I tried keeping a copy, but the program crashed.)\n", "print( \"\\nRe-reading properties file ...\")\n", "properties = pd.read_csv('../input/properties_2016.csv')\n", "##### PROCESS DATA FOR XGBOOST\n", "print( \"\\nProcessing data for XGBoost ...\")\n", "for c in properties.columns:\n", "    properties[c]=properties[c].fillna(-1)\n", "    if properties[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties[c].values))\n", "        properties[c] = lbl.transform(list(properties[c].values))\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "x_test = properties.drop(['parcelid'], axis=1)\n", "# shape        \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "# drop out ouliers\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.418 ]\n", "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "print('After removing outliers:')     \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "##### RUN XGBOOST\n", "print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.037,\n", "    'max_depth': 5,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 0.8,   \n", "    'alpha': 0.4, \n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "# Enough with the ridiculously overfit parameters.\n", "# I'm going back to my version 20 instead of copying Jayaraman.\n", "# I want a num_boost_rounds that's chosen by my CV,\n", "# not one that's chosen by overfitting the public leaderboard.\n", "# (There may be underlying differences between the train and test data\n", "#  that will affect some parameters, but they shouldn't affect that.)\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dtest = xgb.DMatrix(x_test)\n", "# cross-validation\n", "#print( \"Running XGBoost CV ...\" )\n", "#cv_result = xgb.cv(xgb_params, \n", "#                   dtrain, \n", "#                   nfold=5,\n", "#                   num_boost_round=350,\n", "#                   early_stopping_rounds=50,\n", "#                   verbose_eval=10, \n", "#                   show_stdv=False\n", "#                  )\n", "#num_boost_rounds = len(cv_result)\n", "# num_boost_rounds = 150\n", "num_boost_rounds = 242\n", "print(\"\\nXGBoost tuned with CV in:\")\n", "print(\"   https://www.kaggle.com/aharless/xgboost-without-outliers-tweak \")\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "# train model\n", "print( \"\\nTraining XGBoost ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "print( \"\\nPredicting with XGBoost ...\")\n", "xgb_pred = model.predict(dtest)\n", "print( \"\\nXGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred).head() )\n", "##### COMBINE PREDICTIONS\n", "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n", "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT\n", "pred = XGB_WEIGHT*xgb_pred + BASELINE_WEIGHT*BASELINE_PRED + lgb_weight*p_test\n", "print( \"\\nCombined predictions:\" )\n", "print( pd.DataFrame(pred).head() )\n", "##### WRITE THE RESULTS\n", "print( \"\\nPreparing results for write ...\" )\n", "y_pred=[]\n", "for i,predict in enumerate(pred):\n", "    y_pred.append(str(round(predict,4)))\n", "y_pred=np.array(y_pred)\n", "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n", "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n", "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n", "# set col 'ParceID' to first col\n", "cols = output.columns.tolist()\n", "cols = cols[-1:] + cols[:-1]\n", "output = output[cols]\n", "from datetime import datetime\n", "print( \"\\nWriting results to disk ...\" )\n", "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "print( \"\\nFinished ...\" )"], "cell_type": "code", "metadata": {"_uuid": "e8a1fcc7f8ac269bc6ee57c5c65e934c180cf321", "_cell_guid": "223e1e62-3931-438b-a8e4-0965f8c362e7", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "0e0552a60b1f1eb0eee05fd8edb9e3b343a085a6", "_cell_guid": "132e1128-1d8a-496c-97fa-6b658a835251", "collapsed": true}, "execution_count": null, "outputs": []}, {"source": ["\"\"\"\n", "properties_2016.csv\n", "properties_2017.csv\n", "sample_submission.csv\n", "train_2016_v2.csv\n", "train_2017.csv\n", "zillow_data_dictionary.xlsx\n", "\"\"\"\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import lightgbm as lgb\n", "import gc\n", "print('Loading data ...')\n", "train = pd.read_csv('../input/train_2016_v2.csv')\n", "prop = pd.read_csv('../input/properties_2016.csv')\n", "for c, dtype in zip(prop.columns, prop.dtypes):\t\n", "    if dtype == np.float64:\t\t\n", "        prop[c] = prop[c].astype(np.float32)\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)\n", "train_columns = x_train.columns\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "del df_train; gc.collect()\n", "split = 90000\n", "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "x_valid = x_valid.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)\n", "d_valid = lgb.Dataset(x_valid, label=y_valid)\n", "params = {}\n", "params['learning_rate'] = 0.002\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'mae'\n", "params['sub_feature'] = 0.5\n", "params['num_leaves'] = 60\n", "params['min_data'] = 500\n", "params['min_hessian'] = 1\n", "watchlist = [d_valid]\n", "clf = lgb.train(params, d_train, 500, watchlist)\n", "del d_train, d_valid; gc.collect()\n", "del x_train, x_valid; gc.collect()\n", "print(\"Prepare for the prediction ...\")\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", "sample['parcelid'] = sample['ParcelId']\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "del sample, prop; gc.collect()\n", "x_test = df_test[train_columns]\n", "del df_test; gc.collect()\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "print(\"Start prediction ...\")\n", "# num_threads > 1 will predict very slow in kernal\n", "clf.reset_parameter({\"num_threads\":1})\n", "p_test = clf.predict(x_test)\n", "del x_test; gc.collect()\n", "print(\"Start write result ...\")\n", "sub = pd.read_csv('../input/sample_submission.csv')\n", "for c in sub.columns[sub.columns != 'ParcelId']:\n", "    sub[c] = p_test\n", "sub.to_csv('lgb_starter.csv', index=False, float_format='%.4f')"], "cell_type": "code", "metadata": {"_uuid": "4713288fc2324ea417586c78d3639a11534a6177", "_cell_guid": "159128b2-6bc4-474e-87e8-cad33d7d36de"}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "outputs": []}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python"}}}