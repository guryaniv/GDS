{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport matplotlib.pyplot as plt #plotting\nimport time #time random\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce9144bb76b143da6db0c7896183b7c402911af4"},"cell_type":"markdown","source":"# 1. Loading Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train=pd.read_csv('../input/train.csv')\nprint(train.columns.values)\ntest=pd.read_csv('../input/test.csv')\nprint(test.columns.values)\n#train=np.array(train)\n#np.shape(train)\n#train[0]\n#rang=list(range(len(set(train[:,-1]))))\n#print(rang)\n#rang=enumerate(rang)\n#rang=list(rang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20997b1548b5e3b5f1e0ed03bddf0e7b5673df09"},"cell_type":"code","source":"combine=[train,test]\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ad89eac21ba6c07e163b07995b88e0b894896f3"},"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5127752bdc0a0fbf44edac65ef1d67d9506a86d6"},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41118d462e793f6197373a2f4fc173b5992e8d58"},"cell_type":"markdown","source":"# 2. Remove the PassengerId, Name, Ticket and Cabin attribute"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"67ccf00e9c989e69084ea8ee0465476824250a29","collapsed":true},"cell_type":"code","source":"train=train.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\ntest=test.drop(['Name','Ticket','Cabin'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a6126fc6df58ad5bd889b39228f6b5fa889da6"},"cell_type":"code","source":"train.head()\ntest.head()\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3601f7bedce940bd8504d99e6c77f421638f95b"},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ac3c4ffb6d642b8b2ff51a4243a343212b7d86"},"cell_type":"code","source":"train['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4736eeb50e559495080c0eae1e0c134bea6b36ea"},"cell_type":"code","source":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a368b2f10e233122f6fab5f23d046e27de76220d"},"cell_type":"code","source":"train = train.drop(['AgeBand'], axis=1)\ncombine = [train, test]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaea2e1f0a43bd4dd50823a3d44ba2b5f28b6f90"},"cell_type":"code","source":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d655917ea91acaf332264542f857b9c490bfe4b"},"cell_type":"code","source":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a42fe542cfed6c8377eb82090f9cab563f175da5"},"cell_type":"code","source":"train = train.drop(['Parch', 'SibSp'], axis=1)\ntest = test.drop(['Parch', 'SibSp'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16cf386d227ad3aec7cbf36ca0ebe2d7af4b0ea7"},"cell_type":"code","source":"combine = [train, test]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1aeca97b9ba9f936bc726734c6dd86639562134"},"cell_type":"markdown","source":"# 3. Fill in the nulls in Embarked and Age with the Mode"},{"metadata":{"trusted":true,"_uuid":"f2107de97d505e2c52969703fa06cfa41251c2d5","collapsed":true},"cell_type":"code","source":"train['Embarked'] = train['Embarked'].fillna(train.Embarked.dropna().mode()[0])\ntrain['Age'] = train['Age'].fillna(train.Age.dropna().median())\ntest['Age'] = test['Age'].fillna(test.Age.dropna().median())\ntest['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a894c44111909aee90f0da74228b734b2373feab"},"cell_type":"markdown","source":"# 4. Convert Categorical Data to Discrete numeric"},{"metadata":{"trusted":true,"_uuid":"73c8444ba2cb31215769e8679956b28b86a93a86","collapsed":true},"cell_type":"code","source":"train['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain['Sex'] = train['Sex'].map({'male':0,'female':1}).astype(int)\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest['Sex'] = test['Sex'].map({'male':0,'female':1}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80d1ff951cbf8a999418eaaf012dfe8d443d2b04"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15feb7ec47dc4f4093d3bf140e49139c84b38abc"},"cell_type":"markdown","source":"# 5. Convert to a NumPy Array"},{"metadata":{"trusted":true,"_uuid":"30e17cc0d34e5ef6038a49884d250f3aea29d374","collapsed":true},"cell_type":"code","source":"Train=np.array(train)\nt=int(time.time())\nnp.random.seed(1533756006)\nnp.random.shuffle(Train)\nX_train=Train[:,1:]\nY_train=Train[:,0]\ntest=np.array(test)\nX_test=test[:,1:] #the first col. is the Id.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85e5c74ee84fea7d2763c758d746d49c7a655a6"},"cell_type":"markdown","source":"# 6. Apply Feature Normalization."},{"metadata":{"trusted":true,"_uuid":"4a5e775cb9529e28d75224b28b71c2b668b9c66e","collapsed":true},"cell_type":"code","source":"def FetNorm(X):\n    #Calculate Mean, Then Std deviation for each column\n    #X=X-Mean/std\n    mean=np.mean(X,axis=0)\n    std=np.std(X,axis=0)\n    X=(X-mean)/(std)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79c5e9fa206b5fd793d0fbcf7620da968b5c5962"},"cell_type":"code","source":"X_train=FetNorm(X_train)\nX_test=FetNorm(X_test)\nprint(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"955345ea29312deb32c66365dc303b27bdcd3d09"},"cell_type":"code","source":"def create_placeholders():\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n    \n    Returns:\n    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n    \n    Tips:\n    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n      In fact, the number of examples during test/train is different.\n    \"\"\"\n\n    X=tf.placeholder(tf.float32,shape=(X_train.shape[1],None))\n    Y=tf.placeholder(tf.float32,shape=(1,None))\n    keep_prob = tf.placeholder(tf.float32)\n    \n    return X, Y,keep_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bac7f7c32472bfb96b7e1c8ed76433966e2abaed"},"cell_type":"code","source":"def initialize_parameters(input_shape):\n    \"\"\"\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [8, input_shape]\n                        b1 : [8, 1]\n                        W2 : [3, 8]\n                        b2 : [3, 1]\n                        W3 : [1, 3]\n                        b3 : [1, 1]\n    \n    Returns:\n    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n    \"\"\"\n\n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 6 lines of code)\n    W1 = tf.get_variable(\"W1\", [8,input_shape], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b1 = tf.get_variable(\"b1\", [8,1], initializer = tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [3,8], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b2 = tf.get_variable(\"b2\", [3,1], initializer = tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\", [1,3], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b3 = tf.get_variable(\"b3\", [1,1], initializer = tf.zeros_initializer())\n    ### END CODE HERE ###\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0d98203ac72aae8e46a8a55113c308061f118c0"},"cell_type":"code","source":"def forward_propagation(X, parameters, keep_prob):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n    Z1 = tf.matmul(W1,X)+b1                                # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.leaky_relu(Z1)                              # A1 = relu(Z1)\n    A1 = tf.nn.dropout(A1,keep_prob)                    #dropout layer.\n    Z2 = tf.matmul(W2,A1)+b2                               # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.leaky_relu(Z2)                               # A2 = relu(Z2)\n    A2 = tf.nn.dropout(A2,keep_prob)                    #dropout layer.\n    Z3 = tf.matmul(W3,A2)+b3                               # Z3 = np.dot(W3,A2) + b3\n    ### END CODE HERE ###\n    \n    return Z3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"459d547a675ba4ef06940e39aa4d9aeabf8add4c"},"cell_type":"code","source":"def compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    ### START CODE HERE ### (1 line of code)\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n    ### END CODE HERE ###\n    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56af76c03074fd318e43ca8fe4cc2e451c620e0b","collapsed":true},"cell_type":"code","source":"def model(X_train, Y_train, X_CV, Y_CV,X_test, learning_rate = 0.009,\n          num_epochs = 4000, minibatch_size = 32,print_cost = True,th=0.56,kp=0.55):\n#def model(X_train, Y_train, learning_rate = 0.01,\n#         num_epochs = 6000, minibatch_size = 32, print_cost = True):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    Arguments:\n    X_train -- training set, of shape (input size = 12288, number of training examples =)\n    Y_train -- test set, of shape (output size = 6, number of training examples = )\n    X_test -- training set, of shape (input size = 12288, number of training examples = )\n    Y_test -- test set, of shape (output size = 6, number of test examples = )\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of shape (n_x, n_y)\n    ### START CODE HERE ### (1 line)\n    X, Y,keep_prob = create_placeholders()\n    ### END CODE HERE ###\n\n    # Initialize parameters\n    ### START CODE HERE ### (1 line)\n    parameters = initialize_parameters(X_train.shape[0])\n    ### END CODE HERE ###\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ### START CODE HERE ### (1 line)\n    Z3 = forward_propagation(X, parameters,keep_prob)\n    ### END CODE HERE ###\n    \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    cost = compute_cost(Z3, Y)\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    threshold=tf.constant(th)\n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            #num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            #seed = seed + 1\n            #minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            #for minibatch in minibatches:\n\n                # Select a minibatch\n                #(minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n            _ , ccost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train,keep_prob:kp})\n                ### END CODE HERE ###\n                \n            epoch_cost += ccost / m\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        predicted=tf.to_float(tf.greater(tf.sigmoid(Z3),threshold))\n        actual=Y\n        correct_prediction = tf.equal(predicted , actual)\n        TP = tf.count_nonzero(predicted * actual)\n        TN = tf.count_nonzero((predicted - 1) * (actual - 1))\n        FP = tf.count_nonzero(predicted * (actual - 1))\n        FN = tf.count_nonzero((predicted - 1) * actual) \n        precision = TP / (TP + FP)\n        recall = TP / (TP + FN)\n        f1 = 2 * precision * recall / (precision + recall)\n        Test=tf.to_float(tf.greater(tf.sigmoid(Z3),threshold))\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        Test=Test.eval({X: X_test,keep_prob: 1.})\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train,keep_prob: 1.}))\n        print (\"Train F1:\", f1.eval({X: X_train, Y: Y_train,keep_prob: 1.}))\n        print (\"CV Accuracy:\", accuracy.eval({X: X_CV, Y: Y_CV,keep_prob: 1.}))\n        print (\"CV F1:\", f1.eval({X: X_CV, Y: Y_CV,keep_prob: 1.}))\n\n        return (f1.eval({X: X_CV, Y: Y_CV,keep_prob: 1.})/abs(f1.eval({X: X_train, Y: Y_train,keep_prob: 1.})-f1.eval({X: X_CV, Y: Y_CV,keep_prob: 1.}))),Test,parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"39ce8f5c423157f3d8ec28394ccd733800a97748"},"cell_type":"code","source":"middlept=885\n_,Test,parameters=model(X_train.T[:,:middlept],Y_train.reshape(1,-1)[:,:middlept] \\\n                 ,X_train.T[:,middlept:],Y_train.reshape(1,-1)[:,middlept:],X_test.T,num_epochs = 8000,th=0.56,kp=0.9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d2672822e4586c5f2752f8c901a27141aeb287"},"cell_type":"markdown","source":"## Hyperparameter tuning"},{"metadata":{"trusted":true,"_uuid":"5acf655e1521aaeaaf971482062abf940538c166"},"cell_type":"code","source":"#f1=[]\n#maxi=-1;\n#maxf=0\n#for i in range(50,95,5):\n#    F1,Test,parameters=model(X_train.T[:,:middlept],Y_train.reshape(1,-1)[:,:middlept]  \\\n#             ,X_train.T[:,middlept:],Y_train.reshape(1,-1)[:,middlept:],X_test.T,kp=(i/100),num_epochs = 6000,print_cost = False)\n#    f1.append(F1)\n#    if(max(f1)>maxf):\n #       maxf=max(f1)\n#        maxi=i\n#print(maxf,maxi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a69e544417f32b9d860d1a89f69fc361de7a094d","collapsed":true},"cell_type":"code","source":"data={'PassengerID':list(test[:,0].astype(int).reshape(-1,)),'Survived':list((Test).astype(int).reshape(-1,))}\ndf = pd.DataFrame(data, columns = ['PassengerID', 'Survived'])\ndf.to_csv('sol.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}