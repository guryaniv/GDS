{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport keras\n\ntsv_file = '../input/movie-review-sentiment-analysis-kernels-only/train.tsv'\ntrain_data = pd.read_table(tsv_file)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = train_data.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea3fab102a30c80bd97d9d85b7e7f162cb6621ae"},"cell_type":"code","source":"sentiments = []\nfeatures = []\n\nfor i in range(0, len(train_data)):\n    sentiments.append(train_data[i][3])\n    \nfor i in range(0, len(train_data)):\n    features.append(train_data[i][2])\n    \nsentences = features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6584355f504df519b2a6ce27afe0261177f06b62"},"cell_type":"code","source":"max_features = 16467\n# cut texts after this number of words\n# (among top max_features most common words)\nmaxlen = 55\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bee7af9481b4d39e035b5d10c5ba51f508affd4"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n\nsentiments = to_categorical(sentiments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2520e1d0a93ff969e6cb8321d5ace1fdc54266d4"},"cell_type":"code","source":"from numpy import asarray\nimport os\n\nembeddings_index = {}\nf = open(os.path.join('../input/glove6b100dtxt', 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa8cc450988ecb352136bf3b87fee9af9e5697f"},"cell_type":"code","source":"word_index = tokenizer.word_index\n\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77c4219975d5fba83cd2d3e3dd8f04d5533d47fe"},"cell_type":"code","source":"train_features = sequences[:156060]\ntrain_targets = sentiments[:156060]\n\nval_features = sequences[134848:]\nval_targets = sentiments[134848:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fc776f9de91b4edca467dad595ca03e0a898803"},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Embedding, GRU, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd6fef828ef861d31a1aec52f5fbee0a1bc1977d"},"cell_type":"code","source":"print('Pad sequences (samples x time)')\ntrain_features = sequence.pad_sequences(train_features, maxlen=maxlen, padding='pre')\nprint('train_features shape:', train_features.shape)\ntrain_features = np.array(train_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2207dbde547254f253b6155c9f6e392fa7cc6732"},"cell_type":"code","source":"print('Pad sequences (samples x time)')\nval_features = sequence.pad_sequences(val_features, maxlen=maxlen, padding='pre')\nprint('val_features shape:', val_features.shape)\nval_features = np.array(val_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac9b5e4c121bbf8afe3af874eee823930a1a57b"},"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *\nfrom keras.initializers import *\nfrom keras.optimizers import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b4cc92d9e3bf751849521cb89fc5006bb7fbfd"},"cell_type":"code","source":"maxlen = 55\nmax_features = 15289\nembed_size = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ac2d198c7d073ddfc97c1c542640252e65f3986"},"cell_type":"code","source":"# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2719148d6ec511316e1cc7e55207b0e0b90984e5"},"cell_type":"code","source":"def squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cda276dc9f4f264d69b200e3aa2eb6a31b093e45"},"cell_type":"code","source":"def BiGRUCapsNet():\n    inp = Input(shape=(maxlen,))\n    embedding = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    spatial_dropout = SpatialDropout1D(rate=0.2)(embedding)\n    \n    bi_gru_1 = Bidirectional(CuDNNGRU(64, return_sequences=True, \n                             kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(spatial_dropout)\n    \n    bi_gru_2 = Bidirectional(CuDNNGRU(64, return_sequences=True, \n                             kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(bi_gru_1)\n\n    capsule = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(bi_gru_1)\n    capsule = Flatten()(capsule)\n    \n    gru_atten_1 = Attention(maxlen)(bi_gru_1)\n    gru_atten_2 = Attention(maxlen)(bi_gru_2)\n\n    avg_pool = GlobalAveragePooling1D()(bi_gru_2)\n    max_pool = GlobalMaxPooling1D()(bi_gru_2)\n    \n    features = concatenate([gru_atten_1, gru_atten_2, capsule, avg_pool, max_pool])\n    outp = Dense(5, activation='softmax')(features)\n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92779b5ed8e9e32c2e517a26a3e79f6ccc5fb86"},"cell_type":"code","source":"model = BiGRUCapsNet()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3555905c0533cd5f6976672d3da6178cf5e1cbfb"},"cell_type":"code","source":"model.fit(x=train_features, y=train_targets, epochs=20, batch_size=128, validation_data=(val_features, val_targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ee2f9216fb2668dfda7acccdd2aae408da38b84"},"cell_type":"code","source":"tsv_file = '../input/movie-review-sentiment-analysis-kernels-only/test.tsv'\ntest_data = pd.read_table(tsv_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41328d94fb3172e6d1f162265b2cf9c13d1d5eb4"},"cell_type":"code","source":"test_data = test_data.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe2fd895a87b6611ec72782d168208277ac469b3"},"cell_type":"code","source":"features = []\n\nfor i in range(0, len(test_data)):\n    features.append(test_data[i][2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb26ebf8ebab9a4db13b7bd0a4100a6a909dae34"},"cell_type":"code","source":"sentences = features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3644f1759acb3ad93a6e8044b53ce11a8db8588"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\ntest_sequences = tokenizer.texts_to_sequences(sentences)\ntest_sequences = sequence.pad_sequences(test_sequences, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b26427b2a9552a96f1d27152d2a6bbd3f34a63"},"cell_type":"code","source":"predictions = model.predict(np.array(test_sequences))\n\nclass_predictions = []\n\nfor i in range(0, len(predictions)):\n    class_predictions.append(list.index(list(predictions[i]), max(predictions[i])))\n    \nclass_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0807d224eb5377d8030492140031cf444615475a"},"cell_type":"code","source":"ids = list(test_data[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0636f63b79c94e1641c60d5b08d795145843994"},"cell_type":"code","source":"submission = pd.DataFrame(np.transpose(np.array([ids, class_predictions])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f576adcc42063a57879b37516636b96d5f4b073c"},"cell_type":"code","source":"submission.columns = ['PhraseId', 'Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bbba32f22bce42e3139fa017558d95f4831e5ef"},"cell_type":"code","source":"submission.to_csv('Movie-Review-Sentiment-Predictions-1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"764993bd585a0af17c15e382fb0b43b609154780"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}