{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization,Dropout\nimport catboost\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ec15c5c5d996c4492f9462bcb50292a110048dd"},"cell_type":"code","source":"Train_org = pd.read_csv('../input/train.csv')\nTest_org = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d12e6e0133443f9e542e8b84e4bd77ee85eb56b"},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88eda770425fd81ea8a1891f85fd390d0dce61c4"},"cell_type":"code","source":"Train = reduce_mem_usage(Train_org)\nTest = reduce_mem_usage(Test_org)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08932ef97ccac1b08f5b916340bd9f3c0e847f0a"},"cell_type":"code","source":"del Train_org\ndel Test_org","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb073af5647bf009506ce84d6aa6a09251dea279"},"cell_type":"code","source":"Train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04c4d2b5ff9c56e7c78db3fefe3a7a28fccd037a"},"cell_type":"code","source":"Train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f40f261175bb262a4fdef54645c9f1ef551ed3bc"},"cell_type":"code","source":"Train.corr()['winPlacePerc'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f983290c5c9e861e91c54f2fbd7b721f582897aa"},"cell_type":"code","source":"Train=Train.drop(['maxPlace','numGroups'],axis=1)\nTest=Test.drop(['maxPlace','numGroups'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bfb8f7f73204d703a2111d42088eeb4372d2e38","scrolled":true},"cell_type":"code","source":"sns.set(font_scale=1)\nf, ax = plt.subplots(figsize=(15, 15))\nhm = sns.heatmap(Train.corr(), cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd459e42cdd2da62d85c538339f5c613cb6ca1ae"},"cell_type":"code","source":"# Feature Engineering\nTrain[\"distance\"] = Train[\"rideDistance\"]+Train[\"walkDistance\"]+Train[\"swimDistance\"]\nTrain[\"skill\"] = Train[\"headshotKills\"]+Train[\"roadKills\"]\nTest[\"distance\"] = Test[\"rideDistance\"]+Test[\"walkDistance\"]+Test[\"swimDistance\"]\nTest[\"skill\"] = Test[\"headshotKills\"]+Test[\"roadKills\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6990a73d1caa5ee1d65b0016072e0100e7a6c1d1"},"cell_type":"code","source":"Train_size = Train.groupby(['matchId','groupId']).size().reset_index(name='group_size')\nTest_size = Test.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n\nTrain_mean = Train.groupby(['matchId','groupId']).mean().reset_index()\nTest_mean = Test.groupby(['matchId','groupId']).mean().reset_index()\n\nTrain_max = Train.groupby(['matchId','groupId']).max().reset_index()\nTest_max = Test.groupby(['matchId','groupId']).max().reset_index()\n\nTrain_min = Train.groupby(['matchId','groupId']).min().reset_index()\nTest_min = Test.groupby(['matchId','groupId']).min().reset_index()\n\n#Train_median = Train.groupby(['matchId','groupId']).median().reset_index()\n#Test_median = Test.groupby(['matchId','groupId']).median().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f53685340263bd036114248e470d738be0f3d94"},"cell_type":"code","source":"Train_match_mean = Train.groupby(['matchId']).mean().reset_index()\nTest_match_mean = Test.groupby(['matchId']).mean().reset_index()\n\nTrain = pd.merge(Train, Train_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\nTest = pd.merge(Test, Test_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\ndel Train_mean\ndel Test_mean\n\nTrain = pd.merge(Train, Train_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\nTest = pd.merge(Test, Test_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ndel Train_max\ndel Test_max\n\nTrain = pd.merge(Train, Train_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\nTest = pd.merge(Test, Test_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ndel Train_min\ndel Test_min\n\n#Train = pd.merge(Train, Train_median, suffixes=[\"\", \"_median\"], how='left', on=['matchId', 'groupId'])\n#Test = pd.merge(Test, Test_median, suffixes=[\"\", \"_median\"], how='left', on=['matchId', 'groupId'])\n#del Train_median\n#del Test_median\n\nTrain = pd.merge(Train, Train_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\nTest = pd.merge(Test, Test_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\ndel Train_match_mean\ndel Test_match_mean\n\nTrain = pd.merge(Train, Train_size, how='left', on=['matchId', 'groupId'])\nTest = pd.merge(Test, Test_size, how='left', on=['matchId', 'groupId'])\ndel Train_size\ndel Test_size\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e50a3a982977d4cf761199e8a84d348b7ad12511"},"cell_type":"code","source":"target = 'winPlacePerc'\ntrain_columns = list(Test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d6dd5272035f622b02547d3bd4a9109494961c"},"cell_type":"code","source":"train_columns.remove(\"Id\")\ntrain_columns.remove(\"matchId\")\ntrain_columns.remove(\"groupId\")\ntrain_columns.remove(\"Id_mean\")\ntrain_columns.remove(\"Id_max\")\ntrain_columns.remove(\"Id_min\")\n#train_columns.remove(\"Id_medain\")\ntrain_columns.remove(\"Id_match_mean\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d7cd8b88d1bd33b8e931e84ef40bbb22dae4cf"},"cell_type":"code","source":"len(train_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aa55a1d24d4214254c0567bbe9a8a052f7f41e9"},"cell_type":"code","source":"X = Train[train_columns]\nY = Test[train_columns]\nT = Train[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6225ecfbd17ad3df36c97573a75c156b7439ef85"},"cell_type":"code","source":"#from sklearn import preprocessing\n#scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(X)\n\n#X = scaler.transform(X)\n#Y = scaler.transform(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab4d4c9da2b846303e584478bb06bd3a11862c8"},"cell_type":"code","source":"del Train\ndel Test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08419bfeaf74bf51f2f86805c16cf25ac759f0c0"},"cell_type":"code","source":"x_train, x_test, t_train, t_test = train_test_split(X, T, test_size = 0.15, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c8c596dacb78aad0ce9d8a4e167e0806909de82"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(256, kernel_initializer='he_normal', input_dim=x_train.shape[1], activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(64, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, kernel_initializer='normal', activation='linear'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7320959782df4ae21e60c3dfc402b520939c5829"},"cell_type":"code","source":"from keras import optimizers\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd37237c1247ce63ba0ab753862b382a51b7cab3"},"cell_type":"code","source":"optimizer = optimizers.Adam(lr=0.05, epsilon=1e-8, decay=1e-4, amsgrad=False)\n\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d158612196adab3104bf0f871ac4b08ca7c0928f"},"cell_type":"code","source":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return LearningRateScheduler(schedule, verbose)\n\nlr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.9, step_size=1, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode = 'min', patience=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19cda2f0d59ff5b54b2d387e5761830f2ecccd82"},"cell_type":"code","source":"saved_model = model.fit(x_train, t_train, \n                 validation_data=(x_test, t_test),\n                 epochs=80,\n                 batch_size=65536,\n                 callbacks=[lr_sched,early_stopping], \n                 verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b58771fefadee5717e79530b4086181d02f53b8"},"cell_type":"code","source":"plt.plot(saved_model.history['loss'])\nplt.plot(saved_model.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation mae values\nplt.plot(saved_model.history['mean_absolute_error'])\nplt.plot(saved_model.history['val_mean_absolute_error'])\nplt.title('Mean Abosulte Error')\nplt.ylabel('Mean absolute error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0771466737645cb80a56772a57660f50ffc756c2"},"cell_type":"code","source":"y_pred=model.predict(Y)\nsubmission=pd.DataFrame()\nsubmission['Id']=Test_org['Id']\nsubmission['winPlacePerc']=y_pred\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77d0e29b4bf4c823f94f26cfffb6c46af8b2ebee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}