{"cells":[{"metadata":{"_uuid":"0e7f1eca6087c4c45c31bc6045dea57a3f37b89f"},"cell_type":"markdown","source":"# Aim and Motivation \n[Nirant](https://www.kaggle.com/nirant)'s latest kernel on spaCy: [Hitchhiker's Guide to NLP in spaCy](https://www.kaggle.com/nirant/hitchhiker-s-guide-to-nlp-in-spacy) has made me realize that spaCy maybe as good or even better than NLTK for Natural Language Processing. My recent kernels deal with deep learning and I want to extend that by using text data for deep learning and intend to use spaCy for processing and modelling this data. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Usual imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"../input\"))\n\n# Plotly based imports for visualization\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n!python -m spacy download en_core_web_lg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57d08400a530b786ad3226110e34aefb85e94b1"},"cell_type":"code","source":"# Loading data\nwines = pd.read_csv('../input/winemag-data_first150k.csv')\nwines.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17fe81e7e90dfb32681f56a03b9136bbb74343b8"},"cell_type":"code","source":"# Creating a spaCy object\nnlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6662c89d96296025b13d25e9addf6bb6a5dee520"},"cell_type":"markdown","source":"spaCy also comes with a built-in named entity visualizer that lets you check your model's predictions in your browser. You can pass in one or more <code>Doc</code> objects and start a web server, export HTML files or view the visualization directly from a Jupyter Notebook."},{"metadata":{"_uuid":"2c482d29cffb29c2b444086a5035b15468ff13e5"},"cell_type":"markdown","source":"### Named Entity Recognition\n Named Entity Recognition is an information extraction task where named entities in unstructured sentences are located and classified  in some pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."},{"metadata":{"trusted":true,"_uuid":"677e753450ddea24b1289e3033a2f7928710ef25"},"cell_type":"code","source":"doc = nlp(wines[\"description\"][3])\nspacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e004fb15995ec7723ca12ad54baf98029ecf90e"},"cell_type":"code","source":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25330472382f10be59dd09d3f4cb7a8ae5686081"},"cell_type":"markdown","source":"### Lemmatization\nIt is the  process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Words like \"ran\" and \"running\" are converted to \"run\" to avoid having words with similar meanings in our data."},{"metadata":{"trusted":true,"_uuid":"b5c09cd0724ac75d603b3e1be1afb006249eadc4"},"cell_type":"code","source":"review = str(\" \".join([i.lemma_ for i in doc]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868105f99baf2742713656a089ad5b38f80d8329"},"cell_type":"code","source":"doc = nlp(review)\nspacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e949f2d634aa8742015b15c8cccf38f42a16621f"},"cell_type":"markdown","source":"The sentence looks much different now that it is lemmatized."},{"metadata":{"_uuid":"8cd922f8229f6aba3f08dfb25d8793a5e2b0ee57"},"cell_type":"markdown","source":"### Parts of Speech tagging\nThis is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,[1] based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc."},{"metadata":{"trusted":true,"_uuid":"a8ed2a71e2d90ad317e7b0d69d0e6544ff19f17c"},"cell_type":"code","source":"# POS tagging\nfor i in nlp(review):\n    print(i,\"=>\",i.pos_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee0b36f52b552dd72c5fda7808d41db5749491cf"},"cell_type":"code","source":"# Parser for reviews\nparser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1207bc8f6ca068de633cd8d68d1424877baf85a"},"cell_type":"code","source":"tqdm.pandas()\nwines[\"processed_description\"] = wines[\"description\"].progress_apply(spacy_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e64d8d97ef7443b594431dc4662450264460aae1"},"cell_type":"markdown","source":"# What is topic-modelling?\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. \n\nThe \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is. It involves various techniques of dimensionality reduction(mostly non-linear) and unsupervised learning like LDA, SVD, autoencoders etc.\n\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Topic_model)"},{"metadata":{"trusted":true,"_uuid":"fd00ec3f6d99df6434671334c513511485de5a18"},"cell_type":"code","source":"# Creating a vectorizer\nvectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ndata_vectorized = vectorizer.fit_transform(wines[\"processed_description\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc59a5306eb806598d98f79daabe0217897a5bbe"},"cell_type":"code","source":"NUM_TOPICS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0140ca0b3eb106edeee8afac1720e25eae5675"},"cell_type":"code","source":"# Latent Dirichlet Allocation Model\nlda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\ndata_lda = lda.fit_transform(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"822195007e669911dffd6162bfc09539f4dd70ee"},"cell_type":"code","source":"# Non-Negative Matrix Factorization Model\nnmf = NMF(n_components=NUM_TOPICS)\ndata_nmf = nmf.fit_transform(data_vectorized) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c7c3e0f3e59c54f76fbfac0bc1085a81438b25b"},"cell_type":"code","source":"# Latent Semantic Indexing Model using Truncated SVD\nlsi = TruncatedSVD(n_components=NUM_TOPICS)\ndata_lsi = lsi.fit_transform(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"4965ca621bdd6abcf8dd4eb1a41699603cfa2284"},"cell_type":"code","source":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a9f1099afa5c6c98ca90103080aa397cd146d3c"},"cell_type":"code","source":"# Keywords for topics clustered by Latent Dirichlet Allocation\nprint(\"LDA Model:\")\nselected_topics(lda, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9da862a4cacb585d5ac0e75541c30d8c1add90d1"},"cell_type":"code","source":"# Keywords for topics clustered by Latent Semantic Indexing\nprint(\"NMF Model:\")\nselected_topics(nmf, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e789cfaaf1bca8737ee73f6a6c8a1b388126f13b"},"cell_type":"code","source":"# Keywords for topics clustered by Non-Negative Matrix Factorization\nprint(\"LSI Model:\")\nselected_topics(lsi, vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d45ca676b4f8f25de9b237c1e41ab4e652cd7b3"},"cell_type":"code","source":"# Transforming an individual sentence\ntext = spacy_tokenizer(\"Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\")\nx = lda.transform(vectorizer.transform([text]))[0]\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a9eeef90515577592dfebab1635c1f77a7a6868"},"cell_type":"markdown","source":"The index in the above list with the largest value represents the most dominant topic for the given review.\n\n\n# Visualizing LDA results with pyLDAvis"},{"metadata":{"trusted":true,"_uuid":"520045555b8cd5ebf89634e580a23f8a221e34c1"},"cell_type":"code","source":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda, data_vectorized, vectorizer, mds='tsne')\ndash","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb464a9c9c42de1d31dc0bcf4e850cea40af468c"},"cell_type":"markdown","source":"## How to interpret this graph?\n1. Topics on the left while their respective keywords are on the right.\n2. Larger topics are more frequent and closer the topics, mor the similarity\n3. Selection of keywords is based on their frequency and discriminancy.\n\n**Hover over the topics on the left to get information about their keywords on the right.**"},{"metadata":{"_uuid":"1f13a1c5ecfac3f41146fe9ac3a63cb6eabdc6e3"},"cell_type":"markdown","source":"# Visualizing LSI(SVD) scatterplot\nWe will be visualizing our data for 2  topics to see similarity between keywords which is measured by distance with the markers using LSI model"},{"metadata":{"trusted":true,"_uuid":"19e1075ff2b3d1b62db2cfa9bb86d70e78507980"},"cell_type":"code","source":"svd_2d = TruncatedSVD(n_components=2)\ndata_2d = svd_2d.fit_transform(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8195af8b1d63c853343520cd8fa624507771ce6d"},"cell_type":"code","source":"trace = go.Scattergl(\n    x = data_2d[:,0],\n    y = data_2d[:,1],\n    mode = 'markers',\n    marker = dict(\n        color = '#FFBAD2',\n        line = dict(width = 1)\n    ),\n    text = vectorizer.get_feature_names(),\n    hovertext = vectorizer.get_feature_names(),\n    hoverinfo = 'text' \n)\ndata = [trace]\niplot(data, filename='scatter-mode')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee288a919e1f8b6fc473b62417fee9b4539e401"},"cell_type":"markdown","source":"## The text version of scatter plot looks messy but you can zoom it for great results"},{"metadata":{"trusted":true,"_uuid":"5a416105fd12940af0d8d77d3d03bf4addb771af"},"cell_type":"code","source":"trace = go.Scattergl(\n    x = data_2d[:,0],\n    y = data_2d[:,1],\n    mode = 'text',\n    marker = dict(\n        color = '#FFBAD2',\n        line = dict(width = 1)\n    ),\n    text = vectorizer.get_feature_names()\n)\ndata = [trace]\niplot(data, filename='text-scatter-mode')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"075d5be90085a1d5a197612b5e569768de478b57"},"cell_type":"markdown","source":"Let's see what happens when we use a spaCy based bigram tokenizer for topic modelling"},{"metadata":{"trusted":true,"_uuid":"2a49363c3f94f7d6a8b40dfd011a8b48d58b7e27"},"cell_type":"code","source":"def spacy_bigram_tokenizer(phrase):\n    doc = parser(phrase) # create spacy object\n    token_not_noun = []\n    notnoun_noun_list = []\n    noun = \"\"\n\n    for item in doc:\n        if item.pos_ != \"NOUN\": # separate nouns and not nouns\n            token_not_noun.append(item.text)\n        if item.pos_ == \"NOUN\":\n            noun = item.text\n        \n        for notnoun in token_not_noun:\n            notnoun_noun_list.append(notnoun + \" \" + noun)\n\n    return \" \".join([i for i in notnoun_noun_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d38a2f8e70cc004d5c50693efefa7e54565cbaaa"},"cell_type":"code","source":"bivectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, ngram_range=(1,2))\nbigram_vectorized = bivectorizer.fit_transform(wines[\"processed_description\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd6892137ce6f8b2b790d62628d666b53eedb597"},"cell_type":"markdown","source":"## LDA for bigram data"},{"metadata":{"trusted":true,"_uuid":"7a2ced631e81c128738660a5d50ae5e5861a54c9"},"cell_type":"code","source":"bi_lda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\ndata_bi_lda = bi_lda.fit_transform(bigram_vectorized)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f2f160b2983dfff02801e3562af163710f10f6"},"cell_type":"markdown","source":"### Topics for bigram model"},{"metadata":{"trusted":true,"_uuid":"607d1d2e5eb51d585eb5abaac8fa2cf4d6430647"},"cell_type":"code","source":"print(\"Bi-LDA Model:\")\nselected_topics(bi_lda, bivectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa5db271862a9d6ca236111867c97f3dcee924d1"},"cell_type":"code","source":"bi_dash = pyLDAvis.sklearn.prepare(bi_lda, bigram_vectorized, bivectorizer, mds='tsne')\nbi_dash","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e246e81d3d9687a5c1ab5120208fc45ce76c3d"},"cell_type":"markdown","source":"**Very few keywords with 2 words have been found like \"spin dry\" , \"black cherry\", etc.**"},{"metadata":{"_uuid":"13934cbb75a849b3c6f9b09338fbb5039d6fbad8"},"cell_type":"markdown","source":"Kindly upvote and comment if you like this."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}