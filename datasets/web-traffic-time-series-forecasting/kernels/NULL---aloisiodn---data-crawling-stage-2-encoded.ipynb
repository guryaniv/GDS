{"metadata": {"language_info": {"file_extension": ".py", "name": "python", "version": "3.6.1", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_cell_guid": "6e4d701a-b37c-44b1-b9c8-9d82acbc451a", "_uuid": "05e11db0fa8e8a17f34b7a3d5d90290109c40d07", "collapsed": true}, "cell_type": "code", "source": ["# crawling data from 2017-09-01 to 2017-09-07\n", "import urllib\n", "import pandas as pd\n", "import numpy as np\n", "import multiprocessing\n", "import warnings\n", "import json\n", "\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "\n", "def get_views(web_info):\n", "    global date\n", "    purl = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/' \\\n", "          '{0}/{1}/{2}/{3}/daily/{4}/{5}' \\\n", "        .format(web_info[0], web_info[1], web_info[2], urllib.parse.quote(web_info[3]).replace(\"/\", \"%2F\"), date[0], date[-1])\n", "    #print(url)\n", "    #print(qurl)\n", "    res = np.array([np.nan for i in date])\n", "\n", "    ok = True\n", "\n", "\n", "    for tries in range(5):\n", "\n", "        try:\n", "            url = urllib.request.urlopen(purl)\n", "            ret =url.read().decode()\n", "            api_res = json.loads(ret)['items']\n", "        except:\n", "            ok = False\n", "\n", "        if ok:\n", "            break\n", "    if not ok:\n", "        print(purl, 'erro')\n", "        return res\n", "\n", "    for i in api_res:\n", "        time = i['timestamp'][0:-2]\n", "        res[date.index(time)] = i['views']\n", "    return res\n", "\n", "\n", "def get_views_main(input_page):\n", "    pool_size = 4 #multiprocessing.cpu_count()\n", "    pool = multiprocessing.Pool(processes=pool_size)\n", "    res = pool.map(get_views, input_page)\n", "    pool.close()\n", "    pool.join()\n", "    return res\n", "\n", "\n", "date = [\n", "    '20170901',\n", "    '20170902',\n", "    '20170903',\n", "    '20170904',\n", "    '20170905',\n", "    '20170906',\n", "    '20170907',\n", "    '20170908',\n", "    '20170909'\n", "]\n", "\n", "import time\n", "\n", "print(\"Reading...\")\n", "\n", "pages = pd.read_csv(\"/data02/data/WTF/train_2.csv\", usecols=['Page'])\n", "page_details = pd.DataFrame([i.split(\"_\")[-3:] for i in pages[\"Page\"]],columns=[\"project\", \"access\", \"agent\"])\n", "page_details['PageFull'] = pages\n", "\n", "def name_split(row):\n", "    return row.PageFull.split('_'+row.project+'_')[0]\n", "\n", "page_details['Page'] = page_details.apply(name_split, axis=1)\n", "del page_details['PageFull']\n", "\n", "print(\"Crawling...\")\n", "start = time.time()\n", "page_web_traffic = np.array(get_views_main(page_details.values))\n", "\n", "print(\"Time:\", time.time()-start)\n", "\n", "print(\"total:\", len(page_web_traffic))\n"], "outputs": [], "execution_count": null}], "nbformat": 4}