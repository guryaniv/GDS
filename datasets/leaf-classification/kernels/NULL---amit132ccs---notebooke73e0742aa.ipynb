{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0351fe5-cd2f-33d4-0cfc-1dfe9e449573"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aed8918a-1ea7-fc6b-3adb-05a8a17156a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "def encode(train, test):\n",
        "    label_encoder = LabelEncoder().fit(train.species)\n",
        "    labels = label_encoder.transform(train.species)\n",
        "    classes = list(label_encoder.classes_)\n",
        "    test_ids = test.id\n",
        "    \n",
        "    train = train.drop(['species', 'id'], axis=1)\n",
        "    test = test.drop('id', axis=1)\n",
        "\n",
        "    return train, labels, test, test_ids, classes\n",
        "\n",
        "train, labels, test, test_ids, classes = encode(train, test)\n",
        "\n",
        "# standardize train features\n",
        "scaler = StandardScaler().fit(train.values)\n",
        "scaled_train = scaler.transform(train.values)\n",
        "\n",
        "# split train data into train and validation\n",
        "sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\n",
        "for train_index, valid_index in sss.split(scaled_train, labels):\n",
        "    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n",
        "    y_train, y_valid = labels[train_index], labels[valid_index]\n",
        "    \n",
        "\n",
        "nb_features = 64 # number of features per features type (shape, texture, margin)   \n",
        "nb_class = len(classes) \n",
        "\n",
        "# reshape train data\n",
        "X_train_r = np.zeros((len(X_train), nb_features, 3))\n",
        "X_train_r[:, :, 0] = X_train[:, :nb_features]\n",
        "X_train_r[:, :, 1] = X_train[:, nb_features:128]\n",
        "X_train_r[:, :, 2] = X_train[:, 128:]\n",
        "\n",
        "# reshape validation data\n",
        "X_valid_r = np.zeros((len(X_valid), nb_features, 3))\n",
        "X_valid_r[:, :, 0] = X_valid[:, :nb_features]\n",
        "X_valid_r[:, :, 1] = X_valid[:, nb_features:128]\n",
        "X_valid_r[:, :, 2] = X_valid[:, 128:]\n",
        " \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c324243-032e-70dc-821b-e6ad735275a4"
      },
      "outputs": [],
      "source": [
        "X_train_r = np.reshape(X_train_r,(891,3,8,8))\n",
        "X_valid_r = np.reshape(X_valid_r,(99,3,8,8))\n",
        "y_train = y_train.astype(np.int32)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd4573c8-387b-5bcd-2402-0a3377f562ab"
      },
      "outputs": [],
      "source": [
        "import theano\n",
        "import lasagne\n",
        "from lasagne import layers\n",
        "from lasagne.updates import nesterov_momentum\n",
        "from nolearn.lasagne import NeuralNet\n",
        "from nolearn.lasagne import visualize\n",
        "\n",
        "\n",
        "net1 = NeuralNet(\n",
        "    layers=[('input', layers.InputLayer),\n",
        "            ('conv2d1', layers.Conv2DLayer),\n",
        "            ('dropout1', layers.DropoutLayer),\n",
        "            ('dense1', layers.DenseLayer), \n",
        "            ('dropout2', layers.DropoutLayer),\n",
        "            ('dense2', layers.DenseLayer),\n",
        "            ('output', layers.DenseLayer),\n",
        "            ],\n",
        "    # input layer\n",
        "    input_shape=(None, 3, 8, 8),\n",
        "    # layer conv2d1\n",
        "    conv2d1_num_filters=512,\n",
        "    conv2d1_filter_size=(1, 1),\n",
        "    conv2d1_nonlinearity=lasagne.nonlinearities.rectify,\n",
        "    conv2d1_W=lasagne.init.GlorotUniform(),  \n",
        "    # dropout1\n",
        "    dropout1_p=0.5,    \n",
        "    # dense1\n",
        "    dense1_num_units=2048,\n",
        "    dense1_nonlinearity=lasagne.nonlinearities.rectify, \n",
        "    # dropout2\n",
        "    dropout2_p=0.5,  \n",
        "    # dense2\n",
        "    dense2_num_units=1024,\n",
        "    dense2_nonlinearity=lasagne.nonlinearities.rectify,   \n",
        "    # output\n",
        "    output_nonlinearity=lasagne.nonlinearities.softmax,\n",
        "    output_num_units=nb_class,\n",
        "    # optimization method params\n",
        "    update=nesterov_momentum,\n",
        "    update_learning_rate=0.01,\n",
        "    update_momentum=0.9,\n",
        "    max_epochs=20,\n",
        "    verbose=1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f069944-f7da-df8e-d26a-aad503690e19"
      },
      "outputs": [],
      "source": [
        "nn = net1.fit(X_train_r, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "363277e0-b67c-5c0e-e765-4a1a560f7d4d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss\n",
        " \n",
        "test = np.array(test)\n",
        "\n",
        "#reshape test data\n",
        "X_test = np.zeros((len(test), nb_features, 3))\n",
        "X_test[:, :, 0] = test[:, :nb_features]\n",
        "X_test[:, :, 1] = test[:, nb_features:128]\n",
        "X_test[:, :, 2] = test[:, 128:]\n",
        "X_test = np.reshape(X_test,(594,3,8,8))\n",
        " \n",
        "test_predictions = net1.predict_proba(X_test)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a856c71-40fd-d401-eb02-872dcacb51b8"
      },
      "outputs": [],
      "source": [
        "submission1 = pd.DataFrame(test_predictions, columns=classes)\n",
        "submission1.insert(0, 'id', test_ids)\n",
        "submission1.reset_index()\n",
        "#submission1 = submission1.drop(submission1.columns[0], axis=1)\n",
        "submission1.to_csv('submission1.csv')\n",
        "submission1.tail()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}