{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ec556a2d-bf3b-4a56-3927-a48d696ca623"
      },
      "source": [
        "**Cluster Visualization - Assets based on Structural NAN values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c638ca75-42a5-5603-243f-406666063ac8"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93b5675c-65c5-9be8-86a5-632af2fe21ca"
      },
      "outputs": [],
      "source": [
        "#load the data. It comes in train.h5 so we need to us HDFStore\n",
        "df = pd.HDFStore(\"../input/train.h5\", \"r\").get(\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a39f361-566b-8212-fff5-69c89b8957b2"
      },
      "outputs": [],
      "source": [
        "# I make vectors that hold the ratio of NAN values for a given id in a given column\n",
        "unique_ids = pd.unique(df.id)\n",
        "len(unique_ids)\n",
        "NaN_vectors = np.zeros(shape=(len(unique_ids), df.shape[1]))\n",
        "\n",
        "for i, i_id in enumerate(unique_ids):\n",
        "    data_sub = df[df.id ==i_id]\n",
        "    NaN_vectors[i,:] = np.sum(data_sub.isnull(),axis=0) /float(data_sub.shape[0])\n",
        "    \n",
        "NaN_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc7d4e71-dbd5-2dc6-faee-731217bfd54e"
      },
      "outputs": [],
      "source": [
        "# get all the NaN vectors in which every collumn for that ID is NaN. What we are looking for \n",
        "#is collumns in which the features fundamentally do not exist. \n",
        "bin_NaN = 1*(NaN_vectors==1)\n",
        "print(\"Still has the shape of {} by {}\".format(bin_NaN.shape[0],bin_NaN.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a1ce23d-d7e3-bb93-0adb-acab37dd764c"
      },
      "outputs": [],
      "source": [
        "# we now have a vector of things that are either 1 where nothing exists in teh column or zero something\n",
        "#exists Now we take a covariance over these bins to see which ones move togehter. we are looking only based on columns\n",
        "bin_cov=np.corrcoef(bin_NaN.T)\n",
        "bin_cov.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77c087e6-49ba-be75-1e66-1aad3a441d3b"
      },
      "outputs": [],
      "source": [
        "# plot bin_cov\n",
        "plt.matshow(bin_cov)\n",
        "\n",
        "#if you think abou what this shows it is show the probability that when an entire column is missing what\n",
        "# is the probability that another column will be completely missing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e350bc14-5d70-8a1c-1ded-e792368884e5"
      },
      "outputs": [],
      "source": [
        "# In this graph i make the matrix sparse by considering only things that have perfect correlation. This\n",
        "# gives us insight into the relationship of the pairs.\n",
        "plt.matshow(bin_cov == 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a3979287-a84e-706a-2341-f1af89557559"
      },
      "outputs": [],
      "source": [
        "# What we are doing here is looking at all the column pairs that when they are missing  they are always\n",
        "# missing together. ie they have a corralation of 1. We also get the count. it stands to reason that\n",
        "# if this happens in only one or two id's out of 1400 then perhaps it is a statistical anomoly or could be \n",
        "# reflective of a non structural issue. This is actually very enlightening and we see there are 60\n",
        "# some odd pairs that satisfy this criteria. More importantly is that it happens for lots of tickers.\n",
        "# Maybe we have soemthing here.\n",
        "bin_NaN\n",
        "edges = []\n",
        "count =np.dot(bin_NaN.T,bin_NaN)\n",
        "for i in range(bin_cov.shape[0]):\n",
        "    for j in range(bin_cov.shape[0]-i):\n",
        "        if i!=i+j and bin_cov[i,i+j]==1:\n",
        "            edges.append([i,i+j,count[i,i+j]])\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e037fdd2-db33-0421-8da1-b22aaca639b5"
      },
      "outputs": [],
      "source": [
        "#lets see how many unique counts there are. it looks like a few of these counts happen multiple times.\n",
        "# this is interesting and could imply some structural issue.\n",
        "ucount = [i[2] for i in edges]\n",
        "print(np.unique(ucount))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a8741a6-7562-d688-5191-4716d11f14d6"
      },
      "outputs": [],
      "source": [
        "print('rows: {}'.format(bin_NaN.shape[0]))\n",
        "print('cols: {}'.format(len(edges)))\n",
        "\n",
        "# the idea here is that we create a feature vector. We look at all the ids which have all their data\n",
        "# missing in a certain collumn. above we found that if all the data is missing in a certain collumn it\n",
        "# would be missing in another collumn as well. so we look at all these pairs (shown as edges) and we \n",
        "# then create a matrix of id x edges. We then put a 0 or a 1 in the collumn to indicate that the pair of \n",
        "# data is missing or not. This serves as a feature. I will then go on to cluster over these features. \n",
        "\n",
        "nan_features = np.zeros((bin_NaN.shape[0],len(edges)))\n",
        "for i in range(bin_NaN.shape[0]):\n",
        "    for j, edge in enumerate(edges):\n",
        "        nan_features[i,j] = 1*(bin_NaN[i,edge[0]] & bin_NaN[i,edge[1]])\n",
        "\n",
        "print('this is just a check that indexing is correct: {}'.format(np.sum(nan_features,axis=1).shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2183b235-fadb-0ceb-7609-c8464d4ec51b"
      },
      "outputs": [],
      "source": [
        "# we take a look at the silouette score as we increase the number of clusters to understand the optimal\n",
        "#number of clusters. We see here that it continues to increase which we would expect. I chose to cut it\n",
        "# off around 12\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "#Range for k\n",
        "kmin = 2\n",
        "kmax = 25\n",
        "sil_scores = []\n",
        "\n",
        "#Compute silouhette scoeres\n",
        "for k in range(kmin,kmax):\n",
        "    km = KMeans(n_clusters=k, n_init=20).fit(nan_features)\n",
        "    sil_scores.append(silhouette_score(nan_features, km.labels_))\n",
        "\n",
        "#Plot\n",
        "plt.plot(range(kmin,kmax), sil_scores)\n",
        "plt.title('KMeans Results')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "283ae6e1-bf98-6176-b70c-6e80bcedd645"
      },
      "outputs": [],
      "source": [
        "# DBSCAN The only thing that is important from here is the labels. \n",
        "# the way that DB scan works is that you give it eps and min_samples and it\n",
        "# finds core groups. eps is the distance cut off and min is how many elements\n",
        "# at minimum you need to define a cluster\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = nan_features\n",
        "\n",
        "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "print('Estimated number of clusters: %d' % n_clusters_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "14828a1a-6fa7-d3f3-c608-b7a0dd2ec028"
      },
      "outputs": [],
      "source": [
        "#COLOR MAPPING - we run a kmeans cluster but we need to decide how many\n",
        "#clusters we want to use. This is another way for us to cluster. Here we use \n",
        "k=12\n",
        "km = KMeans(n_clusters=k, n_init=20).fit(nan_features)\n",
        "colors=km.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60f8b819-9a21-b6ed-75e7-d88ddbf768bc"
      },
      "outputs": [],
      "source": [
        "#now we try to visualize the data of these features.\n",
        "# WOW LOOK AT THESE RESULTS. THAT IS BEAUTIFUL!!\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from time import time\n",
        "\n",
        "n_iter = 5000\n",
        "\n",
        "for i in [2, 5, 30, 50, 100]:\n",
        "    t0 = time()\n",
        "    model = TSNE(n_components=2, n_iter = n_iter,random_state=0, perplexity =i)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = model.fit_transform(nan_features)\n",
        "    t1 =time()\n",
        "\n",
        "    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n",
        "    plt.scatter(Y[:, 0], Y[:, 1], c= colors)\n",
        "    plt.title('t-SNE with perplexity = {}'.format(i))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0f4dab0-42a5-eec9-d45e-4bf45c89b0b4"
      },
      "outputs": [],
      "source": [
        "# Now i do the same in 3d to try to better understand these clusters\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "n_iter = 5000\n",
        "\n",
        "for i in [2, 5, 30, 50, 100]:\n",
        "    fig = plt.figure(1, figsize=(8, 6))\n",
        "    ax = Axes3D(fig, elev=-150, azim=110)\n",
        "    t0 = time()\n",
        "    model = TSNE(n_components=3, random_state=0, perplexity=i, n_iter=n_iter)\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "    Y = model.fit_transform(nan_features)\n",
        "    t1 =time()\n",
        "\n",
        "    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n",
        "\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2],c=db.labels_,\n",
        "               cmap=plt.cm.Paired)\n",
        "    ax.set_title(\"3D T-SNE - Perplexity = {}\".format(i))\n",
        "    ax.set_xlabel(\"1st dim\")\n",
        "    ax.w_xaxis.set_ticklabels([])\n",
        "    ax.set_ylabel(\"2nd dim\")\n",
        "    ax.w_yaxis.set_ticklabels([])\n",
        "    ax.set_zlabel(\"3rd dim\")\n",
        "    ax.w_zaxis.set_ticklabels([])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec508dc1-d7c8-6436-d795-32c2e0170c3f"
      },
      "outputs": [],
      "source": [
        "# I will use PCA now to plot\n",
        "from sklearn import decomposition\n",
        "\n",
        "# I chose this number pretty much at random, you can change it. using 22 features to describe\n",
        "# something with 110 x variables still seems high.\n",
        "n_eigens = 22\n",
        "# Creating PCA object\n",
        "pca = decomposition.PCA(n_components=n_eigens, svd_solver ='randomized', whiten=True)\n",
        "X_pca =pca.fit_transform(nan_features)\n",
        "X_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f64c663-722a-20de-91c7-99ee2395094b"
      },
      "source": [
        "**2D -PCA Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2259ef23-3f4c-d860-7351-08dd953f31db"
      },
      "outputs": [],
      "source": [
        "# This is a 2D pca plot... mehhhh\n",
        "plt.scatter(X_pca[:,0],X_pca[:,1],c=colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4eddb2fd-aba3-a869-df32-96064669205f"
      },
      "outputs": [],
      "source": [
        "# To getter a better understanding of interaction of the dimensions\n",
        "# plot the first three PCA dimensions\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(1, figsize=(8, 6))\n",
        "ax = Axes3D(fig, elev=-150, azim=110)\n",
        "X_reduced = decomposition.PCA(n_components=3).fit_transform(nan_features)\n",
        "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],c=colors,\n",
        "           cmap=plt.cm.Paired)\n",
        "ax.set_title(\"First three PCA directions\")\n",
        "ax.set_xlabel(\"1st eigenvector\")\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"2nd eigenvector\")\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"3rd eigenvector\")\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "40b8eb67-aa03-f95c-e414-0c5c265d2788"
      },
      "outputs": [],
      "source": [
        "# here we plot a graph that looks at how many PCA components explain the variation\n",
        "n_eigens=10\n",
        "X_reduced = decomposition.PCA(n_components=n_eigens).fit(nan_features)\n",
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(8, 5));\n",
        "    plt.title('Explained Variance Ratio over Component');\n",
        "    plt.plot(X_reduced.explained_variance_ratio_);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fca591e6-61bc-660e-cb9f-5d9939c9dc16"
      },
      "outputs": [],
      "source": [
        "with plt.style.context('fivethirtyeight'):\n",
        "    plt.figure(figsize=(8, 5));\n",
        "    plt.title('Cumulative Explained Variance over EigenFace');\n",
        "    plt.plot(X_reduced.explained_variance_ratio_.cumsum());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "998a9d89-6bc3-3adb-199f-97a05893c9dd"
      },
      "outputs": [],
      "source": [
        "print('PCA captures {:.2f} percent of the variance in the dataset'.format(X_reduced.explained_variance_ratio_.sum() * 100))\n",
        "print('PCA components have dimensions {} by {}'.format(*X_reduced.components_.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71b6d0be-fa15-0c53-bcd9-b9abf329504f"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}