{"nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["### What is this kernel for?\n", "To demo how to easily add text content from wikipedia to our dataset.\n", "\n", "This might allow us to extract some interesting features to use in the competition. I might explore this idea in another Kernel.\n", "\n", "\n", "---------\n", "__Note__ Unfortunately Kaggle blocks Internet access so this kernel won't retrieve the data but it should run just fine outside Kaggle.\n", "\n"], "metadata": {"_execution_state": "idle", "_uuid": "6f67e4180602a11191cacdb397a9b4b6a3860d34", "_cell_guid": "3db0605c-7ca1-4972-b061-ea2beb29cb98"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "from bs4 import BeautifulSoup\n", "from tqdm import tqdm, tqdm_notebook\n", "import time\n", "import requests\n", "\n", "\n", "SLEEP_TIME_S = 0.1"], "metadata": {"_execution_state": "idle", "_uuid": "8093be598878cd036d6c7ff7ee4b2416d57b7e14", "collapsed": true, "_cell_guid": "1521d106-994c-414c-b126-c89a4cba60bf"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def extract_URL_and_Name(page):\n", "    \"\"\" From the page name in the input file extract the Name and the URL \"\"\"\n", "    return (['_'.join(page.split('_')[:-3])]\n", "            + ['http://' + page.split(\"_\")[-3:-2][0] +\n", "               '/wiki/' + '_'.join(page.split('_')[:-3])])"], "metadata": {"_execution_state": "idle", "_uuid": "c56a3cd9ecd2b83323584c02af2152bc42f08540", "collapsed": true, "_cell_guid": "75fba246-4d7b-4df0-bb68-edba34642579"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Load the dataset\n", "train = pd.read_csv('../input/train_1.csv')\n", "\n", "# We will just take a sample of the data, \n", "# remove this line to run on all the data\n", "train = train.sample(2)\n", "\n", "# Extract the Page name and URL:\n", "page_data = pd.DataFrame(\n", "    list(train['Page'].apply(extract_URL_and_Name)),\n", "    columns=['Name', 'URL'])\n"], "metadata": {"_execution_state": "idle", "_uuid": "1574984b479d1774795a58879bf069fb8bef73e1", "collapsed": true, "_cell_guid": "9347faac-296b-4342-b7e5-ce0d976820c3"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["page_data.head()"], "metadata": {"_uuid": "6709faf2d86610adfb0aae435f72d3c420efe03c", "collapsed": true, "_cell_guid": "8100dfd0-f134-43f1-8589-1d317296e17d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Since Kaggle kernels don't have internet access this method will always return \n", "# an empty string\n", "def fetch_wikipedia_text_content(row):\n", "    \"\"\"Fetch the all text data of a given page\"\"\"\n", "    try:\n", "        r = requests.get(row['URL'])\n", "        # Sleep for 100 ms so that we don't use too many Wikipedia resources \n", "        time.sleep(SLEEP_TIME_S)\n", "        to_return = [x.get_text() for x in \n", "                     BeautifulSoup(\n", "                         r.content, \"html.parser\"\n", "                     ).find(id=\"mw-content-text\").find_all('p')]\n", "    except:\n", "        to_return = [\"\"]\n", "    return to_return"], "metadata": {"_execution_state": "idle", "_uuid": "838e2a0c95bf1e41a8a9324a7226b7b06bad8c6e", "collapsed": true, "_cell_guid": "d11acfc4-897e-43a4-b10c-5655b3c5d4e0"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This will fail due to lack of Internet\n", "tqdm.pandas(tqdm_notebook)\n", "page_data['TextData'] = page_data.progress_apply(fetch_wikipedia_text_content, axis=1)\n", "\n", "page_data.head()"], "metadata": {"_execution_state": "idle", "_uuid": "0d6e195e63cc988a1a4d8a5ee6a39c15b25820ae", "collapsed": true, "_cell_guid": "89182bc3-d6ed-43c9-a8fe-bff0898c439f"}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"name": "python", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 1}