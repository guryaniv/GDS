{"cells": [{"cell_type": "markdown", "source": "This is the stacking model using NN. This gives the public LB score at 0.30409, and private LB score 0.31063. The score will be ranked at 4 on public LB, and 5 on private LB (generalized well?) This and the other kernels I posted are the best part of my model, the other parts are not very informative.\n\nThe core idea of this script is to replace linear combination and eliminate magic numbers. From the script, it contains no magic number anymore, however, the actual base model still got magic number. If we remove all the magic numbers from the base model, an earlier version of this script got LB score 0.3065. With this version, I think the LB score would be 0.305. This is not tested on public LB, but this is the relative improvement for the magic number version (from 0.305 to 0.304).\n\nThis script is far from optimal, for example fixing the investment=null in test data would bring some improvements and so on. If BoxCox is used, the performance can be substantially improved. If you combine it with your own model, top 3 \n on both private/public board would definitely be possible.\n\nYou can read the method nn() and prepare_data() for more details.\n\nI cannot run this script in kernel as it needs the base model's output which can be generated using \n\n1.  https://www.kaggle.com/schoolpal/nn-model-lb-0-306-to-0-308\n2. https://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094\n3. https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script\n\nwith 5 fold non-shuffle training.\n", "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "04d4701d-7579-401f-9350-e7b6c368df5f", "_execution_state": "idle", "_uuid": "1151a9e323056b0972ed7d8b9d35e0ed176697ab", "collapsed": false}}, {"cell_type": "code", "source": "import os,sys\nfrom keras.layers.advanced_activations import *\nfrom keras.callbacks import LearningRateScheduler\nimport pickle\nfrom keras.layers.merge import *\nfrom keras.layers.noise import *\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer,Imputer,RobustScaler\nfrom keras.optimizers import SGD,RMSprop,Adam,Adadelta\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense, Dropout, Activation, Input, Embedding,Flatten,Lambda\nfrom keras.layers.normalization import BatchNormalization\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\nimport pdb\nimport keras\nNORM=False\nLOG=True\n\ndef get_excluded():\n    # Taken from wti200's kernel\n    excluded={\n        \"young_male\", \"school_education_centers_top_20_raion\", \"0_17_female\", \"railroad_1line\", \"7_14_female\", \"0_17_all\", \"children_school\",\"ecology\", \"16_29_male\", \"mosque_count_3000\", \"female_f\", \"church_count_1000\", \"railroad_terminal_raion\",\"mosque_count_5000\", \"big_road1_1line\", \"mosque_count_1000\", \"7_14_male\", \"0_6_female\", \"oil_chemistry_raion\",\"young_all\", \"0_17_male\", \"ID_bus_terminal\", \"university_top_20_raion\", \"mosque_count_500\",\"ID_big_road1\",\"ID_railroad_terminal\", \"ID_railroad_station_walk\", \"ID_big_road2\", \"ID_metro\", \"ID_railroad_station_avto\",\"0_13_all\", \"mosque_count_2000\", \"work_male\", \"16_29_all\", \"young_female\", \"work_female\", \"0_13_female\",\"ekder_female\", \"7_14_all\", \"big_church_count_500\",\"leisure_count_500\", \"cafe_sum_1500_max_price_avg\", \"leisure_count_2000\",\"office_count_500\", \"male_f\", \"nuclear_reactor_raion\", \"0_6_male\", \"church_count_500\", \"build_count_before_1920\",\"thermal_power_plant_raion\", \"cafe_count_2000_na_price\", \"cafe_count_500_price_high\",\"market_count_2000\", \"museum_visitis_per_100_cap\", \"trc_count_500\", \"market_count_1000\", \"work_all\", \"additional_education_raion\",\"build_count_slag\", \"leisure_count_1000\", \"0_13_male\", \"office_raion\",\"raion_build_count_with_builddate_info\", \"market_count_3000\", \"ekder_all\", \"trc_count_1000\", \"build_count_1946-1970\",\"office_count_1500\", \"cafe_count_1500_na_price\", \"big_church_count_5000\", \"big_church_count_1000\", \"build_count_foam\",\"church_count_1500\", \"church_count_3000\", \"leisure_count_1500\",\"16_29_female\", \"build_count_after_1995\", \"cafe_avg_price_1500\", \"office_sqm_1000\", \"cafe_avg_price_5000\", \"cafe_avg_price_2000\",\"big_church_count_1500\", \"full_all\", \"cafe_sum_5000_min_price_avg\",\"office_sqm_2000\", \"church_count_5000\",\"0_6_all\", \"detention_facility_raion\", \"cafe_avg_price_3000\"\"young_male\", \"school_education_centers_top_20_raion\", \"0_17_female\", \"railroad_1line\", \"7_14_female\", \"0_17_all\", \"children_school\",\"ecology\", \"16_29_male\", \"mosque_count_3000\", \"female_f\", \"church_count_1000\", \"railroad_terminal_raion\",\"mosque_count_5000\", \"big_road1_1line\", \"mosque_count_1000\", \"7_14_male\", \"0_6_female\", \"oil_chemistry_raion\",\"young_all\", \"0_17_male\", \"ID_bus_terminal\", \"university_top_20_raion\", \"mosque_count_500\",\"ID_big_road1\",\"ID_railroad_terminal\", \"ID_railroad_station_walk\", \"ID_big_road2\", \"ID_metro\", \"ID_railroad_station_avto\",\"0_13_all\", \"mosque_count_2000\", \"work_male\", \"16_29_all\", \"young_female\", \"work_female\", \"0_13_female\",\"ekder_female\", \"7_14_all\", \"big_church_count_500\",\"leisure_count_500\", \"cafe_sum_1500_max_price_avg\", \"leisure_count_2000\",\"office_count_500\", \"male_f\", \"nuclear_reactor_raion\", \"0_6_male\", \"church_count_500\", \"build_count_before_1920\",\"thermal_power_plant_raion\", \"cafe_count_2000_na_price\", \"cafe_count_500_price_high\",\"market_count_2000\", \"museum_visitis_per_100_cap\", \"trc_count_500\", \"market_count_1000\", \"work_all\", \"additional_education_raion\",\"build_count_slag\", \"leisure_count_1000\", \"0_13_male\", \"office_raion\",\"raion_build_count_with_builddate_info\", \"market_count_3000\", \"ekder_all\", \"trc_count_1000\", \"build_count_1946-1970\",\"office_count_1500\", \"cafe_count_1500_na_price\", \"big_church_count_5000\", \"big_church_count_1000\", \"build_count_foam\",\"church_count_1500\", \"church_count_3000\", \"leisure_count_1500\",\"16_29_female\", \"build_count_after_1995\", \"cafe_avg_price_1500\", \"office_sqm_1000\", \"cafe_avg_price_5000\", \"cafe_avg_price_2000\",\"big_church_count_1500\", \"full_all\", \"cafe_sum_5000_min_price_avg\",\"office_sqm_2000\", \"church_count_5000\",\"0_6_all\", \"detention_facility_raion\", \"cafe_avg_price_3000\"\n    }\n    return excluded\n\ndef step_decay(epoch):\n    lr=0.01\n    start=5\n    step=5\n    if epoch<start:\n        return lr\n    else:\n        lr=lr/np.power(2.0,(1+(epoch-start)/step))\n        return lr\n\n\n\n    return weights\n# Time based sample weights, this improves the LB score only\n# a little bit(0.0002)\ndef time_weights(train,prices,price_sq):\n    weights=np.ones(len(price_sq))\n    weights[(train.timestamp.dt.year==2011)]*=0.5\n    weights[(train.timestamp.dt.year==2012)]*=0.8\n    weights[(train.timestamp.dt.year==2013)]*=1.4\n    weights[(train.timestamp.dt.year==2014)]*=0.9\n    weights[(train.timestamp.dt.year==2015)]*=2\n    return weights\n\n\ndef prepare_data():\n    excluded=get_excluded()\n    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n    #These scores are generated using the following kernel with 5 fold non-shuffle traning:\n    # https://www.kaggle.com/schoolpal/nn-model-lb-0-306-to-0-308\n    # https://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094\n    # https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script\n    \n    (xgb_train,xgb_test)=pickle.load(open('xgb_predicted.pkl'))\n    (xgb_train_log,xgb_test_log)=pickle.load(open('xgb_predicted_log.pkl'))\n    (lgb_train,lgb_test)=pickle.load(open('lgb_predicted.pkl'))\n    (nn_train,nn_test)=pickle.load(open('nn_predicted_log.pkl'))\n    df_train['xgb_score']=xgb_train\n    df_train['xgb_score_log']=xgb_train_log\n    df_train['log_xgb_score']=np.log(xgb_train)\n    df_train['log_xgb_score_log']=np.log(xgb_train_log)\n    df_train['nn_score']=nn_train\n    df_train['nn_score_log']=np.log(nn_train)\n    df_train['lgb_score']=lgb_train\n    df_train['lgb_score_log']=np.log(lgb_train)\n    df_test['xgb_score']=xgb_test\n    df_test['xgb_score_log']=xgb_test_log\n    df_test['log_xgb_score']=np.log(xgb_test)\n    df_test['log_xgb_score_log']=np.log(xgb_test_log)\n    df_test['nn_score']=nn_test\n    df_test['nn_score_log']=np.log(nn_test)\n    df_test['lgb_score']=lgb_test\n    df_test['lgb_score_log']=np.log(lgb_test)\n \n    full_sq=df_train.full_sq.copy()\n    full_sq[full_sq<5]=np.NaN\n    \n    price_sq=df_train.price_doc/full_sq\n    df_train=df_train[(price_sq<600000) & (price_sq>10000)]\n    price_sq=price_sq[(price_sq<600000) & (price_sq>10000)]\n\n\n    y_train=df_train.price_doc\n    df_train.drop(['price_doc'],inplace=True,axis=1)\n    num_train=df_train.shape[0]\n    \n    da=pd.concat([df_train,df_test])\n    da=da.reset_index(drop=True)\n\n\n    da['build_year_0']=(da.build_year==0).astype(int)\n    da['build_year_1']=(da.build_year==1).astype(int)\n    da['build_year_null']=(da.build_year.isnull()).astype(int)\n    da['build_year_not_1']=(da.build_year!=1).astype(int)\n    da['olds']=da.timestamp.dt.year-da.build_year\n\n    da['investment_very_old']=((da.product_type=='Investment') & (da.olds>54)).astype(int)\n    da['not_investment_very_old']=((da.product_type!='Investment') | (da.olds<=54)).astype(int)\n    da['investment_old']=((da.product_type=='Investment') & (da.olds>10)).astype(int)\n    da['not_investment_old']=((da.product_type!='Investment') | (da.olds<=10)).astype(int)\n    da['investment_new']=((da.product_type=='Investment') & (da.olds<=10)).astype(int)\n    da['not_investment_new']=((da.product_type!='Investment') | (da.olds>10)).astype(int)\n\n    da['own_new']=((da.product_type=='OwnerOccupier') & (da.olds==0)).astype(int)\n    da['not_own_new']=((da.product_type!='OwnerOccupier') | (da.olds==0)).astype(int)\n    da['year_month']=da.timestamp.dt.year\n    da['year_month']=(da['year_month']*100+da.timestamp.dt.month)\n    da['year_month']=da['year_month'].astype(str)\n    da['month']=da.timestamp.dt.month.astype(str)\n    da['close_to_green']=(da.green_zone_km<0.05).astype(int)\n    da['close_to_rail']=(da.railroad_km<0.5).astype(int)\n    da['close_to_school']=(da.school_km<0.1).astype(int)\n\n    cols=[]\n    # The actual feature used, only use the feature related to the building of the property but not the property itself (e.g. full_sq,life_sq)\n    \n    cols1=['timestamp','id','product_type','olds','max_floor','year_month','close_to_green','close_to_rail','close_to_school','build_year_1','build_year_not_1','investment_olds6','not_investment_very_old','investment_old','not_investment_old','month']\n    \n    price_cols=['log_xgb_score','lgb_score_log','nn_score_log']\n    cols=set(cols1)\n    cols.update(price_cols)\n    da=da.loc[:,cols]\n    df_cat=None\n    to_remove=[]\n    for c in da.columns:\n        if da[c].dtype=='object':\n            oh=pd.get_dummies(da[c],prefix=c)\n            \n            if df_cat is None:\n                df_cat=oh\n            else:\n                df_cat=pd.concat([df_cat,oh],axis=1)\n            to_remove.append(c)\n    da.drop(to_remove,inplace=True,axis=1)\n    if df_cat is not None:\n        sums=df_cat.sum(axis=0)\n        to_remove=sums[sums<200].index.values\n        df_cat=df_cat.loc[:,df_cat.columns.difference(to_remove)]\n        da = pd.concat([da, df_cat], axis=1)\n    # The macro features\n    macro_cols=['timestamp','eurrub','unemployment','brent','emigrant','mortgage_growth']\n    macro=pd.read_csv('input/macro.csv',parse_dates=['timestamp'])\n    # Add the unemployment data from OCED and emigrant data from Fedora state static website\n    # macro=macro_lib.fix(macro)\n    macro=macro.loc[:,macro_cols]\n    da=da.join(macro.set_index('timestamp'),on='timestamp')\n    da[da==np.inf]=np.NaN\n    if 'index' in da.columns:\n        da.drop(['index'],inplace=True,axis=1)\n    sample_weights=time_weights(df_train,y_train,price_sq)\n    train=da[:num_train].drop(['timestamp','id'],axis=1)\n    test=da[num_train:].drop(['timestamp','id'],axis=1)\n    aux_cols=[c for c in train.columns if c not in price_cols and (c in cols1 or c not in cols)]\n\n    train_prices=train.loc[:,price_cols]\n    train_aux1=train.loc[:,aux_cols]\n    test_prices=test.loc[:,price_cols]\n    test_aux1=test.loc[:,aux_cols]\n    bin_inds=[]\n    for c in train.columns:\n        if train.loc[:,c].unique().shape[0]==2 and train.loc[:,c].unique().sum()==1:\n            bin_inds.append(train.columns.get_loc(c))\n    return train_prices,test_prices,train_aux1,test_aux1,y_train,da[num_train:].id,sample_weights,bin_inds\n\ndef norm(train,test):\n    all_data=np.vstack((train,test))\n    original=all_data.copy()\n    bin_inds=[]\n    for ci in range(all_data.shape[1]):\n        if np.unique(all_data[:,ci]).shape[0]==2 and all_data[:,ci].max()==1:\n            bin_inds.append(ci)\n    if len(bin_inds)>0:\n        bin_data=original[:,bin_inds].astype(int)\n        bin_data[np.isnan(bin_data)]=0\n        all_data=np.delete(all_data,bin_inds,axis=1)\n\n    \n    imputer=Imputer(strategy='mean',copy=True,axis=0)\n    all_data=imputer.fit_transform(all_data)\n    STD_LIMIT=4\n    to_remove=[]\n    for ci in range(all_data.shape[1]):\n        dc=all_data[:,ci].copy()\n        mean=np.mean(all_data[:,ci])\n        std=np.std(all_data[:,ci])\n        if std==0:\n            to_remove.append(ci)\n        else:\n            all_data[(dc-mean)/float(std)>STD_LIMIT,ci]=mean\n            all_data[(dc-mean)/float(std)<-STD_LIMIT,ci]=mean\n    all_data=np.delete(all_data,to_remove,axis=1)\n\n    train=all_data[0:train.shape[0],:]\n    test=all_data[train.shape[0]:,:]\n\n    scaler=StandardScaler()\n    scaler.fit(np.vstack((train,test)))\n    train=scaler.transform(train)\n    test=scaler.transform(test)\n\n    if len(bin_inds)>0:\n        all_data=np.vstack((train,test))\n        all_data=np.hstack((all_data,bin_data))\n        train=all_data[0:train.shape[0],:]\n        test=all_data[train.shape[0]:,:]\n\n    norm=Normalizer(norm='l2')\n    norm.fit(np.vstack((train,test)))\n    train=norm.transform(train)\n    test=norm.transform(test)\n    return train,test\ndef nn(train_price,test_price,train_aux1,test_aux1,y_train,sample_weights,bin_inds):\n    ymin=None\n    train_aux1,test_aux1=norm(train_aux1,test_aux1)\n\n    if LOG:\n        y_train=np.log1p(y_train)\n    price_in=Input(shape=(train_price.shape[1],),name='price_data')\n    aux_in1=Input(shape=(train_aux1.shape[1],),name='aux_data1')\n    model=Sequential()\n    # The following part learn the linear combination weights\n    aux_out=Dense(256,activation='relu')(aux_in1)\n    aux_out=Dropout(0.3)(aux_out)\n    aux_out=Dense(128,activation='relu')(aux_out)\n    aux_out=Dropout(0.3)(aux_out)\n    aux_out=Dense(64,activation='relu')(aux_out)\n    aux_out=Dense(3,activation='softmax')(aux_out)\n    # Do the linear combination\n    out=dot([price_in,aux_out],-1)\n    \n    scale_out=Dense(1,activation='hard_sigmoid',kernel_regularizer=regularizers.l1())(aux_in1)\n    # Scale the instace in the range 0.85-1.02, the weights are learned.\n    # This corrects some errors in the base model\n    scale_out=Lambda(lambda x: (0.17*x+0.85) )(scale_out)\n    out=multiply([scale_out,out])\n    graph=Model(inputs=[price_in,aux_in1],outputs=out)\n    print(graph.summary())\n    model.add(graph)\n    lrate=LearningRateScheduler(step_decay)\n    optimizer=SGD(lr=0.01, momentum=0.5,nesterov=True)\n    model.compile(loss = 'mse', optimizer = optimizer)\n    model.fit({'price_data':train_price,'aux_data1':train_aux1},y_train,batch_size=64,verbose=1,epochs=1,callbacks=[lrate],shuffle=False,sample_weight=sample_weights)#,validation_split=0.1)\n    predicted=np.expm1(model.predict({'price_data':test_price,'aux_data1':test_aux1})[:,0])\n\n    return predicted\nif __name__=='__main__':\n    pass\n#    train_prices,test_prices,train_aux1,test_aux1,y_train,id_test,sample_weights,bin_inds=prepare_data()\n#    predicted = nn(train_prices.values,test_prices.values,train_aux1.values,test_aux1.values,y_train.values,sample_weights,bin_inds)\n#    output = pd.DataFrame({'id': id_test, 'price_doc': predicted})\n#    output=output.reset_index(drop=True)\n#    test=pd.read_csv('input/test.csv',parse_dates=['timestamp'])\n#    output.to_csv('dnn2.csv', index=False)\n", "execution_count": null, "outputs": [], "metadata": {"trusted": false, "_cell_guid": "920544f2-19b3-4295-9a27-49f43777b9a4", "_execution_state": "idle", "_uuid": "6e408cdf781a0d62adb2079aebe98ba984aa8f5a"}}], "nbformat": 4, "nbformat_minor": 0, "metadata": {"language_info": {"version": "3.6.1", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}