{"nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"metadata": {"_uuid": "a8bd8014c05a6352a3ecd75024354c5cddbe6310", "_cell_guid": "db9be573-d8ae-4c47-a3ec-165180ba3fba"}, "source": ["Hi everyone !<br/>\n", "For this Kernel I want to make a tutorial about Text-mining with Spark. What's Spark ? What's Text-mining ? Why Spark could be interesting in Kaggle challenge ? And why Spark and Text-mining are good together ? Are the questions I will try to anwser.\n", "<br/>\n", "I will not make data visualization in this tutorial, I'm pretty sure a lot of people are far better than me for visualization. Unfortunatelly this notebook will not run on Kaggle because you need to install Spark.\n", "<br/>\n", "<h1> What's Apache Spark </h1>\n", "Spark ( https://spark.apache.org/ ) is a framework for distributed operations (basically the opposite of using one core on one computer) it works with several language : Java,Scala,R and Python. You use it with Hadoop when you have a too many data for one computer. So why create a tutorial for this challenge with only few data ? Well, when I look at all big challenge on Kaggle I'm pretty sure that some people do feature engineering and even ML with Spark in order to gain some time, and I really think that Spark will be more and more popular for ML and Kaggle challenge and I want to help people to begin with Spark.<br/>\n", "<br/>\n", "First you need to know that generelly Spark run with Hadoop on several computer but you can easly install it on your own computer (there are a lot of tutorial on internet) but remember the main purpose of Spark is to run on several computer.<br/>\n", "<br/>\n", "<h4> MapReduce operation </h4>\n", "Spark use the programming model mapreduce. A MapReduce program is composed of a Map() procedure (method) and a Reduce() procedure (method). In our case you (for data manipulation on dataset) Map is an operation that apply a function on each row of your dataset (example : replace the id by 1 on every line) and Reduce an operation that apply a function for combining your row (example : sum all the id of our data).<br/>\n", "<br/>\n", "In this notebook I would only use map so basically when you read :<br/>\n", "example_2=example_1.map(lambda x : normalize_token(x))<br/>\n", "It mean \"apply function normalize_token on every row of dataset example_1 this new dataset name is example_2\"<br/>\n", "<br/>\n", "So if I have 1 computer with 4 core the function will apply on 4 lines simultaneously, if I have 5 computer with 4 core on each the function will apply on 20 lines simultaneously !<br/>\n", "<br/>\n", "<h4> RDD VS Dataframe </h4>\n", "Since the beginning I'm speaking about dataset but it's not really correct, you need to know they are two different way to store your data in spark, both are distributed (your data are on several computer ).<br/>\n", "Basically Dataframe are similar to Pandas dataframe, with column, row a lot of function that already exist and are fast(example : droping NA) but you can't store every type and do what you want (a line with more element than an other etc..)<br/>\n", "On the other side you have RDD (Resilient Distributed Dataset) basically it's just a big array and every element(row) is a tuple(similar to an array), you can create your own function with MapReduce, all element(row) can have different size, and no one care about the type of your object in your RDD.<br/>\n", "You can easly switch RDD to DF and more easly DF to RDD, we will use both because both have advantages.<br/>\n", "<br/>\n", "<h1> Text-mining 101 </h1>\n", "<br/>\n", "In order to apply a Machine-learning algorithms you need a vector of number, so we need to transform our text into a vector. Classique methods that do this transformation need a BoW (bag of words) basically an array with word that help to classify the sentence.<br/>\n", "But before tranforming our text into vector we need to clean it. <br/>\n", "<br/>\n", "<h4> Cleaning your text </h4>\n", "Indeed there are some useless words like \"the\", \"I\",\"of\" etc... they are use too often or they doesn't mean precise things, they are call \"stopwords\". <br/>\n", "It's not the only transformation we can do, it's also important to change word to their lemma, basically change 'is','are'... to \"be\" and change \"rows\" to \"row\", this way word with the same meaning would write the same way.<br/>\n", "<br/>\n", "We can do more transformation like replace some word/acronym by other but I think you get the idea : you need to keep/extract information and reduce noise of your data.<br/>\n", "<br/>\n", "All those transformation exist with NLTK (natural language toolkit) a free library for text-mining in python<br/>\n", "<br/>\n", "After all this transformation you get an array of word (or BoW) now you can transform it into a vector of word<br/>\n", "<br/>\n", "<h4> BoW to vector </h4>\n", "<br/>\n", "There are two popular method for this, word2vec and tf-idf. I will not explain them they are a lot of explaination on internet and english is not my main language. But you juste need to know that :<br/>\n", "- W2V use the way word are used in order to change word into vectors and have similar vector for word that are used in a similar way<br/>\n", "- tf-idf mean term frequency inverse document frequency, for each phrase it create a vector of the size of your vocabulary and give them an important weight if they appear few time.<br/>\n", "<br/>\n", "Both methods can be good (like every machine learning algorithms)<br/>\n", "Once you have change your BoW to a vector you just need to apply a RandomForest or other and it's done.<br/>\n", "<br/>\n", "<h1> Why Spark is good with text-mining </h1>\n", "By reading this tutorial you probably realise that we transform a lot our data for text-mining challenge, and all those transformation are easy to parallelize. Indeed it's just applying a function on each row (so a map). So if you start a Text-mining challenge with billion of line, Spark would be really faster than using one core.<br/>\n", "<br/>\n", "After explaining the basics lets have a look at the code"], "cell_type": "markdown"}, {"source": ["import nltk\n", "import numpy as np\n", "from nltk.tokenize import TreebankWordTokenizer\n", "from nltk.corpus import stopwords\n", "from nltk.stem.porter import PorterStemmer\n", "from nltk.stem import WordNetLemmatizer\n", "import string\n", "sc"], "metadata": {"collapsed": true, "_uuid": "b9a769df73c1d699b0b7b47bf526293e9704e705", "_cell_guid": "d87f92d5-cd21-408d-87ad-67eeccb1efdc"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "3a4588e8a90bbae51fc600b9b21024e01c179718", "_cell_guid": "c84dcc2a-d7e4-4961-a8ec-3d5112b5841b"}, "source": ["First let's load our training data"], "cell_type": "markdown"}, {"source": ["df=spark.read.csv(\"train.csv\",header=True)\n", "df.show()"], "metadata": {"collapsed": true, "_uuid": "8e1e100f183bf5d28b010ba70b1568a802ad86c3", "_cell_guid": "e2bb89a3-07c2-4bfb-ae54-82765c0f7085", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "3d5dd0ec71930ad1f073aa389266bdcd68f743e5", "_cell_guid": "f189dd48-911b-4b55-9268-ce558a64996d"}, "source": ["In this function we create a Bag of word with our sentence, and count the number of char (it could be an interessting feature)"], "cell_type": "markdown"}, {"source": ["def normalize_token(x) :\n", "    tokenizer = TreebankWordTokenizer()\n", "    lower=x.text.lower()\n", "    text_token=tokenizer.tokenize(lower)\n", "    count_char=len(lower)\n", "    return x+(count_char,text_token,)\n", "\n", "rdd_normalize_token=df.rdd.map(lambda x : normalize_token(x))\n", "rdd_normalize_token.first()"], "metadata": {"collapsed": true, "_uuid": "4911bc0c82564b7fcb56853fd7f690cfb7ad3a94", "_cell_guid": "a2fd1ee9-3266-4408-9912-915a357f3f32", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "4930b48716143f01d615ac0d8e1acd95abf9d59c", "_cell_guid": "b11ba34b-a6ae-4240-ae0d-37db8abac3c4"}, "source": ["In this function we remove stopword, little word and punctuation, we count the number of character that are punctuation and replace all number by \"number\" (I want to keep the information that an author can use a lot of number) in order to normalize them"], "cell_type": "markdown"}, {"source": ["def remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct) :\n", "    tab_result=[]\n", "    count_punct=0\n", "    for elt in x[4] :\n", "        if elt in list_punct :\n", "            count_punct=count_punct+1\n", "        try :\n", "            number=float(elt)\n", "            tab_result.append(\"number\")\n", "        except :\n", "            if (len(elt)>seuil) &(elt not in stop_words) :\n", "                tab_result.append(elt)            \n", "    return x[:4]+(count_punct,tab_result,)\n", "    \n", "seuil=1\n", "stop_words=set(stopwords.words('english'))\n", "list_punct=list(string.punctuation)\n", "rdd_stop_word=rdd_normalize_token.map(lambda x : remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct))\n", "rdd_stop_word.first()"], "metadata": {"collapsed": true, "_uuid": "a3d82c9dccbf2dc33d98bec53174fbb6bea26057", "_cell_guid": "8a7caa4e-213b-485f-a9ac-8a1f25e846db", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "39068aec3fac550b79292b62eab97cedd7fd40a3", "_cell_guid": "1dbbb563-68e6-4634-b053-24a0f4df61f1"}, "source": ["We lemmatize our BoW with NLTK, we use POSTagging to help lemmatization"], "cell_type": "markdown"}, {"source": ["def lemmatisation(x,dict_cor) :\n", "    tab_result=[]\n", "    wordnet_lemmatizer = WordNetLemmatizer()\n", "    pos_tmp = nltk.pos_tag(x[5])\n", "    for elt in pos_tmp :\n", "        if elt[1][0] in dict_cor :\n", "            attrib=dict_cor[elt[1][0]]\n", "        else :\n", "            attrib = \"n\"\n", "        tab_result.append(wordnet_lemmatizer.lemmatize(elt[0], pos=attrib))\n", "    return x[:5]+(tab_result,)\n", "        \n", "dict_cor={\n", "    \"N\" : \"n\",\n", "    \"V\" : \"v\",\n", "    \"J\" : \"r\",\n", "    \"A\" : \"a\",\n", "}\n", "\n", "rdd_lemma=rdd_stop_word.map(lambda x : lemmatisation(x,dict_cor))\n", "rdd_lemma.first()"], "metadata": {"collapsed": true, "_uuid": "bfe2f68368b51f4892ec816785864dd70e39154a", "_cell_guid": "8941e9c7-f9d2-46e7-a0e5-a3974b631db8", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "077cc4694630eb15eb610f1df30f21c4e4e72b77", "_cell_guid": "b02874f7-2742-4635-8b46-a7de6a17675c"}, "source": ["We calcul the number of word in our BoW (it could be an interessting feature)"], "cell_type": "markdown"}, {"source": ["def size_word(x) :\n", "    nb_word=len(x[5])\n", "    return x+(nb_word,)\n", "\n", "rdd_size=rdd_lemma.map(lambda x :size_word(x))\n", "rdd_size.first()"], "metadata": {"collapsed": true, "_uuid": "facfb721712d108c8342bdf2514de9fea4be18cd", "_cell_guid": "c6ae4f0b-d08f-4e49-8679-c5b9d3f610f7", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "798861516b11770b38492a845881954d6a852fc7", "_cell_guid": "47164835-6de1-4b6b-bfbb-d30cd11458bc"}, "source": ["After all our modification we transform our RDD to a dataframe"], "cell_type": "markdown"}, {"source": ["df_final=rdd_size.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\"])\n", "print df_final.count()\n", "df_final.show()"], "metadata": {"collapsed": true, "_uuid": "5ffa687a73fa4742048d68da9ac541969a9e71f7", "_cell_guid": "606b18ba-a894-486a-96e3-ed951650836e", "scrolled": true}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "cd4869cee3b1801fde514126ad1e977da7e1b2cd", "_cell_guid": "8a5ba84f-a7c6-443d-91d3-a8945ba2ec3c"}, "source": ["Now we can save our DF to parquet (a distributed format, more efficient than csv)"], "cell_type": "markdown"}, {"source": ["df_final.write.parquet(\"tokenize_03_12_v3\",mode=\"overwrite\")"], "metadata": {"collapsed": true, "_uuid": "999be78b3b623905e52c9d4b7cb1c28a7a433c9f", "_cell_guid": "ce9547e4-2b95-49ac-bc41-65cebdb96dd9", "scrolled": false}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "2e48905fb5bdc2375301e5cb2ee491904be26ea2", "_cell_guid": "757dbfd1-364c-4d21-ba0d-5f9f0f53c2cd"}, "source": ["All those previous transformation where on training set, let's do the same on test set"], "cell_type": "markdown"}, {"source": ["df_test=spark.read.csv(\"test.csv\",header=True)\n", " #The map is important in order to have the same number of column as train set\n", "df_test_1=df_test.rdd.map(lambda x : x+(1,)).toDF([\"id\",\"text\",\"author\"])\n", "df_test_2=df_test_1.rdd.map(lambda x : normalize_token(x))\n", "df_test_3=df_test_2.map(lambda x : remove_stop_word_and_changer_number(x,seuil,stop_words,list_punct))\n", "df_test_4=df_test_3.map(lambda x : lemmatisation(x,dict_cor))\n", "df_test_5=df_test_4.map(lambda x :size_word(x))\n", "df_final_test=df_test_5.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\"]).drop(\"Author\")\n", "print df_final_test.count()\n", "print df_final_test.show()\n", "df_final_test.write.parquet(\"tokenize_test_03_12_v3\",mode=\"overwrite\")"], "metadata": {"collapsed": true, "_uuid": "a76f9f443bcc7f03f5a9b1533768f01ba82ac5fb", "_cell_guid": "69b23e46-6d59-4614-8ccf-b717300c6057"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"collapsed": true, "_uuid": "dd267e9da9534c06667776130cac15eba8d55ed6", "_cell_guid": "a906452a-efdf-4b22-a575-96e8a1928445"}, "source": ["Now we can start Machine-learning"], "cell_type": "markdown"}, {"source": ["import nltk\n", "import numpy as np\n", "from nltk.tokenize import TreebankWordTokenizer\n", "from nltk.corpus import stopwords\n", "from nltk.stem.porter import PorterStemmer\n", "from nltk.stem import WordNetLemmatizer\n", "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n", "from pyspark.ml import Pipeline\n", "from pyspark.ml.classification import RandomForestClassifier\n", "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n", "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n", "from pyspark.ml.classification import NaiveBayes\n", "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n", "from pyspark.ml.feature import Word2Vec\n", "from pyspark.ml.regression import RandomForestRegressor\n", "from pyspark.ml import Pipeline\n", "from pyspark.ml.classification import LogisticRegression\n", "from pyspark.ml.evaluation import BinaryClassificationEvaluator,  RegressionEvaluator\n", "from pyspark.ml.feature import HashingTF, Tokenizer\n", "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n", "from pyspark.ml.linalg import Vectors, VectorUDT"], "metadata": {"collapsed": true, "_uuid": "a7b761a4234b2361dbe6516afc08bf45bfc99eaa", "_cell_guid": "0f269929-7f64-4df8-b658-da185d4f93b4"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "3b8fe605ebc9ea9cce9b77e41ae47be2e7aace1c", "_cell_guid": "cca510f3-2f42-4926-9924-098ea37136e3"}, "source": ["We load our data"], "cell_type": "markdown"}, {"source": ["df = spark.read.parquet(\"tokenize_03_12_v3\")\n", "df_test = spark.read.parquet(\"tokenize_test_03_12_v3\")\n", "df.show()"], "metadata": {"collapsed": true, "_uuid": "482151269b9077c0398000b23620ab1f7cb11ba9", "_cell_guid": "f303bd20-0299-4b6f-9eb2-ed8bc1260254"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "32dc6b9ea6cca5fca1f6c03bacb3304abdb16c52", "_cell_guid": "a3335a2e-ad58-4de6-b70e-ba047a996ac1"}, "source": ["Let's create different categorie for each author, we would then create one model for each author"], "cell_type": "markdown"}, {"source": ["def add_label(x) :\n", "    if x.Author==\"EAP\" :\n", "        EAP=1\n", "        HPL=0\n", "        MWS=0\n", "    elif x.Author==\"HPL\" :\n", "        EAP=0\n", "        HPL=1\n", "        MWS=0\n", "    elif x.Author==\"MWS\" :\n", "        EAP=0\n", "        HPL=0\n", "        MWS=1\n", "    else :\n", "        EAP=0\n", "        HPL=0\n", "        MWS=0\n", "    return x+(EAP,HPL,MWS)\n", "\n", "rdd_label=df.rdd.map(lambda x : add_label(x))\n", "df_add_label=rdd_label.toDF([\"id\",\"phrase\",\"Author\",\"nb_carac\",\"nb_punct\",\"words\",\"size\",\"label_EAP\",\"label_HPL\",\"label_MWS\"])\n", "df_add_label.show()"], "metadata": {"collapsed": true, "_uuid": "fb75fc099b8ad4f9c39ebcb8d99cd4a887d2ee74", "_cell_guid": "08c8fe09-7f02-44dd-8f21-9f54cf670919"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "ec1c92fa759ce0b17c6e7e77eec7703a9d8594d5", "_cell_guid": "392e10be-d885-47ad-a6a3-3a583b6eea21"}, "source": ["They are several way to transform a BoW to an vector, tf-idf and word2Vec are pretty popular, I get better result whith w2v (train on test corpus and train corpus). But you can try with tf-idf, w2v or even both (concatenation is just after)"], "cell_type": "markdown"}, {"source": ["method=\"both\"\n", "\n", "if method==\"tf-idf\" :\n", "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=256)\n", "    featurizedData = hashingTF.transform(df_add_label)\n", "\n", "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n", "    idfModel = idf.fit(featurizedData)\n", "    rescaledData = idfModel.transform(featurizedData)\n", "\n", "elif method==\"w2v\" :\n", "    word2Vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"words\", outputCol=\"features\",seed=42, maxIter=20)\n", "    df_all_words=df_test.select(\"words\").union(df_add_label.select(\"words\"))\n", "    model_w2v = word2Vec.fit(df_all_words)\n", "\n", "    rescaledData = model_w2v.transform(df_add_label)\n", "    \n", "elif method==\"both\" :\n", "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=256)\n", "    featurizedData = hashingTF.transform(df_add_label)\n", "\n", "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features_tf_idf\")\n", "    idfModel = idf.fit(featurizedData)\n", "    rescaledData_tmp = idfModel.transform(featurizedData)\n", "    \n", "    word2Vec = Word2Vec(vectorSize=100, minCount=1, inputCol=\"words\", outputCol=\"features\",seed=42, maxIter=20)\n", "    df_all_words=df_test.select(\"words\").union(df_add_label.select(\"words\"))\n", "    model_w2v = word2Vec.fit(df_all_words)\n", "\n", "    rescaledData = model_w2v.transform(rescaledData_tmp)\n", "    \n", "rescaledData.show()"], "metadata": {"collapsed": true, "_uuid": "d0cb92f878eb5106736b265717da41b7aaf15c82", "_cell_guid": "42bc930d-118b-4486-b3a4-a2fed3bc94d8"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "4a93fb0bafea193b77ee38b0ed3db013c96fdc11", "_cell_guid": "22972c72-6f15-4945-86bb-b100d802d12a"}, "source": ["Concatenate all feature (from w2v and/or tf-idf and external caracter) in order to train a model"], "cell_type": "markdown"}, {"source": ["if method==\"both\" :\n", "    df_ML=rescaledData.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features)+list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n", "    .toDF(rescaledData.columns)\n", "else :\n", "    df_ML=rescaledData.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features) + [x.size,x.nb_carac,x.nb_punct]),))\\\n", "    .toDF(rescaledData.columns)\n", "df_ML.first()"], "metadata": {"collapsed": true, "_uuid": "4f4d232f4613b0ffd0385b5c5223feedcad57dd2", "_cell_guid": "d9bbc63d-2b3c-4281-8ba3-e1bedc5abbd6"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "d0495c42945221287f7ff2bfce59d0dd52be8797", "_cell_guid": "b69d92ad-e123-4e94-b9e2-319edeb4a4a0"}, "source": ["Now we train a model for each author, with cross validation and parameter tuning"], "cell_type": "markdown"}, {"source": ["trainingData=rescaledData\n", "#print trainingData.first()\n", "\n", "nb_eap = RandomForestRegressor(featuresCol=\"features\", predictionCol=\"EAP\",labelCol=\"label_EAP\", maxDepth=10, numTrees=20)\n", "\n", "paramGrid = ParamGridBuilder() \\\n", "    .addGrid(nb_eap.numTrees, [ 40,50,60]) \\\n", "    .addGrid(nb_eap.maxDepth, [ 5,10]) \\\n", "    .build()\n", "\n", "crossval = CrossValidator(estimator=nb_eap,\n", "                          estimatorParamMaps=paramGrid,\n", "                          evaluator= RegressionEvaluator(predictionCol=\"EAP\",labelCol=\"label_EAP\"),\n", "                          numFolds=3)\n", "\n", "model_eap=crossval.fit(trainingData)"], "metadata": {"collapsed": true, "_uuid": "b2769d1227909153f3a3b58fbd083360fb0b8718", "_cell_guid": "c2912f03-e525-4a9f-a1b8-75fd84b95ceb"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"source": ["trainingData=rescaledData\n", "#print trainingData.first()\n", "\n", "nb_hpl = RandomForestRegressor(featuresCol=\"features\",labelCol=\"label_HPL\", predictionCol=\"HPL\", maxDepth=10, numTrees=20)\n", "\n", "paramGrid = ParamGridBuilder() \\\n", "    .addGrid(nb_hpl.numTrees, [40,50,60]) \\\n", "    .addGrid(nb_hpl.maxDepth, [ 5,10]) \\\n", "    .build()\n", "\n", "crossval = CrossValidator(estimator=nb_hpl,\n", "                          estimatorParamMaps=paramGrid,\n", "                          evaluator= RegressionEvaluator(labelCol=\"label_HPL\", predictionCol=\"HPL\"),\n", "                          numFolds=3)\n", "\n", "model_hpl=crossval.fit(trainingData)"], "metadata": {"collapsed": true, "_uuid": "5146cc65b121133ea1d4f67a3a74f4e84380020f", "_cell_guid": "dec1ba1d-823f-4ce7-8424-349a0d3d3270"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"source": ["trainingData=rescaledData\n", "#print trainingData.first()\n", "\n", "nb_mws = RandomForestRegressor(featuresCol=\"features\",labelCol=\"label_MWS\", predictionCol=\"MWS\", maxDepth=10, numTrees=20)\n", "\n", "paramGrid = ParamGridBuilder() \\\n", "    .addGrid(nb_mws.numTrees, [40,50,60]) \\\n", "    .addGrid(nb_mws.maxDepth,  [5,10]) \\\n", "    .build()\n", "\n", "crossval = CrossValidator(estimator=nb_mws,\n", "                          estimatorParamMaps=paramGrid,\n", "                          evaluator=RegressionEvaluator(labelCol=\"label_MWS\", predictionCol=\"MWS\"),\n", "                          numFolds=3)\n", "\n", "model_mws=crossval.fit(trainingData)"], "metadata": {"collapsed": true, "_uuid": "b98a5580e46ae977dd06a0b27f703d8c613858b9", "_cell_guid": "5aca0488-2349-47a5-a525-23d1bdc6b972"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"source": ["print model_eap.bestModel \n", "print model_hpl.bestModel \n", "print model_mws.bestModel "], "metadata": {"collapsed": true, "_uuid": "4f91e6ae6d6da141c006080dc37079b7282db3c4", "_cell_guid": "b4b746d8-7747-4123-9895-1a7e4c02517e"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "2fe7273e185ea08cf43c55438681ca31591e7f1c", "_cell_guid": "eaa34482-e0b1-46e7-b002-b2fb5b390d45"}, "source": ["Now we apply our model to the test set"], "cell_type": "markdown"}, {"source": ["df_test.show()\n", "if method==\"tf-idf\" :\n", "    featurizedData_test = hashingTF.transform(df_test)\n", "    listfeaturized=featurizedData_test.collect()\n", "    rescaledData_test = idfModel.transform(featurizedData_test)\n", "    listidfMode=rescaledData_test.collect()\n", "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(rescaledData_test)\n", "    df_to_predict=featureIndexer.transform(rescaledData_test)\n", "     df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n", "    .toDF(df_to_predict_tmp.columns)\n", "\n", "elif method==\"w2v\" :\n", "    df_to_predict_tmp= model_w2v.transform(df_test)\n", "    df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features) + [x.size,x.nb_carac,x.nb_punct]),))\\\n", "    .toDF(df_to_predict_tmp.columns)\n", "    print df_to_predict.first()\n", "    \n", "elif method==\"both\" :\n", "    list0=df_test.collect()\n", "    featurizedData_test = hashingTF.transform(df_test)\n", "    rescaledData_test = idfModel.transform(featurizedData_test)\n", "    featureIndexer = VectorIndexer(inputCol=\"features_tf_idf\", outputCol=\"indexedFeatures\").fit(rescaledData_test)\n", "    df_to_predict=featureIndexer.transform(rescaledData_test)\n", "    df_to_predict_tmp= model_w2v.transform(df_to_predict)\n", "    df_to_predict=df_to_predict_tmp.rdd.map(lambda x : x[:-1]+(Vectors.dense(list(x.features)+list(x.features_tf_idf) + [x.size,x.nb_carac,x.nb_punct]),))\\\n", "    .toDF(df_to_predict_tmp.columns)\n", "    print df_to_predict.first()\n", "\n", "df_test_1 = model_eap.transform(df_to_predict).drop('rawPrediction').drop('probability')\n", "df_test_2 = model_hpl.transform(df_test_1).drop('rawPrediction').drop('probability')\n", "df_test_3 = model_mws.transform(df_test_2).drop('phrase').drop('words').drop('rawFeatures').drop('rawPrediction').drop('probability').drop('features').drop('indexedFeatures')\n", "print df_test_3.show()"], "metadata": {"collapsed": true, "_uuid": "5818cb615724d5c33d47dc8e9d95a9c484cedd95", "_cell_guid": "3b9b46d3-7298-4bd1-9a98-813839f46d99"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "946f47a708de6b9ae7dd6e268beef37d3f1f6a1b", "_cell_guid": "a6f310b7-6c70-457a-a9d0-0f30ab063e55"}, "source": ["Just to be sure that we haven't huge value, then select only column for Kaggle"], "cell_type": "markdown"}, {"source": ["def change_val(x) :\n", "    EAP=x.EAP\n", "    HPL=x.HPL\n", "    MWS=x.MWS\n", "    if x.EAP>1 :\n", "        EAP=1\n", "    if x.HPL>1 :\n", "        HPL=1\n", "    if x.MWS>1 :\n", "        MWS=1\n", "    if x.EAP<0 :\n", "        EAP=0\n", "    if x.HPL<0 :\n", "        HPL=0\n", "    if x.MWS<0 :\n", "        MWS=0\n", "    return (x.id,EAP,HPL,MWS)\n", "\n", "df_save=df_test_3.rdd.map(lambda x : change_val(x)).toDF(['id','EAP','HPL','MWS'])"], "metadata": {"collapsed": true, "_uuid": "cdbc9ad10f0cf115151f490b8d25f163d9dc0193", "_cell_guid": "23126f08-cb7c-4b8e-a85b-6029400774c2"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "5f4a82ef94e3818aa563a2344f137e4d09a15f8f", "_cell_guid": "ee737c94-8607-4cec-ade3-df6ea5966ea0"}, "source": ["Now we reduce the distribution of the DF to 1 (in ordre to have one csv) then we save it !"], "cell_type": "markdown"}, {"source": ["df_save=df_test_3.coalesce(1)\n", "print df_save.count()\n", "print df_save.first()"], "metadata": {"collapsed": true, "_uuid": "64f99565ffd9763c18c38618b78e66b455d061ef", "_cell_guid": "fdf921d8-50a1-4562-80d6-2f3b1bb6c850"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"source": ["df_save.select(['id','EAP','HPL','MWS']).write.csv(\"result_test_03_12_v4\",sep=\",\",header=True,mode=\"overwrite\")"], "metadata": {"collapsed": true, "_uuid": "797eee17f06b4cfeb8a0c1c9857e1825613dc7e0", "_cell_guid": "058ce720-ca0e-4211-b53c-d9eac049b163"}, "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "af21c94ccc8f32c685dbe23bac2a8b78b77d465b", "_cell_guid": "3d8e8d04-2399-4fd1-9822-6e8f1e65c5c3"}, "source": ["Well I hope this notebook will help you for your text-mining project, I also hope you learn things and want to try Spark, If you have question I will try to anwser them. Thanks for reading !"], "cell_type": "markdown"}, {"source": [], "metadata": {"collapsed": true, "_uuid": "7fd98d602a358a552d603a7af29030ebc875a8f4", "_cell_guid": "979f4680-268b-4ae5-9f65-0768147ed17b"}, "outputs": [], "cell_type": "code", "execution_count": null}], "nbformat": 4}