{"cells":[{"metadata":{"_uuid":"3a45e2f667e2307a446f281fd53dd3fb9cf3896f"},"cell_type":"markdown","source":"## Pytorch to implement simple feed-forward NN model (0.89+)\n\n* As below discussion, NN model can get lB 0.89+\n* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82499#latest-483679\n* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n* Add flatten layer as below discussion (0.86 to 0.897)\n* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n\n## LightGBM (LB 0.899)\n\n* Fine tune parameters (0.898 to 0.899)\n* Reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899\n\n\n## Plan to do\n* Modify model structure on NN model\n* Focal loss\n* Feature engineering\n* Tune parameters oof LightGBM"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0508722b8fa65e54571703633e6c878477e0443f"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c16d8edbeb1444c54e97ffb534633172d06356"},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33bea67299d3edc1c8d87e9e0de371dc717d8b6b"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996ed97f55d61b4634a2b942eb34ccb7b3930aca"},"cell_type":"code","source":"train_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"699372b84aaa697f192e7da0f1a14468dd3dfa1b"},"cell_type":"code","source":"train_features.shape,test_features.shape,train_target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f3159307f8a5c5f2993a6d874aafe776490e08"},"cell_type":"code","source":"#### Scaling feature #####\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain_features = sc.fit_transform(train_features)\ntest_features = sc.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bdb1f695eb045d4dc6710234c7df355ecb9679a"},"cell_type":"markdown","source":"## Split K- fold validation"},{"metadata":{"trusted":true,"_uuid":"b7832757a0bcae1b027da430bfa0192dd2eaef7c"},"cell_type":"code","source":"# Implement K-fold validation to improve results\nn_splits = 5 # Number of K-fold Splits\n\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(train_features, train_target))\nsplits[:3]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55c0a7544d135109579772b0140569d9ea8f9ed8"},"cell_type":"markdown","source":"## Cycling learning rate\n\n*copy from ==> https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py"},{"metadata":{"trusted":true,"_uuid":"8ffbc25621cbd735e0ba5f7bbbc4f7d8da37c9d9"},"cell_type":"code","source":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a1cde968ad94fe9a8876e69bd1792fc9c26a028"},"cell_type":"markdown","source":"## Build Simple NN model (Pytorch)\n\n* add flatten layer before fc layer (improve to 0.89+)\n* https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863\n\n* Model structure\n* (batch_size, 200) ==> Flatten ==> (batch_size* 200,1) ==> fc1 ==> (batch_size* 200, hidden_layer) ==>Reshape ==>(batch_size, hidden_layer * 200) ==> fc2 ==> (batch_size, 1)"},{"metadata":{"trusted":true,"_uuid":"925b6d89e56800f25609f2ac5c3cbf20608ba799"},"cell_type":"code","source":"class Simple_NN(nn.Module):\n    def __init__(self ,input_dim ,hidden_dim, dropout = 0.75):\n        super(Simple_NN, self).__init__()\n        \n        self.inpt_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(1, hidden_dim)\n        self.fc2 = nn.Linear(int(hidden_dim*input_dim), 1)\n        #self.fc3 = nn.Linear(int(hidden_dim/2*input_dim), int(hidden_dim/4))\n        #self.fc4 = nn.Linear(int(hidden_dim/4*input_dim), int(hidden_dim/8))\n        #self.fc5 = nn.Linear(int(hidden_dim/8*input_dim), 1)\n        #self.bn1 = nn.BatchNorm1d(hidden_dim)\n        #self.bn2 = nn.BatchNorm1d(int(hidden_dim/2))\n        #self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n        #self.bn4 = nn.BatchNorm1d(int(hidden_dim/8))\n    \n    def forward(self, x):\n        b_size = x.size(0)\n        x = x.view(-1, 1)\n        y = self.fc1(x)\n        y = self.relu(y)\n        y = y.view(b_size, -1)\n        \n        out= self.fc2(y)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aafb908b6967fe72a35603de367372145ee722d"},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"867d8ce51beef40c85d5751f89d832af9b5d8ada"},"cell_type":"markdown","source":"## Start training\n* Epoch = 40\n* Batch size = 256\n* Cycling step = 150"},{"metadata":{"trusted":true,"_uuid":"70429be47146b43f3129eacc3812d10d6836cd8a","scrolled":false},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\n## Hyperparameter\nn_epochs = 40\nbatch_size = 256\n\n## Build tensor data for torch\ntrain_preds = np.zeros((len(train_features)))\ntest_preds = np.zeros((len(test_features)))\n\nx_test = np.array(test_features)\nx_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\navg_losses_f = []\navg_val_losses_f = []\n\n## Start K-fold validation\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    \n    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    ##Loss function\n    #loss_fn = FocalLoss(2)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    \n    #Build model, initial weight and optimizer\n    model = Simple_NN(200,16)\n    model.cuda()\n    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001,weight_decay=1e-5) # Using Adam optimizer\n    \n    \n    ######################Cycling learning rate########################\n\n    step_size = 2000\n    base_lr, max_lr = 0.001, 0.005  \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=max_lr)\n    \n    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n               step_size=step_size, mode='exp_range',\n               gamma=0.99994)\n\n    ###################################################################\n\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n        #avg_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(x_batch)\n            ###################tuning learning rate###############\n            if scheduler:\n                #print('cycle_LR')\n                scheduler.batch_step()\n\n            ######################################################\n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item()/len(train_loader)\n            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n        model.eval()\n        \n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(test_features)))\n        \n        avg_val_loss = 0.\n        #avg_val_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n    avg_losses_f.append(avg_loss)\n    avg_val_losses_f.append(avg_val_loss) \n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / len(splits)\n\nauc  =  round(roc_auc_score(train_target,train_preds),4)      \nprint('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"259888d043476052d0fb3e46636b3f4ebcdd6279"},"cell_type":"markdown","source":"## LightGBM Model\n* reference this kernel : https://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899 "},{"metadata":{"trusted":true,"_uuid":"3de6ce6d8ed069e31f560369a3fd2bfb36f9dd0c"},"cell_type":"code","source":"## Use no scaling data to train LGBM\ntrain_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4718e20b804da92ac60815f73305831278a28ed"},"cell_type":"code","source":"#LGBM Paramater tuning\nparam = {\n        'num_leaves': 7,\n        'learning_rate': 0.01,\n        'feature_fraction': 0.04,\n        'max_depth': 17,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'metric': 'auc',\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4761da539c859a12494702edcaaad19939948505"},"cell_type":"markdown","source":"## LGBM training"},{"metadata":{"trusted":true,"_uuid":"9ee91342c38a5cf2e1006bd2a06eccf7b0760dec"},"cell_type":"code","source":"oof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 15000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_features, num_iteration=clf.best_iteration) / 5\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e39e52be147a7e710334cc5592d9864f36716ed7"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"a0e712896ef68f884a8e594fe0bfff4d7c70387e"},"cell_type":"markdown","source":"## Ensemble two model (NN+ LGBM)\n* NN model accuracy is too low, ensemble looks don't work."},{"metadata":{"trusted":true,"_uuid":"b50f903c99d37da6fa408ef67ae5d3bf3ab2e69d"},"cell_type":"code","source":"esemble = 0.6*oof + 0.4* train_preds\nprint('NN auc = {:<8.5f}'.format(auc))\nprint('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, oof)))\nprint('NN+LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d5b23564b6bedc07cad7f23942b9d189f51b5f"},"cell_type":"code","source":"test_preds.shape,predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304aa085cbbe3be1a807d0eb88bf79417e831a1f"},"cell_type":"code","source":"esemble_pred = 0.4* test_preds+ 0.6 *predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55ee1aa541d2ccdeafff7ecb67873f55633182a0"},"cell_type":"code","source":"id_code_test = test_df['ID_code']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59cd33ab56622c0b276632e242c9cbf77bbefbd1"},"cell_type":"markdown","source":"## Create submit file"},{"metadata":{"trusted":true,"_uuid":"6d3cb9fee66d025f11e3dc366b5d7bc48a628ad8"},"cell_type":"code","source":"my_submission_nn = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : test_preds})\nmy_submission_lbgm = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions})\nmy_submission_esemble = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : esemble_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13d5a462941d6aa309c6a707e66ce776fc011e39"},"cell_type":"code","source":"my_submission_nn.to_csv('submission_nn.csv', index = False, header = True)\nmy_submission_lbgm.to_csv('submission_lbgm.csv', index = False, header = True)\nmy_submission_esemble.to_csv('submission_esemble.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdaf09d52b7b46fb5a33cc69e37889d29742b0ed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}