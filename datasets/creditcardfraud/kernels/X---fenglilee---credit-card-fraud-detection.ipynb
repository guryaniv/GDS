{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 30)\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afbec4624b019b787eaa360cf2ef3d8f7b9a7c3c"},"cell_type":"code","source":"df = pd.read_csv(\"../input/creditcard.csv\")\ndf = df.drop(columns=[\"Time\"])\ndf\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd800acfd0ba34057cc41f004b6b3bb4594276c6"},"cell_type":"code","source":"columns = df.columns\nprint(columns)\nprint(df.shape)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d985851318ac68e6a92ccc16b26ff4f8a66f1758"},"cell_type":"code","source":"class_counts = pd.value_counts(df['Class'], sort=True)\nclass_counts = pd.DataFrame(class_counts).reset_index()\nprint(class_counts)\nf, ax = plt.subplots(figsize=(10, 8))\nsns.barplot(x='index', y='Class', data=class_counts, ax=ax)\n","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"3e8f8a6b45aac879328aa44d78989ac425c43bf6"},"cell_type":"markdown","source":"Apparently,  it is an extremely unbalanced data. if we simply apply our classify model to this data without any other data processing,  it will be very hard for us to get an ideal result. There ars serval solutions for this kind of  unbalanced data:\n1. UNDER-sampling,\n2. OVER-sampling\n3. Synthetic Minority Over-Sampling Technique\n\nIn this notebook, I would like to use UNDER-sampling as it is the simplest. Later, I will try to use Synthetic Minority Over-Sampling Technique and compare it with UNDER-sampling\n\n"},{"metadata":{"trusted":true,"_uuid":"30176e56662f15bb2b001152b778e4581e21b597"},"cell_type":"code","source":"corr = abs(df.corr())\ncorr = corr.nlargest(n=10, columns='Class')\ncolumns = list(corr.index)\nprint(columns)\ncorr = corr[corr.index]\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corr, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corr.columns, xticklabels=corr.index, ax=ax)\n","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"735a1b6403befda4efb3100001b06d50ddc79d0b"},"cell_type":"markdown","source":"1. I calculate the correlation bewteen each feature  and select ten features depends on the coefficient to feature 'Class'. It can be foubd that other feathures only has correlation with feature 'Class'  and has nothing to do with each others. As this dataset has been already handled by PCA, the features displayed here is dependent. So, there is no need to do more feature selection in this case.\n2. Next, I will plot a boxplot to display the difference of each feature according the their class."},{"metadata":{"trusted":true,"_uuid":"daaf40df0ddfe3a0507e4ffe1141c20d31daedd0"},"cell_type":"code","source":"target = 'Class'\nvariables = [item for item in columns if item != 'Class']\nvariables = np.reshape(variables, [3, 3])\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 18))\nfor i in range(3):\n    for j in range(3):\n        sns.boxplot(x=target, y=variables[i, j], data=df, ax=axes[i, j])\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09fd60205889943aa886307c7e6200734ca34416"},"cell_type":"code","source":"from scipy.stats import norm\nfrom scipy import stats\nsns.distplot(df['V17'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df['V17'], plot=plt)","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"242804ea2b39e6d1ca2d81b31dd3800726e9321f"},"cell_type":"markdown","source":"From above I konw that for different classes, the features clearly has a different distributions. Commonly, we prefer to consider the variables in each column as a normal distributions, which is the base hypothesis of most mathine learning models. Apparently, V17 does not obey normal distribution, it has a clearly long tail.\n"},{"metadata":{"trusted":true,"_uuid":"c9eb704dbd26e306b1828af77c52a14314127d76"},"cell_type":"code","source":"from collections import Counter\nfeatures = df.columns\nfeatures = list(features)\nfeatures.remove(\"Class\")\nfeatures\nX, y = df[features], df['Class']\nCounter(y)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"728814c65b83278624fb59638af317ff46324537"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX_resampled_smote, y_resampled_smote = SMOTE().fit_sample(X, y)\nCounter(y_resampled_smote)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"696bbe855dd7cb92d3ea39cbc67df9ebb9f15fa6"},"cell_type":"markdown","source":"Using smote method, I create many samples belongs to Class 1, they are similar to the origin samples. Now the dataset has been balanced. Next, I will apply classfication algorithm to the dataset."},{"metadata":{"trusted":true,"_uuid":"b45810b9fbe4f351c0066feac08802c5a7b2a4b3"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled_smote, y_resampled_smote,test_size=0.3, random_state=0)\nCounter(y_test)","execution_count":80,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"702ad5c2c45a3a9db4e0e80a2cc325dd0b4e1352"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nc_param_range = [0.01,0.1,1,10,100]\nfor c in c_param_range:\n    logistic = LogisticRegression(C=c)   \n    logistic.fit(X_train, y_train)\n    score = logistic.score(X_test, y_test)\n    print('score at c=%s is %s' %(c, score, ))","execution_count":82,"outputs":[]},{"metadata":{"_uuid":"d3df37cd58a32ec1c6061d1bcdb9638038e96fd7"},"cell_type":"markdown","source":"Next,  I will caculate the confusion_matrix of our prediction, I chose c equal to 10 for example."},{"metadata":{"trusted":true,"_uuid":"d38925824ad0cf9f96d64d5547aab99b52266036"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nlogistic = LogisticRegression(C=10)   \nlogistic.fit(X_train, y_train)\npredict_test = logistic.predict(X_test)\nc_matrix = confusion_matrix(y_test, predict_test)\nc_matrix","execution_count":83,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}