{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n#import the required dataset\ndata=pd.read_csv(\"../input/train.csv\")\ndata.head()\n\n#select the required data for processing.\nreq_data=data[['question_text','target']]\nreq_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"#perform the basic nlp operations on the questions.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\nlines=req_data['question_text'].values.tolist()\n\n#store the english stopwords in a dictionary.\nstops=set(stopwords.words('english'))\n\n#create a list for storing the tokens.\nquest_tokens=list()\n\n#perform the basic operations required.\n\nfor line in lines:\n    #convert the words to tokens.\n    \n    tokens=word_tokenize(line) \n    \n    #convert the tokens to lower case.\n    \n    lower=[w.lower() for w in tokens]\n    \n    #remove all the punctuations.\n    \n    no_punct=[w.translate(str.maketrans('','',string.punctuation)) for w in lower]\n    \n    #remove all the stopwords.\n    \n    no_stops=[w for w in no_punct if w not in stops]\n    \n    #remove all non alphabetic characters\n    \n    final_tokens=[l for l in no_stops if l.isalpha()]\n    \n    #append the tokens to the list.\n    quest_tokens.append(final_tokens)\n\n\n#build a word2vec model.\n\nimport gensim\nfrom gensim.models import Word2Vec\n\nw2v_model=Word2Vec(quest_tokens,size=100,window=5,workers=4,min_count=1)\n\nprint(len(w2v_model.wv.vocab))\n\n#Build a tokenizer.\n\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\n#find the maximum length of the questions.\nmax_len=max(len(s.split())for s in req_data['question_text'])\n\n#initialise a tokenizer object\ntokenizer=Tokenizer()\n\ntokenizer.fit_on_texts(quest_tokens)\nsequences=tokenizer.texts_to_sequences(quest_tokens)\n\nword_index=tokenizer.word_index\n\n#pad the sequences \npadded_sequences=pad_sequences(sequences,maxlen=max_len)\nreviews=req_data['target']\n\n#build the embedding matrix.\n\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in word_index.items():\n    if i>num_words:\n        continue\n    embedding_vector=word_index.get(word)\n    \n    if embedding_vector is not None:\n        embedding_matrix[i]=embedding_vector\n\n#Build the keras model.\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,GRU,Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\nmodel=Sequential()\n\nmodel.add(Embedding(num_words,\n                    100,\n                    embeddings_initializer=Constant(embedding_matrix),\n                   input_length=max_len,\n                   trainable=False))\n\nmodel.add(LSTM(units=32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(LSTM(units=8,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(units=1,activation='relu'))\n\n#compile the model.\nmodel.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')\n\n#Seperate the training and testing model.\n\nval_split=0.2\nindices=np.arange(padded_sequences.shape[0])\nnp.random.shuffle(indices)\n\npadded_sequences=padded_sequences[indices]\nreviews=reviews[indices]\n\n#seperate training and testing.\nvalidation_samples=int(padded_sequences.shape[0]*val_split)\n\nx_train=padded_sequences[:-validation_samples]\ny_train=reviews[:-validation_samples]\nx_test=padded_sequences[-validation_samples:]\ny_test=reviews[-validation_samples:]\n\n\n#Run the model.\nmodel.fit(x_train,y_train,batch_size=128,epochs=3,validation_data=(x_test,y_test),verbose=2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}