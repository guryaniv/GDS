{"cells":[{"metadata":{"_uuid":"38be8b2c0415b72970807ad9e951b50ebe8555cf"},"cell_type":"markdown","source":"**Table of contents**\n\n* [Introduction](#Introduction)\n* [Preparation](#Preparation)\n  * [Dependencies](#Dependencies)\n  * [Load the datasets](#Load-the-datasets)\n* [ARIMA](#ARIMA)\n* [Time series data exploration](#Time-series-data-exploration)\n  * [Distribution of sales](#Distribution-of-sales)\n  * [How does sales vary across stores](#How-does-sales-vary-across-stores)\n  * [How does sales vary across items](#How-does-sales-vary-across-items)\n  * [Time-series visualization of the sales](#Time-series-visualization-of-the-sales)\n\n# Introduction\n\nKernel for the [demand forecasting](https://www.kaggle.com/c/demand-forecasting-kernels-only) Kaggle competition.\n\nAnswer some of the questions posed:\n\n* What's the best way to deal with seasonality?\n* Should stores be modeled separately, or can you pool them together?\n* Does deep learning work better than ARIMA?\n* Can either beat xgboost?\n\n\n\n  \n  # Preparation\n  \n  ## Dependencies"},{"metadata":{"_uuid":"155a10ac96338af61df8e5481ae98060276e9dfd","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set()\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport statsmodels.api as sm\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n# import the_module_that_warns\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom fbprophet import Prophet\n\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD,Adadelta,Adam,RMSprop \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58a45a9ff3fd02b31232abd5791eca57affd8a94"},"cell_type":"markdown","source":"## Load the datasets"},{"metadata":{"_uuid":"b88da5611bebd0df7ac88b6659defcc262820563","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# First let us load the datasets into different Dataframes\ndef load_data(datapath):\n    data = pd.read_csv(datapath)\n   # Dimensions\n    print('Shape:', data.shape)\n    # Set of features we have are: date, store, and item\n    display(data.sample(10))\n    return data\n    \n    \ntrain_df = load_data('../input/demand-forecasting-kernels-only/train.csv')\ntest_df = load_data('../input/demand-forecasting-kernels-only/test.csv')\nsample_df = load_data('../input/demand-forecasting-kernels-only/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a84690c95c5c0aae511c15440f0e7361f1e5232"},"cell_type":"markdown","source":"# Time series data exploration\n\n(This portion was [forked](https://www.kaggle.com/danofer/getting-started-with-time-series-features).)\n\nThe goal of this kernel is data exploration of a time-series sales data of store items.   \nThe tools `pandas`, `matplotlib`  and, `plotly`  are used for slicing & dicing the data and visualizations.\n"},{"metadata":{"_uuid":"b48bfa06a99d35bb56717fa986d3256e3f2d3119"},"cell_type":"markdown","source":"## Distribution of sales\nNow let us understand how the sales varies across all the items in all the stores"},{"metadata":{"_uuid":"7b69edd51e1a32ad0ca485d1bc15d00617c1b59e","scrolled":false,"trusted":true},"cell_type":"code","source":"# Sales distribution across the train data\ndef sales_dist(data):\n    \"\"\"\n        Sales_dist used for Checing Sales Distribution.\n        data :  contain data frame which contain sales data\n    \"\"\"\n    sales_df = data.copy(deep=True)\n    sales_df['sales_bins'] = pd.cut(sales_df.sales, [0, 50, 100, 150, 200, 250])\n    print('Max sale:', sales_df.sales.max())\n    print('Min sale:', sales_df.sales.min())\n    print('Avg sale:', sales_df.sales.mean())\n    print()\n    return sales_df\n\nsales_df = sales_dist(train_df)\n\n# Total number of data points\ntotal_points = pd.value_counts(sales_df.sales_bins).sum()\nprint('Sales bucket v/s Total percentage:')\ndisplay(pd.value_counts(sales_df.sales_bins).apply(lambda s: (s/total_points)*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"169207e3808f81d2c9f4130417b5bde34063b373","trusted":true},"cell_type":"code","source":"# Let us visualize the same\nsales_count = pd.value_counts(sales_df.sales_bins)\nsales_count.sort_values(ascending=True).plot(kind='barh', title='Sales distribution', );\n# sns.countplot(sales_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29f21269211e572cabe4b1045aaf6266a568f802"},"cell_type":"markdown","source":"As we can see, almost 92% of sales are less than 100. Max, min and average sales are 231, 0 and 52.25 respectively.   \nSo any prediction model has to deal with the skewness in the data appropriately. \n\n## How does sales vary across stores\nLet us get a overview of sales distribution in the whole data."},{"metadata":{"_uuid":"6659dfd63098c8b6b5eb4511034be43a705527d9","trusted":true},"cell_type":"code","source":"# Let us understand the sales data distribution across the stores\ndef sales_data_understanding(data):    \n    store_df = data.copy()\n    plt.figure(figsize=(20,10))\n    sales_pivoted_df = pd.pivot_table(store_df, index='store', values=['sales','date'], columns='item', aggfunc=np.mean)\n    sales_pivoted_df.plot(kind=\"hist\",figsize=(20,10))\n    # Pivoted dataframe\n    display(sales_pivoted_df)\n    return (store_df,sales_pivoted_df)\n\nstore_df,sales_pivoted_df = sales_data_understanding(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f7089515341e96adb62576ae64a489788775eff"},"cell_type":"markdown","source":"This pivoted dataframe has average sales per each store per each item.  \nLet use this dataframe and produce some interesting visualizations!"},{"metadata":{"_uuid":"f83879948cebbf4fefabdce306ff6ef3385aa0a5","trusted":true,"collapsed":true},"cell_type":"code","source":"# Let us calculate the average sales of all the items by each store\nsales_across_store_df = sales_pivoted_df.copy()\nsales_across_store_df['avg_sale'] = sales_across_store_df.apply(lambda r: r.mean(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"053de0c3fd5fc9cdac35a6aec18cc655217abc42","trusted":true},"cell_type":"code","source":"# Scatter plot of average sales per store\nsales_store_data = go.Scatter(\n    y = sales_across_store_df.avg_sale.values,\n    mode='markers',\n    marker=dict(\n        size = sales_across_store_df.avg_sale.values,\n        color = sales_across_store_df.avg_sale.values,\n        colorscale='Viridis',\n        showscale=True\n    ),\n    text = sales_across_store_df.index.values\n)\ndata = [sales_store_data]\n\nsales_store_layout = go.Layout(\n    autosize= True,\n    title= 'Scatter plot of avg sales per store',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'Stores',\n        ticklen= 10,\n        zeroline= False,\n        gridwidth= 1,\n    ),\n    yaxis=dict(\n        title= 'Avg Sales',\n        ticklen= 10,\n        zeroline= False,\n        gridwidth= 1,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=sales_store_layout)\npy.iplot(fig,filename='scatter_sales_store')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d318e77358d48e08ea61f9b56d9d7a6475e8c77","collapsed":true},"cell_type":"markdown","source":"From the visualization, it is clear that the stores with ID 2 and 8 have higher average sales than the remaining stores and is a clear indication that they are doing good money!\n\nWhereas store with ID 7 has very poor performance in terms of average sales.\n\n## How does sales vary across items"},{"metadata":{"_uuid":"d043e12c5d444d9aa0d5f55d984f8f37f95d99ea","scrolled":false,"trusted":true},"cell_type":"code","source":"def sales_insight(sales_pivoted_df):\n    # Let us calculate the average sales of each of the item across all the stores\n    sales_across_item_df = sales_pivoted_df.copy()\n    # Aggregate the sales per item and add it as a new row in the same dataframe\n    sales_across_item_df.loc[11] = sales_across_item_df.apply(lambda r: r.mean(), axis=0)\n    # Note the 11th index row, which is the average sale of each of the item across all the stores\n    #display(sales_across_item_df.loc[11:])\n    avg_sales_per_item_across_stores_df = pd.DataFrame(data=[[i+1,a] for i,a in enumerate(sales_across_item_df.loc[11:].values[0])], columns=['item', 'avg_sale'])\n    # And finally, sort by avg sale\n    avg_sales_per_item_across_stores_df.sort_values(by='avg_sale', ascending=False, inplace=True)\n    # Display the top 10 rows\n    display(avg_sales_per_item_across_stores_df.head())\n    return (sales_across_item_df,avg_sales_per_item_across_stores_df)\n\nsales_across_item_df,avg_sales_per_item_across_stores_df = sales_insight(sales_pivoted_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cabcd0b709c1d8f2d8941119b5c9456196357b21"},"cell_type":"markdown","source":"Great! Let us visualize these average sales per item!"},{"metadata":{"_uuid":"20ccd99509aab1f355db59df0924b681d8562c03","scrolled":false,"trusted":true},"cell_type":"code","source":"avg_sales_per_item_across_stores_sorted = avg_sales_per_item_across_stores_df.avg_sale.values\n# Scatter plot of average sales per item\nsales_item_data = go.Bar(\n    x=[i for i in range(0, 50)],\n    y=avg_sales_per_item_across_stores_sorted,\n    marker=dict(\n        color=avg_sales_per_item_across_stores_sorted,\n        colorscale='Blackbody',\n        showscale=True\n    ),\n    text = avg_sales_per_item_across_stores_df.item.values\n)\ndata = [sales_item_data]\n\nsales_item_layout = go.Layout(\n    autosize= True,\n    title= 'Scatter plot of avg sales per item',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'Items',\n        ticklen= 55,\n        zeroline= False,\n        gridwidth= 1,\n    ),\n    yaxis=dict(\n        title= 'Avg Sales',\n        ticklen= 10,\n        zeroline= False,\n        gridwidth= 1,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=sales_item_layout)\npy.iplot(fig,filename='scatter_sales_item')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39aa0cc4715d9a80ca537b91c0ed420a50e03a6c"},"cell_type":"markdown","source":"Amazing! The sales is uniformly distributed across all the items.   \nTop items with highest average sale are 15, 28, 13, 18 and with least average sales are 5, 1, 41 and so on.\n\n## Time-series visualization of the sales\nLet us see how sales of a given item in a given store varies in a span of 5 years."},{"metadata":{"_uuid":"ec5507252a10e0b03ce84015b8dccd1ce78b2822","trusted":true},"cell_type":"code","source":"def Time_visualization(data):\n    store_item_df = data.copy()\n    # First, let us filterout the required data\n    store_id = 10   # Some store\n    item_id = 40    # Some item\n    print('Before filter:', store_item_df.shape)\n    store_item_df = store_item_df[store_item_df.store == store_id]\n    store_item_df = store_item_df[store_item_df.item == item_id]\n    print('After filter:', store_item_df.shape)\n    #display(store_item_df.head())\n\n    # Let us plot this now\n    store_item_ts_data = [go.Scatter(\n        x=store_item_df.date,\n        y=store_item_df.sales)]\n    py.iplot(store_item_ts_data)\n    return store_item_df\n\nstore_item_df = Time_visualization(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48824374b884c59acd488f4c35d68c6b9b4486ed"},"cell_type":"markdown","source":"Woww! Clearly there is a pattern here! Feel free to play around with different store and item IDs.   \nAlmost all the items and store combination has this pattern!\n\nThe sales go high in June, July and August months. The sales will be lowest in December, January and February months. That's something!!\n\nLet us make it more interesting. What if we aggregate the sales on a montly basis and compare different items and stores.   \nThis should help us understand how different item sales behave at a high level."},{"metadata":{"_uuid":"1924731e6c60a3205ccfa559cc699a0a02327a01","trusted":true},"cell_type":"code","source":"def sales_monthly(data):\n    multi_store_item_df = data.copy()\n    # First, let us filterout the required data\n    store_ids = [1, 1, 1, 1]   # Some stores\n    item_ids = [10, 20, 30, 40]    # Some items\n    print('Before filter:', multi_store_item_df.shape)\n    multi_store_item_df = multi_store_item_df[multi_store_item_df.store.isin(store_ids)]\n    multi_store_item_df = multi_store_item_df[multi_store_item_df.item.isin(item_ids)]\n    print('After filter:', multi_store_item_df.shape)\n    #display(multi_store_item_df)\n    # TODO Monthly avg sales\n\n    # Let us plot this now\n    multi_store_item_ts_data = []\n    for st,it in zip(store_ids, item_ids):\n        flt = multi_store_item_df[multi_store_item_df.store == st]\n        flt = flt[flt.item == it]\n        multi_store_item_ts_data.append(go.Scatter(x=flt.date, y=flt.sales, name = \"Store:\" + str(st) + \",Item:\" + str(it)))\n    py.iplot(multi_store_item_ts_data)\n    return (multi_store_item_df)\n\nmulti_store_item_df = sales_monthly(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a304efef2242bbda712ec06298984e7bd1dd901"},"cell_type":"markdown","source":"Interesting!!   \nThough the pattern remains same across different stores and items combinations, the **actual sale value consitently varies with the same scale**. \n\nAs we can see in the visualization, item 10 has consistently highest sales through out the span of 5 years!   \nThis is an interesting behaviour that can be seen across almost all the items. \n\n"},{"metadata":{"_uuid":"bc8f5a920af25f40907e3cc2b674ea0ea5439383","collapsed":true},"cell_type":"markdown","source":"# ARIMA\n\nARIMA is Autoregressive Integrated Moving Average Model, which is a component of SARIMAX, i.e. Seasonal ARIMA with eXogenous regressors.\n\n(sources: [1](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/), [2](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3), [3](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases))\n\n\nhttp://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases"},{"metadata":{"_uuid":"04a2e7ceec8c4c84490228e96582d40236c9f150"},"cell_type":"markdown","source":"# LIGHTGBM"},{"metadata":{"_uuid":"20563f9eebae194c30bae3d8690abdeef534210c","trusted":true,"collapsed":true},"cell_type":"code","source":"def split_data(train_data,test_data):\n    train_data['date'] = pd.to_datetime(train_data['date'])\n    test_data['date'] = pd.to_datetime(test_data['date'])\n\n    train_data['month'] = train_data['date'].dt.month\n    train_data['day'] = train_data['date'].dt.dayofweek\n    train_data['year'] = train_data['date'].dt.year\n\n    test_data['month'] = test_data['date'].dt.month\n    test_data['day'] = test_data['date'].dt.dayofweek\n    test_data['year'] = test_data['date'].dt.year\n\n    col = [i for i in test_data.columns if i not in ['date','id']]\n    y = 'sales'\n    train_x, test_x, train_y, test_y = train_test_split(train_data[col],train_data[y], test_size=0.2, random_state=2018)\n    return (train_x, test_x, train_y, test_y,col)\n\ntrain_x, test_x, train_y, test_y,col = split_data(train_df,test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb05703f57802e934a21d1bfc56b17e3e2889ce9"},"cell_type":"code","source":"train_x.shape, test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"181f70dbffb99aae4042804a1e01dbf60abd6992"},"cell_type":"code","source":"# reshape input to be [samples, time steps, features]\ntrain_x = np.array(train_x).reshape(train_x.shape[0], 1, train_x.shape[1])\ntest_x = np.array(test_x).reshape(test_x.shape[0], 1, test_x.shape[1])\ntrain_x.shape,test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d359228c61cf85bc4783722cdf512df3eab37c0"},"cell_type":"code","source":"_optimiser = ['Adam','Nadam','RMSprop']\nmodel = Sequential()\nmodel.add(LSTM(144, batch_input_shape=(32, 1, 5), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer=_optimiser[0])\nmodel.summary()\nmodel.fit(train_x,train_y, batch_size=32,epochs=5)\nsubmission = pd.read_csv(\"../input/demand-forecasting-kernels-only/sample_submission.csv\")\nsubmission['sales'] = model.predict(test_x)\nsubmission.to_csv(\"submission_Adam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2458fdeb1bf95b810c04362845737f4babbcc107"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"92e7f016296ef5f8f041cd3d1c794077e94db4da"},"cell_type":"code","source":"_optimiser = ['Adam','Nadam','RMSprop']\nmodel = Sequential()\nmodel.add(LSTM(144, batch_input_shape=(32, 1, 5), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer=_optimiser[1])\nmodel.summary()\nmodel.fit(train_x,train_y, batch_size=40,epochs=5)\nsubmission = pd.read_csv(\"../input/demand-forecasting-kernels-only/sample_submission.csv\")\nsubmission['sales'] = model.predict(test_x, batch_size=32,verbose = 1)\nsubmission.to_csv(\"submission_Nadam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4787b9fc52b697579d4be971065f920451a6b24b"},"cell_type":"code","source":"_optimiser = ['Adam','Nadam','RMSprop']\nmodel = Sequential()\nmodel.add(LSTM(144, batch_input_shape=(32, 1, 5), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer=_optimiser[2])\nmodel.summary()\nmodel.fit(train_x,train_y, batch_size=40,epochs=5)\nsubmission = pd.read_csv(\"../input/demand-forecasting-kernels-only/sample_submission.csv\")\nsubmission['sales'] = model.predict(test_x, batch_size=32,verbose = 1)\nsubmission.to_csv(\"submission_RMS\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"631d0fd27491ad5161e3aaa685856c152245709a","collapsed":true},"cell_type":"code","source":"# from bayes_opt import BayesianOptimization\n# def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.02, output_process=False):\n#     # prepare data\n#     train_data = lgb.Dataset(data=X, label=y)\n#     # parameters\n#     def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n#         params = {'application':'regression_l1','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n#         params[\"num_leaves\"] = int(round(num_leaves))\n#         params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n#         params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n#         params['max_depth'] = int(round(max_depth))\n#         params['lambda_l1'] = max(lambda_l1, 0)\n#         params['lambda_l2'] = max(lambda_l2, 0)\n#         params['min_split_gain'] = min_split_gain\n#         params['min_child_weight'] = min_child_weight\n#         cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n#         return max(cv_result['auc-mean'])\n#     # range \n#     lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n#                                             'feature_fraction': (0.1, 0.9),\n#                                             'bagging_fraction': (0.8, 1),\n#                                             'max_depth': (5, 8.99),\n#                                             'lambda_l1': (0, 5),\n#                                             'lambda_l2': (0, 3),\n#                                             'min_split_gain': (0.001, 0.1),\n#                                             'min_child_weight': (5, 50)}, random_state=0)\n#     # optimize\n#     lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n#     # output optimization process\n#     if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n#     # return best parameters\n#     return lgbBO.res['max']['max_params']\n\n# opt_params = bayes_parameter_opt_lgb(train_x, train_y, init_round=5, opt_round=10, n_folds=3, random_seed=6, n_estimators=100, learning_rate=0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"474b3175efb19409db38dbf2fa3f88e64ad88a94","collapsed":true},"cell_type":"code","source":"# opt_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b490247d4f116d6c62981f859d4f6535381cbfa8","collapsed":true},"cell_type":"code","source":"sample_df['sales'] = model.predict(test_x)\nsample_df.to_csv('lgb_bayasian_param.csv', index=False)\nsample_df['sales'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5700ad8bd0edaf7618672118f042fb68d600e811","collapsed":true},"cell_type":"code","source":"def average(df1):\n    avg  = df1\n    df2 = pd.read_csv(\"../input/private/sub_val-0.132358565029612.csv\")\n    avg['sales'] = (df1[\"sales\"]*0.3 + df2[\"sales\"]*0.7)\n    return avg\n\navg = average(sample_df)\navg.to_csv(\"Submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b06499bd216b6c5cb37006b871337a5f1301c609"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}