{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"817a76c7aff5b9b883c87c37f6fced98366e3c42"},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be92e2c8cd3ffd8d665a698c7894e79282053367"},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e96eb506e459e13d82aec79bb29a6f0c48bf06fa"},"cell_type":"code","source":"import pandas as pd\npd.read_csv(\"../input/news_sample.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2645fcea214180684fea30a3e11cd93b9b1283c5"},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\nenv=twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6df0b1849b53a52e3cb8085217503a95c6ca2da"},"cell_type":"code","source":"(market_train_df,news_train_df)=env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9d6f14c90c0d64bd1f793f0c29ec69fd6d4e95d"},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d46fd65914f62c2713540c51922b1cbeb05ad35"},"cell_type":"code","source":"print(market_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"1628106927ec0c0162c43c523bd6bdb1b71775b8"},"cell_type":"code","source":"market_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540b1b106acb58bb3b7ddc86e0f4ad9d371e09be"},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom time import time\nimport seaborn as sns\nimport os\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer,StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder, RobustScaler\nfrom sklearn.metrics import accuracy_score\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, Activation\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89425a1ba18168e1f85ea11ce625d0a74ec540b4"},"cell_type":"code","source":"toy=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4def6bf7416f000ad01bfa548e3971e2e0a32084"},"cell_type":"code","source":"# This competition settings\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d7b9dd093a50f1f0f7a151d1100d403acab3f15"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee7892fb207a2bfc5dd3649bcbb339676ce25740"},"cell_type":"code","source":"market_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a70e48c529b90144ce94892f4fa7423d8d5a77"},"cell_type":"code","source":"if toy:\n    market_train_df=market_train_df.tail(100_000)\n    news_train_df=news_train_df.tail(300_000)\nelse:\n    market_train_df=market_train_df.tail(750_000)\n    news_train_df=news_train_df.tail(1500_000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9d8d2b6bf228a09e862e5f34672680bc2fe090c"},"cell_type":"code","source":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd97161441971433c0cb142109800024698f622"},"cell_type":"code","source":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n    gc.collect()\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n    gc.collect()\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    gc.collect()\n    \n    return market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aebb877573008221ca20d11d2ffff38a067e5d30"},"cell_type":"code","source":"market_df = join_market_news(market_train_df, news_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f73ad394b4fda1b66392cc122575724e2fa0052"},"cell_type":"code","source":"market_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a1da0a21e56d9ade1a4385a668c7802bee489c5"},"cell_type":"code","source":"market_df.shape()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1c16223f79f3cfe1aeffceccd59fe394a5d38fc"},"cell_type":"code","source":"# Features\ncat_cols = ['assetCode']\ntime_cols=['year', 'week', 'day', 'dayofweek']\nmkt_numeric_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n    \nnews_numeric_cols = [\n#        'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n#        'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n#        'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n#        'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n#        'returnsOpenNextMktres10', 'universe', 'urgency_min', 'urgency_count',\n#        'takeSequence_max', 'bodySize_min', 'bodySize_max', 'bodySize_mean',\n#        'bodySize_std', 'wordCount_min', 'wordCount_max', 'wordCount_mean',\n#        'wordCount_std', 'sentenceCount_min', 'sentenceCount_max',\n#        'sentenceCount_mean', 'sentenceCount_std', 'companyCount_min',\n#        'companyCount_max', 'companyCount_mean', 'companyCount_std',\n#        'marketCommentary_min', 'marketCommentary_max', 'marketCommentary_mean',\n#        'marketCommentary_std', \n        'relevance_min', 'relevance_max',\n        'relevance_mean', 'relevance_std', 'sentimentNegative_min',\n        'sentimentNegative_max', 'sentimentNegative_mean',\n        'sentimentNegative_std', 'sentimentNeutral_min', 'sentimentNeutral_max',\n        'sentimentNeutral_mean', 'sentimentNeutral_std',\n        'sentimentPositive_min', 'sentimentPositive_max',\n        'sentimentPositive_mean', 'sentimentPositive_std',\n        'sentimentWordCount_min', 'sentimentWordCount_max',\n        'sentimentWordCount_mean', 'sentimentWordCount_std',\n        'noveltyCount12H_min', 'noveltyCount12H_max', 'noveltyCount12H_mean',\n        'noveltyCount12H_std', 'noveltyCount24H_min', 'noveltyCount24H_max',\n        'noveltyCount24H_mean', 'noveltyCount24H_std', 'noveltyCount3D_min',\n        'noveltyCount3D_max', 'noveltyCount3D_mean', 'noveltyCount3D_std',\n        'noveltyCount5D_min', 'noveltyCount5D_max', 'noveltyCount5D_mean',\n        'noveltyCount5D_std', 'noveltyCount7D_min', 'noveltyCount7D_max',\n        'noveltyCount7D_mean', 'noveltyCount7D_std']\n#         'volumeCounts12H_min',\n#        'volumeCounts12H_max', 'volumeCounts12H_mean', 'volumeCounts12H_std',\n#        'volumeCounts24H_min', 'volumeCounts24H_max', 'volumeCounts24H_mean',\n#        'volumeCounts24H_std', 'volumeCounts3D_min', 'volumeCounts3D_max',\n#        'volumeCounts3D_mean', 'volumeCounts3D_std', 'volumeCounts5D_min',\n#        'volumeCounts5D_max', 'volumeCounts5D_mean', 'volumeCounts5D_std',\n#        'volumeCounts7D_min', 'volumeCounts7D_max', 'volumeCounts7D_mean',\n#        'volumeCounts7D_std']\n    \nnumeric_cols = mkt_numeric_cols + news_numeric_cols\n    \nfeature_cols = cat_cols + time_cols + numeric_cols\n    \n    # Labels\nlabel_cols = ['returnsOpenNextMktres10']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183affc29a526ca703d77f55ec4f40fd9b5947ff"},"cell_type":"code","source":"print (np.unique(feature_cols).shape)\nprint (len(feature_cols))\nprint (numeric_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ceccc3aa62ea4cc77777e8b9384567eb9df6ef8"},"cell_type":"code","source":"# Split to train, validation and test.\n# ToDo: remove shuffle, use generator.\n#market_train_df, market_test_df = train_test_split(market[market.time > '2009'].sample(100000, random_state=42), shuffle=True, random_state=24)\nmarket_train_df, market_test_df = train_test_split(market_df, shuffle=True, random_state=24)\nmarket_train_df, market_val_df = train_test_split(market_train_df, shuffle=True, random_state=24)\n\n# Look at min/max and quantiles\nmarket_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9f3330ebd7e6178c1f62840ecb2b1bc0f937dc7"},"cell_type":"code","source":"class Prepro:\n    \"\"\"\n    Bring all preprocessing here: scale, encoding\n    Should be fit on train data and called on train, validation or test data\n    \"\"\"\n    \n    def __init__(self, feature_cols, cat_cols, time_cols, numeric_cols, label_cols):\n        self.feature_cols = feature_cols\n        self.cat_cols = cat_cols\n        self.time_cols = time_cols\n        self.numeric_cols = numeric_cols\n        self.label_cols = label_cols\n        self.cats={}\n    \n    def transformXy(self, df):\n        \"\"\"\n        Preprocess and return X,y\n        \"\"\"\n        df = df.copy()\n        # Scale, encode etc. features\n        X = self.transform(df)\n        # Scale labels\n        df[self.label_cols] = self.y_scaler.transform(df[self.label_cols])\n        y = df[self.label_cols]\n        return(X,y)\n    \n    def transform(self, df):\n        \"\"\"\n        Preprocess and return X\n        \"\"\"\n        # Add day, week, year\n        df = self.prepare_time_cols(df)\n        # Fill nans\n        df[self.numeric_cols] = df[self.numeric_cols].fillna(0)\n        # Preprocess categorical features\n        for col in cat_cols:\n            df[col] = df[col].apply(lambda cat_name: self.prepare_cat_cols(cat_name, col))\n        # Scale numeric features and labels\n        df[self.numeric_cols+self.time_cols] = self.numeric_scaler.transform(df[self.numeric_cols+self.time_cols])\n        # Return X\n        return df[self.feature_cols]\n    \n    def fit(self, df):\n        \"\"\"\n        Fit preprocessing scalers, encoders on given train df\n        To be called on train df only\n        \"\"\"\n        # Extract day, week, year from time\n        df = df.copy()\n        df = self.prepare_time_cols(df)\n        # Handle strange cases, impute etc.\n        df = self.prepare_train_df(df)\n        # Use QuantileTransformer to handle outliers\n        # Fit for labels\n        self.y_scaler = QuantileTransformer()\n        self.y_scaler.fit(df[self.label_cols])\n        # Fit for numeric and time\n        self.numeric_scaler = QuantileTransformer()\n        self.numeric_scaler.fit(df[self.numeric_cols + self.time_cols])\n        # Fit for categories\n        # Organize dictionary, each category column has list with values\n        self.cats=dict()\n        for col in cat_cols:\n            self.cats[col] = list(df[col].unique())\n\n    def prepare_train_df(self, train_df):\n        \"\"\"\n        Clean na, remove strange cases.\n        For train dataset only. \n        \"\"\"\n        # Handle nans\n        train_df = train_df.copy()\n        # Need better imputer\n        # for col in numeric_cols:\n        #     market_train_df[col] = market_train_df[col].fillna(market_train_df[col].mean())\n        train_df.tail()\n        train_df[self.numeric_cols] = train_df[self.numeric_cols].fillna(0)\n\n        # # Remove strange cases with close/open ratio > 2\n        max_ratio  = 2\n        train_df = train_df[np.abs(train_df['close'] / train_df['open']) <= max_ratio]\n        return(train_df)\n\n    def prepare_time_cols(self, df):\n        \"\"\" \n        Extract time parts, they are important for time series \n        \"\"\"\n        df = df.copy()\n        df['year'] = df['time'].dt.year\n        # Maybe remove month because week of year can handle the same info\n        df['day'] = df['time'].dt.day\n        # Week of year\n        df['week'] = df['time'].dt.week\n        df['dayofweek'] = df['time'].dt.dayofweek\n        return(df)\n        \n    def prepare_cat_cols(self, cat_name, col):\n        \"\"\"\n        Encode categorical features to numbers\n        \"\"\"\n        try:\n            # Transform to index of name in stored names list\n            index_value = self.cats[col].index(cat_name)\n        except ValueError:\n            # If new value, add it to the list and return new index\n            self.cats[col].append(cat_name)\n            index_value = len(self.cats[col])\n        index_value = 1.0/(index_value+1.0)\n        return(index_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b87d68e1baff9caf5e5d93029427ce78da65d97d"},"cell_type":"code","source":"# Preprocess and split to X_train, X_val, X_test, y_train ...\nprepro = Prepro(feature_cols, cat_cols, time_cols, numeric_cols, label_cols)\nprepro.fit(market_train_df)\n\n# Clean train df,handle strange cases\nmarket_train_df = prepro.prepare_train_df(market_train_df)\n\nX_train, y_train = prepro.transformXy(market_train_df)\nX_val, y_val = prepro.transformXy(market_val_df)\nX_test, y_test = prepro.transformXy(market_test_df)\n\n# Display for visual check. \npd.concat([X_train,y_train], axis=1).describe()\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86c45a4a359e6234edaf35cbf0ce38f457b28aa0"},"cell_type":"code","source":"# Initialize the constructor\nmodel = Sequential()\n\n# Add an input layer \ninput_size = X_train.shape[1]\n\n# Add layers - no worries which are.\n# ToDo: find a good architecture of NN\nmodel.add(Dense(256, input_shape=(input_size,), kernel_initializer='glorot_normal'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\n\n# Add an output layer \nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"152e7d4759f27aa30b4b661bbcf244af79d89e41"},"cell_type":"code","source":"weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(patience=5, verbose=1)\n\n# Low, avg and high scor training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\nreduce_lr = ReduceLROnPlateau(factor=0.2,\n                              patience=5, min_lr=0.001)\n\n# Train\ntraining = model.fit(X_train,y_train\n                                ,batch_size=512\n                                ,epochs=100\n                                ,validation_data=[X_val, y_val]\n                                #,steps_per_epoch=100\n                                 #, validation_steps=100\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1895c1fa7b61a67dbdbc494ca165fda7d5106b02"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}