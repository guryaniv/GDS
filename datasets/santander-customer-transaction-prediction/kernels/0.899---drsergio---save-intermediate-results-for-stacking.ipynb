{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nfrom time import time\nfrom scipy.stats import norm, rankdata\nfrom catboost import CatBoostClassifier\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, MinMaxScaler, RobustScaler\n\nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = (10, 10)\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"seed = 1337\nprecision = 5\n\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7521ee1f4f5def5f333489cc6b140309615a8e2e"},"cell_type":"code","source":"index_column = 'ID_code'\ntarget_column = 'target'\n\nraws_features = train_data.columns[2:]\ntrain_X, train_y = train_data[raws_features], train_data[target_column]\ntest_X = test_data[raws_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02eb8127268b7fed534a81b2ca1c8462d53621e7"},"cell_type":"code","source":"class CVClassifier():\n    def __init__(self, estimator, n_splits=5, stratified=True, num_round=20000, early_stop=200, shuffle=False, **params):\n        self.n_splits_ = n_splits\n        self.scores_ = []\n        self.clf_list_ = []\n        self.estimator_ = estimator\n        self.stratified_ = stratified\n        self.num_round_ = num_round\n        self.early_stop = early_stop\n        self.shuffle = shuffle\n        if params:\n            self.params_ = params\n        \n    def cv(self, train_X, train_y):\n        if self.stratified_:\n            folds = StratifiedKFold(self.n_splits_, shuffle=self.shuffle, random_state=seed)\n        else:\n            folds = KFold(self.n_splits_, shuffle=self.shuffle, random_state=seed)\n        oof = np.zeros(len(train_y))\n        for fold, (train_idx, val_idx) in enumerate(folds.split(train_X, train_y)):\n            print('fold %d' % (fold + 1))\n            trn_data, trn_y = train_X.iloc[train_idx], train_y[train_idx]\n            val_data, val_y = train_X.iloc[val_idx], train_y[val_idx]\n            if self.estimator_ == 'lgbm':\n                train_set = lgb.Dataset(data=trn_data.values, label=trn_y.values)\n                val_set = lgb.Dataset(data=val_data.values, label=val_y.values)              \n                clf = lgb.train(params=params, train_set=train_set, num_boost_round=self.num_round_, valid_sets=[train_set, val_set], \n                                verbose_eval=500, early_stopping_rounds=self.early_stop)\n                oof[val_idx] = clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)\n                \n            elif self.estimator_ == 'xgb':\n                train_set = xgb.DMatrix(data=trn_data, label=trn_y)\n                val_set = xgb.DMatrix(data=val_data, label=val_y)\n                watchlist = [(train_set, 'train'), (val_set, 'valid')]\n                clf = xgb.train(self.params_, train_set, self.num_round_, watchlist, early_stopping_rounds=self.early_stop, verbose_eval=500)\n                oof[val_idx] = clf.predict(val_set, ntree_limit=clf.best_ntree_limit)\n            \n            elif self.estimator_ == 'cat':\n                clf = CatBoostClassifier(self.num_round_, task_type='GPU', early_stopping_rounds=self.early_stop, **self.params_)\n                # clf = CatBoostClassifier(self.num_round_, early_stopping_rounds=self.early_stop, **self.params_)\n                clf.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=500)\n                oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n\n            # sklearn model\n            else:\n                clf = self.estimator_.fit(trn_data, trn_y)\n                try:\n                    oof[val_idx] = clf.predict_proba(val_data)[:, 1]\n                except AttributeError:\n                    oof[val_idx] = clf.decision_function(val_data)\n            \n            self.clf_list_.append(clf)\n            fold_score = roc_auc_score(train_y[val_idx], oof[val_idx])\n            self.scores_.append(fold_score)\n            print('Fold score: {:<8.5f}'.format(fold_score))\n            \n        self.oof_ = oof\n        self.score_ = roc_auc_score(train_y, oof)\n        \n        print('\\nKFold CV: %s' % list(map(lambda x: np.round(x, precision), list(self.scores_))))\n        print('Mean Kfold CV: {0} +/- {1}'.format(np.round(np.mean(self.scores_), precision), np.round(np.std(list(self.scores_)), precision)))\n        print(\"Estimated CV: {0:0.5f}\".format(self.score_))\n        \n    def predict(self, test_X):\n        self.predictions_ = np.zeros(len(test_X))\n        \n        if self.estimator_ == 'lgbm':\n            self.feature_importance_df_ = pd.DataFrame()\n            for fold, clf in enumerate(self.clf_list_):\n                fold_importance_df = pd.DataFrame()\n                fold_importance_df[\"feature\"] = clf.feature_name()\n                fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n                fold_importance_df[\"fold\"] = fold + 1\n                self.feature_importance_df_ = pd.concat([self.feature_importance_df_, fold_importance_df], axis=0)\n                # self.predictions_ += clf.predict(test_X, num_iteration=clf.best_iteration) * (self.scores_[fold] / sum(self.scores_))\n                self.predictions_ += clf.predict(test_X, num_iteration=clf.best_iteration) / self.n_splits_     \n        elif self.estimator_ == 'xgb':\n            for fold, clf in enumerate(self.clf_list_):\n                # self.predictions_ += clf.predict(xgb.DMatrix(test_X), ntree_limit=clf.best_ntree_limit) * (self.scores_[fold] / sum(self.scores_))\n                self.predictions_ += clf.predict(xgb.DMatrix(test_X), ntree_limit=clf.best_ntree_limit) / self.n_splits_\n        elif self.estimator_ == 'cat':\n            for fold, clf in enumerate(self.clf_list_):\n                # self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] / sum(self.scores_))\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] / self.n_splits_\n        else:\n            for fold, clf in enumerate(self.clf_list_):\n                # self.predictions_ += clf.predict_proba(test_X)[:, 1] * (self.scores_[fold] / sum(self.scores_))\n                self.predictions_ += clf.predict_proba(test_X)[:, 1] / self.n_splits_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d276b1d563ed161216cfbf817f1c7e38c6d2293","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Class for Bayesian Optimisation\nclass CVForBO():\n    def __init__(self, model, train_X, train_y, test_X, base_params, int_params=[], n_splits=5, num_round=20000, early_stop=200, shuffle=False):\n        self.oofs_ = []\n        self.params_ = []\n        self.predictions_ = []\n        self.cv_scores_ = []\n        self.model_ = model\n        self.train_X_ = train_X\n        self.train_y_ = train_y\n        self.test_X_ = test_X\n        self.base_params_ = base_params\n        self.int_params_ = int_params\n        self.n_splits_ = n_splits\n        self.num_round_ = num_round\n        self.early_stop = early_stop\n        self.shuffle = shuffle\n        \n    def cv(self, **opt_params):\n        for p in self.int_params_:\n            if p in opt_params:\n                opt_params[p] = int(np.round(opt_params[p]))\n        self.base_params_.update(opt_params)\n        \n        cv_model = CVClassifier(self.model_, n_splits=self.n_splits_, num_round=self.num_round_, early_stop=self.early_stop, shuffle=self.shuffle, **self.base_params_)\n        cv_model.cv(self.train_X_, self.train_y_)\n        cv_model.predict(self.test_X_)\n        \n        self.oofs_.append(cv_model.oof_)\n        self.predictions_.append(cv_model.predictions_)\n        self.params_.append(self.base_params_)\n        self.cv_scores_.append(cv_model.score_)\n        return cv_model.score_\n    \n    def post_process(self, model_type=None, oof_path='inter_oofs.csv', pred_path='inter_preds.csv', params_path='inter_params.csv'):\n        if not model_type:\n            model_type=self.model_\n        cols = ['{}_{}_{}'.format(model_type, str(self.cv_scores_[k]).split('.')[-1][:5], k) for k in range(len(self.cv_scores_))]\n        self.oof_df = pd.DataFrame(np.array(self.oofs_).T, columns=cols)\n        self.pred_df = pd.DataFrame(np.array(self.predictions_).T, columns=cols)\n        self.params_df = pd.DataFrame(self.params_).T.rename(columns={c_old: c_new for c_old, c_new in enumerate(cols)})\n        \n        self.oof_df.to_csv(oof_path)\n        self.pred_df.to_csv(pred_path)\n        self.params_df.to_csv(params_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1d7c89e76090125738329e2d185eb8ad360631d"},"cell_type":"code","source":"def standard_scaler(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    scaler = StandardScaler()\n    train_X_df = pd.DataFrame(scaler.fit_transform(train_X_df), columns=train_X_df.columns)\n    test_X_df = pd.DataFrame(scaler.transform(test_X_df), columns=test_X_df.columns)\n    return train_X_df, test_X_df\n\ndef quantile_transformer(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    transformer = QuantileTransformer(output_distribution='normal', random_state=seed)\n    train_X_df = pd.DataFrame(transformer.fit_transform(train_X_df), columns=train_X_df.columns)\n    test_X_df = pd.DataFrame(transformer.transform(test_X_df), columns=test_X_df.columns)\n    return train_X_df, test_X_df\n\ndef min_max_scaler(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    scaler = MinMaxScaler()\n    train_X_df = pd.DataFrame(scaler.fit_transform(train_X_df), columns=train_X_df.columns)\n    test_X_df = pd.DataFrame(scaler.transform(test_X_df), columns=test_X_df.columns)\n    return train_X_df, test_X_df\n\ndef power_transformer(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    scaler = PowerTransformer()\n    train_X_df = pd.DataFrame(scaler.fit_transform(train_X_df), columns=train_X_df.columns)\n    test_X_df = pd.DataFrame(scaler.transform(test_X_df), columns=test_X_df.columns)\n    return train_X_df, test_X_df\n\ndef robust_scaler(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5.0, 95.0))\n    train_X_df = pd.DataFrame(scaler.fit_transform(train_X_df), columns=train_X_df.columns)\n    test_X_df = pd.DataFrame(scaler.transform(test_X_df), columns=test_X_df.columns)\n    return train_X_df, test_X_df\n\ndef features_generation(train_X, test_X):\n    train_X_df = train_X.copy()\n    test_X_df = test_X.copy()\n    train_test_df = train_X_df.append(test_X_df)\n    \n    # Normalize the data, so that it can be used in norm.cdf(), as though it is a standard normal variable\n    scaler = StandardScaler()\n    train_test_df = pd.DataFrame(scaler.fit_transform(train_test_df), columns=train_test_df.columns)\n    print(\"Number of null/inf in dataframe: %d\" % train_test_df.isin([np.inf, -np.inf, np.nan]).sum().sum())\n    \n    new_feats = []\n    for col in train_test_df.columns:\n        # Square\n        train_test_df[col+'^2'] = train_test_df[col].pow(2)\n        \n        # Cube\n        train_test_df[col+'^3'] = train_test_df[col].pow(3)\n\n        # 4th power\n        train_test_df[col+'^4'] = train_test_df[col].pow(4)\n\n        # Cumulative percentile (not normalized)\n        train_test_df[col+'_cp'] = rankdata(train_test_df[col])\n\n        # Cumulative normal percentile\n        train_test_df[col+'_cnp'] = norm.cdf(train_test_df[col])\n        \n        new_feats.extend([col+'^2', col+'^3', col+'^4', col+'_cp', col+'_cnp'])\n    \n    for col in new_feats:\n        train_test_df[col] = ((train_test_df[col] - train_test_df[col].mean()) / train_test_df[col].std())\n    \n    train_X_df = train_test_df.iloc[0:200000]\n    test_X_df = train_test_df.iloc[200000:]\n    del train_test_df; gc.collect()\n    \n    return train_X_df, test_X_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"958ae0a609730bd4ae4e345aa950a9221664dfc5"},"cell_type":"code","source":"%%time\n\n# cat_params = {\n#     'objective': 'Logloss',\n#     'eval_metric': 'AUC',\n#     'od_type': 'Iter',\n#     'depth': 2,\n#     'bootstrap_type': 'Bernoulli',\n#     'random_seed': seed,\n#     'allow_writing_files': False}\n\n# model_name='cat'\n# shuffle = True\n# n_splits = 2\n# num_round = 50000\n# early_stop = 3000\n# model_hpo = CVForBO(model=model_name, train_X=train_X, train_y=train_y, test_X=test_X, \n#                     base_params=cat_params, int_params=[], n_splits=n_splits, \n#                     num_round=num_round, early_stop=early_stop, shuffle=shuffle)\n\n# cat_BO = BayesianOptimization(model_hpo.cv, {\n#     'subsample': (0.2, 0.6), \n#     'l2_leaf_reg': (20, 100), \n#     'random_strength': (2, 20), \n#     'eta': (0.001, 0.02),\n#     'colsample_bylevel': (0.03, 0.2),\n#     }, random_state=seed)\n\n# cat_BO = BayesianOptimization(model_hpo.cv, {\n#     'l2_leaf_reg': (20, 100),\n#     'random_strength': (2, 20),\n#     'eta': (0.001, 0.02),\n#     'subsample': (0.2, 0.6),\n#     }, random_state=seed)\n\n# cat_BO.maximize(init_points=8, n_iter=15)\n\n# print(cat_BO.max)\n# model_hpo.post_process()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111ce6f5afbea91f95937f413c9f6aeea19eb8d7","_kg_hide-output":true},"cell_type":"code","source":"%%time\n# model_seeds_list = [1337, 99999, 2018, 516, 986, 6846, 654456, 357951, 17971, 55599]\nmodel_seeds_list = [2018, 516, 986]\n\n# ======================\n# LightGBM\n# ======================\n# model_name = 'lgbm'\n# scaler = False\n# generate_features = False\n# num_round = 1000000\n# n_splits = 12\n# shuffle = False\n# early_stop = 3500\n# lgbm_params = {\n#     'bagging_freq': 5,\n#     'bagging_fraction': 0.38,\n#     'boost_from_average':'false',\n#     'boost': 'gbdt',\n#     'feature_fraction': 0.045,\n#     'learning_rate': 0.0095,\n#     'max_depth': -1,  \n#     'metric':'auc',\n#     'min_data_in_leaf': 80,\n#     'min_sum_hessian_in_leaf': 10.0,\n#     'num_leaves': 13,\n#     'num_threads': 8,\n#     'tree_learner': 'serial',\n#     'objective': 'binary', \n#     'verbosity': 1,\n#     'random_state': seed\n# }  \n    \n# ======================\n# CatBoost\n# ======================\n# model_name = 'cat'\n# scaler = False\n# generate_features = False\n# num_round = 20000\n# n_splits = 5\n# early_stop = 500\n# shuffle = True\n# cat_params = {\n#     'eval_metric': 'AUC',\n#     'bootstrap_type': 'Bernoulli',\n#     'objective': 'Logloss',\n#     'od_type': 'Iter',\n#     'depth': 2, \n#     'eta': 0.018, \n#     'l2_leaf_reg': 30, \n#     'random_strength': 5.41,\n#     'random_seed': seed,\n#     'allow_writing_files': False\n# }\n\nmodel_name = 'cat'\nscaler = False\ngenerate_features = False\nnum_round = 100000\nn_splits = 12\nearly_stop = 2000\nshuffle = True\n        \ncat_params = {\n    'eval_metric': 'AUC',\n    'bootstrap_type': 'Bernoulli',\n    'objective': 'Logloss',\n    'od_type': 'Iter',\n    'depth': 2, \n    'eta': 0.0095, \n    'l2_leaf_reg': 30, \n    'random_strength': 5.41,\n    'subsample': 0.4, \n    'random_seed': seed,\n    'allow_writing_files': False,\n}\n\n# ======================\n# GaussianNB\n# ======================\n# model_name = GaussianNB()\n# scaler = 'qt'\n# generate_features = False\n# n_splits = 5\n# shuffle = True\n\n# ======================\n# Logistic Regression\n# ======================\n# logreg_params = {\n#     'tol': 0.0001, \n#     'C': 2.5, \n#     'random_state': seed, \n#     'max_iter': 5000,\n#     'fit_intercept': True,\n#     'solver': 'liblinear',\n#     'multi_class': 'ovr',\n#     'random_state': seed,\n#     'verbose': 0\n# }\n# model_name = LogisticRegression(**logreg_params)\n# scaler = False\n# generate_features = True\n# n_splits = 5\n# early_stop = 500\n# shuffle = True\n\n# Generate features\nif generate_features:\n    train_X_df, test_X_df = features_generation(train_X, test_X)\nelse:\n    train_X_df, test_X_df = train_X, test_X\n\n# Transformer/Scaler\nif scaler == 'ss':\n    train_X_df, test_X_df = standard_scaler(train_X, test_X)\nelif scaler == 'qt':\n    train_X_df, test_X_df = quantile_transformer(train_X, test_X)\nelif scaler == 'mm':\n    train_X_df, test_X_df = min_max_scaler(train_X, test_X)\nelif scaler == 'pt':\n    train_X_df, test_X_df = power_transformer(train_X, test_X)\nelif scaler == 'rs':\n    train_X_df, test_X_df = robust_scaler(train_X, test_X)\n\n# Classifier name (to be used when storing csv)\nclf_name = model_name if isinstance(model_name, str) else model_name.__class__.__name__.lower()\nclf_name = clf_name + '_' + scaler if scaler else clf_name\n    \noofs = []\npredictions = []\nkfold_scores = {}\nfinal_cv_score = {}\nfor i, seed_ in enumerate(model_seeds_list):\n    i += 1\n    print(\"\\n#%d: processing seed %d\" % (i, seed_))\n    \n    if model_name == 'cat':\n        params = cat_params.copy()\n        params['random_seed'] = seed_\n    elif model_name == 'lgbm':\n        params = lgbm_params.copy()   \n        params['random_state'] = seed_\n    elif model_name == GaussianNB():\n        params = {}\n    elif model_name == LogisticRegression():\n        params = logreg_params.copy()   \n        params['random_state'] = seed_\n        model_name.set_params(**params)\n    else: \n        params = {}\n    \n    model = CVClassifier(model_name, n_splits=n_splits, stratified=True, num_round=num_round, early_stop=early_stop, shuffle=shuffle, **params)\n    model.cv(train_X_df, train_y)\n    model.predict(test_X_df)\n\n    kfold_scores[seed_] = model.scores_\n    final_cv_score[seed_] = model.score_\n    oofs.append(pd.DataFrame(np.array(model.oof_).T, columns=['seed_' + str(i)]))\n    predictions.append(pd.DataFrame(np.array(model.predictions_).T, columns=['seed_' + str(i)]))\n    \n# Bagged OOF and test predictions\noof_df = pd.concat(oofs, axis=1)\noof_df = pd.concat([oof_df, train_y], axis=1)\noof_df.insert(loc=0, column=index_column, value=train_data[index_column].values)\n\npredictions_df = pd.concat(predictions, axis=1)\npredictions_df.insert(loc=0, column=index_column, value=test_data[index_column].values)\n\n# CV scores\nkfold_scores_df = pd.DataFrame([kfold_scores]).T.reset_index()\nkfold_scores_df.rename(columns={'index': 'seed', 0: 'cv_score_per_each_fold'}, inplace=True)\nkfold_scores_df.insert(loc=1, column='cv_std', value=kfold_scores_df['cv_score_per_each_fold'].map(\n    lambda x: np.round(np.std(x), precision)))\n\nfinal_cv_score_df = pd.DataFrame(index=list(final_cv_score.keys()), data=list(final_cv_score.values()), \n                                 columns=['cv_mean_score']).reset_index()\nfinal_cv_score_df.rename(columns={'index': 'seed'}, inplace=True)\nfinal_cv_score_df['cv_mean_score'] = final_cv_score_df['cv_mean_score'].map(lambda x: np.round(x, precision))\nfinal_cv_score_df = final_cv_score_df.merge(kfold_scores_df, how='left', on='seed')\n\n# Saving data\noof_df.to_csv(clf_name + '_train_OOF_bagged.csv', index=False)\npredictions_df.to_csv(clf_name + '_test_bagged.csv', index=False)\nfinal_cv_score_df.to_csv(clf_name + '_cv_results.csv', index=False)\n\nprint('\\nBagged CV scores: %s' % list(map(lambda x: np.round(x, precision), list(final_cv_score.values()))))\nprint('Bagged mean CV: {0} +/- {1}\\n'.format(np.round(np.mean(list(final_cv_score.values())), precision), \n                                             np.round(np.std(list(final_cv_score.values())), precision)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4679bb946162ab8cf4c981a3a15946ae80352fd"},"cell_type":"code","source":"# ================================\n# LightGBM\n# ================================\n# 12 Folds\n# KFold CV: [0.90147, 0.89969, 0.89186, 0.90704, 0.89463, 0.90097, 0.90226, 0.90352, 0.8987, 0.90397, 0.90671, 0.8996]\n# Mean Kfold CV: 0.90087 +/- 0.00427\n# Estimated CV: 0.90077\n# Wall time: 95 min\n\n# # 12 Folds 3 seeds\n# Bagged CV scores: [0.90077, 0.90029, 0.90017]\n# Bagged mean CV: 0.90041 +/- 0.00026\n# Wall time: 6h 23min 10s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28ea7138f53e258c97c9456b9f5c3754d541b0bc"},"cell_type":"code","source":"# ================================\n# Linear Regression\n# ================================\n# 2 folds, 'tol': 0.0001, 'C': 1.0\n# KFold CV: [0.89664, 0.89267]\n# Mean Kfold CV: 0.89466 +/- 0.00198\n# Estimated CV: 0.89463\n# Wall time: 10min 10s\n\n# 2 folds, 'tol': 0.0001, 'C': 2.0\n# KFold CV: [0.89667, 0.89272]\n# Mean Kfold CV: 0.8947 +/- 0.00198\n# Estimated CV: 0.89467\n# Wall time: 10min 8s\n\n# 2 folds, 'tol': 0.0001, 'C': 3.0\n# KFold CV: [0.89665, 0.89273]\n# Mean Kfold CV: 0.89469 +/- 0.00196\n# Estimated CV: 0.89466\n# Wall time: 12min 18s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fcab9dbb35985a40cdf127ed0e592036a7e46a4"},"cell_type":"code","source":"# ================================\n# Gaussian Naive Bayes\n# ================================\n# standard_scaler\n# KFold CV: [0.89241, 0.88971, 0.88199, 0.89609, 0.89236, 0.8892, 0.88226, 0.88616, 0.88875, 0.88533]\n# Mean Kfold CV: 0.88843 +/- 0.00432\n# Estimated CV: 0.88844\n\n# quantile_transformer\n# KFold CV: [0.8933, 0.89025, 0.88326, 0.89701, 0.89259, 0.8903, 0.88379, 0.88746, 0.88942, 0.88624]\n# Mean Kfold CV: 0.88936 +/- 0.00409\n# Estimated CV: 0.88937\n\n# min_max_scaler\n# KFold CV: [0.89241, 0.88971, 0.88199, 0.89609, 0.89236, 0.8892, 0.88226, 0.88616, 0.88875, 0.88533]\n# Mean Kfold CV: 0.88843 +/- 0.00432\n# Estimated CV: 0.88844\n    \n# power_transformer\n# KFold CV: [0.89225, 0.88938, 0.88188, 0.89513, 0.89155, 0.88827, 0.88183, 0.88573, 0.88782, 0.88494]\n# Mean Kfold CV: 0.88788 +/- 0.00417\n# Estimated CV: 0.88789\n    \n# robust_scaler\n# KFold CV: [0.89241, 0.88971, 0.88199, 0.89609, 0.89236, 0.8892, 0.88226, 0.88616, 0.88875, 0.88533]\n# Mean Kfold CV: 0.88843 +/- 0.00432\n# Estimated CV: 0.88844","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6eadb93a82cf9802781ad004e371780d3281d75"},"cell_type":"code","source":"predictions_df_mean = predictions_df.copy()\npred_cols = predictions_df_mean.columns\npredictions_df_mean[target_column] = predictions_df_mean.loc[:, ~predictions_df_mean.columns.isin([index_column, target_column])].apply(np.mean, axis=1)  # 0 column is index\npredictions_df_mean = predictions_df_mean[[index_column, target_column]]\npredictions_df_mean.to_csv(clf_name + '_test.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}