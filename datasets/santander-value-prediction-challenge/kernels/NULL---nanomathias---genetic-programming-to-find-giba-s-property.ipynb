{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e893ec9b4d5edeeee071c2bc405f8844d28b5b3d"},"cell_type":"markdown","source":"# Genetic Algorithm\nLooking at [Giba's property](https://www.kaggle.com/titericz/the-property-by-giba) made me wonder how to come up with this ordering of the rows and columns, and I thought that might be a problem suitable for genetic algorithms - whether that is actually the case, or if there is a much faster closed-form solution to this problem (?), I do not know. I've opted for implementing the algorithm from scratch rather than using a library, since this was very much done for my own education. I'm sure everything can be done better, faster, more pythonic etc. \n\nStarting out on this notebook earlier today I knew nothing about genetic algorithms, except the overall concepts [from this tutorial](https://blog.sicara.com/getting-started-genetic-algorithms-python-tutorial-81ffa1dd72f9) - now I reckon I might go buy a book to actually learn about it more thoroughly. Any recommendations would be awesome :) .. any comments/improvements for the code below would also be very much appreciated."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm_notebook\n\nfrom IPython.display import clear_output, display\nfrom sklearn.externals.joblib import Parallel, delayed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6874c7189cff2d810c06d60153c5b15f4f11e8a3"},"cell_type":"markdown","source":"# Giba's Property\nFor the purpose of this notebook I'll only look at the training df, and only at the small subset presented by Giba. I imagine the algorithm should scale pretty well to the entire dataset though, albeit with minor modifications when including test data. Let's first get the subset presented by Giba"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Get the data\ntrain_df = pd.read_csv('../input/train.csv').set_index('ID')\n\n# Get columns and rows in question\ngiba_cols = [\n    \"f190486d6\",\"58e2e02e6\",\"eeb9cd3aa\",\"9fd594eec\",\"6eef030c1\",\"15ace8c9f\",\n    \"fb0f5dbfe\",\"58e056e12\",\"20aa07010\",\"024c577b9\",\"d6bb78916\",\n    \"b43a7cfd5\",\"58232a6fb\"\n]\ngiba_rows = [\n    '7862786dc', 'c95732596', '16a02e67a', 'ad960f947', '8adafbb52',\n    'fd0c7cfc2', 'a36b78ff7', 'e42aae1b8', '0b132f2c6', '448efbb28',\n    'ca98b17ca', '2e57ec99f', 'fef33cb02'\n]\n\ngiba_df = train_df.loc[giba_rows, [\"target\"]+giba_cols]\ngiba_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19be8544cc35dd5f421037505f4ad796100eaf07"},"cell_type":"markdown","source":"# Ordering rows & columns with Genetic Algorithm\nIt's pretty easy to see the structure in the above - timeseries in columns and rows, and column `f190486d6` is two steps ahead of the target. The following is my quick-n-dirty class with fitness function, breeding functions, mutation functions, etc. \n\nOne thing to note in the `fitness()` function is that I insert the `target` and `target+1` into the dataframe before score evaluation - I do this simply to direct it towards the structure above, but I reckon it isn't strictly neccesary. This should work for the entire training set as well, but for the test set one would have to modify it, especially if test&train rows are intermingled. For now I just look at Giba's subset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6dde689788f20fb54208f6b6ac074e9cf6e7b02d"},"cell_type":"code","source":"class GeneticOptimizer():\n    \n    def __init__(self, \n                 n_population=100,\n                 n_breeders=10, \n                 n_lucky=2, \n                 n_generations=10, \n                 max_row_mutations=10, \n                 max_col_mutations=10, \n                 max_combined_rows=10, \n                 max_combined_cols=10):\n        \n        # Set variables\n        self.n_population = n_population\n        self.n_generations = n_generations\n        self.n_breeders = n_breeders\n        self.n_lucky = n_lucky\n        self.max_row_mutations = max_row_mutations\n        self.max_col_mutations = max_col_mutations\n        self.max_combined_rows = max_combined_rows\n        self.max_combined_cols = max_combined_cols\n        self.history = []\n        self.fittest = []\n    \n    @staticmethod\n    def fitness(X, weights, individual):\n        \"\"\"\n        Lower score means better alignment, see sample df at:\n        https://www.kaggle.com/titericz/the-property-by-giba\n        \"\"\"\n\n        # Get a copy of our dataframe       \n        X = X.loc[individual['rows'], ['target','target+1'] + individual['cols'].tolist()]\n\n        # Shift matrix to get fitness\n        shiftLeftUp = X.iloc[1:, 1:].values\n        deleteRightDown = X.iloc[:-1, :-1].values    \n\n        # Calculate & return score\n        score = np.sum((shiftLeftUp - deleteRightDown).astype(bool).astype(int) * weights)\n        return score\n    \n    @staticmethod\n    def hash_individual(individual):\n        return hash(frozenset(individual))\n    \n    @staticmethod\n    def swap_random(seq, n):\n        \"\"\"Swaps a n-length subsequence around in seq\"\"\"\n        l = len(seq)\n        idx = range(l)\n        i1, i2 = np.random.choice(idx, 2, replace=False)\n        i1 = l-n if n + i1 >= l else i1\n        i2 = l-n if n + i2 >= l else i2\n        for m in range(n):\n            seq[i1+m], seq[i2+m] = seq[i2+m], seq[i1+m]\n            \n    @staticmethod\n    def get_parallel(verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n        return Parallel(\n            n_jobs=n_jobs,\n            pre_dispatch=pre_dispatch,\n            verbose=verbose\n        )\n            \n    def create_initial_population(self, columns, index):\n        population = []\n        for _ in range(self.n_population):\n            np.random.shuffle(columns)\n            np.random.shuffle(index)\n            population.append({'cols': np.copy(columns), 'rows': np.copy(index)})\n        return np.array(population)\n    \n    def compute_population_performance(self, population, X, weights, **kwargs):        \n        parallel = self.get_parallel(**kwargs)\n        performance = parallel(\n            delayed(self.fitness)(X, weights, individual) for individual in population\n        )\n        return np.array(performance)\n    \n    def select_from_population(self, population, performance, best_sample=3, lucky_few=1):\n        \n        # Sort the population to have best first\n        sorted_population = population[np.argsort(performance)]\n        \n        # Save the fittest individual of the generation\n        self.fittest.append(sorted_population[0])\n        \n        # Create next generation with best and random\n        nextGeneration = []\n        for i in range(best_sample):\n            nextGeneration.append(sorted_population[i])\n        for i in range(lucky_few):\n            nextGeneration.append(np.random.choice(sorted_population))\n            \n        # Shuffle new generation and return\n        np.random.shuffle(nextGeneration)        \n        return nextGeneration\n    \n    def create_child(self, breeders):\n        \n        # Mom, dad and child\n        mom = breeders[np.random.randint(0, len(breeders))]\n        dad = breeders[np.random.randint(0, len(breeders))]        \n        child_columns, child_index = [0]*self.n_cols, [0]*self.n_rows\n        \n        # Convenience function\n        def set_trait(array, index, mom_trait, dad_trait):\n            if np.random.rand() > 0.5:\n                if mom_trait not in array:\n                    array[index] = mom_trait\n            else:\n                if dad_trait not in array:\n                    array[index] = dad_trait\n        \n        # Get characteristics from parent 1\n        for i in range(self.n_cols):\n            set_trait(child_columns, i, mom['cols'][i], dad['cols'][i])\n        for i in range(self.n_rows):\n            set_trait(child_index, i, mom['rows'][i], dad['rows'][i])\n            \n        # Fill in missing values (in a sense also a mutation factor)\n        missing_cols = [c for c in mom['cols'] if c not in child_columns]\n        for i in range(self.n_cols):\n            if child_columns[i] == 0:\n                child_columns[i] = missing_cols.pop()\n                \n        missing_rows = [c for c in mom['rows'] if c not in child_index]\n        for i in range(self.n_rows):\n            if child_index[i] == 0:\n                child_index[i] = missing_rows.pop()\n                \n        return {'cols': np.array(child_columns), 'rows': np.array(child_index)}\n    \n    def create_children(self, breeders, n_children, **kwargs):\n        parallel = self.get_parallel(**kwargs)\n        nextPopulation = parallel(\n            delayed(self.create_child)(breeders) for _ in range(n_children)\n        )\n        return np.array(nextPopulation)\n    \n    def mutate_individual(self, individual):\n        if self.max_row_mutations > 0:\n            for _ in np.arange(0, np.random.randint(0, self.max_row_mutations)):\n                n = np.random.randint(1, self.max_combined_rows)\n                self.swap_random(individual['rows'], n)\n        if self.max_col_mutations > 0:\n            for _ in np.arange(0, np.random.randint(0, self.max_col_mutations)):\n                n = np.random.randint(1, self.max_combined_cols)\n                self.swap_random(individual['cols'], n)\n        return individual\n    \n    def mutate_population(self, population, **kwargs):\n        parallel = self.get_parallel(**kwargs)\n        nextPopulation = parallel(\n            delayed(self.mutate_individual)(individual) for individual in population\n        )\n        return np.array(nextPopulation)\n    \n    def get_fittest_target_error(self, X, validation_index):\n        \"\"\"Assume first column in individual is 2 steps behind target\"\"\"\n        \n        individual = self.fittest[-1]\n        \n        target_idx = [i for i in individual['rows'] if i in validation_index]\n        target = np.log1p(X.loc[target_idx, 'target'])\n        \n        target2p_col = individual['cols'][0]\n        target2p = np.log1p(X.loc[target_idx, target2p_col].shift(-2))\n        \n        return np.sqrt((target-target2p).dropna()**2).sum()        \n    \n    def fit(self, X, y, weights=None, validation_index=None, **kwargs): \n        \n        # Do not modify original\n        X = X.copy()\n    \n        # How many rows & columns do we have\n        self.n_cols = len(X.columns)\n        self.n_rows = len(X.index)        \n        \n        # Create initial population\n        population = self.create_initial_population(X.columns.tolist(), X.index.tolist())\n        \n        # Add target and target+1 to X, so as to direct the order of result\n        X.insert(0, 'target+1', y.shift(1))\n        X.insert(0, 'target', y)\n        X.fillna(0, inplace=True)\n        \n        # If no weights specified, all columns equally important\n        if weights is None:\n            weights = np.ones(X.shape[1])\n        \n        # Run the algorithm for n_generations\n        for epoch in range(self.n_generations):\n            \n            # Get performance for each individual in population            \n            performance = self.compute_population_performance(population, X, weights, **kwargs)\n            \n            # Get breeders\n            breeders = self.select_from_population(population, performance)\n            \n            # If we have a validation index, then get the train error for the best performer\n            if validation_index is not None:\n                train_error = self.get_fittest_target_error(X, validation_index)\n            else:\n                train_error = 'NaN'   \n            \n            # Update population\n            population = self.create_children(breeders, self.n_population, **kwargs)\n            \n            # Mutate population before next generation\n            population = self.mutate_population(population, **kwargs)            \n            \n            # Save to history & display\n            clear_output()\n            self.history.append({\n                \"pop_loss\": np.mean(performance),\n                \"std_pop_loss\": np.std(performance),\n                \"top_performer_loss\": np.min(performance),\n                'generation': epoch+1,\n                'Train RMSLE': train_error\n            })\n            display(pd.DataFrame(self.history).set_index('generation'))\n            \n            # Just in case\n            gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e4906138ef19523319975ae56d54e2e0813df7b"},"cell_type":"markdown","source":"This class basically creates an initially fully random population of column/row orders, and based on this breeds new combinations which minimize the fitness function - the lower the fitness function score, the close we are to a matrix that has the structure observed in Giba's subset. Let's try to run it for a few generations."},{"metadata":{"trusted":true,"_uuid":"643d00946450ff23166228ed02425bebff498b96","collapsed":true},"cell_type":"code","source":"# Weigh different columns differently in scoring (most important are those close to target)\nweights = np.exp(-np.linspace(0, np.sqrt(giba_df.shape[1]), giba_df.shape[1]))\n\n# Instantiate class and run on training data        \ngp_opt = GeneticOptimizer(\n    n_population=1000000,\n    n_breeders=1000,\n    n_lucky=100,\n    n_generations=10,\n    max_row_mutations=5,\n    max_col_mutations=5,\n    max_combined_rows=5,\n    max_combined_cols=5\n)\n\n# Fit to data\ngp_opt.fit(\n    giba_df[giba_cols], giba_df['target'], \n    n_jobs=4,\n    verbose=1,\n    weights=weights,\n    validation_index=giba_df.index.values\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"17660a3561b3d57daa7239b5795cb2803f3ae0af"},"cell_type":"markdown","source":"Locally I've managed to get a top performer that matched Giba's solution perfectly (more generations, and slightly different population settings). I imagine this approach will scale well to the entire training (and test, with modifications), where the best solution may be less neat."},{"metadata":{"trusted":true,"_uuid":"95de3038b8f9fd416ab80544f637c70c14e3f8b9","collapsed":true},"cell_type":"code","source":"best = gp_opt.fittest[-1]\ngiba_df.loc[best['rows'], ['target'] + best['cols'].tolist()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9df6cb423ce0cd1306a70225de84df38002931ef"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}