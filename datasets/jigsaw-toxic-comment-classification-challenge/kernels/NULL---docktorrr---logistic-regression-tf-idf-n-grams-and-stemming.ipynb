{"cells":[{"metadata":{"_uuid":"1506cadd27539e1517e7b21151cfdfd16d00f4b6","_cell_guid":"e931d9f1-4cc1-43a9-b919-d2e4685a46e0"},"cell_type":"markdown","source":"# Logistic Regression with TF-IDF, n-grams and stemming"},{"metadata":{"_uuid":"8cf6e4358ffc88bcc7fe1455937b21fe728a745d","_cell_guid":"88ffba45-a30d-4e89-a2cb-8fbf7a7e4428"},"cell_type":"markdown","source":"In this kernel I will compare different sets of TF-IDF features to find best one to classify each toxicity type. I will use unigrams, unigrams together with bigrams, and apply stemming and lemmatization techniques for each of the these TF-IDF matrices.\n\nIn our case best ROC-AUC scores are achieved with logistic regression (it was compared to linear SVM and naive bayes classifiers). So we will use it for cross-validation and training in this kernel."},{"metadata":{"collapsed":true,"_uuid":"4de0b6de12ab8d26583496eba8954debdf51932a","_cell_guid":"98b4e7c3-91b7-4e5e-a804-4c8b737a5f30","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin \nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk import WordNetLemmatizer, pos_tag, word_tokenize\nfrom nltk.corpus import wordnet","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"7179011620219b07eda2f50279146f9643142a48","_cell_guid":"7beec2c4-1db6-4fe6-8b36-bbcd7c4750d0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntrain_data.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"2ead1d20625e6f1fa1412f7026464415c1b6c643","_cell_guid":"b9215664-4247-43d4-bda3-ba498a10552e","trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2896ecf53a532822e827074cee91e3ed037646b0","_cell_guid":"e5b03b43-0abf-4a7f-82b4-b7e6a28dedea","trusted":true},"cell_type":"code","source":"label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2de635a57228dfcf68941d96111ee70c48abb2cc","_cell_guid":"ad590e4c-7637-4c1e-9364-c54e98f64364","trusted":true},"cell_type":"code","source":"def combined_cv_scores(X, ys, params):\n    \"\"\" CV scores for different set of labels (ys) \"\"\"\n    scores = {}\n    for col in ys.columns:\n        clf = LogisticRegression(C=params[col])\n        s = cross_val_score(clf, X, ys[col], scoring='roc_auc')\n        scores[col] = np.mean(s)\n        print('{}: {}, mean {}'.format(col, s, np.mean(s)))\n    return scores","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"46275eced7f0f54d2271d57d712b61cfb877525f","_cell_guid":"12abc076-7c1d-4707-8840-ddc50a9980cb"},"cell_type":"markdown","source":"## Unigrams"},{"metadata":{"_uuid":"ae1ddb7088f1d9ac8b11d408786193f3d33ca8c5","_cell_guid":"c730f31c-de1c-4c3d-bd69-caced8e1110b"},"cell_type":"markdown","source":"With TfidfVectorizer we can extract \"bag of words\" from the text and apply TF-IDF (term frequency - inverse document frequency) weights. Experimentaly I've figured out that applying sublinear tf scaling (1 + log(tf)) improves overall results. First let's extract unigrams from the text data."},{"metadata":{"_uuid":"73eb219ef700e0fbb75024f03904ff869acba92a","_cell_guid":"4e6c19fb-5158-4637-af24-745f06d0dd27","trusted":true},"cell_type":"code","source":"vectorizer_unigram = TfidfVectorizer(sublinear_tf=True)\nX_unigram = vectorizer_unigram.fit_transform(train_data['comment_text'])\nX_unigram.shape","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"f2047ef1269db240368c43256383cbb9b26ab570","_cell_guid":"a7357ef5-0e1b-416a-a3ab-e655bb62f32d"},"cell_type":"markdown","source":"Using grid search we can tune parameters of logistic regression (l2 penalty works best). Tuning is out of the scope of this kernel as it takes quite a lot of time. I'm just setting optimal C parameter for each of six labels and fixing corresponding cross validation scores:"},{"metadata":{"collapsed":true,"_uuid":"51322dd2d80958541a11f6e5b2857c6fb1bc89bd","_cell_guid":"d1d6755b-0df0-4e2c-ad7a-0c5fe4972b6b","trusted":true},"cell_type":"code","source":"# 'C' parameter of logistic regression, obtained by GridSearchCV \nparams_unigram = {\n    'toxic': 4,\n    'severe_toxic': 2,\n    'obscene': 3,\n    'threat': 4,\n    'insult': 3,\n    'identity_hate': 3,\n}","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"bac5ba222b84a4ecae9d113fa586cbe9009cd8aa","_cell_guid":"74318d0f-7f6a-48f7-88ad-28ed6ddf1631","trusted":true},"cell_type":"code","source":"%%time\nscores_unigram = combined_cv_scores(X_unigram, train_data[label_names], params_unigram)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"7c3509a53f073808c5e11e641d29610654b67cca","_cell_guid":"37d71f1e-9f51-457c-9c4f-6bf0f358e2a5"},"cell_type":"markdown","source":"## Bigrams"},{"metadata":{"_uuid":"32475b3c0aeec21f0abadc596bcbb92794691016","_cell_guid":"3870ecdb-e284-4f6e-8204-2cacfd9d41b7"},"cell_type":"markdown","source":"Now let's extract unigrams as well as bigrams from the text (and of course apply tf-idf vectorizing)."},{"metadata":{"_uuid":"16a05d1d13b58450137087525631bc1a5d98c1ae","_cell_guid":"8621c288-33f5-49e3-9e45-b1361e294938","trusted":true},"cell_type":"code","source":"vectorizer_bigram = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=True)\nX_bigram = vectorizer_bigram.fit_transform(train_data['comment_text'])\nX_bigram.shape","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"cb78dcee85c07f5bcabcee759d07300ef8421470","_cell_guid":"83df60c0-7733-4ef3-ac0e-5eb0c44c2720"},"cell_type":"markdown","source":"Again let's set C parameters obtained from the grid search and get corresponding CV scores:"},{"metadata":{"collapsed":true,"_uuid":"dc402b5d6da2eec3e757d111064be38137a52832","_cell_guid":"31d895bb-f88b-4840-b46a-46c07dbc65c7","trusted":true},"cell_type":"code","source":"# 'C' parameter of logistic regression, obtained by GridSearchCV \nparams_bigram = {\n    'toxic': 50,\n    'severe_toxic': 4,\n    'obscene': 30,\n    'threat': 40,\n    'insult': 12,\n    'identity_hate': 12,\n}","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"38c319aa13a246e7ee3218f539c8bd1940926dc3","_cell_guid":"fdb8ed35-7097-45e0-91bb-ea9b249e88cd","trusted":true},"cell_type":"code","source":"%%time\nscores_bigram = combined_cv_scores(X_bigram, train_data[label_names], params_bigram)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"403891ef3f5bae544169eea76ac941c2ddcbba64","_cell_guid":"9264fd61-4e09-473d-b5ec-bd7df326f7eb"},"cell_type":"markdown","source":"## Stemming, unigrams"},{"metadata":{"_uuid":"9e171fdf8fe5b74848f6d0d4765ac21936652bcf","_cell_guid":"9fb39ad8-bb65-4454-ba89-5cab88d6b518"},"cell_type":"markdown","source":"Now we'll use stemming technique - cutting the words to their root form. Let's use Snowball english stemmer algorithm from the nltk package. Ignooring english stopwords do not improve the scoring, so we'll not enable this option. First let's apply stemming to unigrams."},{"metadata":{"collapsed":true,"_uuid":"d45ca37f3eb059dc8af302b8caf6e3121e1719ff","_cell_guid":"9f16f8ef-897f-4732-aa99-fdb5030808d9","trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer('english', ignore_stopwords=False)\n\nclass StemmedTfidfVectorizer(TfidfVectorizer):\n    \n    def __init__(self, stemmer, *args, **kwargs):\n        super(StemmedTfidfVectorizer, self).__init__(*args, **kwargs)\n        self.stemmer = stemmer\n        \n    def build_analyzer(self):\n        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n        return lambda doc: (self.stemmer.stem(word) for word in analyzer(doc.replace('\\n', ' ')))","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"3705ebad3536e0c526dc04830b966ed7b5d4205f","_cell_guid":"6b6daeac-f8a6-4034-a447-56aeef5551de","trusted":true},"cell_type":"code","source":"vectorizer_stem_u = StemmedTfidfVectorizer(stemmer=stemmer, sublinear_tf=True)\nX_train_stem_u = vectorizer_stem_u.fit_transform(train_data['comment_text'])\nX_train_stem_u.shape","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"671297a254ff898148ee6156d3c4f6974b53e6a6","_cell_guid":"ae1c7144-5602-4bab-9417-a92bde603afb","trusted":true},"cell_type":"code","source":"# 'C' parameter of logistic regression, obtained by GridSearchCV \nparams_stem_u = {\n    'toxic': 3,\n    'severe_toxic': 1,\n    'obscene': 3,\n    'threat': 4,\n    'insult': 2,\n    'identity_hate': 2,\n}","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"1e3ce882af57e6f39b4ce7ff93daf6d658d92325","_cell_guid":"d12c706f-eefc-4363-be10-055804fcdce0","trusted":true},"cell_type":"code","source":"%%time\nscores_stem_u = combined_cv_scores(X_train_stem_u, train_data[label_names], params_stem_u)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"42436a9279895c893119712695a579b7f7a648f2","_cell_guid":"4a442a6c-3dc8-4aeb-a661-dc894ef864f8"},"cell_type":"markdown","source":"## Stemming, bigrams"},{"metadata":{"_uuid":"f19f72133f1e6d319091bff3f326b3baafad7733","_cell_guid":"6b16b18a-cd47-4bdf-9cfc-6ad68c0f47a7"},"cell_type":"markdown","source":"Next apply stemming to unigrams+bigrams features."},{"metadata":{"_uuid":"22a181f5bcf5b9f4c086cb0d5aa666a437cd437e","_cell_guid":"699205b4-312a-4a35-8401-9cf1eebc525d","trusted":true},"cell_type":"code","source":"vectorizer_stem_b = StemmedTfidfVectorizer(stemmer=stemmer, ngram_range=(1,2), sublinear_tf=True)\nX_train_stem_b = vectorizer_stem_b.fit_transform(train_data['comment_text'])\nX_train_stem_b.shape","execution_count":17,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4c9aa7706a89feb2362651e05679314e0e666db9","_cell_guid":"d0f5e1ec-7a77-4999-a66b-03991c07c6a6","trusted":true},"cell_type":"code","source":"# 'C' parameter of logistic regression, obtained by GridSearchCV \nparams_stem_b = {\n    'toxic': 20,\n    'severe_toxic': 3,\n    'obscene': 20,\n    'threat': 40,\n    'insult': 8,\n    'identity_hate': 12,\n}","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"69ca63ef985228fc02f48efe898e7e1e58e30636","_cell_guid":"080205c0-ea32-4850-9fdd-cca5d492777f","trusted":true},"cell_type":"code","source":"%%time\nscores_stem_b = combined_cv_scores(X_train_stem_b, train_data[label_names], params_stem_b)","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d407d8473a121b9b1702aaadb028313ed15bc0a8","_cell_guid":"25414ffd-b9b1-43b4-85a3-29a8526db965"},"cell_type":"markdown","source":"## Lemmatization"},{"metadata":{"_uuid":"2c8a20d957a5022cd8762d3e374c7a5ba1b74cf0","_cell_guid":"a70dbc6a-6f5e-4917-ae52-ef5f68260dc0"},"cell_type":"markdown","source":"Now let's apply lemmatization - getting grammatically correct normal form of the word with the use of morphology. We will use WordNetLemmatizer from nltk package and part-of-speech word tagging."},{"metadata":{"collapsed":true,"_uuid":"07495ad775a1a9424884aa11de50e020f22bf7b5","_cell_guid":"8b2688b5-49b6-45ed-8117-b1f141caccf0","trusted":true},"cell_type":"code","source":"def lemmatize(text):\n    \"\"\" Tokenize text and lemmatize word tokens \"\"\"\n    def get_pos(tag):\n        if tag.startswith('J'):\n            return wordnet.ADJ\n        elif tag.startswith('V'):\n            return wordnet.VERB\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n        elif tag.startswith('R'):\n            return wordnet.ADV\n        return wordnet.NOUN\n    \n    wnl = WordNetLemmatizer()\n    return [wnl.lemmatize(token, get_pos(tag)) for token, tag in pos_tag(word_tokenize(text))]","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"9fceb13823a698f5abe198e449219ba60c110ff2","_cell_guid":"97b9f937-5e4a-4870-9c7a-fc8b5447bab9","trusted":true},"cell_type":"code","source":"vectorizer_lemma_u = TfidfVectorizer(tokenizer=lemmatize, sublinear_tf=True)\nX_train_lemma_u = vectorizer_lemma_u.fit_transform(train_data['comment_text'])\nX_train_lemma_u.shape","execution_count":21,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0bcee37c251aa4e75e4921658c8d898384913fc4","_cell_guid":"131a7806-a1a5-48fd-865f-83ce9d73bf03","trusted":true},"cell_type":"code","source":"# 'C' parameter of logistic regression, obtained by GridSearchCV \nparams_lemma_u = {\n    'toxic': 4,\n    'severe_toxic': 2,\n    'obscene': 4,\n    'threat': 4,\n    'insult': 3,\n    'identity_hate': 3,\n}","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"0404d2394284a9c69df702a8e5b57b39e7fc588f","_cell_guid":"584b5b43-e362-4096-b04f-5c98873d9084","trusted":true},"cell_type":"code","source":"%%time\nscores_stem_b = combined_cv_scores(X_train_lemma_u, train_data[label_names], params_lemma_u)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"b72a01a7eab7eb070acf3778aa6a27de7f911995","_cell_guid":"ad5a50c7-a108-414e-8238-5ea6830ffc5a"},"cell_type":"markdown","source":"The score is not improving even compared with pure unigrams."},{"metadata":{"_uuid":"681e7c83aced06a516f8eda849641076ec86a25d","_cell_guid":"92c61627-6c5b-4581-ac0b-e6e2c83a4877"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"_uuid":"250c7dac306fc998df697fb23cd8cdfee6acca6e","_cell_guid":"df2fa9be-6684-4697-a825-7408b7e27b99"},"cell_type":"markdown","source":"As we can see stemming gives best results. Severe_toxic and identity_hate type classification gives best score with unigram features, the rest of the types do best with bigrams. So now we can apply corresponding vectorizers to test data and make predictions."},{"metadata":{"collapsed":true,"_uuid":"f9cff8af51c22f46feac7295766c12a888dae482","_cell_guid":"8526f53d-9431-40ae-8eed-21a8d7a6900d","trusted":true},"cell_type":"code","source":"models = {\n    'toxic': {'classifier': LogisticRegression(C=20), 'features': 'stem_b'},\n    'severe_toxic': {'classifier': LogisticRegression(C=3), 'features': 'stem_b'},\n    'obscene': {'classifier': LogisticRegression(C=3), 'features': 'stem_u'},\n    'threat': {'classifier': LogisticRegression(C=40), 'features': 'stem_b'},\n    'insult': {'classifier': LogisticRegression(C=8), 'features': 'stem_b'},\n    'identity_hate': {'classifier': LogisticRegression(C=2), 'features': 'stem_u'},\n}","execution_count":24,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"301237a3764fd5b37e3788d63315e09ca89caf78","_cell_guid":"3b85d6db-c150-471f-ac3c-d28d16ae57eb","trusted":true},"cell_type":"code","source":"def fit_predict_results(models, train_features, test_features, train_data, test_data):\n    result = pd.DataFrame(columns=(['id']+list(models.keys())))\n    result.id = test_data['id']\n    for label, model in models.items():\n        clf = model['classifier']\n        X = train_features[model['features']]\n        clf.fit(X, train_data[label])\n        X = test_features[model['features']]\n        predicts = clf.predict_proba(X)\n        result[label] = predicts[:,1]\n    return result","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"666de8594508fc195a0a0f541c8f496fb6876d9c","_cell_guid":"55091fe3-98ae-491c-9f64-cf582163acb9","trusted":true},"cell_type":"code","source":"# read test data\ntest_data = pd.read_csv('../input/test.csv')\ntest_data.head()","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"cff4f460b9ae8bedee7472ee125ad66e06054e31","_cell_guid":"5a6e10d2-6883-4eff-8e6c-856a160a8b92","trusted":true},"cell_type":"code","source":"# Test unigrams with stemming\nX_test_stem_u = vectorizer_stem_u.transform(test_data['comment_text'])\nX_test_stem_u.shape","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"6b721e93970bb6449d00872ba84b53414ae06b21","_cell_guid":"7e38c37c-f60f-4bf5-bffe-e4854d62912c","trusted":true},"cell_type":"code","source":"# Test bigrams with stemming\nX_test_stem_b = vectorizer_stem_b.transform(test_data['comment_text'])\nX_test_stem_b.shape","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"9056c9a9f72db27b17ca9dfa9808963d9ebe257f","_cell_guid":"f21fffbc-decf-463f-b01b-6c38a714ad40","trusted":true},"cell_type":"code","source":"%%time\ntrain_features = {\n    'stem_u': X_train_stem_u,\n    'stem_b': X_train_stem_b\n}\ntest_features = {\n    'stem_u': X_test_stem_u,\n    'stem_b': X_test_stem_b\n}\n\nresult = fit_predict_results(models, train_features, test_features, train_data, test_data)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"72dac233501f3ddbe7d9fd1b0fe4e94823f0f67c","_cell_guid":"2fac5523-5555-4342-8e96-9364c02cf30c"},"cell_type":"markdown","source":"Just for check - comments classified as threat."},{"metadata":{"_uuid":"0b37e4eb355a4d084737460f26151855bd325ccf","scrolled":true,"_cell_guid":"fb2add37-befe-4eef-ae55-1e0379394f69","trusted":true},"cell_type":"code","source":"result[result.threat>0.5][:5]","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"2b99a62fda8dc1622d85ff469cc107f74a69555a","_cell_guid":"5ccdfa21-d554-47e4-beec-42b6ee2bab5d","trusted":true},"cell_type":"code","source":"test_data['comment_text'][1053]","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"b7340fb0de085681f3d4f3ee81da441dd468f0d6","_cell_guid":"8f06a8c0-a118-4a65-b62c-685529804bc0","trusted":true,"scrolled":true},"cell_type":"code","source":"result.to_csv('submission.csv', index=False)","execution_count":33,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}