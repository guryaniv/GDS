{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7d4aa51-afc3-ef96-11e6-f944b9bdda73"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_data():\n",
        "    df = pd.read_csv(\"../input/train.csv\")\n",
        "    print (\"Shape of base training File = \", df.shape)\n",
        "    # Remove missing values and duplicates from training data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
        "    return df\n",
        "\n",
        "df = read_data()\n",
        "df_train, df_test = train_test_split(df, test_size = 0.02)\n",
        "print (df_train.head(2))\n",
        "print (df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad969c65-aac3-f40c-4650-b50ccc143fa6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "\n",
        "def eda(df):\n",
        "    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n",
        "           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n",
        "    \n",
        "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
        "    \n",
        "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
        "    \n",
        "    question_ids_counter = Counter(question_ids_combined)\n",
        "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
        "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
        "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n",
        "    \n",
        "    \n",
        "eda(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "words = re.compile(r\"\\w+\",re.I)\n",
        "stopword = stopwords.words('english')\n",
        "\n",
        "def tokenize_questions(df):\n",
        "    question_1_tokenized = []\n",
        "    question_2_tokenized = []\n",
        "\n",
        "    for q in df.question1.tolist():\n",
        "        question_1_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    for q in df.question2.tolist():\n",
        "        question_2_tokenized.append([i.lower() for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    df[\"Question_1_tok\"] = question_1_tokenized\n",
        "    df[\"Question_2_tok\"] = question_2_tokenized\n",
        "    \n",
        "    return df\n",
        "\n",
        "def train_dictionary(df):\n",
        "    \n",
        "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
        "    \n",
        "    dictionary = corpora.Dictionary(questions_tokenized)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=10000000)\n",
        "    dictionary.compactify()\n",
        "    \n",
        "    return dictionary\n",
        "    \n",
        "df_train = tokenize_questions(df_train)\n",
        "dictionary = train_dictionary(df_train)\n",
        "print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
        "\n",
        "df_test = tokenize_questions(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6c05814-8314-8af4-cabc-b39c51a58293"
      },
      "outputs": [],
      "source": [
        "def get_vectors(df, dictionary):\n",
        "    \n",
        "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
        "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
        "    \n",
        "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
        "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
        "    \n",
        "    print(question1_csc)\n",
        "    return question1_csc.transpose(),question2_csc.transpose()\n",
        "\n",
        "\n",
        "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
        "\n",
        "print (q1_csc.shape)\n",
        "print (q2_csc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b90d4fd-5c98-9f58-3e6e-1ed968b6f869"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "from sklearn.metrics.pairwise import manhattan_distances as md\n",
        "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
        "from sklearn.metrics import jaccard_similarity_score as jsc\n",
        "from sklearn.metrics.pairwise import polynomial_kernel as poke\n",
        "from sklearn.metrics.pairwise import rbf_kernel as rbfk\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
        "mms_scale_man = MinMaxScaler()\n",
        "mms_scale_euc = MinMaxScaler()\n",
        "mms_scale_mink = MinMaxScaler()\n",
        "\n",
        "def get_similarity_values(q1_csc, q2_csc):\n",
        "    cosine_sim = []\n",
        "    manhattan_dis = []\n",
        "    eucledian_dis = []\n",
        "    jaccard_dis = []\n",
        "    minkowsk_dis = []\n",
        "    polynomial_kern =[]\n",
        "    rbf_kern =[]\n",
        "    \n",
        "    for i,j in zip(q1_csc, q2_csc):\n",
        "        sim = cs(i,j)\n",
        "        cosine_sim.append(sim[0][0])\n",
        "        sim = md(i,j)\n",
        "        manhattan_dis.append(sim[0][0])\n",
        "        sim = ed(i,j)\n",
        "        eucledian_dis.append(sim[0][0])\n",
        "        sim = poke(i,j)*1000-1000\n",
        "        polynomial_kern.append(sim[0][0])        \n",
        "        sim = 1000-rbfk(i,j)*1000\n",
        "        rbf_kern.append(sim[0][0])\n",
        "        i_ = i.toarray()\n",
        "        j_ = j.toarray()\n",
        "        try:\n",
        "            sim = jsc(i_,j_)\n",
        "            jaccard_dis.append(sim)\n",
        "        except:\n",
        "            jaccard_dis.append(0)\n",
        "            \n",
        "        sim = minkowski_dis.pairwise(i_,j_)\n",
        "        minkowsk_dis.append(sim[0][0])\n",
        "    \n",
        "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis,polynomial_kern,rbf_kern   \n",
        "\n",
        "\n",
        "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
        "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis,polynomial_kern,rbf_kern = get_similarity_values(q1_csc[0:1000,:], q2_csc[0:1000,:])\n",
        "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
        "print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
        "print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
        "print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
        "print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
        "print (\"polynomial kern sample = \\n\", polynomial_kern[0:2])\n",
        "print (\"rbf_kern sample = \\n\", rbf_kern[0:2])\n",
        "\n",
        "eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
        "manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
        "minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
        "    \n",
        "mms_scale_man.fit(manhattan_dis_array)\n",
        "mms_scale_euc.fit(eucledian_dis_array)\n",
        "mms_scale_mink.fit(minkowsk_dis_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c4e9eb2-f52a-950e-8ddb-14110f64ba7b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def calculate_logloss(y_true, y_pred):\n",
        "    loss_cal = log_loss(y_true, y_pred)\n",
        "    return loss_cal\n",
        "\n",
        "q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
        "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink, y_pred_poly, y_pred_rbf = get_similarity_values(q1_csc_test, q2_csc_test)\n",
        "y_true = df_test.is_duplicate.tolist()\n",
        "\n",
        "y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
        "y_pred_man = y_pred_man_array.tolist()\n",
        "\n",
        "y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
        "y_pred_euc = y_pred_euc_array.tolist()\n",
        "\n",
        "y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
        "y_pred_mink = y_pred_mink_array.tolist()\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_cos)\n",
        "print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_man)\n",
        "print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_euc)\n",
        "print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_jac)\n",
        "print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_mink)\n",
        "print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_poly)\n",
        "print (\"The calculated log loss value on the test set for polynomial kernel sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_rbf)\n",
        "print (\"The calculated log loss value on the test set for rbf kernel sim is = %f\" %logloss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a828b3a9-228a-7faa-75f6-44116c6fedae"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}