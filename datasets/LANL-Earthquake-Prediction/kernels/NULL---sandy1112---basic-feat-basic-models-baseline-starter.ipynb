{"cells":[{"metadata":{"_uuid":"04b003365a6a4f107e86bad2b6ca2e67a14d3fe9"},"cell_type":"markdown","source":"The idea of this kernel is to see the baseline scores using the basic features (shown in inversion's kernel) and just putting them through 5 fold CV to establish starting scores. Any model with extensive feature engineering and extensive CV strategy will use these baseline scores as a starting point. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n## Parts of code taken from https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n## Basic features taken from https://www.kaggle.com/inversion/basic-feature-benchmark\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.precision = 15\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acf53953f6e0a9ab79824dfee2ba5dc425b05074"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dabd0ad32ee51a347a7dd3fd555dd2f8c22363b9"},"cell_type":"code","source":"rows = 150_000\nsegments = int(np.floor(train.shape[0] / rows))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['ave', 'std', 'max', 'min'])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.mean()\n    X_train.loc[segment, 'std'] = x.std()\n    X_train.loc[segment, 'max'] = x.max()\n    X_train.loc[segment, 'min'] = x.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5573b7794708cd3979cc96300a0858ed3536f6bf"},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Define Function for Cross Validation**"},{"metadata":{"trusted":true,"_uuid":"aada2848f83896b3dbfbeabb6d69af1eabc087af"},"cell_type":"code","source":"n_folds = 5\n\ndef mae_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train_scaled)\n    mae= -cross_val_score(model, X_train_scaled, y_train, scoring=\"neg_mean_absolute_error\", cv = kf)\n    return(mae)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b217376bd6e56d4d3bd1b44b3077ebf169ca1d7"},"cell_type":"markdown","source":"Let's try some simple models and view teir CV scores"},{"metadata":{"trusted":true,"_uuid":"ecb65d624f72739b7d204f9d8a8a16137233fce0"},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = mae_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d8a5b6d3f196d971f17e051fbde6643f0859a8f"},"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = mae_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a278be39c2578c73e4e7b61198ebdaa524057a1"},"cell_type":"code","source":"KRR = KernelRidge(alpha=0.8, kernel='polynomial', degree=3, coef0=3.5)\nscore = mae_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e0903e2d21d77a9b0816647250179bacb1de8af"},"cell_type":"code","source":"SVReg = SVR(gamma='scale', C=1.0, epsilon=0.2)\nscore = mae_cv(SVReg)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a74b4690bf443aace690a852764c86eaff74385f"},"cell_type":"code","source":"NuSVReg = NuSVR(gamma='scale', C=1.0, nu=0.1)\nscore = mae_cv(NuSVReg)\nprint(\"NuSVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd2ee69d0f019f92906369f5c3c48a7593e57ee"},"cell_type":"markdown","source":"**Create Class for simple averaging of models**"},{"metadata":{"trusted":true,"_uuid":"f2379a4133f26b00804fe72e864254441ac42eb0"},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41942daa3e373509863a1caa1a29961fd20009a6"},"cell_type":"code","source":"averaged_models = AveragingModels(models = (KRR, SVReg, NuSVReg))\n\nscore = mae_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcc9e82b4c8174845d9c3ea5693cbe54ca726f02"},"cell_type":"markdown","source":"**Fit models to training data**"},{"metadata":{"trusted":true,"_uuid":"1405d075bb05903546b6923a3eac908113d66e93"},"cell_type":"code","source":"NuSVReg.fit(X_train_scaled, y_train.values.flatten())\nSVReg.fit(X_train_scaled, y_train.values.flatten())\nKRR.fit(X_train_scaled, y_train.values.flatten())\nENet.fit(X_train_scaled, y_train.values.flatten())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f10f1a7c0494fa35c6b46d698650698c43f96efe"},"cell_type":"markdown","source":"Make predictions"},{"metadata":{"trusted":true,"_uuid":"41c00515ba76028d45c5f0697075b767cd369918"},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\nfor seg_id in X_test.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\nX_test_scaled = scaler.transform(X_test)\nsubmission['time_to_failure_NuSVReg'] = NuSVReg.predict(X_test_scaled)\nsubmission['time_to_failure_SVReg'] = SVReg.predict(X_test_scaled)\nsubmission['time_to_failure_KRR'] = KRR.predict(X_test_scaled)\nsubmission['time_to_failure'] = (submission['time_to_failure_NuSVReg']+submission['time_to_failure_SVReg']+submission['time_to_failure_KRR'])/3.0\nsubmission.drop(['time_to_failure_NuSVReg','time_to_failure_SVReg','time_to_failure_KRR'],axis=1, inplace=True)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}