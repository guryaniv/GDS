{"cells":[{"metadata":{"_cell_guid":"2386d98e-728c-44d4-9158-9f51d910d61a","_uuid":"d22e617bfd98e8df727eed405fd73cfb8b76a56b"},"cell_type":"markdown","source":"# Loading Required Libraries"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"b4f953c1e3e65c804d84bab1bbb8096284eba9bb","_cell_guid":"3e2702ef-cdb1-4fc1-8a8d-043bf290454c","id":"VO6TORg-yl3l","scrolled":false,"colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\n\n# Importing required libraries\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\n\n# keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\n# from keras.layers import Embedding\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\n\n# gensim\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"59166afb907bdaec7f6bc502283b1576c886cfc0","_cell_guid":"e50b8e65-c196-40a6-9e17-3f82b2ee00a9","id":"POSiAyaZy5Lm","colab_type":"code","trusted":false},"cell_type":"code","source":"# defining function to clean text and retrive closs-validation datasets\ndef cleantxt(txt):\n    \"\"\"\n    Cleans the string passed. Cleaning Includes-\n    1. remove special characters/symbols\n    2. convert text to lower-case\n    3. retain only alphabets\n    4. remove words less than 3 characters\n    5. remove stop-words\n    \"\"\"  \n    # collecting english stop words from nltk-library\n    stpw = stopwords.words('english')\n    \n    # Adding custom stop-words\n    stpw.extend(['www','http','utc'])\n    stpw = set(stpw)\n    \n    # using regex to clean the text\n    txt = re.sub(r\"\\n\", \" \", txt)\n    txt = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", txt)\n    txt = txt.lower()\n    txt = re.sub(r\"[^a-z ]\", \" \", txt)\n    txt = re.sub(r\"\\b\\w{1,3}\\b\", \" \",txt)\n    txt = \" \".join([x for x in txt.split() if x not in stpw])\n    return txt\n\n\ndef load_data():\n    \"\"\"\n    Loads data and returns train, val, and test splits\n    \"\"\"\n    # Load the train dataset\n    df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n    \n    # Clean the text\n    df['comment_text'] = df.comment_text.apply(lambda x : cleantxt(x))\n    \n    # separate explanatory and dependent variables\n    X = df.iloc[:,1]\n    y = df.iloc[:,2:]\n\n    # split for cross-validation (train-60%, validation 20% and test 20%)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fedc9b5a07c8c77381f2a3d2ead7a447b176d22c","_cell_guid":"3a95424d-bd70-4e0d-8d9d-ace05da076a3","id":"B2XrS6MByhLp","colab_type":"text"},"cell_type":"markdown","source":"# Implementation of CNN and RNN using word embeddings using word2vec and GloVe for Multi-Label text classification\n"},{"metadata":{"_uuid":"44b9f25456cd348d1f2ebc66ee7550b793ebb17d","_cell_guid":"7cdfbb35-22ef-4c14-8b21-0f32f9a36eb1","id":"CZAqRc-8yhLt","colab_type":"text"},"cell_type":"markdown","source":"## 1. Loading Data"},{"metadata":{"collapsed":true,"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"51e82dd01d9e21749cd222c2209153628d01eccb","_cell_guid":"a66755fd-26a8-4956-ade4-54f7c5dce812","id":"pgUlZs87yhL_","scrolled":true,"colab_type":"code","trusted":false},"cell_type":"code","source":"# Load the data\nX_train, X_val, X_test, y_train, y_val, y_test = load_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fc9d863868ee3fd4bab805929dbd66056d07f21","_cell_guid":"daf4d78b-f06c-4795-a450-f4d52f56017b","id":"xrRmlaX-yhMJ","colab_type":"text"},"cell_type":"markdown","source":"### 2. Tokenize text of the training data with keras text preprocessing functions ###"},{"metadata":{"_cell_guid":"50f3d6c6-978f-4432-93ac-8ebca6229c41","collapsed":true,"scrolled":true,"_uuid":"c7869a47bdb2bac61cd9ee294b420225d5702b3b","trusted":false},"cell_type":"code","source":"# Adding list of Bad words to tokanizer\nbad_words = pd.read_csv(\"../input/bad-words/bad_words.csv\")\nbad_words =  list(bad_words.bad_words.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51767870-5140-42f8-be67-3fa12779f6ed","_uuid":"0aedc385ea0a454a8a5bfa956ea2addc06c9d569","trusted":false,"collapsed":true},"cell_type":"code","source":"# Set Maximum number of words to be embedded\nNUM_WORDS = 5000\n\n# Define/Load Tokenize text function\ntokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower=True)\n\n# Fit the function on the text\ntokenizer.fit_on_texts(X_train)\n\n# Count number of unique tokens\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72e8c46f-090f-4aba-aa2a-2a95eac58932","_uuid":"cbaa5b2e987cf1fb35f8430783e09d7ac1284dd7","trusted":false,"collapsed":true},"cell_type":"code","source":"# size of bad words\nnum_badwords = len(bad_words)\nnum_badwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a63c2e0-424e-4f34-8d6b-3a37cf379ce4","collapsed":true,"_uuid":"a649db269c1a6ab5f3640bbb684e48f9a2fded24","trusted":false},"cell_type":"code","source":"n = 0\ntemp_bw = bad_words\nfor word, i in word_index.items():\n    if word in bad_words:\n        temp_bw.remove(word)\n        n = n+1\n    if i > (NUM_WORDS-num_badwords+n):\n        for bw in temp_bw:\n            tokenizer.word_index[bw] = i\n            i=i+1\n        break           ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"outputId":"9424bff9-4cab-430e-b6b3-58d3006b2550","executionInfo":{"user_tz":-60,"status":"ok","user":{"userId":"103216699183948428762","displayName":"Ashish Poigal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128"},"elapsed":8649,"timestamp":1523837964999},"_uuid":"8e08b7965c4d034d92c7b8bc7cb1ffa3da77934a","colab":{"height":34,"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/"},"_cell_guid":"e503736e-05b7-46c5-a50b-aca174879f9d","id":"IEAt7RF4yhMm","scrolled":true,"colab_type":"code","trusted":false},"cell_type":"code","source":"# Convert train and val to sequence\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_valid=tokenizer.texts_to_sequences(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"T2O3DOboyhMz","colab":{"height":52,"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/"},"executionInfo":{"user_tz":-60,"status":"ok","user":{"userId":"103216699183948428762","displayName":"Ashish Poigal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128"},"elapsed":1605,"timestamp":1523838021410},"_uuid":"b7c6251b25843a9fab0db68cb59e70cf2f6bcd34","outputId":"7d8878d3-a9f6-46e4-ddda-443b5da6caed","_cell_guid":"af164756-4e7b-4f0c-acb3-816305ca7b9f","collapsed":true,"colab_type":"code","trusted":false},"cell_type":"code","source":"# Limit size of train/val to 50 and pad the sequence\nX_train = pad_sequences(sequences_train,maxlen=50)\nX_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n\n# Convert target to array\ny_train = np.asarray(y_train)\ny_val = np.asarray(y_val)\n\n# Printing shape\nprint('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\nprint('Shape of label train and validation tensor:', y_train.shape,y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc59dbf5861e99ce827e7a578bbd8f0a9634012b","_cell_guid":"ea1bb063-cab1-468c-bd36-4dd81c2c591c","id":"8hIDotl3yhNc","colab_type":"text"},"cell_type":"markdown","source":"# Build network and train it untill validation loss reduces (EarlyStopping)"},{"metadata":{"collapsed":true,"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"b648d26622154d789479c67724a8e22a37b6b532","_cell_guid":"949be9f8-1983-410a-944e-f1cc56df66a5","id":"4c2jgSmGyhNf","scrolled":true,"colab_type":"code","trusted":false},"cell_type":"code","source":"embedding_vecor_length = 300\nmodel = Sequential()\nmodel.add(Embedding(5000, embedding_vecor_length, input_length=X_train.shape[1]))\nmodel.add(LSTM(256))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(6, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"dd0374ebc2a71a58b66ecc12f90a0d17d17c7c00","outputId":"f4e2826d-b933-4e3d-a98d-9ca5962c5933","_cell_guid":"465040ac-0a16-4ca3-8d80-71d121bbc9f3","id":"xdAOHA2dyhNn","scrolled":false,"colab_type":"code","trusted":false,"collapsed":true},"cell_type":"code","source":"# Compiling Model using optimizer\nopt = Adam(lr=1e-3)\nmodel.compile(loss='binary_crossentropy',optimizer=opt)\n\n# Fitting Model to the data\ncallbacks = [EarlyStopping(monitor='val_loss')]\nhist_adam = model.fit(X_train, np.asarray(y_train), batch_size=300, epochs=20, verbose=2, validation_data=(X_val, np.asarray(y_val)),\n         callbacks=callbacks)  # starts training","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04ca7f41-48dd-4615-b825-549769232f15","scrolled":false,"_uuid":"7dd0a36d71a55f9ec45d2174c1cf987f56e419ae","trusted":false,"collapsed":true},"cell_type":"code","source":"#plotting Loss\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.xlabel('Epoch', fontsize=14)\nplt.plot(hist_adam.history['loss'], color='b', label='Training Loss')\nplt.plot(hist_adam.history['val_loss'], color='r', label='Validation Loss')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"532a1ec3af6233e091d08b87d525d49a2f3a538f","_cell_guid":"78074769-9cd2-4cba-a763-86722a6b837c","id":"6DCC0q6fyhN3","colab_type":"text"},"cell_type":"markdown","source":"# Predict on test data"},{"metadata":{"collapsed":true,"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"2c071e7ed7efefa58b88251ed6abf51484ed4a9d","_cell_guid":"1d9710fb-d034-4fe7-b6d6-60a8a06f1715","id":"MMxTx2ZyyhN6","colab_type":"code","trusted":false},"cell_type":"code","source":"# convert test to sequence and padding the sequence\nsequences_test=tokenizer.texts_to_sequences(X_test)\nX_test2 = pad_sequences(sequences_test,maxlen=X_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"colab":{"autoexec":{"startup":false,"wait_interval":0}},"_uuid":"46aadb7d1a8c53af7d928d9eeecd3863b93e6c4f","_cell_guid":"5bd2f085-45d6-4274-8e16-c5247fe63dc9","id":"gErAMYukyhOa","colab_type":"code","trusted":false},"cell_type":"code","source":"# Creating empty prediction array\ncol = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n\n# Predict on train, val and test datasets\npred_train = model.predict(X_train)\npred_test = model.predict(X_test2)\npred_val = model.predict(X_val)\n\n# Emply array to collect AUC scores\nAUC = np.zeros((3,6))\nAUC","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2513e888-1522-436a-9785-18c653f7b5a6","collapsed":true,"_uuid":"4ed66eb12ae4abf8eb1f34611f8f31a71d7504fc","trusted":false},"cell_type":"code","source":"from sklearn import metrics\nfor i,x in enumerate(col):\n    auc = np.array([metrics.roc_auc_score(y_train[:,i], pred_train[:,i]),\n                    metrics.roc_auc_score(y_val[:,i], pred_val[:,i]),\n                    metrics.roc_auc_score(y_test[x], pred_test[:,i])])\n    print(x,\"Train AUC:\",auc[0],\", Val AUC:\",auc[1],\", Test AUC:\",auc[2])\n    AUC[:,i] = auc\n    \navg_auc = AUC.mean(axis=1)\nprint(\"Average Train AUC:\",avg_auc[0],\", Average Val AUC:\",avg_auc[1],\", Average Test AUC:\",avg_auc[2])","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"views":{},"provenance":[],"name":"CNN_word2vec_.ipynb","default_view":{},"toc_visible":true,"version":"0.3.2"},"language_info":{"pygments_lexer":"ipython3","version":"3.6.4","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}