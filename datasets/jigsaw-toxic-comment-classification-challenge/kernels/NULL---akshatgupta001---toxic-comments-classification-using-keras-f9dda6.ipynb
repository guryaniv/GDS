{"cells":[{"metadata":{"trusted":true,"_uuid":"c5be0b51fc5e797da3df1960b38c910df44a6cf5"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K\nfrom keras.layers import Convolution1D, GlobalMaxPooling1D, GlobalAveragePooling1D,GlobalMaxPool1D\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, concatenate,Concatenate\nfrom keras.layers.merge import concatenate\nfrom keras.layers.core import Dense, Activation, Dropout\nimport codecs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c49f4cc4-aa70-41ec-bfad-bf6f00c9d446","_uuid":"34d2dc98c609de470a7d1ab9694576ee9ab7022e","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport xgboost as xgb\nnp.random.seed(25)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1f88cb9-0a20-4a92-bd78-1fff07b25e15","_uuid":"aadab28ec4efb4d9384b80c7188c687b2c877b1a","collapsed":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a4a1c3c-0a59-4ebb-8942-9f90085b7458","_uuid":"4b783331a1c972ac13121020dd94acb28e8f72c4","collapsed":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7dfbde8-7395-4f1d-9d6e-dbbb03b04568","_uuid":"1434058056af0c85e0b9349a99ba6c9c1ee20ebd","collapsed":true,"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3c7e083-6380-4e5a-a63b-d8e143015527","_uuid":"3d52fa047d79cf2493fd46b39801c4237bbf324b"},"cell_type":"markdown","source":"Let's explore some comments."},{"metadata":{"_cell_guid":"d3d968f8-56b6-48a8-8f48-67860d22ee13","_uuid":"be8efc3f78339f982611e97559be4503f610cef7","collapsed":true,"trusted":true},"cell_type":"code","source":"train['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16a6740b-bcf4-425f-8c5e-a5358d001147","_uuid":"5f23c4c12ca8c0f39387c98464cbace8d65c2160","collapsed":true,"trusted":true},"cell_type":"code","source":"train['comment_text'][1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9c418fe3-bac9-4dc5-95e9-a1f95b220d3e","_uuid":"eb2e393cabac8f5e19485f7a6f794c8e683b5aeb","collapsed":true,"trusted":true},"cell_type":"code","source":"train.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9447f20a-6c44-4255-bce1-d0ffd83226f2","_uuid":"3cb447b923f05aa3321fa060e663c2f764e9cc4c"},"cell_type":"markdown","source":"No null values are here. Let's see more about each category."},{"metadata":{"_cell_guid":"e64cf3a3-4e5f-4d24-82ab-2a6aa295d542","_uuid":"91a34312dc84df21cc7e1a908caba1e81515ec90","collapsed":true,"trusted":true},"cell_type":"code","source":"types = ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']\n\ntrain[types].describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ed1b777-5653-4459-bf28-d9f94418cf9c","_uuid":"6ea9baf9c61a51f99847999a8fb0c9a3cee795aa","collapsed":true,"trusted":true},"cell_type":"code","source":"count_list = []\nfor i in types:\n    count_list.append(train[i].sum())\n    \nsns.barplot(x=types, y=count_list)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f5acc8d5-4b57-4606-80a9-1ba70c371244","_uuid":"dd8dd644c0d0e8048f0f13f81a3b60a0d6a8321d"},"cell_type":"markdown","source":"It looks like highly skewed data. Most of the comments do not belong to any of these categories. So we'll do undersampling for majority class."},{"metadata":{"_cell_guid":"8177b6fc-c3d9-4306-95de-2bf70445fbeb","_uuid":"542c28c89a641e4a44a29ec98981ac871afa1bb1"},"cell_type":"markdown","source":"# Sampling"},{"metadata":{"_cell_guid":"cbda6bff-8953-442b-8ae0-4634b4195afc","_uuid":"14ed330ce86abaada9a9ce4b96660db41b387572","collapsed":true,"trusted":true},"cell_type":"code","source":"types = ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']\n\nsampled_train1 = train[(train['toxic'] ==0) & (train['severe_toxic'] ==0) & (train['obscene'] ==0)\n                       & (train['threat'] ==0) & (train['insult'] ==0) & (train['identity_hate'] ==0)]\n\nsampled_train2 = train[(train['toxic'] !=0) | (train['severe_toxic'] !=0) | (train['obscene'] !=0) \n                | (train['threat'] !=0) | (train['insult'] !=0) | (train['identity_hate'] !=0)]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"baed0500-673b-4827-a51d-79fd21d2befa","_uuid":"d5ce81a994adbfce33ede953d301bdde9e679b84","collapsed":true,"trusted":true},"cell_type":"code","source":"sampled_train2.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa8b8638-26f7-4f1f-9a39-11a42297f6a2","_uuid":"d3e67eca7d7a196cba97bb175908b84e654c5cde","collapsed":true,"trusted":true},"cell_type":"code","source":"sampled_train = sampled_train2.append(sampled_train1)\nsampled_train = sampled_train.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a30624db-0007-4e4e-85a9-184e2054fd1c","_uuid":"d9f1cd3a4c6fb14aa24dbb60cd9b35e777233962","collapsed":true,"trusted":true},"cell_type":"code","source":"sampled_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9456aa2f-5f07-4d61-8397-29b4422e22f3","_uuid":"c721949b7e2b7e2b7b60ac8afac0514ac0d507b1","collapsed":true,"trusted":true},"cell_type":"code","source":"sampled_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"548e52c7-c118-4571-95cd-b1f76fbaf980","_uuid":"0ea65cfdaa3adf4e9118aeaee0e1b2c3838b8946"},"cell_type":"markdown","source":"# Text Cleaning"},{"metadata":{"_cell_guid":"7cdc67b1-0970-41a6-a7f1-338d7dea8d56","_uuid":"710ef65792c17d4d19be2aa19db396356d62fe44","collapsed":true,"trusted":true},"cell_type":"code","source":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nstop_words = set(stopwords.words('english'))\n\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    txt = str(text)\n    \n    # Replace apostrophes with standard lexicons\n    txt = txt.replace(\"isn't\", \"is not\")\n    txt = txt.replace(\"aren't\", \"are not\")\n    txt = txt.replace(\"ain't\", \"am not\")\n    txt = txt.replace(\"won't\", \"will not\")\n    txt = txt.replace(\"didn't\", \"did not\")\n    txt = txt.replace(\"shan't\", \"shall not\")\n    txt = txt.replace(\"haven't\", \"have not\")\n    txt = txt.replace(\"hadn't\", \"had not\")\n    txt = txt.replace(\"hasn't\", \"has not\")\n    txt = txt.replace(\"don't\", \"do not\")\n    txt = txt.replace(\"wasn't\", \"was not\")\n    txt = txt.replace(\"weren't\", \"were not\")\n    txt = txt.replace(\"doesn't\", \"does not\")\n    txt = txt.replace(\"'s\", \" is\")\n    txt = txt.replace(\"'re\", \" are\")\n    txt = txt.replace(\"'m\", \" am\")\n    txt = txt.replace(\"'d\", \" would\")\n    txt = txt.replace(\"'ll\", \" will\")\n    txt = txt.replace(\"--th\", \" \")\n    \n    # More cleaning\n    txt = re.sub(r\"alot\", \"a lot\", txt)\n    txt = re.sub(r\"what's\", \"\", txt)\n    txt = re.sub(r\"What's\", \"\", txt)\n    \n    \n    # Remove urls and emails\n    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n    \n    # Replace words like sooooooo with so\n    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n    \n    # Remove punctuation from text\n    txt = ''.join([c for c in text if c not in punctuation])\n    \n    # Remove all symbols\n    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stop_words])\n        \n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"738c7e5b-ce77-479d-99d3-79b849a26372","_uuid":"50c655db148402a235b786fd11fb4b98e6af2792","collapsed":true,"trusted":true},"cell_type":"code","source":"# clean comments\nsampled_train['comment_text'] = sampled_train['comment_text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))\ntest['comment_text'] = test['comment_text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6c474d2-28fe-4fbc-8d99-8ae1b0361895","_uuid":"f8ece874bf7ae4cfc7e191850e4a09d0b7cd63a4"},"cell_type":"markdown","source":"# Model"},{"metadata":{"_cell_guid":"ea38022d-7ec1-4cde-ba21-354f19056f78","_uuid":"69d745d1da9b39217e277e01921dcb4d7d5ab029","collapsed":true,"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 400\nMAX_NB_WORDS = 50000 #200000","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b306f4e2-3720-478e-a514-d93fb17de9c4","_uuid":"4e9ddcf9a8c676bce93d52d189e36c02c583fd2d","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(lower=False, filters='',num_words = MAX_NB_WORDS)\ntokenizer.fit_on_texts(sampled_train['comment_text'])\n\nsequences = tokenizer.texts_to_sequences(sampled_train['comment_text'])\ntest_sequences = tokenizer.texts_to_sequences(test['comment_text'])\n\ntrain_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nprint('Shape of train data tensor:', train_data.shape)\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nnb_words = (np.max(train_data) + 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56daceee-3fcc-4ff0-b4a3-52907f2cad78","_uuid":"d563a71e30ba48204a856a4d354c516d7e441b67","collapsed":true,"trusted":true},"cell_type":"code","source":"from keras.layers.recurrent import LSTM, GRU\nmodel = Sequential()\nmodel.add(Embedding(nb_words,50,input_length=MAX_SEQUENCE_LENGTH))\n# model.add(SpatialDropout1D(0.2))\n# model.add(Bidirectional(GRU(20, return_sequences=True)))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2948dd1-2c3a-4dc2-9b9c-1300a445abd9","_uuid":"bd79226f8703dae0bae909a4b739d44e62b26490","collapsed":true,"trusted":true},"cell_type":"code","source":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']\ny = sampled_train[labels].values\nmodel.fit(train_data, y, validation_split=0.2, nb_epoch=5, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8e69172-b0a1-42b4-859c-cd31ef517ba5","_uuid":"6b0c84c0fc96d5498daf7802c048791111173f33","collapsed":true,"trusted":true},"cell_type":"code","source":"pred = model.predict(test_data)\npred[:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b32d9c7-fe29-4a1b-86a0-0094f730b1e6","_uuid":"65ef574ede08f8dd0bdf8c6bac59fae7aa2c4b49","collapsed":true,"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n\nsample_submission[labels] = pred\n\nsample_submission.to_csv(\"result.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"931d1863-9d58-4a64-bceb-6d36b2bd76cf","_uuid":"f318b5e0b21c88acf4d7cf7ba8a95f4145f252cc","collapsed":true,"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3478cdc4-2d0e-4b93-b86e-c00fd0b3556d","_uuid":"341ee4a18b5ef063dfb4b12078c2e3fe5d1e2160","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}