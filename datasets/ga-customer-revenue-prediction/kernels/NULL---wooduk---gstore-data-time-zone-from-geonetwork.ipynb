{"cells":[{"metadata":{"_uuid":"959cf2c1ce25d71350d34747ecce74add010b718"},"cell_type":"markdown","source":"# Using geoNetwork information to infer local times for visits \n\n![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Standard_World_Time_Zones.png/1024px-Standard_World_Time_Zones.png)\n\n## Introduction\n\nPerhaps people visiting the GStore at lunchtime from work have a greater propensity to make a purchase than a visitor who is at home in the evening. Or perhaps not. But testing any hypothesis based on a hunch that the time of day might influence propensity to visit or purchase, requires that we know what the time was for the visitor when they made their visit. \n\nIn the competition data the `visitStartTime` parameter records the time at which each visit begins as POSIX, which as I understand it is UTC, for all visits. This won’t necessarily reflect the time on the visitor’s clock. For example, if I am here in Japan and I visit the GStore at 1 PM (JST) this would be recorded as 4 AM (UTC) in the `visitStartTime` parameter.  \n\nFortunately, the data also includes some information about the visitor’s location. This might allow us to work out what the visitor’s wall clock time was at the time of their visit. I've had issues in the past with the reliability of location information that is derived from IP addresses (as I believe these are). Will the location data available be of sufficient quality to allow us to compliment `visitStartTime` with local time information? \n\nThe objectives for this notebook are:\n\n 1. To make an assessment of the quality of the location data \n 2. Explore how to use the location information to calculate the local time for a visit\n 3. Make a visual comparison of the affects of looking at visits by local time, rather than UTC for all visitors.\n \nI'll be relying on the `pytz` and `geopy` packages (and using Google's location API.) I'm also using `seaborn` to draw some heatmaps.\n\n(Because of the use of Google’s API, I’ll include the code in comments and supply the data I fetched from the API in an additional data file)."},{"metadata":{"trusted":true,"_uuid":"57a614319e4b0707d062c76e45f6187b2b80cbac","_kg_hide-input":false},"cell_type":"code","source":"# imports and declarations\nimport pickle\nimport json\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.display import display, HTML\nimport geopy\nimport pytz\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nplt.style.use('ggplot')\ng = geopy.GoogleV3(\"...\")   #os.getenv('GOOGLE_API_KEY'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a37270e9229d16ed7af7825f66320c83e561b41"},"cell_type":"markdown","source":"## Expand the geoNetwork data\n\nFirst task is to load in the data. The `geoNetwork` field provides information about the location of the visitor for each visit. It is stored as JSON. Let's flatten that out into the columns of a DataFrame.\n\nWe'll also drop the columns that contain no useful information (i.e. 'not available in demo dataset') and save the resulting dataFrame back to disk.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# read both train and test files in and concatenate\ndf1 = pd.read_csv('../input/ga-customer-revenue-prediction/train.csv',usecols=['visitId','fullVisitorId','visitStartTime','geoNetwork'],dtype=str)\ndf2 = pd.read_csv('../input/ga-customer-revenue-prediction/test.csv',usecols=['visitId','fullVisitorId','visitStartTime','geoNetwork'],dtype=str)\ndf1['dsrc']='train'; df2['dsrc']='test' # remember which was which!\ndf=pd.concat([df1,df2]); del df1,df2; df.reset_index(inplace=True,drop=True);\nregular_columns=set(df.columns)-set(['geoNetwork'])\n\n# 'expand' the json in geoNetwork into columns\nexpandedCols=pd.io.json.json_normalize(df['geoNetwork'].apply(json.loads))\nexpandedCols.columns=['geoNetwork'+'.'+x for x in expandedCols.columns]\n        \n# There is some special text in the raw data 'not available in demo dataset' \n# Takes up an unneccassary amount of space, remove columns that are full of that\norig_cols = expandedCols.columns\nexpandedCols=expandedCols.applymap(lambda v: None if v=='not available in demo dataset' else v).dropna(axis=1,how='all')\ndropped_cols = set(orig_cols) - set(expandedCols.columns)\nprint(f'Dropping {dropped_cols} because not available in demo dataset')\n          \n# save this result to disk\n\nfdf=pd.concat([df[list(regular_columns)],expandedCols],axis=1)\n\nwith open('../input/exgeo_time_train_test.pickle', 'wb') as fh:\n     pickle.dump(fdf, fh, protocol=pickle.HIGHEST_PROTOCOL)\n        \ndel df,expandedCols,fdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6fcc8e1ab5cd36cea3f07630b54ee00087df57"},"cell_type":"code","source":"# load up the 'expanded' geoNetwork and visit data that was created in previous step\nwith open('../input/exgeo_time_train_test.pickle', 'rb') as fh:\n     df=pickle.load(fh)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae3124ca189c32499a341bff5ac46cec92cd461"},"cell_type":"markdown","source":"## Examine the contents of geoNetwork information\n\nDo some counting of missing values and show the result."},{"metadata":{"_uuid":"3f7e087d9ad7295ba2e29b7ef58e4f3f0dc85746","trusted":true},"cell_type":"code","source":"summary_df={}\nfor c in ['geoNetwork.continent','geoNetwork.subContinent','geoNetwork.country','geoNetwork.region','geoNetwork.city','geoNetwork.metro','geoNetwork.networkDomain']:\n    n_missing=df[df[c].isnull() | (df[c]=='(not set)')].shape[0]\n    summary_df[c]={'n_missing':int(n_missing),'p_missing':n_missing/df[c].shape[0],'n_unique':df[c].nunique()}\n    \ndisplay(HTML(pd.DataFrame(summary_df).T[['n_missing','p_missing','n_unique']].style.format({'n_missing':\"{:,.0f}\",'n_unique':\"{:,.0f}\",'p_missing':'{:.1%}'}).render()))\n\ndef no_info(v): return sum([((e=='(not set)') or (e==None) or (e=='unknown.unknown') or (e==np.nan)) for e in v]) == len(v)\ndf['no_info']=df[['geoNetwork.country','geoNetwork.city','geoNetwork.continent','geoNetwork.region','geoNetwork.subContinent','geoNetwork.metro']].apply(lambda r: no_info(r),axis=1)\ndf['no_info']=df[['geoNetwork.country','geoNetwork.city','geoNetwork.region']].apply(lambda r: no_info(r),axis=1)\n\nprint(f\"{df['no_info'].sum():,} visits with no location information\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6757024129e61d50cd20e635001080f827e62ce2"},"cell_type":"markdown","source":"In summary:\n\n * There are 2,406 visits (0.07%) without any location information. Would not be able to add any time zone information for these visits and would have to default to UTC or remove these if that bothered us.\n \n * Nearly all visits have `country`, `continent` and `subContinent` information (looks like derived from same source as they all appear together or not at all).\n     * The continents are `Asia, Europe, Americas, Africa, Oceania, (not set)`.\n\n* `region`, `city` and `metro` are much less reliably present: only 40% have `city` and/or `region` information and only 20% have `metro`.\n\n* `metro` is [The Designated Market Area (DMA) from where traffic arrived.](https://developers.google.com/analytics/devguides/reporting/core/dimsmets#view=detail&group=geo_network&jump=ga_metro) [Metro areas are the same as DMAs (Designated Market Areas) created by Nielsen Media Research](https://www.simpleviewinc.com/blog/post/2017/31/City-v-Metro-in-Google-Analytics-What-s-the-Difference-/980/\")\n \n* `region` [In U.S., a region is a state, New York, for example.](https://developers.google.com/analytics/devguides/reporting/core/dimsmets#view=detail&group=geo_network&jump=ga_region) but also contains other regions like `England` or `Kanto_JP`.\n\n* `networkDomain` is [The domain name of users ISP, derived from the domain name registered to the ISPs IP address.](https://developers.google.com/analytics/devguides/reporting/core/dimsmets#view=detail&group=geo_network&jump=ga_networkdomain) [Sometimes it might be a known company offices IP address range and resolve to their company domain](https://webmasters.stackexchange.com/questions/105762/google-analytics-what-is-not-set-under-network-domain). Unlikely to make use of networkDomain in deriving a time zone.\n\n\n"},{"metadata":{"_uuid":"8105ec2be9646aa87a57be98360e164acdd96438"},"cell_type":"markdown","source":"## Converting to local time using `pytz`\n\n`pytz` can be used to localise a UTC time to the wallclock time for another time zone as follows:\n"},{"metadata":{"trusted":true,"_uuid":"598edf0a82e229b0ef6c79cb8359dc4b2f9e6dad"},"cell_type":"code","source":"udt = pytz.utc.localize(datetime.now()) # create a timezone aware datetime object\nldt = udt.astimezone(pytz.timezone('Asia/Tokyo'))\nprint(f'E.g. {udt.strftime(\"%H:%M on %d %b %Y\")} in UTC ---> {ldt.strftime(\"%H:%M on %d %b %Y\")} in Tokyo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de9289d9dbc9f8509bce7cb786d1ec0fa6a61280"},"cell_type":"markdown","source":"The timezone above was specified using a string of the `Area/Location` where, `Area` is the name of a continent, an ocean, or `Etc`. `Location` is the name of a specific location within the area – usually a city or small island. \n\nInterestingly:\n> [Country names are not used in this scheme, primarily because they would not be robust, owing to frequent political and boundary changes. The names of large cities tend to be more permanent.](https://en.wikipedia.org/wiki/Tz_database)\n\nSo we can see we have a few problems with constructing time zone names directly from the geographical information in the data:\n\n * while almost all the visits have a `continent` assigned (only 0.2% do not), we don't have the same set of continent _and oceans_ as in the `pytz` database.\n\n* Only 40% of visits have any `city` information, we cant be sure that the cities we have in the data match the chosen cities for time zone name [ref](https://en.wikipedia.org/wiki/Tz_database).\n\nWe're going to have to do something else.\n"},{"metadata":{"_uuid":"98975f747ed88230b1900e945d3660d6141fb1d3"},"cell_type":"markdown","source":"## Using Google's APIs to get time zone\n\nAn alternative approach would be to use a geocoding service to transform the location data we do have in the data into the standard time zone names. A combination of Google's Geocoding and Timezone APIs can help here. \n\nMy first approach was to make the most detailed location string from the data we have available for each visit (labelled `geoString`) and then pass this to the geocoding API. \n\nThis gives back (hopefully) a location (lat/lon) that we can send to the Timezone API and get back the timezone information we need.\n"},{"metadata":{"trusted":true,"_uuid":"6900135b83a8574cf5e44d97732119c60bc52e59"},"cell_type":"code","source":"# try to make up the best string from city, region and country information.\ndef geo_string(r):\n    fr={k: v for k, v in r.items() if v not in ['(not set)',None,'nan', np.nan]}\n    # if you have city or region, drop country.\n    if fr.get('geoNetwork.city',None) or fr.get('geoNetwork.region',''):\n        fr.pop('geoNetwork.country', None)\n        \n    return(', '.join(list(filter(lambda v: None if v=='' else v,[fr.get('geoNetwork.city',None), fr.get('geoNetwork.region',''), fr.get('geoNetwork.country','')]))))\n    \ndf['geoString']=df[['geoNetwork.city','geoNetwork.country','geoNetwork.region','geoNetwork.metro']].apply(lambda r: geo_string(r), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97aaf2f24df4fd55843fee8261ced2544f79235e"},"cell_type":"code","source":"# fetch the timezone imformation based on the geoString we encoded.\n### careful - this makes calls to Google APIs --> could mean real $$\n## commented out here\n\n# results = []\n# errors=[]\n                                                                  \n# for c,code in df['geoString'].unique():\n#     if c> -1:   # was used for manual hackery to start at offset\n#         print(f'{c}. {code}') \n#         try:\n#             gx = g.geocode(code)\n#             if gx:\n#                 result = {code:gx.raw}\n#                 result[code]['timezone'] = g.timezone(gx.point)\n#                 result[code]['src']=code\n#                 results.append(result[code])\n#             else:\n#                 gx = g.geocode(code.split('/')[0])\n#                 if gx:\n#                     result = {code:gx.raw}\n#                     result[code]['timezone'] = g.timezone(gx.point)\n#                     result[code]['src']=code\n#                     results.append(result[code])\n\n#         except:\n#             print(f'ERROR fetching {c}. {code}')\n#             errors.append([code])\n\n# print(f'There were {len(errors)} errors.')\n# print(errors)\n\n# with open('geocode_tz.pickle', 'wb') as handle:\n#      pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2704a11c88212f29bb9d454b06ee279e6b5874b4"},"cell_type":"markdown","source":"## Checking the results of the geocoding\n\n### Erroneous country information\n\nFirst passes of the script produced a number of errors where a time zone could not be found for a location string. Looking at the output it seemed that the country information in the data was erroneous and was causing confusion. \ne.g.:\n\n    Mountain View, California, Japan, ERROR\n    Mountain View, California, China, ERROR\n    Mexico City, Mexico City, Brazil, ERROR\n    London, England, United States, ERROR\n    ...\n \nAfter trying a few hacks, I modified the creation of the `geoString` to leave out `country` information if we already had more detailed information (`city` or `region`) and re-ran the geocoding script.\n\nThere were about the same number of visits that have no location information are there, but also a handful of other cases:\n\n| geoString (input to geocoding API) | missing timezone objects |\n|----|----|\n| Riyadh, Riyadh Province \t| 1328\n| Guatemala City, Guatemala Department  |\t54\n| Hung Yen Province \t| 25\n| Managua, Managua Department \t| 15\n| Tay Ninh Province \t| 13\n| Kobe, Hyogo Prefecture \t| 6\n| Micronesia \t| 1\n\nSome of these look like they should have been picked up. Riyadh for example accounts fora thousand or so visits. Just because it was annoying me I hand-crafted some of the `geoString` values and tried them again. The code below merges the results together. Nothing like a bit of hackery...\n"},{"metadata":{"trusted":true,"_uuid":"7e4f4f0a6f3faae35b7138b0537089c1edaaeb80"},"cell_type":"code","source":"# execute this to load pre-fetched results and apply timezone to each visit\nwith open('../input/ga-support-geocode/geocode_tz.pickle', 'rb') as handle:\n     gtzinfo=pickle.load(handle)\n\nmappings = {\n    'Riyadh':'Riyadh, Riyadh Province',\n    'Guatemala City':'Guatemala City, Guatemala Department',\n    'Hung Yen':'Hung Yen Province',\n    'Tay Ninh':'Tay Ninh Province',\n    'Kobe, Japan':'Kobe, Hyogo Prefecture',\n    'Managua':'Managua, Managua Department',\n    'Micronesia':'Micronesia'\n        }\n\nwith open('../input/ga-support-geocode/geocode_tz_handcrafted.pickle', 'rb') as handle:\n     gtzinfo_hc=pickle.load(handle)\n     for g in gtzinfo_hc:\n        g['src']=mappings[g['src']] # restore to what geoString would have been before handcrafting\n\n# gtzinfo is an array of dict objects. 'src' field gives the geoString string that\n# we used to obtain location/timezone information and provides key back to original data\n# filter all the other information out for now.\nfilt_gtzinfo={}\nfor r in gtzinfo+gtzinfo_hc:\n    if r['src']=='':\n        continue\n    filt_gtzinfo[r['src']]=r.get('timezone')\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a954910e659e974fc5f7f4be91ce5fa9552630a5"},"cell_type":"markdown","source":"## Create timezone-aware datetimes from POSIX visitStartTime\n\nFinally, the meat of the issue - We have time zone information for each visit so we can go onto converting to `visitStartTime` to local times."},{"metadata":{"trusted":true,"_uuid":"0211793045811e3c89df24c0c9ba977661715cc1"},"cell_type":"code","source":"# join this timezone onto the visits data\ndf['tz']=df['geoString'].apply(lambda s: filt_gtzinfo.get(s,pytz.utc))\n\n# convert POSIX to pandas datetime object\ndf['visitStartTime_dt_utc'] = pd.to_datetime(df['visitStartTime'],unit='s',utc=True)\n\n# convert UTC to local time\ndf['visitStartTime_dt_local']=df[['visitStartTime_dt_utc','tz']].apply(lambda r: r['visitStartTime_dt_utc'].astimezone(r['tz']),axis=1)\n\n# make a note of the offset from utc in hours (for analysis)\ndf['utcoffset']=df['visitStartTime_dt_local'].apply(lambda t: int(t.strftime('%z')[:-2]))\n\n# let's have a shufty\ndisplay(df[['geoString','visitStartTime','visitStartTime_dt_utc','visitStartTime_dt_local','tz','utcoffset']].sample(10))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6c563cab835aa1fdb516775b37ec8c7b4a78d29"},"cell_type":"markdown","source":"## O-oh... Multi-time-zone countries...\n\nIf we just have country level information and that country spans multiple time zones then the time zone (like United States) we get back a timezone for a 'generic' location for that country. For example, searching for United States yields a time zone for Chicago.\n\n    `[('United States', 'America/Chicago')]`\n    \nWhich seems reasonable behaviour to me, but if the visitor was in New York, San Francisco, or Alaska then we will get their local time wrong by up to 3 hours.\n\nMaybe this is not such a big deal. But let's understand the scale of the problem.\n\nFrom [wikipedia](wikipedia) these countries span more than one time zone:\n\n    Russia, USA, Canada, Brazil, Mexico, Indonesia, Kiribati, DRC, Micronesia, Kazahstan, Mongolia, Papua New Guinea, Ukraine\n    \nFor each of these countries, how many times do we only have country-level location information? And as a consequence what will be our assignment error?"},{"metadata":{"trusted":true,"_uuid":"282b0721e84cc38b5c5a56e988b1b69690720d64"},"cell_type":"code","source":"mtz_countries={'Russia':[2,12],'United States':[-9,-4],'Brazil':[-5,-2],'Mexico':[-8,-5],'Indonesia':[7,9],'Kiribati':[12,14], 'DRC':[1,2],'Micronesia':[10,11], 'Kazakhstan':[5,6], 'Mongolia':[7,8], 'Papua New Guinea':[10,11], 'Ukraine':[2,3]}\n\n# In the Google data have 'Congo - Kinshasa' for DRC and 'Congo - ??' for Republic of congo\n# just straighten that out to match our list of multi zone countries.\ndef replace_congo(v):\n    if 'Congo' in v:\n        if 'Kinshasa' in v:\n            return 'DRC'\n        else:\n            return 'Republic of Congo'\n    else:\n        return v\n    \ndf['geoNetwork.country']=df['geoNetwork.country'].apply(lambda v: replace_congo(v))\n\nmtz_summary=[]\n\ndef is_nullv(v):\n    return (v in ['(not set)',np.nan])\n\ndef test_notnull(r):\n    t=[not is_nullv(e) for e in r]\n    return pd.Series(t).any()\n           \nfor c in mtz_countries.keys():\n    cv=df[(df[\"geoNetwork.country\"]==c)]\n    cs={}\n    cs['country']=c\n    cs['visits']=cv.shape[0]\n    cs['better_than_country']=cv[[\"geoNetwork.city\",\"geoNetwork.metro\",\"geoNetwork.region\"]].apply(lambda r: test_notnull(r),axis=1).sum()\n    if cv.shape[0]>0:\n        cs['with_region_p']=cv[\"geoNetwork.region\"].apply(lambda v:  not is_nullv(v)).sum()/cv.shape[0]\n        cs['with_metro_p']=cv[\"geoNetwork.metro\"].apply(lambda v:  not is_nullv(v)).sum()/cv.shape[0]\n        cs['with_city_p']=cv[\"geoNetwork.city\"].apply(lambda v:  not is_nullv(v)).sum()/cv.shape[0]\n        cs['better_than_country_p']=cv[[\"geoNetwork.city\",\"geoNetwork.metro\",\"geoNetwork.region\"]].apply(lambda r: test_notnull(r),axis=1).sum()/cv.shape[0]\n    \n        mtz_summary.append(cs)\n    \ncol_order=['country', 'visits', 'better_than_country_p',\n       'with_city_p', 'with_metro_p', 'with_region_p']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc1dbb3132ee73494e279ab1ee27d379fbfc3619"},"cell_type":"code","source":"x=pd.DataFrame(mtz_summary)[col_order].set_index('country')\ndisplay(HTML(x.style.format({'visits':'{:,.0f}','better_than_country_p':'{:,.1%}','with_city_p':'{:,.1%}','with_metro_p':'{:,.1%}','with_region_p':'{:,.1%}'}).render()))  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31e8108d0cf7d97b6d94f7867e42c704f087f694"},"cell_type":"markdown","source":"Perhaps what stands out most here is that we only have better than country level data for 50% of visits from the USA, which accounts for some 700k+ visits. So by assigning a Chicago time zone to these visitors, what error might we have introduced?\n\nWorst is that they were all in Alaska (UTC - 8) and we assign Chicago (UTC - 5) --> 3 hours out for half of our US based visits.\n\nMaybe the US visits we do have better-than-country level information for can tell us more about how the non-labelled visits are distributed. This makes a biggish assumption that there is nothing systematic about when visits are not labelled with city or region information.\n"},{"metadata":{"trusted":true,"_uuid":"acc214ecf6df5cb6efa6f69c86ec8e4a5aa47720"},"cell_type":"code","source":"us_visits=df[(df['geoNetwork.country']=='United States') & (df.tz.apply(lambda tz: tz.zone.split('/')[0])=='America') & (df.tz.apply(lambda tz: tz.zone not in ['America/Sao_Paulo','America/Santiago','America/Buenos_Aires']))].copy().reset_index()\nus_visits_with_region_or_city = us_visits[(~us_visits['geoNetwork.region'].isnull()) | (~us_visits['geoNetwork.region'].isnull())].reset_index()\nus_visits_without_region_or_city = us_visits[(us_visits['geoNetwork.region'].isnull()) & (us_visits['geoNetwork.region'].isnull())].reset_index()\n\nmaxutc=us_visits_with_region_or_city.utcoffset.max();minutc=us_visits_with_region_or_city.utcoffset.min()\nprob_each_time_zone = pd.DataFrame(us_visits_with_region_or_city.utcoffset.value_counts()/us_visits_with_region_or_city.shape[0]).reset_index()\nprob_each_time_zone.columns=['utc_offset','prob']\nprob_each_time_zone.sort_values(by='utc_offset',inplace=True)\nprob_each_time_zone['error']=prob_each_time_zone['utc_offset']-(-5)\ndisplay(prob_each_time_zone)\nweighted_offset = prob_each_time_zone.apply(lambda r: r.prob * r.utc_offset, axis=1).sum()\nweighted_error = prob_each_time_zone.apply(lambda r: r.prob * r.error, axis=1).sum()\nprint(f'The weighted average utc offset for United States is {weighted_offset:.1f} hours.')\nprint(f'The error by assigning to Chicago for unkown region/city is on average {weighted_error:.1f} hours.') \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0e44931ebb10d2c18ae8a39f4bfc0b635cfc019"},"cell_type":"markdown","source":"Just as another way to prove the same thing to myself:\n * Take the US visits that only have country information.\n * To each visit assign a US timezone with probability according to that which we observe in the better-than-country labelled data (as above cell)\n * Take the difference between this and Chicago (-5) and take mean to get average error through assignment.\n\nAgain, this does assume there is nothing systematic about the missing geoNetwork data."},{"metadata":{"trusted":true,"_uuid":"e2caeaa54ef44d6e11d14dc3d8a61e43682b7015"},"cell_type":"code","source":"tzs=us_visits_with_region_or_city['tz'].value_counts().index\np=(us_visits_with_region_or_city['tz'].value_counts()/us_visits_with_region_or_city.shape[0]).values\nN=us_visits_without_region_or_city.shape[0]\n\nus_visits_without_region_or_city['tz']=pd.Series(np.random.choice(tzs, N, p=p))\n\n# convert UTC to local time\nus_visits_without_region_or_city['visitStartTime_dt_local']=us_visits_without_region_or_city[['visitStartTime_dt_utc','tz']].apply(lambda r: r['visitStartTime_dt_utc'].astimezone(r['tz']),axis=1)\n\n# make a note of the offset from utc in hours (for analysis)\nus_visits_without_region_or_city['utcoffset']=us_visits_without_region_or_city['visitStartTime_dt_local'].apply(lambda t: int(t.strftime('%z')[:-2]))\n\nus_visits_without_region_or_city['off_error']=us_visits_without_region_or_city['utcoffset']+5\n\nprint(f'Mean error by assigning Chicago: {us_visits_without_region_or_city.off_error.mean():.2f} hours.')\nprint(f'Std Dev of error by assigning Chicago: {us_visits_without_region_or_city.off_error.std():.2f} hours.')\nprint(f'Median error by assigning Chicago: {us_visits_without_region_or_city.off_error.median():.2f} hours.')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b5afe4cc91fd6d76905fb3c24cfa22cd9057f0d"},"cell_type":"markdown","source":"## Visual comparison of the affect of applying local time\n\nI'm going to use a heatmap to plot a 2D histogram of number of visits by hour of the day and day of the week, both for `visitStartTime`s in UTC and the visitors 'local' time.\n\nThere will be a heatmap that groups together all visits for each `continent` described in the Google Store data (rather than from timezone description).  We note that this doesn't account for the erroneus labelling of country and continent as noticed in previous analysis."},{"metadata":{"trusted":true,"_uuid":"aec55785178cc25fbd957fa108909c883fe12b4a"},"cell_type":"code","source":"# extract hour of day and day of week for utc and local time for analysis\ndf['dow_utc']=df['visitStartTime_dt_utc'].dt.dayofweek\ndf['hod_utc']=df['visitStartTime_dt_utc'].dt.hour\ndf['dow_loc']=pd.Series([ts.dayofweek for ts in df['visitStartTime_dt_local']])\ndf['hod_loc']=pd.Series([ts.hour for ts in df['visitStartTime_dt_local']])\n\ntp_tz=df['tz'].apply(lambda z: z.zone.split('/')[0]).unique()\n\ndf['visits']=1\n\ncontinents=df['geoNetwork.continent'].unique()\nfig,ax=plt.subplots(2,len(continents),figsize=(16,6),sharey=True)\nfor i,c in enumerate(continents):\n    x=df[df['geoNetwork.continent']==c][['dow_utc','hod_utc','visits']].groupby(['dow_utc','hod_utc']).sum().reset_index().pivot(index='hod_utc',columns='dow_utc',values='visits')\n    sns.heatmap(x,ax=ax[0,i])\n    ax[0,i].set_title(c,fontsize=20)\n    ax[0,i].set_xlabel('day of week',fontsize=16)\n    if i==0:\n        ax[0,i].set_ylabel('hour of day (UTC)',fontsize=16)\n    else:\n        ax[0,i].set_ylabel('')\n \nfor i,c in enumerate(continents):\n    x=df[df['geoNetwork.continent']==c][['dow_loc','hod_loc','visits']].groupby(['dow_loc','hod_loc']).sum().reset_index().pivot(index='hod_loc',columns='dow_loc',values='visits')\n    sns.heatmap(x,ax=ax[1,i], cmap=\"bone\")\n    #ax[1,i].set_title(c,fontsize=20)\n    ax[1,i].set_xlabel('day of week',fontsize=16)\n    if i==0:\n        ax[1,i].set_ylabel('hour of day (local)',fontsize=16)\n    else:\n        ax[1,i].set_ylabel('')\n\n#tmp=fig.text(.01, -.2, \"These heatmaps compare the number of visits in each hour of the day and day of week across the different continents as labelled in the geoNetwork field.\\nThe top row are for visitStartTime in UTC, the bottom row is once the visitStartTime has been converted into the visitor's local time\\nWould visitors from the Americas really be more likely to visit in the evenings, while Europeans favour lunchtimes and Asains the morning?\", ha='left',fontsize=20,linespacing=1.5)\nplt.tight_layout()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73a0390af7efeacd7ce7bfd6e8ae3371dacb5a8e"},"cell_type":"markdown","source":"These heatmaps compare the number of visits in each hour of the day and day of week across the different continents as labelled in the geoNetwork field.\n\nThe top row are for visitStartTime in UTC, the bottom row is once the visitStartTime has been converted into the visitor's local time. Note each heat map has it's own color scale, scaled to the number of visits in that group (I'm interested in the distribution of visits not the absolute number here).\n\nLooking at the UTC maps, does it seem reasonable that visitors from the Americas really be more likely to visit in the evenings, while Europeans favour lunchtimes and Asains the morning? The bottom row seems more intuitive, with the highest number of visits clustered to daytime hours on week days.\n\n# Conclusion\n\nApplying local time seems to have been reasonably effective in allowing visits to be labelled in local time, at least at the aggregate level. The comparison charts look as one might expect intuitively.\n\nThe level of missing and erroneous data will mean that some individual visits will not have been labelled with precision. This will need to be taken into account in any subsequent analysis. It might be best to group up the time of day of the visits into parts of the day such as morning, afternoon and evening and investigate that as a predictor rather than the precise hour.\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"56821e8918f5fe768edb4d4e624a400ed9cf06ff"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}