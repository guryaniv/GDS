{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Hello everyone, I am relatively new to data science. I am describing the steps that I took for feature engineering in this project. Any feedback  will be highly appreciated."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"My kernel is highly inspired by the top kernels for this project."},{"metadata":{"trusted":true,"_uuid":"d2d6d654822644feb3152d140beeb4f20bba06d8"},"cell_type":"code","source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport gzip\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f0ef62eb6e3f20a7e23f7492fd82e8e78da4e96"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb09181df43fafaff9cfaa397e765f32cd45b48e"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57a80fc5850f862ccbe1a700c993dafcdc4ea292"},"cell_type":"markdown","source":"The Training data consists of *59* columns and *595212* rows."},{"metadata":{"trusted":true,"_uuid":"d1837cbb90343adc686bcfe4fb947b0b8ee1874d"},"cell_type":"code","source":"#checking for duplicates\ntrain_df.drop_duplicates()\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d90a996f665e84c6707118e52e66b0d23535e28"},"cell_type":"markdown","source":"There are no duplicates in the training dataset. So, we don't have to deal with that."},{"metadata":{"_uuid":"7693c6624defae6cd6ccf130c980d9f3311ce02b","trusted":true},"cell_type":"code","source":"# Checking number of rows and columns in test data\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d949bdf899ddb2980472dfccf5c2b25feff7131d"},"cell_type":"markdown","source":"The test data has *58 *features i.e., the test data has all the features as training data except for the target variable which we have to predict."},{"metadata":{"trusted":true,"_uuid":"04cb89add3db0cd89a1af6c63055730293e3966c"},"cell_type":"code","source":"# preview the data\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bd168148a902dc4278d95d0c60b39e2e8eebb04"},"cell_type":"markdown","source":"Next, let's see the distribution of target classes in the training data."},{"metadata":{"trusted":true,"_uuid":"e1b7c16a77382159a2d501cbf9e17719f019bd6e"},"cell_type":"code","source":"# Target varibale distribution\nsns.countplot(x=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db7dea42496bb361e2a6d4264bfb7c8a6b105215"},"cell_type":"markdown","source":"As, we can see the dataset is highly imbalanced. We wil now perform *undersampling* to deal with it. We choose *undersampling* as the training dataset is huge. "},{"metadata":{"trusted":true,"_uuid":"001b3326e2e57e1663f440f96da6c53d3416673a"},"cell_type":"code","source":"# Handling imbalanced dataset by undersampling\ndesired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train_df[train_df.target == 0].index\nidx_1 = train_df[train_df.target == 1].index\n\n# Get original number of records per target value\nrecords_0 = len(train_df.loc[idx_0])\nrecords_1 = len(train_df.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*records_1)/(records_0*desired_apriori)\nundersampled_records_0 = int(undersampling_rate*records_0)\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_records_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain_df = train_df.loc[idx_list].reset_index(drop=True)\n# Random shuffle\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e2661e6e5805afc3204431d22f33e1c5c56c6bb"},"cell_type":"markdown","source":"Next, we will move to feature selection. First, we will check the distribution of different *binary features*."},{"metadata":{"trusted":true,"_uuid":"6936fd277e03d1898609493d320e9436ba470047"},"cell_type":"code","source":"# Frequency Distribution of each binary variable\nbin_col = [col for col in train_df.columns if '_bin' in col]\nfor feature in bin_col:\n    print (train_df[feature].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2f86b20a4eb9c9b27b6a355dff466b4de74ce70"},"cell_type":"markdown","source":"Four binary features - *ps_ind_09_bin, ps_ind_10_bin, ps_ind_11_bin and ps_ind_12_bin* are completely dominated by zeroes. So, we will drop them."},{"metadata":{"trusted":true,"_uuid":"ac2c6e8a945d0dcaf1f660ce4e47c0378cd85fb9"},"cell_type":"code","source":"# Dropping ps_ind_09_bin, ps_ind_10_bin, ps_ind_11_bin and ps_ind_12_bin as they are completely dominated by zeros\ntrain_df = train_df.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin','ps_ind_13_bin'], axis=1)\ntest_df = test_df.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin','ps_ind_13_bin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9d475f6feaa73c57022b052f22b8fdb84d523e"},"cell_type":"markdown","source":"We will now check for missing values."},{"metadata":{"trusted":true,"_uuid":"f3e5fe295f8ebb3ee11173b2924ed9731a578b3b"},"cell_type":"code","source":"# checking for missing values\nfor feature in train_df.columns:\n    missings = train_df[train_df[feature] == -1][feature].count()\n    if missings > 0:\n        print (str(feature)+\"\\t\\t\"+str(missings))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b3063eb8d245e613609cd08a51a89a57ea81d55"},"cell_type":"markdown","source":"We drop two features due to high proportion of missing records."},{"metadata":{"trusted":true,"_uuid":"2b9704ae0d3000a90aaa12eaec83a82fef1b9dcb"},"cell_type":"code","source":"# Dropping ps_car_03_cat and ps_car_05_cat as they have a large proportion of records with missing values\ntrain_df = train_df.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1)\ntest_df = test_df.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5020edd62363f401fc964b33d80af878bc2dc11"},"cell_type":"markdown","source":"For other features, we replace the missing value by meam or mode except categorical features. For categorical features we treat *-1 (missing value)* as an *additional category value*."},{"metadata":{"trusted":true,"_uuid":"1db509dc19adc78478d603048983f182a616cf7b"},"cell_type":"code","source":"# Replacing missing values of features other than categorical\n# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain_df['ps_reg_03'] = mean_imp.fit_transform(train_df[['ps_reg_03']]).ravel()\ntrain_df['ps_car_14'] = mean_imp.fit_transform(train_df[['ps_car_14']]).ravel()\ntrain_df['ps_car_11'] = mode_imp.fit_transform(train_df[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3fae6a791200613f45f9c02ec0091bda20832d2"},"cell_type":"code","source":"train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba97e89104a759e9d8a0592af11a477c1f470ed"},"cell_type":"markdown","source":"We create a correlation matrix for float features."},{"metadata":{"trusted":true,"_uuid":"bf324d588db02d3b5d6dc89f68e1856bca6bb7a2"},"cell_type":"code","source":"train_float = train_df.select_dtypes(include=['float64'])\n# correlation matrix for float features\ncolormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9befd2b902ac3293fe1540e661ade11687709acb"},"cell_type":"markdown","source":"Pair of higly correlated features: (ps_reg_02, ps_reg_03) (ps_car_12, ps_car_13) (ps_car_12, ps_car_14) (ps_car_13, ps_car_15)"},{"metadata":{"_uuid":"79e2d78833935a72b836cf2b71de3e9b889a24e7"},"cell_type":"markdown","source":"Will treat the higly correlated features later while fitting models"},{"metadata":{"_uuid":"f743b59e817b893b5391cb4fb307c8ceaa88fee5"},"cell_type":"markdown","source":"Next, let's plot the categorical features against the target variable"},{"metadata":{"trusted":true,"_uuid":"75aaedc9a7e9645141f9095a39e7aa6b42f15308"},"cell_type":"code","source":"# plotting ps_ind_02_cat against target variable\nsns.barplot(x=\"ps_ind_02_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1c916a42c82728a89fb615d7072028a044d0f1"},"cell_type":"code","source":"# plotting ps_ind_04_cat against target variable\nsns.barplot(x=\"ps_ind_04_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58c36c0ef2541c93fd37572917907c7358ba36ce"},"cell_type":"code","source":"# plotting ps_ind_05_cat against target variable\nsns.barplot(x=\"ps_ind_05_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5aa344d50532430907e4ba40ac835095a8faccf"},"cell_type":"code","source":"# plotting ps_car_01_cat against target variable\nsns.barplot(x=\"ps_car_01_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee38ef20022177debdd81af447222ca77b9bc4f5"},"cell_type":"code","source":"# plotting ps_car_02_cat against target variable\nsns.barplot(x=\"ps_car_02_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f63dbcda9878a240492895bdc646d52e4bcd21"},"cell_type":"code","source":"# plotting ps_car_04_cat against target variable\nsns.barplot(x=\"ps_car_04_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aa6e2f3ba817a1407c7b01f2f49798b38d7581a"},"cell_type":"code","source":"# plotting ps_car_06_cat against target variable\nsns.barplot(x=\"ps_car_06_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd6726ccd2c80fcf1bb7f6cb3e1d897042fe33af"},"cell_type":"code","source":"# plotting ps_car_07_cat against target variable\nsns.barplot(x=\"ps_car_07_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbd96c7184c5700501f036e09d0c76798c18a97"},"cell_type":"code","source":"# plotting ps_car_08_cat against target variable\nsns.barplot(x=\"ps_car_08_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb37eeef4dd0e17026ad2722488ee6798f5c16e"},"cell_type":"code","source":"# plotting ps_car_09_cat against target variable\nsns.barplot(x=\"ps_car_09_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1bb2f0f33cd63b5e92015c3e87e1aba43e627b9"},"cell_type":"code","source":"# plotting ps_car_10_cat against target variable\nsns.barplot(x=\"ps_car_10_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6085222ae0e658b7cd63562a6254f935fe41f4ad"},"cell_type":"code","source":"# plotting ps_car_11_cat against target variable\nsns.barplot(x=\"ps_car_11_cat\", y=\"target\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a719f4ec92ceff7f649629bb7a13f2e789aa373"},"cell_type":"code","source":"# Dropping irrevalent feature\ntrain_df = train_df.drop(['ps_car_10_cat'], axis=1)\ntest_df = test_df.drop(['ps_car_10_cat'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bed158d9e19b2459e4ace4dd75caad9aa5b6d0d5"},"cell_type":"markdown","source":"We will now handle missing values in test data in the same way as training data."},{"metadata":{"trusted":true,"_uuid":"ff465725cca167a4dddc4b67193c632c7f30229e"},"cell_type":"code","source":"# checking for missing values in test data\nfor feature in test_df.columns:\n    missings = test_df[test_df[feature] == -1][feature].count()\n    if missings > 0:\n        print (str(feature)+\"\\t\\t\"+str(missings))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e677aa595d4b75a960c4695d45e4a74ede2b2ae8"},"cell_type":"code","source":"# Replacing missing values of features other than categorical\n# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntest_df['ps_reg_03'] = mean_imp.fit_transform(test_df[['ps_reg_03']]).ravel()\ntest_df['ps_car_14'] = mean_imp.fit_transform(test_df[['ps_car_14']]).ravel()\ntest_df['ps_car_11'] = mode_imp.fit_transform(test_df[['ps_car_11']]).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57b12e22682896932debaf276f94a2e5305d9861"},"cell_type":"code","source":"X_train = train_df.drop([\"target\",\"id\"], axis=1)\nY_train = train_df[\"target\"]\nX_test  = test_df.drop(\"id\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20f3241ae6ba42e2ba889a41afb8624e0f497d79"},"cell_type":"markdown","source":"We will do feature scaling using standardization."},{"metadata":{"trusted":true,"_uuid":"23ed364f083537493f5729a412e6d993a6056a85"},"cell_type":"code","source":"# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0946693bdaa01333b57453d27d4536774be31ad0"},"cell_type":"markdown","source":"Next, we use logistic regression to determine feature importance."},{"metadata":{"trusted":true,"_uuid":"e8806bb8b675f15ed1ac586d569a3eb539f4c032"},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n# calculating the coefficient of the features\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de2d1d41daceae9c364f863e23613770e588b603"},"cell_type":"code","source":"# Dropping less important feature from each pair of highly correlated feature  \ntrain_df = train_df.drop([\"ps_reg_03\",\"ps_car_12\",\"ps_car_15\"], axis=1)\ntest_df = test_df.drop([\"ps_reg_03\", \"ps_car_12\", \"ps_car_15\"], axis=1)\nX_train = train_df.drop([\"target\",\"id\"], axis=1)\nY_train = train_df[\"target\"]\nX_test  = test_df.drop(\"id\", axis=1).copy()\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18608f873201df545ada308cb8a688ec5c2a8dfa"},"cell_type":"markdown","source":"We are now ready to fit different models."},{"metadata":{"_uuid":"d824938807c34b22e9c6b1c4f6ca6183c0954ad8"},"cell_type":"markdown","source":"After fitting models like logistic regression, k-nearest neighbour, perceptron, gaussian Naive Bayes, decision tree, random forest, XGB classifier and artificial neural network and tuning the parameters of different models, I observed that the tuned **artificial neural network**  outperformed all other models in cross-validation. I used **three** hidden layers with **256** hidden nodes in each. I used **dropout** in each layer. I have used a batch size of **64** and number_of_epochs=**800**. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}