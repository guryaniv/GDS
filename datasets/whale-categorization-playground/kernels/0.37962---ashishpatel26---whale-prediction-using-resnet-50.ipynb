{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"from collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom scipy.stats import logistic\nfrom os.path import join\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Dense, Dropout, Lambda, Convolution2D, MaxPooling2D, Flatten\nfrom keras.losses import categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nimport os\nimport matplotlib.pyplot as plt\n","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"batch_size = 24\nembedding_dim = 50\nimage_size = 224\npath_base = '../input/whale-categorization-playground/'\npath_train = join(path_base,'train')\npath_test = join(path_base,'test')\npath_model = join(path_base,'MyModel.hdf5')\npath_csv = '../input/whale-categorization-playground/train.csv'","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"721a989504b5d31f9c341e1c4db86a2098c72423","collapsed":true},"cell_type":"code","source":"class sample_gen(object):\n    def __init__(self, file_class_mapping, other_class = \"new_whale\"):\n        self.file_class_mapping= file_class_mapping\n        self.class_to_list_files = defaultdict(list)\n        self.list_other_class = []\n        self.list_all_files = list(file_class_mapping.keys())\n        self.range_all_files = list(range(len(self.list_all_files)))\n\n        for file, class_ in file_class_mapping.items():\n            if class_ == other_class:\n                self.list_other_class.append(file)\n            else:\n                self.class_to_list_files[class_].append(file)\n\n        self.list_classes = list(set(self.file_class_mapping.values()))\n        self.range_list_classes= range(len(self.list_classes))\n        self.class_weight = np.array([len(self.class_to_list_files[class_]) for class_ in self.list_classes])\n        self.class_weight = self.class_weight/np.sum(self.class_weight)\n\n    def get_sample(self):\n        class_idx = np.random.choice(self.range_list_classes, 1, p=self.class_weight)[0]\n        examples_class_idx = np.random.choice(range(len(self.class_to_list_files[self.list_classes[class_idx]])), 2)\n        positive_example_1, positive_example_2 = \\\n            self.class_to_list_files[self.list_classes[class_idx]][examples_class_idx[0]],\\\n            self.class_to_list_files[self.list_classes[class_idx]][examples_class_idx[1]]\n\n\n        negative_example = None\n        while negative_example is None or self.file_class_mapping[negative_example] == \\\n                self.file_class_mapping[positive_example_1]:\n            negative_example_idx = np.random.choice(self.range_all_files, 1)[0]\n            negative_example = self.list_all_files[negative_example_idx]\n        return positive_example_1, negative_example, positive_example_2\n    \ndef read_and_resize(filepath):\n    im = Image.open((filepath)).convert('RGB')\n    im = im.resize((image_size, image_size))\n    return np.array(im, dtype=\"float32\")\n\n\ndef augment(im_array):\n    if np.random.uniform(0, 1) > 0.9:\n        im_array = np.fliplr(im_array)\n    return im_array\n\ndef gen(triplet_gen):\n    while True:\n        list_positive_examples_1 = []\n        list_negative_examples = []\n        list_positive_examples_2 = []\n\n        for i in range(batch_size):\n            positive_example_1, negative_example, positive_example_2 = triplet_gen.get_sample()\n            path_pos1 = join(path_train, positive_example_1)\n            path_neg = join(path_train, negative_example)\n            path_pos2 = join(path_train, positive_example_2)\n            \n            positive_example_1_img = read_and_resize(path_pos1)\n            negative_example_img = read_and_resize(path_neg)\n            positive_example_2_img = read_and_resize(path_pos2)\n\n            positive_example_1_img = augment(positive_example_1_img)\n            negative_example_img = augment(negative_example_img)\n            positive_example_2_img = augment(positive_example_2_img)\n            \n            list_positive_examples_1.append(positive_example_1_img)\n            list_negative_examples.append(negative_example_img)\n            list_positive_examples_2.append(positive_example_2_img)\n\n        A = preprocess_input(np.array(list_positive_examples_1))\n        B = preprocess_input(np.array(list_positive_examples_2))\n        C = preprocess_input(np.array(list_negative_examples))\n        \n        label = None\n        \n        yield ({'anchor_input': A, 'positive_input': B, 'negative_input': C}, label)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"679829a47db08ed4dc97b61a5d6781f70419fa2a","collapsed":true},"cell_type":"code","source":"def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus'):\n    anchor, positive, negative = inputs\n    positive_distance = K.square(anchor - positive)\n    negative_distance = K.square(anchor - negative)\n    if dist == 'euclidean':\n        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n    elif dist == 'sqeuclidean':\n        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n    loss = positive_distance - negative_distance\n    if margin == 'maxplus':\n        loss = K.maximum(0.0, 1 + loss)\n    elif margin == 'softplus':\n        loss = K.log(1 + K.exp(loss))\n    return K.mean(loss)\n\ndef triplet_loss_np(inputs, dist='sqeuclidean', margin='maxplus'):\n    anchor, positive, negative = inputs\n    positive_distance = np.square(anchor - positive)\n    negative_distance = np.square(anchor - negative)\n    if dist == 'euclidean':\n        positive_distance = np.sqrt(np.sum(positive_distance, axis=-1, keepdims=True))\n        negative_distance = np.sqrt(np.sum(negative_distance, axis=-1, keepdims=True))\n    elif dist == 'sqeuclidean':\n        positive_distance = np.sum(positive_distance, axis=-1, keepdims=True)\n        negative_distance = np.sum(negative_distance, axis=-1, keepdims=True)\n    loss = positive_distance - negative_distance\n    if margin == 'maxplus':\n        loss = np.maximum(0.0, 1 + loss)\n    elif margin == 'softplus':\n        loss = np.log(1 + np.exp(loss))\n    return np.mean(loss)\n\ndef check_loss():\n    batch_size = 10\n    shape = (batch_size, 4096)\n\n    p1 = normalize(np.random.random(shape))\n    n = normalize(np.random.random(shape))\n    p2 = normalize(np.random.random(shape))\n    \n    input_tensor = [K.variable(p1), K.variable(n), K.variable(p2)]\n    out1 = K.eval(triplet_loss(input_tensor))\n    input_np = [p1, n, p2]\n    out2 = triplet_loss_np(input_np)\n\n    assert out1.shape == out2.shape\n    print(np.linalg.norm(out1))\n    print(np.linalg.norm(out2))\n    print(np.linalg.norm(out1-out2))","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90064c8daa36a84ca2f6b95ec540ebddb3dfdb0d"},"cell_type":"code","source":"check_loss()","execution_count":45,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d2dd749451e69ac7c7cddf46e879a3715ba2e61","collapsed":true},"cell_type":"code","source":"def GetModel():\n    base_model = ResNet50(weights='imagenet', include_top=False, pooling='max')\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    x = base_model.output\n    x = Dropout(0.6)(x)\n    x = Dense(embedding_dim)(x)\n    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n    embedding_model = Model(base_model.input, x, name=\"embedding\")\n\n    input_shape = (image_size, image_size, 3)\n    anchor_input = Input(input_shape, name='anchor_input')\n    positive_input = Input(input_shape, name='positive_input')\n    negative_input = Input(input_shape, name='negative_input')\n    anchor_embedding = embedding_model(anchor_input)\n    positive_embedding = embedding_model(positive_input)\n    negative_embedding = embedding_model(negative_input)\n\n    inputs = [anchor_input, positive_input, negative_input]\n    outputs = [anchor_embedding, positive_embedding, negative_embedding]\n       \n    triplet_model = Model(inputs, outputs)\n    triplet_model.add_loss(K.mean(triplet_loss(outputs)))\n\n    return embedding_model, triplet_model","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e467fae795f6fe546e845b1df200020301d1ed33","collapsed":true},"cell_type":"code","source":"data = pd.read_csv(path_csv)\ntrain, test = train_test_split(data, train_size=0.7, random_state=1337)\nfile_id_mapping_train = {k: v for k, v in zip(train.Image.values, train.Id.values)}\nfile_id_mapping_test = {k: v for k, v in zip(test.Image.values, test.Id.values)}\ngen_tr = gen(sample_gen(file_id_mapping_train))\ngen_te = gen(sample_gen(file_id_mapping_test))\n\ncheckpoint = ModelCheckpoint(path_model, monitor='loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\ncallbacks_list = [checkpoint, early]  # early","execution_count":47,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cff6195879e97b150606af7721bb06c3ff40488"},"cell_type":"code","source":"def ShowImg(img):\n    plt.figure()\n    plt.imshow(img.astype('uint8'))\n    plt.show()\n    plt.close()\n    \nbatch = next(gen_tr)\n\nimg = batch[0]['anchor_input'][0]\nprint(img.shape)\nmean = [103.939, 116.779, 123.68]\nimg[..., 0] += mean[0]\nimg[..., 1] += mean[1]\nimg[..., 2] += mean[2]\nimg = img[..., ::-1]\nShowImg(img)","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"368b3aae41868bf9e1b5c44cf2906be90357e248"},"cell_type":"markdown","source":"# Installation of Resnet 50 Weight to keras"},{"metadata":{"trusted":true,"_uuid":"2a37832c065231ff3e96d285ec5a8175fad31299"},"cell_type":"code","source":"!ls ../input/resnet50/","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebfa38645f6b21bd944f7921ee2967d22fead5ce","collapsed":true},"cell_type":"code","source":"cache_dir  = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6695593f0d88d7c9fcd278ac42478163df36bebd"},"cell_type":"code","source":"!cp ../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 ~/.keras/models/","execution_count":51,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1efed44eb5e4fdf1e3fc018eee82590217ace13d"},"cell_type":"code","source":"!ls ~/.keras/models","execution_count":52,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56d2f72f300261fc236286c55f7b43c6ec642fcd"},"cell_type":"code","source":"embedding_model, triplet_model = GetModel()","execution_count":53,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20e8f18cecce0d4ac4031b70f4f5684eec716717"},"cell_type":"code","source":"for i, layer in enumerate(embedding_model.layers):\n    print(i, layer.name, layer.trainable)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a94595ed0f78c4f252be1284b6edff4b73da2cd1","collapsed":true},"cell_type":"code","source":"for layer in embedding_model.layers[175:]:\n    layer.trainable = True\nfor layer in embedding_model.layers[:175]:\n    layer.trainable = False","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"757e049ccb357f072457432ee29a2c66b23a7a58"},"cell_type":"code","source":"triplet_model.compile(loss=None, optimizer=Adam(0.01))\nhistory = triplet_model.fit_generator(gen_tr, \n                              validation_data=gen_te, \n                              epochs=4, \n                              verbose=1, \n                              workers=4,\n                              steps_per_epoch=200, \n                              validation_steps=20)","execution_count":56,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edd76af9c13afc4672e243c569e5003ea5e7221d"},"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.legend()\nplt.show()","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ba76c4665ebb1d6d065c3f2902964cd0b24e73d"},"cell_type":"code","source":"for layer in embedding_model.layers[150:]:\n    layer.trainable = True\nfor layer in embedding_model.layers[:150]:\n    layer.trainable = False\ntriplet_model.compile(loss=None, optimizer=Adam(0.0001))\n\nhistory = triplet_model.fit_generator(gen_tr, \n                                    validation_data=gen_te, \n                                    epochs=3, \n                                    verbose=1, \n                                    workers=4,\n                                    steps_per_epoch=70, \n                                    validation_steps=30)","execution_count":60,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2073383f0ac74aa66018cc3914855f46c6e73b96"},"cell_type":"code","source":"plt.plot(history.history['loss'], label='traning_loss')\nplt.plot(history.history['val_loss'], label='validation_loss', color = 'r')\nplt.legend()\nplt.show()","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96a8bf7225428db200b01b5d6ead5f92b992242b"},"cell_type":"code","source":"def data_generator(fpaths, batch=16):\n    i = 0\n    imgs = []\n    fnames = []\n    for path in fpaths:\n        if i == 0:\n            imgs = []\n            fnames = []\n        i += 1\n        img = read_and_resize(path)\n        imgs.append(img)\n        fnames.append(os.path.basename(path))\n        if i == batch:\n            i = 0\n            imgs = np.array(imgs)\n            yield fnames, imgs\n            \n    if i != 0:\n        imgs = np.array(imgs)\n        yield fnames, imgs\n        \n    raise StopIteration()","execution_count":62,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93c43c75d69dd9e8f32ba4af15938d9e1d9e12da"},"cell_type":"code","source":"data = pd.read_csv(path_csv)\nfile_id_mapping = {k: v for k, v in zip(data.Image.values, data.Id.values)}\nimport glob\ntrain_files = glob.glob(join(path_train, '*.jpg'))\ntest_files = glob.glob(join(path_test, '*.jpg'))","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d97d6431d149d82f5166e296ab4b7285a9f48a"},"cell_type":"code","source":"train_preds  = []\ntrain_file_names = []\nfor fnames, imgs in tqdm(data_generator(train_files, batch=32)):\n    predicts = embedding_model.predict(imgs)\n    predicts = predicts.tolist()\n    train_preds += predicts\n    train_file_names += fnames\ntrain_preds = np.array(train_preds)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d14eb882f2161ccd81232a23e9c3a04faaaa25c8"},"cell_type":"code","source":"test_preds = []\ntest_file_names = []\nfor fnames, imgs in tqdm(data_generator(test_files, batch=32)) :\n    predicts = embedding_model.predict(imgs)\n    predicts = predicts.tolist()\n    test_preds += predicts\n    test_file_names += fnames\ntest_preds = np.array(test_preds)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b74ad80b63d2921372ea961d08a57d21ab1f19c9"},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nneigh = NearestNeighbors(n_neighbors=6)\nneigh.fit(train_preds)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b31df83a0c48c2508db42d6e2c9ed877b1dca87"},"cell_type":"code","source":"distances_test, neighbors_test = neigh.kneighbors(test_preds)\ndistances_test, neighbors_test = distances_test.tolist(), neighbors_test.tolist()","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e72546552f519b658837252a34887cd3bfd0fd1b"},"cell_type":"code","source":"preds_str = []\n\nfor filepath, distance, neighbour_ in zip(test_file_names, distances_test, neighbors_test):\n    sample_result = []\n    sample_classes = []\n    for d, n in zip(distance, neighbour_):\n        train_file = train_files[n].split(os.sep)[-1]\n        class_train = file_id_mapping[train_file]\n        sample_classes.append(class_train)\n        sample_result.append((class_train, d))\n\n    if \"new_whale\" not in sample_classes:\n        sample_result.append((\"new_whale\", 0.1))\n    sample_result.sort(key=lambda x: x[1])\n    sample_result = sample_result[:5]\n    preds_str.append(\" \".join([x[0] for x in sample_result]))","execution_count":69,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f523cd3d2d3c9d032eac1f0f563fde115ee1df"},"cell_type":"code","source":"preds_str","execution_count":70,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"f826221d3ceb07b8991bb8cea9edcb1a3e59a434"},"cell_type":"code","source":"df = pd.DataFrame(preds_str, columns=[\"Id\"])\ndf['Image'] = [x.split(os.sep)[-1] for x in test_file_names]\ndf.to_csv(\"sub.csv\", index=False)","execution_count":73,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2e0d8a9a5565501f0d06a19b94ddb681c6758b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7905facd441a7d8a852b7e3e401a9549f7b02a4f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}