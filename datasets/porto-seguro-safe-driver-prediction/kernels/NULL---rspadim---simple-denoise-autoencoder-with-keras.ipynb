{"cells":[{"metadata":{"_uuid":"e172d32f4703cd778258e0882c4e80743cd10849"},"cell_type":"markdown","source":"From 1st place ideas, https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629\n\nSingle denoise autoencoder (http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf) with \"SwapNoise\""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras \nimport gc\nimport matplotlib.pyplot as plt\nfrom keras.utils import Sequence\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Input, Concatenate, Dropout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b68a7e87a36d068c01f9a7d980e077ed1d9728c1"},"cell_type":"markdown","source":"Reading Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Reading datasets')\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint('Merging test and train')\ntest['target'] = np.nan\ntrain = train.append(test).reset_index() # merge train and test\ndel test\nprint('Done, shape=',np.shape(train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6e9096d8c77a6fab690eee73e69a3b0ff692ad1"},"cell_type":"markdown","source":"Rank Gauss transformation"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29a7e53372569af14d1dfad6b49374e36010e4a2"},"cell_type":"code","source":"def rank_gauss(x, title=None):\n    #Trying to implement rankGauss in python, here are my steps\n    # 1) Get the index of the series\n    # 2) sort the series\n    # 3) standardize the series between -1 and 1\n    # 4) apply erfinv to the standardized series\n    # 5) create a new series using the index\n    # Am i missing something ??\n    # I subtract mean afterwards. And do not touch 1/0 (binary columns). \n    # The basic idea of this \"RankGauss\" was to apply rank trafo and them shape them like gaussians. \n    # Thats the basic idea. You can try your own variation of this.\n    \n    if(title!=None):\n        fig, axs = plt.subplots(3, 3)\n        fig.suptitle(title)\n        axs[0][0].hist(x)\n\n    from scipy.special import erfinv\n    N = x.shape[0]\n    temp = x.argsort()\n    if(title!=None):\n        print('1)', max(temp), min(temp))\n        axs[0][1].hist(temp)\n    rank_x = temp.argsort() / N\n    if(title!=None):\n        print('2)', max(rank_x), min(rank_x))\n        axs[0][2].hist(rank_x)\n    rank_x -= rank_x.mean()\n    if(title!=None): \n        print('3)', max(rank_x), min(rank_x))\n        axs[1][0].hist(rank_x)\n    rank_x *= 2\n    if(title!=None):\n        print('4)', max(rank_x), min(rank_x))\n        axs[1][1].hist(rank_x)\n    efi_x = erfinv(rank_x)\n    if(title!=None): \n        print('5)', max(efi_x), min(efi_x))\n        axs[1][2].hist(efi_x)\n    efi_x -= efi_x.mean()\n    if(title!=None):\n        print('6)', max(efi_x), min(efi_x))\n        axs[2][0].hist(efi_x)\n        plt.show()\n\n    return efi_x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87b76a47c82f018d52a134abd9f96fc9cacc2dd"},"cell_type":"markdown","source":"Categorical to RankGauss, Binary to -1/1"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"709daa5d14be87e2d0de842f636edea2ef259754","collapsed":true},"cell_type":"code","source":"for i in train.columns:\n    if i.endswith('cat'): # could be train[i].dtype == 'object' + labelencode, or maybe one hot encode...\n        print('Categorical: ',i)\n        train[i] = rank_gauss(train[i].values, i) # display rank gauss tranformation\n    elif i.endswith('bin'):\n        print('Binary: ',i)\n    else:\n        print('Numeric: ',i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed445f28edb67507e6a426977ba5cc97e2a281b3"},"cell_type":"markdown","source":"Read/Write Locker Help"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"69a79572e5df8d4fe7936ba89c369e540ed40144"},"cell_type":"code","source":"# i'm doing this cause i don't know if some keras backend have threading problems...\nimport threading\nclass ReadWriteLock:\n    def __init__(self):\n        self._read_ready = threading.Condition(threading.Lock())\n        self._readers = 0\n    def acquire_read(self):\n        self._read_ready.acquire()\n        try:\n            self._readers += 1\n        finally:\n            self._read_ready.release()\n    def release_read(self):\n        self._read_ready.acquire()\n        try:\n            self._readers -= 1\n            if not self._readers:\n                self._read_ready.notifyAll()\n        finally:\n            self._read_ready.release()\n    def acquire_write(self):\n        self._read_ready.acquire()\n        while self._readers > 0:\n            self._read_ready.wait()\n    def release_write(self):\n        self._read_ready.release()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ff8ef15e35a609eec68d99eb1a21ab9cdcf2de1"},"cell_type":"markdown","source":"DAE Generator"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1958ad44e654265864783cd990a3f690bccb0a78"},"cell_type":"code","source":"from math import ceil\nclass DAESequence(Sequence):\n    def __init__(self, df, batch_size=128, random_cols=.15, random_rows=1, use_cache=False, use_lock=False, verbose=True):\n        self.df = df.values.copy()     # ndarray baby\n        self.batch_size = int(batch_size)\n        self.len_data = df.shape[0]\n        self.len_input_columns = df.shape[1]\n        if(random_cols <= 0):\n            self.random_cols = 0\n        elif(random_cols >= 1):\n            self.random_cols = self.len_input_columns\n        else:\n            self.random_cols = int(random_cols*self.len_input_columns)\n        if(self.random_cols > self.len_input_columns):\n            self.random_cols = self.len_input_columns\n        self.random_rows = random_rows\n        self.cache = None\n        self.use_cache = use_cache\n        self.use_lock = use_lock\n        self.verbose = verbose\n        \n        self.lock = ReadWriteLock()\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        if(not self.use_cache):\n            return\n        if(self.use_lock):\n            self.lock.acquire_write()\n        if(self.verbose):\n            print(\"Doing Cache\")\n        self.cache = {}\n        for i in range(0, self.__len__()):\n            self.cache[i] = self.__getitem__(i, True)\n        if(self.use_lock):\n            self.lock.release_write()\n        gc.collect()\n        if(self.verbose):\n            print(\"Done\")\n\n    def __len__(self):\n        return int(ceil(self.len_data / float(self.batch_size)))\n\n    def __getitem__(self, idx, doing_cache=False):\n        if(not doing_cache and self.cache is not None and not (self.random_cols <=0 or self.random_rows<=0)):\n            if(idx in self.cache.keys()):\n                if(self.use_lock):\n                    self.lock.acquire_read()\n                ret0, ret1 = self.cache[idx][0], self.cache[idx][1]\n                if(self.use_lock):\n                    self.lock.release_read()\n                if (not doing_cache and self.verbose):\n                    print('DAESequence Cache ', idx)\n                return ret0, ret1\n        idx_end = min(idx + self.batch_size, self.len_data)\n        cur_len = idx_end - idx\n        rows_to_sample = int(self.random_rows * cur_len)\n        input_x = self.df[idx: idx_end]\n        if (self.random_cols <= 0 or self.random_rows <= 0 or rows_to_sample<=0):\n            return input_x, input_x # not dae\n        # here start the magic\n        random_rows = np.random.randint(low=0, high=self.len_data-rows_to_sample, size=rows_to_sample)\n        random_rows[random_rows>idx] += cur_len # just to don't select twice the current rows\n        cols_to_shuffle = np.random.randint(low=0, high=self.len_input_columns, size=self.random_cols)\n        noise_x = input_x.copy()\n        noise_x[0:rows_to_sample, cols_to_shuffle] = self.df[random_rows[:,None], cols_to_shuffle]\n        if(not doing_cache and self.verbose):\n            print('DAESequence ', idx)\n        return noise_x, input_x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58c818032fde555aaf1c8f93c2c21bb57618f48"},"cell_type":"markdown","source":"Creating Model and Fitting with multi gpu (not most performace, but 'works', there's a bottleneck with cpu->gpu mem copy)"},{"metadata":{"trusted":true,"_uuid":"5694b5b1c11488c53529d8b1556bd1066d346532","collapsed":true},"cell_type":"code","source":"print(\"Create Model\")\ndae_data = train[train.columns.drop(['id','target'])] # only get \"X\" vector\n\n# reduce data size, we are in kaggle =)\ndae_data = dae_data[0:1000]\n\nlen_input_columns, len_data = dae_data.shape[1], dae_data.shape[0]\nNUM_GPUS=1\n#kernel_initializer='Orthogonal'  # this one give non NaN more often than others \n\n# from https://kaggle2.blob.core.windows.net/forum-message-attachments/250927/8325/nn.cfg.log\n#L0: 221(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:222x1500  out(x3):1501x128 (0.00210051 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0672672|0.0672671|-4.74564e-05|0.0388202]\n#L1: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n#L2: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n#L3: 1500(in)-221 'l'linear  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x221  out(x3):222x128 (0.00144055 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258198|-1.80977e-05|0.0149005]\n\n# is uni:1 = uniform? what about sp:1 ?\n# std ~= sqrt(2 / (input+output))   (first layer)\n# std ~= sqrt(1 / (input+output))   (others layer)\nkernel_initializer_0=keras.initializers.RandomNormal(mean=-4.74564e-05, stddev=0.0388202, seed=None)   # sqrt(2/(221+1500)) = 0.0341 vs 0,0388\nkernel_initializer_1=keras.initializers.RandomNormal(mean=8.51905e-06, stddev=0.0148989, seed=None)    # sqrt(1/(1500+1500)) = 0.018 vs 0,014\nkernel_initializer_2=keras.initializers.RandomNormal(mean=8.51905e-06, stddev=0.0148989, seed=None)    # sqrt(1/(1500+1500)) = 0.018 vs 0,014\nkernel_initializer_3=keras.initializers.RandomNormal(mean=-1.80977e-05, stddev=0.0149005, seed=None)   # sqrt(1/(1500+221)) = 0.024 vs 0,014\n\nprint(\"Input len=\", len_input_columns, len_data)\nmodel_dae = Sequential()\nmodel_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden1', input_shape=(len_input_columns,), kernel_initializer=kernel_initializer_0))\nmodel_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden2', kernel_initializer=kernel_initializer_1))\nmodel_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden3', kernel_initializer=kernel_initializer_2))\nmodel_dae.add(Dense(units=len_input_columns, activation='linear', dtype='float32', name='Output', kernel_initializer=kernel_initializer_3))\nmodel_opt = keras.optimizers.SGD(lr=0.003, decay=1-0.995, momentum=0, nesterov=False) # decay -> Oscar Takeshita comment\n\ntry:\n    print('Loading model from file')\n    model_dae = keras.models.load_model('DAE.keras.model.h5')\nexcept Exception as e:\n    print(\"Can't load previous fitting parameters and model\", repr(e))\nif(NUM_GPUS>1):\n    try:\n        multi_gpu_model = keras.utils.multi_gpu_model(model_dae, gpus=NUM_GPUS)\n        multi_gpu_model.compile(loss='mean_squared_error', optimizer=model_opt)\n        print(\"MULTI GPU MODEL\")\n        print(multi_gpu_model.summary())\n    except Exception as e:\n        print(\"Can't run multi gpu, error=\", repr(e))\n        model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n        NUM_GPUS=0\nelse:\n    model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n\nprint(\"BASE MODEL\")\nprint(model_dae.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4e7f7c5768759b6dff301ea239fe4a235583d0b"},"cell_type":"markdown","source":"Fitting model with data"},{"metadata":{"trusted":true,"_uuid":"5286f08ee919696c00f529398cb8bc7f0c0bd6c0","scrolled":true,"collapsed":true},"cell_type":"code","source":"from math import ceil\nbatch_size = 128\nmulti_process_workers = 2\nif (NUM_GPUS > 1):\n    multi_gpu_model.fit_generator(\n        DAESequence(dae_data, batch_size=batch_size*NUM_GPUS, verbose=False),\n        steps_per_epoch=int(ceil(dae_data.shape[0]/(batch_size*NUM_GPUS))),\n        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n        epochs=1000,\n        verbose=1,\n        callbacks=[\n            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights \n        ])\nelse: # single CPU/GPU\n    model_dae.fit_generator(\n        DAESequence(dae_data, batch_size=batch_size, verbose=False),\n        steps_per_epoch=int(ceil(dae_data.shape[0]/batch_size)),\n        epochs=1000,\n        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n        verbose=1, callbacks=[\n            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights\n        ])\n    \n#model_dae.save('DAE.keras.model.h5') # save weights\nplt.hist(model_dae.get_weights(), bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa6805483f0c52889fa2f23dcf155432accaeb4b"},"cell_type":"markdown","source":"Predict from data and we are done"},{"metadata":{"trusted":true,"_uuid":"c26670cff92ce5842a6decd66db5375b33544019","collapsed":true},"cell_type":"code","source":"# here we can measure the error from input -> output, it's not what we want, right?\nfrom sklearn.metrics import mean_squared_error\nif (NUM_GPUS > 1):\n    dae_denoised_data = multi_gpu_model.predict(dae_data)\nelse:\n    dae_denoised_data = model_dae.predict(dae_data)\n\nprint(\"DAE MSE from train data: \", mean_squared_error(dae_data, dae_denoised_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0c2df22825f3ce018f80e6e38b4fbbdd4cd4d5c"},"cell_type":"markdown","source":"Thanks :) and good studies!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c8fb4af090294f014d7b0957f4644587da5c91d1"},"cell_type":"markdown","source":"Ops =) you want the hidden layer variables right? :)"},{"metadata":{"trusted":true,"_uuid":"028ce9a64282cd35fbc904878348f33db8a13f11","collapsed":true},"cell_type":"code","source":"#after you have DAE fitted...\nyour_new_df=train[['id','target']].copy()\n# let's cut it again...\n# reduce data size, we are in kaggle =)\nyour_new_df = your_new_df[0:1000]\n\nfor i in ['1','2','3']:\n    print('Hidden layer',i)\n    columns_names = ['Hidden_'+str(i)+'_'+str(l) for l in range(0, len_input_columns*10)]\n    for l in columns_names:\n        your_new_df[l] = 0 # create columns (maybe it's not optimized)\n    intermediate_layer_model = Model(inputs=model_dae.input, outputs=model_dae.get_layer('Hidden' + i).output)\n    your_new_df[columns_names] = intermediate_layer_model.predict(dae_data)\n\nprint('DONE!')\nprint(your_new_df)\n#your_new_df.to_pickle(\"DAE-hidden-features.pickle\")\n\n# now =) use it and win :P","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f7e87e87651d6e4eab8e9a0f182a3cd58fba4522"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}