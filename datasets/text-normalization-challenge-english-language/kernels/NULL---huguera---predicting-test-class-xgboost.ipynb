{"nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.6.1", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "cells": [{"source": ["# Labeling Test set - Machine Learning approach\n", "\n", "In order to label the test set, this kernel uses XGBoost to predit its 'class'.\n", "\n", "The entire process is done using a sample of the data. Feel free to modify and improve it!"], "cell_type": "markdown", "metadata": {"_cell_guid": "8d797194-37f5-470e-a57d-0fb34360ff08", "_uuid": "18473364c9386a68167ebb92f055ead7fe21a12b"}}, {"source": ["### Libraries"], "cell_type": "markdown", "metadata": {"_cell_guid": "29395dae-faaf-4515-bd0f-7f3cdfc38419", "_uuid": "f88f613cba47afb551f46c2f74d75aba1b5d8108"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "ce414e3e-6e72-4361-97b8-2c589f3afc91", "_uuid": "0a8ce979d9a7d9eaa94380dcce05fd58b52b3542"}, "source": ["import pandas as pd\n", "import numpy as np\n", "seed = 42\n", "np.random.seed(seed)\n", "\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.datasets import dump_svmlight_file,load_svmlight_file\n", "from xgboost.sklearn import XGBClassifier\n", "from sklearn.model_selection import train_test_split"]}, {"source": ["### Methods"], "cell_type": "markdown", "metadata": {"_cell_guid": "47d48e94-5a3b-427a-b16e-e4c54cfec620", "_uuid": "13de2f602e73a33db2220d4ef7f74b397ee7f57b"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "a37471ea-6668-4471-b922-ab4778e4fa39", "_uuid": "c40fbae8904299ba8f211d3866004c7dc275da4a"}, "source": ["def instantiate_xgb(rounds=100, lr = 0.1, depth=3, sub=1, col=1, seed=seed) :\n", "    clf = XGBClassifier( learning_rate=           lr,\n", "                         n_estimators=           rounds,\n", "                         max_depth=              depth,\n", "                         subsample=              sub,\n", "                         colsample_bytree=       col,\n", "                         objective=              'multi:softmax',\n", "                         seed=                   seed)\n", "                     \n", "    return clf\n", "\n", "def get_count_vectorizer(key, train, test, components=25, iters=25):\n", "    print ('Count Vectorizer: {}'.format(key))\n", "    count = CountVectorizer(analyzer=u'char', ngram_range=(1, 3)).fit(train[key].apply(str))\n", "    X_train = count.transform(train[key].apply(str))\n", "    X_test = count.transform(test[key].apply(str))\n", "    \n", "    print ('TruncatedSVD: {}'.format(key))\n", "    svd = TruncatedSVD(n_components=components, n_iter=iters, random_state=seed\n", "                                     ).fit(X_train)\n", "    \n", "    X_train = svd.transform(X_train)\n", "    X_test = svd.transform(X_test)\n", "    \n", "    print ('Shapes: {}\\t{}'.format(X_train.shape, X_test.shape))\n", "    return X_train, X_test\n", "\n", "def get_tfidf(key, train, test, components=25, iters=25):\n", "    print ('TFIDF: {}'.format(key))\n", "    tfidf = TfidfVectorizer(\n", "        min_df=5, strip_accents='unicode', lowercase =True,\n", "        analyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n", "        smooth_idf=True, sublinear_tf=True, stop_words = 'english'\n", "        ).fit(train[key].apply(str))\n", "\n", "    X_train = tfidf.transform(train[key].apply(str))\n", "    X_test = tfidf.transform(test[key].apply(str))\n", "    \n", "    print ('TruncatedSVD: {}'.format(key))\n", "    svd = TruncatedSVD(n_components=25, n_iter=25, random_state=seed).fit(X_train)\n", "    X_train = svd.transform(X_train)\n", "    X_test = svd.transform(X_test)\n", "    \n", "    print ('Shapes: {}\\t{}'.format(X_train.shape, X_test.shape))\n", "    return X_train, X_test"]}, {"source": ["### Reading and processing data"], "cell_type": "markdown", "metadata": {"_cell_guid": "d3e4b214-109a-4dec-a261-183365b18884", "_uuid": "d898c5bee010dd7555f170624cd1b9224ebd4d6c"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "7b9c7b9a-35d8-4be8-85e3-379e8cd8a2ef", "_uuid": "9a587c75306e877f89c0f1a5e7f3a4595a8efff9"}, "source": ["x_train = pd.read_csv('../input/en_train.csv', nrows=100000)\n", "x_test = pd.read_csv('../input/en_test.csv')"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "07d7f2ac-c65d-49cf-b686-ccbf7af3cd1a", "_uuid": "5cd49b4e793f0d27e381229b8289f1134eca6999"}, "source": ["y_train = pd.factorize(x_train['class'])\n", "x_train = x_train.drop(['class'], axis=1)\n", "key = 'before'"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "ac75c9c2-80fc-4746-b5cf-e53200033ae1", "_uuid": "7d04a006dcdd1615734b5182368b562dcdc1b72e"}, "source": ["train_cnt, test_cnt = get_count_vectorizer(key, x_train, x_test)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "541f8fe3-ac97-4c83-b543-3ce92d18cb74", "_uuid": "2ff5b077dd404f14807ea30bd07190818c489b15"}, "source": ["train_tfidf, test_tfidf = get_tfidf(key, x_train, x_test)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "0e817592-db6a-44d3-a9e3-7242e6c046fe", "_uuid": "1c018b60aa846b35d163ea7ef3def325a206161f"}, "source": ["x_train[key+'_len'] = x_train[key].map(lambda x: len(str(x)))\n", "x_test[key+'_len'] = x_test[key].map(lambda x: len(str(x)))\n", "\n", "x_train['sentence_length'] = x_train.groupby(['sentence_id'])['sentence_id'].transform(np.size)\n", "x_test['sentence_length'] = x_test.groupby(['sentence_id'])['sentence_id'].transform(np.size)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "2e0507bd-b67f-4b5d-be21-946bfd07a6f7", "_uuid": "c52877ae603fbf7e643c98b2348a826cf12466da"}, "source": ["x_train = np.concatenate((x_train.drop(['before', 'after','sentence_id'],axis=1), \n", "                          train_cnt,\n", "                          train_tfidf), axis=1)\n", "x_test = np.concatenate((x_test.drop(['before', 'sentence_id'], axis=1),\n", "                         test_cnt,\n", "                         test_tfidf), axis=1)\n", "\n", "print ('Shapes: {}\\t{}'.format(x_train.shape, x_test.shape))\n", "del train_cnt, train_tfidf, test_cnt, test_tfidf"]}, {"source": ["### Start training"], "cell_type": "markdown", "metadata": {"_cell_guid": "9e6ddd6f-9f2f-43c7-b632-cdf506a78ce0", "_uuid": "da1767ab8d7916e3b41909ce4a42769199a49235"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "751d6591-5dde-4fd4-8d4b-a1049450cb1c", "_uuid": "d2a9f373df6cea87fb950c38bfee08de83b5ed47"}, "source": ["X_train, X_valid, Y_train, Y_valid = train_test_split(\n", "    x_train, y_train[0], test_size=0.3, random_state=seed)\n", "\n", "clf = instantiate_xgb(rounds=100, seed=seed)\n", "\n", "clf.fit(X_train, Y_train, \n", "        eval_set=[(X_valid, Y_valid)],\n", "        eval_metric='merror',\n", "        early_stopping_rounds=30,\n", "        verbose=10)"]}, {"source": ["### Predicting and writing to csv"], "cell_type": "markdown", "metadata": {"_cell_guid": "75074877-8cfd-4146-a195-67febd74ef55", "_uuid": "6d0b275563c92ee6a3f53fbd99feffb49d0bff08"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "c4cdf808-bc6f-4fe3-b4a1-7faac897f2b5", "_uuid": "c21c9d18403b7d2ce0f2c8b5b460c24340d23536"}, "source": ["preds = clf.predict(x_test)\n", "y = pd.Series(preds).apply(lambda x: y_train[1][x])\n", "y.to_csv('./test_class_preds.csv', index=False)"]}, {"source": ["## Visualizing predictions on validation set"], "cell_type": "markdown", "metadata": {"_cell_guid": "e6360e74-2cd6-4e75-ba19-dcfdd5340b5d", "_uuid": "9ecbc9ea06cd83fe3bfeeefca9b18af63396ace0"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "f61cc4e7-4366-42e4-8be5-c6c9d3778816", "_uuid": "5fceba0f1e46172150b7e195e09356f87208ae8d"}, "source": ["df = pd.DataFrame()\n", "df['valid_class'] = pd.Series(Y_valid).apply(lambda x: y_train[1][x])\n", "df['valid_preds'] = pd.Series(clf.predict(X_valid)).apply(lambda x: y_train[1][x])\n", "\n", "print(df.head(30))"]}, {"source": ["XGBoost showed itself a very good option predicting the 'class' attribute.\n", "\n", "Futher feature engineering still can be done and I plan to improve this kernel in the next versions.\n", "\n", "#### To be  continued..."], "cell_type": "markdown", "metadata": {"_cell_guid": "f7d8f510-72a1-40d7-bd1c-05f2410df497", "_uuid": "4774abb4feef00b6868cb14fbd4c64a5d33f7bbe"}}]}