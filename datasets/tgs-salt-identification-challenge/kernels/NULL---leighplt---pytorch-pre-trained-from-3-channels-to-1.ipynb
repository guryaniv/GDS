{"cells":[{"metadata":{"_uuid":"d60ed94a7be1554705049ec2a7f39a412540f23e"},"cell_type":"markdown","source":"Image from dataset are grayscele, but pre-trained model is for rgb image. So chenge it."},{"metadata":{"trusted":true,"_uuid":"37c5c102aef7a1d9c387e2ef0fd23ac21c52b7cf"},"cell_type":"code","source":"# Loading libs\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nimport torchvision\nfrom torchvision import models\n\nimport cv2\nfrom pathlib import Path\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa2744ad750399793239de287274941e01b8ad8"},"cell_type":"code","source":"# Up TensorFlow, so driver up too - > torch can upgrade too\ntorch.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Model from:\n# https://github.com/ternaus/TernausNet/blob/master/unet_models.py\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\nclass UNet11(nn.Module):\n    def __init__(self, num_filters=32):\n        \"\"\"\n        :param num_classes:\n        :param num_filters:\n        \"\"\"\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Convolutions are from VGG11\n        self.encoder = models.vgg11().features\n        \n        # \"relu\" layer is taken from VGG probably for generality, but it's not clear \n        self.relu = self.encoder[1]\n        \n        self.conv1 = self.encoder[0]\n        self.conv2 = self.encoder[3]\n        self.conv3s = self.encoder[6]\n        self.conv3 = self.encoder[8]\n        self.conv4s = self.encoder[11]\n        self.conv4 = self.encoder[13]\n        self.conv5s = self.encoder[16]\n        self.conv5 = self.encoder[18]\n\n        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n        \n        self.final = nn.Conv2d(num_filters, 1, kernel_size=1, )\n\n    def forward(self, x):\n        conv1 = self.relu(self.conv1(x))\n        conv2 = self.relu(self.conv2(self.pool(conv1)))\n        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n        conv3 = self.relu(self.conv3(conv3s))\n        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n        conv4 = self.relu(self.conv4(conv4s))\n        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n        conv5 = self.relu(self.conv5(conv5s))\n\n        center = self.center(self.pool(conv5))\n\n        # Deconvolutions with copies of VGG11 layers of corresponding size \n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n        return torch.sigmoid(self.final(dec1))\n\ndef unet11(**kwargs):\n    model = UNet11(**kwargs)\n    return model\n\ndef get_model():\n    np.random.seed(717)\n    torch.cuda.manual_seed(717);\n    torch.manual_seed(717);\n    model = unet11()\n    model.train()\n    return model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac908ed05b1c89c6b4c7ecc43389bffca5af7d1b"},"cell_type":"code","source":"directory = '../input'\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7114b9f3da03d4688ecfdecd7c7008a0be0c8004"},"cell_type":"code","source":"def load_image(path, mask = False):\n    \"\"\"\n    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n    \n    if pad = True:\n        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n    else:\n        returns image as numpy.array\n    \"\"\"\n    img = cv2.imread(str(path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    height, width, _ = img.shape\n\n    # Padding in needed for UNet models because they need image size to be divisible by 32 \n    if height % 32 == 0:\n        y_min_pad = 0\n        y_max_pad = 0\n    else:\n        y_pad = 32 - height % 32\n        y_min_pad = int(y_pad / 2)\n        y_max_pad = y_pad - y_min_pad\n        \n    if width % 32 == 0:\n        x_min_pad = 0\n        x_max_pad = 0\n    else:\n        x_pad = 32 - width % 32\n        x_min_pad = int(x_pad / 2)\n        x_max_pad = x_pad - x_min_pad\n    \n    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n    if mask:\n        # Convert mask to 0 and 1 format\n        img = img[:, :, 0:1] // 255\n        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))\n    else:\n        img = img / 255.0\n        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e0c6c34c6916e43b8f4e8e1f6eb708f8049b3d"},"cell_type":"code","source":"class TGSSaltDataset(data.Dataset):\n    def __init__(self, root_path, file_list, is_test = False):\n        self.is_test = is_test\n        self.root_path = root_path\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        file_id = self.file_list[index]\n        \n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        \n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        \n        image = load_image(image_path)\n        \n        if self.is_test:\n            return (image,)\n        else:\n            mask = load_image(mask_path, mask = True)\n            return image, mask\n\ndepths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n\ntrain_path = os.path.join(directory, 'train')\nfile_list = list(depths_df['id'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c552307d0a8b080017ec4fd34294aa29d66293d4"},"cell_type":"code","source":"torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82e4b9d3fb425042aadd47c926a47199d271023"},"cell_type":"code","source":"file_list_val = file_list[::10]\nfile_list_train = [f for f in file_list if f not in file_list_val]\ndataset = TGSSaltDataset(train_path, file_list_train)\ndataset_val = TGSSaltDataset(train_path, file_list_val)\n\nmodel = get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"767dae95adfbe7fefcb4cc3dd397ea8ba0e3f8d1","scrolled":false},"cell_type":"code","source":"%%time\nepoch = 3\nlearning_rate = 1e-4\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nfor e in range(epoch):\n    train_loss = []\n    for image, mask in data.DataLoader(dataset, batch_size = 30, shuffle = True):\n        image = image.type(torch.FloatTensor).to(device)\n        y_pred = model(image)\n        loss = loss_fn(y_pred, mask.to(device))\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    val_loss = []\n    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n        image = image.cuda()\n        y_pred = model(image)\n\n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print(\"Epoch: %d, Train: %.5f, Val: %.5f\" % (e, np.mean(train_loss), np.mean(val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c153204224a96dfc312a870d8326d46f6a21127f"},"cell_type":"markdown","source":"## Convert model to 1 channel input image\nRepeat again with converting model to 1 channel input image. Data from set all grayscale, so rgb channels are equal."},{"metadata":{"trusted":true,"_uuid":"f99d7d9abca91e1cda5a3007a9edcfee736a153b"},"cell_type":"code","source":"def count_params(model):\n    \"\"\"Count the number of parameters\"\"\"\n    param_count = np.sum([torch.numel(p) for p in model.parameters()])\n    return param_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d3821a929d1b54ad69297ac7d7cb2f863a16c2"},"cell_type":"code","source":"print('Total parametrs before squeeze: ',count_params(model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a40b1cad11b6e007a04c4ebb450215ac020a5e05"},"cell_type":"code","source":"features_3ch = model.encoder[0](image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d2f683a7924fe7eb6bd2b09768fab111e841e7a"},"cell_type":"code","source":"def squeeze_weights(m):\n        m.weight.data = m.weight.data.sum(dim=1)[:,None]\n        m.in_channels = 1\nmodel.encoder[0].apply(squeeze_weights);\nprint('Total parametrs after squeeze: ',count_params(model))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6d881a053a953e7a60f092c87bfb320f8e16a3b"},"cell_type":"code","source":"features_1ch = model.encoder[0](image[:,0][:,None])\n(features_1ch-features_3ch).sum().item()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8adc9511b288c853e878d446b0d4ebc9d964c289"},"cell_type":"markdown","source":"Difference  very small."},{"metadata":{"_uuid":"e23e7b7bfe1b82a59d373e1c2f2b1f5637305a13"},"cell_type":"markdown","source":"## Traing again new model with squeeze params"},{"metadata":{"trusted":true,"_uuid":"77be320abd2731e781591c466669e7cfe88740ec"},"cell_type":"code","source":"model2 = get_model()\nmodel2.encoder[0].apply(squeeze_weights);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"241c10feb3a963598bab530afc865cf03227c141"},"cell_type":"code","source":"%%time\nepoch = 3\nlearning_rate = 1e-4\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\nfor e in range(epoch):\n    train_loss = []\n    for image, mask in data.DataLoader(dataset, batch_size = 30, shuffle = True):\n        optimizer.zero_grad()\n        image = image[:,0][:,None].type(torch.FloatTensor).to(device) # select only 1 channel (all channel equal)\n        y_pred = model2(image)\n        loss = loss_fn(y_pred, mask.to(device))\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    val_loss = []\n    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n        image = image[:,0][:,None].type(torch.FloatTensor).to(device) # select only 1 channel (all channel equal)\n        y_pred = model2(image)\n\n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print(\"Epoch: %d, Train: %.5f, Val: %.5f\" % (e, np.mean(train_loss), np.mean(val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e8525cc39bac0184321a69e632c3e0218f135b"},"cell_type":"markdown","source":"As we can see results are similar. But we have less parametrs (but really small change)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}