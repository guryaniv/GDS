{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc921041-64d4-9ec9-1345-e6133aa30b6c"
      },
      "source": [
        "This is a follow up on my previous kernel about the structure in missing values and whether one can make use of it. You can find the previous kernel [here](https://www.kaggle.com/cgump3rt/two-sigma-financial-modeling/investigate-missing-values). The short summary from the training data is:\n",
        "\n",
        "* fraction of missing data can is significant for many feature columns,\n",
        "* NaNs are **not** distributed randomly per feature but span a continuous range in time\n",
        "* all NaNs for an asset ID appear at the beginning of the period during which this asset is part of the portfolio.\n",
        "\n",
        "I saw that there are already quite a few interesting kernels by other people who also tried to cluster assets based on the NaN structure. For example have a look at the following kernels if you are interested (list non-exhaustive)\n",
        "\n",
        "* by [jrisk123](https://www.kaggle.com/jriskas/two-sigma-financial-modeling/asset-cluster-visualization)\n",
        "* by [lesibius](https://www.kaggle.com/lesibius/two-sigma-financial-modeling/financial-instrument-types)\n",
        "* by [chbimo](https://www.kaggle.com/chbimo/two-sigma-financial-modeling/correlation-of-nan-structure-between-id)\n",
        "* by [Allunia](https://www.kaggle.com/allunia/two-sigma-financial-modeling/feature-dynamics-looking-at-id-groups)\n",
        "* by [reziproke](https://www.kaggle.com/reziproke/two-sigma-financial-modeling/nan-structure-on-id-level-eda)\n",
        "* (sorry if I missed you, add comment and I will update the list...)\n",
        "\n",
        "Before giving the clusterization yet another try, something came to my mind when I was skiing last week. What if data is published in fixed intervals (e.g. every working day at 9am, or Mondays, or the first Tuesday of the month)? In this scenario, the NaN structure does not necessarily characterize different asset categories, but also depends when an asset was bought relative to the next _publication date_. Imagine data were to be published always on Mondays. Then the number of missing NaN values would probably tell you which weekday you bough this asset.\n",
        "\n",
        "Long story short: let's see whether we can find some pattern in the total number of NaN values per timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c3d8410-fe81-b9a4-f707-225db94e02f9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2781947c-849a-a3e7-2c16-3d2753ba9844"
      },
      "outputs": [],
      "source": [
        "with pd.HDFStore('../input/train.h5') as train:\n",
        "    df = train.get('train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e0fb0074-eb2c-f084-612a-84c144eadc6a"
      },
      "source": [
        "As a first step, we get a new data frame containing the total number of NaN values for each column and timestamp. Since we are not that much interested in the total numbers, but rather in how they change, we take the difference between two subsequent timestamps. Finally, we make a plot showing the evolution of the total number of NaNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72111de2-7c2e-ef16-7c22-bae445b0e6c5"
      },
      "outputs": [],
      "source": [
        "# get total number of NaN values per feature column for every timestamp\n",
        "nans = df.groupby('timestamp').apply(lambda x: x.isnull().sum())\n",
        "# get change with respect \n",
        "nans = nans.diff().drop(['id','timestamp','y'],axis=1)\n",
        "nans.plot(figsize=(13,10))\n",
        "plt.legend(ncol=5,loc='best');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80e92484-e4af-b1d0-4485-c17bb77aa944"
      },
      "source": [
        "Voila, at a first glance, it looks that data becomes available in regular intervals. It is interesting to see that there huge spikes in the beginning while later on, the magnitude becomes much smaller. But please not that these are absolute numbers. So the total number of assets in the portfolio is important. My guess is that we start with a large number of assets at timestamp 0. Therefore, we have a lot of NaNs which get filled over time. Over the training period, only comparably small chunks of assets are added leading to a smaller number of NaNs which gets filled later on.  \n",
        "There are also few timestamps at which the number of NaNs in the dataset increases. My suspicion is that these peaks occur at times when new assets are bought. So let's check this quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "973553e5-9752-ae09-3866-e855e698c1f1"
      },
      "outputs": [],
      "source": [
        "# get the number of assets per timestamp\n",
        "nassets = df.groupby('timestamp').apply(len)\n",
        "# get the change with respect to the previous timestamp\n",
        "delta_assets = nassets.diff()\n",
        "\n",
        "# plot again\n",
        "nans.plot(figsize=(13,10))\n",
        "delta_assets.plot(style=['.b'],ax=plt.gca(),label='number of assets')\n",
        "plt.legend(ncol=5,loc='best');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bf9ce5f-2bf3-85a1-de29-453ffa1364e0"
      },
      "source": [
        "Ok, so increases in the number of NaNs are explained by new assets being bought. Just out of curiosity I made a plot summing the number of NaNs for the different groups of feature columns (_fundamental_, _derived_ and _technical_). This time I divide by the number of assets present and also by the number of feature columns in each group. Therefore, the numbers can be interpreted as average probability that a fundamental/derived/technical feature column has a NaN at each timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfff54b0-0ed4-e2ad-3dca-92c87308f427"
      },
      "outputs": [],
      "source": [
        "# get number of columns for each group\n",
        "n_fundamental = len([g for g in nans.columns.tolist() if re.search(\"fundamental_*\",g)])\n",
        "n_derived = len([g for g in nans.columns.tolist() if re.search(\"derived_*\",g)])\n",
        "n_technical = len([g for g in nans.columns.tolist() if re.search(\"technical_*\",g)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "74e23a67-1ac5-0eac-aeca-eb4f3dbb6be0"
      },
      "outputs": [],
      "source": [
        "nans['fundamental'] = nans.filter(regex='fundamental_*').sum(axis=1)/nassets/n_fundamental\n",
        "nans['derived'] = nans.filter(regex='derived_*').sum(axis=1)/nassets/n_derived\n",
        "nans['technical'] = nans.filter(regex='technical_*').sum(axis=1)/nassets/n_technical\n",
        "nans[['fundamental','derived','technical']].plot(figsize=(12,6));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "85379307-6dfa-3ce2-8d56-c76476f9582d"
      },
      "source": [
        "In this graph one can observe a pattern which was already present in the other plot above, but harder to see. While information updates seem to be published regurlarly, one can still observe a pattern which correlates with the dates at which assets are bought. It looks like every time new assets are bought (introducing more NaNs), a few timestamps later this is reverted. We can try to find the _lag_ of this anti-correlation to get a feeling for the number of NaNs we can expect for each asset in the beginning.  \n",
        "Just to be clear on what the next code is doing (I know it's easy to get lost ;-)):\n",
        "* We have the change in the number of assets between to timestamps (`delta_assets`).\n",
        "* We have the change in the total number of NaNs for each feature column and timestamp (`nans`).  \n",
        "\n",
        "I am going to shift `delta_assets` for different lag values and calculate the correlation for each feature column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9a2fe88-f6ba-f98d-5cee-2880c66c12a2"
      },
      "outputs": [],
      "source": [
        "max_lag = 100\n",
        "corrs = np.zeros((max_lag,nans.shape[1]))\n",
        "for l in range(1,max_lag):\n",
        "    c = delta_assets.shift(l)\n",
        "    corrs[l] = nans.corrwith(c)\n",
        "\n",
        "plt.figure(figsize=(15,9))\n",
        "sns.heatmap(corrs,xticklabels=nans.columns.tolist(),linewidths=0.05,linecolor='gray')\n",
        "plt.ylabel('lag');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d1610c2a-05f1-16e5-77fe-0edc316f0db4"
      },
      "source": [
        "So, let's interpret the plot above. Derived and fundamental features (if not present from the beginning) become available after 3 timestamps as do most of the technical features as well. Some technical features, however, take much longer than 3 time stamps. For example, technical_1 seems to become available after 15 timestamps. It's an anti-correlation because the number of assets goes up and few days later the number of NaNs goes down again. Even though, I am not sure how useful this it, I found it interesting how one can easily learn something about the _nature_ of the different technical features (which, of course, is kind of expected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0b0a54d-8106-8301-94d9-6a3e83d3024b"
      },
      "source": [
        "Turning to clusterization. The first question is, of course, what kind of data we should use as features during the clusterization. Most attempts I have seen so far were using the fraction of timestamps for which a given feature column was not filled with data. In my opinion, this is non-optimal choice for the following reason:\n",
        "1. I expected the NaN patterns are intrinsically related to type of the asset.\n",
        "1. They are independent of the time the asset is kept in the portfolio.\n",
        "1. They are driven by external or technical constraints (e.g. publication of information at given dates, technical indicators requiring a decent amount of history etc).\n",
        "\n",
        "Especially point 2) makes the usage of relative fractions sub-optimal (it might be okey if assets are kept for ~the same time). Using the absolute number of timestamps for which a given feature is not available is not optimal either. If an asset is kept for 1000 timestamps, is it a real difference if the data is not available for the first 3 or first 5 days? My suggestion is kind of hybrid solution: I will only consider the first 100 timestamps for each asset and take the relative fraction of non-availability in these first 100 timestamps (which of course is only acting like a global scaling factor). The underlying assumption is here that the NaN structure during the first 100 timestamps provides sufficient information for distinguishing different asset categories. I have to admit that the choice of 100 is completely random at this point. So here is how it looks in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02e84545-411d-ffae-32e4-71951248b8e9"
      },
      "outputs": [],
      "source": [
        "# group by asset ID and get number of missing values for each feature column\n",
        "ids = sorted(df.id.unique())\n",
        "columns = df.columns.drop(['id','timestamp','y']).insert(0,'length')\n",
        "nan_df = pd.DataFrame(data=None,index=ids,columns=columns,dtype=float)\n",
        "# iterate over all asset ID\n",
        "for name,group in df.groupby('id'):\n",
        "    # for every feature column\n",
        "    for c in columns:\n",
        "        if c == 'length':\n",
        "            nan_df.loc[name,c] = int(len(group))\n",
        "        else:\n",
        "            # total number of rows with missing data\n",
        "            nan_df.loc[name,c] = float(group[c].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "473e2072-1769-af36-ef30-4597da2d374e"
      },
      "outputs": [],
      "source": [
        "nan_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "84eb91bd-bcab-94bd-d870-1cce6c2f8f7e"
      },
      "outputs": [],
      "source": [
        "# truncate all numbers at 100\n",
        "capped = nan_df.copy()\n",
        "capped[capped > 100] = 100\n",
        "capped = capped.div(capped['length'],axis='index').drop(['length'],axis=1)\n",
        "capped.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0d5e683-119a-1168-f860-61a61c90ae4e"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import dbscan\n",
        "_,labels = dbscan(capped.values)\n",
        "plt.hist(labels,bins=25,range=(-0.5,24.5));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "29e62cf4-fd19-922c-4e66-f33b8ace4f4d"
      },
      "source": [
        "At the first attempt it looks like there are three quite prominent categories while the others are probably all outliers. I think this is roughly inline with the results reported by others that the number of clusters should be around 5. One should make a more thorough test of the clusterisation result, also playing with the parameters of DBScan. If you want to have a go, just fork the notebook. The next step would then be to see whether assets of different clusters actually behave differently (e.g. different correlation between features and target values etc)."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}