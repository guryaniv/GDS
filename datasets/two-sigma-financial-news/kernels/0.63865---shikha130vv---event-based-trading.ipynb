{"cells":[{"metadata":{"_uuid":"225708f447eee93041881f9d6c3a3e890cb16718"},"cell_type":"markdown","source":"## In-depth Introduction\nFirst let's import the module and create an environment."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c72280eaad31bc4f43854541967d7f4d958ff89c"},"cell_type":"code","source":"import pandas as pd\nfrom ast import literal_eval\nimport sklearn.model_selection as skm\nimport lightgbm as lgb\nimport numpy as np\nimport itertools as itr\n\nimport plotly.offline as pyo\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\n\nimport matplotlib.pyplot as plt\n\ninit_notebook_mode(connected=True)\n(market_train_df, news_train_df) = env.get_training_data()\nsampling = False\nSERIES_LEN = 31\norg_cols = market_train_df.columns.values\norg_market_train_df = market_train_df\norg_news_train_df = news_train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6034b46fce8c9d55d403de32e7cebe8cb9fef96d"},"cell_type":"markdown","source":"## **`get_training_data`** function\n\nReturns the training data DataFrames as a tuple of:\n* `market_train_df`: DataFrame with market training data\n* `news_train_df`: DataFrame with news training data\n\nThese DataFrames contain all market and news data from February 2007 to December 2016.  See the [competition's Data tab](https://www.kaggle.com/c/two-sigma-financial-news/data) for more information on what columns are included in each DataFrame."},{"metadata":{"trusted":true,"_uuid":"40b85d3b94091ecaf267f4f63cfdc575b206fc8b"},"cell_type":"code","source":"def add_missing_rows(in_df):\n    print(\"-----------------------------------\")\n    print(\"Rows before adding missing values:\", in_df.shape[0])\n    asset_code_list = list(in_df[\"assetCode\"].unique())\n    time_list = list(in_df[\"time\"].unique())\n    dic_asseCode_sno = {k: v for v, k in enumerate(asset_code_list)}\n    expected_data = list(itr.product(asset_code_list, time_list))\n    expected_df = pd.DataFrame(expected_data, columns=[\"assetCode\",\"expectedTime\"])\n    expected_df[\"assetCodeT\"] = expected_df[\"assetCode\"].map(dic_asseCode_sno)\n    in_df[\"assetCodeT\"] = in_df[\"assetCode\"].map(dic_asseCode_sno)\n    expected_df = expected_df.sort_values([\"expectedTime\"])\n    expected_df = expected_df.drop([\"assetCode\"], axis=1)\n    in_df = in_df.sort_values([\"time\"])\n    merged_df = pd.merge_asof(expected_df, in_df, left_on=[\"expectedTime\"], right_on=[\"time\"], direction=\"nearest\", by=\"assetCodeT\", tolerance=pd.Timedelta(str(SERIES_LEN) + \" days\"))\n    missing_rows= merged_df[merged_df[\"time\"] != merged_df[\"expectedTime\"]]\n    generated_rows = missing_rows[~missing_rows[\"time\"].isna()]\n    print(missing_rows.shape[0], generated_rows.shape[0])\n    merged_df[\"time\"] = merged_df[\"expectedTime\"]\n    print(\"Rows after adding missing values:\", merged_df.shape[0])\n    return merged_df[org_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"551f8188e7a9e95882e511968032702ea2db56b2"},"cell_type":"code","source":"#Handle null values\ndef remove_rows_with_nulls(in_df):\n    print(\"-----------------------------------\")\n    in_df_cols = in_df.columns.values\n    print(\"Total Rows before removing nulls: \", in_df.shape[0])\n    for col in in_df_cols:\n        df_col = in_df[[col]].values\n        num_na = df_col[pd.isna(df_col)].shape[0]\n        #print(\"Col: {0} Num NA: {1}\".format(col, num_na))\n    in_df = in_df.dropna(axis=0)\n    print(\"Total Rows after removing nulls: \", in_df.shape[0])\n    return in_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3ca81618f77a1b47ea40d82af140e30f85eca3c"},"cell_type":"code","source":" def remove_assets_with_missing_rows(in_df):  \n    print(\"-----------------------------------\")\n    time_list = list(in_df[\"time\"].unique())\n    dic_time_sno = {k: v for v, k in enumerate(time_list)}\n    print(\"Total Rows before asset with missing rows: \", in_df.shape[0])\n\n    grouped_df = in_df[[\"time\",\"assetCode\"]].groupby([\"assetCode\"],as_index=False).agg({\"time\":{\"min_time\":\"min\", \"max_time\":\"max\",\"num_rows\":\"count\"}})\n    grouped_df.columns = [\"assetCode\",\"min_time\",\"max_time\",\"num_rows\"]\n\n    grouped_df[\"min_timeT\"] = grouped_df[\"min_time\"].map(dic_time_sno)\n    grouped_df[\"max_timeT\"] = grouped_df[\"max_time\"].map(dic_time_sno)\n    grouped_df[\"expected_rows\"] = grouped_df[\"max_timeT\"] - grouped_df[\"min_timeT\"] + 1\n    grouped_df[\"num_missing_rows\"] = grouped_df[\"expected_rows\"] - grouped_df[\"num_rows\"]\n    print(grouped_df.head(2).T)\n    grouped_df_missing = grouped_df[grouped_df[\"num_missing_rows\"]>0]\n    missing_asset = list(grouped_df_missing[\"assetCode\"])\n    in_df = in_df[~in_df[\"assetCode\"].isin(missing_asset)]\n    print(\"Total Rows after removing asset with missing rows: \", in_df.shape[0])\n    return in_df[org_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eee8b8b3a1b1a17053067ddd8d028cfdaf231dd0"},"cell_type":"code","source":"#Keep rows for each asset so tht we can have fixed length series data\ndef remove_rows_extra_from_series_len(in_df):\n    print(\"-----------------------------------\")\n    time_list = list(in_df[\"time\"].unique())\n    dic_time_sno = {k: v for v, k in enumerate(time_list)}\n    print(\"Total Rows before remove_rows_extra_from_series_len: \", in_df.shape[0])\n    in_df[\"timeT\"] = in_df[\"time\"].map(dic_time_sno)\n    print(in_df.shape)\n\n    grouped_df = in_df[[\"time\",\"assetCode\"]].groupby([\"assetCode\"],as_index=False).agg({\"time\":{\"min_time\":\"min\", \"max_time\":\"max\",\"num_rows\":\"count\"}})\n    grouped_df.columns = [\"assetCode\",\"min_time\",\"max_time\",\"num_rows\"]\n\n    grouped_df[\"min_timeT\"] = grouped_df[\"min_time\"].map(dic_time_sno)\n    grouped_df[\"max_timeT\"] = grouped_df[\"max_time\"].map(dic_time_sno)\n    grouped_df[\"expected_rows\"] = grouped_df[\"max_timeT\"] - grouped_df[\"min_timeT\"] + 1\n    grouped_df[\"del_timeT\"] = grouped_df[\"min_timeT\"] + (grouped_df[\"num_rows\"] - (SERIES_LEN * (grouped_df[\"num_rows\"] // SERIES_LEN)))\n\n    in_df = pd.merge(in_df, grouped_df, how=\"inner\", left_on=[\"assetCode\"], right_on=[\"assetCode\"])\n    in_df = in_df[in_df[\"timeT\"] >= in_df[\"del_timeT\"]]\n    print(\"Total Rows after remove_rows_extra_from_series_len: \", in_df.shape[0])\n    return in_df[org_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"592be57000793a1fdec9dc6ebcc94d1473f143be"},"cell_type":"code","source":"def get_merged_df(in_market_data, in_news_data):\n    print(\"-----------------------------------\")\n    print(\"Rows before merging\",in_market_data.shape[0])\n    in_market_data['mktDate'] = in_market_data['time'].dt.date #strftime('%Y-%m-%d %H:%M:%S').str.slice(0,10)\n    in_news_data[\"newsDate\"] = in_news_data[\"firstCreated\"].dt.date #strftime('%Y-%m-%d %H:%M:%S').str.slice(0,10)\n    \n    #Extract asset code for news records\n    in_news_data[\"assetCode_new\"] = in_news_data[\"assetCodes\"].map(lambda x: list(eval(x))[0])\n    in_news_data[\"assetCode_len\"] = in_news_data[\"assetCodes\"].map(lambda x: len(list(eval(x))))\n    \n    in_news_data_final = in_news_data\n    if 1 == 2:\n        numAssetCodes = in_news_data[\"assetCode_len\"].max()\n        additionalData = in_news_data\n        colNames = list(in_news_data_final.columns.values)\n        for i in range(1,numAssetCodes):\n            additionalData = additionalData[additionalData[\"assetCode_len\"] > i]\n            additionalData[\"assetCode_new\"] =  additionalData[\"assetCodes\"].map(lambda x: list(eval(x))[i] if len(list(eval(x)))>i else \"\")\n            unexpectedData = additionalData[additionalData[\"assetCode_new\"] == \"\"].head(5)\n            print(unexpectedData[[\"assetCodes\",\"assetCode_new\",\"assetCode_len\"]])\n            in_news_data_final = in_news_data_final.append(additionalData[colNames])\n\n    in_news_data_grouped = in_news_data_final.groupby([\"newsDate\",\"assetCode_new\"], as_index=False).mean()\n    df_merged = pd.merge(in_market_data,in_news_data_grouped, how=\"left\",left_on=[\"mktDate\",\"assetCode\"], right_on=[\"newsDate\",\"assetCode_new\"])\n    dic_assetCode_sno = {k: v for v, k in enumerate(df_merged['assetCode'].unique())}\n    dic_sno_assetCode = {v: k for v, k in enumerate(df_merged['assetCode'].unique())}\n    date_list = list(df_merged[\"time\"].dt.date.sort_values().unique())\n    dic_date_sno = {k: v for v, k in enumerate(date_list)}\n    dic_sno_date = {v: k for v, k in enumerate(date_list)}\n    df_merged['assetCodeT'] = df_merged['assetCode'].map(dic_assetCode_sno)\n    df_merged['mktDateT'] = df_merged['mktDate'].map(dic_date_sno)\n    print(\"Rows after merging\",df_merged.shape[0])\n    return df_merged, dic_sno_assetCode, dic_sno_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00a9a1a8002708cbb72eda7d0674331a69d47a66"},"cell_type":"code","source":"def get_x_y(merged_df, dic_sno_assetCode, dic_sno_date):\n    print(\"-----------------------------------\")\n    feature_cols = [\"volume\",\"close\",\"open\", \"returnsClosePrevRaw1\",\"returnsOpenPrevRaw1\",\"returnsClosePrevMktres1\",\"returnsOpenPrevMktres1\", \n                    \"returnsClosePrevRaw10\", \"returnsOpenPrevRaw10\",\"returnsClosePrevMktres10\",\"returnsOpenPrevMktres10\",\n                   \"urgency\",\"marketCommentary\",\"relevance\",\"sentimentClass\",\"sentimentNegative\",\"sentimentNeutral\",\"sentimentPositive\",\n                   \"noveltyCount12H\",\"noveltyCount24H\", \"noveltyCount3D\",\"noveltyCount5D\",\"noveltyCount7D\",\n                   \"volumeCounts12H\",\"volumeCounts24H\",\"volumeCounts3D\",\"volumeCounts5D\",\"volumeCounts7D\",\"returnsOpenNextMktres10\",\"assetCodeT\", \"mktDateT\"]\n    df_feature = merged_df[feature_cols]\n    df_feature = df_feature.fillna(0)\n    num_rows = df_feature.shape[0]/SERIES_LEN\n    arr_data = np.array(np.split(df_feature[feature_cols].values, num_rows))\n    max_date = merged_df[\"mktDate\"].max()\n    min_date = merged_df[\"mktDate\"].min()\n    df_lstm = pd.DataFrame({\"data\":list(arr_data)})\n    df_lstm[\"x_data\"] = df_lstm[\"data\"].apply(lambda x: np.array(x[:-1,:-2]))\n    df_lstm[\"y_data\"] = df_lstm[\"data\"].apply(lambda x: x[-1,-3])\n    df_lstm[\"assetCodeT\"] = df_lstm[\"data\"].apply(lambda x: x[-1,-2])\n    df_lstm[\"mktDateT\"] = df_lstm[\"data\"].apply(lambda x: x[-1,-1])\n    df_lstm[\"assetCode\"] = df_lstm[\"assetCodeT\"].map(dic_sno_assetCode)\n    df_lstm[\"mktDate\"] = df_lstm[\"mktDateT\"].map(dic_sno_date)\n    train_x = np.array(df_lstm[\"x_data\"])\n    train_y = np.array(df_lstm[\"y_data\"].apply(lambda x: 1 if x>0 else 0))\n\n    x_data = []\n    for x in train_x:\n        x_data.append(x)\n    train_x = np.array(x_data)\n    train_y = train_y.reshape(train_y.shape[0],1)\n    min_y = df_lstm[\"y_data\"].min()\n    max_y = df_lstm[\"y_data\"].max()\n    return train_x, train_y, df_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ed98adfb2f2499e9fec6ef1dee797a59ec893b"},"cell_type":"code","source":"market_train_df = org_market_train_df\nnews_train_df = org_news_train_df\npred_market_train_df = market_train_df\nmarket_train_df = add_missing_rows(market_train_df)\nmarket_train_df = remove_rows_with_nulls(market_train_df)\nnews_train_df = remove_rows_with_nulls(news_train_df)\nmarket_train_df = remove_assets_with_missing_rows(market_train_df)\nmarket_train_df = remove_rows_extra_from_series_len(market_train_df)\nmerged_df, dic_sno_assetCode, dic_sno_date = get_merged_df(market_train_df, news_train_df)\ntrain_x, train_y, df_lstm = get_x_y(merged_df, dic_sno_assetCode, dic_sno_date)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c257e2dadd1b1f55c69adf84908b561ee10d23ad"},"cell_type":"code","source":"import keras\nlearning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='loss', \n                                            patience=1, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\n\nverbose, epochs, batch_size = 1, 50, 16\nn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(n_outputs, activation='sigmoid'))\nopt = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efbda81e7a19a46e9e3dcb76886d4b3a42fcd448"},"cell_type":"code","source":"def make_random_predictions_lstm(market_obs_df, news_obs_df, predictions_df):\n    global pred_market_train_df\n    global news_train_df\n    print(\"-------------------Start Predition---------------\")\n    obs_time = market_obs_df[\"time\"][0]\n    obs_date = market_obs_df[\"time\"].dt.date[0]\n    all_time_list = np.array(pred_market_train_df[\"time\"].sort_values().unique())\n    all_date_list = np.array(pred_market_train_df[\"time\"].dt.date.sort_values().unique())\n    prv_time_list = all_time_list[all_time_list < obs_time]\n    prv_date_list = all_date_list[all_date_list < obs_date]\n    x_time_list = list(prv_time_list[-SERIES_LEN+1:])\n    x_date_list = list(prv_date_list[-SERIES_LEN+1:])\n    print(obs_time)\n    \n    obs_asset_list = list(market_obs_df[\"assetCode\"].unique())\n    import itertools as itr\n    expected_data = list(itr.product(obs_asset_list, x_time_list))\n    expected_df = pd.DataFrame(expected_data, columns=[\"assetCode\",\"expectedTime\"])\n    \n    prv_market_df = pred_market_train_df[((pred_market_train_df[\"assetCode\"].isin(obs_asset_list)) & (pred_market_train_df[\"time\"].isin(x_time_list)))]\n    prv_news_df = news_train_df[((news_train_df[\"assetCode_new\"].isin(obs_asset_list)) & (news_train_df[\"firstCreated\"].dt.date.isin(x_date_list)))]\n    \n    dic_assetCode_sno = {k: v for v, k in enumerate(obs_asset_list)}\n    dic_sno_assetCode = {v: k for v, k in enumerate(obs_asset_list)}\n    \n    expected_df[\"assetCodeT\"] = expected_df['assetCode'].map(dic_assetCode_sno)\n    prv_market_df[\"assetCodeT\"] = prv_market_df['assetCode'].map(dic_assetCode_sno)\n    \n    expected_df = expected_df.drop([\"assetCode\"], axis=1)\n    \n    expected_df = expected_df.sort_values([\"expectedTime\",\"assetCodeT\"])\n    prv_market_df = prv_market_df.sort_values([\"time\", \"assetCodeT\"])\n    \n    prv_market_df = remove_rows_with_nulls(prv_market_df)\n    expected_df = remove_rows_with_nulls(expected_df)\n        \n    mkt_actual_df = pd.merge_asof(expected_df, prv_market_df, left_on=[\"expectedTime\"], right_on=[\"time\"], by='assetCodeT', direction=\"nearest\", tolerance=pd.Timedelta(str(SERIES_LEN) + \" days\"))\n    mkt_actual_df[\"time\"] = mkt_actual_df[\"expectedTime\"]\n    \n    final_market_df = market_obs_df.append(mkt_actual_df[org_cols])\n    final_news_df = news_obs_df.append(prv_news_df[news_obs_df.columns])\n    \n    merged_obs_df, dic_sno_assetCode, dic_sno_date = get_merged_df(final_market_df, final_news_df)\n    train_x, train_y, df_lstm = get_x_y(merged_obs_df, dic_sno_assetCode, dic_sno_date)\n    pred = model.predict(train_x)\n    \n    df_lstm[\"pred\"] = pred\n    \n    #min_pred = np.min(np.array([pred[pred>0.0].min(), pred[pred<0.0].max()*(-1.0)]))\n    \n    predictions_df.set_index(\"assetCode\")\n    df_lstm.set_index(\"assetCode\")\n    final_df = predictions_df.join(df_lstm[[\"pred\"]], how=\"left\")\n    final_df[\"pred\"] = final_df[\"pred\"]*2-1\n    predictions_df.reset_index()\n    df_lstm.reset_index()\n    \n    predictions_df.confidenceValue = final_df[\"pred\"]\n    \n    predictions_df = predictions_df.fillna(0)\n    news_train_df = news_train_df.append(news_obs_df)\n    market_obs_df[\"universe\"]=1\n    market_obs_df_final = pd.merge(market_obs_df, df_lstm[[\"assetCode\",\"pred\"]],how=\"left\", left_on = [\"assetCode\"], right_on=[\"assetCode\"])\n    market_obs_df_final[\"returnsOpenNextMktres10\"] = market_obs_df_final[\"pred\"].fillna(0)\n    pred_market_train_df = pred_market_train_df.append(market_obs_df_final[org_cols])\n    pred_market_train_df = pred_market_train_df[org_cols]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"840aa03b49d675953f080e4069f79f435282bb43"},"cell_type":"markdown","source":"## `get_prediction_days` function\n\nGenerator which loops through each \"prediction day\" (trading day) and provides all market and news observations which occurred since the last data you've received.  Once you call **`predict`** to make your future predictions, you can continue on to the next prediction day.\n\nYields:\n* While there are more prediction day(s) and `predict` was called successfully since the last yield, yields a tuple of:\n    * `market_observations_df`: DataFrame with market observations for the next prediction day.\n    * `news_observations_df`: DataFrame with news observations for the next prediction day.\n    * `predictions_template_df`: DataFrame with `assetCode` and `confidenceValue` columns, prefilled with `confidenceValue = 0`, to be filled in and passed back to the `predict` function.\n* If `predict` has not been called since the last yield, yields `None`."},{"metadata":{"trusted":true,"_uuid":"724c38149860c8e9058474ac9045c2301e8a20da"},"cell_type":"code","source":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11e95f2e3d493ee6e1023c7a4191310adde5d2bf"},"cell_type":"markdown","source":"Note that we'll get an error if we try to continue on to the next prediction day without making our predictions for the current day."},{"metadata":{"trusted":true,"_uuid":"ba72731adf652d6011652e906d8b340d6572904e"},"cell_type":"markdown","source":"### **`predict`** function\nStores your predictions for the current prediction day.  Expects the same format as you saw in `predictions_template_df` returned from `get_prediction_days`.\n\nArgs:\n* `predictions_df`: DataFrame which must have the following columns:\n    * `assetCode`: The market asset.\n    * `confidenceValue`: Your confidence whether the asset will increase or decrease in 10 trading days.  All values must be in the range `[-1.0, 1.0]`.\n\nThe `predictions_df` you send **must** contain the exact set of rows which were given to you in the `predictions_template_df` returned from `get_prediction_days`.  The `predict` function does not validate this, but if you are missing any `assetCode`s or add any extraneous `assetCode`s, then your submission will fail."},{"metadata":{"_uuid":"9cd8317a5e52180b592ee2abc1d2177214642a3c"},"cell_type":"markdown","source":"Let's make random predictions for the first day:"},{"metadata":{"trusted":true,"_uuid":"4ab8a589c328c9eacfc735aec9b45f3bfe9c1d68"},"cell_type":"code","source":"if 1==1: \n    market_obs_df, news_obs_df, predictions_template_df = next(days)\n    make_random_predictions_lstm(market_obs_df, news_obs_df,predictions_template_df)\n    env.predict(predictions_template_df) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c26bd8f2d01e7fd4e2d576e25b772efd2f2920"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff62c167b459c5895383fb05fd9260c14be8c1b8"},"cell_type":"markdown","source":"Now we can continue on to the next prediction day and make another round of random predictions for it:"},{"metadata":{"_uuid":"8056b881707072c379ad2e89b9c59c3c041a2ab7"},"cell_type":"markdown","source":"## Main Loop\nLet's loop through all the days and make our random predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."},{"metadata":{"trusted":true,"_uuid":"ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"},"cell_type":"code","source":"for (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_random_predictions_lstm(market_obs_df, news_obs_df,predictions_template_df)\n    env.predict(predictions_template_df)\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"},"cell_type":"markdown","source":"## **`write_submission_file`** function\n\nWrites your predictions to a CSV file (`submission.csv`) in the current working directory."},{"metadata":{"trusted":true,"_uuid":"2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d38aa8a67cad3f0c105db7e764ec9b805db39ceb"},"cell_type":"code","source":"# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f464f37885ffa763a2592e2867d74685f75be506"},"cell_type":"markdown","source":"As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."},{"metadata":{"_uuid":"2e3a267ea3149403c49ff59515a1a669ca2d1f9f"},"cell_type":"markdown","source":"## Restart the Kernel to run your code again\nIn order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n![Restart button](https://i.imgur.com/hudu8jF.png)"},{"metadata":{"trusted":true,"_uuid":"7a3531c0da844ef2ba5cf3ba5b8a79cd85b73d8b"},"cell_type":"code","source":"import numpy as np\ndef make_random_predictions(market_obs_df, news_obs_df, predictions_df):\n    print (market_obs_df.shape, news_obs_df.shape, predictions_df.shape)\n    print(predictions_df.head(5))\n    merged_obs_df = Get_Merged_DF(market_obs_df, news_obs_df)\n    merged_obs_df = merged_obs_df[feature_cols]\n    merged_obs_df = merged_obs_df.fillna(0)\n    x_obs_data = np.array(merged_obs_df[feature_cols])\n    print(x_obs_data.shape)\n    x_obs_data = 1 - ((maxs - x_obs_data) / rng)\n    obs_pred = clf.predict(x_obs_data)\n    print(obs_pred.shape)\n    print(predictions_df.shape)\n    print((obs_pred * 2 - 1).shape)\n    predictions_df.confidenceValue = obs_pred * 2 - 1\n    predictions_df = predictions_df.fillna(0)\nif 1==2:\n    market_train_df_sample.head(1)\n    market_train_df_sample.tail(1)\n    news_train_df_sample.head(1)\n    news_train_df_sample.tail(1)\n    news_train_df_sample.head(1) \n    df_merged.head(1)\n    df_merged.tail(1)\nif 1==2:\n    all_cols = feature_cols.append(label_col)\n    df_merged_feature = df_merged[feature_cols]\n    df_merged_feature = df_merged_feature.fillna(0)\n\n    #df_merged_feature = df_merged_feature.dropna(axis=0)\n\n    feature_data = np.array(df_merged_feature)\n    mins = np.min(feature_data, axis=0)\n    maxs = np.max(feature_data, axis=0)\n    rng = maxs - mins\n    feature_data = 1 - ((maxs - feature_data) / rng)\n    label_data = df_merged[label_col] >= 0\nif 1==2:\n    label_data = np.array(label_data)\n    maxs = maxs.reshape(1,-1)\n    rng = rng.reshape(1,-1)\n    print(maxs.shape)\n    print(rng.shape)\n    print(feature_data.shape)\n    x_train, x_valid, y_train, y_valid = skm.train_test_split(np.array(feature_data), np.array(label_data), test_size=0.2)\n    print(x_train.shape)\n    print(x_valid.shape)\n    print(y_train.shape)\n    print(y_valid.shape)\n    d_train = lgb.Dataset(x_train, label=y_train)\n    params = {}\n    params['learning_rate'] = 0.001\n    params['boosting_type'] = 'gbdt'\n    params['objective'] = 'binary'\n    params['metric'] = 'binary_logloss'\n    params['sub_feature'] = 0.5\n    params['num_leaves'] = 300\n    params['min_data'] = 50\n    params['max_depth'] = 10\n\n    clf = lgb.train(params, d_train, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca84adc4d9df0a84a984345d600c9d7696aca338"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f297274646fe8ae693eda26ee89ac2cf1b981c60"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d40649fcea8616c9480a4bf8683b11d1bec0de01"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bed9146071686066d858b077c30b1ec8d1699245"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0209aa8a2a06b43e7f4d4bce252508fcc00eea27"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eced475ffc2edc91baeb38b582da6b54102d7e55"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}