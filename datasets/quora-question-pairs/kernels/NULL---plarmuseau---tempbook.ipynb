{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23de6269-51ba-2d76-2ed4-bf56f0020ed8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import os\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# timing function\n",
        "import time   \n",
        "start = time.clock() #_________________ measure efficiency timing\n",
        "\n",
        "input_folder='../input/'\n",
        "train = pd.read_csv(input_folder + 'train.csv',encoding='utf8')[:5000]\n",
        "test  = pd.read_csv(input_folder + 'test.csv',encoding='utf8')[:5000]\n",
        "\n",
        "# lege opvullen\n",
        "train.fillna(value='leeg',inplace=True)\n",
        "test.fillna(value='leeg',inplace=True)\n",
        "\n",
        "print(\"Original data: trainQ: {}, testQ: {}\".format(train.shape, test.shape) )\n",
        "end = time.clock()\n",
        "print('open:',end-start)\n",
        "\n",
        "def cleantxt(x):   \n",
        "    # Pad punctuation with spaces on both sides\n",
        "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':' , '[' ,']' ,'{' ,'}' ,'^']:\n",
        "        x = x.replace(char, ' ' + char + ' ')\n",
        "    return x\n",
        "train['question1']=train['question1'].map(cleantxt)\n",
        "train['question2']=train['question2'].map(cleantxt)\n",
        "test['question1']=test['question1'].map(cleantxt)\n",
        "test['question2']=test['question2'].map(cleantxt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6a091ea-6e57-706a-7e72-461d961d68d2"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from math import*\n",
        "print(train.head())\n",
        "\n",
        "import re\n",
        "r = re.compile(\"[ ,.?|']\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "nr_tr=len(train)\n",
        "nr_te=len(test)\n",
        "\n",
        "train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist())\n",
        "test_qs = pd.Series(test['question1'].tolist() + test['question2'].tolist())\n",
        "all_qs = pd.DataFrame(train_qs.append(test_qs) )\n",
        "all_qs = all_qs.reset_index()\n",
        "all_qs.columns=['index','Q']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer( ngram_range=(1,1),use_idf=True, tokenizer=TreebankWordTokenizer().tokenize)\n",
        "tfidf2 = TfidfVectorizer( ngram_range=(2,2),use_idf=True, tokenizer=TreebankWordTokenizer().tokenize)\n",
        "tfidfall=tfidf.fit_transform(all_qs['Q'])\n",
        "tfidfall2=tfidf2.fit_transform(all_qs['Q'])#print(tfidf)#print(tfidf.stop_words_ )\n",
        "wordnr = pd.DataFrame(pd.Series(list(tfidf.vocabulary_.keys()), index=tfidf.vocabulary_.values()),columns=['woord']) # #print(words)  #woordenboek !\n",
        "nrword = pd.DataFrame.from_dict(tfidf.vocabulary_,orient='index')\n",
        "nrword.columns=['tfnr']#nrword['idf']=tfidf.idf_#print(nrword)\n",
        "end = time.clock()\n",
        "print('__________tfidf:',end-start)\n",
        "\n",
        "print('Test examples for first Q pair')\n",
        "Qnr=0\n",
        "print('cosinesim',  (tfidfall.getrow(Qnr)*tfidfall.getrow(Qnr+nr_tr).T).toarray()[0][0]     )\n",
        "print('cosinesim2',tfidfall[Qnr:Qnr+1].todense()*tfidfall[nr_tr+Qnr:nr_tr+Qnr+1].todense().T)\n",
        "\n",
        "coo = tfidfall[Qnr:Qnr+1].tocoo(copy=False)\n",
        "vector1=pd.DataFrame(data=coo.data,index=coo.col,columns=['tfidf']) #print(vector1.T)\n",
        "coo2 = tfidfall[Qnr+nr_tr:Qnr+nr_tr+1].tocoo(copy=False)\n",
        "vector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) #print(vector2.T)\n",
        "cosin= vector1*vector2\n",
        "vector1['tfidf2']=vector2['tfidf']\n",
        "vector1['woord']=wordnr['woord'] #print(vector1)\n",
        "vector1=vector1.fillna(0) #print(vector1.T)\n",
        "eucl = (vector1['tfidf']-vector1['tfidf2'])**2\n",
        "print('cosin',cosin.sum())\n",
        "print('eucl',sqrt(eucl.sum()))\n",
        "vector1['woord']=wordnr['woord'] #print('vector',vector1.T)\n",
        "q1_word = train.ix[Qnr]['question1'].lower().split()\n",
        "q2_word = train.ix[Qnr]['question2'].lower().split()\n",
        "alien_word = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in q1_word if w not in q2_word]\n",
        "print ('weighted sumdiff',sum(alien_word))\n",
        "print(q1_word,q2_word,alien_word)\n",
        "\n",
        "def word_match(row):\n",
        "    #print(row)\n",
        "    q1_word = {}\n",
        "    q2_word = {}\n",
        "    q1_word=TreebankWordTokenizer().tokenize(row['question1'].lower())\n",
        "    q2_word=TreebankWordTokenizer().tokenize(row['question2'].lower())        \n",
        "    q1_word = [w for w in q1_word if w not in stops]\n",
        "    q2_word = [w for w in q2_word if w not in stops]\n",
        "\n",
        "    if len(q1_word) == 0 or len(q2_word)==0:\n",
        "        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
        "    shared_word_in_q1 = [w for w in q1_word if w in q2_word]\n",
        "    shared_word_in_q2 = [w for w in q2_word if w in q1_word]\n",
        "    alien_word_in_q1 = [w for w in q1_word if w not in q2_word]\n",
        "    alien_word_in_q2 = [w for w in q2_word if w not in q1_word] \n",
        "    stop_word_in_q1 = [w for w in q1_word if w in stops]\n",
        "    stop_word_in_q2 = [w for w in q2_word if w in stops] \n",
        "\n",
        "    nr_rij=row['id']\n",
        "    #T= (tfidfall[nr_rij:nr_rij+1]*tfidfall[nr_rij+nr_tr:nr_rij+nr_tr+1].T).toarray()[0][0]      \n",
        "    AD = len(shared_word_in_q2)/(len(shared_word_in_q2)+len(alien_word_in_q1)*2+len(alien_word_in_q2)*2)\n",
        "    D = (len(shared_word_in_q1)+len(shared_word_in_q2))/(len(q1_word)+len(q2_word))\n",
        "    K = len(shared_word_in_q1)/len(q1_word)*0.5+len(shared_word_in_q2)/len(q2_word)*0.5\n",
        "    O = len(shared_word_in_q2)/sqrt(len(q1_word)*len(q2_word))\n",
        "    S = (len(shared_word_in_q1)-len(stop_word_in_q1)+len(shared_word_in_q2)-len(stop_word_in_q2))/(len(q1_word)-len(stop_word_in_q1)+len(q2_word)-len(stop_word_in_q2))\n",
        "    A1 = (len(alien_word_in_q1)-len(stop_word_in_q1))/(len(q1_word)-len(stop_word_in_q1))    \n",
        "    A2 = (len(alien_word_in_q2)-len(stop_word_in_q2))/(len(q2_word)-len(stop_word_in_q2))\n",
        "    coo1 = tfidfall.getrow(nr_rij).tocoo(copy=False)\n",
        "    coo2 = tfidfall.getrow(nr_rij+nr_tr).tocoo(copy=False)\n",
        "    vector1=pd.DataFrame(data=coo1.data,index=coo1.col,columns=['tfidf']) \n",
        "    vector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) \n",
        "    #print('v1',vector1.T)\n",
        "    cosin= vector1*vector2\n",
        "    vector1['tfidf2']=vector2['tfidf']\n",
        "    eucl = (vector1['tfidf']-vector1['tfidf2'])**2    \n",
        "    T=(cosin.sum())[0]\n",
        "    E=sqrt(eucl.sum())\n",
        "    vector1['woord']=wordnr['woord']\n",
        "    vector1=vector1.fillna(0)\n",
        "    vector2['woord']=wordnr['woord']\n",
        "    shared_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in shared_word_in_q1]\n",
        "    alien_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in alien_word_in_q1]\n",
        "    #print('aliens',alien_word_in_q2,'vector2',vector2.T)\n",
        "    alien_word_weight2 = [list(vector2[vector2['woord']==w]['tfidf'])[0] for w in alien_word_in_q2]\n",
        "    eucl_word_weight2 = [list( (vector2[vector2['woord']==w]['tfidf']-0)**2 ) [0] for w in alien_word_in_q2]\n",
        "    ADw = sum(shared_word_weight1)/(sum(shared_word_weight1)+sum(alien_word_weight1*2)+sum(alien_word_weight2*2))\n",
        "    ED=sqrt(sum(eucl_word_weight2))\n",
        "    WK = sum(shared_word_weight1)/sum(vector1['tfidf'])*0.5+sum(shared_word_weight1)/sum(vector2['tfidf'])*0.5\n",
        "    WO = sum(shared_word_weight1)/sqrt(sum(vector1['tfidf'])*sum(vector2['tfidf']))\n",
        "    #print(eucl_word_weight2)\n",
        "    return D,A1,A2,S,T,sum(alien_word_weight1),sum(alien_word_weight2),E,ED,AD,O,K,ADw,WK,WO\n",
        "\n",
        "\n",
        "temp = train.apply(word_match,axis=1)\n",
        "train_wm = pd.DataFrame(temp.tolist(), columns=['match', 'alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia'])\n",
        "end = time.clock()\n",
        "print('wm:',len(train_wm)/end-start)\n",
        "train[['match','alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia']]=train_wm\n",
        "end = time.clock()\n",
        "print('____different similarities:',end-start)\n",
        "def plotter(kolom):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.hist(train[train['is_duplicate'] == 0][kolom].fillna(0), bins=20, normed=False, label='Not Duplicate')\n",
        "    plt.hist(train[train['is_duplicate'] == 1][kolom].fillna(0), bins=20, normed=False, alpha=0.7, label='Duplicate')\n",
        "    plt.legend()\n",
        "    plt.title('Label distribution over', fontsize=15)\n",
        "    plt.xlabel(kolom, fontsize=15)\n",
        "\n",
        "plotter('match')    \n",
        "plotter('alien1')    \n",
        "plotter('alien2')    \n",
        "plotter('matchS')\n",
        "plotter('cosin')\n",
        "plotter('difw1')\n",
        "plotter('difw2')\n",
        "plotter('Eucl')\n",
        "plotter('Eudiff')\n",
        "plotter('AntiDice')\n",
        "plotter('Ochia')\n",
        "plotter('Kulz')\n",
        "plotter('WeightADice')\n",
        "plotter('WeightKulz')\n",
        "plotter('WeightOchia')\n",
        "end = time.clock()\n",
        "print('clean and make freq word dict:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2706363a-8aa1-8f02-f29a-ff66c6e69c5a"
      },
      "outputs": [],
      "source": [
        "print(train[train['Ochia']>0.98])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import scipy\n",
        "import xgboost as xgb\n",
        "import difflib\n",
        "\n",
        "y=train['is_duplicate']     \n",
        "feats = train.columns.values.tolist()\n",
        "feats=[x for x in feats if x not in ['question1','question2','id','qid1','qid2','is_duplicate']]\n",
        "print(\"features\",feats)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train[feats], y, test_size=0.1, random_state=0)\n",
        "#XGBoost model\n",
        "params = {\"objective\":\"binary:logistic\",'eval_metric':'logloss',\"max_depth\":7}\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "bst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=200,verbose_eval=25) #change to higher #s\n",
        "print('training done')\n",
        "\n",
        "end = time.clock()\n",
        "print('____________trained:',end-start)\n",
        "print(\"log loss for training data set\",log_loss(y, bst.predict(xgb.DMatrix(train[feats]))))\n",
        "#Predicting for test data set\n",
        "sub = pd.DataFrame() # Submission data frame\n",
        "sub['test_id'] = []\n",
        "sub['is_duplicate'] = []\n",
        "header=['test_id','question1','question2','id','qid1','qid2','is_duplicate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "202c1e2f-6e7b-72cf-7ca8-564b2938c55e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_match_test(row):\n",
        "    #print(row)\n",
        "    q1_word = {}\n",
        "    q2_word = {}\n",
        "    q1_word=TreebankWordTokenizer().tokenize(row['question1'].lower())\n",
        "    q2_word=TreebankWordTokenizer().tokenize(row['question2'].lower())        \n",
        "    q1_word = [w for w in q1_word if w not in stops]\n",
        "    q2_word = [w for w in q2_word if w not in stops]\n",
        "\n",
        "    if len(q1_word) == 0 or len(q2_word)==0:\n",
        "        return 0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
        "    shared_word_in_q1 = [w for w in q1_word if w in q2_word]\n",
        "    shared_word_in_q2 = [w for w in q2_word if w in q1_word]\n",
        "    alien_word_in_q1 = [w for w in q1_word if w not in q2_word]\n",
        "    alien_word_in_q2 = [w for w in q2_word if w not in q1_word] \n",
        "    stop_word_in_q1 = [w for w in q1_word if w in stops]\n",
        "    stop_word_in_q2 = [w for w in q2_word if w in stops] \n",
        "\n",
        "    nr_rij=row['test_id']+nr_tr*2\n",
        "    #T= (tfidfall[nr_rij:nr_rij+1]*tfidfall[nr_rij+nr_tr:nr_rij+nr_tr+1].T).toarray()[0][0]      \n",
        "    AD = len(shared_word_in_q2)/(len(shared_word_in_q2)+len(alien_word_in_q1)*2+len(alien_word_in_q2)*2)\n",
        "    D = (len(shared_word_in_q1)+len(shared_word_in_q2))/(len(q1_word)+len(q2_word))\n",
        "    K = len(shared_word_in_q1)/len(q1_word)*0.5+len(shared_word_in_q2)/len(q2_word)*0.5\n",
        "    O = len(shared_word_in_q2)/sqrt(len(q1_word)*len(q2_word))\n",
        "    S = (len(shared_word_in_q1)-len(stop_word_in_q1)+len(shared_word_in_q2)-len(stop_word_in_q2))/(len(q1_word)-len(stop_word_in_q1)+len(q2_word)-len(stop_word_in_q2))\n",
        "    A1 = (len(alien_word_in_q1)-len(stop_word_in_q1))/(len(q1_word)-len(stop_word_in_q1))    \n",
        "    A2 = (len(alien_word_in_q2)-len(stop_word_in_q2))/(len(q2_word)-len(stop_word_in_q2))\n",
        "    coo1 = tfidfall.getrow(nr_rij).tocoo(copy=False)\n",
        "    coo2 = tfidfall.getrow(nr_rij+nr_te).tocoo(copy=False)\n",
        "    vector1=pd.DataFrame(data=coo1.data,index=coo1.col,columns=['tfidf']) \n",
        "    vector2=pd.DataFrame(data=coo2.data,index=coo2.col,columns=['tfidf']) \n",
        "    #print('v1',vector1.T)\n",
        "    cosin= vector1*vector2\n",
        "    vector1['tfidf2']=vector2['tfidf']\n",
        "    eucl = (vector1['tfidf']-vector1['tfidf2'])**2    \n",
        "    T=(cosin.sum())[0]\n",
        "    E=sqrt(eucl.sum())\n",
        "    vector1['woord']=wordnr['woord']\n",
        "    vector1=vector1.fillna(0)\n",
        "    vector2['woord']=wordnr['woord']\n",
        "    shared_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in shared_word_in_q1]\n",
        "    alien_word_weight1 = [list(vector1[vector1['woord']==w]['tfidf'])[0] for w in alien_word_in_q1]\n",
        "    #print('aliens',alien_word_in_q2,'vector2',vector2.T)\n",
        "    alien_word_weight2 = [list(vector2[vector2['woord']==w]['tfidf'])[0] for w in alien_word_in_q2]\n",
        "    eucl_word_weight2 = [list( (vector2[vector2['woord']==w]['tfidf']-0)**2 ) [0] for w in alien_word_in_q2]\n",
        "    ADw = sum(shared_word_weight1)/(sum(shared_word_weight1)+sum(alien_word_weight1*2)+sum(alien_word_weight2*2))\n",
        "    ED=sqrt(sum(eucl_word_weight2))\n",
        "    WK = sum(shared_word_weight1)/sum(vector1['tfidf'])*0.5+sum(shared_word_weight1)/sum(vector2['tfidf'])*0.5\n",
        "    WO = sum(shared_word_weight1)/sqrt(sum(vector1['tfidf'])*sum(vector2['tfidf']))\n",
        "    #print(eucl_word_weight2)\n",
        "    return D,A1,A2,S,T,sum(alien_word_weight1),sum(alien_word_weight2),E,ED,AD,O,K,ADw,WK,WO\n",
        "\n",
        "\n",
        "temp = test.apply(word_match_test,axis=1)\n",
        "train_wm = pd.DataFrame(temp.tolist(), columns=['match', 'alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia'])\n",
        "end = time.clock()\n",
        "print('wm:',len(train_wm)/end-start)\n",
        "test[['match','alien1','alien2','matchS','cosin','difw1','difw2','Eucl','Eudiff','AntiDice','Ochia','Kulz','WeightADice','WeightKulz','WeightOchia']]=train_wm\n",
        "end = time.clock()\n",
        "\n",
        "def plottest(kolom):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.hist(test[kolom].fillna(0), bins=20, normed=False)\n",
        "    plt.legend()\n",
        "    plt.title('Label distribution over', fontsize=15)\n",
        "    plt.xlabel(kolom, fontsize=15)\n",
        "    \n",
        "plottest('match')    \n",
        "plottest('alien1')    \n",
        "plottest('alien2')    \n",
        "plottest('matchS')\n",
        "plottest('cosin')\n",
        "plottest('difw1')\n",
        "plottest('difw2')\n",
        "plottest('Eucl')\n",
        "plottest('Eudiff')\n",
        "plottest('AntiDice')\n",
        "plottest('Ochia')\n",
        "plottest('Kulz')\n",
        "plottest('WeightADice')\n",
        "plottest('WeightKulz')\n",
        "plottest('WeightOchia')\n",
        "\n",
        "sub=pd.DataFrame({'test_id':test['test_id'], 'is_duplicate':bst.predict(xgb.DMatrix(test[feats]))})\n",
        "print(sub.head())\n",
        "sub.to_csv('quora_xgb.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36a40ed1-81ec-317b-a7f1-cefe1697f632"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2c7ef04-fc3b-1ee3-6f5e-8fc25ae0c398"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}