{"metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_cell_guid": "699e900c-5546-4221-aef6-e2c479c92a4c", "_uuid": "0ed42b009beea8e3a34b104c93360712491c9f96"}, "source": ["<H1>Minimizing the \"Real\" Cost of Credit Card Fraud</H1>\n", "<H3>by Michael Klear</H3>\n", "This <a href='https://www.kaggle.com/dalpozz/creditcardfraud'>awesome dataset</a> provides a great real-world example of the challenges of credit card fraud detection. Why? Take a look:\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "460ba72e-29de-4de0-9679-90c81bcb7218", "_uuid": "5f37fe94ae396d66a38fc9d01d2d8dddb6312b68"}, "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "import xgboost as xgb\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "sns.set_style('whitegrid')\n", "\n", "df = pd.read_csv('../input/creditcard.csv')\n", "df = df.sample(frac=1, random_state=123) #Shuffle samples\n", "\n", "class_imb = df.Class.sum()/len(df) #Record class imbalance\n", "\n", "#Pie plot\n", "labels = ['Genuine Transactions', 'Fraudulent Transactions']\n", "sizes = [(len(df)-df.Class.sum()), df.Class.sum()]\n", "colors = ['blue', 'red']\n", "\n", "plt.pie(sizes, labels=labels, colors=colors,\n", "        autopct='%1.1f%%', shadow=True, startangle=140)\n", "plt.title('Genuine Transactions vs. Fraudulent Transactions')\n", "plt.axis('equal')\n", "plt.show()"], "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "37406a53-a670-4ab7-b5de-daa21067bc88", "_uuid": "f6d1e99ffd38e910f0b34cd5745b22935885c15a"}, "source": ["The number of fraudent transactions in this dataset is tiny compared to the body of all the \"honest\" transactions recorded. In a purely theoretical binary classification problem, this situation has its difficulties. In the \"real world,\" it's even more complicated. That's because <b>the cost of a misclassification can vary wildly between type I errors (false positives) and type II errors (false negatives)</b>. Because we live in the real world<sup>[citation needed]</sup>, it may be best to define the cost of each type of error before we predict anything.<br>\n", "<H2>The Cost of Misclassification</H2>\n", "<H3>Type II Error: The Cost of an Undetected Fraudulent Charge</H3><br>\n", "For the sake of this task, let's assume that we (the credit card company) are responsible for refunding the full amount of a fraudulent transaction. The cost of a type II error, then is simply the transaction amount for that particular transaction.\n", "<H3>Type I Error: The Cost of a \"False Alarm\"</H3><br>\n", "If our model misclassifies a genuine transaction as fraudulent and blocks the transaction, we have a few costs to consider:\n", "<li>The cost of investigating the transaction</li>\n", "<li>The cost of inconvenience to the customer</li>\n", "<li>The lost business (the transaction did not occur)</li><br>\n", "Let's attempt to quantify these costs:\n", "<li>We will probably need to pay out an average of one employee hour to investigate the transaction.  Let's call this \\$25. </li>\n", "<li>The cost of inconvenience to the customer is hard to quantify. The biggest risk is losing the customer altogether. Let's say the probability of losing a customer due to inconvenience is 1/100 and the cost of losing a customer (the lifetime expected profit from a customer) is \\$1,000. Thus, the mean cost due to inconvenience to a customer would be one percent of \\$1,000, or \\$10.</li>\n", "<li>If we collect 1% of the transaction amount in fees, the cost of the lost transaction is simply one percent of the transaction amount.</li><br>\n", "We now have a rough notion of the cost of both type of errors. This will come in handy when we evaluate our predictive model.<br>\n", "<H2>Training/Test Split</H2>\n", "I'd like to reserve a good amount of samples for model evaluation, so we'll use a 50-50 training/test split. Half of the data will be used to fit the model, and the other half will be used to evaluate it.<br>\n", "I'll also define our features. I omit time, as it's not a feature that makes sense to use in a production environment (that is to say that any information our model picks up from this feature would be useless in the future, as future times do not appear in our training data.)"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ea370ce5-7de9-42ab-8476-489187264c1b", "collapsed": true, "_uuid": "318117e6a56d341e565daade6115394d3185d81c"}, "execution_count": null, "source": ["#Define our features, omitting time\n", "features = ['V{}'.format(x) for x in range(1, 29)] + ['Amount']\n", "#Set cutoff at 50%\n", "cutoff=int(.5*len(df))\n", "#Split data into training and test \n", "train = df[:cutoff]\n", "test = df[cutoff:]\n", "\n", "#Split up into X's and Y's\n", "X_train = train[features]\n", "Y_train = train.Class\n", "\n", "X_test = test[features]\n", "Y_test = test.Class"], "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "8b56cf8d-a38b-4be3-9cee-784bfbb78248", "_uuid": "de37293cb90c0538fad08f33403a32ed65f6a553"}, "source": ["<H2>Making Predictions</H2><br>\n", "<H3>Model 1: Make No Predictions</H3><br>\n", "With such a high class imbalance, predicting 100% geniune transaction is a viable option that yields 99.8% overall accuracy. It's certainly the simplest solution that would be easy to put into production.<br>\n", "<H4>Error Breakdown</H4>"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6eff3ec9-7e1e-4fd4-a5ad-d4ca45b42026", "_uuid": "b6863caa3b2d85e097f78c94e76d9b14fbbc2f29"}, "execution_count": null, "source": ["#record results:\n", "results1 = pd.DataFrame()\n", "results1['true_value'] = np.array(Y_test)\n", "results1['predicted'] = 0\n", "results1['correct'] = np.where(results1.predicted == results1.true_value, 1, 0)\n", "results1['tI_error'] = np.where((results1.predicted == 1) & (results1.true_value == 0), 1, 0)\n", "results1['tII_error'] = np.where((results1.predicted == 0) & (results1.true_value == 1), 1, 0)\n", "\n", "labels = ['False Negatives (type II errors)', 'Correctly Identified']\n", "sizes = [results1.tII_error.sum(), results1.correct.sum()]\n", "colors = ['red', 'blue']\n", "\n", "plt.pie(sizes, labels=labels, colors=colors,\n", "        autopct='%1.1f%%', shadow=True, startangle=140)\n", "plt.title('Model 1 Performance: Error Breakdown')\n", "plt.axis('equal')\n", "plt.show()\n", "\n", "results1['Amount'] = np.array(X_test['Amount'])\n", "#type I error cost, as defined\n", "results1['cost_I'] = np.where(results1.tI_error == 1, (25 + 10 + (.01*results1.Amount)), 0)\n", "#type II error cost, as defined\n", "results1['cost_II'] = np.where(results1.tII_error == 1, results1.Amount, 0)\n", "results1['total_cost'] = results1[['cost_I', 'cost_II']].sum(axis=1)\n", "total_cost_I = results1.cost_I.sum()\n", "total_cost_II = results1.cost_II.sum()\n", "total_cost = total_cost_I + total_cost_II\n", "        \n", "fig, ax = plt.subplots(figsize=(8, 5))\n", "plt.bar(\n", "    ['cost due to type I errors', 'cost due to type II errors', 'total_cost'], \n", "    [total_cost_I, total_cost_II, total_cost],\n", "    color=['yellow', 'orange', 'green']\n", ");\n", "plt.title('Contributions to Cost');\n", "plt.ylabel('Cost in Dollars');\n", "plt.show();\n", "\n", "print(\"The total cost of this model on our test set is: ${}\".format(str(total_cost)))"], "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "6f75ce52-80bd-4c7e-a236-09b5e56a9fc9", "_uuid": "7fc34d78baef2f7411e66e43f17acd587c49680c"}, "source": ["<H3>Results Interpretation</H3><br>\n", "The total cost of using the model in production is almost \\$30,000, despite the fact that it's highly accurate. Let's see if we can improve this cost with a predictive model.<br>\n", "<H2>Model 2: Gradient Boosted Model</H2><br>\n", "Now we'll try to reduce the cost by making some predictions and trying to catch fraud. I'm going to make use of xgboost to impliment the gradient boosting algorithm. This wouldn't be a proper Kaggle kernel without xgboost."], "cell_type": "markdown"}, {"metadata": {"_uuid": "d9d007ee9a75799eadd7c302c99c4a506b276bb1", "_cell_guid": "a137f815-7f92-49f0-8e5e-32588913da3f"}, "execution_count": null, "source": ["#Set up our data and parameters\n", "dtrain = xgb.DMatrix(X_train, label=Y_train)\n", "param = {\n", "     'colsample_bytree': 0.4,\n", "     'eta': 14/100,\n", "     'max_depth': 3,\n", "     'nthread': 4,\n", "     'objective': 'binary:logistic',\n", "     'scale_pos_weight': .5/class_imb,\n", "     'silent': 1\n", "}\n", "\n", "plst = param.items()\n", "\n", "#Train the model (cue 'Eye of the Tiger')\n", "num_round = 500\n", "bst = xgb.train(plst, dtrain, num_round, verbose_eval=False)\n", "\n", "#Grab predictions at p=.5 threshold\n", "dtest = xgb.DMatrix(X_test)\n", "Y_ = bst.predict(dtest)\n", "Y_ = np.where(Y_>.5, 1, 0)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "cd90f2c9-8ed7-44bc-8787-3f0070c18c3d", "_uuid": "7bfd054ad780c8e71d988b5ce44cb033efc87e7b"}, "execution_count": null, "source": ["#record results:\n", "results2 = pd.DataFrame()\n", "results2['true_value'] = np.array(Y_test)\n", "results2['predicted'] = Y_\n", "results2['correct'] = np.where(results2.predicted == results2.true_value, 1, 0)\n", "results2['tI_error'] = np.where((results2.predicted == 1) & (results2.true_value == 0), 1, 0)\n", "results2['tII_error'] = np.where((results2.predicted == 0) & (results2.true_value == 1), 1, 0)\n", "\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n", "\n", "ax1.set_title('Model 2 Performance: Error Breakdown')\n", "lbl = ['Incorrectly Identified', \n", "       'Correctly Identified ({}%)'.format(str(100*results2.correct.sum()/len(results2))[:5])\n", "]\n", "sz = [results2.tII_error.sum() + results2.tI_error.sum(), results2.correct.sum()]\n", "clrs = ['red', 'blue']\n", "ax1.pie(sz, labels=lbl, colors=clrs, shadow=True, startangle=90)\n", "\n", "ax2.set_title('Error Types')\n", "labels = [\n", "    'Type I Errors (False Positives): {}'.format(results2.tI_error.sum()),\n", "    'Type II Errors (False Negatives): {}'.format(results2.tII_error.sum())\n", "]\n", "sizes = [\n", "    results2.tI_error.sum(),\n", "    results2.tII_error.sum()\n", "]\n", "\n", "ax2.pie(sizes, labels=labels, colors=['cyan', 'purple'], startangle=215, \n", "        autopct='%1.1f%%')\n", "plt.subplots_adjust(wspace=1.5)\n", "plt.show()\n", "\n", "results2['Amount'] = np.array(X_test['Amount'])\n", "#type I error cost, as defined\n", "results2['cost_I'] = np.where(results2.tI_error == 1, (25 + 10 + (.01*results2.Amount)), 0)\n", "#type II error cost, as defined\n", "results2['cost_II'] = np.where(results2.tII_error == 1, results2.Amount, 0)\n", "results2['total_cost'] = results2[['cost_I', 'cost_II']].sum(axis=1)\n", "total_cost_I = results2.cost_I.sum()\n", "total_cost_II = results2.cost_II.sum()\n", "total_cost = total_cost_I + total_cost_II\n", "        \n", "fig, ax = plt.subplots(figsize=(8, 5))\n", "plt.bar(\n", "    ['cost due to type I errors', 'cost due to type II errors', 'total cost'], \n", "    [total_cost_I, total_cost_II, total_cost],\n", "    color=['cyan', 'purple', 'green']\n", ");\n", "plt.title('Contributions to Cost');\n", "plt.ylabel('Cost in Dollars');\n", "plt.show();\n", "\n", "print(\"The total cost of this model on our test set is: ${}\".format(str(total_cost)[:7]))"], "outputs": [], "cell_type": "code"}, {"metadata": {"_cell_guid": "6e35e4c5-1374-4e0e-84d2-d44797813ae3", "_uuid": "9a51fefaebdf342258f98038bcfffeb220b89a09"}, "source": ["<H3>Results Interpretation</H3><br>\n", "The total cost of using the model on our test set is over \\$8,000, and the model is more accurate overall than a the first (null) model. This reduces our expense due to fraud to less than a third of what it would have been without taking action. Not bad!<br>\n", "We can still see, however, that the majority of our cost comes from type II errors, or instances of undetected fraud. Given the disparity in the cost of each type of misclassification, we may be better off with a lower overall accuracy if we can \"catch\" more fraudulent charges. One technique would be, of course, to tune up our xgboost implimentation (More than I've already done). Another is to try a different type of model. Maybe I'll come back and try that, but for now I'm happy to have saved my hypothetical company over $20,000!<br>"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "2e05567a4e32bbb0d0601a8ac11d298b566a67e2", "_cell_guid": "aea7bec8-50e7-409d-a015-f5895956d6e2"}, "execution_count": null, "source": [], "outputs": [], "cell_type": "code"}]}