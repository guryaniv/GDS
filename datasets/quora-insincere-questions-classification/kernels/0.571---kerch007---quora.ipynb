{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/embeddings/glove.840B.300d\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n\nimport os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2fb8656863f996290ce51b5f52c7a5fc03ff536"},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv' )\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb439573edbe213cf11e5d7ec56815a5ca4dd2ab"},"cell_type":"code","source":"def cleanSentences(text, remove_stopwords=True, stem_words=True):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().replace(\"<br />\", \" \")\n    text = text.split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n     # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5d78f31911f32164b30076003b1210ae9f9194b"},"cell_type":"code","source":"data['question_text'] = data['question_text'].apply(lambda x: cleanSentences(x))\nX = data.question_text\nY = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\nX_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"218984221457a3f74fcf52cdad6183026ea654f2"},"cell_type":"code","source":"data_test = pd.read_csv('../input/test.csv' )\ndata_test['question_text'] = data_test['question_text'].apply(lambda x: cleanSentences(x))\ndata_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8513acf013140100806caf16753cc17ccbed44b1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58ee1b1f74c97b8dd4035c71fc9cfffebf06ae5"},"cell_type":"code","source":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = X_train\nval_X = X_test\ntest_X = data_test[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = y_train\nval_y = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25529d817d39e6e5daacf682e2138100c9a20f31"},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6905a74aed5b81b575ab4d3002e06621d593705c"},"cell_type":"code","source":"model.fit(train_X, train_y, batch_size=512, epochs=5, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ac6a2a5da5c52cc84ec1bff38c02865cdfb396e"},"cell_type":"code","source":"prediction = model.predict(test_X)\nprediction_recovered = np.round(prediction )\n\nbinary_prediction = [1 if item[0]>0.5 else 0 for item in prediction_recovered]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d20603bc6d8534be4f5bdcecc99ea630ce72aa99"},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\",\"../input\"]).decode('utf8'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25dd3770e9ab94395b7ffa7f47da851d79c9c853"},"cell_type":"code","source":"import pandas as pd\n\nmy_submission = pd.read_csv(\"../input/sample_submission.csv\")\nmy_submission = pd.DataFrame({'qid': data_test.qid, 'prediction':  binary_prediction})\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8555e819b8cb42982dad508505019a6a674eed04"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}