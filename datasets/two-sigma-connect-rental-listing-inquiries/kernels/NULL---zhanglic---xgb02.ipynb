{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "035c16ea-12b9-3a34-60ce-73a3f6962298"
      },
      "source": [
        "It seems the current [high scoring script][1] is written in R using H2O. So let us do one in python using XGBoost. \n",
        "\n",
        "Thanks to [this script][2] for feature engineering ideas. \n",
        "\n",
        "We shall start with importing the necessary modules\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/gospursgo/two-sigma-connect-rental-listing-inquiries/h2o-starter-pack/run/835757\n",
        "  [2]: https://www.kaggle.com/aikinogard/two-sigma-connect-rental-listing-inquiries/random-forest-starter-with-numerical-features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b30297fd-d7dc-601d-1c3f-5ceb50bf0a8d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import xgboost as xgb\n",
        "from sklearn import model_selection, preprocessing, ensemble\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "29fa9103-bd3b-ad6c-0afd-c2bda6456f80"
      },
      "source": [
        "Now let us write a custom function to run the xgboost model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "251ffb3b-e0d0-8409-5fc9-260c08ff804e"
      },
      "source": [
        "Let us read the train and test files and store it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8dea5f1-e6c1-54d8-15fe-6711825b7e50"
      },
      "outputs": [],
      "source": [
        "data_path = \"../input/\"\n",
        "train_file = data_path + \"train.json\"\n",
        "test_file = data_path + \"test.json\"\n",
        "train_df = pd.read_json(train_file)\n",
        "test_df = pd.read_json(test_file)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0de6a1e-328d-6177-1e08-60321c259fc9"
      },
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d72ef73f-09bc-d9ef-e4ac-a5bab39a5058"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0165afcd-ce08-fd0c-7ac6-3b1f1cd99c77"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f943207-f45b-84b8-35b0-85f908192165"
      },
      "source": [
        "We do not need any pre-processing for numerical features and so create a list with those features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5b5da1a-5066-b665-bf44-03c20558fcf9"
      },
      "outputs": [],
      "source": [
        "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9e18805f-cb26-7181-7c52-1a50e6cca5fe"
      },
      "source": [
        "Now let us create some new features from the given features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc2ea8f0-074a-7233-4a26-80dc6870b973"
      },
      "outputs": [],
      "source": [
        "# count of photos #\n",
        "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
        "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
        "\n",
        "# count of \"features\" #\n",
        "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
        "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
        "\n",
        "# count of words present in description column #\n",
        "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "# convert the created column to datetime object so as to extract more features \n",
        "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
        "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
        "\n",
        "# Let us extract some features like year, month, day, hour from date columns #\n",
        "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
        "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
        "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
        "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
        "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
        "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
        "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
        "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
        "\n",
        "# adding all these new features to use list #\n",
        "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bfcfd8e6-484e-5137-d562-b35a4e756aef"
      },
      "source": [
        "We have 4 categorical features in our data\n",
        "\n",
        " - display_address\n",
        " - manager_id\n",
        " - building_id\n",
        " - listing_id\n",
        "\n",
        "So let us label encode these features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c15ea019-8eac-ae3f-2d23-8a68b2169b05"
      },
      "outputs": [],
      "source": [
        "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
        "for f in categorical:\n",
        "        if train_df[f].dtype=='object':\n",
        "            #print(f)\n",
        "            lbl = preprocessing.LabelEncoder()\n",
        "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
        "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
        "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
        "            features_to_use.append(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc36f663-6122-8727-c466-ed50a04554c9"
      },
      "source": [
        "We have features column which is a list of string values. So we can first combine all the strings together to get a single string and then apply count vectorizer on top of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7e55f87-1de7-0ab2-7b01-6b1d772849b7"
      },
      "outputs": [],
      "source": [
        "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
        "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
        "print(train_df[\"features\"].head())\n",
        "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
        "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
        "te_sparse = tfidf.transform(test_df[\"features\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0a57223-cfc6-ad42-0447-cb33cc13febb"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1fc1374a-8c1d-2460-5e00-8513c8a13993"
      },
      "source": [
        "Now let us stack both the dense and sparse features into a single dataset and also get the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c313f80c-880b-fa4a-907f-637e26acdf3a"
      },
      "outputs": [],
      "source": [
        "train_df[features_to_use].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15a9e31e-1caf-2860-4f7a-85999bb2a8e6"
      },
      "outputs": [],
      "source": [
        "train_X = train_df[features_to_use]\n",
        "test_X = test_df[features_to_use]\n",
        "\n",
        "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
        "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
        "print(train_X.shape, test_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bbcb6917-d9b6-a71a-de02-73acb4c9eb30"
      },
      "outputs": [],
      "source": [
        "train_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e2b5c47-2b14-f56f-a4ae-b85294d8d281"
      },
      "outputs": [],
      "source": [
        "train_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01143cf0-0529-b836-2fbc-e3c1f46127c7"
      },
      "outputs": [],
      "source": [
        "train_df['interest_level']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bad1197b-dc73-ea6e-9ed4-e944ed300db3"
      },
      "source": [
        "Now let us do some cross validation to check the scores. \n",
        "\n",
        "Please run it in local to get the cv scores. I am commenting it out here for time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "989e560b-b273-fb03-5c9d-ca0d0782d393"
      },
      "outputs": [],
      "source": [
        "features_to_use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a599c136-1925-59ac-1a66-5a32e299c007"
      },
      "outputs": [],
      "source": [
        "def modelfit(alg, X,y, useTrainCV=True,\n",
        "             cv_fold=5,early_stopping_rounds = 20):\n",
        "    if useTrainCV:\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        xgb_param['num_class']=3\n",
        "        xgtrain = xgb.DMatrix(X,label = y)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=\n",
        "                          alg.get_params()['n_estimators'],nfold=cv_fold,\n",
        "                          metrics='mlogloss',early_stopping_rounds = early_stopping_rounds)\n",
        "        print(cvresult)\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "        \n",
        "    #Fit the algorithm on the data\n",
        "    alg.fit(X, y,eval_metric='mlogloss')\n",
        "        \n",
        "    #Predict training set:\n",
        "    y_pred = alg.predict(X)\n",
        "    y_predprob = alg.predict_proba(X)\n",
        "    print(y_predprob)\n",
        "        \n",
        "    #Pring model report\n",
        "    print (\"\\nModel Report\")\n",
        "    #print (\"Accuracy : %.4g\" % accuracy_score(y, \n",
        "                                            #y_pred))\n",
        "    print (\"logloss score (Train) : %f\" % log_loss(y,  y_predprob))\n",
        "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
        "    feat_imp.plot(kind='hbar',title='Feature Importances')\n",
        "    plt.ylabel('Feature Importance Score')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7399489a-9d03-b90e-fbe4-f447386aa500"
      },
      "outputs": [],
      "source": [
        "xgb1 = XGBClassifier(\n",
        "    learning_rate = 0.1,\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    min_child_weight=1,\n",
        "    gamma = 0,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='multi:softprob',\n",
        "    nthread =4,\n",
        "    scale_pos_weight=1,\n",
        "    seed = 0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a5a0743-c82a-5f36-6ec8-247ea926c505"
      },
      "outputs": [],
      "source": [
        "modelfit(xgb1, train_X, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9ae65091-6f1e-1c9c-140a-afb6952d6aa2"
      },
      "outputs": [],
      "source": [
        "xgb1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f02f2e1-f157-7fe4-2bb3-5a00ff1a07b7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28aeed48-de25-28da-23ac-826932d4f528"
      },
      "outputs": [],
      "source": [
        "param_test1={\n",
        "    'max_depth':range(1,10,2),\n",
        "    'min_child_weight':range(1,10,3)\n",
        "}\n",
        "\n",
        "gs1 = GridSearchCV(xgb1,param_grid=param_test1, \n",
        "                   scoring='neg_log_loss', n_jobs=-1,iid=False, cv=5)\n",
        "gs1.fit(train_X,train_y)\n",
        "gs1.grid_scores_, gs1.best_params_,gs1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19f667c9-a9a3-f92b-c12a-01dc45a5997b"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "afcfb932-2081-21a3-f2af-1296f34236d4",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55f89a90-4ac1-ea20-c07a-c289ffae9304",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77685e7f-2c3f-78b1-7794-44a5f0ff6991",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8165935e-b992-c01d-e903-1d4ca110304e",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5dbdeb44-bf07-8fcb-2534-086f6ec3b82a",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59555bb8-b27e-9c33-fc9c-3e91ce44f5f1",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c29df0a9-ea45-342e-e126-90b7adb8cec3"
      },
      "source": [
        "Now let us build the final model and get the predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e599ea34-9295-776b-32de-7b4e46fa13cc"
      },
      "outputs": [],
      "source": [
        "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
        "out_df = pd.DataFrame(preds)\n",
        "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
        "out_df[\"listing_id\"] = test_df.listing_id.values\n",
        "#out_df.to_csv(\"xgb2.csv\", index=False)\n",
        "out_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dba1497e-5a5e-f93c-ed98-e122f7df6db7"
      },
      "source": [
        "\n",
        "Hope this helps the python users as a good starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64a38118-3e95-0737-e80b-a6a176b36e80"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}