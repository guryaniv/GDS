{"cells": [{"source": ["import pandas as pd\n", "#from bs4 import BeautifulSoup\n", "import re\n", "from nltk.corpus import stopwords\n", "from gensim.models import word2vec\n", "import pickle\n", "import nltk.data\n", "import os\n", "# Load the punkt tokenizer\n", "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "106d0ecd-97f8-4369-bc24-7f4fa07d05f9", "_uuid": "0a79ba9b17656235fdfa54d89292c06c024b2878", "collapsed": true}, "execution_count": 8}, {"source": ["# Read data from files \n", "path = '../input/'\n", "TRAIN_DATA_FILE=f'{path}train.csv'\n", "TEST_DATA_FILE=f'{path}test.csv'\n", "\n", "\n", "train = pd.read_csv(TRAIN_DATA_FILE, header=0)\n", "test = pd.read_csv(TEST_DATA_FILE, header=0)\n", "\n", "# Verify the number of comments that were read\n", "print(\"Read %d labeled train reviews and  %d unlabelled test reviews\" % (len(train),len(test)))\n", "all_comments = train['comment_text'].fillna(\"_na_\").tolist() + test['comment_text'].fillna(\"_na_\").tolist() \n", "\n", "\n", "with open(\"all_comments.csv\", \"w+\") as comments_file:\n", "    i=0\n", "    for comment in all_comments:\n", "        comment = re.sub(\"[^a-zA-Z]\",\" \",str(comment))\n", "        comments_file.write(\"%s\\n\" % comment)\n", "        "], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d8422b84-2faa-47bd-9128-50ba01c5efdf", "_uuid": "90065e90e9c00fe54491f7fb392792859d746274"}, "execution_count": 9}, {"source": ["##### 1. Preprocess Text Reviews For Creating Word Vectors\n", "\n", "The <b>sentences</b> iterable can be simply a list, but for larger corpora, consider a generator that streams the sentences directly from disk/network, without storing everything in RAM. See BrownCorpus, Text8Corpus or LineSentence in the gensim.models.word2vec module for such examples.\n", "\n", "<b>min_count</b> ignore all words and bigrams with total collected count lower than this.\n", "\n", "<b>threshold</b> represents a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. see the scoring setting.\n", "\n", "<b>max_vocab_size</b> is the maximum size of the vocabulary. Used to control pruning of less common words, to keep memory under control. The default of 40M needs about 3.6GB of RAM; increase/decrease max_vocab_size depending on how much available memory you have.\n", "\n", "<b>delimiter</b> is the glue character used to join collocation tokens, and should be a byte string (e.g. b\u2019_\u2019).\n", "\n", "<b>scoring</b> specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n", "\n", "\u2018default\u2019: from \u201cEfficient Estimaton of Word Representations in Vector Space\u201d by\n", "Mikolov, et. al.: (count(worda followed by wordb) - min_count) * N / (count(worda) * count(wordb)) > threshold`, where <b>N</b> is the total vocabulary size.\n", "<b>npmi</b>: normalized pointwise mutual information, from \u201cNormalized (Pointwise) Mutual\n", "Information in Colocation Extraction\u201d by Gerlof Bouma: ln(prop(worda followed by wordb) / (prop(worda)*prop(wordb))) / - ln(prop(worda followed by wordb) where prop(n) is the count of n / the count of everything in the entire corpus.\n", "\u2018npmi\u2019 is more robust when dealing with common words that form part of common bigrams, and ranges from -1 to 1, but is slower to calculate than the default.\n", "\n", "To use a custom scoring function, create a function with the following parameters and set the scoring parameter to the custom function. You must use all the parameters in your function call, even if the function does not require all the parameters.\n", "\n", "<b>worda_count</b>: number of occurrances in sentences of the first token in the phrase being scored wordb_count: number of occurrances in sentences of the second token in the phrase being scored bigram_count: number of occurrances in sentences of the phrase being scored len_vocab: the number of unique tokens in sentences min_count: the min_count setting of the Phrases class corpus_word_count: the total number of (non-unique) tokens in sentences\n", "A scoring function without any of these parameters (even if the parameters are not used) will raise a ValueError on initialization of the Phrases class. The scoring function must be picklable.\n", "\n", "common_terms is an optionnal list of \u201cstop words\u201d that won\u2019t affect frequency count of expressions containing them."], "cell_type": "markdown", "metadata": {"_cell_guid": "ee3084da-f8e7-46d8-aa97-85d30d88a465", "_uuid": "f28e2d424b98b84d4388f6fd5f78021499112e7e"}}, {"source": ["class FileToComments(object):    \n", "    def __init__(self, filename):\n", "        self.filename = filename\n", "        self.stop = set(nltk.corpus.stopwords.words('english'))\n", "        \n", "    def __iter__(self):\n", "        \n", "        def comment_to_wordlist(comment, remove_stopwords=True):\n", "            comment = str(comment)\n", "            words = comment.lower().split()\n", "            #if remove_stopwords:\n", "            #    stops = set(stopwords.words(\"english\"))\n", "            #    words = [w for w in words if not w in stops]\n", "            return(words)\n", "    \n", "        for line in open(self.filename, 'r'):\n", "            #line = unicode(line, 'utf-8')\n", "            tokenized_comment = comment_to_wordlist(line, tokenizer)\n", "            yield tokenized_comment\n", "        \n", "all_comments = FileToComments('all_comments.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "07d70257-eff0-449c-9b35-b9e62a879660", "_uuid": "17119a8b789fb5a69200f90a7eee8a08a4d67987", "collapsed": true}, "execution_count": 10}, {"source": ["from gensim.models import Phrases\n", "from gensim.models.phrases import Phraser\n", "\n", "# Train Tokenizer on all comments\n", "bigram = Phrases(all_comments, min_count=30, threshold=15)\n", "bigram_phraser = Phraser(bigram) "], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "3c5be76c-780d-4cc9-a3f6-ed0ca9a179bb", "_uuid": "9418ad7911cba6c17fd11ee5e26765422b229cfc", "collapsed": true}, "execution_count": 11}, {"source": ["all_tokens = [bigram_phraser[comment] for comment in all_comments]\n", "\n", "stops = set(stopwords.words(\"english\"))\n", "\n", "clean_all_tokens = []\n", "for token in all_tokens:\n", "    words = [w for w in token if not w in stops]\n", "    clean_all_tokens += [words]\n", "print('tokens cleaned')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d06e0897-312d-4e9f-9e1e-2ad26789a1d5", "_uuid": "53f86ffbf719f656d404947c9584fd93a54517e8"}, "execution_count": 12}, {"source": ["#Pickle the tokens file for further use\n", "import pickle\n", "with open('tokenized_all_comments.pickle', 'wb') as filename:\n", "    pickle.dump(clean_all_tokens, filename, protocol=pickle.HIGHEST_PROTOCOL)\n", "print('files saved to tokenized_all_comments.pickle...')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b36438d2-b5a9-4856-98a0-a2fdb43c1c74", "_uuid": "4aceeff44c39d52c3e73724d4eea04b81763fd4c"}, "execution_count": 13}, {"source": ["##### 2. Training and Saving Your Model\n", "\n", "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n", "\n", " - Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results. \n", " - Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n", " - Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n", " - Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300. \n", " - Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n", " - Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n", " - Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."], "cell_type": "markdown", "metadata": {"_cell_guid": "90dac04f-c579-4f6a-8999-62d56a8fc32c", "_uuid": "4e2bdd3f5cf0b1bb9811a3a3e0bb5ad3e7309333"}}, {"source": ["#Load Pre-saved tokenized comments\n", "with open('tokenized_all_comments.pickle', 'rb') as filename:\n", "    all_comments = pickle.load(filename)\n", "    \n", "import logging\n", "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n", "    level=logging.INFO)\n", "\n", "\n", "# Set values for various parameters\n", "num_features = 300    # Word vector dimensionality                      \n", "min_word_count = 20   # Minimum word count                        \n", "num_workers = 16       # Number of threads to run in parallel\n", "context = 10          # Context window size                                                                                    \n", "downsampling = 1e-3   # Downsample setting for frequent words\n", "\n", "# Initialize and train the model (this will take some time)\n", "print(\"Training model...\")\n", "model = word2vec.Word2Vec(all_comments,\n", "                          workers=num_workers,\n", "                          size=num_features,\n", "                          min_count = min_word_count,\n", "                          window = context,\n", "                          sample = downsampling\n", "                         )\n", "\n", "# init_sims will make the model much more memory-efficient.\n", "model.init_sims(replace=True)\n", "model_name = \"%sfeatures_%sminwords_%scontext\" % (num_features,min_word_count,context)\n", "model.save(model_name)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4ae63767-02f3-435d-9c09-000929cd57c6", "_uuid": "642d6c8cf8aeeb0f647009ffe78e7996d384ca02"}, "execution_count": 15}, {"source": ["# You can load the model later using this:\n", "#from gensim.models import Word2Vec\n", "#import gensim\n", "#w2v_model = Word2Vec.load(\"300features_20minwords_10context\")\n", "\n", "# You can also retrain existing models by loading the features and retraining\n", "# I'll probably publish another iteration in the next few days"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "40ddcf1c-9ba0-4873-a34e-b1d1acb33866", "_uuid": "52fb326921b922372f3e5bd9ffc54557ff0df3d2", "collapsed": true}, "execution_count": 16}], "metadata": {"language_info": {"version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "celltoolbar": "Raw Cell Format"}, "nbformat": 4, "nbformat_minor": 1}