{"cells":[{"metadata":{"_uuid":"348c2ead0c41304335ec687c44023d76d103f404","_cell_guid":"281a03a4-98a9-49f6-bb79-5970123c295e"},"cell_type":"markdown","source":"# Introduction\n\n![Porto Seguro Auto](https://segurodecarroaqui.com.br/wp-content/uploads/2017/12/sulamerica-seguro-auto.png)\n\nThis notebook starts by giving an introduction in the data of Porto Seguro competition.  Then follows with preparing and running few predictive models using cross-validation and stacking and prepares a submission.\n\nThe notebook is using elements from the following kernels:\n* [Data Preparation and Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration) by Bert Carremans.  \n* [Steering Whell of Fortune - Porto Seguro EDA](https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda) by Heads or Tails  \n* [Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial) by Anisotropic \n* [Simple Stacker](https://www.kaggle.com/yekenot/simple-stacker-lb-0-284) by Vladimir Demidov\n\n\n"},{"metadata":{"_uuid":"f77f38e3d4d878611ad2c49e111c1ded2696abef","_cell_guid":"e2069ed3-e22c-4eb1-977f-406558d69e52"},"cell_type":"markdown","source":"# Analysis packages"},{"metadata":{"_uuid":"1b3f8b74c22665be2e9ac568cd8620b66b4ca1b3","collapsed":true,"_cell_guid":"2002583d-0c13-4c63-bb58-635f08aafb3b","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\npd.set_option('display.max_columns', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2896d3c810f445543ffc48dc8f08375d31149a8","_cell_guid":"946c458b-1655-47f8-952c-42e373c7c8a3"},"cell_type":"markdown","source":"# Load the data"},{"metadata":{"_uuid":"a78ec3de0190d7c48e99a68c9c646b46928c9ca6","collapsed":true,"_cell_guid":"57e70e54-ebb3-407f-b261-6661d330e32d","trusted":false},"cell_type":"code","source":"trainset = pd.read_csv('../input/train.csv')\ntestset = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c8441625620cc8b27c03f866a9b05c9919bc8a1","_cell_guid":"287c6b5e-85f6-48a5-91e5-b8b5a835cb94"},"cell_type":"markdown","source":"# Few quick observations"},{"metadata":{"_uuid":"bf40ffaa895f6ff2f53a8a87bb4064a9e3a3ae25","_cell_guid":"7c7c03ee-0491-4024-b6e5-04edacc5a457"},"cell_type":"markdown","source":"We can make few observations based on the data description in the competition:\n* Few **groups** are defined and features that belongs to these groups include patterns in the name (ind, reg, car, calc). The **ind** indicates most probably **individual**, **reg** is probably **registration**, **car** is self-explanatory, **calc** suggests a **calculated** field;\n* The postfix **bin** is used for binary features; \n* The postfix **cat** to  is used for categorical features;\n* Features without the **bin** or **cat** indications are real numbers (continous values) of integers (ordinal values);\n* A missing value is indicated by **-1**;\n* The value that is subject of prediction is in the **target** column. This one indicates whether or not a claim was filed for that insured person;\n* **id** is a data input ordinal number.\n\nLet's glimpse the data to see if these interpretations are confirmed."},{"metadata":{"_uuid":"6ecb79c2be112caa8231392d5bf746bb7ca2886b","collapsed":true,"_cell_guid":"9e3121d2-8e0f-4f4b-ac8e-fa95e2e1d7a4","trusted":false},"cell_type":"code","source":"trainset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04f5a68b951bb01c652e807b34596d99f8b5a647","_cell_guid":"056ae6b5-ad53-403e-ab66-1ad838113b6a"},"cell_type":"markdown","source":"Indeed, we can observe the **cat** values are **categorical**, integer values ranging from **0** to **n**, **bin** values are **binary** (either 0 or 1)."},{"metadata":{"_uuid":"75276e84c660bdf6a2de193407113e265953873f","_cell_guid":"5427d7a1-fa93-47e8-86a5-a088597776af"},"cell_type":"markdown","source":"Let's see how many rows and columns are in the data."},{"metadata":{"_uuid":"b850d47917d88e1268ae9eacb7a1d3335f090246","collapsed":true,"_cell_guid":"d5da0528-46c0-4c2d-9d9b-9383500d0b25","trusted":false},"cell_type":"code","source":"print(\"Train dataset (rows, cols):\",trainset.shape, \"\\nTest dataset (rows, cols):\",testset.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63a9c1318b4d690c39bf9b566d1e992d79768719","_cell_guid":"537fbd44-8c4b-42f0-b1b7-333a9dfe93e6"},"cell_type":"markdown","source":"There are *59* columns in the training dataset and only *58* in the testing dataset. Since from this dataset should have been extracted the **target**, this seems fine. Let's check the difference between the columns set in the two datasets, to make sure everything is fine."},{"metadata":{"_uuid":"90fe9110e5d4693832c31e868f2ea12ad36ca6d7","collapsed":true,"_cell_guid":"ddf97e4c-196d-4490-8ccf-adbb92584dab","trusted":false},"cell_type":"code","source":"print(\"Columns in train and not in test dataset:\",set(trainset.columns)-set(testset.columns))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54c629fab5ecb824249d802f3043dc8b8217dac","_cell_guid":"176738ba-7ce2-46bf-8f25-84d610a1fa40"},"cell_type":"markdown","source":"# Introduction of metadata\n\nTo make easier the manipulation of data, we will associate few meta-information to the variables in the trainset. This will facilitate the selection of various types of features for analysis, inspection or modeling. We are using as well a **category** field for the `car`, `ind`, `reg` and `calc` types of features.\n\nWhat metadata will be used:\n\n* **use**: input, ID, target\n* **type**: nominal, interval, ordinal, binary\n* **preserve**: True or False\n* **dataType**: int, float, char\n* **category**: ind, reg, car, calc   \n"},{"metadata":{"_uuid":"a74e77e3ba71564b95c04b4259718da43bc1af41","collapsed":true,"_cell_guid":"76340237-9235-431c-beae-29fc54387c24","trusted":false},"cell_type":"code","source":"# uses code from https://www.kaggle.com/bertcarremans/data-preparation-exploration (see references)\ndata = []\nfor feature in trainset.columns:\n    # Defining the role\n    if feature == 'target':\n        use = 'target'\n    elif feature == 'id':\n        use = 'id'\n    else:\n        use = 'input'\n         \n    # Defining the type\n    if 'bin' in feature or feature == 'target':\n        type = 'binary'\n    elif 'cat' in feature or feature == 'id':\n        type = 'categorical'\n    elif trainset[feature].dtype == float or isinstance(trainset[feature].dtype, float):\n        type = 'real'\n    elif trainset[feature].dtype == int:\n        type = 'integer'\n        \n    # Initialize preserve to True for all variables except for id\n    preserve = True\n    if feature == 'id':\n        preserve = False\n    \n    # Defining the data type \n    dtype = trainset[feature].dtype\n    \n    category = 'none'\n    # Defining the category\n    if 'ind' in feature:\n        category = 'individual'\n    elif 'reg' in feature:\n        category = 'registration'\n    elif 'car' in feature:\n        category = 'car'\n    elif 'calc' in feature:\n        category = 'calculated'\n    \n    \n    # Creating a Dict that contains all the metadata for the variable\n    feature_dictionary = {\n        'varname': feature,\n        'use': use,\n        'type': type,\n        'preserve': preserve,\n        'dtype': dtype,\n        'category' : category\n    }\n    data.append(feature_dictionary)\n    \nmetadata = pd.DataFrame(data, columns=['varname', 'use', 'type', 'preserve', 'dtype', 'category'])\nmetadata.set_index('varname', inplace=True)\nmetadata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb27c30ad6fde2ac184f75f9a518953f40616d4d","_cell_guid":"bd1afe89-4a5d-49d8-b915-c8a565f65cd4"},"cell_type":"markdown","source":"We can extract, for example, all categorical values:"},{"metadata":{"_uuid":"5b5e255606f8843ec78e5d728ef59d4ed1375c39","collapsed":true,"_cell_guid":"e3004eb2-5e6e-4a5c-b518-66ea1f2e1f8a","trusted":false},"cell_type":"code","source":"metadata[(metadata.type == 'categorical') & (metadata.preserve)].index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b71e5de505748ec6bcc7ff657f055c1febab84ba","collapsed":true,"_cell_guid":"47c5a5be-5d67-4351-9aac-80ac88319cf4"},"cell_type":"markdown","source":"Let's inspect all features, to see how many category distinct values do we have:"},{"metadata":{"_uuid":"b0a7e7bcd4c3bcbacdbe9d27445311bca98f419a","collapsed":true,"_cell_guid":"0c3e3216-e9e1-469e-8fd6-7c352c506e6d","trusted":false},"cell_type":"code","source":"pd.DataFrame({'count' : metadata.groupby(['category'])['category'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86dde8143ee93379a0ce921590755457bb52ff01","_cell_guid":"21f23cf0-02a3-44df-9e50-9b6eada7bf3a"},"cell_type":"markdown","source":"We have 20 *calculated* features, 16 *car*, 18 *individual* and 3 *registration*.\n\nLet's inspect now all features, to see how many use and type distinct values do we have:"},{"metadata":{"_uuid":"eecafce6c72d3af958dd5046164ff3a89d873173","collapsed":true,"_cell_guid":"92c52e5e-d0b0-441f-8d8a-1a249a07d83b","trusted":false},"cell_type":"code","source":"pd.DataFrame({'count' : metadata.groupby(['use', 'type'])['use'].size()}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"821383e932bfcb340c47e79ddc888c80f3dda959","_cell_guid":"aa2394f9-85b5-43ed-80a4-b45683158830"},"cell_type":"markdown","source":"There are one nominal feature (the **id**), 20 binary values, 21 real (or float numbers), 16 categorical features - all these being as well **input** values and one **target** value, which is as well **binary**, the **target**."},{"metadata":{"_uuid":"bfb5b557d2ad7980a671988b1dabd317b2814cdf","_cell_guid":"fd7d33c5-bedd-443c-901f-829ec684b6b6"},"cell_type":"markdown","source":"# Data analysis and statistics\n\n"},{"metadata":{"_uuid":"2a3404c492850517b19e306438833aedbba6a3f0","_cell_guid":"4848bab7-d74c-49cd-a082-58c94942f10d"},"cell_type":"markdown","source":"## Target variable"},{"metadata":{"_uuid":"0669d32402c88c936a2bdb30689a3d53b3dc4632","collapsed":true,"_cell_guid":"c83a95cc-8ace-4fb9-aef1-70c4dd9ffec5","trusted":false},"cell_type":"code","source":"plt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\nx = trainset['target'].value_counts().index.values\ny = trainset[\"target\"].value_counts().values\n# Bar plot\n# Order the bars descending on target mean\nsns.barplot(ax=ax, x=x, y=y)\nplt.ylabel('Number of values', fontsize=12)\nplt.xlabel('Target value', fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81f0868f6062bff8b9557937b0721fac9d470d6f","_cell_guid":"37d9bca2-9760-4d9c-b313-9cc389028a43"},"cell_type":"markdown","source":"Only 3.64% of the target data have 1 value. This means that the training dataset is highly imbalanced. We can either undersample the records with target = 0 or oversample records with target = 1; because is a large dataset, we will do undersampling of records with target = 0."},{"metadata":{"_uuid":"a4890a90d955ab7691cc27f53f5caa6a695f573c","_cell_guid":"d2820148-b020-4770-8327-e7cb018e6345"},"cell_type":"markdown","source":"## Real features"},{"metadata":{"_uuid":"cf15f00d2feb3aef38fdc256affa1392c617607d","collapsed":true,"_cell_guid":"683174d3-2bb3-4441-bfcf-dc04774d9bc4","trusted":false},"cell_type":"code","source":"variable = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ntrainset[variable].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c24b5d308982f82c42342765d9cb8d089e98e4c","collapsed":true,"_cell_guid":"bc195f0d-c5dc-4799-9e96-1ffd72ceb241","trusted":false},"cell_type":"code","source":"(pow(trainset['ps_car_12']*10,2)).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2492a1733b9496502319f45b4825cc86efa58981","collapsed":true,"_cell_guid":"5b00ba93-877d-48d9-9c4b-e4a1b1658665","trusted":false},"cell_type":"code","source":"(pow(trainset['ps_car_15'],2)).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da32db42746c811c73eeb7bbb4987f21034052dc","_cell_guid":"af45a0bf-7801-4f36-a563-19c04a8729f2"},"cell_type":"markdown","source":"### Features with missing values\n\n**ps_reg_o3**, **ps_car_12**, **ps_car_14** have missing values (their minimum value is -1)\n\n\n### Registration features\n\n**ps_reg_01** and **ps_reg_02** are fractions with denominator 10 (values of 0.1, 0.2, 0.3 )\n\n### Car features\n\n**ps_car_12** are (with some approximations) square roots (divided by 10) of natural numbers whilst **ps_car_15** are square roots of natural numbers. Let's represent the values using *pairplot*.\n\n\n"},{"metadata":{"_uuid":"7cc1083fdef3e0287911e09b046835bc5ab791fb","collapsed":true,"_cell_guid":"82ef086c-edf1-4d8a-a021-b7281c55f4aa","trusted":false},"cell_type":"code","source":"sample = trainset.sample(frac=0.05)\nvar = ['ps_car_12', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample,  hue='target', palette = 'Set1', diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a01694c9a27c0179fce7fda2e59e97be10ae7ec","_cell_guid":"31d21091-e87c-4891-8369-e8266a43087a"},"cell_type":"markdown","source":"### Calculated features\n\nThe features **ps_calc_01**, **ps_calc_02** and **ps_calc_03** have very similar distributions and could be some kind of ratio, since the maximum value is for all three 0.9. The other calculated values have maximum value an integer value (5,6,7, 10,12). "},{"metadata":{"_uuid":"dd7fc9658cc496bd8f4f0fea7cab8cee6a26fc14","_cell_guid":"91dfb070-5fd2-45c8-b71b-7476e6ece8b8"},"cell_type":"markdown","source":"Let's visualize the real features distribution using density plot."},{"metadata":{"_uuid":"aad7cee8ea090aec13dbdf2af8cab2a82c95971b","collapsed":true,"_cell_guid":"5a1019ce-77ec-439c-944d-f1a671c5ce61","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(3,4,figsize=(16,12))\n\nfor feature in var:\n    i += 1\n    plt.subplot(3,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0348424ff1dac6c8f4ecc3d39e172c155cab407a","_cell_guid":"4845f470-de3e-463e-8e72-7abe220a64bb"},"cell_type":"markdown","source":"**ps_reg_02**, **ps_car_13**, **ps_car_15** shows the most different distributions between sets of values associated with `target=0` and `target=1`."},{"metadata":{"_uuid":"46fb13733beb908f685154a433a393ddc86d6e15","_cell_guid":"7b6baf57-ef52-4d60-8104-3950d0acac91"},"cell_type":"markdown","source":"Let's visualize the correlation between the real features"},{"metadata":{"_uuid":"bb24dfeaef64fad2d8407a48431ff35cc9ea0a08","collapsed":true,"_cell_guid":"e7b85dc7-6c90-48b2-8cf2-f90bf285c3bb","trusted":false},"cell_type":"code","source":"def corr_heatmap(var):\n    correlations = trainset[var].corr()\n\n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(50, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nvar = metadata[(metadata.type == 'real') & (metadata.preserve)].index\ncorr_heatmap(var)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a98ab723c30b3d14711f3105874acb9573e01fd","_cell_guid":"5b02530c-3bca-44ac-886f-f72037ae51ad"},"cell_type":"markdown","source":"Let's visualize the plots of the variables with strong correlations. These are:\n\n* ps_reg_01 with ps_reg_02 (0.47);  \n* ps_reg_01 with ps_reg_03 (0.64);  \n* ps_reg_02 with ps_reg_03 (0.52);  \n* ps_car_12 with ps_car_13 (0.67);  \n* ps_car_13 with ps_car_15 (0.53);  \n\n\nTo show the pairs of values that are correlated we use *pairplot*. Before representing the pairs, we subsample the data, using only 2% in the sample.\n\n"},{"metadata":{"_uuid":"6140e1bdb65e209421d60b897a2d0572b77501b9","collapsed":true,"_cell_guid":"729d6ac8-0df8-4b93-b8ae-b0baf6cdc108","trusted":false},"cell_type":"code","source":"sample = trainset.sample(frac=0.05)\nvar = ['ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_15', 'target']\nsample = sample[var]\nsns.pairplot(sample,  hue='target', palette = 'Set1', diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"369fcd5ed04de0664a42ac5e89588ff807a6450c","_cell_guid":"4e885d11-4a6f-4e71-b9da-5c43e0e37966"},"cell_type":"markdown","source":"# Binary features\n\n"},{"metadata":{"_uuid":"83ccf6267436e00f023b996cb869f14f18969ea5","collapsed":true,"_cell_guid":"8cd41d5d-58e9-429b-8392-f4c1970a6053","trusted":false},"cell_type":"code","source":"\nv = metadata[(metadata.type == 'binary') & (metadata.preserve)].index\ntrainset[v].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6703c00bf3f6e82d31f42f5dae7dc2fede262e05","_cell_guid":"c6e7af56-9d44-4387-b128-ac0de3a28989"},"cell_type":"markdown","source":"Let's plot the distribution of the binary data in the training dataset. With `blue` we represent the percent of `0` and with `red` the percent of `1`."},{"metadata":{"scrolled":true,"_uuid":"8964eb20fcc75ae90230de8edc635bc9d36b4465","collapsed":true,"_cell_guid":"8f68458a-cb2d-4701-b253-7e64744df992","trusted":false},"cell_type":"code","source":"bin_col = [col for col in trainset.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((trainset[col]==0).sum()/trainset.shape[0]*100)\n    one_list.append((trainset[col]==1).sum()/trainset.shape[0]*100)\nplt.figure()\nfig, ax = plt.subplots(figsize=(6,6))\n# Bar plot\np1 = sns.barplot(ax=ax, x=bin_col, y=zero_list, color=\"blue\")\np2 = sns.barplot(ax=ax, x=bin_col, y=one_list, bottom= zero_list, color=\"red\")\nplt.ylabel('Percent of zero/one [%]', fontsize=12)\nplt.xlabel('Binary features', fontsize=12)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.legend((p1, p2), ('Zero', 'One'))\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0f98992db6f2e68ec61d506e96a24b9cb96e772","_cell_guid":"1613c9da-025e-40ba-924e-268081c3b6d6"},"cell_type":"markdown","source":"**ps_ind_10_bin**, **ps_ind_11_bin**, **ps_ind_12_bin** and **ps_ind_13_bin** have very small number of  values `1` (lesss than 0.5%) whilst the number of  value `1` is very large for **ps_ind_16_bin** and **ps_cals_16_bin** (more than 60%).\n\nLet's see now the distribution of binary data and the corresponding values of **target** variable.\n"},{"metadata":{"_uuid":"106a0845d515f7398c3c48d2b7c2f4214d4f801d","collapsed":true,"_cell_guid":"8a82720b-5c7d-482d-82f1-36c772de4c73","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.type == 'binary') & (metadata.preserve)].index\nvar = [col for col in trainset.columns if '_bin' in col]\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6,3,figsize=(12,24))\n\nfor feature in var:\n    i += 1\n    plt.subplot(6,3,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce4951a2b3c78ca142f5a9e30d035c01d35d5154","_cell_guid":"a9f0f20a-ffa4-4d1c-8c04-17a1182240c0"},"cell_type":"markdown","source":"**ps_ind_06_bin**, **ps_ind_07_bin**, **ps_ind_16_bin**, **ps_ind_17_bin**  shows high inbalance between distribution of values of `1` and `0` for values of target equals with `1` and `0`, **ps_ind_08_bin** shows a small inbalance while the other features are well balanced, having similar density plots."},{"metadata":{"_uuid":"81928253a93239c3bcb6d1f92ee73b9a87313855","_cell_guid":"68fe0609-a160-43ae-abf2-3217e3bcd70a"},"cell_type":"markdown","source":"## Categorical features"},{"metadata":{"_uuid":"b1f731d358ecdff19fe68dd5292eac52639bba40","_cell_guid":"9e50138b-cf57-4da2-b4f9-2bea91ef4a67"},"cell_type":"markdown","source":"We will represent the distribution on `categorical` data in two ways. \nFirst, we calculate the percentage of `target=1` per category value and represent these percentages\nusing bar plots."},{"metadata":{"_uuid":"53c7c0ddd528e0c82ce91f24668f764323685607","collapsed":true,"_cell_guid":"6c9280f3-73b2-4b1a-8882-a7d34e3f12b8","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\n\nfor feature in var:\n    fig, ax = plt.subplots(figsize=(6,6))\n    # Calculate the percentage of target=1 per category value\n    cat_perc = trainset[[feature, 'target']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax,x=feature, y='target', data=cat_perc, order=cat_perc[feature])\n    plt.ylabel('Percent of target with value 1 [%]', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"387dde4554b73211b2f5e8f4a4934a77e291e656","_cell_guid":"0deb11fa-64ca-418d-96fb-a0a7bfed6fc0"},"cell_type":"markdown","source":"Alternativelly we represent the `categorical` features using density plot. We select values with `target=0` and `target=1` and represent both density plots on the same graphic."},{"metadata":{"_uuid":"2518f28a3a0dc7d5dea734b2ccf9dcb280f9e295","collapsed":true,"_cell_guid":"fd82daf0-61d2-429a-9535-6b0b78baf714","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.type == 'categorical') & (metadata.preserve)].index\ni = 0\nt1 = trainset.loc[trainset['target'] != 0]\nt0 = trainset.loc[trainset['target'] == 0]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(4,4,figsize=(16,16))\n\nfor feature in var:\n    i += 1\n    plt.subplot(4,4,i)\n    sns.kdeplot(t1[feature], bw=0.5,label=\"target = 1\")\n    sns.kdeplot(t0[feature], bw=0.5,label=\"target = 0\")\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b7b5aad6d6cf3771cfd2040199679096574f784","_cell_guid":"39899e6b-bacf-4ae8-b319-cc8fd88baff8"},"cell_type":"markdown","source":"**ps_car_03_cat**, **ps_car_05_cat** shows the most different density plot between values associated with `target=0` and `target=1`."},{"metadata":{"_uuid":"0bd4befb2add8a1c7c6dfda7947f4c64609496c7","_cell_guid":"a3133bea-39da-4b0f-9595-c6a64deec78d"},"cell_type":"markdown","source":"## Data unbalance between train and test data "},{"metadata":{"_uuid":"7695c76efb3b8e1f7082119f3f617266816b3a22","_cell_guid":"8091bb27-9c58-4a56-94fa-22171b3b1184"},"cell_type":"markdown","source":"Let's compare the distribution of the features in the train and test datasets. \n\nWe start with the `reg` or `registration` features."},{"metadata":{"_uuid":"1d2e10e4270f8f865fac91482c18230f5d14bdda","collapsed":true,"_cell_guid":"04e3a4d4-88c0-420d-af61-3fa12a9382da","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.category == 'registration') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(1,3,figsize=(12,4))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(1,3,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4067cd5e36b169b34498b3944362f09dafdf45de","_cell_guid":"d4c3f531-7d15-491b-86a6-e81096c5627a"},"cell_type":"markdown","source":"All `reg` features shows well balanced train and test sets.\n\nLet's continue with `car` features."},{"metadata":{"_uuid":"d48fd90ec55c1d4ab8976b08e5869401d3adb8b9","collapsed":true,"_cell_guid":"e7e91f91-ab96-40f8-8b84-5eda41b315fe","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.category == 'car') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(4,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(4,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a39b537ee42d38f26a006ad0bc44a08d88ee389","_cell_guid":"fa84df52-5eda-45ff-b70f-2c9ac3535d57"},"cell_type":"markdown","source":"From the `car` features, all variables looks well balanced between `train` and `test` set.\n\nLet's look now to the `ind` (`individual`) values."},{"metadata":{"_uuid":"bcc9bb5f10b38769337a081d8a26955377116fad","collapsed":true,"_cell_guid":"87f5fa40-f03c-4414-842a-2f3cf60a4004","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.category == 'individual') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8201f4c4e408b78c89292665c8e34fcc6ceaa63d","_cell_guid":"1e6cb5d9-dab6-4f3c-80a5-e1d032766d96"},"cell_type":"markdown","source":"All `ind` features are well balanced between `train` and `test` sets.\n\nLet's check now `calc` features."},{"metadata":{"_uuid":"4547a5a7a65a29e81c76ef483f3c5c5fef862e10","collapsed":true,"_cell_guid":"4cdcabf9-c3cf-488c-a586-180e09ae91c7","trusted":false},"cell_type":"code","source":"var = metadata[(metadata.category == 'calculated') & (metadata.preserve)].index\n\n# Bar plot\nsns.set_style('whitegrid')\n\nplt.figure()\nfig, ax = plt.subplots(5,4,figsize=(20,16))\ni = 0\nfor feature in var:\n    i = i + 1\n    plt.subplot(5,4,i)\n    sns.kdeplot(trainset[feature], bw=0.5, label=\"train\")\n    sns.kdeplot(testset[feature], bw=0.5, label=\"test\")\n    plt.ylabel('Distribution', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    #plt.setp(labels, rotation=90)\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e96b1efc9c2a84ff2014294a111fdebc98dcadb","_cell_guid":"06051044-3726-4008-b42d-1e8f10659390"},"cell_type":"markdown","source":"All `calc` features are well balanced between `train` and `test` sets. \n\nIn reference [5] it is also noticed the well balancing between `train` and `test` sets. It is also suggested that `calc` features might be all engineered and actually not relevant. This can only be assesed by careful succesive elimination using `CV` score using one or more predictive models.\n\n"},{"metadata":{"_uuid":"bd6769994890dd423540e196fb053dab8eafbc68","_cell_guid":"4df3e25e-3fa7-409d-8c5d-c61bf9959720"},"cell_type":"markdown","source":"# Check data quality"},{"metadata":{"_uuid":"5eff84cf0b4dfedf5b10b6c780412144e9e324bc","_cell_guid":"bf3e0be4-f3bf-4119-a568-3764da955f8e"},"cell_type":"markdown","source":"Let's inspect the features with missing values:"},{"metadata":{"_uuid":"3b421623b4455e6a2f489437d3dc1be6fadf5e5d","collapsed":true,"_cell_guid":"51c19a83-3785-46c8-a80d-b04cc2d4788f","trusted":false},"cell_type":"code","source":"vars_with_missing = []\n\nfor feature in trainset.columns:\n    missings = trainset[trainset[feature] == -1][feature].count()\n    if missings > 0:\n        vars_with_missing.append(feature)\n        missings_perc = missings/trainset.shape[0]\n        \n        print('Variable {} has {} records ({:.2%}) with missing values'.format(feature, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5caf108c2ee0a505e50a25634666da43842847b","_cell_guid":"57aa1a21-234e-4b98-a68d-ffa7ec607788"},"cell_type":"markdown","source":"# Prepare the data for model\n\n\n\n"},{"metadata":{"_uuid":"fd2b17cf3c250223b32360cfa0e33ddafb2a31ce","_cell_guid":"4ef17478-27a4-4a04-ae37-437958773f29"},"cell_type":"markdown","source":"### Drop **calc** columns\n\nWe also drop the **calc** columns, as recommended in [5]. These seems to be all engineered and, according to Dmitry Altukhov, he was able to improve his CV score while succesivelly removing all of them.\n"},{"metadata":{"_uuid":"939c5e2d8dca9ec605a192a4f4d9015335c18b2d","collapsed":true,"_cell_guid":"2b52688f-5c83-4c91-b1df-d660b6cf25b7","trusted":false},"cell_type":"code","source":"col_to_drop = trainset.columns[trainset.columns.str.startswith('ps_calc_')]\ntrainset = trainset.drop(col_to_drop, axis=1)  \ntestset = testset.drop(col_to_drop, axis=1)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64b4da3ac6bf0ba9c93802f9d409ce79fef3b269","_cell_guid":"15969c98-291e-4bfd-b75a-c6de137d6b96"},"cell_type":"markdown","source":"### Drop variables with too many missing values\n\nWe select from the variables with missing values two, **ps_car_03_cat** and **ps_car_05_cat** to drop."},{"metadata":{"_uuid":"0ef1518c4eec07ce7b42011def0f669420dc7bc5","collapsed":true,"_cell_guid":"71c11853-10c7-4469-945e-fc4255d5205a","trusted":false},"cell_type":"code","source":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrainset.drop(vars_to_drop, inplace=True, axis=1)\ntestset.drop(vars_to_drop, inplace=True, axis=1)\nmetadata.loc[(vars_to_drop),'keep'] = False  # Updating the meta\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e2745f2e45449a0c1b9016fb4e8aa3a388c8924","collapsed":true,"_cell_guid":"2ee5880c-249c-4537-945f-5180b391f1e4","trusted":false},"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16e39f39b25595572bce12cdcc41fcb52e15d527","_cell_guid":"a85a4770-18b3-473e-aee0-383d833a0fc5"},"cell_type":"markdown","source":"### Replace ps_car_11_cat with encoded value\n\nUsing the **target_encode** function, we replace the **ps_car_11_cat** with an encoded value in both **train** and **test** datasets.\n\n"},{"metadata":{"_uuid":"175b635f34f67d0f75e9ec5b828535c866613b3e","collapsed":true,"_cell_guid":"250e64ea-952f-4fe8-8272-3f5a2de5691a","trusted":false},"cell_type":"code","source":"train_encoded, test_encoded = target_encode(trainset[\"ps_car_11_cat\"], \n                             testset[\"ps_car_11_cat\"], \n                             target=trainset.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrainset['ps_car_11_cat_te'] = train_encoded\ntrainset.drop('ps_car_11_cat', axis=1, inplace=True)\nmetadata.loc['ps_car_11_cat','keep'] = False  # Updating the metadata\ntestset['ps_car_11_cat_te'] = test_encoded\ntestset.drop('ps_car_11_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"747308f6dfa9d264b9f42b859e7bbe95ecdf22b2","_cell_guid":"3b06fc74-f12f-4c3c-a720-7e3a391b81fd"},"cell_type":"markdown","source":"### Balance target variable\n\nThe target variable is highly unbalanced. This can be improved by either undersampling values with **target = 0** or oversampling values with **target = 1**.  Because there is a rather large training set, we opt for the **undersampling**."},{"metadata":{"_uuid":"8317086889f6f9f7b4925da9fd958d7fd9538628","collapsed":true,"_cell_guid":"fc2a8b3a-77ed-4f8e-a86b-122e74694a8b","trusted":false},"cell_type":"code","source":"desired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = trainset[trainset.target == 0].index\nidx_1 = trainset[trainset.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(trainset.loc[idx_0])\nnb_1 = len(trainset.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=314, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrainset = trainset.loc[idx_list].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9f14122ca585cb8137d36e8e19e9572a6fb233","_cell_guid":"b9e46d8d-3f5c-4624-9926-59f1c4fce8b2"},"cell_type":"markdown","source":"### Replace **-1** values with NaN\n\nMost of the classifiers we would use have preety good strategies to manage missing (or NaN) values.\n"},{"metadata":{"_uuid":"bed75bb4bbb924c0bea10be97a7d6aa0efb67485","collapsed":true,"_cell_guid":"8591cda0-bdf3-48ad-86b4-6ed606666765","trusted":false},"cell_type":"code","source":"trainset = trainset.replace(-1, np.nan)\ntestset = testset.replace(-1, np.nan)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6d1a3ee6eb86bc2f1b9a4b600a697ea9b51dc6","_cell_guid":"1048085d-fd5d-418d-8740-1a32abce4917"},"cell_type":"markdown","source":"### Dummify **cat** values\n\nWe will create dummy variables for the **categorical** (**cat**) features\n"},{"metadata":{"_uuid":"03942e0268b07f0c949a760e871f801f6d2a7a1a","collapsed":true,"_cell_guid":"270767bb-8e56-49d4-b5fe-15888b6b87ff","trusted":false},"cell_type":"code","source":"cat_features = [a for a in trainset.columns if a.endswith('cat')]\n\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(trainset[column]))\n    trainset = pd.concat([trainset,temp],axis=1)\n    trainset = trainset.drop([column],axis=1)\n    \nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(testset[column]))\n    testset = pd.concat([testset,temp],axis=1)\n    testset = testset.drop([column],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88905028245ab69533fc727824ba8c029589ef45","_cell_guid":"a7d27b8c-3886-469a-bb06-be20495fa44c"},"cell_type":"markdown","source":"### Drop unused and **target** columns\n\nWe separate the **id** and **target** (drop these columns)"},{"metadata":{"_uuid":"1cd1a99c96ea834238625c294d9a0bea47750e78","collapsed":true,"_cell_guid":"6ecb5d41-e926-46d7-8f4a-9936d74566f8","trusted":false},"cell_type":"code","source":"id_test = testset['id'].values\ntarget_train = trainset['target'].values\n\ntrainset = trainset.drop(['target','id'], axis = 1)\ntestset = testset.drop(['id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31fbb955ad41759dcd7a19534c547511a8728917","_cell_guid":"9227e987-14d7-4a31-b3c3-d01bd0095872"},"cell_type":"markdown","source":"Let's inspect the training and test sets:"},{"metadata":{"_uuid":"ccf6e4be97276f62a8661a203ccb6b43fe4d4004","collapsed":true,"_cell_guid":"adf6c595-1425-41ba-9d1a-4a6c886a70d4","trusted":false},"cell_type":"code","source":"print(\"Train dataset (rows, cols):\",trainset.values.shape, \"\\nTest dataset (rows, cols):\",testset.values.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f038aa47029147de282abd17d12596a171b52b0c","_cell_guid":"bb8b33c4-35a3-4f5b-8b3b-32464dc30311"},"cell_type":"markdown","source":"\n# Prepare the model\n\n### Ensable class for cross validation and ensamble\n\nPrepare an **Ensamble** class to split the data in KFolds, train the models and ensamble the results.\n\nThe class has an **init** method (called when an Ensamble object is created) that accepts 4 parameters:\n\n* **self** - the object to be initialized  \n* **n_splits** - the number of cross-validation splits to be used  \n* **stacker** - the model used for stacking the prediction results from the trained base models    \n* **base_models** - the list of base models used in training  \n\nA second method, **fit_predict** has four functions:\n* split the training data in **n_splits** folds;  \n* run the **base models** for each fold;  \n* perform prediction using each model;  \n* ensamble the resuls using the **stacker**;  \n\n\n"},{"metadata":{"_uuid":"bbd73b3f7292d947424b782bdd6024476211ce2c","collapsed":true,"_cell_guid":"35840d14-3dc7-4f4f-a1e5-4f1b4213e2fa","trusted":false},"cell_type":"code","source":"class Ensemble(object):\n    def __init__(self, n_splits, stacker, base_models):\n        self.n_splits = n_splits\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, X, y, T):\n        X = np.array(X)\n        y = np.array(y)\n        T = np.array(T)\n\n        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=314).split(X, y))\n\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n\n            S_test_i = np.zeros((T.shape[0], self.n_splits))\n\n            for j, (train_idx, test_idx) in enumerate(folds):\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                X_holdout = X[test_idx]\n\n\n                print (\"Base model %d: fit %s model | fold %d\" % (i+1, str(clf).split('(')[0], j+1))\n                clf.fit(X_train, y_train)\n                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n                print(\"cross_score [roc-auc]: %.5f [gini]: %.5f\" % (cross_score.mean(), 2*cross_score.mean()-1))\n                y_pred = clf.predict_proba(X_holdout)[:,1]                \n\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n            S_test[:, i] = S_test_i.mean(axis=1)\n\n        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n        # Calculate gini factor as 2 * AUC - 1\n        print(\"Stacker score [gini]: %.5f\" % (2 * results.mean() - 1))\n\n        self.stacker.fit(S_train, y)\n        res = self.stacker.predict_proba(S_test)[:,1]\n        return res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d9c8ae3c859c38f13b4c1c4745401bd04952678","_cell_guid":"9bd57ead-e374-4797-8e80-02f6dc3747fa"},"cell_type":"markdown","source":"### Parameters for the base models\n\nFor the base models, we prepare three different LightGBM models and one XGB model. \n\nEach model is used to train the data (using as well cross-validation, with 3 folds).\n"},{"metadata":{"_uuid":"df02467ca15c2f39e8833c3d80d2aa7b95dd156f","collapsed":true,"_cell_guid":"cc73616c-f10b-4ff5-a232-2af5603ec249","trusted":false},"cell_type":"code","source":"# LightGBM params\n# lgb_1\nlgb_params1 = {}\nlgb_params1['learning_rate'] = 0.02\nlgb_params1['n_estimators'] = 650\nlgb_params1['max_bin'] = 10\nlgb_params1['subsample'] = 0.8\nlgb_params1['subsample_freq'] = 10\nlgb_params1['colsample_bytree'] = 0.8   \nlgb_params1['min_child_samples'] = 500\nlgb_params1['seed'] = 314\nlgb_params1['num_threads'] = 4\n\n# lgb2\nlgb_params2 = {}\nlgb_params2['n_estimators'] = 1090\nlgb_params2['learning_rate'] = 0.02\nlgb_params2['colsample_bytree'] = 0.3   \nlgb_params2['subsample'] = 0.7\nlgb_params2['subsample_freq'] = 2\nlgb_params2['num_leaves'] = 16\nlgb_params2['seed'] = 314\nlgb_params2['num_threads'] = 4\n\n# lgb3\nlgb_params3 = {}\nlgb_params3['n_estimators'] = 1100\nlgb_params3['max_depth'] = 4\nlgb_params3['learning_rate'] = 0.02\nlgb_params3['seed'] = 314\nlgb_params3['num_threads'] = 4\n\n# XGBoost params\nxgb_params = {}\nxgb_params['objective'] = 'binary:logistic'\nxgb_params['learning_rate'] = 0.04\nxgb_params['n_estimators'] = 490\nxgb_params['max_depth'] = 4\nxgb_params['subsample'] = 0.9\nxgb_params['colsample_bytree'] = 0.9  \nxgb_params['min_child_weight'] = 10\nxgb_params['num_threads'] = 4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4097717d0122bec868112d875b1d02375be5b79c","_cell_guid":"b3d5f8a5-9620-4b95-94d7-b13d832d6dad"},"cell_type":"markdown","source":"### Initialize the models with the parameters\n\nWe init the 3 base models and the stacking model. For the base models we are using the predefined parameters initialized above.\n\n"},{"metadata":{"_uuid":"964f87923e83295c440608ee67cdfa835fab1f2a","collapsed":true,"_cell_guid":"e330a2c7-3290-45e1-93ec-46a2d25f1ea9","trusted":false},"cell_type":"code","source":"# Base models\nlgb_model1 = LGBMClassifier(**lgb_params1)\n\nlgb_model2 = LGBMClassifier(**lgb_params2)\n       \nlgb_model3 = LGBMClassifier(**lgb_params3)\n\nxgb_model = XGBClassifier(**xgb_params)\n\n# Stacking model\nlog_model = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a51e870d0794ba244642a0dbae52f1040bec5368","_cell_guid":"dcb23a45-aad9-4b3b-89f2-06a768d788af"},"cell_type":"markdown","source":"### Initialize the ensambling object\n\nUsing Ensamble.init we init the stacking object\n"},{"metadata":{"_uuid":"2173ced2d97a9db2923150839cb27d039950b49f","collapsed":true,"_cell_guid":"042c664a-3ca5-49c5-9e86-e3f8a3ce5b14","trusted":false},"cell_type":"code","source":"stack = Ensemble(n_splits=3,\n        stacker = log_model,\n        base_models = (lgb_model1, lgb_model2, lgb_model3, xgb_model))  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1423c1f9622afc1e3cd0d2b304ba0a511180753","_cell_guid":"8485cdb2-e917-4255-b5d8-e771e24b5f02"},"cell_type":"markdown","source":"# Run the predictive models\n\n\nCalling the **fit_predict** method of **stack** object, we run the training of the base models, predict the **target** with each model, ensamble the results using the **stacker** model and output the stacked result.\n"},{"metadata":{"_uuid":"ce1e89856cd164aac9043f3a2242b918bc9163b6","collapsed":true,"_cell_guid":"10d633bf-a873-4e50-ab91-8bcb79445379","trusted":false},"cell_type":"code","source":"y_prediction = stack.fit_predict(trainset, target_train, testset)        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ef3d5ef59d32e14b4fb00d550e53290fdb9944","_cell_guid":"5406f24d-4e6d-4837-832b-f832b347a5fd"},"cell_type":"markdown","source":"# Prepare the submission\n"},{"metadata":{"_uuid":"72272c6b6a73a07b5618c8367163de393abc4b11","collapsed":true,"_cell_guid":"18782e38-282f-4760-90a2-840898ff9227","trusted":false},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = id_test\nsubmission['target'] = y_prediction\nsubmission.to_csv('stacked.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3bdb16045bb3072b9798debd7fe30e009cc06e6","_cell_guid":"e2f5b22c-480a-49aa-993b-53a7c032f21d"},"cell_type":"markdown","source":"# References"},{"metadata":{"_uuid":"3be2c42831a7eb1cef0789ac05775feaa0b6a118","_cell_guid":"ba4f180c-43fe-46bc-9d82-357e6a18d3ab"},"cell_type":"markdown","source":"[1] Porto Seguro Safe Driver Prediction, Kaggle Competition, https://www.kaggle.com/c/porto-seguro-safe-driver-prediction   \n[2] Bert Carremans, Data Preparation and Exploration, Kaggle Kernel, https://www.kaggle.com/bertcarremans/data-preparation-exploration   \n[3] Head or Tails, Steering Whell of Fortune - Porto Seguro EDA, Kaggle Kernel, https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda   \n[4] Anisotropic, Interactive Porto Insights - A Plot.ly Tutorial, Kaggle Kernel, https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial  \n[5] Dmitry Altukhov, Kaggle Porto Seguro's Safe Driver Prediction (3rd place solution),  https://www.youtube.com/watch?v=mbxZ_zqHV9c  \n[6] Vladimir Demidov, Simple Staker LB 0.284, https://www.kaggle.com/yekenot/simple-stacker-lb-0-284  \n[7] Anisotropic, Introduction to Ensembling/Stacking in Python, https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python  \n\n\n"},{"metadata":{"_uuid":"e60b8f67756bd5d00ebe7f0a408eb5e214c2bcdb","collapsed":true,"_cell_guid":"1158bd1c-2d07-4268-b2da-df5aef11192d"},"cell_type":"markdown","source":"# Feedback\n\nI will appreciate your suggestions and observations."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}