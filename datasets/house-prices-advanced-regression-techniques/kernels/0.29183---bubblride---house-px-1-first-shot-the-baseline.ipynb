{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nfrom pctl_scale import PercentileScaler  # pip install pctl-scale\nfrom sklearn.preprocessing import RobustScaler","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Summary\n* X: Only ratio-scale with many distinct values (e.g. square feet something). Not using nominal or ordinal-scale variables, nor ratio-scale with few distinct values (e.g. number of bathrooms)\n* Data prep: `pctl_scale.PercentileScaler` to transform all values within the 5% and 95% percentile like MinMax and all outliers with growth saturations formulas towards 0 or 1. \n* Missing Values are set `0.0` assuming that the feature just don't exist for the example. For example if the measure \"kitchen size in square feet\" is missing, maybe there is no kitchen at all.\n* y: sklearn's `RobustScaler` is used\n* Model assumptions: Multiplies weights with input data in some way. Thus, multiplying with `0.0` will automagically ignore missing values (set to `0.0`)\n\nWhat model?\n\n* Linear Regression\n* Baseline model\n    * Identify high correlations between target and predictors $|\\rho(y, x_i)|>0.4$ with p-values below 0.01\n    * For given $x_i$ (see before) find pairs $(x_i, x_j)$ with a high p-value indicating a poor relationship\n    * Estimate $y=\\theta_0 + \\theta_1 x_i + \\sum_{j=2}^{?} \\theta_j x_j + \\epsilon$\n"},{"metadata":{"_uuid":"9895b542924be884574deaa4794fe15b632c9e12"},"cell_type":"markdown","source":"## Data Prep"},{"metadata":{"trusted":true,"_uuid":"75579f3db0f51cee290fc6c1ca697c54f11a4492"},"cell_type":"code","source":"def dataprep_fit(df):\n    #df2 = df.copy()\n    transformer = dict()\n\n    # X with PercentileScaler\n    col_predictor = [\n        'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n        '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea',\n        'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n        'PoolArea', 'MiscVal', 'LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n    lo = .05\n    up = .95\n    naimpute = 0\n    \n    for i, s in enumerate(col_predictor):\n        # compute percentiles\n        obj = PercentileScaler(upper=up, lower=lo, naimpute=naimpute)\n        obj.fit(df[s])\n        # store and apply\n        transformer[s] = obj\n        #df2[s] = obj.transform(df[s])  \n    \n    # y with RobustScaler\n    col_target = ['SalePrice']\n\n    for i, s in enumerate(col_target):\n        tmp = df[s].values.reshape(-1, 1)\n        obj = RobustScaler()\n        obj.fit(tmp)\n        # store and apply\n        transformer[s] = obj\n        #df2[s] = obj.transform(tmp)  \n        \n    # done\n    return transformer, col_predictor, col_target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75579f3db0f51cee290fc6c1ca697c54f11a4492"},"cell_type":"code","source":"def dataprep_transform(df, transformer, xcols, ycols):\n    x = pd.DataFrame(index=df.index)\n    for i, s in enumerate(xcols):\n        obj = transformer[s]\n        x[s] = obj.transform(df[s].values.reshape(-1, 1))\n\n    if ycols:\n        y = pd.DataFrame(index=df.index)\n        for i, s in enumerate(ycols):\n            obj = transformer[s]\n            y[s] = obj.transform(df[s].values.reshape(-1, 1))\n    else:\n        y = None\n        \n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"facd4da1de0d2e7273e0e36c7bb65f7a585a875d"},"cell_type":"code","source":"#df = pd.read_csv('../input/train.csv', dtype=str)  # throws errors\ndf = pd.read_csv('../input/train.csv')\n\n# fit transform\ntransformer, xcols, ycols = dataprep_fit(df)\nx0, y0 = dataprep_transform(df, transformer, xcols, ycols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a5a7e94acc9b4079924c1de3ac5647599ec42cc"},"cell_type":"code","source":"#y0['SalePrice'].describe()\n#x0[col_predictor].applymap(lambda e: e if e>0 else np.nan).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ffddb6b35263cd890849981feb3357191f3ea7"},"cell_type":"code","source":"y = y0.values #.reshape(-1,1)\nx = x0.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f7b1967f394cbe650fe7a5847a8aef8b959f247"},"cell_type":"markdown","source":"## Find the highest correlation with y"},{"metadata":{"trusted":true,"_uuid":"59a9b0bbb2ad72a53622bd725782a78ecff47c40"},"cell_type":"code","source":"from scipy.stats import pearsonr\n\nn = x.shape[1]\nrho = np.empty(shape=(n,))\npval = np.empty(shape=(n,))\n\nfor i in range(n):\n    idx = x[:,i] > 0\n    rho[i], pval[i] = pearsonr(y[idx].ravel(), x[idx,i].ravel())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1843175a25422f3cc11101e556408c3301c4a49e"},"cell_type":"markdown","source":"The results are still a little bit messy"},{"metadata":{"trusted":true,"_uuid":"3a53fe4519734f78d631dc77b4ed602b45c58fbb"},"cell_type":"code","source":"np.c_[rho, pval].round(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea590316f9c45bd0f632fb0d3b3d684b357d527f"},"cell_type":"markdown","source":"Let's flag sufficient correlations"},{"metadata":{"trusted":true,"_uuid":"e13cffbb74d2f2d397668585642a58ca02e6de5e"},"cell_type":"code","source":"# only consider absolute correlation above 0.4 with p-values below 0.01\ncandidates = np.logical_and(np.abs(rho) > 0.4, pval < 0.01)\ncandidates","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8ccdec398cf2096a1d2d66de8c417c87361b12a"},"cell_type":"markdown","source":"Sort by the absolute correlation and set insufficient correlation to zero while sorting (so they appear at the bottom)"},{"metadata":{"trusted":true,"_uuid":"66fd827441ba7983e720c8890dd570884c733f26"},"cell_type":"code","source":"idx = np.argsort(np.abs(rho) * candidates)[::-1]\nidx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc51e6cc11cb9d8db9d4e26c31d4b805b61c711"},"cell_type":"code","source":"pd.DataFrame(index=idx, data=np.c_[col_predictor, rho.round(3), pval.round(4)][idx])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74308ec6718fe63a4bd19db963540126d9121c60"},"cell_type":"markdown","source":"`GrLivArea` or `x_8` will be the first predictor.\nHaving a big living room seems to be a very big selling point."},{"metadata":{"_uuid":"180054945b8152c65f833a23b9372da8e59fd16f"},"cell_type":"markdown","source":"## Find a second predictor that is uncorrelated to the first predictor"},{"metadata":{"trusted":true,"_uuid":"35fa61038d933653b8c0f3782b1acbadd3c94c14"},"cell_type":"code","source":"def compute_corr(y, x, colnam, rhomin=None, pmax=None, rhomax=None, pmin=None, sort='desc'):\n    from scipy.stats import pearsonr\n\n    n = x.shape[1]\n    rho = np.empty(shape=(n,))\n    pval = np.empty(shape=(n,))\n\n    for i in range(n):\n        idx = x[:,i] > 0\n        rho[i], pval[i] = pearsonr(y[idx].ravel(), x[idx,i].ravel())\n        \n    # only consider absolute correlation above 0.4 with p-values below 0.01\n    cand = np.ones(shape=(n,), dtype=bool)\n    if rhomin: cand = np.logical_and(cand, np.abs(rho) > rhomin)\n    if pmax: cand = np.logical_and(cand, pval < pmax)\n    if rhomax: cand = np.logical_and(cand, np.abs(rho) < rhomax)\n    if pmin: cand = np.logical_and(cand, pval > pmin)\n    \n    arr = -1 * np.abs(rho) * cand\n    if sort=='desc': arr = np.abs(rho) * cand + 10 * np.logical_not(cand)\n    idx = np.argsort(arr, axis=0)\n\n    return pd.DataFrame(index=idx, data=np.c_[cand, colnam, rho, pval][idx])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4a27678b32832bcfffdadad2d4e29901987cbcb"},"cell_type":"code","source":"\"\"\"this usually never works: find pval(xi,xj)>0.05 and both high cor \nxidx = (6,9,4,5,19,1,0,18)\nres = compute_corr(\n    x[:,8], x[:, xidx], \n    [s for i,s in enumerate(col_predictor) if i in xidx], \n    pmin=0.01, sort='asc')\n\"\"\"\nres = compute_corr(x[:,8], x, col_predictor, pmin=0.05, sort='desc')\nres.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fd11aa7981bcb4e0ac7bc9053b0bfc628f7a7dd"},"cell_type":"markdown","source":"All these high p-value variables have a lot of missing data.\nThus, will use them as kind of dummy variables."},{"metadata":{"trusted":true,"_uuid":"f81588b188e3e2a1e0c09d98c91a19e39df6ff35"},"cell_type":"code","source":"colnam2 = ['BsmtFinSF2', '3SsnPorch', 'MiscVal', 'LowQualFinSF', 'PoolArea']\ntmp = x0[colnam2].applymap(lambda e: e if e>0 else np.nan)\ntmp.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e2f2ed40ad7e0e19127070dfb64422d8b7ef227"},"cell_type":"markdown","source":"## Build a model"},{"metadata":{"trusted":true,"_uuid":"60c95d2d0955c3c5ac83df06a16b49d05c825f63"},"cell_type":"code","source":"xcols = ['GrLivArea', 'BsmtFinSF2', '3SsnPorch', 'MiscVal', 'LowQualFinSF', 'PoolArea']\ny = y0.values\nX = x0[xcols].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6036f302d5e1380390c9ec588b7d1bf83777c0e0"},"cell_type":"markdown","source":"some splitting"},{"metadata":{"trusted":true,"_uuid":"34760b2f151ca1db458f5b1a22208e821a45b77a"},"cell_type":"code","source":"y_train = y[:1200]\nX_train = X[:1200,:]\n\ny_valid = y[1201:]\nX_valid = X[1201:,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fee1c7147f15cbb343c4fbdcec41ed87e8370941"},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true,"_uuid":"cd13f5794ef71f3bceb42cdeca6a05b92f714df1"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\n\nhyperparam = {\n    #\"fit_intercept\": [True, False]\n}\n\nopti = GridSearchCV(\n    estimator = LinearRegression(\n        normalize=False,\n        copy_X=True,\n        fit_intercept=True\n    ),\n    param_grid = hyperparam, \n    cv = 5,\n    n_jobs = -1,\n    return_train_score = True\n)\n\nopti.fit(X=X_train, y=y_train)\n\nprint(opti.best_estimator_, \"\\n\",\n      opti.best_params_, \"\\n\")\n\nprint(\"{0:8.4f} [CV average score of the best model]\".format(\n      opti.best_score_ ) )\n\nbestmodel = opti.best_estimator_\nprint(\"{0:8.4f} [Performance on the leave-one out validation/test set]\".format(\n      r2_score(y_valid, bestmodel.predict(X_valid))) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69c76f088838adfd12707cab403068af655b1e13"},"cell_type":"markdown","source":"Submit it"},{"metadata":{"trusted":true,"_uuid":"ff6d7e4deff791c9157bd302691495eb63108d78"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\n\nxcols = ['GrLivArea', 'BsmtFinSF2', '3SsnPorch', 'MiscVal', 'LowQualFinSF', 'PoolArea']\nx_test, _ = dataprep_transform(df_test, transformer, xcols, None)\n\ny_output = bestmodel.predict(x_test.values)\ny_predicted = transformer['SalePrice'].inverse_transform(y_output)\n\nresult = pd.DataFrame(columns=['Id', 'SalePrice'], index=df_test.index)\nresult['Id'] = df_test['Id']\nresult['SalePrice'] = y_predicted\n\n#result\nresult.to_csv('linear-regression-1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7acf72680ddc8dffd094df755507e47ee1541b4b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}