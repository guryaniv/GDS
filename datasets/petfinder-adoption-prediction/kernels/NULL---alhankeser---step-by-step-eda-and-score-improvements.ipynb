{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Table of Contents\n", "- [Getting started](#Getting-Started)\n", "    - [Set score baseline](#Set-score-baseline)\n", "    - [Sampling](#Sampling)\n", "    - [Feature selection](#Feature-selection)\n", "- [Feature by Feature Exploration](#Feature-by-Feature-Exploration)\n", "    - [Type](#Type)\n", "    - [Photo](#Photo)\n", "    - Video\n", "    - Description\n", "    - Quantity\n", "    - Age\n", "    - Health\n", "    - Breed\n", "    - Color\n", "    - Size\n", "    - Fur\n", "    - Gender\n", "    - State\n", "    - Rescuer\n", "- External Data\n", "    - PetFinder.com (vs .my)\n", "    - What breeds are good with...\n", "- Model Tweaking"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Getting Started"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["# Imports\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from imblearn.over_sampling import RandomOverSampler\n", "import warnings\n", "import json\n", "import os\n", "\n", "# Options\n", "warnings.filterwarnings(action='ignore')\n", "sns.set_palette('YlGnBu')\n", "%config InlineBackend.figure_format='retina'"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["def reset_dfs():\n", "    global train_df, test_df\n", "    train_df = pd.read_csv('../input/train/train.csv')\n", "    test_df = pd.read_csv('../input/test/test.csv')\n", "reset_dfs()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Set score baseline\n", "Let's put the cart before the horse. \n", "Let's get our data cleaning and feature building pipeline set up so that as we can progressively validate our hypotheses moving forward. \n", "\n", "The idea here is to produce the simplest, most basic predictions to use as a starting point for the rest of the process of score improvement. \n", "\n", "#### Explore the accompanying script, which contains the resulting data cleaning / feature building functions: \n", "https://www.kaggle.com/alhankeser/slow-and-steady-feature-building\n", "\n", "According to the competition details, the evaluation metric is a **quadratic weighted kappa**, which sounds super complicated. Luckily, there is a function that I found [here](https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/quadratic_weighted_kappa.py) and that is used in few kernels. \n", "\n", "**Baseline Local Score: 0.07917**  \n", "Leaderboard Score: 0.090  \n", "https://www.kaggle.com/alhankeser/slow-and-steady-feature-building?scriptVersionId=9976702"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["'''\n", "Score:  0.07917\n", "   AdoptionSpeed  Type_2\n", "0              2       1\n", "1              0       1\n", "''';"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Sampling\n", "In typical beginner fashion, I'm going to use a new and shiny strategy that I just learned about [here](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets): sampling to overcome issues related to imbalances in the target feature counts.\n", "\n", "**Is there an imbalance in the target feature?**"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": false}, "outputs": [], "source": ["reset_dfs()\n", "plt.figure(figsize=(8,5))\n", "sns.countplot(train_df['AdoptionSpeed']);"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"scrolled": false}, "outputs": [], "source": ["g = sns.catplot(\"AdoptionSpeed\", col=\"Type\",\n", "                hue=\"Gender\", data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It appears that there is a low proportion of pets who are adopted on the day they was listed on PetFinder.\n", "\n", "Let's look into a few oversampling strategies to address the problem statement described [here](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets). \n", "\n", "### df.sample()\n", "There is a built-in pandas method to quickly under- or over-sample.  \n", "Below, I've created a funtion to take a list of lists that contain the target values to resample from/to.  \n", "The first item in each list is the target value being sample, either under or over, depending on whether there are more or less of the second item in each list. "]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def sample(df, target_val_sets):\n", "    for target_val_set in target_val_sets:\n", "        df_class_0 = df[df['AdoptionSpeed'] == target_val_set[0]]\n", "        count_1 = df['AdoptionSpeed'].value_counts()[target_val_set[1]]\n", "        df_class_0_sampled = df_class_0.sample(count_1,replace='True')\n", "        df = pd.merge(df.drop(df_class_0.index),\n", "                      df_class_0_sampled, how='outer')\n", "    return df\n", "\n", "# Over sample where AdoptionSpeed == 0, to match the count of AdoptionSpeed == 1\n", "train_df = sample(train_df, [[0, 1]])"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Note how counts of 0 and 1 now match:\n", "sns.countplot(train_df['AdoptionSpeed']);"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["g = sns.catplot(\"AdoptionSpeed\", col=\"Type\",\n", "                hue=\"Gender\", data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**What effect does randomly sampling using the pandas .sample() method have on our current local score**?  \n", "Current Baseline: 0.07924  \n", "New Score:  0.09349  \n", "https://www.kaggle.com/alhankeser/slow-and-steady-feature-building?scriptVersionId=9976791  "]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["'''\n", "Score:  0.09349\n", "   AdoptionSpeed  Type_2\n", "0              2       1\n", "1              3       0\n", "''';"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### RandomOverSampler()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = train_df.drop('AdoptionSpeed', axis=1)\n", "y = train_df['AdoptionSpeed']\n", "from imblearn.over_sampling import RandomOverSampler\n", "\n", "ros = RandomOverSampler(sampling_strategy='minority',\n", "                                    random_state=1)\n", "X_ros, y_ros = ros.fit_sample(X, y)\n", "\n", "print(X_ros.shape[0] - X.shape[0], 'new randomly picked points')"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"scrolled": false}, "outputs": [], "source": ["resampled_df = pd.DataFrame(list(X_ros), columns=train_df.drop('AdoptionSpeed', axis=1).columns)\n", "resampled_df['AdoptionSpeed'] = list(y_ros)\n", "resampled_df.head(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What is different here is that the minority ('AdoptionSpeed' == 0) is getting oversampled to match ('AdoptionSpeed' == 4). Not sure if that is good or bad..."]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["sns.countplot(resampled_df['AdoptionSpeed']);"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"scrolled": false}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"Type\",\n", "                hue=\"Gender\", data=resampled_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That certainly did *something*, right?  \n", "Let's check our local score. \n", "\n", "**What effect does random over-sampling with imbalanced-learn affect have on our local score?**  \n", "What effect does randomly sampling using the pandas .sample() method have on our current local score?  \n", "Current Baseline: 0.09349  \n", "New Local Score: 0.12413  \n", "Leaderboard Score: 0.086  \n", "https://www.kaggle.com/alhankeser/slow-and-steady-feature-building?scriptVersionId=9977273"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["'''\n", "Score:  0.12413\n", "   AdoptionSpeed  Type_2\n", "0              2       1\n", "1              0       1\n", "\n", "''';"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sampling at this point may not be the right strategy as it will be difficult to asses its value, given that we have not necessarily provided the random oversampler with much to go by. Increasing the number of samples with only \"Type\" as a feature may reduce accuracy rather than improve it. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Feature selection\n", "You may have noticed that we have only one feature that we're using to build our model (Type == 2).  \n", "\n", "Obviously, we could do with more and get a better score, before even engineering anything new.  \n", "\n", "What would be nice if we build in some forward feature selection into our workflow so that as we engineer features, we find out if they make the cut or not! (I could be totally wrong here, but come along for the ride anyway)  \n", "https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Feature by Feature Exploration"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Starting to form hypotheses\n", "It's easy to jump straight into \"building\" features before thinking through the bigger picture.  \n", "\n", "In this case, it might be a literal picture, since animal photos could very well be the big feature. \n", "\n", "Before jumping into the data, it's useful to take a look at mypetfinder.my to see what features standout as means by which visitors would go about narrowing their options and choosing a pet to adopt. \n", "\n", "**What comes across in looking at petfinder.my:**\n", "- **Insight:** Photos are presented as the primary means to navigate the website.\n", "    - **So what:** this feature could override other attributes as a predictor for adoption speed. \n", "- **Insight:** Filtering is quite challenging and hidden. \n", "    - **So what:** attributes that may *appear* important to us, may not be part of the typical visitor's journey on the website and therefore may not be playing a major role in adoption speeds. \n", "- **Insight:** Once on a pet detail page, additional photos are found at the bottom (and not easily scannable at top)\n", "    - **So what:** The first photo may be playing an outsized role. If there's a way to infer what the first photo in the pet's list of photos is, that might be the one to focus on. \n", "\n", "**Who would you rather adopt?**\n", "Some pets might be the ideal breed, in perfect health and just the right age, but if their photo(s) don't do them justice, then they may have a slow adoption speed. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Peak Cuteness | Maybe Cute\n", "- | -\n", "![Dog 1](../input/train_images/0a9f9f178-7.jpg) | ![Dog 2](../input/train_images/0b3deeb66-2.jpg)\n", "![Cat 1](../input/train_images/0a7798d2b-8.jpg ) | ![Cat 2](../input/train_images/0a2073b86-2.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Type"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Photo  \n", "\n", "Questions: \n", "- Is there a way to tell if photos are of good or bad quality? \n", "- If we can do the above, does it seem to make a difference on speed of adoption?\n", "- Do most pets have photos? If they don't does it make a big difference? \n", "- Does it matter if pets have many photos? Too few? "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's get a better understanding of what a \"good\" photo looks like, according to Google's Vision API.\n", "\n", "Here is a pretty good image that we could consider as \"good\": \n", "![Dog 1](../input/train_images/0a9f9f178-7.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What does Google think this photo is about? "]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["score = 0\n", "with open('../input/train_metadata/0a9f9f178-7.json') as f:\n", "    good_image = json.load(f)\n", "for label in good_image['labelAnnotations']:\n", "    if label['description'] == 'dog':\n", "        score = label['score']\n", "print(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's get a better understanding of what a \"not good\" photo looks like, according to Google's Vision API.\n", "\n", "Here is a not so great image that we could consider as \"not so good\": \n", "![Dog 2](../input/train_images/0b3deeb66-2.jpg)"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["score = 0\n", "with open('../input/train_metadata/0b3deeb66-2.json') as f:\n", "    good_image = json.load(f)\n", "for label in good_image['labelAnnotations']:\n", "    if label['description'] == 'dog':\n", "        score = label['score']\n", "print(score)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The second image did worse than I had expected! Turns out the closest label to \"dog\" is the following: "]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["{\n", "    \"description\": \"dog like mammal\",\n", "    \"score\": 0.77083683,\n", "};"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Watch out:** The latest version of the Google Vision API may have gotten better at labeling images. Running a few of these image through [this page](https://cloud.google.com/vision/) resulted in more precise scores.  "]}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [], "source": ["def get_photo_score(x, match='exact', start=1, stop=2, multiple=False):\n", "    pet_id = x\n", "    pet_type = train_df[train_df['PetID'] == pet_id]['Type'].values[0]\n", "    pet_type_dict = {1: 'dog', 2: 'cat'}\n", "    pet_type = pet_type_dict[pet_type]\n", "    scores = []\n", "    score = 0\n", "    i = start\n", "    while (i > 0) & (i < stop):\n", "        json_file = '../input/train_metadata/' + pet_id + '-' + str(i) + '.json'\n", "        if os.path.isfile(json_file):\n", "            with open(json_file) as f:\n", "                try:\n", "                    image_data = False\n", "                    image_data = pd.DataFrame(json.load(f)['labelAnnotations'])\n", "                except Exception:\n", "                    pass\n", "            try:\n", "                if match == 'exact':\n", "                    scores.append(image_data[image_data['description'] == pet_type]['score'].values[0])\n", "                if match == 'contains':\n", "                    scores.append(image_data[image_data['description'].str.contains(pet_type)]['score'].values.max())\n", "            except Exception:\n", "                scores.append(.0)\n", "                pass\n", "            i += 1\n", "        else:\n", "            break\n", "    try:\n", "        if not multiple:\n", "            if (stop-start) > 1:\n", "                score = np.array(scores).mean()\n", "            if (stop-start) == 1:\n", "                score = np.array(scores).max()\n", "        if multiple:\n", "            score = np.array(scores)\n", "    except Exception:\n", "        pass\n", "    return score\n", "\n", "train_df['FirstPhotoScore'] = train_df['PetID'].apply(lambda x: get_photo_score(x, match='exact', start=1, stop=2, multiple=False))"]}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [], "source": ["train_df['FirstPhotoScore > 0'] = train_df['FirstPhotoScore'] > 0\n", "\n", "sns.catplot(\"AdoptionSpeed\", col=\"Type\",\n", "                hue=\"FirstPhotoScore > 0\", data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 219, "metadata": {}, "outputs": [], "source": ["def group_photo_score(x):\n", "    score = x['FirstPhotoScore']\n", "    pet_type = x['Type']\n", "    if pet_type == 1:\n", "        good_threshold = 0.96\n", "    if pet_type == 2:\n", "        good_threshold = 0.99\n", "    if score > good_threshold: \n", "        return 'Good'\n", "    if (score < good_threshold) & (score > .5): \n", "        return 'Okay'\n", "    return 'Not Great'\n", "train_df['FirstPhotoScoreRange'] = train_df[['Type','FirstPhotoScore']].apply(lambda x: group_photo_score(x), axis=1)\n", "\n", "sns.catplot(\"AdoptionSpeed\", col=\"FirstPhotoScoreRange\",\n", "             data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 100, "metadata": {}, "outputs": [], "source": ["train_df['AllPhotoScores'] = train_df['PetID'].apply(lambda x: get_photo_score(x, match='exact', start=1, stop=99, multiple=False))"]}, {"cell_type": "code", "execution_count": 203, "metadata": {"scrolled": false}, "outputs": [], "source": ["def mean_photo_score(x):\n", "    mean_score = x['AllPhotoScore'] / x['PhotoAmt']\n", "    pet_type = x['Type']\n", "    if pet_type == 1:\n", "        good_threshold = 0.96\n", "    if pet_type == 2:\n", "        good_threshold = 0.99\n", "    if mean_score > good_threshold: \n", "        return 'Good'\n", "    if (mean_score < good_threshold) & (mean_score > 0): \n", "        return 'Okay'\n", "    return 'Not Great'\n", "\n", "train_df['AllPhotoScoreRange'] = train_df[['AllPhotoScore', 'PhotoAmt', 'Type']].apply(lambda x: mean_photo_score(x), axis=1)\n", "\n", "sns.catplot(\"AdoptionSpeed\", col=\"AllPhotoScoreRange\",\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 107, "metadata": {}, "outputs": [], "source": ["train_df['AllPhotoScoresList'] = train_df['PetID']\\\n", "    .apply(lambda x: get_photo_score(x, match='exact', start=1, stop=99, multiple=True))"]}, {"cell_type": "code", "execution_count": 209, "metadata": {}, "outputs": [], "source": ["train_df['AllPhotoScoresList'].head()"]}, {"cell_type": "code", "execution_count": 204, "metadata": {"scrolled": false}, "outputs": [], "source": ["def count_good_photos(x):\n", "    count = 0\n", "    pet_type = x['Type']\n", "    if pet_type == 1:\n", "        good_threshold = 0.96\n", "    if pet_type == 2:\n", "        good_threshold = 0.99\n", "    try:\n", "        count = len(x[x > good_threshold])\n", "    except Exception:\n", "        pass\n", "    if count > 2:\n", "        count = '> 3'\n", "    return count\n", "\n", "train_df['GoodPhotos'] = train_df[['Type', 'AllPhotoScoresList']].apply(lambda x: count_good_photos(x), axis=1)\n", "\n", "sns.catplot(\"AdoptionSpeed\", col=\"GoodPhotos\",\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 213, "metadata": {}, "outputs": [], "source": ["def count_secondary_good_photos(x):\n", "    count = 0\n", "    pet_type = x['Type']\n", "    scores = x['AllPhotoScoresList']\n", "    if pet_type == 1:\n", "        good_threshold = 0.96\n", "    if pet_type == 2:\n", "        good_threshold = 0.99\n", "    try:\n", "        scores = scores[1:]\n", "        count = len(scores[scores > good_threshold])\n", "    except Exception:\n", "        pass\n", "    if count > 2:\n", "        return 'Good'\n", "    if count > 0:\n", "        return 'Okay'\n", "    return 'Not Great'\n", "\n", "train_df['GoodSecondaryPhotos'] = train_df[['AllPhotoScoresList', 'Type']].apply(lambda x: count_secondary_good_photos(x), axis=1)\n", "\n", "sns.catplot(\"AdoptionSpeed\", col=\"GoodSecondaryPhotos\",\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 216, "metadata": {}, "outputs": [], "source": ["train_df['FirstAndSecondaryPhotos'] =  train_df['FirstPhotoScoreRange'] + '__' + train_df['GoodSecondaryPhotos']"]}, {"cell_type": "code", "execution_count": 221, "metadata": {}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"FirstAndSecondaryPhotos\",col_wrap=4,\n", "                 data=train_df[train_df['FirstPhotoScoreRange'] == 'Good'],\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 222, "metadata": {}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"FirstAndSecondaryPhotos\",col_wrap=4,\n", "                 data=train_df[train_df['FirstPhotoScoreRange'] == 'Okay'],\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 223, "metadata": {}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"FirstAndSecondaryPhotos\",col_wrap=4,\n", "                 data=train_df[train_df['FirstPhotoScoreRange'] == 'Not Great'],\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 164, "metadata": {}, "outputs": [], "source": ["scores = np.array([34,33,23234,12,45,456,454,6,6])"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["with open('../input/train_metadata/0a9f9f178-7.json') as json_file:\n", "    json_text = json.load(json_file)"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["image_data = pd.DataFrame(json_text['labelAnnotations']).drop(['mid', 'topicality'], axis=1).rename({'description': 'Description', 'score': 'Score'}, axis=1)"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["image_data['PetID'] = '0a9f9f178'"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["image_data['ImageID'] = 1"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["image_data"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["file_name = '0a9f9f178-7.json'"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["file_name.split('-')[0]"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["int(file_name.split('-')[1].split('.')[0])"]}, {"cell_type": "code", "execution_count": 112, "metadata": {"scrolled": true}, "outputs": [], "source": ["def image_data_matches(df, images_df):\n", "    return len(set(images_df['PetID'].unique()) - set(train_df['PetID'].unique())) == 0\n", "\n", "def create_image_df(df, images_df_file_name):\n", "    print('Building new image data csv...', df.name)\n", "    json_folder_path = path + '/input/' + df.name + '_metadata/'\n", "    json_files = [f_name for f_name in os.listdir(json_folder_path)\n", "                  if f_name.endswith('.json')]\n", "    pet_type_dict = {1: 'dog', 2: 'cat'}\n", "    all_images_list = []\n", "    for index, f_name in enumerate(json_files):\n", "        with open(os.path.join(json_folder_path, f_name)) as json_file:\n", "            json_text = json.load(json_file)\n", "            try:\n", "                label_annotations = json_text['labelAnnotations']\n", "            except:\n", "                continue\n", "            image_data = pd.DataFrame(label_annotations)\\\n", "                .drop(['mid', 'topicality'], axis=1)\\\n", "                .rename({'description': 'Description',\n", "                         'score': 'Score'},\n", "                        axis=1)\n", "            pet_id = f_name.split('-')[0]\n", "            image_data['PetID'] = pet_id\n", "            image_data['ImageID'] = int(f_name.split('-')[1].split('.')[0])\n", "            image_data['PetLabel'] = pet_type_dict[df[df['PetID'] == pet_id]['Type'].values[0]]\n", "        all_images_list.append(image_data)\n", "    images_df = pd.concat(all_images_list)\n", "    images_df.to_csv(images_df_file_name, index=False)\n", "    return images_df\n", "\n", "def get_image_data(df, force_new_csv=False):\n", "    images_df_file_name = path + '/' + df.name + '_image_data.csv'\n", "    try:\n", "        images_df = pd.read_csv(images_df_file_name)\n", "        no_image_file = False\n", "    except Exception:\n", "        no_image_file = True\n", "    if no_image_file or force_new_csv or not image_data_matches(df, images_df):\n", "        images_df = create_image_df(df, images_df_file_name)\n", "    return images_df\n", "\n", "def rate_image(x):\n", "    pet_label = x['PetLabel']\n", "    score = x['Score']\n", "    if pet_label == 'dog':\n", "        good_threshold = 0.96\n", "    if pet_label == 'cat':\n", "        good_threshold = 0.99\n", "    if score > good_threshold:\n", "        return 2\n", "    return 1\n", "    \n", "def cap_max_image_rating(x):\n", "    if x > 2:\n", "        return 2\n", "    return x\n", "        \n", "def append_image_data(df):\n", "    images_df = get_image_data(df)\n", "    images = images_df[(images_df['PetLabel'] == images_df['Description'])]\\\n", "                             [['PetID','Score','PetLabel', 'ImageID']]\n", "    images['ImageRating'] = images[['PetLabel', 'Score']].apply(lambda x: rate_image(x), axis=1)\n", "    first_images = images[images['ImageID'] == 1][['PetID', 'ImageRating']]\n", "    first_images.rename({'ImageRating': 'FirstImageRating'}, axis=1, inplace=True)\n", "    second_images = images[(images['ImageID'] > 1) & (images['ImageRating'] > 1)].groupby('PetID')['ImageRating'].count().reset_index()\n", "    second_images.rename({'ImageRating': 'SecondImageRating'}, axis=1, inplace=True)\n", "    second_images['SecondImageRating'] = second_images['SecondImageRating'].apply(lambda x: cap_max_image_rating(x))\n", "    image_ratings = pd.merge(first_images, second_images, on='PetID', how='left')\n", "    df = pd.merge(df, image_ratings[['PetID', 'FirstImageRating', 'SecondImageRating']], on='PetID', how='left')\n", "    df['FirstImageRating'].fillna(0, inplace=True)\n", "    df['SecondImageRating'].fillna(0, inplace=True)\n", "    df['TotalImageRating'] = df['FirstImageRating'] + (df['SecondImageRating'] * .1)\n", "    df.loc[df['TotalImageRating'] == 1.0, 'TotalImageRating'] = 0.0\n", "    df.loc[(df['TotalImageRating'] == 1.2) | \n", "           (df['TotalImageRating'] == 1.1), 'TotalImageRating'] = 1.0\n", "    df.loc[df['TotalImageRating'] == 2.1, 'TotalImageRating'] = 2.0\n", "    df.loc[df['TotalImageRating'] == 2.2, 'TotalImageRating'] = 3.0\n", "    return df\n", "\n", "reset_dfs()\n", "\n", "train_df.name = 'train'\n", "test_df.name = 'test'\n", "\n", "train_df = append_image_data(train_df)\n", "test_df = append_image_data(test_df)\n", "\n", "train_df.columns"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["images_df = pd.read_csv('../train_image_data.csv')\n", "len(set(images_df['PetID'].unique()) - set(train_df['PetID'].unique()))\n", "\n", "train_df_copy = train_df.copy()"]}, {"cell_type": "code", "execution_count": 77, "metadata": {}, "outputs": [], "source": ["image_ratings = pd.DataFrame(columns=['PetID', 'ImageRating'])\n", "image_ratings"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["reset_dfs()\n", "train_df.shape"]}, {"cell_type": "code", "execution_count": 73, "metadata": {}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"FirstImageRating\",col_wrap=4,\n", "                 data=train_df.log,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 75, "metadata": {"scrolled": false}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"SecondImageRating\",col_wrap=4,\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 100, "metadata": {"scrolled": false}, "outputs": [], "source": ["train_df['TotalImageRating'] = train_df['FirstImageRating'] + (train_df['SecondImageRating'] * .01)\n", "sns.catplot(\"AdoptionSpeed\", col=\"FirstImageRating\",col_wrap=4,\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 94, "metadata": {}, "outputs": [], "source": ["sns.catplot(\"AdoptionSpeed\", col=\"SecondImageRating\",col_wrap=4,\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": 109, "metadata": {}, "outputs": [], "source": ["train_df['TotalImageRating'] = train_df['FirstImageRating'] + (train_df['SecondImageRating'] * .1)\n", "train_df.loc[train_df['TotalImageRating'] == 1.0, 'TotalImageRating'] = 0.0\n", "train_df.loc[(train_df['TotalImageRating'] == 1.2) | (train_df['TotalImageRating'] == 1.1), 'TotalImageRating'] = 1.0\n", "train_df.loc[train_df['TotalImageRating'] == 2.1, 'TotalImageRating'] = 2.0\n", "train_df.loc[train_df['TotalImageRating'] == 2.2, 'TotalImageRating'] = 3.0\n", "sns.catplot(\"AdoptionSpeed\", col=\"TotalImageRating\",col_wrap=4,\n", "                 data=train_df,\n", "                kind=\"count\", height=5, aspect=.8);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["images_df = pd.read_csv('../train_image_data.csv')\n", "def get_adoptionspeed(x):\n", "    return train_df[train_df['PetID'] == x['PetID']]['AdoptionSpeed']\n", "images_df['AdoptionSpeed'] = images_df[['PetID']].apply(lambda x: get_adoptionspeed(x), axis=1)\n", "images_df.head()\n", "# images = images_df[(images_df['PetLabel'] == 'dog') & (images_df['ImageID'] == 1)].groupby('Description')[['PetLabel']].count().sort_values('PetLabel', ascending=False)\n", "# images.head(50)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Video"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Description"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quantity"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Age"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Health"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Breed"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Color"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Size"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Fur"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Gender"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## State"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Rescuer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}}, "nbformat": 4, "nbformat_minor": 2}