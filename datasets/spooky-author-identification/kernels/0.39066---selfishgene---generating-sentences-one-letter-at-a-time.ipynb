{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "00c95d81-b7a3-43d6-8992-6a85db4f3698", "_uuid": "bea0b8c41aa3f6cd77f8cdd6fc18b2e88b30bc34"}, "cell_type": "markdown", "source": ["# Generating Sentences One Letter at a Time\n", "In this script we will create several character based language models using very simple conditional probability distributions (CPDs) by calculating histograms from the data (these are simple generative models that we will sample sentences from). We will also build basic logistic regression classfiers over bag of character and/or bag of word features (these are simple discriminative models that we will use to identify authors).  \n", "\n", "## Generative Part\n", "In the main part of the script, we will use **Markov Models** that remember either 0, 1, 2, 3 or 4 previous characters and emit a probability distrubiton over the next character in the sequence.  \n", "Markov Models are perhaps the simplest and most streight forward way to model temporal sequences of discrete symbols (in our case, we will use characters as our discrete symbols).  \n", "\n", "We will first see how discriminative these generative models can be for the task of author identification (spoiler: dispite their simplicity they are surprisingly good).  \n", "\n", "We then continue to generate text in the style of the authors using those models to see what they have learned (another spoiler: they learn quite a bit, but not quite at the \"famous author\" level yet).   \n", "\n", "In the end we will create a submission using our best markov model for the use of everyone who wishes to tweak it.\n", "\n", "## Discriminative Part\n", "\n", "In the very last part of the script, we employ a fully discriminative approach that is either character based or word based bag of word features:\n", "1. Extract **Bag of Character n-grams** features\n", "1. Create a submission for **Logistic Regression over *BagOfChar***\n", "1. Extract **Bag of Word n-grams** features\n", "1. Create a submission for **Logistic Regression over *BagOfWord***\n", "1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***\n", "\n", "\n", "**Note: ** I use only pandas, numpy and native python here to make it simpler to \"get into\" the code for those of you who wish to do so (only in the last discriminative part I use sklearn as well)\n", "\n", "![nice animation](https://cdn-images-1.medium.com/max/1600/1*MbHRwYNA8F29hzes8EPHiQ.gif)\n", "\n", "Check out this [blog post](https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71) from which I stole this nice animation above. It explains the basics of markov models and even contains code to apply markov models at the word level as basic discrete symbols. In this script, however, we will use characters as the basic discrete symbols. "]}, {"metadata": {"_cell_guid": "67a8db5f-392e-4c6f-b905-85bfb979b7ed", "_uuid": "842777125de010e71d5dfe076a7e7a41d1ab091e"}, "cell_type": "markdown", "source": ["# Short Math Introduction\n", "In this script we try to model the distribution of sentences $$P(Sentence)$$  \n", "A sentence is just a sequence of $n$ characters and therrefore we can write $$P(Sentence) = P(c_1,c_2,c_3,...,c_n)$$  \n", "\n", "**In this script** we will model this sequence of characters using short term memory conditional distributions.  \n", "* **no memory** (this is also known as naive bayes): \n", "$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2)P(c_3)...P(c_n) = \\prod_{t=1}^{n}P(c_t)$$ \n", "* **1 time step memory** (this is the classical markov chain): \n", "$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2)...P(c_n|c_{n-1}) = P(c_1)\\prod_{t=2}^{n}P(c_t|c_{t-1})$$ \n", "* **2 time step memory**: \n", "$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)\\prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2})$$ \n", "* **3 time step memory**: \n", "$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)\\prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3})$$ \n", "* **4 time step memory**: \n", "$$P(c_1,c_2,c_3,...,c_n) = P(c_1)P(c_2|c_1)P(c_3|c_2,c_1)P(c_4|c_3,c_2,c_1)\\prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$$ \n", "\n", "## Note: \n", "In this corpus, the authors use 34 total characters (regular characters plus punctuation marks).  \n", "What this means is that:  \n", "Storing the $P(c_t)$ distribution will involve storing $34$ numbers.  \n", "Storing the $P(c_t|c_{t-1})$ distribution will involve storing $34^2 = 1,156$ numbers.  \n", "Storing the $P(c_t|c_{t-1},c_{t-2})$ distribution will involve storing $34^3 = 39,304$ numbers.  \n", "Storing the $P(c_t|c_{t-1},c_{t-2},c_{t-3})$ distribution will involve storing $34^4 = 1,336,336$ numbers.  \n", "Storing the $P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4})$ distribution will involve storing $34^5 = 45,435,424$ numbers.  \n", "\n", "Keep in mind also that the entire corpus of this competition contains about ~3M characters so that we should start encountering substantial finite sample size effects for 3 and 4 history conditional probability distributions."]}, {"metadata": {"_cell_guid": "6410c90a-ecf3-47cb-9365-0ab609f679e5", "_uuid": "a8c18745f3bc90df325475338a4569a50aa051cb", "_kg_hide-input": false, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import matplotlib\n", "from sklearn import model_selection, preprocessing, linear_model\n", "from sklearn.metrics import log_loss, accuracy_score\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "matplotlib.style.use('fivethirtyeight')"]}, {"metadata": {"_cell_guid": "62929ec9-f685-4c67-bf93-e5809b9c1abe", "_uuid": "91b1fbf4352ccdeef3feda46413b68723117637c"}, "cell_type": "markdown", "source": ["## Load training data and seperate it into a train and validation sets"]}, {"metadata": {"_cell_guid": "fbd33ca3-320a-483f-9030-f6a7dfde89e3", "_uuid": "837f7b120f60725150db4f9c22f14c9f594504a2", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% load and organize data\n", "data = pd.read_csv('../input/train.csv')\n", "\n", "stratifiedCV = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=1)\n", "trainInds, validInds = next(stratifiedCV.split(data['text'], data['author']))\n", "\n", "trainText  = data.loc[trainInds,'text'].reset_index(drop=True)\n", "validText  = data.loc[validInds,'text'].reset_index(drop=True)\n", "trainLabel = data.loc[trainInds,'author'].reset_index(drop=True)\n", "validLabel = data.loc[validInds,'author'].reset_index(drop=True)"]}, {"metadata": {"_cell_guid": "43dac597-d6c9-4703-8aa4-c330c184462f", "_uuid": "a7e4bc1d9d1c0f571b59171d5ab50fac5b500c78"}, "cell_type": "markdown", "source": ["## Collect all chars into one large string for each author"]}, {"metadata": {"_cell_guid": "03008fd9-3115-4297-8a19-85b909a5677c", "_uuid": "df3c3cf18de9fc66d51c55ccbeff90164c868de4", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% some utility code\n", "# dictionary to manually converts greek/spanish chars into closest english chars\n", "toEnglishDict = {}\n", "srcStr = ['\u00e0','\u00e2','\u00e4','\u00e5','\u00e6','\u00e7','\u00e8','\u00e9','\u00ea','\u00eb','\u00ef','\u00ee','\u00f1','\u00f4','\u00f6','\u00f5','\u00fc','\u00fb','\u03b1','\u03b4','\u03bd','\u03bf','\u03c0','\u03c2','\u03c5','\u1f36']\n", "dstStr = ['a','a','a','a','a','c','e','e','e','e','i','i','n','o','o','o','u','u','a','d','n','o','p','s','y','i']\n", "for src,dst in zip(srcStr,dstStr):\n", "    toEnglishDict[src] = dst\n", "    \n", "# function that converts all non english chars to their closest english char counterparts\n", "def myunidecode(inString):\n", "    outString = ''\n", "    for ch in inString:\n", "        if ch in toEnglishDict.keys():\n", "            outString += toEnglishDict[ch]\n", "        else:\n", "            outString += ch\n", "    return outString"]}, {"metadata": {"_cell_guid": "97eda04f-6386-4deb-96e9-2fd60bc7ed94", "_uuid": "8103a9cf3f33f9fa88d3d2b294583767558fcc14", "_kg_hide-input": false, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% go over all train data and collect one large char sequence for each author\n", "charsDict = {}\n", "for key in ['all','EAP','HPL','MWS']:\n", "    charsDict[key] = []\n", "\n", "for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n", "    # the decoding is done for spanish/greek chars to be converted to close english chars\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    chars = [char for char in decodedSentence]\n", "    \n", "    charsDict['all']  += chars\n", "    charsDict[author] += chars"]}, {"metadata": {"_cell_guid": "671379ac-fd3a-45c1-bcdf-2432c9960435", "_uuid": "62dc53e002586c98788811c756bbf5cc0a8f86d0"}, "cell_type": "markdown", "source": ["# Show the Char usage Distribution for each Author\n", "$$P(c_t|Author)$$"]}, {"metadata": {"_cell_guid": "b14c8208-d894-40ee-a4d0-e004e93ab615", "_uuid": "7dc31fd5032fc5501bff3616729f837bc6581eaf", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% show char usage histogram for the three authors\n", "charEncoder = preprocessing.LabelEncoder()\n", "charEncoder.fit(charsDict['all'])\n", "\n", "charCounts_EAP = np.histogram(charEncoder.transform(charsDict['EAP']),range(len(charEncoder.classes_)+1),density=True)[0]\n", "charCounts_HPL = np.histogram(charEncoder.transform(charsDict['HPL']),range(len(charEncoder.classes_)+1),density=True)[0]\n", "charCounts_MWS = np.histogram(charEncoder.transform(charsDict['MWS']),range(len(charEncoder.classes_)+1),density=True)[0]\n", "\n", "# sort the char classes by their usage frequency\n", "sortedChars = np.flipud(np.argsort(charCounts_EAP + charCounts_HPL + charCounts_MWS))"]}, {"metadata": {"_cell_guid": "436abae9-7c73-44d6-9d0e-0da539e10a34", "_uuid": "49fa8c587b2a8dbf87075479d81099bc1843214b", "_kg_hide-output": false, "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["barWidth = 0.21\n", "x = np.arange(len(charCounts_EAP))\n", "\n", "plt.figure(figsize=(12,7)); plt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);\n", "plt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');\n", "plt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');\n", "plt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');\n", "plt.legend(fontsize=24); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); plt.xlabel('$C_t$');\n", "plt.xticks(x,[\"'%s'\" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=13);"]}, {"metadata": {"_cell_guid": "128558f1-b145-454b-bbb1-8824ca67f7bb", "_uuid": "c7d182278e2993dde5b8c68bd8fc0da511413677"}, "cell_type": "markdown", "source": ["Interestingly, there are differences between the authors here!  \n", "## Lets look at the same plot only with log scale on the y axis"]}, {"metadata": {"_cell_guid": "d6f28df1-30e9-49b9-90dc-43194aab88d9", "_uuid": "d9f049d48d1207655e4534465b40201db7e042d2", "_kg_hide-output": false, "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["barWidth = 0.21\n", "x = np.arange(len(charCounts_EAP))\n", "\n", "plt.figure(figsize=(12,7)); \n", "plt.title('Character Usage Frequncy - $P(C_t)$ ',fontsize=25);\n", "plt.bar(x-barWidth, charCounts_EAP[sortedChars], barWidth, color='r', label='Edgar Allen Poe');\n", "plt.bar(x         , charCounts_HPL[sortedChars], barWidth, color='g', label='Howard Phillips Lovecraft');\n", "plt.bar(x+barWidth, charCounts_MWS[sortedChars], barWidth, color='b', label='Mary Wollstonecraft Shelley');\n", "plt.legend(fontsize=21); plt.ylabel('Usage Frequncy - $P(C_t)$', fontsize=20); \n", "plt.yscale(\"log\", nonposy='clip'); plt.xlabel('$C_t$');\n", "plt.xticks(x,[\"'%s'\" %(charEncoder.classes_[i]) for i in sortedChars], fontsize=11);"]}, {"metadata": {"_cell_guid": "f289e60a-97dd-4e1a-8a77-b2741fc9d109", "_uuid": "35fa3098ccd7a53be2e46efe665f059e195be3c3"}, "cell_type": "markdown", "source": ["Very large differences can be seen in usage of punctuation marks.  \n", "For example, the \":\" character is used much more extensivley by MWS and least used by HPL.\n", "\n", "Even though these differences look small, they are extreemly statistically significant since there are about 2,600,000 chars in the training set. These differences are not there by chance, and perhaps we can utilize these differences for classification. Lets try."]}, {"metadata": {"_cell_guid": "cc39acfc-bffe-4d14-9a3b-9cc052d42a02", "_uuid": "9d88f4dbc77786fb57f5249f5e53934ce1e8d87f"}, "cell_type": "markdown", "source": ["# How do we Identify the Author of each Sentence using our CPDs?\n", "In this competition specifically, we would like to model the conditional probability distribution of the author given a sentence: $$P(Author | Sentence) = P(Author | c_1,c_2,c_3,...,c_n)$$  \n", "\n", "We have three different authors here, so this boils down to:  \n", "\n", "$$P(EAP | c_1,c_2,c_3,...,c_n)$$    \n", "$$P(HPC | c_1,c_2,c_3,...,c_n)$$    \n", "$$P(MWS | c_1,c_2,c_3,...,c_n)$$  \n", "\n", "Using the bayes rule we can invert this into $$P(Author | Sentence) = \\frac{P(Sentence | Author)P(Author)}{P(Sentence)}$$  \n", "The prior distribution $P(Author)$ over authors is extreemly simple to calcualte (it's just the frequency of occurence of each author in the training set).  \n", "\n", "For any specific sentence, the probability of that sentence $P(Sentence)$ is identical for all authors and therefore doesn't create any additional discrimination between them, so when creating a classifier we can just ignore this and compare the 3 following quantities:  \n", "\n", "$$P(c_1,c_2,c_3,...,c_n | EAP)P(EAP)$$    \n", "$$P(c_1,c_2,c_3,...,c_n | HPC)P(HPC)$$    \n", "$$P(c_1,c_2,c_3,...,c_n | MWS)P(MWS)$$  \n", "meaning:\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{{P(c_1,c_2,c_3,...,c_n|Author)P(Author)}\\}   $$"]}, {"metadata": {"_cell_guid": "ad5a4e32-5649-4503-9406-7e042e4fb5b4", "_uuid": "cfba7885c7f8ffddcc91f1de380116c237864cf0"}, "cell_type": "markdown", "source": ["# Calculate Classification Accuracy based only on single char distribution\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=1}^{n}P(c_t|Author)\\}   $$  \n", "\n", "**Note:** we assume here an equal prior just for simplicity (i.e. $P(Author) = \\frac{1}{3}$ for all authors)  \n"]}, {"metadata": {"_cell_guid": "cc90a92e-2146-4499-9c8c-a498d173c745", "_uuid": "b1f01ca0cb494790b9e18fb230041491ee9c9783", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% meassure classification accuracy on validation set using only character frequncy\n", "authorsList = ['EAP','HPL','MWS']\n", "authorPredictionList = []\n", "for k, (sentence, author) in enumerate(zip(validText,validLabel)):\n", "    chars = [char for char in myunidecode(sentence.lower())]\n", "    # convert to log so we can sum probabilities instead of multiply\n", "    logP_EAP = sum([np.log(charCounts_EAP[charEncoder.classes_ == ch]) for ch in chars])\n", "    logP_HPL = sum([np.log(charCounts_HPL[charEncoder.classes_ == ch]) for ch in chars])\n", "    logP_MWS = sum([np.log(charCounts_MWS[charEncoder.classes_ == ch]) for ch in chars])\n", "    \n", "    authorPredictionList.append(authorsList[np.argmax(np.array([logP_EAP,logP_HPL,logP_MWS]))])\n", "\n", "print(52*'-')\n", "print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\n", "print(52*'-')"]}, {"metadata": {"_cell_guid": "4579bad2-3fc7-40de-b54f-9a2a1006558e", "_uuid": "e92fd40c9f187f8591552e1e933af333ca2d6cbb"}, "cell_type": "markdown", "source": ["Interestingly, even though this is perhaps the stupidest model one can think of, the discrimination accuracy is well above chance level."]}, {"metadata": {"_cell_guid": "bb5bdc3a-b469-4aa7-9538-89979ecc77b1", "_uuid": "0095450bd3298e5f7d7717a4a109ce3b4d918e9a"}, "cell_type": "markdown", "source": ["## Generate Sample Text for each author using the independent chars model\n", "$$  c_t \\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\: P(c_t|Author)   $$"]}, {"metadata": {"_cell_guid": "b33348ab-2110-4075-89f3-5bf1a70add34", "_uuid": "81970ad3dcaba2cb91817b150b97131be7069005", "_kg_hide-input": false, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate sample text by sampling one charachter at a time for the independent character model\n", "np.random.seed(1234)\n", "\n", "maxSentenceLength = 95\n", "numSentencesPerAuthor = 5\n", "\n", "charProbModel = {}\n", "charProbModel['all'] = (charCounts_EAP + charCounts_HPL + charCounts_MWS)/3.0\n", "charProbModel['EAP'] = charCounts_EAP\n", "charProbModel['HPL'] = charCounts_HPL\n", "charProbModel['MWS'] = charCounts_MWS\n", "\n", "for author in ['EAP','HPL','MWS','all']:\n", "    print((6+maxSentenceLength)*'-')\n", "    print('Author %s:' %(author))\n", "    print(12*'-')\n", "    for i in range(numSentencesPerAuthor):\n", "        generatedSentence = ''\n", "        for j in range(maxSentenceLength):\n", "            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n", "            generatedSentence += newChar\n", "            \n", "            if (newChar == '.') or (j == maxSentenceLength):\n", "                break\n", "                \n", "        print('%d: \"%s\"' %(i+1,generatedSentence))\n", "print((4+maxSentenceLength)*'-')"]}, {"metadata": {"_cell_guid": "6e842a31-dcff-4dc5-b6dc-bb7caf1f422e", "_uuid": "3a468424e8fa9eb9c45cc61ad2dcff91cdd4ad12"}, "cell_type": "markdown", "source": ["Well, as expected, we can see that this really doesn't resemble any human generated text.  \n", "But perhaps we can do better with some memory?"]}, {"metadata": {"_cell_guid": "25dfb527-09d4-4025-b942-a82243067be4", "_uuid": "4e9b55fdacfbd8fdea121948c35c7d7295d85c8f"}, "cell_type": "markdown", "source": ["## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\n", "Use history of 1 character "]}, {"metadata": {"_cell_guid": "8cf627b5-9dab-4efb-b075-590ce3b2b0fb", "_uuid": "41ef0c50c4b894cab80f044df754f7e6e35e72cf", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% gather all pairs of characters into a single dataframe\n", "historyLength = 1\n", "\n", "historyList  = []\n", "nextCharList = []\n", "authorList   = []\n", "for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n", "    \n", "    historyList  += history\n", "    nextCharList += nextChar\n", "    authorList   += [author]*len(history)\n", "        \n", "corpusDF = pd.DataFrame(columns=['author','history','next char'])\n", "corpusDF['author']    = authorList\n", "corpusDF['history']   = historyList\n", "corpusDF['next char'] = nextCharList\n", "\n", "corpusDF.head(8)"]}, {"metadata": {"_cell_guid": "df23a5bf-799d-46fd-a176-4d75044939c4", "_uuid": "71849b9a60dead2d0e94b5abdcf8153855ee1201"}, "cell_type": "markdown", "source": ["## Build Markov Model that Remebers only the previous char\n", "$$P(c_t|c_{t-1},Author)$$"]}, {"metadata": {"_cell_guid": "714a10dd-1eb9-4b49-8d86-b271a937c18a", "_uuid": "4152aaaeae091574bf8168901c481f36eb48b941", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate P(c(t)|c(t-1)) model (Markov Model with memory of 1 time step)\n", "charCondProbModel_H1 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondProbModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "    \n", "charCondCountModel_H1 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondCountModel_H1[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "\n", "corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\n", "for author in corpusDF['author'].unique():\n", "    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n", "    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n", "    for history in authorCorpusDF['history'].unique():\n", "        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n", "\n", "        encodedHistory = charEncoder.transform([history])[0]\n", "        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n", "        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n", "\n", "        charCondProbModel_H1[author][encodedHistory,:]  = encodedNextCharProb\n", "        charCondCountModel_H1[author][encodedHistory,:] = encodedNextCharCounts\n", "\n", "    condCount = charCondCountModel_H1[author]\n", "    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n", "\n", "charCondProbModel_H1['all']  = (charCondProbModel_H1['EAP']  + charCondProbModel_H1['HPL']  + charCondProbModel_H1['MWS'] )/3.0\n", "charCondCountModel_H1['all'] =  charCondCountModel_H1['EAP'] + charCondCountModel_H1['HPL'] + charCondCountModel_H1['MWS']\n", "\n", "print('average Sparsity level = %.1f%s' %(100*(charCondCountModel_H1['all'] < 1).sum() / (condCount > -1).sum().astype(float),'%'))"]}, {"metadata": {"_cell_guid": "e548e874-0455-4141-a26f-ad5afaa53bf8", "_uuid": "ad3802458bf07cd0c51b7305be7896e21d2494ea"}, "cell_type": "markdown", "source": ["# Show the Conditional Probability Distribution of the entire corpus \n", "$$P(c_t|c_{t-1}) $$"]}, {"metadata": {"_cell_guid": "0f03a50a-bd60-4ebd-91b3-267e811aab9b", "_uuid": "5a7e3709126d42e2d2fbe87db1491cc50a21a2eb", "_kg_hide-output": false, "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["condProb = charCondProbModel_H1['all']\n", "\n", "plt.figure(figsize=(12,10))\n", "plt.imshow(condProb, cmap='hot');  plt.colorbar(); plt.clim(0,1);\n", "plt.grid('off'); plt.title('P(next char | prev char) for all Authors - $P(c_t|c_{t-1})$', fontsize=22);\n", "plt.xlabel('$c_t$ - next character', fontsize=18); plt.ylabel('$c_{t-1}$ - previous character', fontsize=18);\n", "plt.xticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\n", "plt.yticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);"]}, {"metadata": {"_cell_guid": "91c86238-36da-49e7-a2a9-78b2dd1c1031", "_uuid": "63cc4955b46d125b0f8253632de5b2ded1913c67"}, "cell_type": "markdown", "source": ["There are a few clear \"columns\" in the dataset - for the \"a\",\"e\",\"i\" and \"o\" characters (which are vowels, by the way). What this means is that these characters are quite likely to come after many other characters. Contrast that with \"y\" that is likely to occur mostly after \"l\", \"m\" and \"b\" (to form the pairs \"ly\", \"my\" and \"by\").\n", "\n", "Note that we now have about 25% of character pairs that never occur in the training set at all. Even though we only have ~1,150 possible charachter pairs and ~2,600,000 pairs.  \n", "In this particular case, it's safe to assume that the pairs that don't occur are simply very rare or non existent in the languge, but it's important to keep in mind that some of these zeros are perhaps due to finite sample size. This can be quanitfied of course, but we will skip this in this tutorial."]}, {"metadata": {"_cell_guid": "37a9ae24-bf09-4999-8690-231132bad2ec", "_uuid": "5742f9eca7acbaace3603e4f2c96ec0a17676958"}, "cell_type": "markdown", "source": ["# Show the Author Specific Conditional Probability Distributions \n", "$$P(c_t|c_{t-1},Author) $$"]}, {"metadata": {"_cell_guid": "8302ad72-5aba-4679-be35-22f61e1c5c81", "_uuid": "fc3f9745af783421a98ba7d6213faaf2ebc76920", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["shortToFullNameDict = {}\n", "shortToFullNameDict['EAP'] = 'Edgar Allen Poe'\n", "shortToFullNameDict['HPL'] = 'Howard Phillips Lovecraft'\n", "shortToFullNameDict['MWS'] = 'Mary Wollstonecraft Shelley'\n", "\n", "plt.figure(figsize=(13,28))\n", "for k, author in enumerate(['EAP','HPL','MWS']):\n", "    condProb = charCondProbModel_H1[author]\n", "    plt.subplot(3,1,k+1); plt.imshow(condProb, cmap='hot'); \n", "    plt.grid('off'); plt.colorbar(); plt.clim(0,1);\n", "    plt.title('P(next char | prev char, %s) - $P(c_t|c_{t-1},Author)$' %(shortToFullNameDict[author]), fontsize=17);\n", "    plt.xlabel('$c_t$ - next character', fontsize=15); plt.ylabel('$c_{t-1}$ - previous character', fontsize=15);\n", "    plt.xticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\n", "    plt.yticks(range(condProb.shape[0]),[\"'%s'\" %(ch) for ch in charEncoder.classes_]);\n", "plt.tight_layout();"]}, {"metadata": {"_cell_guid": "8c79df0f-cc69-4ed7-a6e6-ee0a97dbf71e", "_uuid": "895c40f95fb79a4fc648590beed98aa0220b3cce"}, "cell_type": "markdown", "source": ["We can see a few differences between the authors, especially in the top few rows that indicate the different usage of punctuation marks by our authors."]}, {"metadata": {"_cell_guid": "12b34b21-03be-4387-b067-0a0ffb29f1ad", "_uuid": "767f805254bc829dd7862f90f5c108d697dcc680"}, "cell_type": "markdown", "source": ["## Calculate Classification Accuracy of a Classic Markov Model\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=2}^{n}P(c_t|c_{t-1},Author)\\}   $$"]}, {"metadata": {"_cell_guid": "574c5390-e94c-4456-9a06-2cb4db072513", "_uuid": "c071ab1bca3d5e23c51a989de10120d10b599689", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% meassure classification accuracy on validation set using Markov Model with memory of 1 time step\n", "uniformPriorFraction    = 0.0001\n", "allAuthorsPriorFraction = 0.0001\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/len(charEncoder.classes_)\n", "\n", "condP_H1 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]\n", "    condP_H1[author] += prior[1]*charCondProbModel_H1['all']\n", "    condP_H1[author] += prior[2]*uniformPriorValue\n", "\n", "authorPredictionList = []\n", "for k, (sentence, author) in enumerate(zip(validText,validLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    \n", "    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n", "    for histChar, nextChar in zip(history,nextChar):\n", "        encodedHistChar = charEncoder.transform([histChar])[0]\n", "        encodedNextChar = charEncoder.transform([nextChar])[0]\n", "        \n", "        logP_EAP += np.log(condP_H1['EAP'][encodedHistChar,encodedNextChar])\n", "        logP_HPL += np.log(condP_H1['HPL'][encodedHistChar,encodedNextChar])\n", "        logP_MWS += np.log(condP_H1['MWS'][encodedHistChar,encodedNextChar])\n", "    \n", "    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n", "    \n", "print(52*'-')\n", "print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\n", "print(52*'-')"]}, {"metadata": {"_cell_guid": "571beabf-823a-461b-9478-6bb0b5d93d7b", "_uuid": "6897ce2802aec902e273ba0b2a27ff97875883d3"}, "cell_type": "markdown", "source": ["The classification accuracy increases some more. By just looking at the distribution of pairs of charachters. "]}, {"metadata": {"_cell_guid": "a45dd146-248d-4c7f-9911-6e185c375c2b", "_uuid": "0ffdfde68444a8f14b08d6e103a4098f9d313239"}, "cell_type": "markdown", "source": ["## Generate Sample Text for each Author using our Markov Model\n", "$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},Author)   $$"]}, {"metadata": {"_cell_guid": "6fff75d0-958e-46c7-9a4b-4d4016c25660", "_uuid": "c3f10b0dafd59eb4351aa8a43eb6ad30d87c6ef0", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate sample text by sampling one charachter at a time from the 1 time step memory Markov Model\n", "np.random.seed(123)\n", "\n", "maxSentenceLength = 90\n", "numSentencesPerAuthor = 6\n", "\n", "uniformPriorFraction    = 0.0001\n", "allAuthorsPriorFraction = 0.0009\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/(len(charEncoder.classes_))\n", "\n", "condP_H1 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H1[author]  = prior[0]*charCondProbModel_H1[author]\n", "    condP_H1[author] += prior[1]*charCondProbModel_H1['all']\n", "    condP_H1[author] += prior[2]*uniformPriorValue\n", "\n", "condP_H1['all']  = (prior[0]+prior[1])*charCondProbModel_H1['all']\n", "condP_H1['all'] += prior[2]*uniformPriorValue\n", "\n", "for author in ['EAP','HPL','MWS','all']:\n", "    print((6+maxSentenceLength)*'-')\n", "    print('Author %s:' %(author))\n", "    print(12*'-')\n", "    for i in range(numSentencesPerAuthor):\n", "        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n", "        generatedSentence = firstChar\n", "        for j in range(maxSentenceLength-1):\n", "            encodedHistChar = charEncoder.transform([generatedSentence[-1]])[0]\n", "            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedHistChar,:])][0]\n", "            generatedSentence += newChar\n", "            \n", "            if (newChar == '.') or (j == maxSentenceLength):\n", "                break\n", "        print('%d: \"%s\"' %(i+1,generatedSentence))\n", "print((4+maxSentenceLength)*'-')"]}, {"metadata": {"_cell_guid": "cb701825-ac17-47ff-9358-1e7000a23f9b", "_uuid": "75e3e7c6300686e6e54789d1c1d01ac7cda65e6a"}, "cell_type": "markdown", "source": ["This already has a text like feeling to it. After punctioation we see a whitespace, every now and then we see the letter \"a\" and \"i\" seperated by whitespaces and the short word \"he\" appears several times in the text. we are getting somewhere, let's add a little bit more memory."]}, {"metadata": {"_cell_guid": "593e93af-5b0a-41c0-ae22-c6ecc11d9dfb", "_uuid": "643fd26611349382c4fc4544b8e385d7b075a05b"}, "cell_type": "markdown", "source": ["## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\n", "Use history of size 2 characters"]}, {"metadata": {"_cell_guid": "a249b9aa-5aca-45a0-a211-000448884afb", "_uuid": "d3c4d1ab9195ae1a30a14ad3b1a790a4c9d65373", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% gather all triplets of characters into a single dataframe\n", "historyLength = 2\n", "\n", "historyList  = []\n", "nextCharList = []\n", "authorList   = []\n", "for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n", "    \n", "    historyList  += history\n", "    nextCharList += nextChar\n", "    authorList   += [author]*len(history)\n", "        \n", "corpusDF = pd.DataFrame(columns=['author','history','next char'])\n", "corpusDF['author']    = authorList\n", "corpusDF['history']   = historyList\n", "corpusDF['next char'] = nextCharList\n", "\n", "corpusDF.head(8)"]}, {"metadata": {"_cell_guid": "2fee8167-079b-42b2-b2a8-63dc49836c8f", "_uuid": "d0cb56d041591094019058ca4e8fea0c6b5cd079"}, "cell_type": "markdown", "source": ["## Build Markov Model that remebers the Two previous chars\n", "$$ P(c_t|c_{t-1},c_{t-2},Author)  $$"]}, {"metadata": {"_cell_guid": "76b40bfe-d801-482a-8e8b-86b2d870fde3", "_uuid": "f944b738eba926fce7ebe9e3ea6e3e669ff9f4ba", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate P(c(t)|c(t-1),c(t-2)) model (Markov Model with memory of 2 time steps)\n", "historyLength = 2\n", "\n", "charCondProbModel_H2 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondProbModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "    \n", "charCondCountModel_H2 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondCountModel_H2[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "\n", "corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\n", "for author in corpusDF['author'].unique():\n", "    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n", "    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n", "    for history in authorCorpusDF['history'].unique():\n", "        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n", "\n", "        encodedHistory = charEncoder.transform([ch for ch in history])\n", "        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n", "        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n", "\n", "        charCondProbModel_H2[author][encodedHistory[0],encodedHistory[1],:]  = encodedNextCharProb\n", "        charCondCountModel_H2[author][encodedHistory[0],encodedHistory[1],:] = encodedNextCharCounts\n", "\n", "    condCount = charCondCountModel_H2[author]\n", "    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n", "\n", "charCondProbModel_H2['all']  = (charCondProbModel_H2['EAP']  + charCondProbModel_H2['HPL']  + charCondProbModel_H2['MWS'] )/3.0\n", "charCondCountModel_H2['all'] =  charCondCountModel_H2['EAP'] + charCondCountModel_H2['HPL'] + charCondCountModel_H2['MWS']\n", "\n", "condCount = charCondCountModel_H2['all']\n", "print('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))"]}, {"metadata": {"_cell_guid": "3b173b26-5f30-43bc-9a64-f51fd71b4b77", "_uuid": "9d57c1709e10e0fefa6aa6791ff5adab9916c67a"}, "cell_type": "markdown", "source": ["Note the Sparsity Levels increase quite a bit. This is due to the fact that there are 34^3 (~40,000) possible triplets, and most of them are illegal combinations in the english language.  \n", "\n", "But remember what we discussed earlier, here the problem of finite sample size is much more pronounced so we can be much less \"confident\" in these zeros. meaning, we can't be sure they are actually zeros and not simply very rare events that just didn't happen to occur in the particular realization of the training sample.    \n", "\n", "For this reason we will add a small constant number to the conditional probability distribution."]}, {"metadata": {"_cell_guid": "dc93dd2f-9d1b-49af-85de-11560a5d368e", "_uuid": "a20e1faa027521f1f1cc6f2c29c5c966f8258ebe"}, "cell_type": "markdown", "source": ["## Calculate Classification Accuracy of our 2 time step Markov Model\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=3}^{n}P(c_t|c_{t-1},c_{t-2},Author)\\}   $$"]}, {"metadata": {"_cell_guid": "d4b81598-7c0a-465f-aafc-40666049732d", "_uuid": "f78d9c85287fb88551a2b9dea9feeda7e117b48c", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% meassure classification accuracy on validation set using Markov Model with memory of 2 time steps\n", "uniformPriorFraction    = 0.0001\n", "allAuthorsPriorFraction = 0.0001\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/len(charEncoder.classes_)\n", "\n", "condP_H2 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]\n", "    condP_H2[author] += prior[1]*charCondProbModel_H2['all']\n", "    condP_H2[author] += prior[2]*uniformPriorValue\n", "\n", "authorPredictionList = []\n", "for k, (sentence, author) in enumerate(zip(validText,validLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    \n", "    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n", "    for histChars, nextChar in zip(history,nextChar):\n", "        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n", "        encodedNextChar  = charEncoder.transform([nextChar])[0]\n", "        \n", "        logP_EAP += np.log(condP_H2['EAP'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n", "        logP_HPL += np.log(condP_H2['HPL'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n", "        logP_MWS += np.log(condP_H2['MWS'][encodedHistChars[0],encodedHistChars[1],encodedNextChar])\n", "    \n", "    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n", "\n", "print(52*'-')\n", "print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\n", "print(52*'-')"]}, {"metadata": {"_cell_guid": "2198807e-f7e6-4b49-8e38-14fb957956a0", "_uuid": "dba804b17c65bfd7264803179c0458e7ef8829fa"}, "cell_type": "markdown", "source": ["The Classification Accuracy keeps rising, which is quite nice.  \n", "We are now quite capable in distinguishing between the three authors.  \n", "***Does this also mean that our model has the ability to write text like our authors?***"]}, {"metadata": {"_cell_guid": "cd99de2a-a6d4-40f5-b357-f1d3e7069aa0", "_uuid": "37c7b66cc0bd98c84e90ce232256b51552b699bf"}, "cell_type": "markdown", "source": ["## Generate Sample Text for each Author using our 2 time step Markov Model\n", "$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},Author)   $$"]}, {"metadata": {"_cell_guid": "12cb1539-4646-4a28-9956-b471e3125c2e", "_uuid": "61a7396a5966fc8f7be33c06c75c2fa0c8bf0bf1", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate sample text by sampling one charachter at a time from the 2 time step Markov Model\n", "np.random.seed(1000)\n", "\n", "maxSentenceLength = 95\n", "numSentencesPerAuthor = 9\n", "\n", "uniformPriorFraction    = 0.0001\n", "allAuthorsPriorFraction = 0.0009\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/(len(charEncoder.classes_))\n", "\n", "condP_H2 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H2[author]  = prior[0]*charCondProbModel_H2[author]\n", "    condP_H2[author] += prior[1]*charCondProbModel_H2['all']\n", "    condP_H2[author] += prior[2]*uniformPriorValue\n", "\n", "condP_H2['all']  = (prior[0]+prior[1])*charCondProbModel_H2['all']\n", "condP_H2['all'] += prior[2]*uniformPriorValue\n", "\n", "for author in ['EAP','HPL','MWS','all']:\n", "    print((6+maxSentenceLength)*'-')\n", "    print('Author %s:' %(author))\n", "    print(12*'-')\n", "    for i in range(numSentencesPerAuthor):\n", "        firstChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n", "        encodedFirstChar = charEncoder.transform([firstChar])[0]\n", "        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]\n", "        generatedSentence = firstChar + secondChar\n", "        \n", "        for j in range(maxSentenceLength-1):\n", "            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-2:]])            \n", "            currCondProb = condP_H2[author][encodedHistChars[0],encodedHistChars[1],:]\n", "            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1\n", "            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n", "            generatedSentence += newChar\n", "            \n", "            if (newChar == '.') or (j == maxSentenceLength):\n", "                break\n", "        print('%d: \"%s\"' %(i+1,generatedSentence))\n", "print((4+maxSentenceLength)*'-')"]}, {"metadata": {"_cell_guid": "878e207c-616f-4848-a9ff-d3aa34190687", "_uuid": "75020628183f847872f054e1049022d07814e7fc"}, "cell_type": "markdown", "source": ["Clearly the answer is no.  Our model is not as capable as our authors.  \n", "Nevertheless, we see that the text looks even better now. A lot of short 2-4 letter words like  \"on\", \"to\", \"we\", \"me\", \"of\", \"the\", \"for\", \"now\", \"age\", \"hate\", \"thin\", \"eyes\" appear quite often.   \n", "A completely different world relative to the independent model we saw first that looked like a complete jumble."]}, {"metadata": {"_cell_guid": "c6d5c483-8071-4492-8e4c-401e217e104c", "_uuid": "090481c2cbe50010dbb83256f8d1be9f1648e683"}, "cell_type": "markdown", "source": ["# Let's Repeat the process with History size of 3 chars\n", "## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\n", "Use history of size 3 characters"]}, {"metadata": {"_cell_guid": "24be1e75-f405-433a-bf8a-290a00241b0d", "_uuid": "29a2f8516b8ef49dfcdc0edd6491b98a2e5ee1eb", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% gather all quadruplets of characters into a single dataframe\n", "historyLength = 3\n", "\n", "historyList  = []\n", "nextCharList = []\n", "authorList   = []\n", "for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n", "    \n", "    historyList  += history\n", "    nextCharList += nextChar\n", "    authorList   += [author]*len(history)\n", "        \n", "corpusDF = pd.DataFrame(columns=['author','history','next char'])\n", "corpusDF['author']    = authorList\n", "corpusDF['history']   = historyList\n", "corpusDF['next char'] = nextCharList\n", "\n", "corpusDF.head(8)"]}, {"metadata": {"_cell_guid": "8abc1939-7943-4905-8ad1-8c5fe0424466", "_uuid": "363f7bd9b27b467950281a88e34e90baba21fbfe"}, "cell_type": "markdown", "source": ["## Build Markov Model that remebers the 3 previous chars\n", "$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)  $$"]}, {"metadata": {"_cell_guid": "c13672f5-07f1-45e3-9445-74b0fab58d05", "_uuid": "0cb0f67b2d748a518a59316f992cea4b6777aafe", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate P(c(t)|c(t-1),c(t-2),c(t-3)) model (Markov Model with memory of 3 time steps)\n", "historyLength = 3\n", "\n", "charCondProbModel_H3 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondProbModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "    \n", "charCondCountModel_H3 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondCountModel_H3[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "\n", "corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\n", "for author in corpusDF['author'].unique():\n", "    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n", "    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n", "    for history in authorCorpusDF['history'].unique():\n", "        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n", "\n", "        encodedHistory = charEncoder.transform([ch for ch in history])\n", "        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n", "        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n", "\n", "        charCondProbModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:]  = encodedNextCharProb\n", "        charCondCountModel_H3[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],:] = encodedNextCharCounts\n", "\n", "    condCount = charCondCountModel_H3[author]\n", "    print('%s Sparsity level = %.1f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n", "\n", "charCondProbModel_H3['all']  = (charCondProbModel_H3['EAP']  + charCondProbModel_H3['HPL']  + charCondProbModel_H3['MWS'] )/3.0\n", "charCondCountModel_H3['all'] =  charCondCountModel_H3['EAP'] + charCondCountModel_H3['HPL'] + charCondCountModel_H3['MWS']\n", "\n", "condCount = charCondCountModel_H3['all']\n", "print('average Sparsity level = %.1f%s' %(100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))"]}, {"metadata": {"_cell_guid": "39bf2f05-a0ce-4054-a03b-6befaec7b9d3", "_uuid": "a2b4493ddf69fe541806e1daab0fee0c8d6d937a"}, "cell_type": "markdown", "source": ["Note the sparsity is increasing, as one would expect."]}, {"metadata": {"_cell_guid": "1228ccd9-9c1d-468a-9a19-0dd8ff571e7c", "_uuid": "0897b456b66d62d2f2540558559f5e93ea136a3e"}, "cell_type": "markdown", "source": ["## Calculate Classification Accuracy of Markov Model that remebers 3 time steps back\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=4}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)\\}   $$"]}, {"metadata": {"_cell_guid": "579b827a-2c51-4d3b-bec5-ae5fadddb82a", "_uuid": "caf41f65851b54282fd9fdfa6c7b1ec84df07953", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% meassure classification accuracy on validation set using Markov Model with memory of 3 time steps\n", "uniformPriorFraction    = 0.05\n", "allAuthorsPriorFraction = 0.05\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/(len(charEncoder.classes_))\n", "\n", "condP_H3 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]\n", "    condP_H3[author] += prior[1]*charCondProbModel_H3['all']\n", "    condP_H3[author] += prior[2]*uniformPriorValue\n", "\n", "condP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']\n", "condP_H3['all'] += prior[2]*uniformPriorValue\n", "\n", "authorPredictionList = []\n", "for k, (sentence, author) in enumerate(zip(validText,validLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    \n", "    logP_EAP = 0.0; logP_HPL = 0.0; logP_MWS = 0.0\n", "    for histChars, nextChar in zip(history,nextChar):\n", "        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n", "        encodedNextChar  = charEncoder.transform([nextChar])[0]\n", "        \n", "        logP_EAP += np.log(condP_H3['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n", "        logP_HPL += np.log(condP_H3['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n", "        logP_MWS += np.log(condP_H3['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedNextChar])\n", "    \n", "    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n", "\n", "print(52*'-')\n", "print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\n", "print(52*'-')"]}, {"metadata": {"_cell_guid": "3e9fad7f-1fcc-4972-8ebf-5381ca9059a3", "_uuid": "2529de1d007245f95bc2cce1fb6a6d4f75a72e8b"}, "cell_type": "markdown", "source": ["The accuracy is already quite high!"]}, {"metadata": {"_cell_guid": "9337aee6-6fc8-46d7-8153-2e1a644e7946", "_uuid": "77b5f0ee4c108baa2d21f55eb97e0730e6e1f02e"}, "cell_type": "markdown", "source": ["## Generate Sample Text for each Author using our 3 time step Markov Model\n", "$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},c_{t-3},Author)   $$"]}, {"metadata": {"_cell_guid": "965200a8-b2d4-4e16-a405-b04fddf2ffed", "_uuid": "f58e6ac023d3b201769d9e4a04f58ca23f08c2b3", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate sample text by sampling one charachter at a time from the 3 time step Markov Model\n", "np.random.seed(123)\n", "\n", "maxSentenceLength = 95\n", "numSentencesPerAuthor = 9\n", "\n", "uniformPriorFraction    = 0.05\n", "allAuthorsPriorFraction = 0.05\n", "\n", "prior = np.array([1.0-uniformPriorFraction-allAuthorsPriorFraction, allAuthorsPriorFraction, uniformPriorFraction])\n", "uniformPriorValue = 1.0/(len(charEncoder.classes_))\n", "\n", "condP_H3 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    condP_H3[author]  = prior[0]*charCondProbModel_H3[author]\n", "    condP_H3[author] += prior[1]*charCondProbModel_H3['all']\n", "    condP_H3[author] += prior[2]*uniformPriorValue\n", "\n", "condP_H3['all']  = (prior[0]+prior[1])*charCondProbModel_H3['all']\n", "condP_H3['all'] += prior[2]*uniformPriorValue\n", "\n", "for author in ['EAP','HPL','MWS','all']:\n", "    print((6+maxSentenceLength)*'-')\n", "    print('Author %s:' %(author))\n", "    print(12*'-')\n", "    for i in range(numSentencesPerAuthor):\n", "        # sample c(1) ~ P(c(t))\n", "        firstChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=charProbModel[author])][0]\n", "        encodedFirstChar = charEncoder.transform([firstChar])[0]\n", "        # sample c(2) ~ P(c(t)|c(t-1))\n", "        secondChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H1[author][encodedFirstChar,:])][0]\n", "        encodedSecondChar = charEncoder.transform([secondChar])[0]\n", "        # sample c(3) ~ P(c(t)|c(t-1),c(t-2))\n", "        thirdChar  = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=condP_H2[author][encodedFirstChar,encodedSecondChar,:])][0]\n", "        generatedSentence = firstChar + secondChar + thirdChar\n", "        \n", "        for j in range(maxSentenceLength-1):\n", "            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            \n", "            currCondProb = condP_H3[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],:]\n", "            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum directly to 1\n", "            \n", "            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))\n", "            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n", "            generatedSentence += newChar\n", "            \n", "            if (newChar == '.') or (j == maxSentenceLength):\n", "                break\n", "        print('%d: \"%s\"' %(i+1,generatedSentence))\n", "print((4+maxSentenceLength)*'-')"]}, {"metadata": {"_cell_guid": "38b13990-c9d2-4a40-ad4a-157984a7f553", "_uuid": "5d9a744204864d9c96f8120bfa6088aa86a28243"}, "cell_type": "markdown", "source": ["Note the large number of legal english words in the generated text.  \n", "Our probabalistic model has managed to learn a lot of english words.  \n", "\n", "Also note the relativley **long** 6+ letter words the model generates, such as \"**exceeded**\", \"**remain**\", \"**expect**\" \"**danger**\" and \"**struct**\", this while our model only remebers directly 4 character sequences, it manages to concatenate several such sequences together to form a longer coherent sequence at least some of the time."]}, {"metadata": {"_cell_guid": "4efd64c2-ea95-4599-9649-1084ee771256", "_uuid": "e296a3eac1d745db4bd37c90941316d0e4d3d390"}, "cell_type": "markdown", "source": ["# Let's Repeat the process one last time with History size of 4 chars\n", "## Gather DataFrame with \"Author\", \"History\" and \"Next Char\" Fields\n", "Use history of size 4 characters"]}, {"metadata": {"_cell_guid": "00109242-cc73-44c2-809c-bde0a19fa9e7", "_uuid": "a4681bb35a7bedaa0e5079188bf3a2d86af105ec", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% gather all 5-wise of characters into a single dataframe\n", "historyLength = 4\n", "\n", "historyList  = []\n", "nextCharList = []\n", "authorList   = []\n", "for k, (sentence, author) in enumerate(zip(trainText,trainLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    sentenceDF = pd.DataFrame(columns=['author','history','next char'])\n", "    \n", "    historyList  += history\n", "    nextCharList += nextChar\n", "    authorList   += [author]*len(history)\n", "        \n", "corpusDF = pd.DataFrame(columns=['author','history','next char'])\n", "corpusDF['author']    = authorList\n", "corpusDF['history']   = historyList\n", "corpusDF['next char'] = nextCharList\n", "\n", "corpusDF.head(15)"]}, {"metadata": {"_cell_guid": "3503925f-95ac-4c5e-8da8-34df34485080", "_uuid": "9eb02fb69ffac995b8d656015d68f2bf794a9c12"}, "cell_type": "markdown", "source": ["## Build Markov Model that remebers the 4 previous chars\n", "$$ P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)  $$"]}, {"metadata": {"_cell_guid": "43277f04-2c10-44c5-9628-8a68e10d381a", "_uuid": "965171de0597c9066be53f085a19a92660cef0fa", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% generate P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4)) model (Markov Model with memory of 4 time steps)\n", "historyLength = 4\n", "\n", "charCondProbModel_H4 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondProbModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "    \n", "charCondCountModel_H4 = {}\n", "for author in ['EAP','HPL','MWS']:\n", "    charCondCountModel_H4[author] = np.zeros( (1+historyLength)*[charEncoder.classes_.shape[0]] )\n", "\n", "corpusGroupedByAuthor = corpusDF.groupby(by='author',axis=0)\n", "for author in corpusDF['author'].unique():\n", "    authorCorpusDF = corpusGroupedByAuthor.get_group(author).loc[:,['history','next char']].reset_index(drop=True)\n", "    authorCorpusGroupedByHistory = authorCorpusDF.groupby(by='history',axis=0)\n", "    for history in authorCorpusDF['history'].unique():\n", "        authorHistoryDF = authorCorpusGroupedByHistory.get_group(history).reset_index(drop=True).loc[:,'next char'].reset_index(drop=True)\n", "\n", "        encodedHistory = charEncoder.transform([ch for ch in history])\n", "        encodedNextCharCounts = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=False)[0]\n", "        encodedNextCharProb   = np.histogram(charEncoder.transform(authorHistoryDF),range(len(charEncoder.classes_)+1),density=True)[0]\n", "\n", "        charCondProbModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:]  = encodedNextCharProb\n", "        charCondCountModel_H4[author][encodedHistory[0],encodedHistory[1],encodedHistory[2],encodedHistory[3],:] = encodedNextCharCounts\n", "\n", "    condCount = charCondCountModel_H4[author]\n", "    print('%s Sparsity level = %.2f%s' %(author, 100*(condCount < 1).sum() / (condCount > -1).sum().astype(float),'%'))\n", "\n", "charCondProbModel_H4['all']  = (charCondProbModel_H4['EAP']  + charCondProbModel_H4['HPL']  + charCondProbModel_H4['MWS'] )/3.0\n", "charCondCountModel_H4['all'] =  charCondCountModel_H4['EAP'] + charCondCountModel_H4['HPL'] + charCondCountModel_H4['MWS']\n", "\n", "condCount = charCondCountModel_H4['all']\n", "print('average Sparsity level = %.2f%s' %(100*((condCount < 1).sum() / (condCount > -1).sum().astype(float)),'%'))"]}, {"metadata": {"_cell_guid": "54449f88-f9ca-45ab-9207-a7491d080de1", "_uuid": "4817556fab2da9afaee1434546cec142dc69f82b"}, "cell_type": "markdown", "source": ["## Calculate Classification Accuracy of Markov Model that remebers 4 time steps back\n", "$$  \\mathbf{predicted\\:Author} = argmax\\:\\{ \\prod_{t=5}^{n}P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)P(Author)\\}   $$  \n", "**Note:** I've added also the prior over authors, to give a small additional performance boost"]}, {"metadata": {"_cell_guid": "fa3de034-ee60-4a18-85f5-33b32953ce34", "_uuid": "3204432d89d1da58937044b3b25f39e53fc870a8", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% meassure classification accuracy on validation set using Markov Model with memory of 4 time steps\n", "condP_H4_PriorWeight_specific = 70\n", "condP_H4_PriorWeight_all      = 30\n", "\n", "condP_H3_PriorWeight_specific = 70\n", "condP_H3_PriorWeight_all      = 30\n", "\n", "uniformPriorWeight            = 10\n", "\n", "logP_EAP_prior = np.log((trainLabel == 'EAP').mean())\n", "logP_HPL_prior = np.log((trainLabel == 'HPL').mean())\n", "logP_MWS_prior = np.log((trainLabel == 'MWS').mean())\n", "\n", "numChars = len(charEncoder.classes_)\n", "prior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, \n", "                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])\n", "prior = prior.astype(float) / prior.sum()\n", "\n", "uniformPriorValue = 1.0/numChars\n", "\n", "condP_H4 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))\n", "    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]\n", "    condP_H4[author] += prior[1]*charCondProbModel_H4['all']\n", "    \n", "    # get \"prior\" from P(c(t)|c(t-1),c(t-2),c(t-3))\n", "    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])\n", "    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])\n", "    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific\n", "    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all\n", "\n", "    condP_H4[author] += prior[4]*uniformPriorValue\n", "\n", "condP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0\n", "\n", "authorPredictionList = []\n", "logProbGivenAuthor = np.zeros((len(validLabel),3))\n", "for i, (sentence, author) in enumerate(zip(validText,validLabel)):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    \n", "    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;\n", "    for histChars, nextChar in zip(history,nextChar):\n", "        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n", "        encodedNextChar  = charEncoder.transform([nextChar])[0]\n", "        \n", "        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "        \n", "        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]\n", "        \n", "    authorPredictionList.append(authorsList[np.argmax([logP_EAP,logP_HPL,logP_MWS])])\n", "\n", "print(52*'-')\n", "print('==> Validation Set Classification Accuracy = %.1f%s' %(100*(validLabel == authorPredictionList).mean(),'%'))\n", "print(52*'-')"]}, {"metadata": {"_cell_guid": "aba94fa4-03f8-4a91-b5ea-732d52188f6c", "_uuid": "bbdd8d5fea84254da5ee54cd960588e8075ad3b0"}, "cell_type": "markdown", "source": ["The identification accuracy has reached quite a high level now.   \n", "\n", "But we do see a saturation effect here. The improvment from history of 3 characters to 4 characters is not as large as the improvment from 2 character history to 3 character history."]}, {"metadata": {"_cell_guid": "dd26fe14-5e96-4cbf-b951-4cba641d9720", "_uuid": "079a4d50d312f0e7be40cd71abb1985dbd0fdffe"}, "cell_type": "markdown", "source": ["## Let's calculate also the log loss\n", "In order to relate this to LB results"]}, {"metadata": {"_cell_guid": "34e8eaad-799b-419a-a40e-fdfb1586e923", "_uuid": "c128219c6d471257596c3d5d16c95a6f0d4c67cc", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% calculate log loss\n", "minimalLogP = -15.0\n", "uniformPriorWeight = 0.09\n", "\n", "authorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])\n", "authorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP\n", "\n", "authorProb = np.exp(authorLogProb_norm)\n", "authorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])\n", "\n", "y_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm\n", "\n", "labelEncoder = preprocessing.LabelEncoder()\n", "y_GT = labelEncoder.fit_transform(validLabel)\n", "\n", "print(34*'-')\n", "print('Validation Set Log Loss = %.5f' %(log_loss(y_GT, y_Hat)))\n", "print(34*'-')"]}, {"metadata": {"_cell_guid": "a0e6a97f-e745-4518-978b-60025bcb816c", "_uuid": "f1d84fc1136fe625a005e6fd7b97e0748a021015"}, "cell_type": "markdown", "source": ["## Generate Sample Text for each Author using our 4 time step Markov Model\n", "$$  c_t\\: {\\raise.17ex\\hbox{$\\scriptstyle\\mathtt{\\sim}$}} \\:  P(c_t|c_{t-1},c_{t-2},c_{t-3},c_{t-4},Author)   $$  \n", "**Just for fun**, let's start all sentences with 'disp' and see how they evolve from there"]}, {"metadata": {"_cell_guid": "c1b25e6f-1e32-4791-9cba-9ad5ab81c862", "_uuid": "7fb851655e4985542daf35d1473e528306645551", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["np.random.seed(1000)\n", "\n", "maxSentenceLength = 95\n", "numSentencesPerAuthor = 9\n", "\n", "condP_H4_PriorWeight_specific = 10\n", "condP_H4_PriorWeight_all      = 10\n", "\n", "condP_H3_PriorWeight_specific = 1\n", "condP_H3_PriorWeight_all      = 1\n", "\n", "uniformPriorWeight            = 1\n", "\n", "numChars = len(charEncoder.classes_)\n", "prior = np.array([condP_H4_PriorWeight_specific, condP_H4_PriorWeight_all, \n", "                  condP_H3_PriorWeight_specific, condP_H3_PriorWeight_all, uniformPriorWeight])\n", "prior = prior.astype(float) / prior.sum()\n", "\n", "uniformPriorValue = 1.0/numChars\n", "\n", "condP_H4 = {}\n", "authorsList = ['EAP','HPL','MWS']\n", "for author in authorsList:\n", "    # get P(c(t)|c(t-1),c(t-2),c(t-3),c(t-4))\n", "    condP_H4[author]  = prior[0]*charCondProbModel_H4[author]\n", "    condP_H4[author] += prior[1]*charCondProbModel_H4['all']\n", "    \n", "    # get prior from P(c(t)|c(t-1),c(t-2),c(t-3))\n", "    condP_H4_from_CondP_H3_specific = np.tile(charCondProbModel_H3[author][np.newaxis,:,:,:],[numChars,1,1,1,1])\n", "    condP_H4_from_CondP_H3_all      = np.tile(charCondProbModel_H3['all'][np.newaxis,:,:,:],[numChars,1,1,1,1])\n", "    condP_H4[author] += prior[2]*condP_H4_from_CondP_H3_specific\n", "    condP_H4[author] += prior[3]*condP_H4_from_CondP_H3_all\n", "\n", "    condP_H4[author] += prior[4]*uniformPriorValue\n", "\n", "condP_H4['all']  = (condP_H4['EAP'] + condP_H4['HPL'] + condP_H4['MWS'])  / 3.0\n", "\n", "for author in ['EAP','HPL','MWS','all']:\n", "    print((6+maxSentenceLength)*'-')\n", "    print('Author %s:' %(author))\n", "    print(12*'-')\n", "    for i in range(numSentencesPerAuthor):\n", "        generatedSentence = 'disp'\n", "        for j in range(maxSentenceLength-1):\n", "            encodedHistChars = charEncoder.transform([ch for ch in generatedSentence[-historyLength:]])            \n", "            currCondProb = condP_H4[author][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],:]\n", "            currCondProb = currCondProb/currCondProb.sum() # just in case the probabilities don't sum exactly to 1\n", "            \n", "            # sample c(t) ~ P(c(t)|c(t-1),c(t-2),c(t-3))\n", "            newChar = charEncoder.classes_[np.random.choice(range(len(charCounts_EAP)),size=1,p=currCondProb)][0]\n", "            generatedSentence += newChar\n", "            \n", "            if (newChar == '.') or (j == maxSentenceLength):\n", "                break\n", "        print('%d: \"%s\"' %(i+1,generatedSentence))\n", "print((4+maxSentenceLength)*'-')"]}, {"metadata": {"_cell_guid": "6c36d7db-d4b8-4543-be5f-571375d7b483", "_uuid": "96c7373a14fd3e16f601106174bb50a1d751d2df"}, "cell_type": "markdown", "source": ["Here we can almost see sentences:  \n", "\"...and i says: \"yes, the...\"  \n", "\"...when i spoken to put to think till my feel the also...\"   \n", "\"...displayed on the done dark happines atter...\"  \n", "\"...\"why, so much as countain, and sent...\"\n", "\n", "Since we really know exactly what the model entails and it's simplicity, i.e. just storing conditional probabilities, and we see the generative performance of this model, that can remember english words and almost construct something that looks like actual sentences, this might make some of us wonder \"could it be that our brains are just a somewhat better probability estimation machine?\"  \n", "My answer to this question would be \"most likely yes\" :-)"]}, {"metadata": {"_cell_guid": "2161e250-74f1-4cea-9b62-de9616ca74f3", "_uuid": "3f3ada711f84bd0ad1c66c81ad9a2ef23ed39a57"}, "cell_type": "markdown", "source": ["# Create a Submission on the Test Set\n", "This submission might be useful for an ensemble if you haven't used any char based models yet"]}, {"metadata": {"_cell_guid": "7738dff7-aded-4a31-84e9-34349d4995cc", "_uuid": "fdc57136c72cf8708ee2e303df4a759236c90f43", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% create a submission\n", "# load test data\n", "testData = pd.read_csv('../input/test.csv')\n", "testText = testData.loc[:,'text'].reset_index(drop=True)\n", "\n", "# calculate log prob predictions\n", "logProbGivenAuthor = np.zeros((len(testText),3))\n", "for i, sentence in enumerate(testText):\n", "    decodedSentence = myunidecode(sentence.lower())\n", "    charSequences = [decodedSentence[k:k+historyLength+1] for k in range(len(decodedSentence)-historyLength)]\n", "    \n", "    history  = [seq[:-1] for seq in charSequences]\n", "    nextChar = [seq[ -1] for seq in charSequences]\n", "    \n", "    logP_EAP = logP_EAP_prior; logP_HPL = logP_HPL_prior; logP_MWS = logP_MWS_prior;\n", "    for histChars, nextChar in zip(history,nextChar):\n", "        encodedHistChars = charEncoder.transform([ch for ch in histChars])\n", "        encodedNextChar  = charEncoder.transform([nextChar])[0]\n", "        \n", "        logP_EAP += np.log(condP_H4['EAP'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "        logP_HPL += np.log(condP_H4['HPL'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "        logP_MWS += np.log(condP_H4['MWS'][encodedHistChars[0],encodedHistChars[1],encodedHistChars[2],encodedHistChars[3],encodedNextChar])\n", "    \n", "        logProbGivenAuthor[i,:] = [logP_EAP,logP_HPL,logP_MWS]"]}, {"metadata": {"_cell_guid": "bfeb3c3f-29cd-4906-a974-e8f18c2ed2e5", "_uuid": "01fee243bf426df9372514b1d40ef69dadb66eb0", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# convert log probabilities to final predictions\n", "minimalLogP = -15.0\n", "uniformPriorWeight = 0.09\n", "\n", "authorLogProb_norm = logProbGivenAuthor - np.tile(logProbGivenAuthor.max(axis=1)[:,np.newaxis], [1,3])\n", "authorLogProb_norm[authorLogProb_norm < minimalLogP] = minimalLogP\n", "authorProb = np.exp(authorLogProb_norm)\n", "authorProb_norm = authorProb / np.tile(authorProb.sum(axis=1)[:,np.newaxis],[1, 3])\n", "y_Hat = uniformPriorWeight*(1/3.0) + (1.0-uniformPriorWeight)*authorProb_norm\n", "\n", "# write a submission\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,['EAP', 'HPL', 'MWS']] = y_Hat\n", "submission.to_csv(\"Markov_char_given_4charHistory.csv\", index=False)\n", "submission.head(10)"]}, {"metadata": {"_cell_guid": "0a892f17-360a-41e5-a7a7-f40b1955dda6", "_uuid": "c0367ce89cd1def7d8a5a55c1982f90e2f6413e8"}, "cell_type": "markdown", "source": ["# Apply Fully Discriminative Approach\n", "1. Extract **Bag of Character n-grams** features\n", "1. Create a submission for **Logistic Regression over *BagOfChar***\n", "1. Extract **Bag of Word n-grams** features\n", "1. Create a submission for **Logistic Regression over *BagOfWord***\n", "1. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***"]}, {"metadata": {"_cell_guid": "91a7fa24-c908-4410-b0ff-447c15eeb6f4", "_uuid": "6e4a5e8c61b98a16a97cf20b9c1af0e803984cb9"}, "cell_type": "markdown", "source": ["### 1. Extract **Bag of Character n-grams** features"]}, {"metadata": {"_cell_guid": "3102d943-bbf7-4513-8112-aa70a013b51a", "_uuid": "0a16e192a725f5bb817847b9ec5e417900d8df4c", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import time\n", "import scipy\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "#%% Create a Bag of Char n-grams + logistic regression model\n", "ngramLength = 5\n", "\n", "featureExtractionStartTime = time.time()\n", "print('-'*52)\n", "print('fitting \"CountVectorizer()\" for bag of char %d-grams' %(ngramLength))\n", "\n", "BagOfCharsExtractor = CountVectorizer(min_df=8, max_features=250000, \n", "                                      analyzer='char', ngram_range=(1,ngramLength), \n", "                                      binary=False,lowercase=True)\n", "\n", "BagOfCharsExtractor.fit(pd.concat((trainText,validText,testText)))\n", "\n", "X_train_char = BagOfCharsExtractor.transform(trainText)\n", "X_valid_char = BagOfCharsExtractor.transform(validText)\n", "X_test_char  = BagOfCharsExtractor.transform(testText)\n", "\n", "featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\n", "print(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))\n", "print('number of \"bag of char %d-gram\" features = %d' %(ngramLength, X_train_char.shape[1]))\n", "print('-'*52)\n", "\n", "# scale inputs so that they will be in similar value range\n", "stdScaler = preprocessing.StandardScaler(with_mean=False)\n", "stdScaler.fit(scipy.sparse.vstack(((X_train_char,X_valid_char,X_test_char))))\n", "\n", "X_train_norm = stdScaler.transform(X_train_char)\n", "X_valid_norm = stdScaler.transform(X_valid_char)\n", "X_test_norm  = stdScaler.transform(X_test_char)\n", "\n", "# create labels for classification\n", "yLabelEncoder = preprocessing.LabelEncoder()\n", "yLabelEncoder.fit(pd.concat((trainLabel,validLabel)))\n", "\n", "y_train = yLabelEncoder.transform(trainLabel)\n", "y_valid = yLabelEncoder.transform(validLabel)\n", "\n", "##%% check performance on validation set\n", "validationStartTime = time.time()\n", "print('-'*42)\n", "print('fitting \"LogisticRegression()\" classifier')\n", "\n", "logisticRegressor_char = linear_model.LogisticRegression(C=0.01, solver='sag')\n", "logisticRegressor_char.fit(X_train_norm, y_train)\n", "\n", "trainAccuracy = accuracy_score(y_train, logisticRegressor_char.predict(X_train_norm))\n", "validAccuracy = accuracy_score(y_valid, logisticRegressor_char.predict(X_valid_norm))\n", "trainLogLoss  = log_loss(y_train, logisticRegressor_char.predict_proba(X_train_norm))\n", "validLogLoss  = log_loss(y_valid, logisticRegressor_char.predict_proba(X_valid_norm))\n", "\n", "validationDurationInMinutes = (time.time()-validationStartTime)/60.0\n", "\n", "print('Validation took %.2f minutes' % (validationDurationInMinutes))\n", "print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\n", "print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\n", "print('-'*42)"]}, {"metadata": {"_cell_guid": "36f18513-a280-4a63-81e8-75fef2740be4", "_uuid": "06412dd5b3bcf5674bf7536b59a295740e52a05d"}, "cell_type": "markdown", "source": ["### 2. Create a submission for **Logistic Regression over *BagOfChar***\n"]}, {"metadata": {"_cell_guid": "1fd3a31d-7e77-40f9-9307-092ea4ee2861", "_uuid": "20ede82fde50cc4430d11459b1d4ccdd3fa85db3", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# write a submission\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_char.predict_proba(X_test_norm)\n", "submission.to_csv(\"LogisticRegression_Over_BagOfCharNGrams.csv\", index=False)\n", "submission.head(10)"]}, {"metadata": {"_cell_guid": "debe90b5-8ff6-46e1-be50-c9448bc0f5ef", "_uuid": "98f4cb50c786298a01cef42e3e47539a06368ac8"}, "cell_type": "markdown", "source": ["### 3. Extract **Bag of Word n-grams** features"]}, {"metadata": {"_cell_guid": "02dcd5a2-9333-4003-baec-2dd7f70293c5", "_uuid": "b1915889af860022ec2294afe5965f77997e4fa2", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["ngramLength = 2\n", "\n", "featureExtractionStartTime = time.time()\n", "print('-'*52)\n", "print('fitting \"CountVectorizer()\" for bag of word %d-grams' %(ngramLength))\n", "\n", "BagOfWordsExtractor = CountVectorizer(min_df=5, max_features=250000, \n", "                                      analyzer='word', ngram_range=(1,ngramLength), \n", "                                      binary=False,lowercase=True)\n", "\n", "BagOfWordsExtractor.fit(pd.concat((trainText,validText,testText)))\n", "\n", "X_train_word = BagOfWordsExtractor.transform(trainText)\n", "X_valid_word = BagOfWordsExtractor.transform(validText)\n", "X_test_word  = BagOfWordsExtractor.transform(testText)\n", "\n", "featureExtractionDurationInMinutes = (time.time()-featureExtractionStartTime)/60.0\n", "print(\"feature extraction took %.2f minutes\" % (featureExtractionDurationInMinutes))\n", "print('number of \"bag of word %d-gram\" features = %d' %(ngramLength, X_train_word.shape[1]))\n", "print('-'*52)\n", "\n", "# scale inputs so that they will be in similar value range\n", "stdScaler = preprocessing.StandardScaler(with_mean=False)\n", "stdScaler.fit(scipy.sparse.vstack(((X_train_word,X_valid_word,X_test_word))))\n", "\n", "X_train_norm = stdScaler.transform(X_train_word)\n", "X_valid_norm = stdScaler.transform(X_valid_word)\n", "X_test_norm  = stdScaler.transform(X_test_word)\n", "\n", "#\u00a3%% check performance on validation set\n", "validationStartTime = time.time()\n", "print('-'*42)\n", "print('fitting \"LogisticRegression()\" classifier')\n", "\n", "logisticRegressor_word = linear_model.LogisticRegression(C=0.01, solver='sag')\n", "logisticRegressor_word.fit(X_train_norm, y_train)\n", "\n", "trainAccuracy = accuracy_score(y_train, logisticRegressor_word.predict(X_train_norm))\n", "validAccuracy = accuracy_score(y_valid, logisticRegressor_word.predict(X_valid_norm))\n", "trainLogLoss  = log_loss(y_train, logisticRegressor_word.predict_proba(X_train_norm))\n", "validLogLoss  = log_loss(y_valid, logisticRegressor_word.predict_proba(X_valid_norm))\n", "\n", "validationDurationInMinutes = (time.time()-validationStartTime)/60.0\n", "\n", "print('Validation took %.2f minutes' % (validationDurationInMinutes))\n", "print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\n", "print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\n", "print('-'*42)"]}, {"metadata": {"_cell_guid": "57a32d34-6062-4478-9012-51d2c314d1f9", "_uuid": "4c2260d9410952cf8acfbb9c9a44281c8fb26799"}, "cell_type": "markdown", "source": ["### 4. Create a submission for **Logistic Regression over *BagOfWord***"]}, {"metadata": {"_cell_guid": "2cb3a48a-7dfc-47ec-9e41-278cd8458d6e", "_uuid": "5e50837618a6e9575e9dd2d7fa31156a358e7640", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# write a submission\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor_word.predict_proba(X_test_norm)\n", "submission.to_csv(\"LogisticRegression_Over_BagOfWordNGrams.csv\", index=False)\n", "submission.head(10)"]}, {"metadata": {"_cell_guid": "d808a158-3f77-4893-80df-a6b4e23d139b", "_uuid": "2d85f23089cc64f4aa837e0f17c3fab79019ead1"}, "cell_type": "markdown", "source": ["### 5. Create a submission for **Logistic Regression over both *BagOfWord and BagOfChar***"]}, {"metadata": {"_cell_guid": "4ea035b3-511f-4b37-82f5-e7a879228b0c", "_uuid": "ff600f404b0c60a6a223cb173d6cc9fbe5b70f22", "_kg_hide-input": true, "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#%% combine word and char features\n", "\n", "# combine and scale features \n", "X_train = scipy.sparse.hstack((X_train_word,X_train_char))\n", "X_valid = scipy.sparse.hstack((X_valid_word,X_valid_char))\n", "X_test  = scipy.sparse.hstack((X_test_word,X_test_char))\n", "\n", "stdScaler = preprocessing.StandardScaler(with_mean=False)\n", "stdScaler.fit(scipy.sparse.vstack(((X_train,X_valid,X_test))))\n", "\n", "X_train = stdScaler.transform(X_train)\n", "X_valid = stdScaler.transform(X_valid)\n", "X_test  = stdScaler.transform(X_test)\n", "\n", "##%% check performance on validation set\n", "\n", "validationStartTime = time.time()\n", "print('-'*42)\n", "print('fitting \"LogisticRegression()\" classifier')\n", "\n", "logisticRegressor = linear_model.LogisticRegression(C=0.01, solver='sag')\n", "logisticRegressor.fit(X_train, y_train)\n", "\n", "trainAccuracy = accuracy_score(y_train, logisticRegressor.predict(X_train))\n", "trainLogLoss = log_loss(y_train, logisticRegressor.predict_proba(X_train))\n", "validAccuracy = accuracy_score(y_valid, logisticRegressor.predict(X_valid))\n", "validLogLoss = log_loss(y_valid, logisticRegressor.predict_proba(X_valid))\n", "\n", "validationDurationInMinutes = (time.time()-validationStartTime)/60.0\n", "\n", "print('Validation took %.2f minutes' % (validationDurationInMinutes))\n", "print('Train: %.1f%s Accuracy, log loss = %.4f' % (100*trainAccuracy,'%',trainLogLoss))\n", "print('Valid: %.1f%s Accuracy, log loss = %.4f' % (100*validAccuracy,'%',validLogLoss))\n", "print('-'*42)\n", "\n", "# write a submission\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,yLabelEncoder.classes_.tolist()] = logisticRegressor.predict_proba(X_test)\n", "submission.to_csv(\"LogisticRegression_Over_BagOfWord_BagOfChar.csv\", index=False)\n", "submission.head(10)"]}, {"metadata": {"_cell_guid": "f3d87f19-538e-4e88-a0f6-de91e97976b3", "_uuid": "4330eef31c9afd2759ebaf627b34ef0dfc25f4dc", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": []}]}