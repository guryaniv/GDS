{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"metadata": {"_uuid": "3b09a292f157c9bcd444425bce50c7da1074a9b5", "collapsed": false, "_cell_guid": "56eb56e0-2c7e-46c9-bc66-826f7fb12500", "_execution_state": "idle"}, "source": "In this notebook, I scrape some weather data from the web, and use that in conjunction with the given data to train an xgboost model", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "255839b2d7f695117a3510903904f2e7db2f0d94", "collapsed": false, "_execution_state": "idle"}, "source": "", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c26b05826641a65a69127fe1c84e38d3e776960e", "collapsed": true, "trusted": false, "_cell_guid": "3a6408ca-7d39-41bc-acb3-d8e2952e424d", "_execution_state": "idle"}, "source": ["#Import all the stuff we need\n", "\n", "import numpy as np \n", "import pandas as pd \n", "import xgboost as xgb\n", "\n", "from bs4 import BeautifulSoup\n", "from urllib.request import urlopen\n", "from datetime import datetime, date, timedelta"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "1603cad4a6322b18fa4ea6090f19a354c7311465", "collapsed": false, "_execution_state": "idle"}, "source": "train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\nweather = pd.read_csv('../input/nyc-taxi-wunderground-weather/weatherdata.csv')\n", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "77260455c7e89f837241643df7e7e3622265bfd4", "_cell_guid": "42c812de-3818-4b71-8812-1b15986261f0"}, "source": "### Now, let's synthesize the data and add more columns based on some calculations. Even though most of these don't really add any new data, they make help XGBoost make connections it would otherwise miss.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "b00c725ae9819555e5ccec7e0eba075715d4c5f1", "collapsed": true, "trusted": false, "_cell_guid": "05b9b8ee-e82e-4679-b548-299c5a610360", "_execution_state": "idle"}, "source": ["# Full date isn't very useful, so let's just number the days\n", "def get_days_delta(datestr):\n", "    datetimeobj = datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S')\n", "    return (date(2016, 6, 30) - datetimeobj.date()).days\n", "\n", "# also grab the hour\n", "def get_hour(datestr):\n", "    datetimeobj = datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S')\n", "    return datetimeobj.hour\n", "\n", "# and the minute\n", "def get_minute(datestr):\n", "    datetimeobj = datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S')\n", "    return datetimeobj.minute\n", "\n", "# the distance between the two - don't bother with curavture of the earth effects\n", "def get_dist(plong, plat, dlong, dlat):\n", "    return np.sqrt((plong - dlong)**2 + \n", "                    (plat - dlat)**2)\n", "\n", "# the \"manahatten distance\", which is lengths added, not in quadriture\n", "# note: probably shouldn't just be lats and longs added, unless streets really\n", "# do run exactly N/S and E/W\n", "def get_mdist(plong, plat, dlong, dlat):\n", "    return np.abs(plong - dlong) + np.abs(plat - dlat)\n", "\n", "# the direction of each trip\n", "def get_dir(plong, plat, dlong, dlat):\n", "    return np.arctan2((plat - dlat), (plong - dlong))\n", "\n", "# whether it's a holiday or not\n", "def is_hol(datestr):\n", "    datetimeobj = datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S')\n", "    holiday_list = [date(2016, 1, 1), date(2016, 1, 18), date(2016, 2, 15), date(2016, 5, 30)]\n", "\n", "    return datetimeobj.date() in holiday_list\n", "\n", "# Here's where they all get added. Pretty self-explanatory, note I also do the log of each distance\n", "# as well as the day of the week\n", "def add_columns(df, weather):\n", "    df['day'] = np.vectorize(get_days_delta)(df['pickup_datetime'])\n", "    df['dow'] = np.vectorize(lambda x: x%7)(df['day'])\n", "    df['hour'] = np.vectorize(get_hour)(df['pickup_datetime'])\n", "    df['min'] = np.vectorize(get_minute)(df['pickup_datetime'])\n", "    df['is_hol'] = np.vectorize(is_hol)(df['pickup_datetime'])\n", "    df['storeflag'] = np.vectorize(lambda x: 0 if x=='N' else 1)(df['store_and_fwd_flag'])\n", "    df['dist'] = np.vectorize(get_dist)(df['pickup_longitude'], \n", "                                               df['pickup_latitude'], \n", "                                               df['dropoff_longitude'], \n", "                                               df['dropoff_latitude'])\n", "    df['mdist'] = np.vectorize(get_mdist)(df['pickup_longitude'], \n", "                                                 df['pickup_latitude'], \n", "                                                 df['dropoff_longitude'], \n", "                                                 df['dropoff_latitude'])\n", "    df['logdist'] = np.log(df['dist'] + 1)\n", "    df['logmdist'] = np.log(df['mdist'] + 1)\n", "    df['dir'] = np.vectorize(get_dir)(df['pickup_longitude'], \n", "                                               df['pickup_latitude'], \n", "                                               df['dropoff_longitude'], \n", "                                               df['dropoff_latitude'])\n", "    \n", "    # Now here is where the weather data is added\n", "    # Data frame must be sorted by date beforehand\n", "    # It starts with the first trip, scans through weather data until it passes\n", "    # the time of the trip start, then uses the next point. Starts with that point for\n", "    # next trip\n", "    start_index = 0\n", "    trip_precips = []\n", "    trip_temps = []\n", "\n", "    for ele in df['pickup_datetime']:\n", "        ts = datetime.strptime(ele, '%Y-%m-%d %H:%M:%S')\n", "        while True:\n", "            if ts > datetime.strptime(weather['timestamp'][start_index], '%Y-%m-%d %H:%M:%S'):\n", "                start_index += 1\n", "            else:\n", "                trip_precips.append(weather['precip'][start_index])\n", "                trip_temps.append(weather['temp'][start_index])\n", "                break\n", "    \n", "    # For speed, the data is initially put into lists, then added to dataframe later\n", "    df['temp'] = trip_temps\n", "    df['precip'] = trip_precips\n", "    df['isprecip'] = df['precip'] > 0"], "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7ce078260a8e3693bdaf1ebfc04dbd267b33dd58", "_cell_guid": "2ad981a7-63c1-40df-99bc-0c11dab517da"}, "source": ["## Now that we've got all of that done, it's time to import the data and do the processing"], "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "3c4d76cd077332c6b8d14fc25fb1ad11314d7211", "trusted": false, "_cell_guid": "714f3c15-d8bb-4643-9ee5-0c5058272862", "_execution_state": "busy"}, "source": "add_columns(train, weather)\nadd_columns(test, weather)\n\n# very important: since the evaluation metric is the RMSLE, predict the log of the trip duration\ntrain['logtime'] = np.log(train['trip_duration'] + 1)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6a06db1f0d63d4e1059c59ddce8e56aedc87a0fb", "_cell_guid": "978345c8-798f-4527-8f74-c0cc46fb8002"}, "source": "### Next, I need to pick out which columns I actually care about. \n\nCurrently, it looks like the log of the distances really aren't helping, so I've commented them out.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "7192aa39f08b97d4757e1d3bc12df91a066103ce", "collapsed": true, "trusted": false, "_cell_guid": "e56ba42e-12d3-4989-a29f-ec0352faef0a"}, "source": "cols = ['vendor_id', \n        'passenger_count', \n        'pickup_longitude', \n        'pickup_latitude', \n        'dropoff_longitude', \n        'dropoff_latitude', \n        'day', \n        'dow', \n        'hour', \n        'min', \n        'dist', \n        'dir', \n        'temp', \n        'precip', \n        #'logdist',\n        'storeflag', \n        'is_hol',\n        'mdist',\n        #'logmdist'\n       ]", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "f08574f13959b1c829e133d3ef202005ad9676a1", "_cell_guid": "c9b36c13-2de8-4336-926e-f2db91cd27f3"}, "source": "Now, it's almost time for the training of the model. First, I shuffle the frame and split off two chunks for verification, and put them into xgboost's desired form (DMatrix)", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "5aefd9b6a4933cc6e10cb58749e735d8dc7bd030", "collapsed": true, "trusted": false, "_cell_guid": "17069119-08df-4e6c-9a4d-c8fb367d53f3"}, "source": "ts = train.sample(frac=1)\n\ncvtrain = ts[:1200000][cols]\ncvtraintimes = ts[:1200000]['logtime']\ncvverify = ts[1200000:1300000][cols]\ncvverifytimes = ts[1200000:1300000]['logtime']\ncvverify2 = ts[1300000:][cols]\ncvverifytimes2 = ts[1300000:]['logtime']\n\nxgbtrain = xgb.DMatrix(cvtrain, label=cvtraintimes)\nxgbverify = xgb.DMatrix(cvverify, label=cvverifytimes)\nxgbverify2 = xgb.DMatrix(cvverify2, label=cvverifytimes2)\nxgbtest = xgb.DMatrix(test[cols])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4128ed27f601831d6fd6e687874f58deaf547829", "_cell_guid": "bd80e2c6-a7e0-40e2-ab02-ec7c4bb86e10"}, "source": "Finally, set parameters for xgboost and train the model", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "9e5ed5c652a76dcd4d7021407c07fd58f48c6f16", "trusted": false, "_cell_guid": "eee80744-9948-4911-84d2-3d2a8db03e35"}, "source": "xgbparams = {'max_depth':10, \n               'n_estimators':3000,\n               'learning_rate':0.035,\n               'subsample':0.8, \n               'tree_method': 'exact',\n               'alpha':5,\n               'lambda':10\n               }\n\nnum_round = 2000\n\nmdl = xgb.train(xgbparams, xgbtrain, num_round, [(xgbtrain, 'train'), (xgbverify, 'verify')], early_stopping_rounds=10)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c9c1a75fc0d3b87ca16164283f2a85e373421e6b", "_cell_guid": "9343500e-6e42-4dfd-9bd0-a12d9abff73d"}, "source": "The output keeps track of the score for both the train and verify datasets. Now, I see what the score for the 2nd verify dataset is", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "c1538cf00c84750ba71ba28446bc6eef74c01024", "trusted": false, "_cell_guid": "264cb691-657f-43bf-be61-a08077a77641"}, "source": "def rmsle(predictor, X, y):\n    return np.sqrt(np.mean((predictor.predict(X) - y)**2))\n\nprint(rmsle(mdl, xgb.DMatrix(cvverify2), cvverifytimes2))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "976e83d8b1e80ac4e621d038d1c6a1cb836aa000", "_cell_guid": "d982bcd5-67cf-45c9-9891-a8c41c973468"}, "source": "Finally, predict on the test set and save the data to a file", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "a316664867ab596d3faf646156819351e9a0d6fa", "trusted": false, "_cell_guid": "7ecdb49c-39f1-4969-8a50-b108f0c7de00"}, "source": "preds = mdl.predict(xgbtest)\nxgb_test_preds_frame = pd.DataFrame({'trip_duration': np.exp(preds) - 1}, index=test['id'])\nxgb_test_preds_frame.to_csv('xgb_test_preds_frame.csv')", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "506a77b05c0f83eaa15e90ce64adc36b77f0ff7b", "_cell_guid": "5b4a93a0-cbaf-464e-ad62-9877392dc5ce"}, "source": "So far, the best this model has gotten on the public leaderboard is 0.388. However, the last verification step always comes out at 0.380 or lower; sometimes as low as 0.375. So either the training data isn't representative of the test cases it's evaluating on, or I've got some weird overfitting going on. I've tried to mitigate that with setting alpha, lambda, and subsample to be more conservative, but it doesn't seem to be helping much. ", "outputs": [], "execution_count": null, "cell_type": "markdown"}]}