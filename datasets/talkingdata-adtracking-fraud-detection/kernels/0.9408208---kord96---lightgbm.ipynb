{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport time\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport gc\nimport matplotlib.pyplot as plt\nimport os\ndtypes = {\n        'ip'            : 'uint32',\n        'app'           : 'uint16',\n        'device'        : 'uint8',\n        'os'            : 'uint16',\n        'channel'       : 'uint16',\n        'is_attributed' : 'uint8',\n        'click_id'      : 'uint32',\n            };\n    \nparams = {\n        'learning_rate': 0.10,\n        'num_leaves': 7,  # 2^max_depth - 1\n        'max_depth': 3,  # -1 means no limit\n        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n        'max_bin': 100,  # Number of bucketed bin for feature values\n        'subsample': 0.7,  # Subsample ratio of the training instance.\n        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n        'scale_pos_weight':200 # because training data is extremely unbalanced \n    }","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"## Running the full calculation.\n#### A function is written here to run the full calculation with defined parameters.\n\ndef DO(frm,to,fileno):\n\n    print('loading train data...',frm,to)\n    train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n\n    print('loading test data...')\n    test_df = pd.read_csv(\"../input/test.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n\n    len_train = len(train_df)\n    train_df=train_df.append(test_df)\n    \n    del test_df\n        \n    gc.collect()\n    train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('int8')\n    train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('int8') \n    train_df = do_next_Click( train_df,agg_suffix='nextClick', agg_type='float32'  ); gc.collect()\n    train_df = do_prev_Click( train_df,agg_suffix='prevClick', agg_type='float32'  ); gc.collect()  ## Removed temporarily due RAM sortage. \n    \n    \n    del train_df['day']\n    gc.collect()\n    gc.collect()\n    \n    \n    print('\\n\\nBefore appending predictors...\\n\\n',sorted(predictors))\n    target = 'is_attributed'\n    word= ['app','device','os', 'channel', 'hour']\n    for feature in word:\n        if feature not in predictors:\n            predictors.append(feature)\n    categorical = ['app', 'device', 'os', 'channel', 'hour']\n    print('\\n\\nAfter appending predictors...\\n\\n',sorted(predictors))\n\n    test_df = train_df[len_train:]\n    val_df = train_df[(len_train-val_size):len_train]\n    train_df = train_df[:(len_train-val_size)]\n\n    print(\"\\ntrain size: \", len(train_df))\n    print(\"\\nvalid size: \", len(val_df))\n    print(\"\\ntest size : \", len(test_df))\n\n    sub = pd.DataFrame()\n    sub['click_id'] = test_df['click_id'].astype('int')\n\n    gc.collect()\n\n    print(\"Training...\")\n    start_time = time.time()\n\n    (bst,best_iteration) = lgb_modelfit_nocv(params, \n                            train_df, \n                            val_df, \n                            predictors, \n                            target, \n                            objective='binary', \n                            metrics='auc',\n                            early_stopping_rounds=30, \n                            verbose_eval=True, \n                            num_boost_round=1000, \n                            categorical_features=categorical)\n\n    print('[{}]: model training time'.format(time.time() - start_time))\n    del train_df\n    del val_df\n    gc.collect()\n\n\n\n    print(\"Predicting...\")\n    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n#     if not debug:\n#         print(\"writing...\")\n    sub.to_csv('sub_it%d.csv'%(fileno),index=False,float_format='%.9f')\n    print(\"done...\")\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"13892fe052b6a81485e9dddc09743634f760da82"},"cell_type":"code","source":"####### Chunk size defining and final run  ############\nnrows=184903891-1\nnchunk=25000000\nval_size=2500000\n\nfrm=nrows-65000000\nif debug:\n    frm=0\n    nchunk=100000\n    val_size=10000\n\nto=frm+nchunk\n\nsub=DO(frm,to,FILENO)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}