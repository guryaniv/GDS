{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cef65542-8a3b-cd0c-f32c-ad4044632f5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f4c8a97-813d-bf90-78fe-03920cc0278a"
      },
      "outputs": [],
      "source": [
        "creditCardData = pd.read_csv(\"../input/creditcard.csv\")\n",
        "creditCardData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9649eb34-cd6c-063e-43dd-77f074a3a64e"
      },
      "outputs": [],
      "source": [
        "print ('# of columns: ', len(creditCardData.columns))\n",
        "creditCardData.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9445f4eb-c9a3-cfcb-d3d9-19c88fe24659"
      },
      "source": [
        "So as per the description there are 284807 rows with 28 transformed feature columns V1-V28 and 2 original features Time and Value and a Class label "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c374fa81-020d-842c-c63b-0b85de5b056a"
      },
      "outputs": [],
      "source": [
        "#Checking for missing data\n",
        "creditCardData.isnull().any().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92a1f44b-d5b4-9b9d-c7b4-2cb2663481c6"
      },
      "outputs": [],
      "source": [
        "#Plotting a heatmap to visualize the correlation between the variables\n",
        "sns.heatmap(creditCardData.corr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9303dd21-96b1-0353-265d-d99d66acd582"
      },
      "source": [
        "The features V1-V28 are totally uncorrelated which should be the case as they are obtained by performing PDA on the original dataset\n",
        "\n",
        "## Analysis 1: When do people shop\n",
        "We will visualize when people shop and when credit fraud happens and if there is a pattern. For this however we need to convert time from seconds to days, hours and weeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4619abcf-dc8e-5279-be37-7273437c8089"
      },
      "outputs": [],
      "source": [
        "# As the time provided is in seconds we can use it as seconds since epoch as we won't care about years\n",
        "def convert_totime(seconds):\n",
        "    return datetime.datetime.fromtimestamp(seconds);\n",
        "\n",
        "timeAnalysis = creditCardData[['Time', 'Amount', 'Class']].copy()\n",
        "timeAnalysis['datetime'] = timeAnalysis.Time.apply(convert_totime)\n",
        "# As the max time is 172792 seconds and 172792 / (60*60) is about 48 hrs so we only have data for 2 days so only \n",
        "# plotting data against hours make sense\n",
        "timeAnalysis['hour of the day'] = timeAnalysis.datetime.dt.hour\n",
        "timeAnalysisGrouped = timeAnalysis.groupby(['Class', 'hour of the day'])['Amount'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1060635-1d36-053c-c9c8-e867b1daf1dc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "validTransactions = timeAnalysisGrouped[0].copy()\n",
        "validTransactions.name = 'Number of transactions'\n",
        "validTransactions.plot.bar(title = '# of legitimate credit card transactions per hour', legend = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b361a352-5a27-628b-1261-769a225ffe6d"
      },
      "source": [
        "Note: An interesting thing happened here. When I did this calculation on my laptop, the distribution did not look right, so I added 7 hours to each transaction and I got something like the figure above. I think its is due to the fact that the Kaggle's server must be running in UTC while my system is in MST (the difference between UTC and MST is 7 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "241b06ee-93a3-61ae-65a9-60ac38a97793"
      },
      "outputs": [],
      "source": [
        "## Run this section only if your distribution is somewhat off like it shows most transactions \n",
        "## happened during the night\n",
        "timeDelta = datetime.datetime.utcnow() - datetime.datetime.now() \n",
        "plt.figure(figsize = (10, 6))\n",
        "timeAnalysis['hour of the day'] = timeAnalysis.datetime + timeDelta\n",
        "timeAnalysis['hour of the day'] = timeAnalysis['hour of the day'].dt.hour\n",
        "timeAnalysisGrouped = timeAnalysis.groupby(['Class', 'hour of the day'])['Amount'].count()\n",
        "validTransactions = timeAnalysisGrouped[0].copy()\n",
        "validTransactions.name = 'Number of transactions'\n",
        "validTransactions.plot.bar(title = '# of legitimate credit card transactions per hour', legend = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "154a71b3-3cd1-6fc1-0c5f-9955a5f6e3d4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "fraudTransactions = timeAnalysisGrouped[1].copy()\n",
        "fraudTransactions.name = 'Number of transactions'\n",
        "fraudTransactions.plot.bar(title = '# of fraud credit card transactions per hour', legend = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a8233a03-ce67-8721-86af-73e64f310ca1"
      },
      "source": [
        "2 A.M. has an unsual uptick for the number of frauds committed. But it could also be that my assumption that the first transaction happened at 7 A.M. is incorrect. One thing is clear though that the fraud transactions are better spread out than the legitimate transactions. This can be due to the fact that there are very few fradulent transactions and hence they won't have a clear trend like in the case of legitimate transactions\n",
        "\n",
        "\n",
        "## Analysis 2 - Are fraudulent transactions of higher value than normal transactions\n",
        "It would be interesting to see if fraudulent transactions are in general of higher value than normal transactions or not. To check this lets setup a hypothesis test. Lets define our Null and Alternative hypothesis\n",
        "\n",
        "- H<sub>0</sub> : Fraudulent transactions are of similar or lower value as normal transactions\n",
        "- H<sub>A</sub> : Fraudulent transactions are of higher value as normal transactions\n",
        "\n",
        "I took H<sub>0</sub> to be similar or lower because H<sub>0</sub> and H<sub>A</sub> should together cover all the possibilities\n",
        "\n",
        "Before we begin lets first look at the distribution of amounts of transaction done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4ecb2ea-e965-53db-5ec6-96ed67f96988"
      },
      "outputs": [],
      "source": [
        "# Valid Transactions\n",
        "timeAnalysis[timeAnalysis.Class == 0].Amount.plot.hist(title = 'Histogram of valid transactions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8fc43fbb-cdb5-0205-3ea5-a5b3c9693fec"
      },
      "outputs": [],
      "source": [
        "# As the value of most transaction seems to be only about 2K - 2.5K. Lets limit the data further\n",
        "timeAnalysis[(timeAnalysis.Class == 0) & (timeAnalysis.Amount <= 4000)].Amount.plot.hist(title = 'Histogram of valid transactions (Amount <= 4K)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f077e3d-f00e-5fdb-5766-be2846c0ae12"
      },
      "outputs": [],
      "source": [
        "# Now lets look at the Fraudulent transactions\n",
        "timeAnalysis[timeAnalysis.Class == 1].Amount.plot.hist(title = 'Histogram of fraudulent transactions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aadb11e8-e105-331a-ce80-dd3a08b62939"
      },
      "source": [
        "Hmmmm, there doesn't appears to be any difference visually. But lets wait till we perform the hypothesis test to draw the final conclusion.\n",
        "\n",
        "For the hypothesis test I will be performing a Z-test, with the valid transactions acting as the population. Though a T-test can also be performed but given that our sample set (fraudulent transactions) is of size 492 there shouldn't be any difference, as for sample set >= 30 the t distribution and z distribution are nearly the same.\n",
        "\n",
        "Lets start. We will be performing the test for 99% significance level, this means that we should get a z-score of atleast 2.326 or higher. If someone does not know the formula for z-score, here it is\n",
        "\n",
        "$$ z-score = (\\bar{x} - \\mu) / S.E$$\n",
        "\n",
        "Where\n",
        "- $\\bar{x}$ : mean of the sample\n",
        "- $\\mu$ : population mean\n",
        "- S.E : Standard Error\n",
        "\n",
        "The standard error in our case is given by the formula : $\\sigma/\\sqrt{n}$, where $\\sigma$ is the Standard deviation of the population and n is the sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a1d1f9c-ef5c-3184-df3f-f556ff03558b"
      },
      "outputs": [],
      "source": [
        "population = timeAnalysis[timeAnalysis.Class == 0].Amount\n",
        "sample = timeAnalysis[timeAnalysis.Class == 1].Amount\n",
        "sampleMean = sample.mean()\n",
        "populationStd = population.std()\n",
        "populationMean = population.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d658f00a-3b7e-22b8-8260-76a5414ba0cb"
      },
      "outputs": [],
      "source": [
        "z_score = (sampleMean - populationMean) / (populationStd / sample.size ** 0.5)\n",
        "z_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a95d0d34-cc85-0073-4ddc-df6bf74eee73"
      },
      "source": [
        "As the z-score is more than 2.326 we reject the Null hypothesis. So there is a 99% chance that the amount spend on fraudulent transactions are on average significantly higher than normal transactions. But as we observed in the histograms in absolute terms normal transactions are of higher value.\n",
        "\n",
        "## Going all out on hypothesis testing\n",
        "Lets now perform a hypothesis test for each of the 28 features to see if the feature value for the fraud data is significantly different from the valid transaction or not. The significance level for this experiment will be 99% and it will be a 2-tailed test. The corresponding z-critical value is 3.37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d46c7f5-faec-2c77-8d43-6cf326667b6e"
      },
      "outputs": [],
      "source": [
        "# Gettting the PDA columns\n",
        "PDA_columns = [x for x in creditCardData.columns if 'V' in x]\n",
        "\n",
        "valid_transactions = creditCardData[creditCardData.Class == 0]\n",
        "Fraud_transactions = creditCardData[creditCardData.Class == 1]\n",
        "#Getting the number of rows\n",
        "sample_size = Fraud_transactions.shape[0]\n",
        "for col in PDA_columns:\n",
        "    mean = valid_transactions[col].mean()\n",
        "    std = valid_transactions[col].std()\n",
        "    zScore = (Fraud_transactions[col].mean() - mean) / (std/sample_size**0.5)\n",
        "    print ('Column', col, 'is', 'Significant' if abs(zScore) >= 3.37 else 'insignificant')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "336824f6-f795-f7e9-814a-bf4e087c6556"
      },
      "source": [
        "## Conclusion\n",
        "The amount spend on fraudulent transactions is on average significantly higher than normal transactions but in absolute terms higher amounts are spent on valid transaction. This means we can't really create an additional boolean feature such as 'If amount spent is higher than a given value', on the other hand there is significant difference in average amount spent, maybe it can be used to identify frauds.\n",
        "\n",
        "Also, as it would seem as per my calculation the fraudulent transactions are more spread out during the day as compared to normal transactions. Maybe scrutinizing late night transactions can lead to a better detection rate. Finally, features - V13, V15, V22, V23, V25, V26 are not very good at differentiating between fraud and valid transactions. So, maybe removing them will lead to a better result. "
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}