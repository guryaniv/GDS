{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "31f8a6c7-572c-d4a1-c105-ff3312d5ebed"
      },
      "source": [
        "This program performs the stock market prediction using Random Forest model in pyspark. Although the size of the data does not require to perform the task on Spark, I wrote this code for those who want to see how to do prediction on Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "60e2c6cc-d5d8-de55-f77f-565a0ce75936"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "from pyspark.sql.functions import udf, concat, col, lit\n",
        "from pyspark.sql.types import IntegerType, ArrayType, StringType, DoubleType\n",
        "import string\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer, CountVectorizer, Tokenizer, StopWordsRemover, NGram\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "conf = SparkConf()\n",
        "sc = SparkContext(conf)\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "#the data file is read from HDFS\n",
        "#the file stockMarketAndNewsData.csv a modified version of the file combined_News_DJIA.csv, whereby the newlines that are wrongly introduced in the data are removed\n",
        "\n",
        "data = sqlContext.read.load('/directory-of-your-file/stockMarketAndNewsData.csv', \n",
        "                          delimiter=',',\n",
        "                          format='com.databricks.spark.csv', \n",
        "                          header='true', \n",
        "                          inferSchema='true')\n",
        "\n",
        "\n",
        "#replace null values with empty string\n",
        "data = data.na.fill(' ')\n",
        "\n",
        "# Only the columns that represent the news\n",
        "newsColumns = [x for x in data.columns if x not in ['Date', 'Label']]\n",
        "\n",
        "#merge news from different news sources per day\n",
        "\n",
        "data = data.withColumn(\"allNews\", data.Top1)\n",
        "for i in range(2, len(newsColumns)+1):\n",
        "    colName = 'Top' + str(i)\n",
        "    data = data.withColumn('allNews', concat(col(\"allNews\"), lit(\" \"), col(colName)))\n",
        "\n",
        "#remove puntuation marks from the news\n",
        "\n",
        "removePunctuation = udf(lambda x: ''.join([' ' if ch in string.punctuation else ch for ch in x]))\n",
        "data = data.withColumn('allNews', removePunctuation(data.allNews))\n",
        "\n",
        "#split the news into words\n",
        "\n",
        "splitNews = udf(lambda s: [x for x in s.split(' ') if (x != u'' and len(x) >= 2)], ArrayType(StringType(), True))\n",
        "data = data.withColumn('words', splitNews(data.allNews)).select('Date', 'label', 'words')\n",
        "\n",
        "#remove the stop words\n",
        "\n",
        "myStopwordRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"stopRemoved\")\n",
        "data = myStopwordRemover.transform(data)\n",
        "\n",
        "# Create ngrams of size 2\n",
        "\n",
        "myngram = NGram(inputCol=\"stopRemoved\", outputCol=\"ngrams\", n=2)\n",
        "data = myngram.transform(data)\n",
        "data = data.withColumn('ngrams', data.ngrams.cast(ArrayType(StringType(), True)))\n",
        "\n",
        "# Apply count vectorizer to convert to vector of counts of the ngrams\n",
        "\n",
        "myCountVectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"countVect\", minDF=1.0)\n",
        "data = myCountVectorizer.fit(data).transform(data)\n",
        "\n",
        "# Transform the label using StringINdexer\n",
        "\n",
        "si_label = StringIndexer(inputCol=\"label\", outputCol=\"label2\", handleInvalid=\"skip\")\n",
        "data = si_label.fit(data).transform(data)\n",
        "data.drop('label')\n",
        "data = data.withColumn('label', data.label2)\n",
        "\n",
        "# Divide into training and test data\n",
        "\n",
        "trainData = data[data['Date'] < '20150101']\n",
        "testData = data[data['Date'] >= '20141231']\n",
        "\n",
        "# define the random forest classifier model\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"countVect\", numTrees=3, maxDepth=4, maxBins=200)\n",
        "\n",
        "# perform a grid search on a set of parameter values\n",
        "\n",
        "grid = ParamGridBuilder().addGrid(rf.numTrees, [2, 5])\\\n",
        "                         .addGrid(rf.maxDepth, [2, 5])\\\n",
        "                         .build()\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "cv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator)\n",
        "cvModel = cv.fit(trainData)\n",
        "evaluator.evaluate(cvModel.transform(testData))"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}