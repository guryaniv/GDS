{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\n\nimport bcolz\nimport numpy as np\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom keras import backend as K\n\n\nOR_IM_WIDTH = 101\nOR_IM_HEIGHT = 101\nOR_IM_CHANNEL = 3\n\nIM_WIDTH = 128\nIM_HEIGHT = 128\nIM_CHAN = 1\n\n\ndef save_arr (fname, arr):\n    c = bcolz.carray(arr, rootdir=fname, mode='w')\n    c.flush()\n    \n\ndef load_array(fname):\n    return bcolz.open(fname)[:]\n\n\ndef upsample(img):\n    return resize(img, (IM_HEIGHT, IM_WIDTH, IM_CHAN), mode='constant', preserve_range=True)\n\n    \ndef downsample(img):\n    return resize(img, (OR_IM_HEIGHT, OR_IM_WIDTH), mode='constant', preserve_range=True)\n\n\ndef rle_decode(rle, shape):\n    \"\"\"\n    rle: run-length string or list of pairs of (start, length)\n    shape: (height, width) of array to return \n    Returns\n    -------\n        np.array: 1 - mask, 0 - background\n    \"\"\"\n    if isinstance(rle, float) and math.isnan(rle):\n        rle = []\n    if isinstance(rle, str):\n        rle = [int(num) for num in rle.split(' ')]\n    # [0::2] means skip 2 since 0 until the end - list[start:end:skip]\n    starts, lengths = [np.asarray(x, dtype=int) for x in (rle[0:][::2], rle[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 255\n    img = img.reshape(1, shape[0], shape[1])\n    img = img.T\n    return img\n\n\ndef rle_encode(img):\n    \"\"\"\n    img: np.array: 1 - mask, 0 - background\n    Returns\n    -------\n    run-length string of pairs of (start, length)\n    \"\"\"\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle if rle else float('nan')\n\n\ndef iou(y_true, y_pred):\n    \"\"\" Intersection over Union Metric\n    \"\"\"\n    component1 = y_true.astype(dtype=bool)\n    component2 = y_pred.astype(dtype=bool)\n\n    overlap = component1 * component2 # Logical AND\n    union = component1 + component2 # Logical OR\n\n    iou = overlap.sum() / float(union.sum())\n    return iou\n\n\ndef iou_batch(y_true, y_pred):\n    batch_size = y_true.shape[0]\n    metric = []\n    for i in range(batch_size):\n        value = iou_metric(y_true[i], y_pred[i])\n        metric.append(value)\n    return np.mean(metric)\n\n\ndef mean_iou(y_true, y_pred):\n    \"\"\"Keras valid metric to use with a keras.Model\n    \"\"\"\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from utils import *\n\nimport os\nimport glob\nimport random\nimport tqdm\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom skimage.transform import resize\nfrom keras.preprocessing import image as image_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5761519eed5271c727e4a0d5f95d08bb96353f9e"},"cell_type":"code","source":"train_path = \"../input/train/images/\"\ntrain_masks_path = \"../input/train/masks/\"\ntest_path = \"../input/test/images\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d606d797c67b5e66608a73578ce3c00a5e2b879"},"cell_type":"code","source":"train_files = sorted(glob.glob(os.path.join(train_path, \"*.png\")))\nmasks_files = sorted(glob.glob(os.path.join(train_masks_path, \"*.png\")))\ntest_files = sorted(glob.glob(os.path.join(test_path, \"*.png\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"21b9008fa5a33c7555688e5afd3ec5e04d1ab6e9"},"cell_type":"code","source":"assert len(train_files) == len(masks_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ccb1bcaf53eefc627a09f95e14d9a2f9663223b6"},"cell_type":"code","source":"ids_train  = []\nX_train = np.zeros((len(train_files), OR_IM_HEIGHT, OR_IM_WIDTH, OR_IM_CHANNEL), dtype=np.uint8)\ny_train = np.zeros((len(masks_files), OR_IM_HEIGHT, OR_IM_WIDTH, OR_IM_CHANNEL), dtype=np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aac0c3a32e38f2e527877ffc9100105d70afbb45"},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"615702b36c3c82f4980e52c623f512315d24ebdd"},"cell_type":"code","source":"for i, (train_path, mask_path) in tqdm.tqdm_notebook(enumerate(zip(train_files, masks_files)), total=len(train_files)):\n    train_id = os.path.basename(train_path)[:-4]\n    mask_id = os.path.basename(mask_path)[:-4]\n    assert train_id == mask_id\n    ids_train.append(train_id)\n    \n    x = image_utils.img_to_array(image_utils.load_img(train_path))\n    X_train[i] = x\n\n    y = image_utils.img_to_array(image_utils.load_img(mask_path))\n    y_train[i] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79e695bba0027fbfb1244fb2cd8abc62fd3dd92a"},"cell_type":"code","source":"len(ids_train), X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4410cb1ca28c3a233ec8ee23b97408291caed6a8"},"cell_type":"markdown","source":"# Sanity Check"},{"metadata":{"trusted":true,"_uuid":"9a564c615d9b75bdbd1821c978df5f9dcc349ef3"},"cell_type":"code","source":"n_images = 6\nfig, axarr = plt.subplots(2, n_images, figsize=(15, 5))\nfor image in range(n_images):\n    n = random.randint(1, X_train.shape[0])\n    axarr[0, image].imshow(X_train[n])\n    axarr[1, image].imshow(y_train[n])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fad0cf7f1a8c3ed09eb3f6bc3575d8dff0d8a5a"},"cell_type":"markdown","source":"# Test Data"},{"metadata":{"trusted":true,"_uuid":"d242f51b654c3b8919e0017a59d5ebfc1d8e6d95"},"cell_type":"code","source":"ids_test = []\nX_test = np.zeros((len(test_files), OR_IM_HEIGHT, OR_IM_WIDTH, OR_IM_CHANNEL), dtype=np.uint8)\n\nfor i, test_path in tqdm.tqdm_notebook(enumerate(test_files), total=len(test_files)):\n    test_id = os.path.basename(test_path)[:-4]\n    ids_test.append(test_id)\n    \n    x = image_utils.img_to_array(image_utils.load_img(test_path))\n    X_test[i] = x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cf00ccb0a0c44bb6c6b797b0cb2e1ca035ad2a2"},"cell_type":"code","source":"len(ids_test), X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99e82a9f73059e6ea5f3948908e7f11d4ad5709a"},"cell_type":"code","source":"n_images = 6\nfig, axarr = plt.subplots(1, n_images, figsize=(15, 5))\nfor image in range(n_images):\n    n = random.randint(1, X_test.shape[0])\n    axarr[image].imshow(X_test[n])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd26795b6175a13d0fa2ea7a0f70535858a6c01c"},"cell_type":"markdown","source":"# Stratify training data\n\nWe measure how much salt (mask) is on each photo and we divide this in n groups.\n\nSince the mask is just black and white we can just sum each pixel (black=1) of the mask and divide by the size of the img"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"00373151367e4fbb0831b0311516fa474b4c0007"},"cell_type":"code","source":"coverage_train = np.zeros((X_train.shape[0], ), dtype=np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33a234dcda61633c6e975f66033136d2add4cf07"},"cell_type":"code","source":"for i, (image, mask) in tqdm.tqdm_notebook(enumerate(zip(X_train, y_train)), total=X_train.shape[0]):\n    coverage_train[i] = np.mean(mask) / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0284a784558facd9843e5ef97afc2e715c2d8ac"},"cell_type":"code","source":"coverage_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2a9b81cc7ed23f86101cd116133b3ce57a44b795"},"cell_type":"code","source":"strata_train = np.zeros((X_train.shape[0], ), dtype=np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"799325f9531dba3e2c97889f7f4fdc2e227e5e06","collapsed":true},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\nv_cov_to_class = np.vectorize(cov_to_class)\nstrata_train = v_cov_to_class(coverage_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0c4740ddc66e3d16e6004e79b39e718420c218d"},"cell_type":"code","source":"strata_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88bb4268a2c8fe69c0935c09304f2168fccdc438"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(coverage_train, kde=False, ax=axs[0])\nsns.distplot(strata_train, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73b2d022063739dda46984a580af882ca678d1b5"},"cell_type":"markdown","source":"# Sanity Check for Strata"},{"metadata":{"trusted":true,"_uuid":"9e45038400e047a3d57047761fe0c0eeb4133826"},"cell_type":"code","source":"n_images = 11\nfig, axarr = plt.subplots(2, n_images, figsize=(18, 3))\nfor image in range(n_images):\n    statum_img = X_train[strata_train == image]\n    statum_mask = y_train[strata_train == image]\n    n = random.randint(1, statum_img.shape[0])\n    axarr[0, image].imshow(statum_img[n])\n    axarr[1, image].imshow(statum_mask[n])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"325974e1a46cb6e4e08e11dc4dc0b7506bc15861"},"cell_type":"markdown","source":"# Save Arrays\n\nUpsample first"},{"metadata":{"trusted":true,"_uuid":"1e236dd6a0dc9c8e296ed12028e837bb6f7ea493"},"cell_type":"code","source":"X_train_up = np.array([upsample(img) for img in tqdm.tqdm_notebook(X_train, total=X_train.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01d8bbbe4b168ce60256c962c563b05493cb72f6"},"cell_type":"code","source":"y_train_up = np.array([upsample(img) for img in tqdm.tqdm_notebook(y_train, total=y_train.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1da46c90c905975c30f7ad3976a426031d889fa4"},"cell_type":"code","source":"X_test_up = np.array([upsample(img) for img in tqdm.tqdm_notebook(X_test, total=X_test.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"856c82ff668766901b9f4224b027f1a8ebf0895a"},"cell_type":"code","source":"X_train_up.shape, y_train_up.shape, X_test_up.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aaebd1176c887dfa065897ab97a280fec042cd1","collapsed":true},"cell_type":"code","source":"# save_arr(\"ids_train\", ids_train)\n# save_arr(\"X_train\", X_train_up)\n# save_arr(\"y_train\", y_train_up)\n# save_arr(\"strata_train\", strata_train)\n# save_arr(\"ids_test\", ids_test)\n# save_arr(\"X_test\", X_test_up)\n\n# from tensorflow.python.client import device_lib\n# device_lib.list_local_devices()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e53eaa2fb563e2cbb945b9afab7c597bbca870f4"},"cell_type":"code","source":"random_state = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9eee602507f623484f439b257bcc2a429de3b6ae","collapsed":true},"cell_type":"code","source":"ids_train_ = ids_train\nX_train_ = X_train_up\ny_train_ = y_train_up.astype(np.bool)\nstrata_train = strata_train\nids_test = ids_test\nX_test = X_test_up","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8aaa344c4096a77848399bc4a938da990bb014be"},"cell_type":"code","source":"im_width  = X_train_.shape[1]\nim_height = X_train_.shape[2]\nim_chan = X_train_.shape[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c3a809f3d3b151b354414a37f36912606ab99a8"},"cell_type":"code","source":"X_train_.shape, y_train_.shape, im_width, im_height, im_chan","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489cb56fab903f248f5ebcc70c817a744ac043e6"},"cell_type":"markdown","source":"# Data Augmentation\n\nWe flip the images along the y axis"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8432eb4e0c45fc6cad019c9c05febeca9fecf345"},"cell_type":"code","source":"X_train_  = np.append(X_train_, [np.fliplr(x) for x in X_train_], axis=0)\ny_train_ = np.append(y_train_, [np.fliplr(x) for x in y_train_], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8f8436470701771ad47ff42dccff36325b3e93b4"},"cell_type":"code","source":"strata_train = np.append(strata_train, strata_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93611fc4d85d6965838ce854c06570007415074b"},"cell_type":"code","source":"X_train_.shape, y_train_.shape, strata_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e49485ce508ce39fb45eccc51e1c071be00d46db"},"cell_type":"markdown","source":"# Train/valid slip"},{"metadata":{"trusted":true,"_uuid":"d2a58cbd308f2c2a9e1ecd94b8c36d96c475c403","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nrandom_state  = 42\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_,y_train_,test_size=0.2, stratify=strata_train, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74aa7f695aa325e2f45fe8063977194051bd619a"},"cell_type":"code","source":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67df7ae34b660646bc825f5eacc1ba0c686a410d"},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a6a36304fdc329f586b502d19ae2e3dc248e04a"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Lambda\nfrom keras.layers import Conv2D, Conv2DTranspose\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import concatenate\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19fb29176e684d6ee3ccdc2887650ada14ed5be7"},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    uncov1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7967ae9b1353d1b7dc60ab6174a85f0d477bf686","collapsed":true},"cell_type":"code","source":"input_layer  = Input((im_height, im_width, im_chan))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"255461d4732e605b9f3ad8032c6518524a264729"},"cell_type":"code","source":"output_layer = build_model(input_layer, 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b527201bbcbd29a4a879eda8b51c67595fbd2b8","collapsed":true},"cell_type":"code","source":"model = Model(input_layer , output_layer)\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy', mean_iou])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9b3bdd46af54d57e2bb0835b7485aced874af57"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"102ff2c062c0a7bad6711330e7f6024a811f3926","collapsed":true},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"unet-dropout.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n\nepochs = 100\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ad1f7809752604ad4348e9f7e9d784f79af194","scrolled":false},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    validation_data=[X_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43f70ccfb5aeb1c9fc07d0d6e7b4ff6b1a9887d4"},"cell_type":"markdown","source":"# Check the predictions"},{"metadata":{"trusted":true,"_uuid":"4de2096304183d37f0baa67fccea30642a0c0781"},"cell_type":"code","source":"pred_test = model.predict(X_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55946bd9023308597642b9c0f26816b80606baa1","collapsed":true},"cell_type":"code","source":"threshold = 0.5\npred_test_tresh = np.int32(pred_test > threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37cf5ee666bd55ad7a610b1c487337cc84e468e9","collapsed":true},"cell_type":"code","source":"from utils import *\nimport tqdm\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom skimage.transform import resize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ea563d331b4c67fc267fe95ef9b75f07389050f"},"cell_type":"code","source":"import pandas as pd\nthreshold = 0.5\npred_test_tresh = np.int32(pred_test > threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a65fdec19ca6b5bcb29e500d907aa6f245a7ca9"},"cell_type":"code","source":"preds_test_downsample  = []\nfor i in tqdm.tnrange(len(pred_test)):\n    # Resize it back to original size: 101x101\n    preds_test_downsample.append(np.int32(downsample(pred_test[i]) > threshold))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6739fd4302d11199bb4bf07f2b1ee71b5e04082"},"cell_type":"code","source":"pred_dict  = {img_id: rle_encode(preds_test_downsample[i]) for i, img_id in tqdm.tqdm_notebook(enumerate(ids_test), total=len(ids_test))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9fd48ddbc045dc521d8dd9bf1cdcd0b86f94ff4"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict, orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a072e2add8d7d3be9dd4ee4929b8765ecaff08c2"},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"abcf8826d6e36a8d24639ef4e4694b7be200b78d"},"cell_type":"code","source":"sub.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eac3558922f32a07cdc727371dc2763b057e03ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}