{"cells":[{"metadata":{"_uuid":"e5e3a7197d69585e01509b742eacb86e667ed581","_cell_guid":"32cda7de-e238-4e47-8f06-fc24f8194890","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be71ae8b6af2e87da4bb1b0e97d7398eecfd6256","_cell_guid":"efe45ad5-c28e-47bb-9e08-06f188cebe52","trusted":true},"cell_type":"code","source":"%pylab inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d047a6e9dd792421c8f7179b4f078f0dec9c7316","_cell_guid":"0f7c2181-050a-4821-ab9e-37695244950e","collapsed":true,"trusted":true},"cell_type":"code","source":"#read the data\ndf_train = pd.read_csv('../input/train.tsv',sep=\"\\t\")\ndf_test = pd.read_csv('../input/test.tsv',sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54004d22647a4def6b8749d7a7df9ee597ccba0","_cell_guid":"cd034261-04ed-4807-b499-dbe119a4429b","collapsed":true,"trusted":true},"cell_type":"code","source":"def compute_rmsle(y_true,y_predict):\n    \"\"\"\n    description:\n    The function returns the Root Mean Squared Logarthmic Error (RMSLE)\n    \n    input:\n    y_true    = np-array of true values of dimension (n,)\n    y_predict = np-array of true values of dimension (n,)\n    \n    output:\n    rmsle     = root mean squared logarthmic error\n    \"\"\"\n    n = len(y_true)\n    rmsle = np.sqrt(np.sum(log(y_predict+1) + log(y_true +1 )) / n )\n    return rmsle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb05ebf605599b643c3eb1d0d25427656d67da9b","_cell_guid":"77cca08e-b79d-4ce7-88e2-e084bd67cf68","collapsed":true,"trusted":true},"cell_type":"code","source":"def transform_brandname(x):\n    \"\"\"\n    description:\n    The function returns a cleaner version of the brand name\n    \n    input:\n    x = uncleaned version of brand name\n    \n    output:\n    x = cleaned version of brand name\n    \"\"\"\n    if x is NaN:\n        return 'no_brand'\n    elif x is nan:\n        return 'no_brand'\n    else:\n        return x\n\ndef transform_catlevel(x):\n    \"\"\"\n    description:\n    The function returns a cleaner version of the category level\n    \n    input:\n    x = uncleaned version of category level\n    \n    output:\n    x = cleaned version of category level\n    \"\"\"\n    if x is None:\n        return 'no_level'\n    else:\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d13ea32b81d00ffae8164531e3e7aa8955308af2","_cell_guid":"24435eca-2662-475d-b003-48f8215a0ca1","collapsed":true,"trusted":true},"cell_type":"code","source":"def cleandata(df):\n    \"\"\"\n    description:\n    The function cleans the dataset\n    Transformations done:\n        1. replacing all null descriptions with \"No description yet\"\n        2. split categories and impute missing values\n        3. replacing missing brands with no_brands\n        4. replacing all categories with no_level\n        5. remove the temporary columns\n    input:\n    df  = dataframe of the data\n    \n    output:\n    df = cleaned dataframe\n    \"\"\"\n    \n    #1. replacing all null descriptions with \"No description yet\"\n    df.item_description = df.item_description.replace(np.NaN,\"No description yet\")\n    \n    \n    #2. split categories and impute missing values\n    df['c_c'] = list(map(lambda x:str(x).split('/'),df.category_name))\n    df[['l1','l2','l3','l4','l5']] = pd.DataFrame(df.c_c.values.tolist(),index=df.index)\n    df.drop('c_c',axis = 1,inplace=True)\n    \n    #3. replacing missing brands with no_brands\n    df['pro_bn']  =list(map(lambda x: transform_brandname(x), df.brand_name))\n    df.drop('brand_name',axis = 1,inplace=True)\n    \n    #4. replacing all categories with no_level\n    df['pro_l1'] = list(map(lambda x: transform_catlevel(x), df.l1) )\n    df['pro_l2'] = list(map(lambda x: transform_catlevel(x), df.l2) )\n    df['pro_l3'] = list(map(lambda x: transform_catlevel(x), df.l3) )\n    df['pro_l4'] = list(map(lambda x: transform_catlevel(x), df.l4) )\n    df['pro_l5'] = list(map(lambda x: transform_catlevel(x), df.l5) )\n    \n    #5. remove the temporary columns\n    df.drop('l1',axis = 1,inplace=True)\n    df.drop('l2',axis = 1,inplace=True)\n    df.drop('l3',axis = 1,inplace=True)\n    df.drop('l4',axis = 1,inplace=True)\n    df.drop('l5',axis = 1,inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b99743bedac3f57556e298295366ea1b767dca","_cell_guid":"a2678dbc-3f79-4971-92ea-66d9eebbe813","collapsed":true,"trusted":true},"cell_type":"code","source":"#call clean data\ndf_train = cleandata(df_train)\ndf_test = cleandata(df_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded8659504423b232d9a4c335865f26f5a9f92bb","_cell_guid":"52485920-9dab-4d8f-b89b-4b2385aa047e","collapsed":true,"trusted":true},"cell_type":"code","source":"#extract all the labels and transform it with a label encoder\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nall_names = df_train.name.tolist() +  df_test.name.tolist()\nall_names = list(set(all_names))\nle.fit(all_names)\ndf_train['name'] = le.fit_transform(df_train.name)\ndf_test['name'] = le.fit_transform(df_test.name)\n\ndel le","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"336b87d1c74b1d3e142dd5a5b9cc248b2a657823","_cell_guid":"82346343-70ca-489a-be86-def88a03bd4a","trusted":true},"cell_type":"code","source":"#similarly perform label encoding operation for the category levels\nc_l = ['pro_l1','pro_l2','pro_l3','pro_l4','pro_l5']\n\nfor l in c_l:\n    print('processing %s' % l)\n    le = LabelEncoder()\n    all_l = df_train[l].tolist() +  df_test[l].tolist()\n    all_l = list(set(all_names))\n    le.fit(all_l)\n    df_train[l] = le.fit_transform(df_train[l])\n    df_test[l] = le.fit_transform(df_test[l])\n\n    del le","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa1d8b58e705506c93325f33136851d323d3e625","_cell_guid":"ffdbe817-fd74-4025-b961-58b4557cffc0","collapsed":true,"trusted":true},"cell_type":"code","source":"#create a tf-idf feature vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1,1),max_features= 100000, min_df = 0.15, stop_words = 'english')\ntfidf_matrix =  tf.fit_transform(df_train.item_description)\nfeature_names = tf.get_feature_names() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de5942b9b0abcfae5333d3b2d863b0b1b391f06","_cell_guid":"ab5f9079-c092-47f3-b976-2d585dcb93d6","collapsed":true,"trusted":true},"cell_type":"code","source":"#create a list of this\nfeature_names\ntxt_features = feature_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15a2ea2bd0156da43cfeae640b85156f5b6833c7","_cell_guid":"66356580-0041-4e12-a78a-b3c8354b0560","collapsed":true,"trusted":true},"cell_type":"code","source":"#apply the transform on the train and test\nfor txt_f in txt_features:\n    df_train[txt_f] = list(map(lambda x: 1 if x.find(txt_f) != -1 else 0,df_train.item_description))\n    df_test[txt_f] = list(map(lambda x: 1 if x.find(txt_f) != -1 else 0,df_test.item_description))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61af7745a4d4dd21d8b603509cd9547362c62689","_cell_guid":"9475a7bd-2925-465b-91cc-96a6d58685a4","collapsed":true,"trusted":true},"cell_type":"code","source":"#create final dataset for cleaning and building a predictive model\nX = pd.get_dummies(df_train.shipping)\n#X = np.column_stack([X,pd.get_dummies(df_train.shipping)])\nX = np.column_stack([X,df_train['name']])\nX = np.column_stack([X,df_train[c_l]])\nX = np.column_stack([X,df_train[txt_features]])\ny = log(df_train.price+1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e95ad61fd7efa596f8d46fdf29bb7b809480363a","_cell_guid":"fac34f91-a830-4e01-9205-3ff72a9cba30","collapsed":true,"trusted":true},"cell_type":"code","source":"#create a train-test split on the train dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"387f2bde14397399575c43d5f9fcd1dca0093379","_cell_guid":"690e543a-20d8-468d-bb9b-151f2950cd6c","trusted":true},"cell_type":"code","source":"#produce a simple random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\nRF_regr = RandomForestRegressor(n_estimators= 300, max_features= 'sqrt', n_jobs= -1, max_depth=16, min_samples_split=5, min_samples_leaf=5)\nRF_regr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03c5b5e57c3f97035f8b9830e78f41cff31f3ad2","_cell_guid":"25a56c4a-2ba5-46d9-8634-36cb455c1ed7","collapsed":true,"trusted":true},"cell_type":"code","source":"#generate predictions\ny_test_pred = RF_regr.predict(X_test)\n#y_test_pred = exp(y_test_pred) - 1 ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72d6b7bee04f7bb1a194931a3729f8d2cc7aed75","_cell_guid":"6baa6925-9686-46f7-a07b-301cffd2ade4","trusted":true},"cell_type":"code","source":"#calculate the mean squared error\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test,y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60711b02fd7efaf85bca72843158542a6ac1ab78","_cell_guid":"3734c66e-d2e2-4bc8-84fb-09206fe289ad","collapsed":true,"trusted":true},"cell_type":"code","source":"#create test for generating predictions\nX = pd.get_dummies(df_test.shipping)\n#X = np.column_stack([X,pd.get_dummies(df_test.shipping)])\nX = np.column_stack([X,df_test['name']])\nX = np.column_stack([X,df_test[c_l]])\nX = np.column_stack([X,df_test[txt_features]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3ed0d9f5f397c82c97ed2dea5859f5e3bc191f4","_cell_guid":"d5695d04-1e1f-4f36-acff-0e9c5ca6bc6f","collapsed":true,"trusted":true},"cell_type":"code","source":"#perform inverse transformations\nlog_test_prices = RF_regr.predict(X)\ntest_prices = exp(log_test_prices) - 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42f49a682f73da9b016696416879089f085d2807","_cell_guid":"ea372714-1ebc-492e-ba3b-a7771236f48e","collapsed":true,"trusted":true},"cell_type":"code","source":"#create submission csv\ndf_test['price'] = list(map(lambda x: x-1 , test_prices))\ndf_test.to_csv('mer_all_labels.csv',columns=['test_id','price'],index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb76e0c177eae9664c934a83bb57bee2d9339a61","_cell_guid":"f955ea29-030b-40aa-bf32-7113949f5e99","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}