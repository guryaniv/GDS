{"cells":[{"metadata":{"scrolled":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#IMPORTING ALL THE REQUIRED PACKAGES.\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\nimport gensim\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport codecs\nimport keras\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers import Dense, Input, Flatten, Dropout, Merge\nfrom keras.callbacks import EarlyStopping\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nstopwords0 = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#SETTING GLOBAL VARIABLES\nEMBEDDINGDIM = 300\nMAXVOCABSIZE = 175303 \nMAXSEQLENGTH = 200 \nbatchsize = 256 \nepochs = 3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d05d9cac57bc8f2cbfec78f53855193e35fc8839","_cell_guid":"59e215d5-3954-4167-a1f1-c3a7461c3a22","trusted":true},"cell_type":"code","source":"#READING AND SETTING UP THE TRAIN.CSV FILE\ntraincomments = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\", sep=',', header=0)\ntraincomments.columns=['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nprint(\"num train: \", traincomments.shape[0])\ntraincomments.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"726ec66f9b49792304e231c511f04244f3a7bcc0","_cell_guid":"c463b28e-a582-47a7-a5e6-f92da9fe8bef","trusted":true},"cell_type":"code","source":"labelnames = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nytrain = traincomments[labelnames].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6457d9bdd79f40d1d3ef79228ef6a67e89f12318","_cell_guid":"4677146a-2625-4c07-8386-768bf6066010","trusted":true},"cell_type":"code","source":"#READING AND SETTING UP THE TEST.CSV FILE\ntestcomments = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\", sep=',', header=0)\ntestcomments.columns=['id', 'comment_text']\nprint(\"num test: \", testcomments.shape[0])\ntestcomments.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ff3be951a7562e78aa2ebf957bc43d9b121bd071","_cell_guid":"e3d5772e-7ce1-4656-936f-f0ae25396cd2","trusted":true},"cell_type":"code","source":"#CLEANING UP THE TEXT\n#Function to clean up the text\ndef standardizetext(df, textfield):\n    df[textfield] = df[textfield].str.replace(r\"http\\S+\", \"\")\n    df[textfield] = df[textfield].str.replace(r\"http\", \"\")\n    df[textfield] = df[textfield].str.replace(r\"@\\S+\", \"\")\n    df[textfield] = df[textfield].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[textfield] = df[textfield].str.replace(r\"@\", \"at\")\n    df[textfield] = df[textfield].str.lower()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04063b2e274a600359b73cde63e39218110f5ef3","_cell_guid":"05ded08a-a63a-4112-850f-f84110c81ecb","trusted":true,"collapsed":true},"cell_type":"code","source":"#Cleaning the train data and making the new CSV file -> train_clean_data.csv\ntraincomments.fillna('_NA_')\ntraincomments = standardizetext(traincomments, \"comment_text\")\ntraincomments.to_csv(\"traincleandata.csv\")\ntraincomments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03127aa9060e38b7c1326f4649d74a877b7641f6","_cell_guid":"b469f9fe-c341-46c9-b660-4e10e0e16a3b","trusted":true,"collapsed":true},"cell_type":"code","source":"#Cleaning the test data and making the new CSV file -> test_clean_data.csv\ntestcomments.fillna('_NA_')\ntestcomments = standardizetext(testcomments, \"comment_text\")\ntestcomments.to_csv(\"testcleandata.csv\")\ntestcomments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f4b8646b4358f91ef877e7c65146133c70dfcbb","_cell_guid":"811d9d63-aa94-447a-b532-7fb7fe3f0cff","trusted":true,"collapsed":true},"cell_type":"code","source":"#TOKENIZING THE TEXT\ntokenizer = RegexpTokenizer(r'\\w+')\ncleantraincomments = pd.read_csv(\"traincleandata.csv\")\ncleantraincomments['comment_text'] = cleantraincomments['comment_text'].astype('str') \ncleantraincomments.dtypes\ncleantraincomments[\"tokens\"] = cleantraincomments[\"comment_text\"].apply(tokenizer.tokenize)\n# delete Stop Words\ncleantraincomments[\"tokens\"] = cleantraincomments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stopwords0])\ncleantraincomments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"715c7fbbcc88e9c5ae74357ed5213917df8fee49","_cell_guid":"fbfd1702-efbc-4269-bab6-f5d3bea69a2d","trusted":true,"collapsed":true},"cell_type":"code","source":"cleantestcomments = pd.read_csv(\"testcleandata.csv\")\ncleantestcomments['comment_text'] = cleantestcomments['comment_text'].astype('str') \ncleantestcomments.dtypes\ncleantestcomments[\"tokens\"] = cleantestcomments[\"comment_text\"].apply(tokenizer.tokenize)\ncleantestcomments[\"tokens\"] = cleantestcomments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stopwords0])\ncleantestcomments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1273ae2dc9b3840eb073a70f2264ec7458d22baf","_cell_guid":"495c41d0-2afc-479c-b733-bb69206c4f70","trusted":true,"collapsed":true},"cell_type":"code","source":"alltrainingwords = [word for tokens in cleantraincomments[\"tokens\"] for word in tokens]\ntrainingsentencelengths = [len(tokens) for tokens in cleantraincomments[\"tokens\"]]\nTRAININGVOCAB = sorted(list(set(alltrainingwords)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(alltrainingwords), len(TRAININGVOCAB)))\nprint(\"Max sentence length is %s\" % max(trainingsentencelengths))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87009c037782f54dd9c3f418aedd0a9a15c5af2a","_cell_guid":"1c616eb8-c258-4e95-8121-b2d023e58a66","trusted":true,"collapsed":true},"cell_type":"code","source":"alltestwords = [word for tokens in cleantestcomments[\"tokens\"] for word in tokens]\ntestsentencelengths = [len(tokens) for tokens in cleantestcomments[\"tokens\"]]\nTESTVOCAB = sorted(list(set(alltestwords)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(alltestwords), len(TESTVOCAB)))\nprint(\"Max sentence length is %s\" % max(testsentencelengths))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"87f59235841f6549e372b74cc484ace578293441","_cell_guid":"f23d823d-ec6c-49a2-ba86-50b15d4a4593","trusted":true},"cell_type":"code","source":"#WORD2VEC\nword2vecpath = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vecpath, binary=True)\ndef getaverageword2vec(tokenslist, vector, generatemissing=False, k=300):\n    if len(tokenslist)<1:\n        return np.zeros(k)\n    if generatemissing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokenslist]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokenslist]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n#GETTING EMBEDDINGS\ndef getword2vecembeddings(vectors, cleancomments, generatemissing=False):\n    embeddings = cleancomments['tokens'].apply(lambda x: getaverageword2vec(x, vectors, \n                                                                          generatemissing=generatemissing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d342e226c056fbed5d4bdd5ac07068f6fc527362","_cell_guid":"b3595e29-0db7-4cc6-bd77-b6a8eb5214e7","trusted":true,"collapsed":true},"cell_type":"code","source":"#TRAIN EMBEDDING\ntrainingembeddings = getword2vecembeddings(word2vec, cleantraincomments, generatemissing=True)\ntokenizer = Tokenizer(num_words=MAXVOCABSIZE, lower=True, char_level=False)\ntokenizer.fit_on_texts(cleantraincomments[\"comment_text\"].tolist())\ntrainingsequences = tokenizer.texts_to_sequences(cleantraincomments[\"comment_text\"].tolist())\n\ntrainwordindex = tokenizer.word_index\nprint('Found %s unique tokens.' % len(trainwordindex))\n\ntraincnndata = pad_sequences(trainingsequences, maxlen=MAXSEQLENGTH)\n\ntrainembeddingweights = np.zeros((len(trainwordindex)+1, EMBEDDINGDIM))\nfor word,index in trainwordindex.items():\n    trainembeddingweights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDINGDIM)\nprint(trainembeddingweights.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f52236073f542da9a7225ca82835c4f168e450ab","_cell_guid":"e9d59cc2-5bdd-4899-9a7c-3cabf46dcb56","trusted":true},"cell_type":"code","source":"testsequences = tokenizer.texts_to_sequences(cleantestcomments[\"comment_text\"].tolist())\ntestcnndata = pad_sequences(testsequences, maxlen=MAXSEQLENGTH)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f0a5a6760bc4123971b1047a453bb98bc96fd1d9","_cell_guid":"dca2a8d0-bf4b-4d5f-873d-60abe362cabc","trusted":true},"cell_type":"code","source":"#DEFINING THE CNN\ndef ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=trainable)\n\n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n    convs = []\n    filter_sizes = [3,4,5]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n        convs.append(l_pool)\n\n    l_merge = Merge(mode='concat', concat_axis=1)(convs)\n\n    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n    pool = MaxPooling1D(pool_size=3)(conv)\n\n    if extra_conv==True:\n        x = Dropout(0.5)(l_merge)  \n    else:\n        # Original Yoon Kim model\n        x = Dropout(0.5)(pool)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    # Finally, we feed the output into a Sigmoid layer.\n    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n    preds = Dense(labels_index, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6e8739adcd91046e7824fd07185121eeb72fdb78","_cell_guid":"869c78df-fd59-45be-bc10-b6ea34618ca0","trusted":true},"cell_type":"code","source":"x_train = traincnndata\ny_tr = ytrain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17a51cf41ebc2c2ab966b7b41847b2e53da996d","_cell_guid":"a97112a8-7502-4dd1-b012-3594eaea2c6b","trusted":true,"collapsed":true},"cell_type":"code","source":"model = ConvNet(trainembeddingweights, MAXSEQLENGTH, len(trainwordindex)+1, EMBEDDINGDIM, \n                len(list(labelnames)), False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fe710bb2fc540742c06edabd2287068b469a6169","_cell_guid":"88718843-bb8a-400a-8cfd-4ff4948580ca","trusted":true},"cell_type":"code","source":"#DEFINING CALLBACKS\nearlystopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbackslist = [earlystopping]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba2258b0f57bf11c2f666c4a56c7cb3891cdbee8","_cell_guid":"73063c77-cdf5-471c-8d56-a54bf439d886","trusted":true,"collapsed":true},"cell_type":"code","source":"#TRAINING THE NETWORK\nhist = model.fit(x_train, y_tr, epochs=epochs, callbacks=callbackslist, validation_split=0.1, shuffle=True, batch_size=batchsize)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6738aa50eb235b3bf8ae51cea4311efc9690feb3","_cell_guid":"9d530456-c94b-4255-9ccc-0314e6d333a4","trusted":true},"cell_type":"code","source":"ytest = model.predict(testcnndata, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d50b4877cc148ad31b31d6355d0d9d315c60e390","_cell_guid":"44976f8e-e1d7-4bb5-b815-34ac33d0dcf1","trusted":true,"collapsed":true},"cell_type":"code","source":"#CREATING THE SUBMISSION.CSV FILE\nsubmissiondf = pd.DataFrame(columns=['id'] + labelnames)\nsubmissiondf['id'] = testcomments['id'].values \nsubmissiondf[labelnames] = ytest \nsubmissiondf.to_csv(\"./cnn_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3afe119dd9db5940bda4c7c31e093629518b1c97","_cell_guid":"2c0f0ac9-642a-45cf-bada-2735d8a1a457","trusted":true,"collapsed":true},"cell_type":"code","source":"#GENERATING THE GRAPHS\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"341a0047130d5f90add22851e6d42f64d4a23edd","_cell_guid":"8eefaa45-ad86-4458-8218-e258572f4c48","trusted":true,"collapsed":true},"cell_type":"code","source":"plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}