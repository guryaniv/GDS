{"cells": [{"source": ["# Spooky Author Identification (pt-BR)\n", "This is an instructional notebook for Brazilian Portuguese Speakers\n", "\n", "Este notebook \u00e9 um exemplo de submiss\u00e3o para a competi\u00e7\u00e3o Spooky Author Identification do Kaggle. Ele foi pensado com o objetivo de que iniciantes no ramo de Machine Learning tenham um guia simples de como fazer sua primeira submiss\u00e3o.\n", "\n", "O que estamos usando aqui \u00e9 um classificador do tipo Naive Bayes e usando o b\u00e1sico dos pacotes d sicikit learn para trabalhar com texto.\n", "\n", "Vamos iniciar importando as bibliotecas b\u00e1sicas"], "metadata": {}, "cell_type": "markdown"}, {"source": ["import pandas as pd   # manipulacao de dados do CSV\n", "import numpy as np    # algebra linear e calculos em geral"], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": 1}, {"source": ["Nosso pr\u00f3ximo passo \u00e9 carregar os arquivos usando pandas."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "submission_df = pd.read_csv('../input/sample_submission.csv')"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 3}, {"source": ["Vamos dar uma olhada nesses arquivos? O primeiro ponto importante \u00e9 sabermos que tipo de informa\u00e7\u00e3o n\u00f3s temos."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 4}, {"source": ["Legal. O arquivo test_df tem o mesmo format, exceto que n\u00e3o tem a coluna de author. \u00c9 o que estamos querendo prever, certo?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["test_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 5}, {"source": ["O arquivo de submission \u00e9 um exemplo de como devemos mandas as previs\u00f5es. Vamos usar ele como template mais pra frente. O importante aqui \u00e9 notar que na submiss\u00e3o, a previs\u00e3o \u00e9 baseada em probabilidades."], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 6}, {"source": ["Mais um passo importante na hora de entendermos os dados \u00e9 saber se o nosso dataset est\u00e1 equilibrado. Isso pode alterar o nosso tipo de abordagem de classifica\u00e7\u00e3o."], "metadata": {}, "cell_type": "markdown"}, {"source": ["count_by_author = train_df.groupby('author')['id'].count()\n", "count_by_author"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 7}, {"source": ["Ok. Ele n\u00e3o \u00e9 perfeitamente equilibrado, mas tamb\u00e9m n\u00e3o \u00e9 muito ruim. Vamos calcular o nosso baseline, ou seja: qual seria a performance de um classificador tirivial, que sempre prediz a classe mais prov\u00e1vel? (Nesse caso, Edgard Allan Poe)"], "metadata": {}, "cell_type": "markdown"}, {"source": ["count_by_author.max()/count_by_author.sum()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 8}, {"source": ["Em resumo: nosso classificador \"inteligente\" tem que acertar mais de 40% pra ser melhor que o classificador trivial."], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Prepara\u00e7\u00e3o dos Dados\n", "\n", "O sklear, que \u00e9 a biblioteca de ML que vamos usar, trabalha apenas com dados num\u00e9ricos. Logo, vamos ter que dar uma massageada nos dados, porque tudo o que temos no CSV \u00e9 texto. Lembra o que vimos l\u00e1 em cima?\n", "\n", "### Bag of Words\n", "O primeiro passo que vamos fazer \u00e9 transformar o texto de cada linha em uma representa\u00e7\u00e3o de Bag of Words. Isso vai incluir o processo de tokeniza\u00e7\u00e3o."], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "bow = CountVectorizer()"], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": 9}, {"source": ["Vamos pedir para que o CountVectorizer \"aprenda\" o vocabul\u00e1rio do nosso texto. Isso vai fazer com que ele seja capaz de trabalhar com Bag of Words.\n", "\n", "*Importante*: veja que estamos ensinando o vocabul\u00e1rio usando os dois datasets: train e test! Isso \u00e9 importante porque pode ser que existam palavras no dataset de test que n\u00e3o exista no dataset de treino."], "metadata": {}, "cell_type": "markdown"}, {"source": ["bow.fit(test_df.append(train_df)['text'])"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 10}, {"source": ["\u00d3timo! Agora ele conhece o vocabul\u00e1rio. Quer ver?\n", "\n", "Vamos come\u00e7ar espiando o testo da primeira amostra do dataset de treino."], "metadata": {}, "cell_type": "markdown"}, {"source": ["print(train_df.iloc[0].text)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 11}, {"source": ["print(bow.vocabulary_['process'])\n", "print(bow.vocabulary_['afforded'])\n", "print(bow.vocabulary_['ascertaining'])\n", "print(bow.vocabulary_['this'])"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 12}, {"source": ["print(\"Tamanho do Vocabulario aprendido:\", len(bow.vocabulary_))"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 13}, {"source": ["Nesse processo as palavras foram todas convertidas para minusculas"], "metadata": {}, "cell_type": "markdown"}, {"source": ["'This' in bow.vocabulary_"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 14}, {"source": ["\u00d3timo. Agora que j\u00e1 temos um vocabul\u00e1rio, hora de transformar o nosso texto em n\u00fameros, usando a representa\u00e7\u00e3o de Bag of Words. O resultado vai ser uma matriz em que cada linha \u00e9 uma amostra (alinhada com o dataset de treinamento) e cada coluna representa o n\u00famero de vezes que aquela palavra apareceu."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_X_bow = bow.transform(train_df['text'])\n", "train_X_bow"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 15}, {"source": ["\u00c9 um pouco dif\u00edcil de trabalhar com matrizes esparsas. Sabemos que essa matriz tem 19579 linhas (s\u00e3o as amostras) e 28300 colunas (s\u00e3o as palavras). Vamos espiar uma linha pra entender melhor o que est\u00e1 acontecendo? Vamos espiar a primeira linha."], "metadata": {}, "cell_type": "markdown"}, {"source": ["x_bow_0 = train_X_bow[0].toarray().reshape(-1)\n", "x_bow_0"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 16}, {"source": ["Ainda dif\u00edcil de ver. Vamos reordenar, pra ficar mais f\u00e1cil. Vamos ordenar em ordem decrescente, para saber quais as palavras mais frequentes nesse texto. Pelo modelo de bag of words, essas seriam as mais importantes."], "metadata": {}, "cell_type": "markdown"}, {"source": ["idx = np.argsort(-x_bow_0)[:20]\n", "print(idx)\n", "print(x_bow_0[idx])\n", "print(np.array(bow.get_feature_names())[idx])"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 17}, {"source": ["Fica a\u00ed a reflex\u00e3o: voc\u00ea acha que essa ordena\u00e7\u00e3o de fato reflete a import\u00e2ncia dessas palavras? Tem jeito de melhorar isso?\n", "\n", "Dica: stop words e TF-IDF"], "metadata": {}, "cell_type": "markdown"}, {"source": ["### TF-IDF\n", "TF-IDF para os \u00edntimos, \u00e9 a sigla de Term Frequency - Inverse Document Frequency. Trata-se de uma transforma\u00e7\u00e3o sobre o modelo de Bag of Words para tentar resolver alguns dos problemas que vimos ali em cima.\n", "\n", "Essa transforma\u00e7\u00e3o faz uma m\u00e1gica: ele diminui a import\u00e2ncia de palavras que aparecem em muitos documentos (como the, of, etc) e aumenta a import\u00e2ncia de palavras que s\u00e3o mais raras naquelo documento: ou seja - que provavelmente s\u00e3o mais alinhadas com o estilo do autor ou do assunto. Quer saber como calcular o TF-IDF? \u00c9 f\u00e1cil, mas a gente n\u00e3o vai tratar disso aqui. [https://en.wikipedia.org/wiki/Tf%E2%80%93idf]\n", "\n", "Primeiro n\u00f3s precisamos fazer o TF-IDF Transformer \"entender\" quais as palavras comuns e quais n\u00e3o s\u00e3o comuns."], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.feature_extraction.text import TfidfTransformer\n", "tfidf = TfidfTransformer()\n", "tfidf.fit(train_X_bow)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 18}, {"source": ["Agora que ele j\u00e1 sabe quais as palavras comuns, hora de transformar o nosso bag of words"], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_X_tfidf = tfidf.transform(train_X_bow)\n", "train_X_tfidf"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 19}, {"source": ["Note que o tamanho da matriz \u00e9 exatamente o mesmo. Vamos espiar o conte\u00fado?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["x_tfidf_0 = train_X_tfidf[0].toarray().reshape(-1)\n", "x_tfidf_0"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 20}, {"source": ["Ainda dif\u00edcil de ver porque \u00e9 bem esparso. Vamos ordenar?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["idx = np.argsort(-x_tfidf_0)[:20]\n", "print(idx)\n", "print(x_tfidf_0[idx])\n", "print(np.array(bow.get_feature_names())[idx])"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 21}, {"source": ["E a\u00ed? Agora faz mais sentido esse crit\u00e9rio de import\u00e2ncia (ou caracter\u00edstica) de cada autor?\n", "\n", "Fica aqui mais uma reflex\u00e3o: \u00e9 sempre desej\u00e1vel termos essa representa\u00e7\u00e3o"], "metadata": {}, "cell_type": "markdown"}, {"source": ["### LabelEncoding do target\n", "O nome do autor tamb\u00e9m \u00e9 texto. Vamos ter que dar um jeito de converter os nomes dos autores para valores num\u00e9rico, porque \u00e9 o jeito que o sklearn trabalhar. Os 3 textos que temos que transformar s\u00e3o EAP, HPL e MWS. \n", "\n", "O m\u00e9todo que vamos usar \u00e9 chamado de Label Encoding. Ou seja: a cada texto, vamos atribuir um inteiro. Como por exemplo: EAP = 0, HPL = 1 e MWS = 2.\n", "\n", "Antes de atribuirmos, precisamos que o Encoder \"aprenda\" quais as categorias existentes."], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.preprocessing import LabelEncoder\n", "le = LabelEncoder()\n", "le.fit(train_df['author'])\n", "le.classes_"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 22}, {"source": ["\u00d3timo! Agora o LabelEncoder j\u00e1 sabe quais s\u00e3o as categorias que ele tem que mapear. Agora precisamos efetivamente converter a coluna author. Bora l\u00e1."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_y = le.transform(train_df['author'])\n", "train_y"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 23}, {"source": ["## Treinando um Modelo Preditivo\n", "Agora que j\u00e1 temos os dados preparados e transformados em dados num\u00e9ricos, podemos treinar o nosso modelo de machine learning.\n", "\n", "Para essa competi\u00e7\u00e3o, vamos usar um classificador que em geral tem uma boa performance trabalhando com texto. Ele \u00e9 baseado em estat\u00edstica bayesiana e \u00e9 normalmente chamado de Naive Bayes (Naive porque ele acredita que os dados n\u00e3o tenham rela\u00e7\u00e3o entre si...). N\u00e3o vamos entrar em detalhes aqui do que \u00e9 um modelo bayesiano. O que importrante pra gente: ele \u00e9 bom em calcular probabilidades. Se ele sabe que 40% dos textos s\u00e3o do Edgard Alan Poe e que a palavra \"ascertaining\" tem 0,03% de chance de aparecer num texto do Poe e que \"uniform\" tem 0,01% de chance de aparecer num texto do Poe, um texto que tem as palavras \"uniform\" e \"ascertaining\" tem qual a probabilidade de ser um texto do Poe? E da Mary Shelley? E do Lovecraft?\n", "\n", "Esse \u00e9 o tipo de c\u00e1lculo que esse classificador faz. Quer ver mais detalhes? https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n", "\n", "Curiosidade: os filtros de Spam funcionam exatamente com esse tipo de classificador."], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.naive_bayes import MultinomialNB\n", "nb = MultinomialNB(alpha=1.0)"], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": 24}, {"source": ["Para treinar o modelo, temos que passar pra ele as \"features\" ou caracter\u00edsticas - nesse caso, TFIDF - e quais s\u00e3o os targets, pra que ele possa calcular as probabilidades."], "metadata": {}, "cell_type": "markdown"}, {"source": ["nb.fit(train_X_tfidf, train_y)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 25}, {"source": ["Modelo treinado. Com o modelo treinado, podemos come\u00e7ar us\u00e1-lo para fazer predi\u00e7\u00f5es. S\u00f3 de farra, vamos usar o pr\u00f3prio dataset de treinamento para fazer uma previs\u00e3o e ver como ele se comporta."], "metadata": {}, "cell_type": "markdown"}, {"source": ["y_pred = nb.predict(train_X_tfidf)\n", "y_pred"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 26}, {"source": ["Ou seja: ele previu que o primeiro texto \u00e9 do Poe (0), o segundo tamb\u00e9m, ... e o \u00faltimo \u00e9 do Lovecraft (1)\n", "\n", "## Avaliando o Modelo\n", "Mas e a\u00ed. Esse modelo \u00e9 bom? Qual \u00e9 a taxa de acerto?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.metrics import accuracy_score\n", "accuracy_score(train_y, y_pred)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 27}, {"source": ["Boa! 89%! Nada mal! ;-) Bem melhor que os 40% do nosso classificador trivial.\n", "\n", "Ponto de aten\u00e7\u00e3o: veja que n\u00f3s estamos usando os dados de treino pra prever os dados de treino. No mundo real n\u00e3o \u00e9 assim que funciona: o seu modelo tem que prever dados que nunca viu na vida. Logo, n\u00e3o \u00e9 um n\u00famero que d\u00e1 pra confiar. Vamos ver isso um pouco mais pra frente."], "metadata": {}, "cell_type": "markdown"}, {"source": ["No comecinho desse notebok n\u00f3s falamos que o que a competi\u00e7\u00e3o pede na verdade \u00e9 qual \u00e9 a probabilidade de autoria. Veja um exemplo do arquivo de submiss\u00e3o."], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 28}, {"source": ["N\u00f3s conseguimos gerar essas probabilidades usando o m\u00e9todo predict_proba."], "metadata": {}, "cell_type": "markdown"}, {"source": ["y_pred_proba = nb.predict_proba(train_X_tfidf)\n", "y_pred_proba"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 29}, {"source": ["Nessa competi\u00e7\u00e3o, a m\u00e9trica que eles escolheram n\u00e3o foi accuracy.\n", "\n", "\u00c9 uma outra m\u00e9trica chamada Log Loss, ou Cross Entropy. O que essa m\u00e9trica faz (atrav\u00e9s de uma f\u00f3rmula n\u00e3o muito complicada) \u00e9 penalizar quando voc\u00ea calcula as probabilidades de forma muito errada e te premia quando as probabilidades est\u00e3o certas. \n", "\n", "Ou seja: se voc\u00ea previou com 100% de certeza que o texto \u00e9 do Edgard Alan Poe mas errou, ele te d\u00e1 uma penaliza\u00e7\u00e3o alta. Se voc\u00ea previou com 40% e errou, ele ainda te penaliza, mas a penaliza\u00e7\u00e3o \u00e9 menor.\n", "\n", "Se quiser saber como calular, pode dar uma espiada aqui: https://en.wikipedia.org/wiki/Cross_entropy\n", "\n", "*Importante*: ao contr\u00e1rio da Acur\u00e1cia, o Log Loss melhora quando diminui. Quando menor, melhor."], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.metrics import log_loss\n", "log_loss(y_pred=y_pred_proba, y_true=train_y)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 30}, {"source": ["E isso a\u00ed? \u00c9 bom ou ruim? Log Loss \u00e9 uma m\u00e9trica dif\u00edcil de interpretar. O que posso dizer \u00e9 que nesse momento, com esse Log Loss voc\u00ea estaria mais ou menos na metade do Leader Board da competi\u00e7\u00e3o. Mas lembre-se: n\u00e3o d\u00e1 pra confiar, porque estamos avaliando o modelo com os pr\u00f3prios dados que usamos pra treinar."], "metadata": {}, "cell_type": "markdown"}, {"source": ["### Cross Validation\n", "Para poder avaliar o modelo mais pr\u00f3ximo do mundo real, n\u00f3s temos que fazer previs\u00f5es com dados que n\u00e3o foram vistos durante o treino. Isso normalmente \u00e9 feito separando os dados, reservando parte apenas para avaliar o modelo. Essa t\u00e9cnica \u00e9 chamada de Holdout ou Split e funciona bem quando voc\u00ea tem muitos dados.\n", "\n", "O que n\u00f3s vamos usar aqui \u00e9 um m\u00e9todo diferente, chamado de K-Fold cross validation. \u00c9 parecido com o holdout, s\u00f3 o que n\u00f3s vamos fazer \u00e9 separar o dataset em k grupos diferentes. E a\u00ed vamos usar o primeiro grupo como holdout e os seguintes para treinar. Em seguida pegamos o segundo como holdout e os outros pra treinar. E assim por diante. Na pr\u00e1tica, se temos um 10-Fold cross validation, vamos treinar o modelo 10 vezes e avaliar 10 vezes usando dados que n\u00e3o foram usados durante o treinamento.\n", "\n", "Vamos come\u00e7ar criando a classe que \"splita\" o nosso DataFrame"], "metadata": {}, "cell_type": "markdown"}, {"source": ["from sklearn.model_selection import cross_val_predict, StratifiedKFold\n", "cv = StratifiedKFold(n_splits=10, random_state=42)"], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": 31}, {"source": ["O pr\u00f3ximo passo \u00e9 usar o cross_val_predict (d\u00e1 pra usar o cross_val_score, tamb\u00e9m). O que ele faz \u00e9 fazer fit e predict nas 10 folds e gerar o que a comunidade chama de \"Out of Fold Prediction\". No final, calculamos a acur\u00e1cia."], "metadata": {}, "cell_type": "markdown"}, {"source": ["y_pred = cross_val_predict(MultinomialNB(alpha=1.0),\n", "                           train_X_tfidf,\n", "                           train_y,\n", "                           cv=cv)\n", "accuracy_score(train_y, y_pred)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 32}, {"source": ["Notou que o valor \u00e9 menor do que quando calculamos a acur\u00e1cia apenas usando o dataset de treinamento? Esse n\u00famero provavelmente \u00e9 muito mais pr\u00f3ximo do desempenho que ele vai ter na vida real.\n", "\n", "Vamos fazer o mesmo com o log_loss, que \u00e9 a m\u00e9trica oficial dessa competi\u00e7\u00e3o."], "metadata": {}, "cell_type": "markdown"}, {"source": ["y_pred_proba = cross_val_predict(MultinomialNB(alpha=1.0),\n", "                                 train_X_tfidf,\n", "                                 train_y,\n", "                                 cv=cv,\n", "                                 method='predict_proba')\n", "from sklearn.metrics import log_loss\n", "log_loss(train_y, y_pred_proba)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 33}, {"source": ["Notou que ele \u00e9 pior que o train score?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Preparando a Submiss\u00e3o\n", "\n", "Hora de preparar a nossa submiss\u00e3o. Vamos pegar o nosso modelo e prever os resultados a partir do arquivo de teste. \u00c9 assim que o Kaggle avalia o seu modelo: entendendo como ele se comporta em um conjunto de dados que n\u00e3o foi visto durante o treinamento. Antes de fazer a previs\u00e3o, precisamos aplicar exatamente as mesmas transforma\u00e7\u00f5es que fizemos no dataset de treinamento.\n", "\n", "Come\u00e7ando com a convers\u00e3o pra bag of words..."], "metadata": {}, "cell_type": "markdown"}, {"source": ["test_X_bow = bow.transform(test_df['text'])\n", "test_X_bow"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 34}, {"source": ["Aplicando o TF-IDF..."], "metadata": {}, "cell_type": "markdown"}, {"source": ["test_X_tfidf = tfidf.transform(test_X_bow)\n", "test_X_tfidf"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 35}, {"source": ["E, finalmente, fazendo a previs\u00e3o. Vamos usar o predict_proba, porque \u00e9 o que o Kaggle espera."], "metadata": {}, "cell_type": "markdown"}, {"source": ["y_pred_proba = nb.predict_proba(test_X_tfidf)\n", "y_pred_proba"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 36}, {"source": ["### Gerando o arquivo de submiss\u00e3o\n", "\n", "Agora s\u00f3 falta gerar o arquivo que vai ser submetido. Lembrando, o formato de arquivo esperado \u00e9 o seguinte:"], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 37}, {"source": ["Agora precisamos atribuir os valores que foram previstos pelo modelo."], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df['EAP'] = y_pred_proba[:, 0]\n", "submission_df['HPL'] = y_pred_proba[:, 1]\n", "submission_df['MWS'] = y_pred_proba[:, 2]"], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": 38}, {"source": ["O resultado final vai ser parecido com isso."], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df.head()"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 39}, {"source": ["Ou seja: parao primeiro texto, nosso modelo acha que tem 29% de chance de ser do Poe, 8% de chance de ser do Lovecraft e 62% da Mary Shelley. E a\u00ed? Qual \u00e9 o autor real? Bom, a realidade \u00e9 que nigu\u00e9m sabe. O kaggle mantem essa informa\u00e7\u00e3o secreta e usa esses dados secretos para calcular o score real do seu modelo.\n", "\n", "S\u00f3 por refer\u00eancia, esse \u00e9 o texto que ele est\u00e1 prevendo na primeira linha. Voc\u00ea acha que acertamos dizendo que \u00e9 da Mary Shelley? Nunca saberemos. :-)"], "metadata": {}, "cell_type": "markdown"}, {"source": ["test_df.iloc[0].text"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 40}, {"source": ["Agora s\u00f3 falta gravar o arquivo. E submeter."], "metadata": {}, "cell_type": "markdown"}, {"source": ["submission_df.to_csv('basic-submission-multonmial-nb.csv', index=False)"], "metadata": {}, "cell_type": "code", "outputs": [], "execution_count": 43}, {"source": ["Anote aqui, pra depois deixar documentado.\n", "\n", "Qual foi o seu Log Loss calculado no Cross Validation?\n", "\n", "CV=?\n", "\n", "Qual foi o seu Los Loss calculado pelo Kaggle no Leader Board?\n", "\n", "LB=?"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Sugest\u00f5es de Exerc\u00edcios\n", "Daqui pra frente, a sugest\u00e3o \u00e9 tentar melhorar o modelo. Tente alterar os par\u00e2metros de modelagem e descobrir qual o melhor modelo que voc\u00ea conseguir. Tente alterar o par\u00e2metro alpha no classificador. Seus resultados melhoram? E se n\u00e3o usarmos Tf-Idf, usando direto o Bag of Words? E se us\u00e1ssemos stop words, teria diferen\u00e7a?\n", "\n", "Se souber trabalhar com outros classificadores, tente mudar o tipo de classificador (Ex: RandomForest).\n", "\n", "Use a sua Cross Validation para descobrir qual o melhor modelo e no final fa\u00e7a uma nova submiss\u00e3o. Documente os resultados."], "metadata": {}, "cell_type": "markdown"}, {"source": [], "metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}