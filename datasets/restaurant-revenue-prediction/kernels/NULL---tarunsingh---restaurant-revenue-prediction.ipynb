{"cells": [{"metadata": {"_cell_guid": "bb3b48f7-0e1c-4474-af64-96f4e7019f8a", "_uuid": "c5a994496ad58eab89bbf32da41a95a5c4c56b57"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import math as mh\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# For visualizations\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "% config InlineBackend.figure_format = 'png'\n", "% matplotlib inline\n", "\n", "# For data parsing\n", "from datetime import datetime\n", "\n", "# For choosing attributes that have good gaussian distribution\n", "from scipy.stats import shapiro\n", "\n", "# Needed for getting parameters for models\n", "from sklearn.cross_validation import LeaveOneOut\n", "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n", "\n", "# Models\n", "from sklearn.svm import SVR, LinearSVR\n", "from sklearn.ensemble import RandomForestRegressor, ExtraTreesClassifier\n", "from sklearn.linear_model import Ridge, Lasso\n", "from sklearn import cluster\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# For scaling/normalizing values\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f14654a2-39ab-48b2-b826-1372aab78354", "_uuid": "be8d54871acbff0e965c228f95f5dd26299ef55a"}, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "print(\"Train :\",train.shape)\n", "print(\"Test:\",test.shape)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "699fbcfa-2348-4ae6-8f8b-71842f24c763", "_uuid": "1c2c681da53bd083f2d253a19faee6854c247fad"}, "source": ["# Calculate number of samples in training and test datasets\n", "num_train = train.shape[0]\n", "num_test = test.shape[0]\n", "print(num_train, num_test)\n", "\n", "# For feature engineering, combine train and test data\n", "data = pd.concat((train.loc[:, \"Open Date\" : \"P37\"],\n", "                  test.loc[:, \"Open Date\" : \"P37\"]), ignore_index=True)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "af735b66-e309-4f71-8e86-1c6eca4282e2", "_uuid": "e007c07ffaf5f4011d6169bccd9d307bb29833c3"}, "source": ["data.tail()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6c2c342d-96cb-4641-8ace-769aa14a3754", "_uuid": "bfd22a66de0868180cd9086073d2178f95114be4"}, "source": ["#Get name of all headers of data frame\n", "#list(data)\n", "data.columns.values.tolist()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "cdb4a0c6-477f-48f1-93a1-bc6cd4d564b0", "_uuid": "05873c89aeec681b5bfd0cf50bba28ede486d270"}, "cell_type": "markdown", "source": ["**Check for Missing values and plot for each column**"]}, {"metadata": {"_cell_guid": "4618cf09-8181-41a0-86c5-e7dbd22fd2c6", "_uuid": "0e9560bf34ce732cbfd588997253edd95b34654e"}, "source": ["print(data.isnull().sum().T) #No null values in any column, so no imputation and removal of rows\n", "missing_df = data.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df = missing_df.loc[missing_df['missing_count']>0]\n", "missing_df = missing_df.sort_values(by='missing_count')\n", "\n", "ind = np.arange(missing_df.shape[0])\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Count of missing values\")\n", "ax.set_title(\"Number of missing values in each column\")\n", "plt.show()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "cdedd0ad-71b1-4fba-aaca-464ad6134f45", "_uuid": "b3def039c1c33641fea429880ec84f489cf06a57"}, "source": ["# Convert date to days\n", "# Have to drop date \n", "import time\n", "from datetime import datetime as dt\n", "# train\n", "all_diff = []\n", "for date in data[\"Open Date\"]:\n", "    diff = dt.now() - dt.strptime(date, \"%m/%d/%Y\")\n", "    all_diff.append(int(diff.days/1000))\n", "\n", "data['Days_from_open'] = pd.Series(all_diff)\n", "print(data.head())"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "e358274f-9a12-4343-b18e-93051a186acb", "_uuid": "48d0d1b21acd2ba87d8bd9692900e085120cede5"}, "source": ["#Drop Open Date Column\n", "data = data.drop('Open Date', 1)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"scrolled": true, "_cell_guid": "e4c49225-dee9-4b1c-9166-2dd99f89c576", "_uuid": "a19b6fcf5be0129e15afa9d7d52368d55accf8a3"}, "source": ["# Plotting mean of P-variables over each city helps us see which P-variables are highly related to City\n", "# since we are given that one class of P-variables is geographical attributes.\n", "distinct_cities = train.loc[:, \"City\"].unique()\n", "\n", "# Get the mean of each p-variable for each city\n", "means = []\n", "for col in train.columns[5:42]:\n", "    temp = []\n", "    for city in distinct_cities:\n", "        temp.append(train.loc[train.City == city, col].mean())     \n", "    means.append(temp)\n", "    \n", "# Construct data frame for plotting\n", "city_pvars = pd.DataFrame(columns=[\"city_var\", \"means\"])\n", "for i in range(37):\n", "    for j in range(len(distinct_cities)):\n", "        city_pvars.loc[i+37*j] = [\"P\"+str(i+1), means[i][j]]\n", "        \n", "# Plot boxplot\n", "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n", "sns.boxplot(x=\"city_var\", y=\"means\", data=city_pvars)\n", "\n", "# From this we observe that P1, P2, P11, P19, P20, P23, and P30 are approximately a good\n", "# proxy for geographical location."], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "1a456fd6-07f1-43d2-8815-1a03e6103c50", "_uuid": "a145d232f8a433df8dbe3caf66bf35e658f6f405"}, "source": ["# K Means treatment for city (mentioned in the paper)\n", "def adjust_cities(data, train, k):\n", "    \n", "    # As found by box plot of each city's mean over each p-var\n", "    relevant_pvars =  [\"P1\", \"P2\", \"P11\", \"P19\", \"P20\", \"P23\", \"P30\"]\n", "    train = train.loc[:, relevant_pvars]\n", "    \n", "    # Optimal k is 20 as found by DB-Index plot    \n", "    kmeans = cluster.KMeans(n_clusters=k)\n", "    kmeans.fit(train)\n", "    \n", "    # Get the cluster centers and classify city of each data instance to one of the centers\n", "    data['City Cluster'] = kmeans.predict(data.loc[:, relevant_pvars])\n", "    del data[\"City\"]\n", "    \n", "    return data\n", "\n", "# Convert unknown cities in test data to clusters based on known cities using KMeans\n", "data = adjust_cities(data, train, 20)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "063c627f-6c41-46b5-950d-662568f6bc9d", "_uuid": "5cd710ce35609725efc62e12919d529c1f093a13"}, "source": ["# The two categories of City Group both appear very frequently\n", "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n", "sns.countplot(x='City Group', data=train, palette=\"Greens_d\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "e3abd2f8-a32f-414c-8fe7-8e0d4c9d36d8", "_uuid": "6ee823095527b7ae57f716c005dedb9badfed500"}, "source": ["# One hot encode City Group\n", "data = data.join(pd.get_dummies(data['City Group'], prefix=\"CG\"))\n", "\n", "# Since only n-1 columns are needed to binarize n categories, drop one of the new columns.  \n", "# And drop the original columns.\n", "data = data.drop([\"City Group\", \"CG_Other\"], axis=1)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "fdc8909b-c5ea-461a-a99b-e3b50e1d1036", "_uuid": "4de568b690f7d054be2300bedbbcb9daa9ed3f6c"}, "source": ["#Check the data type of all columns\n", "data.dtypes"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4a2f8d08-44e0-457d-8272-35b633db41dc", "_uuid": "1dc3284aa77b030603f003f16dd47d6300c98ef6"}, "source": ["#Check the type column \n", "# Two of the four Restaurant Types (DT and MB), are extremely rare\n", "sns.countplot(x='Type', data=data, palette=\"Greens_d\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "06de99ec-9905-4c4b-bd67-49bb0802bcdd", "_uuid": "8f69d7c9a0a4381f46a5627d2b1061ecc8e72473"}, "source": ["# One hot encode Restaurant Type\n", "data = data.join(pd.get_dummies(data['Type'], prefix=\"T\"))\n", " \n", "# Drop the original column\n", "data = data.drop([\"Type\"], axis=1)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "fb095f07-41a3-40b3-8487-9feb3b4f2ecd", "_uuid": "e4567a98f89d8abe933889c09bf29ee1432b32ae"}, "source": ["data.head()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "78c08065-2794-4179-98cd-16e2c5830355", "_uuid": "c0778fa12495d54da13ca85664b675e24536b8ba"}, "source": ["#Count distinct values for each column in Data frame\n", "data.apply(lambda x: len(x.unique()))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "572b1009-ff82-4c07-8f15-c8a99bc9c5d8", "_uuid": "bdeb3aad8f2c9dfc1a7ac5dc25e0aacea477bce0"}, "source": ["# Scale all input features to between 0 and 1.\n", "min_max_scaler = MinMaxScaler()\n", "data = pd.DataFrame(data=min_max_scaler.fit_transform(data),columns=data.columns, index=data.index)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "356b99ad-e521-407e-b9dc-2572cb52b3f1", "_uuid": "dc5eb661c12d650f7db0df4b88768e4e1678eaec"}, "source": ["train.head()"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "2577af7f-c95f-427b-a749-86eb044f62ed", "_uuid": "2ea9332bee6ad9acecba3cb3ec6fe9f42c3ce009"}, "source": ["#Revenue Distribution of Train Set\n", "# Check distribution of revenue and log(revenue) (Other Transformation could be Sqrt Transformation)\n", "plt.rcParams['figure.figsize'] = (16.0, 6.0)\n", "pvalue_before = shapiro(train[\"revenue\"])[1]\n", "pvalue_after = shapiro(np.log(train[\"revenue\"]))[1]\n", "graph_data = pd.DataFrame(\n", "        {\n", "            (\"Revenue\\n P-value:\" + str(pvalue_before)) : train[\"revenue\"],\n", "            (\"Log(Revenue)\\n P-value:\" + str(pvalue_after)) : np.log(train[\"revenue\"])\n", "        }\n", "    )\n", "graph_data.hist()\n", "\n", "#Shapiro Wilks test for normality\n", "# log transform revenue as it is approximately normal. If this distribution for revenue holds in the test set,\n", "# log transforming the variable before training models will improve performance vastly.\n", "# However, we cannot be completely certain that this distribution will hold in the test set.\n", "train[\"revenue\"] = np.log(train[\"revenue\"])"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "8e24c6f2-9a4f-4a9b-a287-48435f080218", "_uuid": "e778a05cda532a09f70f19ae8722810c366c9bad"}, "source": ["# Split into train and test datasets\n", "train_processed = data[:num_train]\n", "test_processed = data[num_train:]"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["from sklearn import cross_validation, linear_model,ensemble\n", "import matplotlib.pyplot as plt\n", "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n", "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n", "from sklearn.svm import SVR\n", "\n", "regr = linear_model.LinearRegression()\n", "regr.get_params()\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true}, "source": ["import warnings\n", "warnings.filterwarnings('ignore')"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_kg_hide-input": false, "_cell_guid": "e421d915-6c43-4580-9369-e8e07d44cd1b", "_uuid": "db1262cac127dbf277a663c4fe5d2a48d4d49f46"}, "source": ["# build model\n", "from sklearn import cross_validation,linear_model,ensemble\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn import metrics\n", "from sklearn.metrics import make_scorer, mean_squared_error\n", "import matplotlib.pyplot as plt\n", "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n", "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n", "from sklearn.svm import SVR\n", "\n", "# simple regression\n", "print(\"Simple regression\")\n", "\n", "#create linear regression model object\n", "regr = linear_model.LinearRegression()\n", "#regr.get_params() -- Check the list of paramters for the given model\n", "\n", "# create a parameter grid: map the parameter names to the values that should be searched\n", "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n", "\n", "def RMSE(y_true,y_pred):\n", "    rmse = mh.sqrt(mean_squared_error(y_true, y_pred))\n", "    print('RMSE: %2.3f' % rmse)\n", "    return rmse\n", "\n", "'''def R2(y_true,y_pred):    \n", "     r2 = r2_score(y_true, y_pred)\n", "     print('R2: %2.3f' % r2)\n", "     return r2\n", "'''\n", "    \n", "def two_score(y_true,y_pred):\n", "    score = RMSE(y_true,y_pred) #set score here and not below if using MSE in GridCV\n", "    #score = R2(y_true,y_pred)\n", "    return score\n", "\n", "my_score = make_scorer(two_score, greater_is_better=False) # change for false if using MSE\n", "\n", "# instantiate the grid\n", "grid = GridSearchCV(regr, parameters, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\n", "\n", "# fit the grid with data\n", "grid.fit(train_processed, train[\"revenue\"])\n", "\n", "# Re-train on full training set using the best parameters found in the last step.\n", "# examine the best model\n", "print(\"Best score :\",grid.best_score_)\n", "print(\"Best params :\",grid.best_params_)\n", "print(\"Best estimator:\",grid.best_estimator_)\n", "regr.set_params(**grid.best_params_)\n", "regr.fit(train_processed, train[\"revenue\"])\n", "\n", "# results\n", "results_regr = regr.predict(test_processed)\n", "results_regr_exp=np.exp(results_regr)\n", "print(results_regr_exp)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true}, "source": ["submission_lin_reg = pd.DataFrame(columns=['Prediction'],index=test.index, data=results_regr_exp)\n", "submission_lin_reg.index.name = 'Id'"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_kg_hide-input": false, "_kg_hide-output": true}, "source": ["submission_lin_reg.describe().astype(int)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["# Ridge model\n", "model_grid = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]\n", "ridge_clf = Ridge()\n", "\n", "# Use a grid search and leave-one-out CV on the train set to find the best regularization parameter to use.\n", "grid = GridSearchCV(ridge_clf, model_grid, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\n", "grid.fit(train_processed, train[\"revenue\"])\n", "\n", "# Re-train on full training set using the best parameters found in the last step.\n", "# examine the best model\n", "print(\"Best score :\",grid.best_score_)\n", "print(\"Best params :\",grid.best_params_)\n", "print(\"Best estimator:\",grid.best_estimator_)\n", "ridge_clf.set_params(**grid.best_params_)\n", "ridge_clf.fit(train_processed, train[\"revenue\"])\n", "\n", "# results_ridge = np.exp(ridge_clf.predict(test_processed))\n", "results_ridge = ridge_clf.predict(test_processed)\n", "results_ridge_exp=np.exp(results_ridge)\n", "print(results_ridge_exp)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["# Lasso model\n", "model_grid = [{'normalize': [True, False], 'alpha': np.logspace(0,10)}]\n", "lasso_clf = Lasso()\n", "\n", "# Use a grid search and leave-one-out CV on the train set to find the best regularization parameter to use.\n", "grid = GridSearchCV(lasso_clf, model_grid, cv=LeaveOneOut(train.shape[0]), scoring='mean_squared_error')\n", "grid.fit(train_processed, train[\"revenue\"])\n", "\n", "# Re-train on full training set using the best parameters found in the last step.\n", "print(\"Best score :\",grid.best_score_)\n", "print(\"Best params :\",grid.best_params_)\n", "print(\"Best estimator:\",grid.best_estimator_)\n", "lasso_clf.set_params(**grid.best_params_)\n", "lasso_clf.fit(train_processed, train[\"revenue\"])\n", "\n", "#Predict the test set\n", "results_lasso = lasso_clf.predict(test_processed)\n", "results_lasso_exp = np.exp(results_lasso)\n", "print(results_lasso_exp)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {}, "source": ["#SVR()\n", "from sklearn.svm import SVR, LinearSVR\n", "\n", "svr = SVR(C=1, epsilon=0.1)\n", "svr.fit(train_processed, train[\"revenue\"])\n", "results_svm = svr.predict(test_processed)\n", "results_svm_exp = np.exp(results_svm)\n", "print(results_svm_exp)"], "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}