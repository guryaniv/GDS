{"cells":[{"metadata":{"_uuid":"d81b9c6682c8cdf1d5bfe9d1d08f8c5268c4c101"},"cell_type":"markdown","source":"# ETL HDF5\n\n## Motivation\n\nReading .parquet is slow, and it's painful to join the metadata to the measurement data.\n\nI wanted a way to save the joined data so it's easy to read back.\n\nHopefully this will help some of you having similar problems - also, welcome to HDF5!\n\n## Description\n\nBelow are some utility classes to extract, transform, and load the data into HDF5 format.\n\nI've also provided some generators to load it back out in chunks.\n\n## Design decision\n\nBecause faults are highly correlated between phases, I've decided to save each row as a trio of signal measurements, with a trio of targets. eg.\n\nX.shape = [N, 3, 8e5]\ny.shape = [N, 3]\n\nThe plan is to perform classification together, predicting one of 2^3=8 classes.\n\nI've included  a utility class BinaryEncoder to translate from targets in 3-tuple form <-> decimal.\n\n### Notes\n\nI've set the compression level in HdfEtl to 1 to not exceed the kernel disk space, but you should set it to 0 to get the fastest read times. Also uncomment the line loading the test data in start()."},{"metadata":{"trusted":true,"_uuid":"7f401571f5401197b68a744aa45e10a9376d6ab4"},"cell_type":"code","source":"! ls ../input","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, gc\nimport pandas as pd\nimport numpy as np\nimport pyarrow.parquet as pq\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import matthews_corrcoef\nfrom scipy import signal\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom numba import jit, int32\nimport tables\nfrom scipy import signal\nfrom scipy.signal import butter\nimport pywt\nfrom statsmodels.robust import mad","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class MeasurementDataType(tables.IsDescription):\n    msr = tables.Int8Col(shape=(3,800000), pos=1)\n    target = tables.BoolCol(shape=(3,), pos=2)\n\nclass HdfEtl():\n    \"\"\"Extracts data from .parquet, performs transformation into tidy(ish) \n    format, and loads into HDF5 file.\n    \"\"\"\n    \n    N_PHASE = 3\n    PQ_CHUNK = 3*300\n    COMPRESSION = 1 # slower read with higher compression\n\n    trn_h5_key = 'train'\n    val_h5_key = 'validation'\n    tst_h5_key = 'test'\n    \n    def __init__(self, path):\n        self.path = path\n        self.meta_trn_f = f'{path}/metadata_train.csv'\n        self.msr_trn_f  = f'{path}/train.parquet'\n        self.meta_tst_f = f'{path}/metadata_test.csv'\n        self.msr_tst_f  = f'{path}/test.parquet'\n        \n    def start(self, out_f='data.h5', filt=None, val_size = 0.2):\n        \"\"\"Load in the .parquet files in chunks, transform, and save as HDF5\"\"\"\n        self.h5_f = out_f\n        self.init_h5()\n        self.etl(self.meta_trn_f, self.msr_trn_f, self.trn_h5_key, \n                 self.PQ_CHUNK, filt, val_size)\n        #self.etl(self.meta_tst_f, self.msr_tst_f, self.tst_h5_key, \n        #         self.PQ_CHUNK, filt)\n\n    def etl(self, meta_f, pq_f, h5_key, chunk_size, filt=None, val_size=0.0):\n        \"\"\"\"\"\"\n        meta_df = pd.read_csv(meta_f) \n        sig_start = meta_df['signal_id'].min()\n        sig_end   = meta_df['signal_id'].max()\n\n        # load chunk of parquet file\n        for msr_df in self.read_parquet(pq_f, sig_start, sig_end, chunk_size):\n            #print(f'Adding columns: {msr_df.shape[1]}')\n            n_msr = msr_df.shape[0]                   # number of measurements ~8e5\n            start_col = int(msr_df.columns[0])        # first signal_id\n            n_obs = int(msr_df.shape[1]/self.N_PHASE) # number of observations in this chunk (tuple of 3 signals)\n                \n            msr = np.zeros(shape=(n_obs,self.N_PHASE,n_msr), dtype='int8')\n            for i in range(n_obs):\n                msr[i, :, :] = np.array(msr_df.iloc[:,i*self.N_PHASE:(i+1)*self.N_PHASE].T)\n\n            # filter measurements if provided\n            if filt is None:\n                fmsr = msr\n            else:\n                fmsr = filt(msr)\n                \n            targets = np.zeros(shape=(n_obs, self.N_PHASE))\n            if 'target' in meta_df.columns:\n                for i in range(n_obs):\n                    targets[i,:] = meta_df.loc[start_col+i:start_col+i+self.N_PHASE-1, 'target'].T\n\n            if val_size > 0:\n                val_idxs = np.random.random(targets.shape[0]) < val_size\n                self.write_h5(h5_key, fmsr[~val_idxs,:,:], targets[~val_idxs,:])\n                self.write_h5(self.val_h5_key, fmsr[val_idxs,:,:], targets[val_idxs,:])\n            else:    \n                self.write_h5(h5_key, fmsr, targets)\n            gc.collect()\n            \n    def init_h5(self):\n        if os.path.exists(self.h5_f):\n            os.remove(self.h5_f)\n            \n        filt = tables.Filters(complevel=self.COMPRESSION)\n        \n        h5f = tables.open_file(self.h5_f, 'w', title='vsb-power-line-fault-detection')\n        h5f.create_table('/', self.trn_h5_key, MeasurementDataType, 'Train Data',\n                        expectedrows=int(8712*0.8/3), filters=filt)\n        h5f.create_table('/', self.val_h5_key, MeasurementDataType, 'Validation Data',\n                        expectedrows=int(8712*0.2/3), filters=filt)\n        h5f.create_table('/', self.tst_h5_key, MeasurementDataType, 'Test Data',\n                        expectedrows=int(20337/3), filters=filt)\n        print(f'Initialised: {h5f}')\n        h5f.close()\n            \n    def write_h5(self, key, msr, targets):\n        print(f'\\nWriting {targets.shape[0]} rows to {self.h5_f} {key}')\n        h5f = tables.open_file(self.h5_f, 'r+')\n        table = h5f._get_node(key)\n        measurement_data = table.row\n        for i in range(msr.shape[0]):\n            measurement_data['msr'] = msr[i,:,:]\n            measurement_data['target'] = targets[i,:]\n            measurement_data.append()\n        table.flush()\n        h5f.close()\n    \n    def read_parquet(self, fn, from_col, to_col, chunk_size):\n        print(f'Reading: \"{fn}\"...')\n        for i in tqdm(range(from_col, to_col+1, chunk_size)):\n            if i+chunk_size <= to_col:\n                cols = [str(j) for j in range(i,i+chunk_size)]\n            else:\n                cols = [str(j) for j in range(i,to_col+1)]\n            yield pq.read_pandas(fn, columns=cols).to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eee52901d557d92e68dc075c2f90cdc97e3b836"},"cell_type":"markdown","source":"Thanks to [Jack](https://www.kaggle.com/jackvial/dwt-signal-denoising) for this HPF/DN code."},{"metadata":{"trusted":true,"_uuid":"0609a57ba292fed4dafae26578128567dd549e9b"},"cell_type":"code","source":"# from: https://www.kaggle.com/jackvial/dwt-signal-denoising\nclass Filterer:\n    \n    def __init__(self):\n        self.n_samples = 800000\n        self.sample_duration = 0.02\n        self.SAMPLE_RATE = self.n_samples * (1 / self.sample_duration)\n        \n    def filter_xs(self, xs):\n        result = np.zeros(xs.shape)\n        for i in tqdm(range(xs.shape[0])):\n            result[i,:,:] = self.filter_x(xs[i,:,:])\n        return result\n\n    def filter_x(self, x):\n        result = np.zeros(x.shape)\n        for i in range(x.shape[0]):\n            result[i,:] = self.filter_sig(x[i,:])\n        return result\n\n    def filter_sig(self, sig):\n        return self.denoise_signal(\n            self.high_pass_filter(sig, self.SAMPLE_RATE, 10000), \n            'db4', 1)\n    \n    def maddest(self, d, axis=None):\n        \"\"\"Mean Absolute Deviation\"\"\"\n        return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\n    def high_pass_filter(self, x, sample_rate, low_cutoff):\n        nyquist = 0.5 * sample_rate\n        norm_low_cutoff = low_cutoff / nyquist\n        sos = butter(10, Wn=[norm_low_cutoff], btype='highpass', output='sos')\n        filtered_sig = signal.sosfilt(sos, x)\n        return filtered_sig\n\n    def denoise_signal(self, x, wavelet, level):\n        coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n        sigma = (1/0.6745) * self.maddest(coeff[-level])\n        uthresh = sigma * np.sqrt(2*np.log(len(x)))\n        coeff[1:] = (pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:])\n        return pywt.waverec(coeff, wavelet, mode='per')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"54b2e1d220d292f49d9630c108580982a5f0245c"},"cell_type":"code","source":"# ETL data to HDF5\nfilt = Filterer()\netl = HdfEtl('../input')\netl.start('filt.h5', filt.filter_xs, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"0438245d92b35679facd7bac194a8810745a84bb"},"cell_type":"markdown","source":"## Generators"},{"metadata":{"trusted":true,"_uuid":"8fbeef6297c4080cb429680d2b61eecb9c9ab78b"},"cell_type":"code","source":"class SignalDataset:\n    \n    def __init__(self, h5f):\n        self.h5f = h5f\n        self.trn_node  = h5f.get_node('/train')\n        self.val_node  = h5f.get_node('/validation')\n        self.test_node = h5f.get_node('/test')\n\n    @property\n    def trn_x(self): return self.trn_node.cols.msr\n    \n    @property\n    def trn_y(self): return self.trn_node.cols.target\n    \n    @property\n    def val_x(self): return self.val_node.cols.msr\n    \n    @property\n    def val_y(self): return self.val_node.cols.target\n    \n    @property\n    def test_x(self): return self.test_node.cols.msr\n    \n    @property\n    def test_y(self): return self.test_node.cols.target\n    \n    def gen_trn_x(self, chunk_size = 100):\n        return self.generator(self.trn_x, chunk_size)\n    \n    def gen_trn_y(self, chunk_size = 100):\n        return self.generator(self.trn_y, chunk_size)\n    \n    def gen_val_x(self, chunk_size = 100):\n        return self.generator(self.val_x, chunk_size)\n    \n    def gen_val_y(self, chunk_size = 100):\n        return self.generator(self.val_y, chunk_size)\n    \n    def gen_test_x(self, chunk_size = 100):\n        return self.generator(self.test_x, chunk_size)\n    \n    def gen_test_y(self, chunk_size = 100):\n        return self.generator(self.test_y, chunk_size)\n    \n    def generator(self, data, chunk_size):\n        n = data.shape[0]\n        for i in range(0, n, chunk_size):\n            start_idx = i\n            end_idx = i+chunk_size if i+chunk_size < n else n\n            yield data[start_idx:end_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c914dfd9f505c88c9e33c9786ccd8ddffa7220c9"},"cell_type":"code","source":"h5f = tables.open_file('filt.h5')\nsds = SignalDataset(h5f)\ngen = sds.gen_trn_x()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"083d7251c75d81cc38221e355262f695920c583e"},"cell_type":"code","source":"trn_xc = gen.__next__()\nprint(trn_xc.shape)\ntrn_xc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4952772934e770bf90ac94bf057ebc7cec9fba3"},"cell_type":"markdown","source":"## Feature Extraction and 3-tuple Target encoding"},{"metadata":{"trusted":true,"_uuid":"2dfc88594ca70ca1606300d70192c0dae70f96d6"},"cell_type":"code","source":"class FeatureExtractor:\n   \n    def to_df(self, gen):\n        df = pd.DataFrame(columns=[\n            'min0', 'min1', 'min2', \n            'max0', 'max1', 'max2',\n            'mean0', 'mean1', 'mean2'])\n        \n        for xs in gen:\n            ds = [self.extract_features(xs[i,:,:]) for i in range(xs.shape[0])]\n            df = pd.concat([df, pd.DataFrame(ds)])\n        return df\n    \n    def extract_features(self, x):\n        min0, min1, min2 = x[0,:].min(), x[1,:].min(), x[2,:].min()\n        max0, max1, max2 = x[0,:].max(), x[1,:].max(), x[2,:].max()\n        mean0, mean1, mean2 = x[0,:].mean(), x[1,:].mean(), x[2,:].mean()\n        return {'min0':min0, 'min1':min1, 'min2':min2, \n                'max0':max0, 'max1':max1, 'max2':max2, \n                'mean0':mean0, 'mean1':mean1, 'mean2':mean2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd4243ff936c72c1c27ec17cf12999b34f4538ec"},"cell_type":"code","source":"class BinaryEncoder:\n    \n    cost_matrix = np.array([\n        [0, 1, 1, 2, 1, 2, 2, 3],\n        [1, 0, 2, 1, 2, 1, 2, 2],\n        [1, 2, 0, 1, 2, 3, 1, 2],\n        [2, 1, 1, 0, 3, 2, 2, 1],\n        [1, 2, 2, 3, 0, 1, 1, 2],\n        [2, 1, 3, 2, 1, 0, 2, 1],\n        [2, 2, 1, 2, 1, 2, 0, 1],\n        [3, 2, 2, 1, 2, 1, 1, 0]])\n    \n    def cost_decimal(self, d1, d2):\n        return self.cost_matrix[d1, d2]\n    \n    def cost_binary(self, b1, b2):\n        return sum(np.logical_xor(b1, b2))\n    \n    def v_encode(self, vb):\n        return [self.encode(x) for x in vb]\n    \n    def encode(self, b):\n        return 4*b[0] + 2*b[1] + b[2]\n    \n    def v_decode(self, vd):\n        return [self.decode(x) for x in vd]\n    \n    def decode(self, d):\n        if d == 0:\n            return np.array([0,0,0])\n        if d == 1:\n            return np.array([0,0,1])\n        if d == 2:\n            return np.array([0,1,0])\n        if d == 3:\n            return np.array([0,1,1])\n        if d == 4:\n            return np.array([1,0,0])\n        if d == 5:\n            return np.array([1,0,1])\n        if d == 6:\n            return np.array([1,1,0])\n        if d == 7:\n            return np.array([1,1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c1d796ebc94402b67246d1af2449d76ea504df9"},"cell_type":"code","source":"print(sds.trn_x.shape)\nprint(sds.val_x.shape)\n#print(sds.test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a533c80a52919c16c642f6f0037d7106b61ced"},"cell_type":"code","source":"fe = FeatureExtractor()\ntrn_x  = fe.to_df(sds.gen_trn_x())\nval_x  = fe.to_df(sds.gen_val_x())\n#test_x = fe.to_df(sds.gen_test_x())\nprint(trn_x.shape)\nprint(val_x.shape)\n#print(test_x.shape)\ntrn_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ec1a8fccb7604a0f3c875e236fec279a6940e0f"},"cell_type":"code","source":"be = BinaryEncoder()\ntrn_y  = be.v_encode(sds.trn_y[:])\nval_y  = be.v_encode(sds.val_y[:])\n#test_y = be.v_encode(sds.test_y[:])\nprint(len(trn_y))\nprint(len(val_y))\n#print(len(test_y))\nprint(sds.trn_y[:10])\nprint(trn_y[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed0f5cca7db23d1f2dc0e87762afacad2827c19d"},"cell_type":"code","source":"h5f.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"748db9042c3f7f982137720b1efdd29beb4f7955"},"cell_type":"markdown","source":"To be continued..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}