{"nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "4a7a4d22c10d6672afd30df6ddc745187dd26a29", "collapsed": false, "_cell_guid": "02a2deea-1a7b-4338-9be2-efb73b8ff097", "_execution_state": "idle"}, "outputs": [], "source": "Hi everyone, I want to approach this challenge using Deep Neural Networks of Tensorflow. For the current settings and training steps = 1000 steps, my RMSLE score is 0.44. I believe a much better result can be achieved with more investigating into feature expanding and engineering and especially more training steps (e.g. more computing power). If you have any input or want to join me as a team in this approach, feel free to contact me.\n\nI'm new to Kaggle so I'm not sure how to run code on online kernels. I have several steps of data processing and model training in separated code files and save the new datasets as csv files for later use. So let I will post the codes separately with detail instructions. \n\nSpecial thanks to @Mathijs Waegemakers for sharing the Weather Dataset and the code.\n\n*** Traveling Distance:\n\nWith the given data fields \u201cpickup_longtitude\u201d, \u201cpickup_latitude\u201d, \u201cdropoff_longtitude\u201d and \u201cdropoff_latitude\u201d, I can try to calculate the traveling distance of each taxi trip. Even though a highly accurate distance can be obtained via Google Maps API, the limit of 2,500 API calls per day of Google has made that approach inefficient for execution. Therefore, I decided to utilize the Python package \u201cgeopy\u201d to estimate distance between 2 points in meter measurement. When you run the \"distance_geopy.py\" code as below, it will create 2 csv files: \"meter_train.csv\" and \"meter_test.csv\" accordingly.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "372e21a249db92ae45b466650fca42cd582846f2", "trusted": false, "_cell_guid": "3b6cd537-d8ae-4f3f-bdf7-cdeae937a1d3"}, "outputs": [], "source": "'''\n@author: Glenn-Galang\n'''\nfrom numpy import genfromtxt\nfrom geopy.distance import great_circle\n\ntrain = genfromtxt('train.csv', delimiter=',')\n\nN1 = train.shape[0]\n\nfor i in range(1,N1):\n\tpick_up = (train[i,6],train[i,5])\n\tdrop_off = (train[i,8],train[i,7])\n\tdistance = 1000*(great_circle(pick_up,drop_off).km)\n\twith open(\"meter_train.csv\", \"a\") as myfile:\n\t\t\tmyfile.write(str(distance)+\",\\n\")\n\t\t\ntest = genfromtxt('test.csv', delimiter=',')\n\nN2 = test.shape[0]\n\nfor i in range(1,N2):\n\tpick_up = (test[i,5],test[i,4])\n\tdrop_off = (test[i,7],test[i,6])\n\tdistance = 1000*(great_circle(pick_up,drop_off).km)\n\twith open(\"meter_test.csv\", \"a\") as myfile:\n\t\tmyfile.write(str(distance)+\",\\n\")", "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "4c11fea345611aa2a2397021cdf18233aed75f02", "collapsed": false, "_cell_guid": "9ccd59a1-6769-4253-bf47-dd071d647a6f", "_execution_state": "idle"}, "outputs": [], "source": "***  Distance from city center:\n\nIn general, there will always be more traffic in the city center than other suburban neighbourhoods. So I again using Python package \"geopy\" to calculate the radical distance between city center (set as Central Park) to pickup and dropoff points. When you run the \"distance_from_central.py\" code as below, it will create 2 csv files: \"distance_from_central_train.csv\" and \"distance_from_central_test.csv\" accordingly. ", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "332a06afd055a73fe6f90543dc9ead5f106979b7", "collapsed": false, "_cell_guid": "986fcbc8-37d8-4040-a186-7ff2cca2bb48", "_execution_state": "idle", "trusted": false}, "outputs": [], "source": "'''\n@author: Glenn-Galang\n'''\nfrom numpy import genfromtxt\nfrom geopy.distance import great_circle\n    \ncentral = (40.781277,-73.966622)\n\t\ntrain = genfromtxt('train.csv', delimiter=',')\n\nN1 = train.shape[0]\n\nfor i in range(1,N1):\n\tpick_up = (train[i,6],train[i,5])\n\tdrop_off = (train[i,8],train[i,7])\n\tdistance_pickup = 1000*(great_circle(central,pick_up).km)\n\tdistance_dropoff = 1000*(great_circle(central,drop_off).km)\n\twith open(\"distance_from_central_train.csv\", \"a\") as myfile:\n\t\t\tmyfile.write(str(distance_pickup)+\",\"+str(distance_dropoff)+\",\\n\")\n\t\t\ntest = genfromtxt('test.csv', delimiter=',')\n\nN2 = test.shape[0]\n\nfor i in range(1,N2):\n\tpick_up = (test[i,5],test[i,4])\n\tdrop_off = (test[i,7],test[i,6])\n\tdistance_pickup = 1000*(great_circle(central,pick_up).km)\n\tdistance_dropoff = 1000*(great_circle(central,drop_off).km)\n\twith open(\"distance_from_central_test.csv\", \"a\") as myfile:\n\t\t\tmyfile.write(str(distance_pickup)+\",\"+str(distance_dropoff)+\",\\n\")       ", "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "d09dcdfc32b76f1bffb76ff34318a2a2db8317e9", "collapsed": false, "_cell_guid": "6be492d3-6ef1-45e6-8035-e5d8bcdb3a26", "_execution_state": "idle"}, "outputs": [], "source": "*** Data Processing:\n\nNow I have several datasets in separated CSV files. I will run \"data_processing.py\" to combine these datasets, conduct some exploratory data analysis and data cleaning.  I preprossessed the Weather dataset to change all the \"T\" (which mean \"Trace\") in 3 columns \"Precipitation\", \"Snow_fall\" and \"Snow_depth\" to 0.005. The detailed steps are in the comment of each code parts. The final datasets which will be used for model training and testing will be saved as \"train_df.csv\", \"val_df.csv\" and \"test_df.csv\".", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "bdc54421d2af823384c7cea2e9f433ba158b9bb1", "collapsed": false, "_cell_guid": "4414e03c-daaa-4c4a-8be7-a38a5be622d9", "_execution_state": "idle", "trusted": false}, "outputs": [], "source": "'''\n@author: Glenn-Galang\n'''\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import original train dataset\ntrain_df = pd.read_csv('train.csv')\n\n# import original train dataset\ntest_df = pd.read_csv('test.csv')\n\n# import weather dataset\nweat_data = pd.read_csv('weather_data_nyc_centralpark_2016.csv')\n\n# function to add snow weather by pickup date\nweat_data['date'] = pd.to_datetime( weat_data['date'] ).dt.date\nweat_data.set_index('date', inplace = True)\ndef addWeather( df ):\n    \n    df['date'] =  pd.to_datetime(df['pickup_datetime']).dt.date\n    \n    dates = df['date'].unique()\n    \n    df.set_index('date', inplace = True)\n   \n    weat_cols = ['precipitation', 'snow_fall', 'snow_depth']\n    \n    for col in weat_cols:\n        df[col] = np.nan\n        \n        for date in dates:\n            val = weat_data.loc[date, col]\n            \n            if( 'T' != val ):\n                df.loc[date, col] = float(val)\n        \n    df.reset_index(drop = True, inplace = True)\n        \n    return df\n\n#1. Processing train dataset\n\n# import weather data\ntrain_df = addWeather(train_df)\n\n# import distance data, no header, then attach it to train_df\nmeter_train_df = pd.read_csv('meter_train.csv', header=None)\ntrain_df['meter'] = meter_train_df[0]\n\n# import distance to central data, no header, then attach it to train_df\ndistance_from_central_train_df = pd.read_csv('distance_from_central_train.csv', header=None)\ntrain_df['pickup_distance'] = distance_from_central_train_df[0]\ntrain_df['dropoff_distance'] = distance_from_central_train_df[1]\n\n# compute pickup hour, date, month for each ride\ntrain_df['pickup_datetime'] = pd.to_datetime(train_df['pickup_datetime'])\ntrain_df['pickup_hour'] = train_df.pickup_datetime.dt.hour\ntrain_df['pickup_date'] = train_df.pickup_datetime.dt.day\ntrain_df['pickup_month'] = train_df.pickup_datetime.dt.month\n\n# comput pickup day of the week for each ride from 0 (Monday) to 6 (Sunday) \ntrain_df['day_week'] = train_df.pickup_datetime.dt.weekday\n\n# remove columns that we don't need\ndel train_df['id']\ndel train_df['pickup_datetime']\ndel train_df['dropoff_datetime']\ndel train_df['pickup_longitude']\ndel train_df['pickup_latitude']\ndel train_df['dropoff_longitude']\ndel train_df['dropoff_latitude']\n\n# get the list of column names\nlist(train_df)\n\n# checking our target Y: trip_duration and plot\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.trip_duration.values))\nplt.xlabel('index')\nplt.ylabel('trip duration')\nplt.show()\n\n# remove some unsual long or short trips and plot again\nq1 = train_df.trip_duration.quantile(0.001)\nq2 = train_df.trip_duration.quantile(0.999)\ntrain_df = train_df[(train_df.trip_duration > q1) & (train_df.trip_duration < q2)]\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.trip_duration.values))\nplt.xlabel('index')\nplt.ylabel('trip duration')\nplt.show()\n\n# checking our meter data field and plot\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.meter.values))\nplt.xlabel('index')\nplt.ylabel('meter')\nplt.show()\n\n# remove some trip shorter than 100 meters and longer than 80 km and plot again\ntrain_df = train_df[(train_df.meter > 100) & (train_df.meter < 80000)]\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.meter.values))\nplt.xlabel('index')\nplt.ylabel('meter')\nplt.show()\n\n# count values in other X columns to detect unusual values\ntrain_df['vendor_id'].value_counts()\ntrain_df['passenger_count'].value_counts()\ntrain_df['store_and_fwd_flag'].value_counts()\ntrain_df['pickup_hour'].value_counts()\ntrain_df['pickup_date'].value_counts()\ntrain_df['pickup_month'].value_counts()\ntrain_df['day_week'].value_counts()\n\n# remove trips with 0, 8 or 9 passenger(s) and check again\ntrain_df = train_df[(train_df.passenger_count < 8) & (train_df.passenger_count != 0)]\ntrain_df['passenger_count'].value_counts()\n\n# (optional) move trip_duration to end column\ntrain_df['duration'] = train_df['trip_duration']\ndel train_df['trip_duration']\n\n# split train and valuation\ntrain=train_df.sample(frac=0.8,random_state=200)\nval=train_df.drop(train.index)\n\n# write the processed train and valuation dataset to csv\ntrain.to_csv('train_df.csv', index=False)\nval.to_csv('val_df.csv', index=False)\n\n#2. Processing test dataset\n\n# import weather data\ntest_df = addWeather(test_df)\n\n# import distance data, no header, then attach it to train_df\nmeter_test_df = pd.read_csv('meter_test.csv', header=None)\ntest_df['meter'] = meter_test_df[0]\n\n# import distance to central data, no header, then attach it to train_df\ndistance_from_central_test_df = pd.read_csv('distance_from_central_test.csv', header=None)\ntest_df['pickup_distance'] = distance_from_central_test_df[0]\ntest_df['dropoff_distance'] = distance_from_central_test_df[1]\n\n# get the list of column names\nlist(test_df)\n\n# compute pickup hour, date, month for each ride\ntest_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'])\ntest_df['pickup_hour'] = test_df.pickup_datetime.dt.hour\ntest_df['pickup_date'] = test_df.pickup_datetime.dt.day\ntest_df['pickup_month'] = test_df.pickup_datetime.dt.month\n\n# comput pickup day of the week for each ride from 0 (Monday) to 6 (Sunday) \ntest_df['day_week'] = test_df.pickup_datetime.dt.weekday\n\n# remove columns that we don't need\ndel test_df['id']\ndel test_df['pickup_datetime']\ndel test_df['pickup_longitude']\ndel test_df['pickup_latitude']\ndel test_df['dropoff_longitude']\ndel test_df['dropoff_latitude']\n\n# write the processed test_df to csv\ntest_df.to_csv('test_df.csv', index=False)", "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "a21f7e0279d8dda5c2e78304e6bf7f4535ce92b5", "collapsed": false, "_cell_guid": "0e151dc4-37b8-4a97-a86b-671adfa2f217", "_execution_state": "idle"}, "outputs": [], "source": "*** Model training, evaluating and prediction:\n\nI use a straight-forward Deep Neural Networks Regression model from Tensorflow with 3 hidden layers and hidden units = [10,10,10]. With current 1000 training steps, my RMSLE score is 0.44. I also included some other metrics from package \"sklearn\" for reference purpose.", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e4e74249e96a8b58b078ceb8aa796485be780d59", "collapsed": false, "_cell_guid": "b528cbc3-5689-4f9c-8779-49c6304a8b3d", "_execution_state": "idle", "trusted": false}, "outputs": [], "source": "import pandas as pd\nimport numpy as np\n#from sklearn import metrics\nimport itertools\nimport tensorflow as tf\n\n# (Optional) Extra logging \ntf.logging.set_verbosity(tf.logging.ERROR)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ntf.logging.set_verbosity(tf.logging.INFO)\n\n# Import data\ntrain_df = pd.read_csv('train_df.csv')\nevaluate_df = pd.read_csv('val_df.csv')\ntest_df = pd.read_csv('test_df.csv')\n\n# define Root Mean Squared Logarithmic Error for evaluation\ndef rmsle(real,predicted):\n    sum=0.000\n    length=len(predicted)\n    for x in range(length):\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p - r)**2\n    return (sum/length)**0.5\n        \n\nMODEL_DIR = \"tf_model_full\"\n\ncategorical_features = ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'pickup_hour', 'pickup_date', 'pickup_month', 'day_week']\ncontinuous_features = ['meter', 'pickup_distance', 'dropoff_distance', 'precipitation', 'snow_fall', 'snow_depth']\nLABEL_COLUMN = 'duration'\n\n# convert types of categorical features to string\nfor k in categorical_features:\n    train_df[k] = train_df[k].apply(str)\n    evaluate_df[k] = evaluate_df[k].apply(str)\n    test_df[k] = test_df[k].apply(str)\n\n# Converting Data into Tensors\ndef input_fn(df, training = True):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # the values of that column stored in a constant Tensor.\n    continuous_cols = {k: tf.constant(df[k].values)\n                       for k in continuous_features}\n\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {k: tf.SparseTensor(\n        indices=[[i, 0] for i in range(df[k].size)],\n        values=df[k].values,\n        dense_shape=[df[k].size, 1])\n        for k in categorical_features}\n\n    # Merges the two dictionaries into one.\n    feature_cols = dict(list(continuous_cols.items()) +\n                        list(categorical_cols.items()))\n\n    if training:\n        # Converts the label column into a constant Tensor.\n        label = tf.constant(df[LABEL_COLUMN].values)\n\n        # Returns the feature columns and the label.\n        return feature_cols, label\n    \n    # Returns the feature columns    \n    return feature_cols\n\ndef train_input_fn():\n    return input_fn(train_df)\n\ndef eval_input_fn():\n    return input_fn(evaluate_df)\n\ndef test_input_fn():\n    return input_fn(test_df, False)\n    \n# engineering features\nengineered_features = []\n\nfor continuous_feature in continuous_features:\n    engineered_features.append(\n        tf.contrib.layers.real_valued_column(continuous_feature))\n\n\nfor categorical_feature in categorical_features:\n    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n        categorical_feature, hash_bucket_size=1000)\n\n    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,\n                                                                  combiner=\"sum\"))\n# defining model\nregressor = tf.contrib.learn.DNNRegressor(\n    feature_columns=engineered_features, hidden_units=[10,10,10], model_dir=MODEL_DIR)\n\n# Fit the model\nwrap = regressor.fit(input_fn=train_input_fn, steps=1000)\n    \n# Evaluate model with rmsle metric\nval_df = regressor.predict_scores(input_fn=eval_input_fn)\nval_prediction = list(itertools.islice(val_df,evaluate_df['duration'].size))\nval_prediction_array = np.asfarray(val_prediction)\nval_y_array = np.asfarray(evaluate_df['duration']) \nprint(rmsle(val_y_array, val_prediction_array))\n'''\n# Evaluating Our Model    \nprint('Evaluating ...')\nresults = regressor.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))\n\n# Other evaluation metrics for reference\nprint(metrics.explained_variance_score(val_y_array, val_prediction_array))\nprint(metrics.mean_absolute_error(val_y_array, val_prediction_array))\nprint(metrics.mean_squared_error(val_y_array, val_prediction_array))\nprint(metrics.median_absolute_error(val_y_array, val_prediction_array))\nprint(metrics.r2_score(val_y_array, val_prediction_array))\n    \n# Predict with Our Model\npredicted_output = regressor.predict_scores(input_fn=test_input_fn)\npredictions = list(itertools.islice(predicted_output,test_df['vendor_id'].size))\nprediction_array = np.asfarray(predictions)\n\n# write predictions to csv\nnp.savetxt(\"prediction.csv\", prediction_array, delimiter=\",\")\n'''\n", "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "dda39b117fe5232acfcf43a5d5c758f298ac3d46", "collapsed": false, "_cell_guid": "3a93dfa1-4ed4-4faa-bcb1-8c3be63fe9f8", "_execution_state": "idle"}, "outputs": [], "source": "This still has a lot of room for improvement. So if you have any idea or would like to join my team to work on Deep Neural Networks, feel free to let me know.", "cell_type": "markdown", "execution_count": null}], "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4}