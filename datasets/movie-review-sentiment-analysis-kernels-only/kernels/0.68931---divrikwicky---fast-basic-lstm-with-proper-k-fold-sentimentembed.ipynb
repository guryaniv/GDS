{"cells":[{"metadata":{"_uuid":"59802896f3564cd41b68fac1a1f2eac7db92a37c"},"cell_type":"markdown","source":"**Fast and Basic Solution to Movie Review Sentiment Analysis using LSTM\n**\n\nI have used some of my previous code from Quora Duplicate Question Competition. https://github.com/aerdem4/kaggle-quora-dup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport textblob\n\nprint(\"Loading data...\")\ntrain = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\nprint(\"Train shape:\", train.shape)\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\nprint(\"Test shape:\", test.shape)\n\nenc = OneHotEncoder(sparse=False)\nenc.fit(train[\"Sentiment\"].values.reshape(-1, 1))\nprint(\"Number of classes:\", enc.n_values_[0])\n\nprint(\"Class distribution:\\n{}\".format(train[\"Sentiment\"].value_counts()/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db45eb314d6840613f1646cc1267093da740b436"},"cell_type":"markdown","source":"For the examples which occur in both sets, we can directly use the labels from train set as our prediction."},{"metadata":{"trusted":true,"_uuid":"0c15a8123acb6cc1d1967e47fb15d7c4333b9370","collapsed":true},"cell_type":"code","source":"print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(set(train[\"Phrase\"]).intersection(set(test[\"Phrase\"])))/test.shape[0]))\ntest = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f23154aa33cb61e925d0c9fb557915ceafc226c5"},"cell_type":"markdown","source":"Let's see if all the words in the test set occurs in the train set:"},{"metadata":{"trusted":true,"_uuid":"04bea1b921fb53f77ee8b069d864d2c103fec0ff","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv1 = CountVectorizer()\ncv1.fit(train[\"Phrase\"])\n\ncv2 = CountVectorizer()\ncv2.fit(test[\"Phrase\"])\n\nprint(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\nprint(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\nprint(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d0cefa184547a5e02f5f389216e1a496f90e982"},"cell_type":"markdown","source":"**Numerical Feature Extraction**"},{"metadata":{"trusted":true,"_uuid":"2b4043b8831b8a42bd946f762375a3a97516dfc2","collapsed":true},"cell_type":"code","source":"def transform(df):\n    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n    df[\"sentence_start\"] = df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n    return df\n\ntrain = transform(train)\ntest = transform(test)\n\ndense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\"]\n\ntrain.groupby(\"Sentiment\")[dense_features].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8173232d306f83f14b5a03a7940caa89d2434f17"},"cell_type":"markdown","source":"**Splitting Data into folds**\n\nIf we split the data totally random, we may bias our validation set because the phrases in the same sentence may be distributed to train and validation sets. We need to guarantee that all phrases of one sentence is in one fold. We can assume that SentenceId%NUM_FOLDS preserves this while splitting the data randomly."},{"metadata":{"trusted":true,"_uuid":"bfaeff0ff308a70c2b254c1c6d90a97491f567c4","collapsed":true},"cell_type":"code","source":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"572a4bda2d8b1cd035f9c6fffa403e7f1b0fbbe4"},"cell_type":"markdown","source":"**Transfer Learning Using GLOVE Embeddings**"},{"metadata":{"trusted":true,"_uuid":"3e18956c6b5c5c88e7097c41138a2afb3f4bc1e1","collapsed":true},"cell_type":"code","source":"EMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\nEMBEDDING_DIM = 100\n\nall_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n\ndef get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()\nprint(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f93b76d710ea94ed48627babc49bc5261647789"},"cell_type":"markdown","source":"**Prepare the sequences for LSTM**"},{"metadata":{"trusted":true,"_uuid":"5d2f6c292c46b367b0ae7f98f8a1fbafc163a0e0","collapsed":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    sent = textblob.TextBlob(word).sentiment\n    if embedding_vector is not None:\n        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n    else:\n        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7f3d8de4369b676ffab460533ace55ed08a10ab"},"cell_type":"markdown","source":"**Define the Model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"878174c7d497cf794531afbcc8d81b5f59c803dd"},"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\ndef build_model():\n    embedding_layer = Embedding(nb_words,\n                                EMBEDDING_DIM + 2,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    dropout = SpatialDropout1D(0.2)\n    mask_layer = Masking()\n    lstm_layer = LSTM(50)\n    \n    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n    dense_input = Input(shape=(len(dense_features),))\n    \n    dense_vector = BatchNormalization()(dense_input)\n    \n    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n    \n    feature_vector = concatenate([phrase_vector, dense_vector])\n    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n    \n    output = Dense(5, activation=\"softmax\")(feature_vector)\n    \n    model = Model(inputs=[seq_input, dense_input], outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6879dab4a2dc2daa0a63ab689a7a579a4c4baa5e"},"cell_type":"markdown","source":"**Train the Model:**"},{"metadata":{"trusted":true,"_uuid":"0d4cdf9226840afc92b25b0ca75b4ff520dabf7b","scrolled":false,"collapsed":true},"cell_type":"code","source":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n    print()\n    \ntest_preds /= NUM_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfc473082e4f349ceb59a6df2e96f173f762195c"},"cell_type":"markdown","source":"**Making submission...**"},{"metadata":{"trusted":true,"_uuid":"39ed4370163acc2daa2fa48a43549c84166bdcb7","collapsed":true},"cell_type":"code","source":"print(\"Select the class with the highest probability as prediction...\")\ntest[\"pred\"] = test_preds.argmax(axis=1)\n\nprint(\"Use these predictions for the phrases which don't exist in train set...\")\ntest.loc[test[\"Sentiment\"].isnull(), \"Sentiment\"] = test.loc[test[\"Sentiment\"].isnull(), \"pred\"]\n\nprint(\"Make the submission ready...\")\ntest[\"Sentiment\"] = test[\"Sentiment\"].astype(int)\ntest[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"51b72d89f24f76c696d3d11523f023a9c3ed4ae3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}