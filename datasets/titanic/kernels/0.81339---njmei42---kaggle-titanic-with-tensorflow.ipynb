{"cells":[{"metadata":{"_uuid":"683a495ed02f010ee04c908a9b65163dc6213999","_cell_guid":"5110b336-7fee-4ac8-840c-9733362c5cf2"},"cell_type":"markdown","source":"# Disclaimer:\nFor the Titanic dataset, the best performance will likely be achieved by a non-ANN model. But as a student interested in applying artificial neural networks, I thought it'd be a fun/educational challenge! I'm still very very new to machine learning and neural networks so feedback is much appreciated!"},{"metadata":{"_uuid":"1acb6d833521394738055c504c79c6fd676592d4","_cell_guid":"168ed2be-9c48-4627-be72-19cc7f38ddb1"},"cell_type":"markdown","source":"# 0) Libraries"},{"metadata":{"_uuid":"ca5b782a1f1724961b7c03f336cb6eaa77416f03","_cell_guid":"a3ff0c25-b53a-4b6e-822f-66b11cba5023","trusted":true},"cell_type":"code","source":"from collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Scikit-learn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"4ca6f580a7007fc66b78819642477071fadbb2ed","_cell_guid":"34c8a0e1-2b1f-4a06-a6d7-ba4fb255b7c2"},"cell_type":"markdown","source":"# 1) Loading in and checking data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Okay, let's load in our datasets!\nraw_train_df = pd.read_csv(\"/kaggle/input/train.csv\")\nraw_test_df = pd.read_csv(\"/kaggle/input/test.csv\")\nexample_submission_df = pd.read_csv(\"/kaggle/input/gender_submission.csv\")\n\ntrain_df = raw_train_df.copy(deep=True)\ntest_df = raw_test_df.copy(deep=True)\ntrain_test_lst = [train_df, test_df]","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"3bf3ebeb33855b1b1eaed1a5e9767784c3aa5804","_cell_guid":"fdb8d563-e9e9-4fc0-a005-4a09e930db8b"},"cell_type":"markdown","source":"### First, Let's take a look at the train and test data to make sure everything was loaded okay"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Taking a look at the first few values in the dataframe\ndisplay(train_df.head())\n# Taking a look at the summary statistics for each feature\ndisplay(train_df.describe())","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"aa019690fce8d137c74268edd04e1332ff0f5ae6","_cell_guid":"02704733-8343-4e4c-8334-99e95f2f79a6"},"cell_type":"markdown","source":"The minimum fare of 0.00 stuck out to me as a little odd so I decided to take a little deeper look.\nSeems like maybe crew members or staff working on the Titanic? That said, there is a Jonkheer in the list too so there might have been some free tickets involved for special personages..."},{"metadata":{"_uuid":"18d000cde44e0c61b7cd00c7a9b60dea44712ca7","_cell_guid":"13b649e6-b944-4cbb-b981-a539ca49f9cb","trusted":true},"cell_type":"code","source":"train_df[train_df['Fare'] == 0]","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"97a4a39bde68baa8d3e3ed61378e799784cfa8f4","_cell_guid":"be8ec174-7444-478f-bd58-85eabeeea407","trusted":true},"cell_type":"code","source":"display(test_df.head())\ndisplay(test_df.describe())","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"beddf0ab0b4752b8076a3f820a71bcdfd40d2b70","_cell_guid":"9eff2868-c9c6-4a93-97f7-f45611209f56"},"cell_type":"markdown","source":"# 2) Data preprocessing"},{"metadata":{"_uuid":"d8b211bb26590a8e4e41bcdcb2689dec6516e05a","_cell_guid":"94be94ec-1cdc-4c82-a76d-7873194bcd4c"},"cell_type":"markdown","source":"### Both train and test datasets appear to have NaN values this could cause problems for our model, so let's look at what is missing and how much"},{"metadata":{"_uuid":"52b25321d1d86f979f536a40f018b86a5009293b","_cell_guid":"28ef30d0-441b-4f4c-8500-88ef3f7d6324","trusted":true},"cell_type":"code","source":"display(train_df.isnull().sum())\nprint(\"Total individuals in train set is: {}\".format(len(train_df)))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"a2b7f6db26558adcb114011cd54dd73bc30e34e4","_cell_guid":"a9b3b228-5dc4-4ca1-8db0-5bf137f02882","trusted":true},"cell_type":"code","source":"display(test_df.isnull().sum())\nprint(\"Total individuals in test set is: {}\".format(len(test_df)))","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"704fbbfb89d29e5f34f8d7929e9d778b6f0075f4","_cell_guid":"d5d41e4f-ab62-478e-8964-8c95d3880afa"},"cell_type":"markdown","source":"### The huge amount of missing Cabin data is worrying, but let's see if it has any predictive power before figuring out what to do"},{"metadata":{"_uuid":"9234e2cd553001288dfc63de21741c8322bc0f79","_cell_guid":"eb43abe3-9294-4533-b81d-c1ce2f976923","trusted":true},"cell_type":"code","source":"# Let's only consider data that has non-NaN Cabin values (Age or Embarked can still be NaN!)\ncabin_df = train_df[train_df['Cabin'].notnull()]\n\n# Let's create a new feature 'deck_level' that groups passengers by deck levels\ncabin_df = cabin_df.assign(deck_level=pd.Series([entry[:1] for entry in cabin_df['Cabin']]).values)\ndisplay(cabin_df.head())\n\nprint(\"Survival chances based on deck level:\")\ncabin_df.groupby(['deck_level'])['Survived'].mean()\n","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"2a514ed076f9e0d7f0f83ad9b7164be5ddd6a544","collapsed":true,"_cell_guid":"2d7d4f35-da01-49e0-a61e-3f542325ea54"},"cell_type":"markdown","source":"So it looks like deck level may be a useful feature to learn. The NaNs are troubling though, we can get around them (hopefully) by adding a new option for the deck_level to be 'U' (for unknown).\n\nLater, we'll use one hot encoding on deck_level before sending it to our neural network."},{"metadata":{"_uuid":"6809667c833cc9891a1029b62b759b8a881d5c9b","_cell_guid":"66d049f9-fd0e-4475-8f67-b1757f7514de","trusted":true},"cell_type":"code","source":"def process_deck_level(train_test_lst):\n    new = []\n    for dataset in train_test_lst:\n        dataset = dataset.copy(deep=True)\n        # Take the first letter of the Cabin entry if it's not nan. Otherwise, it should be labelled as 'U'.\n        dataset = dataset.assign(deck_level=pd.Series([entry[:1] if not pd.isnull(entry) else 'U' for entry in dataset['Cabin']]))\n        # Okay, now let's drop the Cabin column from our dataset\n        dataset = dataset.drop(['Cabin'], axis = 1)\n        new.append(dataset)\n    return (new)\n\ntrain_df, test_df = process_deck_level(train_test_lst)\n\n# Let's check that we did the right thing...\ndisplay(train_df.head())\ndisplay(test_df.head())\n# Let's also recheck what's still missing\ndisplay(train_df.isnull().sum())\ndisplay(test_df.isnull().sum())","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"9c8407acaadae17713ca2be69d69fe10e478acba","_cell_guid":"0a69337c-cd89-42a7-82bc-06ca9d3557b8"},"cell_type":"markdown","source":"Okay looking better already! \n### Now let's try to address the missing embarked data! First off what are the possible values of embarked?"},{"metadata":{"_uuid":"b038008cb32757336edd88c7878022729b9bb57a","_cell_guid":"e9c5912e-7316-4107-af2e-e37568c8c227","trusted":true},"cell_type":"code","source":"display(set(train_df['Embarked']))\nprint(\"Survival chances based on embarcation:\")\ntrain_df.groupby(['Embarked'])['Survived'].mean()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"860cd2d6b85aab8698baf950dd861aed3c67ec7b","_cell_guid":"23747cf8-4725-460a-bfa6-76b67e876db0"},"cell_type":"markdown","source":"It looks like people who embarked from Q had a low survival rate and S had an especially low survival rate...\n\nFor this feature, we'll also fill NaN values with 'N' for 'Not known' since filling with C/Q/S looks like it would make a big difference."},{"metadata":{"_uuid":"6f0d9d39e7d8fb1ebb413617ba2a4c1f89f76d10","_cell_guid":"ccd43ffb-0b2e-4825-9d9b-ff85632ce8fc","trusted":true},"cell_type":"code","source":"# Replace NaN values in the 'Embarked' column with 'N'\ntrain_df[['Embarked']] = train_df[['Embarked']].fillna('N')\n# Let's check that we filled things correctly!\ndisplay(set(train_df['Embarked']))\ndisplay(train_df.isnull().sum())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"7594f5ea64c08cf2f822344fa729c99212686452","_cell_guid":"c201a6bc-7841-4d30-8c42-b94082b84ba4"},"cell_type":"markdown","source":"### Let's take a quick look at the test data and see what to do about the one fare datapoint that is missing"},{"metadata":{"_uuid":"e483c0720342672d0cd426c697f434bd0ee25d82","_cell_guid":"5cb23e56-db0e-43c1-a860-3a90fc0767fe","trusted":true},"cell_type":"code","source":"test_df[test_df['Fare'].isnull()]","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"2a5411674a8eb7627242eb314f1af508eb8197cf","_cell_guid":"34ae276a-1fc6-4329-90bd-c4e26ad6d9c1"},"cell_type":"markdown","source":"### Let's use Pclass to fill our missing value!"},{"metadata":{"_uuid":"1f0c619aac8fe8fdfd76bbad529d107f0f2419c2","_cell_guid":"11104976-7318-43cc-a8f0-7f8f7e11acca","trusted":true},"cell_type":"code","source":"Pclass_Fare_grouping = test_df.groupby([\"Pclass\"])['Fare']\nPclass_Fare_grouping.plot(kind='hist', bins=15, legend=True, title='Fare histogram grouped by Pclass')\nplt.xlabel('Fare')\nprint(\"Mean Fare for each Pclass:\")\ndisplay(Pclass_Fare_grouping.mean())\nprint(\"Median Fare for each Pclass:\")\ndisplay(Pclass_Fare_grouping.median())","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"66e280ee2d0d97f0261aa595536017a7a8e4f708","_cell_guid":"918a5990-8221-4098-96dd-0b330b24f1a6"},"cell_type":"markdown","source":"The tail for the Fare for Pclass 3 is a bit long so it's probably safer to fill with the median value for that Pclass.\n\nAll this work for one missing fare is overkill, but it's a good exercise in thinking about how to impute data!"},{"metadata":{"_uuid":"c4a4ad9a07da915927803f6f4c2c2ae421b3c7d8","_cell_guid":"ed79eccf-7260-4515-b458-87e0d6e1e1de","trusted":true},"cell_type":"code","source":"test_df[['Fare']] = test_df[['Fare']].fillna(Pclass_Fare_grouping.median()[3])\n# Let's check that our one fill worked!\ndisplay(test_df[test_df['PassengerId'] == 1044])\ndisplay(test_df.isnull().sum())","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"2402b0e25729397cb13c95d4a96c00157753e9fb","_cell_guid":"119247fb-2a6d-4ee3-94b3-aaa8726a5fd4"},"cell_type":"markdown","source":"### Now to figure out what to do about missing age data. Let's do a quick analysis of age before imputing any values!"},{"metadata":{"_uuid":"f38a712ffe126966efda5f28886873b1d5a30701","_cell_guid":"f1c763be-ad00-4136-9772-2c30ae841cc3"},"cell_type":"markdown","source":"### Let's first just take a look at the age distribution in our training set."},{"metadata":{"_uuid":"4185d967ab85f11b963df71ea1fec40077fb797e","_cell_guid":"7f12e7e8-3a78-4222-b356-c1bcb623fa75","trusted":true},"cell_type":"code","source":"ax = train_df[['Age']].plot(kind='hist', bins=20)\nplt.xlabel(\"Age\")\n_ = plt.title(\"Age histogram\")","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"12794085c4d77a4c9a0ff1623f8dcc1caddbf84b","_cell_guid":"42109037-eecd-4c67-8652-ae63202b10ee"},"cell_type":"markdown","source":"### Next, let's look at the relationship between Age and survival"},{"metadata":{"_uuid":"ad0a68381302dfc86f5b2ae8b219676312bdccfb","_cell_guid":"466affab-9e79-4f0d-aeae-c6b7d5cab7f9","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', pd.cut(train_df['Age'], np.arange(0, 100, 5))]).size().unstack(0).plot.bar(stacked=True, alpha=0.75)\n_ = plt.title(\"Age histogram grouped by survival\")","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"d8764cbf1092cc946598c94353cc90016515272c","_cell_guid":"364bf38a-1a24-4db8-ae79-9900939018af"},"cell_type":"markdown","source":"* So an initial analysis shows that younger passengers ( < 6) were much more likely to survive than not.\n\n* Agewise, the worst outcomes were for folks in their late teens and early 20's. \n\n* Bad outcomes also for people between age ~24 and ~32 as well."},{"metadata":{"_uuid":"4ed7da5d44bd97082dc1499ff6aad7b50615e15e","_cell_guid":"5b8301ec-60fc-4939-8489-019966e46636"},"cell_type":"markdown","source":"### What about the effect of gender and age on survival?"},{"metadata":{"_uuid":"85cd7bb57d9b3d3ea817cdc2720457a660c783d8","_cell_guid":"b91e5a64-517a-4bcb-9909-b8bbd371b0cb","trusted":true},"cell_type":"code","source":"train_df.groupby(['Survived', 'Sex', pd.cut(train_df['Age'], np.arange(0, 100, 5))]).size().unstack(0).plot.bar(stacked=True, alpha=0.75)\n_ = plt.title(\"Age histogram grouped by survival and gender\")","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"b297ee0da51552630c02db017694743558172e97","collapsed":true,"_cell_guid":"5ee7a0e0-03f4-455b-b647-f268a62e0726"},"cell_type":"markdown","source":"* This plot is a bit messy, but the left side shows pretty clearly that females had a high survival rate.\n\n* Looking at right side paints the opposite picture (with the exception if you were a male under 6, then youre survival chances were pretty good).\n\n### This quick set of observations seem to suggest that getting age right for young children and those between 20 and 30 is important for predicting survival."},{"metadata":{"_uuid":"1ec6981002eba896200b179d6249d2e1377da44a","_cell_guid":"2ec5a861-7008-439c-b47b-28a58ec6c3a2"},"cell_type":"markdown","source":"### One promising strategy to impute age that seems to work well in other kernels is to use the name title"},{"metadata":{"_uuid":"4eaf32475c776e01e04154ea214427f87961bb3a","_cell_guid":"70b995c5-5632-45a6-a51d-0da21d2361c4","trusted":true},"cell_type":"code","source":"# All name formats seem to be something like:\n# \"last_name, title. first_name \"nickname\" (full_name)\"\n# To get title, we split the string by comma and select the second half. Then we split that second half by '.' and take the first half\n# i.e.\n# 1) [\"last_name\", \"title. first_name \"nickname\" (full_name)\"] (select element 1!)\n# 2) [\"title\", \"first_name \"nickname\" (full_name)\"] (select element 0!)\ntrain_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in train_df['Name']]\n# Let's see if the above strategy works\nprint(\"Train set titles (and counts):\")\nprint(Counter(train_titles))\n\nprint(\"\\nTest set titles (and counts):\")\ntest_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in test_df['Name']]\nprint(Counter(test_titles))\n\nprint(\"\\n===============================\")\n\nage_missing_train_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in train_df[train_df['Age'].isnull()]['Name']]\nprint(\"\\nTrain set titles (and counts) with missing ages:\")\nprint(Counter(age_missing_train_titles))\n\nage_missing_test_titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in test_df[test_df['Age'].isnull()]['Name']]\nprint(\"\\nTest set titles (and counts) with missing ages:\")\nprint(Counter(age_missing_test_titles))","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"b4d9a62a78f5edcba222181c8cdf1fe5771d2fec","_cell_guid":"97f9ab06-c773-436a-aa97-557403403059"},"cell_type":"markdown","source":"### Looks like we have a nice list of titles, let's add them to our dataframe for now"},{"metadata":{"_uuid":"d459369ccf095c0483ecac18d220c1e921a39dc0","_cell_guid":"3a17c6ef-bec9-425b-8b89-26f31d0674b4","trusted":true},"cell_type":"code","source":"# Let's add the titles as a new feature for our dataset\ndef naive_process_title(train_test_lst):\n    new = []\n    for dataset in train_test_lst:\n        dataset = dataset.copy(deep=True)\n        titles = [name.split(',')[1].lstrip(' ').split('.')[0] for name in dataset['Name']]\n        dataset = dataset.assign(title=pd.Series(titles).values)\n        new.append(dataset)\n    return (new)\n\ntrain_df, test_df = naive_process_title([train_df, test_df])\n\n# Taking a look at our dataframes to make sure we did the right thing...\ndisplay(train_df.head())\ndisplay(test_df.head())","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"d1b3cfa4fb0fe2bb142d3b0bf44e74dccb69e9ae","_cell_guid":"1c41dc87-cd02-4d8f-9ea6-8436c2f267de"},"cell_type":"markdown","source":"### I'm not super well versed with titles from \"back in the day\" so let's see if we can discover how age (the thing we want to impute) relates to title"},{"metadata":{"_uuid":"09e77725d0e3f9a2ab5a250aa69e846b0336d256","collapsed":true,"_cell_guid":"9106df67-d258-478d-aa08-9ca30b98574e","trusted":true},"cell_type":"code","source":"def plot_title_age_hist(title, train_df, bins=20):\n    title_ages = train_df[train_df['title'] == title]['Age']\n    title_ages.plot(kind='hist', bins=bins, legend=True)\n    title_ages.describe()\n    plt.xlabel(\"Age\")\n    plt.title(\"Age histogram for '{}' title\".format(title))","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"1ae1b1228c174c869325e604b725d6cfced204dd","_cell_guid":"b4e21465-b79d-4cff-822a-ee9d3ef76820","trusted":true},"cell_type":"code","source":"title_groups = train_df.groupby(['title'])\ndisplay(title_groups['Age'].describe())\nplot_title_age_hist(\"Master\", train_df, bins=10)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"17c9978d53057cd3cafe7383a3060848a913abe3","_cell_guid":"2c4d39f3-ebca-43b7-b862-7d047c70152b"},"cell_type":"markdown","source":"* It looks like 'Master' is a reliable signal for young boy."},{"metadata":{"_uuid":"5da28e7be84a3b2a6492f587a779ddae725342de","_cell_guid":"1e28f255-2bd7-4c64-8225-7a4dde20a23e","trusted":true},"cell_type":"code","source":"plot_title_age_hist('Miss', train_df)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"57fec7fc70c1445e89fa03ada671646e7130c083","_cell_guid":"e15c31e4-b68c-4599-8025-d4785a6038b6"},"cell_type":"markdown","source":"* Miss looks like the corresponding title, but it can take on a much much larger variation of values..."},{"metadata":{"_uuid":"2935a42043a1026bad969717c1739b47e8259bd7","_cell_guid":"462363dd-73a8-4004-b474-0658993493fc"},"cell_type":"markdown","source":"### Let's see if we can get more specific ages to impute for the \"miss\" title by using the 'Parch' feature"},{"metadata":{"_uuid":"5a1bfa17122282705d3f003cb3fd68026b3ae007","_cell_guid":"e0a90256-0012-46e4-a5e7-ad56be0caa44","trusted":true},"cell_type":"code","source":"def title_feature_age_analysis(title, feature, train_df, bins):\n    # Let's loop through all values of our feature of interest (in this case \"Parch\")\n    for i in range(max(train_df[train_df['title'] == title][feature]) + 1):\n        # Create an age histogram for a given feature level that also has our title of interest\n        train_df[(train_df['title'] == title) & (train_df[feature] == i)]['Age'].plot(kind=\"hist\", bins=bins, legend = True, label=\"{} {}\".format(feature, i), alpha = 0.5)\n        # Print common descriptive stats for our title and the given level of our feature\n        print(\"Statistics for '{}' title with {} of: {}\".format(title, feature, i))\n        display(train_df[(train_df['title'] == title) & (train_df[feature] == i)]['Age'].describe())\n        print(\"Median\\t{}\\n\".format(train_df[(train_df['title'] == title) & (train_df[feature] == i)]['Age'].median()))\n        print(\"=========================\\n\")\n        plt.xlabel(\"Age\")\n        _ = plt.title(\"Age histogram for '{}' title grouped by {}\".format(title, feature))\n\ntitle_feature_age_analysis('Miss', 'Parch', train_df, bins=20)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"baadd65ce4f2447d216c82b32c2ca07da0262c2a","_cell_guid":"f7e73ac1-baa7-408f-a75e-79f115e8e69f"},"cell_type":"markdown","source":"### Cool! A parch of 1 or 2 together with the 'Miss' title seems to be quite indicative of younger age! Does our finding in the train dataset hold up in the test dataset?"},{"metadata":{"_uuid":"ff0d3bd079c941040297d8a135527e10cfecc8d0","_cell_guid":"bb53b27b-7078-41e8-9fa7-52d7a811db12","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Miss', 'Parch', test_df, bins=20)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"bbde2194f4648fcece9452ef5d4b5021992172d5","_cell_guid":"f422d24b-7967-414e-a60b-3c014433a04f"},"cell_type":"markdown","source":"### Besides 'Miss' and 'Master' we'll also have to fill many more missing ages with 'Mr' and 'Mrs'. Let's see if we can use Parch to help us out again!"},{"metadata":{"_uuid":"255fc0aa9007a83baf57476c7a44204e8a4a08a2","_cell_guid":"a10381f4-2160-4fe5-8430-3ba292b2a43b","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Mrs', \"Parch\", train_df, bins=20)","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"d487a546fe530f724d8bdad98796e313ae4aa641","_cell_guid":"076c97f4-5005-4ae5-b008-6ac7ace1f547","trusted":true},"cell_type":"code","source":"title_feature_age_analysis('Mr', \"Parch\", train_df, bins=20)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"c42f9e64b672141a7bbc505700ec7bd82030a6a3","_cell_guid":"75e048b9-1ab4-41b3-995a-04611659b871"},"cell_type":"markdown","source":"So it looks like \"Parch\" is not super helpful for narrowing the age of 'Mr' and 'Mrs' titles. Let's try using the median to fill these titles then..."},{"metadata":{"_uuid":"81e104df60e30755827bdb2aff6a6f96c47144b7","_cell_guid":"f461ac10-29fd-48d5-ba6a-4c58709ff968"},"cell_type":"markdown","source":"### We've taken a bit of a look at titles and their relation to age, time to fill in our missings age values with the above information"},{"metadata":{"_uuid":"587d84ec52dc590f7bd21dd060873ae92676e4f0","_cell_guid":"9035b261-af70-4851-85c0-a3fefb743bb7","trusted":true},"cell_type":"code","source":"def age_imputer(train_test_lst):\n    new = []\n    for dataset in train_test_lst:\n        dataset = dataset.copy(deep=True)\n        # This is the list of unique titles for individuals with a NaN age\n        missing_age_titles = list(set([name.split(',')[1].lstrip(' ').split('.')[0] for name in dataset[dataset['Age'].isnull()]['Name']]))\n        print(\"Titles for individuals with missing age are: {}\".format(missing_age_titles))\n        for title in missing_age_titles:\n            # Fill in missing ages for 'Mr'/'Mrs'/'Master'/'Ms'/'Dr' titles\n            if (title in ['Mr', 'Mrs', 'Master', 'Ms', 'Dr']):\n                median = dataset[(dataset['title'] == title)]['Age'].median()\n                # Treat 'Ms' as 'Mrs'\n                if (title == 'Ms'):\n                    median = dataset[(dataset['title'] == 'Mrs')]['Age'].median()\n                dataset[(dataset['title'] == title) & (dataset['Age'].isnull())] = dataset[(dataset['title'] == title) & (dataset['Age'].isnull())].fillna(median)\n            # Fill in missing ages for \"Miss\" titles\n            elif (title == 'Miss'):\n                for level in range(max(dataset[dataset['title'] == title]['Parch']) + 1):\n                    df = dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)]\n                    if (not df.empty):\n                        median = dataset[(dataset['title'] == title) & (dataset['Parch'] == level)]['Age'].median()\n                        dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)] = dataset[(dataset['title'] == 'Miss') & (dataset['Age'].isnull()) & (dataset['Parch'] == level)].fillna(median)\n        new.append(dataset)\n    return (new)\n\ntrain_df, test_df = age_imputer([train_df, test_df])","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"2b98ba263d29d425ec77c2cb94b8ffe65e2448a9","_cell_guid":"3e2ebc27-ff29-4eda-848a-776577adf6b9","trusted":true},"cell_type":"code","source":"display(train_df.isnull().sum())\ndisplay(test_df.isnull().sum())","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"999b939ac203ce534a85e2e73bafe7fb5775cbd8","_cell_guid":"4c2cb38f-ccbe-44d5-9d35-72e5574fc2c4"},"cell_type":"markdown","source":"### Looks like all the NaNs got filled in. But we should do some sanity checks to verify that things got filled in **correctly**."},{"metadata":{"_uuid":"877ed7c8c94e751db888b4f1e057eb93af2a801c","_cell_guid":"f3b3be94-3292-466d-9808-0f72774c20a5","trusted":true},"cell_type":"code","source":"display(raw_train_df[raw_train_df['Age'].isnull()])","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"517601b5b7f6b7384cd6ec2ed0b9bd83b6e0ef40","_cell_guid":"9059d4e4-451e-499f-87c7-4d2e782c0eb6","trusted":true},"cell_type":"code","source":"# Select passengers that have NaN ages in our raw_train_df\ntrain_df.loc[train_df['PassengerId'].isin(raw_train_df[raw_train_df['Age'].isnull()]['PassengerId'])]","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"568d7cb8f21259a18b9e4abc4296a36cac63cdce","_cell_guid":"07b89b8e-c850-43fe-8c48-706a2372eb0d"},"cell_type":"markdown","source":"### How does our new distribution look?"},{"metadata":{"_uuid":"ffe242d8a583304a0e6e65e189edf45829ad6852","_cell_guid":"bbd97e9f-8e7f-4901-b053-3a402d63e811","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\n# First column plot\ntrain_df[['Age']].plot(kind='hist', bins=20, ax=ax1, legend=False)\nax1.set_xlabel(\"Age\")\nax1.set_title(\"NaN filled\")\nymin, ymax = ax1.get_ylim()\n# Second column plot\nraw_train_df[['Age']].plot(kind='hist', bins=20, ax=ax2, sharey=True, legend=False)\nax2.set_ylim(ymin, ymax)\nax2.set_xlabel(\"Age\")\n_ = ax2.set_title(\"Original distribution\")","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"9cb5075555ab34098ed704a968833b51a684245d","_cell_guid":"3af17f6d-2b11-4451-b2fd-12590ac30716"},"cell_type":"markdown","source":"Looks like things got filled in properly. We now have a massive peak at 30 and 35 years of age due to the large number of fills we made (for 'Mr' and 'Mrs' titles) but the rest of the distribution looks to be preserved..."},{"metadata":{"_uuid":"8b58ac1f09802a8ae72fc6275f626ddcb71a1f09","_cell_guid":"a5b85cca-e4b7-46c9-b165-836dd2a0fc3e"},"cell_type":"markdown","source":"# 3) Some feature engineering"},{"metadata":{"_uuid":"658caf12f32624face05f060d4bb80ed3c020f19","_cell_guid":"bccddb14-2f0e-49a3-a374-fffbfe3d57fd"},"cell_type":"markdown","source":"While going through the data, I happened to stumble on the unfortunate 'Sage' family which had a 0 survival rate despite having women and children (which normally have a high survival rate)."},{"metadata":{"_uuid":"dbe89b420535a30b209aedcbc76c9d345b262771","_cell_guid":"59e66b14-ee7b-4846-be77-b1556a409527","trusted":true},"cell_type":"code","source":"raw_train_df[raw_train_df['Name'].str.startswith('Sage,')]","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"a0aae01ccb0c8a2ca59267416c60bd669fd839c2","_cell_guid":"021c6973-7eb2-48c2-93a5-56217df41eeb"},"cell_type":"markdown","source":"Maybe, family size and/or the Pclass also played a big role in survivorship. Let's engineer some features and explore!"},{"metadata":{"_uuid":"498fa55b51c67b982bc7e5eb0e84f630a077a101","_cell_guid":"dc9b787a-79c6-48d2-ab86-4303d328a4f1","trusted":true},"cell_type":"code","source":"# Let's add family_size which is just the sum of 'SibSp' and 'Parch'\ntrain_df['family_size'] = train_df['SibSp'] + train_df['Parch']\ntest_df['family_size'] = test_df['SibSp'] + test_df['Parch']\n# Check that things were added properly\ndisplay(train_df.head())\n# Plot family size grouped by survival\ntrain_df.groupby(['Survived'])['family_size'].plot(kind='hist', legend=True, alpha=0.5)\nplt.xlabel(\"Family Size\")\n_ = plt.title(\"Histogram of family size grouped by survival\")","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"4e3ee1a4a9b6a5a20a3fd673c4174381eea05f47","_cell_guid":"bdce72b1-5370-42b0-94c8-1a381365f73d"},"cell_type":"markdown","source":"So yeah... large family is definitely not good for survival..."},{"metadata":{"_uuid":"8fb8b1660bfb28c450854830fe27cf7db291f0a4","_cell_guid":"9f59c397-aaa9-4924-b411-d973fb9832e4","trusted":true},"cell_type":"code","source":"_ =train_df.groupby(['Survived', pd.cut(train_df['Pclass'], np.arange(0, 4))]).size().unstack(0).plot.bar(stacked=True)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"e430eb1456c854c84ac861aa9e66c6e017fa321d","_cell_guid":"f90ae86b-163e-4daf-afc7-d841bcab83d1"},"cell_type":"markdown","source":"Being in Pclass gives much higher chance of survival but let's drill in more and look at gender too"},{"metadata":{"_uuid":"7c417368c155f83bd14ae456f61e30c16147410a","_cell_guid":"d2baff85-d6fa-41ca-a5c8-fefb6472faef","trusted":true},"cell_type":"code","source":"_ = train_df.groupby(['Survived', 'Sex', pd.cut(train_df['Pclass'], np.arange(0, 4))]).size().unstack(0).plot.bar(stacked=True)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"17c177069e1d087182ab31d018ede13713ee8d6c","_cell_guid":"d036951e-7213-4c0f-849a-833fec77ca54"},"cell_type":"markdown","source":"Looks like if you were a female in class 1 or 2 your chances were pretty great. Pclass 3 females had more of a 50/50 chance.\n\nAs a male things look much more grim. But Pclass 1 and 2 males still fare better than those in 3.\n\nSince I'm using tensorflow, we can create the 'Pclass' and 'Sex' feature cross in the data pipeline (below)."},{"metadata":{"_uuid":"c41b6c1300a9a9b1c1c193f1de393361a7424ed4","_cell_guid":"57fee955-f887-4500-8ad5-22c6f282bdcd"},"cell_type":"markdown","source":"# 4) Assembling pipeline for tensorflow"},{"metadata":{"_uuid":"6894dd731c77a4bf4dbdf4ac03bab7d42f6f5f48","collapsed":true,"_cell_guid":"3b6240cc-b5da-4dd0-884b-2a70b91d9e6b","trusted":true},"cell_type":"code","source":"# To get things to work nicely with tensorflow we'll need to subtract one from 'Pclass' so our classes start at 0\ntrain_df['Pclass'] = train_df['Pclass'] - 1\ntest_df['Pclass'] = test_df['Pclass'] - 1","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"866f278f77e202b5e1749a957745189876668677","_cell_guid":"603e0553-aa56-42c6-a307-e0f1d8e24c7c","trusted":true},"cell_type":"code","source":"train_df","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"e22a2f2ce139ad5fa574bb02edc82917410b050e","_cell_guid":"1b922d83-ce21-4e09-8c3b-05ee655caed7","trusted":true},"cell_type":"code","source":"# Let's remind ourselves of the data columns we have\ntrain_df.columns","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"b6102ca2584a4e170c76661c7840404e5a29d5ae","collapsed":true,"_cell_guid":"71563361-0423-4379-b07d-3cd79affa3aa","trusted":true},"cell_type":"code","source":"def build_feature_columns():\n    \"\"\"\n    Build our tensorflow feature columns!\n    \n    For a great overview of the different feature columns in tensorflow and when to use them, see:\n    https://www.tensorflow.org/versions/master/get_started/feature_columns\n    \"\"\"\n    Pclass = tf.feature_column.categorical_column_with_identity(\"Pclass\", num_buckets = 3)\n    Sex = tf.feature_column.categorical_column_with_vocabulary_list(\"Sex\", [\"female\", \"male\"])\n    Age = tf.feature_column.numeric_column(\"Age\")\n    SibSp = tf.feature_column.numeric_column(\"SibSp\")\n    Parch = tf.feature_column.numeric_column(\"Parch\")\n    Fare = tf.feature_column.numeric_column(\"Fare\")\n    # I end up not using the 'Embarked' feature but you can try it if you'd like!\n    #Embarked = tf.feature_column.categorical_column_with_vocabulary_list(\"Embarked\", [\"C\", \"N\", \"Q\", \"S\"])\n    # I end up not using the deck_level feature but you can try it if you'd like!\n    #deck_level = tf.feature_column.categorical_column_with_vocabulary_list(\"deck_level\", [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\", \"U\"])\n    family_size = tf.feature_column.numeric_column(\"family_size\")\n    Pclass_x_Sex = tf.feature_column.crossed_column(keys = [Pclass, Sex], hash_bucket_size = 10)\n    # Let's bucket age into 5 year boundaries\n    age_buckets = tf.feature_column.bucketized_column(Age, boundaries=list(range(5, 100, 10)))\n    fare_buckets = tf.feature_column.bucketized_column(Fare, boundaries=list(range(1, 600, 10)))\n    # Wrapping categorical columns in indicator columns\n    #Embarked = tf.feature_column.indicator_column(Embarked)\n    #deck_level = tf.feature_column.indicator_column(deck_level)\n    Pclass = tf.feature_column.indicator_column(Pclass)\n    Sex = tf.feature_column.indicator_column(Sex)\n    Pclass_x_Sex = tf.feature_column.indicator_column(Pclass_x_Sex)\n    # Time to put together all the feature we'll use!\n    #feature_columns = set([Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, age_buckets, fare_buckets, family_size, Pclass_x_Sex])\n    #feature_columns = set([Pclass, Sex, Age, SibSp, Parch, Fare, age_buckets, fare_buckets, family_size, Pclass_x_Sex])\n    #feature_columns = set([Pclass, Sex, Age, SibSp, Parch, Fare, age_buckets, family_size, Pclass_x_Sex])\n    #feature_columns = set([Pclass, Sex, Age, SibSp, Parch, fare_buckets, age_buckets, family_size, Pclass_x_Sex])\n    feature_columns = set([Pclass, Sex, fare_buckets, age_buckets, family_size, Pclass_x_Sex])\n    return(feature_columns)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"27eec935f2eb1c9041f0e431c19140ae9ba5ef3e","collapsed":true,"_cell_guid":"bfeec7a6-9efa-458f-938e-b02c8022151e","trusted":true},"cell_type":"code","source":"def input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"\n    This is our input function that will pass data into the tensorflow DNN class we'll create.\n    It takes in a pandas dataframe.\n    It outputs a tensorflow dataset one_shot_iterator\n    \"\"\"\n    # Convert pandas df to dict of numpy arrays\n    features = {key:np.array(value) for key, value in dict(features).items()}\n    # Put together the tensorflow dataset. Configures batching/repeating.\n    dataset = Dataset.from_tensor_slices((features, targets))\n    dataset = dataset.batch(batch_size).repeat(num_epochs)\n    # Shuffle data\n    if (shuffle):\n        dataset = dataset.shuffle(buffer_size = 20000)\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    return (features, labels)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"6ed5c638088eb5f5ccf68c59e54affa66a04ddf5","_cell_guid":"8cc4ab03-6bbd-478d-9931-6d28c80a70a1"},"cell_type":"markdown","source":"We'll take our train data and do a 70/30 split so that we can get a sense for the validation performance before trying it on the test set."},{"metadata":{"_uuid":"541639e5f74de68f379a160c8c63c9d0095f6f5c","collapsed":true,"_cell_guid":"3f893f2b-4fab-4687-aebf-f503045739d8","trusted":true},"cell_type":"code","source":"train_ex_df = train_df.sample(frac=0.70)\ntrain_targ_series = train_ex_df['Survived']\n#train_ex_df = train_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"family_size\", \"deck_level\"]]\n#train_ex_df = train_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\", \"deck_level\"]]\ntrain_ex_df = train_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\"]]\n\nxval_ex_df = train_df.drop(train_ex_df.index)\nxval_targ_series = xval_ex_df['Survived']\n#xval_ex_df = xval_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"family_size\", \"deck_level\"]]\n#xval_ex_df = xval_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\", \"deck_level\"]]\nxval_ex_df = xval_ex_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\"]]","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"3af244b89788b3f7831be2be4d5e9f7a481baa9a","_cell_guid":"b6a25d18-4404-4dd9-a44c-db6488506876"},"cell_type":"markdown","source":"# 5) Building our (deep neural network) DNN classifier model with tensorflow"},{"metadata":{"_uuid":"08e7ce8313a098968489d1ff6c737f95812372b2","collapsed":true,"_cell_guid":"ad70ab20-dd2d-4773-8992-b2678ca42130","trusted":true},"cell_type":"code","source":"def plot_acc(train_accs, val_accs):\n    fig, ax = plt.subplots(1, 1)\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Period\")\n    ax.set_title(\"DNN model accuracy vs. Period\")\n    ax.plot(train_accs, label = \"train\")\n    ax.plot(val_accs, label = \"validation\")\n    ax.legend()\n    fig.tight_layout()\n    \n    print(\"Final accuracy (train):\\t\\t{:.3f}\".format(train_accs[-1]))\n    print(\"Final accuracy (validation):\\t{}\".format(val_accs[-1]))\n\ndef train_dnn_classifier(periods, learning_rate, steps, batch_size, hidden_units, train_ex, train_targ, val_ex, val_targ):\n    #steps per period (spp)\n    spp = steps / periods\n    # Make our tensorflow DNN classifier (we'll use the ProximalAdagradOptimizer with L1 regularization to punish overly complex models)\n    optim = tf.train.ProximalAdagradOptimizer(learning_rate = learning_rate,\n                                      l1_regularization_strength=0.1)\n    # We'll use the stock DNNClassifier\n    # We'll also add dropout at a 20% rate to make our network more robust (hopefully)\n    # Finally, we'll make our activation function a leaky_relu\n    dnn_classifier = tf.estimator.DNNClassifier(feature_columns = build_feature_columns(),\n                                                hidden_units = hidden_units,\n                                                optimizer = optim,\n                                                dropout = 0.25,\n                                                activation_fn = tf.nn.leaky_relu)\n    # Input functions\n    train_input_fn = lambda: input_fn(train_ex,\n                                     train_targ,\n                                     batch_size = batch_size)\n    pred_train_input_fn = lambda: input_fn(train_ex,\n                                          train_targ,\n                                          num_epochs = 1,\n                                          shuffle = False)\n    pred_val_input_fn = lambda: input_fn(val_ex,\n                                         val_targ,\n                                         num_epochs = 1,\n                                         shuffle = False)\n    #train and validation accuracy per period\n    train_app = []\n    val_app = []\n    for period in range(periods):\n        # Train our classifier\n        dnn_classifier.train(input_fn = train_input_fn, steps = spp)\n        # Check how our classifier does on training set after one period\n        train_pred = dnn_classifier.predict(input_fn = pred_train_input_fn)\n        train_pred = np.array([int(pred['classes'][0]) for pred in train_pred])\n        # Check how our classifier does on the validation set after one period\n        val_pred = dnn_classifier.predict(input_fn = pred_val_input_fn)\n        val_pred = np.array([int(pred['classes'][0]) for pred in val_pred])\n        # Calculate accuracy metrics\n        train_acc = accuracy_score(train_targ, train_pred)\n        val_acc = accuracy_score(val_targ, val_pred)\n        print(\"period {} train acc: {:.3f}\".format(period, train_acc))\n        # Add our accuracies to running list\n        train_app.append(train_acc)\n        val_app.append(val_acc)\n    print(\"Training done!\")\n    plot_acc(train_app, val_app)\n    return (dnn_classifier)","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"3299f0ec38fb315587a06103874f195e4c9e073f","_cell_guid":"32234ac2-5cdd-40b4-a920-91974ed49f31"},"cell_type":"markdown","source":"# 6) Training our model"},{"metadata":{"_uuid":"6a30398e2d94bd085f0faf9be8d59cb4741bfccb","_cell_guid":"41bcf882-bdda-458b-ace5-b42e2055e8f9","trusted":true},"cell_type":"code","source":"tf.logging.set_verbosity(tf.logging.ERROR)\nclassifier = train_dnn_classifier(periods = 25,\n                                 learning_rate = 0.05,\n                                 steps = 4000,\n                                 batch_size = 75,\n                                 hidden_units = [100, 100, 42],\n                                 train_ex = train_ex_df,\n                                 train_targ = train_targ_series,\n                                 val_ex = xval_ex_df,\n                                 val_targ = xval_targ_series)","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"17215a443e8b5012da64e659a2a421233c53db39","_cell_guid":"6ed1d220-c43d-4dbc-9020-723aa5dbb15b"},"cell_type":"markdown","source":"# 7) Making predictions for test dataset"},{"metadata":{"_uuid":"25d0f24f72e39aa1c60b831d23071329e4930191","collapsed":true,"_cell_guid":"0934980a-3f0d-455c-8859-aed3612251ee","trusted":true},"cell_type":"code","source":"#test_ex_df = test_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\", \"deck_level\"]]\n#test_ex_df = test_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"family_size\", \"deck_level\"]]\ntest_ex_df = test_df[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"family_size\"]]\n# Create a dummy series that will be compatible with our input_fn\ntest_targ_series = pd.Series(np.zeros(len(test_df), dtype=int))\n\npred_test_input_fn = lambda: input_fn(test_ex_df,\n                                     test_targ_series,\n                                     num_epochs = 1,\n                                     shuffle = False)\n\ntest_pred = classifier.predict(input_fn = pred_test_input_fn)\ntest_pred = np.array([int(pred['classes'][0]) for pred in test_pred])","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"99773f12efde28697e32952981b774c7fb28223e","collapsed":true,"_cell_guid":"ad7c919b-7bca-450d-a7b8-2404575b5b90","trusted":false},"cell_type":"code","source":"submission_df = test_df[[\"PassengerId\"]]\nsubmission_df = submission_df.assign(Survived=pd.Series(test_pred).values)\ndisplay(submission_df)\nsubmission_df.to_csv('2018-03-31_submission_5.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f27ded3f8ed42868f7053a23b741fcb3dc76c54","collapsed":true,"_cell_guid":"e41c98ef-b768-40f0-b6f9-60ff0561ccb7","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}