{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport keras\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import *\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\nfrom keras import optimizers\nfrom keras import regularizers\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"195d9b1b2b0a4346e97759f1cdcfdaf21078a86c"},"cell_type":"markdown","source":"First, let's take a look at the soil type descriptions:"},{"metadata":{"trusted":true,"_uuid":"5f186c3d29a1f368a70afd9ff43d3c92af9dd26b","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"descriptions = {\n    1: 'Cathedral family - Rock outcrop complex, extremely stony.',\n    2: 'Vanet - Ratake families complex, very stony.',\n    3: 'Haploborolis - Rock outcrop complex, rubbly.',\n    4: 'Ratake family - Rock outcrop complex, rubbly.',\n    5: 'Vanet family - Rock outcrop complex complex, rubbly.',\n    6: 'Vanet - Wetmore families - Rock outcrop complex, stony.',\n    7: 'Gothic family.',\n    8: 'Supervisor - Limber families complex.',\n    9: 'Troutville family, very stony.',\n    10: 'Bullwark - Catamount families - Rock outcrop complex, rubbly.',\n    11: 'Bullwark - Catamount families - Rock land complex, rubbly.',\n    12: 'Legault family - Rock land complex, stony.',\n    13: 'Catamount family - Rock land - Bullwark family complex, rubbly.',\n    14: 'Pachic Argiborolis - Aquolis complex.',\n    15: 'unspecified in the USFS Soil and ELU Survey.',\n    16: 'Cryaquolis - Cryoborolis complex.',\n    17: 'Gateview family - Cryaquolis complex.',\n    18: 'Rogert family, very stony.',\n    19: 'Typic Cryaquolis - Borohemists complex.',\n    20: 'Typic Cryaquepts - Typic Cryaquolls complex.',\n    21: 'Typic Cryaquolls - Leighcan family, till substratum complex.',\n    22: 'Leighcan family, till substratum, extremely stony.',\n    23: 'Leighcan family, till substratum - Typic Cryaquolls complex.',\n    24: 'Leighcan family, extremely stony.',\n    25: 'Leighcan family, warm, extremely stony.',\n    26: 'Granile - Catamount families complex, very stony.',\n    27: 'Leighcan family, warm - Rock outcrop complex, extremely stony.',\n    28: 'Leighcan family - Rock outcrop complex, extremely stony.',\n    29: 'Como - Legault families complex, extremely stony.',\n    30: 'Como family - Rock land - Legault family complex, extremely stony.',\n    31: 'Leighcan - Catamount families complex, extremely stony.',\n    32: 'Catamount family - Rock outcrop - Leighcan family complex, extremely stony.',\n    33: 'Leighcan - Catamount families - Rock outcrop complex, extremely stony.',\n    34: 'Cryorthents - Rock land complex, extremely stony.',\n    35: 'Cryumbrepts - Rock outcrop - Cryaquepts complex.',\n    36: 'Bross family - Rock land - Cryumbrepts complex, extremely stony.',\n    37: 'Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.',\n    38: 'Leighcan - Moran families - Cryaquolls complex, extremely stony.',\n    39: 'Moran family - Cryorthents - Leighcan family complex, extremely stony.',\n    40: 'Moran family - Cryorthents - Rock land complex, extremely stony.',\n}\n\nwords = sum([desc.split(' ') for _, desc in descriptions.items()], [])\nfreq_dict = {word: words.count(word) for word in set(words)}\nfreq_dict = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\nfor word, count in freq_dict:\n    if count > 2:\n        print(count, word)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3d0b9548bd189f4f9e2e003a23a7a28620036c9"},"cell_type":"markdown","source":"We select the most common and meaningful of these words to create a list of keywords that we will later add to our features."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b7d3dce5dde132467d36cb17ea17f56b25acb40"},"cell_type":"code","source":"keywords = [\n    'Rock outcrop complex',\n    'Rock land complex',\n    'Rock',\n    'extremely stony',\n    'very stony',\n    'stony',\n    'rubbly',\n    'till substratum',\n    'Bullwark',\n    'Catamount',\n    'Cryaquolis',\n    'Cryorthents',\n    'Cryumbrepts',\n    'Leighcan',\n    'Legault',\n    'Moran',\n    'Ratake',\n    'Typic',\n    'Vanet',\n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48a6f27ce58d152ece9380823d40bb3ddde387a1"},"cell_type":"markdown","source":"Then, we create a dictionnary mapping each soil type to a vector indicating which keywords its description contains."},{"metadata":{"trusted":true,"_uuid":"77583215f2a41406bab755519e499903f51a2aa9","_kg_hide-input":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"keyword_dict = {}\nfor soil_id, desc in descriptions.items():\n    keyword_dict[soil_id] = np.array([keyword in desc for keyword in keywords])\nprint(keyword_dict[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5fa09109b28807a6cc595f6a0814a5480cda634"},"cell_type":"markdown","source":"Then we process the data by adding custom features (including the text features) and one-hot-encoding the target variable."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def feature_engineering(df):\n    # Numeric features\n    df['Euclidean_Distance_To_Hydrology'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['Hillshade_Total'] = df.filter(like='Hillshade').sum(axis=1)\n    df['Hillshade_Slope'] = (df['Hillshade_3pm'] - df['Hillshade_9am']) / df['Hillshade_9am'].apply(lambda x: 1 if x == 0 else x)\n    \n    # Text features\n    soils = df.filter(like='Soil_Type')\n    for col in soils.columns:\n        id = int(col[9:])\n        soils.loc[:, col] = soils.loc[:, col] * id\n    soils = soils.sum(axis=1)\n    keyword_array = np.stack([keyword_dict[x] for x in soils])\n    \n    keyword_df = pd.DataFrame(keyword_array, index=soils.index, columns=['keyword_{}'.format(k) for k in keywords])\n    df = pd.concat([df, keyword_df], axis=1)\n    \n    return df\n\ndef expand(df):\n    target = df.pop('Cover_Type')\n    target = pd.get_dummies(target, prefix='Cover_Type')\n    df = pd.concat([df, target], axis=1)\n    return df\n\nprint('processing train data...')\ndf = pd.read_csv('../input/train.csv')\ndf = df.sample(frac=1) # Shuffling is necessary for the training set as the dataset is not shuffled.\n\ntrain_df = feature_engineering(expand(df))\nprint('\\tdone.')\ntrain_df.info()\n\nprint('processing test data...')\ndf = pd.read_csv('../input/test.csv')\n\ntest_df = feature_engineering(df)\nprint('\\tdone.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75ddc11edb5551ba4b3a3e4893a3f29ac5092ba7"},"cell_type":"markdown","source":"Normalizing the data helps the neural network to deal with it better. To make sure we are normalizing the training and test data in the same way, we first compute the mean and std of each column of the test set."},{"metadata":{"trusted":true,"_uuid":"dfb752720ffce8c7c937d58b4a74d7c49afd1e7b","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"print('computing mean and std...')\nmean_std_df = test_df.agg(['mean', 'std'])\nprint('\\tdone.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a3387135b75ac232c1e2678d3cf02bd487e588"},"cell_type":"markdown","source":"Then we use this data to normalize the train and test data."},{"metadata":{"trusted":true,"_uuid":"25ecbf0e1bf0f17dc10e8752c9ab58a31ac5602c","_kg_hide-input":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"norm_columns = [\n    'Elevation',\n    'Aspect',\n    'Slope',\n    'Horizontal_Distance_To_Hydrology',\n    'Vertical_Distance_To_Hydrology',\n    'Horizontal_Distance_To_Roadways',\n    'Hillshade_9am',\n    'Hillshade_Noon',\n    'Hillshade_3pm',\n    'Horizontal_Distance_To_Fire_Points',\n    'Euclidean_Distance_To_Hydrology',\n    'Hillshade_Total',\n    'Hillshade_Slope',\n]\n\nmean_std_df = mean_std_df[norm_columns]\n\ndef normalize(df):\n    df[norm_columns] = (df[norm_columns] - mean_std_df.loc['mean', :]) / mean_std_df.loc['std', :].apply(lambda x: 1 if x == 0 else x)\n    no_variance = mean_std_df.loc['std', mean_std_df.loc['std', :] == 0].index.tolist()\n    print('Dropped no_variance columns: {}'.format(len(no_variance)))\n    df.drop(no_variance, axis=1, inplace=True)\n    return df\n\ntrain_df = normalize(train_df)\ntest_df = normalize(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"432645ed0847a3360467ccf5f4279cebbac84800"},"cell_type":"markdown","source":"We will run the training in two steps. First, we will train the model on the training set, and get it to a high enough accuracy that it can hopefully predict the test set classes well. Then, we will use its predicted class proportions on the test set to appropriately upsample the training set and train a new model on it."},{"metadata":{"trusted":true,"_uuid":"299f56a8c42a6e344ac49b40b90131ce1c3a55f9","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def load_data(df, val_ids=None):\n    print('building test set...')\n    if val_ids is None:\n        X_test = df.iloc[:1512, :]\n        val_ids = X_test['Id']\n    else:\n        X_test = df[df['Id'].isin(val_ids)]\n    X_test.drop('Id', axis=1, inplace=True)\n    data = df.drop('Id', axis=1)\n    y_test = X_test.filter(like='Cover_Type')\n    X_test.drop(y_test.columns, axis=1, inplace=True)\n    data.drop(X_test.index, axis=0, inplace=True)\n\n    print('building training set...')\n    X_train = data\n    y_train = X_train.filter(like='Cover_Type')\n    X_train.drop(y_train.columns, axis=1, inplace=True)\n\n    return X_train, y_train, X_test, y_test, X_train.shape[1], val_ids\n\nX_train, y_train, X_test, y_test, input_size, val_ids = load_data(train_df)\n\ndef make_nn(size='big'):\n    inputs = Input(shape=(input_size,))\n    if size=='big':\n        x = Dense(256, activation='relu')(inputs)\n        x = Dense(256, activation='relu')(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dense(64, activation='relu')(x)\n        x = Dense(32, activation='relu')(x)\n        x = Dense(16, activation='relu')(x)\n        x = Dense(7, activation='softmax')(x)\n        \n    else:\n        x = Dense(256, activation='relu')(inputs)\n        x = Dense(256, activation='relu')(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dense(64, activation='relu')(x)\n        x = Dense(32, activation='relu')(x)\n        x = Dense(16, activation='relu')(x)\n        x = Dense(7, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n    return model\n\ndef fit_nn(model, lr, bs, num_epochs):\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizers.Adam(lr),\n                  metrics=['accuracy'])\n    clr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_delta=0.5, cooldown=2, min_lr=1e-7, verbose=1)\n    checkpoint = ModelCheckpoint('checkpoint.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    \n    model.fit(\n        X_train,\n        y_train,\n        batch_size=bs,\n        epochs=num_epochs,\n        callbacks=[clr, checkpoint],\n        shuffle=True,\n        validation_data=(X_test, y_test)\n        )\n\n    return model\n\nfit_nn(make_nn('big'), 1e-3, 32, 30)\nmodel = load_model('checkpoint.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"244981a153d46023676ac61224c8b8ca28783646"},"cell_type":"markdown","source":"After 30 epochs, the NN should be good enough. Let's use it to estimate the class proportions on the test set:"},{"metadata":{"trusted":true,"_uuid":"cefedc32aa71f15df7437f7182ef63e0962f7470","collapsed":true},"cell_type":"code","source":"test_id_col = test_df.pop('Id')\n\ny = model.predict(test_df)\ny = [np.argmax(x)+1 for x in y]\ny = pd.Series(y)\n\nsns.countplot(y)\nclass_proportions = y.value_counts(normalize=True)\nprint(class_proportions)\n\nclass_fracs = class_proportions / class_proportions.iloc[-1] - 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49de5f9611d6321ad18673e953782bb8fbd60ced"},"cell_type":"markdown","source":"Now we can use these proportions to upsample the dataset."},{"metadata":{"trusted":true,"_uuid":"584383a87f0f6fcb7a9c2033ff4ecdf5e2a92531","collapsed":true},"cell_type":"code","source":"for ct in class_fracs.index[:-1]:\n    class_samples = train_df[train_df['Cover_Type_{}'.format(ct)] == 1]\n    train_df = pd.concat([train_df, class_samples.sample(frac=class_fracs[ct], replace=True)])\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"804806439d882a0c1796442e18b95e64eb270622"},"cell_type":"markdown","source":"There sure are a lot more samples in the dataset now!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d1c884ab9a5183ab8f9787fa5692c4083cd615c9"},"cell_type":"code","source":"X_train, y_train, X_test, y_test, _, _ = load_data(train_df, val_ids)\n\ny_test_classes = pd.Series([np.argmax(x)+1 for x in y_test])\nprint(y_test_classes.value_counts(normalize=True))\n\nfit_nn(make_nn('small'), 1e-3, 256, 10)\nmodel = load_model('checkpoint.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08068891e26e8f81bec196df61793933af7c17e"},"cell_type":"markdown","source":"Finally, we can use the model to make predictions.\n\nWe'll also observe whether the model predicts the less common classes at all."},{"metadata":{"trusted":true,"_uuid":"62a2a550307361949d1922972d2033e58c09b88b","collapsed":true},"cell_type":"code","source":"y = model.predict(test_df)\ny = pd.Series([np.argmax(x)+1 for x in y])\n\nsns.countplot(y)\nprint(y.value_counts(normalize=True))\n\nsubmit = pd.DataFrame({'Id': test_id_col, 'Cover_Type': y})\n\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}