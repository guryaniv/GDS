{"cells":[{"metadata":{"_cell_guid":"7075e932-38a1-4de3-841e-d96d48ecd5de","_uuid":"2e333bdb0cfcff17eb8f035cdee3acb4b606dd4c"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9d2dbdb3-6c74-4f96-9865-2951dfd653ce","_uuid":"bb41ad86b25fecf332927b0c8f55dd710101e33f"},"cell_type":"markdown","source":"# Evolve a neural network with a genetic algorithm\n\n\nBuilding the perfect deep learning network involves a hefty amount of art to accompany sound science. One way to go about finding the right hyperparameters is through brute force trial and error: Try every combination of sensible parameters, send them to your Spark cluster, go about your daily jive, and come back when you have an answer.\n\nBut there’s gotta be a better way!\n\nHere, we try to improve upon the brute force method by applying a genetic algorithm to evolve a network with the goal of achieving optimal hyperparameters in a fraction the time of a brute force search\n\n# How much faster?\nLet’s say it takes five minutes to train and evaluate a network on your dataset. And let’s say we have four parameters with five possible settings each. To try them all would take (5**4) * 5 minutes, or 3,125 minutes, or about 52 hours.\n\nNow let’s say we use a genetic algorithm to evolve 10 generations with a population of 20 (more on what this means below), with a plan to keep the top 25% plus a few more, so ~8 per generation. This means that in our first generation we score 20 networks (20 * 5 = 100 minutes). Every generation after that only requires around 12 runs, since we don’t have the score the ones we keep. That’s 100 + (9 generations * 5 minutes * 12 networks) = 640 minutes, or 11 hours.\n\nWe’ve just reduced our parameter tuning time by almost 80%! That is, assuming it finds the best parameters…","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"35802c95-fedf-4255-8f0b-9e41ccf49ee7","_uuid":"a30dc5251d9539fa5bf058a9f8d327897ff3a9b3"},"cell_type":"markdown","source":"# How do genetic algorithms work?\nAt its core, a genetic algorithm…\n\n- Creates a population of (randomly generated) members\n- Scores each member of the population based on some goal. This score is called a fitness function.\n- Selects and breeds the best members of the population to produce more like them\n- Mutates some members randomly to attempt to find even better candidates\n- Kills off the rest (survival of the fittest and all), and\n- Repeats from step 2. Each iteration through these steps is called a generation.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"97bb1464-fb17-4ff7-bb45-1fb6a6bd50ca","_uuid":"ed14c7523e9fbadd766c02ac5ec01b2a8f9269da"},"cell_type":"markdown","source":"# Applying genetic algorithms to Neural Networks\nWe’ll attempt to evolve a GRU based network. Our goal is to find the best parameters for Toxic Comment Classification task.\n\nWe’ll tune four parameters:\n\n- Number of layers (or the network depth)\n- Neurons per layer (or the network width)\n- Dense layer activation function\n- Network optimizer","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"443d8a7a-c35f-4d54-a723-9500acab3dc6","_uuid":"db3afd86144a919fa300a8c4c83667187a110c11"},"cell_type":"markdown","source":"Lets start by Initializing the variables","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e509f294-2277-43d4-9270-5e60b2a7cd79","_uuid":"11c1efb604324c7e5d7b303e28189daa64883fc4","collapsed":true,"trusted":false},"cell_type":"code","source":"import random\nimport logging\nfrom functools import reduce\nfrom operator import add\nimport random\nimport logging\nfrom tqdm import tqdm\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import EarlyStopping\ntqdm.monitor_interval = 0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae73aa4f-b942-4942-a6ca-384bcde2000a","_uuid":"fb7698402824ae923aa8d869a6e0647f909744df"},"cell_type":"markdown","source":"This will be the class that will save the configuration of the parameters of the model, creates the model, trains on the model, and predicts with the model on the test set. It is also the class that represents the network to be evolved.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5af90052-7247-44ca-9f90-030b1d4944d1","_uuid":"a2a8854fe0c21fb2b787445541449491b1a3d81b","collapsed":true,"trusted":false},"cell_type":"code","source":"class Network():\n    \"\"\"Represent a network and let us operate on it.\n    \"\"\"\n\n    def __init__(self, nn_param_choices=None):\n        \"\"\"Initialize our network.\n        Args:\n            nn_param_choices (dict): Parameters for the network, includes:\n                nb_neurons (list): [64, 128, 256]\n                nb_layers (list): [1, 2, 3, 4]\n                activation (list): ['relu', 'elu']\n                optimizer (list): ['rmsprop', 'adam']\n        \"\"\"\n        self.accuracy = 0.\n        self.nn_param_choices = nn_param_choices\n        self.network = {}  # (dic): represents MLP network parameters\n        self.predictions = []\n\n    def create_random(self):\n        \"\"\"Create a random network.\"\"\"\n        for key in self.nn_param_choices:\n            self.network[key] = random.choice(self.nn_param_choices[key])\n\n    def create_set(self, network):\n        \"\"\"Set network properties.\n        Args:\n            network (dict): The network parameters\n        \"\"\"\n        self.network = network\n    def compile_model(self, network, nb_classes, embedding_matrix):\n        \"\"\"Compile a sequential model.\n        Args:\n            network (dict): the parameters of the network\n        Returns:\n            a compiled network.\n        \"\"\"\n        # Get our network parameters.\n        nb_layers = network['nb_layers']\n        nb_neurons = network['nb_neurons']\n        activation = network['activation']\n        optimizer = network['optimizer']\n        embed_size = 50 # how big is each word vector\n        max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n        maxlen = 100 # max number of words in a comment to use\n\n        inp = Input(shape=(maxlen,))\n        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n        # Add each layer.\n        for i in range(nb_layers):\n\n            # Need input shape for first layer.\n            x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n            x = Dense(nb_neurons, activation=activation)(x)\n            x = Dropout(0.2)(x)  # hard-coded dropout\n        x = GlobalMaxPool1D()(x)\n        x = Dense(nb_classes, activation=\"sigmoid\")(x)\n        model = Model(inputs=inp, outputs=x)\n\n        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return model\n    def train_and_score(self, network, train, label,  embedding_matrix, test):\n        \"\"\"Train the model, return test loss.\n        Args:\n            network (dict): the parameters of the network\n            dataset (str): Dataset to use for training/evaluating\n        \n        if dataset == 'cifar10':\n            nb_classes, batch_size, input_shape, x_train, \\\n                x_test, y_train, y_test = get_cifar10()\n        elif dataset == 'mnist':\n            nb_classes, batch_size, input_shape, x_train, \\\n                x_test, y_train, y_test = get_mnist()\n        \"\"\"\n        batch_size=32\n        epochs=2\n        validation_split=0.1\n        model = self.compile_model(network, 6, embedding_matrix)\n        early_stopper = EarlyStopping(patience=5)\n        model.fit(train, label,\n                  batch_size=batch_size,\n                  epochs=10000,  # using early stopping, so no real limit\n                  verbose=0,\n                  validation_split=0.1,\n                  callbacks=[early_stopper])\n        score = model.evaluate(x_test, y_test, verbose=0)\n        y_test = model.predict([test], batch_size=1024, verbose=1)\n        '''\n        sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n        list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n        sample_submission[list_classes] = y_test\n        sample_submission.to_csv('submission.csv', index=False)\n        '''\n        \n        self.predictions = y_test\n        return score[1]  # 1 is accuracy. 0 is loss.\n\n    def train(self, train, label, embedding_matrix, test):\n        \"\"\"Train the network and record the accuracy.\n        Args:\n            dataset (str): Name of dataset to use.\n        \"\"\"\n        if self.accuracy == 0.:\n            self.accuracy = self.train_and_score(self.network, train, label, embedding_matrix, test)\n\n    def print_network(self):\n        \"\"\"Print out a network.\"\"\"\n        logging.info(self.network)\n        logging.info(\"Network accuracy: %.2f%%\" % (self.accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"74cec9cc-756a-4857-b7f2-51d90fb1cae1","_uuid":"51d2074aedaf6d26e107ba4e0706a52485ad3649"},"cell_type":"markdown","source":"This is the Optimizer class that holds a genetic algorithm for evolving a network.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d057e788-ec66-429c-ab81-7e7fd5159efe","_uuid":"5835fbb7788c08f6a061a0754d9affbad795c7b2","collapsed":true,"trusted":false},"cell_type":"code","source":"class Optimizer():\n    \"\"\"Class that implements genetic algorithm for MLP optimization.\"\"\"\n\n    def __init__(self, nn_param_choices, retain=0.4,\n                 random_select=0.1, mutate_chance=0.2):\n        \"\"\"Create an optimizer.\n        Args:\n            nn_param_choices (dict): Possible network paremters\n            retain (float): Percentage of population to retain after\n                each generation\n            random_select (float): Probability of a rejected network\n                remaining in the population\n            mutate_chance (float): Probability a network will be\n                randomly mutated\n        \"\"\"\n        self.mutate_chance = mutate_chance\n        self.random_select = random_select\n        self.retain = retain\n        self.nn_param_choices = nn_param_choices\n\n    def create_population(self, count):\n        \"\"\"Create a population of random networks.\n        Args:\n            count (int): Number of networks to generate, aka the\n                size of the population\n        Returns:\n            (list): Population of network objects\n        \"\"\"\n        pop = []\n        for _ in range(0, count):\n            # Create a random network.\n            network = Network(self.nn_param_choices)\n            network.create_random()\n\n            # Add the network to our population.\n            pop.append(network)\n\n        return pop\n\n    @staticmethod\n    def fitness(network):\n        \"\"\"Return the accuracy, which is our fitness function.\"\"\"\n        return network.accuracy\n\n    def grade(self, pop):\n        \"\"\"Find average fitness for a population.\n        Args:\n            pop (list): The population of networks\n        Returns:\n            (float): The average accuracy of the population\n        \"\"\"\n        summed = reduce(add, (self.fitness(network) for network in pop))\n        return summed / float((len(pop)))\n\n    def breed(self, mother, father):\n        \"\"\"Make two children as parts of their parents.\n        Args:\n            mother (dict): Network parameters\n            father (dict): Network parameters\n        Returns:\n            (list): Two network objects\n        \"\"\"\n        children = []\n        for _ in range(2):\n\n            child = {}\n\n            # Loop through the parameters and pick params for the kid.\n            for param in self.nn_param_choices:\n                child[param] = random.choice(\n                    [mother.network[param], father.network[param]]\n                )\n\n            # Now create a network object.\n            network = Network(self.nn_param_choices)\n            network.create_set(child)\n\n            # Randomly mutate some of the children.\n            if self.mutate_chance > random.random():\n                network = self.mutate(network)\n\n            children.append(network)\n\n        return children\n\n    def mutate(self, network):\n        \"\"\"Randomly mutate one part of the network.\n        Args:\n            network (dict): The network parameters to mutate\n        Returns:\n            (Network): A randomly mutated network object\n        \"\"\"\n        # Choose a random key.\n        mutation = random.choice(list(self.nn_param_choices.keys()))\n\n        # Mutate one of the params.\n        network.network[mutation] = random.choice(self.nn_param_choices[mutation])\n\n        return network\n    \n       \n\n        return model\n    def evolve(self, pop):\n        \"\"\"Evolve a population of networks.\n        Args:\n            pop (list): A list of network parameters\n        Returns:\n            (list): The evolved population of networks\n        \"\"\"\n        # Get scores for each network.\n        graded = [(self.fitness(network), network) for network in pop]\n\n        # Sort on the scores.\n        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n\n        # Get the number we want to keep for the next gen.\n        retain_length = int(len(graded)*self.retain)\n\n        # The parents are every network we want to keep.\n        parents = graded[:retain_length]\n\n        # For those we aren't keeping, randomly keep some anyway.\n        for individual in graded[retain_length:]:\n            if self.random_select > random.random():\n                parents.append(individual)\n\n        # Now find out how many spots we have left to fill.\n        parents_length = len(parents)\n        desired_length = len(pop) - parents_length\n        children = []\n\n        # Add children, which are bred from two remaining networks.\n        while len(children) < desired_length:\n\n            # Get a random mom and dad.\n            male = random.randint(0, parents_length-1)\n            female = random.randint(0, parents_length-1)\n\n            # Assuming they aren't the same network...\n            if male != female:\n                male = parents[male]\n                female = parents[female]\n\n                # Breed them.\n                babies = self.breed(male, female)\n\n                # Add the children one at a time.\n                for baby in babies:\n                    # Don't grow larger than desired length.\n                    if len(children) < desired_length:\n                        children.append(baby)\n\n        parents.extend(children)\n\n        return parents","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0cd0dd32-7ede-4f06-9fe4-b77be3af2515","_uuid":"ffdca4c305b167446bc5fca00d01d03f3bd67f6c"},"cell_type":"markdown","source":"This will setup some logs and create the functions that will train the AI with GA","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c1fc59c4-51e2-4a2c-a0cd-14293fb798e5","_uuid":"8721a2539f9180c2a12271258cb86dcf32cd203e","collapsed":true,"trusted":false},"cell_type":"code","source":"logging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%m/%d/%Y %I:%M:%S %p',\n    level=logging.DEBUG,\n    filename='log.txt'\n)\n\ndef train_networks(networks, train, label, test):\n    \"\"\"Train each network.\n    Args:\n        networks (list): Current population of networks\n        dataset (str): Dataset to use for training/evaluating\n    \"\"\"\n    pbar = tqdm(total=len(networks))\n    for network in networks:\n        network.train(train, label, embedding_matrix, test)\n        pbar.update(1)\n    pbar.close()\n\ndef get_average_accuracy(networks):\n    \"\"\"Get the average accuracy for a group of networks.\n    Args:\n        networks (list): List of networks\n    Returns:\n        float: The average accuracy of a population of networks.\n    \"\"\"\n    total_accuracy = 0\n    for network in networks:\n        total_accuracy += network.accuracy\n\n    return total_accuracy / len(networks)\n\ndef generate(generations, population, nn_param_choices, train, label, embedding_matrix, test):\n    \"\"\"Generate a network with the genetic algorithm.\n    Args:\n        generations (int): Number of times to evole the population\n        population (int): Number of networks in each generation\n        nn_param_choices (dict): Parameter choices for networks\n        dataset (str): Dataset to use for training/evaluating\n    \"\"\"\n    optimizer = Optimizer(nn_param_choices)\n    networks = optimizer.create_population(population)\n\n    # Evolve the generation.\n    for i in range(generations):\n        logging.info(\"***Doing generation %d of %d***\" %\n                     (i + 1, generations))\n\n        # Train and get accuracy for networks.\n        train_networks(networks, train, label, test)\n\n        # Get the average accuracy for this generation.\n        average_accuracy = get_average_accuracy(networks)\n\n        # Print out the average accuracy each generation.\n        logging.info(\"Generation average: %.2f%%\" % (average_accuracy * 100))\n        logging.info('-'*80)\n\n        # Evolve, except on the last iteration.\n        if i != generations - 1:\n            # Do the evolution.\n            networks = optimizer.evolve(networks)\n\n    # Sort our final population.\n    networks = sorted(networks, key=lambda x: x.accuracy, reverse=True)\n\n    # Print out the top 5 networks.\n    PROBABILITIES_NORMALIZE_COEFFICIENT = 1.4\n    print_networks(networks[:5])\n    top_predictions = []\n    for network in networks[:5]:\n        top_predictions.append(network.predictions)\n        \n    test_predicts = np.ones(top_predictions[0].shape)\n    for pred in top_predictions:\n        test_predicts *= pred\n\n    test_predicts **= (1. / len(top_predictions))\n    test_predicts **= PROBABILITIES_NORMALIZE_COEFFICIENT\n\n    path = '../input/'\n    comp = 'jigsaw-toxic-comment-classification-challenge/'\n    sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    sample_submission[list_classes] = test_predicts\n    sample_submission.to_csv('genetic_algorithm_predict.csv', index=False)\n\ndef print_networks(networks):\n    \"\"\"Print a list of networks.\n    Args:\n        networks (list): The population of networks\n    \"\"\"\n    logging.info('-'*80)\n    for network in networks:\n        network.print_network()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c297fa80-beea-464b-ac90-f380ebdb02fe","_uuid":"d961885dfde18796893922f72ade1bf64456404e"},"cell_type":"markdown","source":"We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"66a6b5fd-93f0-4f95-ad62-3253815059ba","_uuid":"729b0f0c2a02c678631b8c072d62ff46146a82ef","collapsed":true,"trusted":false},"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98f2b724-7d97-4da8-8b22-52164463a942","_uuid":"b62d39216c8d00b3e6b78b825212fd190757dff9"},"cell_type":"markdown","source":"|Set some basic config parameters:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2807a0a5-2220-4af6-92d6-4a7100307de2","_uuid":"d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3","collapsed":true,"trusted":false},"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b3a8d783-95c2-4819-9897-1320e3295183","_uuid":"4dd8a02e7ef983f10ec9315721c6dda2958024af"},"cell_type":"markdown","source":"Read in our data and replace missing values:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ac2e165b-1f6e-4e69-8acf-5ad7674fafc3","_uuid":"8ab6dad952c65e9afcf16e43c4043179ef288780","collapsed":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54a7a34e-6549-45f7-ada2-2173ff2ce5ea","_uuid":"e8810c303980f41dbe0543e1c15d35acbdd8428f"},"cell_type":"markdown","source":"Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed).","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"79afc0e9-b5f0-42a2-9257-a72458e91dbb","_uuid":"c292c2830522bfe59d281ecac19f3a9415c07155","collapsed":true,"trusted":false},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8c4f6a3-3a19-40b1-ad31-6df2690bec8a","_uuid":"e1cb77629e35c2b5b28288b4d6048a86dda04d78"},"cell_type":"markdown","source":"Read the glove word vectors (space delimited strings) into a dictionary from word->vector.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7d19392b-7750-4a1b-ac30-ed75b8a62d52","_uuid":"e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7370416a-094a-4dc7-84fa-bdbf469f6579","_uuid":"20cea54904ac1eece20874e9346905a59a604985"},"cell_type":"markdown","source":"Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4d29d827-377d-4d2f-8582-4a92f9569719","_uuid":"96fc33012e7f07a2169a150c61574858d49a561b","trusted":false,"collapsed":true},"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62acac54-0495-4a26-ab63-2520d05b3e19","_uuid":"574c91e270add444a7bc8175440274bdd83b7173","collapsed":true,"trusted":false},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55dc26fb-371b-4016-b413-c5368725f21a","_uuid":"73d177c599eb0a63fea8bf084584acfc0f1bfe62"},"cell_type":"markdown","source":"Lets Evolve the network!!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1272248d-ce84-4937-a3e4-30dd102cd89d","collapsed":true,"_uuid":"1c9048c08f93055b357a77ff4765e39b489a91c0","trusted":false},"cell_type":"code","source":"generations = 10  # Number of times to evole the population.\npopulation = 20  # Number of networks in each generation.\ndataset = 'cifar10'\n\nnn_param_choices = {\n    'nb_neurons': [64, 128, 256, 512, 768, 1024],\n    'nb_layers': [1, 2, 3, 4],\n    'activation': ['relu', 'elu', 'tanh', 'sigmoid'],\n    'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n                    'adadelta', 'adamax', 'nadam'],\n}\n\nlogging.info(\"***Evolving %d generations with population %d***\" %\n                (generations, population))\n\ngenerate(generations, population, nn_param_choices, X_t, y, embedding_matrix, X_te)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d7a52c1-6be9-4fc2-848b-69bb25977a47","_uuid":"46b2394a1a6d726d2df2c9fbb0e0012cfec9ce03"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d35cb74d-dee6-4c2c-823a-e37099a909ca","_uuid":"6bf99f3b40e505e7e2a45c57f6a583941378621a"},"cell_type":"markdown","source":"Please do try add more parameter choices to increase the variations for training!\n\nPS: The original code is mostly referenced from this link https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164. Hope the code runs! Please fork it to run as kaggle don't allow such long time of processing. Unfortunately I do not have the resource to try the code at this moment, so if anyone found any errors please comment and I will update thanks!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bd4bb18e-cccf-484e-aa1b-bcc6ff5b0fa2","_uuid":"1234db8048f22bf23cf1328f6abd6808ce1aff5f"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","version":"3.6.4","name":"python"}},"nbformat":4,"nbformat_minor":1}