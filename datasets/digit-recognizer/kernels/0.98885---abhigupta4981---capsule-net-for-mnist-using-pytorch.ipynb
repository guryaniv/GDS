{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"np.random.seed(2)\ntorch.manual_seed(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"235c5900d1049bca329ba23de65ded75533d0a5a"},"cell_type":"code","source":"from torchvision import datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageOps, ImageEnhance\nimport math\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"788758e45eda228003a315930b02edf907f3cdae"},"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, file_path, transform=transforms.Compose([transforms.ToPILImage(), \n                                                                transforms.ToTensor(), \n                                                                transforms.Normalize(mean=(0.5,), \n                                                                                     std=(0.5,))])):\n        df = pd.read_csv(file_path)\n        if len(df.columns)==n_pixels:\n            self.X = df.values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.y = None\n        else:\n            self.X = df.iloc[:, 1:].values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.y = torch.from_numpy(df.iloc[:, 0].values)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        return self.transform(self.X[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03d712cf417d6444b1ab9576657527e0a1848987"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8294470265f0837ee00a38f7baa312a3be8536f"},"cell_type":"code","source":"n_pixels = len(test_df.columns)\nn_pixels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c0257ed7cf2929152b35df876bada88d3ad6c59"},"cell_type":"code","source":"num_workers = 0\nbatch_size = 64\ntransform = transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation(degrees=20),\n                                transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\ntrain_dataset = dataset('../input/train.csv', transform=transform)\ntest_dataset = dataset('../input/test.csv')\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4a2e4d9a99bc19a86d760f0c386a97ba907f48f"},"cell_type":"code","source":"dataiter = iter(train_dl)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(str(labels[idx].item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4b9ed25363b033fa543a4158dfd288b82fd8899"},"cell_type":"code","source":"import torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a30049b6a2aa69e256af14263fc9a6df18ad27a7"},"cell_type":"code","source":"class ConvLayer(nn.Module):\n    def __init__(self, in_channels=1, out_channels=256):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=1, padding=0)\n    def forward(self, x):\n        x = F.relu(self.conv(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cdb6c8a20cabe8ae785ea4bceea1422e81afcbf"},"cell_type":"code","source":"class PrimaryCaps(nn.Module):\n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n        super(PrimaryCaps, self).__init__()\n        self.capsules = nn.ModuleList([\n            nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=2, padding=0)\n            for _ in range(num_capsules)\n        ])\n    def forward(self, x):\n        batch_size = x.size(0)\n        u = [capsule(x).view(batch_size, 32*6*6, 1) for capsule in self.capsules]\n        u = torch.cat(u, dim=-1)\n        u_squashed = self.squash(u)\n        return u_squashed\n    \n    def squash(self, x):\n        squared_norm = (x**2).sum(dim=-1, keepdim=True)\n        scale = squared_norm/(1+squared_norm)\n        output = scale * x/torch.sqrt(squared_norm)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3303b04fded4d9a75900e7cd9576b08ad214af5b"},"cell_type":"code","source":"def softmax(x, dim=1):\n    transposed_inp = x.transpose(dim, len(x.size())-1)\n    softmaxed = F.softmax(transposed_inp.contiguous().view(-1, transposed_inp.size(-1)), dim=-1)\n    return softmaxed.view(*transposed_inp.size()).transpose(dim, len(x.size())-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cf5fe335649261c5b64e625872a3b6521290a88"},"cell_type":"code","source":"def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n    for iterations in range(routing_iterations):\n        c_ij = softmax(b_ij, dim=2)\n        s_j = (c_ij*u_hat).sum(dim=2, keepdim=True)\n        v_j = squash(s_j)\n        if iterations < routing_iterations-1:\n            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n            b_ij = b_ij + a_ij\n    return v_j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93f2c2a737ebe238037ef6e544c7534546fc5472"},"cell_type":"code","source":"TRAIN_ON_GPU = torch.cuda.is_available()\nif TRAIN_ON_GPU: print('training on gpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dc394aa7ea00415c98fceb762f0f88453d70308"},"cell_type":"code","source":"class DigitCaps(nn.Module):\n    def __init__(self, num_caps=10, previous_layer_nodes=32*6*6,\n                 in_channels=8, out_channels=16):\n        super(DigitCaps, self).__init__()\n        self.num_caps = num_caps\n        self.previous_layer_nodes = previous_layer_nodes\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.W = nn.Parameter(torch.randn(num_caps, previous_layer_nodes,\n                                          in_channels, out_channels))\n    \n    def forward(self, x):\n        x = x[None, :, :, None, :]\n        W = self.W[:, None, :, :, :]\n        x_hat = torch.matmul(x, W)\n        b_ij = torch.zeros(*x_hat.size())\n        if TRAIN_ON_GPU: b_ij = b_ij.cuda()\n        v_j = dynamic_routing(b_ij, x_hat, self.squash, routing_iterations=3)\n        return v_j\n    \n    def squash(self, x):\n        squared_norm = (x**2).sum(dim=-1, keepdim=True)\n        scale = squared_norm/(1+squared_norm)\n        out = scale * x/torch.sqrt(squared_norm)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a029fe6c71e9280538fd20a8021fa26611f6a331"},"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n        super(Decoder, self).__init__()\n        input_dim = input_vector_length*input_capsules\n        self.lin_layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim*2),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim*2, 28*28),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        classes = (x**2).sum(dim=-1)**0.5\n        classes = F.softmax(classes, dim=-1)\n        _, max_length_indices = classes.max(dim=1)\n        sparse_matrix = torch.eye(10)\n        if TRAIN_ON_GPU: sparse_matrix = sparse_matrix.cuda()\n        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n        x = x*y[:, :, None]\n        flattened_x = x.view(x.size(0), -1)\n        reconstructed = self.lin_layers(flattened_x)\n        return reconstructed, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6798b1acb1b4513ec0ad8d22c2ff3d15643dfe6"},"cell_type":"code","source":"class CapsuleNetwork(nn.Module):\n    def __init__(self):\n        super(CapsuleNetwork, self).__init__()\n        self.conv_layer = ConvLayer()\n        self.primary_capsule = PrimaryCaps()\n        self.digit_capsule = DigitCaps()\n        self.decoder = Decoder()\n    def forward(self, x):\n        primary_caps_out = self.primary_capsule(self.conv_layer(x))\n        caps_out = self.digit_capsule(primary_caps_out).squeeze().transpose(0, 1)\n        reconstructed, y = self.decoder(caps_out)\n        return caps_out, reconstructed, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa71d20627bcaef9f908cf8da2ce663a48845c6c"},"cell_type":"code","source":"capsule_net = CapsuleNetwork()\n\nprint(capsule_net)\n\nif TRAIN_ON_GPU: capsule_net = capsule_net.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00b2237913c40550e25e2c15c6cc54d06dda52f8"},"cell_type":"code","source":"class CapsuleLoss(nn.Module):\n    def __init__(self):\n        super(CapsuleLoss, self).__init__()\n        self.reconstruction_loss = nn.MSELoss(size_average=False)\n    \n    def forward(self, x, labels, images, reconstructions):\n        batch_size = x.size(0)\n        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n        left = F.relu(0.9-v_c).view(batch_size, -1)\n        right = F.relu(v_c-0.1).view(batch_size, -1)\n        margin_loss = labels * left + 0.5 * (1.-labels) * right\n        margin_loss = margin_loss.sum()\n        images = images.view(reconstructions.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n        return (margin_loss + 0.0005 * reconstruction_loss)/images.size(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90e4dd28ac16df35dd220772c7703db818bf97cf"},"cell_type":"code","source":"import torch.optim as optim\ncriterion = CapsuleLoss()\noptimizer = optim.Adam(capsule_net.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a6963df16722a6b467bbbcb6d9525781684e61"},"cell_type":"code","source":"def train(capsule_net, criterion, optimizer, n_epochs, print_every=300):\n    losses = []\n    for epoch in range(1, n_epochs+1):\n        train_loss = 0.0\n        capsule_net.train() \n        for batch_i, (images, target) in enumerate(train_dl):\n            target = torch.eye(10).index_select(dim=0, index=target)\n            if TRAIN_ON_GPU: images, target = images.cuda(), target.cuda()\n            optimizer.zero_grad()\n            caps_output, reconstructions, y = capsule_net(images)\n            loss = criterion(caps_output, target, images, reconstructions)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            if batch_i != 0 and batch_i % print_every == 0:\n                avg_train_loss = train_loss/print_every\n                losses.append(avg_train_loss)\n                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n                train_loss = 0 \n    return losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a12c81728eb48683380e5c9519e7557f045c788"},"cell_type":"code","source":"n_epochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c714b512a5adc59654d23281f92508638e60d2b8"},"cell_type":"code","source":"losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef442969ca2ee287c19f507096299e9bd04ba7fe"},"cell_type":"code","source":"plt.plot(losses)\nplt.title('Training Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97dd7a6067435c943c11d640ca8eed49845aad72"},"cell_type":"code","source":"out = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec305877f548c583897fab41e8d944cdcfce5b65"},"cell_type":"code","source":"capsule_net.eval()\nfor image in test_dl:\n    if TRAIN_ON_GPU: image = image.cuda()\n    caps_out, reconstructed, y = capsule_net(image)\n    _, pred = torch.max(y.data.cpu(), 1)\n    out.extend(pred.numpy().tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"695e4c26379887a442fb3c0951e0fe49a40c4093"},"cell_type":"code","source":"len(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eccd351d015c21df953b14bdbd77aec1c7e1c4b6"},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c851ee288a9ea951aecb6055469e5a4f1f6376d5"},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8fc6defda9eb33653153a08bc71d13d16c37076"},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8800718c0f82f074b4fd823d7e1b5177f5ab5f03"},"cell_type":"code","source":"sub['Label'] = out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6812de553fe0b076c98c3cc27803a2f93bd84b90"},"cell_type":"code","source":"sub.to_csv('capsule.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}