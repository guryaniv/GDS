{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["Hi there! In this notebook, we will do a little EDA and some baseline classification on the Credit Card Fraud Detection dataset. Bear in mind that this is a work in progress. "], "metadata": {"_cell_guid": "d484c684-873f-4682-86f3-3b0e5e395aef", "_uuid": "3d0a50c5c78ef08330b3cd9180b9b000ee364f44"}}, {"cell_type": "markdown", "source": ["Here we load packages and import data."], "metadata": {"_cell_guid": "44724820-22ab-4ef5-950f-a977de253abe", "_uuid": "d02044c251d1fe335a045a8461f1512fa97b32c8"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "df = pd.read_csv(\"../input/creditcard.csv\")"], "metadata": {"collapsed": true, "_cell_guid": "1178feac-b0a1-48f8-b578-6275ac8658dd", "_uuid": "e47bccfed06c51f3d54ae8a9490a5233ba7b2b87"}}, {"cell_type": "markdown", "source": ["We want to know which variables in the dataset give the best class seperation. To do this, we can simply visualize the distribution of class instances for each variable. This will allow us to discard ones that have poor seperation right off the bat. A more complex model would take into accound class seperation in conditional distributions, but for what we want to accomplish right now class independence is a fine assumption."], "metadata": {"_cell_guid": "ea1c92f7-0b55-403c-835e-0c7726b44fb4", "_uuid": "88e82eb60c07ef584bcc8b582e1da11cd650a219"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["n_features = len(df.columns)\n", "n_rows, n_cols = 6, 6\n", "\n", "fig, ax = plt.subplots(n_rows, n_cols, figsize=(15,15))\n", "\n", "feature = 0\n", "for i in range(n_rows):\n", "    for j in range(n_cols):\n", "        if feature > n_features - 1:\n", "            ax[i,j].plot()\n", "        else:\n", "            fraud = df.iloc[:,feature][df[\"Class\"] == 1].values\n", "            non_fraud = df.iloc[:,feature][df[\"Class\"] == 0].values\n", "\n", "            ax[i,j].hist(non_fraud, \n", "                     color=\"blue\", \n", "                     weights=np.zeros_like(non_fraud) + 1. / non_fraud.size,\n", "                     bins=15,\n", "                     alpha=0.5)\n", "            ax[i,j].hist(fraud, \n", "                     color=\"red\", \n", "                     weights=np.zeros_like(fraud) + 1. / fraud.size,\n", "                     bins=8,\n", "                     alpha=0.5)\n", "            ax[i,j].set_title(df.columns[feature])\n", "            \n", "        feature += 1\n", "    if feature > n_features:\n", "        break\n", "\n", "plt.tight_layout()\n", "plt.show()"], "metadata": {"_cell_guid": "4fc22484-2421-4649-b568-541a71eb56e5", "_uuid": "5df76c54898e71800ed732be6e0f7ef3e163239a"}}, {"cell_type": "markdown", "source": ["Out of curiosity, let's see what a subset of these points looks like in the input space. "], "metadata": {"_cell_guid": "7bc784f8-3311-43b0-9ee1-bfe4d3818a11", "_uuid": "8296cd095dc49918bf3617c989a107138a734479"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["selected_features = [\"V3\", \"V4\", \"V10\", \"V11\", \"V12\", \"V14\", \"V16\", \"V17\", \"Class\"]\n", "idx = np.random.choice(np.arange(len(df)),10000)\n", "\n", "df_vis = df.iloc[idx,:]\n", "\n", "fig, ax = plt.subplots(len(selected_features), len(selected_features), figsize=(15,15), sharex=True, sharey=True)\n", "for i, featurei in enumerate(selected_features):\n", "    for j, featurej in enumerate(selected_features):\n", "        fraud = df_vis.loc[:,[featurei, featurej]][df[\"Class\"] == 1].values\n", "        non_fraud = df_vis.loc[:,[featurei, featurej]][df[\"Class\"] == 0].values\n", "        \n", "        if i == j: \n", "            ax[i,j].hist(non_fraud[:,0], color=\"blue\", alpha=0.5, weights=np.zeros_like(non_fraud[:,0]) + 1. / non_fraud[:,0].size)\n", "            ax[i,j].hist(fraud[:,0], color=\"red\", alpha=0.5, weights=np.zeros_like(fraud[:,0]) + 1. / fraud[:,0].size)\n", "        else:\n", "            ax[i,j].scatter(non_fraud[:,0], non_fraud[:,1], color=\"blue\", marker='.', alpha=0.5)\n", "            ax[i,j].scatter(fraud[:,0], fraud[:,1], color=\"red\",  marker='.', alpha=0.5)\n", "                \n", "plt.show()"], "metadata": {"_cell_guid": "31d1acda-01b5-4f2d-afb5-35dbbf142078", "_uuid": "147db3cc7025e3aba1c77551d6a32c48bc35d5a5"}}, {"cell_type": "markdown", "source": ["For baseline classification, we need to split the data into training and testing sets. We will use a relatively high test proportion due to the highly unbalanced nature of the dataset. "], "metadata": {"_cell_guid": "c3c29dfb-729e-4383-95ad-d80446bfd64f", "_uuid": "e14f33b6e66ff9d014793dd0142cefafd2a9e7e5"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["test_proportion = 0.4\n", "\n", "m = len(df)\n", "test_size = int(test_proportion * m)\n", "indices = np.random.permutation(m)\n", "train_indices, test_indices = indices[test_size:], indices[:test_size]\n", "\n", "train =  df.iloc[train_indices,:]\n", "test = df.iloc[test_indices,:]\n", "\n", "train = train.loc[:,selected_features]\n", "test = test.loc[:,selected_features]\n", "\n", "train_data = train.loc[:, train.columns != \"Class\"]\n", "train_data = train_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n", "train_label = train[\"Class\"].values\n", "\n", "test_data = test.loc[:, test.columns != \"Class\"]\n", "test_data = test_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n", "test_label = test[\"Class\"].values"], "metadata": {"_cell_guid": "c0f3d280-4549-4773-b464-7fc79fdee93f", "_uuid": "487a2db4b95fc1e477a211bc43a20cca11291175"}}, {"cell_type": "markdown", "source": ["As a baseline, we will see how a logistic regression performs."], "metadata": {"_cell_guid": "4a839333-84e2-4f51-8c0c-7af565c93f29", "_uuid": "5609aed7d5741b8bc1bfebd2792c95e1959a3073"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import average_precision_score, precision_recall_curve\n", "\n", "model = LogisticRegression()\n", "model.fit(X = train_data, y = train_label)\n", "\n", "prediction = model.decision_function(test_data)\n", "auprc = average_precision_score(test_label, prediction)\n", "\n", "precision, recall, _ = precision_recall_curve(test_label, prediction)\n", "\n", "plt.plot(recall, precision)\n", "plt.xlabel(\"Recall\")\n", "plt.ylabel(\"Precision\")\n", "plt.grid()\n", "plt.show()\n", "\n", "print(\"Area under PR curve:\", auprc)"], "metadata": {"_cell_guid": "c5fc377e-d570-4653-937c-d068e131cbe5", "_uuid": "34ea128735c49e16dbb03522d3be14919fdbae22"}}, {"cell_type": "markdown", "source": ["The support vector machine with RBF kernel performs about the same as the LR."], "metadata": {"_cell_guid": "af16d498-55a6-4c0a-8022-d1422b6abced", "_uuid": "060f40bfbee39ec0039d0146060893b5a894ca1d"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["from sklearn.svm import SVC\n", "from sklearn.metrics import average_precision_score, precision_recall_curve\n", "\n", "model = SVC()\n", "model.fit(X = train_data, y = train_label)\n", "\n", "prediction = model.decision_function(test_data)\n", "auprc = average_precision_score(test_label, prediction)\n", "\n", "precision, recall, _ = precision_recall_curve(test_label, prediction)\n", "\n", "plt.plot(recall, precision)\n", "plt.xlabel(\"Recall\")\n", "plt.ylabel(\"Precision\")\n", "plt.grid()\n", "plt.show()\n", "\n", "print(\"Area under PR curve:\", auprc)"], "metadata": {"_cell_guid": "c4ac2c67-4017-4778-a87b-7312d7b1dc89", "_uuid": "23b581f88c7bd6ac9fdd40e7222bd7e075b1985e"}}, {"cell_type": "markdown", "source": ["A more complex model may perform better. To test this, we will train a neural network on ALL of the inputs in the dataset, not just the few we selected earlier. "], "metadata": {"_cell_guid": "99bfd3ed-71e5-405d-9cf4-a941987efaa4", "_uuid": "fbe8944d6941df45f38e2f255b0ba68f4f4d3018"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["test_proportion = 0.4\n", "\n", "m = len(df)\n", "test_size = int(test_proportion * m)\n", "indices = np.random.permutation(m)\n", "train_indices, test_indices = indices[test_size:], indices[:test_size]\n", "\n", "train =  df.iloc[train_indices,:]\n", "test = df.iloc[test_indices,:]\n", "\n", "# train = train.loc[:,selected_features]\n", "# test = test.loc[:,selected_features]\n", "\n", "train_data = train.loc[:, train.columns != \"Class\"]\n", "train_data = train_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n", "train_label = train[\"Class\"].values\n", "\n", "test_data = test.loc[:, test.columns != \"Class\"]\n", "test_data = test_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n", "test_label = test[\"Class\"].values"], "metadata": {"collapsed": true, "_cell_guid": "7a7e33d4-8a53-4f11-9192-00633e62d12e", "_uuid": "a6a063fe2b4e833dd119dcce061c88baf68b75db"}}, {"cell_type": "markdown", "source": ["These are just a few helper functions to feed data into the network. "], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["def to_one_hot(c, depth):\n", "    I = np.identity(depth)\n", "    return I[c,:]\n", "\n", "def train_batch(batch_size):\n", "    for j in range(int(len(train_data)/batch_size)):\n", "        start = batch_size*j\n", "        end = start + batch_size\n", "        \n", "        train_data_batch = train_data[start:end].values\n", "        train_label_batch = train_label[start:end]\n", "        \n", "        train_label_batch = np.apply_along_axis(lambda x: to_one_hot(x, depth=2), 0, train_label_batch)\n", "        \n", "        yield train_data_batch, train_label_batch\n", "        \n", "def get_test_data():\n", "    return test_data.values, np.apply_along_axis(lambda x: to_one_hot(x, depth=2), 0, test_label)"], "metadata": {"collapsed": true, "_cell_guid": "9fdc45f0-e356-4ee4-988d-b4253bc9c073", "_uuid": "6a22fbf6880f6f9cb7bcac8040f81f654d19b078"}}, {"cell_type": "markdown", "source": ["In this TensorFlow session, we train a relatively simple network containing just one small hidden layer, and evaluate its performance using average precision, like the other classifiers. "], "metadata": {}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": ["import tensorflow as tf\n", "from sklearn.metrics import average_precision_score, precision_recall_curve\n", "\n", "x = tf.placeholder(tf.float32, [None, 30]) # inputs\n", "t = tf.placeholder(tf.float32, [None, 2]) # targets\n", "\n", "def predict(x): # model\n", "    size_l1 = 10\n", "    weights = {\"l1\": tf.Variable(tf.random_normal([30, size_l1])), \n", "               \"output\": tf.Variable(tf.random_normal([size_l1, 2]))}\n", "    \n", "    biases = {\"l1\": tf.Variable(tf.random_normal([size_l1])), \n", "               \"output\": tf.Variable(tf.random_normal([2]))}\n", "    \n", "    h1 = tf.add(tf.matmul(x, weights[\"l1\"]), biases[\"l1\"])\n", "    h1 = tf.nn.relu(h1)\n", "    \n", "    h2 = tf.add(tf.matmul(h1, weights[\"output\"]), biases[\"output\"])\n", "    output = tf.nn.relu(h2)\n", "    \n", "    return output\n", "\n", "y = predict(x) # logits\n", "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=y)) # loss \n", "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss) \n", "\n", "correct = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1)) \n", "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # confusion matrix accuracy\n", "\n", "decision_variable = tf.nn.softmax(y) # for precision-recall curve\n", "\n", "with tf.Session() as sess: \n", "    sess.run(tf.global_variables_initializer())\n", "    \n", "    # train phase\n", "    batch_size = 128\n", "    n_epochs = 3\n", "    for epoch in range(n_epochs):\n", "        epoch_loss = 0\n", "        batch_generator = train_batch(batch_size)\n", "        for batch in batch_generator:\n", "            batch_x, batch_t = batch \n", "            _, curr_loss = sess.run([optimizer, loss], feed_dict={t: batch_t, x: batch_x})\n", "            epoch_loss += curr_loss\n", "            \n", "        print(\"Epoch \" + str(epoch+1) + \" loss: \" + str(epoch_loss))\n", "        \n", "    # test phase\n", "    test_x, test_t = get_test_data()\n", "    test_y = sess.run(decision_variable, feed_dict={x: test_x})\n", "    \n", "    auprc = average_precision_score(test_t[:,0], test_y[:,0])\n", "    precision, recall, _ = precision_recall_curve(test_t[:,0], test_y[:,0])\n", "\n", "    # plot pr curve\n", "    plt.plot(recall, precision)\n", "    plt.grid()\n", "    plt.show()\n", "    \n", "    # print average precision\n", "    print(auprc)"], "metadata": {"_cell_guid": "3e4b1309-a363-4832-a83d-29f5e162d097", "_uuid": "ae4727fd8a2a7d97c1d11d1dfa78d03db700149e"}}, {"cell_type": "markdown", "source": ["Clearly, the network easily outperforms other classifiers when it comes to fraud detection."], "metadata": {"collapsed": true, "_cell_guid": "0a8d3a0f-db65-48a9-990e-7fb9363995bc", "_uuid": "13a956f78731666c3d5b3e30e1f00b3168d0bf7d"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "source": [], "metadata": {"collapsed": true}}]}