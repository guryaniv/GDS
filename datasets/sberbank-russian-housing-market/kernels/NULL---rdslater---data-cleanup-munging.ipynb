{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f5175694-1341-c418-79e3-d37f238c6619"
      },
      "source": [
        "Data Cleanup--our Favorite Topic (NOT!).  Lots of missing data.  Early on we just ignored, but if you use a tool other than XGBoost or Vowpal Wabbit, those things need to be fit in.  Here's what I've been up to since I can 'beat the standandard'\n",
        "\n",
        "\n",
        "First lets put things in a big happy data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b4894c7-ac05-5c86-2e3f-5c736e8e71ae"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "train=pd.read_csv(\"../input/train.csv\")\n",
        "test=pd.read_csv(\"../input/test.csv\")\n",
        "n=train.shape[0]\n",
        "test.price_doc=np.nan\n",
        "ids=test['id']\n",
        "target=train.price_doc\n",
        "train=train.append(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ada253ab-f909-1e4c-02e7-66aaa253364d"
      },
      "source": [
        "Fill in the floor values--rather than do it 'generally' I thought a better guess would be to take statistics from the local area.\n",
        "\n",
        "Yes I probably should have wrote a function, but I just copied and pasted as a whittled down the missing data.  \n",
        "\n",
        "Notice the exception--that subarea didn't  have enough data, so I switched to median over 'mode'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b3c83b31-0a5f-cc2b-f90e-069a58b53bdf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "a=train.sub_area.unique()\n",
        "for i in a:\n",
        "    if train[(train['floor'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "        train.loc[(train['floor'].isnull()) & (train['sub_area']==i),'floor']=train.loc[(train['floor'].notnull()) & (train['sub_area']==i),'floor'].mode().values[0]\n",
        "print('floors done',train['product_type'].value_counts())\n",
        "#fill in nans for max_floor\n",
        "for i in a:\n",
        "    if train[(train['max_floor'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "        if i=='Poselenie Shhapovskoe':\n",
        "            train.loc[(train['max_floor'].isnull()) & (train['sub_area']==i),'max_floor']=train.loc[(train['max_floor'].notnull()) & (train['sub_area']==i),'max_floor'].median()\n",
        "        else:\n",
        "            train.loc[(train['max_floor'].isnull()) & (train['sub_area']==i),'max_floor']=train.loc[(train['max_floor'].notnull()) & (train['sub_area']==i),'max_floor'].mode().values[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d13d82bd-10da-740d-7119-5b7a6f890e48"
      },
      "source": [
        "Do the same thing for materials, build year and 'state'  Again some areas were short on Data, So i put them in by hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "76ba20c5-8680-d199-d319-2c9486fdc2bf"
      },
      "outputs": [],
      "source": [
        "#materials\n",
        "\n",
        "for i in a:\n",
        "    if train[(train['material'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "        train.loc[(train['material'].isnull()) & (train['sub_area']==i),'material']=train.loc[(train['material'].notnull()) & (train['sub_area']==i),'material'].mode().values[0]\n",
        "\n",
        "#build year\n",
        "for i in a:\n",
        "    if train[(train['build_year'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "        if i=='Poselenie Voronovskoe':\n",
        "            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=2014\n",
        "        elif i=='Poselenie Shhapovskoe':\n",
        "            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=2011\n",
        "        else:\n",
        "            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=train.loc[(train['build_year'].notnull()) & (train['sub_area']==i),'build_year'].mode().values[0]\n",
        "\n",
        "#state\n",
        "for i in a:\n",
        "    if train[(train['state'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "        if (i=='Poselenie Klenovskoe' or i=='Poselenie Kievskij'):\n",
        "            train.loc[(train['state'].isnull()) & (train['sub_area']==i),'state']=2\n",
        "        else:\n",
        "            train.loc[(train['state'].isnull()) & (train['sub_area']==i),'state']=train.loc[(train['state'].notnull()) & (train['sub_area']==i),'state'].mode().values[0]\n",
        "\n",
        "#and now the rail stations\n",
        "cols=['ID_railroad_station_walk','ID_railroad_station_avto','green_part_2000','metro_km_walk','metro_min_walk']\n",
        "for j in cols:\n",
        "    for i in a:\n",
        "        if train[(train[j].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "            train.loc[(train[j].isnull()) & (train['sub_area']==i),j]=train.loc[(train[j].notnull()) & (train['sub_area']==i),j].mode().values[0]\n",
        "cols=['railroad_station_walk_km','railroad_station_walk_min']\n",
        "for j in cols:\n",
        "    for i in a:\n",
        "        if train[(train[j].isnull()) & (train['sub_area']==i)].shape[0]>0:\n",
        "            train.loc[(train[j].isnull()) & (train['sub_area']==i),j]=train.loc[(train[j].notnull()) & (train['sub_area']==i),j].median()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c900653b-44ef-0581-883d-c34579ff7fdf"
      },
      "source": [
        "Now for the square footage I tried to do a linear fit as there is a fairly strong relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "edd36fa7-f11f-bffa-6d8c-dd73900130eb"
      },
      "outputs": [],
      "source": [
        "#doing things a bit differnt for rooms, base it off sq ft.\n",
        "x=train.loc[(train.full_sq.notnull())&(train.num_room.notnull()),'full_sq']\n",
        "y=train.loc[(train.full_sq.notnull())&(train.num_room.notnull()),'num_room']\n",
        "rooms=np.polyfit(x,y,1)\n",
        "train.loc[train['num_room'].isnull(),'num_room']=train.loc[train['num_room'].isnull(),'full_sq']*rooms[0]+rooms[1]\n",
        "\n",
        "#kitchen space will be same\n",
        "x=train.loc[(train.kitch_sq.notnull())&(train.kitch_sq.notnull()),'full_sq']\n",
        "y=train.loc[(train.kitch_sq.notnull())&(train.kitch_sq.notnull()),'kitch_sq']\n",
        "kitch=np.polyfit(x,y,1)\n",
        "train.loc[train['kitch_sq'].isnull(),'kitch_sq']=train.loc[train['kitch_sq'].isnull(),'full_sq']*kitch[0]+kitch[1]\n",
        "\n",
        "#and fix up the life-sq\n",
        "x=train.loc[(train.full_sq.notnull())&(train.life_sq.notnull()),'full_sq']\n",
        "y=train.loc[(train.full_sq.notnull())&(train.life_sq.notnull()),'life_sq']\n",
        "life=np.polyfit(x,y,1)\n",
        "train.loc[train['life_sq'].isnull(),'life_sq']=train.loc[train['life_sq'].isnull(),'full_sq']*life[0]+life[1]\n",
        "#fix some of the off values\n",
        "train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>1000),'life_sq']=train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>1000),'life_sq']/1000\n",
        "train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>106),'life_sq']=train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>106),'life_sq']/100\n",
        "print('fits done',train['product_type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1ce17efe-9cd0-ccf5-b2fb-31e5c7619459"
      },
      "source": [
        "Now for city-region data I just do the median values--I tried to do some clustering, but didn't have the best of luck and was unsatisfied with the results.\n",
        "\n",
        "For Hospitals / schools I tried to do a fit of the children to school for known values.\n",
        "\n",
        "Also the test data was missing some product types.  Based on the data population, I called them 'Investments'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37d08cf8-c4f1-7b7a-e0af-c31ee68b1844"
      },
      "outputs": [],
      "source": [
        "\n",
        "#city /region data\n",
        "cols=['build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995','build_count_before_1920',\n",
        "'build_count_block','build_count_brick','build_count_foam','build_count_frame','build_count_mix','build_count_monolith',\n",
        "'build_count_panel','build_count_slag','build_count_wood','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\n",
        "'cafe_avg_price_3000','cafe_avg_price_500','cafe_avg_price_5000','cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg',\n",
        "'cafe_sum_1500_max_price_avg','cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\n",
        "'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg','cafe_sum_5000_min_price_avg',\n",
        "'cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg','raion_build_count_with_builddate_info','raion_build_count_with_material_info','prom_part_5000']\n",
        "for j in cols:\n",
        "    train.loc[train[j].isnull(),j]=train.loc[train[j].notnull(),j].median()\n",
        "            \n",
        "print('city data done',train['product_type'].value_counts())\n",
        "\n",
        "#hospital and preschoo\n",
        "i='raion_popul'\n",
        "j='hospital_beds_raion'\n",
        "x=train.loc[(train[i].notnull())&(train[j].notnull()),i]\n",
        "y=train.loc[(train[i].notnull())&(train[j].notnull()),j]\n",
        "fit=np.polyfit(x,y,1)\n",
        "train.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n",
        "\n",
        "i='children_preschool'\n",
        "j='preschool_quota'\n",
        "x=train.loc[(train[i].notnull())&(train[j].notnull()),i]\n",
        "y=train.loc[(train[i].notnull())&(train[j].notnull()),j]\n",
        "fit=np.polyfit(x,y,1)\n",
        "train.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n",
        "\n",
        "i='children_school'\n",
        "j='school_quota'\n",
        "x=train.loc[(train[i].notnull())&(train[j].notnull()),i]\n",
        "y=train.loc[(train[i].notnull())&(train[j].notnull()),j]\n",
        "fit=np.polyfit(x,y,1)\n",
        "train.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n",
        "\n",
        "train.loc[train['product_type'].isnull(),'product_type']='Investment'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21468454-1638-a837-19ec-573b6c26e0d3"
      },
      "source": [
        "Now its time to turn everything into numbers.  I hand-coded the ecology--this was one of my first columns I went after. Then I one-hot encode the sub-area and materials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b2c4547-db11-8f3d-715d-7f5268b3ad65"
      },
      "outputs": [],
      "source": [
        "binary=[]\n",
        "for i in train:\n",
        "    if train[i].dtypes=='object':\n",
        "        #print(train[i].value_counts())\n",
        "        if train[i].value_counts().shape[0]==2:\n",
        "            binary.append(i)\n",
        "for i in binary:\n",
        "    train[i]=pd.factorize(train[i])[0]\n",
        "#change the echology to a 1-4 and ohe for NANs\n",
        "train.loc[train['ecology']=='no data','ecology_dat']=0\n",
        "train.loc[train['ecology']!='no data','ecology_dat']=1\n",
        "train.loc[train['ecology']=='no data','ecology']=2\n",
        "train.loc[train['ecology']=='poor','ecology']=1\n",
        "train.loc[train['ecology']=='satisfactory','ecology']=2\n",
        "train.loc[train['ecology']=='good','ecology']=3\n",
        "train.loc[train['ecology']=='excellent','ecology']=4\n",
        "train.ecology=pd.to_numeric(train.ecology)\n",
        "#sub_area to ohe\n",
        "train=pd.concat([train,pd.get_dummies(train.sub_area)],axis=1)\n",
        "train=pd.concat([train,pd.get_dummies(train.material,prefix='material')],axis=1)\n",
        "train.drop(['sub_area','material'],inplace=True,axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c65d2b4b-07a0-92ec-bd47-2d1aa6080f25"
      },
      "source": [
        "Do some final cleanup and split into test/train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f4e96e0-8dff-b726-d1d2-33cfdb1a5354"
      },
      "outputs": [],
      "source": [
        "train.drop(['price_doc','id'],inplace=True,axis=1)\n",
        "\n",
        "for i in train:\n",
        "    out=train[i].isnull().sum()\n",
        "    if out>0:\n",
        "        print(i)\n",
        "    \n",
        "test=train.iloc[n:,]\n",
        "train=train.iloc[0:n,]\n",
        "print(test.shape,train.shape,n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d5b8dac-08a7-3064-46a5-cb243b4e443d"
      },
      "source": [
        "I know many people prefer function programming, but this was I was able to find out when there were fallouts (areas with not enough data).  Also when you get in the grove you start cut-n-paste your code and substituting in new variables.\n",
        "\n",
        "Best of luck--"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}