{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### TODO List\n", "\n", "* [ ] Clustering analysis on test dataset to take advantage of those observations.\n", "* [ ] Impute missing incidence angle information.\n", "\n", "This specific competition seems a perfect use case for CNNs/image detection. But right now I am working on my feature engineering capabilities and am improving my understanding of logistic regression, so I want to see what I can accomplish with logistic regression alone. Let's begin.\n", "\n", "## Data Exploration"]}, {"cell_type": "code", "metadata": {"_cell_guid": "8c2e30b7-ed21-4162-9e62-59dcf3872e59", "collapsed": true, "_uuid": "07b5314de9f06e813145bc53eb81420320fa8cef"}, "outputs": [], "source": ["# import some packages\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline"], "execution_count": 1}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# let's load in the data and look around\n", "df_train = pd.read_json('../input/train.json')\n", "df_train.info()"], "execution_count": 6}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like we have 1604 images where whether or not the image contains an iceberg is labeled. The only features are the HH and HV bands, the image id, incidence angle (note: has missing values ... that's why it shows up as an object instead of a numeric), and whether or not there is an iceberg.\n", "\n", "Now let's look at the test dataset as well."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_json('../input/test.json')\n", "df_test.info()"], "execution_count": 7}, {"cell_type": "markdown", "metadata": {}, "source": ["Five times as many test images! With this ratio, it would no doubt be helpful to do some unsupervised clustering on the test dataset to improve our model. I'll add that to the to do list.\n", "\n", "No lat/lon information, time of year, etc. that might have proved useful. All the features to engineer will have to come from those radar echos.\n", "\n", "Thanks to [MuonNeutrino](https://www.kaggle.com/muonneutrino/exploration-transforming-images-in-python), an excellent starting point is investigating the statistical properties of the two separate bands."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# coerce incidence angle to numeric\n", "df_train['inc_angle'] = pd.to_numeric(df_train['inc_angle'], errors='coerce')\n", "\n", "# combine training and test set for feature engineering\n", "df_full = pd.concat([df_train, df_test], axis=0, ignore_index=True)"], "execution_count": 11}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["def get_stats(df, label=1):\n", "    df['max'+str(label)] = [np.max(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['min'+str(label)] = [np.min(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['minpos'+str(label)] = [np.argmin(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['med'+str(label)] = [np.median(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['std'+str(label)] = [np.std(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['mean'+str(label)] = [np.mean(np.array(x)) for x in df['band_'+str(label)] ]\n", "    df['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in df['band_'+str(label)] ]\n", "    df['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in df['band_'+str(label)] ]\n", "    df['mid50_'+str(label)] = df['p75_'+str(label)]-df['p25_'+str(label)]\n", "\n", "    return df\n", "\n", "df_full = get_stats(df_full, 1)\n", "df_full = get_stats(df_full, 2)"], "execution_count": 14}, {"cell_type": "code", "metadata": {"scrolled": false}, "outputs": [], "source": ["def plot_var(name, nbins=50):\n", "    minval = df_full[name].min()\n", "    maxval = df_full[name].max()\n", "    plt.hist(df_full.loc[df_full.is_iceberg==1,name],range=[minval,maxval],\n", "             bins=nbins,color='b',alpha=0.5,label='Boat')\n", "    plt.hist(df_full.loc[df_full.is_iceberg==0,name],range=[minval,maxval],\n", "             bins=nbins,color='r',alpha=0.5,label='Iceberg')\n", "    plt.legend()\n", "    plt.xlim([minval, maxval])\n", "    plt.xlabel(name)\n", "    plt.ylabel('Number')\n", "    plt.show()\n", "    \n", "for col in ['inc_angle','min1','max1','std1','med1','mean1','mid50_1', 'p25_1', 'p75_1', 'minpos1', 'maxpos1']:\n", "    plot_var(col)"], "execution_count": 19}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like the minimum, maximum, median, and mean reflectivities are useful features ... particularly the maximum reflectivity! This makes intuitive sense. The icebergs are scattering back more information to the radar. Also the first and third quartile features are useful.\n", "\n", "Now let's look at the second band."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for col in ['min2','max2','std2','med2','mean2','mid50_2','p25_2', 'p75_2']:\n", "    plot_var(col)"], "execution_count": 20}, {"cell_type": "markdown", "metadata": {}, "source": ["Interesting. The minimum reflectivity doesn't appear useful at all for band_2. However, the maximum reflectivity is! Additionally, we can almost guarantee that if the standard deviation of the values is over 3 then the image contains an iceberg.\n", "\n", "So far, it seems that the features most usefull are:\n", "* min1, max1, std1, med1, mean1, p25_1, p75_1 (from band_1)\n", "* max2, std2 (from band_2)\n", "\n", "Now let's look at the correlation between these features:"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df_full_stats = df_full.drop(['id','is_iceberg','band_1','band_2', 'inc_angle'], axis=1)\n", "corr = df_full_stats.corr()\n", "fig = plt.figure(1, figsize=(10,10))\n", "plt.imshow(corr,cmap='inferno')\n", "labels = np.arange(len(df_full_stats.columns))\n", "plt.xticks(labels,df_full_stats.columns,rotation=90)\n", "plt.yticks(labels,df_full_stats.columns)\n", "plt.title('Correlation Matrix of Global Variables')\n", "cbar = plt.colorbar(shrink=0.85,pad=0.02)\n", "plt.show()"], "execution_count": 21}, {"cell_type": "markdown", "metadata": {}, "source": ["Unfortunately max1 and max2 are pretty highly correlated, so we might not get much additional information including both in there. The minimums aren't too correlated with the maximums in both bands, however, so each might be useful.\n", "\n", "Going back to the value of the maximum reflectivity, let's see if the number of pixels above some threshold could be useful."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["np_test = np.array(df_full.loc[1, 'band_1'])"], "execution_count": 43}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["len(np_test[np_test > -10])"], "execution_count": 45}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["def get_threshold_size(df, label=1, threshold=-15):\n", "    df['gt_'+str(threshold)+'_'+str(label)] = [len(np.array(x)[np.array(x) > threshold]) for x in df_full['band_'+str(label)]]\n", "    \n", "    return df"], "execution_count": 59}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for lbl in range(1, 3):\n", "    for thr in range(-15, 35, 5):\n", "        get_threshold_size(df_full, label=lbl, threshold=thr)"], "execution_count": 60}, {"cell_type": "code", "metadata": {"scrolled": false}, "outputs": [], "source": ["for col in ['gt_-15_1', 'gt_-10_1', 'gt_-5_1', 'gt_0_1', 'gt_5_1', 'gt_10_1', 'gt_15_1',\n", "            'gt_20_1', 'gt_25_1', 'gt_30_1']:\n", "    plot_var(col)"], "execution_count": 61}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like the thresholds beyond 10 aren't useful."]}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["df_full.drop(['gt_15_1', 'gt_20_1', 'gt_25_1', 'gt_30_1'], axis=1, inplace=True)"], "execution_count": 63}, {"cell_type": "code", "metadata": {"scrolled": false}, "outputs": [], "source": ["for col in ['gt_-15_2', 'gt_-10_2', 'gt_-5_2', 'gt_0_2', 'gt_5_2', 'gt_10_2', 'gt_15_2',\n", "            'gt_20_2', 'gt_25_2', 'gt_30_2']:\n", "    plot_var(col)"], "execution_count": 62}, {"cell_type": "markdown", "metadata": {}, "source": ["For band 2, the thresholds beyond -5 aren't useful."]}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["df_full.drop(['gt_0_2', 'gt_5_2', 'gt_10_2', 'gt_15_2', 'gt_20_2', 'gt_25_2', 'gt_30_2'], \n", "             axis=1, inplace=True)"], "execution_count": 64}, {"cell_type": "markdown", "metadata": {}, "source": ["At this point let's remind ourselves all of the columns we have. And drop features we have deemed not useful."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df_full.info()"], "execution_count": 66}, {"cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["df_full.drop(['inc_angle', 'maxpos1', 'minpos1', 'maxpos2', 'minpos2'], axis=1, inplace=True)"], "execution_count": 67}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df_full.info()"], "execution_count": 68}, {"cell_type": "markdown", "metadata": {}, "source": ["## Logistic Regression\n", "\n", "Now that we have a bunch of numeric features, let's perform a regression model."]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["X = df_full.drop(['band_1', 'band_2', 'id', 'is_iceberg'], axis=1)[:1604]\n", "y = df_full['is_iceberg'][:1604]"], "execution_count": 78}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["# sklearn imports\n", "from sklearn.cross_validation import StratifiedKFold\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import log_loss\n", "from sklearn.metrics import classification_report\n", "\n", "skf = StratifiedKFold(y, n_folds=3)"], "execution_count": 93}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["for train_index, test_index in skf:\n", "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n", "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n", "    logmodel = LogisticRegression()\n", "    logmodel.fit(X_train, y_train)\n", "    predictions = logmodel.predict(X_test)\n", "    predictions_prob = logmodel.predict_proba(X_test)[:,1]\n", "    print(classification_report(y_test, predictions))\n", "    print(log_loss(y_test, predictions_prob))"], "execution_count": 94}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like we should expect a loss near 0.4. Let's submit it and see what happens!"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["X_test = df_full.drop(['band_1', 'band_2', 'id', 'is_iceberg'], axis=1)[1604:]\n", "logmodel = LogisticRegression()\n", "logmodel.fit(X, y)\n", "predictions_prob = logmodel.predict_proba(X_test)[:,1]"], "execution_count": 96}, {"cell_type": "code", "metadata": {}, "outputs": [], "source": ["df_predictions = pd.DataFrame({'id' : df_full['id'][1604:], 'is_iceberg' : predictions_prob})\n", "df_predictions.to_csv('logistic_regression_submission.csv', index=False)"], "execution_count": 102}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}