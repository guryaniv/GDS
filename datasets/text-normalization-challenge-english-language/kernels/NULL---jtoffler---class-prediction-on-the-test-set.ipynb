{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": ["# Class Prediction on the Test Set\n", "\n", "After running BingQing Wei's \"XGBoost with Context\" kernel, you should have a new file in your working directory called xgb_model.  Below is code you can use to produce a file called \"classes.csv\".  Once you have this file, you should be able to run another one of my kernels, \"Better Late Than Never\", which will produce an output file that you can submit.  If you use his model out of the box you should get a LB score of .9910.  If you change boosts (the third parameter in the line that starts with \"model = xgb.train(...)\") to 220 instead of 50, you should get a LB score of .9911.  Note that changing from 50 to 220 will make the runtime significantly longer."]}, {"outputs": [], "metadata": {"_cell_guid": "e5d4a529-673a-4a2f-9e53-c1d89258d4b4", "collapsed": true, "_uuid": "378f34c60320c764b06dee5a393fa46c9bf90a69"}, "cell_type": "code", "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "import os\n", "import pickle\n", "import gc\n", "import xgboost as xgb\n", "import numpy as np\n", "import re\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "\n", "max_num_features = 10\n", "pad_size = 1\n", "boundary_letter = -1\n", "space_letter = 0\n", "#max_data_size = 320000\n", "\n", "out_path = r'.'\n", "df = pd.read_csv(r'en_test_2.csv')\n", "\n", "x_data = []\n", "#y_data =  pd.factorize(df['class'])\n", "#labels = y_data[1]\n", "#y_data = y_data[0]\n", "gc.collect()\n", "for x in df['before'].values:\n", "    x_row = np.ones(max_num_features, dtype=int) * space_letter\n", "    for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n", "        x_row[i] = ord(xi)\n", "    x_data.append(x_row)\n", "\n", "def context_window_transform(data, pad_size):\n", "    pre = np.zeros(max_num_features)\n", "    pre = [pre for x in np.arange(pad_size)]\n", "    data = pre + data + pre\n", "    neo_data = []\n", "    for i in np.arange(len(data) - pad_size * 2):\n", "        row = []\n", "        for x in data[i : i + pad_size * 2 + 1]:\n", "            row.append([boundary_letter])\n", "            row.append(x)\n", "        row.append([boundary_letter])\n", "        neo_data.append([int(x) for y in row for x in y])\n", "    return neo_data\n", "\n", "#x_data = x_data[:max_data_size]\n", "#y_data = y_data[:max_data_size]\n", "x_data = np.array(context_window_transform(x_data, pad_size))\n", "gc.collect()\n", "x_data = np.array(x_data)\n", "#y_data = np.array(y_data)\n", "\n", "print('Total number of samples:', len(x_data))\n", "#print('Use: ', max_data_size)\n", "#x_data = np.array(x_data)\n", "#y_data = np.array(y_data)\n", "\n", "print('x_data sample:')\n", "print(x_data[0])\n", "#print('y_data sample:')\n", "#print(y_data[0])\n", "#print('labels:')\n", "#print(labels)\n", "\n", "model = xgb.Booster({'nthread': 4})\n", "model.load_model('xgb_model')\n", "\n", "labels = [u'PLAIN', u'PUNCT', u'DATE', u'LETTERS', u'CARDINAL', u'VERBATIM',\n", "       u'DECIMAL', u'MEASURE', u'MONEY', u'ORDINAL', u'TIME', u'ELECTRONIC',\n", "       u'DIGIT', u'FRACTION', u'TELEPHONE', u'ADDRESS']\n", "\n", "dtest = xgb.DMatrix(x_data)\n", "preds = model.predict(dtest)\n", "preds2 = [labels[int(x)] for x in preds]\n", "preds3 = np.array(preds2)\n", "preds4 = pd.DataFrame(preds3.reshape(len(preds3), 1))\n", "preds4.to_csv(os.path.join(out_path, 'classes.csv'))\n", "\n"]}], "nbformat_minor": 1, "nbformat": 4}