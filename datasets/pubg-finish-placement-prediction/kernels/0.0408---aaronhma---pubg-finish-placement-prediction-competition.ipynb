{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# &copy; Copyright 2018 - Present Aaron Ma(10-years old). All Rights Reserved. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Jupyter Specific Packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\n%matplotlib inline\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom IPython.display import display\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv(\"../input/train_V2.csv\")#.sample(20000)\ntest = pd.read_csv(\"../input/test_V2.csv\")#.sample(20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1a99f44c0fe9af6d4ae4a8d4fd2e91b3c1c3b1d"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41eb7e95100ec7dd36ee5988c28aa270517b34df"},"cell_type":"code","source":"for col in [\"matchId\",\"Id\",\"groupId\"]:\n    print(\"Does {} Feature Overlap Between Train/Test Set?         {}\".format(col, any(np.intersect1d(test[col].unique(), train[col].unique()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da1dd0fb7cf91d8ceb4705f44216ee44014b3994"},"cell_type":"code","source":"matchcount = train.matchId.nunique()\nprint(\"Number of unique matches: {}\".format(train.matchId.nunique()))\nprint(\"Train Shape Before: {} Rows, {} Cols\".format(*train.shape))\ntrain = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.50):]),:]\nprint(\"Train Shape After: {} Rows, {} Cols\".format(*train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057f47ccd7d5a582722a23973e1b55414d66ffab"},"cell_type":"code","source":"# Label Encoder\nfrom sklearn import preprocessing\n# Encoder:\nlbl = preprocessing.LabelEncoder()\nfor col in ['matchType']:\n    lbl.fit(train[col])\n    train[col] = lbl.transform(train[col])\n    test[col] = lbl.transform(test[col])\nid_cols = [\"Id\",\"groupId\",\"matchId\"]\nexclude = [\"Id\",\"groupId\",\"matchId\"]\ntrainlen = train.shape[0]\n# LGBM Dataset\nmatchcount = train.matchId.nunique()\n\ntraining = train.loc[train.matchId.isin(sorted(train.matchId.unique())[:int(matchcount* 0.85)]),\n                    [x for x in train.columns if x not in exclude]]\nprint(\"Training Shape: {} Rows, {} Cols\".format(*training.shape))\nvalidating = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.85):]),\n                       [x for x in train.columns if x not in exclude]]\nprint(\"Validating Shape: {} Rows, {} Cols\".format(*validating.shape))\n\ntrain_y = training.winPlacePerc\ntraining.drop(\"winPlacePerc\", axis =1, inplace=True)\nvalid_y = validating.winPlacePerc\nvalidating.drop(\"winPlacePerc\", axis =1, inplace=True)\n                                                             \nlgb_train = lgb.Dataset(training, train_y,feature_name = \"auto\")\nlgb_valid = lgb.Dataset(validating, valid_y, feature_name = \"auto\")\ndel training, validating","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d70f069924c965648755de263ef608c030df8fc"},"cell_type":"code","source":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_boost_round' : 5000\n                }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebd4aca2fd4fd86f183eb570190bcd3797233e85"},"cell_type":"code","source":"stage = 'model training'\ngbm = lgb.train(lgbm_params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=[lgb_train, lgb_valid],\n                feature_name='auto',\n                early_stopping_rounds=50,\n                verbose_eval=250\n                )\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(gbm, max_num_features=25, ax=ax)\nplt.title(\"Light GBM Feature Importance\\n\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd43c8fcfd221d56a120eaf3de1213231f609215"},"cell_type":"code","source":"pred = gbm.predict(test.loc[:,[x for x in test.columns if x not in id_cols]])\ntest['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\naux = test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ntest_sub = test.merge(aux, how='left', on=['matchId','groupId'])\n    \nsubmission = test_sub[['Id', 'winPlacePerc']]\nsubmission.to_csv('submissions_PubGG_LGBM.csv', index=False)\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dfd0278932292b958e6d4ed194b0cfc68685047"},"cell_type":"code","source":"notcat = [\"assists\",\"boosts\",\"damageDealt\",\"DBNOs\",\"heals\",\"headshotKills\",\"heals\",\"killPlace\",\"killPoints\",\"kills\",\n         \"killStreaks\",\"longestKill\",\"maxPlace\",\"numGroups\",\"revives\",\"rideDistance\",\"roadKills\",\"swimDistance\",\n         \"teamKills\",\"vehicleDestroys\",\"walkDistance\",\"weaponsAcquired\",\"winPoints\"]\ny = train.winPlacePerc.copy()\ntrain.drop(\"winPlacePerc\",axis =1, inplace=True)\ndef agg_dataset(dataset):\n    for id_col in id_cols:\n        agg_features = dataset.groupby(id_col).agg({k:[\"sum\",\"mean\",\"std\"] for k in\n                                [\"killPlace\",\"walkDistance\",\"numGroups\",\"maxPlace\",\"kills\",\"longestKill\",\"weaponsAcquired\"]})\n        agg_features.columns = pd.Index([\"{}_agg_\".format(id_col) + e[0] +\"_\"+ e[1] for e in agg_features.columns.tolist()])\n        dataset = pd.merge(dataset,agg_features, on = id_col, how= \"left\")\n    return dataset\ntrain = agg_dataset(train)\ntest = agg_dataset(test)\n# Remove Columns with 95%+ Missing\nmissing = round(train.isnull().sum()/ train.shape[0]*100).reset_index().rename({\"index\":\"columns\",0:\"missing\"}, axis =1 )\nhigh_missing_columns = missing.loc[missing.missing > 65, \"columns\"]\nprint(\"Columns to remove (65% missing Values and Over)\\n\", list(high_missing_columns))\ntrain.drop(high_missing_columns,axis =1, inplace= True)\ntest.drop(high_missing_columns,axis =1, inplace= True)\ntrain = pd.concat([train.reset_index(drop=True), y.reset_index(drop=True)], axis=1)\n\ntraining = train.loc[train.matchId.isin(sorted(train.matchId.unique())[:int(matchcount* 0.85)]),\n                    [x for x in train.columns if x not in exclude]]\nprint(\"Training Shape: {} Rows, {} Cols\".format(*training.shape))\nvalidating = train.loc[train.matchId.isin(sorted(train.matchId.unique())[int(matchcount* 0.85):]),\n                       [x for x in train.columns if x not in exclude]]\nprint(\"Validating Shape: {} Rows, {} Cols\".format(*validating.shape))\n\ntrain_y = training.winPlacePerc\ntraining.drop(\"winPlacePerc\", axis =1, inplace=True)\nvalid_y = validating.winPlacePerc\nvalidating.drop(\"winPlacePerc\", axis =1, inplace=True)\n                                                             \nlgb_train = lgb.Dataset(training, train_y,feature_name = \"auto\")\nlgb_valid = lgb.Dataset(validating, valid_y, feature_name = \"auto\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d4a2873492c6ec79b2a83e8f147ae44552cef443"},"cell_type":"code","source":"print(\"Light Gradient Boosting Regressor: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_boost_round' : 5000\n                }\n\nstage = 'model training'\ngbm = lgb.train(lgbm_params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=[lgb_train, lgb_valid],\n                feature_name='auto',\n                early_stopping_rounds=50,\n                verbose_eval=250\n                )\n\n# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(gbm, max_num_features=25, ax=ax)\nplt.title(\"Light GBM Feature Importance\\n\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe9f89841f9aabad107c386f2c2d77373847c71d"},"cell_type":"code","source":"pred = gbm.predict(test.loc[:,[x for x in test.columns if x not in id_cols]])\ntest['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\naux = test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ntest_sub = test.merge(aux, how='left', on=['matchId','groupId'])\n    \nsubmission = test_sub[['Id', 'winPlacePerc']]\nsubmission.to_csv('submissions_AGG_PubGG_LGBM.csv', index=False)\nprint(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97ca50e3007d29c97725535583c72f4b7885272"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}