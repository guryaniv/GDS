{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "042c0d49e1208b9c30b727e2d14af3439c35fd3b", "_cell_guid": "830a28a5-c272-4433-b248-52da479e2830"}, "source": ["# House Price Prediction in King County Using Keras"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a50a9a89f2c75542ab488fae13b0b78fb39da7a3", "_cell_guid": "c20ba660-d140-4234-882a-055cbc09808a"}, "source": ["By *ironfrown*\n", "\n", "This is a deep learning version of King County house price prediction using Keras deep learning package with Tensorflow backend. Running with GPU support is preferable. Without any major feature engineering, this approach gives MAE of around $77K.\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ef55b15b777267165fa815006f78fdd27534ccbd", "_cell_guid": "27651695-fa01-43f0-9db6-d7b05a0bab07"}, "source": ["## Preparation"]}, {"cell_type": "markdown", "metadata": {"_uuid": "53f7e335bf9b238d6208e2e698c418699cd1c0c6", "_cell_guid": "9d691e40-c79d-4089-9cd3-c698a7d36475"}, "source": ["*Load some standard Python libraries.*"]}, {"cell_type": "code", "metadata": {"_uuid": "992bb3c5d9086cebff35a36a96945867a710c65e", "collapsed": true, "_cell_guid": "53a7f6d1-12f4-4874-a5b5-8379c804b941"}, "execution_count": null, "outputs": [], "source": ["from __future__ import print_function\n", "import os\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {"_uuid": "aa62aa7f38daf18f14ec23704828de8a1bf7b883", "_cell_guid": "c488024e-2fde-455b-84c0-10c091a96869"}, "source": ["*Load Keras libraries used in this example.*"]}, {"cell_type": "code", "metadata": {"_uuid": "e521963b9759a0a2fa076af13d0aecb700758849", "_cell_guid": "d5832990-83e4-44b9-a59a-23b96a56d004"}, "execution_count": null, "outputs": [], "source": ["import keras\n", "from keras import metrics\n", "from keras import regularizers\n", "from keras.models import Sequential\n", "from keras.layers import Dense, Dropout, Flatten, Activation\n", "from keras.layers import Conv2D, MaxPooling2D\n", "from keras.optimizers import Adam, RMSprop\n", "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n", "from keras.utils import plot_model\n", "from keras.models import load_model"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a9af9d8e77e32d6709221d9040ba53900194e95d", "_cell_guid": "ae73d597-0d5f-43e3-9cfc-ca22628998da"}, "source": ["## Load all data"]}, {"cell_type": "markdown", "metadata": {"_uuid": "53d0f01ea8db30613a67bb1ee1b86e0a1d28cbab", "_cell_guid": "72acf346-259d-49db-a3a7-40cf1a309d84"}, "source": ["*Load data from CSV file and define the label column.*"]}, {"cell_type": "code", "metadata": {"_uuid": "ea69f9accd0179ffef96cdb9ad4988cd5d912a2f", "scrolled": true, "collapsed": true, "_cell_guid": "ca4e3925-9062-482c-b9d8-f1e391939fc4"}, "execution_count": null, "outputs": [], "source": ["kc_data_org = pd.read_csv(\"../input/kc_house_data.csv\")"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3e4875f45acfbf55092aa83d34ca7211f7622c72", "_cell_guid": "59d23609-ad2c-4cea-b0ff-a31ce8c25da9"}, "source": ["*Transform dates into year, month and day and select columns.*"]}, {"cell_type": "code", "metadata": {"_uuid": "74278e63c6615bfd23b6b9e92b1b20da0acef4a2", "scrolled": true, "_cell_guid": "1dd6747c-885f-4d01-866b-5460d1fdbdcd"}, "execution_count": null, "outputs": [], "source": ["kc_data_org['sale_yr'] = pd.to_numeric(kc_data_org.date.str.slice(0, 4))\n", "kc_data_org['sale_month'] = pd.to_numeric(kc_data_org.date.str.slice(4, 6))\n", "kc_data_org['sale_day'] = pd.to_numeric(kc_data_org.date.str.slice(6, 8))\n", "\n", "kc_data = pd.DataFrame(kc_data_org, columns=[\n", "        'sale_yr','sale_month','sale_day',\n", "        'bedrooms','bathrooms','sqft_living','sqft_lot','floors',\n", "        'condition','grade','sqft_above','sqft_basement','yr_built',\n", "        'zipcode','lat','long','sqft_living15','sqft_lot15','price'])\n", "label_col = 'price'\n", "\n", "print(kc_data.describe())"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ec7cf27228fbbabf311b577e78c2a9be7c358bef", "_cell_guid": "4eb2feba-12b6-4f64-b9fa-849f4431c89a"}, "source": ["## Split data for training and validation"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f01e58fe3636bb8dcf243967fbb07a8d32a374f2", "_cell_guid": "39f83ce2-bded-4e30-a09d-543fa38a70cb"}, "source": ["*Function to split a range of data frame / array indeces into three sub-ranges.*"]}, {"cell_type": "code", "metadata": {"_uuid": "36ef3e310f992ff0f3ab7dd2f58458a32a003fd4", "collapsed": true, "_cell_guid": "eb613042-355e-41f6-a99c-bda3c24cd5e8"}, "execution_count": null, "outputs": [], "source": ["def train_validate_test_split(df, train_part=.6, validate_part=.2, test_part=.2, seed=None):\n", "    np.random.seed(seed)\n", "    total_size = train_part + validate_part + test_part\n", "    train_percent = train_part / total_size\n", "    validate_percent = validate_part / total_size\n", "    test_percent = test_part / total_size\n", "    perm = np.random.permutation(df.index)\n", "    m = len(df)\n", "    train_end = int(train_percent * m)\n", "    validate_end = int(validate_percent * m) + train_end\n", "    train = perm[:train_end]\n", "    validate = perm[train_end:validate_end]\n", "    test = perm[validate_end:]\n", "    return train, validate, test"]}, {"cell_type": "markdown", "metadata": {"_uuid": "54a54552a95332eea814e0846905dcc99ba74d76", "_cell_guid": "20e5d60a-6427-476b-98c0-0790aa515543"}, "source": ["*Split index ranges into three parts, however, ignore the third.*"]}, {"cell_type": "code", "metadata": {"_uuid": "04d4b08488a3b1d03378a45f28a902ad1fcd3d27", "collapsed": true, "_cell_guid": "3bbaa40e-2d07-4668-aa58-23620e695699"}, "execution_count": null, "outputs": [], "source": ["train_size, valid_size, test_size = (70, 30, 0)\n", "kc_train, kc_valid, kc_test = train_validate_test_split(kc_data, \n", "                              train_part=train_size, \n", "                              validate_part=valid_size,\n", "                              test_part=test_size,\n", "                              seed=2017)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5dbe5b8ec4d91cdb4d06bebc3df537c34078940e", "_cell_guid": "af43e3fd-49f3-4a64-9e1d-946e04d72e42"}, "source": ["*Extract data for training and validation into x and y vectors.*"]}, {"cell_type": "code", "metadata": {"_uuid": "e098d6d1413b910857c0109a36bf65f4c0da1f3e", "_cell_guid": "101223b4-b34f-41a0-a36a-c20289f20041"}, "execution_count": null, "outputs": [], "source": ["kc_y_train = kc_data.loc[kc_train, [label_col]]\n", "kc_x_train = kc_data.loc[kc_train, :].drop(label_col, axis=1)\n", "kc_y_valid = kc_data.loc[kc_valid, [label_col]]\n", "kc_x_valid = kc_data.loc[kc_valid, :].drop(label_col, axis=1)\n", "\n", "print('Size of training set: ', len(kc_x_train))\n", "print('Size of validation set: ', len(kc_x_valid))\n", "print('Size of test set: ', len(kc_test), '(not converted)')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "445d4ebbdc9ea8c8827c08c370452ac5b8dfae97", "_cell_guid": "8e0e528f-f3dc-4bd1-b88c-b8649715034e"}, "source": ["## Prepare data for training and validation of the Keras model"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4d7d6cc9626ba57a4ffb567c920e3f74ca2c737d", "_cell_guid": "e1a33fd8-c437-4a66-b960-22746771a2c3"}, "source": ["*Function to get statistics about a data frame.*"]}, {"cell_type": "code", "metadata": {"_uuid": "e57300ecb08215fdb43f73a319c02f23051c1f52", "collapsed": true, "_cell_guid": "b6bdc1ac-7e33-4e5f-8489-40ef4e247401"}, "execution_count": null, "outputs": [], "source": ["def norm_stats(df1, df2):\n", "    dfs = df1.append(df2)\n", "    minimum = np.min(dfs)\n", "    maximum = np.max(dfs)\n", "    mu = np.mean(dfs)\n", "    sigma = np.std(dfs)\n", "    return (minimum, maximum, mu, sigma)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b06a85f699fa3fa96906e1155ded54d6ce71e550", "_cell_guid": "d3e35c68-8445-44fa-84d2-e9491b1d1d4d"}, "source": ["*Function to Z-normalise the entire data frame - note stats for Z transform passed in.*"]}, {"cell_type": "code", "metadata": {"_uuid": "29a9ad9d369a654666b4471ee4c961c361ebfb77", "collapsed": true, "_cell_guid": "9076dd85-1234-43c2-a177-47dbb0aa8fbc"}, "execution_count": null, "outputs": [], "source": ["def z_score(col, stats):\n", "    m, M, mu, s = stats\n", "    df = pd.DataFrame()\n", "    for c in col.columns:\n", "        df[c] = (col[c]-mu[c])/s[c]\n", "    return df"]}, {"cell_type": "markdown", "metadata": {"_uuid": "1ecda20bcecb55422141a9b3376d1f0ec8daab08", "_cell_guid": "84609260-0722-44b7-97f8-655d89764b20"}, "source": ["*Normalise training and validation predictors using the stats from training data only (to ensure the same transformation applies to both training and validation data), and then convert them into numpy arrays to be used by Keras.*"]}, {"cell_type": "code", "metadata": {"_uuid": "2510528492ed0544ba96c9bfcbf427dd38efe205", "_cell_guid": "90a93bfc-27a3-4fa4-9790-2c23f4ac00d1"}, "execution_count": null, "outputs": [], "source": ["stats = norm_stats(kc_x_train, kc_x_valid)\n", "arr_x_train = np.array(z_score(kc_x_train, stats))\n", "arr_y_train = np.array(kc_y_train)\n", "arr_x_valid = np.array(z_score(kc_x_valid, stats))\n", "arr_y_valid = np.array(kc_y_valid)\n", "\n", "print('Training shape:', arr_x_train.shape)\n", "print('Training samples: ', arr_x_train.shape[0])\n", "print('Validation samples: ', arr_x_valid.shape[0])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "992d47c8522fe912d22d51b5e694b44f8c01a68a", "_cell_guid": "cca5ff68-ac34-4d1c-9ffe-d70b2357a9f8"}, "source": ["## Create Keras model"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3800c5d0c46017edf28188524003c301ee5d8398", "_cell_guid": "09c266e1-a1cf-4a0c-905f-4c3246bebbee"}, "source": ["***Three functions to define alternative Keras models***\n", "\n", "*The first is very simple, consisting of three layers and Adam optimizer.*"]}, {"cell_type": "code", "metadata": {"_uuid": "8457d07383484ecd6b2bacdc9f1de4f438017372", "collapsed": true, "_cell_guid": "8b924410-ecbf-4a68-a41c-28c596834b13"}, "execution_count": null, "outputs": [], "source": ["def basic_model_1(x_size, y_size):\n", "    t_model = Sequential()\n", "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n", "    t_model.add(Dense(50, activation=\"relu\"))\n", "    t_model.add(Dense(y_size))\n", "    print(t_model.summary())\n", "    t_model.compile(loss='mean_squared_error',\n", "        optimizer=Adam(),\n", "        metrics=[metrics.mae])\n", "    return(t_model)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "535e03cb14a40704898f8df29707e86b083dae16", "_cell_guid": "5fc799e1-acef-4f2c-9684-4292ee723e55"}, "source": ["*The second with Adam optimizer consists of 4 layers and the first uses 10% dropouts.*"]}, {"cell_type": "code", "metadata": {"_uuid": "096dc7ad5771289acd3a653d8d07b76cd6757621", "collapsed": true, "_cell_guid": "be0e0159-5771-47ed-a362-52c8b74ab9c1"}, "execution_count": null, "outputs": [], "source": ["def basic_model_2(x_size, y_size):\n", "    t_model = Sequential()\n", "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n", "    t_model.add(Dropout(0.1))\n", "    t_model.add(Dense(50, activation=\"relu\"))\n", "    t_model.add(Dense(20, activation=\"relu\"))\n", "    t_model.add(Dense(y_size))\n", "    print(t_model.summary())\n", "    t_model.compile(loss='mean_squared_error',\n", "        optimizer=Adam(),\n", "        metrics=[metrics.mae])\n", "    return(t_model)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "c0a6f3c10ddce3e56046b38d396f292e3e7bc3b8", "_cell_guid": "c26f0612-565d-49cf-ac84-5e9e8ec76e10"}, "source": ["*The third is the most complex, it extends the previous model with Nadam optimizer, dropouts and L1/L2 regularisers.*"]}, {"cell_type": "code", "metadata": {"_uuid": "97da2a6de65856cdeee64987bb2e2c82cf26c9df", "collapsed": true, "_cell_guid": "6b3406de-9441-4914-8262-71762be2c72e"}, "execution_count": null, "outputs": [], "source": ["def basic_model_3(x_size, y_size):\n", "    t_model = Sequential()\n", "    t_model.add(Dense(80, activation=\"tanh\", kernel_initializer='normal', input_shape=(x_size,)))\n", "    t_model.add(Dropout(0.2))\n", "    t_model.add(Dense(120, activation=\"relu\", kernel_initializer='normal', \n", "        kernel_regularizer=regularizers.l1(0.01), bias_regularizer=regularizers.l1(0.01)))\n", "    t_model.add(Dropout(0.1))\n", "    t_model.add(Dense(20, activation=\"relu\", kernel_initializer='normal', \n", "        kernel_regularizer=regularizers.l1_l2(0.01), bias_regularizer=regularizers.l1_l2(0.01)))\n", "    t_model.add(Dropout(0.1))\n", "    t_model.add(Dense(10, activation=\"relu\", kernel_initializer='normal'))\n", "    t_model.add(Dropout(0.0))\n", "    t_model.add(Dense(y_size))\n", "    t_model.compile(\n", "        loss='mean_squared_error',\n", "        optimizer='nadam',\n", "        metrics=[metrics.mae])\n", "    return(t_model)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ec3d528080a6fd0d59481c74f21f56f135744cdc", "_cell_guid": "eb46a40b-3b81-499f-ad4f-0543545a7246"}, "source": ["*Now we create the model - use one of the above functions.*"]}, {"cell_type": "code", "metadata": {"_uuid": "8535f5484e9e0e34523d9adcd3ef9b6fc7cf336f", "_cell_guid": "7a410744-5fb1-46de-9f03-8375ca5cb995"}, "execution_count": null, "outputs": [], "source": ["model = basic_model_3(arr_x_train.shape[1], arr_y_train.shape[1])\n", "model.summary()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0f2c81bdc94159272fa7ebee59477cc3abfd3b29", "_cell_guid": "a239a746-8dff-46e1-a56d-b3c7eb6e6aef"}, "source": ["## Fit/Train Keras model"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2cde05a8d7a29974d8c7337b36458c5789327f18", "_cell_guid": "04322115-7855-4259-ae2f-7dc505b985d3"}, "source": ["*Define how many epochs of training should be done and what is the batch size.*"]}, {"cell_type": "code", "metadata": {"_uuid": "34a91122b03887c31e16ca44aed196a52ff87e4e", "_cell_guid": "c3bdf585-6a74-4cf2-9b70-3b3b7e55c49a"}, "execution_count": null, "outputs": [], "source": ["epochs = 500\n", "batch_size = 128\n", "\n", "print('Epochs: ', epochs)\n", "print('Batch size: ', batch_size)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "28fa5969199f4d3af62b68149fc363fb94c8f820", "_cell_guid": "04eb668a-2327-42de-b508-1a22ce820a5d"}, "source": ["*Specify Keras callbacks which allow additional functionality while the model is being fitted.*\n", "- ***ModelCheckpoint*** *allows to save the models as they are being built or improved.*\n", "- ***TensorBoard*** *interacts with TensorFlow interactive reporting system.*\n", "- ***EarlyStopping*** *watches one of the model measurements and stops fitting when no improvement.*"]}, {"cell_type": "code", "metadata": {"_uuid": "9ad300afc4cf4dd232012d633ca475c3b87af6a8", "collapsed": true, "_cell_guid": "315c61d3-9fde-41ae-ad75-a446a31ad488"}, "execution_count": null, "outputs": [], "source": ["keras_callbacks = [\n", "    # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=2)\n", "    # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True, verbose=0)\n", "    # TensorBoard(log_dir='/tmp/keras_logs/model_3', histogram_freq=0, write_graph=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None),\n", "    EarlyStopping(monitor='val_mean_absolute_error', patience=20, verbose=0)\n", "]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "cdf7f60278e5292290177b61a61329b4fd7b4630", "_cell_guid": "45b7b912-8175-414b-b556-372bb4d8e885"}, "source": ["*Fit the model and record the history of training and validation.*<br/>\n", "*As we specified EarlyStopping with patience=20, with luck the training will stop in less than 200 epochs.*<br/>\n", "***Be patient, the fitting process takes time, use verbose=2 for visual feedback.***"]}, {"cell_type": "code", "metadata": {"_uuid": "bb70d9a24530fe50cfaf69daa90714dc53922bec", "scrolled": true, "collapsed": true, "_cell_guid": "52230370-92b1-4f9c-8b7f-4f82208a7841"}, "execution_count": null, "outputs": [], "source": ["history = model.fit(arr_x_train, arr_y_train,\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    shuffle=True,\n", "    verbose=0, # Change it to 2, if wished to observe execution\n", "    validation_data=(arr_x_valid, arr_y_valid),\n", "    callbacks=keras_callbacks)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "da2984e773d6d777a4a3688861737619a5eac6b8", "_cell_guid": "2bcea5d4-bc13-4f7f-8a72-036029842258"}, "source": ["## Evaluate and report performance of the trained model"]}, {"cell_type": "code", "metadata": {"_uuid": "b0be62f94a064ff15436a0db15826267bd8b6506", "_cell_guid": "655cc9e2-4c66-442e-b492-c779a81f45a3"}, "execution_count": null, "outputs": [], "source": ["train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n", "valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n", "\n", "print('Train MAE: ', round(train_score[1], 4), ', Train Loss: ', round(train_score[0], 4)) \n", "print('Val MAE: ', round(valid_score[1], 4), ', Val Loss: ', round(valid_score[0], 4))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "80e67ebe59369a554151f0ff4a1008c9da685271", "_cell_guid": "7e165926-1bf7-4833-a034-3d5d90c5bb25"}, "source": ["*This function allows plotting of the training history*"]}, {"cell_type": "code", "metadata": {"_uuid": "004742724c0731a87996fe0e912f2fac882a89d3", "collapsed": true, "_cell_guid": "46f992fc-36f3-4889-bdfa-2909f6b7164c"}, "execution_count": null, "outputs": [], "source": ["def plot_hist(h, xsize=6, ysize=10):\n", "    # Prepare plotting\n", "    fig_size = plt.rcParams[\"figure.figsize\"]\n", "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n", "    fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True)\n", "    \n", "    # summarize history for MAE\n", "    plt.subplot(211)\n", "    plt.plot(h['mean_absolute_error'])\n", "    plt.plot(h['val_mean_absolute_error'])\n", "    plt.title('Training vs Validation MAE')\n", "    plt.ylabel('MAE')\n", "    plt.xlabel('Epoch')\n", "    plt.legend(['Train', 'Validation'], loc='upper left')\n", "    \n", "    # summarize history for loss\n", "    plt.subplot(212)\n", "    plt.plot(h['loss'])\n", "    plt.plot(h['val_loss'])\n", "    plt.title('Training vs Validation Loss')\n", "    plt.ylabel('Loss')\n", "    plt.xlabel('Epoch')\n", "    plt.legend(['Train', 'Validation'], loc='upper left')\n", "    \n", "    # Plot it all in IPython (non-interactive)\n", "    plt.draw()\n", "    plt.show()\n", "\n", "    return"]}, {"cell_type": "markdown", "metadata": {"_uuid": "02b5226b26bd537610da01d9beb3e38e65b1daca", "_cell_guid": "935ab404-a58c-47c3-84c1-52997731f51b"}, "source": ["*Now plot the training history, i.e. the Mean Absolute Error and Loss (Mean Squared Error), which were both defined at the time of model compilation. Note that the plot shows validation error as less than training error, which is quite deceptive. The reason for this is that training error is calculated for the entire epoch (and at its begining it was much worse than at the end), whereas the validation error is taken from the last batch (after the model improved). See the above evaluation statistics to confirm that the evaluation puts these errors in the correct order at the very end.*"]}, {"cell_type": "code", "metadata": {"_uuid": "eb351f894aee72be6230be8ff40f3538a76dcaa9", "_cell_guid": "50541547-c5f8-4159-98e5-d519e043ca9c"}, "execution_count": null, "outputs": [], "source": ["plot_hist(history.history, xsize=8, ysize=12)"]}]}