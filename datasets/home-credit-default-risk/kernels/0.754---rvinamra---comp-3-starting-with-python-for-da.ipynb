{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# WILL KOEHRSEN'S NOTEBOOK HAS BEEN FOLLOWED TO LEARN MORE ABOUT\n# DA USING PYTHON AS A LANGUAGE\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder # for dealing with categorical variables\n\nimport os # File system management\n\nimport warnings # Suppress warnings\nwarnings.filterwarnings('ignore')\n\n#Importing libraries for plotting and visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# reading the training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint ('Training data shape: ', app_train.shape)\napp_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f50acb9d5feb16a23b6ddd0bb0ff3e648a268a1c","collapsed":true},"cell_type":"code","source":"# reading the test data\napp_test = pd.read_csv('../input/application_test.csv')\nprint ('Testing data shape: ', app_test.shape)\napp_test.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5580ce24663949252d0f8a0805315be4d795efe","collapsed":true},"cell_type":"code","source":"# determining the two target values\napp_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fe834b0cf2d308b7d1b861712cb7ec5ebf52c14","collapsed":true},"cell_type":"code","source":"app_train['TARGET'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99bfce357dfd7bda43bfe28ef18e88e2500bb1e4"},"cell_type":"code","source":"# function to calculate missing values by column\ndef missing_val_table(df):\n    # total missing values\n    mis_val = df.isnull().sum()\n    \n    # percentage of missing values\n    mis_val_percent = 100*mis_val/len(df)\n    \n    # make a table with results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis = 1)\n    \n    #rename the columns\n    mis_val_tab_rename = mis_val_table.rename(columns = {0: 'Missing Values', 1: '% of Total Values'})\n    \n    #sort the table in descending order of percentage\n    mis_val_tab_rename = mis_val_tab_rename[mis_val_tab_rename.iloc[:,1]\n    != 0].sort_values('% of Total Values', ascending=False).round(1)\n    \n    #print some summary info\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n          \"There are \"+str(mis_val_tab_rename.shape[0])+ \" columns that have missing values.\")\n    \n    #return the dataframe with the required info\n    return mis_val_tab_rename\n                                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edc45a35e86c625a7189887207f8f5f175e0cdf8","collapsed":true},"cell_type":"code","source":"#missing value stats\nmissing_val = missing_val_table(app_train)\nmissing_val.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5a35039fa680fa6cd2e0f29094355135b86f7dc","collapsed":true},"cell_type":"code","source":"# inspecting data types of columns\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"84e02866b6fdffaf48fe8f93e74cf9fb4ac0a203","collapsed":true},"cell_type":"code","source":"# looking at number of unique entries in columns\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8d1a62154ec3acc0133be8cb5a983b10a3ba524","collapsed":true},"cell_type":"code","source":"#Categorical variable with 2 unique values will be encoded by \n#label encoder whereas, others by one hot-encoding\n\n# creating a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n#Iterate through columns to find the matching criteria for Label Encoder\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            #transforming both training ans test data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            le_count += 1\n            \nprint ('%d columns were label encoded.' %le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60716e6d796ccb35083b00f9306d8220c09560af","collapsed":true},"cell_type":"code","source":"# one hot-encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76ff02f755c9ea155888774b9a3cdf402969799c","collapsed":true},"cell_type":"code","source":"# aligning the test and train df, so as to make # of variables equal\ntrain_labels = app_train['TARGET']\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# adding the target back in training dataset\napp_train['TARGET'] = train_labels\n\nprint('Training features shape: ', app_train.shape)\nprint('Testing features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b964ae2455c76a21ea3e007c4d09b62d67714288","collapsed":true},"cell_type":"code","source":"# since in the DAYS_BIRTH column days are calculated realtive to loan\n# application data, they are negative\n\n# to see the stats in years\n(app_train['DAYS_BIRTH']/-365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1922b12fd24b57b962d8cb79e0626072e9dad26a","collapsed":true},"cell_type":"code","source":"# similarly describing days employed to spot outliers, if any\n\napp_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a292fefd274fbbe05949f30ce8fd0bb1741fd679","collapsed":true},"cell_type":"code","source":"# the maximum value of DAYS_EMPLOYED looks like an outlier\n# since it is positive unlike others and also equals 100 years \napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\nplt.xlabel('Days Employment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4777a1d0f0fa777179c0a538dc24f3bc9383b030","collapsed":true},"cell_type":"code","source":"# comparing anomalous and non-anomalous clients by their default rate\n\nanom = app_train[app_train[\"DAYS_EMPLOYED\"] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint ('The non-anomalies default on %0.2f%% of loans' % (100*non_anom['TARGET'].mean()))\nprint ('The anomalies default on %0.2f%% of loans' % (100*anom['TARGET'].mean()))\nprint ('There are %d anomalous days of employment' %len(anom))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0918dbc25fbc4803586c4c5e51544d2ea8fb8382","collapsed":true},"cell_type":"code","source":"# for dealing with anamolous values, we will replace the values with NaN\n# then create a boolean variable which states whether value was \n# anamolous or non-anamolous\n\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\napp_train['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\nplt.xlabel('Days Employment')\nprint('There are %d anomalies in the test data out of %d entries' % \n      (app_train[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3511a54f8ce3902a5b9b5d0df8f32012955a2eb4","collapsed":true},"cell_type":"code","source":"# making the same changes to anamolous values to the test data\n\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % \n      (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n\napp_test['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\nplt.xlabel('Days Employment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57d1861605f3ec047b238b0de5b3c5746eda5ff8","collapsed":true},"cell_type":"code","source":"# making a correlation table to determine the realtions between \n# two variables, if any\n\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# displaying the correlations \n\nprint ('Most Positive Correlations:\\n', correlations.tail(15))\nprint ('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ff7e4e3e85ce7e16c5d1d36df3c96e2c1a82664","collapsed":true},"cell_type":"code","source":"# exploring the realtion between age and loan repayment\n\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3892aed19e1676477c633f37d7e463d9fc702608","collapsed":true},"cell_type":"code","source":"# above relation implies that as the client gets older, \n# chances of him/her repaying the loan increases\n# making an histogram of age distribution\n\nplt.style.use('fivethirtyeight') # plot style\n\nplt.hist(app_train['DAYS_BIRTH']/365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age in years');\nplt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e56f949f76b55f222858228ec0cdcbca5df35446","collapsed":true},"cell_type":"code","source":"# doing target variable-wise visualization of age distribution\n# kernel density estimation plot\n\nplt.figure(figsize = (10, 8))\n\n# KDE plots of loans repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH']/365,\n           label = 'Target: 0')\n\n# KDE plots of loans not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH']/365,\n           label = 'Target: 1')\n\n# plot labels\nplt.xlabel('Age in years'); plt.ylabel('Density'); plt.title('Age Distribution')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06ea4c3b6967fdea79b718ca80ed432173b9a2d7","collapsed":true},"cell_type":"code","source":"# the graph showed an expected slight skewness towards younger\n# population which is expected from correlation value\n\n# now we will look at average failure to repay loans\n# for that we will be making age categories of 5 years each\n\n# age info in a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n\n# making age categories\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], \n                        bins = np.linspace(20,70, num = 11))\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80f07bb0a94348d59d72fb2adce1635719c40be0","collapsed":true},"cell_type":"code","source":"# grouping by age group and calculating average for each variables\n# the mean target value will tell percentage of loans unpaid \n\nage_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dec920008042f1431cc18b3f0d7737efb329ec97","collapsed":true},"cell_type":"code","source":"# plotting histogram for better visualization of failure to pay \n# loans by age group\n\nplt.figure(figsize = (8,8)) # dimensions of the plot area\n\nplt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)');\nplt.ylabel('Failure to Repay in %age')\nplt.title('Failure to repay by Age group');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21ddb0c9390ef0356190c1a7ebeb3d8c983a59c2","collapsed":true},"cell_type":"code","source":"# the above graph shows a very clear tend of younger people \n# at a greater risk of being a defaulter\n\n# now exploring the 3 variables EXT_SOURCE_1/2/3 as they had the \n# strongest negative correlation\n# documentation is not very clear about the meaning these variables\n# convey, but we will be using them anyway as they could prove\n# important for predicting target variable\n# looking at the correlations again\n\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2',\n                     'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fc7e46b40baeec08007ec087966cff11adccf49","collapsed":true},"cell_type":"code","source":"# making a correlation heatmap for better visualization\n\nplt.figure(figsize = (8,6))\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, \n           annot = True, vmax = 0.6)\nplt.title('Correlation heatmap');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5920ea70d8a5d1ed47e7e075d7c8b19b74c583fe","collapsed":true},"cell_type":"code","source":"# all the ext_source show negative corr with target\n# implies that as the ext_source increases, client is more likely\n# to repay the loan\n\n# distribution of each of these variable color coded according \n# to target variable\n\nplt.figure(figsize = (10,12)) \n\n# iterating through sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    # creating a new sub-plot for each variable\n    plt.subplot(3, 1, i+1)\n    # plotting repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], \n               label = 'Target: 0')\n     # plotting loans not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], \n               label = 'Target: 1')\n    \n    # plot labels\n    plt.title('Distribution of %s by Target Value' %source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dec034294a3f86c4d0345785a56b1a5f778d0805","collapsed":true},"cell_type":"code","source":"# making pair plots\n# the code may be obscure, but hang on buddy\n\n# copying data for plotting \nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# adding the age of client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# dropping na values and limiting to first 100k fields\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# function to calculate correlation coeff. between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n               xy = (.2, .8), xycoords=ax.transAxes,\n               size = 20)\n    \n# creating a pairgrid object\ngrid = sns.PairGrid(data = plot_data, size=3, diag_sharey=False,\n                   hue = 'TARGET', \n                   vars = [x for x in list(plot_data.columns) if \n                          x != 'TARGET'])\n\n# sspecifying position of plots in the grid\n\ngrid.map_upper(plt.scatter, alpha = 0.2)\ngrid.map_diag(sns.kdeplot)\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', \n            size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"647208483c066f04c163de2a6786633f1825c5d2"},"cell_type":"code","source":"# EXT_SOURCE_1 and YEARS_BIRTH seem to show a moderate linear\n# relation which could be used in Feature Engineering\n\n#---- FEATURE ENGINEERING ----#\n\n# using POLYNOMIAL FEATURES\n# avoiding higher degrees so as to prevent overfitting \n# making a new dataframe for polynomial features\n\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n                          'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n                          'DAYS_BIRTH']]\n\n# handling missing values\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# imputing missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# creating the polynomial object with a specific degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81c917810a1849ba57646cdc173392c81e1f58b9","collapsed":true},"cell_type":"code","source":"# training the polynomial features\npoly_transformer.fit(poly_features)\n\n# transforming the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint ('Polynomial Features Shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e7b54681106bc29668057de6540fe36eacb12f7","collapsed":true},"cell_type":"code","source":"# getting the names of the new features\n\npoly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1',\n                'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12e88cbbe82edfe2dddb69d83eef41efda994a6b","collapsed":true},"cell_type":"code","source":"# finding correlations of new variables with the target variable \n# creating a database for the features\n\npoly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names\n                            (['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# adding in the target\npoly_features['TARGET'] = poly_target\n\n# finding the correlations with the target \npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# displaying the most negative and the most positive\nprint (poly_corrs.head(10))\nprint (poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ab9e518c35a052c4c685ae4019ebbfbd2c64bb9","collapsed":true},"cell_type":"code","source":"# the new variables seem to have a better correlation with the target\n# variable which might help in making a better model\n# adding the new features to the original databases to try it out\n\npoly_features_test = pd.DataFrame(poly_features_test, \n                    columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# merging the dataframes based on primary key\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', \n                                how = 'left')\n\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', \n                                how = 'left')\n\n# aligning the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly,\n                                join = 'inner', axis = 1)\n\n# printing the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape: ', app_test_poly.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42c94f86605e0fb5287d15ddd4457426e5e8ccfb","collapsed":true},"cell_type":"code","source":"# using financial knowledge to improve the model\n\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b2f612f73de226af4a5caf8cdb2d1f921451411"},"cell_type":"code","source":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b43f032ffd9e18503e5f1263f9443040236faad6","collapsed":true},"cell_type":"code","source":"# kde plots to visualize these new variables created\n\nplt.figure(figsize = (12,20))\n\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', \n                             'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # creating a new subplot for better clarity\n    plt.subplot(4, 1, i+1)\n    \n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0,\n                                    feature], label = 'Target: 0')\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1,\n                                    feature], label = 'Target: 1')\n    \n    # labelling the plots\n    plt.title('Distribution of %s by Target' %feature)\n    plt.xlabel('%s' %feature); plt.ylabel('Density')\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9091eee5ff95f979be7067c4e56bb26f78139482","collapsed":true},"cell_type":"code","source":"# the graph don't give much about the usefulness, have to use it \n# in the actual model\n\n#----LOGISTIC REGRESSION----#\n\n# preprocessing by filling in the missing values and \n# normalizing the range of features <feature scaling>\n\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n\n# temporarily dropping target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n\n# feature names\nfeatures = list(train.columns)\n\n# copying the test data\ntest = app_test.copy()\n\n# replacing missing values with the median\nimputer = Imputer(strategy = 'median')\n\n# scale each feature to 0-1 for normalization\nscaler = MinMaxScaler(feature_range = (0,1))\n\n# fitting on the training data\nimputer.fit(train)\n\n# transforming both the training and the testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# normalizing both the sets with scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68e26232da044aca3355c9728801c2cd82d59278","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# making the model with a specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# training using the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60866713110f521c85d4493f9a76ef47beb9a74d"},"cell_type":"code","source":"# the model has been trained and now could be used to make predictions\n# on probabilities of loan repayment\n# we will be using predict.proba methos with returns mx2 matrix. \n# we need probability of non-repayment of the loan, so will use second column\n\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0ea6c7dcd07469b9ea7ba469877d1309d9c7606","collapsed":true},"cell_type":"code","source":"# the solution must be in the required format with SK_CURR_ID and TARGET\n\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c38c43377a2434d7530b7330cfafd8c29ad5e88"},"cell_type":"code","source":"# saving the submission file to be submitted\nsubmit.to_csv('log_reg_mod.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"979a12416f57f38c56c34678b963bc4cf7081cc5"},"cell_type":"code","source":"#---RANDOM FOREST---#\nfrom sklearn.ensemble import RandomForestClassifier\n\n# making the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100,\n                random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"429b9d3a162dd422b21cce23300b3ef07f5d9f11","collapsed":true},"cell_type":"code","source":"# training on the TRAIN data\nrandom_forest.fit(train, train_labels)\n\n# extracting importance of features\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# making predictions on the test data\nprediction = random_forest.predict_proba(test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b3e70f6c6e74e7744ef534c277601f95d9f1e37","collapsed":true},"cell_type":"code","source":"# making a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = prediction\n\n# saving the file\nsubmit.to_csv('random_forest_mod.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0508b1837403f5c944939cb8c5be0acbe6c13c36","collapsed":true},"cell_type":"code","source":"# using the engineered variables to see if they have an impact \n# on the overall score\n\npoly_features_names = list(app_train_poly.columns)\n\n# imputing the polynomial features\n\nimputer = Imputer(strategy = 'median')\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# scaling the features to normalize\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators=100, \n                    random_state=50, verbose=1, n_jobs = -1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1474f029988c2cb6b157312285e6d0eb0c6c3148","collapsed":true},"cell_type":"code","source":"# training the data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# making the predictions\npredictions = random_forest_poly.predict_proba(poly_features_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3521609f510498a2b47e23bdc8690d14442a0a4f","collapsed":true},"cell_type":"code","source":"# using the domain features which was created based on financial \n# knowledge\n\napp_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2e5f9bb217f2f7e90d463b1f3705c7f2fecc7a0"},"cell_type":"code","source":"# submission file\n# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_domain_mod.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ddbd24565066016c3eb6e8edb1203145f3fc039"},"cell_type":"code","source":"# looking at importance of features through visualization\n\ndef plot_feature_importances(df):\n    \"\"\"\n    Plot importances of a feature returned by a model. Can work with with \n    any measure of feature importance, in case higher importance implies \n    better.\n    \n    Args:\n        df (dataframe): must have features in a column called 'features'\n        importances in a column called 'importance'\n    Returns:\n        a plot of 15 most important features \n        \n        df (dataframe): feature importances sorted by importance (highest\n                to lowest) with a column for normalized importance\n    \"\"\"\n    \n    # sort features according to importance\n    \n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # normalizing the feature importance\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    \n    # making a horizontal bar graph for feature importance\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # reversing to plot the most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n           df['importance_normalized'].head(15),\n           align = 'center', edgecolor = 'k')\n    \n    # setting the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # plot labelling\n    plt.xlabel('Normalized Importace'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d57238f8b1065f8594cd1e4c812bfc17338c6a5c","collapsed":true},"cell_type":"code","source":"# displaying the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac9387621f28e1e09d36f907fe9a763783c381b7","collapsed":true},"cell_type":"code","source":"# displaying the feature importances inclusive of domain features\nfeature_importances_domain_sorted = plot_feature_importances(\n    feature_importances_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47c9f2185835d9672beade883dd01c8db73f6312","collapsed":true},"cell_type":"code","source":"# all the domain specific features made it to the most important features\n# using the light gradient boosting model\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\n# ohe --> one hot encoding\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \"\"\" Train and test a LightGBM using cross validation.\n    \n    Parameters:\n    features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    # extracting the IDs\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # extracting the labels for training\n    labels = features['TARGET']\n    \n    # removing the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Aligning the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # integer label encoding\n    elif encoding == 'le':\n        \n        # creating a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # iterating through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # mapping the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # recording the categorical indices\n                cat_indices.append(i)\n    \n    # catching error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # extracting feature names\n    feature_names = list(features.columns)\n    \n    # converting to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # creating the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterating through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # creating the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Training the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # recording the best iteration\n        best_iteration = model.best_iteration_\n        \n        # recording the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # making predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # recording the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # recording the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # cleaning up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # making the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # making the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # adding the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f3f62ee2082ad5a231941ffdec22e0b8eb709d","collapsed":true},"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline Metrics')\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f57b3033ded0de23e27b4ecd3dddb135ee3abe94"},"cell_type":"code","source":"submission.to_csv('lgb_mod_1.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83fe2e77fb311728f05c034ce01506bd84037213"},"cell_type":"code","source":"# using domain engineered variables\napp_train_domain['TARGET'] = train_labels\n\n# testing the domain knowledge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bfb02139c6ee3cc856d95652d7566f4314cef337"},"cell_type":"code","source":"submission_domain.to_csv('lgb_mod_domain.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"73da3083fbc81c88aabc23f648a8149a02d5f1fb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}