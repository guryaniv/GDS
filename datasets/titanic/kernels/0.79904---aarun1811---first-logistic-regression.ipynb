{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "133cc7a5-0eb7-4352-bcc8-3247d9f6f1d4"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "from sklearn import cross_validation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Import the linear regression class\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Sklearn also has a helper that makes it easy to do cross validation\n",
        "from sklearn.cross_validation import KFold\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5487cbc1-f1fb-aeb0-edba-3ed809fa3a87"
      },
      "outputs": [],
      "source": [
        "titanic = pd.read_csv(\"../input/train.csv\")\n",
        "titanic['Age'].fillna(titanic['Age'].median(), inplace = True)\n",
        "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
        "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
        "titanic.loc[titanic['Embarked'].isnull(), 'Embarked'] = 'S'\n",
        "titanic.loc[titanic['Embarked'] == 'S', 'Embarked'] = 0\n",
        "titanic.loc[titanic['Embarked'] == 'C', 'Embarked'] = 1\n",
        "titanic.loc[titanic['Embarked'] == 'Q', 'Embarked'] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b40bd83-9b89-7a70-f8b6-db1b47e4be07"
      },
      "outputs": [],
      "source": [
        "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "alg = LinearRegression()\n",
        "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
        "predictions = []\n",
        "for train, test in kf:\n",
        "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
        "    train_predictors = (titanic[predictors].iloc[train,:])\n",
        "    # The target we're using to train the algorithm.\n",
        "    train_target = titanic[\"Survived\"].iloc[train]\n",
        "    # Training the algorithm using the predictors and target.\n",
        "    alg.fit(train_predictors, train_target)\n",
        "    # We can now make predictions on the test fold\n",
        "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
        "    predictions.append(test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af5c5845-24ad-5378-b9a1-eba2c1147c09"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
        "# We concatenate them on axis 0, as they only have one axis.\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
        "predictions[predictions > .5] = 1\n",
        "predictions[predictions <=.5] = 0\n",
        "\n",
        "accuracy = metrics.accuracy_score(titanic['Survived'], predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "500d5e4d-6691-59a7-63ef-678a90411a75"
      },
      "outputs": [],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66d8b287-a92b-8a19-b094-7c885c8b4efb"
      },
      "outputs": [],
      "source": [
        "titanic_test = pd.read_csv(\"../input/test.csv\")\n",
        "titanic_test['Age'].fillna(titanic['Age'].median(), inplace = True)\n",
        "\n",
        "titanic_test.loc[titanic_test['Sex'] == 'male','Sex'] = 0\n",
        "titanic_test.loc[titanic_test['Sex'] == 'female','Sex'] = 1\n",
        "\n",
        "titanic_test.loc[titanic_test['Embarked'].isnull(), 'Embarked'] = 'S'\n",
        "\n",
        "titanic_test.loc[titanic_test['Embarked'] == 'S', 'Embarked'] = 0\n",
        "titanic_test.loc[titanic_test['Embarked'] == 'C', 'Embarked'] = 1\n",
        "titanic_test.loc[titanic_test['Embarked'] == 'Q', 'Embarked'] = 2\n",
        "\n",
        "titanic_test.loc[titanic_test['Fare'].isnull(), 'Fare'] = titanic_test['Fare'].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "961e8519-6557-041f-6c4d-23b430651ba9"
      },
      "outputs": [],
      "source": [
        "from sklearn import cross_validation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "alg = LogisticRegression(random_state=1)\n",
        "\n",
        "# Train the algorithm using all the training data\n",
        "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
        "\n",
        "# Make predictions using the test set.\n",
        "predictions = alg.predict(titanic_test[predictors])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4e1a23f5-e475-8706-7621-fac2ded76d6e"
      },
      "outputs": [],
      "source": [
        "from sklearn import cross_validation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "\n",
        "# Initialize our algorithm with the default paramters\n",
        "# n_estimators is the number of trees we want to make\n",
        "# min_samples_split is the minimum number of rows we need to make a split\n",
        "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
        "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
        "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
        "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
        "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4d138197-87eb-bb2a-4376-46cb80ff2ca3"
      },
      "outputs": [],
      "source": [
        "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
        "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
        "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
        "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17927090-612e-079d-e545-13a450305710"
      },
      "outputs": [],
      "source": [
        "# Generating a familysize column\n",
        "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
        "\n",
        "# The .apply method generates a new series\n",
        "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8929a6e7-c856-3538-85b2-65910a0a5a63"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# A function to get the title from a name.\n",
        "def get_title(name):\n",
        "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    # If the title exists, extract and return it.\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"\n",
        "\n",
        "# Get all the titles and print how often each one occurs.\n",
        "titles = titanic[\"Name\"].apply(get_title)\n",
        "print(pd.value_counts(titles))\n",
        "\n",
        "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
        "for k,v in title_mapping.items():\n",
        "    titles[titles == k] = v\n",
        "\n",
        "# Verify that we converted everything.\n",
        "print(pd.value_counts(titles))\n",
        "\n",
        "# Add in the title column.\n",
        "titanic[\"Title\"] = titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d96153f3-e3f1-25af-6039-f7945edd88af"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "# A dictionary mapping family name to id\n",
        "family_id_mapping = {}\n",
        "\n",
        "# A function to get the id given a row\n",
        "def get_family_id(row):\n",
        "    # Find the last name by splitting on a comma\n",
        "    last_name = row[\"Name\"].split(\",\")[0]\n",
        "    # Create the family id\n",
        "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
        "    # Look up the id in the mapping\n",
        "    if family_id not in family_id_mapping:\n",
        "        if len(family_id_mapping) == 0:\n",
        "            current_id = 1\n",
        "        else:\n",
        "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
        "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
        "        family_id_mapping[family_id] = current_id\n",
        "    return family_id_mapping[family_id]\n",
        "\n",
        "# Get the family ids with the apply method\n",
        "family_ids = titanic.apply(get_family_id, axis=1)\n",
        "\n",
        "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
        "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
        "\n",
        "# Print the count of each unique id.\n",
        "print(pd.value_counts(family_ids))\n",
        "\n",
        "titanic[\"FamilyId\"] = family_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eea29c7f-6a19-facc-633e-88a394ae56ad"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\", \"NameLength\"]\n",
        "\n",
        "# Perform feature selection\n",
        "selector = SelectKBest(f_classif, k=5)\n",
        "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
        "\n",
        "# Get the raw p-values for each feature, and transform from p-values into scores\n",
        "scores = -np.log10(selector.pvalues_)\n",
        "\n",
        "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
        "plt.bar(range(len(predictors)), scores)\n",
        "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
        "plt.show()\n",
        "\n",
        "# Pick only the four best features.\n",
        "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
        "\n",
        "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)\n",
        "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
        "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1247210a-03b9-499d-bc22-8a5537f1826c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import numpy as np\n",
        "\n",
        "# The algorithms we want to ensemble.\n",
        "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
        "algorithms = [\n",
        "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
        "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
        "]\n",
        "\n",
        "# Initialize the cross validation folds\n",
        "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
        "\n",
        "predictions = []\n",
        "for train, test in kf:\n",
        "    train_target = titanic[\"Survived\"].iloc[train]\n",
        "    full_test_predictions = []\n",
        "    # Make predictions for each algorithm on each fold\n",
        "    for alg, predictors in algorithms:\n",
        "        # Fit the algorithm on the training data.\n",
        "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
        "        # Select and predict on the test fold.  \n",
        "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
        "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
        "        full_test_predictions.append(test_predictions)\n",
        "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
        "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
        "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
        "    test_predictions[test_predictions <= .5] = 0\n",
        "    test_predictions[test_predictions > .5] = 1\n",
        "    predictions.append(test_predictions)\n",
        "\n",
        "# Put all the predictions together into one array.\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# Compute accuracy by comparing to the training data.\n",
        "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "349300dd-797e-3583-2382-4a7bb89824af"
      },
      "outputs": [],
      "source": [
        "titles = titanic_test[\"Name\"].apply(get_title)\n",
        "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
        "for k,v in title_mapping.items():\n",
        "    titles[titles == k] = v\n",
        "titanic_test[\"Title\"] = titles\n",
        "# Check the counts of each unique title.\n",
        "print(pd.value_counts(titanic_test[\"Title\"]))\n",
        "\n",
        "# Now, we add the family size column.\n",
        "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
        "\n",
        "# Now we can add family ids.\n",
        "# We'll use the same ids that we did earlier.\n",
        "print(family_id_mapping)\n",
        "\n",
        "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
        "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
        "titanic_test[\"FamilyId\"] = family_ids\n",
        "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "74ebd02f-d0ff-62e5-ea88-0663f4205f53"
      },
      "outputs": [],
      "source": [
        "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
        "\n",
        "algorithms = [\n",
        "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
        "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
        "]\n",
        "\n",
        "full_predictions = []\n",
        "for alg, predictors in algorithms:\n",
        "    # Fit the algorithm using the full training data.\n",
        "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
        "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
        "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
        "    full_predictions.append(predictions)\n",
        "\n",
        "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
        "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
        "predictions[predictions <= .5] = 0\n",
        "predictions[predictions > .5] = 1\n",
        "predictions = predictions.astype(int)\n",
        "submission = pd.DataFrame({\n",
        "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
        "        \"Survived\": predictions\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3190d2e0-d305-5f0e-e32c-03ba3a4256ac"
      },
      "outputs": [],
      "source": [
        "submission.to_csv(\"kaggle.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4dda07d6-4c8d-bfb2-6917-835a79a771ec"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}