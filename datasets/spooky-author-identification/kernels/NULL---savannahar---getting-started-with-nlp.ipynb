{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"### This notebook is completely inspired from [Approaching Any Problem of NLP](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)\n\nAll the steps covered in that notebook will be replicated here with changes and meaning to each and every step we take.\n\nThis notebook is for all those like me, who want to get started with NLP\n\n### The book is going to Cover the following points\n* TFIDF\n* Count Features\n* Logistic Regression\n*  Naive Bais\n* SVM\n* xgboost\n* GridSearch\n* word vectors\n* LSTM\n* GRU\n* Ensembling\n\nWe'll cover all these points and let's have our dataset from Spooky Author Identification\n\n### Imports"},{"metadata":{"trusted":true,"_uuid":"c18131155887fa3c3ebad4e4d4fb2824056bb756"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport tqdm as tqdm #for progress bars\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import GRU, LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils # array and list manipulation \nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n#The purpose of the pipeline is to assemble several steps that can be cross-validated \n#together while setting different parameters.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD #Dimensionality reduction\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB #suitable for classification with discrete features \nfrom keras.layers import Bidirectional, GlobalAvgPool1D, Conv1D, Flatten, SpatialDropout1D, MaxPooling1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26f3d82734bcc365bc27c2fba917a887da5ac3f8"},"cell_type":"markdown","source":"### Let's load the dataset"},{"metadata":{"trusted":true,"_uuid":"0b594dffbefb420e71e3f74acb97e836d50829c6"},"cell_type":"code","source":"train = pd.read_csv('../input/spooky-author-identification/train.csv')\ntest = pd.read_csv('../input/spooky-author-identification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eef1578b2138793c06318c5ab5037ed012f9faf"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83968cbe9c72279d9493fba4e8c227b55376af72"},"cell_type":"markdown","source":"### What is the problem and what are we expected to do ?\nSo the problem given is 'Given a text sentence we have to identifiy which author might have written it'\n\nAs described in the Overview, 3 Authors are there EAP, HPL and MWS so basically we have the task of classifiying between 3 authors - text classification between 3 classes\n\n### Metric\n\nFor this problem we have given metric as multiclass log loss as our evaluation metric  - http://wiki.fast.ai/index.php/Log_Loss#Multi-class_Classification\n\n (taken from: https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"},{"metadata":{"trusted":true,"_uuid":"180eb72a096685aa8f69e36aa26501b67c821061"},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps = 1e-15):\n    #We have to make sure actual is binary \n    if(len(actual.shape) == 1):\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n    clip = np.clip(predicted, eps, 1-eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0/rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bf56c329ffbd26c9529c033d92480044b5a7fc1"},"cell_type":"markdown","source":"We'll use label encoder to convert text to label -> 'a','b','c' = 0,1,2 etc\n\nHere we'll convert our authors to integer\n\nEAP,HPL,MWS => 0,1,2"},{"metadata":{"trusted":true,"_uuid":"d80ad8045ded46301c23008e2c6b697400c76e99"},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)\nset(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4465435fc6c8669a3895bf8da87313dfcd89922c"},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, stratify = y, random_state=42, test_size=0.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7c2e6f038c1db4a586d7b3c4d0916b71f046d2b"},"cell_type":"code","source":"print(xtrain.shape, xvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fccf7c32dc167978799d44e71fd6afdd1fc8569"},"cell_type":"markdown","source":"### Let's try out basic model\nLet's do tfifd followed by Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"2597f37967b34c6d8d26c3a19442ad61c5119b1b"},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df = 3, max_features=None, strip_accents='unicode', analyzer='word', token_pattern = r'\\w{1,}',\n                     ngram_range=(1,3), use_idf = 1, smooth_idf = 1, sublinear_tf = 1, stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d69476b1fb7124c6836c5836e32990f7c3f2d6fb"},"cell_type":"code","source":"# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6620b2bf0f7ca03673961c6de56e9daa041d6380"},"cell_type":"code","source":"#so basically it coverts your words into int and also give it's frequencies along with it's IDF\nfor x in xtrain_tfv:\n    print(x)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7460837f338662040dd7c1e13bff313538138374"},"cell_type":"code","source":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec8fbf4b3abc2b8b7571867e566c880e8a1aea52"},"cell_type":"markdown","source":"And there we go. We have our first model with a multiclass logloss of 0.626.\n\nNow let's use wordcount as a feature, this can be done by countvectorizer "},{"metadata":{"trusted":true,"_uuid":"82fec146a5420ff1fff5cedb6dc2215f923f752a"},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5458887f4cddc8e7794a9f0f9ca0806ef751129"},"cell_type":"code","source":"clf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3097b8566f0d3a8c33bfce92495a1a3f421b959a"},"cell_type":"markdown","source":"We imporved 0.1 that's a significant change\n\nNow let's try out naive bias\n\nOn tfidf"},{"metadata":{"trusted":true,"_uuid":"8614791ce1d1eaf3afa455f989917f471e70c64c"},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee7418e55e318a25a8daaec7bc2be2bdbcefd86c"},"cell_type":"code","source":"#On count vector\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5197bbeca070e104323f84ae9502dcb1be98f13"},"cell_type":"markdown","source":"That's a very good score\n\nNow let's try SVM\n\nAs SVM takes a lot of time so let's reduce the number of features using  Singular Value Decomposition "},{"metadata":{"trusted":true,"_uuid":"b9854403e3f02e2594a54b02508b85f404a2723f"},"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6022a72c7d0fa7b8eb075238a694d109ce041a9a"},"cell_type":"code","source":"# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ca8f0b75507eb57f315e8280edc7a5456061463"},"cell_type":"code","source":"clf = SVC(C=1.0, probability=True)\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f158f069d947c66c08e866959dbc8391c531156"},"cell_type":"markdown","source":"Does't do we'll\n\nLet's apply xgboost"},{"metadata":{"trusted":true,"_uuid":"0227cb760a7182bb0c58d45991d27d26265d6f60"},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n\n# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n\n# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n\n# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06224cdc309b7629659afb7c459418b94e28acc7"},"cell_type":"markdown","source":"It does't do we'll "},{"metadata":{"_uuid":"7941cb30b984c0e40bc1640adf058202cbb78daf"},"cell_type":"markdown","source":"## GridSearch\n\nLet's try to search the best hyperparameter for best optimization Grid search using logistic regression.\n\nBefore starting with grid search we need to create a scoring function. This is accomplished using the make_scorer function of scikit-learn."},{"metadata":{"trusted":true,"_uuid":"3c534f831121853bcdfa560eb97a28fd304a1f0e"},"cell_type":"code","source":"mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb8bd2a85c95a2a530b38960cea8748df2d03c0e"},"cell_type":"code","source":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd992619a4e711496ea4b0ed53de83b95eca36d5"},"cell_type":"code","source":"param_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f312eabeface608d3e7397afc6d82e9fb4f2b02"},"cell_type":"code","source":"model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer, verbose=10, iid = True, n_jobs=-1, refit=True, cv = 2)\nmodel.fit(xtrain_tfv, ytrain)\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bda671cc2e5840a84c3756f48990edbaad0d27be"},"cell_type":"markdown","source":"The score comes similar to what we had for SVM. This technique can be used to finetune xgboost or even multinomial naive bayes as below. We will use the tfidf data here"},{"metadata":{"trusted":true,"_uuid":"9ed2e31f1bf47ebf4d7dc72fd7227322a5836042"},"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cbcb865c26c2c242195b713ebc67f4d8012db80"},"cell_type":"markdown","source":"In NLP problems, it's customary to look at word vectors. Word vectors give a lot of insights about the data. Let's dive into that.\n"},{"metadata":{"_uuid":"6d7ee1cc769a318c27dd4544c555874cfe2dda48"},"cell_type":"markdown","source":"## Wrod Vector\nWithout going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it. I am a fan of GloVe vectors, word2vec and fasttext. In this post, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip"},{"metadata":{"trusted":true,"_uuid":"89a8f28a93c895753040999bcf7a101aa9b5d29e","_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"!wget  http://www-nlp.stanford.edu/data/glove.840B.300d.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1ea778d867c85a35b38495a984326bc8b99e445"},"cell_type":"code","source":"!ls ../input/glove840b300dtxt/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1606d93ba0af6ac871dd92fa360ab10871b8f907","_kg_hide-output":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\nfor line in tqdm.tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eff4d8732a4f854e5465057b00844575f244fd1"},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc0926ad525bc0031a322a484671f3a902847a35"},"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm.tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm.tqdm(xvalid)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a3e9e5fe1be8de9d7d48040354c43c483ac55b"},"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8723c8fc17a5968ef5a2f6e72f4295f2acb711d5"},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74aa5a196da1de6a222b0c9c71d99c0123e3cde9"},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b421f3739c8e437b787bcdef82a0f8f57e52241b"},"cell_type":"markdown","source":"## Deep learning\nLet's first scale the data"},{"metadata":{"trusted":true,"_uuid":"0716d242c2effae45051a5085b0a24d09bc46049"},"cell_type":"code","source":"# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffbcd4394a57d5cdcdfb0ea28003620091cb29db"},"cell_type":"code","source":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a312633592f87f80082e34ea50894434d94f95a"},"cell_type":"code","source":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc61b1d7a3eaf8c62e9bd8d254648941e81bb64b","_kg_hide-output":true},"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=15, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2faabfdc64015564892fe4bcd35d8ac480d7ca05"},"cell_type":"markdown","source":"To move further, i.e. with LSTMs we need to tokenize the text data\n"},{"metadata":{"trusted":true,"_uuid":"488359b357931a6f32866e3f946c2e6233337027"},"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d97225608e57f0e9dd66e470eaa19f919f72b8e"},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm.tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fc8f2af355617e52e17197e37e50d1e1afad5ca"},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\n\n# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e7dcb694be710900bf3b904b6814be3d6512e13","_kg_hide-output":true},"cell_type":"code","source":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a934e96c00bb6293b1202f3141a12170d287bcd"},"cell_type":"code","source":"# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdec2f4b1702b4a75f5b7aaa657a745414fddeba"},"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7557610a9320c41d9bd31cbedc4bdeba82477919"},"cell_type":"markdown","source":"Finally you can give a try with ensemble of all these models"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}