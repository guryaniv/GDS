{"cells":[{"metadata":{"_uuid":"f02564ac2448ffe2891d40c4d59a1c650fb311c8","_cell_guid":"a18a4a64-1075-47ae-8e81-4523a177457e"},"cell_type":"markdown","source":"## Mercari Price Prediction: CNN with GloVE (end-to-end single model)"},{"metadata":{"_uuid":"df515c0e36b5edd5965874d56499b32bde7dd13d","_cell_guid":"8b90893c-8651-4281-b5c1-f51559e740ea"},"cell_type":"markdown","source":"In this kernel, I demonstrate my solution for Kaggle Mercari Price Prediction competition, using a single Deep Learning model (CNN with GloVE for word embeddings initialization). The complete description of this architecture is available in this [blog post](https://medium.com/@gabrielpm_cit/how-i-lost-a-silver-medal-in-kaggles-mercari-price-suggestion-challenge-using-cnns-and-tensorflow-4013660fcded).\n\nThe architecture was initially inspired by this [CNN kernel](https://www.kaggle.com/agrigorev/tensorflow-starter-conv1d-embeddings-0-442-lb/code) and also by this very [didactic post](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/). Some tricks brought gains in terms of accuracy, e.g. word embeddings initialization with GloVE (with a strategy for OOV words), skip connections in the architecture and some basic engineered features. This single model lasts 48 minutes (letting some spare time for an ensemble) and scores around 0.41117, making the 35th position (out of 2,384 teams) in the Private Leaderboard.  \n\n![My Deep Learning architecture](https://cdn-images-1.medium.com/max/1750/1*IR9RTdORhQwtrSr-AjOvlw.png)\n\nTherefore, the [competition deadline inconsistencies](https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/49777) for submission mislead me and many other competitors. So I did not select the kernel for the 2nd Phase, my score wasn't computed and I lost a silver medal :(  \n\nLetting that discussion apart, here is my solution for the contest (full description [here](https://medium.com/@gabrielpm_cit/how-i-lost-a-silver-medal-in-kaggles-mercari-price-suggestion-challenge-using-cnns-and-tensorflow-4013660fcded))."},{"metadata":{"_uuid":"fd3e6dc4a9f244a30354e4fd9b419b744e0e300f","_cell_guid":"6bd257e3-f8fb-4bd8-b2cb-4698782b1647","trusted":false,"collapsed":true},"cell_type":"code","source":"#Flag to set whether all training set should be used to predict prices for test set (True)\nSUBMISSION = True\nprint(\"SUBMISSION: {}\".format(SUBMISSION))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07ba1838-f2e6-4ed8-8a57-8602eb1722a6","collapsed":true,"_uuid":"c8c3a41aeee3d32de6504ac888c83a43e9161115","trusted":false},"cell_type":"code","source":"import os \nimport multiprocessing as mp\nfrom joblib import Parallel, delayed\n\nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['OMP_NUM_THREADS'] = '4'\nos.environ['JOBLIB_START_METHOD'] = 'forkserver'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d8c1b2e-0609-4c51-8dd9-46fb3d12bb8e","collapsed":true,"_uuid":"b292a146380432f3cbe55bd496a13100093d57d7","trusted":false},"cell_type":"code","source":"import gc\nimport re\nimport math\nfrom time import time\nfrom collections import Counter\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom fastcache import clru_cache as lru_cache\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"604a446b-0e5f-4c04-ab9f-339e469949d0","collapsed":true,"_uuid":"b98afd7324e86515381d7044370332e0d8495ede","trusted":false},"cell_type":"code","source":"t_start = time()\n\ndef print_elapsed(text=''):\n    took = (time() - t_start) / 60.0\n    print('==== \"%s\" elapsed %.3f minutes' % (text, took))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff879f64-83ea-451e-937c-1a2f5c10db65","collapsed":true,"_uuid":"cb982f7ef742adffcfd2f1ce2c2dbae28110b490","trusted":false},"cell_type":"code","source":"#Competition metric\ndef rmsle(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d9a5a3395659d6d03e9d4122478838ed8d4e3bd","_cell_guid":"eba76b26-8eb7-40a5-b3ed-919c23deccea"},"cell_type":"markdown","source":"### Loading GloVE"},{"metadata":{"_cell_guid":"667fea7f-3942-472b-9cde-b2fbec7b7acc","collapsed":true,"_uuid":"76051d5b0a1ca2c887c229f498a81f8789281b0f","trusted":false},"cell_type":"code","source":"print('Loading GloVE...')\n\nGLOVE_PATH = '../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\nembeddings_df = pd.read_table(GLOVE_PATH, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n\nword_embeddings_matrix = embeddings_df.values.astype(np.float32)\nprint('GloVE Word embeddings shape:', word_embeddings_matrix.shape)\nword_embedding_vocab = {t: i for (i, t) in enumerate(embeddings_df.index.tolist())}\n\ndel(embeddings_df)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a03732074ba95ec64d5a8bc31a229c72d8d4d28","_cell_guid":"55f74c2f-4631-4d25-ac9a-9d145f35edfc"},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"_cell_guid":"16207dac-f763-4827-bdf6-c77e091c29c6","collapsed":true,"_uuid":"a4dacfa3ff9727260e235549d6397cf2c2c03a2f","trusted":false},"cell_type":"code","source":"print('Reading train data...')\ndf_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', engine='c')\nprint('Train set size: {}'.format(len(df_train)))\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c5b8ca8381dc952a1bbd996859407a15c0f8ae4","_cell_guid":"da49a26d-2abb-4009-ae01-20aa9b0ed350"},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"_cell_guid":"03042807-b37e-4c66-8ffb-58ce0df6bd19","collapsed":true,"_uuid":"71cb31922da46f9067f9955e8dd6c0a1650be1c9","trusted":false},"cell_type":"code","source":"print('Generating features with statistics for item description textual content')\n\nacronyms_regex = re.compile('([A-Z\\-0-9]{2,})')\nhashtag_regex = re.compile(r'(#[a-z]{2,})')\n\n#Extracts statistics for each description, words lengths, like percentage of upper-case words, hashtags, etc\ndef extract_counts(text):\n    text_size_words_counts = len(text.split(' '))\n    text_size_words_log_counts = math.log1p(text_size_words_counts)\n    full_uppercase_perc = len(acronyms_regex.findall(text)) / float(text_size_words_counts)\n    exclamation_log_count = math.log1p(text.count('!'))\n    star_log_count = math.log1p(text.count('*'))\n    percentage_log_count = math.log1p(text.count('%'))\n    price_removed_marker_log_count = math.log1p(text.count('[rm]'))\n    hashtag_log_count = math.log1p(len(hashtag_regex.findall(text)))    \n    return [text_size_words_log_counts,\n            full_uppercase_perc,\n            exclamation_log_count,\n            star_log_count,            \n            percentage_log_count,\n            price_removed_marker_log_count,\n            hashtag_log_count]\n\nitem_descr_counts = np.vstack(df_train['item_description'].astype(str).apply(extract_counts).values)\n\nitem_descr_counts_scaler = StandardScaler(copy=True)\nX_item_descr_counts = item_descr_counts_scaler.fit_transform(item_descr_counts)\n\ndel(item_descr_counts)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"82019c37-1e9d-47a8-b93c-1464ee723ec8","collapsed":true,"_uuid":"e4b509d1c72b41f6d1dee8b677f6be1e2a0de101","trusted":false},"cell_type":"code","source":"#Removing target attribute (price) from training set, to avoid data leak\nprice = df_train.pop('price')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edb00036-1351-4a20-83bf-5cfc9ecbd0d6","collapsed":true,"_uuid":"162a4183f5ea647275657b6a68fbe8f53a68d2b7","trusted":false},"cell_type":"code","source":"print('Spliting train/validation set')\n#Defining train / eval sets\nvalid_rate = 0.00001 if SUBMISSION else 0.1\nvalid_size = int(len(df_train)*valid_rate)\n\nnp.random.seed(100)\nrows_idxs = np.arange(0,len(df_train))\nnp.random.shuffle(rows_idxs)\n\nvalid_idxs = rows_idxs[-valid_size:]\n\n#Ignoring lower prices in the train set (minimum price is 3.0 on Mercari website)\ntrain_zeroed_prices_idxs = price.iloc[np.in1d(price.index.values, valid_idxs, invert=True)][price < 3.0].index.values\ntrain_idxs = price.iloc[np.in1d(price.index.values, np.hstack([valid_idxs, train_zeroed_prices_idxs]), \n                                invert=True)].index.values\n\n#Validating the train / validation set split\nassert len(df_train) == len(train_zeroed_prices_idxs) + len(train_idxs) + len(valid_idxs)\nassert len(set(train_idxs).intersection(set(valid_idxs))) == 0\n\ntrain_size = len(train_idxs)\n\ndel(rows_idxs)\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"88f1279a-e755-4e03-be9c-bbb39f3ac133","collapsed":true,"_uuid":"a8f7ee33f4dd950d8a06f8afe2611e523b3aa83b","trusted":false},"cell_type":"code","source":"print('Normalizing price')\nprice_log = np.log1p(price)\n\nprice_log_train = price_log.iloc[train_idxs].values\nprice_log_train_mean = price_log_train.mean()\nprice_log_train_std = price_log_train.std()\ndel(price_log_train)\n\ny = (price_log - price_log_train_mean) / price_log_train_std\ny = y.values.reshape(-1, 1)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20cb7c48-ab9f-4470-a780-b381e74737ce","collapsed":true,"_uuid":"c20b4eb8353ec96ebec32cf4023cd5b07a34bf05","trusted":false},"cell_type":"code","source":"print('Filling null values...')\n\ndf_train.name.fillna('unk_name', inplace=True)\ndf_train.category_name.fillna('unk_cat', inplace=True)\ndf_train.brand_name.fillna('unk_brand', inplace=True)\ndf_train.item_description.fillna('unk_descr', inplace=True)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e79191a7-ae28-4be9-afc5-8fc9c2cd73df","collapsed":true,"_uuid":"206ed9fce06b850a134cf7d898d5ef4116d247fb","trusted":false},"cell_type":"code","source":"print('Gessing null Brands from name and category...')\n\ndef concat_categories(x):\n    return set(x.values)\n\n#Getting categories for brands\nbrand_names_categories = dict(df_train[df_train['brand_name'] != 'unk_brand'][['brand_name','category_name']].astype('str').groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\n\n#Brands sorted by length (decreasinly), so that longer brand names have precedence in the null brand search\nbrands_sorted_by_size = list(sorted(filter(lambda y: len(y) >= 3, list(brand_names_categories.keys())), key = lambda x: -len(x)))\n\nbrand_name_null_count = len(df_train.loc[df_train['brand_name'] == 'unk_brand'])\n\n#Try to guess the Brand based on Name and Category\ndef brandfinder(name, category):    \n    for brand in brands_sorted_by_size:\n        if brand in name and category in brand_names_categories[brand]:\n            return brand\n        \n    return 'unk_brand'\n\ntrain_names_unknown_brands = df_train[df_train['brand_name'] == 'unk_brand'][['name','category_name']].astype('str').values\ntrain_estimated_brands = Parallel(n_jobs=4)(delayed(brandfinder)(name, category) for name, category in train_names_unknown_brands)\ndf_train.ix[df_train['brand_name'] == 'unk_brand', 'brand_name'] = train_estimated_brands\n\nfound = brand_name_null_count-len(df_train.loc[df_train['brand_name'] == 'unk_brand'])\nprint(\"Null brands found: %d from %d\" % (found, brand_name_null_count))\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"055ac420-bcb7-41eb-802e-1f6cb557cc72","collapsed":true,"_uuid":"549ed16014ce34e18733f2d3097e790feae1daf1","trusted":false},"cell_type":"code","source":"print('Generating features from category statistics for price ...')\n\nCAT_STATS_MIN_COUNT = 5\nSTD_SIGMAS = 2\n\ndf_train['price_log'] = price_log\ncats_stats_df = df_train.iloc[train_idxs].groupby(['category_name', 'brand_name', 'shipping']).agg({'category_name': len,\n                                                     'price_log': [np.median, np.mean, np.std]})\ncats_stats_df.columns = ['price_log_median', 'price_log_mean', 'price_log_std','count']\n#Removing categories without a minimum threshold of samples, to avoid price data leak \ncats_stats_df.drop(cats_stats_df[cats_stats_df['count'] < CAT_STATS_MIN_COUNT].index, inplace=True)\ncats_stats_df['price_log_std'] = cats_stats_df['price_log_std'].fillna(0)\ncats_stats_df['price_log_conf_variance'] = cats_stats_df['price_log_std'] / cats_stats_df['price_log_mean']\ncats_stats_df['count_log'] = np.log1p(cats_stats_df['count'])\ncats_stats_df['min_expected_log_price'] = (cats_stats_df['price_log_mean'] - cats_stats_df['price_log_std']*STD_SIGMAS).clip(lower=1.0)\ncats_stats_df['max_expected_log_price'] = (cats_stats_df['price_log_mean'] + cats_stats_df['price_log_std']*STD_SIGMAS)\ndel(df_train['price_log'])\n\nlen(cats_stats_df)\n\n\ndef merge_with_cat_stats(df):\n    return df.merge(cats_stats_df.reset_index(), how='left', \n            on=['category_name', 'brand_name', 'shipping'])[['price_log_median', 'price_log_mean', 'price_log_std', \n                                               'price_log_conf_variance', 'count_log', 'min_expected_log_price', 'max_expected_log_price']].fillna(0).values\n\ntrain_cats_stats_features = merge_with_cat_stats(df_train)\n\ncats_stats_features_scaler = StandardScaler(copy=True)\nX_cats_stats_features_scaled = cats_stats_features_scaler.fit_transform(train_cats_stats_features)\n\ndel(train_cats_stats_features)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5becd7c2-468b-4803-a89f-7b34c5ec8752","collapsed":true,"_uuid":"2c9855d82b693d5b7225daa42de19daada535f40","trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4fa1275e-20ef-4a9f-9e90-1cd368334137","collapsed":true,"_uuid":"59a76c8ed78f9c18adb64072191b6e4ce8a9da55","trusted":false},"cell_type":"code","source":"#Joining the dense features\nX_float_features = np.hstack([X_item_descr_counts, X_cats_stats_features_scaled])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2924ecb-9a5a-4b07-be20-9fb2ee992170","collapsed":true,"_uuid":"8b0af1bd1dc9de96eaacda0a192c7f6a9ae4010b","trusted":false},"cell_type":"code","source":"#For Glove, spliting composite words separated by \"-\", because they are rare on Glove\nregex_tokenizer = RegexpTokenizer(r'[a-z][\\w&]*|[\\d]+[\\.]*[\\w]*|[/!?*:%$\"\\'\\-\\+=\\.,](?![/!?*:%$\"\\'-\\+=\\.,])')\n\ndef regex_tokenizer_nltk(text):\n    return regex_tokenizer.tokenize(text.lower())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e437bea-fd2c-4391-adff-313b6412da60","collapsed":true,"_uuid":"645383de8a1151e783972e57cf602175a4a5cb56","trusted":false},"cell_type":"code","source":"class Tokenizer:\n    def __init__(self, min_df=10, limit_length_transform=None, tokenizer=str.split, vocabulary=None, \n                 unk_token_if_ootv=False, workers=1):\n        self.min_df = min_df\n        self.tokenizer = tokenizer\n        self.limit_length_transform = limit_length_transform\n        self.workers = workers\n        self.vocab_idx = vocabulary\n        self.unk_token_if_ootv = unk_token_if_ootv\n        self.max_len = None  \n        \n    def tokenize(self, texts):\n        #Multi-processing\n        if self.workers>1:\n            tokenized_texts = Parallel(n_jobs=self.workers)(delayed(self.tokenizer)(t) for t in texts)\n        else:\n            tokenized_texts = [self.tokenizer(t) for t in texts] \n        return tokenized_texts\n                \n    def fit(self, texts):\n        doc_freq = Counter()\n\n        max_len = 0\n\n        if type(texts) is list:\n            tokenized_texts = texts\n        else: #str\n                        \n            tokenized_texts = self.tokenize(texts)\n            \n            for sentence in tokenized_texts:\n                if self.vocab_idx == None:\n                    doc_freq.update(set(sentence))\n                max_len = max(max_len, len(sentence))\n            \n        self.max_len = max_len\n\n        #If the vocabulary is not passed, build from text\n        if self.vocab_idx == None:\n            vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n            self.vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}     \n\n\n    def fit_transform(self, texts):\n        self.fit(texts)\n        return self.transform(texts)\n\n\n    def text_to_idx(self, tokenized):\n        if self.unk_token_if_ootv:            \n            return [self.vocab_idx[t] if t in self.vocab_idx else self.vocab_idx[UNK_TOKEN] for t in tokenized]\n        else:\n            return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n    \n    def transform(self, texts):\n        n = len(texts)\n        max_length = self.limit_length_transform or self.max_len\n        #Value 0 is reserved for the padding character (<PAD>)\n        result = np.zeros(shape=(n, max_length), dtype=np.int32)\n        \n        if self.workers>1:\n            tokenized_texts = Parallel(n_jobs=self.workers)(delayed(self.tokenizer)(t) for t in texts)\n        else:\n            tokenized_texts = [self.tokenizer(t) for t in texts]              \n        \n        for i, sentence in enumerate(tokenized_texts):\n            text = self.text_to_idx(sentence[:max_length])\n            result[i, :len(text)] = text\n\n        return result\n    \n    def vocabulary_size(self):\n        return len(self.vocab_idx) + 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3a17fb3-1f9e-4da8-995e-4eb8edde6835","collapsed":true,"_uuid":"6526c188d9be07c9cb710dd781dc4384a17c5197","trusted":false},"cell_type":"code","source":"print('Generating cumulative sub-categories features...')\n\ndef paths(tokens):\n    all_paths = ['/'.join(tokens[0:(i+1)]) for i in range(len(tokens))]\n    return ' '.join(all_paths)\n\nwhitespace_regex = re.compile(r'\\s+')\n@lru_cache(1024)\ndef cat_process(cat):\n    cat = cat.lower()\n    cat = whitespace_regex.sub('', cat)\n    split = cat.split('/')\n    return paths(split)\n\ndf_train['category_name_cum'] = df_train.category_name.apply(cat_process)\n\ncat_tok = Tokenizer(min_df=50)\ncat_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['category_name_cum'])\nX_cat = cat_tok.transform(df_train['category_name_cum'])\ncat_voc_size = cat_tok.vocabulary_size()\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f9b5b00-0f64-48eb-a0a5-2e749c6451ab","collapsed":true,"_uuid":"e783b17deeb3aab751178ffae6817feae92cf96d","trusted":false},"cell_type":"code","source":"print('Processing vocabulary for name and description....')\n\ngeneral_tokenizer = Tokenizer(min_df=30, tokenizer=regex_tokenizer_nltk, workers=4)\n\ngeneral_tokenizer.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['name'] + \" \" \\\n                    + df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['item_description'])\nprint(\"Text vocabulary size: {}\".format(len(general_tokenizer.vocab_idx)))\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41578dcf-b869-4ed0-bc98-25fb9975de06","collapsed":true,"_uuid":"3c22ebeea53e58605b971835939e9f21a54b4dad","trusted":false},"cell_type":"code","source":"print('Creating vocabulary and loading/generating word embeddings...')\n\nwords_vocab_with_embeddings = set(general_tokenizer.vocab_idx.keys()).intersection(set(word_embedding_vocab.keys()))\nwords_vocab_no_embeddings = set(general_tokenizer.vocab_idx.keys()) - words_vocab_with_embeddings\nprint('Found word embeddings for corpus: {} from {}'.format(len(words_vocab_with_embeddings), len(general_tokenizer.vocab_idx)))\n\nUNK_TOKEN = '<UNK>'\nPAD_TOKEN = '<PAD>'\n\n#Adding words without embedding in the start of the vocabulary\nwords_vocab_general = {t: i for (i, t) in enumerate([PAD_TOKEN, UNK_TOKEN] + sorted(words_vocab_no_embeddings))}\nwords_without_embeddings_vocab_size = len(words_vocab_general)\nwords_with_embeddings_vocab_size = len(words_vocab_with_embeddings)\n\n#Adding words with embedding in the end of the vocabulary\nfor (i, t) in enumerate(sorted(words_vocab_with_embeddings)):\n    words_vocab_general[t] = i+ words_without_embeddings_vocab_size\n    \n#Creating inverted vocabulary index\ncustom_inv_vocab_words = dict([(idx,w) for w, idx in list(words_vocab_general.items())])\n    \ntotal_vocab_size = len(words_vocab_general)\nprint('Words without embedding: %d\\tTotal vocabulary size: %d' % (words_without_embeddings_vocab_size, total_vocab_size))\n    \nembedding_size = word_embeddings_matrix.shape[1]  \nprint(\"Embedding size: %d\" % (embedding_size))\n\nnp.random.seed(10)\nmax_abs_embedding_random_value = np.sqrt(2 / embedding_size)\nglove_scaling_factor = word_embeddings_matrix.max() / max_abs_embedding_random_value\n\n#For words available in this GloVE dataset, loading embeddings\nwords_with_embeddings_matrix = word_embeddings_matrix[[word_embedding_vocab[custom_inv_vocab_words[i]] \n                                                       for i in range(words_without_embeddings_vocab_size, total_vocab_size)]] \\\n                                / glove_scaling_factor\n\n#For words NOT available in this GloVE dataset, generating random embeddings (according to GloVE distribution)\nwords_without_embeddings_matrix = np.random.normal(loc=words_with_embeddings_matrix.mean(), \n                                                   scale=words_with_embeddings_matrix.std(), \n                                                   size=(words_without_embeddings_vocab_size, embedding_size))\n\nprint(\"words_without_embeddings_matrix:\", words_without_embeddings_matrix.shape) \nprint(\"words_without_embeddings_matrix:\", words_with_embeddings_matrix.shape) \n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b17b3e9-477c-437f-9a2f-602fca38508f","collapsed":true,"_uuid":"1c97cae266c12b9a1244bc7ac4b40c2d590639e2","trusted":false},"cell_type":"code","source":"#Releasing original GloVE embeddings\ndel(word_embeddings_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"792c1602-99af-4956-b132-bbb48b8c6b08","collapsed":true,"_uuid":"506c95b174a756098997c3a3250358bd1a28ae17","trusted":false},"cell_type":"code","source":"#Maximum number of words of Name and Item_Description to be processed\nNAME_MAX_LEN = 20\nITEM_DESCR_MAX_LEN = 70","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b37cb4dd-179b-4b2b-be0f-2e2a642c77f8","collapsed":true,"_uuid":"ef84e38cff6379cbddcab6db3ced465f494bddd0","trusted":false},"cell_type":"code","source":"print('Processing Title...')\n\nname_tok = Tokenizer(min_df=0, limit_length_transform=NAME_MAX_LEN, tokenizer=regex_tokenizer_nltk, \n                     vocabulary=words_vocab_general, unk_token_if_ootv=True, workers=4)\nname_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['name'])\nX_name = name_tok.transform(df_train.name)\nprint(X_name.shape)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d488d5b4-c9a7-4d4e-adbf-e8898ebb78e2","collapsed":true,"_uuid":"0ead5d42e360c222793e78baa1adcddb9b0a4321","trusted":false},"cell_type":"code","source":"print('Processing Description...')\n\ndesc_tok = Tokenizer(min_df=0, limit_length_transform=ITEM_DESCR_MAX_LEN, tokenizer=regex_tokenizer_nltk, \n                     vocabulary=words_vocab_general, unk_token_if_ootv=True, workers=4)\n\ndesc_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])].item_description)\nX_desc = desc_tok.transform(df_train.item_description)\nprint(X_desc.shape)\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b23492a-c8d8-4938-b612-1f69d7b06a24","collapsed":true,"_uuid":"2e3c7eee57252e3349c082b16a29860c0d9524bc","trusted":false},"cell_type":"code","source":"print('Processing Brands...')\n\ndf_train.brand_name = df_train.brand_name.str.lower()\ndf_train.brand_name = df_train.brand_name.str.replace(' ', '_')\n\nbrand_cnt = Counter(df_train.brand_name[df_train.brand_name != 'unk_brand'])\nbrands = sorted(b for (b, c) in brand_cnt.items() if c >= 20)\nbrands_idx = {b: (i + 1) for (i, b) in enumerate(brands)}\n\nX_brand = df_train.brand_name.apply(lambda b: brands_idx.get(b, 0))\nX_brand = X_brand.values.reshape(-1, 1) \nbrand_voc_size = len(brands) + 1\nprint(\"Brands vocab. size: {}\".format(brand_voc_size))\n\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99dde078-5ad6-474d-8e0a-f5b9fbd0818b","collapsed":true,"_uuid":"9c51f8f7b2af60f10d6e3631f7dd9f19abcceeef","trusted":false},"cell_type":"code","source":"print('Processing Item condition and Shipping...')\nX_item_cond = (df_train.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\nX_shipping = df_train.shipping.astype('float32').values.reshape(-1, 1)\nprint_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a5d8f5107988c80901187a3c678d8ed4b70be2a","_cell_guid":"11149f56-1afd-44c5-a055-61a5e34efb55"},"cell_type":"markdown","source":"### CNN training"},{"metadata":{"_cell_guid":"a1457ae4-530d-4175-b445-a8e445c69e07","collapsed":true,"_uuid":"b248c53fe4ac4dd1b5b82182e8c937f8164aafe0","trusted":false},"cell_type":"code","source":"def prepare_batches(seq, step):\n    n = len(seq)\n    res = []\n    for i in range(0, n, step):\n        res.append(seq[i:i+step])\n    return res","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61d0a4d8-95b8-4f27-b25f-d3ab2d66a0df","collapsed":true,"_uuid":"684b86a2fa5689906d38d57d65fc74dd4cdfb40a","trusted":false},"cell_type":"code","source":"def train_model(session, epochs=4, batch_size=500, eval_each_epoch=True, dropout_input_words=0.0):\n    print('\\ntraining the model...')\n    \n    print_elapsed()\n    \n    training_size = train_idxs.shape[0]\n\n    for i in range(int(np.ceil(epochs))):\n        print(\"-----------------EPOCH: {}-------------------\".format(i))\n        t0 = time()\n        np.random.seed(i)\n\n        epoch_size = training_size\n        #If the epoch is not int (eg. 2.5), reduces the last epoch size (number of steps)\n        if i+1 - epochs > 0:\n            epoch_size = int(training_size*(epochs%1))\n            \n        #Training dataset shuffling\n        batches = prepare_batches(np.random.permutation(train_idxs)[:epoch_size], batch_size)\n        \n        for idx in batches:\n            name_batch = X_name[idx]\n            desc_batch = X_desc[idx]\n            \n            #If set, apply dropout to the input words (kind of data augmentation)\n            if dropout_input_words > 0:            \n                name_mask = (np.random.uniform(0,1, size=name_batch.shape) >= dropout_input_words).astype(np.int32)\n                name_batch = name_batch * name_mask\n\n                desc_mask = (np.random.uniform(0,1, size=desc_batch.shape) >= dropout_input_words).astype(np.int32)\n                desc_batch = desc_batch * desc_mask\n        \n            feed_dict = {\n                place_name: name_batch,\n                place_desc: desc_batch,\n                place_brand: X_brand[idx],\n                place_cat: X_cat[idx],\n                place_cond: X_item_cond[idx],\n                place_ship: X_shipping[idx],\n                place_float_stats: X_float_features[idx],\n                place_y: y[idx],\n                place_training: True\n            }\n            session.run(train_step, feed_dict=feed_dict)\n\n        took = time() - t0\n        print('epoch %d took %.3fs' % (i, took))\n        \n        if eval_each_epoch and i < epochs-1:\n            train_set_evaluation(sess)\n            print_elapsed()\n            \n            eval_set_evaluation(session)\n            \n        print_elapsed()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f25927e9-52a4-46c8-8c7b-04510715b17b","collapsed":true,"_uuid":"61322cee3fc1a6671a4f5dbd2950f9a9decb804d","trusted":false},"cell_type":"code","source":"def eval_set_evaluation(session, save_results_to=None):\n    print('\\nEVAL SET Evaluation')\n\n    y_pred_norm_eval = np.zeros(valid_size)\n\n    EVAL_BATCH_SIZE = 5000\n\n    batches = prepare_batches(valid_idxs, EVAL_BATCH_SIZE)\n\n    for b, idx in enumerate(batches):\n        feed_dict = {\n            place_name: X_name[idx],\n            place_desc: X_desc[idx],\n            place_brand: X_brand[idx],\n            place_cat: X_cat[idx],\n            place_cond: X_item_cond[idx],\n            place_ship: X_shipping[idx],\n            place_float_stats: X_float_features[idx],\n            place_training: False\n        }\n        batch_pred = session.run(out, feed_dict=feed_dict)\n        start_idx = (b*EVAL_BATCH_SIZE)\n        y_pred_norm_eval[start_idx:min(start_idx+EVAL_BATCH_SIZE, valid_size)] = batch_pred[:, 0]\n        \n    y_adjusted_pred_eval = np.expm1(y_pred_norm_eval * price_log_train_std + price_log_train_mean)\n\n    print(\"Eval set RMSLE: {}\".format(rmsle(price[valid_idxs].values, y_adjusted_pred_eval)))    \n\n    if not SUBMISSION and save_results_to != None:\n        print('Saving CNN eval results to \"valid_cnn_predictions.csv\"...')    \n        df_out = pd.DataFrame()\n        df_out['train_id'] = df_train.iloc[valid_idxs]['train_id']\n        df_out['price'] = price[valid_idxs].values\n        df_out['pred_price'] = y_adjusted_pred_eval\n        df_out.to_csv(save_results_to, index=False)    \n\n    return y_pred_norm_eval","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53c72ca7-baa5-4859-a1a7-1d0d23b1efce","collapsed":true,"_uuid":"380b55dd20b75831b6e2bf123fe73859cbd9416a","scrolled":true,"trusted":false},"cell_type":"code","source":"def train_set_evaluation(session):\n    print('\\nTRAIN SET Evaluation')\n\n    y_pred_norm = np.zeros(train_size)\n\n    EVAL_BATCH_SIZE = 5000\n\n    batches = prepare_batches(train_idxs, EVAL_BATCH_SIZE)\n\n    for b, idx in enumerate(batches):\n        feed_dict = {\n            place_name: X_name[idx],\n            place_desc: X_desc[idx],\n            place_brand: X_brand[idx],\n            place_cat: X_cat[idx],\n            place_cond: X_item_cond[idx],\n            place_ship: X_shipping[idx],\n            place_float_stats: X_float_features[idx],\n            place_training: False\n        }\n        batch_pred = session.run(out, feed_dict=feed_dict)\n        start_idx = (b*EVAL_BATCH_SIZE)\n        \n        y_pred_norm[start_idx:min(start_idx+EVAL_BATCH_SIZE, train_size)] = batch_pred[:, 0]                \n\n    y_pred_train = y_pred_norm * price_log_train_std + price_log_train_mean\n    y_adjusted_pred_train = np.expm1(y_pred_train)\n\n    print(\"Train set RMSLE: {}\".format(rmsle(price[train_idxs].values, y_adjusted_pred_train)))\n    \n    return price[train_idxs].values, y_adjusted_pred_train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7477ed9d-5421-4536-973a-1cb5f15360bc","collapsed":true,"_uuid":"fa7b412da7ced5e63261f40e6b7b20d73dea1e98","trusted":false},"cell_type":"code","source":"def save_test_set_predictions(preds, filename):\n    df_out = pd.DataFrame()\n    df_out['test_id'] = range(0,preds.shape[0])\n    df_out['price'] = preds\n\n    df_out.to_csv(filename, index=False)    \n    \n    print_elapsed('Predictions exported to {}'.format(filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"735280f3-cfe0-45c5-9ac8-a94faf0c8316","collapsed":true,"_uuid":"69695a2f1da065fb7c03a86ccb5b723ac5f9cba1","trusted":false},"cell_type":"code","source":"def generate_test_submission(session):\n    print('Reading and generating features for test data...')\n\n    #TODO: Read data (read_table) in batches (ex: read smaller batches using chunksize) \n    #to better support larger testset in 2nd phase\n    df_test = pd.read_csv('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t')\n    \n    #Filling nulls\n    df_test.name.fillna('unk_name', inplace=True)\n    df_test.category_name.fillna('unk_cat', inplace=True)\n    df_test.brand_name.fillna('unk_brand', inplace=True)\n    df_test.item_description.fillna('unk_brand', inplace=True)\n    \n    #Guessing null Brands\n    test_names_unknown_brands = df_test[df_test['brand_name'] == 'unk_brand'][['name','category_name']].astype('str').values\n    test_estimated_brands = Parallel(n_jobs=7)(delayed(brandfinder)(name, category) for name, category in test_names_unknown_brands)\n    df_test.ix[df_test['brand_name'] == 'unk_brand', 'brand_name'] = test_estimated_brands\n\n    #Processing categories\n    df_test.category_name = df_test.category_name.apply(cat_process)\n    df_test.brand_name = df_test.brand_name.str.lower()\n    df_test.brand_name = df_test.brand_name.str.replace(' ', '_')\n    \n    #Generating statistic features for Description \n    X_item_descr_counts_test = item_descr_counts_scaler.transform(np.vstack(df_test['item_description'].astype(str) \\\n                                                                            .apply(extract_counts).values))\n\n    #Generating features of price statistics by categories\n    X_cats_stats_features_scaled_test = cats_stats_features_scaler.transform(merge_with_cat_stats(df_test))\n    \n    #Joining dense features\n    X_float_features_test = np.hstack([X_item_descr_counts_test, X_cats_stats_features_scaled_test])\n    \n    #Tokenizing category name\n    X_cat_test = cat_tok.transform(df_test.category_name)\n    X_name_test = name_tok.transform(df_test.name)\n\n    #Tokenizing description\n    X_desc_test = desc_tok.transform(df_test.item_description)\n\n    #Tokenizing category name\n    X_item_cond_test = (df_test.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n    X_shipping_test = df_test.shipping.astype('float32').values.reshape(-1, 1)\n\n    #Processing brands\n    X_brand_test = df_test.brand_name.apply(lambda b: brands_idx.get(b, 0))\n    X_brand_test = X_brand_test.values.reshape(-1, 1)\n    \n    print_elapsed('Finish generating features for test set')\n    \n    \n    print('Prediction for test set using the trained CNN model...')\n\n    n_test = len(df_test)\n    y_pred = np.zeros(n_test)\n\n    test_idx = np.arange(n_test)\n    batches = prepare_batches(test_idx, 5000)\n\n    for idx in batches:\n        feed_dict = {\n            place_name: X_name_test[idx],\n            place_desc: X_desc_test[idx],\n            place_brand: X_brand_test[idx],\n            place_cat: X_cat_test[idx],\n            place_cond: X_item_cond_test[idx],\n            place_ship: X_shipping_test[idx],            \n            place_float_stats: X_float_features_test[idx],\n            place_training: False\n        }\n        \n        batch_pred = session.run(out, feed_dict=feed_dict)\n        y_pred[idx] = batch_pred[:, 0]\n\n    print_elapsed()\n    \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4dea2044-e977-47ea-97a5-f9fdaa69ba5b","collapsed":true,"_uuid":"f2ae903aafbbb753830c1dc05d66e51dd71a2ec9","trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27965dff-c8d2-4967-9d5d-f2e604e68632","collapsed":true,"_uuid":"c52acb4ee807b898f523b2d796d67aeeb3c779ff","scrolled":true,"trusted":false},"cell_type":"code","source":"print('Defining the model...')\n\ndef conv1d(inputs, num_filters, filter_size, pool_size, is_training, reg=0.0, activation=None, padding='same'):\n\n    out = tf.layers.conv1d(\n        inputs=inputs, filters=num_filters, padding=padding,\n        kernel_size=filter_size,\n        strides=1,\n        activation=activation,      \n        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n        kernel_regularizer=tf.contrib.layers.l2_regularizer(reg),\n        )\n    \n    out = tf.layers.max_pooling1d(out, pool_size=pool_size, strides=1, padding='valid')\n                                       \n    return out\n\ndef dense(X, size, reg=0.0, activation=None):\n    out = tf.layers.dense(X, units=size, activation=activation, \n                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n    return out\n\ndef embed(inputs, size, dim):\n    std = np.sqrt(2 / dim)\n    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n    lookup = tf.nn.embedding_lookup(emb, inputs)\n    return lookup\n\n\ndnn_settings = {\n    #Training params\n    'lr_initial': 0.002, \n    'lr_decay_rate': 0.94, \n    'lr_num_epochs_per_decay': 0.1,\n    'batch_size': 256,\n    'epochs': 3.0, #2.5, #3\n    \n    'dropout_input_words': 0.05, #Randomly set to zero (<PAD>) some words of the input text (data augmentation)\n    'dropout_rate': 0.00,\n    'l2_reg': 0.0, \n    'main_batch_norm_decay': 0.93,\n\n     #Model params\n    'cnn_filter_sizes': [3], \n\n    'name_seq_len': NAME_MAX_LEN,\n    'name_num_filters': 128,\n    'name_avg_embedding_num_words': 5,    \n\n    'desc_seq_len': ITEM_DESCR_MAX_LEN,\n    'desc_num_filters': 96, \n    'descr_avg_embedding_num_words': 20,\n\n    'brand_embeddings_dim': 32, \n\n    'cat_embeddings_dim': 32, \n    'cat_seq_len': X_cat.shape[1],\n\n    'word_embeddings_size': embedding_size,\n    'word_embeddings_max_norm': 0.45,\n}\n\nprint(\"SETTINGS:\", dnn_settings)\n\n\nprint()\n\n\ngraph = tf.Graph()\ngraph.seed = 1\n\nALLOW_SOFT_PLACEMENT=True\nLOG_DEVICE_PLACEMENT=False\n\nwith graph.as_default():\n    \n    #As GPUs were not available on Kaggle Kernelm using them only for local development \n    with tf.device('/cpu:0' if SUBMISSION else '/gpu:0'):    \n    \n        session_conf = tf.ConfigProto(\n          allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n          log_device_placement=LOG_DEVICE_PLACEMENT,\n          intra_op_parallelism_threads = 4,\n          inter_op_parallelism_threads = 4)\n\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            \n            with tf.name_scope(\"embedding\"):          \n                \n                #Loading GloVE word embeddings for available words \n                #Reference: https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/\n                words_with_embedding_placeholder = tf.placeholder(tf.float32, [words_with_embeddings_vocab_size,\n                                                                               dnn_settings['word_embeddings_size']])\n                words_with_embedding_variable = tf.Variable(tf.constant(0.0, shape=[words_with_embeddings_vocab_size,\n                                                                                    dnn_settings['word_embeddings_size']]),\n                    #Best results were obtaining using Glove to initialize embeddings, \n                    #and letting them to be trained for the prediction task\n                    trainable=True,                \n                    name=\"words_with_embedding\")\n                words_with_embedding_init = words_with_embedding_variable.assign(words_with_embedding_placeholder)\n                \n                #Loading random word embeddings for OOTV words\n                words_without_embedding_placeholder = tf.placeholder(tf.float32, [words_without_embeddings_vocab_size,\n                                                                                  dnn_settings['word_embeddings_size']])\n                words_without_embedding_variable = tf.Variable(tf.constant(0.0, shape=[words_without_embeddings_vocab_size,\n                                                                                       dnn_settings['word_embeddings_size']]),\n                    trainable=True,                \n                    name=\"words_without_embedding\")\n                words_without_embedding_init = words_without_embedding_variable.assign(words_without_embedding_placeholder)\n                \n                #Creating a regularizer for embeddings values\n                word_embedding_regularizer = tf.nn.l2_loss(words_without_embedding_variable)\n\n                #Concatenating GloVE embeddings and random embeddings in a single variable\n                words_embedding_variable = tf.concat([words_without_embedding_variable, words_with_embedding_variable], \n                                                    axis=0)\n\n            \n            #Model input features\n            place_name = tf.placeholder(tf.int32, shape=(None, dnn_settings['name_seq_len']))\n            place_desc = tf.placeholder(tf.int32, shape=(None, dnn_settings['desc_seq_len']))\n            place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n            place_cat = tf.placeholder(tf.int32, shape=(None, dnn_settings['cat_seq_len']))\n            place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n            place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n            place_float_stats = tf.placeholder(dtype=tf.float32, shape=(None, X_float_features.shape[1]))\n            \n            #Output feature\n            place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n            \n            #Flag to indicate whether the graph is being trained or used for inference\n            place_training = tf.placeholder(tf.bool, shape=(), )\n            \n            #Creating embedding layer for categorical features Brands and Categories\n            brand = embed(place_brand, brand_voc_size, dnn_settings['brand_embeddings_dim'])\n            cat = embed(place_cat, cat_voc_size, dnn_settings['cat_embeddings_dim'])\n                        \n            #Looking up embeddings for each word            \n            name = tf.nn.embedding_lookup(words_embedding_variable, place_name, max_norm=dnn_settings['word_embeddings_max_norm'])\n            desc = tf.nn.embedding_lookup(words_embedding_variable, place_desc, max_norm=dnn_settings['word_embeddings_max_norm'])\n            print(\"name.shape\", name.shape)\n            print(\"desc.shape\", desc.shape)\n            \n            #Creating a special layer to average embeddings of the first words of the name and description\n            #under the assumption that they are the most representative ones\n            name_mean_embeddings = tf.reduce_mean(name[:,:dnn_settings['name_avg_embedding_num_words'],:], axis=1)\n            print(\"name_mean_embeddings.shape\", name_mean_embeddings.shape)\n            desc_mean_embeddings = tf.reduce_mean(desc[:,:dnn_settings['descr_avg_embedding_num_words'],:], axis=1)\n            print(\"desc_mean_embeddings.shape\", desc_mean_embeddings.shape)\n            \n            \n            conv_layers = []\n            for filter_size in dnn_settings['cnn_filter_sizes']:\n                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n                    name_conv = conv1d(name, num_filters=dnn_settings['name_num_filters'], filter_size=filter_size, \n                                       pool_size=dnn_settings['name_seq_len'], \n                                       is_training=place_training, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)                \n                    name_conv = tf.contrib.layers.flatten(name_conv)\n                    print((\"conv-maxpool-%s NAME\" % filter_size), name_conv.shape)\n                    conv_layers.append(name_conv)\n\n                    desc_conv = conv1d(desc, num_filters=dnn_settings['desc_num_filters'], filter_size=filter_size, \n                                       pool_size=dnn_settings['desc_seq_len'], \n                                       is_training=place_training, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n                    desc_conv = tf.contrib.layers.flatten(desc_conv)\n                    print((\"conv-maxpool-%s DESCRIPTION\" % filter_size), desc_conv.shape)\n                    conv_layers.append(desc_conv)\n\n            brand = tf.contrib.layers.flatten(brand)\n            print(brand.shape)\n\n            #cat = tf.layers.average_pooling1d(cat, pool_size=dnn_settings['cat_seq_len'], strides=1, padding='valid')\n            cat = tf.contrib.layers.flatten(cat)\n            print(cat.shape)\n\n            ship = place_ship\n            print(ship.shape)\n\n            cond = tf.one_hot(place_cond, 5)\n            cond = tf.contrib.layers.flatten(cond)\n            print(cond.shape)\n            \n            float_stats = place_float_stats\n            float_stats = tf.contrib.layers.flatten(float_stats)\n            print(float_stats.shape)\n              \n            #Joining all layers outputs for a sequence of Fully Connected layers\n            out = tf.concat(conv_layers + [name_mean_embeddings, desc_mean_embeddings, brand, cat, ship, cond, float_stats], axis=1)\n            print('concatenated dim:', out.shape)\n\n\n            out = tf.contrib.layers.batch_norm(out, decay=dnn_settings['main_batch_norm_decay'], \n                                               center=True, scale=False, epsilon=0.001,           \n                                               is_training=place_training)\n            \n\n            #out = tf.layers.dropout(out, rate=dropout_rate, training=place_training)\n            out = dense(out, 256, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n\n            #out = tf.layers.dropout(out, rate=dropout_rate, training=place_training)\n            out = dense(out, 128, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n            \n            out = tf.contrib.layers.layer_norm(tf.concat([out, ship, cond, float_stats], axis=1))\n\n            out = dense(out, 1, reg=dnn_settings['l2_reg'])\n\n            reg_loss = tf.losses.get_regularization_loss() + dnn_settings['l2_reg']*word_embedding_regularizer\n            \n            #Computing the loss, with L2 regularization\n            loss = tf.losses.mean_squared_error(place_y, out) + reg_loss\n            rmse = tf.sqrt(loss)\n            \n            #Setting learning rate decay\n            lr_decay_steps = int(train_size / dnn_settings['batch_size'] * dnn_settings['lr_num_epochs_per_decay'])\n            \n            global_step = tf.Variable(0, trainable=False)\n            learning_rate_decay = tf.train.exponential_decay(dnn_settings['lr_initial'],\n                                          global_step,\n                                          lr_decay_steps,\n                                          dnn_settings['lr_decay_rate'],\n                                          staircase=True,\n                                          name='exponential_decay_learning_rate')\n            \n            opt = tf.train.AdamOptimizer(learning_rate=learning_rate_decay,\n                                         beta1=0.9,\n                                         beta2=0.999,\n                                         epsilon=1e-08\n                                        )\n            \n            #Necessary to run update ops for batch_norm\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):                \n                train_step = opt.minimize(loss=loss, global_step=global_step)\n\n            print(\"Initializing variables\")\n            init = tf.global_variables_initializer()        \n            sess.run(init)\n            print_elapsed()\n                  \n            #Initializing word embeddings variable\n            sess.run([words_without_embedding_init,\n                      words_with_embedding_init], \n                     feed_dict={words_without_embedding_placeholder: words_without_embeddings_matrix,\n                                words_with_embedding_placeholder: words_with_embeddings_matrix})\n\n            #Training the model\n            train_model(sess, epochs=dnn_settings['epochs'], batch_size=dnn_settings['batch_size'], \n                        dropout_input_words=dnn_settings['dropout_input_words'], \n                        eval_each_epoch=not SUBMISSION)\n            print_elapsed()\n\n            if not SUBMISSION:\n                #Evaluating train set\n                train_actual_prices_debug, train_pred_prices_debug = train_set_evaluation(sess)\n                print_elapsed()\n\n                #Evaluating validation set\n                cnn_pred_eval = eval_set_evaluation(sess)\n                print_elapsed()\n\n            if SUBMISSION:\n                #Generating output CSV file with the predictions for test set\n                cnn_pred_test = generate_test_submission(sess)                \n                cnn_pred_test_scaled = np.expm1(cnn_pred_test * price_log_train_std + price_log_train_mean)\n                save_test_set_predictions(cnn_pred_test_scaled, 'submission_cnn.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1718f342-2a34-438a-a574-14d198d1a7b0","collapsed":true,"_uuid":"7c3c9b3c9154e341deec35292c86fafe19dcfb03","trusted":false},"cell_type":"code","source":"print_elapsed('Finished script')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}