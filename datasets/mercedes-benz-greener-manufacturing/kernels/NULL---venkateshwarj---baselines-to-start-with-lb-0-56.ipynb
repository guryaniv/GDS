{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "42f56561-2bf8-d63b-f0f4-80bad6f10205"
      },
      "source": [
        "Hi, Kagglers!\n",
        "\n",
        "Hereafter I will try to publish **some basic approaches to climb up the Leaderboard**\n",
        "\n",
        "**Competition goal**\n",
        "\n",
        "In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench.\n",
        "<br>Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. <br>Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards. \n",
        "\n",
        "**The Notebook adopts skeleton from (maybe?) this script: https://www.kaggle.com/ermolushka/starter-xgboost**\n",
        "\n",
        "### Stay tuned, this notebook will be updated on a regular basis\n",
        "**P.s. Upvotes and comments would let me update it faster and in a more smart way :)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f56612b7-5e56-a982-e444-2e803d037696"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9b0c5167-5916-1295-6c9a-dd1f6398c2ce"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1860d05-f575-5b2b-af7e-53478e209504"
      },
      "outputs": [],
      "source": [
        "# read datasets\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "# process columns, apply LabelEncoder to categorical features\n",
        "for c in train.columns:\n",
        "    if train[c].dtype == 'object':\n",
        "        lbl = LabelEncoder() \n",
        "        lbl.fit(list(train[c].values) + list(test[c].values)) \n",
        "        train[c] = lbl.transform(list(train[c].values))\n",
        "        test[c] = lbl.transform(list(test[c].values))\n",
        "\n",
        "# shape        \n",
        "print('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e1b670a7-996f-1b2c-4f94-46fbac0ff748"
      },
      "source": [
        "### Add decomposed components: PCA / ICA etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4d2a6a5-f3fb-241f-ff1b-27206031e5d5"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA, FastICA\n",
        "n_comp = 10\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=n_comp, random_state=42)\n",
        "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
        "pca2_results_test = pca.transform(test)\n",
        "\n",
        "# ICA\n",
        "ica = FastICA(n_components=n_comp, random_state=42)\n",
        "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
        "ica2_results_test = ica.transform(test)\n",
        "\n",
        "# Append decomposition components to datasets\n",
        "for i in range(1, n_comp+1):\n",
        "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
        "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
        "    \n",
        "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
        "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
        "    \n",
        "y_train = train[\"y\"]\n",
        "y_mean = np.mean(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "48fabb21-c780-c666-74f1-9fd1dbfe3c4e"
      },
      "source": [
        "### Preparing Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29b158b3-ce67-f979-31d4-11103dd0b968"
      },
      "outputs": [],
      "source": [
        " ()# mmm, xgboost, loved by everyone ^-^\n",
        "import xgboost as xgb\n",
        "\n",
        "# prepare dict of params for xgboost to run with\n",
        "xgb_params = {\n",
        "    'n_trees': 500, \n",
        "    'eta': 0.005,\n",
        "    'max_depth': 4,\n",
        "    'subsample': 0.95,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'base_score': y_mean, # base prediction = mean(target)\n",
        "    'silent': 1\n",
        "}\n",
        "\n",
        "# form DMatrices for Xgboost training\n",
        "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
        "dtest = xgb.DMatrix(test)\n",
        "\n",
        "# xgboost, cross-validation\n",
        "cv_result = xgb.cv(xgb_params, \n",
        "                   dtrain, \n",
        "                   num_boost_round=500, # increase to have better results (~700)\n",
        "                   early_stopping_rounds=50,\n",
        "                   verbose_eval=50, \n",
        "                   show_stdv=False\n",
        "                  )\n",
        "\n",
        "num_boost_rounds = len(cv_result)\n",
        "print(num_boost_rounds)\n",
        "\n",
        "# train model\n",
        "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f634dac-4ad8-7c09-9603-5fa817140136"
      },
      "outputs": [],
      "source": [
        "# check f2-score (to get higher score - increase num_boost_round in previous cell)\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# now fixed, correct calculation\n",
        "print(r2_score(dtrain.get_label(), model.predict(dtrain)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "169562ea-efbe-92b1-6f13-08f34b6597a5"
      },
      "outputs": [],
      "source": [
        "# make predictions and save results\n",
        "y_pred = model.predict(dtest)\n",
        "output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n",
        "output.to_csv('xgboost-depth{}-pca-ica.csv'.format(xgb_params['max_depth']), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c338da2-d3d6-7045-f0ce-a4fea72dda4b"
      },
      "source": [
        "### Prepare Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ffe421af-bc48-91a5-a0da-7cb6ad0f74e4"
      },
      "source": [
        "### Stay tuned, this notebook will be updated on a regular basis\n",
        "**P.s. Upvotes and comments would let me update it faster and in a more smart way :)**"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}