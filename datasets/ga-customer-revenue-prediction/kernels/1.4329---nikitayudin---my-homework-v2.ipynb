{"cells":[{"metadata":{"_uuid":"ec396cdd24815975cad62a8659954d2493a95611"},"cell_type":"markdown","source":"# Introduction\n#### (preliminary notes: this kernel was created for educational purposes only)\nThis kernel is based on several kernels: [this](https://www.kaggle.com/ashishpatel26/bayesian-lgbm-xgb-cat-fe-groupkfold-cv) and [this](https://www.kaggle.com/prashantkikani/teach-lightgbm-to-sum-predictions-fe). Initial data was preprocessed using this [script](https://www.kaggle.com/ogrellier/create-extracted-json-fields-dataset). Optimization routine for hyperparameters estimation is based on this [kernel](https://www.kaggle.com/qwe1398775315/eda-lgbm-bayesianoptimization). My solution consists of these steps:\n* Extract preprocessed data\n* Add some features\n* Train baseline LightGBM model for session-level predictions\n* Encode categorical features using frequencies of categories\n* Train visitor-level LightGBM model to predict revenue\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport gc\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold, GridSearchCV, KFold\ngc.enable()\nfrom sklearn.model_selection import train_test_split\n#from bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a885bd16dccd0feabdc53b7ad49387242fbf3f"},"cell_type":"markdown","source":"## loading data"},{"metadata":{"trusted":true,"_uuid":"30107d4d2056f4bb3d3b6793cdf12abfee78e78d"},"cell_type":"code","source":"train = pd.read_csv('../input/create-extracted-json-fields-dataset/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('../input/create-extracted-json-fields-dataset/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"232f3b7629c22123f061fd9de4551fa9dc5fcdc6"},"cell_type":"markdown","source":"## function for visitor-level cross validation"},{"metadata":{"trusted":true,"_uuid":"ce4fd0174b1996275f2a63c25ed13344237544bd"},"cell_type":"code","source":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e827e3d4be54f5785aeeb2543bf9d76db374b1e"},"cell_type":"markdown","source":"## target definition and feature extraction"},{"metadata":{"trusted":true,"_uuid":"1dd82119910aac986aae189088a5891abff99584"},"cell_type":"code","source":"y_reg = train['totals.transactionRevenue'].fillna(0)\ndel train['totals.transactionRevenue']\n\nif 'totals.transactionRevenue' in test.columns:\n    del test['totals.transactionRevenue']\n\nfor df in [train, test]:\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['date'].dt.dayofweek\n    df['sess_date_hours'] = df['date'].dt.hour\n    df['sess_date_dom'] = df['date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1c7952ab94c86fe890e9efb8a4e0b65e7da53f8"},"cell_type":"code","source":"def browser_mapping(x):\n    browsers = ['chrome','safari','firefox','internet explorer','edge','opera','coc coc','maxthon','iron']\n    if x in browsers:\n        return x.lower()\n    elif  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x):\n        return 'mobile browser'\n    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x):\n        return 'mobile browser'\n    elif '(not set)' in x:\n        return x\n    else:\n        return 'others'\n    \n    \ndef adcontents_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('placement' in x) | ('placememnt' in x):\n        return 'placement'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'ad' in x:\n        return 'ad'\n    else:\n        return 'others'\n    \ndef source_mapping(x):\n    if  ('google' in x):\n        return 'google'\n    elif  ('youtube' in x):\n        return 'youtube'\n    elif '(not set)' in x or 'nan' in x:\n        return x\n    elif 'yahoo' in x:\n        return 'yahoo'\n    elif 'facebook' in x:\n        return 'facebook'\n    elif 'reddit' in x:\n        return 'reddit'\n    elif 'bing' in x:\n        return 'bing'\n    elif 'quora' in x:\n        return 'quora'\n    elif 'outlook' in x:\n        return 'outlook'\n    elif 'linkedin' in x:\n        return 'linkedin'\n    elif 'pinterest' in x:\n        return 'pinterest'\n    elif 'ask' in x:\n        return 'ask'\n    elif 'siliconvalley' in x:\n        return 'siliconvalley'\n    elif 'lunametrics' in x:\n        return 'lunametrics'\n    elif 'amazon' in x:\n        return 'amazon'\n    elif 'mysearch' in x:\n        return 'mysearch'\n    elif 'qiita' in x:\n        return 'qiita'\n    elif 'messenger' in x:\n        return 'messenger'\n    elif 'twitter' in x:\n        return 'twitter'\n    elif 't.co' in x:\n        return 't.co'\n    elif 'vk.com' in x:\n        return 'vk.com'\n    elif 'search' in x:\n        return 'search'\n    elif 'edu' in x:\n        return 'edu'\n    elif 'mail' in x:\n        return 'mail'\n    elif 'ad' in x:\n        return 'ad'\n    elif 'golang' in x:\n        return 'golang'\n    elif 'direct' in x:\n        return 'direct'\n    elif 'dealspotr' in x:\n        return 'dealspotr'\n    elif 'sashihara' in x:\n        return 'sashihara'\n    elif 'phandroid' in x:\n        return 'phandroid'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'mdn' in x:\n        return 'mdn'\n    elif 'duckduckgo' in x:\n        return 'duckduckgo'\n    elif 'seroundtable' in x:\n        return 'seroundtable'\n    elif 'metrics' in x:\n        return 'metrics'\n    elif 'sogou' in x:\n        return 'sogou'\n    elif 'businessinsider' in x:\n        return 'businessinsider'\n    elif 'github' in x:\n        return 'github'\n    elif 'gophergala' in x:\n        return 'gophergala'\n    elif 'yandex' in x:\n        return 'yandex'\n    elif 'msn' in x:\n        return 'msn'\n    elif 'dfa' in x:\n        return 'dfa'\n    elif '(not set)' in x:\n        return '(not set)'\n    elif 'feedly' in x:\n        return 'feedly'\n    elif 'arstechnica' in x:\n        return 'arstechnica'\n    elif 'squishable' in x:\n        return 'squishable'\n    elif 'flipboard' in x:\n        return 'flipboard'\n    elif 't-online.de' in x:\n        return 't-online.de'\n    elif 'sm.cn' in x:\n        return 'sm.cn'\n    elif 'wow' in x:\n        return 'wow'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'partners' in x:\n        return 'partners'\n    else:\n        return 'others'\n\ntrain['device.browser'] = train['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\ntrain['trafficSource.adContent'] = train['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\ntrain['trafficSource.source'] = train['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n\ntest['device.browser'] = test['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\ntest['trafficSource.adContent'] = test['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\ntest['trafficSource.source'] = test['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n\ndef process_device(data_df):\n    print(\"process device ...\")\n    data_df['source.country'] = data_df['trafficSource.source'] + '_' + data_df['geoNetwork.country']\n    data_df['campaign.medium'] = data_df['trafficSource.campaign'] + '_' + data_df['trafficSource.medium']\n    data_df['browser.category'] = data_df['device.browser'] + '_' + data_df['device.deviceCategory']\n    data_df['browser.os'] = data_df['device.browser'] + '_' + data_df['device.operatingSystem']\n    return data_df\n\ntrain = process_device(train)\ntest = process_device(test)\n\ndef custom(data):\n    print('custom..')\n    data['device_deviceCategory_channelGrouping'] = data['device.deviceCategory'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_browser'] = data['device.browser'] + \"_\" + data['channelGrouping']\n    data['channelGrouping_OS'] = data['device.operatingSystem'] + \"_\" + data['channelGrouping']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            data[i + \"_\" + j] = data[i] + \"_\" + data[j]\n    \n    data['content.source'] = data['trafficSource.adContent'] + \"_\" + data['source.country']\n    data['medium.source'] = data['trafficSource.medium'] + \"_\" + data['source.country']\n    return data\n\ntrain = custom(train)\ntest = custom(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bf1e49c96a1820471ca00fc76f8a8aeb012ab55"},"cell_type":"code","source":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]\n\nfor f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f823b26f434382c223c2c339e00276f9c2495e5d"},"cell_type":"code","source":"train_features = [_f for _f in train.columns if _f not in excluded_features]\n# X_train, X_test, y_train, y_test = train_test_split(train[train_features], y_reg, test_size=0.20, random_state=42)\n#X_train, y_train = train[train_features], y_reg for Bayesian Optimization","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4745f9e9c6d4f7ddc496cb68adfc4b073270d3d"},"cell_type":"markdown","source":"## hyperparameters tuning"},{"metadata":{"trusted":true,"_uuid":"7ac7d50853ac5f9001e923b9eae7e0f1f8e05ce6"},"cell_type":"code","source":"'''\ndef lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 2,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 517,\n    \"verbosity\" : -1\n    }\n    lgtrain = lgb.Dataset(X_train, label=np.log1p(y_train.apply(lambda x : 0 if x < 0 else x)))\n    cv_result = lgb.cv(params,\n                       lgtrain,\n                       1500,\n                       categorical_feature=categorical_features,#category_features,\n                       early_stopping_rounds=100,\n                       stratified=False,\n                       nfold=5)\n    return -cv_result['rmse-mean'][-1]\n\ndef lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 2,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 517,\n    \"verbosity\" : -1\n    }\n    t_x,v_x,t_y,v_y = train_test_split(X_train,y_train,test_size=0.2)\n    lgtrain = lgb.Dataset(t_x, label=np.log1p(t_y.apply(lambda x : 0 if x < 0 else x)))\n    lgvalid = lgb.Dataset(v_x, label=np.log1p(v_y.apply(lambda x : 0 if x < 0 else x)))\n    model = lgb.train(params, lgtrain, 5000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    return pred_test_y, model\n    \ndef param_tuning(init_points,num_iter,**args):\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 100),\n                                                'max_depth': (5, 20),\n                                                'lambda_l2': (0.0, 0.5),\n                                                'lambda_l1': (0.0, 0.5),\n                                                'bagging_fraction': (0.1, 0.99),\n                                                'feature_fraction': (0.1, 0.99),\n                                                'min_child_samples': (20, 50),\n                                                })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n    return lgbBO\n\nresult = param_tuning(8, 22)\nresult.res['max']['max_params']\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac35052df91d54cfd95b57a527fdb94522e1871"},"cell_type":"markdown","source":"## tuned hyperparameters"},{"metadata":{"trusted":true,"_uuid":"48e652800315f0c35ad6c56431b399bea82cac60"},"cell_type":"code","source":"#extracted params\nparams={'learning_rate': 0.01,\n        'objective':'regression',\n        'metric':'rmse',\n        'num_leaves': 100,\n        'verbose': 1,\n        'bagging_fraction': 0.99,\n        'feature_fraction': 0.99,\n        \"random_state\":517,\n        'max_depth': 20,\n        \"bagging_seed\" : 517,\n        \"verbosity\" : -1,\n        \"bagging_frequency\" : 5,\n        'lambda_l2': 0,\n        'lambda_l1': 0.5,\n        'min_child_samples': 20\n       }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ad9824c6cfa37e04bc6b4650d6b9ea3b16ed46"},"cell_type":"markdown","source":"## training baseline model"},{"metadata":{"trusted":true,"_uuid":"71a12df24ff7149189152956650ba63b8ef40649"},"cell_type":"code","source":"folds = get_folds(df=train, n_splits=10)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\n\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    print(\"Fold:\",fold_)\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    reg = lgb.LGBMRegressor(**params,\n         n_estimators=1500\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) / len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f171ff9d194011af1ee094b90833179ee12478a4"},"cell_type":"markdown","source":"## forming visitor-level features"},{"metadata":{"trusted":true,"_uuid":"8cfe830198e9bfd97abe4d8dd21863cb6608588c"},"cell_type":"code","source":"train['predictions'] = np.expm1(oof_reg_preds)\ntest['predictions'] = sub_reg_preds\n\ntrn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n\ntrn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34bc57d8b6d002eca2397a1def13eaab31541960"},"cell_type":"code","source":"trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns\ntrn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()\nfull_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ad98d5169b52eac1496c6144897c607274e577e"},"cell_type":"code","source":"sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faa42d4009ec20081fc6cb1bb6311f0d8743feac"},"cell_type":"code","source":"sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c39c2d3b9a5dca8b6931abd12969b278ab2e9f4e"},"cell_type":"code","source":"train['target'] = y_reg\ntrn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64dd9c1846c9d9598fab21312b403b07a2378d07"},"cell_type":"code","source":"del train, test, trn_pred_list, sub_pred_list\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b5d84522c07f8e5e45d0c17ae8a10e1d315798c"},"cell_type":"markdown","source":"## filling nulls"},{"metadata":{"trusted":true,"_uuid":"bcf90d8c8d0525014b09d102e552a5d2a69b507a"},"cell_type":"code","source":"full_columns = full_data.columns\ncategorical_features = list(set(categorical_features).intersection(set(full_columns)))\nnumerical_features = list(set(full_columns).difference(set(categorical_features)))\nfull_data[categorical_features] = full_data[categorical_features].fillna(-1)\nsub_full_data[categorical_features] = sub_full_data[categorical_features].fillna(-1)\nfull_data[numerical_features] = full_data[numerical_features].fillna(0)\nsub_full_data[numerical_features] = sub_full_data[numerical_features].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6289f1dc8ca393aa15210f5b5b54f2126a0e7e64"},"cell_type":"markdown","source":"## encoding features"},{"metadata":{"trusted":true,"_uuid":"c842143a174c973b7577fbc078c807537935ba8c"},"cell_type":"code","source":"for num, category_name in enumerate(categorical_features):\n    unique, counts = np.unique(full_data[category_name], return_counts=True)\n    counts = (counts / counts.sum()).astype(np.float32)\n    for i in range(unique.shape[0]):\n        full_data.loc[full_data.loc[:, category_name] == unique[i], category_name] = counts[i]\n        sub_full_data.loc[sub_full_data.loc[:, category_name] == unique[i], category_name] = counts[i]\n    print('Column %i (of %i) processed!'%(num + 1, len(categorical_features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a8677837a3e8d02afa7ba2f2cdb53934b9b32f9"},"cell_type":"code","source":"X_train = full_data.as_matrix().astype(np.float32)\nX_test = sub_full_data[full_data.columns].as_matrix().astype(np.float32)\ny_train = trn_user_target.as_matrix().astype(np.float32).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e416fc3dae2f1ef7805c55c31103558de9617964"},"cell_type":"code","source":"submission = pd.DataFrame({'PredictedLogRevenue': np.zeros(len(sub_full_data.index))},\n                          index=sub_full_data.index)\ndel full_data, sub_full_data, trn_user_target\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bacda6a5a82a07df0225436456ae1fc9cff0a0d"},"cell_type":"markdown","source":"## training visitor-level model"},{"metadata":{"trusted":true,"_uuid":"d514093e53245f7f3e60b41ab6ecc770a4a9fafe"},"cell_type":"code","source":"n_folds = 20\nsub_preds = np.zeros(X_test.shape[0])\nreg = lgb.LGBMRegressor(**params,\n                        n_estimators=1500)\nkf = KFold(n_splits=n_folds, random_state=517, shuffle=True)\nfor train_idx, test_idx in kf.split(X_train):\n    reg.fit(X_train[train_idx], np.log1p(y_train[train_idx]),\n            eval_set=[(X_train[train_idx], np.log1p(y_train[train_idx])),\n                      (X_train[test_idx], np.log1p(y_train[test_idx]))],\n            eval_names=['TRAIN', 'VALID'],\n            early_stopping_rounds=50,\n            eval_metric='rmse',\n            verbose=101)\n    _preds = reg.predict(X_test, num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_preds += _preds / n_folds\n    print('Iteration completed!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3c49f4cf5384b1cea775ffca234350c30a8f818"},"cell_type":"markdown","source":"## saving predictions"},{"metadata":{"trusted":true,"_uuid":"8fa4014eb964d23ee2e3410d439ae2f30aa856ec"},"cell_type":"code","source":"submission['PredictedLogRevenue'] = sub_preds\nsubmission[['PredictedLogRevenue']].to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}