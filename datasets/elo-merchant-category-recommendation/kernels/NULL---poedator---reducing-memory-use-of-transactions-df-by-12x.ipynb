{"cells":[{"metadata":{"_uuid":"1b31cf102443c65210c764d48774158b7aeaf341"},"cell_type":"markdown","source":"# Description\nThis notebook shows simple ways to reduce memory footprint of the large *historical_transactions* dataframe. \n\nTools used: changing data types, simple encoding of categorical variables, conversion of IDs."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#imports\nimport numpy as np \nimport pandas as pd ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%time historical = pd.read_csv('../input/historical_transactions.csv') # (takes 1-2 minutes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e355c5ded8e54f565661383bf5715d3959f30cf"},"cell_type":"code","source":"#Let's see what is stored in the dataframe\nhistorical.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e96ff3d467ec08ab274128f9de2ae6a2249c56f"},"cell_type":"code","source":"#Let's see how much memory it uses:\nmem_use = historical.memory_usage(deep=True)\noriginal_mem_use = mem_use.sum()\nprint ('total memory used: {:,} bytes'.format(mem_use.sum()))\nmem_use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"653dd46be0aaa86282ee7dd7ed3bdd3166d92407"},"cell_type":"markdown","source":"Apparently this DataFrame uses quite a lot of memory (close to 14 Gb) because of sheer number of records (>29 million) and less efficient data formats used after csv imports. \n\nLet's try to fix the latter:"},{"metadata":{"trusted":true,"_uuid":"d54a836becf85bd13e5e49a017d040876c710858"},"cell_type":"code","source":"# This function was written after quick data analysis exploring contents of each column and choosing datatypes with smaller memory footprint\n# it may be applied both to historical dataframe and new transactions dataframe\n\ndef transactions_reduce (df_trans):\n    df_trans.authorized_flag = (df_trans.authorized_flag == 'Y') # was Y/N \n    df_trans.city_id = df_trans.city_id.astype('int16') \n    df_trans.category_1 = (df_trans.category_1 == 'Y') # was Y/N \n    # historical.installments.unique() => [0,1,5,3,4,2,-1,10,6,12,8,7,9,11,999]\n    df_trans.loc[df_trans.installments == 999,'installments'] = 99  # 999 likely used as code for \"many\", 99 allows using 'int8'\n    df_trans.installments = df_trans.installments.astype('int8')\n    \n    # historical.category_3.unique() => ['A', 'B', 'C', nan]\n    df_trans.category_3.fillna('?', inplace=True)  # this will produce -1 for NaN\n    df_trans.category_3 = df_trans.category_3.apply(ord)-64  # replacing A,B,C with 1,2,3; \n    df_trans.category_3 = df_trans.category_3.astype('int8')\n    \n    df_trans.merchant_category_id = df_trans.merchant_category_id.astype('int16')\n    df_trans.month_lag = df_trans.month_lag.astype('int8')\n    df_trans.purchase_amount = df_trans.purchase_amount.astype('float32')\n    df_trans.purchase_date = pd.to_datetime(df_trans.purchase_date, infer_datetime_format=True)\n    \n    # historical.category_2.unique() => [  1.,  nan,   3.,   5.,   2.,   4.]\n    df_trans.category_2.fillna(0, inplace=True)\n    df_trans.category_2 = df_trans.category_2.astype('int8')\n    \n    df_trans.state_id = df_trans.state_id.astype('int8') # from -1 to 24\n    df_trans.subsector_id = df_trans.subsector_id.astype('int8') # from -1 to 41","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81da672c90416356ad0cd6a30ec3912e17075401"},"cell_type":"code","source":"# applying recuction function  \n%time transactions_reduce(historical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b1481366d72492d53adb4c51fc3a2d924012765"},"cell_type":"code","source":"#Let's see how much memory it uses now:\nmem_use = historical.memory_usage(deep=True)\nprint ('total memory used: {:,} bytes'.format(mem_use.sum()))\nprint (\"Effective memory usage reduction to {0:0.2f}% of original size\".format(mem_use.sum() / original_mem_use * 100))\nmem_use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d7a2cb1178da822e11d616288072fde67dd6370"},"cell_type":"markdown","source":"Now notice that the memory used got down to about a third of the original size.\n\nThe big memory hogs are now Merchant_ID and Card_ID, using 72 bytes per record each.\n\nAll *card_id* look like \"C_ID_0ab67a22ab\" where the last 10 characters are a unique 16-bit number. We may convert it to 'int64' which uses 9 times less memory so \"**C_ID_0ab67a22ab**\" becomes **46011130539**. \n\nSimilar conversion may be done with *merchant_id*.\n\n**Important note:**  To ensure consistency you must:\n1.     apply similar conversion to *card_id* in  *train* and *test* datasets  \n2.    apply similar conversion to *merchant_id* in  *merchants*  dataset\n3.    apply reverse conversion prior to result submission (or just preserve values order in *test* dataframe and copy *card_id* column from *sample* dataframe)\n"},{"metadata":{"trusted":true,"_uuid":"e08b4e8203c97414cf16b984d452881e247ecec1"},"cell_type":"code","source":"%%time\ndef id_gen (old_id):\n    a= old_id[-10:]\n    return int(a,16)\n\nhistorical.loc[:,'card_id'] = historical.card_id.apply(id_gen)\n\nhistorical.merchant_id.fillna('-000000001', inplace=True)  # insert -1 in place of NA\nhistorical.loc[:,'merchant_id'] = historical.merchant_id.apply(id_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2ee790732e58536ccc4e08ba7dea213500225ea"},"cell_type":"code","source":"#Let's see how much memory it uses now:\nmem_use = historical.memory_usage(deep=True)\nprint (\"Effective memory usage reduction to {0:0.2f}% of original size\".format(mem_use.sum() / original_mem_use * 100))\nprint ('total memory used: {:,} bytes'.format(mem_use.sum()))\nmem_use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27a164ece7aeda8bf2e5a894b96e1db714501621"},"cell_type":"markdown","source":"the result may be saver to feather or your favorite format to reduce disk footprint and accelerate loading \n\n> import feather\n\n> feather.write_dataframe(historical, 'historical_transactions.feather') #uses about 1.1 Gb\n\n> pd.read_feather('historical_transactions.feather') # this is used for loading"},{"metadata":{"trusted":true,"_uuid":"3cfcf348d0fa06d424156011d89a64be57a086e6"},"cell_type":"code","source":"# cleanup of memory:\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41a2abcb636afccac1d1d76765377707b00acc96"},"cell_type":"code","source":"\"Effective memory usage reduction to {0:0.2f}% of original size or {1:0.1f}x !\".format(mem_use.sum() / original_mem_use * 100,original_mem_use / mem_use.sum() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738b2d811bd813496ea6ecd2ebe9f4019f0468d4"},"cell_type":"markdown","source":"In this excercise we achieved ** 12x reduction ** of the memory size used \n\nNow with slimmer dataset it may be easier and faster to work, especially on computers with 8G RAM and below.  Most of the code in Kaggle forums/kernels should be applicable without change or with smallest modifications. \n\nRemember to apply card_id and merchant_id transformations to all involved datasets and **use original card_id formats ** in the submission.\n\n**Good luck in the competition!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}