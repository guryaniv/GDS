{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "version": "3.6.3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "cells": [{"execution_count": null, "metadata": {"_uuid": "2d2597817515e1c84f48de80e43bd04d42a7858a", "_cell_guid": "fdbb5a43-6a91-4e5f-b7fd-b541aa99523f"}, "outputs": [], "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "metadata": {"_uuid": "dc2a434a0c61b7d9e5955076ab2e0252b844095e", "_cell_guid": "b0c43711-0bf2-4db4-b54a-ecd6983a079a"}, "outputs": [], "cell_type": "code", "source": ["dataTrain = pd.read_csv(\"../input/train.csv\")\n", "dataTrain.head(5)"]}, {"execution_count": null, "metadata": {"_uuid": "0779983526bc634992527fa924222b5454d559a3", "_cell_guid": "22c24c9d-2137-4e66-9612-930de027f714"}, "outputs": [], "cell_type": "code", "source": ["import seaborn as sns\n", "sns.countplot(dataTrain['author'])"]}, {"execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "cell_type": "code", "source": ["from sklearn.model_selection import train_test_split\n", "train, test = train_test_split(dataTrain, test_size = 0.3)"]}, {"execution_count": null, "metadata": {}, "outputs": [], "cell_type": "code", "source": ["from bs4 import BeautifulSoup\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem.porter import PorterStemmer\n", "english_stemmer=nltk.stem.SnowballStemmer('english')\n", "import re\n", "def text_to_wordlist( text, remove_stopwords=True):\n", "    # Function to convert a document to a sequence of words,\n", "    # optionally removing stop words.  Returns a list of words.\n", "    #\n", "    # 1. Remove HTML\n", "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n", "\n", "    #\n", "    # 2. Remove non-letters\n", "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n", "    #\n", "    # 3. Convert words to lower case and split them\n", "    words = review_text.lower().split()\n", "    #\n", "    # 4. Optionally remove stop words (True by default)\n", "    if remove_stopwords:\n", "        stops = set(stopwords.words(\"english\"))\n", "        words = [w for w in words if not w in stops]\n", "\n", "    b=[]\n", "    stemmer = english_stemmer #PorterStemmer()\n", "    for word in words:\n", "        b.append(stemmer.stem(word))\n", "\n", "    # 5. Return a list of words\n", "    return(b)"]}, {"execution_count": null, "metadata": {"_uuid": "16990b87d4ea96e8c0e50bd9284ca4897e326a7a", "_cell_guid": "7564d427-eeeb-40f3-89b5-d49a0c813441"}, "outputs": [], "cell_type": "code", "source": ["clean_train_reviews = []\n", "for review in train['text']:\n", "    clean_train_reviews.append( \" \".join(text_to_wordlist(review)))\n", "    \n", "clean_test_reviews = []\n", "for review in test['text']:\n", "    clean_test_reviews.append( \" \".join(text_to_wordlist(review)))"]}, {"execution_count": null, "metadata": {}, "outputs": [], "cell_type": "code", "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ),\n", "                              sublinear_tf = True )\n", "\n", "vectorizer = vectorizer.fit(clean_train_reviews)\n", "\n", "train_features = vectorizer.transform(clean_train_reviews)\n", "test_features = vectorizer.transform(clean_test_reviews)"]}, {"execution_count": null, "metadata": {}, "outputs": [], "cell_type": "code", "source": ["from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n", "fselect = SelectKBest(chi2 , k=10000)\n", "train_features = fselect.fit_transform(train_features, train[\"author\"])\n", "test_features = fselect.transform(test_features)"]}, {"execution_count": null, "metadata": {}, "outputs": [], "cell_type": "code", "source": ["from sklearn.linear_model import SGDClassifier, SGDRegressor\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.naive_bayes import MultinomialNB\n", "\n", "model1 = MultinomialNB(alpha=0.001)\n", "model1.fit( train_features, train[\"author\"] )\n", "\n", "model2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n", "model2.fit( train_features, train[\"author\"] )\n", "\n", "model3 = RandomForestClassifier()\n", "model3.fit( train_features, train[\"author\"] )\n", "\n", "model4 = GradientBoostingClassifier()\n", "model4.fit( train_features, train[\"author\"] )\n", "\n", "pred_1 = model1.predict( test_features.toarray() )\n", "pred_2 = model2.predict( test_features.toarray() )\n", "pred_3 = model3.predict( test_features.toarray() )\n", "pred_4 = model4.predict( test_features.toarray() )"]}, {"execution_count": null, "metadata": {}, "outputs": [], "cell_type": "code", "source": ["from sklearn.metrics import accuracy_score\n", "print('prediction 1 accuracy: ', accuracy_score(test['author'], pred_1))\n", "print('prediction 2 accuracy: ', accuracy_score(test['author'], pred_2))\n", "print('prediction 3 accuracy: ', accuracy_score(test['author'], pred_3))\n", "print('prediction 4 accuracy: ', accuracy_score(test['author'], pred_4))"]}], "nbformat": 4}