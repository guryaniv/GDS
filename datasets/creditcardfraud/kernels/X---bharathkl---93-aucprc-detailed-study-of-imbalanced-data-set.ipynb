{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python"}}, "cells": [{"metadata": {"_cell_guid": "b3be40e1-6940-4f28-904c-ee44ab7898e0", "_uuid": "223c25bc9ac9f70929e419c057c8a7a526817f9a"}, "source": ["## 1. Introduction\n", "\n", "In this project of Credit Card Fraud Detection, we have to develop a model that predicts whether a transaction can be categorized into fraud or normal. The column 'Class' in the data set will be the target variable, the value 0 means the transaction is normal and the value 1 means the transaction is fraud. As we know the data set is imbalanced, that is only where we have 492 frauds out of 284,807 transactions made in two days.\n", "\n", "Here, I going to present 3 ways of handling imbalanced dataset:\n", "    \n", "1] **Resampling** :\n", "        \n", "In resampling we are going to undersampe and oversample the data\n", "            \n", "- **Undersampling** : Undersample the majority class.\n", "          \n", "- **Oversampling** : Oversample the minority class.\n", "\n", "           \n", "We are going to resample the data i.e., oversample and undesample data using 2 different python libraries:\n", "           \n", "- Using **sklearn.utils.resample**\n", "                                          \n", " Here we will oversample and undersample the data randomly\n", "              \n", "- Using **imbalanced-learn**\n", "                       \n", " Here we will oversample and undersample by applying machine learning algorithms\n", "               \n", "    \n", " 2] **Cost-Sensitive Training** : Using Penalized Algorithms . We can use any classifier that has parameter 'class_weight' as cost-sensitive algorithm. We will be using Penalized-Random Forest.\n", "    \n", " 3] **Tree Algorithms **: Tree Algorithms generally perform well for imbalance data set compared to other algorithms.\n", "    \n", "We will be using AUC-precision-recall curve as a performance metric. First, we will explore all the models with the respective recall_score, precision_score, fbeta_score and will choose the model based upon the fbeta_score, then that model will be  optimized and used as final model in which we will plot the auc precision_recall curve.\n", "\n", "The **F-beta score** can be interpreted as a weighted average of the precision and recall, where an F-beta score reaches its best value at 1 and worst at 0. The beta parameter determines the weight of precision in the combined score. beta < 1 lends more weight to precision, while beta > 1 favors recall . I will be using beta value of 0.5 to give emphasis on precision  as we dont want to misclassify the normal customer as fraud or vice versa.\n", "\n", "$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n", "\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "e19736f8-a2f0-45d9-ad72-e89e946b0f6d", "_uuid": "b09acfda1105ac9cb3c382773db8e561918393a7"}, "source": ["##  2. Import the necssary libararies"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "3bf862e5-640d-467a-89f7-d6dd23bf1513", "_uuid": "217730702402ec66b702fc8060e8760a95e580bd"}, "execution_count": null, "outputs": [], "source": ["# Import libraries necessary for this project\n", "import numpy as np\n", "import pandas as pd\n", "from time import time\n", "from IPython.display import display # Allows the use of display() for DataFrames\n", "import matplotlib.pyplot as plt\n", "from sklearn.utils import resample\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.metrics import fbeta_score, recall_score, precision_score, average_precision_score, precision_recall_curve\n", "from sklearn.ensemble import RandomForestClassifier\n", "from imblearn.over_sampling import SMOTE\n", "from imblearn.under_sampling import NearMiss\n", "import seaborn as sns\n", "from collections import Counter\n", "from sklearn.grid_search import GridSearchCV \n", "from sklearn.metrics import make_scorer\n", "\n", "# Pretty display for notebooks\n", "%matplotlib inline\n", "\n", "\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "f0d88f6c-9e31-4f90-86ed-d8e329859ec9", "_uuid": "83a485d248ce2380dc16f0f03e9944839a71c806"}, "source": ["## 3. Load the Data set into Pandas Dataframe"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ec7deda0-c6ea-4ec0-b026-6c85f094a40e", "_uuid": "0165812f28793d27dc462997e5a2bfd492dda52c"}, "execution_count": null, "outputs": [], "source": ["# Load the Census dataset\n", "\n", "data = pd.read_csv(\"../input/creditcard.csv\")\n", "\n", "# Success - Display the first record\n", "print(\"Data has {} record with {} features\".format(data.shape[0], data.shape[1]))\n", "display(data.head(n=1))\n", "results = {}"], "cell_type": "code"}, {"metadata": {"_cell_guid": "33bd395c-3315-4125-a222-9fb01512cced", "_uuid": "54613b77d1996f41d64cdbfa8c364c305e374483"}, "source": ["## 4. Data Exploration\n", "    \n", "1] Data set contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, \n", "features have named V1, V2, ... V28. These features are the principal components obtained with PCA, the only features which \n", "have not been transformed with PCA are 'Time' and 'Amount'.\n", "\n", "2] Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset, we can drop this feature as time difference between the first and the subsequent transactions does not have any relation to customer being fraud. \n", "\n", "3] The feature 'Amount' is the transaction Amount. As said in the problem describtion, we can use this feature when we do the cost-effective training.   \n", "\n", "4] The feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Therefore we will analyse these features alone. \n", "\n", "### 4.1 Data Visualization\n", "    \n", " First, we will visualize how the data is distributed for the feature 'Class' and 'Amount'"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "46f06870-600c-4b1d-b481-5c49a827d02d", "_uuid": "01c6aec3f20c65ee53ac40c1dc420a9f91a44b06"}, "execution_count": null, "outputs": [], "source": ["# Create figure\n", "fig = plt.figure(figsize = (12,10));\n", "for i, feature in enumerate(['Class', 'Amount']):\n", "    ax = fig.add_subplot(2, 2, i+1)\n", "    ax.hist(data[feature], bins = 25, color = '#00A0A0')\n", "    ax.set_title(\"'%s'\"%(feature), fontsize = 14)\n", "    ax.set_xlabel(\"Value\")\n", "    ax.set_ylabel(\"Number of Records\")\n", "        \n", "   \n", "    fig.tight_layout()\n", "    fig.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "f9684cb9-d47a-4cd1-b814-734fcd6131eb", "_uuid": "374d6aa7de394dc99091b3f7dd8ae66ab8430020"}, "source": ["#### Observations \n", "\n", "We can see that Class is clearly has very high count in 0's and very few in 1's. \n", "\n", "Similary feature Amount is also positiviely skewed."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "aaa7a149-0323-4785-b20b-ecd5e58470ef", "collapsed": true, "_uuid": "fd16ee0dd5c4c5ede61774f9afa03cf3f9fb7cb6"}, "source": ["### 4.2 Data Preprocesing\n", "\n", "#### 4.2.1 Transforming Skewed Continuous Features\n", "\n", "For highly-skewed feature distributions such as it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "52deacc8-10f7-42ae-9871-c6956e40f5a7", "scrolled": true, "_uuid": "6fbc5b36a733cd3086ed327e09fb29ec02bc4dd8"}, "execution_count": null, "outputs": [], "source": ["# Visualizing the Amount.\n", "data['Amount'] = data['Amount'].replace(np.nan,0)\n", "data['Amount'] = data['Amount'].apply(lambda x: np.log(x + 1))\n", "plt.hist(data['Amount'], bins = 25, color = '#00A0A0')\n", "plt.title(\"'%s'\"%(feature), fontsize = 14)\n", "plt.xlabel(\"Value\")\n", "plt.ylabel(\"Number of Records\")"], "cell_type": "code"}, {"metadata": {"_cell_guid": "6daa749f-060f-47b7-8eb9-40822f80e721", "_uuid": "c60000ff3aad28a0c796a91348fcd794ea7eb1cf"}, "source": ["We can see that now the data distribution is improved. Now we need to normalize within a range similar to other features\n", "in order to feed it into SVM classifier."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "11a5cb70-559b-4587-8e04-7744ded3302c", "_uuid": "23fa76d64f5248378b9913554bed2d91401cc927"}, "source": ["#### 4.2.2 Normalize the Amount Feature\n", "\n", "We will now normalize the Amount feature."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "0dd0cb93-f8f5-4688-a35f-17ab8b47d0a6", "scrolled": true, "_uuid": "2cff7154f6b9e73786865eeede06e45f7c9c6d1c"}, "execution_count": null, "outputs": [], "source": ["from sklearn import preprocessing\n", "print(\"                           Data Before normalizing is given below:                       \")\n", "display(data.head(n=1))\n", "\n", "\n", "scaler = preprocessing.MinMaxScaler()\n", "\n", "data['Amount']  = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))\n", "#data['Time'] = preprocessing.MinMaxScaler().fit_transform(data['Time'].reshape(-1, 1))\n", "\n", "#features[numerical]  = np.log(features[numerical])\n", "\n", "print(\"                           Data After normalizing is given below:                        \")\n", "display(data.head(n=1))"], "cell_type": "code"}, {"metadata": {"_cell_guid": "10b8c232-2c63-43f3-be5d-0a17b8049166", "_uuid": "ac88dab58c39afe6dd93350db538c20acefba4f0"}, "execution_count": null, "outputs": [], "source": ["plt.hist(data['Amount'], bins = 25, color = '#00A0A0')\n", "plt.title(\"'%s'\"%(feature), fontsize = 14)\n", "plt.xlabel(\"Value\")\n", "plt.ylabel(\"Number of Records\")"], "cell_type": "code"}, {"metadata": {"_cell_guid": "3d165868-10cf-4415-9560-c3b9a05391f2", "_uuid": "ea3f7c7eae1a09cf82e9dadda39547650ca64680"}, "source": ["We can see that data is normalized within range between 0 to 1."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "5ff2b6d3-aa3c-42a3-bbb9-e486fa602463", "_uuid": "83bb645f99389ebc30a26f824793b1ba8f6fa7ff"}, "source": ["### 5. Function to split data\n", "\n", "Creating this function as we need to work with original data set by different classifiers. \n", "This will help to resample the data individually by each classfiiers"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b573f0f5-3f92-46a2-8026-65491aba9f82", "collapsed": true, "_uuid": "0a254d1152cde30e161f2aa86ad5b886161f3623"}, "execution_count": null, "outputs": [], "source": ["#Function to split the original data and rturn values basd on the flag\n", "def split_data(flag):\n", "       \n", "    y = data.Class\n", "    X = data.drop(['Time', 'Amount', 'Class'], axis=1)\n", "    X_train, X_test, y_train, y_test = train_test_split(X, \n", "                                                    y, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "        \n", "    if flag == 1:\n", "        return (X,y)\n", "    elif flag  == 2:\n", "        return (X_train, X_test, y_train, y_test)\n", "    else:\n", "        return (X, y, X_train, X_test, y_train, y_test)"], "cell_type": "code"}, {"metadata": {"_cell_guid": "c3f0f0fe-16fb-4555-ab1a-66bf46aba196", "_uuid": "15ce632c031e1a3be7fc0a7b338af9a7632e66ea"}, "source": ["### 6. Resampling\n", "\n", "   We will first use the library sklearn.utils.resample to resample the data\n", "\n", "#### **6.1 Over sampling using sklearn.utils.resample**\n", "\n", "All data points from majority and minority training sets are used. Additionally, instances are randomly picked, with replacement, from the minority training set till the desired balance is achieved. Adding the same minority samples might result in overfitting, thereby reducing the generalization ability of the classifier. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "fd231404-7c46-4d8a-8cae-9356bc021c71", "_uuid": "cc491930adc597054f97c09569180a613b60f998"}, "execution_count": null, "outputs": [], "source": ["#Copying the original data\n", "resample_data = data.drop(['Time', 'Amount'], axis=1)\n", "#samples_n = (resample_data['Class' == 1]).sum()\n", "\n", "# Initializing the dictionary to store the classifier metrics \n", "results['oversampled']={}\n", "\n", "print(\"No. of 0's and 1's in the feature Class before oversampling the data\")\n", "print(resample_data.Class.value_counts())\n", "\n", "\n", "# Separate majority and minority classes\n", "data_majority = resample_data[resample_data.Class == 0]\n", "data_minority = resample_data[resample_data.Class == 1]\n", " \n", "# Upsample minority class\n", "data_minority_oversampled  = resample(data_minority, \n", "                                 replace=True,     \n", "                                 n_samples=284315, \n", "                                 random_state=123) \n", " \n", "# Combine majority class with upsampled minority class\n", "data_oversampled = pd.concat([data_majority, data_minority_oversampled])\n", "\n", "print(\"No. of 0's and 1's in the feature Class after oversampling the data\")\n", " \n", "print(data_oversampled.Class.value_counts())\n", "\n", "y = data_oversampled.Class\n", "X = data_oversampled.drop('Class', axis=1)\n", "\n", "# Split the data into training and testing sets\n", "X_train_o, X_test_o, y_train_o, y_test_o = train_test_split(X, \n", "                                                    y, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "\n", "    \n", "print(\"Training set has {} samples.\".format(X_train_o.shape[0]))\n", "print(\"Testing set has {} samples.\".format(X_test_o.shape[0]))\n", "\n", "start = time()\n", "#Training the Classifier\n", "clf_over_sampled = LogisticRegression().fit(X_train_o, y_train_o)\n", "end = time()\n", "results['oversampled']['train_time'] = end - start\n", "\n", "# Predict on training set\n", "start = time()\n", "y_pred_score_o = clf_over_sampled.predict(X_test_o)\n", "end = time()\n", "results['oversampled']['pred_time'] = end - start\n", "\n", "results['oversampled']['fbeta'] = fbeta_score(y_test_o,y_pred_score_o,beta=2)\n", "results['oversampled']['recall']= recall_score(y_test_o,y_pred_score_o)\n", "results['oversampled']['precision'] = precision_score(y_test_o,y_pred_score_o)\n", "\n", "\n", "print (\"Train Time:\", results['oversampled']['train_time'])\n", "print (\"Prediction Time:\", results['oversampled']['pred_time'])\n", "print (\"fbeta score:\", results['oversampled']['fbeta'])\n", "print('recall_score:', results['oversampled']['recall'])\n", "print('precision_score:', results['oversampled']['precision'])\n", "    \n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "0d832edd-01d2-425a-993e-b644cf6adc52", "_uuid": "d0066027972efcd311063c5e15cfa8029212f205"}, "source": ["#### Note:\n", "\n", "One issue I found here is that this same code which works fine in 64-bit Windows-7 OS was not working in 32-bit Windows-7 OS "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "27c0f456-9767-4af3-9857-c0768fc09212", "_uuid": "338c9fefb66e24c8d06eb5b1a340330510f54cac"}, "source": ["####** 6.2 Under ampling using sklearn.utils.resample**\n", "\n", "All of the training data points from the minority class are used. Instances are randomly removed from the majority training set till the desired balance is achieved. One disadvantage of this approach is that some useful information might be lost from the majority class due to the undersampling. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6365d537-2bd6-4755-9933-a3dbe0704e76", "_uuid": "59433e6db1c6679161eb88128cd0ce08b74e82a1"}, "execution_count": null, "outputs": [], "source": ["# copying the orginal data  \n", "resample_data = data.drop(['Time', 'Amount'], axis=1)\n", "\n", "# Initializing the dictionary to store the classifier metrics \n", "results['undersampled']={}\n", "\n", "#Separate majority and minority classes\n", "print(\"No. of 0's and 1's in the feature Class before undersampling the data\")\n", "print(resample_data.Class.value_counts())\n", "\n", "data_majority = resample_data[resample_data.Class==0]\n", "data_minority = resample_data[resample_data.Class==1]\n", " \n", "# Downsample majority class\n", "data_majority_undersampled = resample(data_majority, \n", "                                 replace=False,   \n", "                                 n_samples=492,   \n", "                                 random_state=5) \n", " \n", "# Combine minority class with downsampled majority class\n", "data_undersampled = pd.concat([data_majority_undersampled, data_minority])\n", " \n", "# Display new class counts\n", "print(\"No. of 0's and 1's in the feature Class after undersampling the data\" )\n", "print(data_undersampled.Class.value_counts())\n", "\n", "y = data_undersampled.Class\n", "X = data_undersampled.drop('Class', axis=1)\n", "\n", "# Split the data into training and testing sets\n", "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X, \n", "                                                    y, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "\n", "    \n", "print(\"Training set has {} samples.\".format(X_train_u.shape[0]))\n", "print(\"Testing set has {} samples.\".format(X_test_u.shape[0]))\n", "\n", "start = time()\n", "# Training the classifier\n", "clf_under_sampled = LogisticRegression().fit(X_train_u, y_train_u)\n", "end = time()\n", "results['undersampled']['train_time'] = end - start\n", "\n", "# Predict the testing set\n", "start = time()\n", "y_pred_score_u = clf_under_sampled.predict(X_test_u)\n", "end = time()\n", "results['undersampled']['pred_time'] = end - start\n", "\n", "results['undersampled']['fbeta'] = fbeta_score(y_test_u,y_pred_score_u,beta=2)\n", "results['undersampled']['recall'] = recall_score(y_test_u,y_pred_score_u)\n", "results['undersampled']['precision'] = precision_score(y_test_u,y_pred_score_u)\n", "\n", "\n", "print (\"Train Time:\", results['undersampled']['train_time'])\n", "print (\"Prediction Time:\", results['undersampled']['pred_time'])\n", "print (\"fbeta score:\", results['undersampled']['fbeta'])\n", "print('recall_score:', results['undersampled']['recall'])\n", "print('precision_score:', results['undersampled']['precision'])\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "78f5c37f-b675-4958-95ff-dba72568037c", "_uuid": "7e8d543d3201e2ad2d06550131bec23291cb24ed"}, "source": ["#### **6.3 Disadvantage of Random Over sampling and Random Under sampling******\n", "\n", "The disadvantage of Random Over Sampling is that adding the same minority samples might result in overfitting, thereby reducing the generalization ability of the classifier.\n", "\n", "The disadvantage of Random Under Sampling approach is that some useful information might be lost from the majority class due to the undersampling."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b6721d01-bae8-4901-934b-a59e8edc90d8", "collapsed": true, "_uuid": "1375fdd6b4702921187a54b2c90d73eb581c3332"}, "source": ["#### **6.4 Over sampling using imblearn.over_sampling.SMOTE**\n", "\n", "\n", "\n", "#####    SMOTE: Synthetic Minority Over-sampling Technique \n", "    \n", "   Over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples rather than by over-sampling with replacement. The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen.       "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "1aab3fbc-60a0-49ef-bbb5-2dd05d0db284", "_uuid": "5eb4b74e6782a0650a8ab9646655f8e8c3755dfc"}, "execution_count": null, "outputs": [], "source": ["# Getting the original features and labels as X,y from split_data function\n", "X_sm,y_sm = split_data(1)\n", "print(\"No. of 0's and 1's in the feature Class before oversampling the data\")\n", "print(Counter(y_sm))\n", "\n", "start = time()\n", "# Oversampling the data using SMOTE\n", "X_resampled_sm, y_resampled_sm = SMOTE().fit_sample(X_sm,y_sm)\n", "end = time()\n", "print(\"No. of 0's and 1's in the feature Class After oversampling the data\")\n", "print(Counter(y_resampled_sm))\n", "\n", "# Initializng the dictionary to store performance metrics\n", "results['SMOTE'] = {}\n", "results['SMOTE']['resample_time'] = end - start\n", "\n", "# Splitting the resampled data \n", "X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_resampled_sm, \n", "                                                    y_resampled_sm, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "start = time()\n", "# Training the Classifier\n", "clf_smote = LogisticRegression().fit(X_train_sm, y_train_sm)\n", "end = time()\n", "results['SMOTE']['train_time'] = end - start\n", "\n", "\n", "# Predict on training set\n", "start = time()\n", "y_pred_score_sm = clf_smote.predict(X_test_sm)\n", "end = time()\n", "results['SMOTE']['pred_time'] = end - start\n", "\n", "\n", "results['SMOTE']['fbeta'] = fbeta_score(y_test_sm,y_pred_score_sm,beta=2)\n", "results['SMOTE']['recall'] = recall_score(y_test_sm,y_pred_score_sm)\n", "results['SMOTE']['precision'] = precision_score(y_test_sm,y_pred_score_sm)\n", "\n", "print (\"Train Time:\", results['SMOTE']['train_time'])\n", "print (\"Prediction Time:\", results['SMOTE']['pred_time'])\n", "print (\"fbeta score:\", results['SMOTE']['fbeta'])\n", "print('recall_score:', results['SMOTE']['recall'])\n", "print('precision_score:', results['SMOTE']['precision'])\n", "\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "ab40cb88-e4c1-4ddf-8260-9fc04013b5e4", "_uuid": "f2d870b5c21a4eb52edc66a301d77775f9eac0a9"}, "source": ["#### **6.6 Under sampling using imblearn.under_sampling.NearMiss******\n", "\n", "\n", "#####    NearMiss \n", "    \n", "The NearMiss family of methods perform undersampling of points in the majority class based on their distance to other points in\n", "the same class. There are 3 variants of this techinque available, they are :\n", "    \n", "- NearMiss-1\n", "    \n", "- NearMiss-2\n", "    \n", "- NearMiss-3\n", "    \n", "1] NearMiss-1 : In NearMiss-1, those points from majority class are retained whose mean distance to the k nearest points in \n", "             minority  is lowest, where k is a tunable hyperparameter. I will be using this method here.\n", "        \n", "2] NearMiss-2 : In contrast to NearMiss-1, NearMiss-2 keeps those points from majority whose mean distance to the k farthest \n", "                points in minority is lowest.\n", "        \n", "3] NearMiss-3 : The final NearMiss variant, NearMiss-3 selects k nearest neighbors in majority class for every point in minority\n", "                class. In this case, the undersampling ratio is directly controlled by k and is not separately tuned.\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "398a3c65-5b4c-4ad9-ba94-aaf04495a872", "_uuid": "a011e423dc8e9bc29c79b3d86aa92ec2bc9501ac"}, "execution_count": null, "outputs": [], "source": ["# Getting the original features and labels as X,y from split_data function\n", "X_nm,y_nm = split_data(1)\n", "print(\"No. of 0's and 1's in the feature Class before undersampling the data\")\n", "print(Counter(y_nm))\n", "\n", "#Initializing the Nearmiss Classifier\n", "nm1 = NearMiss(random_state=0, version=1)\n", "start = time()\n", "# Undersampling the data\n", "X_resampled_nm1, y_resampled_nm1 = nm1.fit_sample(X_nm, y_nm)\n", "end = time()\n", "\n", "print(\"No. of 0's and 1's in the feature Class after undersampling the data\")\n", "print(Counter(y_resampled_nm1))\n", "\n", "#Initializing the dictionary to store the classifier metrics\n", "results['NearMiss'] = {}\n", "results['NearMiss']['resample_time'] = end - start\n", "\n", "#Splitting the undersampled data\n", "X_train_nm, X_test_nm, y_train_nm, y_test_nm = train_test_split(X_resampled_nm1, \n", "                                                    y_resampled_nm1, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "#Training the classifier \n", "start = time()\n", "clf_nm = LogisticRegression().fit(X_train_nm, y_train_nm)\n", "end = time()\n", "results['NearMiss']['train_time'] = end - start\n", "\n", "\n", "# Predict on training set\n", "start = time()\n", "y_pred_score_nm = clf_nm.predict(X_test_nm)\n", "end = time()\n", "results['NearMiss']['pred_time'] = end - start\n", "\n", "\n", "results['NearMiss']['fbeta'] = fbeta_score(y_test_nm,y_pred_score_nm,beta=2)\n", "results['NearMiss']['recall'] = recall_score(y_test_nm,y_pred_score_nm)\n", "results['NearMiss']['precision'] = precision_score(y_test_nm,y_pred_score_nm)\n", "\n", "print (\"Train Time:\", results['NearMiss']['train_time'])\n", "print (\"Prediction Time:\", results['NearMiss']['pred_time'])\n", "print (\"fbeta score:\", results['NearMiss']['fbeta'])\n", "print('recall_score:', results['NearMiss']['recall'])\n", "print('precision_score:', results['NearMiss']['precision'])\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "8b26746d-518f-4795-b0cd-cdf676470e38", "_uuid": "55df83f7879d67c9de548a7f756fb7ca978d2cc5"}, "source": ["### 7. Tree Algorithms\n", "\n", "We will use Decision Tree Algorithm without resampling the data."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "f12eb73c-589c-40cd-98a3-4d9af04e83c1", "scrolled": true, "_uuid": "7934815c2f06562972d2fa4ebbdc5e5e0114e4c9"}, "execution_count": null, "outputs": [], "source": ["# split orginal data into train & test \n", "X_train_dt, X_test_dt, y_train_dt, y_test_dt = split_data(2)\n", "\n", "#Initializing Dictionary to store output of decision tree classifier metrics\n", "results['Decision_Tree'] = {}\n", "\n", "#Initialize classfier\n", "clf_DT = DecisionTreeClassifier(random_state=5)\n", "\n", "start = time()\n", "# Train the Classifier\n", "clf_DT.fit(X_train_dt, y_train_dt)\n", "end = time()\n", "results['Decision_Tree']['train_time'] = end - start\n", "\n", "start = time()\n", "# Test the classifier\n", "y_pred_score_DT = clf_DT.predict(X_test_dt)\n", "end = time()\n", "results['Decision_Tree']['pred_time'] = end - start\n", "    \n", "results['Decision_Tree']['fbeta'] = fbeta_score(y_test_dt,y_pred_score_DT,beta=2)\n", "results['Decision_Tree']['recall'] = recall_score(y_test_dt,y_pred_score_DT)\n", "results['Decision_Tree']['precision'] = precision_score(y_test_dt,y_pred_score_DT)\n", "\n", "print (\"Train Time:\", results['Decision_Tree']['train_time'])\n", "print (\"Prediction Time:\", results['Decision_Tree']['pred_time'])\n", "print (\"fbeta score:\", results['Decision_Tree']['fbeta'])\n", "print('recall_score:', results['Decision_Tree']['recall'])\n", "print('precision_score:', results['Decision_Tree']['precision'])\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "409b6bd3-2503-419c-ad9e-071d27a4e39a", "_uuid": "0652af59e2c71c4e12e55034078e48cb0474c6af"}, "source": ["### 8. Cost sensitive Tree Algorithm\n", "\n", "We know that imbalanced data set can be handled by the Ensemble algorithms, Penalized Algorithms and Tree Algorithms separately. Now, we will combine all these techniques in 1 one algorithm and check the performance. All these 3 techniques can be combined using Random Forest Classifier.\n", "\n", "We know Random Forest is a ensemble algorithm with decision tree as the base learner. Random Forest has a parameter called 'class_weight' parameter, by setting this parameter to \u2019balanced\u2019, weights inversely proportional to the class sizes \n", "are used to multiply the loss function."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "28032e2b-5b16-4823-acef-4f4db54ccacf", "_uuid": "d654a74aa039559886a6ab49601789a8632b59d9"}, "execution_count": null, "outputs": [], "source": ["# split orginal data into train & test \n", "#X_train_rt, X_test_rt, y_train_rt, y_test_rt = split_data(2)\n", "\n", "\n", "# split orginal data into train & test \n", "y = data.Class\n", "X = data.drop(['Time','Class'], axis=1)\n", "\n", "X_train_rt, X_test_rt, y_train_rt, y_test_rt =  train_test_split(X, \n", "                                                    y, \n", "                                                    test_size = 0.3, \n", "                                                    random_state = 0)\n", "\n", "#Initializing Dictionary to store output of classifier metrics\n", "results['Random_forest'] = {}\n", "\n", "clf_RF = RandomForestClassifier(random_state=5, class_weight ='balanced')\n", "start = time()\n", "#Training the Classifier\n", "clf_RF.fit(X_train_rt, y_train_rt)\n", "end = time()\n", "\n", "results['Random_forest']['train_time'] = end - start\n", "start = time()\n", "# PRedicting the test set\n", "y_pred_score_RF = clf_RF.predict(X_test_rt)\n", "end = time()\n", "results['Random_forest']['pred_time'] = end - start\n", "\n", "results['Random_forest']['fbeta'] = fbeta_score(y_test_rt,y_pred_score_RF,beta=2)\n", "results['Random_forest']['recall'] = recall_score(y_test_rt,y_pred_score_RF)\n", "results['Random_forest']['precision'] = precision_score(y_test_rt,y_pred_score_RF)\n", "\n", "\n", "print (\"Train Time:\", results['Random_forest']['train_time'])\n", "print (\"Prediction Time:\", results['Random_forest']['pred_time'])\n", "print (\"fbeta score:\", results['Random_forest']['fbeta'])\n", "print('recall_score:', results['Random_forest']['recall'])\n", "print('precision_score:', results['Random_forest']['precision'])"], "cell_type": "code"}, {"metadata": {"_cell_guid": "f5a74186-7ef8-450e-b50b-906e47e1c248", "_uuid": "d21c0f44ddf73a50b0034c258e6bd38f000ecd1a"}, "source": ["### 9. Ensembled Cost sensitive Tree Algorithm\n", "\n", "One of my friend asked me to check on Gaussian Naive Bayes for handling imbalanced dataset as he was successful with it. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "dd6fae0f-269f-45ed-a0b7-959541a9600a", "_uuid": "fab01ebee530712bf63ddc74f489bbbc0bcf8c6d"}, "execution_count": null, "outputs": [], "source": ["# split orginal data into train & test \n", "from sklearn.naive_bayes import GaussianNB\n", "\n", "# split orginal data into train & test \n", "X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = split_data(2)\n", "\n", "#Initializing Dictionary to store output of classifier metrics\n", "results['GNB'] = {}\n", "\n", "start = time()\n", "\n", "# Training the classifier\n", "clf_gnb = GaussianNB().fit(X_train_gnb, y_train_gnb)\n", "end = time()\n", "results['GNB']['train_time'] = end - start\n", "\n", "\n", "# Predict on training set\n", "start = time()\n", "y_pred_score_gnb = clf_gnb.predict(X_test_gnb)\n", "end = time()\n", "\n", "results['GNB']['pred_time'] = end - start\n", "\n", "\n", "results['GNB']['fbeta'] = fbeta_score(y_test_gnb,y_pred_score_gnb,beta=0.5)\n", "results['GNB']['recall'] = recall_score(y_test_gnb,y_pred_score_gnb)\n", "results['GNB']['precision'] = precision_score(y_test_gnb,y_pred_score_gnb)\n", "\n", "print (\"Train Time:\", results['GNB']['train_time'])\n", "print (\"Prediction Time:\", results['GNB']['pred_time'])\n", "print (\"fbeta score:\", results['GNB']['fbeta'])\n", "print('recall_score:', results['GNB']['recall'])\n", "print('precision_score:', results['GNB']['precision'])"], "cell_type": "code"}, {"metadata": {"_cell_guid": "2de120ce-0131-4734-8e3d-903f7e07774f", "_uuid": "91afa84d7a8e6c172673ac966294a8787719aca6"}, "source": ["### 10. Performance Analysis of the Classifiers\n", "\n", "Now its time to analyze the performance metrics of all the classifiers implemented above.\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "00b31d37-8d26-44cb-9bc1-952415435e32", "_uuid": "2a3d79f1eca2911467284a7c48afa86d1f402dee"}, "source": ["#### 10.1 Summarize the performance \n", "\n", "The summary of the performance metrics is given in the table below\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "2fcfc762-afae-4556-9df6-912a0c1a8717", "_uuid": "17a4e5bba4ba798e557a1bf4f627d162ef9958f4"}, "execution_count": null, "outputs": [], "source": ["results_df = pd.DataFrame(results)  \n", "display(results_df)\n", "#print \"Columns are \", results_df.columns"], "cell_type": "code"}, {"metadata": {"_cell_guid": "4938cbfe-14fc-4255-be3c-1d77e33ade74", "_uuid": "fa52a99d504ddde574ea926d9839c6282db0e1b6"}, "source": ["#### 10.2 Visualizing the performance of classifiers\n", "\n", "We will visualize performance time of the classifiers seperately with that of other metrics such as fbeta_score,recall_score and precision_score as we measure time in seconds and score values between 0 to 1. \n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "0a0bf883-d453-4c47-b962-e9ee6e453a76", "_uuid": "2a6db2019953934da23e855a45450160777070cd"}, "execution_count": null, "outputs": [], "source": ["train_time = [results['undersampled']['train_time'],results['oversampled']['train_time'], \n", "              results['SMOTE']['train_time'],results['NearMiss']['train_time'], \n", "              results['Decision_Tree']['train_time'], results['Random_forest']['train_time'], \n", "              results['GNB']['train_time']]\n", "pred_time = [results['undersampled']['pred_time'],results['oversampled']['pred_time'], \n", "             results['SMOTE']['pred_time'],results['NearMiss']['pred_time'], \n", "             results['Decision_Tree']['pred_time'],results['Random_forest']['pred_time'], \n", "             results['GNB']['pred_time']]\n", "\n", "N = 7\n", "width = 0.25       # the width of the bars\n", "\n", "fig, ax = plt.subplots(figsize=(15,12))\n", "\n", "ind = np.arange(N)\n", "    \n", "plt.bar(ind, train_time, width, label='train_time', color='#33FF7D')\n", "plt.bar(ind + width, pred_time, width, label='pred_time', color='#FFAC33')\n", "\n", "\n", "plt.ylim(0, 25)\n", "plt.ylabel('Time in (Seconds)', fontsize = 20)\n", "plt.xlabel('Classifiers', fontsize = 20)\n", "plt.title('Time Comparison of the Models', fontsize = 25)\n", "\n", "plt.xticks(ind + width, ('Undersampled', 'Oversampled', 'SMOTE', 'NearMiss', 'Decision Tree', \n", "                         'Random Forest', 'Gaussian Naive Bayes'))\n", "plt.legend(loc='best', fontsize = 16 )\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "96a3911d-cb5d-43ee-85ad-e6ff84e4706a", "_uuid": "1626db725b2f313e6c101902480440e3cb5dfa04"}, "execution_count": null, "outputs": [], "source": ["fbeta = [results['undersampled']['fbeta'],results['oversampled']['fbeta'], results['SMOTE']['fbeta'],\n", "         results['NearMiss']['fbeta'], results['Decision_Tree']['fbeta'], results['Random_forest']['fbeta'],\n", "         results['GNB']['fbeta']]\n", "recall = [results['undersampled']['recall'],results['oversampled']['recall'], \n", "          results['SMOTE']['recall'],results['NearMiss']['recall'], \n", "          results['Decision_Tree']['recall'], results['Random_forest']['recall'], \n", "          results['GNB']['recall']]\n", "    \n", "    \n", "precision= [results['undersampled']['precision'],results['oversampled']['precision'], \n", "            results['SMOTE']['precision'],results['NearMiss']['precision'],\n", "            results['Decision_Tree']['precision'], results['Random_forest']['precision'], \n", "            results['GNB']['precision']]\n", "    \n", "N = 7\n", "width = 0.25       # the width of the bars\n", "\n", "fig, ax = plt.subplots(figsize=(15,12))\n", "ind = np.arange(N)\n", "\n", "plt.bar(ind , fbeta, width, label='fbeta', color='#33C4FF')\n", "plt.bar(ind + width, recall, width, label='recall', color='#FF5733')\n", "plt.bar(ind + width*2, precision, width, label='precision', color='#FFE333')\n", "plt.ylim(0, 1 )\n", "plt.ylabel('Scores', fontsize = 20)\n", "plt.xlabel('Classifiers', fontsize = 20)\n", "plt.title('Metrics Comparison of the Models', fontsize = 25)\n", "\n", "plt.xticks(ind + width, ('Undersampled', 'Oversampled', 'SMOTE', 'NearMiss', 'Decision Tree', \n", "                         'Random Forest','Gaussian Naive Bayes'))\n", "plt.legend(loc='best', fontsize = 16 )\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "390ef6f5-a4b5-46e1-9b0a-97f96c0fe704", "_uuid": "64efc0414c3010e86a2ba96208ee549e2a34117f"}, "source": ["### 11.  Choosing the Best Classifier\n", "\n", "We will now choose the best classifier using the figures above. We will choose the top 4 classifiers by ranking them from 1 to 4 \n", "with rank 1 being best .\n", "\n", "1] Time : We will first rank the classifiers based on their training time and prediction time. \n", "\n", "   **Rank 1 :** Undersampled\n", "    \n", "   **Rank 2 :** NearMiss\n", "    \n", "  ** Rank 3 :** Gaussian Naive Bayes\n", "    \n", "  ** Rank 4 :** SMOTE\n", "    \n", "    \n", "2] Metrics : We will now rank the classifiers based on their fbeta_score, prediction_score and recall_score with recall score given the highest priority\n", "\n", "   **Rank 1 :**Oversampled\n", "    \n", "   **Rank 2 :** SMOTE\n", "    \n", "   **Rank 3 :** NearMiss\n", "    \n", "   **Rank 4 :** Undesampled\n", "    \n", "  \n", "Now, of the 2 comparisons made above, we have to given more importance to the metrics than time, becuase at the end classifier with best accuracy is needed  and a little trade-off in the time is acceptible.\n", "\n", "So, with that we choose the classfiers with SMOTE and NearMiss as the best models they have best metrics with not much delay in training and testing time.\n", "\n", "Now, we have to choose the best classifier among these 2. For, that we use the resampling time taken by them, i.e., the time taken by SMOTE to oversample the data and the time taken by the NearMiss to undersample the data. \n", "\n", "The comparison chart is given below\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b4d85cc4-4a96-4dd8-966c-29ab4e10be09", "_uuid": "3322518a3133967ca6f671286d1200be6d639b58"}, "execution_count": null, "outputs": [], "source": ["resample_time = [results['SMOTE']['resample_time'],results['NearMiss']['resample_time']]\n", "N = 2\n", "fig, ax = plt.subplots(figsize=(5,5))\n", "ind = np.arange(N)\n", " \n", "plt.bar(ind, resample_time, width, label='resample_time', color='#33FF7D')\n", "plt.ylim(0, 25)\n", "plt.ylabel('Time in (Seconds)', fontsize = 20)\n", "plt.xlabel('Classifiers', fontsize = 20)\n", "plt.xticks(ind + width, ('SMOTE', 'NearMiss'))\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "d7833c98-afa1-4270-bdbd-7da65532133b", "_uuid": "6e929aa9a529916e03fabf1383357c04e671cf71"}, "source": ["#### From the above chart we can conclude that classifier that uses SMOTE has a better performance compared to the rest of the classifiers. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "857ec8a5-6cc5-4f89-af13-58dad0e318be", "collapsed": true, "_uuid": "02ae65d4df882d6245516afdaf432c2a0916d9ad"}, "source": ["### 12. Optimizing the Best Classifier\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ec4edf30-7ff5-45b6-967a-010fc871c7cd", "_uuid": "53f2715dac98664857b1d02e7db52e9ed5cbb911"}, "execution_count": null, "outputs": [], "source": ["#X_train_opt, X_test_opt, y_train_opt, y_test_dt = split_data(2)\n", "\n", "#parameters list you wish to tune\n", "parameters = { 'C' : [0.001, 0.1, 0.25, 0.55, 0.72, 0.85, 2, 5, 7, 9, 20, 30, 40, 60, 70, 90, 100] }\n", "\n", "# Make an fbeta_score scoring object\n", "scorer = make_scorer(fbeta_score,beta=2)\n", "\n", "# Perform grid search on the classifier using 'scorer' as the scoring method\n", "grid_obj = GridSearchCV(clf_smote,parameters,scoring=scorer)\n", "\n", "# TODO: Fit the grid search object to the training data and find the optimal parameters\n", "grid_fit = grid_obj.fit(X_train_sm,y_train_sm)\n", "\n", "# Get the estimator\n", "best_clf = grid_fit.best_estimator_\n", "print(\"Optimzed Classfieris \", best_clf)\n", "# Make predictions \n", "opt_classifier_score = best_clf.predict(X_test_sm)\n", "\n", "print(\"F-score after optimizing:\", fbeta_score(y_test_sm, opt_classifier_score, beta =2))\n", "print(\"Recall-score after optimizing:\", recall_score(y_test_sm, opt_classifier_score))\n", "print(\"Precision-score after optimizing:\", precision_score(y_test_sm, opt_classifier_score))\n", "\n", "y_pred_prb_final = best_clf.predict_proba(X_test_sm)[:,1]"], "cell_type": "code"}, {"metadata": {"_cell_guid": "3e0d6fce-e161-41fe-87de-d3a1573eb33a", "_uuid": "3e3f9dd914ec12a52a8e0c604f509c18fc277b36"}, "source": ["From we can that model has very high recall score and precision score, high scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). An ideal system with high precision and high recall will return many results, with all results labeled correctly."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "78294a5b-c8bd-47d0-9a39-c5dece3a7522", "_uuid": "94968d6be2e519ed1b806d1cc32eeb03a8264f76"}, "source": ["### 13. AUC-PR Curve\n", "\n", "The precision-recall curve shows the tradeoff between precision and recall for different threshold. Average precision summarizes such a plot as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ce52041b-df46-4ec2-a857-049e953d3469", "_uuid": "0fa4a48bed9589ec00c2a19be0438883ad1f939f"}, "execution_count": null, "outputs": [], "source": ["average_precision = average_precision_score(y_test_sm, opt_classifier_score)\n", "\n", "print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n", "\n", "precision, recall, thresholds = precision_recall_curve(y_test_sm, opt_classifier_score)\n", "\n", "plt.step(recall, precision, color='b', alpha=0.2,\n", "         where='post')\n", "plt.fill_between(recall, precision, step='post', alpha=0.2,\n", "                 color='b')\n", "\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.ylim([0.0, 1.05])\n", "plt.xlim([0.0, 1.0])\n", "plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format(average_precision))\n", "thresholds = np.append(thresholds, 1)\n", "\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "c0d73806-c61a-49f3-a3f4-0d49b2872149", "_uuid": "31828d17f84874a95096cccbc0e17bcf1b7cb165"}, "source": ["We got 93% of area for AUC-PR curve, A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. \n", "\n", "High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). An ideal system with high precision and high recall will return many results, with all results labeled correctly. So, the model will predict high number fraudulent transaction with very high precision."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "3d50061e-629e-4dbb-a7d1-2a33e26e100b", "_uuid": "2841cbfd480970b000c8be8a4d3748470c7d2ab5"}, "source": ["### 14. Finding the best threshold for the classifier"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "e4c57dbb-8be7-4417-9064-bbe48ea26b27", "scrolled": true, "_uuid": "f822d4b467bb87fd3ca81f106c82cb61102116de"}, "execution_count": null, "outputs": [], "source": ["plt.plot(thresholds, precision, color=sns.color_palette()[0])  \n", "plt.plot(thresholds, recall, color=sns.color_palette()[1])  \n", "\n", "leg = plt.legend(('precision', 'recall'), frameon=True)  \n", "leg.get_frame().set_edgecolor('k')  \n", "plt.xlabel('threshold')  \n", "plt.ylabel('%')"], "cell_type": "code"}, {"metadata": {"_cell_guid": "2c5fc987-194f-4d1d-8f26-f756c766fd75", "_uuid": "0e12593c08cb4f87cf4b2ae30c6b419df8a84644"}, "source": ["From above figure, we can conclude that the best threshold for the classifier is around 0.85."], "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}