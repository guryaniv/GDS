{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport gensim\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom tabulate import tabulate\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(\"../input\"))\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceb8a1a50d95557c6c7ac9eae7c6c11596fa88a9","collapsed":true},"cell_type":"code","source":"data = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\ntotal=data.append(test)\nprint('Training data shape: {}'.format(data.shape))\nprint('Test data shape: {}'.format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed3d1167af06ee7ecf8e23c85cfa60c6bf4cd0e3","collapsed":true},"cell_type":"code","source":"total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19fc9dd3a7e61db2f02c2b889ad1dcde6c339eed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abb8a8a71f9f7fe43ef0bd6dc54106f82a5c186e","collapsed":true},"cell_type":"code","source":"X, y = [], []\n\ndata['text']= data['ingredients'].map(\",\".join)\nX=data['text'].str.split(', ', expand=True)\nX=data['ingredients'].map(\",\".join)\nlb = LabelEncoder()\ny = lb.fit_transform(data.cuisine)\n#X, y = np.array(X), np.array(y)\nprint (\"total examples %s\" % len(y),X)\n\nX=X[:40000]\ny=y[:40000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c249ed1066757091bf11df746937af7345df6a8","collapsed":true},"cell_type":"code","source":"# Feed a word2vec with the ingredients\nw2v = gensim.models.Word2Vec(list(total.ingredients), size=350, window=10, min_count=2, iter=20)  #iter = first 10 ingredients !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71efc69cdcaea70b8fc02b31e9b8d083f93bcf9e","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# start with the classics - naive bayes of the multinomial and bernoulli varieties\n# with either pure counts or tfidf features\nmult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\",  OneVsRestClassifier(MultinomialNB()))])\nbern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\nmult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\nbern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\nknn_tf = Pipeline([(\"tfidf_vectorizer\",TfidfVectorizer(analyzer=lambda x: x,sublinear_tf=True)), (\"knn\", OneVsRestClassifier(KNeighborsClassifier()))])\n# SVM - which is supposed to be more or less state of the art \n# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\nsvc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"rbf\"))])\nsvc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", OneVsRestClassifier(SVC(kernel=\"rbf\")))])\nsvc_tfidf2 = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", OneVsRestClassifier(SVC(kernel=\"linear\")))])\nsvc2 = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", OneVsRestClassifier(SVC(kernel=\"linear\")))])\nsvc_ovr = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"svc_ovr\",  OneVsRestClassifier(SVC()))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cc2a73d916dbc4d53e09f0d3447750da28b7fb1"},"cell_type":"code","source":"class MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        if len(word2vec)>0:\n            self.dim=len(word2vec[next(iter(glove_small))])\n        else:\n            self.dim=0\n            \n    def fit(self, X, y):\n        return self \n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n\n    \n# and a tf-idf version of the same\nclass TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        if len(word2vec)>0:\n            self.dim=len(word2vec[next(iter(glove_small))])\n        else:\n            self.dim=0\n        \n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        # if a word was never seen - it must be at least as infrequent\n        # as any of the known words - so the default idf is the max of \n        # known idf's\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf, \n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n    \n        return self\n    \n    def transform(self, X):\n        return np.array([\n                np.mean([self.word2vec[w] * self.word2weight[w]\n                         for w in words if w in self.word2vec] or\n                        [np.zeros(self.dim)], axis=0)\n                for words in X\n            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a95f5bbc5641f91c2389a8efb3d8ebea9c93322","collapsed":true},"cell_type":"code","source":"all_models = [\n    (\"mult_nb\", mult_nb),\n   # (\"mult_nb_tfidf\", mult_nb_tfidf),\n    (\"bern_nb\", bern_nb),\n    (\"knn_tfidf\", knn_tf),\n    (\"svc\", svc),\n    (\"svc2\", svc),    \n    (\"svc_tfidf\", svc_tfidf),\n    (\"svc_tfidf2\", svc_tfidf2),\n    (\"svc_ovr\", svc_ovr),\n]\n\n\nunsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\nscores = sorted(unsorted_scores, key=lambda x: -x[1])\n\n\nprint (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c0c277bd45fde4f35261fe0262bcd3fba17747","collapsed":true},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"34ca18e8045f569c8b0af41073ead931bba1befa"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"00a87854795f76a141e09658e5288123db189776","collapsed":true},"cell_type":"markdown","source":"# Predictions \nprint (\"Predict on test data ... \")\ny_test = model.predict(u1[:len(test)])\ny_pred = lb.inverse_transform(y_test)\n\n# Submission\nprint (\"Generate Submission File ... \")\ntest_id = test.id\nsub = pd.DataFrame({'id': test_id, 'cuisine': y_pred}, columns=['id', 'cuisine'])\nsub.to_csv('svm_output.csv', index=False)\nsub"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cdaeb53c50db435ad4230f30e8d799357eb83e88"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}