{"cells":[{"metadata":{"_uuid":"cd785ebded650f7f223bda80db64d5a31c4e31ab"},"cell_type":"markdown","source":"This notebook contains my own implementation of a pipeline for testing different feature engineering strategies. Linear regression models are trained for each added feature or groups of features in order to evaluate their effectiveness."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom scipy.stats import skew\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport gc\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import train and test data\ntrain = pd.read_csv('../input/train_V2.csv')\ntest = pd.read_csv('../input/test_V2.csv')\n\n# Take a look at the data shape\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"4864789ee9c9c87d0c2a076294f335b45e37d572"},"cell_type":"markdown","source":"The following memory saving function from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage works well but causes overflow errors in this case, so until I get around to modifying it it will not be used."},{"metadata":{"trusted":true,"_uuid":"73a652380545a37c076a43fe0b0a1cf2d1822f83"},"cell_type":"code","source":"# Memory saving function from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cd14a6db35b6302b817bf1dba7ddb1d416f778b"},"cell_type":"markdown","source":"It is always important to examine data for missing values."},{"metadata":{"trusted":true,"_uuid":"4e7cbba4b4d3e12c93f14beeded0b8d52e90d58e"},"cell_type":"code","source":"# Look at missing values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1947e7fd02e8e3c1b1ddf888dbbabae512abd391"},"cell_type":"markdown","source":"We could find the index of the observation with the missing value, but it's simpler to just use the dropna() function, even though it takes a little bit longer."},{"metadata":{"trusted":true,"_uuid":"90d88eaa9d47f0a902b623f63b9b3de1baa0cff6"},"cell_type":"code","source":"# Drop the single missing value\ntrain = train.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f86a867979a01243eeedfbda335f4565b9cd0fd1"},"cell_type":"code","source":"# Columns to drop\ndropCols = ['Id', 'groupId', 'matchId', 'matchType', 'winPlacePerc']\npredictors = [pred for pred in list(train) if pred not in dropCols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6de0e996aee386054026630436af94097a760a8"},"cell_type":"markdown","source":"It is generally recommended to center and scale predictors in order to produce a stable linear regression model."},{"metadata":{"trusted":true,"_uuid":"ab0f84ef8d6109808a5536a354c53f6b7ad219a3"},"cell_type":"code","source":"# Center and scale\ntrans = preprocessing.RobustScaler().fit(train[predictors])\ntrain[predictors] = trans.transform(train[predictors])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665f41672fd6d3931708cc94a97f942f5a329f38"},"cell_type":"markdown","source":"The following function is used to build and grade a linear regression model for the specified set of predictors. Different sets of engineered features will be passed into the function in order to assess their usefulness."},{"metadata":{"trusted":true,"_uuid":"b67f916adfc9f939c5d5cd86f727feb4d6bcf344"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\naccuracies = []\nfeature_names = []\nbase_improve = []\n\n# Fit a linear regression model to the data and output the results\ndef test_feature_accuracy(data, feature_name):\n    \n    # Predictors\n    use_preds = [pred for pred in list(data) if pred not in dropCols]\n    data_X = data[use_preds]\n    # Target\n    data_Y = data['winPlacePerc']\n    \n    print('Training model with feature: ' + feature_name)\n    \n    # 80/20 train test split\n    train_X, test_X, train_Y, test_Y = train_test_split(data_X, data_Y, test_size=0.20, random_state=69)\n    \n    # Normalize to avoid overflow\n    model = linear_model.LinearRegression(normalize=True)\n    \n    # Train model\n    model.fit(train_X, train_Y)\n    \n    # Evaluate model\n    score_val = model.score(test_X, test_Y)\n    \n    print('Model with feature: ' + feature_name + ' scored accuracy: ' + str(score_val * 100) + '%')\n    \n    # Save score\n    accuracies.append(score_val)\n    feature_names.append(feature_name)\n    \n    # Save improvement\n    base_improve.append(accuracies[len(accuracies) - 1] - accuracies[0])\n    \n# Baseline linear regression\ntest_feature_accuracy(train, 'Baseline')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f8413d7a24d2bb756df56477dfafd4112b73dd9"},"cell_type":"code","source":"# Engineer some features\ntrain['PlayersInMatch'] = train.groupby('matchId')['matchId'].transform('count')\ntest['PlayersInMatch'] = test.groupby('matchId')['matchId'].transform('count')\n\ntemp_preds = predictors.copy()\ntemp_preds.append('PlayersInMatch')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'PlayersInMatch')\n\ntrain['TotalDistance'] = train['rideDistance'] + train['swimDistance'] + train['walkDistance']\ntest['TotalDistance'] = test['rideDistance'] + test['swimDistance'] + test['walkDistance']\n\ntemp_preds = predictors.copy()\ntemp_preds.append('TotalDistance')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'TotalDistance')\n\n\ntrain['TeamSize'] = train.groupby(['matchId', 'groupId'])['groupId'].transform('count')\ntest['TeamSize'] = test.groupby(['matchId', 'groupId'])['groupId'].transform('count')\n\ntemp_preds = predictors.copy()\ntemp_preds.append('TeamSize')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'TeamSize')\n\n\ntrain['TeamTotalKills'] = train.groupby(['matchId', 'groupId'])['kills'].transform('sum')\ntest['TeamTotalKills'] = test.groupby(['matchId', 'groupId'])['kills'].transform('sum')\n\ntemp_preds = predictors.copy()\ntemp_preds.append('TeamTotalKills')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'TeamTotalKills')\n\n\ntrain['BoostsOverDistance'] = (train['boosts'] / (train['TotalDistance'] + 1))\ntest['BoostsOverDistance'] = (test['boosts'] / (test['TotalDistance'] + 1))\n\ntemp_preds = predictors.copy()\ntemp_preds.append('BoostsOverDistance')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'BoostsOverDistance')\n\ntrain['HeadshotKillsPerKill'] = train['headshotKills'] / train['kills']\ntrain['HeadshotKillsPerKill'].fillna(0, inplace=True)\ntest['HeadshotKillsPerKill'] = test['headshotKills'] / test['kills']\ntest['HeadshotKillsPerKill'].fillna(0, inplace=True)\n\ntemp_preds = predictors.copy()\ntemp_preds.append('HeadshotKillsPerKill')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'HeadshotKillsPerKil')\n\ntrain['MatchKills'] = train.groupby('matchId')['kills'].transform('sum')\n\ntemp_preds = predictors.copy()\ntemp_preds.append('MatchKills')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'MatchKills')\n\ntrain['PercMatchKills'] = train['kills'] / train['MatchKills']\ntrain['PercMatchKills'].fillna(0, inplace=True)\n\ntemp_preds = predictors.copy()\ntemp_preds.append('PercMatchKills')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'PercMatchKills')\n\ntrain['TeamPercMatchKills'] = train['TeamTotalKills'] / train['MatchKills']\ntrain['TeamPercMatchKills'].fillna(0, inplace=True)\n\ntemp_preds = predictors.copy()\ntemp_preds.append('TeamPercMatchKills')\ntemp_preds.append('winPlacePerc')\ntemp_drop = [pred for pred in list(train) if pred not in temp_preds]\ntest_feature_accuracy(train.drop(temp_drop, axis=1), 'TeamPercMatchKills')\n\ntest['MatchKills'] = test.groupby('matchId')['kills'].transform('sum')\ntest['PercMatchKills'] = test['kills'] / test['MatchKills']\ntest['PercMatchKills'].fillna(0, inplace=True)\ntest['TeamPercMatchKills'] = test['TeamTotalKills'] / test['MatchKills']\ntest['TeamPercMatchKills'].fillna(0, inplace=True)\n\n# Test all new features\ntest_feature_accuracy(train, 'All Simple Features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1919fb0614198caf572f7e5c7d8060650d77663"},"cell_type":"markdown","source":"Most features provide only a marginal increase in accuracy, but all together they provide about a 2% accuracy increase, which is more than negligible. "},{"metadata":{"trusted":true,"_uuid":"1280f827c2ee16beae24539daf3f3735ec6441a7"},"cell_type":"code","source":"# Add the new predictors to list\npredictors = [pred for pred in list(train) if pred not in dropCols]\n\n# Engineer some more\ntrain_mean = train.groupby(['matchId', 'groupId'])[predictors].mean()\ntrain_mean_rank = train_mean.groupby('matchId')[predictors].rank(pct=True)\n\n# Mean\ntrain_with_mean = train.merge(train_mean, suffixes=['', 'Mean'], how='left', on=['matchId', 'groupId'])\nprint('Merged mean')\n\ndel train_mean\ngc.collect()\n\n# Test the model with feature Mean added\ntest_feature_accuracy(train_with_mean, 'Mean')\n\ndel train_with_mean\ngc.collect()\n\n# Mean Rank\ntrain_with_meanrank = train.merge(train_mean_rank, suffixes=['', 'MeanRank'], how='left', on=['matchId', 'groupId'])\nprint('Merged mean rank')\n\ndel train_mean_rank\ngc.collect()\n\n# Test the model with feature MeanRank added\ntest_feature_accuracy(train_with_meanrank, 'MeanRank')\n\ndel train_with_meanrank\ngc.collect()\n\ntrain_max = train.groupby(['matchId', 'groupId'])[predictors].max()\ntrain_max_rank = train_max.groupby('matchId')[predictors].rank(pct=True)\n\ntrain_with_max = train.merge(train_max, suffixes=['', 'Max'], how='left', on=['matchId', 'groupId'])\nprint('merged max')\n\ndel train_max\ngc.collect()\n\n# Test the model with feature Max added\ntest_feature_accuracy(train_with_max, 'Max')\n\ndel train_with_max\ngc.collect()\n\n# MaxRank\ntrain_with_maxrank = train.merge(train_max_rank, suffixes=['', 'MaxRank'], how='left', on=['matchId', 'groupId'])\nprint('merged max rank')\n\ndel train_max_rank\n\ngc.collect()\n\n# Test the model with feature MaxRank added\ntest_feature_accuracy(train_with_maxrank, 'MaxRank')\n\ndel train_with_maxrank\ngc.collect()\n\ntrain_min = train.groupby(['matchId', 'groupId'])[predictors].min()\ntrain_min_rank = train_min.groupby('matchId')[predictors].rank(pct=True)\n\ntrain_with_min = train.merge(train_min, suffixes=['', 'Min'], how='left', on=['matchId', 'groupId'])\nprint('merged min')\n\ndel train_min\ngc.collect()\n\n# Test the model with feature Min added\ntest_feature_accuracy(train_with_min, 'Min')\n\ndel train_with_min\ngc.collect()\n\ntrain_with_minrank = train.merge(train_min_rank, suffixes=['', 'MinRank'], how='left', on=['matchId', 'groupId'])\nprint('merged minrank')\n\ndel train_min_rank\ngc.collect()\n\n# Test the model with feature MinRank added\ntest_feature_accuracy(train_with_minrank, 'MinRank')\n\ndel train_with_minrank\ngc.collect()\n\n# Match mean\ntrain_match_mean = train.groupby('matchId')[predictors].mean()\n\ntrain_with_match_mean = train.merge(train_match_mean, suffixes=['', 'MatchMean'], how='left', on=['matchId'])\nprint('merged match_mean')\n\ndel train_match_mean\ngc.collect()\n\n# Test model with feature MatchMean added\ntest_feature_accuracy(train_with_match_mean, 'MatchMean')\n\ndel train_with_match_mean\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed3300e4499ac4c7b1bb2f9fc7f7c59a22156ad9"},"cell_type":"markdown","source":"Rank features provide a much more noticeable increase in performance, with MeanRank and MaxRank improving the accuracy by roughly 10% each, from about 83% to about 93%. It's clear that rank features are very important in this case."},{"metadata":{"trusted":true,"_uuid":"5502712240c4561542a79153f79c113935bbf59d"},"cell_type":"code","source":"# Compare to baseline\ndef compare_to_baseline(pos):\n    improve = base_improve[pos]\n    feature = feature_names[pos]\n    accuracy = accuracies[pos]\n    \n    print(feature + ' score: ' + str(accuracy))\n    print(feature + ' improvement over baseline: ' + str(improve))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"891d529d58f502b1e0f1f0af5c68f6e9be899e2d"},"cell_type":"markdown","source":"The following loop compares each model to the baseline."},{"metadata":{"trusted":true,"_uuid":"01562f5509b45c7f02d92f58a34a976f08daa754"},"cell_type":"code","source":"for i in range(len(feature_names)):\n    compare_to_baseline(i)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}