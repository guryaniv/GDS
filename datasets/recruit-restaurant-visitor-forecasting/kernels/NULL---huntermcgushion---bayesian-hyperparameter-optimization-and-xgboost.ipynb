{"cells":[{"metadata":{"collapsed":true,"_uuid":"a50b548b36700439f821ae7a6e25a1054f41d5eb","_cell_guid":"dc3983e6-9c13-43b2-91c5-4a726fad78d0","trusted":true},"cell_type":"code","source":"###############################################\n# Import Machine Learning Assets\n###############################################\nfrom bayes_opt import BayesianOptimization\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\n\n###############################################\n# Import Miscellaneous Assets\n###############################################\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom datetime import datetime\nfrom functools import partial\nfrom pprint import pprint as pp\nfrom tqdm import tqdm, tqdm_notebook\n\npd.set_option('display.expand_frame_repr', False)\n\n###############################################\n# Declare Global Variables\n###############################################\nCROSS_VALIDATION_PARAMS = dict(n_splits=5, shuffle=True, random_state=32)\nXGBOOST_REGRESSOR_PARAMS = dict(\n    learning_rate=0.2, n_estimators=200, subsample=0.8, colsample_bytree=0.8, \n    max_depth=10, n_jobs=-1\n)\n\nBAYESIAN_OPTIMIZATION_MAXIMIZE_PARAMS = dict(\n    init_points=1,  # init_points=20,\n    n_iter=2,  # n_iter=60,\n    acq='poi', xi=0.0\n)\nBAYESIAN_OPTIMIZATION_BOUNDARIES = dict(\n    max_depth=(5, 12.99),\n    gamma=(0.01, 5),\n    min_child_weight=(0, 6),\n    scale_pos_weight=(1.2, 5),\n    reg_alpha=(4.0, 10.0),\n    reg_lambda=(1.0, 10.0),\n    max_delta_step=(0, 5),\n    subsample=(0.5, 1.0),\n    colsample_bytree=(0.3, 1.0),\n    learning_rate=(0.0, 1.0)\n)\nBAYESIAN_OPTIMIZATION_INITIAL_SEARCH_POINTS = dict(\n    max_depth=[5, 10],\n    gamma=[0.1511, 3.8463],\n    min_child_weight=[2.4073, 4.9954],\n    scale_pos_weight=[2.2281, 4.0345],\n    reg_alpha=[8.0702, 9.0573],\n    reg_lambda=[2.0126, 3.5934],\n    max_delta_step=[1, 2],\n    subsample=[0.8, 0.8234],\n    colsample_bytree=[0.8, 0.7903],\n    learning_rate=[0.2, 0.1]\n)\n\nreserve_features = [\n    'rs1_x', 'rs1_y', 'rs2_x', 'rs2_y', 'rv1_x', 'rv1_y', 'rv2_x', 'rv2_y',\n    'total_reserve_dt_diff_mean', 'total_reserve_mean', 'total_reserve_sum'\n]\n\nBASE_ESTIMATOR = partial(XGBRegressor)\n# train_df, test_df = None, None\n# oof_predictions, test_predictions = None, None\n# train_input = None\n# best_round = None\n# target = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4e96b67183b77a39122674c1eff8b6c7c8aba96","_cell_guid":"835c2e3b-29fa-4c5a-9c0c-9930c7a4ef83"},"cell_type":"markdown","source":"# Bayesian Optimization Parameter Overview\n\n- BAYESIAN_OPTIMIZATION_MAXIMIZE_PARAMS:\n - init_points: Number of random samples to conduct before maximizing a function\n - n_iter: Number of iterations to fit the Gaussian Process object to maximize our function\n- BAYESIAN_OPTIMIZATION_BOUNDARIES:\n - This needs to be a dictionary containing all of the hyperparameters you want to search, along with their minimum and maximum boundaries\n- BAYESIAN_OPTIMIZATION_INITIAL_SEARCH_POINTS:\n - This is a dictionary containing some points to search first to give the Gaussian Process a sort of baseline for the hyperparameters' values. I've gotten the best results by setting my initial search points to cover a very wide range of values, rather than focusing in specific areas, but your experience may differ. \n - All of the list values in this dictionary must be of the same length.\n\n# Data Preparation/Feature Engineering \n\nThe next cell will contain all the feature engineering code that you've all seen hundreds of times by now.\n<br>\nThank you very much to everyone who contributed to this beast. Your work is truly appreciated.\n<br>\nAfter doing our feature engineering, we'll get back to what this kernel is really about: Bayesian Hyperparameter Optimization. "},{"metadata":{"collapsed":true,"_uuid":"683d6450f2b8b1c4115cc694ce3aeed69131cf7d","_cell_guid":"bde73c18-643f-4aef-89a0-bf5cff643449","trusted":true},"cell_type":"code","source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n}\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(\n        lambda _: (_['visit_datetime'] - _['reserve_datetime']).days, axis=1\n    )\n    \n    ###############################################\n    # aharless's Same-Week Reservation Exclusion\n    ###############################################\n    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']]\n    tmp1 = data[df].groupby(\n        ['air_store_id','visit_datetime'], as_index=False\n    )[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={\n        'visit_datetime':'visit_date', \n        'reserve_datetime_diff': 'rs1', \n        'reserve_visitors':'rv1'\n    })\n    tmp2 = data[df].groupby(\n        ['air_store_id','visit_datetime'], as_index=False\n    )[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={\n        'visit_datetime':'visit_date', \n        'reserve_datetime_diff': 'rs2', \n        'reserve_visitors':'rv2'\n    })\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda _: str(_).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda _: '_'.join(_.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat(\n    [pd.DataFrame({\n        'air_store_id': unique_stores, \n        'dow': [_] * len(unique_stores)\n    }) for _ in range(7)], \n    axis=0, ignore_index=True\n).reset_index(drop=True)\n\n###############################################\n# Jerome Vallet's Optimization\n###############################################\ntmp = data['tra'].groupby(['air_store_id', 'dow']).agg(\n    {'visitors': [np.min, np.mean, np.median, np.max, np.size]}\n).reset_index()\ntmp.columns = [\n    'air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors', 'max_visitors', \n    'count_observations'\n]\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id', 'dow'])\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n\n###############################################\n# Georgii Vyshnia's Features\n###############################################\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda _: str(str(_).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda _: str(str(_).replace('-',' ')))\nlbl = LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name' + str(i)] = lbl.fit_transform(stores['air_genre_name'].map(\n        lambda _: str(str(_).split(' ')[i]) if len(str(_).split(' ')) > i else ''\n    ))\n    stores['air_area_name' + str(i)] = lbl.fit_transform(stores['air_area_name'].map(\n        lambda _: str(str(_).split(' ')[i]) if len(str(_).split(' ')) > i else ''\n    ))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id', 'visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id', 'visit_date'])\n\ntrain['id'] = train.apply(\n    lambda _: '_'.join([str(_['air_store_id']), str(_['visit_date'])]), axis=1\n)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n###############################################\n# JMBULL's Features \n###############################################\ntrain['date_int'] = train['visit_date'].apply(lambda _: _.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda _: _.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n###############################################\n# Georgii Vyshnia's Features\n###############################################\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ncol = [_ for _ in train if _ not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\ntrain_df = train[col]\ntest_df = test[col]\ntarget = pd.DataFrame()\ntarget['visitors'] = np.log1p(train['visitors'].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60446e0d732fe00e1f28d49b36a345c49ebc1694","_cell_guid":"340494ab-e8be-4e26-af70-83d34c0ff45b"},"cell_type":"markdown","source":"# The Function to be Maximized"},{"metadata":{"collapsed":true,"_uuid":"11cfc1a0eef1481422b2d2d6bfd4077ae63202b1","_cell_guid":"5a6033d7-b259-449b-9772-a3a77b9ecb9d","trusted":true},"cell_type":"code","source":"def search_node(**kwargs):\n    # global train_df, test_df, train_input, oof, test_predictions, best_round, target\n    global train_df, target\n\n    ###############################################\n    # Unify Parameters\n    ###############################################\n    received_params = dict(dict(\n        n_estimators=200,\n    ), **{_k: _v if _k not in ('max_depth') else int(_v) for _k, _v in kwargs.items()})\n    \n    current_params = dict(XGBOOST_REGRESSOR_PARAMS, **received_params)\n\n    ###############################################\n    # Initialize Folds and Result Placeholders\n    ###############################################\n    folds = KFold(**CROSS_VALIDATION_PARAMS)\n    evaluation = np.zeros((current_params['n_estimators'], CROSS_VALIDATION_PARAMS['n_splits']))\n    oof_predictions = np.empty(len(train_df))\n    np.random.seed(32)\n\n    progress_bar = tqdm_notebook(\n        enumerate(folds.split(target, target)), \n        total=CROSS_VALIDATION_PARAMS['n_splits'], \n        leave=False\n    )\n    \n    ###############################################\n    # Begin Cross-Validation\n    ###############################################\n    for fold, (train_index, validation_index) in progress_bar:\n        train_input, validation_input = train_df.iloc[train_index], train_df.iloc[validation_index]\n        train_target, validation_target = target.iloc[train_index], target.iloc[validation_index]\n\n        ###############################################\n        # Initialize and Fit Model With Current Parameters\n        ###############################################\n        model = BASE_ESTIMATOR(**current_params)\n        eval_set = [(train_input, train_target), (validation_input, validation_target)]\n        model.fit(train_input, train_target, eval_set=eval_set, verbose=False)\n\n        ###############################################\n        # Find Best Round for Validation Set\n        ###############################################\n        evaluation[:, fold] = model.evals_result_[\"validation_1\"]['rmse']\n        best_round = np.argsort(evaluation[:, fold])[0]\n\n        progress_bar.set_description('Fold #{}:   {:.5f}'.format(\n            fold, evaluation[:, fold][best_round]\n        ), refresh=True)\n\n    ###############################################\n    # Compute Mean and Standard Deviation of RMSLE\n    ###############################################\n    mean_eval, std_eval = np.mean(evaluation, axis=1), np.std(evaluation, axis=1)\n    best_round = np.argsort(mean_eval)[0]\n    search_value = mean_eval[best_round]\n\n    ###############################################\n    # Update Best Score and Return Negative Value\n    # In order to minimize error, instead of maximizing accuracy\n    ###############################################\n    print(' Stopped After {} Epochs... Validation RMSLE: {:.6f} +- {:.6f}'.format(\n        best_round, search_value, std_eval[best_round]\n    ))\n\n    return -search_value","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4a8fec52cb5c845f1076799c8f83a849d7cb814","_cell_guid":"12a1d598-7fc3-45a4-9e72-241e2abd19e8"},"cell_type":"markdown","source":"The ```search_node``` function is going to be called repeatedly when we start the Bayesian optimization process.\n<br>\nWe use ```**kwargs``` as the function's input because the ```BayesianOptimization``` class methods we're going to see below are just going to pass in a dictionary full of hyperparameters that need to be tested.\n<br>\nWe end up with ```current_params``` after updating our baseline parameters with the ones we receive and after casting the necessary hyperparameters to integers.\n\n**THIS IS IMPORTANT! **\n<br>\nThe ```BayesianOptimization``` class works by picking some value for each hyperparameter somewhere within its boundaries.\n<br>\nFor example, we can get floating point values for ```max_depth```. This is a problem because ```XGBRegressor``` expects integers for certain parameters, so it's gonna be angry and confused if you say you want your decision tree's maximum depth to be 5.3.\n<br>\nFor more information, please [read the documentation.](http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor \"Please, read me\") It'll tell you the same thing in less colorful language.\n\nWhat follows is a fairly standard cross-validation loop, in which we record the RMSE for each fold.\n<br>\nAt the end, this function is expected to return the value of that set of tested hyperparameters. \n<br>\nBy default, the Bayesian Optimization functions will attempt to find hyperparameters that maximize this value, but we're dealing with error, which we want to minimize, so we just return the negation of the error.\n\n# Starting Bayesian Optimization"},{"metadata":{"_uuid":"ea98e627b2309cc2161014cd8d06a190115797ff","_cell_guid":"8e3c95f6-f986-4dea-87d7-b0870875a5bb","trusted":true,"collapsed":true},"cell_type":"code","source":"bayes_opt = BayesianOptimization(search_node, BAYESIAN_OPTIMIZATION_BOUNDARIES)\nbayes_opt.explore(BAYESIAN_OPTIMIZATION_INITIAL_SEARCH_POINTS)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    bayes_opt.maximize(**BAYESIAN_OPTIMIZATION_MAXIMIZE_PARAMS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45b00523c9fbbbc3dd42a7000761a30a28a40ca9","_cell_guid":"a603f5c6-79a0-4ad2-853d-607ba09e189f"},"cell_type":"markdown","source":"# Is That It?\n\nPretty much, yeah. \n<br>\nIn the first line, we initialize the ```BayesianOptimization``` class by giving it the function to maximize and its search boundaries.\nThen we tell it to ```explore``` the initial search points we selected above. This is just a way to get familiar with the hyperparameters and their \"value\", so the function has some knowledge before we actually tell it to find the hyperparameters that yield the maximum value.\n\nIn the last line, we call ```maximize```, which actually does two things:\n1. It randomly samples the target function ```init_points``` times. This is like passing random points to ```explore```, but easier\n2. It fits the Gaussian Process object to maximize our function for *exactly*  ```n_iter``` many iterations, so choose wisely"},{"metadata":{"_uuid":"6e697aecd95a6dd35ca69595d226d3fd803c5649","_cell_guid":"a7b40b5c-e4c2-4851-871b-04c60de18dac","trusted":true,"collapsed":true},"cell_type":"code","source":"print('Maximum Value: {}'.format(bayes_opt.res['max']['max_val']))\nprint('Best Parameters:')\npp(bayes_opt.res['max']['max_params'])\nbayes_opt.points_to_csv('bayes_opt_search_points.csv')\n\nbest_params = dict(XGBOOST_REGRESSOR_PARAMS, **dict(\n    n_estimators=200,\n    learning_rate=bayes_opt.res['max']['max_params']['learning_rate'],\n    max_depth=int(bayes_opt.res['max']['max_params']['max_depth']),\n    gamma=bayes_opt.res['max']['max_params']['gamma'],\n    min_child_weight=bayes_opt.res['max']['max_params']['min_child_weight'],\n    max_delta_step=int(bayes_opt.res['max']['max_params']['max_delta_step']),\n    subsample=bayes_opt.res['max']['max_params']['subsample'],\n    colsample_bytree=bayes_opt.res['max']['max_params']['colsample_bytree'],\n    scale_pos_weight=bayes_opt.res['max']['max_params']['scale_pos_weight'],\n    reg_alpha=bayes_opt.res['max']['max_params']['reg_alpha'],\n    reg_lambda=bayes_opt.res['max']['max_params']['reg_lambda']\n))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4d7ba6e8104ceeaa10b4181adf3a47f9083e45","_cell_guid":"ef69f07e-131b-4cb8-8e0c-f523fd2ab610"},"cell_type":"markdown","source":"Above, we're printing the maximum value (minimum error) achieved by Bayesian optimization. Of course, allowing it to run for more iterations will improve this.\n<br>\nThen, we print the hyperparameters that were used to reach the maximum value and save a record of all the points searched.\n<br>\nFinally, we construct a new parameters dictionary using the hyperparameters that yielded the greatest value, so we can...\n\n# Train a Model With the Best Hyperparameters"},{"metadata":{"_uuid":"a22df7cb657222743cb62b7117e6e293c7acbbb4","_cell_guid":"7dbccf0b-53e8-42d6-9432-82a25f16ff59","trusted":true,"collapsed":true},"cell_type":"code","source":"def RMSLE(target, prediction):\n    return metrics.mean_squared_error(target, prediction) ** 0.5\n\n###############################################\n# Initialize Folds and Result Placeholders\n###############################################\nfolds = KFold(**CROSS_VALIDATION_PARAMS)\nimp_df = np.zeros((len(train_df.columns), CROSS_VALIDATION_PARAMS['n_splits']))\nbest_evaluation = np.zeros((best_params['n_estimators'], CROSS_VALIDATION_PARAMS['n_splits']))\noof_predictions, test_predictions = np.empty(train_df.shape[0]), np.zeros(test_df.shape[0])\nnp.random.seed(32)\n\nfor fold, (train_index, validation_index) in enumerate(folds.split(target, target)):\n    train_input, validation_input = train_df.iloc[train_index], train_df.iloc[validation_index]\n    train_target, validation_target = target.iloc[train_index], target.iloc[validation_index]\n    \n    ###############################################\n    # Initialize and Fit Model With Best Parameters\n    ###############################################\n    model = BASE_ESTIMATOR(**best_params)\n    eval_set=[(train_input, train_target), (validation_input, validation_target)]\n    model.fit(train_input, train_target, eval_set=eval_set, verbose=False)\n\n    ###############################################\n    # Record Feature Importances and Best OOF Round\n    ###############################################\n    imp_df[:, fold] = model.feature_importances_\n    best_evaluation[:, fold] = model.evals_result_[\"validation_1\"]['rmse']\n    # best_round = np.argsort(xgb_evaluation[:, fold])[::-1][0]  # FLAG: ORIGINAL\n    best_round = np.argsort(best_evaluation[:, fold])[0]  # FLAG: TEST\n\n    ###############################################\n    # Make OOF and Test Predictions With Best Round\n    ###############################################\n    oof_predictions[validation_index] = model.predict(validation_input, ntree_limit=best_round)\n    test_predictions += model.predict(test_df, ntree_limit=best_round)\n\n    ###############################################\n    # Report Results for Fold\n    ###############################################\n    oof_rmsle = RMSLE(validation_target, oof_predictions[validation_index])\n    print('Fold {}: {:.6f}     Best Score: {:.6f} @ {:4}'.format(\n        fold, oof_rmsle, best_evaluation[best_round, fold], best_round\n    ))\n\nprint('#' * 80 + '\\n')\nprint('OOF RMSLE   {}'.format(RMSLE(target, oof_predictions)))\n\n###############################################\n# Compute Mean and Standard Deviation RMSLE\n###############################################\ntest_predictions /= CROSS_VALIDATION_PARAMS['n_splits']\nmean_eval, std_eval = np.mean(best_evaluation, axis=1), np.std(best_evaluation, axis=1)\nbest_round = np.argsort(mean_eval)[0]\nprint('Best Mean Score: {:.6f} +- {:.6f} @{:4}'.format(\n    mean_eval[best_round], std_eval[best_round], best_round\n))\n\nimportances = sorted(\n    [(train_df.columns[i], imp) for i, imp in enumerate(imp_df.mean(axis=1))], \n    key=lambda x: x[1]\n)\n\nfinal_df = pd.DataFrame(\n    data=list(zip(test['id'], np.expm1(test_predictions))), columns=['id', 'visitors']\n).to_csv('submission_xgb-bayes-opt.csv', index=False, float_format=\"%.9f\")\n\nprint('Feature Importances')\npp(importances)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9bee2c85270bd5eadbb41d1b4a5e8b0014b5da7c","_cell_guid":"a95d1432-203e-4ef4-a7f7-a2457c3e7b4b","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}