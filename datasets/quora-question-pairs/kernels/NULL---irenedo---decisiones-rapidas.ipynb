{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cac1babc-e217-b7e9-0e65-8ff2f95074b2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4c8e4764-af57-dfca-7bb8-c6253085fa6f"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "%matplotlib inline\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "pal = sns.color_palette()\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f12bbcf8-83e9-3bee-b388-5581576d8a89"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv')\n",
        "df_test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "643def06-8ba1-cb2d-be17-9460dddf4f56"
      },
      "outputs": [],
      "source": [
        "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
        "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n",
        "q1 = pd.Series(df_train['question1'][0]) #para sacar la primera pregunta de question1\n",
        "print(q1)\n",
        "dist_train = train_qs.apply(len)\n",
        "dist_test = test_qs.apply(len)\n",
        "#plt.figure(figsize=(15, 10))\n",
        "#plt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\n",
        "#plt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\n",
        "#plt.title('Normalised histogram of character count in questions', fontsize=15)\n",
        "#plt.legend()\n",
        "#plt.xlabel('Number of characters', fontsize=15)\n",
        "#plt.ylabel('Probability', fontsize=15)\n",
        "\n",
        "#print('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), \n",
        "#                          dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "226fc83b-f22f-bb6b-dfe6-1232046b55f6"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "#print(stops)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6dc42c6-a54b-6435-7696-f751aa1d6f3f"
      },
      "outputs": [],
      "source": [
        "train_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n",
        "decision = {} #tabla con los datos estimados, duplicados o no.\n",
        "acierto =  {}#tabla para porcentajes de aciertos en la estimaci\u00f3n\n",
        "duplicados = df_train['is_duplicate'] #tabla con los datos de pares duplicados\n",
        "len(df_train)\n",
        "#print(duplicados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "060120e1-d565-e4dd-b17e-04cfb6ff704c"
      },
      "outputs": [],
      "source": [
        "for index in range(len(train_word_match)):\n",
        "    if (train_word_match[index] > 0.35):\n",
        "        decision[index] = 1\n",
        "    else:\n",
        "        decision[index] = 0\n",
        "#print (decision) \n",
        "counter = 0\n",
        "for index in range(len(train_word_match)):\n",
        "    if (decision[index] == duplicados[index]):\n",
        "        acierto[index]=1\n",
        "        counter = counter +1\n",
        "    else:\n",
        "        acierto[index]=0\n",
        "        \n",
        "porcentaje = (counter*100)/404290\n",
        "print(porcentaje)\n",
        "print(len(decision))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87047d78-ab7c-9c15-e7b5-9fd96c11e14d"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# If a word appears only once, we ignore it completely (likely a typo)\n",
        "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1 / (count + eps)\n",
        "\n",
        "eps = 5000 \n",
        "words = (\" \".join(train_qs)).lower().split()\n",
        "words_q1 = (\" \".join(q1)).lower().split()\n",
        "for w in range(len(words_q1)):\n",
        "    print(words_q1[w])\n",
        " \n",
        "\n",
        "    \n",
        "counts = Counter(words)\n",
        "weights = {word: get_weight(count) for word, count in counts.items()}#estudiar bien que hace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f59cf23-fe89-f138-d82b-beabafaa21c0"
      },
      "outputs": [],
      "source": [
        "#df_train['q1_n_words'] = df_train['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "#df_train['q2_n_words'] = df_train['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "ratiow = []\n",
        "\n",
        "for i in range(404290):\n",
        "    n_words = 0\n",
        "    q1 = pd.Series(df_train['question1'][i]).astype(str) #para sacar la primera pregunta de question1\n",
        "    q2 = pd.Series(df_train['question2'][i]).astype(str)\n",
        "    words_q1 = (\" \".join(q1)).lower().split()\n",
        "    words_q2 = (\" \".join(q2)).lower().split()\n",
        "    media = (len(words_q1) + len(words_q2))/2\n",
        "    minimo = min(len(words_q1), len(words_q2))\n",
        "    for w1 in words_q1:\n",
        "        for w2 in words_q2:\n",
        "            if (w1==w2) and (w1 not in stops):\n",
        "                n_words = n_words + 1\n",
        "    #print(n_words)            \n",
        "    r = n_words/minimo\n",
        "    ratiow.append(r)\n",
        "\n",
        "for w in range(20):\n",
        "    print(ratiow[w])\n",
        "    \n",
        "#print(len(ratiow))                \n",
        "df_train.head()                                   \n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "971301fe-9cf8-dca2-64c6-2cb4e3476dee"
      },
      "outputs": [],
      "source": [
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def same_word_ratio(row):\n",
        "    \n",
        "    ratiow = []\n",
        "    for i in range(404290):\n",
        "        n_words = 0\n",
        "        q1 = pd.Series(row['question1'][i]).astype(str) #para sacar la primera pregunta de question1\n",
        "        q2 = pd.Series(row['question2'][i]).astype(str)\n",
        "        words_q1 = (\" \".join(q1)).lower().split()\n",
        "        words_q2 = (\" \".join(q2)).lower().split()\n",
        "        media = (len(words_q1) + len(words_q2))/2\n",
        "        minimo = min(len(words_q1), len(words_q2))\n",
        "        for w1 in words_q1:\n",
        "            for w2 in words_q2:\n",
        "                if (w1==w2) and (w1 not in stops):\n",
        "                    n_words = n_words + 1\n",
        "        #print(n_words)            \n",
        "        r = n_words/minimo\n",
        "        ratiow.append(r)\n",
        "    return ratiow\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "34d23256-c121-bae4-fa30-11ca1806aa89"
      },
      "outputs": [],
      "source": [
        "decision2 = {} #tabla con los datos estimados, duplicados o no.\n",
        "\n",
        "acierto2 =  {} #tabla para porcentajes de aciertos en la estimaci\u00f3n\n",
        "duplicados2 = df_train['is_duplicate'] #tabla con los datos de pares duplicados\n",
        "#print(duplicados)\n",
        "ratiow = same_word_ratio(df_train)#llamada a la funcion hecha\n",
        "\n",
        "\n",
        "for index in range(len(ratiow)):\n",
        "    if (ratiow[index] > 0.25):\n",
        "        decision2[index] = 1\n",
        "    else:\n",
        "        decision2[index] = 0\n",
        "#print (decision) \n",
        "counter = 0\n",
        "for index in range(len(ratiow)):\n",
        "    if (decision2[index] == duplicados2[index]):\n",
        "        acierto2[index]=1\n",
        "        counter = counter +1\n",
        "    else:\n",
        "        acierto2[index]=0\n",
        "        \n",
        "porcentaje = (counter*100)/404290 \n",
        "print(porcentaje)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4937922-35c1-f4de-9781-8760610b5904"
      },
      "outputs": [],
      "source": [
        "decision2_test = {}\n",
        "ratiow_test = same_word_ratio(df_test)\n",
        "\n",
        "for index in range(len(ratiow_test)):\n",
        "    if (ratiow_test[index] > 0.25):\n",
        "        decision2_test[index] = 1\n",
        "    else:\n",
        "        decision2_test[index] = 0\n",
        "\n",
        "for w in range(20):\n",
        "    print(decision2_test[w])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "62fb1c51-b114-b91e-f9d2-41ece1ebf262"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f1cbe90-aa52-ca2f-f669-7dee88c3e5d7"
      },
      "outputs": [],
      "source": [
        "print('Most common words and weights: \\n')\n",
        "print(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\n",
        "print('\\nLeast common words and weights: ')\n",
        "(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c578dd54-c11a-825e-5682-c3671d06b814"
      },
      "outputs": [],
      "source": [
        "def tfidf_word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
        "        return 0\n",
        "    \n",
        "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
        "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
        "    \n",
        "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
        "    return R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "62730395-0912-eee2-ab2b-7b0da1cb15fb"
      },
      "outputs": [],
      "source": [
        "tfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
        "decision_tfidf = {} #tabla con los datos estimados, duplicados o no.\n",
        "acierto_tfidf =  {} #tabla para porcentajes de aciertos en la estimaci\u00f3n\n",
        "duplicados_tfidf = df_train['is_duplicate'] #tabla con los datos de pares duplicados\n",
        "print(duplicados_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b82c473f-0b4a-5521-8de3-30aaa22e84d2"
      },
      "outputs": [],
      "source": [
        "for index in range(len(tfidf_train_word_match)):\n",
        "    if (tfidf_train_word_match[index] > 0.35):\n",
        "        decision_tfidf[index] = 1\n",
        "    else:\n",
        "        decision_tfidf[index] = 0\n",
        "#print (decision_tfidf) \n",
        "\n",
        "counter = 0\n",
        "for index in range(len(tfidf_train_word_match)):\n",
        "    if (decision_tfidf[index] == duplicados_tfidf[index]):\n",
        "        acierto_tfidf[index]=1\n",
        "        counter = counter +1\n",
        "    else:\n",
        "        acierto_tfidf[index]=0\n",
        "        \n",
        "porcentaje_tfidf = (counter*100)/404265 \n",
        "print(porcentaje_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fc0d859-2b22-60e0-5669-22ccbdfee7bf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a693f760-4548-5e9d-bed0-fec1449ab23a"
      },
      "outputs": [],
      "source": [
        "word_match_test = df_test.apply(word_match_share, axis=1, raw=True)\n",
        "decision_test = {} #tabla con los datos estimados, duplicados o no.\n",
        "#duplicados = df_train['is_duplicate'] #tabla con los datos de pares duplicados\n",
        "len(df_test)\n",
        "#print(duplicados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c36d2fb-79f2-16ec-f9f2-b073914239ac"
      },
      "outputs": [],
      "source": [
        "for index in range(len(word_match_test)):\n",
        "    if (word_match_test[index] > 0.35):\n",
        "        decision_test[index] = 1\n",
        "    else:\n",
        "        decision_test[index] = 0\n",
        "#print (decision) \n",
        "for w in range(20):\n",
        "    print(decision_test[w])\n",
        "len(decision_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9093ae2-1813-8198-9073-de8ab9e9e51b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2457179-179f-8595-227c-a87384a400c2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3e96e96-e70a-7cfe-3cde-2eb7b09a05d8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "caef5022-53f6-0884-4696-eba74dadee98"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({'is_duplicate': decision2_test, 'test_id': df_test['test_id']})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "#sub = pd.DataFrame()\n",
        "#sub['test_id'] = df_test['test_id']\n",
        "#sub['is_duplicate'] = decision_test\n",
        "#sub.to_csv('simple_counw.csv', index=False)\n",
        "submission.head()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}