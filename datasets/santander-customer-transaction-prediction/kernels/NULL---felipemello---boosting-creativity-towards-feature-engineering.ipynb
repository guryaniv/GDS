{"cells":[{"metadata":{"_uuid":"88e4f09d7cd6d188c68e103c1c71ee4ae2ecf188"},"cell_type":"markdown","source":"### The purpose of this kernel is to boost your creativity towards feature engineering with Santander's Customer Transaction variables. If it fulfills this purpose, please, consider upvoting the kernel :)\n\n--------------------------\n\n## **Index**:\n1. Competition and Data Overview; <br>\n2. Defining probability in terms of frequency difference;<br>\n&nbsp; &nbsp;2.1\\. Building our graph<br>\n&nbsp; &nbsp;2.2 Defining the frequency ratio between right and left sides<br>\n3. Analysing the type of graphs;<br>\n&nbsp; &nbsp;3.1 Regular features<br>\n&nbsp; &nbsp;3.2 Reversed features<br>\n&nbsp; &nbsp;3.3 Flat features<br>\n&nbsp; &nbsp;3.4 Extreme features<br>\n4. Feature engineering<br>\n&nbsp; &nbsp;4.1 Using frequency ratio as a predictive score<br>\n&nbsp; &nbsp;4.2 Separating different populations as a gaussian mixture<br>\n5. Summary"},{"metadata":{"_uuid":"64cbbbc80ff6686283765f9e7b2cb6b95165a683"},"cell_type":"markdown","source":"### **1) COMPETITION AND DATA OVERVIEW:**\n\nSantander's competition is about predicting which clients are going to make a transfer in the future. The available dataset is split between train and test, with 200,000 clients in each and no missing values. There are 200 variables available (0, 199), but no information about them is given other than the numbers they hold.\n\nAs far as we know, all variables are independent from each other to the extent that if we separate the clients that made the transfer from the clients that didn't, then shuffle their values, and put them all back together, __[it won't affect your prediction score](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/83882)__.\n\nSo far, only a few teams were able to cross the 0.902 prediction score barrier. A lot of people tried many different ideas, and almost all of them failed. So, I hope you have fun reading this kernel and that it may help you joining the 0.902 club.\n\nLet's take a quick look at the data:"},{"metadata":{"trusted":true,"_uuid":"17b7976b001cf97a4e67d05d11e66f2f8517e55f","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"#import libs\nimport numpy as np\nimport pandas as pd \nimport lightgbm as lgb\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport os\nimport sys\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm, skewnorm\nfrom collections import defaultdict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9f41921e3e0ac607bc2dab81520a74099249f4"},"cell_type":"code","source":"#import data\ntrain_df = pd.read_csv('../input/train.csv')\n#test_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7e0c1eff8f4554ca416d1b058c0e4ad3138b72"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17012608a6981022772836c71773c6ad80952107"},"cell_type":"markdown","source":"### **2) DEFINING PROBABILITY IN TERMS OF FREQUENCY DIFFERENCE**\n\n#### **2.1) BUILDING OUR GRAPH**\n\nBasically on this kernel I would like to show to you how the probability of making a transfer is highly correlated to the difference in frequency between the right and left sides of the distribution.\n\nLet's take a look at the frequency distribution of var_0."},{"metadata":{"trusted":true,"_uuid":"65da167444de67729010f0f0185e730438413f8d","_kg_hide-input":true},"cell_type":"code","source":"def running_mean(x, N):\n   cumsum = np.cumsum(np.insert(x, 0, 0)) \n   return np.concatenate([x[0:N-1], (cumsum[N:] - cumsum[:-N]) / N])\n\nvar = 'var_0' #feat we are going to analyse\nmydf = train_df.copy() #df we are going to analyze\n\ndef df_to_bin(var, mydf):\n    IQR = mydf[var].quantile([0.75]).values - mydf[var].quantile([0.25]).values #inter-quartile range\n    n = 200000 #size of our dataframe\n    bin_size = 2.5*IQR/n**(1/3) #Freedman-Diaconis rule to define the bin size\n    bin_number = int((mydf[var].max() - mydf[var].min())/bin_size)\n\n    #creat new feature based on the bins\n    mydf['new' + var] = pd.cut(mydf[var], bins = bin_number, labels = range(bin_number)).astype('float')\n\n    #separate dfs betweens target ==0 and target == 1\n    df_no_transfer = mydf['new' + var][mydf['target'] == 0].value_counts().sort_index()\n    #get the bin frequency height (H) and the index of that bin (X)\n    H0,X0 = list(df_no_transfer.values), list(df_no_transfer.index) \n\n    df_with_transfer = mydf['new' + var][mydf['target'] == 1].value_counts().sort_index()\n    H1,X1 = list(df_with_transfer.values), list(df_with_transfer.index) \n\n    #If there are any bins with 0 frequency between 0 and the total number of bins, lets put it in our list\n    for i in range(bin_number):\n        if i not in X0:\n            H0.append(0)\n            X0.append(i)\n\n        if i not in X1:\n            H1.append(0)\n            X1.append(i)\n\n    #sort the bins so the list index == bin number\n    H0 = [h for _,h in sorted(zip(X0,H0))]\n    X0 = [x for x,_ in sorted(zip(X0,H0))]\n    H1 = [h for _,h in sorted(zip(X1,H1))]\n    X1 = [x for x,_ in sorted(zip(X1,H1))]\n\n    H0 = running_mean(H0, 2)  \n    H1 = running_mean(H1, 2)  \n    Hsum = H0 + H1\n    \n    return bin_size, bin_number, mydf, H0, H1, Hsum, X0, X1\n\nvar = 'var_0'\nbin_size, bin_number, mydf, H0, H1, Hsum, X0, X1 = df_to_bin(var, train_df)\n\n#plot the result\nfig, ax1 = plt.subplots()\nax1.plot(H0, label='target = 0 (doesnt make a transfer)')\nax1.plot(H1, label='target = 1 (makes a transfer)')\nax1.plot(Hsum, label='total data')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nfig.suptitle(var, fontsize=20)\nax1.set_ylabel('Frequency', color='g', fontsize=12)\nax1.tick_params(axis='y', colors='green')\nplt.xlabel('Bin Number', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf8af05cdef8787f789c92cfede5af3ece554e9d"},"cell_type":"markdown","source":"- In **<font color=orange>orange</font>** you can see the distribution of the clients that *make* a transfer.\n- In **<font color=blue>blue</font>** it is the distribution of the clients that *do not make* a transfer.\n- And finally, in **<font color=green>green</font>**, it is the *total* distribution"},{"metadata":{"_uuid":"e282ba617a59f9af04630de03878b9676408d957"},"cell_type":"markdown","source":"> #### **2.1) DEFINING THE PROBABILITY OF MAKING A TRANSFER**\n\nNow that we have the frequency distribution, let's take a look at the probability of a client making a transfer on each bin of this distribution.\n\n$$Probabiliy = MakesTransfer / TotalData$$\n\nPS: We will only check probability values where bins have at least 80 data points."},{"metadata":{"trusted":true,"_uuid":"03dced7b61bfe03020773b9b223c7390ce0c13eb","_kg_hide-input":true},"cell_type":"code","source":"Hmin = 80\ndef get_probs(bin_number, H1, Hsum, Hmin):\n    probs = []\n    #finds min_i\n    for i in range(bin_number):\n        if Hsum[i] > Hmin:\n            min_i = i\n            break\n\n    #finds max_i\n    for i in reversed(range(bin_number)):\n        if Hsum[i] > Hmin:\n            max_i = i\n            break\n\n    #get probabilities to plot\n    for i in range(0, bin_number):\n        if i < min_i or i > max_i:\n            probs.append(0)\n        else:\n            probs.append(H1[i]/Hsum[i]) \n    \n    return probs, min_i, max_i\n\nprobs, min_i, max_i = get_probs(bin_number, H1, Hsum, Hmin)\n\n#plot the result\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(H0, label='target = 0')\nax1.plot(H1, label='target = 1')\nax1.plot(Hsum, label='total data')\nax2.plot(probs, 'r', label='Probability target = 1')\nax1.legend(bbox_to_anchor=(1.10, 1), loc=3)\nax2.legend(bbox_to_anchor=(1.10, 0.90), loc=3)\nfig.suptitle(var, fontsize=20)\nax1.set_ylabel('Frequency', color='g', fontsize=12)\nax1.tick_params(axis='y', colors='green')\nax2.set_ylabel('Probability', color='r', fontsize=12)\nax2.tick_params(axis='y', colors='red')\nax1.set_xlabel('Bin Number', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccaafce696a7d81327d2068287acb996e63bcb93"},"cell_type":"markdown","source":"As you can see, in **<font color=red>red</font>**, the probability of making a transfer is around 10% for all the values before bin 60, when it starts raising up to 20%. An important observation is that only 10% of the clients in the whole dataset make a transfer. So if we had to randomly guess who will make a transfer before bin 60, we would have a really good score without using any complex model. \n\n- Is something happening after bin 60? \n- Is this behavior common?\n\nLet's see!"},{"metadata":{"_uuid":"281145721a8bcac7d7e5ed2aaa907c224e62f425"},"cell_type":"markdown","source":"#### **2.2) DEFINING THE FREQUENCY RATIO BETWEEN RIGHT AND LEFT SIDES**\n\nAfter plotting a few graphs, it became clear that the side with more \"volume\" usually has a higher probability. Let's dig deeper.\n\nFirst, let's define the **center of the distribution** as the line in between the two most \"straight sequence\" on the sides. To get these sequences, we will have to look at the slope."},{"metadata":{"trusted":true,"_uuid":"351d0944781fad82283edc209dba4c6764c47fda","_kg_hide-input":true},"cell_type":"code","source":"def get_central_point(bin_number, Hsum, H0, H1):\n\n    #finds the slop at each point       \n    slope = []\n    for i in range(bin_number - 1):\n\n        init = Hsum[i]\n        end = Hsum[i+1]\n        x = 1\n        y = end - init\n        slope.append(np.arctan(y/x) * 180/3.1416)\n\n    slope.append(slope[-1]) #last slope is equal to its previous\n    slope = np.array(slope)\n\n    #finds the slop difference at each point   \n    slopediff = [0]\n    for i in range(1, bin_number):\n        slopediff.append(np.abs(slope[i] - slope[i-1]))\n\n    #finds two \"straight lines\" of length 15 on the left and right of the distribution\n    min_len = 15\n    max_slop_diff = 0.05\n    len_max_l = 0\n    len_max_r = 0\n    count = 0\n    #at each iteration, if we don't have the 15 points if max_slop_diff between them,\n    #we will lower this difference threshold\n    while len_max_l < min_len or len_max_r < min_len:\n        count += 1 #if after 5 iterations we dont get those 15 points, we will lower the minimum amount of points\n        slope_dict_left = defaultdict(list)\n        slope_dict_right = defaultdict(list)\n        l, r = 0, 0\n        maxl = 0\n        maxr = 0\n\n        for i in range(bin_number):\n            s = slope[i]\n            sd = slopediff[i]\n            if s > 0 and sd < max_slop_diff and Hsum[i] > 1000:\n                slope_dict_left[l].append(i)\n            else:\n                l_len = len(slope_dict_left[l])\n                if l_len > len(slope_dict_left[maxl]):\n                    maxl = l\n                l += 1\n            if s < 0 and sd < max_slop_diff and Hsum[i] > 1000:\n                slope_dict_right[r].append(i)\n            else:\n                r_len = len(slope_dict_right[r])\n                if r_len > len(slope_dict_right[maxr]):\n                    maxr = r\n                r += 1\n\n        len_max_l = len(slope_dict_left[maxl])\n        len_max_r = len(slope_dict_right[maxr])\n\n        #only used if we dont get the 15 points on left and right side\n        if count > 5:\n            if min_len > 12:\n                min_len = min_len*0.9\n            max_slop_diff = max_slop_diff*1.1\n\n            H0 = running_mean(H0, 2)\n            H1 = running_mean(H1, 2)\n            Hsum = H0 + H1\n\n            #finds the slop at each point       \n            slope = []\n            for i in range(bin_number - 1):\n\n                init = Hsum[i]\n                end = Hsum[i+1]\n                x = 1\n                y = end - init\n                slope.append(np.arctan(y/x) * 180/3.1416)\n            slope.append(slope[-1])\n\n            slope = np.array(slope)\n\n            slopediff = [0]\n            for i in range(1, bin_number):\n                slopediff.append(np.abs(slope[i] - slope[i-1]))\n\n        else:\n            max_slop_diff = max_slop_diff*1.1\n\n    #select the lines with the highest length\n    slope_left = slope_dict_left[maxl]#[int(len_max_l*0.30): int(len_max_l*0.90)]\n    slope_right = slope_dict_right[maxr]#[int(len_max_r*0.10): int(len_max_r*0.70)]\n\n    #now lets align their maximum and minimum height H, with a maxium difference of 100\n    max_diff = 100\n\n    while Hsum[slope_left[-1]] - Hsum[slope_right[0]] > max_diff:\n        slope_left = slope_left[:-2]\n\n    while Hsum[slope_right[0]] - Hsum[slope_left[-1]] > max_diff:\n        slope_right = slope_right[1:]\n\n    while Hsum[slope_right[-1]] - Hsum[slope_left[0]] > max_diff:\n        slope_left = slope_left[1:]\n\n    while Hsum[slope_left[0]] - Hsum[slope_right[-1]] > +max_diff:\n        slope_right = slope_right[:-2]\n\n    #Find the median of the lines\n    i_median_left = int(len(slope_left)/2)\n    i_median_right = int(len(slope_right)/2)\n\n    #Finds the central point of the distribution based on the this two lines\n    l = 0\n    r = 0\n    H_left = Hsum[slope_left]\n    H_right = Hsum[slope_right]\n    indexes = []\n    if len(slope_left) < len(slope_right):\n\n        for h in H_left:\n            indexes.append(np.argmin(np.abs(H_right-h)))\n\n        for index_left, index_right in zip(range(len(slope_left)), indexes):\n            l += slope_left[index_left]\n            r += slope_right[index_right]\n\n        central_point = np.round((l+r)/(2*(index_left+1)))\n    else:\n        for h in H_right:\n            indexes.append(np.argmin(np.abs(H_left-h)))\n\n        for index_right, index_left in zip(range(len(slope_right)), indexes):\n            l += slope_left[index_left]\n            r += slope_right[index_right]\n\n        central_point = np.round((l+r)/(2*(index_right+1)))\n\n    central_point = np.round(central_point)\n    \n    return central_point, Hsum, slope_left, slope_right, i_median_left, i_median_right\n\ncentral_point, Hsum, slope_left, slope_right, i_median_left, i_median_right = get_central_point(bin_number, Hsum, H0, H1)\n\n#plot the result\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(H0, label='target = 0')\nax1.plot(H1, label='target = 1')\nax1.plot(Hsum, label='total data')\nax1.plot(slope_left, Hsum[slope_left], 'k')\nax1.plot(slope_right, Hsum[slope_right], 'k')\nax1.plot([central_point, central_point], [0,3000], 'grey')\nax2.plot(probs, 'r', label='Probability target = 1')\nax1.legend(bbox_to_anchor=(1.10, 1), loc=3)\nax2.legend(bbox_to_anchor=(1.10, 0.90), loc=3)\n\nax1.set_ylabel('Frequency', color='g', fontsize=12)\nax1.tick_params(axis='y', colors='green')\nax1.set_xlabel('Bin Number', fontsize=12)\n\nax2.set_ylabel('Probability', color='r', fontsize=12)\nax2.tick_params(axis='y', colors='red')\n\nfig.suptitle(var, fontsize=20)\nplt.xlabel('bin_number', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"939525877645f714363cbd00ac14de94af978d48"},"cell_type":"markdown","source":"If you look at the graph, you will see that we have set as **black** the sides of the distribution, and in **<font color=grey>grey</font>** what we called the central point.\n\nNow lets plot the frequency difference of opposite bins based on this central point. For example: Let's suppose that the central point is on bin 60. \n- On bin 61, the height ratio will be height_bin_61/height_bin_59. \n- On bin 30, the height ratio will be height_bin_30/height_bin_90."},{"metadata":{"trusted":true,"_uuid":"283918266fcbe6eaf74c5ef166febb30f3e263aa","_kg_hide-input":true},"cell_type":"code","source":"def get_ratio(central_point, bin_number, Hsum, Hmin):\n\n    ratio = []\n    len_1st_half = central_point\n    len_2nd_half = bin_number - central_point\n    i = 0\n    if len_1st_half > len_2nd_half:\n        index = central_point - len_2nd_half\n\n        while i < index:\n            ratio.append(0)\n            i += 1\n\n        for i in range(i, bin_number):\n            i_end = int(central_point + (central_point - i) - 1)\n            if Hsum[i_end] < Hmin or Hsum[i] < Hmin:\n                ratio.append(0)\n            else:\n                ratio.append((Hsum[i])/(Hsum[i_end]))\n\n\n    else:\n        index = central_point - len_2nd_half\n\n        for i in range(i, bin_number):\n            i_end = int(central_point + (central_point - i) - 1)\n            if i > 2*central_point:\n                ratio.append(0)\n                continue\n            if Hsum[i_end] < Hmin or Hsum[i] < Hmin:\n                ratio.append(0)\n            else:\n                ratio.append((Hsum[i])/(Hsum[i_end]))\n            \n\n    return ratio\n\nratio = get_ratio(central_point, bin_number, Hsum, Hmin)\n\n#plot the result\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax3 = ax1.twinx()\nax1.plot(H0, label='target = 0')\nax1.plot(H1, label='target = 1')\nax1.plot(Hsum, label='total data')\nax1.plot(slope_left, Hsum[slope_left], 'k')\nax1.plot(slope_right, Hsum[slope_right], 'k')\nax1.plot([central_point, central_point], [0,3000], 'grey')\nax2.plot(probs, 'r', label='Probability target = 1')\nax3.plot(ratio, 'b', label='height ratio')\nax1.legend(bbox_to_anchor=(1.1, 1), loc=3)\nax2.legend(bbox_to_anchor=(1.1, 0.90), loc=3)\nax3.legend(bbox_to_anchor=(1.1, 0.80), loc=3)\nfig.suptitle(var, fontsize=20)\nax1.set_ylabel('Frequency', color='g', fontsize=12)\nax1.tick_params(axis='y', colors='green')\nax1.set_xlabel('Bin Number', fontsize=12)\n\nax2.set_ylabel('Probability', color='r', fontsize=12)\nax2.tick_params(axis='y', colors='red')\n\nax3.set_ylabel('Frequency ratio', color='b', fontsize=12)\nax3.tick_params(axis='y', colors='blue')\nax3.yaxis.set_label_coords(1.18, 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e31e50c343b4168932520290e867af2516d80d0"},"cell_type":"markdown","source":"The result of this ratio can be notice by the **<font color=blue>blue</font>** line, which shows how many times a bin is higher than its opposite. So, for example, the bin around 90 is 6 times higher/more frequent than the bin around 30. <br>\n*Note: the abrupt cut on the end of the blue line is either because there are less than 80 data points on one of the bins being compared, of because it is a division by 0*\n\nIt becomes clear that there is a strong correlation between the frequency ratio and the probability of a client making a transfer.\n\nOne can hypothesize that, if we know the distribution of our data, maybe we could try to estimate the probability of a client making a transfer based on each variable. But we still haven't answered: Is this behavior common?"},{"metadata":{"_uuid":"700f81649108f512128da0d471d2f8a828c22650"},"cell_type":"markdown","source":"### **3) ANALYSING THE TYPES OF GRAPHS**\nBy analyzing the data, I was able to find basically 4 types of graphs:\n\n**Regular)** In **149** of them, probability positively correlates to height ratio: the side with more volume has a higher probability.\n\n**Reversed**) In **27** of them, probability negatively correlates to height ratio: the side with less volume has a higher probability. <br>\n*Note: It might not be a characteristic of the variable itself, but just my algorithm not being perfect and selecting a good central_point*\n\n**Flat)** In **20** of them, the probability is flat or almost flat: the difference in frequency won't or will barely affect the probabilities, which will stay around 10% no matter the value of the variable. <br>\n*Note: Those variables seem to be the ones of the least impact in the predictive model.*\n\n**Extremes)** Only **4** of them have a behavior in which the probability increases on both extremities.\n\nInteresting observation: There is not a single feature that has a flat probability distribution followed by a decrease on the extremes. That means that we should focus our efforts on finding characteristics that correlates to target = 1, and not target = 0.\n\nLet's see those graphs below:"},{"metadata":{"trusted":true,"_uuid":"aae09d9bdf8b4ff783485b50d1c85bc3899e48bd","_kg_hide-input":true},"cell_type":"code","source":"#function we will use to plot\ndef plot_graph(var, df, Hmin):\n    bin_size, bin_number, df, H0, H1, Hsum, X0, X1 = df_to_bin(var, train_df)\n    probs, min_i, max_i = get_probs(bin_number, H1, Hsum, Hmin)\n    central_point, Hsum, slope_left, slope_right, i_median_left, i_median_right = get_central_point(bin_number, Hsum, H0, H1)\n    ratio = get_ratio(central_point, bin_number, Hsum, Hmin)\n    \n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n    ax3 = ax1.twinx()\n    ax1.plot(H0, label='target = 0')\n    ax1.plot(H1, label='target = 1')\n    ax1.plot(Hsum, label='total data')\n    ax1.plot(slope_left, Hsum[slope_left], 'k')\n    ax1.plot(slope_right, Hsum[slope_right], 'k')\n    ax1.plot([central_point, central_point], [0,3000], 'grey')\n    ax2.plot(probs, 'r', label='Probability target = 1')\n    ax3.plot(ratio, 'b', label='height ratio')\n    ax1.legend(bbox_to_anchor=(1.1, 1), loc=3)\n    ax2.legend(bbox_to_anchor=(1.1, 0.90), loc=3)\n    ax3.legend(bbox_to_anchor=(1.1, 0.80), loc=3)\n    \n    ax1.set_ylabel('Frequency', color='g', fontsize=12)\n    ax1.tick_params(axis='y', colors='green')\n    ax1.set_xlabel('Bin Number', fontsize=12)\n    \n    ax2.set_ylabel('Probability', color='r', fontsize=12)\n    ax2.tick_params(axis='y', colors='red')\n\n    ax3.set_ylabel('Frequency ratio', color='b', fontsize=12)\n    ax3.tick_params(axis='y', colors='blue')\n    ax3.yaxis.set_label_coords(1.18, 0.5)\n    \n    fig.suptitle(var, fontsize=20)\n    plt.xlabel('bin_number', fontsize=12)\n    plt.show()\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87231caebe451e2d1eb870ae45ff06ca2ecf4f63"},"cell_type":"markdown","source":"### **3.1) REGULAR FEATURES EXAMPLE**"},{"metadata":{"trusted":true,"_uuid":"db2d0154256b2e598ea419001681e0821f9a0ecc"},"cell_type":"code","source":"regular_features = ['var_0', 'var_1', 'var_101', 'var_102', 'var_105', 'var_106', 'var_108', 'var_109', 'var_11', 'var_110', 'var_111', 'var_113', 'var_114', 'var_115', 'var_116', 'var_118', 'var_119', 'var_12', 'var_122', 'var_123', 'var_125', 'var_127', 'var_128', 'var_129', 'var_13', 'var_130', 'var_131', 'var_132', 'var_133', 'var_134', 'var_135', 'var_137', 'var_138', 'var_139', 'var_141', 'var_143', 'var_144', 'var_145', 'var_146', 'var_147', 'var_148', 'var_15', 'var_150', 'var_151', 'var_152', 'var_153', 'var_154', 'var_155', 'var_157', 'var_159', 'var_16', 'var_162', 'var_163', 'var_164', 'var_165', 'var_166', 'var_167', 'var_168', 'var_169', 'var_170', 'var_171', 'var_173', 'var_174', 'var_175', 'var_176', 'var_179', 'var_18', 'var_180', 'var_181', 'var_182', 'var_184', 'var_187', 'var_188', 'var_189', 'var_19', 'var_190', 'var_191', 'var_193', 'var_194', 'var_195', 'var_196', 'var_197', 'var_198', 'var_2', 'var_22', 'var_24', 'var_25', 'var_26', 'var_28', 'var_3', 'var_32', 'var_34', 'var_35', 'var_36', 'var_37', 'var_39', 'var_4', 'var_40', 'var_42', 'var_43', 'var_44', 'var_48', 'var_49', 'var_5', 'var_50', 'var_51', 'var_52', 'var_53', 'var_55', 'var_56', 'var_59', 'var_6', 'var_60', 'var_61', 'var_62', 'var_63', 'var_64', 'var_66', 'var_67', 'var_68', 'var_69', 'var_70', 'var_71', 'var_72', 'var_73', 'var_74', 'var_75', 'var_76', 'var_78', 'var_79', 'var_80', 'var_81', 'var_82', 'var_83', 'var_84', 'var_85', 'var_86', 'var_88', 'var_89', 'var_9', 'var_90', 'var_91', 'var_92', 'var_93', 'var_94', 'var_95', 'var_96', 'var_97', 'var_99']\nfor var in regular_features[:3]:\n    plot_graph(var, train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bdbbd775c2df23b4d406022b97fec80c62a902a"},"cell_type":"markdown","source":"### **3.2) REVERSED FEATURES EXAMPLE**"},{"metadata":{"trusted":true,"_uuid":"42d6e6ea7cd5bd4fec8e62f87dbaf37b45b1a4cf"},"cell_type":"code","source":"reversed_features = ['var_104', 'var_107', 'var_112', 'var_121', 'var_14', 'var_140', 'var_142', 'var_149', 'var_156', 'var_160', 'var_172', 'var_177', 'var_178', 'var_186', 'var_192', 'var_199', 'var_20', 'var_21', 'var_23', 'var_31', 'var_33', 'var_45', 'var_57', 'var_65', 'var_77', 'var_8', 'var_87']\nfor var in reversed_features[:3]:\n    plot_graph(var, train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c9f4925b1e317e2aaf355a19d9bafeade82a13b"},"cell_type":"markdown","source":"### **3.3) FLAT FEATURES EXAMPLE**"},{"metadata":{"trusted":true,"_uuid":"b3281d00c9dfe4682a4f62637987ba8f62c6a4ae"},"cell_type":"code","source":"flat_features = ['var_10', 'var_100', 'var_103', 'var_117', 'var_124', 'var_126', 'var_136', 'var_158', 'var_161', 'var_17', 'var_183', 'var_185', 'var_27', 'var_29', 'var_30', 'var_38', 'var_41', 'var_47', 'var_7', 'var_98']\nfor var in flat_features[:3]:\n    plot_graph(var, train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c00876ab84a82ff72a74fc979c3c94aa4797955"},"cell_type":"markdown","source":"### **3.4) EXTREME FEATURES EXAMPLE**"},{"metadata":{"trusted":true,"_uuid":"c3781bdeccbe1bb5fe5405af883efeb1cd57093c"},"cell_type":"code","source":"extreme_features = ['var_120', 'var_46', 'var_54', 'var_58']\nfor var in extreme_features[:3]:\n    plot_graph(var, train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b89e63522cbf25bb90e254b002de6d3496a8da87"},"cell_type":"markdown","source":"### **4) FEATURE ENGINEERING**\n\n#### **4.1) USING FREQUENCY RATIO AS A PREDICTIVE SCORE**\nBased on this correlation between frequency ratio and probability of making a transfer, I gave to each client on each variable a score indicating the probability of making a transfer. Then, I summed this score and compared to the target. The final result was a new feature with 0.43 correlation with the target. The higher the score, the higher were the chances of making a transfer. <br>\n*Note: I won't get in details on how I did it, but if you want to know, let me know on the comments.*\n\nBesides the high correlation, my prediction score stayed about the same. I would guess that I wasn't adding any new information to the model that it didn't already know.\n\nHowever, I didn't give up as I had a last idea to try.\n\n#### **4.2) SEPARATING DIFFERENT POPULATIONS AS A GAUSSIAN MIXTURE**\n\nIf you look closely at the variables, you can easily see that it is possible to identify a mixture of distributions, like if there were different populations. Take a look, for example, at the right side of the 4, 5 and 26 distribution.\n"},{"metadata":{"trusted":true,"_uuid":"8799b9fad23cc79bc236ba04257203305671a232"},"cell_type":"code","source":"for var in ['var_4', 'var_5', 'var_26']:\n    plot_graph(var, train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b22d2bb33b8681e3bb2986d6b98e6e18306d178"},"cell_type":"markdown","source":"All of these variables have a bump of frequency that matches the rising of the probability of making a transfer. If we separate those populations, maybe each one of them could be a different variable, and our algorithm would be able to identify better who is going to make a transfer. I won't get into details on how I approached that, but you can see what I mean by a simple example below."},{"metadata":{"trusted":true,"_uuid":"4cdb978bb4ca2221d41435dfe0cf337a9fc0a278","_kg_hide-input":true},"cell_type":"code","source":"from itertools import starmap\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import mlab\nfrom scipy.stats import truncnorm\nfrom sklearn.mixture import GaussianMixture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66e11b368d26d4fac22106e1b59fc9f673318f95","_kg_hide-input":true},"cell_type":"code","source":"#EXAMPLE VARIABLE 4\nmydf = train_df\nvar = 'var_4'\nprint(var)\n\nn_components = 2\nmydf.sort_values(var, inplace = True)\nsamples = np.array(mydf[var])\nsamples_max = max(samples)\nsamples_min = min(samples)\n\n\nmixture = GaussianMixture(n_components=n_components, weights_init = [0.80, 0.20], means_init = [[11], [14.5]]).fit(samples.reshape(-1, 1))\nmeans_hat = mixture.means_.flatten()\nweights_hat = mixture.weights_.flatten()\nsds_hat = np.sqrt(mixture.covariances_).flatten()\n\n\nmeans_hat = [m for m, _ in sorted(zip(means_hat, sds_hat))]\nsds_hat = [s for _, s in sorted(zip(means_hat, sds_hat))]\n\n#print('mixture converged', mixture.converged_)\n#print('means', means_hat)\n#print('standard deviations', sds_hat)\n#print('weights', weights_hat)\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nfor mu1_h, sd1_h, weight, i in zip(means_hat, sds_hat, weights_hat, range(n_components)):\n\n    x_axis = np.linspace(samples_min, samples_max, int(np.round(weight*len(samples))))\n\n    Y = np.random.normal(mu1_h, sd1_h, int(np.round(weight*len(samples))))\n    Y = np.exp(mixture.score_samples(x_axis.reshape(-1,1)))\n    ax2.plot(x_axis, Y)\n    ax1.hist(np.random.normal(mu1_h, sd1_h, int(np.round(weight*len(samples)))))\n\n    mydf[var+'_' + str(i)] = np.nan\n\nmydf.sort_values('ID_code', inplace = True)\nplt.show\n\nplot_graph('var_4', train_df, Hmin = 80)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c5ef32fe516c4c3687d0d0fd79930ab5b4a5b23"},"cell_type":"markdown","source":"Unfortunatelly, I also wasn't sucessful with this feature engineering approach, but maybe I just didn't try it the right way.\n\n### **5) SUMMARY**\n\nWe proved that there is a correlation between making a transfer and the frequency ratio of the two sides of the distribution. We hypothesized that maybe it is possible to separate different populations from each variable and create new features from this information.\n\nI hope you had fun reading this kernel. Please, if it gave you new ideas or if you learned something new about the data, consider upvoting, and let me know on the comments if you have succeeded implementing them.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}