{"cells": [{"metadata": {"_cell_guid": "c0cfc83c-29ee-4673-b2fd-ee77aaadd35c", "_uuid": "b29004616894b0edd6bfcad97744e7ea27b608ac"}, "source": ["# A PyTorch Neural Network for price prediction (Linear Regression) using loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam\n", "\n", "\n", "Learning curve:\n", "![logo](https://github.com/QuantScientist/Deep-Learning-Boot-Camp/raw/master/Kaggle-PyTorch/mercari/rms.png)\n", "\n", "\n", "## Introduction\n", "- Work In Progress: will update as I make progress\n", "- Heavily based on: https://www.kaggle.com/bguberfain/naive-catboost for preprocessing . \n", "\n", "## On github\n", "https://github.com/QuantScientist/Deep-Learning-Boot-Camp/tree/master/Kaggle-PyTorch\n", "\n", "#### S\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%reset -f\n", "from __future__ import print_function\n", "from __future__ import division\n", "\n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "\n", "import torch\n", "import sys\n", "import torch\n", "from torch.utils.data.dataset import Dataset\n", "from torch.utils.data import DataLoader\n", "from torchvision import transforms\n", "from torch import nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.autograd import Variable\n", "\n", "from sklearn import cross_validation\n", "from sklearn import metrics\n", "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n", "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n", "\n", "print('__Python VERSION:', sys.version)\n", "print('__pyTorch VERSION:', torch.__version__)\n", "\n", "import numpy\n", "import numpy as np\n", "\n", "\n", "\n", "# ! watch -n 0.1 'ps f -o user,pgrp,pid,pcpu,pmem,start,time,command -p `lsof -n -w -t /dev/nvidia*`'\n", "# sudo apt-get install dstat #install dstat\n", "# sudo pip install nvidia-ml-py #install Python NVIDIA Management Library\n", "# wget https://raw.githubusercontent.com/datumbox/dstat/master/plugins/dstat_nvidia_gpu.py\n", "# sudo mv dstat_nvidia_gpu.py /usr/share/dstat/ #move file to the plugins directory of dstat\n", "\n", "import pandas\n", "import pandas as pd\n", "\n", "import logging\n", "handler=logging.basicConfig(level=logging.INFO)\n", "lgr = logging.getLogger(__name__)\n", "\n", "\n", "# !pip install psutil\n", "import psutil\n", "import os\n", "def cpuStats():\n", "        print(sys.version)\n", "        print(psutil.cpu_percent())\n", "        print(psutil.virtual_memory())  # physical memory usage\n", "        pid = os.getpid()\n", "        py = psutil.Process(pid)\n", "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n", "        print('memory GB:', memoryUse)\n", "\n", "cpuStats()\n", "\n", "# Data params\n", "TARGET_VAR= 'target'\n", "BASE_FOLDER = '../input/'"], "metadata": {"_cell_guid": "1450a423-d442-4adf-96f2-2878f1d0d33c", "_uuid": "8d8ffc194d3352e15cfffecbbcc257525e8c740c", "collapsed": true}}, {"metadata": {"_cell_guid": "401431a3-e74c-4718-82eb-91b92e1dc726", "_uuid": "cdb3b4d60ca404b8161646065625fcff58b8ccd1"}, "source": ["# CUDA"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n", "use_cuda = torch.cuda.is_available()\n", "# use_cuda = False\n", "\n", "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n", "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n", "Tensor = FloatTensor\n", "\n", "print(\"USE CUDA=\" + str (use_cuda))\n", "\n", "#torch.backends.cudnn.benchmark = True"], "metadata": {"_cell_guid": "49d9abae-5aae-4517-982a-cd9c33f8f354", "_uuid": "82af9f893b48d52894a85883c1db722d8ea7a8a1", "collapsed": true}}, {"metadata": {"_cell_guid": "3ed40f58-da9b-41b2-862e-7465a30313f2", "_uuid": "eb9be36bd1a358382664ef40cc73d50e4e172ea5"}, "source": ["### References\n", "\n", "\n", "## Load the data, split the training data into a training and validation set\n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# fix seed\n", "seed=17*19\n", "np.random.seed(seed)\n", "torch.manual_seed(seed)\n", "if use_cuda:\n", "    torch.cuda.manual_seed(seed)\n", "\n", "#####\n", "# Load in the data\n", "#####\n", "print('loading data')\n", "\n", "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n", "df_test = pd.read_csv('../input/test.tsv', sep='\\t')\n", "\n", "print('Train shape:{}\\nTest shape:{}'.format(df_train.shape, df_test.shape))\n", "\n", "df_train.head(5)"], "metadata": {"_cell_guid": "a4d9e0cc-0b2e-4657-8dcb-7e867f9c9a1d", "_uuid": "b28e4558e0db0bf35f090ecf7ef81e38b2e68618", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# df_train.plot(kind='scatter', x='item_condition_id', y='price', title='Weight and height in adults')\n"], "metadata": {"_cell_guid": "7831345d-fa0b-4908-bace-4bcf7d9f2246", "_uuid": "09fc5862141836e71edcdcff453651ec6aa02007", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["from sklearn.preprocessing import LabelEncoder\n", "from sklearn.pipeline import Pipeline\n", "from collections import defaultdict\n", "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n", "from sklearn import preprocessing\n", "\n", "d = defaultdict(LabelEncoder)\n", "TARGET_VAR='price'\n", "\n", "def split_cat(s):\n", "    try:\n", "        return s.split('/')[0],s.split('/')[1],s.split('/')[2],\n", "    except:\n", "        return [0,0,0]\n", "\n", "df_train[['cat1','cat2','cat3']] = pd.DataFrame(df_train.category_name.apply(split_cat).tolist(),\n", "                                   columns = ['cat1','cat2','cat3'])\n", "df_test[['cat1','cat2','cat3']] = pd.DataFrame(df_test.category_name.apply(split_cat).tolist(),\n", "                                   columns = ['cat1','cat2','cat3'])\n", "\n", "print('making the magic...')\n", "corpus1 = df_train.name.values.astype('U').tolist() + df_test.name.values.astype('U').tolist()\n", "corpus2 = df_train.item_description.values.astype('U').tolist() + df_test.item_description.values.astype('U').tolist()\n", "\n", "vectorizer1 = CountVectorizer(min_df=1,stop_words='english')\n", "vectorizer1.fit(corpus1)\n", "\n", "vectorizer2 = CountVectorizer(min_df=1,stop_words='english')\n", "vectorizer2.fit(corpus2)\n", "\n", "train_cor1 = vectorizer1.transform(df_train.name.values.astype('U').tolist())\n", "train_cor2 = vectorizer2.transform(df_train.item_description.values.astype('U').tolist())\n", "\n", "test_cor1 = vectorizer1.transform(df_test.name.values.astype('U').tolist())\n", "test_cor2 = vectorizer2.transform(df_test.item_description.values.astype('U').tolist())\n", "\n", "\n", "df_train['cor1'] = np.mean(train_cor1,1)\n", "df_train['cor2'] = np.mean(train_cor2,1)\n", "\n", "df_test['cor1'] = np.mean(test_cor1,1)\n", "df_test['cor2'] = np.mean(test_cor2,1)\n", "\n", "\n", "df_train['len1'] = df_train.name.str.len()\n", "df_train['len2'] = df_train.item_description.str.len()\n", "\n", "df_test['len1'] = df_test.name.str.len()\n", "df_test['len2'] = df_test.item_description.str.len()\n", "\n", "print(\"label encoding...\")\n", "le = preprocessing.LabelEncoder()\n", "le.fit(df_train.brand_name.values.tolist() + df_test.brand_name.values.tolist())\n", "df_train['brands']= le.transform(df_train.brand_name.values.tolist())\n", "df_test['brands']= le.transform(df_test.brand_name.values.tolist())\n", "\n", "df_train = df_train.fillna(999)\n", "df_test = df_test.fillna(999)\n", "\n", "le = preprocessing.LabelEncoder()\n", "le.fit(df_train.cat1.values.tolist() + df_test.cat1.values.tolist())\n", "df_train['cat1']= le.transform(df_train.cat1.values.tolist())\n", "\n", "le = preprocessing.LabelEncoder()\n", "le.fit(df_train.cat2.values.tolist() + df_test.cat2.values.tolist())\n", "df_train['cat2']= le.transform(df_train.cat2.values.tolist())\n", "\n", "le = preprocessing.LabelEncoder()\n", "le.fit(df_train.cat3.values.tolist() + df_test.cat3.values.tolist())\n", "df_train['cat3']= le.transform(df_train.cat3.values.tolist())\n", "\n", "df_train = df_train.fillna(999)\n", "df_test = df_test.fillna(999)\n", "\n", "\n", "answers_1_SINGLE = np.abs(df_train[TARGET_VAR])\n", "drop_features = ['train_id', 'name', 'category_name', 'brand_name', 'price', 'item_description']\n", "df_train = df_train.drop(drop_features, axis=1)\n", "\n", "df_train = df_train.fillna(999)\n", "df_test = df_test.fillna(999)\n", "\n", "df_train.head()\n"], "metadata": {"_cell_guid": "2867469a-d13e-48a2-a1c9-75f84c9f952a", "_uuid": "d06e320e331373f111021ac9adffd8f81bccfedf", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["df_train.to_csv('train_clean.csv', header=False,  index = False)    \n", "df_train= pd.read_csv('train_clean.csv', header=None, dtype=np.float32)    \n", "df_train = pd.concat([df_train, answers_1_SINGLE], axis=1)\n", "feature_cols = list(df_train.columns[:-1])\n", "print (feature_cols)\n", "target_col = df_train.columns[-1]\n", "trainX, trainY = df_train[feature_cols], df_train[target_col]\n", "df_train.head()"], "metadata": {"_cell_guid": "46646008-4395-4656-a082-588225267a4d", "_uuid": "b207986d70a840bfa77481dce6decd423378b957", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Make sure the shape and data are OK\n", "# Make sure the shape and data are OK\n", "print(trainX.shape)\n", "print(trainY.shape)\n", "print(type(trainY))\n", "print(type(trainY))\n", "\n", "from sklearn.model_selection import train_test_split\n", "\n", "data_train, data_val, labels_train, labels_val = train_test_split(trainX, trainY, \n", "                                                                    test_size=0.20, random_state=999)\n", "data_train=data_train.values\n", "labels_train=labels_train.values\n", "print(data_train.shape)\n", "print(labels_train.shape)\n", "print(type(data_train))\n", "print(type(labels_train))\n", "\n", "data_val=data_val.values\n", "labels_val=labels_val.values\n", "print(data_val.shape)\n", "print(labels_val.shape)\n", "print(type(data_val))\n", "print(type(labels_val))\n"], "metadata": {"_cell_guid": "55a04a87-6efd-4bb9-a298-b1db37378d2e", "_uuid": "ec1da30b148afbf4a2cc0bb705257d05b44d1a9f", "collapsed": true}}, {"metadata": {"_cell_guid": "bb89f815-ce44-479f-b3bc-53929723c62f", "_uuid": "39efba1314e5699e15c7a62da2480c1f14c39fa6"}, "source": ["## Set necessary paramaters/hyperparamaters"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["print('designing model')\n", "# Training Parameters\n", "learning_rate = 0.005\n", "# Network Parameters\n", "N_FEATURES=data_train.shape[1] # # Number of features for the input layer\n", "num_classes = 1 # Linear\n", "dropout = 0.5 # Dropout, probability to keep units\n", "print ('Num of features:' + str (N_FEATURES))"], "metadata": {"_cell_guid": "9e11311b-1904-416b-b481-7ee253fce27a", "_uuid": "9dc858a7e9deac088bb1bc0688f48a28acc8187c", "collapsed": true}}, {"metadata": {"_cell_guid": "af60a383-192d-4e54-a849-a98c77a615a8", "_uuid": "0f4f7f4ba9d7cf100c61657aedd14bada23d047c"}, "source": ["# PyTorch tensors"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Convert the np arrays into the correct dimention and type\n", "# Note that BCEloss requires Float in X as well as in y\n", "def XnumpyToTensor(x_data_np):\n", "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n", "    print(x_data_np.shape)\n", "    print(type(x_data_np))\n", "\n", "    if use_cuda:\n", "        lgr.info (\"Using the GPU\")    \n", "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n", "    else:\n", "        lgr.info (\"Using the CPU\")\n", "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n", "    \n", "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'\n", "    print(x_data_np.shape)\n", "    print(type(x_data_np))    \n", "    return X_tensor\n", "\n", "\n", "# Convert the np arrays into the correct dimention and type\n", "# Note that BCEloss requires Float in X as well as in y\n", "def YnumpyToTensor(y_data_np):    \n", "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n", "    print(y_data_np.shape)\n", "    print(type(y_data_np))\n", "\n", "    if use_cuda:\n", "        lgr.info (\"Using the GPU\")            \n", "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n", "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n", "    else:\n", "        lgr.info (\"Using the CPU\")        \n", "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n", "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n", "\n", "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n", "    print(y_data_np.shape)\n", "    print(type(y_data_np))    \n", "    return Y_tensor"], "metadata": {"_cell_guid": "0cfbb24b-947b-4e00-a1c8-0f3f0e5797cc", "_uuid": "9ccbd470ff2fe126a4a5de420faa69eab449fbfb", "collapsed": true}}, {"metadata": {"_cell_guid": "e99ca51d-52e0-451c-b041-d91a49728948", "_uuid": "1334e80f2f35179b386d0e9576324bc1b173263c"}, "source": ["## Design the  neural network  (very naive so that it can run here)\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["DEBUG_ON=True\n", "def debug(msg, x):\n", "    if DEBUG_ON:\n", "        print (msg + ', (size():' + str (x.size()))\n", "\n", "dropout = torch.nn.Dropout(0.3)\n", "relu=torch.nn.LeakyReLU()\n", "N_HIDDEN=16\n", "\n", "net_overfitting = torch.nn.Sequential(\n", "    torch.nn.Linear(N_FEATURES, N_HIDDEN),\n", "    torch.nn.ReLU(),\n", "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n", "    torch.nn.ReLU(),\n", "    torch.nn.Linear(N_HIDDEN, 1),\n", ")\n", "\n", "net_dropped = torch.nn.Sequential(\n", "    torch.nn.Linear(N_FEATURES, N_HIDDEN),\n", "    nn.BatchNorm1d(N_HIDDEN),\n", "    torch.nn.Dropout(0.3),\n", "    torch.nn.ReLU(),\n", "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n", "    torch.nn.Dropout(0.3),\n", "    torch.nn.ReLU(),\n", "    torch.nn.Linear(N_HIDDEN, 1),\n", ")\n", "\n", "class LinReg(nn.Module):    \n", "    def __init__(self, n_input, n_hidden, n_output):\n", "        super(LinReg, self).__init__()    \n", "        self.n_input=n_input\n", "        self.n_hidden=n_hidden\n", "        self.n_output= n_output \n", "                            \n", "        linear1=torch.nn.Linear(n_input,n_hidden)\n", "        torch.nn.init.xavier_uniform(linear1.weight)        \n", "        \n", "        linear2=torch.nn.Linear(n_hidden,1)\n", "        torch.nn.init.xavier_uniform(linear2.weight)        \n", "                \n", "        self.classifier = torch.nn.Sequential(\n", "#                                             linear1, nn.BatchNorm1d(n_hidden),dropout, relu,\n", "                                            linear1,dropout, relu,\n", "                                            linear2,              \n", "                                  )                                                                 \n", "    def forward(self, x):        \n", "#         debug('x',x)           \n", "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size                          \n", "        x=x.contiguous() \n", "        x=self.classifier(x)                   \n", "        return x\n", "    \n", "model=LinReg(N_FEATURES,N_HIDDEN,1)\n", "\n", "print (model)\n", "print(net_overfitting)\n", "print(net_dropped)"], "metadata": {"_cell_guid": "8fd6bd75-c563-4059-8227-88c55e95236f", "_uuid": "64a429641983054798ad95f6d51aeee390e677d7", "collapsed": true}}, {"metadata": {"_cell_guid": "c2767a6d-9134-48f0-8a0a-425dc5ad31ad", "_uuid": "0a20d8c1b2feae0212c19e40e9276dd4a78fb6d6"}, "source": ["## Define the loss function\n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["criterion = torch.nn.MSELoss(size_average=True)\n", "print (criterion)"], "metadata": {"_cell_guid": "86526176-f4b9-4a71-86f4-5906178e0600", "_uuid": "62426df78525ac8f215b643167fbcc967369e77e", "collapsed": true}}, {"metadata": {"_cell_guid": "19adb2de-8691-4478-8c81-277d45a5a77c", "_uuid": "4b98f46e4979267e0be9d3eb39640a4d302390ed"}, "source": ["## Define the Optimizer\n", "\n", "Gradient descent is defined as the training method used to minimize the loss function"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n", "print (optimizer)"], "metadata": {"_cell_guid": "39549353-1452-44ba-8aee-e1e7f5edf95f", "_uuid": "945e59241670fbdf8a3415da5505225030ef5afd", "collapsed": true}}, {"metadata": {"_cell_guid": "e66ea734-7dc1-4925-ae12-b8de831d986f", "_uuid": "9528edb12282ac4ab480595e1fd7920fb0ea4111"}, "source": ["# Prepare the Tensors"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["LR = 0.005\n", "BATCH_SIZE=32\n", "EPOCH = 10 \n", "\n", "import gc\n", "df_train=None\n", "\n", "gc.collect()\n", "\n", "\n", "data_train = np.array(data_train, dtype=np.float32)\n", "\n", "labels_train=labels_train.reshape((labels_train.shape[0],1)) # Must be reshaped for PyTorch!\n", "labels_train = np.array(labels_train, dtype=np.float)\n", "\n", "X_tensor = (torch.from_numpy(data_train)).type(torch.FloatTensor) # Note the conversion for pytorch\n", "Y_tensor = (torch.from_numpy(labels_train)).type(torch.FloatTensor) # Note the conversion for pytorch    \n", "\n", "\n", "import torch.utils.data as Data\n", "dataset = Data.TensorDataset(data_tensor = X_tensor, target_tensor = Y_tensor)\n", "loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\n", "\n", "print (loader)"], "metadata": {"_cell_guid": "7289bd5f-03b0-4eb0-9879-cefc8f5315a8", "_uuid": "0da8af2f0ad74ff286f44ec18b3b51b788af6247", "collapsed": true}}, {"metadata": {"_cell_guid": "1ecdd6d6-7a02-4bd6-948d-deba69c9591d", "_uuid": "a8589435c1cf182d04f8fa2af4d085c93adec52e"}, "source": ["## Train the model (you need a GPU for this, and increase EPOCHs)"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n", "LR = 0.005\n", "BATCH_SIZE=32\n", "EPOCH = 10 \n", "\n", "net_SGD = LinReg(N_FEATURES,N_HIDDEN,1)\n", "net_Momentum = LinReg(N_FEATURES,N_HIDDEN,1)\n", "net_RMSprop = LinReg(N_FEATURES,N_HIDDEN,1)\n", "net_Adam = LinReg(N_FEATURES,N_HIDDEN,1)\n", "\n", "\n", "opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)\n", "opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = 0.9)\n", "opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr = LR, alpha = 0.9)\n", "opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas= (0.9, 0.99))\n", "\n", "loss_func = torch.nn.MSELoss()\n", "\n", "loss_SGD = []\n", "loss_Momentum = []\n", "loss_RMSprop =[]\n", "loss_Adam = []\n", "\n", "# losses = [loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam]\n", "losses = [loss_SGD, loss_Adam]\n", "# nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n", "nets = [net_SGD, net_Adam]\n", "# optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n", "optimizers = [opt_SGD, opt_Adam]\n", "\n", "print (nets[0])\n", "\n", "for epoch in range(0, EPOCH + 1):\n", "    print('Training Epoch= {}/{} '.format(epoch,EPOCH))                    \n", "    for step, (batch_x, batch_y) in enumerate(loader):\n", "        var_x = Variable(batch_x)\n", "        var_y = Variable(batch_y)\n", "        for net, optimizer, loss_history in zip(nets, optimizers, losses): \n", "#             print ('Model:' + type(net).__name__) \n", "#             print ('Opt:' + type(optimizer).__name__)\n", "            prediction = net(var_x)            \n", "            loss = loss_func(prediction, var_y)            \n", "            optimizer.zero_grad()            \n", "            loss.backward()            \n", "            optimizer.step()            \n", "            loss_history.append(loss.data[0])\n", "            \n", "#     if epoch % 5  == 0:        \n", "    loss_run = loss.data[0]                \n", "    print(step, loss_run)               \n", "    print('Training MSELoss=%.4f' % loss_run)                    "], "metadata": {"_cell_guid": "6438b4c7-e84c-4f60-906f-a29bb0948975", "_uuid": "8ca1fff8cc5aa4b7b56b308db119b1304627a885", "collapsed": true}}, {"metadata": {"_cell_guid": "2365a984-6550-45d6-8961-996281be384e", "_uuid": "af1fd98d20452a58ca1495fe3e91641b1bda41b6"}, "source": ["# Visualize Loss Graph using Visdom\u00b6\n", "### Make sure you have Visdom installed and running\n", "- pip install visdom\n", "- python -m visdom.server &"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "labels = ['SGD', 'Adam']\n", "          \n", "for i, loss_history in enumerate(losses):\n", "    plt.plot(loss_history, label = labels[i])\n", "          \n", "plt.legend(loc = 'best')\n", "plt.xlabel('Steps')\n", "plt.ylabel('Loss')\n", "plt.ylim((0, 0.2))\n", "plt.show()"], "metadata": {"_cell_guid": "a9799b4d-014a-4429-8a69-a907c0290e03", "_uuid": "742cab678c9d01dcdededb8e74d04db5732375ec", "collapsed": true}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "plt.plot(loss_history)\n", "plt.show()\n", "\n", "# # # ! pip install visdom\n", "\n", "# # from visdom import Visdom\n", "# # viz = Visdom()\n", "\n", "# # num_epoch=int(epochs/div_factor)\n", "\n", "# # x = np.reshape([i for i in range(num_epoch)],newshape=[num_epoch,1])\n", "# # loss_data = np.reshape(loss_arr,newshape=[num_epoch,1])\n", "\n", "# # win3=viz.line(\n", "# #     X = x,\n", "# #     Y = loss_data,\n", "# #     opts=dict(\n", "# #         xtickmin=0,\n", "# #         xtickmax=num_epoch,\n", "# #         xtickstep=1,\n", "# #         ytickmin=0,\n", "# #         ytickmax=20,\n", "# #         ytickstep=1,\n", "# #         markercolor=np.random.randint(0, 255, num_epoch),\n", "# #     ),\n", "# # )"], "metadata": {"_cell_guid": "e801d2b7-668d-4a7c-ae5f-47f6641f4709", "_uuid": "3256a95e9378c2b02c0d0ff410326c1d74bb846e", "collapsed": true}}, {"metadata": {"_cell_guid": "685ee3e5-5f3c-4d40-919a-cbed96b99987", "_uuid": "fbf7c99c8441d49a6bf2f1475b9926a1ccf04728"}, "source": ["## Prepare the test data\n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# df_test.to_csv('test_clean.csv', header=False,  index = False)    \n", "# df_test= pd.read_csv('test_clean.csv', header=None, dtype=np.float32)    \n", "# feature_cols = list(df_train.columns[:-1])\n", "# print (feature_cols)\n", "# trainX = df_test[feature_cols]]"], "metadata": {"_cell_guid": "76093cf8-ef5a-4065-b6b9-9441192838cb", "_uuid": "0b2c63dd7b23e0964651ec584af5f9acc0d6c926", "collapsed": true}}, {"metadata": {"_cell_guid": "1c0c8b85-4345-4e93-8bc8-452e7f2c91c3", "_uuid": "48d9669fd271c245b96945f4b5eaae608978e6b1"}, "source": ["## Make predictions\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n", "print('making predictions\\n')\n"], "metadata": {"_cell_guid": "77a84110-23dd-4ec5-afdf-3838099a608f", "_uuid": "671e1d099596d27cb8e4f73dde05a2da551b1425", "collapsed": true}}, {"metadata": {"_cell_guid": "ceec4e8b-b660-48cc-ada7-6738f73a634b", "_uuid": "8cee5a4aa9ad034a1e4b96cf461ced1572491b06"}, "source": ["## Write output to file\n", "\n", "Lastly we take the predictions and construct a dataframe which we output to a .csv and can then submit for evalutation!"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["import pandas as pd\n", "x=pd.read_csv ('../input/0609034-0608800-submission/0.609034_0.608800_submission.csv')\n", "x.to_csv('sample_submission.csv', index=False)"], "metadata": {"_cell_guid": "0e0ceb34-138b-4e7b-a5a1-5b82d398f328", "_uuid": "d2808efad7d4b3eef78c657e7d06ed6ae003eb76", "collapsed": true}}, {"metadata": {"_cell_guid": "9d8d2289-7cc7-41c7-9273-37d2900f8d62", "_uuid": "55de4dbef7c3f1ed61bcd9f9a15601248c7888d2"}, "source": [], "cell_type": "markdown"}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "pygments_lexer": "ipython3", "name": "python"}}}