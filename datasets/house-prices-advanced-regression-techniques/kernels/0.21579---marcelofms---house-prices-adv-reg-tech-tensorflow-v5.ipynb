{"cells":[{"metadata":{"_uuid":"dd70840f062210a99eb3722aca470b282c2fb9c2"},"cell_type":"markdown","source":"For knowledge pursuit, let's submite the datasets to Tensorflow to compare results based in the previous version using XGBoost and LassoCV."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LassoCV, LassoLarsCV\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Capture data of interest"},{"metadata":{"trusted":true,"_uuid":"2f2fd1aa1c3ea22dbb84e2d114d383550ea00cc4"},"cell_type":"code","source":"train_file_path = '../input/train.csv' # this is the path to the Iowa data that you will use\ntest_file_path = '../input/test.csv' # this is the path to the Iowa data that you will use\n\ntrain_data = pd.read_csv(train_file_path)\ntest_data = pd.read_csv(test_file_path)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1a5dbf3beebc5289e5ea1d215162b4870095736"},"cell_type":"markdown","source":"Verify training data"},{"metadata":{"trusted":true,"_uuid":"73c5a3b2593dcc9c3911f96dbae9fcb9724a3c6e"},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8272d66c21351756af38115e5fc3664931262904"},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aec40e193828887d18cca0bf65e5113de7df653f"},"cell_type":"markdown","source":"Verify test data"},{"metadata":{"trusted":true,"_uuid":"37d0bb2aca1b5bb8cfd676ef0de0e19d36bbee33"},"cell_type":"code","source":"test_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf6ca37043830ef68fc9c6629038afa7cf53a9cb"},"cell_type":"markdown","source":"For benchmark purpose, let's apply a strict regression using the most proeminent numeric attributes. Delimiting information used in the predictions as data of interest."},{"metadata":{"trusted":true,"_uuid":"2cab0d32d208d2021e1d7ceadac44d0a660dfa52"},"cell_type":"code","source":"data_interest = np.array(train_data.columns[train_data.dtypes != 'object'])\n  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2df21cc9a6a562db55a8a376282b4b7864bd3b73"},"cell_type":"markdown","source":"Split the data to train and test"},{"metadata":{"trusted":true,"_uuid":"677c4d192319d2d177f19416396eb00760ebbf6d"},"cell_type":"code","source":"# define os conjuntos de teste e treinamento\ntrain_x, test_x, train_y, test_y = train_test_split(train_data[data_interest], train_data['SalePrice'], test_size=0.25)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b574ab05e0130076fb74cd2fc2afc6c6503e0f11"},"cell_type":"markdown","source":"Testing the preliminar model for future reference"},{"metadata":{"trusted":true,"_uuid":"0bac3e1a1fdc499d565d18d2a8b9f51ac4547188"},"cell_type":"code","source":"#Versão usando Regressor+pipeline\nreg_model = make_pipeline(Imputer(),XGBRegressor())\nreg_model.fit(train_x, train_y)\n\npredicted_home_prices = reg_model.predict(test_x)\n\nprint(\"Regressor - Mean Absolute Error : \" + str(mean_absolute_error(predicted_home_prices, test_y)))\n\nscores = np.sqrt(-cross_val_score(reg_model, train_data[data_interest], train_data['SalePrice'], scoring=\"neg_mean_squared_error\", cv=5))\nprint(scores.mean())\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"656fd04a6fa2e0bacfb369dd11ad3c81f6dc8915"},"cell_type":"markdown","source":"First assessment of correlation"},{"metadata":{"trusted":true,"_uuid":"90d8b3b38054a53376bf25c43342c1499fd443a7"},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n\ncorr_coef = train_data.corr()\ncorr_coef = corr_coef.fillna(0)\ntop_coef = pd.concat([corr_coef['SalePrice'].sort_values().head(20),corr_coef['SalePrice'].sort_values().tail(20)])\nax_coefs = top_coef.plot.bar()\nprint('Mean of correlation for full set of attributes:' + str(corr_coef['SalePrice'].mean()))\nax_coefs.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a051d5a8bc9f6d026d6e52ed437b7b5e08d891"},"cell_type":"markdown","source":"Preparing the data's new model and Pre-processing the data to solve problems and high level skewness"},{"metadata":{"trusted":true,"_uuid":"bca0dfd44d4c7e6d4980addc3636dbcdcccc3e00"},"cell_type":"code","source":"train_data = train_data.fillna(train_data.mean())\ntest_data = test_data.fillna(test_data.mean())\ndata = pd.concat([train_data, test_data], sort=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e513375c280e4abc37ab74d108372060eeb40d8"},"cell_type":"markdown","source":"Removing the attributes with high level of biases or outliers (check the [EDA Kernel](https://www.kaggle.com/marcelofms/house-prices-competition-eda-kernel) to details)"},{"metadata":{"trusted":true,"_uuid":"74163bb4f8203b8414051845097c63f28178789d"},"cell_type":"code","source":"\nremove_list = ['Alley', 'LandContour', 'Utilities', 'LandSlope', 'Condition2', 'RoofStyle', 'HouseStyle', 'Street',\n               'RoofMatl', 'Foundation', 'BsmtCond', 'Heating', 'Electrical', 'FireplaceQu', 'GarageYrBlt', \n               'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'BsmtFinSF2', 'LotShape',\n               'BsmtFinType1', 'BsmtFinType2', 'BldgType', 'Exterior2nd', 'MiscVal', 'MSSubClass', 'Functional',\n               'Exterior1st', 'MSZoning', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\ndata = data.drop(remove_list, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b3ec974e207d3623f316b5ba592f53989a3b43b"},"cell_type":"markdown","source":"Normalization of data to make compatible with Tensorflow\n\n*As neither of two methods bellow weren't capable of improve the prediction (no real gains), both were disabled.*"},{"metadata":{"trusted":true,"_uuid":"a00663dd223b0e03f0de780e79a5beb7285fe790","scrolled":true},"cell_type":"code","source":"# Normalize the data to better fit to the tensorflow\n#interest_data = data[data.columns.difference(['SalePrice']) & data.columns.difference(['Id'])]\n#cols_interest = np.array(interest_data.columns[interest_data.dtypes != 'object'])\n\n#mean = data[cols_interest].mean(axis=0)\n#std = data[cols_interest].std(axis=0)\n#data[cols_interest] = (data[cols_interest] - mean) / std\n\n#log transform skewed numeric features:\n#numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n#skewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n#skewed_feats = skewed_feats[skewed_feats > 0.75]\n#skewed_feats.drop(columns=['SalePrice'], inplace=True, errors='ignore')\n#skewed_feats = skewed_feats.index\n\n#data[skewed_feats] = np.log1p(data[skewed_feats])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6253b1c9d1409617110baf2f5b0880b81c6e678"},"cell_type":"markdown","source":"Encoding the data"},{"metadata":{"trusted":true,"_uuid":"d1c2f8d3dfa7c17665e853a1000fdd1a647d1608"},"cell_type":"code","source":"#one-hot encoding\ndata_cat = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3df0e5aa3bc24165fa630788370f3bf19015abeb"},"cell_type":"markdown","source":"Split the data in test and train again."},{"metadata":{"trusted":true,"_uuid":"75cbdb3a8861fedfa4ad320c1df8f8ee8ee827f6","_kg_hide-output":true},"cell_type":"code","source":"#split data \ntrain_data_cat = data_cat[data_cat['SalePrice'].notnull()]\ntest_data_cat = data_cat[data_cat['SalePrice'].isnull()]\n\ntest_data_cat = test_data_cat.drop(columns=['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15da8c5bc5935b95228f74fb26e919da86310fd6"},"cell_type":"markdown","source":"Check new levels of correlation"},{"metadata":{"trusted":true,"_uuid":"d510632a95c87e7b18f716396a403893ff9f5e45"},"cell_type":"code","source":"df_data_cat = train_data_cat\ndf_data_cat = df_data_cat.assign(SalePrice = train_data['SalePrice'])\ncorr_coef = df_data_cat.corr()\ncorr_coef = corr_coef.fillna(0)\ntop_coef = pd.concat([corr_coef['SalePrice'].sort_values().head(20),corr_coef['SalePrice'].sort_values().tail(20)])\nval_mean_coef = pd.DataFrame(corr_coef).iloc[1].mean(axis=0)\nax_coefs = top_coef.plot.bar()\nax_coefs.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff25aeee60c977f9bf62500880109975de19acc3"},"cell_type":"markdown","source":"Prepare the sets of data training and test to fit the model"},{"metadata":{"trusted":true,"_uuid":"36ff3ad9cb309a9d69e01986e4a039654002edc6"},"cell_type":"code","source":"train_x = train_data_cat[train_data_cat.columns.difference(['SalePrice'])]\ntest_x = test_data_cat\n#train_y = train_data_cat['SalePrice']\ntrain_y = train_data['SalePrice']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f4ca0c09188067bbd8ddca807ac5adf2458b301"},"cell_type":"markdown","source":"Apply cross validation to check the model"},{"metadata":{"trusted":true,"_uuid":"9f8923e62790d2005b98f163bd6a02ac7e534be2"},"cell_type":"code","source":"data_model = XGBRegressor(max_depth=3, n_estimators=500, learning_rate=0.1).fit(train_x, train_y)\nscores = np.sqrt(-cross_val_score(data_model, train_x, train_y, scoring=\"neg_mean_squared_error\", cv=5))\nprint('Scores for XGBRegressor')\nprint(scores.mean())\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20d13945b8e4aeec065eaf4e1a76e944e06f87f0"},"cell_type":"markdown","source":"Preparing the model evaluation"},{"metadata":{"trusted":true,"_uuid":"382412a4d3028089c702503263499cd52ddaa18a"},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a31f90b92d936be91d4039fe392708b9d18da803"},"cell_type":"markdown","source":"First try using pre loaded Estimators of Tensorflow (TO-DO)"},{"metadata":{"trusted":true,"_uuid":"43946efa76157c8db9e208a98e4782811881eb12"},"cell_type":"code","source":"# convert the dataframes to tensor datasets\ndef df_to_tensords(df_features, df_labels):\n    features = {}\n    features_columns = []\n\n    for column in df_features:\n        features[column] = df_features[column]\n        features_columns.append(tf.feature_column.numeric_column(key=column))\n    \n    ds = tf.data.Dataset.from_tensor_slices((dict(features), df_labels))    \n    return ds, features_columns       \n      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd5e0d35903a0875c89a04c7d04e5d1282b5c0f2"},"cell_type":"code","source":"#Shuffle the train and test data\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.25)\n\ninput_ds, input_features =  df_to_tensords(X_train, y_train)\neval_ds, eval_features = df_to_tensords(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f4d63615acab23dd17e058e53f912ccd884611"},"cell_type":"code","source":"#create the inputs methods for the model\ndef input_train():\n    return(input_ds.shuffle(1000).batch(128).repeat().make_one_shot_iterator().get_next())\n\ndef input_eval():\n    return (eval_ds.shuffle(1000).batch(128).make_one_shot_iterator().get_next())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26bba564a8ca7b16c50bcca6a5678cddbe453ae7"},"cell_type":"code","source":"#build and check the model - LinearRegressor\n\n#model = tf.estimator.LinearRegressor(feature_columns=input_features,\n#    optimizer=lambda: tf.train.AdamOptimizer(\n#        learning_rate=tf.train.exponential_decay(\n#            learning_rate=0.1,\n#            global_step=tf.train.get_global_step(),\n#            decay_steps=10000,\n#           decay_rate=0.96)))\n\n#model.train(input_fn=input_train, steps=500)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae503c57263462d5acef2b622e7eeb81180b97da"},"cell_type":"code","source":"#DNNLinearCombinedRegressor\n#model = tf.estimator.DNNLinearCombinedRegressor(linear_feature_columns=input_features,\n#                                                    linear_optimizer=lambda: tf.train.AdamOptimizer(\n#                                                        learning_rate=tf.train.exponential_decay(\n#                                                        learning_rate=0.1,\n#                                                        global_step=tf.train.get_global_step(),\n#                                                        decay_steps=10000,\n#                                                        decay_rate=0.96)),\n#                                                   loss_reduction=tf.losses.Reduction.MEAN)\n\n#model.train(input_fn=input_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77ba3f9fba00d32edefb8aac39e51da12d6e0cd7"},"cell_type":"markdown","source":"That's Estimator was discard due the lack of proper documentation about the mandatory use of bucketized features. The output created as data for the features doesn't allow better predictions or analysis."},{"metadata":{"trusted":true,"_uuid":"fd062befdb4f32d1f297552dcc50e7eaa948034c"},"cell_type":"code","source":"#BoostedTreesREgressor\n#model = tf.estimator.BoostedTreesRegressor(feature_columns=input_features, \n#                                          n_batches_per_layer=100)\n\n#model.train(input_fn=input_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f5023b4c0bb1c187bc02b2d1a2a8067cef230e4"},"cell_type":"markdown","source":"Assessment of the MSE for the model."},{"metadata":{"trusted":true,"_uuid":"f927f72e933a85bcfc0408473e2f245e5adbd76f"},"cell_type":"code","source":"eval_result = model.evaluate(input_fn=input_eval)\n\naverage_loss = eval_result[\"average_loss\"]\n\nprint(\"Loss for the test set: {:7.2f}\".format(average_loss))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"316b2e9d8d9706dfcf3bb950043ced3e0c274fcb"},"cell_type":"markdown","source":"Using Neural Network with Keras"},{"metadata":{"trusted":true,"_uuid":"93941af650b43e76dc0466157bbb4f844a04efa9"},"cell_type":"code","source":"def build_model():\n  model = keras.Sequential([\n    keras.layers.Dense(64, activation=tf.nn.relu,\n                       input_shape=(train_x.shape[1],)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(1)\n  ])\n\n  #just some experiments\n  #optimizer = tf.train.RMSPropOptimizer(0.001)\n  #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001, name='GradientDescent')\n  optimizer = tf.train.AdamOptimizer(0.01)  \n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae'])\n  return model\n\nmodel = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c5aacaf51f0c221f91837bd888c5591ad0c07f5"},"cell_type":"code","source":"# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 500\n\n# Store training stats\nhistory = model.fit(train_x, train_y, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[PrintDot()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"867168d7b71deb0a1d56c96b661e2ffc416c1618"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_history(history):\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error')\n  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n           label='Train Loss')\n  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n           label = 'Val loss')\n  plt.legend()\n  #plt.ylim([0, 5])\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfb06981a5e39ffbb5a2f3286198fbc1784b568a"},"cell_type":"code","source":"[loss, mae] = model.evaluate(train_x, train_y, verbose=0)\n\nprint(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9fdcc3eb09a30981cbeec61a4d6acac2d96ca9"},"cell_type":"code","source":"model = build_model()\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n\nhistory = model.fit(train_x, train_y, epochs=EPOCHS,\n                    validation_split=0.2, verbose=0,\n                    callbacks=[early_stop, PrintDot()])\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b61e59d506e4cfa3e9f3f6a34762085bc1c65a4"},"cell_type":"markdown","source":"Creating submission from the final configuration of the selected model"},{"metadata":{"trusted":true,"_uuid":"949097bd41daa8cf0b4d106a5759abf3b955b379","scrolled":true},"cell_type":"code","source":"#alig the columns e remove the discard attributes from test data set\n\n#test_data_filtered = test_data_cat.drop(np.array(ls_coefs[ls_coefs==0].index), axis=1, errors='ignore')\ntest_x = test_x[train_x[train_x.columns.difference(['SalePrice'])].columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3389555b026625464026a6ad78834066397cf261","scrolled":true},"cell_type":"code","source":"preds_val = model.predict(test_x)\n\nresult = test_data.assign(SalePrice=preds_val)\n\nsubmission_file_path = 'submission.csv'\nresult.to_csv(submission_file_path,sep=',',columns=['Id', 'SalePrice'], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f535877f3af5a63c6e61b17a677f574d30ea2d7","trusted":true},"cell_type":"code","source":"result.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}