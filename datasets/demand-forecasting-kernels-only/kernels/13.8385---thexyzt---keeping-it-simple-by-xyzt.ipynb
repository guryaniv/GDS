{"cells":[{"metadata":{"_uuid":"5ee23497e387c08e1aa0b7c1a9af658812677a87"},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I go through how I worked to find a decent solution for this challenge using simple uncomplicated techniques. No machine learning, no fancy black-box models. Throw away your ARIMAs and Gradient Boosts. Think simple."},{"metadata":{"_uuid":"c265d5f4c10385ed1d763e52e182dbb17ea5122b"},"cell_type":"markdown","source":"# Setup and Loading Data"},{"metadata":{"trusted":true,"_uuid":"053e0f77865e9e7351c3a1cc26341f591b1282ce"},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 12)\n\nPATH = \"../input\"\ntrain = pd.read_csv(f\"{PATH}/train.csv\", low_memory=False, \n                    parse_dates=['date'], index_col=['date'])\ntest = pd.read_csv(f\"{PATH}/test.csv\", low_memory=False, \n                   parse_dates=['date'], index_col=['date'])\nsample_sub = pd.read_csv(f\"{PATH}/sample_submission.csv\")\n\n# Make the sample submission (Score: 48.75440)\n# sample_sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e2783477b83f3481d1acb2581b1518f7da9d498"},"cell_type":"markdown","source":"# A \"Dumb\" Prediction\n\nFind the average of sales of an item at a store on the day and month of sales and use that as the prediction. This effectively gives us a sample size of 5 (since the training set is five years long) to find the mean. This is clearly a sub-optimal solution because almost no thought goes into it. But it is helpful to code such solutions to get acquianted with the \"Getting Data -> Submitting Prediction\" pipeline from start to finish, and generally getting a feel for the data. It also provides a helpful benchmark for future solutions. \n\n*Any method that scores worse than this prediction is probably doing something incredibly wrong.*\n\n**Note: The following code block takes > 1 hour to run. It is extremely inefficient.**"},{"metadata":{"trusted":true,"_uuid":"cb886796e7f50c27a867eb355acb3c2a90aeae1d"},"cell_type":"code","source":"def dumb_prediction(train, test, submission):\n    for _, row in test.iterrows():\n        item, store = row['item'], row['store']\n        day, month = row.name.day, row.name.month\n        itemandstore = (train.item == item) & (train.store == store)\n        dayandmonth = (train.index.month == month) & (train.index.day == day)\n        train_rows = train.loc[itemandstore & dayandmonth]\n        pred_sales = int(round(train_rows.mean()['sales']))\n        submission.at[row['id'], 'sales'] = pred_sales\n    return submission\n\n# dumb_pred = dumb_prediction(train, test, sample_sub.copy())\n# dumb_pred.to_csv(\"dumb_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f24118bf5884a77047eeab25bf7949ba6556ee5d"},"cell_type":"markdown","source":"### This solution gets a score of 22.13108.\nNothing impressive, but not completely terrible either. Pretty much the kind of error you can expect for such a silly model."},{"metadata":{"trusted":true,"_uuid":"b0d717d212b3ebb7884220b42acc8ee07e231767"},"cell_type":"markdown","source":"# Slightly Better Prediction\n\nThe previous method simply took the historical average of an item (on the same date and at the same store) and used it to predict the sales on the test set. We can improve this by understanding the data better. Is the a difference between sales on different days? That is, Mondays vs. Fridays, Weekends vs Weekdays? Are there special days without sales? Is there a difference between these stores? Is there a difference between the items?\n\nTo understand these trend, we need to dive into the data!"},{"metadata":{"_uuid":"f151cc3a42c5571ac8699bfab588a1a89f4c0d88"},"cell_type":"markdown","source":"## Exploring the data"},{"metadata":{"trusted":true,"_uuid":"0147272958e037feb15d92b6c9bf9e847d1eb33f"},"cell_type":"code","source":"# Expand dataframe with more useful columns\ndef expand_df(df):\n    data = df.copy()\n    data['day'] = data.index.day\n    data['month'] = data.index.month\n    data['year'] = data.index.year\n    data['dayofweek'] = data.index.dayofweek\n    return data\n\ndata = expand_df(train)\ndisplay(data)\n\ngrand_avg = data.sales.mean()\nprint(f\"The grand average of sales in this dataset is {grand_avg:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6861a4b18986047974a0e7d45ca9eda4be32021d"},"cell_type":"markdown","source":"### Changes by year\n\nAll items and stores seem to enjoy a similar growth in sales over the years."},{"metadata":{"trusted":true,"_uuid":"6a35eea206dbf655750052469385b67bd1bc5e49"},"cell_type":"code","source":"agg_year_item = pd.pivot_table(data, index='year', columns='item',\n                               values='sales', aggfunc=np.mean).values\nagg_year_store = pd.pivot_table(data, index='year', columns='store',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_year_item / agg_year_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_year_store / agg_year_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f19801b0850bf2fc5f87a8e225e0b0e0f79ebaf"},"cell_type":"markdown","source":"### Changes by month\n\nAll items and stores seem to share a common pattern in sales over the months as well.\n"},{"metadata":{"trusted":true,"_uuid":"3bf3a39598d1c737baf7cc09f23c43a9e663723b"},"cell_type":"code","source":"agg_month_item = pd.pivot_table(data, index='month', columns='item',\n                                values='sales', aggfunc=np.mean).values\nagg_month_store = pd.pivot_table(data, index='month', columns='store',\n                                 values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_month_item / agg_month_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_month_store / agg_month_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0205e201e627d34bf7ec866b5231f98039fb7079"},"cell_type":"markdown","source":"### Changes by day of the week\n\nAll items and stores also seem to share a common pattern in sales over the days of the week as well."},{"metadata":{"trusted":true,"_uuid":"e3eb21871f6a945a2d61849d5ee7a1030357c9fc"},"cell_type":"code","source":"agg_dow_item = pd.pivot_table(data, index='dayofweek', columns='item',\n                              values='sales', aggfunc=np.mean).values\nagg_dow_store = pd.pivot_table(data, index='dayofweek', columns='store',\n                               values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_dow_item / agg_dow_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_dow_store / agg_dow_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6b99ee6131d0144d99fb5aec8ea2028dc2366f3"},"cell_type":"markdown","source":"### Are these patterns degenerate?\n\nThis is an important question. Not checking for degeneracies in the data can lead to missing important trends in complex datasets. For example, when looking at the monthly patterns, we average over all days of the month, years and either items or stores. But what if sales have a multi-dimensional dependence on two of these parameters that isn't easily separable? So, always check for degeneracies in the data!"},{"metadata":{"trusted":true,"_uuid":"636cb3fe4ff48f3e77b6e6b8a2ee057446c71199"},"cell_type":"code","source":"agg_dow_month = pd.pivot_table(data, index='dayofweek', columns='month',\n                               values='sales', aggfunc=np.mean).values\nagg_month_year = pd.pivot_table(data, index='month', columns='year',\n                                values='sales', aggfunc=np.mean).values\nagg_dow_year = pd.pivot_table(data, index='dayofweek', columns='year',\n                              values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(18, 5))\nplt.subplot(131)\nplt.plot(agg_dow_month / agg_dow_month.mean(0)[np.newaxis])\nplt.title(\"Months\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(132)\nplt.plot(agg_month_year / agg_month_year.mean(0)[np.newaxis])\nplt.title(\"Years\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(133)\nplt.plot(agg_dow_year / agg_dow_year.mean(0)[np.newaxis])\nplt.title(\"Years\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ee55973dd2caf82ec98d67b583383cce3f03fe1"},"cell_type":"markdown","source":"In this case, however, there don't seem to be any sneaky degeneracies. We can effectively treat the \"month\", \"year\", \"day of the week\", \"item\" and \"store\" as completely independent modifiers to sales prediction. This leads to a *very very simple* prediction model.\n\n\"Relative sales\" in the plots above are the sales relative to the average. Since there are very regular patterns in the \"month\", \"day of week\", and \"year\" trends. All we have to do is simply memorize these trends and apply them to our predictions by multiplying them to the expected average sales. We get the expected average sales for an item at a store from the historical numbers in the training set."},{"metadata":{"_uuid":"f549615ecac3f272f314f81ea1ed2521401090da"},"cell_type":"markdown","source":"### What about the item-store relationship?"},{"metadata":{"trusted":true,"_uuid":"b9d0a17a4d4cdcf88069e7f5236ba62521723e83"},"cell_type":"code","source":"agg_store_item = pd.pivot_table(data, index='store', columns='item',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(14, 5))\nplt.subplot(121)\nplt.plot(agg_store_item / agg_store_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Store\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_store_item.T / agg_store_item.T.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Item\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa2f84efb28d9c15a499dc39d5b63ab186793aa"},"cell_type":"markdown","source":"Same here. Just a constant pattern and no degeneracies. So, you just need a model for how items sell at different stores, which is easily captured by an average sales look-up table or yet another \"relative sales\" pattern model.\n\n> *Aside: Based on the extremely regularity of the data, how neat it is, and how few degeneracies there are - I am fairly confident this is probably simulated data.*"},{"metadata":{"_uuid":"19f415e7519cbfb6e12931a5b2915307b7c2e0e7"},"cell_type":"markdown","source":"## Writing the \"slightly better predictor\"\n\nWe just need an item-store average sale look-up table, and then the \"day of week\", \"monthly\", \"yearly\" models."},{"metadata":{"trusted":true,"_uuid":"14f37ffffa1bc221f7684d4efa5f742fd08edf59"},"cell_type":"code","source":"# Item-Store Look Up Table\nstore_item_table = pd.pivot_table(data, index='store', columns='item',\n                                  values='sales', aggfunc=np.mean)\ndisplay(store_item_table)\n\n# Monthly pattern\nmonth_table = pd.pivot_table(data, index='month', values='sales', aggfunc=np.mean)\nmonth_table.sales /= grand_avg\n\n# Day of week pattern\ndow_table = pd.pivot_table(data, index='dayofweek', values='sales', aggfunc=np.mean)\ndow_table.sales /= grand_avg\n\n# Yearly growth pattern\nyear_table = pd.pivot_table(data, index='year', values='sales', aggfunc=np.mean)\nyear_table /= grand_avg\n\nyears = np.arange(2013, 2019)\nannual_sales_avg = year_table.values.squeeze()\n\np1 = np.poly1d(np.polyfit(years[:-1], annual_sales_avg, 1))\np2 = np.poly1d(np.polyfit(years[:-1], annual_sales_avg, 2))\n\nplt.figure(figsize=(8,6))\nplt.plot(years[:-1], annual_sales_avg, 'ko')\nplt.plot(years, p1(years), 'C0-')\nplt.plot(years, p2(years), 'C1-')\nplt.xlim(2012.5, 2018.5)\nplt.title(\"Relative Sales by Year\")\nplt.ylabel(\"Relative Sales\")\nplt.xlabel(\"Year\")\nplt.show()\n\nprint(f\"2018 Relative Sales by Degree-1 (Linear) Fit = {p1(2018):.4f}\")\nprint(f\"2018 Relative Sales by Degree-2 (Quadratic) Fit = {p2(2018):.4f}\")\n\n# We pick the quadratic fit\nannual_growth = p2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ae3d1b14aa23ec71104ba68a546ac2c1b1e19b6"},"cell_type":"markdown","source":"We can do a simple linear regression on the yearly growth datapoints. But if you look carefully, you can tell that the growth is slowing down. The quadratic fit works better since it better captures the curvature in the growth curve. Since we only have 5 points, this is the highest degree polynomial fit you should do to avoid overfitting."},{"metadata":{"_uuid":"6436cc9550730bca752f625876cb5a6b6e2ae0ed"},"cell_type":"markdown","source":"Now, we write the predictor. It's quite simple! When we are asked to predict the sales of Item X at Store Y on, say, a Monday in February - all we have to do is to look up the historical average of the sales of Item X at Store Y and then multiply it by a factor corresponding to Monday and then a factor corresponding to February to account for the seasonal and weekly changes in item sales at the stores. Finally, we multiply by the annual growth factor for the year we are predicting for. And thus, we have a very simple forecast of the item's sales.\n\nThis predictor will run quite fast and should parse through the whole test dataset in less than 20 seconds. A significant improvement over the \"dumb\" prediction method both in accuracy and compute efficiency."},{"metadata":{"trusted":true,"_uuid":"2ff0f909b2971492fd0dccacca25deee6cbc9f84"},"cell_type":"code","source":"def slightly_better(test, submission):\n    submission[['sales']] = submission[['sales']].astype(np.float64)\n    for _, row in test.iterrows():\n        dow, month, year = row.name.dayofweek, row.name.month, row.name.year\n        item, store = row['item'], row['store']\n        base_sales = store_item_table.at[store, item]\n        mul = month_table.at[month, 'sales'] * dow_table.at[dow, 'sales']\n        pred_sales = base_sales * mul * annual_growth(year)\n        submission.at[row['id'], 'sales'] = pred_sales\n    return submission\n\nslightly_better_pred = slightly_better(test, sample_sub.copy())\nslightly_better_pred.to_csv(\"sbp_float.csv\", index=False)\n\n# Round to nearest integer (if you want an integer submission)\nsbp_round = slightly_better_pred.copy()\nsbp_round['sales'] = np.round(sbp_round['sales']).astype(int)\nsbp_round.to_csv(\"sbp_round.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02762f1416f4d81eefa7dcee83f7fcb692f1f04c"},"cell_type":"markdown","source":"### This solution gets a public score of 13.88569, and 13.87573 when rounding to the nearest integer!\n**(A nice improvement especially given the simplicity of the solution)**\n\n*Note: Rounding to the nearest integer likely gives a marginally better score because the ground truth values are integers and rounding on average gets you closer to the actual values if your model is good.*"},{"metadata":{"trusted":true,"_uuid":"dfacc0ae23430c2b469b3d7ee4d089a608730497"},"cell_type":"markdown","source":"# How can we do better?\n\nNow that we have a very simple and effective model, there are many different direction we can go in improving the model. Here are a few ideas:\n\n* Try seeing how well the model does on the training set itself and what the SMAPE metric looks like. Does the noise properties make sense? Is there a trend in the SMAPE? Finding regions of high SMAPE in the training set can be a rough indicator of where accuracy is taking a hit on the test set!\n\n* Is the sales data normally distributed around the trends we found? If not, that can distort our predictions. Correctly for the noise distribution can help lower the SMAPE (and ultimately, make a better predictor).\n\n* Are there other trends we missed? ***Try not to depend on black-box algorithms!*** Use your domain knowledge of stores and think about what could affect item sales.\n\n# Conclusion\n\nWhile it is enticing to throw a complicated magical algorithm at any and all datasets blindly, it is usually easier and more meaningful to simply think about the data and come up with simpler models. This kernel was written to show how easy-to-understand methods such as finding averages and simple regressions used under the guidance of domain knowledge (i.e., thinking about how stores work) do equally as well, if not much better than overly-complicated algorithms."},{"metadata":{"_uuid":"12520523a8a33d12a11664afadde666052f94d62"},"cell_type":"markdown","source":"# Additional: Tweaking the predictor\n\nOne of the small tweaks we can make to the model is to weigh data by recency. So, we weigh older data less and much recent data more! One easy way to do this is to use an exponential decay function for your weight. We want the weights to get exponentially smaller the further back in the past we go.\n\n*Since this is simulated data, if the simulation had some hidden variables that changed with time, perhaps this is a simple way to encode that into the model without knowing what it is.*\n\nHere I use the following equation for the weights: $$\\exp\\left(\\frac{year - 2018}{5}\\right)$$\n\nThe factor of 5 is arbitrarily picked for simulated data. In real data, it might make sense since you would expect store sales to lose predictive power after a decade or so. "},{"metadata":{"trusted":true,"_uuid":"0223a8231f7bc0177286c9e6bd6017914927d857"},"cell_type":"code","source":"years = np.arange(2013, 2019)\nannual_sales_avg = year_table.values.squeeze()\n\nweights = np.exp((years - 2018)/5)\n\nannual_growth = np.poly1d(np.polyfit(years[:-1], annual_sales_avg, 2, w=weights[:-1]))\nprint(f\"2018 Relative Sales by Weighted Fit = {annual_growth(2018)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4105c5b9e74c6b52c7137a44aded9262adfdfe6b"},"cell_type":"code","source":"def weighted_predictor(test, submission):\n    submission[['sales']] = submission[['sales']].astype(np.float64)\n    for _, row in test.iterrows():\n        dow, month, year = row.name.dayofweek, row.name.month, row.name.year\n        item, store = row['item'], row['store']\n        base_sales = store_item_table.at[store, item]\n        mul = month_table.at[month, 'sales'] * dow_table.at[dow, 'sales']\n        pred_sales = base_sales * mul * annual_growth(year)\n        submission.at[row['id'], 'sales'] = pred_sales\n    return submission\n\nweighted_pred = weighted_predictor(test, sample_sub.copy())\n\n# Round to nearest integer\nwp_round = weighted_pred.copy()\nwp_round['sales'] = np.round(wp_round['sales']).astype(int)\nwp_round.to_csv(\"weight_predictor.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fcfaa76e02e84b12915c700e04eb62a06937fa5"},"cell_type":"markdown","source":"### This solution gets a public score of 13.85181!\n\n**Which is quite nice, indeed!**"},{"metadata":{"_uuid":"8dbfc3dfcc4d773c9732180852909fdae8b92352"},"cell_type":"markdown","source":"# Additional II: Exploiting a noisy degeneracy\n\nI noticed (quite late, I admit) that there is a small degeneracy I missed above. If you look at the plots above in the notebook, you notice that the store in which an item being sold has a *very stable* relative sales factor. However, the \"day of the week\" on which an item is being sold has a larger spread (or is more noisy to the eye).\n\nThis reveals a small error I made earlier: Making a store-item look up table. This should have been a \"Day of week\" - Item look up table. This would encode any built-in degeneracies over those dimensions and greatly improve the model."},{"metadata":{"trusted":true,"_uuid":"e2490b3915bfe1e537d327e2b652f5aa29cce83b"},"cell_type":"code","source":"# Day of week - Item Look up table\ndow_item_table = pd.pivot_table(data, index='dayofweek', columns='item', values='sales', aggfunc=np.mean)\ndisplay(dow_item_table)\n\n# Store pattern\nstore_table = pd.pivot_table(data, index='store', values='sales', aggfunc=np.mean)\nstore_table.sales /= grand_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98af5d77a27c3cd4c74d268c024242bafa88e8c1"},"cell_type":"code","source":"def awesome_predictor(test, submission):\n    submission[['sales']] = submission[['sales']].astype(np.float64)\n    for _, row in test.iterrows():\n        dow, month, year = row.name.dayofweek, row.name.month, row.name.year\n        item, store = row['item'], row['store']\n        base_sales = dow_item_table.at[dow, item]\n        mul = month_table.at[month, 'sales'] * store_table.at[store, 'sales']\n        pred_sales = base_sales * mul * annual_growth(year)\n        submission.at[row['id'], 'sales'] = pred_sales\n    return submission\n\npred = awesome_predictor(test, sample_sub.copy())\nrounded = pred.copy()\nrounded['sales'] = np.round(rounded['sales']).astype(int)\nrounded.to_csv(f\"awesome_prediction.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e48602744f3379c1dc28e792c2b85933c2f88900"},"cell_type":"markdown","source":"### This solution gets a public score of 13.84508!\n\n**Awesome!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}