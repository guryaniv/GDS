{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12bf2fd3-c34a-d53f-240a-5dbd4feab113"
      },
      "source": [
        "This is a take on Scirpus' MCMC notebook. Most of it is similar until the end in which I compare it with OOF stacking methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71120554-87aa-e461-d0d3-d91040b2c482"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "78056b59-0436-6f9f-3f42-39d58be4ae61"
      },
      "source": [
        "The following cell creates the two models with noise based on a target.\n",
        "One should note that the first model has more noise than the second model so one would expect model 1 to perform worse than model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfb499a7-5137-637a-04c1-c423f1786764"
      },
      "outputs": [],
      "source": [
        "size = 500\n",
        "true_intercept = 1\n",
        "true_slope = 2\n",
        "x = np.linspace(0, 1, size)\n",
        "# y = a + b*x\n",
        "true_regression_line = true_intercept + true_slope * x\n",
        "# add noise\n",
        "model1 = true_regression_line + np.random.normal(scale=.5, size=size) #Noisy\n",
        "model2 = true_regression_line + np.random.normal(scale=.2, size=size) #Less Noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0e79a4d-483a-c265-1e60-3c9465546af6"
      },
      "outputs": [],
      "source": [
        "np.random.seed = 0\n",
        "permutation_set = np.random.permutation(size)\n",
        "train_set = permutation_set[0:size//2]\n",
        "test_set = permutation_set[size//2:size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d32a7bf8-87a9-4a08-0b8b-c30ed073bfe7"
      },
      "source": [
        "Let us see what the MAE looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e290451b-e23d-d38f-4308-9a96a4cd08a9"
      },
      "outputs": [],
      "source": [
        "print(mean_absolute_error(true_regression_line[test_set],model1[test_set]))\n",
        "print(mean_absolute_error(true_regression_line[test_set],model2[test_set]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fd672844-7d55-03b1-7afa-c686db58a406"
      },
      "source": [
        "As expected the noisier model does worse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1c14c03a-35c4-d4b4-e70c-cb7cae364e67"
      },
      "source": [
        "Now let us look at the straight average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f4dd744e-f03c-21ac-bdc2-3408a678dd1a"
      },
      "outputs": [],
      "source": [
        "print(mean_absolute_error(true_regression_line[test_set],(model1*.5+model2*.5)[test_set]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ceb449e7-e977-7f0f-98eb-b55ac402722d"
      },
      "source": [
        "As one can see this isn't as good as our top model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebcb3903-ed60-a403-51f2-30a94120d5d3"
      },
      "source": [
        "Now comes the cool part.  We are going to use MCMC to draw samples from our data and get stats on how we can obtain a model that gets the best out of our raw models.\n",
        "\n",
        "Important:  Please look at the documentation [here][1] (https://pymc-devs.github.io/pymc3/index.html) for details\n",
        "\n",
        "\n",
        "  [1]: https://pymc-devs.github.io/pymc3/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a23b1bda-4853-a214-c2d9-eb8bdc30fe1a"
      },
      "outputs": [],
      "source": [
        "data = dict(x1=model1[train_set], x2=model2[train_set], y=true_regression_line[train_set])\n",
        "with pm.Model() as model:\n",
        "    # specify glm and pass in data. The resulting linear model, its likelihood and \n",
        "    # and all its parameters are automatically added to our model.\n",
        "    pm.glm.glm('y ~ x1 + x2', data)\n",
        "    step = pm.NUTS() # Instantiate MCMC sampling algorithm\n",
        "    trace = pm.sample(2000, step, progressbar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b0e9af59-4534-ec33-a358-03f40ad0419b"
      },
      "source": [
        "It takes a while - now is time to look at what goodness it gives to us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ea87feb-96d1-ac52-09b2-11a90707ad5d"
      },
      "outputs": [],
      "source": [
        "pm.traceplot(trace, figsize=(7,7))\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e1d921bb-9c51-9faf-8608-b07494936936"
      },
      "source": [
        "One can see that for every drawn sample it gives the parameter values for the intercept, x1 and x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3afeac54-5894-738d-5d6c-93a8f87e321f"
      },
      "outputs": [],
      "source": [
        "intercept = np.median(trace.Intercept)\n",
        "print(intercept)\n",
        "x1param = np.median(trace.x1)\n",
        "print(x1param)\n",
        "x2param = np.median(trace.x2)\n",
        "print(x2param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fe9804b1-df81-a387-1780-a7f325cee91a"
      },
      "source": [
        "I created a quick imitation of test/train split in order to compare of OOF ensembling methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b2e33ede-bcbe-3b59-6ec3-cb9485de7bb4"
      },
      "outputs": [],
      "source": [
        "model1_train = model1[train_set]\n",
        "model2_train = model2[train_set]\n",
        "x_train = np.vstack((model1_train, model2_train)).T\n",
        "\n",
        "model1_test = model1[test_set].T\n",
        "model2_test = model2[test_set].T\n",
        "x_test = np.vstack((model1_test, model2_test)).T\n",
        "\n",
        "y = true_regression_line[train_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "59637cab-3cda-d32e-e64f-1bf87e3de9bf"
      },
      "source": [
        "Now to check if Linear Regression  finds a similar solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "113ad939-6418-d61a-f199-5227d8a1842e"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "clfLR = LinearRegression()\n",
        "clfLR.fit(x_train, y)\n",
        "y_pred_LR = clfLR.predict(x_test)\n",
        "print(clfLR.intercept_)\n",
        "print(clfLR.coef_[0])\n",
        "print(clfLR.coef_[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c7b85438-ea19-cb2a-c6c7-c581f934f048"
      },
      "source": [
        "And a simple neural net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "25a11197-3208-a64a-2d7e-f8fdfe78503e"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "clfMLP = MLPRegressor()\n",
        "clfMLP.fit(x_train, y)\n",
        "y_pred_MLP = clfMLP.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c2b94f4e-fb9d-69e7-16d5-c0f4ee80a0a5"
      },
      "source": [
        "And a GBM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6314985-8183-32a4-a865-ad00ac975a7e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "clfGBR = GradientBoostingRegressor(random_state=0)\n",
        "clfGBR.fit(x_train, y)\n",
        "y_pred_GBR = clfGBR.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d4c5b45-5e05-1d32-5625-1ba880839cb0"
      },
      "source": [
        "Now let's compare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3f7493b-2612-b30b-af02-6dd774e3b292"
      },
      "outputs": [],
      "source": [
        "print('Model 1:',mean_absolute_error(true_regression_line[test_set],model1[test_set]))\n",
        "print('Model 2:', mean_absolute_error(true_regression_line[test_set],model2[test_set]))\n",
        "print('Average:',mean_absolute_error(true_regression_line[test_set],(model1*.5+model2*.5)[test_set]))\n",
        "print('MCMC:',mean_absolute_error(true_regression_line[test_set],\n",
        "                                  (intercept+x1param*model1+x2param*model2)[test_set]))\n",
        "print('LR:',mean_absolute_error(true_regression_line[test_set], y_pred_LR))\n",
        "print('MLP:',mean_absolute_error(true_regression_line[test_set], y_pred_MLP))\n",
        "print('GBM:',mean_absolute_error(true_regression_line[test_set], y_pred_GBR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a54d6c3-5642-8490-9480-5c73f42965fd"
      },
      "source": [
        "Looks like MCMC did not outperform linear regression, however it was pretty close. Both of them come up with similar coefficients. Additionally, MCMC gives you a good sense of the standard deviation, although it runs significantly slower. Seems like a good tool to use the in the ensembling tool belt! Thanks Scirpus!"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}