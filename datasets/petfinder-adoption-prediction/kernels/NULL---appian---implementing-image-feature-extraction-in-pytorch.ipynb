{"cells":[{"metadata":{"_uuid":"bec1b5eeda91c019614999a19b7ededa55900799"},"cell_type":"markdown","source":"This kernel extracts features from pet image. \n\n- Pytorch implementation.\n- Resize image to square while keeping its aspect ratio.\n- Profile image is used.\n- Pretrained densenet121 is used but you can use resnet or other architectures by replacing a few lines of code.\n\nThe kernel is inspired by dieter's great kernel https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn\nPlease check his kernel too if you haven't."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport glob\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import functional as F\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5be0dd4411d83880151865d78a8d649334cec7db"},"cell_type":"markdown","source":"## Prepare dataset for feature extraction"},{"metadata":{"trusted":true,"_uuid":"1a83eecb627f4b8790c2152d5b46d831084aea76"},"cell_type":"code","source":"def get_profile_path(category):\n\n    data = []\n\n    for path in sorted(glob.glob('../input/%s_images/*-1.jpg' % category)):\n\n        data.append({\n            'PetID': path.split('/')[-1].split('-')[0],\n            'path': path,\n        })\n            \n    return pd.DataFrame(data)\n\ntrain = get_profile_path('train')\ntest = get_profile_path('test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ce85df49d87bae0a43a698853c9ed811c35bf3e"},"cell_type":"code","source":"def resize_to_square(image, size):\n    h, w, d = image.shape\n    ratio = size / max(h, w)\n    resized_image = cv2.resize(image, (int(w*ratio), int(h*ratio)), cv2.INTER_AREA)\n    return resized_image\n\ndef image_to_tensor(image, normalize=None):\n    tensor = torch.from_numpy(np.moveaxis(image / (255. if image.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n    if normalize is not None:\n        return F.normalize(tensor, **normalize)\n    return tensor\n\ndef pad(image, min_height, min_width):\n    h,w,d = image.shape\n\n    if h < min_height:\n        h_pad_top = int((min_height - h) / 2.0)\n        h_pad_bottom = min_height - h - h_pad_top\n    else:\n        h_pad_top = 0\n        h_pad_bottom = 0\n\n    if w < min_width:\n        w_pad_left = int((min_width - w) / 2.0)\n        w_pad_right = min_width - w - w_pad_left\n    else:\n        w_pad_left = 0\n        w_pad_right = 0\n\n    return cv2.copyMakeBorder(image, h_pad_top, h_pad_bottom, w_pad_left, w_pad_right, cv2.BORDER_CONSTANT, value=(0,0,0))\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \n    def __init__(self, df, size):\n        self.df = df\n        self.size = size\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n\n        row = self.df.iloc[idx]\n\n        image = cv2.imread(row.path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = resize_to_square(image, self.size)\n        image = pad(image, self.size, self.size)\n        tensor = image_to_tensor(image, normalize={'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]})\n            \n        return tensor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6ea91221d996761ae4658e244c8d318a864f953"},"cell_type":"markdown","source":"I originally implemented these image transforms using albumentations but the library was not included in kaggle kernel and had to adopt some functionalities. https://github.com/albu/albumentations"},{"metadata":{"_uuid":"e01733039306734302e0be1886c5d973deaf5951"},"cell_type":"markdown","source":"## Let's check how images are transformed."},{"metadata":{"trusted":true,"_uuid":"e28e333e52a6c5b02cd21f74573029ea4dbdf62b"},"cell_type":"code","source":"random.seed(70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"080cc541a2d883473da3b49990454ba22e23da1c"},"cell_type":"code","source":"size = 224\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(idx=0):\n\n    dataset = Dataset(train, size)\n\n    image1 = cv2.imread(dataset.df.iloc[idx].path)\n    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n\n    tensor = dataset[idx]\n    image2 = np.transpose(tensor.numpy(), (1,2,0))\n\n    show_image_pair(image1, image2)\n\nfor idx in [random.choice(range(1000)) for i in range(3)]:\n    test_dataset(idx)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a81fbb69fc8d75d274b8d2c45df6903ba5a46c6e"},"cell_type":"markdown","source":"## Extract features\n\nYou can use register_forward_hook to extract features without modifying the forward pass. https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.register_forward_hook\n\nOfficial pytorch pretrained models can be found here. https://pytorch.org/docs/stable/torchvision/models.html"},{"metadata":{"trusted":true,"_uuid":"d5d23e89b7d117324224748432107685356d84dc"},"cell_type":"code","source":"model_name = 'densenet121'\nlayer_name = 'features'\n\n# If you want to use other models such as resnet18, uncomment lines below\n#model_name = 'resnet18'\n#layer_name = 'avgpool'\n\nget_model = getattr(torchvision.models, model_name)\n\ndef extract_features(df):\n\n    model = get_model(pretrained=True)\n    model = model.cuda()\n    model.eval()\n\n    # register hook to access to features in forward pass\n    features = []\n    def hook(module, input, output):\n        N,C,H,W = output.shape\n        output = output.reshape(N,C,-1)\n        features.append(output.mean(dim=2).cpu().detach().numpy())\n    handle = model._modules.get(layer_name).register_forward_hook(hook)\n\n    dataset = Dataset(df, size)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n\n    for i_batch, inputs in tqdm(enumerate(loader), total=len(loader)):\n        _ = model(inputs.cuda())\n\n    features = np.concatenate(features)\n\n    features = pd.DataFrame(features)\n    features = features.add_prefix('IMAGE_')\n    features.loc[:,'PetID'] = df['PetID']\n    \n    handle.remove()\n    del model\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdcefdc5a1a887a315a2de158ddf2af1cfb2645e"},"cell_type":"code","source":"features_train = extract_features(train)\nfeatures_test = extract_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f676bce5ac52b29151e85c3bad4caff208ba6a63"},"cell_type":"code","source":"features_train.to_csv('%s_size%d_train.csv' % (model_name, size), index=False)\nfeatures_test.to_csv('%s_size%d_test.csv' % (model_name, size), index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}