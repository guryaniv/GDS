{"cells": [{"outputs": [], "metadata": {"_uuid": "21eec13fe15702223fc0913afbb7bc3ec3018aa3", "_cell_guid": "4846c062-7350-566d-0693-181f5c6925d1", "trusted": true, "_execution_state": "idle"}, "cell_type": "code", "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "execution_count": 1}, {"execution_count": null, "metadata": {"_uuid": "1b53e0d972c0ce18f1284049e9d712d38b37e045", "_cell_guid": "bd6bb321-860d-d0fe-059d-622034a0f835"}, "cell_type": "markdown", "source": "### Import plotting libraries ###", "outputs": []}, {"outputs": [], "metadata": {"_uuid": "784292a59e400171a120e7d85dd69fb04dec153c", "_cell_guid": "68fe4408-e57e-15c1-4a88-27d9ae7ee7e1", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(color_codes=True)\nsns.set_style(\"white\")\n\nfrom plotly.offline import plot\nimport plotly.graph_objs as go\n\nimport sklearn.ensemble\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstops = set(stopwords.words(\"english\"))", "execution_count": 2}, {"outputs": [], "metadata": {"_uuid": "a364b4cae09525068bfdbc4fbaacaa67287f8a95", "_cell_guid": "e9425502-b733-74a2-da78-769cf1e31917", "trusted": true, "_execution_state": "idle"}, "cell_type": "code", "source": "train_set = pd.read_csv('../input/train.csv')\n\ntest_set = pd.read_csv('../input/test.csv')\n\nprint('There are {} records in train'.format(train_set.shape[0]))\nprint('There are {} records in train'.format(test_set.shape[0]))", "execution_count": 3}, {"outputs": [], "metadata": {"_uuid": "1bac82a69039dc97e2c0d1dfecab61bce295142d", "_cell_guid": "8adc5737-61ba-2855-ba75-2fdd6378e667", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "target = 'is_duplicate'\nID = 'id'", "execution_count": 4}, {"outputs": [], "metadata": {"trusted": true, "collapsed": true, "_uuid": "a5ce0068a61a0c22591b1c738d32e7fe41dc2f41"}, "cell_type": "code", "source": "train_set['question1'] = train_set['question1'].fillna('')\ntrain_set['question2'] = train_set['question2'].fillna('')\n\ntest_set['question1'] = test_set['question1'].fillna('')\ntest_set['question2'] = test_set['question2'].fillna('')", "execution_count": 5}, {"outputs": [], "metadata": {"trusted": true, "collapsed": true, "_uuid": "2628282c1349349b49e41f9efefb2edbdbbf52d3"}, "cell_type": "code", "source": "def clean_text(text):\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(\"\\.\\s\", '.', text)\n    text = re.sub(':\\)', '.', text)\n    text = re.sub(\"^\\s\\w+\", '\\w+', text)\n    text = re.sub('\\.', ' ', text)\n    return text", "execution_count": 6}, {"outputs": [], "metadata": {"trusted": true, "collapsed": true, "_uuid": "e630bc88a1d06bbb24cda1bb2c73beffa3b805d6"}, "cell_type": "code", "source": "def replace(sentence):\n    sentence = sentence.replace('what\\'s', 'what is').replace('don\\'t', 'do not')\n    sentence = sentence.replace('i\\'m', 'i am').replace('can\\'t', 'can not')\n    sentence = sentence.replace('doesn\\'t', 'does not').replace('it\\'s', 'it is')\n    sentence = sentence.replace('didn\\'t', 'did not').replace('isn\\'t', 'is not')\n    sentence = sentence.replace('won\\'t', 'will not').replace('aren\\'t', 'are not')\n    sentence = sentence.replace('shouldn\\'t', 'should not').replace('haven\\'t', 'have not')\n    sentence = sentence.replace('hasn\\'t', 'has not').replace('he\\'s', 'he is')\n    sentence = sentence.replace('wouldn\\'t', 'would not').replace('he\\'s', 'he is')\n    sentence = sentence.replace('that\\'s', 'that is').replace('wasn\\'t', 'was not')\n    sentence = sentence.replace('how\\'s', 'how is')\n    sentence = sentence.replace('you\\'ve', 'you have').replace('you\\'re', 'you are')\n    sentence = sentence.replace('i\\'ve', 'i have').replace('they\\'re', 'they are')\n    sentence = sentence.replace('i\\'ll', 'i will').replace('they\\'ve', 'they have')\n    sentence = sentence.replace('we\\'re', 'we are').replace('you\\'ll', 'you will')\n    sentence = sentence.replace('we\\'re', 'we are').replace('we\\'ve', 'we have')\n    sentence = sentence.replace('we\\'ll', 'we will').replace('it\\'ll', 'it will').replace('they\\'ll', 'they will')\n    sentence = sentence.replace('who\\'ll', 'who will').replace('who\\'ve', 'who have')\n    sentence = sentence.replace('he\\'ll', 'he will').replace('that\\'ll', 'that will')\n    sentence = sentence.replace('does\\'nt', 'does not').replace('could\\'ve', 'could have')\n    sentence = sentence.replace('would\\'ve', 'would have').replace('what\\'re', 'what are')\n    sentence = sentence.replace('i\\'am', 'i am').replace('who\\'re', 'who are')\n    sentence = sentence.replace('should\\'ve', 'should have').replace('did\\'nt', 'did not')\n    sentence = sentence.replace('hold\\'em', 'hold them').replace('there\\'re', 'there are')\n    sentence = sentence.replace('do\\'nt', 'do not').replace('could\\'nt', 'could not')\n    return sentence", "execution_count": 7}, {"outputs": [], "metadata": {"_uuid": "43b948617c253bad80712f10ec6ca8021d4a0c3a", "_cell_guid": "1758be31-5a90-5fd2-f648-34d19722847e", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "def find_unigrams(question):\n    question = clean_text(question)\n    question = replace(question)\n    \n    word_tokens = question.split(' ')\n    word_tokens = [w for w in word_tokens if not w  in stops]\n    word_tokens = [w for w in word_tokens if not w == '']\n    return word_tokens", "execution_count": 8}, {"outputs": [], "metadata": {"_uuid": "6a26d7eb0874cb7161481ce00b21ffc8908b4093", "_cell_guid": "2735b59c-6390-39ef-fc04-2278fcaa78ec", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "def shared_words_in_q2(row):\n    q1_tokens = row['q1_tokens']\n    q2_tokens = row['q2_tokens']\n    \n    matching_words = [w for w in q2_tokens if w in q1_tokens]\n    return len(matching_words) / (len(q1_tokens) + len(q2_tokens))", "execution_count": 9}, {"outputs": [], "metadata": {"_uuid": "f144ed0b942700f4b0d54996f7c891ed22394ec4", "_cell_guid": "a5fee247-640c-bce8-ab6f-21646a31b668", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "def shared_words_in_q1(row):\n    q1_tokens = row['q1_tokens']\n    q2_tokens = row['q2_tokens']\n    matching_words = [w for w in q1_tokens if w in q2_tokens]\n    \n    return len(matching_words) / (len(q1_tokens) + len(q2_tokens))", "execution_count": 10}, {"outputs": [], "metadata": {"_uuid": "daca641724cd82d3d2bd9d9f007da1d5141f8a07", "_cell_guid": "5f0dfb22-1482-a70a-5ab8-5d021ea239b8", "trusted": true, "collapsed": true, "_execution_state": "idle"}, "cell_type": "code", "source": "train_set['q1_tokens'] = train_set['question1'].map(find_unigrams)\ntrain_set['q2_tokens'] = train_set['question2'].map(find_unigrams)\n\ntrain_set['q1_length'] = train_set['q1_tokens'].apply(len)\ntrain_set['q2_length'] = train_set['q2_tokens'].apply(len)\ntrain_set['len_diff'] = train_set.apply(lambda x: np.abs(x['q1_length'] - x['q2_length']), axis=1)\n\ntrain_set['shared_words_q1'] = train_set.apply(lambda x: shared_words_in_q1(x), axis=1)\ntrain_set['shared_words_q2'] = train_set.apply(lambda x: shared_words_in_q2(x), axis=1)", "execution_count": 11}, {"outputs": [], "metadata": {"_uuid": "4094f1471a58bd7bafc062c6a8d256742983ff12", "_execution_state": "idle", "trusted": true, "_cell_guid": "ba911d75-0e2e-44d0-97c6-07be9260a5b1"}, "cell_type": "code", "source": "gb_qid  = train_set.groupby('qid1').filter(lambda x: len(x) > 1).groupby('qid1')\nduplicate_qid1 = sorted(list(gb_qid.groups))", "execution_count": 12}, {"outputs": [], "metadata": {"_uuid": "028ab05f03c250032b4fbd35e46ece4055c4f9e8", "_execution_state": "idle", "trusted": true, "_cell_guid": "c69b753b-9612-433d-a93c-1e288ebd77e6"}, "cell_type": "code", "source": "stats = gb_qid['is_duplicate'].agg({np.sum, np.size})\nonly_duplicates = stats.loc[stats['sum'] == stats['size']].sort_values(['size'], ascending=False)\nduplicate_df = train_set.loc[train_set['qid1'].isin(only_duplicates.index)]", "execution_count": 13}, {"outputs": [], "metadata": {"_uuid": "f19934e2d991b46f38120cfdb36811fdbdc09275", "_execution_state": "busy", "trusted": true, "_cell_guid": "c5a43786-6fb2-4454-8c7c-47841a3d6c27"}, "cell_type": "code", "source": "train_set.loc[train_set['qid1'].isin(duplicate_qid1), 'graph_root'] = 1\ntrain_set['graph_root'].fillna(0, inplace=True)\ntrain_set['graph_root'] = train_set['graph_root'].astype(int)", "execution_count": 14}, {"outputs": [], "metadata": {"trusted": true, "collapsed": true, "_uuid": "1f505f21869c5119b5596b0c043e218a9483b38a"}, "cell_type": "code", "source": "for node in only_duplicates.index:\n    group = train_set.loc[train_set['qid1'] == node]\n    group1 = train_set.loc[train_set['qid1'].isin(group['qid2'])]\n    \n    if len(group1) > 0:\n        train_set.loc[train_set['qid1'] == node, 'neighbors'] = len(group1)\n        \ntrain_set['neighbors'].fillna(0, inplace=True)\ntrain_set['neighbors'] = train_set['neighbors'].astype(int)", "execution_count": 15}, {"outputs": [], "metadata": {"_uuid": "e0d0f4e0490f86c02a920c443a115264bbb330c1", "_cell_guid": "a9410e58-245f-ab28-76ca-2520db61b848", "trusted": true, "collapsed": true, "_execution_state": "busy"}, "cell_type": "code", "source": "clf = RandomForestClassifier()\ntrain_features = ['len_diff', 'shared_words_q1']\n\ndef train_data(clf, train_features):\n    X = train_set[train_features]\n    y = train_set[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n    clf = clf.fit(X_train, y_train)\n\n    y_proba = clf.predict_proba(X_test)\n    log_loss_score = log_loss(y_test, y_proba)\n    metrics.append(log_loss_score)\n    return clf", "execution_count": 43}, {"outputs": [], "metadata": {"_uuid": "7c0e79aad3700044e39464ecf1fe388e67cb3536", "_cell_guid": "86bf4262-08f4-a4c6-334a-30d1068c6bff", "trusted": true, "_execution_state": "busy"}, "cell_type": "code", "source": "metrics = []\nfor i in np.arange(5):\n    train_data(clf, train_features)", "execution_count": 44}, {"outputs": [], "metadata": {"trusted": true, "_uuid": "f8586b648ba7206c0c2e037c495592cdfcb5c128"}, "cell_type": "code", "source": "metrics", "execution_count": 45}, {"outputs": [], "metadata": {"_uuid": "8364657695c2cbb91d88fbe7b0b0c10728ce727f", "_cell_guid": "80681035-bde6-016b-6676-c63f01851c99", "trusted": true, "collapsed": true, "_execution_state": "busy"}, "cell_type": "code", "source": "def preprocess():\n    test_set['q1_tokens'] = test_set['question1'].map(find_unigrams)\n    test_set['q2_tokens'] = test_set['question2'].map(find_unigrams)\n\n    test_set['q1_length'] = test_set['q1_tokens'].apply(len)\n    test_set['q2_length'] = test_set['q2_tokens'].apply(len)\n    test_set['len_diff'] = test_set.apply(lambda x: np.abs(x['q1_length'] - x['q2_length']), axis=1)\n\n    test_set['shared_words_q1'] = test_set.apply(lambda x: shared_words_in_q1(x), axis=1)\n    test_set['shared_words_q2'] = test_set.apply(lambda x: shared_words_in_q2(x), axis=1)\n    return test_set", "execution_count": 20}, {"outputs": [], "metadata": {"trusted": true, "collapsed": true, "_uuid": "6f7d7cf5c32511a7dff46fe56cbbcc881756160f"}, "cell_type": "code", "source": "test_set = preprocess()", "execution_count": 21}, {"outputs": [], "metadata": {"_uuid": "8667fc7869a736195f2229371a58c0bbdf17e306", "_cell_guid": "b1bf53b5-aac3-511c-283d-96aa27e2e5d9", "trusted": true, "collapsed": true, "_execution_state": "busy"}, "cell_type": "code", "source": "def generate_predictions(train_features):\n    test_ids = test_set['test_id']\n    predictions = clf.predict_proba(test_set[train_features])\n\n    submission = pd.DataFrame(test_ids)\n\n    prediction_set = []\n    for i in range(len(predictions)):\n        prediction_set.append(predictions[i][1])\n    \n    prediction_set = pd.DataFrame(prediction_set, columns=[target])\n    submission = pd.concat([submission, prediction_set], axis=1)\n    return submission", "execution_count": 46}, {"outputs": [], "metadata": {"_uuid": "e5fc8d9f11413c6bd4af3cb6a95057a3d1fc1a2f", "_cell_guid": "8a4544ae-63ff-40e5-926b-813d00f667d1", "trusted": true, "_execution_state": "busy"}, "cell_type": "code", "source": "submission = generate_predictions(['len_diff', 'shared_words_q1'])", "execution_count": 47}, {"outputs": [], "metadata": {"_uuid": "69f3c15af12526dbed212f405c0c69429f4d1fbb", "_cell_guid": "9071a76d-599a-f34d-69b4-4f092eb9c850", "trusted": true, "_execution_state": "busy"}, "cell_type": "code", "source": "print(set(pd.isnull(submission[target])))\nsubmission.to_csv(\"submission.csv\", index=False)", "execution_count": 48}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "_is_fork": false, "_change_revision": 0}, "nbformat_minor": 1}