{"cells":[{"metadata":{"_uuid":"4713524d0f63c5a8c67872c8f16333e9562862c3","_cell_guid":"da15e4eb-3183-423c-9b96-ca948f9a1881","trusted":true},"cell_type":"code","source":"import itertools\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n%matplotlib inline\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras import regularizers, initializers\nfrom keras.optimizers import SGD, adam\nfrom keras.activations import softmax\nfrom keras.metrics import categorical_accuracy\nfrom keras.callbacks import Callback\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"30867eff79de5bd5bf48ec9e70cf773b9ac4d610","_cell_guid":"dbab603d-8179-4648-b4db-8328a67d0c1e"},"cell_type":"markdown","source":"## Functions for reading data"},{"metadata":{"_uuid":"4ce7e0b1707f54282effd9ac96b246d4b3bb8fc1","_cell_guid":"9ba419ac-cb50-4dc7-b540-951485f3e41e","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_training_data():\n    \"\"\"\n    This function reads the training data from the Kaggle directory.\n    It returns X_train and y_train arrays.\n    \"\"\"\n    train = pd.read_csv(\"../input/train.csv\")\n    y_train = train.iloc[:, 0].values\n    X_train = train.iloc[:, 1:].values\n    print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n    return X_train.astype('float32'), y_train\n\ndef get_test_data():\n    \"\"\"\n    This function reads the test data from the Kaggle directory.\n    It returns X_test array.\n    \"\"\"\n    test = pd.read_csv(\"../input/test.csv\")\n    X_test = test.values\n    print (\"X_test.shape\", X_test.shape)\n    return X_test.astype('float32')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"48293e084ff0686d8316e0196cb76024c685ef6d","_cell_guid":"9e479a51-31c3-437a-9a4d-58dc53cd7c07"},"cell_type":"markdown","source":"## Functions for splitting training dataset"},{"metadata":{"_uuid":"41f0cea98d8295fd7db7a89f8254f5fd9de963ff","_cell_guid":"86d0834d-3784-46d2-976c-523474c09732","collapsed":true,"trusted":true},"cell_type":"code","source":"from math import floor\nfrom functools import reduce\nclasses = np.arange(0, 10)\n\ndef plot_splits_distribution(y_dict):\n    datasets = ['train', 'val', 'test']\n    fig, axes = plt.subplots(figsize=(10,3), nrows=1, ncols=3, sharey=True)\n    for i, dataset in enumerate(datasets):\n        ax = axes[i]\n        ax.set_title(dataset)\n        ax.hist(y_dict[dataset], density=True, label=dataset)\n        ax.set_ylabel('Frequency')\n        ax.set_xlabel('Classes')\n        ax.set_xticks(classes)\n\n    plt.tight_layout()\n\ndef random_sampling(X, y, classes, training_ratio=0.8, val_ratio = 0.1):\n    training_size = len(X)\n    indecis = np.arange(0, training_size)\n    np.random.shuffle(indecis)\n    \n    training_last_index = floor(training_size*training_ratio)\n    val_last_index = floor(training_size*(training_ratio+val_ratio))\n    \n    training_indecis = indecis[:training_last_index]\n    val_indecis = indecis[training_last_index:val_last_index]\n    test_indecis = indecis[val_last_index:]\n    \n    assert ((len(training_indecis)+len(val_indecis)+len(test_indecis))==training_size)\n    \n    X_train, y_train = X[training_indecis], y[training_indecis].reshape(-1, 1)\n    X_val, y_val = X[val_indecis], y[val_indecis].reshape(-1, 1)\n    X_test, y_test = X[test_indecis], y[test_indecis].reshape(-1, 1)\n\n    print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n    print(\"X_val.shape, y_val.shape\", X_val.shape, y_val.shape)\n    print(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n    \n    X_dict = {'train': X_train,\n              'val': X_val,\n              'test': X_test}\n    y_dict = {'train': y_train,\n              'val': y_val,\n              'test': y_test}\n    \n    plot_splits_distribution(y_dict)\n    \n    return X_dict, y_dict\n\ndef stratified_sampling(X, y, classes, training_ratio=0.8, val_ratio = 0.1):\n    training_size = len(X)\n    training_indecis = []\n    val_indecis = []\n    test_indecis = []\n\n    for a_class in classes:\n        #Get array indecis where y_train value is the same as the class in this iteration\n        class_indecis = np.argwhere(y==a_class)[:,0]\n        #Shuffle the indecis\n        np.random.shuffle(class_indecis)\n        #Compute the split points for training, validation and test sets\n        class_size = len(class_indecis)\n        training_last_index = floor(class_size*training_ratio)\n        val_last_index = floor(class_size*(training_ratio + val_ratio))\n        #Slice the class_indecis array for each set, then add to the list\n        training_indecis.append(class_indecis[:training_last_index])\n        val_indecis.append(class_indecis[training_last_index:val_last_index])\n        test_indecis.append(class_indecis[val_last_index:])\n\n    #A function to concatenate all arrays in a list\n    reduce_func = lambda a,b: np.concatenate([a,b], axis=0)\n    training_indecis = reduce(reduce_func, training_indecis)\n    val_indecis = reduce(reduce_func, val_indecis)\n    test_indecis = reduce(reduce_func, test_indecis)\n    \n    assert ((len(training_indecis)+len(val_indecis)+len(test_indecis))==training_size)\n    \n    X_train, y_train = X[training_indecis], y[training_indecis].reshape(-1, 1)\n    X_val, y_val = X[val_indecis], y[val_indecis].reshape(-1, 1)\n    X_test, y_test = X[test_indecis], y[test_indecis].reshape(-1, 1)\n\n    print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n    print(\"X_val.shape, y_val.shape\", X_val.shape, y_val.shape)\n    print(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n    \n    X_dict = {'train': X_train,\n              'val': X_val,\n              'test': X_test}\n    y_dict = {'train': y_train,\n              'val': y_val,\n              'test': y_test}\n    \n    plot_splits_distribution(y_dict)\n    \n    return X_dict, y_dict","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"82acac36414ffe1f76d324891ec90abf0c9c11a5","_cell_guid":"73816d47-11c5-48b0-aa70-78e384989dde"},"cell_type":"markdown","source":"## Functions for preprocessing"},{"metadata":{"_uuid":"d84ceb0d66a2b7fa1eea052deadc1603d173def4","_cell_guid":"c2189f97-5175-4be1-b77d-2b18ca612c22","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n\nclass ImageDataScaler (BaseEstimator, TransformerMixin):\n    def __init__(self, factor):\n        self.factor = factor\n    \n    def fit(self, *_):\n        return self\n    \n    def transform(self, X, *_):\n        return X / self.factor\n\ndef preprocess(X_dict, y_dict):\n    \"\"\"\n    A function to perform standard scaling on the X_train and OneHotEncoding on the y_train categories.\n    It returns:\n        X_scaler: A StandardScaler object fit to the X_train data\n        X_train_scaled\n        y_mlb: A MultiLabelBinarizer object fit to y_train\n        y_train_categorical\n    \"\"\"\n    X_train = X_dict['train']\n    X_val = X_dict['val']\n    X_test = X_dict['test']\n    y_train = y_dict['train']\n    y_val = y_dict['val']\n    y_test = y_dict['test']\n    \n    y_mlb = MultiLabelBinarizer().fit(y_train)\n    X_scaler = ImageDataScaler(factor=255).fit(X_train)\n    \n    X_train = X_scaler.transform(X_train)\n    X_val = X_scaler.transform(X_val)\n    X_test = X_scaler.transform(X_test)\n    \n    y_train = y_mlb.transform(y_train)\n    y_val = y_mlb.transform(y_val)\n    y_test = y_mlb.transform(y_test)\n    \n    print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n    print(\"X_val.shape, y_val.shape\", X_val.shape, y_val.shape)\n    print(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n    \n    X_dict = {'train': X_train,\n              'val': X_val,\n              'test': X_test}\n    y_dict = {'train': y_train,\n              'val': y_val,\n              'test': y_test}\n    transformers = {'X': X_scaler,\n                    'y': y_mlb}\n    \n    return X_dict, y_dict, transformers\n\ndef preprocess_test_data(X_scaler, X_test):\n    \"\"\"\n    A function to perform feature scaling on X_test using the X_scaler that is fit to the training data\n    It returns X_test_scaled\n    \"\"\"\n    X_test_scaled = X_scaler.transform(X_test)\n    print (\"X_test_scaled.shape\", X_test_scaled.shape)\n    return X_test_scaled","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"4f6dd288b16bb9b373d47b4af4678ab8e54b5249","_cell_guid":"eb326988-c760-4fe4-823c-d4c1b1a62ba3"},"cell_type":"markdown","source":"## Functions for visualization"},{"metadata":{"_uuid":"4b7d2142b2b28d35b5e27f6978f890b30daa9b17","_cell_guid":"dec25a7a-e28f-4beb-8975-03e18dba7c0e","collapsed":true,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.axes_grid1 import make_axes_locatable\n\nrows, cols = 28, 28\ndef show_digit(idx, X, y):\n    \"\"\"\n    A function to plot a digit with its labels from X and y arrays for a given index.\n    \"\"\"\n    pixels = X[idx, :]\n    label = y[idx, 0]\n    image = pixels.reshape(rows, cols)\n    plt.imshow(image, cmap='gray_r')\n    plt.title(label)\n    plt.show()\n\ndef show_n_images_prediction(indecis, X, X_scaler, model, dataset_name):\n    \"\"\"\n    This function plots a series of digits specified in the indecis array. The X is the unscaled digit images array.\n    Arguments:\n        indecis: list or 1d numpy array of the indecis\n        X: Numpy array of digit images of shapes (m, n_x) or (m, row, col, channel) or (m, channel, row, col)\n        X_scaler: a StandardScaler object fit to X_train data\n        model: a classification model with 'predict' method\n    \"\"\"\n    X_scaled = X_scaler.transform(X)\n    ncols = 5\n    nrows = (len(indecis)-1) // ncols + 1\n    fig = plt.figure(figsize=(2*ncols, 2.5*nrows))\n    for i, idx in enumerate(indecis):\n        pixels = X[idx, :].reshape(1, -1)\n        probs = model.predict(np.expand_dims(X_scaled[idx, :], axis=0))\n        pred = np.argmax(probs)\n        image = pixels.reshape((rows, cols))\n        fig.add_subplot(nrows, ncols, i+1)\n        plt.imshow(image, cmap='gray_r')\n        plt.title(\"{:.2f}% -> {}\".format(probs[0, pred]*100, pred))\n    plt.suptitle('{} images from {} with the model prediction'.format(len(indecis), dataset_name), fontsize=16)\n    plt.show()\n\ndef plot_history(history, metric_name):\n    \"\"\"\n    To visualize the loss and the metric variation vs. epochs\n    \"\"\"\n    fig = plt.figure(figsize=(10,4))\n    fig.add_subplot(1,2,1)\n    plt.plot(history.history['loss'], label='Training')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    fig.add_subplot(1,2,2)\n    plt.plot(history.history[metric_name], label='Training')\n    plt.plot(history.history['val_'+metric_name], label='Validation')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    plt.legend()\n    plt.tight_layout()\n    \ndef plot_confusion_matrix(cms, classes, titles, normalize=False, cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        pass\n        #print('Confusion matrix, without normalization')\n    \n    #print(cm)\n    \n    fig, axes = plt.subplots(figsize=(16,6), nrows=1, ncols=3)\n    for i, cm in enumerate(cms):\n        ax = axes[i]\n        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n        divider = make_axes_locatable(ax)\n        cax = divider.append_axes('right', size='5%', pad=0.05)\n        fig.colorbar(im, cax=cax)\n        \"\"\"\n        fig.colorbar(im, ax=ax)\n        ax.set_aspect('auto')\n        \"\"\"\n        ax.set_title(titles[i])\n        tick_marks = np.arange(len(classes))\n        ax.set_xticks(tick_marks)\n        ax.set_yticks(tick_marks)\n        ax.set_ylabel('True label')\n        ax.set_xlabel('Predicted label')\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() / 2.\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            ax.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    \n    plt.tight_layout()\n    \n\ndef compute_accuracy(y_true, model, X):\n    \"\"\"\n    This function computes and returns the model accuracy using this formula:\n        accuracy = correct_predictions / total_predictions\n    \"\"\"\n    y_pred = np.argmax(model.predict(X), axis=1).reshape(-1, 1)\n    correct_predictions = np.sum(y_pred==y_true)\n    total_predictions = len(y_pred)\n    accuracy = correct_predictions/total_predictions\n    return accuracy, y_pred\n\ndef visualize_results(model, history, metric_name,\n                      X_dict, y_dict, transformers, X_test, num_images=10):\n    \"\"\"\n    Arguments:\n        * model -------- A trained keras Nueral Network classification model that can predict y(digit classes) from input X\n        * history ------ The training history dictionary\n        * metric_name -- the name of metric used during the training\n        * X_train ------ \n    This function creates the following visualiztions:\n        1. Training Loss (and a given metric) vs. Epochs\n        2. A random sample of X_test images with their predictions\n        3. Confusion matrix\n    \"\"\"\n    \n    plot_history(history, metric_name)\n    classes = transformers['y'].classes_\n    datasets = ['train', 'val', 'test']\n    cms = []\n    cm_titles = []\n    for dataset in datasets:\n        accuracy, y_pred = compute_accuracy(y_dict[dataset], model, X_dict[dataset])\n        model_accuracy = \"({}): Acc: {:.2f}% | Err: {:.2f}%\".format(dataset, accuracy*100, (1-accuracy)*100) \n        cm=confusion_matrix(y_dict[dataset], y_pred)\n        cms.append(cm)\n        cm_titles.append(model_accuracy)\n    \n    plot_confusion_matrix(cms, classes, titles=cm_titles)\n    \n    indecis = np.random.randint(0, len(X_test), num_images)\n    show_n_images_prediction(indecis, X_test, transformers['X'], model, 'X_test')","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"2a20508fd69d586717aee3caff52a24b7865a356","_cell_guid":"0c06faec-c989-4380-937d-b7d54a350c1a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Plotting loss during the training\nclass PlotLosses(Callback):\n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.i += 1\n        \n        clear_output(wait=True)\n        plt.plot(self.x, self.losses, label=\"loss\")\n        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n        plt.legend()\n        plt.show()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"82fdf743d8db718ad7bd59ea972990983b9d87e3","_cell_guid":"4c5083f9-d297-4007-bccf-6a34de3f7410"},"cell_type":"markdown","source":"## First Attempt: Build a deep neural network, train and test\n\nLet's just design a NN model architecture. Since there are 10 classes to predict, I build a NN model with 5 layers, each layer having 10 hidden units as a default. Also by default, no L2 regularization and no dropout.\nI initializes the weights using the he_normal() method to help the optimization to converge quicker. All layers except for the last one will use relu actvcation. The last layer will have softmax activation."},{"metadata":{"_uuid":"957da84b73b61709df7c7b779625a34b34715354","_cell_guid":"fc3ca85d-f05e-4337-86ec-2797e72b38e0","collapsed":true,"trusted":true},"cell_type":"code","source":"def baseline_model(X_train_scaled, y_train_categorical, learning_rate=0.0025, decay_rate=1e-6, loss_f='categorical_crossentropy', \n                   metrics=[categorical_accuracy], n_units=10, l2_rate=0, dropout_rate=0):\n    m, input_dim = X_train_scaled.shape\n    _, output_dim = y_train_categorical.shape\n        \n    model = Sequential()\n    model.add(Dense(units=n_units,activation='relu',kernel_initializer=initializers.he_normal(),\n                    kernel_regularizer=regularizers.l2(l2_rate), bias_initializer='zeros',input_dim=input_dim))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(units=n_units,activation='relu',kernel_initializer=initializers.he_normal(),\n                    kernel_regularizer=regularizers.l2(l2_rate), bias_initializer='zeros'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(units=n_units,activation='relu',kernel_initializer=initializers.he_normal(),\n                    kernel_regularizer=regularizers.l2(l2_rate), bias_initializer='zeros'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(units=n_units,activation='relu',kernel_initializer=initializers.he_normal(),\n                    kernel_regularizer=regularizers.l2(l2_rate), bias_initializer='zeros'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(units=output_dim,activation='softmax',kernel_initializer=initializers.he_normal(),\n                    kernel_regularizer=regularizers.l2(l2_rate), bias_initializer='zeros'))\n    \n    optimizer_f = adam(lr=learning_rate, decay=decay_rate)\n    model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics)\n    \n    return model\n\ndef LeNet5_model(X_train_scaled, y_train_categorical, learning_rate=0.0025, decay_rate=1e-6, loss_f='categorical_crossentropy', \n                   metrics=[categorical_accuracy], l2_rate=0, dropout_rate=0):\n    m, input_dim = X_train_scaled.shape[0], X_train_scaled.shape[1:]\n    _, output_dim = y_train_categorical.shape\n    \n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_dim))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(dropout_rate))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(dropout_rate))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(output_dim, activation='softmax'))\n    \n    optimizer_f = adam(lr=learning_rate, decay=decay_rate)\n    model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics)\n    \n    return model\n\ndef modify_regularizations(model, reg_funcs):\n    layers = model.layers\n    for i, layer in enumerate(layers):\n        layer.kernel_regularizer = reg_funcs[i]\n    return model","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"795608ac18051fe25794e74aaaa11c027c108133","_cell_guid":"8f7a1365-6a1c-4655-ace8-79ec0be581f6"},"cell_type":"markdown","source":"### Getting the data..."},{"metadata":{"_uuid":"7de70ac645a158bb01cdbbe8ea0c1d638139ba7b","_cell_guid":"5f4839d9-6fe5-4fc6-bb44-f8b421a79908","trusted":true},"cell_type":"code","source":"X_train, y_train = get_training_data()\nX_test = get_test_data()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"1864fc0733f3253c72a0be89c8bf73bb6e684c8e","_cell_guid":"0a5b2136-b960-4803-9570-5caad4be492f"},"cell_type":"markdown","source":"## Random Sampling\n**Note**\n\nThe training data will be split into 3 sets:\n* (80%) training set\n* (10%) validation (development) set\n* (10%) test set\n\nIn the first attempt, the data will be split randomly."},{"metadata":{"_uuid":"01516fb0f69de63024bc63b258c61483565ba1cf","_cell_guid":"36a2f8d3-4c63-4ad9-b7c0-ff2661afa9fa","trusted":true},"cell_type":"code","source":"X_dict, y_dict = random_sampling(X_train, y_train, classes, training_ratio=0.8, val_ratio=0.1)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"c7ae83d0343832ec88f1b8e32d0bc6730e3cb4e3","_cell_guid":"c9485660-82fb-4286-9f7f-1e125484ec21"},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"_uuid":"7f3ec15f923a5732dd35a02b25e6acea16a59209","_cell_guid":"3c6df1af-14d2-4ce3-a419-7c5cdbdda462","trusted":true},"cell_type":"code","source":"X_scaled, y_categorical, transformers = preprocess(X_dict, y_dict)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"4b40c0a8b5ea44a488ea9d63a5df2588ffed3b36","_cell_guid":"eea9965e-f18b-45de-9734-9d5878146407"},"cell_type":"markdown","source":"\n### Model 1: The Baseline Model (Random Sampling from training set)"},{"metadata":{"_uuid":"f698bf4f635dfa3c1d57569beca54fd7b68f2eab","_cell_guid":"65963b77-72bd-4655-94fe-de1294222cbf","trusted":false,"collapsed":true},"cell_type":"code","source":"model1 = baseline_model(X_scaled['train'], y_categorical['train'])\nbatch_size = 128\nepochs = 100\nplot_losses = PlotLosses()\nhistory = model1.fit(X_scaled['train'], y_categorical['train'], batch_size=batch_size,\n                  epochs=epochs,  validation_data=(X_scaled['val'], y_categorical['val']), verbose=False,\n                  callbacks=[plot_losses])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fce1d11307a3cf944e545f5948ae7e502b313489","_cell_guid":"78b37343-ce10-4e70-beb4-da68f4006014","trusted":false,"collapsed":true},"cell_type":"code","source":"visualize_results(model1, history, 'categorical_accuracy', X_scaled, y_dict, transformers, X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc813b4ce9c7c915349878b1c321381eb67bba57","_cell_guid":"20b403ac-f389-4d1e-b8c3-94444f41385b"},"cell_type":"markdown","source":"## Stratifed Sampling"},{"metadata":{"_uuid":"7325195211a9c4005f96137f20def4e719ad83c3","_cell_guid":"ac21e112-5072-4312-b4bb-3a13aec4aff8"},"cell_type":"markdown","source":"Sample from the training data so that the training, validation and test sets have the same distribution."},{"metadata":{"_uuid":"1cd6f074701dd74ddc7077b8fab07a0aa3bc6669","_cell_guid":"9aa6f3ba-2d0e-45b5-a772-486223ac4038","trusted":true},"cell_type":"code","source":"X_dict, y_dict = stratified_sampling(X_train, y_train, classes, training_ratio=0.8, val_ratio=0.1)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"ee9e62477b1834b39ba971cd4e35405b6e37f05a","_cell_guid":"0f028867-b047-4796-bd4e-3b976fa39daf"},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"_uuid":"268e7c226ac9af5d75136784119818ef7324d446","_cell_guid":"eb6cdc80-b503-4976-b494-e1a28e9570cf","trusted":true},"cell_type":"code","source":"X_scaled, y_categorical, transformers = preprocess(X_dict, y_dict)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"ee9241b370102d2fe01ab5a56fd66b9fae2c93b6","_cell_guid":"2cea59c8-0422-4fc9-9a11-e456955fe484"},"cell_type":"markdown","source":"### Model 2: The BaseLine model (trained by stratified sampling from the training data)"},{"metadata":{"_uuid":"1a57c21b2c2710eccd9ea8b7100bebc03aef07c5","_cell_guid":"12e62b9d-f1cb-40b0-8862-45369e996eda","collapsed":true,"trusted":false},"cell_type":"code","source":"model2 = baseline_model(X_scaled['train'], y_categorical['train'])\nbatch_size = 128\nepochs = 100\nplot_losses = PlotLosses()\nhistory = model2.fit(X_scaled['train'], y_categorical['train'], batch_size=batch_size,\n                  epochs=epochs,  validation_data=(X_scaled['val'], y_categorical['val']), verbose=False,\n                  callbacks=[plot_losses])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a3aff4feafbfa7249a10dc53fec6576e66c8905","_cell_guid":"25b01da5-3fd6-4ca1-8b3e-6e9c8072da38","collapsed":true,"trusted":false},"cell_type":"code","source":"visualize_results(model2, history, 'categorical_accuracy', X_scaled, y_dict, transformers, X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff956c2418ae0ffb7dc8ae492845834992aea93","_cell_guid":"a908c701-5627-4b9d-b6bc-6d84e1a4b4e4"},"cell_type":"markdown","source":"## Error Analysis\n| Levels of Error | Error | Insight | Action Plan |\n|------------------|------|-----------|----------------|\n| Bayesian (Human) Error | 0 % | | |\n| Training Error | ~1.5 % | Avoidable bias | Change model architecture: # of layers, # of hidden units, CNN, etc |\n| Validation Error | ~9 % | Variance - Overfitting to the training set | Use L2 regularization, dropout or get more data|\n| Test Error | ~9 % | Ok | |\n\nIt looks like there is significant overfitting to the training set, as evident on the loss plot. So next I will try adding regularizations to the modeling to help reduce overfitting."},{"metadata":{"_uuid":"d97dce7325f65137a74bca91a0212b498bf7b6cd","_cell_guid":"3082e1ba-df10-4c52-ad00-23eb019e62eb"},"cell_type":"markdown","source":"### Model 3: Removing avoidable bias (by increasing number of hidden units of Model 2)"},{"metadata":{"_uuid":"4287ff133ba75ac3c2a59d820869eeecbf8e2742","_cell_guid":"4ad60a11-48cc-4baa-820d-1329ab7e3970","collapsed":true,"trusted":false},"cell_type":"code","source":"model3 = baseline_model(X_scaled['train'], y_categorical['train'], n_units=50 )\nbatch_size = 128\nepochs = 100\nplot_losses = PlotLosses()\nhistory = model3.fit(X_scaled['train'], y_categorical['train'], batch_size=batch_size,\n                  epochs=epochs,  validation_data=(X_scaled['val'], y_categorical['val']), verbose=False,\n                  callbacks=[plot_losses])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecd543dda336047bafb7530da341d749294a360f","_cell_guid":"6b11fe7b-9407-4c16-ba42-00ea0e3bcd1f","collapsed":true,"trusted":false},"cell_type":"code","source":"visualize_results(model3, history, 'categorical_accuracy', X_scaled, y_dict, transformers, X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9156a9fc567d3c7c31a7c1fe165c9964a2c2238d","_cell_guid":"e031ab4e-7d19-41f7-bb3c-7e279d104de5"},"cell_type":"markdown","source":"### Model 4: Removing the overfitting (by adding L2 Regularization to Model 3)"},{"metadata":{"_uuid":"d5332cc930804bd3c48fc4d234c0f6d6503e6688","_cell_guid":"17b99ffa-2b49-4722-8f42-53280bf8f547","collapsed":true,"trusted":false},"cell_type":"code","source":"model4 = baseline_model(X_scaled['train'], y_categorical['train'], n_units=50 ,l2_rate=0.0002)\nbatch_size = 128\nepochs = 100\nplot_losses = PlotLosses()\nhistory = model4.fit(X_scaled['train'], y_categorical['train'], batch_size=batch_size,\n                  epochs=epochs,  validation_data=(X_scaled['val'], y_categorical['val']), verbose=False,\n                  callbacks=[plot_losses])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95ca0b7755a1da1afc450e40293a23ac3e73660a","_cell_guid":"2313b385-9e73-473d-b687-b46d54366fc2","collapsed":true,"trusted":false},"cell_type":"code","source":"visualize_results(model4, history, 'categorical_accuracy', X_scaled, y_dict, transformers, X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c17e38adb83fedf99245cd4158a6c2c71dd1ac","_cell_guid":"88c84a0f-e75f-44e1-9e29-9ed4e339c1b9"},"cell_type":"markdown","source":"### Model 5: LeNet-5 Model"},{"metadata":{"_uuid":"cff99a4500e154087e8e2e52eda87a9c1fb98078","_cell_guid":"9e2ff7f7-c86a-4935-bf18-c92123321c56","trusted":true,"collapsed":true},"cell_type":"code","source":"nrows=ncols=28\nnchannels = 1\nfor key in X_scaled.keys():\n    X_scaled[key] = X_scaled[key].reshape(-1, nrows, ncols, nchannels)\n\nX_test = X_test.reshape(-1, nrows, ncols, nchannels)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"83d8bfc2a9cc14bd049a8319757bba8b4772dec8","_cell_guid":"e2fd1feb-1887-43a0-ac62-f2dd29e57732","trusted":true},"cell_type":"code","source":"model5 = LeNet5_model(X_scaled['train'], y_categorical['train'], dropout_rate=0.5)\nbatch_size = 128\nepochs = 50\nplot_losses = PlotLosses()\nhistory = model5.fit(X_scaled['train'], y_categorical['train'], batch_size=batch_size,\n                  epochs=epochs,  validation_data=(X_scaled['val'], y_categorical['val']), verbose=False,\n                  callbacks=[plot_losses])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"c6ba8b51d45c4d894e761988349d1b756c0f18cd","_cell_guid":"fcc644f4-0c96-41d5-a6ea-ceb2f04cd6e6","trusted":true},"cell_type":"code","source":"visualize_results(model5, history, 'categorical_accuracy', X_scaled, y_dict, transformers, X_test)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"e056c593fc11d534f40a70c7f1374dffac34f652","_cell_guid":"f18ed8c6-6748-41a7-9fea-84434de02977","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(\n    data=np.vstack([np.arange(1, len(X_test)+1), \n                    np.argmax(model5.predict(transformers['X'].transform(X_test)), axis=1)]).T,\n    columns=['ImageId', 'Label']\n)\nshow_n_images_prediction(list(range(0, 10)), X_test, transformers['X'], model5, \"X_test\")\nsubmission.head(10)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"cde0aeadc5810e0d47dccf0e3f07c571492ed32c","_cell_guid":"ea77e8d6-e18d-4ec4-a360-db447852e983","collapsed":true,"trusted":false},"cell_type":"code","source":"submission.to_csv('STahamtan_LeNet5_MNIST_Submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}