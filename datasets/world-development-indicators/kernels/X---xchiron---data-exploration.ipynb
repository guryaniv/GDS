{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sqlite3\nimport os\nimport re\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom gensim import corpora, models, similarities\noutput_notebook()\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\nconn = sqlite3.connect('../input/database.sqlite')\nc = conn.cursor()","execution_count":167,"outputs":[]},{"metadata":{"_uuid":"a56bfdc2a82d1167c5017e69270a0ae1665c56fb"},"cell_type":"markdown","source":"First, I want to take a look to see how many countries are in this dataset and also how many features are in this dataset."},{"metadata":{"trusted":true,"_uuid":"d44fc6060ca76c97bf1de5d8f35c145bc1d62a4a"},"cell_type":"code","source":"DataStats = pd.read_sql(\n                       \"\"\"\n                        SELECT\n                            Series.[Number of Indicies]\n                            ,Country.[Number of Countries]\n                        FROM (\n                            SELECT 1 [idx]\n                                ,count(*) [Number of Indicies]\n                                ,NULL [Number of Countries]\n                            FROM   Series  \n                        ) Series\n                        INNER JOIN (\n                            SELECT 1 [idx]\n                                ,NULL [Number of Indicies]\n                                ,count(*) [Number of Countries]\n                            FROM Country\n                        ) Country\n                            on Series.idx=Country.idx\n                       \"\"\", con=conn)\nprint(DataStats)","execution_count":168,"outputs":[]},{"metadata":{"_uuid":"618b2cf02e3d37622201d0c95dbf8bc333047257"},"cell_type":"markdown","source":"## Data Integrity Checks\nNext we'll check for data integrity.  By grouping on CountryCode and Year, we can determine the count of how many metrics are collected per year for each country.  This will help us narrow down how a histogram distribution of how complete our data is.  I included a filter of 1990 as reviewing the SeriesNotes database, quite a lot of metrics before then was interpolated using data"},{"metadata":{"_uuid":"476a3b54f693c197e3a4696b00ceef85c08eaeed"},"cell_type":"markdown","source":"### Metrics Per Year\nFirst breakdown is number of metrics per year.  This will allow us to see the distribution of the number of metrics for every year and country that was gathered.  This will help us define how consistent the data collection process was that World Bank undertook.  From here, we can see a pretty clear peak around 550 metrics so finding the year where those metrics are collected might be the best set to do time series analysis on.  We also see a large peak close to zero as some metrics may be collected on a 5 year interval while others are collected on a 1 year interval.  This probably explains the significant amount of missing data."},{"metadata":{"trusted":true,"_uuid":"2e235580af63a58307e5a2bbfbd6500a1dded419"},"cell_type":"code","source":"metricsPerYear = pd.read_sql(\n                       \"\"\"\n                        SELECT CountryName\n                            ,count(IndicatorCode) [metricsPerYear]\n                        FROM   Indicators\n                        GROUP BY CountryCode, Year                  \n                       \"\"\", con=conn)\nmetricsPerYear.hist(column='metricsPerYear', bins=50)","execution_count":169,"outputs":[]},{"metadata":{"_uuid":"cf9a8074ee95691775927e0c1e2e3c3e393e0fa0"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"faa884400f4187149bef79adb76086f66be324c6"},"cell_type":"markdown","source":"### Metrics per country\nNext we'll look at how many metrics are collected per country.  This will help us define how many metrics to use to analyze a country.  Since the SQL query has ordered the country by count of metrics, we can just print the head or tail of the dataframe to determine which countries will have the most complete data data and which countries will have the least."},{"metadata":{"trusted":true,"_uuid":"b7894c8fcd3afab4e4479fa5e1514b39ce385e33"},"cell_type":"code","source":"metricsPerCountry = pd.read_sql(\n                       \"\"\"\n                        SELECT CountryName\n                            ,count(distinct(IndicatorCode)) [metricsPerCountry]\n                        FROM   Indicators\n                        GROUP BY CountryCode                  \n                        ORDER BY [metricsPerCountry] desc\n                       \"\"\", con=conn)\nmetricsPerCountry.hist(column='metricsPerCountry', bins=50)","execution_count":170,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"482821b67beb4b241c1fc54e6bc4ce004d1de5c2"},"cell_type":"code","source":"print(metricsPerCountry.head(20))","execution_count":171,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a2c45dbbb328603e2353db6801e41984adf0e51"},"cell_type":"code","source":"print(metricsPerCountry.tail(20))","execution_count":172,"outputs":[]},{"metadata":{"_uuid":"0fb87446fa72c68fd3934402cfa993039b550b22"},"cell_type":"markdown","source":"### Testing out the data\nI wanted to peak a little into the data to see what is provided in here so I ran a little plot of time series data on a specific metric and country.  This provided me with good results, showing that there's enough to plot and is easy enough to play around with."},{"metadata":{"trusted":true,"_uuid":"a17eff22274be971cb30bc43cd8428fb778ff2f2"},"cell_type":"code","source":"PlayAround = pd.read_sql(\n                       \"\"\"\n                       \n                        SELECT Ind.CountryName\n                            ,Ind.Year\n                            ,Ind.Value\n                            ,Ser.IndicatorName\n                        FROM   Indicators Ind\n                        INNER JOIN Series Ser\n                            on Ser.SeriesCode=Ind.IndicatorCode\n                        WHERE Ind.IndicatorCode = 'SM.POP.NETM'\n                        and Ind.CountryCode = 'NAC'\n                        \n                       \"\"\", con=conn)\nprint(PlayAround.head(5))\nplt.plot(PlayAround['Value'])","execution_count":173,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32e9599c9153e9d1393c7f2ce7a53dc3e0594fa8","collapsed":true},"cell_type":"markdown","source":"### Indicator Categorization Mapping\nUtilizing code from Krishna Ravikumar's notebook for [Choosing Topics To Explore...](https://www.kaggle.com/kmravikumar/choosing-topics-to-explore).  I can get a feel for the various types of metrics that will be collected from this dataset based on titles and create a mapping table using Stored Procedures so that when users select a topic to explore, they can be provided with a list of metrics associated with that topic.  This could also be easy to use as a machine learning tool to drill down into related indicators."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ffb0ce2a1166c97e3288495b66681f298f82903"},"cell_type":"code","source":"\nIndicatorSQLResults = pd.read_sql(\n                       \"\"\"\n                        SELECT IndicatorName\n                            ,IndicatorCode\n                        FROM   Indicators\n                       \"\"\", con=conn)\nIndicator_array =  IndicatorSQLResults[['IndicatorName','IndicatorCode']].drop_duplicates().values\n\nmodified_indicators = []\nunique_indicator_codes = []\nfor ele in Indicator_array:\n    indicator = ele[0]\n    indicator_code = ele[1].strip()\n    if indicator_code not in unique_indicator_codes:\n        # delete , ( ) from the IndicatorNames\n        new_indicator = re.sub('[,()]',\"\",indicator).lower()\n        # replace - with \"to\" and make all words into lower case\n        new_indicator = re.sub('-',\" to \",new_indicator).lower()\n        modified_indicators.append([new_indicator,indicator_code])\n        unique_indicator_codes.append(indicator_code)\n\nIndicators = pd.DataFrame(modified_indicators,columns=['IndicatorName','IndicatorCode'])\nIndicators = Indicators.drop_duplicates()\n\nkey_word_dict = {}\nkey_word_dict['Demography'] = ['population','birth','death','fertility','mortality','expectancy']\nkey_word_dict['Food'] = ['food','grain','nutrition','calories']\nkey_word_dict['Trade'] = ['trade','import','export','good','shipping','shipment']\nkey_word_dict['Health'] = ['health','desease','hospital','mortality','doctor']\nkey_word_dict['Economy'] = ['income','gdp','gni','deficit','budget','market','stock','bond','infrastructure']\nkey_word_dict['Energy'] = ['fuel','energy','power','emission','electric','electricity']\nkey_word_dict['Education'] = ['education','literacy']\nkey_word_dict['Employment'] =['employed','employment','umemployed','unemployment']\nkey_word_dict['Rural'] = ['rural','village']\nkey_word_dict['Urban'] = ['urban','city']","execution_count":174,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a5db6b894af14d1262fe6fbae2fe4f94d1ebbe0","scrolled":true},"cell_type":"code","source":"feature = 'Food'\nfor indicator_ele in Indicators.values:\n    for ele in key_word_dict[feature]:\n        word_list = indicator_ele[0].split()\n        if ele in word_list or ele+'s' in word_list:\n            print(indicator_ele)\n            break","execution_count":175,"outputs":[]},{"metadata":{"_uuid":"38eafc8139606a665f39d905eb4fabc65ece0cfd"},"cell_type":"markdown","source":"### Latent Semantic Analysis of Metrics\nKrishna Ravikumar's notebook is really helpful in getting the initial analysis started.  However, I wanted to drill down further into each category to see if we can get more metric concepts related to a category without direct key word searches.  To do so, I will need to train the set of metrics using LSI and Cosine similarity to define document similarity to a specific search term."},{"metadata":{"trusted":true,"_uuid":"d41b22d9951c21dc88dd231ad783c04d04a5a766"},"cell_type":"code","source":"# set a list of stop words to remove from the corpus, can always add to this list manually\nstoplist = set('for a of the and to in [ on from per'.split())\n\n#break down each line to a comma delimited list of words\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] \n         for document in Indicators.values]\n\n#generate a word frequency count\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\n#filters out words that only show up once in the set of corpuses\ntexts = [[token for token in text if frequency[token] > 1]\n         for text in texts]\n\n# pretty-printer\nfrom pprint import pprint  \npprint(texts[:5])\n\n","execution_count":176,"outputs":[]},{"metadata":{"_uuid":"94086f1f43e27be5b71d2b6d77ee3219378bce3a"},"cell_type":"markdown","source":"From filtering down by finding any words that only appear once, we can also identify the most frequently used words to see if any of them does not belong and add them to our stop-word list."},{"metadata":{"trusted":true,"_uuid":"0f71c4f0669d25decd3aad186a7142e13eb5fcda"},"cell_type":"code","source":"import operator\nsorted_X = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True)\nsorted_X[:20]","execution_count":177,"outputs":[]},{"metadata":{"_uuid":"a25450d63053ade7e15e45a3c1a8f049f8c308ee"},"cell_type":"markdown","source":"Now we can build out the dictionary that will be used for LSI and see which dictionary value is associated with each key"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ceda690866726b6a8a4ff397bd3e71994593225c"},"cell_type":"code","source":"dictionary = corpora.Dictionary(texts)\nprint(dictionary)","execution_count":178,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c983c25db52dffc1a74c58df1a91edc06469f4ef"},"cell_type":"code","source":"#use this to check the id of specific words in the dictionary\ndictionary.token2id['working']","execution_count":179,"outputs":[]},{"metadata":{"_uuid":"5ea7c3c7cddc9f1bee27cac7fcced87c55b808c3"},"cell_type":"markdown","source":"Printing out the indexing, we can compare it side by side with the document to see the dictionary value being stored into this matrix."},{"metadata":{"trusted":true,"_uuid":"ef4d30bdf501f0bb8df1588292089cffd40484f5"},"cell_type":"code","source":"corpus = [dictionary.doc2bow(text) for text in texts]\npprint(corpus[:10])","execution_count":180,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93063a34e13a78bfadad288aea74e061bd730fed"},"cell_type":"code","source":"pprint(Indicators.values[:10])","execution_count":181,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ed63616aa648438e86b05f463146a7a48465eeb"},"cell_type":"markdown","source":"Now we start the LSI modling of our data.  First we'll generate the model off of our corpus."},{"metadata":{"trusted":true,"_uuid":"d3289360bef9055626f22dba3aad069de5508fb3","collapsed":true},"cell_type":"code","source":"from gensim.test.utils import common_dictionary\nfrom gensim.models import LsiModel\n\nmodel = LsiModel(corpus,id2word=dictionary)\nvectorized_corpus = model[corpus]","execution_count":182,"outputs":[]},{"metadata":{"_uuid":"89f6cb03c0bc6ba134223c34ca727dc6de8220be"},"cell_type":"markdown","source":"Next we test this on a specific key word to see how well it performs."},{"metadata":{"trusted":true,"_uuid":"9f98b66a620baab47fcb4716d4b45906f9eb47b8"},"cell_type":"code","source":"#key_word_dict['Demography'] = ['population','birth','death','fertility','mortality','expectancy']\n#key_word_dict['Food'] = ['food','grain','nutrition','calories']\n#key_word_dict['Trade'] = ['trade','import','export','good','shipping','shipment']\n#key_word_dict['Health'] = ['health','desease','hospital','mortality','doctor']\n#key_word_dict['Economy'] = ['income','gdp','gni','deficit','budget','market','stock','bond','infrastructure']\n#key_word_dict['Energy'] = ['fuel','energy','power','emission','electric','electricity']\n#key_word_dict['Education'] = ['education','literacy']\n#key_word_dict['Employment'] =['employed','employment','umemployed','unemployment']\n#key_word_dict['Rural'] = ['rural','village']\n#key_word_dict['Urban'] = ['urban','city']\n\ndoc = \"Trade\"\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lsi=model[vec_bow]\nindex = similarities.MatrixSimilarity(model[corpus])\nsims = index[vec_lsi]\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\npprint(sims[:10])","execution_count":206,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21dcf886009eb240e1eae1683bf9ff1fd81ec13f"},"cell_type":"code","source":"ct=0\nfor val in sims:\n    if val[1] > 0.1:\n        pprint(Indicators.values[val[0]][0])\n        ct=ct+1\n        if ct > 30:\n            break","execution_count":207,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}