{"cells":[{"metadata":{"_cell_guid":"53b2f44e-fc9f-61d8-80d8-5bc03993c6bd","_uuid":"4b7f67e2d97dd7cbbe6ba443864cabff45d6aa52"},"cell_type":"markdown","source":"In this notebook I'm practicing feature selection with lasso, and ridge regression.  \n\nI've been anxious to test these techniques on a new data set so here it goes.  \n\nI've also been told using Lasso to show which features in your data area most important can be very effective for a number of work situations.  I'll visualize this toward the end where one can clearly see the best variable to predict price.  \n\nWe'll start and use Sci-kit learn.  "},{"metadata":{"_cell_guid":"2312a211-8ef3-9d54-b2a1-493b7465e103","_uuid":"4590abe3e245e740e25a991108cc6a5337597ade","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n\n\nkc = pd.read_csv(\"../input/kc_house_data.csv\")\nprint(kc.head())\nprint(kc.shape)\nprint(kc.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04482262-ac19-cde9-2d4b-f1b7c49a4dc4","_uuid":"7a974d5367a06ac9f559fdd4400797f6559330f6","trusted":true,"collapsed":true},"cell_type":"code","source":"# Dropping a few columns.  Waterfront and view are binary. \nkc = kc.drop(['id', 'date', 'view', 'waterfront'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"515e4469-382a-11c4-6a92-6dd3fa5f2c51","_uuid":"7f52750e2a79ef9c39fc7508a4b1cf43a9743b3e","trusted":true},"cell_type":"code","source":"p = sns.pairplot(kc[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], hue='bedrooms', palette='afmhot',size=1.4)\np.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3034cdc-6ba6-18c4-2cc5-833f886fb384","_uuid":"84f700e4237bd799b91b86848ab28ad2d3ab7d5c","trusted":true,"collapsed":true},"cell_type":"code","source":"# extract our target variable -- House prices -- into an array \ny = kc.price.values\ny # is an array with the price variable \n\n# Drop price from the house dataframe \nkc = kc.drop(['price'], axis=1)\n\n# Create a matrix from the remaining data\nX = kc.as_matrix()\n\n# Store the column/feature names into a list \"colnames\"\ncolnames = kc.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55b53b5a-a36e-124c-1fe5-b999ed7ada9d","_uuid":"1f882894ca54431a657f9f51082094d9a7fe2d5f"},"cell_type":"markdown","source":"The LASSO (Least Absolute Shrinkage and Selection Operator) is a regression method that involves penalizing the absolute size of the regression coefficients.\n\nBy penalizing or constraining the sum of the absolute values of the estimates you make some of your coefficients zero. The larger the penalty applied, the further estimates are shrunk towards zero.  This is convenient when we want some automatic feature/variable selection, or when dealing with highly correlated predictors, where standard regression will usually have regression coefficients that are too large.  "},{"metadata":{"_cell_guid":"ad418bbb-be52-9d8d-b56d-b31413b7e73a","_uuid":"d997ae399ceab1861bf4d0536f6d8a0cff8b1d97","trusted":true},"cell_type":"code","source":"# create a lasso regressor\nlasso = Lasso(alpha=0.2, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X,y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e9c063e-467f-3d1d-84de-348a467af1a2","_uuid":"0bdd734fd0d29eb8bd2b5b1ad87aa4a1cce0b407","trusted":true},"cell_type":"code","source":"# Plot the coefficients\nplt.plot(range(len(colnames)), lasso_coef)\nplt.xticks(range(len(colnames)), colnames.values, rotation=60) \nplt.margins(0.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a843a2d1-4edc-fcc9-9d53-d49603f4168f","_uuid":"347fed40496c9cb3a8a8f80ca7af13e088ee651d"},"cell_type":"markdown","source":"This would be a great chart to show your boss or someone that's not very familiar with statistics.  It clearly shows that latitude is the most important variable terms of predicting price.  What's that real estate phrase that comes to mind here....\"Location, location, location.\""},{"metadata":{"_cell_guid":"3392ab08-b4f5-036f-4026-cd4313129033","_uuid":"807fb7f7f92032dfdd9575ccfbc7da8efbb198d5"},"cell_type":"markdown","source":"Now below, we will perform cross-validation on the data set and get our scores.  This might be more appropriate if we had a test/train set and were predicting.  However, having the R square gives us some context and helps going into the next cell where we are looking at how alpha changes our R square.  "},{"metadata":{"_cell_guid":"182b20c6-4fcc-d532-7822-017f7d5b5d9b","_uuid":"4fe3a273d777fabecc481d57151590fd26839923","trusted":true},"cell_type":"code","source":"# Import the necessary module\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\n# find the mean of our cv scores here\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d41b4b12-2df4-c102-5b0a-3adac7ed65bb","_uuid":"616cdef6fc7729992feeafc40b27ec570cde698f"},"cell_type":"markdown","source":"Below we'll run a ridge regression and see how score varies with different alphas.  This will show how picking a different alpha score changes the R2.  "},{"metadata":{"_cell_guid":"33a28327-83d1-8592-e9de-05a8c2a42de0","_uuid":"33e2a9d181a60400dcbf81937074baa33a4f0778","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\n# Create an array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Use this function to create a plot    \ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e180c97-61bf-055f-3e7c-9d6df5b22525","_uuid":"b5313b30273a91aa7a91963aac2f3e669c762b1a","trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"13ef45bd628414305fc2cbc65c6ff13242e70ace"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_change_revision":0,"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"_is_fork":false},"nbformat":4,"nbformat_minor":1}