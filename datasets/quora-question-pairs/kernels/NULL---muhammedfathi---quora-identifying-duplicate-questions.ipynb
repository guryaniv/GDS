{"cells":[{"metadata":{"_uuid":"bc66338dfe20a388eb086ddd2604dc7a335d314a"},"cell_type":"markdown","source":"## This version updated by adding a text cleaning section, this pushes score a little bit"},{"metadata":{"_uuid":"fd4379aba1f58adf96d2ba36548f066bc33bf1ff"},"cell_type":"markdown","source":"# Quora Question-pair \n\n\nThis competition is about modelling whether a pair of questions on Quora is asking the same question. For this problem we have about **400.000** training examples. Each row consists of two sentences and a binary label that indicates to us whether the two questions were the same or not.\n\nInspired by this nice [kernel](https://www.kaggle.com/arthurtok/d/mcdonalds/nutrition-facts/super-sized-we-macdonald-s-nutritional-metrics) from [Anisotropic](https://www.kaggle.com/arthurtok) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\n\npal = sns.color_palette()\n\nprint('# File sizes')\nfor f in os.listdir('../input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"It's worth noting that there is a lot more testing data than training data. This could be a sign that some of the test data is dummy data designed to deter hand-labelling, and not included in the calculations, like we recently saw in the [DSTL competition](https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/leaderboard).\n\nLet's open up one of the datasets."},{"metadata":{"_uuid":"76ecc0081150a93c0acddd77bbeb4baceba91b52"},"cell_type":"markdown","source":"## Training set"},{"metadata":{"trusted":true,"_uuid":"024184f43c21f336c865ef8e3a882ba04c8a2f82"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv').fillna(\"\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccfe861781a1cfd7266db301c971bbcf332a2fb8"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52e206684a270571e86f49d0ac442ff3075fa35b"},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04840290d8ef27ebd457d92032878f07dcf7ac63"},"cell_type":"markdown","source":"We are given a minimal number of data fields here, consisting of:\n\n**`id`:** Looks like a simple rowID    \n**`qid{1, 2}`:** The unique ID of each question in the pair    \n**`question{1, 2}`:** The actual textual contents of the questions.    \n**`is_duplicate`:** The **label** that we are trying to predict - whether the two questions are duplicates of each other."},{"metadata":{"_uuid":"f1ff308c9fc39211aaaa7de943795657500c95e2"},"cell_type":"markdown","source":"### Test data"},{"metadata":{"trusted":true,"_uuid":"b5d34682b02eca75c52724cec63a05dbb488e898"},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/test.csv\")[:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"458ac2003aadc0b28838b5ede97d0832b5337e0c"},"cell_type":"markdown","source":"## Cleaning the Text\n\nthis part is Inspired by this  [kernel](https://www.kaggle.com/muhammedfathi/the-importance-of-cleaning-text/edit) "},{"metadata":{"trusted":true,"_uuid":"b9a5430670603b68c2d1be39de180417a6dfdb87"},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0696481d6ffcd65db68609c96305f05eae2f601f"},"cell_type":"code","source":"# Check for any null values\nprint(df_train.isnull().sum())\nprint(df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d26302d913bf272a6ca7c63eb205e563f994c108"},"cell_type":"code","source":"# Add the string 'empty' to empty strings\ndf_train = df_train.fillna('empty')\ndf_test = df_test.fillna('empty')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69ead9ba437894a1f736108feb32ab7e039d85c3"},"cell_type":"code","source":"# Preview some of the pairs of questions\na = 0 \nfor i in range(a,a+10):\n    print(df_train.question1[i])\n    print(df_test.question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d69535a3ce4b396a76d07f18abebe0c49ea2fbd"},"cell_type":"code","source":"stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac86e390492ef4c419ce982f9c05bffe9daf1817"},"cell_type":"code","source":"def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" America \", text)\n    text = re.sub(r\" USA \", \" America \", text)\n    text = re.sub(r\" u s \", \" America \", text)\n    text = re.sub(r\" uk \", \" England \", text)\n    text = re.sub(r\" UK \", \" England \", text)\n    text = re.sub(r\"india\", \"India\", text)\n    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n    text = re.sub(r\"china\", \"China\", text)\n    text = re.sub(r\"chinese\", \"Chinese\", text) \n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\"quora\", \"Quora\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\"KMs\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text) \n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iPhone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"gps\", \"GPS\", text)\n    text = re.sub(r\"gst\", \"GST\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"dna\", \"DNA\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the US\", \"America\", text)\n    text = re.sub(r\"Astrology\", \"astrology\", text)\n    text = re.sub(r\"Method\", \"method\", text)\n    text = re.sub(r\"Find\", \"find\", text) \n    text = re.sub(r\"banglore\", \"Banglore\", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    \n    # Remove punctuation from text\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n#     # Optionally, shorten words to their stems\n#     if stem_words:\n#         text = text.split()\n#         stemmer = SnowballStemmer('english')\n#         stemmed_words = [stemmer.stem(word) for word in text]\n#         text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9725ce6f7a8beea641219a03681dab64eefdf8b"},"cell_type":"code","source":"def process_questions(question_list, questions, question_list_name, dataframe):\n    '''transform questions and display progress'''\n    for question in questions:\n        question_list.append(text_to_wordlist(question))\n        if len(question_list) % 100000 == 0:\n            progress = len(question_list)/len(dataframe) * 100\n            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60c38630ca44c107ce05f9dda8e712e65f2acbd7"},"cell_type":"code","source":"train_question1 = []\nprocess_questions(train_question1, df_train.question1, 'train_question1', df_train)\n\ntrain_question2 = []\nprocess_questions(train_question2, df_train.question2, 'train_question2', df_train)\n\ntest_question1 = []\nprocess_questions(test_question1, df_test.question1, 'test_question1', df_test)\n\ntest_question2 = []\nprocess_questions(test_question2, df_test.question2, 'test_question2', df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96e12bf8eb0de76ef5d5fad764d28ea3c352f624"},"cell_type":"code","source":"df_train['question1'] = train_question1\ndf_train['question2'] = train_question2\ndf_test['question1'] = test_question1\ndf_test['question2'] = test_question2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35b6880a0a36a0f9736a7d5a7cc376a9609eb8f4"},"cell_type":"code","source":"# Preview some transformed pairs of questions\na = 0 \nfor i in range(a,a+10):\n    print(train_question1[i])\n    print(train_question2[i])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"485ac62dca9ff8b969657bd4d5c0dcf984a3995e"},"cell_type":"code","source":"df_train.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e997aed96797b67d0059784fe7499b65873f194d","scrolled":true},"cell_type":"code","source":"print('Total numberof questions pairs for training: {}'.format(len(df_train)))\nprint(\"Duplicate pairs: {}%\".format(round(df_train['is_duplicate'].mean()*100 , 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of questions appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"363f30ef74fb45b543f1e6e1336f50264d6bbcc5"},"cell_type":"markdown","source":"6     In terms of questions, everything looks as I would expect here. Most questions only appear a few times, with very few questions appearing several times (and a few questions appearing many times). One question appears more than 160 times, but this is an outlier."},{"metadata":{"_uuid":"0a4fe17560ae12f20fd25135b44b3a16d6838e26"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"a88f2b8aaed2d65b481fc79cedaeed1e056cdf9f"},"cell_type":"markdown","source":"# Feature construction\n\nWe will now construct a basic set of features that we will later use to embed our samples with.\n\nThe first we will be looking at is rather standard TF-IDF encoding for each of the questions. In order to limit the computational complexity and storage requirements we will only encode the top terms across all documents with TF-IDF and also look at a subsample of the data."},{"metadata":{"_uuid":"727412883fd25d6a85d19fc1631770e5311e23bd","trusted":true},"cell_type":"code","source":"dfs = df_train[0:2500]\ndfs.groupby('is_duplicate')['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e0cc2cf00409c74b71b4c2dae487180f46d861a"},"cell_type":"markdown","source":"\nThe subsample still has a very similar label distribution, ok to continue like that, without taking a deeper look how to achieve better sampling than just taking the first rows of the dataset.\n\nCreate a dataframe where the top 50% of rows have only question 1 and the bottom 50% have only question 2, same ordering per halve as in the original dataframe."},{"metadata":{"trusted":true,"_uuid":"50eeaa8fdf97238cf1ba214d22b95d3dda6f9d7f"},"cell_type":"code","source":"dfq1, dfq2 = dfs[['qid1', 'question1']] , dfs[['qid2', 'question2']]\ndfq1.columns = ['qid1', 'question']\ndfq2.columns = ['qid2', 'question']\n\ndfqa = pd.concat((dfq1, dfq2), axis=0).fillna(\"\")\nnrowa_for_q1 = dfqa.shape[0] / 2\ndfqa.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4afa3e486687fc2221c506efc73dac5415d0b186"},"cell_type":"markdown","source":"Transform questions by TF-IDF."},{"metadata":{"trusted":true,"_uuid":"61469d368f0acc0e592a28f5ffd230d7225d1beb"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n\nmq1 = TfidfVectorizer(max_features = 256).fit_transform(dfqa['question'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13da12bbead7785ec359b4f4303b2754537c6784"},"cell_type":"markdown","source":"Since we are looking at pairs of data, we will be taking the difference of all question one and question two pairs with this. This will result in a matrix that again has the same number of rows as the subsampled data and one vector that describes the relationship between the two questions."},{"metadata":{"trusted":true,"_uuid":"5710969e561e1264c1d73430592a8fe3570ca9ee"},"cell_type":"code","source":"diff_ecoding = np.abs(mq1[::2] - mq1[1::2])\ndiff_ecoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f0be9711a9607b32dadf8b67b9e9a5340e3866d"},"cell_type":"markdown","source":"# 3D t-SNE embedding\n\nWe will use t-SNE to embed the TF-IDF vectors in three dimensions and create an interactive scatter plot with them."},{"metadata":{"trusted":true,"_uuid":"fec5b09a0893e1b8d3ad81f379b5677a399c0ee9"},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(\n    n_components=3,\n    init='random',\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2,\n    angle=0.5\n).fit_transform(diff_ecoding.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a580809993386d94478a53a2b4a42ebaa7ebf335"},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = dfs['is_duplicate'].values,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='test')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e20791013b717bc9ab9bbcbd42cab66e53cee8c"},"cell_type":"markdown","source":"That three dimensional embedding looks nice, but is not telling us much about the structure of the space that we created. There seem to be no clusters of either class present, so let's go on to the next section."},{"metadata":{"_uuid":"cc16c6b615467eb5e463f1de1fcbe1011c370976"},"cell_type":"markdown","source":"## Feature EDA\n\nLet us now construct a few features\n\n* character length of questions 1 and 2\n* number of words in question 1 and 2\n* normalized word share count.\n\nWe can then have a look at how well each of these separate the two classes."},{"metadata":{"trusted":true,"_uuid":"870c5067737abcc2cc02a822cadcd0af035177a8"},"cell_type":"code","source":"df_train['q1len'] = df_train['question1'].str.len()\ndf_train['q2len'] = df_train['question2'].str.len()\n\ndf_train['q1_n_words'] = df_train['question1'].apply(lambda row: len(row.split(\" \")))\ndf_train['q2_n_words'] = df_train['question2'].apply(lambda row: len(row.split(\" \")))\n\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip() , row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip() , row['question2'].split(\" \")))\n    # len(w1 & w2) length of words that shared in two questions\n    # (len(w1) + len(w2)) length of words of two questions\n    return 1.0 * len(w1 & w2) / (len(w1) + len(w2))\n\ndf_train['word_share'] = df_train.apply(normalized_word_share, axis=1)\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"019a10e2477ba840602c9e38e5b1843f440c0926"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = \"is_duplicate\", y = 'word_share', data = df_train[0:50000])\nplt.subplot(1,2,2)\nsns.distplot(df_train[df_train['is_duplicate'] == 1.0]['word_share'][0:10000], color = 'green')\nsns.distplot(df_train[df_train['is_duplicate'] == 0.0]['word_share'][0:10000], color = 'red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d22006b95f3d8542e96d12c2046c56f67d990ade"},"cell_type":"markdown","source":"The distributions for normalized word share have some overlap on the far right hand side, meaning there are quite a lot of questions with high word similarity but are both duplicates and non-duplicates."},{"metadata":{"trusted":true,"_uuid":"2c4f790e1242795da9ff34dfac55e4ac213972b8"},"cell_type":"code","source":"df_subsampled = df_train[0:2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5962c7ad3e754bfe4f4ec2c725c8a22f563f425"},"cell_type":"code","source":"\n\ntrace = go.Scatter(\n    y = df_subsampled['q2len'].values,\n    x = df_subsampled['q1len'].values,\n    mode='markers',\n    marker=dict(\n        size= df_subsampled['word_share'].values * 60,\n        color = df_subsampled['is_duplicate'].values,\n        colorscale = 'Portland',\n        showscale = True,\n        opacity=0.5,\n        colorbar = dict(title = 'duplicate')\n    ),\n    text = np.round(df_subsampled['word_share'].values, decimals=2)\n)\n\ndata = [trace]\nlayout= go.Layout(\n    autosize= True,\n    title= 'Scatter plot of character lengths of question one and two',\n    hovermode= 'closest',\n        xaxis=dict(\n        showgrid=False,\n        zeroline=False,\n        showline=False\n    ),\n    yaxis=dict(\n        title= 'Question 2 length',\n        ticklen= 5,\n        gridwidth= 2,\n        showgrid=False,\n        zeroline=False,\n        showline=False,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatterWords')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17ea91c34c701c74abb1c17189be1f9768d88be6"},"cell_type":"markdown","source":"Scatter plot of question pair character lengths where color indicates duplicates and the size the word share coefficient we've calculated earlier."},{"metadata":{"_uuid":"33d9ccfd7b7fa274ea1d54d040c148be878617f8"},"cell_type":"markdown","source":"# Animation over average number of words\n\nFor that we will calculate the average number of words in both questions for each row.\n\nIn the end we want to have a scatter plot, just like the one above, but giving us one more dimension, in that case the average number of words in both questions. That will allow us to see the dependence on that variable. We also expect that as the number of words is increased, the character lengths of Q1 and Q2 will increase."},{"metadata":{"trusted":true,"_uuid":"17cca10869de093d4bf3ebb26631852b44821b6f"},"cell_type":"code","source":"from IPython.display import display, HTML\n\ndf_subsampled['q_n_words_avg'] = np.round((df_subsampled['q1_n_words'] + df_subsampled['q2_n_words'])/2.0).astype(int)\nprint(df_subsampled['q_n_words_avg'].max())\ndf_subsampled = df_subsampled[df_subsampled['q_n_words_avg'] < 20]\ndf_subsampled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e32345a3462f7ef567ce143e76ae290e1f8122a3"},"cell_type":"code","source":"word_lens = sorted(list(df_subsampled['q_n_words_avg'].unique()))\n# make figure\nfigure = {\n    'data': [],\n    'layout': {\n        'title': 'Scatter plot of char lenghts of Q1 and Q2 (size ~ word share similarity)',\n    },\n    'frames': []#,\n    #'config': {'scrollzoom': True}\n}\n\n# fill in most of layout\nfigure['layout']['xaxis'] = {'range': [0, 200], 'title': 'Q1 length'}\nfigure['layout']['yaxis'] = {\n    'range': [0, 200],\n    'title': 'Q2 length'#,\n    #'type': 'log'\n}\nfigure['layout']['hovermode'] = 'closest'\n\nfigure['layout']['updatemenus'] = [\n    {\n        'buttons': [\n            {\n                'args': [None, {'frame': {'duration': 300, 'redraw': False},\n                         'fromcurrent': True, 'transition': {'duration': 300, 'easing': 'quadratic-in-out'}}],\n                'label': 'Play',\n                'method': 'animate'\n            },\n            {\n                'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate',\n                'transition': {'duration': 0}}],\n                'label': 'Pause',\n                'method': 'animate'\n            }\n        ],\n        'direction': 'left',\n        'pad': {'r': 10, 't': 87},\n        'showactive': False,\n        'type': 'buttons',\n        'x': 0.1,\n        'xanchor': 'right',\n        'y': 0,\n        'yanchor': 'top'\n    }\n]\n\nsliders_dict = {\n    'active': 0,\n    'yanchor': 'top',\n    'xanchor': 'left',\n    'currentvalue': {\n        'font': {'size': 20},\n        'prefix': 'Avg. number of words in both questions:',\n        'visible': True,\n        'xanchor': 'right'\n    },\n    'transition': {'duration': 300, 'easing': 'cubic-in-out'},\n    'pad': {'b': 10, 't': 50},\n    'len': 0.9,\n    'x': 0.1,\n    'y': 0,\n    'steps': []\n}\n\n# make data\nword_len = word_lens[0]\ndff = df_subsampled[df_subsampled['q_n_words_avg'] == word_len]\ndata_dict = {\n    'x': list(dff['q1len']),\n    'y': list(dff['q2len']),\n    'mode': 'markers',\n    'text': list(dff['is_duplicate']),\n    'marker': {\n        'sizemode': 'area',\n        #'sizeref': 200000,\n        'colorscale': 'Portland',\n        'size': dff['word_share'].values * 120,\n        'color': dff['is_duplicate'].values,\n        'colorbar': dict(title = 'duplicate')\n    },\n    'name': 'some name'\n}\nfigure['data'].append(data_dict)\n\n# make frames\nfor word_len in word_lens:\n    frame = {'data': [], 'name': str(word_len)}\n    dff = df_subsampled[df_subsampled['q_n_words_avg'] == word_len]\n\n    data_dict = {\n        'x': list(dff['q1len']),\n        'y': list(dff['q2len']),\n        'mode': 'markers',\n        'text': list(dff['is_duplicate']),\n        'marker': {\n            'sizemode': 'area',\n            #'sizeref': 200000,\n            'size': dff['word_share'].values * 120,\n            'colorscale': 'Portland',\n            'color': dff['is_duplicate'].values,\n            'colorbar': dict(title = 'duplicate')\n        },\n        'name': 'some name'\n    }\n    frame['data'].append(data_dict)\n\n    figure['frames'].append(frame)\n    slider_step = {'args': [\n        [word_len],\n        {\n            'frame': {'duration': 300, 'redraw': False},\n            'mode': 'immediate',\n            'transition': {'duration': 300}\n        }\n     ],\n     'label': str(word_len),\n     'method': 'animate'}\n    sliders_dict['steps'].append(slider_step)\n\n    \nfigure['layout']['sliders'] = [sliders_dict]\n\npy.iplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"917d2c9abece204b6b7824010d2b5d5d801e6f6e"},"cell_type":"markdown","source":"What is interesting about that, is that as the number of words increases, the distribution of character lengths of the first and second question becomes less and less spherical."},{"metadata":{"_uuid":"93d55d7970d8555aea97b9e34dfed172779ff486"},"cell_type":"markdown","source":"# Embedding with engineered features\n\nWe will now revisit the t-SNE embedding with the manually engineered features.\n\nFor that we use the number of words in both questions, character lengths and their word share coefficient. t-SNE is sensitive to scaling of different dimensions and we want all of the dimensions to contribute equally to the distance measure that t-SNE is trying to preserve."},{"metadata":{"trusted":true,"_uuid":"d43ecd10086d15260c972c96b0fe2f4e7e648d22"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndf_subsampled = df_train[0:3000]\n\nX = MinMaxScaler().fit_transform(df_subsampled[['q1_n_words', 'q1len', 'q2_n_words', 'q2len', 'word_share']])\ny = df_subsampled['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71bb828af2bfdf600e7281d0c18f322b789c6565"},"cell_type":"code","source":"tsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=300,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e85adf72ceae16a849f34767a842f5fee7883f82"},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f58e753b63fcf4b3037093bed294f3858221acd6"},"cell_type":"markdown","source":"The embedding of the engineered features has much more structure than the previous one where we were only computing differences of TF-IDF encodings.\n\nIn the cluster of the negatives we have few positives whereas in the cluster of positives we have a lot more negatives. That matches our observation from the boxplot of word share coefficient above, where we could see that the negative class has a lot of overlap with the positive class for high word share coefficients."},{"metadata":{"_uuid":"31bffe250b0234a1ea19ef5a7ed1905a5c82a8e7"},"cell_type":"markdown","source":"# Parallel Coordinates\n\nWe now want to get another perspective on high dimensional data, such as the TF-IDF encoded questions. For that purpose I'll encode the concatenated questions into a set of N dimensions, s.t. each row in the dataframe then has one N dimensional vector associated to it.\nWith this we can then have a look at how these coordinates (or TF-IDF dimensions) vary by label.\n\nThere are many EDA methods to visualize high dimensional data, I'll show parallel coordinates here.\n\nTo make a nice looking plot, I've chosen N to be quite small, much smaller actually than you would encode it in a machine learning algorithm."},{"metadata":{"trusted":true,"_uuid":"ec71729d6a09f3afed5937116b68fd5593935b2c"},"cell_type":"code","source":"from pandas.tools.plotting import parallel_coordinates\n\ndf_subsampled = df_train[0:500]\n\nN = 64\n\n#encoded = HashingVectorizer(n_features = N).fit_transform(df_subsampled.apply(lambda row: row['question1']+' '+row['question2'], axis=1).values)\nencoded = TfidfVectorizer(max_features = N).fit_transform(df_subsampled.apply(lambda row: row['question1']+' '+row['question2'], axis=1).values)\n# generate columns in the dataframe for each of the 32 dimensions\ncols = ['hashed_'+str(i) for i in range(encoded.shape[1])]\nfor idx, col in enumerate(cols):\n    df_subsampled[col] = encoded[:,idx].toarray()\n\nplt.figure(figsize=(12,8))\nkws = {\n    'linewidth': 0.5,\n    'alpha': 0.7\n}\nparallel_coordinates(\n    df_subsampled[cols + ['is_duplicate']],\n    'is_duplicate',\n    axvlines=False, colormap=plt.get_cmap('plasma'),\n    **kws\n)\n#plt.grid(False)\nplt.xticks([])\nplt.xlabel(\"encoded question dimensions\")\nplt.ylabel(\"value of dimension\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72547d40616e7b7f8fe6f3d0f90061b041d06343"},"cell_type":"markdown","source":"In the parallel coordinates we can see that there are some dimensions that have high TF-IDF features values for duplicates and others high values for non-duplicates."},{"metadata":{"trusted":true,"_uuid":"c07a940512d7ed9c014b52cb801fb4c92ad18203"},"cell_type":"markdown","source":"# Question character length correlations by duplication label\n\nThe pairplot of character length of both questions by duplication label is showing us that, duplicated questions seem to have a somewhat similar amount of characters in them.\n\nAlso we can see something quite intuitive, that there is rather strong correlation in the number of words and the number of characters in a question."},{"metadata":{"trusted":true,"_uuid":"dd3745195e0056a572f9394c519d72efe90780fb"},"cell_type":"code","source":"n = 10000\nsns.pairplot(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'is_duplicate']][0:n], hue='is_duplicate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19e95e027d77fa197c89829cc4d3f983d4f22dab"},"cell_type":"markdown","source":"# Model starter\n\nTrain a model with the basic feature we've constructed so far.\n\nFor that we will use Logisitic regression, for which we will do a quick parameter search with CV, plot ROC and PR curve on the holdout set and finally generate a submission."},{"metadata":{"trusted":true,"_uuid":"2631a441f7e37b20c94558ee8ff3de6035433ab5"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler().fit(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\n\nX = scaler.transform(df_train[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\ny = df_train['is_duplicate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa853f4e7cd56baef99ea8a8efdaf360f24c1d61"},"cell_type":"markdown","source":"### Run cross-validation with a few hyper parameters."},{"metadata":{"trusted":true,"_uuid":"1df0b42333cb51320e329c1854100701acdabb10"},"cell_type":"code","source":"clf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75ff5cc9192e16966cf73f3c4dd5c1cffd9007d4"},"cell_type":"markdown","source":"Print validation results. Here we see that the strongly regularized model has much worse negative log loss than the other two models, regardless of which regularizer we've used."},{"metadata":{"trusted":true,"_uuid":"2dd8af0d0a0b16d3ecc8b669add2ebf52ce507a9"},"cell_type":"code","source":"for i in range(1, len(cv.cv_results_['params'])+1):\n    rank = cv.cv_results_['rank_test_score'][i-1]\n    s = cv.cv_results_['mean_test_score'][i-1]\n    sd = cv.cv_results_['std_test_score'][i-1]\n    params = cv.cv_results_['params'][i-1]\n    print(\"{0}. Mean validation neg log loss: {1:.3f} (std: {2:.3f}) - {3}\".format(\n        rank,\n        s,\n        sd,\n        params\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4425c6544f5b74a42bea2442b5bad7a6cb400f2b"},"cell_type":"code","source":"print(cv.best_params_)\nprint(cv.best_estimator_.coef_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2bb19f4e72f2d4d024a9c6b0c845b1eabd200b2"},"cell_type":"markdown","source":"### ROC\n\nReceiver operator characteristic, used very commonly to assess the quality of models for binary classification.\n\nWe will look at at three different classifiers here, a strongly regularized one and two with weaker regularization. The heavily regularized model has parameters very close to zero and is actually worse than if we would pick the labels for our holdout samples randomly."},{"metadata":{"trusted":true,"_uuid":"aaf8ecb5aa8e87b491af7ae60a832584d265f1fc"},"cell_type":"code","source":"colors = ['r', 'g', 'b', 'y', 'k', 'c', 'm', 'brown', 'r']\nlw = 1\nCs = [1e-6, 1e-4, 1e0]\n\nplt.figure(figsize=(12,8))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for different classifiers')\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\nlabels = []\nfor idx, C in enumerate(Cs):\n    clf = LogisticRegression(C = C)\n    clf.fit(X_train, y_train)\n    print(\"C: {}, parameters {} and intercept {}\".format(C, clf.coef_, clf.intercept_))\n    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=colors[idx])\n    labels.append(\"C: {}, AUC = {}\".format(C, np.round(roc_auc, 4)))\n\nplt.legend(['random AUC = 0.5'] + labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e514d43d4a490c66519643acc815dc137950c982"},"cell_type":"markdown","source":"# Precision-Recall Curve\n\nAlso used very commonly, but more often in cases where we have class-imbalance. We can see here, that there are a few positive samples that we can identify quite reliably. On in the medium and high recall regions we see that there are also positives samples that are harder to separate from the negatives."},{"metadata":{"trusted":true,"_uuid":"15798dbf5f05b0a3311e8b9748db5b8c098e7265"},"cell_type":"code","source":"pr, re, _ = precision_recall_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\nplt.figure(figsize=(12,8))\nplt.plot(re, pr)\nplt.title('PR Curve (AUC {})'.format(auc(re, pr)))\nplt.xlabel('Recall')\nplt.ylabel('Precision')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a58c8bd44f3ff2e497715a9e9dc324f5f1949758"},"cell_type":"markdown","source":"# Prepare submission\n\nHere we read the test data and apply the same transformations that we've used for the training data. We also need to scale the computed features again."},{"metadata":{"trusted":true,"_uuid":"42e5159f377f57d30c2138c2d9b51f75b126b297"},"cell_type":"code","source":"dftest = pd.read_csv(\"../input/test.csv\").fillna(\"\")\n\ndftest['q1len'] = dftest['question1'].str.len()\ndftest['q2len'] = dftest['question2'].str.len()\n\ndftest['q1_n_words'] = dftest['question1'].apply(lambda row: len(row.split(\" \")))\ndftest['q2_n_words'] = dftest['question2'].apply(lambda row: len(row.split(\" \")))\n\ndftest['word_share'] = dftest.apply(normalized_word_share, axis=1)\n\ndftest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0e214470dfa3112bd82c341b198d53e489b46d5"},"cell_type":"markdown","source":"We use the best estimator found by cross-validation and retrain it, using the best hyper parameters, on the whole training set."},{"metadata":{"trusted":true,"_uuid":"2b601cd1c01767ce7c01c054301a9dbdcb3646d9"},"cell_type":"code","source":"retrained = cv.best_estimator_.fit(X, y)\n\nX_submission = scaler.transform(dftest[['q1len', 'q2len', 'q1_n_words', 'q2_n_words', 'word_share']])\n\ny_submission = retrained.predict_proba(X_submission)[:,1]\n\nsubmission = pd.DataFrame({'test_id': dftest['test_id'], 'is_duplicate': y_submission})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0af2708a7f93bd462084e470f58933ea58c00bb5"},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd75ce7ff063ac3cf2de35d7c723a8f8b6a27403"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}