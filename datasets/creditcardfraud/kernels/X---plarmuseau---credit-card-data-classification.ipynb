{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"credit = pd.read_csv(\"../input/creditcard.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"87ee90cb-4101-4c0e-b10b-8ec4bcef86ce","_uuid":"6537a52058ca20a2453cd90cc46c04a2d04b099b","trusted":true},"cell_type":"code","source":"credit.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"60600e72-0d55-497e-9299-8f7ec5cb401c","_uuid":"bbf456c4d427d14fd25d132b189a56880d42adc1","trusted":true},"cell_type":"code","source":"credit.describe()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"83a96b0a-7a43-41c3-88f4-5afa370ea04e","_uuid":"365d3a095cc4719bb55fa3bd7541f170e0358d85","trusted":true},"cell_type":"code","source":"credit.info()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"b739ed86-e83c-4b11-877b-60d12bcd8927","_uuid":"01477572fd428b2e8e490f0ce7d2bb3cd74ee91a","collapsed":true,"trusted":true},"cell_type":"code","source":"def Klasseer(Mtrain,Mtest,Mlabel,klas,rank,start):\n    #data preparation\n    #print(Mtotal)\n    #Mtotal=Mtotal.fillna(-1)\n    #print(Mtotal)\n    #Mtrain=Mtotal[Mtotal[labelveld]!=-1]\n    #Mtest=Mtotal[Mtotal[labelveld]==-1]\n    #Mtest=Mtest.drop(labelveld,axis=1)\n    Mlabel=pd.DataFrame( Mlabel,columns=['label'] )  #[:len(Mtrain)]\n    #Mlabel=Mlabel.fillna(-1)  \n    labelveld='label'\n    print('shapes train',Mtrain.shape,'label',Mlabel.shape,'test',Mtest.shape)\n\n    \n    #totalA=Mtrain.append(Mtest)\n    totalA=np.concatenate((Mtrain,Mtest), axis=0)\n    predictionA=pd.DataFrame(Mlabel,columns=[labelveld])    \n    #totalA=totalA.drop(labelveld,axis=1)\n    #print(totalA.shape,predictionA.shape)\n    #print(prediction)\n    #faze 1\n    # dimmension reduction\n    from scipy.spatial.distance import cosine\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.decomposition import TruncatedSVD\n    from sklearn.preprocessing import Normalizer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, log_loss\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.svm import SVC, LinearSVC, NuSVC\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier,Perceptron\n\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    \n    for ira in range(rank-start,rank+1):\n        print('****20% sample test==',ira)\n        #Ulsa = lsa.fit_transform(Mtrain.values/255)  #train version\n        #print(total)\n        if ira!=0:\n            if ira<len(totalA.T):\n                print(\"lsa dimmension reduction\")                \n                svd = TruncatedSVD(ira)\n                normalizer = Normalizer(copy=False)\n                lsa = make_pipeline(svd, normalizer)\n                UlsA = lsa.fit_transform(totalA) #total version\n                explained_variance = svd.explained_variance_ratio_.sum()\n                print(\"Explained variance of the SVD step knowledge transfer: {}%\".format(\n                    int(explained_variance * 100)))            \n            else:\n                print(\"no reduction\")\n                UlsA=totalA\n        else:\n            print(\"3D-SVD dimmension reduction\")\n            u,s,vh=np.linalg.svd(totalA)\n            print(u.shape, s.shape, vh.shape)\n            UlsA=np.reshape(u, (len(totalA),28))            \n        #    UlsA = totalA\n        #    print(\"no LSA reduction\")\n        print('ulsa',UlsA.shape)\n\n\n        #faze2\n        #training model\n\n        #sample\n        samlen=int(len(Mlabel)/5)\n        X_train, X_test, y_train, y_test = train_test_split(UlsA[:samlen], Mlabel[:samlen],stratify=Mlabel[:samlen], test_size=0.25)\n        print(\"test on 20% sample\")\n        \n        if klas=='Logi':\n            classifiers = [\n    #    SVC(kernel=\"rbf\", C=0.025, probability=True),  20%\n    #    NuSVC(probability=True),\n                LogisticRegression(),\n                 ]\n        if klas=='Quad':\n            classifiers = [\n                QuadraticDiscriminantAnalysis(),\n                 ]           \n        if klas=='Rand':\n            classifiers = [\n                RandomForestClassifier(84),\n                 ]               \n        if klas=='Extr':\n            classifiers = [\n                ExtraTreesClassifier(verbose=1,n_jobs=3),\n                 ]             \n        if klas=='Adab':\n            classifiers = [\n                AdaBoostClassifier(),\n                 ]            \n        if klas=='Deci':\n            classifiers = [\n                DecisionTreeClassifier(),\n                 ]\n        if klas=='Grad':\n            classifiers = [\n                GradientBoostingClassifier(),\n                 ]            \n        if klas=='KNN':\n            classifiers = [\n                KNeighborsClassifier(n_jobs=4),  \n                 ]            \n        if klas=='Line':\n            classifiers = [\n                LinearDiscriminantAnalysis(), \n                 ]  \n        if klas=='Gaus':\n            classifiers = [\n                GaussianNB(),\n                 ] \n        if klas=='Perc':\n            classifiers = [\n                Perceptron(),\n                 ]      \n        if klas=='Elas':\n            classifiers = [\n                ElasticNet(random_state=0),\n                 ]                 \n    # Logging for Visual Comparison\n        log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n        log = pd.DataFrame(columns=log_cols)\n    \n        for clf in classifiers:\n            clf.fit(X_train,y_train)\n            name = clf.__class__.__name__\n        \n            print(\"=\"*30)\n            print(name)\n            \n            #print('****Results****')\n            train_predictions = clf.predict(X_test)\n            acc = accuracy_score(y_test, train_predictions)\n            print(\"Accuracy: {:.4%}\".format(acc))\n        \n            train_predictions = clf.predict_proba(X_test)\n            ll = log_loss(y_test, train_predictions)\n            print(\"Log Loss: {}\".format(ll))\n            \n            log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n            log = log.append(log_entry)\n    \n        print(\"=\"*30)\n\n    print('*** train complete set==',UlsA[:len(Mlabel)].shape)\n     \n    clf.fit(UlsA[:len(Mlabel)],Mlabel)\n    #on complete trainset\n\n    #pr2=pd.DataFrame(clf.predict_proba(Ulsa),index=list(range(0,len(Ulsa),1)))\n\n    predictionA=pd.DataFrame(clf.predict(UlsA),columns=['pred'],index=range(0,len(UlsA)))\n    predictionA[labelveld]=Mlabel \n    print('predict',predictionA.shape)\n    predictionA.fillna(-1)\n    predictionA['diff']=0\n    predictionA['next']=Mlabel\n    #abs(prediction[labelveld]-prediction['pred\n    collist=sorted( Mlabel.label.unique() )\n\n    print(collist)\n    if klas=='Logi':\n        predictionA[collist] = pd.DataFrame(clf.predict_log_proba(UlsA))\n    if klas!='Logi':\n        print(UlsA.shape)\n        temp=pd.DataFrame(clf.predict_proba(UlsA))\n        print(temp.shape)\n        predictionA[collist]=temp\n    \n    from sklearn.metrics import classification_report, confusion_matrix\n    true_labels=predictionA[labelveld][:len(Mtrain)].values.astype('float32')\n    predicted_labels = predictionA['pred'][:len(Mtrain)].values.astype('float32')\n\n    cm = confusion_matrix(true_labels, predicted_labels,labels=collist)\n    print(classification_report(true_labels, predicted_labels))\n    print(\"Confusion matrix\")\n    print(cm)\n    \n    corr=predictionA.drop(['pred','diff'],axis=1).corr()\n    f, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(abs(corr), mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)\n    predictionA=predictionA.fillna('0')\n    #print('Prediction',prediction.head())\n    pred2=predictionA.drop(['pred',labelveld,'diff','next'],axis=1)\n    \n    print(predictionA.shape)\n\n\n    return predictionA #['next']","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"9b4cafb0-aad7-449f-ad00-d34485aba1c6","_uuid":"427975a389292f4e58b5a0a3ca8bb7bd20c13267","trusted":true},"cell_type":"code","source":"no_of_normal_transcations = len(credit[credit['Class']==1])\nno_of_fraud_transcations = len(credit[credit['Class']==0])\nprint(\"no_of_fraud_transcations:\",no_of_normal_transcations)\nprint(\"no_of_OK_transcations:\", no_of_fraud_transcations)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"a4ecaf63d2df62da7e08b451c59a3b0fdc9f207e"},"cell_type":"markdown","source":"# Oversampling the fraud x 40"},{"metadata":{"_cell_guid":"7f5b1264-5fa8-449c-9226-b1460bfe2cbb","_uuid":"cdc85910aba1189d942a901b09d223763996e0ac","collapsed":true,"trusted":true},"cell_type":"code","source":"credit1=credit[credit['Class']==1]\nfor xi in range(40):\n    credit=credit.append(credit1)\nX = credit.iloc[:, 1:29].values\ny = credit.iloc[:, 30].values","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe6f9bdb5e2eae0314a5f070dde0d0ed2b3abbfe"},"cell_type":"code","source":"no_of_normal_transcations = len(credit[credit['Class']==1])\nno_of_fraud_transcations = len(credit[credit['Class']==0])\nprint(\"no_of_fraud_transcations:\",no_of_normal_transcations)\nprint(\"no_of_OK_transcations:\", no_of_fraud_transcations)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"1e30717b-faa3-485b-8c5e-ba2c8f4a25b2","_uuid":"a122766826afe656a0cf8647eea9c9681e04da29","trusted":true},"cell_type":"code","source":"Klasseer(X,X[:1000],y,'KNN',22,0) \n","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"138a4eaa069590f4300ee629a9661db67424f90c"},"cell_type":"markdown","source":"# Perfect"},{"metadata":{"_cell_guid":"55cb6581-ff98-43a7-b2c5-3eb48e1328c4","_uuid":"ddfc17d4d66c15e680b1913cb1e5e5aed14010c7","scrolled":true,"trusted":true},"cell_type":"code","source":"pred=Klasseer(X,X[:300],y,'Extr',28,0) ","execution_count":15,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}