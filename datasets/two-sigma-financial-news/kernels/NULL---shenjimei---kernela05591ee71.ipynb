{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom kaggle.competitions import twosigmanews","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"env = twosigmanews.make_env()\nmarket_train, news_train = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b9cf23c635fd051ed8794b9557110a8439b579"},"cell_type":"code","source":"start = datetime(2013, 1, 1, 0, 0, 0).date()\nmarket_train = market_train.loc[market_train['time'].dt.date >= start].reset_index(drop=True)\nnews_train = news_train.loc[news_train['time'].dt.date >= start].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c6c116a4e5c8727a98dc27a9a49b13355d86f1b"},"cell_type":"code","source":"market_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbe418bc6dfa73c723d2fc18b6230e0c0fa76fa1"},"cell_type":"code","source":"news_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad2198932776a5a8b747bc8cf109ae35d2740862"},"cell_type":"code","source":"def preprocess_news(news_train):\n    drop_list = [\n        'headline','sourceTimestamp','firstCreated','subjects','audiences','assetName'\n    ]\n    for col in ['headlineTag','provider','sourceId']:\n        news_train[col], uniques = pd.factorize(news_train[col])\n        del uniques\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\nnews_train = preprocess_news(news_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62f9106bb3ea034879c7bc51f98a5e3312610fb6"},"cell_type":"code","source":"def unstack_asset_codes(news_train):\n    codes = []\n    indexes = []\n    for i, values in news_train['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train)\nindex_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5447e9a1d2c093b5b07c97a25f79b3d2c83a5523"},"cell_type":"code","source":"def unstack_asset_codes(news_train):\n    codes = []\n    indexes = []\n    for i, values in news_train['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train)\nindex_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ccc2721cf3e007783e472e06d9f912ab7f531f5"},"cell_type":"code","source":"def merge_news_on_index(news_train, index_df):\n    news_train['news_index'] = news_train.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train, index_df)\ndel news_train, index_df\ngc.collect()\nnews_unstack.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee9c7d6ffbd383d5e4df7394737bf42f9826249"},"cell_type":"code","source":"def group_news(news_frame):\n    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack; gc.collect()\nnews_agg.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e1ba6b055213395df291ed54ba6161229392b5c"},"cell_type":"code","source":"market_train['date'] = market_train.time.dt.date\ndf = market_train.merge(news_agg, how='left', on=['assetCode', 'date'])\ndel market_train, news_agg\ngc.collect()\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8093794d0d868c6260f9bc14e3f3778a2944a6a7"},"cell_type":"code","source":"def custom_metric(date, pred_proba, num_target, universe):\n    y = pred_proba*2 - 1\n    r = num_target.clip(-1,1) # get rid of outliers\n    x = y * r * universe\n    result = pd.DataFrame({'day' : date, 'x' : x})\n    x_t = result.groupby('day').sum().values\n    return np.mean(x_t) / np.std(x_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29ca9db020dfdb2f1431f6e8fe1e6fc11c381d88","scrolled":true},"cell_type":"code","source":"date = df.date\nnum_target = df.returnsOpenNextMktres10.astype('float32')\nbin_target = (df.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = df.universe.astype('int8')\n# Drop columns that are not features\ndf.drop(['returnsOpenNextMktres10', 'date', 'universe', 'assetCode', 'assetName', 'time'], \n        axis=1, inplace=True)\ndf = df.astype('float32')  # Set all remaining columns to float32 datatype\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cf1c58f2b8d6ae643f3fb8b392cd4502b94f0b4"},"cell_type":"code","source":"train_index, test_index = train_test_split(df.index.values, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4089ee3f8260a4c5e985c42fde81ff25431845b1"},"cell_type":"code","source":"def evaluate_model(df, target, train_index, test_index, params):\n    #model = XGBClassifier(**params)\n    model = LGBMClassifier(**params)\n    model.fit(df.iloc[train_index], target.iloc[train_index])\n    return log_loss(target.iloc[test_index], model.predict_proba(df.iloc[test_index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97e1ca55dee04b084a5efa67ea0875adaa019bf6"},"cell_type":"code","source":"# param_grid = {\n#     'learning_rate': [0.1, 0.5, 0.02, 0.01],\n#     'num_leaves': [15, 30, 40, 65],\n#     'n_estimators': [20, 30, 50, 100, 200]\n# }\n# best_eval_score = 0\n# for i in range(20):\n#     params = {k: np.random.choice(v) for k, v in param_grid.items()}\n#     score = evaluate_model(df, bin_target, train_index, test_index, params)\n#     if score < best_eval_score or best_eval_score == 0:\n#         best_eval_score = score\n#         best_params = params\n# print(\"Best evaluation logloss\", best_eval_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65feb91eee2b3ab5e5be9c8d4d339cb74a5b44bb"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"510af43d516d71803a28723dac844f5e29937f4e"},"cell_type":"code","source":"print(df.isnull().sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad1dbc4a49bc010aec62dc38ba9e74e5d7ede5de"},"cell_type":"code","source":"# Checking feature correlations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\ncorr = pd.concat([df, bin_target], axis=1).corr()\nplt.figure(figsize=(14, 8))\nplt.title('Overall Correlation of House Prices', fontsize=18)\nsns.heatmap(corr, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4efd129ef878aadfc8ee821416c3add39b72e70c"},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"983cee6a0b0d5b1e9fd194a33c414e8fc6a1530e"},"cell_type":"code","source":"# drop SourceId and after feature , if is no na ,let it be 1, otherwise be 0\ndef dropfeatureTooNa(df):\n    columns = ['sourceId_mean', 'urgency_mean',\n       'takeSequence_mean', 'provider_mean', 'bodySize_mean',\n       'companyCount_mean', 'headlineTag_mean', 'marketCommentary_mean',\n       'sentenceCount_mean', 'wordCount_mean', 'firstMentionSentence_mean',\n       'relevance_mean', 'sentimentClass_mean', 'sentimentNegative_mean',\n       'sentimentNeutral_mean', 'sentimentPositive_mean',\n       'sentimentWordCount_mean', 'noveltyCount12H_mean',\n       'noveltyCount24H_mean', 'noveltyCount3D_mean', 'noveltyCount5D_mean',\n       'noveltyCount7D_mean', 'volumeCounts12H_mean', 'volumeCounts24H_mean',\n       'volumeCounts3D_mean', 'volumeCounts5D_mean', 'volumeCounts7D_mean']\n    new_df = df\n    new_df['have_new_influence'] = 0\n    new_df.loc[new_df['sourceId_mean'].notna(), 'have_new_influence'] = 1\n    new_df = new_df.drop(columns, axis=1)\n    return new_df\n    \nnew_df = dropfeatureTooNa(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7044cea9b775da1a4c4fddc9b71c9d103b24ce9"},"cell_type":"code","source":"new_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e95d1ce50b7a1a00ae0658f488ab437d3bbec79f"},"cell_type":"code","source":"# Checking feature correlations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncorr = pd.concat([new_df, bin_target], axis=1).corr()\nplt.figure(figsize=(14, 8))\nplt.title('Overall Correlation of House Prices', fontsize=18)\nsns.heatmap(corr, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6af2b3939b6fde191c564adfe0950663249be551"},"cell_type":"code","source":"print(new_df.isnull().sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1f81ee9587a0c3b1ccaa7d7ed9bf1b18132e21a"},"cell_type":"code","source":"# filter missing data by median\nnew_df['returnsClosePrevMktres1'] = new_df['returnsClosePrevMktres1'].fillna(new_df['returnsClosePrevMktres1'].dropna().median())\nnew_df['returnsOpenPrevMktres1'] = new_df['returnsOpenPrevMktres1'].fillna(new_df['returnsOpenPrevMktres1'].dropna().median())\nnew_df['returnsClosePrevMktres10'] = new_df['returnsClosePrevMktres10'].fillna(new_df['returnsClosePrevMktres10'].dropna().median())\nnew_df['returnsOpenPrevMktres10'] = new_df['returnsOpenPrevMktres10'].fillna(new_df['returnsOpenPrevMktres10'].dropna().median())\nprint(new_df.isnull().sum(axis=0) > 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e60aeb520948185bbbec74e5ff6f982ccc8c571c"},"cell_type":"markdown","source":"now, data doesn't have na-data, i will use boost model to train this dataframe"},{"metadata":{"trusted":true,"_uuid":"7f6c54c5fa715b42ba254bb23a8af9212b4c3071"},"cell_type":"code","source":"# define \nfrom sklearn.cross_validation import cross_val_score\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, new_df, bin_target, scoring='neg_mean_squared_error', cv=10))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"607f971c95d39ffde37037055ab60777216fcc18"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n# alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 70, 90, 100, 500, 1000, 2000]\n# cv_ridge = [rmse_cv(Ridge(alpha=alpha)).mean() for alpha in alphas]\n\n# cv_ridge = pd.Series(cv_ridge, index = alphas)\n# cv_ridge.plot(title = \"Validation\")\n# plt.xlabel(\"Alpha\")\n# plt.ylabel(\"Rmse\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2169e72f91bfa398722a06ca9eb3600218125c75"},"cell_type":"markdown","source":"best alpha is 500 about Ridge"},{"metadata":{"trusted":true,"_uuid":"9ad7ca1845acf47b7053854defc672eac815b4fc"},"cell_type":"code","source":"# 500 looks like the optimal alpha level, so let's fit the Ridge model with this value\nmodel_ridge = Ridge(alpha=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e2ba967e48ac11b1509e74ef4505ecdc05a1275"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n# Initiating Gradient Boosting Regressor\nmodel_gbr = GradientBoostingRegressor(n_estimators=1200, \n                                      learning_rate=0.05,\n                                      max_depth=4, \n                                      max_features='sqrt',\n                                      min_samples_leaf=15, \n                                      min_samples_split=10, \n                                      loss='huber',\n                                      random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3a950f9238db4e73287889cba524edfd5bb845"},"cell_type":"code","source":"# Initiating XGBRegressor\nimport xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n                             learning_rate=0.06,\n                             max_depth=3,\n                             n_estimators=1150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f393f9b9e60bc0ae4d3e15fea2d2c9ce2b33ed73"},"cell_type":"code","source":"import lightgbm as lgb\n# Initiating LGBMRegressor model\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1080,\n                              max_bin=75, \n                              bagging_fraction=0.80,\n                              bagging_freq=5, \n                              feature_fraction=0.232,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf=6, \n                              min_sum_hessian_in_leaf=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfe4aa2fc10fa10e45bf8ab5d5736b0bfba94425"},"cell_type":"code","source":"from catboost import CatBoostRegressor\n\n# Initiating CatBoost Regressor model\nmodel_cat = CatBoostRegressor(iterations=2000,\n                              learning_rate=0.10,\n                              depth=3,\n                              l2_leaf_reg=4,\n                              border_count=15,\n                              loss_function='RMSE',\n                              verbose=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"ac88d795e63215a74eefbeb919cb7008289eb242"},"cell_type":"code","source":"# cv_ridge = rmse_cv(model_ridge).mean()\ncv_gbr = rmse_cv(model_gbr).mean()\n# cv_xgb = rmse_cv(model_xgb).mean()\n# cv_lgb = rmse_cv(model_lgb).mean()\n# cv_cat = model_cat.fit(new_df, bin_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54ee9a7949002638a5c0e89b7b28e9d7cc427431"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}