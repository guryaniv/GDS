{"cells":[{"metadata":{"_uuid":"f47ef0a943c4df9148fb9c27bd0e11b11c9a1fd5","_cell_guid":"4d3dae43-ecf0-4853-8235-aed1bd4ea90e"},"cell_type":"markdown","source":"# Word Embeddings\n\nA word embedding is an approach to provide a dense vector representation of words that capture something about their meaning.\nWord embeddings are an improvement over simpler bag-of-word model word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words.\n\n## Different types of Word Embedding : \n\nSource : https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n\nEmbedding : http://ruder.io/secret-word2vec/\nhttps://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n\n\n** 1/ Frequency based embedding **\n1. Count Vectors\n2. TF-IDF\n3. Co-occurence Matrix\n\n** 2/ Prediction based embedding **\n1. CBOW\n2. Skip Gram"},{"metadata":{"_uuid":"4a17b3bcf06dfc20a28cbe766f5543e41d3894a0","_cell_guid":"12e80b25-20eb-47b6-8dde-c9afd0903f3e","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"013cd4cd73838cacf42d5840698186b505191409","_cell_guid":"529fd271-8b16-405f-ad1f-5c6276c03caf"},"cell_type":"markdown","source":"# Word2Vec\n\nSource : \n* https://rare-technologies.com/word2vec-tutorial/\n* https://radimrehurek.com/gensim/models/word2vec.html\n\n## When should you use Word2Vec? Why it's important?\nThere are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary.\n\n## How does it work? \n### The training : \nBehind the scenes we are training a neural network with a single hidden layer where we train the model to predict the current word based on the context (using the default architecture). But – we are not going to use the neural network after training!  Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn\n\n### Compute similarity : \nCosinus similarity\nCosine similarity is generally bounded by [-1, 1]. \n\n\n\n## Gensim's Word2Vec model : Parameters\n\n**Input ** : Word2Vec model need a list of list as argument. A.k.a, we have to list all sentence and the nested list corresponds to the list of words contained in the sentence. So, you can **_transform your corpus into a list_**, and **_tokenize_** your sentence to obtain the list of list require by Word2Vec\n\n** Training ** : Several parameter that affect both training speed and quality. \n\n1. _Pruning the internal dictionnary with \"mincount\" [default :  5]_ : Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. A reasonable value for this parameter is between 0-100.\n2. _Size of NN layers [default : 100]_ : which correspond to the “degrees” of freedom the training algorithm. If we choose a bigger _size_, we can reach more accurate models but we need a bigger train set. Reasonable values is between 10 - 100. \n3. _Window [default :  5]_ : The maximum distance between a target word and words around the target word\n4. _ sg  [default :  0]_ : The training algorithme - CBOW (value = 0) or skip gram (value = 1)\n5. _Iter [default : 1]_ : number of epoch. Increase this parameter, improve accurarcy, but increase the training time.\n6. _Workers_ : For training parallelization. If this parameter is equal to 1, there's no parallilization. \n\nWord2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=())¶\n\n\n** Evaluating ** : As Word2Vec is an unsupervised algorithm, there's no good way to evaluate. \n\n** Storing and loading model ** : \nmodel.save('/tmp/mymodel')\nnew_model = gensim.models.Word2Vec.load('/tmp/mymodel')\n\n** Use the model **\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n[('queen', 0.50882536)]\nmodel.doesnt_match(\"breakfast cereal dinner lunch\";.split())\n'cereal'\nmodel.similarity('woman', 'man')\n0.73723527\n"},{"metadata":{"_uuid":"185e8258b623f4a69f2f678156ff4af7e662c4a4","_cell_guid":"cf474520-9491-41dd-9ab0-3835d6c67047"},"cell_type":"markdown","source":"## A simple example with NLTK Corpus\n\nSource et example : https://streamhacker.com/2014/12/29/word2vec-nltk/\n\nAnother : https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n\n"},{"metadata":{"_uuid":"57588c0060f48c8c8c06af9decbffe62c6bb37ed","_cell_guid":"83d1236e-b860-480f-803c-ce2cc4cd43bc","trusted":false,"collapsed":true},"cell_type":"code","source":"#=================regex===============\nimport re\n#=================nltk===============\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nstopword_list = nltk.corpus.stopwords.words('english')\n\"\"\" \nCreate your own stopwords list\n\"\"\"\nstopword_list += ['a', 'about', 'above', 'across', 'after', 'afterwards', 'also']\nstopword_list += ['again', 'against', 'all', 'almost', 'alone', 'along','us','said', 'may', 'even']\nstopword_list += ['this', 'is', 'your', 'must', 'many','would', 'could', 'like','much']\n\nprint(\"StopWords List in English : \\n\", stopword_list)\n\nspecial_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n\ndef text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n    \n    text = \" \".join(text)\n    #Remove Special Characters\n    text=special_character_removal.sub('',text)\n    \n    #Replace Numbers\n    text=replace_numbers.sub('n',text)\n    # Clean the text, with the option to remove stopwords and to stem words.\n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        text = [w for w in text if not w in stopword_list]\n        text = \" \".join(text)\n\n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n        \n    # Return a list of words\n    text = text.split()\n    return(text)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"957fabfbb38af9e32e30d74bb44e21635fc7350d","_cell_guid":"359a2efe-d9e2-40a4-a2ee-94feea66125c","trusted":false,"collapsed":true},"cell_type":"code","source":"\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nImports needed and Logging \n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nimport gensim \nimport logging\nfrom nltk.corpus import brown, movie_reviews, treebank # Different corpus provide by nltk \n\nlogging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n \n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nChoose your dataset : \nyou should have lots and lots of text data in the relevant domain. \nFor example, if I want to make a sentiment analysis, using wikipedia as corpus may not be effective.\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nprint(\">>> Get your data ...\")\nsentences = brown.sents()\nprint(\"Our text data : \\n\", sentences)\n\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nPreprocess your text data (if you want), to avoid noise\n-> Remove stopwords\n-> Stemming ...\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nprint(\">>> Preprocessing data...\")\ncomments = []\nfor text in sentences:\n    comments.append(text_to_wordlist(text,stem_words=False))\nprint(\"Before preprocessing data : \", sentences[0:2])\nprint(\"After preprocessing data : \", comments[0:2])\n\n\"\"\"\nBefore preprocessing data :  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\nAfter preprocessing data :  ['fulton', 'county', 'grand', 'jury', 'friday', 'investigation', 'atlantas', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n\"\"\"\n\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nBuild and train your model \n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nprint(\">>> Build Word2Vec model ...\")\n# >>> Training the Word2Vec model - Straightforward. \nmodel = gensim.models.Word2Vec(comments,\n                               size=150,\n                               window=10,\n                               min_count=1)\n\n# After building the vocabulary, we just need to call train() function to start training the Word2Vec model\nprint(\">>> Train the model ...\")\nmodel.train(comments, \n            total_examples=len(comments), \n            epochs=10)\n\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nSome result \n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\n# >>> Access all the term in vocabulary\nvocab = list(model.wv.vocab.keys())\nprint(type(vocab)) # list\n\n# >>> Get vector for word - vectorial representation of a particular term\nmodel['men']  \n\n# >>> Which words is similar to... ?\nw1 = \"men\"\nmodel.wv.most_similar(positive = w1, topn=5)\n\n# w2 = \"france\"\n# model.wv.most_similar(positive = w2)\n# KeyError: \"word 'france' not in vocabulary\"\n\n# >>> What is the similarity between two words ?\nmodel.wv.similarity(w1 = \"women\", w2 = \"men\")   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6045accaf89b4f422ebb79bf9b4102a808924ef","_cell_guid":"bb45e61a-8977-47c0-950d-78748d970058"},"cell_type":"markdown","source":"## Datavisualization\n\n1. Load or pre-trained word2vec embedding\n2. Finds similar words and appends each of the similar words embedding vector to the matrix\n3. Applies PCA / TSNE \n4. Plot\n\n### PCA\n\n### TSNE"},{"metadata":{"collapsed":true,"_uuid":"bd46615365131631be9103dd1ffabc52861527e5","_cell_guid":"79275185-09de-457f-8960-5051da50aa39","trusted":false},"cell_type":"code","source":"import numpy as np\nimport re\nimport nltk\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\ntsne_plot(model)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2edcd6391b326271a5def1b478cf9ad2782a0c0b","_cell_guid":"c9937209-79d4-4c65-a9ea-7172531cd798","trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ndef display_closestwords_tsnescatterplot(model, word):\n    \n    arr = np.empty((0,300), dtype='f')\n    word_labels = [word]\n\n    # get close words\n    close_words = model.similar_by_word(word)\n    \n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()\n    \ndisplay_closestwords_tsnescatterplot(model, 'men')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"40b6ada090d1e120571d424cad168416e0e41993","_cell_guid":"4d284212-c6de-470e-9655-da49c324c944","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfa570f7e1569177a9f2682dd67c47fa757f2b0e","_cell_guid":"80cc8a5e-e95b-42ce-8dfe-0c0606168fe8"},"cell_type":"markdown","source":"###  How to tune parameter ?"},{"metadata":{"collapsed":true,"_uuid":"6b7861674c29a3417afe5611ee8bde8accc1acb7","_cell_guid":"289cebfe-9039-4e7b-89d8-d575f55c20ec","trusted":false},"cell_type":"code","source":"\"\"\" \nMODEL 1\nprint(\">>> Build Word2Vec model ...\")\n# >>> Training the Word2Vec model - Straightforward. \nmodel_baseline = gensim.models.Word2Vec(comments)\nmodel_baseline.train(comments,  total_examples=len(comments), epochs=10)\n\nmodel_1a = gensim.models.Word2Vec(comments, size = 200)\nmodel_1a.train(comments,  total_examples=len(comments), epochs=10)\n\nmodel_1b = gensim.models.Word2Vec(comments, size = 150)\nmodel_1b.train(comments,  total_examples=len(comments), epochs=10)\n\nmodel_1c = gensim.models.Word2Vec(comments, size = 50)\nmodel_1c.train(comments,  total_examples=len(comments), epochs=10)\n\nmodel_2 = gensim.models.Word2Vec(comments, window = 15)\nmodel_2.train(comments,  total_examples=len(comments), epochs=10)\n\nmodel_baseline.wv.similarity(w1 = \"women\", w2 = \"men\")   \nmodel_1a.wv.similarity(w1 = \"women\", w2 = \"men\")   \nmodel_1b.wv.similarity(w1 = \"women\", w2 = \"men\")   \nmodel_1c.wv.similarity(w1 = \"women\", w2 = \"men\")   \nmodel_2.wv.similarity(w1 = \"women\", w2 = \"men\")  \n\"\"\" ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e9ddd38fb9e3546801f52093158210ac59f11b9","_cell_guid":"c5335ac5-03e5-474b-b887-31c6674f46b9"},"cell_type":"markdown","source":"### Comparison between different corpus\n\nWord2vec training is an unsupervised task, there’s no good way to objectively evaluate the result. Evaluation depends on your end application.\n\n**The Brown corpus** : \n\n**Movie_Reviews** : \n\n**The Penn Treebank Corpus** : The Treebank corpora provide a syntactic parse for each sentence\n"},{"metadata":{"collapsed":true,"_uuid":"cc8f16eb80af931d00dd093b363e1effa996f18f","_cell_guid":"80a53a05-7e03-4ab7-8937-e19058e8f866","trusted":false},"cell_type":"code","source":"from nltk.corpus import brown, movie_reviews, treebank # Different corpus provide by nltk \n\nlogging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n\nprint(\">>> Get your data ...\")\nsentences_brown = brown.sents()\nsentences_movies = movie_reviews.sents()\nsentences_tree = treebank.sents()\n\nprint(\">>> Vizualize your data ...\")\ni = 3\nprint(\"\\nSentence \", i+1,\" Brown Corpus : \", sentences_brown[i])\nprint(\"\\nSentence \", i+1,\" Movies Corpus : \", sentences_movies[i])\nprint(\"\\nSentence \", i+1,\" Treebank Corpus : \", sentences_tree[i])\n\nprint(\"\\n>>> Categories\")\nprint(\"\\nCategories for Brown corpus \", brown.categories())\nprint(\"\\nCategories for Movies corpus \", movie_reviews.categories())\n\n\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nPreprocess your text data (if you want), to avoid noise\n-> Remove stopwords\n-> Stemming ...\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\n\nprint(\">>> Preprocessing data...\")\ncomments_brown = []\ncomments_movies = []\ncomments_tree = []\nfor text in sentences_brown:\n    comments_brown.append(text_to_wordlist(text,stem_words=False))\nfor text in sentences_movies:\n    comments_movies.append(text_to_wordlist(text,stem_words=False))\nfor text in sentences_tree:\n    comments_tree.append(text_to_wordlist(text,stem_words=False))\n\n\n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\nBuild and train your model \n\"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\" \"\"\"\n\nprint(\">>> Build Word2Vec model ...\")\n# >>> Training the Word2Vec model - Straightforward. \nmodel_brown = gensim.models.Word2Vec(comments_brown,\n                               size=150,\n                               window=10,\n                               min_count=1)\nmodel_movies = gensim.models.Word2Vec(comments_movies,\n                               size=150,\n                               window=10,\n                               min_count=1)\nmodel_tree = gensim.models.Word2Vec(comments_tree,\n                               size=150,\n                               window=10,\n                               min_count=1)\n\n# After building the vocabulary, we just need to call train() function to start training the Word2Vec model\nprint(\">>> Train the model ...\")\nmodel_brown.train(comments_brown, \n            total_examples=len(comments_brown), \n            epochs=10)\nmodel_movies.train(comments_movies, \n            total_examples=len(comments_movies), \n            epochs=10)\nmodel_tree.train(comments_tree, \n            total_examples=len(comments_tree), \n            epochs=10)\n\n\"\"\" \nSome result \n\"\"\"\nprint(\"Count words in brown corpus \",len(model_brown.wv.vocab.keys()))\nprint(\"Count words in Movies corpus \",len(model_movies.wv.vocab.keys()))\nprint(\"Count words in Treebank corpus \",len(model_tree.wv.vocab.keys()))\n\nprint(\"\\nCosinus similarity with Brown Corpus \", \n      model_brown.wv.similarity(w1 = \"women\", w2 = \"men\"))\nprint(\"Cosinus similarity with Movies Corpus \", \n      model_movies.wv.similarity(w1 = \"women\", w2 = \"men\"))\nprint(\"Cosinus similarity with Treebank Corpus \", \n      model_tree.wv.similarity(w1 = \"women\", w2 = \"men\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54ef57b99ec5b14ff6df56cc4b04e6ab172ae5ce","_cell_guid":"4267f9da-616a-4b5d-94e9-90a64d835eb3"},"cell_type":"markdown","source":"Results : \n\n| Corpus | Similarity |\n| ------------- |: -------------: | ---------: |\n| Brown | 0.775591753262 |\n| Movies | 0.798121089246 |\n| Treebank | 0.999726610055 |"},{"metadata":{"_uuid":"1a645bedc6f8cdbb9bf9134cbc465479ca9d2603","_cell_guid":"63442747-2781-4659-a1bb-10fc81d60f6c"},"cell_type":"markdown","source":"# PCA after word2vec"},{"metadata":{"collapsed":true,"_uuid":"d674f40831fc244ca5de864bab02ac4fb312b219","_cell_guid":"41cf3487-98cb-486c-9be1-8aa18faa6143","trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nvocab = list(model.wv.vocab)\nX = model[vocab]\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\ndf = pd.concat([pd.DataFrame(X_pca),\n                pd.Series(vocab)],\n               axis=1)\ndf.columns = ['x','y','word']\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.scatter(df['x'],df['y'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5fa45aae11a582a3706a9372b136a99eb3fa8a48","_cell_guid":"0aec0bfb-5bd9-4ba9-95b6-3ac4df3eea6e","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"43761f20fb50c9c68c8ab5a273bb9bac09e5d59d","_cell_guid":"2db86fc4-705a-4315-b1c4-8a208e2d2064","trusted":false},"cell_type":"code","source":"\"\"\"\n# Common word\n\nvocab_brown = set(model_brown.wv.vocab.keys())\nvocab_tree = set(model_tree.wv.vocab.keys())\ncommon = list(vocab_brown.intersection(vocab_tree))\n\n# Count words occurence in a corpus \n\nlst_mostcommon = []\nfor words in common:\n    if model_brown.wv.vocab[words].count > 800:\n        print(\"Word Brown: \", words , \" appears \" , model_brown.wv.vocab[words].count)\n        lst_mostcommon.append(words)\n    if model_tree.wv.vocab[words].count > 800:\n        print(\"Word Tree: \", words , \" appears \" , model_tree.wv.vocab[words].count)\n        lst_mostcommon.append(words)        \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6d7d853a1fceebb1d754dab71fadb424ce51e878","_cell_guid":"54c8f7f0-dfee-4121-8d39-2f011dad138b","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5b189d5ff0e0ec07249436b567b5ac1c089d57e2","_cell_guid":"4eb9b33c-a86b-486a-8d14-00b1cc7dced2","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"17fc5bfaa4bb0b3cd7725f5af051a75ad86b55f5","_cell_guid":"cebb99f0-17fb-4939-85b5-2c4b3c0c3834","trusted":false},"cell_type":"code","source":"if 'election' in model_tree.wv.vocab.keys():\n    print (\"ok\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7f1da47f9f2bd81e7e09df7b21745207da9e463","_cell_guid":"5f94da59-412a-4f44-b665-4642aff71705"},"cell_type":"markdown","source":"Handle with unknown words\nI know that using train() on a new exemplar will not add the words to the vocabulary, only updates the weights themselves."},{"metadata":{"collapsed":true,"_uuid":"0ad98d4aa8af6f8f136009f6686c88f0ab55ce67","_cell_guid":"70f56ea8-b888-4d63-b205-2b88e7a2535b","trusted":false},"cell_type":"code","source":"# Use a filter : \nfor doc in labeled_corpus:\n    words = filter(lambda x: x in model.vocab, doc.words)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b16d4bc1aa89af8ff5e0631b7fb74ea9dc7948eb","_cell_guid":"75678d6f-58e5-4acd-a8ef-2d55c984f535","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7421100c6081b1fdddbd7a0db92f129d13a4432c","_cell_guid":"c22dcaf9-9f6d-41fc-8555-4da5613c0893"},"cell_type":"markdown","source":"## Use a pretrained model \nhttps://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n"},{"metadata":{"collapsed":true,"_uuid":"6cf48b56cbd6e2ae4a084d04b4f060f7a2c4ffe6","_cell_guid":"8674af4c-9e1a-4043-9a64-23b5cc613d5b","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"37815b79959dac87f353310550fac67c2eac5f59","_cell_guid":"c0b2bff6-ab03-4f96-a7dd-6ba83855a7d0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"97972ae6d50b6ce2b5785684b1b6c918f51949fd","_cell_guid":"b890d9cd-77e5-4962-a012-b593733d1e82"},"cell_type":"markdown","source":"# Glove\n\nSource : https://rare-technologies.com/making-sense-of-word2vec/"},{"metadata":{"collapsed":true,"_uuid":"f25d9a93511977faff0e517d3073582bc67ef207","_cell_guid":"ac33efd0-5cc2-4bd3-97db-2859418c783d","trusted":false},"cell_type":"code","source":"from gensim import utils, corpora, matutils, models\nimport glove\n \n# Restrict dictionary to the 30k most common words.\nwiki = models.word2vec.LineSentence('/data/shootout/title_tokens.txt.gz')\nid2word = corpora.Dictionary(wiki)\nid2word.filter_extremes(keep_n=30000)\nword2id = dict((word, id) for id, word in id2word.iteritems())\n \n# Filter all wiki documents to contain only those 30k words.\nfilter_text = lambda text: [word for word in text if word in word2id]\nfiltered_wiki = lambda: (filter_text(text) for text in wiki)  # generator\n \n# Get the word co-occurrence matrix -- needs lots of RAM!!\ncooccur = glove.Corpus()\ncooccur.fit(filtered_wiki(), window=10)\n \n# and train GloVe model itself, using 10 epochs\nmodel_glove = glove.Glove(no_components=600, learning_rate=0.05)\nmodel_glove.fit(cooccur.matrix, epochs=10)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6e943f9bdb6cc89a1603b9593aa33c72b115b570","_cell_guid":"a32a570e-cbdf-4a82-9af8-80bc71b83e40","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}