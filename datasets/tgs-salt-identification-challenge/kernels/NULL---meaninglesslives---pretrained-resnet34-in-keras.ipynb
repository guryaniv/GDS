{"cells":[{"metadata":{"_uuid":"425ff4153a6109e6235b3dc058ac334f0ae9a34c"},"cell_type":"markdown","source":"In my [previous post](https://www.kaggle.com/meaninglesslives/unet-resnet34-in-keras/comments) there were lots of questions regarding how to use pretrained ResNet34 in keras. Thanks to [qubvel](https://www.kaggle.com/pavel92), we now have access to pretrained ResNet34,ResNet18 and many other model weights in keras. Please check out his  [github repo](https://github.com/qubvel/classification_models) for more info and other great models implemented in keras. \n\nIn this Kernel I show you how to use pre-trained Resnet34 in kaggle kernels with image size 128. You can easily experiment with other models that are available in his repo. \n\nPlease check out the following kernels for more ideas:\n\nJack (Jiaxin) Shao: https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss\n\nBruno G. do Amaral: https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n\nPeter Hönigschmid: https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n\nJuan C EsquivelTGS: https://www.kaggle.com/jcesquiveld/tgs-vanilla-u-net-with-simple-augmentation\n\nNPHard: https://www.kaggle.com/meaninglesslives/apply-crf-unet-resnet\n\nYou may also find the following discussion threads very useful:\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64875\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66568\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64093\n\nPS - Most of the code is boilerplate code. You may want to experiment with the decoder architecture (very important), loss function and augmentation to further improve the result."},{"metadata":{"_uuid":"47b586934d6e5ac3595321450a5567fde8450235"},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport six\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom keras.preprocessing.image import load_img\n\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.regularizers import l2\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"trusted":true,"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"trusted":true,"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86620c6a070571895f4f36ec050a25803915ed74"},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1137f0a009f10b5f69e4dade5f689e744e9ce1d6"},"cell_type":"markdown","source":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."},{"metadata":{"trusted":true,"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b13d1ecc7004832e8e042d034922796263054b7"},"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"},"cell_type":"markdown","source":"# Show some example images"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1a6bc85ee458f72c0917edf77895d5abc5eaf3ee"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00655e32f93f96ebd90dbe94e35ee052f52217cd"},"cell_type":"markdown","source":"# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."},{"metadata":{"trusted":true,"_uuid":"2d3c3157512d11e71ac74ce51a937b85bedfe1d1"},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2f1ab00f03e71e6d7f9b2214408b5a9779fc235"},"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build U-Net Model"},{"metadata":{"trusted":true,"_uuid":"0b9071c01819d43f6fa2d0a181411233c8b47e2f"},"cell_type":"code","source":"# https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/unet/models.py\n\ndef handle_block_names_old(stage):\n    conv_name = 'decoder_stage{}_conv'.format(stage)\n    bn_name = 'decoder_stage{}_bn'.format(stage)\n    relu_name = 'decoder_stage{}_relu'.format(stage)\n    up_name = 'decoder_stage{}_upsample'.format(stage)\n    return conv_name, bn_name, relu_name, up_name\n\n\ndef Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                     batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names_old(stage)\n\n        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer\n\n\ndef Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names_old(stage)\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n                            padding='same', name=up_name)(input_tensor)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37227f125bf804dd904cb3268638843b1645d361"},"cell_type":"code","source":"# default parameters for convolution and batchnorm layers of ResNet models\n# parameters are obtained from MXNet converted model\n\ndef get_conv_params(**params):\n    default_conv_params = {\n        'kernel_initializer': 'glorot_uniform',\n        'use_bias': False,\n        'padding': 'valid',\n    }\n    default_conv_params.update(params)\n    return default_conv_params\n\ndef get_bn_params(**params):\n    default_bn_params = {\n        'axis': 3,\n        'momentum': 0.99,\n        'epsilon': 2e-5,\n        'center': True,\n        'scale': True,\n    }\n    default_bn_params.update(params)\n    return default_bn_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa6eb13a9b5b4c0a618fb1b9e257ba0dce925f92"},"cell_type":"code","source":"def handle_block_names(stage, block):\n    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n    conv_name = name_base + 'conv'\n    bn_name = name_base + 'bn'\n    relu_name = name_base + 'relu'\n    sc_name = name_base + 'sc'\n    return conv_name, bn_name, relu_name, sc_name\n\n\ndef basic_identity_block(filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer\n\n\ndef basic_conv_block(filters, stage, block, strides=(2, 2)):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef usual_conv_block(filters, stage, block, strides=(2, 2)):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef usual_identity_block(filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c884b2332a3f8aa650290f5c739bdd2657dd5292"},"cell_type":"code","source":"def build_unet(backbone, classes, last_block_filters, skip_layers,\n               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n               block_type='upsampling', activation='sigmoid',\n               **kwargs):\n\n    input = backbone.input\n    x = backbone.output\n\n    if block_type == 'transpose':\n        up_block = Transpose2D_block\n    else:\n        up_block = Upsample2D_block\n\n    # convert layer names to indices\n    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                    for l in skip_layers])\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        if i < len(skip_layers):\n#             print(backbone.layers[skip_layers[i]])\n#             print(backbone.layers[skip_layers[i]].output)\n            skip = backbone.layers[skip_layers[i]].output\n        else:\n            skip = None\n\n        up_size = (upsample_rates[i], upsample_rates[i])\n        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n\n        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n\n    if classes < 2:\n        activation = 'sigmoid'\n\n    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n    x = Activation(activation, name=activation)(x)\n\n    model = Model(input, x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61f1b7f07a22a9209ad0c2933c2ddf59f93c9d85"},"cell_type":"markdown","source":"# ResNet 34"},{"metadata":{"trusted":true,"_uuid":"4a0fd9136d894ea820adfb7bc4de1a4d1e7a79ad"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\ndef build_resnet(\n     repetitions=(2, 2, 2, 2),\n     include_top=True,\n     input_tensor=None,\n     input_shape=None,\n     classes=1000,\n     block_type='usual'):\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=101,\n                                      data_format='channels_last',\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name='data')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    \n    # get parameters for model layers\n    no_scale_bn_params = get_bn_params(scale=False)\n    bn_params = get_bn_params()\n    conv_params = get_conv_params()\n    init_filters = 64\n\n    if block_type == 'basic':\n        conv_block = basic_conv_block\n        identity_block = basic_identity_block\n    else:\n        conv_block = usual_conv_block\n        identity_block = usual_identity_block\n    \n    # resnet bottom\n    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n    x = BatchNormalization(name='bn0', **bn_params)(x)\n    x = Activation('relu', name='relu0')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n    \n    # resnet body\n    for stage, rep in enumerate(repetitions):\n        for block in range(rep):\n            \n            filters = init_filters * (2**stage)\n            \n            # first block of first stage without strides because we have maxpooling before\n            if block == 0 and stage == 0:\n                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n                \n            elif block == 0:\n                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n                \n            else:\n                x = identity_block(filters, stage, block)(x)\n                \n    x = BatchNormalization(name='bn1', **bn_params)(x)\n    x = Activation('relu', name='relu1')(x)\n\n    # resnet top\n    if include_top:\n        x = GlobalAveragePooling2D(name='pool1')(x)\n        x = Dense(classes, name='fc1')(x)\n        x = Activation('softmax', name='softmax')(x)\n\n    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        \n    # Create model.\n    model = Model(inputs, x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d031f18c8a9200858b2f70998dbe28decfc7c9f"},"cell_type":"code","source":"weights_collection = [\n    # ResNet34\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n        'name': 'resnet34_imagenet_1000.h5',\n        'md5': '2ac8277412f65e5d047f255bcbd10383',\n    },\n\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n        'name': 'resnet34_imagenet_1000_no_top.h5',\n        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n    },\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf91b45898924c2e4ca0409cf04abae55bf0e9a9"},"cell_type":"code","source":"def ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=include_top,\n                         block_type='basic')\n    model.name = 'resnet34'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b3292a02d84d11d2e27a6261c8de060f7f93ed2"},"cell_type":"markdown","source":"# U-Net with ResNet34 Encoder"},{"metadata":{"trusted":true,"_uuid":"93adb00ed255d965b14603931f2705e63ffa020d"},"cell_type":"code","source":"from keras.utils import get_file\n\n\ndef find_weights(weights_collection, model_name, dataset, include_top):\n    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n    w = list(filter(lambda x: x['dataset'] == dataset, w))\n    w = list(filter(lambda x: x['include_top'] == include_top, w))\n    return w\n\n\ndef load_model_weights(weights_collection, model, dataset, classes, include_top):\n    weights = find_weights(weights_collection, model.name, dataset, include_top)\n\n    if weights:\n        weights = weights[0]\n\n        if include_top and weights['classes'] != classes:\n            raise ValueError('If using `weights` and `include_top`'\n                             ' as true, `classes` should be {}'.format(weights['classes']))\n\n        weights_path = get_file(weights['name'],\n                                weights['url'],\n                                cache_subdir='models',\n                                md5_hash=weights['md5'])\n\n        model.load_weights(weights_path)\n\n    else:\n        raise ValueError('There is no weights for such configuration: ' +\n                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n                         'classes = {}, include_top = {}.'.format(classes, include_top))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd5124faf49797d76aa07be02a46fe11c0ec6e21"},"cell_type":"code","source":"def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16, decoder_block_type='transpose',\n                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n\n#     backbone = ResnetBuilder.build_resnet_34(input_shape=input_shape,input_tensor=input_tensor)\n    backbone = ResNet34(input_shape=input_shape, weights='imagenet', classes=1000,include_top=False)\n    skip_connections = list([106,74,37,5])  # for resnet 34\n    model = build_unet(backbone, classes, decoder_filters,\n                       skip_connections, block_type=decoder_block_type,\n                       activation=activation, **kwargs)\n    model.name = 'u-resnet34'\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce4d8b8704d198a2c384296636e3543f344b7bbe"},"cell_type":"code","source":"model = UResNet34(input_shape=(img_size_target,img_size_target,3))\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8e91138fe27675e7f32d7e5eff9d885ff4b9bc0"},"cell_type":"markdown","source":"# Define Loss Function"},{"metadata":{"trusted":true,"_uuid":"1ad2483391c39ae94819792b8d17c6d62cacbe59"},"cell_type":"code","source":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\nimport tensorflow as tf\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7a5689b37243b8f4ff4aa0981cac16af777b39c"},"cell_type":"code","source":"def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0      \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b"},"cell_type":"code","source":"model.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdad5099940be56b00cdfbef38d9efcbd1ff3bb9"},"cell_type":"markdown","source":"# Augmentation"},{"metadata":{"trusted":true,"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dc541449673992ecad078fbe9de7b8153fc3a3b"},"cell_type":"code","source":"x_train = np.repeat(x_train,3,axis=3)\nx_valid = np.repeat(x_valid,3,axis=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7040f72549212dd4f71c13dfbd8bf013481ea369"},"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_my_iou_metric',patience=25,mode='max', verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\",monitor='val_my_iou_metric',mode='max', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1,monitor='val_my_iou_metric',mode='max', patience=10, verbose=1)\n\nepochs = 60\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping,reduce_lr, model_checkpoint],shuffle=True,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","collapsed":true},"cell_type":"code","source":"plt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.title('model iou')\nplt.ylabel('val_my_iou_metric')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","collapsed":true},"cell_type":"code","source":"# Load best model\nmodel.load_weights('./keras.model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"trusted":true,"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46"},"cell_type":"code","source":"preds_valid = model.predict(x_valid,32).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc"},"cell_type":"code","source":"offset = 10\nmax_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[offset:offset+max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4"},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0","collapsed":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc","collapsed":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ced29761f2d1760245112a30a7abd4783b373dd","collapsed":true},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"},"cell_type":"markdown","source":"# Sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."},{"metadata":{"trusted":true,"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","collapsed":true},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}