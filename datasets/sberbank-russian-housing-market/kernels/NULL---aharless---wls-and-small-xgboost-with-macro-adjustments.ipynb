{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5d0b3f2d-a46b-11b4-615c-414821ebfaf2"
      },
      "source": [
        "## WLS Baseline and Small XGBoost with Macro Adjustments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cea20abc-5668-18db-08ac-2e10b8a1b8b4"
      },
      "source": [
        "This is the first single-kernel implementation of my \"model macro and micro separately\" approach.  The macro model is the very simple one from [my previous kernel][1].  The micro model is roughly what was in my \"[Using WLS to Create Features for XGBoost][2]\" kernel except that it omits all macro variables.  Overall, it's a 3-step approach:\n",
        "\n",
        "1. Fit baseline WLS and use coefficients to create composite features\n",
        "2. Add additional features and fit XGBoost.\n",
        "3. Fit macro model to monthly medians and use its predictions to adjust XGBoost predictions. \n",
        "\n",
        "  [1]: https://www.kaggle.com/aharless/simple-macro-model-for-monthly-house-prices\n",
        "  [2]: https://www.kaggle.com/aharless/using-wls-to-create-features-for-xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ca63164-cb8d-5417-9499-bab3791eccfb"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "use_pipe = True  \n",
        "weight_base = \"2011-08-19\"\n",
        "xgb_lr = .02 #  Learning rate.  I use .007, but needs to be larger to run on Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1c07b6d-4791-29ff-c96c-13cc06a7be73"
      },
      "source": [
        "### Read and munge the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4da8ff5c-0f2d-1116-74db-7e3db1e9b9a0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19068c3e-1c53-4504-bd5c-1f94f7b6029c"
      },
      "outputs": [],
      "source": [
        "dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n",
        "# Eliminate spaces and special characters in area names\n",
        "dfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\n",
        "dfa = dfa.merge(macro, \n",
        "                on='timestamp', suffixes=['','_macro'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e4ea376-5244-8d9c-fe19-7b511a961c86"
      },
      "outputs": [],
      "source": [
        "dfa[\"fullzero\"] = (dfa.full_sq==0)\n",
        "dfa[\"fulltiny\"] = (dfa.full_sq<4)\n",
        "dfa[\"fullhuge\"] = (dfa.full_sq>2000)\n",
        "dfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n",
        "\n",
        "dfa[\"nolife\"] = dfa.life_sq.isnull()\n",
        "dfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\n",
        "dfa[\"lifezero\"] = (dfa.life_sq==0)\n",
        "dfa[\"lifetiny\"] = (dfa.life_sq<4)\n",
        "dfa[\"lifehuge\"] = (dfa.life_sq>2000)\n",
        "dfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n",
        "\n",
        "dfa[\"nofloor\"] = dfa.floor.isnull()\n",
        "dfa.floor = dfa.floor.fillna(dfa.floor.median())\n",
        "dfa[\"floor1\"] = (dfa.floor==1)\n",
        "dfa[\"floor0\"] = (dfa.floor==0)\n",
        "dfa[\"floorhuge\"] = (dfa.floor>50)\n",
        "dfa[\"lnfloor\"] = np.log(dfa.floor+1)\n",
        "\n",
        "dfa[\"nomax\"] = dfa.max_floor.isnull()\n",
        "dfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\n",
        "dfa[\"max1\"] = (dfa.max_floor==1)\n",
        "dfa[\"max0\"] = (dfa.max_floor==0)\n",
        "dfa[\"maxhuge\"] = (dfa.max_floor>80)\n",
        "dfa[\"lnmax\"] = np.log(dfa.max_floor+1)\n",
        "\n",
        "dfa[\"norooms\"] = dfa.num_room.isnull()\n",
        "dfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\n",
        "dfa[\"zerorooms\"] = (dfa.num_room==0)\n",
        "dfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n",
        "\n",
        "dfa[\"nokitch\"] = dfa.kitch_sq.isnull()\n",
        "dfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\n",
        "dfa[\"kitch1\"] = (dfa.kitch_sq==1)\n",
        "dfa[\"kitch0\"] = (dfa.kitch_sq==0)\n",
        "dfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\n",
        "dfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1956b91-be30-3167-e9e6-da16eb69e640"
      },
      "outputs": [],
      "source": [
        "dfa[\"material0\"] = dfa.material.isnull()\n",
        "dfa[\"material1\"] = (dfa.material==1)\n",
        "dfa[\"material2\"] = (dfa.material==2)\n",
        "dfa[\"material3\"] = (dfa.material==3)\n",
        "dfa[\"material4\"] = (dfa.material==4)\n",
        "dfa[\"material5\"] = (dfa.material==5)\n",
        "dfa[\"material6\"] = (dfa.material==6)\n",
        "\n",
        "# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\n",
        "dfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\n",
        "dfa.state = dfa.state.fillna(dfa.state.median())\n",
        "\n",
        "# product_type gonna be ugly because there are missing values in the test set but not training\n",
        "# Check for the same problem with other variables\n",
        "dfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\n",
        "dfa.owner_occ.fillna(dfa.owner_occ.mean())\n",
        "\n",
        "dfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b299145e-1131-bf66-e7d9-2f7d0bfa396e"
      },
      "outputs": [],
      "source": [
        "# Build year is ugly\n",
        "# Can be missing\n",
        "# Can be zero\n",
        "# Can be one\n",
        "# Can be some ridiculous pre-Medieval number\n",
        "# Can be some invalid huge number like 20052009\n",
        "# Can be some other invalid huge number like 4965\n",
        "# Can be a reasonable number but later than purchase year\n",
        "# Can be equal to purchase year\n",
        "# Can be a reasonable nubmer before purchase year\n",
        "\n",
        "dfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\n",
        "dfa[\"nobuild\"] = dfa.build_year.isnull()\n",
        "dfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\n",
        "dfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\n",
        "dfa[\"futurebuild\"] = (dfa.sincebuild < 0)\n",
        "dfa[\"newhouse\"] = (dfa.sincebuild==0)\n",
        "dfa[\"tooold\"] = (dfa.sincebuild>1000)\n",
        "dfa[\"build0\"] = (dfa.build_year==0)\n",
        "dfa[\"build1\"] = (dfa.build_year==1)\n",
        "dfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\n",
        "dfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1).apply(np.log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6fd7d25a-295a-278d-d652-4f0950e1adfc"
      },
      "outputs": [],
      "source": [
        "# Interaction terms\n",
        "dfa[\"fullzero_Xowner\"] = dfa.fullzero.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"fulltiny_Xowner\"] = dfa.fulltiny.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"fullhuge_Xowner\"] = dfa.fullhuge.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"lnfull_Xowner\"] = dfa.lnfull * dfa.owner_occ\n",
        "dfa[\"nofloor_Xowner\"] = dfa.nofloor.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"floor0_Xowner\"] = dfa.floor0.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"floor1_Xowner\"] = dfa.floor1.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"lnfloor_Xowner\"] = dfa.lnfloor * dfa.owner_occ\n",
        "dfa[\"max1_Xowner\"] = dfa.max1.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"max0_Xowner\"] = dfa.max0.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"maxhuge_Xowner\"] = dfa.maxhuge.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"lnmax_Xowner\"] = dfa.lnmax * dfa.owner_occ\n",
        "dfa[\"kitch1_Xowner\"] = dfa.kitch1.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"kitch0_Xowner\"] = dfa.kitch0.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"kitchhuge_Xowner\"] = dfa.kitchhuge.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"lnkitch_Xowner\"] = dfa.lnkitch * dfa.owner_occ\n",
        "dfa[\"nobuild_Xowner\"] = dfa.nobuild.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"newhouse_Xowner\"] = dfa.newhouse.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"tooold_Xowner\"] = dfa.tooold.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"build0_Xowner\"] = dfa.build0.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"build1_Xowner\"] = dfa.build1.astype(\"float64\") * dfa.owner_occ\n",
        "dfa[\"lnsince_Xowner\"] = dfa.lnsince * dfa.owner_occ\n",
        "dfa[\"state_Xowner\"] = dfa.state * dfa.owner_occ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f50f1a10-8276-1ede-7183-9516fef680ad"
      },
      "outputs": [],
      "source": [
        "dfa[\"lnruboil\"] = np.log( dfa.oil_urals * dfa.usdrub )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f47ccd19-3222-7068-0810-a879e85df749"
      },
      "outputs": [],
      "source": [
        "# Sets of features that go together\n",
        "\n",
        "# Features derived from full_sq\n",
        "fullvars = [\"fullzero\", \"fulltiny\",\n",
        "           # For now I'm going to drop the one \"fullhuge\" case. Later use dummy, maybe.\n",
        "           #\"fullhuge\",\n",
        "           \"lnfull\" ]\n",
        "\n",
        "# Features derived from floor\n",
        "floorvars = [\"nofloor\", \"floor1\", \"floor0\",\n",
        "             # floorhuge isn't very important, and it's causing problems, so drop it\n",
        "             #\"floorhuge\", \n",
        "             \"lnfloor\"]\n",
        "\n",
        "# Features derived from max_floor\n",
        "maxvars = [\"max1\", \"max0\", \"maxhuge\", \"lnmax\"]\n",
        "\n",
        "# Features derived from kitch_sq\n",
        "kitchvars = [\"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\"]\n",
        "\n",
        "# Features derived from bulid_year\n",
        "buildvars = [\"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n",
        "             \"build0\", \"build1\", \"untilbuild\", \"lnsince\"]\n",
        "\n",
        "# Features (dummy set) derived from material\n",
        "matervars = [\"material1\", \"material2\",  # material3 is rare, so lumped in with missing \n",
        "             \"material4\", \"material5\", \"material6\"]\n",
        "\n",
        "# Features derived from interaction of floor and product_type\n",
        "floorXvars = [\"nofloor_Xowner\", \"floor1_Xowner\", \"lnfloor_Xowner\"]\n",
        "\n",
        "# Features derived from interaction of kitch_sq and product_type\n",
        "kitchXvars = [\"kitch1_Xowner\", \"kitch0_Xowner\", \"lnkitch_Xowner\"]\n",
        "\n",
        "# Features (dummy set) derived from sub_area\n",
        "subarvars = [\n",
        "       'sub_area_Akademicheskoe',\n",
        "       'sub_area_Altufevskoe', 'sub_area_Arbat',\n",
        "       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n",
        "       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n",
        "       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n",
        "       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n",
        "       'sub_area_Caricyno', 'sub_area_Cheremushki',\n",
        "       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n",
        "       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n",
        "       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n",
        "       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n",
        "       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n",
        "       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n",
        "       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n",
        "       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n",
        "       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n",
        "       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n",
        "       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n",
        "       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n",
        "       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n",
        "       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n",
        "       'sub_area_Krylatskoe', 'sub_area_Kuncevo', \n",
        "       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n",
        "       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n",
        "       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n",
        "       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n",
        "       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n",
        "       'sub_area_MoskvorecheSaburovo',\n",
        "       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n",
        "       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n",
        "       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n",
        "       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n",
        "       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n",
        "       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n",
        "       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n",
        "       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n",
        "       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n",
        "       'sub_area_PoselenieFilimonkovskoe', \n",
        "       'sub_area_PoselenieKrasnopahorskoe',\n",
        "       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n",
        "       'sub_area_PoselenieNovofedorovskoe',\n",
        "       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n",
        "       'sub_area_PoselenieRogovskoe', \n",
        "       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n",
        "       'sub_area_PoselenieVnukovskoe',  \n",
        "       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n",
        "       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n",
        "       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n",
        "       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n",
        "       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n",
        "       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n",
        "       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n",
        "       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n",
        "       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n",
        "       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n",
        "       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n",
        "       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n",
        "       'sub_area_Tverskoe', 'sub_area_Veshnjaki', \n",
        "       'sub_area_Vojkovskoe', \n",
        "       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n",
        "       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n",
        "       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo', 'sub_area_Zjuzino'\n",
        "       ]\n",
        "\n",
        "\n",
        "# Lump together small sub_areas\n",
        "\n",
        "dfa = dfa.assign( sub_area_SmallSW =\n",
        "   dfa.sub_area_PoselenieMihajlovoJarcevskoe + \n",
        "   dfa.sub_area_PoselenieKievskij +\n",
        "   dfa.sub_area_PoselenieKlenovskoe +\n",
        "   dfa.sub_area_PoselenieVoronovskoe +\n",
        "   dfa.sub_area_PoselenieShhapovskoe )\n",
        "\n",
        "dfa = dfa.assign( sub_area_SmallNW =\n",
        "   dfa.sub_area_Molzhaninovskoe +\n",
        "   dfa.sub_area_Kurkino )\n",
        "\n",
        "dfa = dfa.assign( sub_area_SmallW =\n",
        "   dfa.sub_area_PoselenieMarushkinskoe +\n",
        "   dfa.sub_area_Vnukovo +\n",
        "   dfa.sub_area_PoselenieKokoshkino )\n",
        "\n",
        "dfa = dfa.assign( sub_area_SmallN =\n",
        "   dfa.sub_area_Vostochnoe +\n",
        "   dfa.sub_area_Alekseevskoe )\n",
        "\n",
        "subarvars += [\"sub_area_SmallSW\", \"sub_area_SmallNW\", \"sub_area_SmallW\", \"sub_area_SmallN\"]\n",
        "                 \n",
        "\n",
        "\n",
        "# For now eliminate case with ridiculous value of full_sq\n",
        "dfa = dfa[~dfa.fullhuge]\n",
        "\n",
        "    \n",
        "# Independent features\n",
        "\n",
        "indievars = [\"owner_occ\", \"state\", \"state_Xowner\"]\n",
        "\n",
        "\n",
        "# Complete list of features to use for fit\n",
        "\n",
        "allvars = fullvars + floorvars + maxvars + kitchvars + buildvars + matervars\n",
        "allvars += floorXvars + kitchXvars + subarvars + indievars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7db350c-4d15-33ab-9272-ff19529ed26c"
      },
      "outputs": [],
      "source": [
        "# The normalized target variable:  log real sale price\n",
        "training = dfa[dfa.price_doc.notnull()]\n",
        "training.lnrp = training.price_doc.div(training.cpi).apply(np.log)\n",
        "y = training.lnrp\n",
        "\n",
        "# Features to use in heteroskedasticity model if I go back to that\n",
        "million1 = (training.price_doc==1e6)\n",
        "million2 = (training.price_doc==2e6)\n",
        "million3 = (training.price_doc==3e6)\n",
        "\n",
        "# Create X matrix for fitting\n",
        "keep = allvars + ['timestamp']  # Need to keep timestamp to calculate weights\n",
        "X = training[keep] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "752d6eee-be1e-3b9f-fee4-d6a9dce59f0c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a5e7af5-0534-1a53-016b-f672ca213c54"
      },
      "outputs": [],
      "source": [
        "def get_weights(df):\n",
        "    # Weight cases linearly on time\n",
        "    # with later cases (more like test data) weighted more heavily\n",
        "    basedate = pd.to_datetime(weight_base).toordinal() # Basedate gets a weight of zero\n",
        "    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n",
        "    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n",
        "    return wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "67c4983a-963e-f2c7-411e-fb94303b473b"
      },
      "outputs": [],
      "source": [
        "wts = get_weights(X)\n",
        "X = X.drop(\"timestamp\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d04f1f9f-5e32-1bad-7ade-75b8e2dc95d1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Imputer, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Make a pipeline that transforms X\n",
        "pipe = make_pipeline(Imputer(), StandardScaler())\n",
        "pipe.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d88258e-aca5-968a-c458-ee4e8d5cb3a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "lr.fit(pipe.transform(X), y, sample_weight=wts)\n",
        "lr.score( pipe.transform(X), y ) # Unweighted R^2, just to see what it looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8fd8317-70ab-24ae-f3bb-a8d7b6031ddf"
      },
      "outputs": [],
      "source": [
        "# Function to create an indicator array that selects positions\n",
        "#   corresponding to a set of features from the regression\n",
        "\n",
        "def get_selector( df, varnames ):\n",
        "    selector = np.zeros( df.shape[1] )\n",
        "    selector[[df.columns.get_loc(x) for x in varnames]] = 1\n",
        "    return( selector )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a74c7302-9aa4-cf13-133d-22fc6e99918d"
      },
      "outputs": [],
      "source": [
        "def append_composite( df, varnames, name, X, Xuse, estimator ):\n",
        "    selector = get_selector(X, varnames)\n",
        "    v = pd.Series( np.matmul( Xuse, selector*estimator.coef_ ), \n",
        "                   name=name, index=df.index )\n",
        "    return( pd.concat( [df, v], axis=1 ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fce0fc5c-d0db-d72d-1192-14ba642cde9a"
      },
      "outputs": [],
      "source": [
        "Xuse = pipe.transform(X)\n",
        "\n",
        "vars = {\"fullv\":fullvars,     \"floorv\":floorvars,   \"maxv\":maxvars, \n",
        "        \"kitchv\":kitchvars,   \"buildv\":buildvars,   \"materv\":matervars, \n",
        "        \"floorxv\":floorXvars, \"kitchxv\":kitchXvars, \"subarv\":subarvars}\n",
        "for v in vars:\n",
        "    training = append_composite( training, vars[v], v, X, Xuse, lr )\n",
        "\n",
        "shortvarlist = list(vars.keys())\n",
        "shortvarlist += indievars\n",
        "\n",
        "Xshort = training[shortvarlist]\n",
        "\n",
        "pipe1 = make_pipeline(Imputer(), StandardScaler())\n",
        "pipe1.fit(Xshort)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea1f344c-3083-4951-6938-0ced25a2a314"
      },
      "outputs": [],
      "source": [
        "# Fit again to make sure result is same\n",
        "lr1 = LinearRegression(fit_intercept=True)\n",
        "lr1.fit(pipe1.transform(Xshort), y, sample_weight=wts)\n",
        "lr1.score( pipe1.transform(Xshort), y )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f934c72b-f17c-62b5-e3d8-c6df2080f63a"
      },
      "source": [
        "### Add more features and run XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea100464-8f9f-078d-0adc-cec7bd1ba356"
      },
      "outputs": [],
      "source": [
        "# Set up test data\n",
        "\n",
        "testing = dfa[dfa.price_doc.isnull()]\n",
        "\n",
        "df_test_full = pd.DataFrame(columns=X.columns)\n",
        "for column in df_test_full.columns:\n",
        "        df_test_full[column] = testing[column]        \n",
        "\n",
        "Xuse = pipe.transform(df_test_full)\n",
        "for v in vars:\n",
        "    df_test_full = append_composite( df_test_full, vars[v], v, X, Xuse, lr )\n",
        "\n",
        "df_test = pd.DataFrame(columns=Xshort.columns)\n",
        "for column in df_test.columns:\n",
        "        df_test[column] = df_test_full[column]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d050e8a-a75d-9523-5064-5892c34d7b47"
      },
      "outputs": [],
      "source": [
        "def append_series( X_train, X_test, train_input, test_input, sername ):\n",
        "    vtrain = pd.Series( train_input[sername], name=sername, index=X_train.index )\n",
        "    X_train_out = pd.concat( [X_train, vtrain], axis=1 )\n",
        "    vtest = pd.Series( test_input[sername], name=sername, index=X_test.index )\n",
        "    X_test_out = pd.concat( [X_test, vtest], axis=1 )\n",
        "    return( X_train_out, X_test_out )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8bf105a9-19f7-bc65-c7e2-1dedab44bf16"
      },
      "outputs": [],
      "source": [
        "# Arbitrary down-weighting of ridiculous prices\n",
        "wts *= (1 - .2*million1 - .1*million2 - .05*million3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca4406d1-dc12-ffc0-87a6-45db764bc0a7"
      },
      "outputs": [],
      "source": [
        "vars_to_add = [\n",
        "    \"kindergarten_km\", \n",
        "    \"railroad_km\", \n",
        "    \"swim_pool_km\", \n",
        "    \"public_transport_station_km\",\n",
        "    \"big_road1_km\",\n",
        "    \"big_road2_km\",\n",
        "    \"church_synagogue_km\",\n",
        "    \"ttk_km\",\n",
        "    \"metro_min_walk\",\n",
        "    \"kremlin_km\",\n",
        "    \"mosque_km\",\n",
        "]\n",
        "Xdata_train = Xshort\n",
        "Xdata_test = df_test\n",
        "print( Xdata_train.shape )\n",
        "print( Xdata_test.shape )\n",
        "for v in vars_to_add:\n",
        "    Xdata_train, Xdata_test = append_series( Xdata_train, Xdata_test, training, testing, v )\n",
        "print( Xdata_train.shape )\n",
        "print( Xdata_test.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7546343d-3b1c-a625-f401-fe5bd24b9a44"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18c976c2-cdaa-acba-520a-054777830b75"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import joblib\n",
        "xgb_params = {\n",
        "    'eta': xgb_lr,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'nthread': 4,\n",
        "    'silent': 1,\n",
        "}\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(Xdata_train, y, weight=wts)\n",
        "dtest = xgb.DMatrix(Xdata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2e40a94-72fe-5751-3dd9-97ac7dad66c4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n",
        "    verbose_eval=50, show_stdv=False)\n",
        "# cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
        "cv_output[\"test-rmse-mean\"][len(cv_output)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f267208c-23e3-c91a-c01d-c3cccdba0da7"
      },
      "outputs": [],
      "source": [
        "num_boost_rounds = len(cv_output)\n",
        "print( num_boost_rounds )\n",
        "model = xgb.train(xgb_params, dtrain, num_boost_round= num_boost_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d240ec20-3c53-ddf1-15d5-487d778f17a3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n",
        "xgb.plot_importance(model, height=0.5, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5eea5e0-2e25-657f-6b0e-1dc3cf82fd60"
      },
      "outputs": [],
      "source": [
        "y_predict = model.predict(dtest)\n",
        "predictions = np.exp(y_predict)*testing.cpi\n",
        "\n",
        "# And put this in a dataframe\n",
        "predxgb_df = pd.DataFrame()\n",
        "predxgb_df['id'] = testing['id']\n",
        "predxgb_df['price_doc'] = predictions\n",
        "predxgb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5c97475-4fb5-42e1-d525-010a1b0d552a"
      },
      "outputs": [],
      "source": [
        "predxgb_df.to_csv('xgb_predicitons.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2fee6ad3-5f67-af74-8fe3-c85d43cf34d5"
      },
      "outputs": [],
      "source": [
        "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
        "test[\"year\"]  = test[\"timestamp\"].dt.year\n",
        "test[\"month\"] = test[\"timestamp\"].dt.month\n",
        "test[\"yearmonth\"] = 100*test.year + test.month\n",
        "test_ids = test[[\"yearmonth\",\"id\"]]\n",
        "test_data = test_ids.merge(predxgb_df,on=\"id\")\n",
        "\n",
        "test_prices = test_data[[\"yearmonth\",\"price_doc\"]]\n",
        "test_p = test_prices.groupby(\"yearmonth\").median()\n",
        "test_p.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72c3794f-e1e6-bb60-9154-6295a7409ace"
      },
      "source": [
        "### Run the macro model and make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6396ed13-874a-4372-2c5f-c281727f46e5"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfd0f480-60e5-e5d4-fe41-72fa61ea36c3"
      },
      "outputs": [],
      "source": [
        "macro[\"timestamp\"] = pd.to_datetime(macro[\"timestamp\"])\n",
        "macro[\"year\"]  = macro[\"timestamp\"].dt.year\n",
        "macro[\"month\"] = macro[\"timestamp\"].dt.month\n",
        "macro[\"yearmonth\"] = 100*macro.year + macro.month\n",
        "macmeds = macro.groupby(\"yearmonth\").median()\n",
        "\n",
        "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
        "train[\"year\"]  = train[\"timestamp\"].dt.year\n",
        "train[\"month\"] = train[\"timestamp\"].dt.month\n",
        "train[\"yearmonth\"] = 100*train.year + train.month\n",
        "prices = train[[\"yearmonth\",\"price_doc\"]]\n",
        "p = prices.groupby(\"yearmonth\").median()\n",
        "\n",
        "df = macmeds.join(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "81f92404-d09b-6730-33a5-c13e7ed624d1"
      },
      "outputs": [],
      "source": [
        "#  Adapted from code at http://adorio-research.org/wordpress/?p=7595\n",
        "#  Original post was dated May 31st, 2010\n",
        "#    but was unreachable last time I tried\n",
        "\n",
        "import numpy.matlib as ml\n",
        " \n",
        "def almonZmatrix(X, maxlag, maxdeg):\n",
        "    \"\"\"\n",
        "    Creates the Z matrix corresponding to vector X.\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n",
        "    for t in range(maxlag,  n):\n",
        "       #Solve for Z[t][0].\n",
        "       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n",
        "       for j in range(1, maxdeg+1):\n",
        "             s = 0.0\n",
        "             for i in range(1, maxlag+1):       \n",
        "                s += (i)**j * X[t-i]\n",
        "             Z[t-maxlag,j] = s\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cb14347d-4b86-dbd1-f8ae-53cd5fca6705"
      },
      "outputs": [],
      "source": [
        "y_macro = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\n",
        "nobs = 47  # August 2011 through June 2015, months with price_doc data\n",
        "tblags = 5    # Number of lags used on PDL for Trade Balance\n",
        "mrlags = 5    # Number of lags used on PDL for Mortgage Rate\n",
        "ztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\n",
        "zmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\n",
        "columns = ['tb0', 'tb1', 'mr0', 'mr1']\n",
        "z = pd.DataFrame( np.concatenate( (ztb, zmr), axis=1), y_macro.index.values, columns )\n",
        "X_macro = sm.add_constant( z )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0180c753-f0de-1a60-267d-b52b99095dc6"
      },
      "outputs": [],
      "source": [
        "macro_fit = sm.OLS(y_macro, X_macro).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "92cce03a-dc23-0985-6dc3-6d262bd29178"
      },
      "outputs": [],
      "source": [
        "test_cpi = df.cpi.loc[201507:201605]\n",
        "test_index = test_cpi.index\n",
        "ztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\n",
        "zmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\n",
        "z_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test), axis=1), test_index, columns )\n",
        "X_macro_test = sm.add_constant( z_test )\n",
        "pred_lnrp = macro_fit.predict( X_macro_test )\n",
        "pred_p = np.exp(pred_lnrp) * test_cpi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebf80cd3-d0ff-8de5-cedc-d0b992be2f43"
      },
      "source": [
        "### Use macro model predictions to adjust XGBoost micro predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78af5200-7f60-8ab8-36c4-7bd10779542b"
      },
      "outputs": [],
      "source": [
        "adjust = pd.DataFrame( pred_p/test_p.price_doc, columns=[\"adjustment\"] )\n",
        "adjust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec5c7268-2b20-eeb7-a02f-b065de4acc22"
      },
      "outputs": [],
      "source": [
        "combo = test_data.join(adjust, on='yearmonth')\n",
        "combo['adjusted'] = combo.price_doc * combo.adjustment\n",
        "adjxgb_df = pd.DataFrame()\n",
        "adjxgb_df['id'] = combo.id\n",
        "adjxgb_df['price_doc'] = combo.adjusted\n",
        "adjxgb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fa532f7-e0b0-cf88-e641-a196c380e7fa"
      },
      "outputs": [],
      "source": [
        "adjxgb_df.to_csv('adjusted_xgb_predicitons.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4db38da5-a601-f605-3108-c276b37c6622"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}