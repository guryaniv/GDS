{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "d0fe2a91-d867-8517-e4e6-156ac2c593cb",
        "_active": false
      },
      "outputs": [],
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "b963daeb-6b13-e110-120a-609204358dfc",
        "_active": false
      },
      "outputs": [],
      "source": "data = pd.read_csv('../input/train.csv')\ndata.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_cell_guid": "a39eddc2-fee1-ac64-b106-494972cbf044",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "print(\"number of features:\" + str(len(data.columns.tolist())))\n\n#we will select some of these features that we think would influance \n#house prices. some of these features are categorical, ordinal or numerical\ncat_f = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'Utilities', \n        'Neighborhood', 'BldgType', 'HouseStyle', 'Heating', 'LandContour', 'LotConfig',\n        'LandSlope', 'Condition1', 'Condition2', 'RoofStyle', 'RoofMatl',\n        'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'CentralAir', 'Electrical',\n        'Functional', 'GarageType', 'GarageFinish', 'PavedDrive', 'SaleType', 'SaleCondition']\nnum_f = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearRemodAdd',\n        'TotalBsmtSF', 'SalePrice']\nord_f = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n         'GarageQual','GarageCond']\n\nall_clms = cat_f + num_f + ord_f\ndata = data[all_clms]\n\n#converting ord_f into numbers\nqual = {'Ex':1, 'Gd':2, 'TA':3, 'Fa':4, 'Po': 5}\n\n#for missing values we take the mean value of quality which is 3\nfor col in ord_f:\n    data[col] = data[col].apply(lambda x: 3 if x is np.nan else qual[x])\n\n#for missing values of numerical columns we fill it with the mean also\nfor col in num_f:\n    median = data[col].median()\n    data[col].where(pd.notnull(data[col]), median, inplace=True)\n\nfor col in cat_f:\n    data[col].where(pd.notnull(data[col]), 'not_a', inplace=True)\n    \n#apply get dummy method to convert the categorical features to numerical\ndata = pd.get_dummies(data)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_cell_guid": "71fba204-472b-4cae-6faa-1fd0cf06c117",
        "_active": false
      },
      "outputs": [],
      "source": "data.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "dd13999f-ee2b-dda3-e036-3a2057882174",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n#splitting data to train and test and try to run GBTR\nX_train, X_test, y_train, y_test = train_test_split(data.drop(['SalePrice'], axis=1),\n                                                    data['SalePrice'],\n                                                    test_size=0.30, random_state=45)\n\n# fit estimator\nest = GradientBoostingRegressor(n_estimators=100, max_depth=1, loss='ls')\nest.fit(X_train, y_train)\n\n# predict class labels\npred = est.predict(X_test)\n\n# score on test data (accuracy)\nacc = est.score(X_test, y_test)\nprint('ACC: %.4f' % acc)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "_cell_guid": "566246bd-b608-ee6e-69e5-96cbf99b12ed",
        "_active": false
      },
      "outputs": [],
      "source": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n              'max_depth': [4, 6],\n              'min_samples_leaf': [3, 5, 9, 17],\n              }\n\nest = GradientBoostingRegressor(n_estimators=3000)\ngs_cv = GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n\n# best hyperparameter setting\nprint(gs_cv.best_params_)\nprint(gs_cv.best_score_)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "_cell_guid": "44be06b1-8855-762c-7f8d-7e1710b73e0e",
        "_active": false
      },
      "outputs": [],
      "source": "validation_set = pd.read_csv('../input/test.csv')",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "_cell_guid": "5895e850-c4ee-61e4-6079-d767b2a8f096",
        "_active": false
      },
      "outputs": [],
      "source": "validation_set.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "_cell_guid": "dd012e5f-0933-77d7-e000-57d4042574b7",
        "_active": false
      },
      "outputs": [],
      "source": "validation_set_id = validation_set['Id']\n\nall_clms.remove('SalePrice')\nvalidation_set = validation_set[all_clms]\n\n#for missing values we take the mean value of quality which is 3\nfor col in ord_f:\n    validation_set[col] = validation_set[col].apply(lambda x: 3 if x is np.nan else qual[x])\n\n#for missing values of numerical columns we fill it with the mean also\nnum_f.remove('SalePrice')\nfor col in num_f:\n    median = validation_set[col].median()\n    validation_set[col].where(pd.notnull(validation_set[col]), median, inplace=True)\n\nfor col in cat_f:\n    validation_set[col].where(pd.notnull(validation_set[col]), 'not_a', inplace=True)\n    \n#apply get dummy method to convert the categorical features to numerical\nvalidation_set = pd.get_dummies(validation_set)\n\n#the validation set might have extra values that turns into columns\n#with git_dummy function or it might have less number of categorical values and won't produce\n#all the columns\nextra_clms   = list(set(validation_set.columns.tolist()) - set(X_train.columns.tolist()))\nmissing_clms = list(set(X_train.columns.tolist()) - set(validation_set.columns.tolist()))\n\n#extending the validation set with the extra clms\nfor c in missing_clms:\n    validation_set[c] = 0\n\n#removing the columns from validation_set that doesnt exist in the model\nfor c in extra_clms:\n    validation_set = validation_set.drop(c, axis=1)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "_cell_guid": "a0023624-c3c0-ce8a-b26f-7bc0a1f29ff7",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "prds = gs_cv.best_estimator_.predict(validation_set)\nresult = pd.DataFrame({'Id': validation_set_id, 'y': prds})\nresult.to_csv('predcs_new.csv', columns=['Id', 'y'], header=['Id', 'SalePrice'], index=False)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3bcbde10-776b-1abc-ae6b-7f19dec91d31",
        "_active": false
      },
      "outputs": [],
      "source": null
    }
  ]
}