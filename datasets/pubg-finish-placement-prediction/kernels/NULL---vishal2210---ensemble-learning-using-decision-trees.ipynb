{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test_V2.csv')\ntrain = pd.read_csv('../input/train_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b7c32bdce806d9da50dfd33f9d98a1dafdf481f"},"cell_type":"code","source":"# np.array(train)[:,12]\nx_train = train.iloc[:,3:-1].values\ny_train = train.iloc[:,-1].values\nx_test = test.iloc[:,3:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5570a0f739476dbb7f51cd186afef1b51a74fb90"},"cell_type":"code","source":"x_test[12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"631f8cdc071daae5c432a7a91d98a1fe72e2a6a6"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelencoder_X1 = LabelEncoder()\nx_train[:,12] = labelencoder_X1.fit_transform(x_train[:,12])\nx_test[:,12] = labelencoder_X1.fit_transform(x_test[:,12])\n\n# onehotencoder = OneHotEncoder(categorical_features = [12])\n# x_train = onehotencoder.fit_transform(x_train).toarray()\n# x_test = onehotencoder.fit_transform(x_test).toarray()\n\ny_train = np.nan_to_num(y_train,copy=True)\nfor i in range(len(y_train)):\n    if y_train[i] > 0.5: \n        y_train[i] = 1\n    else:\n        y_train[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c261872b4c300e06729df93a0acdcd33ad3f680"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_hastie_10_2\nimport matplotlib.pyplot as plt\n\n\"\"\" HELPER FUNCTION: GET ERROR RATE =========================================\"\"\"\ndef get_error_rate(pred, Y):\n    return sum(pred != Y) / float(len(Y))\n\n\"\"\" HELPER FUNCTION: PRINT ERROR RATE =======================================\"\"\"\ndef print_error_rate(err):\n    print('Error rate: Training: %.4f - Test: %.4f' % err)\n\n\"\"\" HELPER FUNCTION: GENERIC CLASSIFIER =====================================\"\"\"\ndef generic_clf(Y_train, X_train, X_test, clf):\n    clf.fit(X_train,Y_train)\n    pred_train = clf.predict(X_train)\n    pred_test = clf.predict(X_test)\n    return get_error_rate(pred_train, Y_train) ,pred_test\n    \n\"\"\" ADABOOST IMPLEMENTATION =================================================\"\"\"\ndef adaboost_clf(Y_train, X_train, X_test, M, clf):\n    n_train, n_test = len(X_train), len(X_test)\n    # Initialize weights\n    w = np.ones(n_train) / n_train\n    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n    \n    for i in range(M):\n        # Fit a classifier with the specific weights\n        clf.fit(X_train, Y_train, sample_weight = w)\n        pred_train_i = clf.predict(X_train)\n        pred_test_i = clf.predict(X_test)\n        # Indicator function\n        miss = [int(x) for x in (pred_train_i != Y_train)]\n        # Equivalent with 1/-1 to update weights\n        miss2 = [x if x==1 else -1 for x in miss]\n        # Error\n        err_m = np.dot(w,miss) / sum(w)\n        # Alpha\n        alpha_m = 0.5 * np.log( (1 - err_m) / float(err_m))\n        # New weights\n        w = np.multiply(w, np.exp([float(x) * alpha_m for x in miss2]))\n        # Add to prediction\n        pred_train = [sum(x) for x in zip(pred_train, \n                                          [x * alpha_m for x in pred_train_i])]\n        pred_test = [sum(x) for x in zip(pred_test, \n                                         [x * alpha_m for x in pred_test_i])]\n#     pred_train, pred_test = np.sign(pred_train), np.sign(pred_test)\n\n#     print('Predictions : ',pred_test[:9])\n    # Return error rate in train and test set\n    return get_error_rate(pred_train, Y_train) , pred_test\n\n\"\"\" PLOT FUNCTION ===========================================================\"\"\"\ndef plot_error_rate(er_train):\n    df_error = pd.DataFrame([er_train, er_test]).T\n    df_error.columns = ['Training']\n    plot1 = df_error.plot(linewidth = 3, figsize = (8,6),\n            color = ['lightblue'], grid = True)\n    plot1.set_xlabel('Number of iterations', fontsize = 12)\n    plot1.set_xticklabels(range(0,450,50))\n    plot1.set_ylabel('Error rate', fontsize = 12)\n    plot1.set_title('Error rate vs number of iterations', fontsize = 16)\n    plt.axhline(y=er_test[0], linewidth=1, color = 'red', ls = 'dashed')\n\n\"\"\" MAIN SCRIPT =============================================================\"\"\"\n\n    \n\nX_train, Y_train =x_train, y_train\nX_test = x_test\n\n# Fit a simple decision tree first\nclf_tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)\ner_tree,pred_test_main = generic_clf(Y_train, X_train, X_test, clf_tree)\n\n# Fit Adaboost classifier using a decision tree as base estimator\n# Test with different number of iterations\ner_train = np.array([er_tree])\n\n# er_train.append(er_tree)\nprint(er_train)\nprint()\nx_range = range(5, 20, 5)\nfor i in x_range:\n    er_i , pred_test = adaboost_clf(Y_train, X_train, X_test, i, clf_tree)\n    if(er_i < er_tree):\n        pred_test_main = pred_test\n    print(er_i)\n#     er_train.append(np.array([er_i]))\n\n# Compare error rate vs number of iterations\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aba71304fba7b5ad8591eadb7f1a2de839ee576d"},"cell_type":"code","source":"# np.isnan(y_train).any()\n# y_train = np.nan_to_num(y_train,copy=True)\nfor i in range(len(pred_test_main)):\n    if pred_test_main[i] > 0.5: \n        pred_test_main[i] = 1\n    else:\n        pred_test_main[i] = 0\nd = {'Id': list(test['Id']), 'winPlacePerc': list(pred_test_main)}\ndf_opt = pd.DataFrame(data=d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c6c172b089fee86b6c4d22e73703b4e24880d32"},"cell_type":"code","source":"# y_train = np.nan_to_num(y_train,copy=True)\ndf_opt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab7bed8caac824e22852fb19724afd0e3cf9e1ea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7acba0c69e921b1faa09b62de734eb2eaf3c5129"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29696ba3f3455637e777dcc6ebb78ef55ffe2f1f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}