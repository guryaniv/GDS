{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Ignore warnings\n#import warnings\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation,Dropout\nfrom keras.models import Model\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.tree import export_graphviz, DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\nimport os\nprint(os.listdir(\"../input\"))\n#warnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c81f2d28ea1e8348783c6045863fb5817561407c"},"cell_type":"code","source":"def cleanupFrame(data_frame):\n    \n    #total Nulls\n    #print(test.isnull().sum(axis = 0))\n    #print(data_frame[data_frame['PassengerId']==1306].Name)\n    #changing Cabin column\n    \n    cabin={'A' : 0, 'B' : 1, 'C':2, 'D':3, 'E':4 , 'F':5 , 'T':6 , 'G':7}\n    data_frame['Cabin']=data_frame['Cabin'].str.extract('([A-Z][0-9])', expand=False).str[:1].map(cabin)\n    data_frame['Cabin']=data_frame['Cabin'].fillna(8)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Cabin\"],prefix=\"Cabin\")\n    \n    #changing Name column\n    name={'Mrs.':0,'Mr.':1,'Master.':2,'Miss.':3,'Major.':4,'Rev.':4,'Dr.':4,'Ms.':3,'Mlle.':3,'Col.':4,'Capt.':4,'Mme.':0,'Countess.':4,'Don.':4,'Jonkheer.':4,'Sir.':4,'Lady.':4,'Dona.':4 }\n    data_frame['Name']=data_frame['Name'].str.extract('(Mrs\\.|Mr\\.|Master\\.|Miss\\.|Major\\.|Rev\\.|Dr\\.|Ms\\.|Mlle\\.|Col\\.|Capt\\.|Mme\\.|Countess\\.|Don\\.|Jonkheer\\.|Sir\\.|Lady\\.|Dona.)', expand=False).str[:].map(name)\n    data_frame['Name']=data_frame['Name'].fillna(lambda x: 0 if data_frame['Sex'] == 'female' else 1)\n    #g = sns.heatmap(data_frame[[\"Age\",\"Name\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Name\"],prefix=\"Name\")\n    #print(data_frame[['Name','Survived']].groupby(['Name']).mean())\n    #print(data_frame[data_frame['Name'].isnull()])\n    #print(data_frame.groupby(['Name'])['Name'].count())\n    \n    #change Sex\n    sex = {'male' : 0, 'female' : 1}\n    data_frame['Sex']=data_frame['Sex'].map(sex)\n    data_frame['Sex']=data_frame['Sex'].fillna(0)\n    \n    #change fare\n    data_frame['Fare']=data_frame['Fare'].fillna(data_frame['Fare'].median())\n    data_frame['Fare_bin'] = pd.qcut(data_frame['Fare'],5,labels=[1,2,3,4,5]).astype(int)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Fare_bin\"],prefix=\"Fare_bin\")\n    #print(data_frame['Fare_bin'].value_counts())\n    \n    #change Age\n    #print(data_frame['Age'].min(),data_frame['Age'].max(),data_frame['Age'].mean())\n    #data_frame['Age']=data_frame['Age'].fillna(data_frame['Age'].mean())\n    #data_frame['Age']=data_frame['Age'].astype(int)\n    # Index of NaN age rows\n       \n    # Edited to use Random Forest\n    index_NaN_age = list(data_frame[\"Age\"][data_frame[\"Age\"].isnull()].index)\n\n    for i in index_NaN_age :\n        age_med = data_frame[\"Age\"].median()\n        age_pred = data_frame[\"Age\"][((data_frame['SibSp'] == data_frame.iloc[i][\"SibSp\"]) & (data_frame['Parch'] == data_frame.iloc[i][\"Parch\"]) \n                                  & (data_frame['Pclass'] == data_frame.iloc[i][\"Pclass\"]))].median()\n        if not np.isnan(age_pred) :\n            data_frame['Age'].iloc[i] = age_pred\n        else :\n            data_frame['Age'].iloc[i] = age_med \n    \"\"\" \n    df_sub = data_frame[['Age','Name_0','Name_1','Name_2','Name_3','Name_4','Fare_bin_1','Fare_bin_2','Fare_bin_3','Fare_bin_4','SibSp']]\n    X_age_train  = df_sub.dropna().drop('Age', axis=1).astype(int)\n    Y_age_train  = data_frame['Age'].dropna()\n    X_age_test = df_sub.loc[np.isnan(data_frame.Age)].drop('Age', axis=1)\n     \n    regressor =RandomForestRegressor(n_estimators = 300)\n    regressor.fit(X_age_train.values, Y_age_train.values)\n    y_pred = np.round(regressor.predict(X_age_test),1)\n    data_frame.Age.loc[df.Age.isnull()] = y_pred\n    \"\"\"\n    bins = [ 0, 4, 12, 18, 30, 50, 65, 100] # This is somewhat arbitrary...\n    age_index = (1,2,3,4,5,6,7)\n    #('baby','child','teenager','young','mid-age','over-50','senior')\n    data_frame['Age_bin'] = pd.cut(data_frame['Age'], bins, labels=age_index).astype(int)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Age_bin\"],prefix=\"Age_bin\")\n    \n    #Changing Embarked -- map default to S as majority is embarked from there \n    embarked={'S' : 0, 'C' : 1, 'Q':2}\n    data_frame['Embarked']=data_frame['Embarked'].map(embarked)\n    data_frame['Embarked']=data_frame['Embarked'].fillna(0)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Embarked\"],prefix=\"Embarked\")\n    \n\n    #changing Ticket column keeping first letter ['A','W','F','L','5','6','7','8','9'] with 20 & C:10,P:11 S:12\n    \n    ticket={'1':1,'2':2,'3':3,'4':4,'C':10,'P':11,'S':12,'A':20,'W':20,'F':20,'L':20,'5':20,'6':20,'7':20,'8':20,'9':20}\n    data_frame['Ticket'] = data_frame['Ticket'].map(lambda x: x[0])\n    data_frame['Ticket'] = data_frame['Ticket'].map(ticket)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Ticket\"],prefix=\"Ticket\")\n    \n    #introducce a new column 'Family Size' Combining familt size >4 into others\n    data_frame['Familly_size'] = data_frame['SibSp'] + data_frame['Parch'] + 1\n    data_frame['Familly_size'] = data_frame['Familly_size'].map(lambda x: 0 if x > 4 else x)\n    data_frame = pd.get_dummies(data_frame, columns = [\"Familly_size\"],prefix=\"Familly_size\")\n    \n    #Putting PCass in Dummies \n    data_frame = pd.get_dummies(data_frame, columns = [\"Pclass\"],prefix=\"Pclass\")\n    \n    #Drop Columns\n    data_frame=data_frame.drop(['PassengerId','SibSp','Parch','Fare','Age'], axis=1) \n    #print(data_frame.describe(),data_frame.shape,data_frame.iloc[0] )\n    \n    return data_frame    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def read_csv(df):\n     \n    test=cleanupFrame(df)\n    \n    #dropping rows with Nulls\n    #test=test.dropna()\n    #print(test.isnull().sum(axis = 0))\n    #Normalize Classifiers\n    #K=(test-test.mean())/test.std()\n    Y_train=test[0:891]['Survived'].values\n    X_train=test[0:891].drop(['Survived'], axis=1).values\n    X_test=test[891:].drop(['Survived'], axis=1).values\n    print(test.head())\n    return X_train, Y_train,X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4666b66067535930c46e7e1741014663ba85854f"},"cell_type":"code","source":"def write_csv(filename,predictions):\n# Writing a CSV file submission. PassengerId,Prediction \n    my_submission = pd.DataFrame({'PassengerId': range(892,892+predictions.shape[0]), 'Survived': 0})\n    my_submission['Survived']=predictions\n    #print(my_submission['PassengerId'], my_submission['Survived'])\n    # you could use any filename. We choose submission here\n    my_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"597163cf87c0d29438356a9b7ea7f0cae070ad9a","scrolled":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/train.csv\")\ndf_test=pd.read_csv(\"../input/test.csv\")\ndf=df_train.append(df_test , ignore_index = True)\nX_train, Y_train,X_test=read_csv(df)\nprint(X_train.shape, Y_train.shape,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85340bd9696e17206d8feed106c99f7813df05f6"},"cell_type":"code","source":"\"\"\"\ndef TitanicModel(input_shape):\n    \n       X_train size[None,classifiers]\n       Y_train size[None,]\n    \n    # Define the inputpadding = 'same', placeholder as a tensor with shape input_shape. Think of this as your input image!\n    X_input = Input(input_shape)\n    X=Dense(224,  activation = \"relu\")(X_input)\n    X=Dropout(p=0.10)(X)\n    X=Dense(120,  activation = \"relu\")(X)\n    X=Dropout(p=0.20)(X)\n    X=Dense(56, activation = \"relu\")(X)\n    X=Dropout(p=0.30)(X)\n    X=Dense(1,  activation = \"sigmoid\")(X)\n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='TitanicModel')    \n    return model\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f48d96269ed9e0d14d41ee2a7b8622d9cab1d6ab","scrolled":false},"cell_type":"code","source":"\"\"\"\n#Neural Network\ntitanicModel = TitanicModel(X_train[0].shape)\ntitanicModel.summary()\nopt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\ntitanicModel.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\ntitanicModel.fit(x=X_train, y=Y_train, epochs=200, batch_size=50)\n\n\ntest_predictions=titanicModel.predict(X_test)\ntest_predictions=(test_predictions>0.5)*1\ntest_predictions=np.reshape(test_predictions,-1)\nwrite_csv('submission.csv',test_predictions)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53325b6cfe2104c189f35d824f9a45f9955383e6"},"cell_type":"code","source":"\"\"\"\n#Gives V22 - 79.25% accuracy%\n#Decision Tree\ndtModel = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=42)\ndtModel.fit(X_train, Y_train)\n# Predict for train data sample\ndtModel_prediction = dtModel.predict(X_train)\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train, dtModel_prediction)\nprint(\"Training Score:\",score)\n\n#Test Prediction\ntest_predictions=dtModel.predict(X_test)\ntest_predictions=(test_predictions>0.5).astype(int)\ntest_predictions=np.reshape(test_predictions,-1)\nwrite_csv('submission.csv',test_predictions)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b1ba9bf7dbb1b4aed1a2f722342e19b855aabfe"},"cell_type":"code","source":"\n#Gives 92 on training but on test set only 76% on test data by default Settings\n#Gives 81.1 with V30 -- RandomForestClassifier(n_estimators=200,min_samples_split=5, min_samples_leaf=4, random_state=42). Any move change any of parameters results in degrading performance. \n# Create and train model on train data sample Random Forest\nrfModel = RandomForestClassifier(n_estimators=150,min_samples_split=5, min_samples_leaf=5,oob_score=True,n_jobs=-1, random_state=42)\nrfModel.fit(X_train, Y_train)\nprint(\"%.4f\" % rfModel.oob_score_)\n#Predict for train data sample\nrfModel_prediction = rfModel.predict(X_train)\n#test=list(rfModel_prediction)\n#print((test==Y_train).sum(axis = 0)/891)\nscore = metrics.accuracy_score(Y_train, rfModel_prediction)\nprint(\"Training Score:\",score,)\n\n#Test Prediction\ntest_predictions=rfModel.predict(X_test)\ntest_predictions=(test_predictions>0.5)*1\ntest_predictions=np.reshape(test_predictions,-1)\nwrite_csv('submission.csv',test_predictions) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa93de782153f1797d84b3b50976bc45da79dfa8"},"cell_type":"code","source":"\"\"\"\n#AdaBoost\n# Create adaboost classifer object\n#dtModel = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=20, random_state=42)\n#adaBoostModel =AdaBoostClassifier()\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\nkfold = StratifiedKFold(n_splits=10)\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[10,20,30,40,50],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1,1.5]}\n\nadaBoostModel = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nadaBoostModel.fit(X_train,Y_train)\n#adaBoostModel_prediction = list(adaBoostModel.predict(X_train))\n#score=(adaBoostModel_prediction==Y_train).sum(axis=0)/891\nprint(\"Training Score:\",adaBoostModel.best_estimator_,adaBoostModel.best_score_ )\n\n\n#Test Prediction\ntest_predictions=adaBoostModel.predict(X_test)\ntest_predictions=(test_predictions>0.5)*1\ntest_predictions=np.reshape(test_predictions,-1)\nwrite_csv('submission.csv',test_predictions) \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f1c7e3dd20fcab75f8dc720d1394bff5e93e65"},"cell_type":"code","source":"\"\"\"\n#XBoost Gradient Boost\nkfold = StratifiedKFold(n_splits=10)\nxgb_param_grid = {\n                  'reg_alpha':[0.1]\n                  }\nxgb = XGBClassifier(learning_rate =0.001,\n                         n_estimators=800,\n                         max_depth=6,\n                         min_child_weight=5,\n                         gamma=0,\n                         subsample=0.8,\n                         colsample_bytree=0.8,\n                         objective= 'binary:logistic',\n                         nthread=4,\n                         scale_pos_weight=1,\n                         reg_alpha=0.1,\n                         seed=27)\nxgbModel = GridSearchCV(xgb,param_grid = xgb_param_grid, cv=kfold, scoring='roc_auc', n_jobs= 4, verbose = 1)\nxgbModel.fit(X_train, Y_train)\nprint(\"Best: %f using %s\" % (xgbModel.best_score_, xgbModel.best_params_))\nmeans = xgbModel.cv_results_['mean_test_score']\nstds = xgbModel.cv_results_['std_test_score']\nparams = xgbModel.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n# Predict for train data sample\nxgbModel_prediction = xgbModel.predict(X_train)\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train, xgbModel_prediction)\nprint(\"Training Score:\",score)\n#Test Prediction\ntest_predictions=xgbModel.predict(X_test)\ntest_predictions=(test_predictions>0.5)*1\ntest_predictions=np.reshape(test_predictions,-1)\nwrite_csv('submission.csv',test_predictions) \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"143d355402cff12176a455ce5cf5186f7db97563"},"cell_type":"code","source":"\"\"\"\n# Gradient boosting tunning\nkfold = StratifiedKFold(n_splits=10)\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n                  'n_estimators' : [100,200,300],\n                  'learning_rate': [0.1, 0.05, 0.01],\n                  'max_depth': [4, 8],\n                  'min_samples_leaf': [100,150],\n                  'max_features': [0.3, 0.1] \n                  }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\nprint(gsGBC.best_score_)\n#Test Prediction\n#test_predictions=gsGBC.predict(X_test)\n#test_predictions=(test_predictions>0.5)*1\n#test_predictions=np.reshape(test_predictions,-1)\n#write_csv('submission.csv',test_predictions)\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}