{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8455c544-a7fc-bf71-d182-04787d106120"
      },
      "source": [
        "test Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b74a8003-7d3b-4844-5f0d-a049fe2880c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.backend as K\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "from sphinx.addnodes import highlightlang\n",
        "\n",
        "#os.environ['PATH'] = os.environ['PATH'] + ';C:\\\\Users\\\\Roman\\\\Anaconda3\\\\Library\\mingw-w64\\\\bin\\\\'\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b0d295e-5d95-2704-57e3-2f50f9d0a623"
      },
      "outputs": [],
      "source": [
        "def prepareInputData(train_df, test_df):\n",
        "    n = train_df.shape[1] - 2  # count of X columns in input data\n",
        "\n",
        "    X = pd.concat([train_df[train_df.columns[-n:]], test_df[test_df.columns[-n:]]], ignore_index=True)\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "    train_X = X.head(train_df.shape[0])\n",
        "    test_X = X.tail(test_df.shape[0])\n",
        "\n",
        "    # usable_columns = list(set(X.columns))\n",
        "    #\n",
        "    # for column in usable_columns:\n",
        "    #     cardinality = len(np.unique(train_X[column]))\n",
        "    #     if cardinality == 1:\n",
        "    #         train_X.drop(column, axis=1)  # Column with only one value is useless so we drop it\n",
        "    #         test_X.drop(column, axis=1)\n",
        "\n",
        "    train_X = train_X.as_matrix()\n",
        "    test_X = test_X.as_matrix()\n",
        "\n",
        "    train_Y = train_df[\"y\"].as_matrix()\n",
        "    test_id = test_df[\"ID\"].as_matrix()\n",
        "\n",
        "    return train_X, train_Y, test_X, test_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d06627d7-b516-b911-50ac-d5ecefd082a3"
      },
      "outputs": [],
      "source": [
        "#def r2_keras(y_true, y_pred):\n",
        "#    SS_res =  K.mean(K.square( y_true - y_pred ))\n",
        "#    SS_tot = K.mean(K.square( y_true - K.mean(y_true) ))\n",
        "#    return ( 1 - SS_res/(SS_tot) )\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    return K.sum(K.square(y_true - y_pred))\n",
        "\n",
        "def r2_keras_neg(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ))\n",
        "    return -( 1 - SS_res/(SS_tot) )\n",
        "\n",
        "def r2_my(y_true, y_pred):\n",
        "    SS_res =  np.sum(np.square( y_true - y_pred ))\n",
        "    SS_tot = np.sum(np.square( y_true - np.mean(y_true) ))\n",
        "    #return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "    return ( 1 - SS_res/(SS_tot) )\n",
        "\n",
        "def makeNnEnsemble(X, Y):\n",
        "    models = []\n",
        "    predicted_true = np.empty(0)\n",
        "    predicted_y = np.empty(0)\n",
        "\n",
        "    scaler_X = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "    scaler_Y = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "    X = scaler_X.fit_transform(X)\n",
        "    Y = scaler_Y.fit_transform(Y.reshape(-1, 1)).reshape(len(Y))\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=False)\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        x_train, x_valid = X[train_index], X[test_index]\n",
        "        y_train, y_valid = Y[train_index], Y[test_index]\n",
        "\n",
        "        x_valid = x_train\n",
        "        y_valid = y_train\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(10, input_dim=X.shape[1], kernel_initializer='normal', activation='linear'))\n",
        "        model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "        # Compile model\n",
        "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r2_keras])\n",
        "\n",
        "        checkpoint = ModelCheckpoint('t.hdf5', save_best_only=True)\n",
        "        s = model.fit(x_train, y_train, epochs=20, verbose=2, validation_data=(x_valid, y_valid), callbacks=[checkpoint], batch_size=64)\n",
        "        model.load_weights(\"t.hdf5\")\n",
        "        ind = np.argmin(s.history[\"val_loss\"])\n",
        "        print(\"best validation loss=\", s.history[\"val_loss\"][ind], \" val_r2_keras=\", s.history[\"val_r2_keras\"][ind],\n",
        "              \" train loss=\", s.history[\"loss\"][ind], \" [\", ind, \"]\")\n",
        "\n",
        "        predicted_true = np.append(predicted_true, y_valid)\n",
        "        a = model.predict(x_valid).reshape(len(y_valid))\n",
        "        predicted_y = np.append(predicted_y, a)\n",
        "        print(\"a.shape: \" + str(a.shape))\n",
        "        print(\"x_valid.shape: \" + str(x_valid.shape))\n",
        "        print(\"y_valid.shape: \" + str(y_valid.shape))\n",
        "        print(\"r2: \" + str(r2_score(y_valid, a)))\n",
        "        print(\"r2 my: \" + str(r2_my(y_valid.reshape(len(y_valid)), a)))\n",
        "        print(\"r2 keras: \" + str(K.eval(r2_keras(K.variable(y_valid), K.variable(a)))) )\n",
        "        print(\"mse: \" + str(mean_squared_error(y_valid, a)) )\n",
        "\n",
        "\n",
        "    print(\"validation r2: \" + str(r2_score(predicted_true, predicted_y)))\n",
        "    \n",
        "\n",
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "train_df.head()\n",
        "\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "test_df.head()\n",
        "\n",
        "cols = train_df.shape[1]\n",
        "\n",
        "train_X, train_Y, test_X, test_id = prepareInputData(train_df, test_df)\n",
        "\n",
        "print(\"rows before averaging: \" + str(train_X.shape[0]))\n",
        "# train_X, train_Y = averageEqualRows(train_X, train_Y)\n",
        "print(\"rows after averaging: \" + str(train_X.shape[0]))\n",
        "\n",
        "makeNnEnsemble(train_X.astype(np.float32), train_Y)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}