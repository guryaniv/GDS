{"cells": [{"cell_type": "markdown", "metadata": {"_uuid": "c860dfe730a9e3d31c53bb54fd8d6aa92d6fc6d5", "_cell_guid": "5fe9af2a-1275-4c2a-abd8-30de796034a7"}, "source": ["This is a approach i am experimenting on, If its possible to get decent result by using pattern in the way sentence are created rather than the traditional Tfidf or word vec and other conventional methord.\n", "I would give credit to kaggle for helping me get more features than i know . Altough right now the model is not good and gives less than 40 performance, I am working on it improve performance. Anyway the no of features i pulled out can be used with any NLP project. Some are common some are creative. Anyone is open how i can improve the model.  "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d59bf73eae24a74ada87ece4abff88cf1642d5ea", "_cell_guid": "0025ed29-6892-44fa-bc50-aaac5559a4b2", "collapsed": true}, "outputs": [], "source": ["\n", "\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input/train.csv\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a902406593419b6cd61ab03673e73251f071c015", "_cell_guid": "5cb967ec-9907-4b82-b30c-dca49626596e", "collapsed": true}, "outputs": [], "source": ["import numpy as np # to do mathametical operation\n", "import string #for text pre-processing\n", "from nltk.corpus import stopwords #for removing stopwords\n", "import re #Regular expression operations\n", "import xgboost as xgb #For predicting the values\n", "from sklearn.model_selection import KFold #for cross validations(CV)\n", "from sklearn import metrics #for getting CV score\n", "from collections import Counter #counting of words in the texts\n", "import operator\n", "from nltk import ngrams\n", "import nltk # major package for language processing\n", "from nltk import word_tokenize # for toconizing\n", "import matplotlib.pyplot as plt"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1192c748f07f0dfa8909b48690283e1df0563913", "_cell_guid": "c5163faf-77b1-4cd2-81e3-3d4b95e89c19", "collapsed": true}, "outputs": [], "source": ["training_df = pd.read_csv(\"../input/train.csv\")"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "09f98e01486ec7a53920d8e296b22d5b3cc8b6c8", "_cell_guid": "e9786c2b-f30b-4866-b826-22444ef1543f", "collapsed": true}, "outputs": [], "source": ["training_df.head()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0e7d163d978b01c86a6f476504fd6fa3f8c8245a", "_cell_guid": "079bba55-b6ee-4721-b5fb-f871e98649aa", "collapsed": true}, "outputs": [], "source": ["training_author_df=training_df.groupby('author',as_index=False).count()\n", "training_author_df"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "10473ce4a8ff70fc7857502be09ca836755f8b0b", "_cell_guid": "bcd9e194-0057-4866-9218-98f18aa2f8e8", "collapsed": true}, "outputs": [], "source": ["# taking in the first field\n", "text_string=training_df.iloc[0]['text']\n", "text_string"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d277c86d618ccc62ac386a4d8ba7a6bf4e74b906", "_cell_guid": "2ba03044-5f3a-4865-bcd7-09ca0ee129cd", "collapsed": true}, "outputs": [], "source": ["string.punctuation\n", "def remove_punctuation_from_string(string1):\n", "    string1=string1.lower() # changing to lower case\n", "    translation_table=dict.fromkeys(map(ord,string.punctuation),' ')\n", "    string2=string1.translate(translation_table)\n", "    return string2\n", "print('After processing')\n", "test_string=remove_punctuation_from_string(text_string)\n", "test_string"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e9b1cd31c6bcaf4cdd901beca30e1204da38b895", "_cell_guid": "91b6b6f8-3681-40de-9f77-dec2fed02c14", "collapsed": true}, "outputs": [], "source": ["def remove_stopwords_from_string(string1):\n", "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #compiling all stopwords.\n", "    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n", "    return string2\n", "\n", "print('After processing')\n", "test_string = remove_stopwords_from_string(test_string)\n", "test_string"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "bfb1ae6985e569b3c61bcbdc50c4747d44e5e69c", "_cell_guid": "d8e3ad4c-3639-4ae7-966e-f26d371a5b65", "collapsed": true}, "outputs": [], "source": ["training_df['text_backup']=training_df['text']"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "aa88eeed9208f822452442e2b293503d81451106", "_cell_guid": "4acb3874-5715-4c5d-8f02-00dc20025f25", "collapsed": true}, "outputs": [], "source": ["# usiing apply to remove the unwanted words.\n", "training_df['text']=training_df['text'].apply(lambda x: remove_punctuation_from_string(x))\n", "training_df['text']=training_df['text'].apply(lambda x: remove_stopwords_from_string(x))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fa881ff81ab56bc1b151f57f06747e0695d26141", "_cell_guid": "c7d383a0-c4b4-4af4-a4de-0645012056f8", "collapsed": true}, "outputs": [], "source": ["# now I have cleaned the data. Its time for processing the data and create features from it.\n", "#Feature 1 : Finding total words in the sentance\n", "training_df['feature1']=training_df['text_backup'].apply(lambda x: len(str(x).split()))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "162021811e012dfb989242baefef151a2bc43245", "_cell_guid": "7bdf346a-173c-4c3c-975b-cee8e8b2aadf", "collapsed": true}, "outputs": [], "source": ["#Feature 2 : Counting no of charecter in a variable\n", "training_df['feature2']=training_df['text_backup'].apply(lambda x: len(str(x)))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "92beca948394b0903ef208e5420a022c1ff7f207", "_cell_guid": "ce5a1e64-be90-4a46-9ede-9a91e60d3ec2", "collapsed": true}, "outputs": [], "source": ["#Feature 3 : Avg leangth of words used in the sentance.\n", "training_df['feature3']=training_df['feature2']/training_df['feature1']"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "bd6c71021d7218bd67573ba1c8d4f84d81123898", "_cell_guid": "5a889d21-c6a0-454a-bc4a-b0a0d2490f4d", "collapsed": true}, "outputs": [], "source": ["# Feature 4: Count total stop words  in a sentence.\n", "stop_words=set(stopwords.words('english'))\n", "training_df['Feature4']=training_df['text_backup'].apply(lambda x: len([w for w in str(x).lower().split() \n", "                                                                       if w in stop_words ]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ed8554efcdcfd0fc7c506a90af68033f023f13a1", "_cell_guid": "76426dfd-8e5a-44f2-9114-2afefb935ae1", "collapsed": true}, "outputs": [], "source": ["#finding the words that are used the most \n", "all_text_without_sw= ''\n", "for i in training_df.itertuples():\n", "    all_text_without_sw = all_text_without_sw +str(i.text)\n", "    #getting count of each word\n", "    counts=Counter(re.findall(r\"[\\w']+\", all_text_without_sw))\n", "    #deleting from counts\n", "    del counts[\"'\"]\n", "    # getting top 50 words\n", "    sorted_x=dict(sorted(counts.items(), key=operator.itemgetter(1), reverse = True)[:50])\n", "    \n", "# Feature 5 : The count of top words\n", "    \n", "training_df['Feature5']= training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() \n", "                                                                     if w in sorted_x]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "23a7e5368de55c0ba30d38bf7b77a80cbfdd842f", "_cell_guid": "020836d4-2758-4710-b134-d25d8db1b044", "collapsed": true}, "outputs": [], "source": ["# Feature 6 : least used words\n", "reverted_x=dict(sorted(counts.items(), key=operator.itemgetter(1))[:1000])\n", "\n", "training_df['Feature6']=training_df['text'].apply(lambda x: len ([w for w in str(x).lower().split() \n", "                                                                  if w in reverted_x]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "155e299ba2241acae73ba3cf5c59bdac0fe4dc18", "_cell_guid": "82f3e30e-ce1c-4a13-bd5b-8e29855bf4fd", "collapsed": true}, "outputs": [], "source": ["# Feature 7 : Find the total no of puntuation\n", "training_df['Feature7']= training_df['text_backup'].apply(lambda x: len([w for w in str(x) \n", "                                                                         if w in string.punctuation]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "29ed951108d2ecb9eb966a0ab01aa785d9973b4f", "_cell_guid": "9a0da503-bac9-4eae-a008-c7e597750ce3", "collapsed": true}, "outputs": [], "source": ["#Feature-8: Count of UPPER case words.\n", "\n", "training_df['Feature8']=training_df['text'].apply(lambda x: \n", "                                                  len([w for w in str(x).replace('I','i')\n", "                                                                 .replace('A','a').split() if w.isupper()==True])) "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1939653f33ec2d1277dec941d328ba49dc38e8f1", "_cell_guid": "99539534-1702-499d-b5ce-533c116373f4", "collapsed": true}, "outputs": [], "source": ["#Feature-9: Count of Title case words\n", "\n", "training_df['Feature9']= training_df['text'].apply(lambda x: len([w for w in str(x).replace('I','i')\n", "                                                                  .replace('A','a').split() if w.istitle==True]))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9122d413ef46a7d1f1bdf941e31742729e08e8c6", "_cell_guid": "7c0dc3d8-9667-4ff4-b11d-e0cf7e14dad0", "collapsed": true}, "outputs": [], "source": ["starting_words = sorted(list(map(lambda word : word[:2],filter(lambda word :\n", "                                                               len(word) > 3,all_text_without_sw.split()))))\n", "sw_counts = Counter(starting_words)\n", "top_30_sw = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\n", "\n", "\n", "\n", "#Feature-10: Count of (Most words start with)\n", "training_df['Feature_10'] = training_df['text'].apply(lambda x: \n", "                                                      len([w for w in str(x).lower().split()\n", "                                                           if w[:2] in top_30_sw and w not in stop_words]) )"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c71b2c555fd5a43932cff9717cab960578cc9d77", "_cell_guid": "42493562-27a2-46f9-bf49-65a65232b5b7", "collapsed": true}, "outputs": [], "source": ["#Feature-11: Count of (Most words end with)\n", "ending_words = sorted(list(map(lambda word : word[-2:],filter(lambda word : len(word) > 3,all_text_without_sw.split()))))\n", "ew_counts = Counter(ending_words)\n", "top_30_ew = dict(sorted(sw_counts.items(), key=operator.itemgetter(1),reverse=True)[:30])\n", "training_df['Feature_11'] = training_df['text'].apply(lambda x: len([w for w in str(x).lower().split() \n", "                                                                     if w[:2] in top_30_ew and w not in stop_words]) )"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6fad415d1048a53c90d3bcd9de0abc73fa126c0c", "_cell_guid": "0511e326-6a66-4c8d-803e-f1d8602d56da", "collapsed": true}, "outputs": [], "source": ["di = {'EAP': 0,'HPL':1, 'MWS':2}\n", "training_df=training_df.replace({\"author\": di})\n", "testing_df=testing_df.replace({\"author\": di})"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b7ba95176fa3cfb80c943e348ca511dbcb605641", "_cell_guid": "9fcbdc4f-d7dc-4484-bcea-00b695e618b9", "collapsed": true}, "outputs": [], "source": ["y=training_df['author']\n", "X=training_df.drop(['author'],1)\n", "y=pd.get_dummies(y)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "74b18c3f0d8ab5bbe03fd84e56f48c1fcbea214f", "_cell_guid": "d838e004-8647-4119-8037-061930e1cc8b", "collapsed": true}, "outputs": [], "source": ["y.head()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c21cb7353762433619da371510ba922e50cd7c41", "_cell_guid": "06d05ecd-cefc-42e7-aaae-ec1a0705a319", "collapsed": true}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "Xtrain,Xtest,ytrain,ytest=train_test_split(X,y, test_size=0.33, random_state=42)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "793c103794c1c219d860aad674cf1b51fc748294", "_cell_guid": "c86ff709-f48e-4eca-a662-6900eb53d6ec", "collapsed": true}, "outputs": [], "source": ["#converting to matrix\n", "Xtrain=Xtrain.values\n", "ytrain=ytrain.values\n", "Xtest=Xtest.values\n", "ytest=ytest.values"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f4674be9290bd0c9a73b7d778e2579b52f48e103", "_cell_guid": "5a300c78-49ee-46bd-b09a-502c8a39cd08"}, "source": ["Builing a keras architecture. "]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6299c7b86974ce30a4060d53067ef1267de2d30c", "_cell_guid": "5b117c09-70ee-41a6-b8d8-6b89bdc253e0", "collapsed": true}, "outputs": [], "source": ["import keras\n", "from keras.models import Sequential\n", "from keras.layers import Dense"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9fd34473e50f37db9856caa6094b84328e39e07a", "_cell_guid": "09b467a7-50dd-4a9a-bb5d-812c9ca4f056", "collapsed": true}, "outputs": [], "source": ["classifier=Sequential() \n", "training_df.shape"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "858c2d5e050d6deb263b9770c74023ea1bf450d7", "_cell_guid": "79e2a211-c442-4ca3-ba9a-7cf3e73da60e", "collapsed": true}, "outputs": [], "source": ["classifier.add(Dense(units=10, kernel_initializer=\"uniform\",activation='relu',input_dim=11))\n", "classifier.add(Dense(units=8, kernel_initializer=\"uniform\", activation='softmax'))\n", "classifier.add(Dense(units=6, kernel_initializer=\"uniform\", activation='relu'))\n", "classifier.add(Dense(units=3, activation='softmax'))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8ace14a00f639fcdd218dd8e7f03d28d66c04cab", "_cell_guid": "c512c491-8a5b-4fb6-8d07-963f345f1d68", "collapsed": true}, "outputs": [], "source": ["classifier.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n", "classifier.fit(Xtrain,ytrain,batch_size=5, epochs=10)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8c4443e48c3e605da2e10459052a2be716388a5d", "_cell_guid": "3735daef-990d-4ecc-8090-c85ba8c59da0", "collapsed": true}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "ry=training_df['author']\n", "rX=training_df.drop(['author'],1)\n", "rXtrain,rXtest,rytrain,rytest=train_test_split(rX,ry, test_size=0.33, random_state=42)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "dd9d33eb2c87cb7e225be73e1df6ab46a520d675", "_cell_guid": "61c05223-f9d7-45e0-99ed-a63b182b3c16", "collapsed": true}, "outputs": [], "source": ["clf=RandomForestClassifier()\n", "clf.fit(rXtrain,rytrain)\n"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7f3fe01bc1507e9d84e29781dd7600513f40a2fa", "_cell_guid": "132b5a7c-da2b-4d67-a5d4-962a5db50196", "collapsed": true}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score\n", "print(cross_val_score(clf, rXtrain, rytrain))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e7daeb150b4eb96cdab3b733820e4b1c9837922e", "_cell_guid": "ecd140cd-cdf9-48ab-8325-f448c033037e", "collapsed": true}, "outputs": [], "source": []}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1}