{"cells":[{"metadata":{"_cell_guid":"361fbec4-2986-4cf5-81c2-80747d140796","_uuid":"16f2014b52ff07d1ef117c831654717ba429b96a","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"32699cc6-f496-42a3-b142-4758f83d469b","_uuid":"383bb1ec47c58a97416357ebc93ad12abc5f3bb0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', index_col='id', nrows=1000)\ntest = pd.read_csv('../input/test.csv', index_col='id')\nsubmission = pd.read_csv('../input/sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d60e4c3a-933d-42de-ad79-8d88dcf3f39d","collapsed":true,"_uuid":"af18ffabb67880545f6214d5511a20919d734040","trusted":true},"cell_type":"code","source":"def standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.replace(r\"\\n\", \" \")\n    df[text_field] = df[text_field].str.lower()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c094f114-6fa7-4da8-8f59-25d20e8a1059","_uuid":"1583af77147e4cd496e86e56907d966dca5c9aba","trusted":true,"collapsed":true},"cell_type":"code","source":"train = standardize_text(train, \"comment_text\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33bdb797-8be3-4359-bddb-59d588282fdd","_uuid":"c516755248c0a6ed5cc87cbd1b2ed3fa9ddb3597","trusted":true,"collapsed":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\ntrain[\"tokens\"] = train[\"comment_text\"].apply(tokenizer.tokenize)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d982aba-6157-4906-87c9-2862289fb9c8","_uuid":"16b620ce88fe465b768c41cfb72c8c773d17b8f0","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nall_words = [word for tokens in train[\"tokens\"] for word in tokens]\nsentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\nprint(\"Max sentence length is %s\" % max(sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94d0a16a-2127-4a8f-9740-0425d33fadb3","_uuid":"02e62e63f4238861e25377274e0342aa4845246e","trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f977742-d3ef-4e60-8280-b83271854e60","_uuid":"7f10586ff1e5cb8540354c23804d9709150880cd","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 10)) \nplt.xlabel('Sentence length')\nplt.ylabel('Number of sentences')\nplt.hist(sentence_lengths)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12f89b98-6f79-4fbe-9f6a-064938284fd2","_uuid":"5f06a9e2e2342ff6258788e34ab3f46f093b3831","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndef cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = train[\"comment_text\"].tolist()\nlist_labels = train[\"toxic\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=40)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35088003-6c34-4c70-9730-fd04cc13286b","_uuid":"d64689a25655d4f7b9af7ab9dcc09a36e0c31561","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib\nimport matplotlib.patches as mpatches\n\n\ndef plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            red_patch = mpatches.Patch(color='orange', label='Normal')\n            green_patch = mpatches.Patch(color='blue', label='Toxic')\n            plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"84840abc58383ebd9b8cfd34503d0f3d0b8fab1c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}