{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\".\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Reading train and test data\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_text = train_df['question_text'].fillna(\"dieter\").tolist()\ntrain_label = train_df['target'].values\n\ntest_df = pd.read_csv('../input/test.csv')\ntest_text = test_df['question_text'].fillna(\"dieter\").tolist()\n\nprint(f'Training size: {len(train_text)}')\nprint(f'Test size: {len(test_text)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70c4376a35e7a844797653dca24b328c6f5b7fd"},"cell_type":"code","source":"# Embeddings parameters\nGLOVE_DIR = '../input/embeddings' \nMAX_SEQUENCE_LENGTH = 30  # Maximum number of words in a sentence\nMAX_NB_WORDS = 40000  # Vocabulary size\nEMBEDDING_DIM = 300  # Dimensions of Glove word vectors\n\n# CNN Intent Model Parameters\nfilter_sizes = [1, 2, 3, 5] # One filter for each Conv2D layer\nnum_filters = 56\ndrop = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3edac544517a4fa0f9f83d49dd923b43bef9f070"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Reshape, Dropout, Concatenate\nfrom keras.layers import Conv2D, MaxPool2D, Embedding, BatchNormalization\nfrom keras.models import Model\n\n\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()\n\ndef featurize_text(train_text, test_text):\n    \"\"\"\n    Takes in input the train_text and test_text\n    and returns the featurized input as (x_train, y_train) and the word_index.\n\n    \"\"\"\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    input_data = train_text + test_text\n    tokenizer.fit_on_texts(input_data)\n    \n    _x_train = tokenizer.texts_to_sequences(train_text)\n    _x_test = tokenizer.texts_to_sequences(test_text)\n    \n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    x_train = pad_sequences(_x_train, maxlen=MAX_SEQUENCE_LENGTH)\n    x_test = pad_sequences(_x_test, maxlen=MAX_SEQUENCE_LENGTH)\n    \n    return x_train, x_test, word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caaa576ccf466cf1f475b9b20d66b4935c3d54f8"},"cell_type":"code","source":"def get_model(word_index):\n    embeddings_index = {}\n    f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d/glove.840B.300d.txt'), encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = ''.join(values[:-300])\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' % len(embeddings_index))\n    print('Found %s unique labels.' % y_train.shape[1])\n    #num_words = min(MAX_NB_WORDS, len(word_index))\n    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n\n    print(\"Creating Model...\")\n    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    \n    embedding = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],\n                          input_length=MAX_SEQUENCE_LENGTH, trainable=True)(inputs)\n    \n    reshape = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedding)\n\n    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBEDDING_DIM), padding='valid',\n                    kernel_initializer='he_normal', activation='elu')(reshape)\n    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBEDDING_DIM), padding='valid',\n                    kernel_initializer='he_normal', activation='elu')(reshape)\n    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), padding='valid',\n                    kernel_initializer='he_normal', activation='elu')(reshape)\n    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), padding='valid',\n                kernel_initializer='he_normal', activation='elu')(reshape)\n    \n    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1, 1), padding='valid')(\n        conv_0)\n    maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1, 1), padding='valid')(\n        conv_1)\n    maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1, 1), padding='valid')(\n        conv_2)\n    maxpool_3 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1, 1), padding='valid')(\n    conv_3)\n\n    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n    flatten = Flatten()(concatenated_tensor)\n    dropout = Dropout(0.2)(flatten)\n    preds = Dense(1, activation='sigmoid')(dropout)\n    model = Model(inputs=inputs, outputs=preds)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91d8e69a5cff63ad6d0064da43c577395b7dc8e2"},"cell_type":"code","source":"# Featurizing the input\nx_train, x_test, w_index = featurize_text(train_text, test_text)\ny_train = to_categorical(np.asarray(train_label))\ndel train_text\ndel test_text\ndel train_df\nassert len(x_train) == len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9068d278a6ebecba16cac296772a52d7a8c0d032"},"cell_type":"code","source":"model = get_model(w_index)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"036d0bc10fa76a1db8c1465934f67216637dfc45"},"cell_type":"code","source":"from keras.callbacks import Callback\nfrom sklearn.metrics import f1_score\nclass F1Evaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            y_pred = (y_pred > 0.35).astype(int)\n            score = f1_score(self.y_val, y_pred)\n            print(\"\\n F1 Score - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"689fe84163723dc900070f32461221d235da7ca1"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nbatch_size = 256\nepochs = 4\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, train_label, test_size = 0.05, random_state=42)\n\n#early_stopping = EarlyStopping(patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('./quora.model', save_best_only=True, verbose=1)\n#reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\nF1_Score = F1Evaluation(validation_data=(X_val, y_val), interval=1)\n\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                 verbose=True, callbacks = [F1_Score, model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0655682783666d30b91711240a992d319222c4c3"},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=256, verbose=True)\ny_pred = (y_pred > 0.35).astype(int)\ny_pred = [x[0] for x in y_pred]\n\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_pred})\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba9834648cb363d2cd606702c6bcf0c32d4bb3ca"},"cell_type":"code","source":"from IPython.display import HTML\nimport base64  \nimport pandas as pd  \n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index =False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(submit_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}