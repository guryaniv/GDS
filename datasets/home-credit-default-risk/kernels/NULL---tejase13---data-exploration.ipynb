{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Objective of the notebook\nThis notebook is meant for EDA of Application Dataset. We will be exploring different kinds of  numeric and categorical variables and also be plotting them. We will make use of Matplotlib and Seaborn library for plotting. \n\nThe secondary objective of the notebook is also to help us understand the different plotting functions of Matplotlib and Seaborn libraries. Generally a good idea of proceeding with the analysis is to find correlation of independent variables with the target variable and then explore some of the highly(both positive and negative) correlated variables. But as the purpose of the notebook is also to experiment with plotting libraries we will explore columns even if they don't have a high correlation.\n\nI would like to thank the brilliant kernels that are uploaded on Kaggle which inspired me to find new ways to explore data and create different kinds of graphs. \n\nThis is undoutedly the best kernel I found so far which inspired a lot of ideas and graphs in this jupyter notebook: https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction"},{"metadata":{"trusted":true,"_uuid":"1422327453e542a329ea1027d285c1b93be4fff3"},"cell_type":"code","source":"#Reading the dataset\napplication = pd.read_csv(\"../input/application_train.csv\")\napplication.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f050749ee0e26deced9b29ce31ab6c712c9c04b"},"cell_type":"markdown","source":"## 1. Is the dataset imbalanced?\nFor problems involving repayment of loans, credit etc. it is highly possible that the dataset is imbalanced"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e305550f52f616a9b53117e6952209dcecb48b3a"},"cell_type":"code","source":"target_col = application['TARGET'].value_counts()\nx = target_col.index.tolist()\ny = target_col.values\ntotal_observations = y.sum()\n\n'''Matplotlib code starts here'''\nfig = plt.figure()\nax = fig.add_subplot(111)\n#or use this: fig, ax = plt.subplots()\nbar_plot = ax.bar(x, y, width=0.5, color='gr')\nax.set_xticks(x)\nax.set_xlabel('Target variable values')\nax.set_xticklabels(['Repay Loan','Default Loan'], rotation=0, fontsize=15)\n\nax.set_ylim(ymin=0, ymax=300000)\nax.set_ylabel('Count of target variable')\nax.set_yticks(np.arange(0, 325000, 25000))\n\n#The commented code below will convert y axis into percentage\n# formatter = FuncFormatter(lambda y, pos: \"%d%%\" % (y))\n# ax.yaxis.set_major_formatter(formatter)\n\nfor rect in bar_plot:\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2., 0.99*height,\n            '%.2f' % ((height/total_observations)*100) + \"%\", ha='center', va='bottom', fontsize=15)\n\nplt.title('Distribution of target variable')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77044e6e06755a9984437cf040d6b51a60844b23"},"cell_type":"markdown","source":"Clearly we can see that the dataset is imbalanced. An approach to final modeling can involve using stratified k fold cross validation and hopefully the model will be capable of identifying the default loans in a better way"},{"metadata":{"_uuid":"9ca15991703c67078a084161fac2b4332302b3df"},"cell_type":"markdown","source":"# 2. Correlation of independent variables with target variables\nLet's list the top 10 postively and negatively correlated variables"},{"metadata":{"trusted":true,"_uuid":"9ed4468d787943f31567bec063e6fe017ff781e0"},"cell_type":"code","source":"corr_vars = application.corr()['TARGET'].sort_values()\npositive_corr = corr_vars[corr_vars > 0].sort_values(ascending=False).drop('TARGET')[:10]\nnegative_corr = corr_vars[corr_vars < 0].sort_values(ascending=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b767480bb6e596a1dc3321995448dda8e75fe522"},"cell_type":"code","source":"positive_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4f40385edce8ab252db292cd28d20b0545872b"},"cell_type":"code","source":"negative_corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfff1e5109cedc17e58be029437137f525bc3662"},"cell_type":"markdown","source":"It might be a good idea to start exploration with 'DAYS_BIRTH' and 'EXT_SOURCE_3' as they are highly correlated with the target variable"},{"metadata":{"_uuid":"1c9fe3df1f2371906580bee8d5127e88fc075bf2"},"cell_type":"markdown","source":"# 3. Columns with missing values"},{"metadata":{"trusted":true,"_uuid":"9bb71c53abccd5be1d3210b40dd7926add81a1b8"},"cell_type":"code","source":"cols_with_missing_values = len(application.isnull().sum()[application.isnull().sum() > 0])\ntotal_cols = application.shape[1] - 1\nprint(\"{} columns out of total {} columns have missing values\".format(cols_with_missing_values, total_cols))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4685186e476ee692496904853c04755a0d6f439d"},"cell_type":"markdown","source":"More than 50% of the columns having missing values. While algorithms like Gradient boosting does not make it necessary to impute missing values we can still see which columns have high proportion of missing values"},{"metadata":{"trusted":true,"_uuid":"87678a7e495340ad6a5c5cd66f7ff3329ff54248"},"cell_type":"code","source":"missing_cols_prcnt = application.isnull().sum()/application.shape[0] * 100\nhigh_missing_values = missing_cols_prcnt[missing_cols_prcnt > 50]\nhigh_missing_values_index = high_missing_values.index.tolist()\nprint(\"{} columns have more than 50% missing values\".format(len(high_missing_values_index)))\nprint(\"Top 10 columns with highest proportion of missing values:\")\nhigh_missing_values.sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ed6f95f9ab3266f4ac0c20f9d456c5fbec7c90"},"cell_type":"markdown","source":"Columns with a lot of missing values is quite high. Now removing all the columns is not possible. So here we can adopt a 2 step approach<br>\n1) Find correlation with target variable and drop those columns which have  absolute correlation lower than 0.02<br>\n2) For the remaining variables, imputation can be done by median for numerical variables and mode for categorical variables"},{"metadata":{"trusted":true,"_uuid":"d72c6e3d8465de68ed16703699002875d5db9877"},"cell_type":"code","source":"correlations = application.corr()['TARGET'].sort_values()\ncorr_missing_cols = correlations.reindex(high_missing_values_index).sort_values()\nmissing_cols_to_be_dropped = corr_missing_cols.index.difference(corr_missing_cols[(corr_missing_cols > 0.02) | (corr_missing_cols < -0.02)].index).tolist()\nprint(\"Number of columns that are to be dropped: {}\".format(len(missing_cols_to_be_dropped)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e07b34754b20c6d0eea0ba2afdd0368d67c2854"},"cell_type":"markdown","source":"# 4. Exploring AMT columns\n1. We will start by having a look at the summary statistics (there are 4 columns in total)\n2. We will plot the distribution along with its boxplot. As for implementation details we will go through how we can dynamically plot multiple subplots"},{"metadata":{"trusted":true,"_uuid":"6cc8f0d9f961b444ccf19d25e48b306b68b35353"},"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}'.format #Used to avoid scientific notation\napplication[[\"AMT_INCOME_TOTAL\", \"AMT_ANNUITY\", \"AMT_CREDIT\", \"AMT_GOODS_PRICE\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"708bc1a3bd10b5847eecb77c793445ec385e6284"},"cell_type":"markdown","source":" Let's initially plot a distribution along with its boxplot using Seaborn"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b0542be54458a5a59e3b88c5b8afd5b10e02a628"},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.distplot(application[\"AMT_CREDIT\"], color = 'blue', ax = ax1)\nsns.boxplot(x=application[\"AMT_CREDIT\"], ax = ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8177502def0ee3c45846b68856a320ac4bb1683"},"cell_type":"markdown","source":"The distribution looks right skewed which makes sense as not many people would be taking a very high amount loan. The next thing we are going to do is to plot multiple plots by using GridSpec. "},{"metadata":{"trusted":true,"_uuid":"e3ea3f85cc55346067143b24bf7cb2b492955dc8"},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nG = gridspec.GridSpec(4, 4)\nG.update(wspace=0.25, hspace=0.5)\nplt.figure(figsize = (20,20))\naxes_l = []\nnumeric_cols = [\"AMT_INCOME_TOTAL\", \"AMT_ANNUITY\", \"AMT_CREDIT\", \"AMT_GOODS_PRICE\"]\nrow_index = 0\ncol_index = 0\naxes_count = 0\nfor i, col in enumerate(numeric_cols):\n    #Plotting distribution plot \n    row_index = i\n    axes_l.append(plt.subplot(G[row_index, col_index]))\n    if application[col].isnull().sum() == 0:\n        sns.distplot(application[col], color = 'blue', ax = axes_l[axes_count])\n    else:\n        sns.distplot(application[col].dropna(), color = 'blue', ax = axes_l[axes_count])\n    plt.title('Distribution plot:'+col)\n              \n    axes_count+=1\n    col_index+=1\n    #Plotting boxplot\n    axes_l.append(plt.subplot(G[row_index, col_index]))\n    if application[col].isnull().sum() == 0:\n        sns.boxplot(application[col], ax = axes_l[axes_count])\n    else:\n        sns.boxplot(application[col].dropna(), ax = axes_l[axes_count])\n    plt.title('Boxplot:'+col)    \n              \n    axes_count += 1\n    col_index = 0\n\n# axes_l\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d7e0914bb8d5a59f038b81946ed80aa0e1efb6e"},"cell_type":"markdown","source":"* Most of the distributions are right skewed. If one wants to proceed with linear modeling then they might as well think about Log transforming the variables\n* AMT_GOODS_PRICE and AMT_CREDIT seem to have similar distribution which is also confirmed by summary statistics. Let's look at the correlation between the 2 columns and also plot a scatterplot\n "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"27740b7434456629cb8b0800e77ac1b2b9fbfe56"},"cell_type":"code","source":"application[['AMT_GOODS_PRICE', 'AMT_CREDIT']].corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a187627f416cb694546b015568f9093b9e9148c0"},"cell_type":"markdown","source":"Let's just check if any of the two variables have missing values"},{"metadata":{"trusted":true,"_uuid":"9a9d3cbea5b4ec1ae882f7b972d4b4d3ed09bda1"},"cell_type":"code","source":"application[['AMT_GOODS_PRICE', 'AMT_CREDIT']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36c4d74a52036e861e57b084a0e9b00c594ad409"},"cell_type":"code","source":"plt.scatter(x = application['AMT_GOODS_PRICE'], y = application['AMT_CREDIT'], alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f60e75df333f11c209114b47c7d07bc52983feab"},"cell_type":"markdown","source":"There is a very high correlation between these 2 columns. So we will be imputing the missing values of AMT_GOODS_PRICE by AMT_CREDIT by taking a factor of avg( AMT_CREDIT)/avg( AMT_GOODS_PRICE) * AMT_CREDIT"},{"metadata":{"_uuid":"e2aef85a7f6623f930c32ae2dcfe83bca5ea658f"},"cell_type":"markdown","source":"# 5. Inspecting age variable\n1. Describe function (summary statistics) will help us in QC'ing the column. We can get to know if there any outliers\n2. Then we will plot the distribution when target = 0 and target = 1 and see if age plays any role in defaulting a loan"},{"metadata":{"trusted":true,"_uuid":"110fc2036b2d278e89fd2eafd25b9ecc6d1ec985"},"cell_type":"code","source":"application[\"AGE\"] = application[\"DAYS_BIRTH\"].abs()/365\napplication[\"AGE\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"370fcde7d41418ca20e865175fa59758fbf45c8e"},"cell_type":"markdown","source":"Max and min shows that the values of this variable all lie within the range of 20 to 70. Thus it looks like this column has clean data"},{"metadata":{"trusted":true,"_uuid":"d1d9b825f10c1b5881a145a52dcce56e30927ef9"},"cell_type":"code","source":"#Plotting age distribution\nfig, ax = plt.subplots()\nsns.distplot(application[\"AGE\"], color = 'blue', bins=20, kde=False, norm_hist=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"726e7e12464bf7d3b63ab307b20e789aaf223502"},"cell_type":"markdown","source":"To see if age is influencing loan default or not we will plot the density plots with target = 0 and target = 1 and hopefully it will give us some clarity"},{"metadata":{"trusted":true,"_uuid":"dd98dd5b9f7dd2979b465ba3d7594f4793e84a4a"},"cell_type":"code","source":"sns.kdeplot(application[application[\"TARGET\"]==1][\"AGE\"], color = 'blue', label = 'target == 1')\nsns.kdeplot(application[application[\"TARGET\"]==0][\"AGE\"], color = 'red', label = 'target == 0')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d20eeb61a164d232893e7b03678b7d98d0b6eba"},"cell_type":"markdown","source":"\nIt is evident that age is influencing the default rate as younger people have a higher tendency of default. "},{"metadata":{"trusted":true,"_uuid":"2bb22e5f76ca256ef0e5fece0ba9366ea3a81d06"},"cell_type":"markdown","source":"# 6. Looking at Days_ columns"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"60497d729476b69b611111acee5bf0014648d11d"},"cell_type":"code","source":"days_cols = []\ndays_cols = [col for col in application.columns if col.find(\"DAYS\")!=-1]\napplication[days_cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2d286a9d9bbe317a22945703b71b3e16d433af6"},"cell_type":"markdown","source":"All the days are negative as time is considered relative to the date of application. However, we observe that Days_employed has maximum value of 365243. According to the business, this value represents Nulls. So we will first look at how many nulls are present and then impute the nulls and create a binary flag column which states whether the value of DAYS_EMPLOYED is null or not."},{"metadata":{"trusted":true,"_uuid":"dcc345e5d1b2f3ad7dcdb2e6b1cd69ece5764016"},"cell_type":"code","source":"application[application[\"DAYS_EMPLOYED\"] > 0][\"DAYS_EMPLOYED\"].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9fc20d9cc46fe708be3c1a5bab49cb7c371ea5c"},"cell_type":"markdown","source":" We will create a binary column where 1 will indicate anomalous data and 0 otherwise. We will also replace anomalous data with 0"},{"metadata":{"trusted":true,"_uuid":"81f4c7001686ea2419344bdb7bda4e48c31b3ab7"},"cell_type":"code","source":"#Creating the binary column and setting the value = 1 wherevr the value of days_employed will be 365243\napplication[\"DAYS_EMPLOYED_ANOMALY\"] = 0\nanomalous_indices = application[application[\"DAYS_EMPLOYED\"] > 0][\"DAYS_EMPLOYED\"].index\napplication.loc[anomalous_indices, \"DAYS_EMPLOYED_ANOMALY\"] = 1\n#Replacing anomalies with 0\napplication[\"DAYS_EMPLOYED\"].replace(365243, 0, inplace=True)\napplication[[\"DAYS_EMPLOYED\", \"DAYS_EMPLOYED_ANOMALY\"]][:15]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db83dc85402fdeed691347a4bc442fcb0fcbc9a3"},"cell_type":"markdown","source":"# 7. Investigating EXTSOURCE columns\nThe information comes from external sources and is normalized between 0 - 1. We don't have much information as to how it is derived. The EXTSOURCE3 is highly correlated to our target variable. We will plot the distributions for these columns which will be similar to Age distribution and we will also have a look at the missing values present in these columns."},{"metadata":{"trusted":true,"_uuid":"8e7538636d1ac09447b839010f2d7dbb75899e44"},"cell_type":"code","source":"application[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d7c7e39da39617246cae3249e233efdbf929759"},"cell_type":"markdown","source":"i) Plotting the distributions for target = 1 and target = 0"},{"metadata":{"trusted":true,"_uuid":"54cd6c1df6440e2c07ddf9ea909a1e7473b98340"},"cell_type":"code","source":"#The code below was obtained from the kaggle kernel mentioned above\n#The code can also be implemented in a way similar to what was described in AMT columns\n\nplt.figure(figsize = (12, 12))\n# iterate through the new features\nfor i, feature in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(application.loc[application['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(application.loc[application['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb98dbe80f9e228782ff92af634e5869ba58598b"},"cell_type":"markdown","source":"There is a clear distinction between distributions for target =0 and target = 1 for EXT_SOURCE_1 and EXT_SOURCE_3. Lower values of ext_source columns indicate a high default chance and higher values of these columns represent a lower default chance"},{"metadata":{"_uuid":"e8ce52f17ae475e28220be671244d3e4d9d63d20"},"cell_type":"markdown","source":"ii) Missing values check"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bcc57a6e3913ab0caaa775f80ed1831f326b7317"},"cell_type":"code","source":"print(\"The following numbers are indicative of missing values in %\")\napplication[[\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"]].isnull().sum()/application.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2e1aee27610effdd5df7b5f0b0cbdb0a9fc1cd"},"cell_type":"markdown","source":"As we don't have much information as how to ext_source_data is derived and with a lot of missing values present, our only guess of imputing the missing values is by median"},{"metadata":{"_uuid":"360fc587f22d0431701101e7a451f13c35256bf6"},"cell_type":"markdown","source":"With this we draw an end to analysis of continuous variables and this by no means is an exhaustive EDA as there are 100's of continuous variables in the entire dataset. But there is one interesting question that arises after looking at the KDE (or distribution) plots. How come the y-axis of these plots have values greater than 1? Here are some links on the same topic:\n* https://en.wikipedia.org/wiki/Kernel_density_estimation\n* https://stackoverflow.com/questions/32274865/seaborn-distplot-y-axis-normalisation-wrong-ticklabels\n* https://stats.stackexchange.com/questions/138484/displaying-frequency-when-using-kernel-density-estimation\n* https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0"},{"metadata":{"trusted":true,"_uuid":"26b01945c30c67e46538ef4a526f6c04a8231804"},"cell_type":"markdown","source":"# It's time for categorical variables now!!\nWhen it comes to visualizing categorical variables, bar charts are one of the most common and popular visualziation technique. It's pretty effective as well. Here we will explore Gender and NAME_INCOME_TYPE variables. <br>\nWe will look at how to plot the following variations of bar chart using matplotlib: \n    1. Normal bar chart (Both horizontal and vertical)\n    2. Group bar chart \n    3. Stacked bar chart "},{"metadata":{"_uuid":"770dc848dbb4fd0181dc25e442c8384e22d1462a"},"cell_type":"markdown","source":"# 8. Age variable"},{"metadata":{"_uuid":"4f4cb7a2ad2deb5150ca5005c7581d62f0de4120"},"cell_type":"markdown","source":"Normal bar chart will allow us to look at the count( or proportion) of each gender"},{"metadata":{"trusted":true,"_uuid":"7472c7668c3581b1daa3fc6f99f5529d93c5a442"},"cell_type":"code","source":"#One line of code to plot bar plot\napplication[\"CODE_GENDER\"].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7a30ed8bedaccfdd010089313c7f3cd46921f958"},"cell_type":"code","source":"gen_typ = application[\"CODE_GENDER\"].value_counts()\ngen_typ_vals = gen_typ.values\ngen_typ_idx =  gen_typ.index.tolist()\nprint(gen_typ_vals, gen_typ_idx)\nplt.bar(gen_typ_idx, gen_typ_vals)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3bacc235128c2d8d2d7b28d08d038e408108c76"},"cell_type":"markdown","source":"Grouped bar chart (Helps us in knowing what proportion of each gender repaid or defaulted the loan)"},{"metadata":{"trusted":true,"_uuid":"81bec653924bd942a04b63d624664243c3f4f3a5"},"cell_type":"code","source":"repay = []\ndefault = []\ngender = application[\"CODE_GENDER\"].unique()\nfor g in gender:\n    default.append(application[(application[\"CODE_GENDER\"]==g) & (application[\"TARGET\"]==1)].shape[0]/application[(application[\"CODE_GENDER\"]==g)].shape[0] * 100)\n    repay.append(application[(application[\"CODE_GENDER\"]==g) & (application[\"TARGET\"]==0)].shape[0]/application[(application[\"CODE_GENDER\"]==g)].shape[0] * 100)\n\nfig, ax = plt.subplots(figsize=(10,5))\npos = list(range(len(gender)))\nwidth = 0.33\nplt.bar(pos, repay, width, color='g')\nplt.bar([p+width for p in pos], default, width, color='r')\nplt.legend([\"Repay\", \"Default\"])\n\n#X-axis manipulations\nax.set_xticks([p+0.5*width for p in pos])\nax.set_xticklabels(list(gender))\nax.set_xlabel('Gender')\n\n#Y-axis manipulations\nvals = ax.get_yticks()\nax.set_yticklabels(['{:,.2%}'.format(x/100) for x in vals])\nax.set_ylabel('% of repaid/defaulted loans')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fbb7aad25566827bbb7bdd14fc8a4964f7f05e0"},"cell_type":"markdown","source":"Stacked percentage bar chart"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b3215cddedf2f0655eb70dba573323ab8e480dbd"},"cell_type":"code","source":"repay = []\ndefault = []\ngender = application[\"CODE_GENDER\"].unique()\nfor g in gender:\n    default.append(application[(application[\"CODE_GENDER\"]==g) & (application[\"TARGET\"]==1)].shape[0]/application[(application[\"CODE_GENDER\"]==g)].shape[0] * 100)\n    repay.append(application[(application[\"CODE_GENDER\"]==g) & (application[\"TARGET\"]==0)].shape[0]/application[(application[\"CODE_GENDER\"]==g)].shape[0] * 100)\n\nfig, ax = plt.subplots(figsize=(10,5))\npos = list(range(len(gender)))\nwidth = 0.5\nplt.bar(pos, repay, width=width, color='g', label='Repay')\nplt.bar(pos, default, width=width, bottom=repay, color='r', label='Default')\nplt.legend([\"Repay\", \"Default\"])\n\nax.set_xticks([p for p in pos])\nax.set_xticklabels(list(gender))\nax.set_xlabel('Gender')\n\nvals = ax.get_yticks()\nax.set_yticklabels(['{:,.2%}'.format(x/100) for x in vals])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de93b3ddc1cf94f2acc28ab264aec00664ac5e1b"},"cell_type":"markdown","source":"Finding: Men seem to have a higher proportion of default % but also men have double the applications as compared to women"},{"metadata":{"_uuid":"160ea76e47c5125bfbff9e7aac0e5ff07049e4ee"},"cell_type":"markdown","source":"# 9. Exploring NAME_INCOME_TYPE variable (Perils of blindly trusting graphs)\nThe visualizations will be very similar to what we did with the GENDER variable but we will look at how certain graphs can be misleading "},{"metadata":{"trusted":true,"_uuid":"c0ed1cf7325fe7ab93375fcf7dc0e4b057ee657a"},"cell_type":"code","source":"repay = []\ndefault = []\ninc_typ = application[\"NAME_INCOME_TYPE\"].unique()\nfor g in inc_typ:\n    default.append(application[(application[\"NAME_INCOME_TYPE\"]==g) & (application[\"TARGET\"]==1)].shape[0]/application[(application[\"NAME_INCOME_TYPE\"]==g)].shape[0] * 100)\n    repay.append(application[(application[\"NAME_INCOME_TYPE\"]==g) & (application[\"TARGET\"]==0)].shape[0]/application[(application[\"NAME_INCOME_TYPE\"]==g)].shape[0] * 100)\n    \nfig,ax = plt.subplots(figsize=(15,5))\nwidth = 0.7\npos = list(range(len(repay)))\nplt.bar(pos, repay, width=width, color='g')\nplt.bar(pos, default, width=width, bottom=repay, color='r', label='Default')\nax.set_xticks([p for p in pos])\nax.set_xticklabels(inc_typ)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"086067d737364b5ecf84aa8032a6a36d342f869d"},"cell_type":"markdown","source":"While it's not surprising to see unemployed with a higher default rate compared to others but what's baffling is that maternity leave category has the highest proportion of default %. <br>\nBefore jumping to conclusions we should make sure what is the count(rows) of each of these categories in the column"},{"metadata":{"trusted":true,"_uuid":"8dc94f55bac48fab5c6693e9c2288a932a0878dc"},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,5))\ninc_type = application[\"NAME_INCOME_TYPE\"].value_counts()\ninc_type.plot.bar(color='b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee766290d3a83f101cb08b42cf4e6a054f08791b"},"cell_type":"code","source":"inc_type","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57758a33b9f9c68f21c42fe9baf3c4f0f3e75bf0"},"cell_type":"markdown","source":"Looking at the count of rows for each category, it becomes clear that there are very few records for 'Unemployed' and 'Maternity leave' and it would have been disastrous jumping to conclusions based on the stacked bar chart alone."},{"metadata":{"_uuid":"111fc057a2ac500a5affa9fabe7e3e7b7b9edf67"},"cell_type":"markdown","source":"# 10. Learnings\nWhile there are a number of other variables that can be explored, we nonetheless have a vast list of learnings from the above exploration:\n\nLet's divide the learning section into two parts: 1. Data exploration and processing 2. Python visualization\n\n**Data Exploration and Processing**\n1. Correlation: Serves as a good starting point for analysis\n2. Missing value treatment:\n    *  Dropping columns: Here we dropped columns that had a very high proportion of missing values and had low correlation with target variable. This may not be always be a good idea\n    * Imputation by common methods: Mean and Median for continuous variables and Mode for categorical variables\n    * Imputation (other techniques): Here in one case we found that there exists a high correlation between two variables and based on that, imputation was carried out. Similarly business or domain knowledge can also be applied to impute certain columns. This process is time-consuming but can be effective\n    * Leave as it is: Certain algorithms like Gradient Boosting are not affected by missing values so it is not necessary to treat them\n3. Data consistency: Summary statistics(describe() in pandas) of the columns can be a way to QC the data\n    * Helps in identifying ouliers present in the data\n    * We looked at a way of how to create a binary column for anomalous data\n\nSome points that are a part of pre-processing but were not covered here\n1. Log transformation of Skewed variables for linear modeling\n2. Outlier treatment: Some commonly known techniques \n    * Univariate detection: Z-score, IQR      \n    * Multivariate detection: Mahalanobis distance, Cook's distance(mostly for linear regression)\n3. One hot encoding for categorical variables\n\n\n**Python Visualization**\n1. Bar graph\n2. Stacked and grouped bar chart\n3. Histogram\n4. Density plots\n5. Scatterplots"},{"metadata":{"trusted":true,"_uuid":"821fdb910dbd54958f3c41fa02a4d653c364432f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f2bec7d1334e0d2c713953191c80051f290d8e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36a8757062e988ffde81dc269dd7cbb0a7992b77"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}