{"metadata": {"language_info": {"mimetype": "text/x-python", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"metadata": {"_uuid": "4cf432a0d22e9e6d1d5261bfd63d9fe7ad03fe88", "collapsed": true, "_cell_guid": "0d16d13f-57b1-42e6-9c75-64c7d2d7d5fe"}, "source": ["import cv2\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.model_selection import KFold\n", "from sklearn import metrics\n", "import keras\n", "from keras.models import Model\n", "from keras.optimizers import Adam\n", "from keras.applications.inception_v3 import InceptionV3\n", "from keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"], "outputs": [], "cell_type": "code", "execution_count": 1}, {"metadata": {"_uuid": "65653e763d41204bc764f7e1151829b735e3fb91", "_cell_guid": "20db2f1a-471f-4092-8692-9ec8bd0ee1de"}, "cell_type": "markdown", "source": ["# Load Datasets\n", "Since we will be using a generator we don't need to actually load in any files into memory, all we need is the filepaths :)"]}, {"metadata": {"_uuid": "5a87f8e975cf12b89ac6a966adee3b8b73249b52", "collapsed": true, "_cell_guid": "36d7aa24-8652-4a54-9993-79e305405312"}, "source": ["path = \"../input/train/\"\n", "\n", "def load_train(path):\n", "    train_set = pd.read_csv('../input/train_labels.csv')\n", "    train_label = np.array(train_set['invasive'].iloc[: ])\n", "    train_files = []\n", "    for i in range(len(train_set)):\n", "        train_files.append(path + str(int(train_set.iloc[i][0])) +'.jpg')\n", "    train_set['name'] = train_files\n", "    return train_files, train_set, train_label\n", "\n", "train_files, train_set, train_label = load_train(path)\n", "\n", "train_set.head()"], "outputs": [], "cell_type": "code", "execution_count": 2}, {"metadata": {"_uuid": "1ef06e8b5391d5a2a8ccea97aad7cd45b6c5cef4", "collapsed": true, "_cell_guid": "3acf64c8-5d75-49d7-9e4e-578067f0b29d"}, "source": ["path = \"../input/test/\"\n", "\n", "def load_test(path):\n", "    test_set = pd.read_csv('../input/sample_submission.csv')\n", "    test_files = []\n", "    for i in range(len(test_set)):\n", "        test_files.append(path + str(int(test_set.iloc[i][0])) +'.jpg')\n", "    return test_files, test_set\n", "\n", "test_files, test_set = load_test(path)\n", "\n", "test_set.head()"], "outputs": [], "cell_type": "code", "execution_count": 4}, {"metadata": {"_uuid": "a4e584f2db16d8ba9e58ad74437a28e488425aed", "_cell_guid": "65b7adf0-65d6-4763-9442-b1da1c6b8090"}, "cell_type": "markdown", "source": ["# Define CNN Model Architecture\n", "*Kaggle can't access the weights file*"]}, {"metadata": {"_uuid": "9fa3d8ba6655ff38a08d8742f2da53ca4abef30d", "collapsed": true, "_cell_guid": "4c72959d-a413-4a8c-8caa-855f52cb292c"}, "source": ["img_height = 800\n", "img_width = 800\n", "img_channels = 3\n", "img_dim = (img_height, img_width, img_channels)\n", "\n", "def inceptionv3(img_dim=img_dim):\n", "    input_tensor = Input(shape=img_dim)\n", "    base_model = InceptionV3(include_top=False,\n", "                   weights='imagenet',\n", "                   input_shape=img_dim)\n", "    bn = BatchNormalization()(input_tensor)\n", "    x = base_model(bn)\n", "    x = GlobalAveragePooling2D()(x)\n", "    x = Dropout(0.5)(x)\n", "    output = Dense(1, activation='sigmoid')(x)\n", "    model = Model(input_tensor, output)\n", "    return model\n", "\n", "model = inceptionv3()\n", "model.summary()"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "13890f2f0a7066552fdab668c93cdc95ef02fb5b", "_cell_guid": "7c87589b-c9e5-48ac-b674-8ee7e3daeb31"}, "cell_type": "markdown", "source": ["# Train Model\n", "Here we use 5-fold cross-validation to train the model. Submission file is saved with the average of all folds. Additionally, prediction arrays are saved for each fold in case we want to hand-pick results from an individual fold."]}, {"metadata": {"_uuid": "09697a7318bcd523a8080dd8d4474b598a58481a", "collapsed": true, "_cell_guid": "12be4cfa-0094-40e5-bbca-0f477e6ea403"}, "source": ["def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n", "    roc_auc = metrics.roc_auc_score\n", "    preds_train = np.zeros(len(x), dtype = np.float)\n", "    preds_test = np.zeros(len(test), dtype = np.float)\n", "    train_scores = []; valid_scores = []\n", "\n", "    i = 1\n", "\n", "    for train_index, test_index in kf.split(x):\n", "        x_train = x.iloc[train_index]; x_valid = x.iloc[test_index]\n", "        y_train = y[train_index]; y_valid = y[test_index]\n", "\n", "        def augment(src, choice):\n", "            if choice == 0:\n", "                # Rotate 90\n", "                src = np.rot90(src, 1)\n", "            if choice == 1:\n", "                # flip vertically\n", "                src = np.flipud(src)\n", "            if choice == 2:\n", "                # Rotate 180\n", "                src = np.rot90(src, 2)\n", "            if choice == 3:\n", "                # flip horizontally\n", "                src = np.fliplr(src)\n", "            if choice == 4:\n", "                # Rotate 90 counter-clockwise\n", "                src = np.rot90(src, 3)\n", "            if choice == 5:\n", "                # Rotate 180 and flip horizontally\n", "                src = np.rot90(src, 2)\n", "                src = np.fliplr(src)\n", "            return src\n", "\n", "        def train_generator():\n", "            while True:\n", "                for start in range(0, len(x_train), batch_size):\n", "                    x_batch = []\n", "                    y_batch = []\n", "                    end = min(start + batch_size, len(x_train))\n", "                    train_batch = x_train[start:end]\n", "                    for filepath, tag in train_batch.values:\n", "                        img = cv2.imread(filepath)\n", "                        img = cv2.resize(img, img_size)\n", "                        img = augment(img, np.random.randint(6))\n", "                        x_batch.append(img)\n", "                        y_batch.append(tag)\n", "                    x_batch = np.array(x_batch, np.float32) / 255.\n", "                    y_batch = np.array(y_batch, np.uint8)\n", "                    yield x_batch, y_batch\n", "\n", "        def valid_generator():\n", "            while True:\n", "                for start in range(0, len(x_valid), batch_size):\n", "                    x_batch = []\n", "                    y_batch = []\n", "                    end = min(start + batch_size, len(x_valid))\n", "                    valid_batch = x_valid[start:end]\n", "                    for filepath, tag in valid_batch.values:\n", "                        img = cv2.imread(filepath)\n", "                        img = cv2.resize(img, img_size)\n", "                        img = augment(img, np.random.randint(6))\n", "                        x_batch.append(img)\n", "                        y_batch.append(tag)\n", "                    x_batch = np.array(x_batch, np.float32) / 255.\n", "                    y_batch = np.array(y_batch, np.uint8)\n", "                    yield x_batch, y_batch\n", "\n", "        def test_generator():\n", "            while True:\n", "                for start in range(0, len(test), batch_size):\n", "                    x_batch = []\n", "                    end = min(start + batch_size, len(test))\n", "                    test_batch = test[start:end]\n", "                    for filepath in test_batch:\n", "                        img = cv2.imread(filepath)\n", "                        img = cv2.resize(img, img_size)\n", "                        x_batch.append(img)\n", "                    x_batch = np.array(x_batch, np.float32) / 255.\n", "                    yield x_batch\n", "\n", "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n", "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n", "                               verbose=1, min_lr=1e-7),\n", "             ModelCheckpoint(filepath='inception.fold_' + str(i) + '.hdf5', verbose=1,\n", "                             save_best_only=True, save_weights_only=True, mode='auto')]\n", "\n", "        train_steps = len(x_train) / batch_size\n", "        valid_steps = len(x_valid) / batch_size\n", "        test_steps = len(test) / batch_size\n", "        \n", "        model = model\n", "\n", "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n", "                      metrics = ['accuracy'])\n", "\n", "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n", "                            callbacks=callbacks, validation_data=valid_generator(), \n", "                            validation_steps=valid_steps)\n", "\n", "        model.load_weights(filepath='inception.fold_' + str(i) + '.hdf5')\n", "\n", "        print('Running validation predictions on fold {}'.format(i))\n", "        preds_valid = model.predict_generator(generator=valid_generator(),\n", "                                      steps=valid_steps, verbose=1)[:, 0]\n", "\n", "        print('Running train predictions on fold {}'.format(i))\n", "        preds_train = model.predict_generator(generator=train_generator(),\n", "                                      steps=train_steps, verbose=1)[:, 0]\n", "\n", "        valid_score = roc_auc(y_valid, preds_valid)\n", "        train_score = roc_auc(y_train, preds_train)\n", "        print('Val Score:{} for fold {}'.format(valid_score, i))\n", "        print('Train Score: {} for fold {}'.format(train_score, i))\n", "\n", "        valid_scores.append(valid_score)\n", "        train_scores.append(train_score)\n", "        print('Avg Train Score:{0:0.5f}, Val Score:{1:0.5f} after {2:0.5f} folds'.format\n", "              (np.mean(train_scores), np.mean(valid_scores), i))\n", "\n", "        print('Running test predictions with fold {}'.format(i))\n", "\n", "        preds_test_fold = model.predict_generator(generator=test_generator(),\n", "                                              steps=test_steps, verbose=1)[:, -1]\n", "\n", "        preds_test += preds_test_fold\n", "\n", "        print('\\n\\n')\n", "\n", "        i += 1\n", "\n", "        if i <= n_fold:\n", "            print('Now beginning training for fold {}\\n\\n'.format(i))\n", "        else:\n", "            print('Finished training!')\n", "\n", "    preds_test /= n_fold\n", "\n", "\n", "    return preds_test"], "outputs": [], "cell_type": "code", "execution_count": 5}, {"metadata": {"_uuid": "709a92be4fadeb381414491e1029ad03a188a385", "collapsed": true, "_cell_guid": "75775f5a-2d75-4e64-88f4-b8b713393765"}, "source": ["batch_size = 5\n", "epochs = 50\n", "n_fold = 5\n", "img_size = (img_height, img_width)\n", "kf = KFold(n_splits=n_fold, shuffle=True)\n", "\n", "test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n", "                        train_label, test_files, n_fold, kf)\n", "\n", "test_set['invasive'] = test_pred\n", "test_set.to_csv('./submission.csv', index = None)"], "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "4ce0e9ea7237b6000f5d1c4df040543bfe7763ad", "collapsed": true, "_cell_guid": "4013acf0-550f-46d5-a8f8-ef404ba7204b"}, "source": [], "outputs": [], "cell_type": "code", "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}