{"cells":[{"metadata":{"_uuid":"b5d7eeaf6706be877f88ded4c6dd27fe27229866"},"cell_type":"markdown","source":"Know Where Your Data Comes From\n-----------------------\n\nBefore jumping in any EDA, you should know as much as possible on the provenance of the data you are analyzing. You need to understand how the data was collected and how it was processed. Are there any past transformations on the data that could affect your analysis?\n\n**You should be able to answer those questions on your dataset:\n**\n\nHow was it collected?\n\nIs it a sample?\n\nWas it properly sampled?\n\nWas the dataset transformed in any way?\n\nAre there some know problems on the dataset?\n\n\nIf you don’t understand where the data is coming from, you will have a hard time drawing any meaningful conclusions from the dataset. You are also at risk of making very important analysis mistakes.\n\nAdditionally, you should make sure the dataset is structured in a standardized manner. The recommended format is the third normal form, also named tidy data. A “tidy” dataset has the following attributes:\n\nEach variable forms a column and contains values\nEach observation forms a row\nEach type of observational unit forms a table\n\nRespecting this standardized format will speed up your analysis since this it’s compatible with many tools and libraries."},{"metadata":{"_uuid":"279f595310d48768d7be052b95f04d50aa22657b"},"cell_type":"markdown","source":"Introducing, **pandas_profiling** for simple and fast exploratory data analysis of a Pandas Datafram\n----------------------------"},{"metadata":{"_uuid":"5b47bdc4c939a595f95262eda306300b14edf664"},"cell_type":"markdown","source":"**Exploratory Data Analysis (EDA)** plays a very important role in understanding the **dataset**. Whether you are going to build a Machine Learning Model or if it's just an exercise to bring out insights from the given data, EDA is the primary task to perform.\n\nExploratory data analysis (EDA) is a statistical approach that aims at discovering and summarizing a dataset. At this step of the data science process, you want to explore the structure of your dataset, the variables and their relationships.\n\n\nWhile it's undeniable that EDA is very important, ***The task of performing Exploratory Data Analysis grows in parallel with the number of columns your dataset has got.***\n\n"},{"metadata":{"_uuid":"81ddea0dadc1f0edb2ce151d61a6f9217842bb53"},"cell_type":"markdown","source":"**For example: **\n----------------------\nAssume you've got a dataset with 10 rows x 2 columns. It's very simply to specify those two column names separately and plot all the required plots to perform EDA. \n\nAlternatively, If the dataset has got 20 columns, you've to repeat the same above exercise for another 10 times. \n\nNow, there's another layer of complexity because the visualization that you choose for a continuous variable and categorical variable is different, hence ***the type of the plot changes when the data type changes.*******\n\n"},{"metadata":{"_uuid":"dea89b88f01e825d23d446079b44fd189ea08f7d"},"cell_type":"markdown","source":"Given all these conditions, EDA sometimes becomes a tedious task - but remember it's all driven by a set of rules - \n\nlike plot **boxplot** and **histogram** for a **continous variable**, \n\nMeasure missing values, \n\nCalculate **frequency** if it's **categorical** variable - thus giving us opportunity to automate things. \n\nThat's the base of this python module **pandas_profiling** that helps one in automating the first-level of EDA.\n"},{"metadata":{"_uuid":"3ee5f21a75f67a581df6f57048e1da16b3e5ca36"},"cell_type":"markdown","source":"**pandas-profiling**\n--------------------------------------------\n\n***Generates profile reports from a pandas DataFrame*.** The pandas **df.describe()** function is great but a little basic for serious exploratory data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\n1. **Essentials**: type, unique values, missing values\n\n2. **Quantile statistics **like minimum value, Q1, median, Q3, maximum, range, interquartile range\n\n3. **Descriptive statistics** like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n\n4. **Most frequent values**\n\n5. **Histogram**\n\n6. **Correlations** highlighting of highly correlated variables, Spearman and Pearson matrixes\n\n\n**Github Link:** You can refer more about this module here on github: https://github.com/pandas-profiling/pandas-profiling\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e19d93c1a0271282a708e5feb3dd1db350ba6115"},"cell_type":"markdown","source":"**Loading Training Dataset**\n-------------"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be31ae98ce3f8655542cc3553dc780377dedcaff"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e3f5365e8ad725c5a66f9e6b07c936195baa7e9c"},"cell_type":"code","source":"pp.ProfileReport(train_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"009ec109a564841ba3f9a1550c362ba961d992e0"},"cell_type":"markdown","source":"**To retrieve the list of variables which are rejected due to high correlation:**\n--------------------"},{"metadata":{"trusted":true,"_uuid":"c0d8373be8de78b4c43faebfce61b502d5534e3f"},"cell_type":"code","source":"profile = pp.ProfileReport(train_data)\nrejected_variables = profile.get_rejected_variables(threshold=0.9)\nrejected_variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b511e48be264676897ade4a90068cbf8fb21987"},"cell_type":"markdown","source":"**Advanced usage**\n-------------------\nA set of options are available in order to adapt the report generated.\n\n**bins (int):** Number of bins in histogram (10 by default).\n\n**Correlation settings:**\n\n**check_correlation (boolean):** Whether or not to check correlation (True by default)\n\n**correlation_threshold (float):** Threshold to determine if the variable pair is correlated (0.9 by default).\n\n**correlation_overrides (list): ** Variable names not to be rejected because they are correlated (None by default).\n\n**check_recoded (boolean):** Whether or not to check recoded correlation (False by default). Since it's an expensive \ncomputation it can be activated for small datasets.\n\n**pool_size (int): **Number of workers in thread pool. The default is equal to the number of CPU."},{"metadata":{"trusted":true,"_uuid":"59e89963cd2102397e5ce2db6c6cd1e8ec9cd6b8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}