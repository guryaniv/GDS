{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import the required libraries\nimport keras\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Flatten, LSTM\nfrom keras.layers import Input, GlobalMaxPool1D, Dropout\nfrom keras.layers import Activation\nfrom keras.layers import Bidirectional\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras import optimizers\n\nimport os\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c202d96b28cd31a66306735a7209e3055adf1c0"},"cell_type":"code","source":"#read the data files\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eac8081be353e01a6b04e6a5c84f869d76191f7f"},"cell_type":"code","source":"#divide our training data into features X and label Y\nX_train = train['comment_text'] #will be used to train our model on\nX_test = test['comment_text'] #will be used to predict the output labels to see how well our model has trained\ny_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d670e96af474b13b1f0269a513b7203755b59227"},"cell_type":"markdown","source":"## PREPROCESSING"},{"metadata":{"_uuid":"f7d844368c86cd215067c3733cfbcac3ea1fa15e"},"cell_type":"markdown","source":"\n\n**Preprocessing is the most important step when it comes to any of the dataset before it is passed to a Machine Learning model. This fact is no less true when it comes to dealing with text dataset.**\n\n**Infact for text dataset preprocessing is a must and without which we cannot move forward in training our model.**\n\n**The fact that our algorithm only understand numbers and integers and cannot understand strings or text data makes preprocessing even more valuable and essential in Natural Language Processing.**\n\n**When it comes to preprocessing a text data the most common steps are:**\n\n**1. The entire sentence needs to be converted in the form of small tokens or into individual words. By doing this it helps us to assign each word with a unique integer value called as index or ID of that word.**\n\n**2. Tokenization also helps us to find the total number of unique words in our vocabulary which is later to be fed in our model.**\n\n**3. Once the tokenization is done, the next common step is to remove stopwords. Stopwords are the the most commonly occuring words in our vocabulary. Words that won't generally matter even if they are taken out from our vocabulary. For example words like 'the', 'and', 'so', 'if' etc are the stopwords. But we don't always have to remove the stopwords. Stopwords are only removed when their absence won't affect our model prediction much, in cases like classification of documents according to the topics, stopwords can be removed as we only have to focus on words that carry more importance in our documents.**\n\n**4. There are various other ways in which preprocessing of the text data can be done.**\n\n**KERAS is a high level deep learning API which is also very easy when it comes to applying deep learning algorithms. Keras offers a 'Tokenizer' class which carries out tokenisation for us and also convert all the text data into lowercase.**\n"},{"metadata":{"trusted":true,"_uuid":"99d1daf2fcd638d5a404a0859128cb0e79e0e3c8"},"cell_type":"code","source":"#import the tokenizer class from the keras api\nfrom keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8146213281c201e7c302a4ffb91a56c9d7895be1"},"cell_type":"code","source":"#let's calculate the vocabulary size as well which will be given as an input to the Embedding layer\ntokens = Tokenizer() #tokenizes our data\ntokens.fit_on_texts(X_train)\nvocab_size = len(tokens.word_index) + 1 #size of the total number of uniques tokens in our dataset\ntokenized_train = tokens.texts_to_sequences(X_train) #converting our tokens into sequence of integers\ntokenized_test = tokens.texts_to_sequences(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9cf518e6627755fe5cbd5d84aff0911f9b03067"},"cell_type":"code","source":"print(X_train[0]) #the first text\nprint(100 * '-')\nprint(tokenized_train[0]) #the correspondin first comment in the vectorized form","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"384ed480bab90c06c988931bf0d059acaa56a11f"},"cell_type":"markdown","source":"\n\n1. Now if you observe the vector representation of each comment, the size of each vector is different.\n\n2. But our machine learning model expects the size of our input data to be same throughout.\n\n3. Hence we will be doing padding.\n"},{"metadata":{"trusted":true,"_uuid":"7a37db6e66922c349141e62a09d2cc2b2a9e61cc"},"cell_type":"code","source":"#import the pad_sequences class from the keras api\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb61265b3919a7c29c54e045108d35468e610c7"},"cell_type":"code","source":"max_len = 300 #maximum length of the padded sequence that we want (one of the hyperparameter that can be tuned)\npadded_train = pad_sequences(tokenized_train, maxlen = max_len, padding = 'post') #post padding our sequences with zeros\npadded_test = pad_sequences(tokenized_test, maxlen = max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d54e9ef98ecbff27e5a2e63f9c6da2fa4dcd9aa"},"cell_type":"code","source":"padded_train[:10] #as you can observe once our sentence ends the padding starts and continues until we have a vector of max_len","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4bc7f6c619440b936a6220f76b3a60e67a5914a"},"cell_type":"markdown","source":"## USING PRE-TRAINED WORD EMBEDDINGS"},{"metadata":{"_uuid":"02625c693482b961de05f6b7656f68cc4fe0ac70"},"cell_type":"markdown","source":"**Pre-trained word embeddings are the ones which are usually trained on a large amount of corpus. Using such pre-trained word embeddings on your model can help achieve the model very good amount of accuracy. Using pre-trained word embeddings helps you to avoid training your own word embedding from scratch. This method can be employed when the available amount of dataset is not too much. Word2Vec(by google) and Glove(by stanford NLP group) are the two most commonly use pre-trained word embeddings.**"},{"metadata":{"trusted":true,"_uuid":"a4cdc9b8b995c710a40c173634c3a5007be68640"},"cell_type":"code","source":"import numpy as np\n\nembedding_dim = 50\n#def create_embedding_matrix(filepath, embedding_dim):\nvocab_size = len(tokens.word_index) + 1  # Adding again 1 because of reserved 0 index\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt', encoding = 'utf-8') as f:\n    for line in f:\n        word, *vector = line.split()\n        if word in tokens.word_index:\n            idx = tokens.word_index[word] \n            embedding_matrix[idx] = np.array(vector, dtype = np.float32)[:embedding_dim]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2824b46b6fcdaf851150c968a8fb91956f186969"},"cell_type":"markdown","source":"## Implementing Bidirectional LSTM's"},{"metadata":{"_uuid":"d7b73f0d02e5d0edca52ebad3da5358f5cf6db5d"},"cell_type":"markdown","source":"**1. Here we will be impelemnting Bidirectional LSTMs which are an extension of traditional LSTMs. The main reason to use bidirectional LSTM's instead of traditional LSTM's is that we can achieve a significant improvement in the performance of the model on sequence classification problems.**\n\n**2. Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.**\n\n**3. Using bidirectional LSTM's helps us to interpret the past as well as the future information of our problem.**"},{"metadata":{"trusted":true,"_uuid":"ca778d5406faa96a967e5ad8b8a0878f89996545"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = max_len, trainable = False))\nmodel.add(Bidirectional(LSTM(50, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(50, activation = \"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(32, activation = \"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4fa3df906eff4570c69a9097b8e4dfbadab277e"},"cell_type":"code","source":"#compile the model\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizers.Adam(lr = 0.01, decay = 0.01/32), metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd1418832dcd6e01d62abff09b95ee084704da20"},"cell_type":"code","source":"#fit the model on the dataset\nhistory = model.fit(padded_train, y_train, epochs = 5, batch_size = 128, validation_split = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8a0eaa33692b7b8cc60e0747439f0350e1a600f"},"cell_type":"markdown","source":"## Making predictions on the test data"},{"metadata":{"trusted":true,"_uuid":"0cb7bcc70f26491e0e1c0c535f7a758a1933ae27"},"cell_type":"code","source":"#list all the output class labels\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n\n#make the predictions\ny_pred = model.predict(padded_test, verbose = 1, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2781c2442219fdeff0978dd6e0c7f0a0b9317442"},"cell_type":"code","source":"#making submission\n#read in the submission file\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n\nsample_submission[list_classes] = y_pred\n\nsample_submission.to_csv(\"BiLSTM_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"480cc6429c07832bab40dbe6ee2455e4c7494492"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}