{"cells":[{"metadata":{"_uuid":"60c128d3cc485b6f745edaeeba18ff6de35d2816"},"cell_type":"markdown","source":"# PyTorch for Tabular Data: Predicting NYC Taxi Fares\n_FULL CREDIT TO FRANCESCO POCHETTI_ <br>\n_http://francescopochetti.com/pytorch-for-tabular-data-predicting-nyc-taxi-fares/_\n\n### Imports"},{"metadata":{"trusted":true,"_uuid":"a0e0d08c20a90456b3f9f22310a596ceae9455a4"},"cell_type":"code","source":"%matplotlib inline\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 6\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\nfrom collections import defaultdict\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\npd.options.mode.chained_assignment = None\n\nfrom torch.nn import init\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\nfrom tqdm import tqdm, tqdm_notebook, tnrange\ntqdm.pandas(desc='Progress')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e47bae450ff2499f8e91eef78c14cec1c13e594c"},"cell_type":"markdown","source":"### Helper Functions"},{"metadata":{"trusted":true,"_uuid":"689b0ea73a6111d6899da0e408f1921ac493250c"},"cell_type":"code","source":"def haversine_distance(df, start_lat, end_lat, start_lng, end_lng, prefix):\n    \"\"\"\n    calculates haversine distance between 2 sets of GPS coordinates in df\n    \"\"\"\n    R = 6371  #radius of earth in kilometers\n       \n    phi1 = np.radians(df[start_lat])\n    phi2 = np.radians(df[end_lat])\n    \n    delta_phi = np.radians(df[end_lat]-df[start_lat])\n    delta_lambda = np.radians(df[end_lng]-df[start_lng])\n    \n        \n    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    d = (R * c) #in kilometers\n    df[prefix+'distance_km'] = d\n\ndef add_datepart(df, col, prefix):\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[prefix + n] = getattr(df[col].dt, n.lower())\n    df[prefix + 'Elapsed'] = df[col].astype(np.int64) // 10 ** 9\n    df.drop(col, axis=1, inplace=True)\n    \ndef reject_outliers(data, m = 2.):\n    d = np.abs(data - np.median(data))\n    mdev = np.median(d)\n    s = d/(mdev if mdev else 1.)\n    return s<m\n\ndef parse_gps(df, prefix):\n    lat = prefix + '_latitude'\n    lon = prefix + '_longitude'\n    df[prefix + '_x'] = np.cos(df[lat]) * np.cos(df[lon])\n    df[prefix + '_y'] = np.cos(df[lat]) * np.sin(df[lon]) \n    df[prefix + '_z'] = np.sin(df[lat])\n    df.drop([lat, lon], axis=1, inplace=True)\n    \ndef prepare_dataset(df):\n    df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime, infer_datetime_format=True)\n    add_datepart(df, 'pickup_datetime', 'pickup')\n    haversine_distance(df, 'pickup_latitude', 'dropoff_latitude', 'pickup_longitude', 'dropoff_longitude', '')\n    parse_gps(df, 'pickup')\n    parse_gps(df, 'dropoff')\n    df.dropna(inplace=True)\n    y = np.log(df.fare_amount)\n    df.drop(['key', 'fare_amount'], axis=1, inplace=True)\n    \n    return df, y\n\ndef split_features(df):\n    catf = ['pickupYear', 'pickupMonth', 'pickupWeek', 'pickupDay', 'pickupDayofweek', \n            'pickupDayofyear', 'pickupHour', 'pickupMinute', 'pickupSecond', 'pickupIs_month_end',\n            'pickupIs_month_start', 'pickupIs_quarter_end', 'pickupIs_quarter_start',\n            'pickupIs_year_end', 'pickupIs_year_start']\n\n    numf = [col for col in df.columns if col not in catf]\n    for c in catf: \n        df[c] = df[c].astype('category').cat.as_ordered()\n        df[c] = df[c].cat.codes+1\n    \n    return catf, numf\n\ndef numericalize(df):\n    df[name] = col.cat.codes+1\n\ndef split_dataset(df, y): return train_test_split(df, y, test_size=0.25, random_state=42)\n\ndef inv_y(y): return np.exp(y)\n\ndef get_numf_scaler(train): return preprocessing.StandardScaler().fit(train)\n\ndef scale_numf(df, num, scaler):\n    cols = numf\n    index = df.index\n    scaled = scaler.transform(df[numf])\n    scaled = pd.DataFrame(scaled, columns=cols, index=index)\n    return pd.concat([scaled, df.drop(numf, axis=1)], axis=1)\n\nclass RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]\n    \ndef rmse(targ, y_pred):\n    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n\ndef emb_init(x):\n    x = x.weight.data\n    sc = 2/(x.size(1)+1)\n    x.uniform_(-sc,sc)\n\nclass MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, y_range, use_bn=True):\n        super().__init__()\n        for i,(c,s) in enumerate(emb_szs): assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont=n_emb, n_cont\n        \n        szs = [n_emb+n_cont] + szs\n        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n        self.outp = nn.Linear(szs[-1], out_sz)\n        nn.init.kaiming_normal_(self.outp.weight.data)\n\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn,self.y_range = use_bn,y_range\n\n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x2 = self.bn(x_cont)\n            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            x = F.relu(l(x))\n            if self.use_bn: x = b(x)\n            x = d(x)\n        x = self.outp(x)\n        if self.y_range:\n            x = torch.sigmoid(x)\n            x = x*(self.y_range[1] - self.y_range[0])\n            x = x+self.y_range[0]\n        return x.squeeze()\n\ndef fit(model, train_dl, val_dl, loss_fn, opt, scheduler, epochs=3):\n    num_batch = len(train_dl)\n    for epoch in tnrange(epochs):      \n        y_true_train = list()\n        y_pred_train = list()\n        total_loss_train = 0          \n        \n        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n        for cat, cont, y in t:\n            cat = cat.cuda()\n            cont = cont.cuda()\n            y = y.cuda()\n            \n            t.set_description(f'Epoch {epoch}')\n            \n            opt.zero_grad()\n            pred = model(cat, cont)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            lr[epoch].append(opt.param_groups[0]['lr'])\n            tloss[epoch].append(loss.item())\n            scheduler.step()\n            opt.step()\n            \n            t.set_postfix(loss=loss.item())\n            \n            y_true_train += list(y.cpu().data.numpy())\n            y_pred_train += list(pred.cpu().data.numpy())\n            total_loss_train += loss.item()\n            \n        train_acc = rmse(y_true_train, y_pred_train)\n        train_loss = total_loss_train/len(train_dl)\n        \n        if val_dl:\n            y_true_val = list()\n            y_pred_val = list()\n            total_loss_val = 0\n            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n                cat = cat.cuda()\n                cont = cont.cuda()\n                y = y.cuda()\n                pred = model(cat, cont)\n                loss = loss_fn(pred, y)\n                \n                y_true_val += list(y.cpu().data.numpy())\n                y_pred_val += list(pred.cpu().data.numpy())\n                total_loss_val += loss.item()\n                vloss[epoch].append(loss.item())\n            valacc = rmse(y_true_val, y_pred_val)\n            valloss = total_loss_val/len(valdl)\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n        else:\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n    \n    return lr, tloss, vloss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed85b2967a70b2decada57951d1782df33e219eb"},"cell_type":"markdown","source":"# Preparing the data"},{"metadata":{"_uuid":"174eae8519fa8e95ce2c5f99d235faf94fcb45ea"},"cell_type":"markdown","source":"Tha data is a half-million random sample from the 55M original training set from the [New York City Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data) Kaggle's challenge"},{"metadata":{"trusted":true,"_uuid":"e6e5592c6cac61d6952f7e4d04452a9128409a6f"},"cell_type":"code","source":"PATH='../input/'","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"1a5fa01ef6b2db33e952a0e1610695ffc19ba4d5"},"cell_type":"code","source":"names = ['key','fare_amount','pickup_datetime','pickup_longitude',\n         'pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count']\ndf = pd.read_csv(f'{PATH}train.csv', nrows=6000000)\n\nprint(df.shape)\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be0221f70a47668f7ed501087e3158eca7051b8f"},"cell_type":"code","source":"print(df.passenger_count.describe())\nprint(df.passenger_count.quantile([.85, .99]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5032ecfd7414c55140dd0a25104b29643ef4b43"},"cell_type":"code","source":"print(df.fare_amount.describe())\nprint(df.fare_amount.quantile([.85, .99]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b3afe88a349bfb180a5b63180ffe85d74eabf4"},"cell_type":"code","source":"df = df.loc[(df.fare_amount > 0) & (df.passenger_count < 6) & (df.fare_amount < 53),:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40dd68a38b6faf6b5ccbf2bd43efd4509a255e2c"},"cell_type":"code","source":"df, y = prepare_dataset(df)\n\nprint(df.shape)\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50bfc93c441790757c3940fc9b73acff1e497ee3"},"cell_type":"code","source":"ax = y.hist(bins=20, figsize=(8,6))\n_ = ax.set_xlabel(\"Ride Value (EUR)\")\n_ = ax.set_ylabel(\"# Rides\")\n_ = ax.set_title('Ditribution of Ride Values (USD)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4ee35f1443759d7daccc98c63546d89071e0d71"},"cell_type":"code","source":"catf, numf = split_features(df)\n\nlen(catf)\ncatf\n\nlen(numf)\nnumf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"842e168e0e05a7c35ad90fe5511fe5d1a8a199b3"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d597245857885516a2e81fea2ba147e8a6a5710"},"cell_type":"code","source":"y_range = (0, y.max()*1.2)\ny_range\n\ny = y.clip(y_range[0], y_range[1])\nX_train, X_test, y_train, y_test = split_dataset(df, y)\n\nX_train.shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f310a9d8bcbbcdf1aa70701c30276240c83c77e"},"cell_type":"code","source":"scaler = get_numf_scaler(X_train[numf])\n\nX_train_sc = scale_numf(X_train, numf, scaler)\nX_train_sc.std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d97e78856362de1f06d7f0164014a824c280ac63"},"cell_type":"code","source":"X_test_sc = scale_numf(X_test, numf, scaler)\n\nX_train_sc.shape\nX_test_sc.shape\nX_test_sc.std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"175420a92abc6cc83a23c21027185523bf806760"},"cell_type":"markdown","source":"## Defining pytorch datasets and dataloaders"},{"metadata":{"trusted":true,"_uuid":"8e94f80a3d180d19f9e9b8e958fb933556d51b53"},"cell_type":"code","source":"trainds = RegressionColumnarDataset(X_train_sc, catf, y_train)\nvalds = RegressionColumnarDataset(X_test_sc, catf, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"603ccc48c0dc463ec6ba4260f6417d7f1c209563"},"cell_type":"code","source":"class RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"057686d0791291768b2ab5eb1322361e65c0a25f"},"cell_type":"code","source":"params = {'batch_size': 128,\n          'shuffle': True,\n          'num_workers': 8}\n\ntraindl = data.DataLoader(trainds, **params)\nvaldl = data.DataLoader(valds, **params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40d405efda4451eaee5310f94ce193ce7abdb5e5"},"cell_type":"markdown","source":"## Defining model and related variables"},{"metadata":{"trusted":true,"_uuid":"bea66ff1157587b162a89c3c1c570520db09695b"},"cell_type":"code","source":"cat_sz = [(c, df[c].max()+1) for c in catf]\ncat_sz\n\nemb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\nemb_szs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73790c51b24724f490566adfcd901336dd45456c"},"cell_type":"code","source":"m = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=len(df.columns)-len(catf), \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[1000,500,250], \n                    drops=[0.001,0.01,0.01], \n                    y_range=y_range).to(device)\n\nopt = optim.Adam(m.parameters(), 1e-2)\nlr_cosine = lr_scheduler.CosineAnnealingLR(opt, 1000)\n\nlr = defaultdict(list)\ntloss = defaultdict(list)\nvloss = defaultdict(list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db96c36938f5a0e11fe761c083b42d7a98e35ac9"},"cell_type":"code","source":"m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c20861bcc1310bdb0b36fdba40acbad74d877d"},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true,"_uuid":"ce8d57c778458001c6229734a37a1f79bd39f087"},"cell_type":"code","source":"epoch_n = 12\n\nlr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, scheduler=lr_cosine, epochs=epoch_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e33593f63bf1a0083ed04150c9a136b48c4217d"},"cell_type":"code","source":"_ = plt.plot(lr[0])\n_ = plt.title('Learning Rate Cosine Annealing over Train Batches Iterations (Epoch 0)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d099d3fc6912339f90c6608537ce25af03b210e"},"cell_type":"code","source":"t = [np.mean(tloss[el]) for el in tloss]\nv = [np.mean(vloss[el]) for el in vloss]\np = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(1, epoch_n+1)})\n\n_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n           title='Train and Validation Loss over Epochs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf8c22a1df5b0bf82d60510eb5aaff8a423d295d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}