{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"## Reference Will Koehrsen's guide - https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n\n#imports\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#List of files\nprint(os.listdir(\"../input/\"))\n\n#Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()\n\n#Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()\n\n#Exploratory Data Analysis\napp_train['TARGET'].value_counts()\napp_train['TARGET'].astype(int).plot.hist();\n\n#Function to calculate missing data\ndef missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis = 1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : 'Percent of Total Values'})\n    \n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n            'Percent of Total Values', ascending=False).round(1)\n    \n        print (\"Your selected df has \" + str(df.shape[1]) + \" columns.\\n\"\n               \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \n                 \" columns that have missing values\")\n        return mis_val_table_ren_columns\n\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)\n\n#number of each type of column\napp_train.dtypes.value_counts()\n\n#number of unique classes\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis =0)\n\n#LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n#Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        #if <= 2 unique categories, train on the training data\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            #Transform data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            le_count += 1\nprint ('%d columns were label encoded' % le_count)\n\n#OneHotEncoder\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training features shape: ', app_train.shape)\nprint('Testing features shape: ', app_test.shape)\n\n#Aligning data\ntrain_labels = app_train['TARGET']\n#keep only columns present in both df\napp_train, app_test = app_train.align(app_test, join = 'inner', axis =1)\n#Add target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)\n\n#Fix ages\n(app_train['DAYS_BIRTH'] / -365).describe()\n\n#Fix days employed\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n\nanom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))\n\n#Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n#Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n\n#Find correlations w/ target & sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n#Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))\n\n#Correlation of birth date and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n\n#Plot age distribution \nplt.style.use('fivethirtyeight')\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\nplt.figure(figsize = (10, 8))\n\n#KDE plot of loans repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n#KDE plot of loans not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n#Labeling plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');\n\n#Age information df\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n#Bin age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)\n\n#Group and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups\n\n#graph age bins versus average of target \nplt.figure(figsize = (8, 8))\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');\n\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n\n#Drop target from training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n#Feature names\nfeatures = list(train.columns)\n\n#Copy the testing data\ntest = app_test.copy()\n\n#Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n#Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n#Fit on the training data\nimputer.fit(train)\n\n#Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n#Repeat with scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n\n#RandomForestClassifer\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\nrandom_forest.fit(train, train_labels)\n\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance' : feature_importance_values})\npredictions = random_forest.predict_proba(test)[:,1]\n\n#Feature importance graph\ndef plot_feature_importances(df):\n    #Sort\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    #Normalize \n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    #Chart\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    #Reverse index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    #Label & Ticks\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    return df\nfeature_importances_sorted = plot_feature_importances(feature_importances)\n\n#Bureau data\nbureau = pd.read_csv('../input/bureau.csv')\nbureau.head()\n\n#Group by client id, count number of previous loans\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()\n\n#Add to training df\ntrain = pd.read_csv('../input/application_train.csv')\ntrain = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n\n#Fill in missing values \ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()\n\n#OneHotEncoder\ncategorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()\n\n#Find correlations w/ target & sort\ncorrelations = train.corr()['TARGET'].sort_values()\n\n#Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))\n\n#Submission\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\nsubmit.head()\nsubmit.to_csv('A2_RFC.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}