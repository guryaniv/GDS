{"nbformat_minor": 1, "cells": [{"cell_type": "code", "source": ["import datetime\n", "import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import xgboost as xgb\n", "import matplotlib.pyplot as plt\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.model_selection import train_test_split\n", "%matplotlib inline"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "3ad8b141-0c5a-4710-bd08-0fa04b707e6d", "_uuid": "6ee9ce5c4dea2be04fc2a9c0ee6d7c36429a5e28", "collapsed": true}}, {"cell_type": "code", "source": ["def LeaveOneOut(data1, data2, groupcolumns, columnName, useLOO=False, cut=1, addNoise=False):\n", "    features = list([])\n", "    for a in groupcolumns:\n", "        features.append(a)\n", "    if(columnName is not None):\n", "        features.append(columnName)\n", "       \n", "    grpCount = data1.groupby(features)['visitors'].count().reset_index().rename(columns={'visitors': 'Count'})\n", "    if(useLOO):\n", "        grpCount = grpCount[(grpCount.Count > cut)]\n", "    grpMean = data1.groupby(features)['visitors'].mean().reset_index().rename(columns={'visitors': 'Mean'})\n", "    grpMedian = data1.groupby(features)['visitors'].median().reset_index().rename(columns={'visitors': 'Median'})\n", "    grpMin = data1.groupby(features)['visitors'].min().reset_index().rename(columns={'visitors': 'Min'})\n", "    grpMax = data1.groupby(features)['visitors'].max().reset_index().rename(columns={'visitors': 'Max'})\n", "    grpStd = data1.groupby(features)['visitors'].std().reset_index().rename(columns={'visitors': 'Std'})\n", "        \n", "    grpOutcomes = grpCount.merge(grpMean, on=features)\n", "    grpOutcomes = grpOutcomes.merge(grpMedian, on=features)\n", "    grpOutcomes = grpOutcomes.merge(grpMin, on=features)\n", "    grpOutcomes = grpOutcomes.merge(grpMax, on=features)\n", "    grpOutcomes = grpOutcomes.merge(grpStd, on=features)\n", "    \n", "    x = pd.merge(data2[features], grpOutcomes,\n", "                 suffixes=('x_', ''),\n", "                 how='left',\n", "                 on=features,\n", "                 left_index=True)[['Count','Mean','Median','Max','Min','Std']]\n", "    x['Outcomes'] = data2['visitors'].values\n", "    \n", "    if(useLOO):\n", "        nonnulls = (~x.Count.isnull())\n", "        x.loc[nonnulls,'Mean'] = ((x[nonnulls].Mean*x[nonnulls].Count)-x[nonnulls].Outcomes)\n", "        x.loc[nonnulls,'Median'] = ((x[nonnulls].Median*x[nonnulls].Count)-x[nonnulls].Outcomes)\n", "        if(addNoise is True):\n", "            x.loc[nonnulls&(x.Std>0),'Mean'] += np.random.normal(0,x[nonnulls&(x.Std>0)].Std,x[nonnulls&(x.Std>0)].shape[0])\n", "            x.loc[nonnulls&(x.Std>0),'Median'] += np.random.normal(0,x[nonnulls&(x.Std>0)].Std,x[nonnulls&(x.Std>0)].shape[0])\n", "        else:\n", "            x.loc[nonnulls,'Count'] -= 1\n", "        x.loc[nonnulls,'Mean'] /=  (x[nonnulls].Count)\n", "        x.loc[nonnulls,'Median'] /= (x[nonnulls].Count)\n", "    x.Count = np.log1p(x.Count)\n", "    x = x.replace(np.inf, np.nan)\n", "    x = x.replace(-np.inf, np.nan)\n", "    #x = x.fillna(x.mean()) \n", "    \n", "    return x[['Count','Mean','Median','Max','Min', 'Std']]\n", "\n", "\n", "def MungeTrain():\n", "    air_visit_data = pd.read_csv('../input/air_visit_data.csv',parse_dates=['visit_date'])\n", "    air_store_info = pd.read_csv('../input/air_store_info.csv')\n", "    \n", "    hpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\n", "    hpg_store_info.drop(['latitude','longitude'],inplace=True,axis=1)\n", "    store_id_relation = pd.read_csv('../input/store_id_relation.csv')\n", "    storeinfo = air_store_info.merge(store_id_relation,on='air_store_id',how='left')\n", "    storeinfo = storeinfo.merge(hpg_store_info,on='hpg_store_id',how='left')\n", "    air_reserve = pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime'])\n", "    air_reserve['visit_date'] = air_reserve.visit_datetime.apply( lambda df : \n", "    datetime.datetime(year=df.year, month=df.month, day=df.day))\n", "    hpg_reserve =pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime'])\n", "    hpg_reserve['visit_date'] = hpg_reserve.visit_datetime.apply( lambda df : \n", "    datetime.datetime(year=df.year, month=df.month, day=df.day))\n", "    date_info = pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date']).rename(columns={'calendar_date':'visit_date'})\n", "    air_reserve_by_date = air_reserve.groupby(['air_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n", "    hpg_reserve_by_date = hpg_reserve.groupby(['hpg_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n", "    \n", "    train = air_visit_data.merge(storeinfo,on='air_store_id',how='left')\n", "    train = train.merge(air_reserve_by_date,on=['air_store_id','visit_date'],how='left')\n", "    train = train.merge(hpg_reserve_by_date,on=['hpg_store_id','visit_date'],how='left')\n", "    train = train.merge(date_info,on='visit_date',how='left')\n", "    train['year'] = train.visit_date.dt.year\n", "    train['month'] = train.visit_date.dt.month\n", "    train.reserve_visitors_x = train.reserve_visitors_x.fillna(0)\n", "    train.reserve_visitors_y = train.reserve_visitors_y.fillna(0)\n", "    train.reserve_visitors_x = np.log1p(train.reserve_visitors_x)\n", "    train.reserve_visitors_y = np.log1p(train.reserve_visitors_y)\n", "    train.visitors = np.log1p(train.visitors)\n", "    #train.drop(['latitude','longitude'],inplace=True,axis=1)\n", "    train = train.fillna(-1)\n", "    train = train.sort_values(by=['visit_date','air_store_id'],ascending=False)\n", "    train = train.reset_index(drop=True)\n", "    return train\n", "\n", "\n", "def MungeTest(columns):\n", "    air_visit_data = pd.read_csv('../input/sample_submission.csv')\n", "    air_visit_data['visit_date'] = air_visit_data.id.apply(lambda x: datetime.datetime(year=int(x[-10:-6]), month=int(x[-5:-3]), day=int(x[-2:])))\n", "    air_visit_data['air_store_id'] = air_visit_data.id.apply(lambda x: x[:-11])\n", "    \n", "    air_store_info = pd.read_csv('../input/air_store_info.csv')\n", "    hpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\n", "    hpg_store_info.drop(['latitude','longitude'],inplace=True,axis=1)\n", "    store_id_relation = pd.read_csv('../input/store_id_relation.csv')\n", "    storeinfo = air_store_info.merge(store_id_relation,on='air_store_id',how='left')\n", "    storeinfo = storeinfo.merge(hpg_store_info,on='hpg_store_id',how='left')\n", "    air_reserve = pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime'])\n", "    air_reserve['visit_date'] = air_reserve.visit_datetime.apply( lambda df : \n", "    datetime.datetime(year=df.year, month=df.month, day=df.day))\n", "    hpg_reserve =pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime'])\n", "    hpg_reserve['visit_date'] = hpg_reserve.visit_datetime.apply( lambda df : \n", "    datetime.datetime(year=df.year, month=df.month, day=df.day))\n", "    date_info = pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date']).rename(columns={'calendar_date':'visit_date'})\n", "    air_reserve_by_date = air_reserve.groupby(['air_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n", "    hpg_reserve_by_date = hpg_reserve.groupby(['hpg_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n", "    \n", "    test = air_visit_data.merge(storeinfo,on='air_store_id',how='left')\n", "    test = test.merge(air_reserve_by_date,on=['air_store_id','visit_date'],how='left')\n", "    test = test.merge(hpg_reserve_by_date,on=['hpg_store_id','visit_date'],how='left')\n", "    test = test.merge(date_info,on='visit_date',how='left')\n", "    test['year'] = test.visit_date.dt.year\n", "    test['month'] = test.visit_date.dt.month\n", "    test.reserve_visitors_x = test.reserve_visitors_x.fillna(0)\n", "    test.reserve_visitors_y = test.reserve_visitors_y.fillna(0)\n", "    test.reserve_visitors_x = np.log1p(test.reserve_visitors_x)\n", "    test.reserve_visitors_y = np.log1p(test.reserve_visitors_y)\n", "    test.visitors = np.log1p(test.visitors)\n", "    #test.drop(['latitude','longitude'],inplace=True,axis=1)\n", "    test = test.fillna(-1)\n", "    test = test.sort_values(by=['visit_date','air_store_id'],ascending=False)\n", "    test = test.reset_index(drop=True)\n", "    return test[list(['id'])+list(columns)]"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "374a6697-b6f9-44dc-b2ed-74cf492f43d1", "_uuid": "4c22ee168e7a9aa24a18a487f4d0d58c3bfb1195", "collapsed": true}}, {"cell_type": "code", "source": ["train = MungeTrain()\n", "delta = train.visit_date.max()-pd.Timedelta(weeks=5)\n", "lastfiveweekstrain = train[train.visit_date>=delta].copy()\n", "lastfiveweekstrain = lastfiveweekstrain.reset_index(drop=True)\n", "train = train[train.visit_date<delta].copy()\n", "train = train.reset_index(drop=True)\n", "test = MungeTest(train.columns)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "86e06ff3-fa8e-4a9f-bd73-f7dcb4a3fa3c", "_uuid": "70cb7c59ba22ef894084bead3feb5e4b8c12b1f2", "collapsed": true}}, {"cell_type": "code", "source": ["# Time to do predictions for 60 weeks total\n", "deltavals = [1,2,3,4,5]\n", "xgbtrainpreds = None\n", "xgbtestpreds = None\n", "for i,deltaweek in enumerate(deltavals):\n", "    print(i)\n", "    delta = pd.Timedelta(weeks=deltaweek)\n", "    blindmin = train.visit_date.max()-delta\n", "    blindmax = train.visit_date.max()\n", "    vismin = blindmin-delta-pd.Timedelta(days=1)\n", "    vismax = blindmax-delta-pd.Timedelta(days=1)\n", "    btrain = None\n", "\n", "    for x in range(int(60./deltaweek)):\n", "        vistrain = train[(train.visit_date<=vismax)].copy()\n", "        blindtrain = train[(train.visit_date>=blindmin)&(train.visit_date<=blindmax)].copy()\n", "        features = ['air_genre_name',\n", "                    'air_area_name', 'hpg_store_id',\n", "                    'hpg_genre_name', 'hpg_area_name', \n", "                    'day_of_week', 'holiday_flg']\n", "        for c in features:\n", "            blindtrain[c+'_Count_Store'] = np.nan\n", "            blindtrain[c+'_Mean_Store'] = np.nan\n", "            blindtrain[c+'_Median_Store'] = np.nan\n", "            blindtrain[c+'_Max_Store'] = np.nan\n", "            blindtrain[c+'_Min_Store'] = np.nan\n", "            blindtrain[c+'_Std_Store'] = np.nan\n", "\n", "\n", "            blindtrain[[c+'_Count_Store',c+'_Mean_Store',\n", "                        c+'_Median_Store',c+'_Max_Store',\n", "                        c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n", "                                                                        blindtrain,\n", "                                                                        list(['air_store_id']),\n", "                                                                        c,\n", "                                                                        useLOO=False,\n", "                                                                        cut=0).values\n", "\n", "        features = ['air_store_id',\n", "                    'air_genre_name',\n", "                    'air_area_name', 'hpg_store_id',\n", "                    'hpg_genre_name', 'hpg_area_name', \n", "                    'day_of_week', 'holiday_flg']\n", "\n", "        for c in features:\n", "            blindtrain[c+'_Count'] = np.nan\n", "            blindtrain[c+'_Mean'] = np.nan\n", "            blindtrain[c+'_Median'] = np.nan\n", "            blindtrain[c+'_Max'] = np.nan\n", "            blindtrain[c+'_Min'] = np.nan\n", "            blindtrain[c+'_Std'] = np.nan\n", "\n", "            blindtrain[[c+'_Count',c+'_Mean',\n", "                        c+'_Median',c+'_Max',\n", "                        c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n", "                                                            blindtrain,\n", "                                                            list([]),\n", "                                                            c,\n", "                                                            useLOO=False,\n", "                                                            cut=0,\n", "                                                            addNoise=False).values\n", "\n", "\n", "            if('air_store_id'!=c):\n", "                blindtrain.drop(c,inplace=True,axis=1)\n", "        if(btrain is None):\n", "            btrain = blindtrain.copy()\n", "        else:\n", "            btrain = pd.concat([btrain,blindtrain])\n", "        vismax -= pd.Timedelta(weeks=deltaweek)\n", "        vismin -= pd.Timedelta(weeks=deltaweek)\n", "        blindmin -= pd.Timedelta(weeks=deltaweek)\n", "        blindmax -= pd.Timedelta(weeks=deltaweek)\n", "\n", "    vistrain = train.copy()\n", "    features = ['air_genre_name',\n", "                'air_area_name', 'hpg_store_id',\n", "                'hpg_genre_name', 'hpg_area_name', \n", "                'day_of_week', 'holiday_flg']\n", "    for c in features:\n", "        lastfiveweekstrain[c+'_Count_Store'] = np.nan\n", "        lastfiveweekstrain[c+'_Mean_Store'] = np.nan\n", "        lastfiveweekstrain[c+'_Median_Store'] = np.nan\n", "        lastfiveweekstrain[c+'_Max_Store'] = np.nan\n", "        lastfiveweekstrain[c+'_Min_Store'] = np.nan\n", "        lastfiveweekstrain[c+'_Std_Store'] = np.nan\n", "\n", "\n", "        lastfiveweekstrain[[c+'_Count_Store',c+'_Mean_Store',\n", "                            c+'_Median_Store',c+'_Max_Store',\n", "                            c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n", "                                                                            lastfiveweekstrain,\n", "                                                                            list(['air_store_id']),\n", "                                                                            c,\n", "                                                                            useLOO=False,\n", "                                                                            cut=0).values\n", "        \n", "        test[c+'_Count_Store'] = np.nan\n", "        test[c+'_Mean_Store'] = np.nan\n", "        test[c+'_Median_Store'] = np.nan\n", "        test[c+'_Max_Store'] = np.nan\n", "        test[c+'_Min_Store'] = np.nan\n", "        test[c+'_Std_Store'] = np.nan\n", "\n", "\n", "        test[[c+'_Count_Store',c+'_Mean_Store',\n", "              c+'_Median_Store',c+'_Max_Store',\n", "              c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n", "                                                              test,\n", "                                                              list(['air_store_id']),\n", "                                                              c,\n", "                                                              useLOO=False,\n", "                                                              cut=0).values\n", "\n", "    features = ['air_store_id',\n", "                'air_genre_name',\n", "                'air_area_name', 'hpg_store_id',\n", "                'hpg_genre_name', 'hpg_area_name', \n", "                'day_of_week', 'holiday_flg']\n", "\n", "    for c in features:\n", "        lastfiveweekstrain[c+'_Count'] = np.nan\n", "        lastfiveweekstrain[c+'_Mean'] = np.nan\n", "        lastfiveweekstrain[c+'_Median'] = np.nan\n", "        lastfiveweekstrain[c+'_Max'] = np.nan\n", "        lastfiveweekstrain[c+'_Min'] = np.nan\n", "        lastfiveweekstrain[c+'_Std'] = np.nan\n", "\n", "        lastfiveweekstrain[[c+'_Count',c+'_Mean',\n", "                            c+'_Median',c+'_Max',\n", "                            c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n", "                                                                lastfiveweekstrain,\n", "                                                                list([]),\n", "                                                                c,\n", "                                                                useLOO=False,\n", "                                                                cut=0,\n", "                                                                addNoise=False).values\n", "        \n", "        test[c+'_Count'] = np.nan\n", "        test[c+'_Mean'] = np.nan\n", "        test[c+'_Median'] = np.nan\n", "        test[c+'_Max'] = np.nan\n", "        test[c+'_Min'] = np.nan\n", "        test[c+'_Std'] = np.nan\n", "\n", "        test[[c+'_Count',c+'_Mean',\n", "              c+'_Median',c+'_Max',\n", "              c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n", "                                                  test,\n", "                                                  list([]),\n", "                                                  c,\n", "                                                  useLOO=False,\n", "                                                  cut=0,\n", "                                                  addNoise=False).values\n", "    \n", "    d_train = xgb.DMatrix(btrain[btrain.columns[3:]], label=btrain.visitors)\n", "    d_valid = xgb.DMatrix(lastfiveweekstrain[btrain.columns[3:]], label=lastfiveweekstrain.visitors)\n", "    d_test = xgb.DMatrix(test[btrain.columns[3:]])\n", "    params = {}\n", "    params['objective'] = 'reg:linear'\n", "    params['eval_metric'] = 'rmse'\n", "    params['eta'] = 0.1\n", "    params['max_depth'] = 7\n", "    params['subsample']= 0.8 \n", "    params['colsample_bytree']= 0.8\n", "    params['silent'] = 1\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "    clf = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=20, verbose_eval=10)\n", "    xgbx = clf.predict(d_valid,ntree_limit=clf.best_iteration+1)\n", "    xgby = clf.predict(d_test,ntree_limit=clf.best_iteration+1)\n", "    trainpreds = pd.DataFrame()\n", "    trainpreds['visit_date'] = lastfiveweekstrain.visit_date.values\n", "    trainpreds['air_store_id'] = lastfiveweekstrain.air_store_id.values\n", "    trainpreds['visitors'] = lastfiveweekstrain.visitors.values\n", "    trainpreds['weeks_'+str(deltaweek)] = xgbx\n", "    testpreds = pd.DataFrame()\n", "    testpreds['visit_date'] = test.visit_date.values\n", "    testpreds['air_store_id'] = test.air_store_id.values\n", "    testpreds['weeks_'+str(deltaweek)] = xgby\n", "    if(xgbtrainpreds is None):\n", "        xgbtrainpreds = trainpreds.copy()\n", "        xgbtestpreds = testpreds.copy()\n", "    else:\n", "        xgbtrainpreds = xgbtrainpreds.merge(trainpreds[['air_store_id','visit_date','weeks_'+str(deltaweek)]],on=['air_store_id','visit_date'])\n", "        xgbtestpreds = xgbtestpreds.merge(testpreds[['air_store_id','visit_date','weeks_'+str(deltaweek)]],on=['air_store_id','visit_date'])"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "91c86994-644d-45e8-b4b5-533b22a386eb", "_uuid": "621145b81ea2fcc2850220ef93975c9621077ee8", "collapsed": true}}, {"cell_type": "code", "source": ["xgbtrainpreds.head()"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "3dc3d012-a336-4873-8132-19bf661a92df", "_uuid": "57a25f80a29f0b1346fee4010f1bacde5be28c7b", "collapsed": true}}, {"cell_type": "code", "source": ["xgbtestpreds.head()"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "935d7bd9-18bd-4c6f-84f8-aa382aee4365", "_uuid": "001559b21a87996ae8093d03117b0a7df6d5d0e9", "collapsed": true}}, {"cell_type": "code", "source": ["X_train, X_test, y_train, y_test = train_test_split(xgbtrainpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']],\n", "                                                    xgbtrainpreds.visitors, test_size=0.2, random_state=42)\n", "d_train = xgb.DMatrix(X_train, label=y_train)\n", "d_valid = xgb.DMatrix(X_test, label=y_test)\n", "d_test = xgb.DMatrix(xgbtestpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']])\n", "params = {}\n", "params['objective'] = 'reg:linear'\n", "params['eval_metric'] = 'rmse'\n", "params['eta'] = 0.2\n", "params['max_depth'] = 3\n", "params['subsample']=0.8 \n", "params['colsample_bytree']=0.8\n", "params['silent'] = 1\n", "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "clf = xgb.train(params, d_train, 250, watchlist, early_stopping_rounds=20, verbose_eval=10)\n", "xgbx = clf.predict(d_valid,ntree_limit=clf.best_iteration+1)\n", "xgby = clf.predict(d_test,ntree_limit=clf.best_iteration+1)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "0d5520e3-ae79-4246-9b47-58e53be38990", "_uuid": "5b82bc0ee578360b61d0e69d625aae8f349bd087", "collapsed": true}}, {"cell_type": "code", "source": ["bestrounds = clf.best_iteration+1\n", "d_train = xgb.DMatrix(xgbtrainpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']], label=xgbtrainpreds.visitors)\n", "clf = xgb.train(params, d_train, int((bestrounds)*1.2), verbose_eval=10)\n", "xgbx = clf.predict(d_train,ntree_limit=bestrounds)\n", "xgby = clf.predict(d_test,ntree_limit=bestrounds)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "bd5caedc-b5a7-4eb5-89ce-a70caead6b75", "_uuid": "865787909862732589494199c1a3fb2e10b2cf86", "collapsed": true}}, {"cell_type": "code", "source": ["print(np.sqrt(mean_squared_error(xgbtrainpreds.visitors.ravel(),\n", "                                 xgbx)))"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "e0fe9e2b-1913-4912-bd47-cbab0c756adf", "_uuid": "40c99701626fb87a9db42067da7cb73d6d0a6b6c", "collapsed": true}}, {"cell_type": "code", "source": ["xgbtestpreds['id'] =  xgbtestpreds[\"air_store_id\"]+'_'+xgbtestpreds[\"visit_date\"].map(str)\n", "xgbtestpreds['id'] = xgbtestpreds.id.str[:-9]\n", "xgbtestpreds['visitors'] = np.expm1(xgby)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "cf5848e8-b8e7-4209-b3f9-3dc6b9c57573", "_uuid": "ff409c9b4262e5f65d33e5eade5a8ba72e71cb4b", "collapsed": true}}, {"cell_type": "code", "source": ["xgbtestpreds[['id','visitors']].to_csv('xgbtimeseries.csv',index=False)"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "016af5ac-db44-43f0-afc4-064310412c89", "_uuid": "b5af8522830e3925f3427abf9451ddf16baf9711", "collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "pygments_lexer": "ipython3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}