{"cells":[{"metadata":{"_uuid":"0b5b83d7aaebc27717ccba9cde487d9353cd8c0a"},"cell_type":"markdown","source":"# Automated Feature Engineering and Selection for Titanic Dataset"},{"metadata":{"_uuid":"3122e20c01daea8f9d9eb9145f029776e2fdba09"},"cell_type":"markdown","source":" The original was done by:\n  ***Liana Napalkova*** on \n***3 September 2018***\n\nThis edit:\n***LGB & Edits - Jan 23 2019***"},{"metadata":{"_uuid":"1890d2b3002c84d8f2bce99ea9651f7f1fb8a6c9"},"cell_type":"markdown","source":"# Table of contents\n1. [Introduction](#introduction)\n2. [Load data](#load_data)\n3. [Clean data](#clean_data)\n4. [Automated feature engineering](#afe)\n5. [\"Curse of dimensionality\": Feature reduction and selection](#frs)\n6. [Training and testing the simple model](#ttm)"},{"metadata":{"_uuid":"ff85a1650e43295ecdf0f1145e55be2dbd935a91"},"cell_type":"markdown","source":"## 1. Introduction <a name=\"introduction\"></a>"},{"metadata":{"_uuid":"f49e4cb233bd53878b292413b12a513386ea8e7f"},"cell_type":"markdown","source":"If you have ever manually created hundreds of features for your ML project (I am sure you did it), then you will be happy to find out how the Python package called \"featuretools\" can help out with this task. The good news is that this package is very easy to use. It is aimed at automated feature engineering. **Of course human expertise cannot be substituted**, but nevertheless **\"featuretools\" can automate a large amount of routine work**. For the exploration purpose I will use a well-known Titanic dataset. The achieved resuls will be compared to the results obtained with handcrafted feature engineering and manual hyperparameter optimisation.\n\nThe main takeaways from this notebook are:\n\n* Firstly, going from 11 total features to 146 using automated feature engineering (\"featuretools\" package).\n* <s>Secondly, applying the feature reduction and selection methods to select X most relevant features out of 146 features.</s> We used all the features that were created in this fork.\n* The accuracy of <s>0.74162</s> ~ 0.79 on the public leaderboard with <s>a basic random forest classifier</s> Light GBM.\n\n <s>**I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated.**</s>\n \n ** You should upvote the original, as that is where the bulk of the work was created!**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas as pd\n#import autosklearn.classification\nimport featuretools as ft\nfrom featuretools.primitives import *\nfrom featuretools.variable_types import Numeric\n##from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n##from sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a7f6c9ef5988729e008453f46d20e595ac040ff"},"cell_type":"markdown","source":"## 2. Load data <a name=\"load_data\"></a>"},{"metadata":{"trusted":true,"_uuid":"77f09dbd953b43a57ed42092546f29728d43f182"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nanswers = pd.read_csv('../input/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"578afc5f38e4b522c59715ae5a8ab308929e2133"},"cell_type":"code","source":"print(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## 3. Clean data <a name=\"clean_data\"></a>\n\nFirst of all, it is necessary to clean the data. Since the main focus of this notebook is to explore \"featuretools\", we will not re-invent the wheel in data cleaning. Therefore, we will apply the code of feature cleaning taken from one of existing Kernels - [Best Titanic Survival Prediction for Beginners](https://www.kaggle.com/vin1234/best-titanic-survival-prediction-for-beginners), where an interested reader can find a very detailed explanation of each steps of the data cleaning procedure."},{"metadata":{"_uuid":"4b895f63735cd93861d09ae10175af52d6e2ba58"},"cell_type":"markdown","source":"How much of our data is null?"},{"metadata":{"trusted":true,"_uuid":"f6c3addc801aa8efd2ced25e12f967d60a517967"},"cell_type":"code","source":"train_df.drop('Survived',axis=1).isnull().sum()/len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7b35eac60cc7b75521d9d6daf60ec789476a4f9"},"cell_type":"markdown","source":"It looks like Cabin and Age are missing quite a few. \n\nPerhaps we can make some edits to Cabin based on the letter.\n\nWell get to Age later."},{"metadata":{"trusted":true,"_uuid":"f76ef7b92256813a691bd95fac7a5cf3cc9bf94e"},"cell_type":"code","source":"cabin_letter = [str(c)[0] for c in train_df.Cabin]\ncabin_letter[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac8300d9fac0236d186808862f81338da6e18795"},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69ca33ccce6d69ddde8eb694eece7dbe6737dd7f"},"cell_type":"code","source":"Counter(cabin_letter)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9200d60935818ce5272afc9a9d8fe6e621841aba"},"cell_type":"markdown","source":"So as we had before, there are 687 null's and then a slightly different balance of letters with the most being in B,C,D and E. \n\nWe will fill the nulls with a capital 'N'."},{"metadata":{"trusted":true,"_uuid":"9776e5a3365e75fa35f07c79e06d242877288fcd"},"cell_type":"code","source":"cabin_train = [str(c)[0].upper() for c in train_df.Cabin]\ntrain_df['Cabin_C'] = cabin_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c30a7b4fa73751d83bcea1b4913d110a034c78"},"cell_type":"code","source":"cabin_test = [str(c)[0].upper() for c in test_df.Cabin]\ntest_df['Cabin_C'] = cabin_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"878a2bfbd4ab8fca00441a73180d072d9dd184c4"},"cell_type":"code","source":"# train_df.Cabin.value_counts()\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"044f1438abd58ad4527313dcd770f570ded81ca4"},"cell_type":"code","source":"combine = train_df.append(test_df)\n\npassenger_id=test_df['PassengerId']\n#combine.drop(['PassengerId'], axis=1, inplace=True)\ncombine = combine.drop(['Ticket', 'Cabin'], axis=1)\n\ncombine.Fare.fillna(combine.Fare.mean(), inplace=True)\n\ncombine['Sex'] = combine.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\nfor name_string in combine['Name']:\n    combine['Title']=combine['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n    \n#replacing the rare title with more common one.\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ncombine.replace({'Title': mapping}, inplace=True)\n\ncombine = combine.drop(['Name'], axis=1)\n\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\n## The Age to impute is done here!\nfor title in titles:\n    age_to_impute = combine.groupby('Title')['Age'].median()[titles.index(title)]\n    combine.loc[(combine['Age'].isnull()) & (combine['Title'] == title), 'Age'] = age_to_impute\ncombine.isnull().sum()\n\nfreq_port = train_df.Embarked.dropna().mode()[0]\ncombine['Embarked'] = combine['Embarked'].fillna(freq_port)\n    \ncombine['Embarked'] = combine['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ncombine['Title'] = combine['Title'].map( {'Mr': 0, 'Mrs': 1, 'Miss': 2, 'Master': 3, 'Rev': 4, 'Dr': 5} ).astype(int)\ncombine.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"344af632adcc22327bef52d43dabea364f749dfe"},"cell_type":"code","source":"## new ======================================\n\ncabin_mapping = {'N':0,'C':1,'E':2,'G':3,'D':4,'A':5,'B':6,'F':7,'T':8}\ncombine.replace({'Cabin_C': cabin_mapping}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d65ac5b21f6018aadf841eed0a198a2cf6da7e1"},"cell_type":"code","source":"combine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1f57cfac8faa72cb76b13f0e0ae35c57cc9b73c"},"cell_type":"code","source":"Counter(combine.Cabin_C)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33d752e2969d8e2a2918f4c8e955a7d6134497e6"},"cell_type":"markdown","source":"## 4. Perform automated feature engineering <a name=\"afe\"></a> "},{"metadata":{"_uuid":"06b96caaa81c1cc9b26c18381bd8fbb4910a570a"},"cell_type":"markdown","source":"Once the data is cleaned, we can proceed to the cake in our party - i.e. automated feature engineering. To work with \"featuretools\" package, we should specify our dataframes \"train_df\" and \"test_df\" as entities of the entity set. The entity is just a table with a uniquely identifying column known as an index. The \"featuretools\" can automatically infer the variable types (numeric, categorical, datetime) of the columns, but it's a good idea to also pass in specific datatypes to override this behavior."},{"metadata":{"trusted":true,"_uuid":"9596532fb1092b9501a9f2783f9a7f6fc240929a"},"cell_type":"code","source":"es = ft.EntitySet(id = 'titanic_data')\n\nes = es.entity_from_dataframe(entity_id = 'combine', dataframe = combine.drop(['Survived'], axis=1), \n                              variable_types = \n                              {\n                                  'Embarked': ft.variable_types.Categorical,\n                                  'Sex': ft.variable_types.Boolean,\n                                  'Title': ft.variable_types.Categorical,\n                                  'Cabin_C': ft.variable_types.Categorical ## New\n                              },\n                              index = 'PassengerId')\n\nes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd28388f205e727b763b3f34096b253ed5935ce1"},"cell_type":"markdown","source":"Once the entity set is created, it is possible to generate new features using so called **feature primitives**. A feature primitive is an operation applied to data to create a new feature. Simple calculations can be stacked on top of each other to create complex features. Feature primitives fall into two categories:\n\n* **Aggregation**: these functions group together child datapoints for each parent and then calculate a statistic such as mean, min, max, or standard deviation. The aggregation works across multiple tables using relationships between tables.\n\n\n* **Transformation**: these functions work on one or multiple columns of a single table.\n\nIn our case we do not have different tables linked between each other. However, we can create dummy tables using \"normalize_entity\" function. What will it give us? Well, this way we will be able to apply both aggregation and transformation functions to generate new features. To create such tables, we will use categorical, boolean and integer variables."},{"metadata":{"trusted":true,"_uuid":"60c17d6ce11837c570db9b6a7a925ec7c976f552"},"cell_type":"code","source":"es = es.normalize_entity(base_entity_id='combine', new_entity_id='Embarked', index='Embarked')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Title', index='Title')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Pclass', index='Pclass')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Parch', index='Parch')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='SibSp', index='SibSp')\nes = es.normalize_entity(base_entity_id='combine', new_entity_id='Cabin_C', index='Cabin_C')\nes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"389a73003d83b90a80a917ec434de166b03f8937"},"cell_type":"code","source":"primitives = ft.list_primitives()\npd.options.display.max_colwidth = 100\nprimitives[primitives['type'] == 'aggregation'].head(primitives[primitives['type'] == 'aggregation'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a51c5e2668e4903f287baa97484bc8030279cd"},"cell_type":"markdown","source":"As we can see, the most of \"transformation\" functions are applied to datetime or time-dependent variables. In our dataset we do not have such variables. Therefore these functions will not be used."},{"metadata":{"trusted":true,"_uuid":"31e6c10351fe360b62b6a3d2a95db1ac28905aa9"},"cell_type":"code","source":"primitives[primitives['type'] == 'transform'].head(primitives[primitives['type'] == 'transform'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f80783a7004d51b2604736d10564cc0f8b108c1"},"cell_type":"markdown","source":"1. Now we will apply a **deep feature synthesis (dfs)** function that will generate new features by automatically applying suitable aggregations, I selected a depth of 2. Higher depth values will stack more primitives. \n\n** I decided to go with 3 here in the fork. **"},{"metadata":{"trusted":true,"_uuid":"e752c9c0efce398c0433b1eca0ec39273767ad0a"},"cell_type":"code","source":"features, feature_names = ft.dfs(entityset = es, \n                                 target_entity = 'combine', \n                                 max_depth = 3 #2\n                                )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fece4219d6d1acca69e1dee45b7cb23376e9794"},"cell_type":"markdown","source":"This is a list of new features. For example, \"Title.SUM(combine.Age\" means the sum of Age values for each unique value of Title."},{"metadata":{"trusted":true,"_uuid":"2f00f8c9d4a381cf30e06152228c211ce106ee77"},"cell_type":"code","source":"feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"129faaa06ce81ed2b7d2ecd46f0d47fe2c83ff59"},"cell_type":"code","source":"len(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f5be45e44e5461aa970daf9d4d1218a2aaa4a41","scrolled":true},"cell_type":"code","source":"features[features['Age'] == 22][[\"Title.SUM(combine.Age)\",\"Age\",\"Title\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"768c7a700c5592bd15ddf7dc38d8b5eda3901181"},"cell_type":"markdown","source":"By using \"featuretools\", we were able to **generate <s>146</s> 184 features just in a moment**.\n\nThe \"featuretools\" is a powerful package that allows saving time to create new features from multiple tables of data. However, it does not completely subsitute the human domain knowledge. Additionally, now we are facing another problem known as the \"curse of dimensionality\"."},{"metadata":{"_uuid":"7e604d085df646ee0bda8f55a3081252d643c86c"},"cell_type":"markdown","source":"## 5. \"Curse of dimensionality\": Feature reduction and selection <a name=\"frs\"></a> "},{"metadata":{"_uuid":"ef3fc2b52b6b4cf3f4f224f833bfc1e61a766029"},"cell_type":"markdown","source":"To deal with the \"curse of dimensionality\", it's necessary to apply the feature reduction and selection, which means removing low-value features from the data. But keep in mind that feature selection can hurt the performance of ML models. The tricky thing is that the design of ML models contains an artistic component. It's definitely not the deterministic process with strict rules that should be followed to achieve success. In order to come up with an accurate model, it is necessary to apply, combine and compare dozens of methods. In this notebook, I will not explain all possible approaches to deal with the \"curse of dimensionality\". I will rather concentrate on the following methods:\n\n   * Determine collinear features\n   * Detect the most relevant features using linear models penalized with the L1 norm"},{"metadata":{"_uuid":"c63654129b5e86598821a560435d6c0e15e1a6e8"},"cell_type":"markdown","source":"### 5.1 Determine collinear features"},{"metadata":{"_uuid":"0d696f4b49459bfdd5373037c0a27eda12ca14bd"},"cell_type":"markdown","source":"Collinearity means high intercorrelations among independent features. If we maintain such features in the mode, it might be difficult to assess the effect of independent features on target variable. Therefore we will detect these features and delete them, though applying a manual revision before removal."},{"metadata":{"trusted":true,"_uuid":"9d3470eb19030d08c52918a204ed721dabc87279"},"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.95\n\n# Absolute value correlation matrix\ncorr_matrix = features.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"589c4c1142bbef9a8d25a7fa9a3654e766fb70a9"},"cell_type":"code","source":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d features to remove.' % (len(collinear_features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"200b842b46c2c0f81a0e107a06fbb9a3955d2fbd","scrolled":true},"cell_type":"code","source":"features_filtered = features.drop(columns = collinear_features)\n\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"659c4400c0aac03be961f77f41a91a8e599f85dd"},"cell_type":"markdown","source":"**Be aware, however, that it is not a good idea to remove features only by correlation without understanding the removal process**. Features that have very high correlation (for example, Embarked.SUM(combine.Age) and Embarked.SUM(combine.Fare)) with significant difference between may require additional inve. Therefore manual guidance is necessary. But this topic is outside of the scope of this Kernel."},{"metadata":{"_uuid":"923aeb7ce6b80fc331fa3190b71aa5e0ee7e2884"},"cell_type":"markdown","source":"### 5.2 Detect the most relevant features using linear models penalized with the L1 norm"},{"metadata":{"_uuid":"3bec20fed09daa496da6d92032e7cccedfa27167"},"cell_type":"markdown","source":"The next step is to use linear models penalized with the L1 norml. "},{"metadata":{"trusted":true,"_uuid":"1e35a8ce4e4f34a1ce782fb5d59b167ca6e63030"},"cell_type":"code","source":"# features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]\n\n# train_X = features_positive[:train_df.shape[0]]\n# train_y = train_df['Survived']\n\n# test_X = features_positive[train_df.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b21eafcf946a18dfad1ad6ebe6b5ad7ce9f27f77"},"cell_type":"markdown","source":"<s>Since the number of features is smaller than the number of observations in \"train_X\", the parameter \"dual\" is equal to False.</s>"},{"metadata":{"trusted":true,"_uuid":"790b724e3d411b4d8cf631d11b45ee113f0198c2"},"cell_type":"code","source":"# lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_y)\n# model = SelectFromModel(lsvc, prefit=True)\n# X_new = model.transform(train_X)\n# X_selected_df = pd.DataFrame(X_new, columns=[train_X.columns[i] for i in range(len(train_X.columns)) if model.get_support()[i]])\n# X_selected_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cff6ed5080d790b805f26fe537aa13201b74fe40"},"cell_type":"code","source":"# X_selected_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d951487a2fd125a70714c564136691228bb0323"},"cell_type":"markdown","source":"** In the fork were not going to remove the highly correlated featuers or other using the L1 norm. **"},{"metadata":{"_uuid":"080009f48e01f05111edf3210ea9b6e71d9178f9"},"cell_type":"markdown","source":"## 6. Training and testing the simple model <a name=\"ttm\"></a> "},{"metadata":{"_uuid":"553ad8a6649da224fa6655696013aafd4bbd0c1c"},"cell_type":"markdown","source":"<s>Finally, we will create a basic random forest classifier with 2000 estimators. Please notice that I skip essential steps such as crossvalidation, the analysis of learning curves, etc.</s>\n\nWere going to put together a light gbm model for this.\n\nAll the parameters are listed out in **lgb_params**."},{"metadata":{"trusted":true,"_uuid":"3b72d98edf16e183fd359f97bc88174544b10794"},"cell_type":"code","source":"lgb_params = {\n    \"max_depth\": 8,\n    \"num_leaves\": 1000,\n    \"learning_rate\": 0.033,\n    \"objective\": \"binary\",\n    \"n_estimators\": 500,\n    \"boosting_type\": \"dart\",\n    \"n_jobs\": -1,\n    \"reg_lambda\": 0.01,\n    \"random_state\": 42\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a066038c5ed8eca68927bb8f71c9757a24a66e62"},"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cf33fda1e6de3397fda2c408e56259919e2968c"},"cell_type":"code","source":"train_X = features[:train_df.shape[0]].drop('Cabin_C',axis=1)\ntrain_y = train_df['Survived']\n\ntest_X = features[train_df.shape[0]:].drop('Cabin_C',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e78a8ef48c911beac42c83e374c5fedd6b275a"},"cell_type":"code","source":"train_y.head()\ntrain_X.head()\n# features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f80ba52b78b838885a839d165e5b5904bf8c2cda"},"cell_type":"code","source":"kfolds = 3\npreds = 0\nfor i in range(kfolds):\n    print('In kfold:',str(i+1))\n    xt,xv,yt,yv = train_test_split(train_X, train_y, test_size=0.2, random_state=(i*42))\n    \n    trn = lgb.Dataset(xt,yt.values.flatten())\n    val = lgb.Dataset(xv,yv.values.flatten())\n    model = lgb.train(lgb_params, train_set=trn,\n                     valid_sets=[val], valid_names=['val'],\n                     verbose_eval=100,\n                     early_stopping_rounds=100)\n    \n    val_pred = model.predict(xv, num_iteration=model.best_iteration+50)\n    pred = model.predict(test_X, num_iteration=model.best_iteration+50)\n    preds += pred\n    print('=========================')\n    print(classification_report(np.round(val_pred,0).astype(int), yv))\n    print(\"    F1 Score  : {:.4f}\".format(f1_score(np.round(val_pred,0).astype(int), yv)))\n    print('=========================')\npreds /= kfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"eecdcf6834b8abffda6d7f6b974827f2dfa2d663"},"cell_type":"code","source":"# random_forest = RandomForestClassifier(n_estimators=2000,oob_score=True)\n# random_forest.fit(X_selected_df, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b80a32924d7ef581601881706b0e152ad8d2c281"},"cell_type":"code","source":"# X_selected_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74a42fbeca793eb28b24b3e22206fd97be2fb944"},"cell_type":"code","source":"# Y_pred = random_forest.predict(test_X[X_selected_df.columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20676b43b888bd255102c77912aa7b848b7a54d7"},"cell_type":"code","source":"# print(Y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc153758e8e73b2487b6a35c5f30c8471b86d04"},"cell_type":"code","source":"my_submission = pd.DataFrame({'PassengerId': passenger_id, 'Survived': np.round(preds,0).astype(int)})\nprint(my_submission.head())\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55f5eb6c7e14ef7ec4acbd3631dd8db89592163d"},"cell_type":"markdown","source":"Using light gbm and making a few changes to the features pulled in (and using all the features and a feature tools depth of 3) we got a score of 0.79425!\n\nWhile it certainly makes sense to limit highly correlated features to get to very good ones, a tool such as featuretools would probably be worth using everything thats created. Additionally, going from 13 features to 184 would still not be considered a very wide data set. So in this use case, why not use all of them?"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}