{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8fe1fed0-845c-2477-684b-4678c1475fac"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00a034a1-d42b-ea50-4552-e946c225fa59"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "color = sns.color_palette()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "362a4cd2-ab20-fbd1-9814-49fb64f6b3bf"
      },
      "source": [
        "## Read Data ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c034876-7d33-61b7-648d-cf134e1d7afe"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv', encoding='utf-8')\n",
        "df_train['id'] = df_train['id'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29fc71a4-c3cb-4042-8772-bbee71969e27"
      },
      "outputs": [],
      "source": [
        "df_train.drop_duplicates(inplace=True)\n",
        "df_train.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6fbf3f4-0b2d-ac1d-7194-71845d706ed7"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('../input/test.csv', encoding='utf-8')\n",
        "df_test['test_id'] = df_test['test_id'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9720c66d-589f-e471-7d95-072d957e825d"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat((df_train, df_test))\n",
        "df_all['question1'].fillna('', inplace=True)\n",
        "df_all['question2'].fillna('', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c82a72b-eb0b-4e48-1469-5d165cbfd8b5"
      },
      "outputs": [],
      "source": [
        "df_train.groupby(\"is_duplicate\")['id'].count().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef0e07e7-ad0e-edcf-e5be-43350eb5318d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from gensim.models import word2vec\n",
        "STOP_WORDS = nltk.corpus.stopwords.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1933c29c-cad9-740d-0928-cf46e9615106"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "for col in ['question1', 'question2']:\n",
        "    for sentence in df_train[col].iteritems():\n",
        "        word_list = sentence[1].split(\" \")\n",
        "        corpus.append(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e15d48c-9741-5fc7-7cb2-49654127624b"
      },
      "outputs": [],
      "source": [
        "corpus[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89abb719-4821-3fbe-dccb-bc5639266f68"
      },
      "outputs": [],
      "source": [
        "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)\n",
        "model.wv['india']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "144c99d9-9313-89a3-06c8-65e3eb73e522"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "def dataplot(data):\n",
        "    labels = []\n",
        "    tokens = []\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    model_tnse = TSNE(n_components=2, random_state=0)\n",
        "    np.set_printoptions(suppress=True)\n",
        "    new_tokens = model_tnse.fit_transform(tokens)\n",
        "    x_axis = []\n",
        "    y_axis = []\n",
        "    for i in range(len(x_axis)):\n",
        "        plt.scatter(x_axis[i],y_axis[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x_axis[i], y_axis[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "209c422b-95f6-5935-6328-55624fbaedef"
      },
      "outputs": [],
      "source": [
        "dataplot(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8313d66-f587-2f7c-35d4-18fff6466990"
      },
      "outputs": [],
      "source": [
        "model.most_similar('trump')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e61f266-254a-4dae-5e87-1a6460456023"
      },
      "source": [
        "## Create Vocab ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7edf8cb-63de-86f7-1b4b-a24b9b5ff070"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18737244-f58e-0ff5-3ec2-ba7eb898d893"
      },
      "outputs": [],
      "source": [
        "counts_vectorizer = CountVectorizer(max_features=10000-1).fit(\n",
        "    itertools.chain(df_all['question1'], df_all['question2']))\n",
        "other_index = len(counts_vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3830fdce-b7a1-05e4-ac70-4f450e3940ee"
      },
      "source": [
        "##Prep Data##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0980bc7-b305-d950-fb78-0392d38ee2a8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "655b3f0b-36d2-72b0-39c1-67a0ae421ccc"
      },
      "outputs": [],
      "source": [
        "words_tokenizer = re.compile(counts_vectorizer.token_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df71c37f-3b3a-7b8d-9123-f64be9892c6e"
      },
      "outputs": [],
      "source": [
        "def create_padded_seqs(texts, max_len=10):\n",
        "    seqs = texts.apply(lambda s: \n",
        "        [counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n",
        "         for w in words_tokenizer.findall(s.lower())])\n",
        "    return pad_sequences(seqs, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dff0d239-d393-251c-f468-36e0a75b8804"
      },
      "outputs": [],
      "source": [
        "df_all = df_all.sample(1000) # Just for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f9a4fb5-0431-0541-2e21-ae9869daf69f"
      },
      "outputs": [],
      "source": [
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n",
        "    train_test_split(create_padded_seqs(df_all[df_all['id'].notnull()]['question1']), \n",
        "                     create_padded_seqs(df_all[df_all['id'].notnull()]['question2']),\n",
        "                     df_all[df_all['id'].notnull()]['is_duplicate'].values,\n",
        "                     stratify=df_all[df_all['id'].notnull()]['is_duplicate'].values,\n",
        "                     test_size=0.3, random_state=1989)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "05d261a2-b176-7190-455a-79e5284e6545"
      },
      "source": [
        "##Training##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6dd4bd1e-eeb9-0a20-f96f-214b0e9845c8"
      },
      "outputs": [],
      "source": [
        "import keras.layers as lyr\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa0caf3a-509a-2bbe-3365-94013f18569c"
      },
      "outputs": [],
      "source": [
        "input1_tensor = lyr.Input(X1_train.shape[1:])\n",
        "input2_tensor = lyr.Input(X2_train.shape[1:])\n",
        "\n",
        "words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\n",
        "seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n",
        "\n",
        "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
        "\n",
        "merge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
        "\n",
        "dense1_layer = lyr.Dense(16, activation='sigmoid')(merge_layer)\n",
        "ouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\n",
        "\n",
        "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18853bca-60fa-d4f8-ced7-9a40a84c1f52"
      },
      "outputs": [],
      "source": [
        "model.fit([X1_train, X2_train], y_train, \n",
        "          validation_data=([X1_val, X2_val], y_val), \n",
        "          batch_size=128, epochs=6, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3dd431f7-d8af-734b-d82b-565810b43f71"
      },
      "source": [
        "##Extract Features From Model##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f462071f-ca0e-1445-2653-6992d69e143c"
      },
      "outputs": [],
      "source": [
        "features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
        "features_model.compile(loss='mse', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f322533-15c3-dde3-7c2c-3ab879b5b286"
      },
      "outputs": [],
      "source": [
        "F_train = features_model.predict([X1_train, X2_train], batch_size=128)\n",
        "F_val = features_model.predict([X1_val, X2_val], batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2611a562-499a-3436-5491-988b65ab74a4"
      },
      "source": [
        "##Train XGBoost##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6165f0bb-29f8-7c1d-b92e-60fd24d18d6e"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f40ca07f-2971-588a-4e2f-6882a37f8a05"
      },
      "outputs": [],
      "source": [
        "dTrain = xgb.DMatrix(F_train, label=y_train)\n",
        "dVal = xgb.DMatrix(F_val, label=y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d2b3691-08bc-30f5-4783-9c9ceab4c03c"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Set our parameters for xgboost\n",
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "params['eta'] = 0.02\n",
        "params['max_depth'] = 4\n",
        "\n",
        "\n",
        "\n",
        "watchlist = [(dTrain, 'train'), (dVal, 'valid')]\n",
        "\n",
        "bst = xgb.train(params, dTrain, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b16eb2b8-3016-c363-2660-38983194ff11"
      },
      "source": [
        "##Predict Test##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "34f3fef5-b30c-20f2-2105-7164a83bef7d"
      },
      "outputs": [],
      "source": [
        "X1_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question1'])\n",
        "X2_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "794ad521-42df-a1ae-604c-dc03e41f4c4c"
      },
      "outputs": [],
      "source": [
        "F_test = features_model.predict([X1_test, X2_test], batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ba7e6ef-b1ea-a061-7d54-a0adee79a29d"
      },
      "outputs": [],
      "source": [
        "dTest = xgb.DMatrix(F_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7e3da29-55d7-28d7-6c27-abcb4f6bbd12"
      },
      "outputs": [],
      "source": [
        "df_sub = pd.DataFrame({\n",
        "        'test_id': df_all[df_all['test_id'].notnull()]['test_id'].values,\n",
        "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
        "    }).set_index('test_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35357642-74fe-a9f4-3c26-cb64d8c15e96"
      },
      "outputs": [],
      "source": [
        "df_sub.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13b7a6dc-ff58-5f8d-c3d5-784b08973887"
      },
      "outputs": [],
      "source": [
        "'df_sub['is_duplicate'].hist(bins=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36b3fabe-40e4-5d8a-9f90-641dd30aa6c0"
      },
      "outputs": [],
      "source": [
        "df_sub.to_csv('final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c065ae7b-6cb2-a258-e373-f701b058e8e1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}