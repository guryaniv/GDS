{"cells":[{"metadata":{"_cell_guid":"cdae024e-9d51-46dd-bd28-70c7b4f91425","_uuid":"84b70525e08f2a7d2ea6894583cebc8eb28d5972"},"cell_type":"markdown","source":"In this Kernel uses Manifestos Project data [add reference], and presents a pipeline of using Deep Learning Algorithms to classify blog/forum comments. In the current version the following types of Neural Network Algorithms have been implemented:\n* **Convolutional Neural Networks (CNN)** (Kim, 2014) with **Word2Vec** (Google) \n* **Long Short Term Memory (LSTM)** Recurrent Neural Networks, with **Word2Vec** (Google)\n\n## CNN & Word2Vec Implementation\nThe general logic behind CNNs is presented in Kim (2014).  To use CNNs for sentence classification, imagine sentences and words as image pixels, where the input is sentences are represented as a matrix. \n\nEach row of the matrix is a vector that represents a sentence. \n\nThis vector is the average of  **word2vec** (Google’s Word2Vec pre-trained model) scores of all words in our sentence.\n\nFor 10 sentences using a 300-dimensional embedding we would have a 10×300 matrix as our input. \nThat’s our “image”.\n"},{"metadata":{"_cell_guid":"1e35fec1-f9a6-4d0d-9f56-a2abcc35d223","_uuid":"beac69605322f4a98a30bd6cb7e33111e2a21242","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout, Add\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nimport gensim\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nimport codecs\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nstop_words = set(stopwords.words('english'))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f52096ff6caa5405a264c436b52f33016c4c9ec"},"cell_type":"markdown","source":"Note that we set the _num_epochs_ is low on purpose, so as not to exceed the 14GB RAM of training of the Word2Vec operation later on. For comparison purposes, it might actually be best to do at least 10, so that the histogram gives some more data points."},{"metadata":{"_cell_guid":"26d1feea-73fc-4438-b042-cbb58dcb6b3b","_uuid":"d0ceacafb5f397e76044154734730acd93c68f15","trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 300 # how big is each word vector\nMAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\nMAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n\n#training params\nbatch_size = 256 \nnum_epochs = 2 ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4edd7ec-7ddf-4c83-b3c5-f6dc751c82d4","_uuid":"53086d9c9e086a9f66a4dc245dec759787b7bd7a","trusted":true},"cell_type":"code","source":"train_comments = pd.read_csv(\"../input/manifestos-aus/train-aus.csv\", sep=',', header=0)\nimport pandas as pd\ndata = pd.DataFrame({'T': ['', 'B', 'C', 'D', 'E']})\nres = pd.get_dummies(data)\nres.to_csv('output.csv')\nprint res\ntrain_comments.columns=['id', 'p401', 'p501', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nprint(\"num train: \", train_comments.shape[0])\ntrain_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"962c528c-a6a8-4b39-bc09-f83c3dc9a846","_uuid":"f1e228fc842d11a46da231b337099d3756f43a1f","trusted":true},"cell_type":"code","source":"label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny_train = train_comments[label_names].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89a34bbd-3df4-4843-9356-3ef48d5ecaba","_uuid":"17ecdd78e8c098cd61e7ad19a5a1d39280983b17","trusted":true},"cell_type":"code","source":"test_comments = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\", sep=',', header=0)\ntest_comments.columns=['id', 'comment_text']\nprint(\"num test: \", test_comments.shape[0])\ntest_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72d3b1e6-9406-4961-b7b0-89c21cf37787","_uuid":"d5dd53b853972acf59c5da3d8fee8812b415b263"},"cell_type":"markdown","source":"**Cleaning Text**"},{"metadata":{"_cell_guid":"3241b3ff-dfcf-4361-965d-6f7c71c8bfee","_uuid":"d2d970eb5d9a4042ffa67b86d6d9732f45a31e68","trusted":true},"cell_type":"code","source":"def standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47da37eb-cc1d-40f5-8655-e2fdf6f36c97","_uuid":"37157fd7ace420f22c708a616b0245740e8e499d","trusted":true},"cell_type":"code","source":"train_comments.fillna('_NA_')\ntrain_comments = standardize_text(train_comments, \"text\")\ntrain_comments.to_csv(\"train_clean_data.csv\")\ntrain_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df5b86cf-a38a-4bb1-9a7c-95ef9d11f564","_uuid":"992f605af050c9d395d42d76cdc6c89ddbd18f14","trusted":true},"cell_type":"code","source":"test_comments.fillna('_NA_')\ntest_comments = standardize_text(test_comments, \"comment_text\")\ntest_comments.to_csv(\"test_clean_data.csv\")\ntest_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41b86444-27b9-476a-a767-b8d5cc3104de","_uuid":"18749b4d4dac9e47a0f2c97e2ea7934889c50b02"},"cell_type":"markdown","source":"**Tokenizing Text**"},{"metadata":{"_cell_guid":"c2a2dea0-3008-4227-9032-d1d3442a1461","_uuid":"5eb646e0533f485c39744bfe380b8ca605c3d843","trusted":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nclean_train_comments = pd.read_csv(\"train_clean_data.csv\")\nclean_train_comments['text'] = clean_train_comments['text'].astype('str') \nclean_train_comments.dtypes\nclean_train_comments[\"tokens\"] = clean_train_comments[\"comment_text\"].apply(tokenizer.tokenize)\n# delete Stop Words\nclean_train_comments[\"tokens\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n   \nclean_train_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"230ca18f-3a1e-4077-87ca-b77869f78acc","_uuid":"64495855c6f06ecedfa4ea7537cbb80fe5671ce9","trusted":true},"cell_type":"code","source":"clean_test_comments = pd.read_csv(\"test_clean_data.csv\")\nclean_test_comments['comment_text'] = clean_test_comments['comment_text'].astype('str') \nclean_test_comments.dtypes\nclean_test_comments[\"tokens\"] = clean_test_comments[\"comment_text\"].apply(tokenizer.tokenize)\nclean_test_comments[\"tokens\"] = clean_test_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n\nclean_test_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8443ff4f-2bfc-43ab-8051-6d4543909e93","_uuid":"5b788b67473bbfb913e35ed7ca5568559331e350","trusted":true},"cell_type":"code","source":"all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\ntraining_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))\n#print(clean_train_comments[\"tokens\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21828cf4-cd50-4045-b779-e46fcd0c7d45","_uuid":"5ea92372c340bfc6c4efd7086c026fe96290d5b9","trusted":true},"cell_type":"code","source":"all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\ntest_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\nTEST_VOCAB = sorted(list(set(all_test_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\nprint(\"Max sentence length is %s\" % max(test_sentence_lengths))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b053648-0414-4ba9-82c5-81342903806a","_uuid":"c65f795647a86c1415839d6ed42654eb8f22ba9d"},"cell_type":"markdown","source":"Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence. In this way we lose the syntax of our sentence, while keeping some semantic information.\n![](https://cdn-images-1.medium.com/max/1400/1*THo9NKchWkCAOILvs1eHuQ.png)"},{"metadata":{"_cell_guid":"a871e0f7-2f49-4f9c-8d32-f0a183d7b439","_uuid":"039e8adccac39202f6ee185407e39fec587ae1f5","trusted":true},"cell_type":"code","source":"word2vec_path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\ndef get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n                                                                                generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3fb56b5-6045-4995-bf5d-038cb67f467d","_uuid":"f199f3c8fa270aab90400934f783712c1e6d2c55","trusted":true},"cell_type":"code","source":"training_embeddings = get_word2vec_embeddings(word2vec, clean_train_comments, generate_missing=True)\n# test_embeddings = get_word2vec_embeddings(word2vec, clean_test_comments, generate_missing=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83fdccfb-9549-40d4-b929-787652a66062","_uuid":"71eb13c4238975062db0a5d6f37f3273a3f43d14","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\ntokenizer.fit_on_texts(clean_train_comments[\"comment_text\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"comment_text\"].tolist())\n\ntrain_word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(train_word_index))\n\ntrain_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n#print(train_cnn_data[:4])\n\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights[3:5])\nprint(\"-----------=====-----------\")\nprint(train_embedding_weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a477520a-f8d2-4ab1-866c-a3a2cebbc7a7","_uuid":"190b1764e182887b402bee3af641bbb6adb0c8b5","trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"comment_text\"].tolist())\nprint(clean_test_comments[\"comment_text\"][4])\nprint(test_sequences[4])\ntest_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d256e6f-aab4-44ac-953b-be50b586be55","_uuid":"45f5ba479f261f3a05e06f69b78183a46819b953"},"cell_type":"markdown","source":"Define a Convolutional Neural Network following Yoon Kim model [2]"},{"metadata":{"_cell_guid":"54b65983-a6ed-4509-921e-c459840a1040","_uuid":"8f89ca5fc4fa41bc002cbac0738dc56cb6d7aca1","trusted":true},"cell_type":"code","source":"from keras.layers.merge import concatenate, add\n\ndef ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n    #the filter\n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=trainable)\n\n    #the unknown image\n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    #the merge function of the first convolution \n    embedded_sequences = embedding_layer(sequence_input)\n\n    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n    convs = []\n    filter_sizes = [3,4,5] # in the loop, first apply 3 as size, then 4 then 5\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        #kernel is the filter\n        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n        convs.append(l_pool)\n\n    l_merge = concatenate(convs, axis=1)\n\n    \n    # activated if extra_convoluted is true at the def\n    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n    pool = MaxPooling1D(pool_size=3)(conv)\n\n    if extra_conv==True:\n        x = Dropout(0.5)(l_merge)  \n    else:\n        # Original Yoon Kim model\n        x = Dropout(0.5)(pool)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    # Finally, we feed the output into a Sigmoid layer.\n    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n    preds = Dense(labels_index, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f32f31b8-eda6-4f59-90ec-14eb9d4a9712","_uuid":"1bbb0e66fc5ad6ef4521a9d400db3127969070c6","trusted":true},"cell_type":"code","source":"x_train = train_cnn_data\ny_tr = y_train\nprint(len(list(label_names)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23c15b32-ef1f-4c6e-8977-161f948470fe","_uuid":"65613319e466f72cda5a1d27a35aae620fb040ad","trusted":true},"cell_type":"code","source":"model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)), False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f3b7c3c-cecd-41bc-a62a-cb22ddbebce6","_uuid":"843d512895d45ced0a2f9463f4fbabdccacd6d7b","trusted":true},"cell_type":"code","source":"#define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d8225c5-4cc6-4c52-824b-5217e62c7c29","_uuid":"fc16b6d9009948108f5227b0ccf0296edd074bfc"},"cell_type":"markdown","source":"Now let's train our Neural Network"},{"metadata":{"_cell_guid":"3d6d73bd-d9e8-494a-88d7-04295a030e3c","_uuid":"813df22e089125425c34e1e73ed039c0471ef9c6","trusted":true},"cell_type":"code","source":"hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"984173d7-b21d-4a4d-b613-35f609f13853","_uuid":"562cbcd921cdf6c3d575eb6350e5918e83cca81d","trusted":true},"cell_type":"code","source":"y_test = model.predict(test_cnn_data, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65022913-00b5-4bb8-a4f6-71fc30406afb","_uuid":"0d1849e93ae0ff6acdc7744c1de2814c6c702f6c","trusted":true},"cell_type":"code","source":"#create a submission\nsubmission_df = pd.DataFrame(columns=['id'] + label_names)\nsubmission_df['id'] = test_comments['id'].values \nsubmission_df[label_names] = y_test \nsubmission_df.to_csv(\"./cnn_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ef1bc6c-6d46-4d7f-8900-b833a8165e09","_uuid":"8e092255d6d67075be858bfe1f3b3dc731cf1653","trusted":true},"cell_type":"code","source":"#generate plots\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c88d722c-1921-41ce-8a8c-ab8efcc4e7a2","_uuid":"edf14e826df5db0f231459b9a3a60624b7f31463","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a084ea8c47ac7248f7fee0c85dd880d08bc868bb"},"cell_type":"markdown","source":"## LSTM (bidirectional RNN) & Word2Vec\nUsing the trained word to vector datasets, this section will classify the test sentences using a type of Recurrent Neural Network (Long Short Term Model) and Word2Vec, using Keras libraries."},{"metadata":{"trusted":true,"_uuid":"ddb616c0457cbd1076a3668985a48db14c384307"},"cell_type":"code","source":"from keras.preprocessing import sequence \nfrom keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Embedding, LSTM \nnum_words = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550a691842190166d6f09c10c1ae71768c1c541c"},"cell_type":"code","source":"x_train = sequence.pad_sequences(x_train, maxlen=200) \nx_test = sequence.pad_sequences(y_tr, maxlen=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bb16a6c86b447f56b36d05a7f4f05ec173be529"},"cell_type":"code","source":"#Define network architecture and compile \nmodel = Sequential() \nmodel.add(Embedding(num_words, 50, input_length=200)) \nmodel.add(Dropout(0.2)) \nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \nmodel.add(Dense(250, activation='relu')) \nmodel.add(Dropout(0.2)) \nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c32f84af11fc71b3ba62a2fb944c9d2a2ce54c2"},"cell_type":"code","source":"print(\"ok\")\nmodel.fit(x_train, x_test, batch_size=64, epochs=2) \n\nprint('\\nAccuracy: {}'. format(model.evaluate(x_test, x_test)[1]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbca83d0-aa29-457f-afff-a7e13b19ddef","_uuid":"ed1776d31f77de38e76530bf190102e271c4117f"},"cell_type":"markdown","source":"**References**:   \n* [1] How to solve 90% of NLP problems: a step-by-step guide\n * https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n* [2] Yoon Kim model\n * https://arxiv.org/abs/1408.5882\n* [3] Understanding Convolutional Neural Networks for NLP:\n * http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}