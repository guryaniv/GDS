{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d5b60d6e-e560-e0b2-18c7-8e054eb217e4"
      },
      "source": [
        "# Plotting Feature Importances\n",
        "\n",
        "Tree-based models provide a way to tell which features have the biggest impact     \n",
        "\n",
        "This function plots feature importances for sklearn models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "597b683d-6d62-1f0c-e858-028bcedd8c3c"
      },
      "source": [
        "## Concoct a dataset with 20 features, 6 of which are informative\n",
        "\n",
        "    * features 0-4 are informative (in X_left)\n",
        "    * one unknown feature in 5-19 is also informative (X_right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc227082-ae44-37c9-d1d3-7befade3a6d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X_left, y = make_classification(n_samples     = 100, \n",
        "                                n_features    = 5, \n",
        "                                n_informative = 5, \n",
        "                                n_redundant   = 0, \n",
        "                                n_repeated    = 0, \n",
        "                                n_classes     = 2, \n",
        "                                random_state  = 7,\n",
        "                                n_clusters_per_class=2)\n",
        "\n",
        "X_right, _ = make_classification(n_samples     = 100, \n",
        "                                 n_features    = 15, \n",
        "                                 n_informative = 1, \n",
        "                                 n_redundant   = 5, \n",
        "                                 n_repeated    = 5, \n",
        "                                 n_classes     = 1, \n",
        "                                 random_state  = 7,\n",
        "                                 n_clusters_per_class=1)\n",
        "\n",
        "import numpy as np\n",
        "X = np.hstack((X_left, X_right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6882e611-a8f7-97ed-ca9e-f14f0cd6433c"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83942df9-40e8-ba05-a1a9-29a595e3f9b3"
      },
      "source": [
        "# plot_feature_importances FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f38bd80-de7b-8707-0e82-f146828251c0"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances(clf, X_train, y_train=None, \n",
        "                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n",
        "    '''\n",
        "    plot feature importances of a tree-based sklearn estimator\n",
        "    \n",
        "    Note: X_train and y_train are pandas DataFrames\n",
        "    \n",
        "    Note: Scikit-plot is a lovely package but I sometimes have issues\n",
        "              1. flexibility/extendibility\n",
        "              2. complicated models/datasets\n",
        "          But for many situations Scikit-plot is the way to go\n",
        "          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        clf         (sklearn estimator) if not fitted, this routine will fit it\n",
        "        \n",
        "        X_train     (pandas DataFrame)\n",
        "        \n",
        "        y_train     (pandas DataFrame)  optional\n",
        "                                        required only if clf has not already been fitted \n",
        "        \n",
        "        top_n       (int)               Plot the top_n most-important features\n",
        "                                        Default: 10\n",
        "                                        \n",
        "        figsize     ((int,int))         The physical size of the plot\n",
        "                                        Default: (8,8)\n",
        "        \n",
        "        print_table (boolean)           If True, print out the table of feature importances\n",
        "                                        Default: False\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        the pandas dataframe with the features and their importance\n",
        "        \n",
        "    Author\n",
        "    ------\n",
        "        George Fisher\n",
        "    '''\n",
        "    \n",
        "    __name__ = \"plot_feature_importances\"\n",
        "    \n",
        "    import pandas as pd\n",
        "    import numpy  as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from xgboost.core     import XGBoostError\n",
        "    from lightgbm.sklearn import LightGBMError\n",
        "    \n",
        "    try: \n",
        "        if not hasattr(clf, 'feature_importances_'):\n",
        "            clf.fit(X_train.values, y_train.values.ravel())\n",
        "\n",
        "            if not hasattr(clf, 'feature_importances_'):\n",
        "                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n",
        "                                    format(clf.__class__.__name__))\n",
        "                \n",
        "    except (XGBoostError, LightGBMError, ValueError):\n",
        "        clf.fit(X_train.values, y_train.values.ravel())\n",
        "            \n",
        "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
        "    feat_imp['feature'] = X_train.columns\n",
        "    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
        "    feat_imp = feat_imp.iloc[:top_n]\n",
        "    \n",
        "    feat_imp.sort_values(by='importance', inplace=True)\n",
        "    feat_imp = feat_imp.set_index('feature', drop=True)\n",
        "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
        "    plt.xlabel('Feature Importance Score')\n",
        "    plt.show()\n",
        "    \n",
        "    if print_table:\n",
        "        from IPython.display import display\n",
        "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
        "        display(feat_imp.sort_values(by='importance', ascending=False))\n",
        "        \n",
        "    return feat_imp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0da6b781-5f0e-4910-57cd-fdcb469b8618"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cef6fb2-ba2f-e61d-62d8-f3b360d1ea8c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "X_train = pd.DataFrame(X)\n",
        "y_train = pd.DataFrame(y)\n",
        "\n",
        "from xgboost              import XGBClassifier\n",
        "from sklearn.ensemble     import ExtraTreesClassifier\n",
        "from sklearn.tree         import ExtraTreeClassifier\n",
        "from sklearn.tree         import DecisionTreeClassifier\n",
        "from sklearn.ensemble     import GradientBoostingClassifier\n",
        "from sklearn.ensemble     import BaggingClassifier\n",
        "from sklearn.ensemble     import AdaBoostClassifier\n",
        "from sklearn.ensemble     import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm             import LGBMClassifier\n",
        "\n",
        "\n",
        "clfs = [XGBClassifier(),              LGBMClassifier(), \n",
        "        ExtraTreesClassifier(),       ExtraTreeClassifier(),\n",
        "        BaggingClassifier(),          DecisionTreeClassifier(),\n",
        "        GradientBoostingClassifier(), LogisticRegression(),\n",
        "        AdaBoostClassifier(),         RandomForestClassifier()]\n",
        "\n",
        "for clf in clfs:\n",
        "    try:\n",
        "        _ = plot_feature_importances(clf, X_train, y_train, top_n=X_train.shape[1], title=clf.__class__.__name__)\n",
        "    except AttributeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "935e2e0b-2f26-2a3c-998a-d740f0f8fe29"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}