{"cells":[{"metadata":{"_uuid":"6f76259f2c8a84bb5a34460fc7c55271402a429d"},"cell_type":"markdown","source":"**This kernel makes BERT embeddings and maps them directly to the tokenization indexes of spacy.**\n\n**So you can use spacy to tokenize and get pos and dependancy info but still use the BERT embedding for the token using the .i property of the spacy tokens.**\n\n**These embeddings get ~0.48 with public MLP kernels that use spacy embeddings.**\n"},{"metadata":{"trusted":true,"_uuid":"51a440e8c1cb431364285ee53f882e38db17baf5"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport  pickle\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47d2e3b1f5f5a5b5606d6d464eb461bddbd82d5b"},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\n!pip install bert-embedding\n!pip install https://github.com/dmlc/gluon-nlp/tarball/master\n!pip install bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a85be7f6debb8a98a1ed987cbb7c025cee2ff75a"},"cell_type":"code","source":"import mxnet as mx\nfrom bert_embedding import BertEmbedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8011c98d9076353fbb18f1b47f6c4b4bf8f1485b"},"cell_type":"code","source":"train_df = pd.read_csv('gap-test.tsv', sep='\\t')\ntest_df = pd.read_csv('gap-development.tsv', sep='\\t')\ndev_df = pd.read_csv('gap-validation.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2d5dd47987ac73db82f551518e5a6643f5994ad"},"cell_type":"code","source":"embed_name = 'bert_12_768_12_uncased'\n#this runs much faster on GPU but I could not get it working in kernel even with GPU on.\n#Should just uninstall mxnet and install mxnet_cu92\n#ctx = mx.gpu(0)\n#bert = BertEmbedding(model='bert_12_768_12', dataset_name='book_corpus_wiki_en_uncased', max_seq_length=512, batch_size=8, ctx=ctx)\nbert = BertEmbedding(model='bert_12_768_12', dataset_name='book_corpus_wiki_en_uncased', max_seq_length=512, batch_size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dac42d27426064183587063393849cd52c569af3"},"cell_type":"code","source":"%%time\ncache_file =  'embed_dev_{0}_orig.pkl'.format(embed_name)      \nif not os.path.isfile(cache_file):\n    dev_bert_orig = bert.embedding(dev_df.Text.values[:])  \n    with open(cache_file, 'wb') as f:\n        pickle.dump(dev_bert_orig, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        dev_bert_orig = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9488c692681a19f4b8b3dd587b34ac4e5398741"},"cell_type":"code","source":"%%time\n\ncache_file =  'embed_train_{0}_orig.pkl'.format(embed_name)     \nif not os.path.isfile(cache_file):\n    train_bert_orig = bert.embedding(train_df.Text.values[:])  \n    with open(cache_file, 'wb') as f:\n        pickle.dump(train_bert_orig, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        train_bert_orig = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58e76a6b33131de229f90c266b9e4dc7145f3e0f"},"cell_type":"code","source":"%%time\n\ncache_file =  'embed_test_{0}_orig.pkl'.format(embed_name)\nif not os.path.isfile(cache_file):\n    test_bert_orig = bert.embedding(test_df.Text.values[:])  \n    with open(cache_file, 'wb') as f:\n        pickle.dump(test_bert_orig, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        test_bert_orig = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"254d1d18c8bd68176a3faf17535c32442d36b146"},"cell_type":"code","source":"\ndef get_embedding(df, embed_orig):    \n    bert_embed = []\n    for i, row in enumerate(df.iloc[:].iterrows()):\n        \n        text = row[1]['Text']\n        #There is only one double space in the GAP data but it does throw off the spacy indexing. \n        #It might be better to not do this line and work out making an extra index in the embedding\n        #Otherwhise you need to do this replace everytime you use the data in your model as well\n        doc = nlp(text.replace('  ', ' '))\n        doc_embed = embed_orig[i];\n        doc_embed_new = [];\n        offset = 0\n        spacy_offset = 0\n        #print(text)\n        for w_i in range(len(doc)):\n            if (w_i + spacy_offset) >= len(doc): break\n            w = doc[w_i+spacy_offset]\n            if (w.i + offset) >= len(doc_embed[1]):\n                print (i, 'need longer embedding')         \n                break\n                \n            spacy_text = w.text.lower()\n            \n            embed_word = doc_embed[0][w.i + offset]\n            embed_vector = doc_embed[1][w.i + offset]\n            part_count = 1\n            \n            #print(i, spacy_text, embed_word, len(embed_word) ,len(spacy_text), spacy_offset)\n            while len(embed_word) > len(spacy_text):\n                spacy_offset+=1\n\n                embed_vector/=part_count            \n                doc_embed_new.append(np.array(embed_vector))\n                w = doc[w_i+spacy_offset] \n                spacy_text += w.text.lower()   \n                offset-=1\n            \n            while(embed_word != spacy_text and len(embed_word) <= len(spacy_text)):\n                offset+=1     \n                part_count+=1\n                \n                if (w.i + offset) >= len(doc_embed[1]): \n                    print('Should not happend', w.text)\n                    break\n                embed_word += doc_embed[0][w.i + offset]\n                embed_vector += doc_embed[1][w.i + offset]\n\n            #if you have issues on new data try just running these two while loops again.\n            \n            embed_vector/=part_count            \n            doc_embed_new.append(np.array(embed_vector))\n            if (spacy_text != embed_word):\n                print(i, 'Should not happend', spacy_text, embed_word, len(embed_word) ,len(spacy_text))\n                \n            \n        bert_embed.append(np.array(doc_embed_new))    \n    return np.array(bert_embed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcb70275b50435dad4ac9f8de446c4b6ac2c9600"},"cell_type":"code","source":"%%time\ncache_file =  'embed_train_{0}_spacy.pkl'.format(embed_name)\nif not os.path.isfile(cache_file):\n    embed_train = get_embedding(train_df, train_bert_orig)\n    with open(cache_file, 'wb') as f:\n        pickle.dump(embed_train, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        embed_train = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb63d512a5026394c09635054c2e9c4623593b9"},"cell_type":"code","source":"%%time\ncache_file =  'embed_test_{0}_spacy.pkl'.format(embed_name)\nif not os.path.isfile(cache_file):\n    embed_test = get_embedding(test_df, test_bert_orig)\n    with open(cache_file, 'wb') as f:\n        pickle.dump(embed_test, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        embed_test = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca6b53dc343bc714d38ccb1c57a6d982edac1d85"},"cell_type":"code","source":"%%time\ncache_file =  'embed_dev_{0}_spacy.pkl'.format(embed_name)\nif not os.path.isfile(cache_file):\n    embed_dev = get_embedding(dev_df, dev_bert_orig)\n    with open(cache_file, 'wb') as f:\n        pickle.dump(embed_dev, f)\nelse:        \n    with open(cache_file, 'rb') as f:\n        embed_dev = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83f1949981d430b9e1dce13e89f7b03835f23d48"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}