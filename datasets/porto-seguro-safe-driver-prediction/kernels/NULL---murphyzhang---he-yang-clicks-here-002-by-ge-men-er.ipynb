{"cells": [{"cell_type": "markdown", "source": ["**Ge men er:**\n", "\n", "I was trying to open these file by excel, that's silly and never work out LOL\n", "\n", "All right, here is the problem. We need to predict** if a driver will file an insurance claim next year**. The train data file has 59 columns, 595212 rows; the test data file has 58 columns, and 892816 rows; and the sample_submission file has 2 columns and 892816 rows.\n", "\n", "The only different column between train and test is 'target' column. The 2 columns in submission are id and target. These id in train and test has no overlap. \n", " \n", " Ge Men er, **please** continue reading and feel free to **add comments**."], "metadata": {"_uuid": "adae1c914e550e06b2625e30f869d89d7fd56658", "_cell_guid": "8c9c7b2e-aaa7-486d-9856-87b10c268d03"}}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # for drawing plot graph\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "metadata": {"_uuid": "4b6293fdc6cac2070cc27f482865fd317abb9695", "_cell_guid": "515c7d9e-8bdf-48aa-b14e-27b4bfd4f31a"}, "execution_count": null, "outputs": []}, {"source": ["#input the train set and test set\n", "train_df = pd.read_csv(\"../input/train.csv\")\n", "test_df = pd.read_csv(\"../input/test.csv\")"], "cell_type": "code", "metadata": {"_uuid": "46db1793d0c0b457b15586d0875ca26277db1e67", "_cell_guid": "16bbf0c2-663e-44f4-91f2-2d1d1f026ad3", "collapsed": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["**Q1: What's the meaning of target?**\n", "\n", "\n", "**MF Answer: ** I guess they want us to predict the probability of whether or not a claim was filed for the driver, so it should be in the range of [0,1]. \n", "\n", "Because I'm not sure if the target value could be 0.3, 0.8 or even 0.5. I made the plot gragh. The above graph shows the target value in train data file are either 1 or 0. Uh-Oh:(\n", "\n", "But, in the sample_submission file, they have all the target value as 0.0364. Do you think it's a hint for us, the value could be any REAL value between [0,1]? \n", "Or we just rougly tend to think every driver's skill is as bad as YANG LAO SHI's. Thus, when target value less or equal than 0.5, we would give a piecewise to make it equals to 0 and elsewhere equals to 1.\n", "\n", "I also find a smiliar discussion on https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/40222\n", ", please search rkoptelov or Daniel Moller.\n", "\n", ">>**HY Answer:** You can write anything here, such as agree:) Seriously, we can see the precentage of 0 and 1 are very different, probably 9:1. Do you think 0 means having a claim or not? Or it doesn't matter??? But how could that work?!\n", "\n", "**Q2: What's the meaning of the other columns?**\n", "\n", "We observed the column name is a combination of three or four parts, the fourth part(if has) could be cat or bin.\n", "\n", "I find this answer from the** data description**. \n", "\n", "2.1  In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). \n", "\n", "2.2 In addition, feature names include the postfix** bin to indicate binary features and cat to indicate categorical features**.\n", "\n", "2.3  **Features without these designations are either continuous or ordinal**. Values of **-1** indicate that the feature was **missing** from the observation. \n", "\n", ">>Do you think we can do more in this part to understand features better, such as same first two parts, compute coefficient or we will compute each two anyhow. "], "metadata": {"_uuid": "53647e50e8bdb8ab6f877285926d3c9b6db1fdc1", "_cell_guid": "e31e730e-b33d-43fb-86cf-1bb75d070958"}}, {"cell_type": "markdown", "source": ["**Ge men er:**\n", "\n", "Seriously,I agree with most of your points : ).Here is mine:\n", "\n", "For train set, it is proper that the target values 0 or 1. The 0 means a driver will not initiate an auto insurance claim in the next year, And 1 means the opposite situation. For test set, we should give the probability that a driver will initiate an auto insurance claim in the next year.  We will order the user by probability and only the order counts. \n", "\n", "The models based on GBDT can predict the probability that the sample is classified to 1. \n", "\n", "If we use the xgboost or lightgbm, the coefficients between features can not influence our result. The missing data is fine for our model.\n", "\n", "We don't know what the featrues represent. I think the only thing we can do is just dummies the category features, But I am worry about this rude method. We can do it by this method , check the result and think other method later.\n", "\n", "The step of this question:\n", "\n", "1) dummies the category features\n", "\n", "2) check the features without the designations ,   continuous  or  category\n", "\n", "3) put them into model Violently\n", "\n", "Finally, try our best to fingure out the meanings of features.\n", "\n", "To humor Mofei"], "metadata": {"_uuid": "2d32a11b5934aa0261374379e785ad07435602b2", "_cell_guid": "a91fd6f6-bb0e-4eab-a4b6-e5b19881a163"}}, {"source": ["#for every category feature,check the number of different value and make sure the number is less than 10(10 is my\n", "#own advice,we can change this value later).\n", "train_col = list(train_df.columns)\n", "for col in train_col:\n", "    if col[-3:] == 'cat':\n", "        values = train_df[col]\n", "        if len(set(values)) > 10:\n", "            print(col,len(set(values)))\n", "# view the result, 'ps_car_11_cat' can't get dummies directly"], "cell_type": "code", "metadata": {"_uuid": "a27de866a2335d5cbfaac92490d99ab2bed40a4a", "_cell_guid": "f9aaa88e-f277-422d-b36f-81109624dcff"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": [">> **MF: I checked the result:**\n", "\n", ">>ps_car_01_cat 13\n", "\n", ">>ps_car_06_cat 18\n", "\n", ">>ps_car_11_cat 104\n", "\n", "\n", ">> I didn't see anything wrong. All the output column has more than 10  unique values.\n", "\n", ">> But I still don't understand why more than 10 would be a problem. Could you explain more for me?\n", "\n", "\n", ">> BTW, what are dummies meaning here?\n", "\n", "\n", "\n"], "metadata": {}}, {"source": ["#check every features  category or continuous\n", "\n", "ConVar = []\n", "for col in train_col:\n", "    values = train_df[col]\n", "    if len(set(values)) > 30:\n", "        ConVar.append(col)\n"], "cell_type": "code", "metadata": {"_uuid": "6b969cafc932c17ed83d229243057cb5e62a3299", "_cell_guid": "5fa71e2f-a2e5-40fa-acf0-8106420c3794"}, "execution_count": null, "outputs": []}, {"source": ["      for col in ConVar:\n", " print(col,len(set(train_df[col])))"], "cell_type": "code", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": [">>**MF**: I understand above one step is checking the number of distinct value in all column, print out these columns witg numbers which are more than 30.\n", " Same question, what's the purpose of this step? I thought we will just jump to xgboost and try it without any processing.\n"], "metadata": {}}, {"source": ["missvar = []\n", "dis = len(train_df)\n", "for col in train_col:\n", "    values = train_df[col]\n", "    miss = [-1 for x in values if x == -1]\n", "    if len(miss)/dis > 0.5:\n", "        print(len(miss))\n", "        missvar.append(col)"], "cell_type": "code", "metadata": {"_uuid": "4481a3217ebf14406eff6d48203219e32854e525", "_cell_guid": "924b3e22-463e-4fa5-b63a-7c769e18e7bf"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": [">> Sorry, Heyang. I didn't get your point of above code. \n", "\n", ">> What's your purpose here?\n", "\n", ">> If you want to print the count of all the missing then why do you make the condition >0,5?\n", ">> Or you think only when the rate is more than 0.5, we need to deal with the missing value?"], "metadata": {}}, {"source": ["values = train_df[missvar[0]]\n", "values_ide = list(set(values))\n", "print(values_ide)\n", "print(missvar,len(set(values)),len(values))"], "cell_type": "code", "metadata": {"_uuid": "619356ed39b6fc3484be0e7b9ececbb609835dcc", "_cell_guid": "69014c4a-4de0-410c-b9c4-cfe7ffbe47f0"}, "execution_count": null, "outputs": []}, {"source": [], "cell_type": "code", "metadata": {"_uuid": "ff438ba6dac073c3c67a351d839e9272fdf4682f", "_cell_guid": "c09fda39-b179-4ac1-be18-318fcb3410c9", "collapsed": true}, "execution_count": null, "outputs": []}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python"}}}