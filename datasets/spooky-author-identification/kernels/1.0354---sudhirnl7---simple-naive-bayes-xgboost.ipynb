{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py"}}, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "13143fee-a436-4a9d-a018-bd54cd1125ec", "_uuid": "ec810c56409599999eb8420eb1c81e09831e1768"}, "source": ["# Import library"]}, {"cell_type": "code", "metadata": {"_cell_guid": "0d13a650-ff5e-438f-845e-01d83e5f1f1e", "_uuid": "3df85235a1116a0fe3abe2d85fe63b45825e0469", "collapsed": true}, "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "import re\n", "import nltk\n", "from nltk.corpus import stopwords \n", "from nltk.stem.porter import PorterStemmer\n", "from nltk.tokenize import word_tokenize\n", "import string\n", "\n", "from sklearn.model_selection import train_test_split,KFold\n", "from sklearn.metrics import confusion_matrix,roc_auc_score,log_loss\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "import xgboost as xgb \n", "seed = 4353"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a1e3cb84-3f79-49c2-bc32-c8e91c630fe1", "_uuid": "9fe04e698c86858078fe83e4dc9a9a2c8e60d6ec"}, "source": ["# Load data set"]}, {"cell_type": "code", "metadata": {"_cell_guid": "9c11c6a1-273c-4951-8b83-b8627063a461", "_uuid": "0fa0e5a41e1ea331f152592178cd68d76fbe1e76", "collapsed": true}, "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bc839ad4-87e4-41ea-8a34-9bb3ce429a78", "_uuid": "51b8437eca7ef83821a4b176b180e6397465c3d4"}, "source": ["# Explore data set"]}, {"cell_type": "code", "metadata": {"_cell_guid": "7eb1ebd0-5d76-43d4-b86d-dadac88fab6a", "_uuid": "997a4c0e229eb55d37099676dd14d5c8224515f0"}, "source": ["print('Number of rows and columns in data set',train.shape)\n", "train.head()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "0ff9bca9-b160-4036-ae87-5fd1b2ad6046", "_uuid": "a12dc30926feaed8d6ebf85378785fbcbe7b46a1"}, "source": ["print('Number of rows and columns in data set',test.shape)\n", "test.head()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "85146249-d6e8-408d-88c5-f3ce2b7d3c34", "_uuid": "27a9b690140727f2fce71b117541f96c862fc7a6"}, "source": ["# Authors Target variable distribution"]}, {"cell_type": "code", "metadata": {"_cell_guid": "a66c5345-ca01-4f01-a7b3-7cfc1df9bed3", "_uuid": "ef40c22a8d0c0e2a07c691354ec98b243a1665fd"}, "source": ["train['author'].value_counts()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "f9945c7c-67c7-4807-90fa-c8d31650fb1c", "_uuid": "9b3de94637523d08c12b1f94aee0cb9299b7a3de"}, "source": ["plt.figure(figsize=(14,5))\n", "sns.countplot(train['author'],)\n", "plt.xlabel('Author')\n", "plt.title('Target variable distribution')\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "099ff203-cf95-4346-80c3-5c162629a3c2", "_uuid": "fa1145d2585db8f38ea913d258aa9234042ac535"}, "source": ["# Text cleaning"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a04a3f3f-9b52-46b4-b519-a794fccf4eae", "_uuid": "c2d0a5b2831220ce346ffb415240ca8d7b1d5c41"}, "source": ["## Remove unwanted punctuation mark"]}, {"cell_type": "code", "metadata": {"_cell_guid": "e718404c-6a83-4290-9c63-dd200018aa3d", "_uuid": "1a8d284d65142efbb3301e0e29faa2d43336ab7d"}, "source": ["print('Original text:\\n',train['text'][0])\n", "review = re.sub('[^A-Za-z0-9]',\" \",train['text'][0]) \n", "print('\\nAfter removal of punctuation:\\n',review)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "344eaf65-39e4-4af8-bdfd-567a569ad2ee", "_uuid": "33c10b65b84f181f4e2e8a1dbe474a338746bc1f"}, "source": ["# Split sentence into word"]}, {"cell_type": "code", "metadata": {"_cell_guid": "edb564c1-fc03-4032-900f-b4c2cee9f62e", "_uuid": "1b6186675545f8135642c28157389dd60170cc12"}, "source": ["review = word_tokenize(train['text'][0]) \n", "print('Word Tokenize:\\n',review)\n", "\n", "review = [word for word in str(train['text'][0]).lower().split() if  word not in set(stopwords.words('english'))]\n", "print('\\nRemoval of Stopwords:\\n',review)\n", "\n", "review = [word for word in str(train['text'][0]).lower().split() if  word in set(stopwords.words('english'))]\n", "print('\\nStopwords in the sentence:\\n',review)\n", "\n", "ps = PorterStemmer()\n", "review = [ps.stem(word) for word in str(train['text'][0]).lower().split()]\n", "print('\\nStemming of word:\\n',review)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "71628a16-0fb3-43b9-8138-54753399990f", "_uuid": "a505ec748a77e715255d31d3bb4d4aa335790160"}, "source": ["# Function for text cleaning"]}, {"cell_type": "code", "metadata": {"_cell_guid": "8c90c145-18f0-4111-aaf5-20957a0f83a4", "_uuid": "aeee4a146a0c7263c1c713d1cd68d294390cd19a", "collapsed": true}, "source": ["def clean_text(df):\n", "    ps = PorterStemmer()\n", "    corpus = []\n", "    for i in range(0, df.shape[0]):        \n", "        review = re.sub('[^A-Za-z0-9]',\" \",df['text'][i])\n", "        review = word_tokenize(review)        \n", "        review = [word for word in review if word.lower() not in set(stopwords.words('english'))]\n", "        review = [ps.stem(word) for word in review]\n", "        review = ' '.join(review)\n", "        corpus.append(review)\n", "    \n", "    return corpus"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "f5358bcc-08c6-451f-b768-660917027e24", "_uuid": "ba4b2103f9ec18db23c1e09fef27627623ffd42e", "collapsed": true}, "source": ["corp_train = clean_text(train)\n", "corp_test = clean_text(test)\n", "train['clean_text'] = corp_train\n", "test['clean_text'] = corp_test\n", "del corp_train,corp_test"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "bb0970c4-c27c-4036-badf-aec6b068aec2", "_uuid": "e56f68d3435b159f9dac54dd879d4b46ab5c889a"}, "source": ["# Determine length of text"]}, {"cell_type": "code", "metadata": {"_cell_guid": "c03574bb-634f-418d-bf07-a95650d0d2a6", "_uuid": "9043043f4ac9b08e02e2af1687675088a9cde53a", "collapsed": true}, "source": ["def text_len(df):\n", "    #i = ['text']\n", "    df['num_words'] = df['text'].apply(lambda x: len(str(x).split()))\n", "    df['num_uniq_words'] = df['text'].apply(lambda x: len(set(str(x).split())))\n", "    df['num_chars'] = df['text'].apply(lambda x: len(str(x)))\n", "    df['num_stopwords'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() \n", "                                                          if w in set(stopwords.words('english'))]))\n", "    df['num_punctuations'] = df['text'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))\n", "    df['num_words_upper'] = df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "    df['num_words_title'] = df['text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "    df['mean_word_len'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "\n", "text_len(train)\n", "text_len(test)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "39945785-a431-4432-a037-a88f6bf59b46", "_uuid": "3f9e5d800d051bcc245e2aef604b882b1edc2e87"}, "source": ["# Data analysis"]}, {"cell_type": "code", "metadata": {"_cell_guid": "509cdb29-4c98-455a-8ff0-9bc1a6d097ef", "_uuid": "72d3faf74342d09cbfe6cd90ce9424dd6755a2b1"}, "source": ["plt.figure(figsize=(14,6))\n", "plt.subplot(211)\n", "sns.heatmap(pd.crosstab(train['author'],train['num_words']),cmap='gist_earth',xticklabels=False)\n", "plt.xlabel('Original text word count')\n", "plt.ylabel('Author')\n", "\n", "plt.subplot(212)\n", "sns.heatmap(pd.crosstab(train['author'],train['num_uniq_words']),cmap='gist_heat',xticklabels=False)\n", "plt.xlabel('Unique text word count')\n", "plt.ylabel('Author')\n", "plt.tight_layout()\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "9e0f2ebf-eeeb-49cf-9554-4936642dd225", "_uuid": "69784080f2efb0bbeb1f32545d2d0e3a9d584037"}, "source": ["plt.figure(figsize=(14,6))\n", "sns.distplot(train['num_words'],bins=100,color='r')\n", "plt.title('Distribution of original text words')\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "266137bd-79b0-49cb-b917-28b2374ae6c7", "_uuid": "a0e181142a16a71c334eaf0bd87415e890708575"}, "source": ["train['num_uniq_words'].value_counts()[0:10].plot(kind='bar',color=['r','y'])\n", "plt.xlabel('Original text word count')\n", "plt.ylabel('Count')\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "09896735-14aa-44db-86ca-e5757b342d18", "_uuid": "0d350acf96913bb3cca7642f054e31fbefa10d48"}, "source": ["plt.figure(figsize=(14,6))\n", "sns.heatmap(train.corr(),annot=True)\n", "plt.show()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "dce34c0c-0fc6-4746-928b-db6751eaddc0", "_uuid": "8766a2c06da9baaa6174ff54cc17987636b35689"}, "source": ["# Bag of words"]}, {"cell_type": "code", "metadata": {"_cell_guid": "b2d43743-6842-4c66-b44b-e2e13e6fa08d", "_uuid": "9d1c0411cbdb547e701e2125aa87d493f0fe326b", "collapsed": true}, "source": ["cv =CountVectorizer(max_features=2000,ngram_range=(1,3),dtype=np.int8,stop_words='english')\n", "X_cv = cv.fit_transform(train['clean_text']).toarray()\n", "X_test_cv = cv.fit_transform(test['clean_text']).toarray()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a686f38b-6d0c-4cf1-9b2e-2892d8319119", "_uuid": "9d6f5654b5838c3845b30c21aa2acfb555004b6c"}, "source": ["# Encoder"]}, {"cell_type": "code", "metadata": {"_cell_guid": "fecc5682-7010-447d-a962-51be49ec4ff0", "_uuid": "4ce9168ffb906f5a4d1d2da87cc49e43c0fff0f1", "collapsed": true}, "source": ["author_name = {'EAP':0,'HPL':1,'MWS':2}\n", "y = train['author'].map(author_name) "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "96e5aeab-a45e-4b4d-92b8-1e278397ee51", "_uuid": "afef8628f9528a89259acf4a70642d58210d61a2"}, "source": ["# Naive Bayes classifier"]}, {"cell_type": "code", "metadata": {"_cell_guid": "10a2d828-15cc-4ef4-9b7c-315df335261b", "_uuid": "cff9b54ad58fa4f19247dc42f58002366dd5ad3a"}, "source": ["mNB = MultinomialNB()\n", "\n", "kf = KFold(n_splits=10,shuffle=True,random_state=seed)\n", "pred_test_full = 0\n", "cv_score = []\n", "i=1\n", "for train_index,test_index in kf.split(X_cv):\n", "    print('{} of KFlod {}'.format(i,kf.n_splits))    \n", "    xtr,xvl = X_cv[train_index], X_cv[test_index]\n", "    ytr,yvl = y[train_index], y[test_index]\n", "    \n", "    mNB.fit(xtr,ytr)\n", "    y_mNB = mNB.predict(xvl)\n", "    cv_score.append(log_loss(yvl,mNB.predict_proba(xvl)))    \n", "    pred_test_full += mNB.predict_proba(X_test_cv)\n", "    i+=1\n", "#roc_auc_score(yvl,mNB.predict_proba(xvl)[:,1]) # not for multi class\n", "print(cv_score)\n", "print('Mean accuracy score',np.mean(cv_score))\n", "print('confusion matrix:\\n',confusion_matrix(yvl,y_mNB))\n", "del xtr,ytr,xvl,yvl"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b8e66b90-1d1e-4530-b305-efbd070cc059", "_uuid": "15bda78b53ac325008211f651aea42bbc649fc7c"}, "source": ["#  Submit prediction for unseen dataset"]}, {"cell_type": "code", "metadata": {"_cell_guid": "5d072eae-4fe2-4b5a-a63f-a6889db1e1e8", "_uuid": "6c1328b6caebcd8209ab3ef050174b4d6c829bef", "collapsed": true}, "source": ["y_pred = pred_test_full/10\n", "submit = pd.DataFrame(test['id'])\n", "submit = submit.join(pd.DataFrame(y_pred))\n", "submit.columns = ['id','EAP','HPL','MWS'] \n", "#submit.to_csv('spooky_pred1.csv.gz',index=False,compression='gzip')\n", "submit.to_csv('spooky_pred1.csv',index=False)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6f5601a2-9c98-49dc-988e-fed27694919c", "_uuid": "e5794470454fc73aad787fcd5d4778c9ec7c7889"}, "source": ["# TfIdf  (Term frequency Inverse document frequency)"]}, {"cell_type": "code", "metadata": {"_cell_guid": "5e9b55d2-8415-4b45-bd2c-3d0d161617f2", "_uuid": "d56bbb8524c704c7baad8bab83b7ebc359d79fd8", "collapsed": true}, "source": ["tfidf = TfidfVectorizer(max_features=2000,dtype=np.float32,analyzer='word',\n", "                        ngram_range=(1, 3),use_idf=True, smooth_idf=True, \n", "                        sublinear_tf=True)\n", "X_tf = tfidf.fit_transform(train['clean_text']).toarray()\n", "X_test_tf = tfidf.fit_transform(test['clean_text']).toarray()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a66e40c4-efce-4907-95a7-b0ee753cf021", "_uuid": "0109815461a89a18f7d92f417b8909418498ee15"}, "source": ["# Naive Bayes classifier"]}, {"cell_type": "code", "metadata": {"_cell_guid": "e905535a-fcca-4485-b13f-f06d13801cd2", "_uuid": "109933aadbebf7ebf85b85dc435ba3a181ca0f05", "scrolled": true}, "source": ["mNB = MultinomialNB()\n", "\n", "kf = KFold(n_splits=10,shuffle=True,random_state=seed)\n", "pred_test_full = 0\n", "cv_score = []\n", "i=1\n", "for train_index,test_index in kf.split(X_tf):\n", "    print('{} of KFlod {}'.format(i,kf.n_splits))    \n", "    xtr,xvl = X_tf[train_index], X_tf[test_index]\n", "    ytr,yvl = y[train_index], y[test_index]\n", "    \n", "    mNB.fit(xtr,ytr)\n", "    y_mNB = mNB.predict(xvl)\n", "    cv_score.append(log_loss(yvl,mNB.predict_proba(xvl)))    \n", "    pred_test_full += mNB.predict_proba(X_test_tf)\n", "    i+=1\n", "#roc_auc_score(yvl,mNB.predict_proba(xvl)[:,1]) # not for multi class\n", "print(cv_score)\n", "print('Mean accuracy score',np.mean(cv_score))\n", "print('confusion matrix:\\n',confusion_matrix(yvl,y_mNB))\n", "del xtr,ytr,xvl,yvl"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a132038d-b076-4006-a21e-949d2e146078", "_uuid": "c76e30888ea6643a6afaed1f76122ee26a2d063e"}, "source": ["#  Submit prediction for unseen dataset"]}, {"cell_type": "code", "metadata": {"_cell_guid": "2d12dfa1-bbdd-4038-8490-68048a9603d1", "_uuid": "618f4bc3cccebf95773aee53ef7ebd414d0d0202", "collapsed": true}, "source": ["y_pred = pred_test_full/10\n", "submit = pd.DataFrame(test['id'])\n", "submit = submit.join(pd.DataFrame(y_pred))\n", "submit.columns = ['id','EAP','HPL','MWS'] \n", "#submit.to_csv('spooky_pred2.csv.gz',index=False,compression='gzip')\n", "submit.to_csv('spooky_pred2.csv',index=False)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "25ffaeb2-6ada-4fc7-a4ba-a27752f10a1b", "_uuid": "4b3f8ba978271531871606390bf0e7730ed82f1f"}, "source": ["# Merge"]}, {"cell_type": "code", "metadata": {"_cell_guid": "fbc26b7d-f775-47e1-8d29-44adfb3ee79a", "_uuid": "38c03ab3c192903d3a0f0f633af3322c5975335c", "collapsed": true}, "source": ["#filter data set\n", "unwanted = ['text','id','clean_text']\n", "X_tf = np.concatenate((X_tf,train.drop(unwanted+['author'],axis=1).values),axis=1)\n", "X_test_tf = np.concatenate((X_test_tf,test.drop(unwanted,axis=1).values),axis=1)\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1451a9a3-b2d9-40a3-a122-3d8a51ebdbf5", "_uuid": "755686e5628c60746a145ee985ff1daf848a6e49"}, "source": ["# Xgboost"]}, {"cell_type": "code", "metadata": {"_cell_guid": "c5480f61-1d5d-46a6-8a92-c95b9fe142aa", "_uuid": "0337aff2e5f965830760a466c9ac1a9879b92b12", "collapsed": true}, "source": ["def runXGB(xtrain,xvalid,ytrain,yvalid,xtest,eta=0.1,early_stop=50,max_depth=5,n_rounds=1000):\n", "    \n", "    params = {        \n", "        'objective':'multi:softprob',\n", "        'learning_rate':eta,\n", "        'max_depth':max_depth,\n", "        'num_class':3,\n", "        'subsample':0.8,\n", "        'colsample_bytree':0.8,\n", "        'eval_metric':'mlogloss',\n", "        'min_child_weight':10,\n", "        'reg_alpha':1.5, \n", "        'reg_lambda':5,\n", "        'scale_pos_weight':1,  \n", "        #'verbose':0,\n", "        'seed':seed,        \n", "        'n_thread':-1 \n", "    }\n", "    \n", "    #plst = list(params.items())\n", "    dtrain =xgb.DMatrix(xtrain,label=ytrain)\n", "    dvalid = xgb.DMatrix(xvalid,label=yvalid)    \n", "    dtest = xgb.DMatrix(xtest)\n", "    watchlist = [(dtrain,'train'),(dvalid,'test')]\n", "    \n", "    model = xgb.train(params,dtrain,n_rounds,evals=watchlist,early_stopping_rounds=early_stop,verbose_eval=10)\n", "    pred = model.predict(dvalid,ntree_limit = model.best_ntree_limit)\n", "    pred_test = model.predict(dtest,ntree_limit = model.best_ntree_limit)\n", "    \n", "    return pred_test,model"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "29af885c-c13f-4688-8f59-fe02c5d30f98", "_uuid": "9c93cd24911c12b601f93d6dfa4eb630a719bdb3", "scrolled": false}, "source": ["kf = KFold(n_splits=2,shuffle=True,random_state=seed)\n", "pred_test_full = 0\n", "cv_score = []\n", "i=1\n", "for train_index,test_index in kf.split(X_tf):\n", "    print('{} of KFlod {}'.format(i,kf.n_splits))    \n", "    xtr,xvl = X_tf[train_index], X_tf[test_index]\n", "    ytr,yvl = y[train_index], y[test_index]\n", "        \n", "    pred_xgb,xg_model = runXGB(xtr,xvl,ytr,yvl,X_test_tf,n_rounds=200,eta=0.5)\n", "    pred_test_full += pred_xgb\n", "    cv_score.append(xg_model.best_score)\n", "    i+=1\n", "#roc_auc_score(yvl,mNB.predict_proba(xvl)[:,1]) # not for multi class\n", "#print(cv_score)\n", "#print('Mean accuracy score',np.mean(cv_score))\n", "#del xtr,ytr,xvl,yvl,X_tf,X_test_tf"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "13282674-2de4-4eee-90fa-13b4144f3378", "_uuid": "efc25288e53cd99f2098a2368ec65c59fd697909"}, "source": ["Increase Kflood to 5,10, number of rounds to 500,1000,2000"]}, {"cell_type": "code", "metadata": {"_cell_guid": "a4174b26-9b70-4f8b-9eec-58bf7f7b2805", "_uuid": "56950412dbaecd48a819249b194ccbe52032fdf3"}, "source": ["print(cv_score)\n", "print('Mean accuracy score',np.mean(cv_score))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b71a5c22-b216-4f91-b481-276f2fa307bf", "_uuid": "2144f9f4518e49323bdb52c3c8a73969ce0a7dee"}, "source": ["#  Submit prediction for unseen dataset"]}, {"cell_type": "code", "metadata": {"_cell_guid": "b382f5db-f061-47e5-bcc9-e90b5ee7df83", "_uuid": "6b153c81cb2ce7f67b3fc0acf6fbac98f6fb58a5", "collapsed": true}, "source": ["y_pred = pred_test_full/2\n", "submit = pd.DataFrame(test['id'])\n", "submit = submit.join(pd.DataFrame(y_pred))\n", "submit.columns = ['id','EAP','HPL','MWS'] \n", "#submit.to_csv('spooky_pred3.csv.gz',index=False,compression='gzip')\n", "submit.to_csv('spooky_pred3.csv',index=False)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4b8dcd0a-c027-48e1-b84f-aae719b17ff3", "_uuid": "2de533d50f6cbd3740971bb9a938b2cb5585d744"}, "source": ["# If like it please Upvote, Thank you for visiting "]}], "nbformat_minor": 1, "nbformat": 4}