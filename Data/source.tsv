Source	Next Source
import datetime import gc import numpy as np import os import pandas as pd import random import nltk import string  import matplotlib.pyplot as plt import seaborn as sns  from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from scipy.stats import skew, kurtosis  from sklearn.metrics import f1_score from sklearn.model_selection import KFold import lightgbm as lgb import xgboost as xgb  from tqdm import tqdm	df_train = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/train.csv') df_train.head()
df_train = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/train.csv') df_train.head()	df_test = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/test.csv') df_test.head()
df_test = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/test.csv') df_test.head()	news_list = pd.read_csv('../input/20-newsgroups/list.csv') news_list.shape
news_list = pd.read_csv('../input/20-newsgroups/list.csv') news_list.shape	print(df_train.shape, df_test.shape)
print(df_train.shape, df_test.shape)	len(df_train['target'].value_counts()) # 20 Newsgroups -- checks out
len(df_train['target'].value_counts()) # 20 Newsgroups -- checks out	# Are the classes balanced?  count_target = df_train['target'].value_counts()  plt.figure(figsize=(8,4)) sns.barplot(count_target.index, count_target.values, alpha=0.8) plt.ylabel('Number of Occurrences', fontsize=12) plt.xlabel('Target', fontsize=12);
# Are the classes balanced?  count_target = df_train['target'].value_counts()  plt.figure(figsize=(8,4)) sns.barplot(count_target.index, count_target.values, alpha=0.8) plt.ylabel('Number of Occurrences', fontsize=12) plt.xlabel('Target', fontsize=12);	df_train['ciphertext'].iloc[0]
df_train['ciphertext'].iloc[0]	df_train.info()
df_train.info()	## Basic features (a lot of these ideas from https://www.kaggle.com/opanichev/lightgbm-and-simple-features)  def add_feats(df): # Some of these features might be strongly correlated          tqdm.pandas('add_basic')     df['len'] = df['ciphertext'].progress_apply(lambda x: len(str(x))) # Characters     df['unique'] = df['ciphertext'].progress_apply(lambda x: len(set(str(x)))) # Unique characters     df['punctuations'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]))     df['uniq_punctuations'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c in string.punctuation])))     df['letters'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isalpha()]))     df['uniq_letters'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isalpha()])))     df['numbers'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isdigit()]))     df['uniq_numbers'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isdigit()])))     df['uppercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isupper()]))     df['uniq_uppercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isupper()])))     df['lowercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.islower()]))     df['uniq_lowercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.islower()])))
## Basic features (a lot of these ideas from https://www.kaggle.com/opanichev/lightgbm-and-simple-features)  def add_feats(df): # Some of these features might be strongly correlated          tqdm.pandas('add_basic')     df['len'] = df['ciphertext'].progress_apply(lambda x: len(str(x))) # Characters     df['unique'] = df['ciphertext'].progress_apply(lambda x: len(set(str(x)))) # Unique characters     df['punctuations'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]))     df['uniq_punctuations'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c in string.punctuation])))     df['letters'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isalpha()]))     df['uniq_letters'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isalpha()])))     df['numbers'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isdigit()]))     df['uniq_numbers'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isdigit()])))     df['uppercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.isupper()]))     df['uniq_uppercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.isupper()])))     df['lowercase'] = df['ciphertext'].progress_apply(lambda x: len([c for c in str(x) if c.islower()]))     df['uniq_lowercase'] = df['ciphertext'].progress_apply(lambda x: len(set([c for c in str(x) if c.islower()])))	add_feats(df_train)
add_feats(df_train)	add_feats(df_test)
add_feats(df_test)	df_train.head()
df_train.head()	"plt.figure(figsize=(12,12)) sns.violinplot(x=\'target\', y=\'unique\', data=df_train) plt.xlabel(\'Target\', fontsize=12) plt.ylabel(\'Number of unique characters in text\', fontsize=12) plt.title(""Number of unique characters by target"", fontsize=15);"
"plt.figure(figsize=(12,12)) sns.violinplot(x=\'target\', y=\'unique\', data=df_train) plt.xlabel(\'Target\', fontsize=12) plt.ylabel(\'Number of unique characters in text\', fontsize=12) plt.title(""Number of unique characters by target"", fontsize=15);"	"plt.figure(figsize=(12,12)) sns.violinplot(x=\'target\', y=\'uniq_punctuations\', data=df_train) plt.xlabel(\'Target\', fontsize=12) plt.ylabel(\'Number of unique punctuations in text\', fontsize=12) plt.title(""Number of unique punctuations by target"", fontsize=15);"
"plt.figure(figsize=(12,12)) sns.violinplot(x=\'target\', y=\'uniq_punctuations\', data=df_train) plt.xlabel(\'Target\', fontsize=12) plt.ylabel(\'Number of unique punctuations in text\', fontsize=12) plt.title(""Number of unique punctuations by target"", fontsize=15);"	fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,20)) sns.violinplot(x='difficulty', y='unique', data=df_train, ax=ax1) ax1.set_xlabel('Difficulty', fontsize=12) ax1.set_ylabel('Number of unique characters in text', fontsize=12) sns.violinplot(x='difficulty', y='uniq_punctuations', data=df_train, ax=ax2) ax2.set_xlabel('Difficulty', fontsize=12) ax2.set_ylabel('Number of unique punctuations in text', fontsize=12) sns.violinplot(x='difficulty', y='numbers', data=df_train, ax=ax3) ax3.set_xlabel('Difficulty', fontsize=12) ax3.set_ylabel('Number of numbers in text', fontsize=12) sns.violinplot(x='difficulty', y='uppercase', data=df_train, ax=ax4) ax4.set_xlabel('Difficulty', fontsize=12) ax4.set_ylabel('Number of uppercase in text', fontsize=12);
fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize=(20,20)) sns.violinplot(x='difficulty', y='unique', data=df_train, ax=ax1) ax1.set_xlabel('Difficulty', fontsize=12) ax1.set_ylabel('Number of unique characters in text', fontsize=12) sns.violinplot(x='difficulty', y='uniq_punctuations', data=df_train, ax=ax2) ax2.set_xlabel('Difficulty', fontsize=12) ax2.set_ylabel('Number of unique punctuations in text', fontsize=12) sns.violinplot(x='difficulty', y='numbers', data=df_train, ax=ax3) ax3.set_xlabel('Difficulty', fontsize=12) ax3.set_ylabel('Number of numbers in text', fontsize=12) sns.violinplot(x='difficulty', y='uppercase', data=df_train, ax=ax4) ax4.set_xlabel('Difficulty', fontsize=12) ax4.set_ylabel('Number of uppercase in text', fontsize=12);	df_train.corr()['target'] # Some of these features seem to have strong negative correlations with the target ## Unique punctuations matter apparently
df_train.corr()['target'] # Some of these features seem to have strong negative correlations with the target ## Unique punctuations matter apparently	cols_to_drop = ['Id','ciphertext'] X = df_train.drop(cols_to_drop, axis=1, errors='ignore')  feature_names = list(X.columns)  y = df_train['target'].values X = X.values  X_test = df_test.drop(cols_to_drop, axis=1, errors='ignore') id_test = df_test['Id'].values
cols_to_drop = ['Id','ciphertext'] X = df_train.drop(cols_to_drop, axis=1, errors='ignore')  feature_names = list(X.columns)  y = df_train['target'].values X = X.values  X_test = df_test.drop(cols_to_drop, axis=1, errors='ignore') id_test = df_test['Id'].values	lgb_params = {     'boosting_type': 'gbdt',     'objective': 'multiclass',     'metric': 'multi_logloss',     'max_depth': 5,     'num_leaves': 31,     'learning_rate': 0.05,     'feature_fraction': 0.85,     'bagging_fraction': 0.85,     'bagging_freq': 5,     'verbose': -1,     'num_threads': -1,     'lambda_l1': 1.0,     'lambda_l2': 1.0,     'min_gain_to_split': 0,     'num_class': df_train['target'].nunique() }
lgb_params = {     'boosting_type': 'gbdt',     'objective': 'multiclass',     'metric': 'multi_logloss',     'max_depth': 5,     'num_leaves': 31,     'learning_rate': 0.05,     'feature_fraction': 0.85,     'bagging_fraction': 0.85,     'bagging_freq': 5,     'verbose': -1,     'num_threads': -1,     'lambda_l1': 1.0,     'lambda_l2': 1.0,     'min_gain_to_split': 0,     'num_class': df_train['target'].nunique() }	cnt = 0 p_buf = [] p_valid_buf = [] n_splits = 5 kf = KFold(     n_splits=n_splits,      random_state=0) err_buf = []    undersampling = 0
cnt = 0 p_buf = [] p_valid_buf = [] n_splits = 5 kf = KFold(     n_splits=n_splits,      random_state=0) err_buf = []    undersampling = 0	print(X.shape, y.shape) print(X_test.shape)
print(X.shape, y.shape) print(X_test.shape)	n_features = X.shape[1]  for train_index, valid_index in kf.split(X, y):     print('Fold {}/{}'.format(cnt + 1, n_splits))     params = lgb_params.copy()       lgb_train = lgb.Dataset(         X[train_index],          y[train_index],          feature_name=feature_names,         )     lgb_train.raw_data = None      lgb_valid = lgb.Dataset(         X[valid_index],          y[valid_index],         )     lgb_valid.raw_data = None      model = lgb.train(         params,         lgb_train,         num_boost_round=10000,         valid_sets=[lgb_train, lgb_valid],         early_stopping_rounds=100,         verbose_eval=100,     )      if cnt == 0:         importance = model.feature_importance()         model_fnames = model.feature_name()         tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]         tuples = [x for x in tuples if x[1] > 0]         print('Important features:')         for i in range(20):             if i < len(tuples):                 print(tuples[i])             else:                 break          del importance, model_fnames, tuples      p = model.predict(X[valid_index], num_iteration=model.best_iteration)     err = f1_score(y[valid_index], np.argmax(p, axis=1), average='macro')      print('{} F1: {}'.format(cnt + 1, err))      p = model.predict(X_test, num_iteration=model.best_iteration)     if len(p_buf) == 0:         p_buf = np.array(p, dtype=np.float16)     else:         p_buf += np.array(p, dtype=np.float16)     err_buf.append(err)      cnt += 1      del model, lgb_train, lgb_valid, p     gc.collect      # Train on one fold #     if cnt > 0: #         break   err_mean = np.mean(err_buf) err_std = np.std(err_buf) print('F1 = {:.6f} +/- {:.6f}'.format(err_mean, err_std))  preds = p_buf/cnt
n_features = X.shape[1]  for train_index, valid_index in kf.split(X, y):     print('Fold {}/{}'.format(cnt + 1, n_splits))     params = lgb_params.copy()       lgb_train = lgb.Dataset(         X[train_index],          y[train_index],          feature_name=feature_names,         )     lgb_train.raw_data = None      lgb_valid = lgb.Dataset(         X[valid_index],          y[valid_index],         )     lgb_valid.raw_data = None      model = lgb.train(         params,         lgb_train,         num_boost_round=10000,         valid_sets=[lgb_train, lgb_valid],         early_stopping_rounds=100,         verbose_eval=100,     )      if cnt == 0:         importance = model.feature_importance()         model_fnames = model.feature_name()         tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]         tuples = [x for x in tuples if x[1] > 0]         print('Important features:')         for i in range(20):             if i < len(tuples):                 print(tuples[i])             else:                 break          del importance, model_fnames, tuples      p = model.predict(X[valid_index], num_iteration=model.best_iteration)     err = f1_score(y[valid_index], np.argmax(p, axis=1), average='macro')      print('{} F1: {}'.format(cnt + 1, err))      p = model.predict(X_test, num_iteration=model.best_iteration)     if len(p_buf) == 0:         p_buf = np.array(p, dtype=np.float16)     else:         p_buf += np.array(p, dtype=np.float16)     err_buf.append(err)      cnt += 1      del model, lgb_train, lgb_valid, p     gc.collect      # Train on one fold #     if cnt > 0: #         break   err_mean = np.mean(err_buf) err_std = np.std(err_buf) print('F1 = {:.6f} +/- {:.6f}'.format(err_mean, err_std))  preds = p_buf/cnt	subm = pd.DataFrame() subm['Id'] = id_test subm['Predicted'] = np.argmax(preds, axis=1) subm.to_csv('submission.csv', index=False)
" import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.ensemble import RandomForestClassifier import os print(os.listdir(""../input"")) from sklearn.feature_extraction.text import TfidfVectorizer"	"train=pd.read_csv(""../input/train.csv"") test=pd.read_csv(""../input/test.csv"") samp=pd.read_csv(""../input/sample_submission.csv"")"
"train=pd.read_csv(""../input/train.csv"") test=pd.read_csv(""../input/test.csv"") samp=pd.read_csv(""../input/sample_submission.csv"")"	test.head(2)
test.head(2)	train.head()
train.head()	train['difficulty'].value_counts()
train['difficulty'].value_counts()	train.info()
train.info()	"print(""Total length of newsgroup :"",len(train[\'target\'].value_counts()))"
"print(""Total length of newsgroup :"",len(train[\'target\'].value_counts()))"	tf=TfidfVectorizer() train_inp=tf.fit_transform(train['ciphertext'])
tf=TfidfVectorizer() train_inp=tf.fit_transform(train['ciphertext'])	train_inp.shape
train_inp.shape	rf=RandomForestClassifier()
rf=RandomForestClassifier()	rf.fit(train_inp,train['target'])
rf.fit(train_inp,train['target'])	test_inp=tf.transform(test['ciphertext'])
test_inp=tf.transform(test['ciphertext'])	pre=pd.DataFrame(rf.predict(test_inp),columns=['Predicted'])
pre=pd.DataFrame(rf.predict(test_inp),columns=['Predicted'])	pd.concat([test[['Id']],pre],axis=1).to_csv('submit.csv',index=False)
#Import needed libraries from nltk.tokenize import word_tokenize, wordpunct_tokenize import itertools import pandas as pd import numpy as np from nltk.corpus import stopwords import matplotlib.pyplot as plt from wordcloud import WordCloud from sklearn.metrics import f1_score from sklearn.model_selection import train_test_split from keras.models import Sequential from keras.layers import Dense,Activation from keras.layers import Flatten, Dropout, Convolution1D, Bidirectional, LSTM, CuDNNLSTM from keras.layers.embeddings import Embedding from sklearn.linear_model import LogisticRegression from keras.preprocessing.sequence import pad_sequences from keras.preprocessing.text import Tokenizer from keras.utils import np_utils	df_train = pd.read_csv('../input/train.csv') df_test = pd.read_csv('../input/test.csv') df_submission = pd.read_csv('../input/sample_submission.csv')
df_train = pd.read_csv('../input/train.csv') df_test = pd.read_csv('../input/test.csv') df_submission = pd.read_csv('../input/sample_submission.csv')	"print(""Train shape : "",df_train.shape) print(""Test shape : "",df_test.shape)"
"print(""Train shape : "",df_train.shape) print(""Test shape : "",df_test.shape)"	"#Tokenize sentences tokenizer = Tokenizer()  text_train = df_train[""ciphertext""].values text_test = df_test[""ciphertext""].values  tokenizer.fit_on_texts(list(text_train)+list(text_test))  print(\'Tokenizing train...\') tokenized_text_train = tokenizer.texts_to_sequences(text_train) print(\'Tokenizing test...\') tokenized_text_test = tokenizer.texts_to_sequences(text_test)"
"#Tokenize sentences tokenizer = Tokenizer()  text_train = df_train[""ciphertext""].values text_test = df_test[""ciphertext""].values  tokenizer.fit_on_texts(list(text_train)+list(text_test))  print(\'Tokenizing train...\') tokenized_text_train = tokenizer.texts_to_sequences(text_train) print(\'Tokenizing test...\') tokenized_text_test = tokenizer.texts_to_sequences(text_test)"	#Pad sentences max_len = 30  print('Padding train...') padded_text_train = pad_sequences(tokenized_text_train, maxlen=max_len) print('Padding test...') padded_text_test = pad_sequences(tokenized_text_test, maxlen=max_len)
#Pad sentences max_len = 30  print('Padding train...') padded_text_train = pad_sequences(tokenized_text_train, maxlen=max_len) print('Padding test...') padded_text_test = pad_sequences(tokenized_text_test, maxlen=max_len)	y = df_train['target']  dummy_y = np_utils.to_categorical(y)
y = df_train['target']  dummy_y = np_utils.to_categorical(y)	#Split in train and validation train_x, valid_x, train_y, valid_y = train_test_split(padded_text_train, dummy_y, test_size=0.15, random_state=42)  test_x = np.array(padded_text_test)
#Split in train and validation train_x, valid_x, train_y, valid_y = train_test_split(padded_text_train, dummy_y, test_size=0.15, random_state=42)  test_x = np.array(padded_text_test)	#Build LSTM Network model max_features = 50000  model_lstm = Sequential() model_lstm.add(Embedding(max_features, 300, input_length=max_len)) model_lstm.add(Bidirectional(CuDNNLSTM(144))) model_lstm.add(Dropout(0.2)) model_lstm.add(Dense(96, activation='elu')) model_lstm.add(Dropout(0.1)) model_lstm.add(Dense(20, activation='softmax')) model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Fit the model model_lstm.fit(train_x, train_y, validation_split= 0.2, epochs=4, batch_size=64)
#Build LSTM Network model max_features = 50000  model_lstm = Sequential() model_lstm.add(Embedding(max_features, 300, input_length=max_len)) model_lstm.add(Bidirectional(CuDNNLSTM(144))) model_lstm.add(Dropout(0.2)) model_lstm.add(Dense(96, activation='elu')) model_lstm.add(Dropout(0.1)) model_lstm.add(Dense(20, activation='softmax')) model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Fit the model model_lstm.fit(train_x, train_y, validation_split= 0.2, epochs=4, batch_size=64)	#Predict on validation and check F1 score pred_valid = model_lstm.predict(valid_x) f1_err = f1_score(np.argmax(valid_y, axis=1), np.argmax(pred_valid, axis=1), average='macro')  print('F1 score on validation set:', f1_err)
#Predict on validation and check F1 score pred_valid = model_lstm.predict(valid_x) f1_err = f1_score(np.argmax(valid_y, axis=1), np.argmax(pred_valid, axis=1), average='macro')  print('F1 score on validation set:', f1_err)	#Predict on test set pred_test = model_lstm.predict(test_x)
#Predict on test set pred_test = model_lstm.predict(test_x)	final_prediction = np.argmax(pred_test, axis=1)
final_prediction = np.argmax(pred_test, axis=1)	df_submission['Predicted'] = final_prediction  df_submission.to_csv('submission.csv', index = False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	import os import time import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tqdm import tqdm import math from sklearn.model_selection import train_test_split from sklearn import metrics  from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, AvgPool2D from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D from keras.optimizers import Adam from keras.models import Model from keras import backend as K from keras.engine.topology import Layer from keras import initializers, regularizers, constraints, optimizers, layers
import os import time import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tqdm import tqdm import math from sklearn.model_selection import train_test_split from sklearn import metrics  from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate, AvgPool2D from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D from keras.optimizers import Adam from keras.models import Model from keras import backend as K from keras.engine.topology import Layer from keras import initializers, regularizers, constraints, optimizers, layers	"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/test.csv"")"
"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/test.csv"")"	# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)
# train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)	train_df = train_df.sample(frac=1) # val_df = val_df.sample(frac=1)
train_df = train_df.sample(frac=1) # val_df = val_df.sample(frac=1)	train_df['l'] = train_df['ciphertext'].apply(lambda t: len(str(t)))
train_df['l'] = train_df['ciphertext'].apply(lambda t: len(str(t)))	train_df.l.describe()
train_df.l.describe()	chars = set([])  for a in train_df.ciphertext.values:     for c in str(a):         chars.add(c)
chars = set([])  for a in train_df.ciphertext.values:     for c in str(a):         chars.add(c)	len(chars)
len(chars)	tokenizer = Tokenizer(num_words=110) tokenizer.fit_on_texts(list(map(lambda w: list(str(w)), train_df.ciphertext.values))) train_X = tokenizer.texts_to_sequences(list(map(lambda w: list(str(w)), train_df.ciphertext.values)))  test_X = tokenizer.texts_to_sequences(list(map(lambda w: list(str(w)), test_df.ciphertext.values)))
tokenizer = Tokenizer(num_words=110) tokenizer.fit_on_texts(list(map(lambda w: list(str(w)), train_df.ciphertext.values))) train_X = tokenizer.texts_to_sequences(list(map(lambda w: list(str(w)), train_df.ciphertext.values)))  test_X = tokenizer.texts_to_sequences(list(map(lambda w: list(str(w)), test_df.ciphertext.values)))	train_X = pad_sequences(train_X, maxlen=300) test_X = pad_sequences(test_X, maxlen=300)
train_X = pad_sequences(train_X, maxlen=300) test_X = pad_sequences(test_X, maxlen=300)	from keras.utils import to_categorical  train_y = to_categorical(train_df['target'].values)
from keras.utils import to_categorical  train_y = to_categorical(train_df['target'].values)	"# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code from keras.callbacks import Callback class CyclicLR(Callback):     """"""This callback implements a cyclical learning rate policy (CLR).     The method cycles the learning rate between two boundaries with     some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).     The amplitude of the cycle can be scaled on a per-iteration or      per-cycle basis.     This class has three built-in policies, as put forth in the paper.     ""triangular"":         A basic triangular cycle w/ no amplitude scaling.     ""triangular2"":         A basic triangular cycle that scales initial amplitude by half each cycle.     ""exp_range"":         A cycle that scales initial amplitude by gamma**(cycle iterations) at each          cycle iteration.     For more detail, please see paper.          # Example         ```python             clr = CyclicLR(base_lr=0.001, max_lr=0.006,                                 step_size=2000., mode=\'triangular\')             model.fit(X_train, Y_train, callbacks=[clr])         ```          Class also supports custom scaling functions:         ```python             clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))             clr = CyclicLR(base_lr=0.001, max_lr=0.006,                                 step_size=2000., scale_fn=clr_fn,                                 scale_mode=\'cycle\')             model.fit(X_train, Y_train, callbacks=[clr])         ```         # Arguments         base_lr: initial learning rate which is the             lower boundary in the cycle.         max_lr: upper boundary in the cycle. Functionally,             it defines the cycle amplitude (max_lr - base_lr).             The lr at any cycle is the sum of base_lr             and some scaling of the amplitude; therefore              max_lr may not actually be reached depending on             scaling function.         step_size: number of training iterations per             half cycle. Authors suggest setting step_size             2-8 x training iterations in epoch.         mode: one of {triangular, triangular2, exp_range}.             Default \'triangular\'.             Values correspond to policies detailed above.             If scale_fn is not None, this argument is ignored.         gamma: constant in \'exp_range\' scaling function:             gamma**(cycle iterations)         scale_fn: Custom scaling policy defined by a single             argument lambda function, where              0 <= scale_fn(x) <= 1 for all x >= 0.             mode paramater is ignored          scale_mode: {\'cycle\', \'iterations\'}.             Defines whether scale_fn is evaluated on              cycle number or cycle iterations (training             iterations since start of cycle). Default is \'cycle\'.     """"""      def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode=\'triangular\',                  gamma=1., scale_fn=None, scale_mode=\'cycle\'):         super(CyclicLR, self).__init__()          self.base_lr = base_lr         self.max_lr = max_lr         self.step_size = step_size         self.mode = mode         self.gamma = gamma         if scale_fn == None:             if self.mode == \'triangular\':                 self.scale_fn = lambda x: 1.                 self.scale_mode = \'cycle\'             elif self.mode == \'triangular2\':                 self.scale_fn = lambda x: 1/(2.**(x-1))                 self.scale_mode = \'cycle\'             elif self.mode == \'exp_range\':                 self.scale_fn = lambda x: gamma**(x)                 self.scale_mode = \'iterations\'         else:             self.scale_fn = scale_fn             self.scale_mode = scale_mode         self.clr_iterations = 0.         self.trn_iterations = 0.         self.history = {}          self._reset()      def _reset(self, new_base_lr=None, new_max_lr=None,                new_step_size=None):         """"""Resets cycle iterations.         Optional boundary/step size adjustment.         """"""         if new_base_lr != None:             self.base_lr = new_base_lr         if new_max_lr != None:             self.max_lr = new_max_lr         if new_step_size != None:             self.step_size = new_step_size         self.clr_iterations = 0.              def clr(self):         cycle = np.floor(1+self.clr_iterations/(2*self.step_size))         x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)         if self.scale_mode == \'cycle\':             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)         else:             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)              def on_train_begin(self, logs={}):         logs = logs or {}          if self.clr_iterations == 0:             K.set_value(self.model.optimizer.lr, self.base_lr)         else:             K.set_value(self.model.optimizer.lr, self.clr())                          def on_batch_end(self, epoch, logs=None):                  logs = logs or {}         self.trn_iterations += 1         self.clr_iterations += 1          self.history.setdefault(\'lr\', []).append(K.get_value(self.model.optimizer.lr))         self.history.setdefault(\'iterations\', []).append(self.trn_iterations)          for k, v in logs.items():             self.history.setdefault(k, []).append(v)                  K.set_value(self.model.optimizer.lr, self.clr())       def f1(y_true, y_pred):     \'\'\'     metric from here      https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras     \'\'\'     def recall(y_true, y_pred):         """"""Recall metric.          Only computes a batch-wise average of recall.          Computes the recall, a metric for multi-label classification of         how many relevant items are selected.         """"""         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))         recall = true_positives / (possible_positives + K.epsilon())         return recall      def precision(y_true, y_pred):         """"""Precision metric.          Only computes a batch-wise average of precision.          Computes the precision, a metric for multi-label classification of         how many selected items are relevant.         """"""         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))         precision = true_positives / (predicted_positives + K.epsilon())         return precision     precision = precision(y_true, y_pred)     recall = recall(y_true, y_pred)     return 2*((precision*recall)/(precision+recall+K.epsilon()))"
"# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code from keras.callbacks import Callback class CyclicLR(Callback):     """"""This callback implements a cyclical learning rate policy (CLR).     The method cycles the learning rate between two boundaries with     some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).     The amplitude of the cycle can be scaled on a per-iteration or      per-cycle basis.     This class has three built-in policies, as put forth in the paper.     ""triangular"":         A basic triangular cycle w/ no amplitude scaling.     ""triangular2"":         A basic triangular cycle that scales initial amplitude by half each cycle.     ""exp_range"":         A cycle that scales initial amplitude by gamma**(cycle iterations) at each          cycle iteration.     For more detail, please see paper.          # Example         ```python             clr = CyclicLR(base_lr=0.001, max_lr=0.006,                                 step_size=2000., mode=\'triangular\')             model.fit(X_train, Y_train, callbacks=[clr])         ```          Class also supports custom scaling functions:         ```python             clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))             clr = CyclicLR(base_lr=0.001, max_lr=0.006,                                 step_size=2000., scale_fn=clr_fn,                                 scale_mode=\'cycle\')             model.fit(X_train, Y_train, callbacks=[clr])         ```         # Arguments         base_lr: initial learning rate which is the             lower boundary in the cycle.         max_lr: upper boundary in the cycle. Functionally,             it defines the cycle amplitude (max_lr - base_lr).             The lr at any cycle is the sum of base_lr             and some scaling of the amplitude; therefore              max_lr may not actually be reached depending on             scaling function.         step_size: number of training iterations per             half cycle. Authors suggest setting step_size             2-8 x training iterations in epoch.         mode: one of {triangular, triangular2, exp_range}.             Default \'triangular\'.             Values correspond to policies detailed above.             If scale_fn is not None, this argument is ignored.         gamma: constant in \'exp_range\' scaling function:             gamma**(cycle iterations)         scale_fn: Custom scaling policy defined by a single             argument lambda function, where              0 <= scale_fn(x) <= 1 for all x >= 0.             mode paramater is ignored          scale_mode: {\'cycle\', \'iterations\'}.             Defines whether scale_fn is evaluated on              cycle number or cycle iterations (training             iterations since start of cycle). Default is \'cycle\'.     """"""      def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode=\'triangular\',                  gamma=1., scale_fn=None, scale_mode=\'cycle\'):         super(CyclicLR, self).__init__()          self.base_lr = base_lr         self.max_lr = max_lr         self.step_size = step_size         self.mode = mode         self.gamma = gamma         if scale_fn == None:             if self.mode == \'triangular\':                 self.scale_fn = lambda x: 1.                 self.scale_mode = \'cycle\'             elif self.mode == \'triangular2\':                 self.scale_fn = lambda x: 1/(2.**(x-1))                 self.scale_mode = \'cycle\'             elif self.mode == \'exp_range\':                 self.scale_fn = lambda x: gamma**(x)                 self.scale_mode = \'iterations\'         else:             self.scale_fn = scale_fn             self.scale_mode = scale_mode         self.clr_iterations = 0.         self.trn_iterations = 0.         self.history = {}          self._reset()      def _reset(self, new_base_lr=None, new_max_lr=None,                new_step_size=None):         """"""Resets cycle iterations.         Optional boundary/step size adjustment.         """"""         if new_base_lr != None:             self.base_lr = new_base_lr         if new_max_lr != None:             self.max_lr = new_max_lr         if new_step_size != None:             self.step_size = new_step_size         self.clr_iterations = 0.              def clr(self):         cycle = np.floor(1+self.clr_iterations/(2*self.step_size))         x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)         if self.scale_mode == \'cycle\':             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)         else:             return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)              def on_train_begin(self, logs={}):         logs = logs or {}          if self.clr_iterations == 0:             K.set_value(self.model.optimizer.lr, self.base_lr)         else:             K.set_value(self.model.optimizer.lr, self.clr())                          def on_batch_end(self, epoch, logs=None):                  logs = logs or {}         self.trn_iterations += 1         self.clr_iterations += 1          self.history.setdefault(\'lr\', []).append(K.get_value(self.model.optimizer.lr))         self.history.setdefault(\'iterations\', []).append(self.trn_iterations)          for k, v in logs.items():             self.history.setdefault(k, []).append(v)                  K.set_value(self.model.optimizer.lr, self.clr())       def f1(y_true, y_pred):     \'\'\'     metric from here      https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras     \'\'\'     def recall(y_true, y_pred):         """"""Recall metric.          Only computes a batch-wise average of recall.          Computes the recall, a metric for multi-label classification of         how many relevant items are selected.         """"""         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))         recall = true_positives / (possible_positives + K.epsilon())         return recall      def precision(y_true, y_pred):         """"""Precision metric.          Only computes a batch-wise average of precision.          Computes the precision, a metric for multi-label classification of         how many selected items are relevant.         """"""         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))         precision = true_positives / (predicted_positives + K.epsilon())         return precision     precision = precision(y_true, y_pred)     recall = recall(y_true, y_pred)     return 2*((precision*recall)/(precision+recall+K.epsilon()))"	"maxlen = 300 max_features = 110 embed_dim = 20  inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_dim, input_length=train_X.shape[1])(inp) x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x) avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) conc = concatenate([avg_pool, max_pool]) conc = Dense(64, activation=""relu"")(conc) # conc = Dropout(0.4)(conc) outp = Dense(20, activation=""softmax"")(conc)      model = Model(inputs=inp, outputs=outp) model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[f1])"
"maxlen = 300 max_features = 110 embed_dim = 20  inp = Input(shape=(maxlen,)) x = Embedding(max_features, embed_dim, input_length=train_X.shape[1])(inp) x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x) avg_pool = GlobalAveragePooling1D()(x) max_pool = GlobalMaxPooling1D()(x) conc = concatenate([avg_pool, max_pool]) conc = Dense(64, activation=""relu"")(conc) # conc = Dropout(0.4)(conc) outp = Dense(20, activation=""softmax"")(conc)      model = Model(inputs=inp, outputs=outp) model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[f1])"	model.summary()
model.summary()	history = model.fit(train_X, train_y, batch_size=32, epochs=20, validation_split=0.1)
history = model.fit(train_X, train_y, batch_size=32, epochs=20, validation_split=0.1)	import matplotlib.pyplot as plt  plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'validation'], loc='upper left') plt.show()  plt.plot(history.history['f1']) plt.plot(history.history['val_f1']) plt.title('model f1') plt.ylabel('f1') plt.xlabel('epoch') plt.legend(['train', 'validation'], loc='upper left') plt.show()
import matplotlib.pyplot as plt  plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'validation'], loc='upper left') plt.show()  plt.plot(history.history['f1']) plt.plot(history.history['val_f1']) plt.title('model f1') plt.ylabel('f1') plt.xlabel('epoch') plt.legend(['train', 'validation'], loc='upper left') plt.show()	y_hat_pred = model.predict(test_X)  def to_v(v):     return list(map(lambda x: np.argmax(x), v))
y_hat_pred = model.predict(test_X)  def to_v(v):     return list(map(lambda x: np.argmax(x), v))	sub_df = pd.DataFrame(list(zip(test_df.Id.values, to_v(y_hat_pred))), columns=['Id', 'Predicted'])
sub_df = pd.DataFrame(list(zip(test_df.Id.values, to_v(y_hat_pred))), columns=['Id', 'Predicted'])	sub_df
sub_df	sub_df.to_csv('submission.csv', index=False)
sub_df.to_csv('submission.csv', index=False)	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	%%bash -e if ! [[ -f ./spm_train ]]; then   wget https://github.com/google/sentencepiece/archive/v0.1.8.zip   echo '8799f4983608897e8eb3370385eda149180d309c7276db939f955d6507d53846  v0.1.8.zip' | sha256sum -c   unzip v0.1.8.zip   conda install -y cmake pkg-config   export SENTENCEPIECE_HOME=$(pwd)/sentencepiece   export PKG_CONFIG_PATH=${SENTENCEPIECE_HOME}/lib/pkgconfig   (cd sentencepiece-0.1.8 && mkdir -p build)   (cd sentencepiece-0.1.8/build && cmake -DCMAKE_INSTALL_PREFIX=${SENTENCEPIECE_HOME} ..  && make -j4 && make install)   (cd sentencepiece-0.1.8/python && python setup.py install)   rm -rf sentencepiece-0.1.8 v0.1.8.zip fi
%%bash -e if ! [[ -f ./spm_train ]]; then   wget https://github.com/google/sentencepiece/archive/v0.1.8.zip   echo '8799f4983608897e8eb3370385eda149180d309c7276db939f955d6507d53846  v0.1.8.zip' | sha256sum -c   unzip v0.1.8.zip   conda install -y cmake pkg-config   export SENTENCEPIECE_HOME=$(pwd)/sentencepiece   export PKG_CONFIG_PATH=${SENTENCEPIECE_HOME}/lib/pkgconfig   (cd sentencepiece-0.1.8 && mkdir -p build)   (cd sentencepiece-0.1.8/build && cmake -DCMAKE_INSTALL_PREFIX=${SENTENCEPIECE_HOME} ..  && make -j4 && make install)   (cd sentencepiece-0.1.8/python && python setup.py install)   rm -rf sentencepiece-0.1.8 v0.1.8.zip fi	"def read_train_text(filename=\'../input/train.csv\'):     return pd.read_csv(filename)  def write_cipher_text(texts, filename=\'spm_train.txt\'):     with open(filename, \'w\',encoding=\'utf-8\') as f:         for text in texts:             f.write(text + ""\ "")  train_df = read_train_text() test_df = read_train_text(filename=\'../input/test.csv\') ciphertexts = list(train_df.ciphertext.values) + list(test_df.ciphertext.values) write_cipher_text(ciphertexts)"
"def read_train_text(filename=\'../input/train.csv\'):     return pd.read_csv(filename)  def write_cipher_text(texts, filename=\'spm_train.txt\'):     with open(filename, \'w\',encoding=\'utf-8\') as f:         for text in texts:             f.write(text + ""\ "")  train_df = read_train_text() test_df = read_train_text(filename=\'../input/test.csv\') ciphertexts = list(train_df.ciphertext.values) + list(test_df.ciphertext.values) write_cipher_text(ciphertexts)"	import sentencepiece as spm spm.SentencePieceTrainer.Train(         '--input=' + os.path.join('spm_train.txt') +         ' --model_prefix=train --vocab_size=1000')
import sentencepiece as spm spm.SentencePieceTrainer.Train(         '--input=' + os.path.join('spm_train.txt') +         ' --model_prefix=train --vocab_size=1000')	def encode_ciphertext(ciphertext):     sp = spm.SentencePieceProcessor()     sp.Load('train.model')     encodedtext = []     for text in ciphertext:         encodedtext.append(sp.encode_as_ids(text))     return encodedtext  train_encoded = encode_ciphertext(train_df.ciphertext) test_encoded = encode_ciphertext(test_df.ciphertext)
def encode_ciphertext(ciphertext):     sp = spm.SentencePieceProcessor()     sp.Load('train.model')     encodedtext = []     for text in ciphertext:         encodedtext.append(sp.encode_as_ids(text))     return encodedtext  train_encoded = encode_ciphertext(train_df.ciphertext) test_encoded = encode_ciphertext(test_df.ciphertext)	from collections import defaultdict, Counter  word_counter = defaultdict(int) for text in train_encoded + test_encoded:     counter = Counter(text)     for l,c in counter.items():         word_counter[l] += c
from collections import defaultdict, Counter  word_counter = defaultdict(int) for text in train_encoded + test_encoded:     counter = Counter(text)     for l,c in counter.items():         word_counter[l] += c	from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split	def build_df(word_counter, df, encodedtext):     keys = list(word_counter.keys())         rows = []     for rowid, row in df.iterrows():         counter = Counter(encodedtext[rowid])         entry = [counter.get(k, 0) for k in keys]         entry += [row['difficulty']]         if 'target' in row:             entry += [row['target']]         rows.append(entry)     return pd.DataFrame(rows)  train = build_df(word_counter, train_df, train_encoded) test = build_df(word_counter, test_df, test_encoded)
def build_df(word_counter, df, encodedtext):     keys = list(word_counter.keys())         rows = []     for rowid, row in df.iterrows():         counter = Counter(encodedtext[rowid])         entry = [counter.get(k, 0) for k in keys]         entry += [row['difficulty']]         if 'target' in row:             entry += [row['target']]         rows.append(entry)     return pd.DataFrame(rows)  train = build_df(word_counter, train_df, train_encoded) test = build_df(word_counter, test_df, test_encoded)	X = train.iloc[:, :-1] Y = train.iloc[:, -1] X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
X = train.iloc[:, :-1] Y = train.iloc[:, -1] X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)	rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1) rf.fit(X_train, y_train) y_pred = rf.predict(X_test) acc = np.sum(y_pred == y_test) / len(y_test) print(acc)
rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1) rf.fit(X_train, y_train) y_pred = rf.predict(X_test) acc = np.sum(y_pred == y_test) / len(y_test) print(acc)	rf.fit(X, Y) y_pred = rf.predict(test) submission = pd.DataFrame(test_df.Id, columns=['Id']) submission['Predicted'] = y_pred submission.to_csv('submission.csv', index=False)
import datetime import gc import numpy as np import os import pandas as pd import random  from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from scipy.stats import skew, kurtosis import lightgbm as lgb  import Levenshtein from sklearn.metrics import f1_score from sklearn.model_selection import KFold  from tqdm import tqdm	id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 
id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 	def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))
def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))	print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)
print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)	train.head()
train.head()	N_DIFF = 4 subm = None
N_DIFF = 4 subm = None	"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = KFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 5,         \'num_leaves\': 31,         \'learning_rate\': 0.05,         \'feature_fraction\': 0.85,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 5,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 1.0,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),     }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"
"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = KFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 5,         \'num_leaves\': 31,         \'learning_rate\': 0.05,         \'feature_fraction\': 0.85,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 5,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 1.0,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),     }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"	"subm = subm.set_index(""Id"") sample = pd.read_csv(""../input/sample_submission.csv"").set_index(""Id"") for idx, row in sample.iterrows():     row[""Predicted""] = subm.loc[idx][""Predicted""]  sample.to_csv(""submission.csv"")"
"subm = subm.set_index(""Id"") sample = pd.read_csv(""../input/sample_submission.csv"").set_index(""Id"") for idx, row in sample.iterrows():     row[""Predicted""] = subm.loc[idx][""Predicted""]  sample.to_csv(""submission.csv"")"	NB_END
import datetime import gc import numpy as np import os import pandas as pd import random  from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from scipy.stats import skew, kurtosis import lightgbm as lgb  import Levenshtein from sklearn.metrics import f1_score from sklearn.model_selection import KFold, StratifiedKFold   from tqdm import tqdm	id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 
id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 	def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))
def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))	print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)
print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)	train.head()
train.head()	N_DIFF = 4 subm = None
N_DIFF = 4 subm = None	"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = StratifiedKFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 5,         \'num_leaves\': 36,         \'learning_rate\': 0.028,         \'feature_fraction\': 0.85,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 5,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 1.0,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),         \'colsample_bytree\': 0.88,          \'min_child_samples\': 10,          \'subsample\': 0.54,          \'class_weight\': \'balanced\' }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"
"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = StratifiedKFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 5,         \'num_leaves\': 36,         \'learning_rate\': 0.028,         \'feature_fraction\': 0.85,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 5,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 1.0,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),         \'colsample_bytree\': 0.88,          \'min_child_samples\': 10,          \'subsample\': 0.54,          \'class_weight\': \'balanced\' }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"	"subm = subm.set_index(""Id"") sample = pd.read_csv(""../input/sample_submission.csv"").set_index(""Id"") for idx, row in sample.iterrows():     row[""Predicted""] = subm.loc[idx][""Predicted""]  sample.to_csv(""submission.csv"")"
"subm = subm.set_index(""Id"") sample = pd.read_csv(""../input/sample_submission.csv"").set_index(""Id"") for idx, row in sample.iterrows():     row[""Predicted""] = subm.loc[idx][""Predicted""]  sample.to_csv(""submission.csv"")"	NB_END
import pandas as pd import numpy as np  import matplotlib.pyplot as plt	DATA_PATH = '../input'
DATA_PATH = '../input'	df_train = pd.read_csv(DATA_PATH + '/train.csv', encoding='cp1252')
df_train = pd.read_csv(DATA_PATH + '/train.csv', encoding='cp1252')	df_train.shape
df_train.shape	df_train['ciphertext_len'] = df_train['ciphertext'].apply(lambda x: len([y.encode() for y in x]))
df_train['ciphertext_len'] = df_train['ciphertext'].apply(lambda x: len([y.encode() for y in x]))	df_train.head()
df_train.head()	from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer	%%time vect = TfidfVectorizer(lowercase=False, analyzer='char', ngram_range=(1,5), max_features=30000) X_train_features_sparse = vect.fit_transform(df_train['ciphertext']) X_train_features_sparse
%%time vect = TfidfVectorizer(lowercase=False, analyzer='char', ngram_range=(1,5), max_features=30000) X_train_features_sparse = vect.fit_transform(df_train['ciphertext']) X_train_features_sparse	from scipy.sparse import hstack
from scipy.sparse import hstack	X_train = X_train_features_sparse.tocsr() X_train
X_train = X_train_features_sparse.tocsr() X_train	y_train = df_train['target']
y_train = df_train['target']	df_test = pd.read_csv(DATA_PATH + '/test.csv', encoding='cp1252')
df_test = pd.read_csv(DATA_PATH + '/test.csv', encoding='cp1252')	%%time X_test_features_sparse = vect.transform(df_test['ciphertext'])
%%time X_test_features_sparse = vect.transform(df_test['ciphertext'])	X_test = X_test_features_sparse.tocsr() X_test
X_test = X_test_features_sparse.tocsr() X_test	del(vect)
del(vect)	diffs = list(range(1, 5))
diffs = list(range(1, 5))	from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split	def split_idx_by_column(df, column, valid_size=None):     idxs, idxs_valid = {}, {}     for d in diffs:         idx = df.index[df[column] == d]         if valid_size is None:             idxs[d] = idx         else:             idx, idx_valid = train_test_split(idx, random_state=42,                                                test_size=valid_size, stratify=df['target'][idx])             idxs[d] = idx             idxs_valid[d] = idx_valid     if valid_size is None:         return idxs     else:         return idxs, idxs_valid
def split_idx_by_column(df, column, valid_size=None):     idxs, idxs_valid = {}, {}     for d in diffs:         idx = df.index[df[column] == d]         if valid_size is None:             idxs[d] = idx         else:             idx, idx_valid = train_test_split(idx, random_state=42,                                                test_size=valid_size, stratify=df['target'][idx])             idxs[d] = idx             idxs_valid[d] = idx_valid     if valid_size is None:         return idxs     else:         return idxs, idxs_valid	train_idxs = split_idx_by_column(df_train, 'difficulty') train_part_idxs, valid_idxs = split_idx_by_column(df_train, 'difficulty', valid_size=0.1) test_idxs = split_idx_by_column(df_test, 'difficulty')
train_idxs = split_idx_by_column(df_train, 'difficulty') train_part_idxs, valid_idxs = split_idx_by_column(df_train, 'difficulty', valid_size=0.1) test_idxs = split_idx_by_column(df_test, 'difficulty')	print('train part sizes:', [z.shape[0] for z in train_part_idxs.values()]) print('valid sizes:', [z.shape[0] for z in valid_idxs.values()]) print('test sizes:', [z.shape[0] for z in test_idxs.values()])
print('train part sizes:', [z.shape[0] for z in train_part_idxs.values()]) print('valid sizes:', [z.shape[0] for z in valid_idxs.values()]) print('test sizes:', [z.shape[0] for z in test_idxs.values()])	y_valid_to_concat = [] for d in diffs:     y_valid_to_concat.append(y_train.loc[valid_idxs[d]]) y_valid = pd.concat(y_valid_to_concat) y_valid.sort_index(inplace=True) y_valid.index
y_valid_to_concat = [] for d in diffs:     y_valid_to_concat.append(y_train.loc[valid_idxs[d]]) y_valid = pd.concat(y_valid_to_concat) y_valid.sort_index(inplace=True) y_valid.index	for d in diffs:     plt.figure()     plt.title(f'Difficulty {d}')     idx = train_part_idxs[d].values     plt.hist(y_train[idx], bins=20, normed=False, alpha=0.5)     idx = valid_idxs[d].values     plt.hist(y_train[idx], bins=20, normed=False, alpha=0.5)
for d in diffs:     plt.figure()     plt.title(f'Difficulty {d}')     idx = train_part_idxs[d].values     plt.hist(y_train[idx], bins=20, normed=False, alpha=0.5)     idx = valid_idxs[d].values     plt.hist(y_train[idx], bins=20, normed=False, alpha=0.5)	from sklearn.pipeline import Pipeline from sklearn.preprocessing import MaxAbsScaler
from sklearn.pipeline import Pipeline from sklearn.preprocessing import MaxAbsScaler	from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegression	pipes = {} for d in diffs:     pipe = Pipeline(memory=None, steps=[         ('scaler', MaxAbsScaler(copy=False)),         ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=2, n_jobs=-1))     ])     pipes[d] = pipe
pipes = {} for d in diffs:     pipe = Pipeline(memory=None, steps=[         ('scaler', MaxAbsScaler(copy=False)),         ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=2, n_jobs=-1))     ])     pipes[d] = pipe	def train(models, X, y, diff_idxs):     for d in diffs:         idx = diff_idxs[d].values         print(f'difficulty = {d}, samples = {idx.shape[0]}')         model = models[d]         model.fit(X[idx], y.loc[idx])     return models
def train(models, X, y, diff_idxs):     for d in diffs:         idx = diff_idxs[d].values         print(f'difficulty = {d}, samples = {idx.shape[0]}')         model = models[d]         model.fit(X[idx], y.loc[idx])     return models	%%time train(pipes, X_train, y_train, train_part_idxs)
%%time train(pipes, X_train, y_train, train_part_idxs)	from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix	def predict(models, X, diff_idxs, show_graph=True, y_truth=None):     y_preds = {}     for d in diffs:         idx = diff_idxs[d].values         model = models[d]         y_pred = model.predict(X[idx])         y_preds[d] = pd.Series(data=y_pred, index=idx)         print(f'difficulty = {d}, valid_preds = {y_preds[d].shape}')         if show_graph:             plt.figure(figsize=(12,4))             plt.subplot(121)             plt.title(f'Difficulty {d}')             plt.hist(y_pred, bins=20, normed=False, label='pred', alpha=0.5)             if y_truth is not None:                 plt.hist(y_truth[idx], bins=20, label='valid', alpha=0.5)             plt.gca().set_xticks(range(20))             plt.grid()             plt.legend()             if y_truth is not None:                 cm = confusion_matrix(y_truth[idx], y_pred)                 plt.subplot(122)                 plt.imshow(cm)                 plt.colorbar()                 plt.ylabel('True label')                 plt.xlabel('Predicted label')     y_pred_to_concat = []     for d in diffs:         y_pred_to_concat.append(y_preds[d])     y_pred = pd.concat(y_pred_to_concat)     y_pred.sort_index(inplace=True)     return y_pred
def predict(models, X, diff_idxs, show_graph=True, y_truth=None):     y_preds = {}     for d in diffs:         idx = diff_idxs[d].values         model = models[d]         y_pred = model.predict(X[idx])         y_preds[d] = pd.Series(data=y_pred, index=idx)         print(f'difficulty = {d}, valid_preds = {y_preds[d].shape}')         if show_graph:             plt.figure(figsize=(12,4))             plt.subplot(121)             plt.title(f'Difficulty {d}')             plt.hist(y_pred, bins=20, normed=False, label='pred', alpha=0.5)             if y_truth is not None:                 plt.hist(y_truth[idx], bins=20, label='valid', alpha=0.5)             plt.gca().set_xticks(range(20))             plt.grid()             plt.legend()             if y_truth is not None:                 cm = confusion_matrix(y_truth[idx], y_pred)                 plt.subplot(122)                 plt.imshow(cm)                 plt.colorbar()                 plt.ylabel('True label')                 plt.xlabel('Predicted label')     y_pred_to_concat = []     for d in diffs:         y_pred_to_concat.append(y_preds[d])     y_pred = pd.concat(y_pred_to_concat)     y_pred.sort_index(inplace=True)     return y_pred	y_valid_pred = predict(pipes, X_train, valid_idxs, y_truth=y_valid)
y_valid_pred = predict(pipes, X_train, valid_idxs, y_truth=y_valid)	from sklearn.metrics import f1_score, precision_recall_fscore_support
from sklearn.metrics import f1_score, precision_recall_fscore_support	f1_score(y_valid, y_valid_pred, average='macro')
f1_score(y_valid, y_valid_pred, average='macro')	precision_recall_fscore_support(y_valid, y_valid_pred, average='macro')
precision_recall_fscore_support(y_valid, y_valid_pred, average='macro')	plt.hist(y_valid, bins=20, label='valid', alpha=0.5) plt.hist(y_valid_pred, bins=20, label='valid_pred', alpha=0.5) plt.gca().set_xticks(range(20)) plt.grid() plt.legend() pass
plt.hist(y_valid, bins=20, label='valid', alpha=0.5) plt.hist(y_valid_pred, bins=20, label='valid_pred', alpha=0.5) plt.gca().set_xticks(range(20)) plt.grid() plt.legend() pass	cm = confusion_matrix(y_valid, y_valid_pred) plt.figure() plt.imshow(cm) plt.colorbar() plt.ylabel('True label') plt.xlabel('Predicted label')
cm = confusion_matrix(y_valid, y_valid_pred) plt.figure() plt.imshow(cm) plt.colorbar() plt.ylabel('True label') plt.xlabel('Predicted label')	from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold from sklearn.model_selection import GridSearchCV	cv = StratifiedKFold(2) params = {     'clf__C': np.logspace(-2, 2, 5) } grids = {} for d in diffs:     pipe = pipes[d]     grid = GridSearchCV(estimator=pipe, cv=cv, param_grid=params,                          scoring='f1_macro', return_train_score=True, verbose=2)     grids[d] = grid
cv = StratifiedKFold(2) params = {     'clf__C': np.logspace(-2, 2, 5) } grids = {} for d in diffs:     pipe = pipes[d]     grid = GridSearchCV(estimator=pipe, cv=cv, param_grid=params,                          scoring='f1_macro', return_train_score=True, verbose=2)     grids[d] = grid	%%time train(grids, X_train, y_train, train_idxs)
%%time train(grids, X_train, y_train, train_idxs)	for d in diffs:     print(f'Difficulty = {d}')     print(grids[d].cv_results_)
for d in diffs:     print(f'Difficulty = {d}')     print(grids[d].cv_results_)	models = {} for d in diffs:     model = grids[d].best_estimator_     models[d] = model     print(f'Difficulty = {d}, C={model.steps[1][1].C}')
models = {} for d in diffs:     model = grids[d].best_estimator_     models[d] = model     print(f'Difficulty = {d}, C={model.steps[1][1].C}')	%%time y_test_pred = predict(models, X_test, test_idxs)
%%time y_test_pred = predict(models, X_test, test_idxs)	plt.hist(y_train, bins=20, label='train', alpha=0.5, density=True) plt.hist(y_test_pred, bins=20, label='pred', alpha=0.5, density=True) plt.gca().set_xticks(range(20)) plt.grid() plt.legend() pass
plt.hist(y_train, bins=20, label='train', alpha=0.5, density=True) plt.hist(y_test_pred, bins=20, label='pred', alpha=0.5, density=True) plt.gca().set_xticks(range(20)) plt.grid() plt.legend() pass	df_subm = pd.read_csv(DATA_PATH +'/sample_submission.csv') df_subm['Predicted'] = y_test_pred df_subm.head()
df_subm = pd.read_csv(DATA_PATH +'/sample_submission.csv') df_subm['Predicted'] = y_test_pred df_subm.head()	df_subm.to_csv('submission.csv', index=False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')
data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')	data.head()
data.head()	data['difficulty'].value_counts()
data['difficulty'].value_counts()	data['target'].value_counts().to_frame().T
data['target'].value_counts().to_frame().T	pd.crosstab(data['difficulty'], data['target'])
pd.crosstab(data['difficulty'], data['target'])	data['ciphertext'].apply(len).describe()
data['ciphertext'].apply(len).describe()	from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV	vectorizer = CountVectorizer(     analyzer = 'char',     lowercase = False,     ngram_range=(1, 6))  estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,                           tol=1e-3, n_jobs=-1)
vectorizer = CountVectorizer(     analyzer = 'char',     lowercase = False,     ngram_range=(1, 6))  estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,                           tol=1e-3, n_jobs=-1)	model = Pipeline([('selector',                     FunctionTransformer(                        lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])
model = Pipeline([('selector',                     FunctionTransformer(                        lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])	X = data.drop('target', axis=1) y = data['target']
X = data.drop('target', axis=1) y = data['target']	from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)	model.fit(X_train, y_train)
model.fit(X_train, y_train)	y_pred = model.predict(X_test)
y_pred = model.predict(X_test)	from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.metrics import f1_score, classification_report, confusion_matrix	print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred))	f1_score(y_test, y_pred, average='macro')
f1_score(y_test, y_pred, average='macro')	def get_f1_score(difficulty):     score = f1_score(y_test[X_test['difficulty'] == difficulty],                       y_pred[X_test['difficulty'] == difficulty], average='macro')      return score
def get_f1_score(difficulty):     score = f1_score(y_test[X_test['difficulty'] == difficulty],                       y_pred[X_test['difficulty'] == difficulty], average='macro')      return score	"print(""f1_score per difficulty"") for i in range(1, 5):     print(""Difficulty: {} ==> {:5f}"".format(i, get_f1_score(i)))"
"print(""f1_score per difficulty"") for i in range(1, 5):     print(""Difficulty: {} ==> {:5f}"".format(i, get_f1_score(i)))"	model.fit(X, y)
model.fit(X, y)	test_pred = model.predict(test)
test_pred = model.predict(test)	subm['Predicted'] = test_pred subm.to_csv('submission.csv', index=False)
subm['Predicted'] = test_pred subm.to_csv('submission.csv', index=False)	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')
data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')	data.head()
data.head()	data['difficulty'].value_counts()
data['difficulty'].value_counts()	data['target'].value_counts().to_frame().T
data['target'].value_counts().to_frame().T	pd.crosstab(data['difficulty'], data['target'])
pd.crosstab(data['difficulty'], data['target'])	data['ciphertext'].apply(len).describe()
data['ciphertext'].apply(len).describe()	from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV	vectorizer = CountVectorizer(     analyzer = 'char',     lowercase = False,     ngram_range=(1, 6))  estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,                           tol=1e-3, n_jobs=-1)
vectorizer = CountVectorizer(     analyzer = 'char',     lowercase = False,     ngram_range=(1, 6))  estimator = SGDClassifier(loss='hinge', max_iter=1000, random_state=0,                           tol=1e-3, n_jobs=-1)	model = Pipeline([('selector',                     FunctionTransformer(                        lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])
model = Pipeline([('selector',                     FunctionTransformer(                        lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])	X = data.drop('target', axis=1) y = data['target']
X = data.drop('target', axis=1) y = data['target']	from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)	model.fit(X_train, y_train)
model.fit(X_train, y_train)	y_pred = model.predict(X_test)
y_pred = model.predict(X_test)	from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.metrics import f1_score, classification_report, confusion_matrix	print(classification_report(y_test, y_pred))
print(classification_report(y_test, y_pred))	f1_score(y_test, y_pred, average='macro')
f1_score(y_test, y_pred, average='macro')	def get_f1_score(difficulty):     score = f1_score(y_test[X_test['difficulty'] == difficulty],                       y_pred[X_test['difficulty'] == difficulty], average='macro')      return score
def get_f1_score(difficulty):     score = f1_score(y_test[X_test['difficulty'] == difficulty],                       y_pred[X_test['difficulty'] == difficulty], average='macro')      return score	"print(""f1_score per difficulty"") for i in range(1, 5):     print(""Difficulty: {} ==> {:5f}"".format(i, get_f1_score(i)))"
"print(""f1_score per difficulty"") for i in range(1, 5):     print(""Difficulty: {} ==> {:5f}"".format(i, get_f1_score(i)))"	model.fit(X, y)
model.fit(X, y)	test_pred = model.predict(test)
test_pred = model.predict(test)	subm['Predicted'] = test_pred subm.to_csv('submission.csv', index=False)
subm['Predicted'] = test_pred subm.to_csv('submission.csv', index=False)	NB_END
"import numpy as np import pandas as pd import os import tqdm import time  from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score, f1_score, classification_report  import warnings warnings.filterwarnings(""ignore"")  print(os.listdir(""../input"")) # Any results you write to the current directory are saved as output."	train = pd.read_csv('../input/train.csv') train.head()
train = pd.read_csv('../input/train.csv') train.head()	train.info()
train.info()	train['difficulty'].value_counts()
train['difficulty'].value_counts()	train['target'].value_counts()
train['target'].value_counts()	train = train.drop(['Id'], axis=1) train.head()
train = train.drop(['Id'], axis=1) train.head()	Xtrain, Xtest, ytrain, ytest = train_test_split(train.iloc[:,:2], train['target'], test_size = 0.1, random_state = 0)
Xtrain, Xtest, ytrain, ytest = train_test_split(train.iloc[:,:2], train['target'], test_size = 0.1, random_state = 0)	diff1 = Xtrain[Xtrain['difficulty'] == 1] diff2 = Xtrain[Xtrain['difficulty'] == 2] diff3 = Xtrain[Xtrain['difficulty'] == 3] diff4 = Xtrain[Xtrain['difficulty'] == 4]
diff1 = Xtrain[Xtrain['difficulty'] == 1] diff2 = Xtrain[Xtrain['difficulty'] == 2] diff3 = Xtrain[Xtrain['difficulty'] == 3] diff4 = Xtrain[Xtrain['difficulty'] == 4]	diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff1.head()
diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff1.head()	diff_test1 = Xtest[Xtest['difficulty'] == 1] diff_test2 = Xtest[Xtest['difficulty'] == 2] diff_test3 = Xtest[Xtest['difficulty'] == 3] diff_test4 = Xtest[Xtest['difficulty'] == 4]  diff_test1['ciphertext'] = diff_test1['ciphertext'].apply(lambda x: x.replace('1', ' ')).fillna(0) diff_test2['ciphertext'] = diff_test2['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff_test3['ciphertext'] = diff_test3['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff_test4['ciphertext'] = diff_test4['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff1.head()
diff_test1 = Xtest[Xtest['difficulty'] == 1] diff_test2 = Xtest[Xtest['difficulty'] == 2] diff_test3 = Xtest[Xtest['difficulty'] == 3] diff_test4 = Xtest[Xtest['difficulty'] == 4]  diff_test1['ciphertext'] = diff_test1['ciphertext'].apply(lambda x: x.replace('1', ' ')).fillna(0) diff_test2['ciphertext'] = diff_test2['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff_test3['ciphertext'] = diff_test3['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff_test4['ciphertext'] = diff_test4['ciphertext'].apply(lambda x: x.replace('8', ' ')).fillna(0) diff1.head()	start = time.time() vect1 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect1 = vect1.fit_transform(diff1['ciphertext']) test_vect1 = vect1.transform(diff_test1['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect2 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect2 = vect2.fit_transform(diff2['ciphertext']) test_vect2 = vect2.transform(diff_test2['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect3 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6), max_features = 660000) train_vect3 = vect3.fit_transform(diff3['ciphertext']) test_vect3 = vect3.transform(diff_test3['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect4 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6), max_features = 660000) train_vect4 = vect4.fit_transform(diff4['ciphertext']) test_vect4 = vect4.transform(diff_test4['ciphertext']) print('Time: ' + str(time.time() - start) + 's')
start = time.time() vect1 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect1 = vect1.fit_transform(diff1['ciphertext']) test_vect1 = vect1.transform(diff_test1['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect2 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect2 = vect2.fit_transform(diff2['ciphertext']) test_vect2 = vect2.transform(diff_test2['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect3 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6), max_features = 660000) train_vect3 = vect3.fit_transform(diff3['ciphertext']) test_vect3 = vect3.transform(diff_test3['ciphertext']) print('Time: ' + str(time.time() - start) + 's')  start = time.time() vect4 = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6), max_features = 660000) train_vect4 = vect4.fit_transform(diff4['ciphertext']) test_vect4 = vect4.transform(diff_test4['ciphertext']) print('Time: ' + str(time.time() - start) + 's')	model1 = LogisticRegression(tol=0.001, C=13.0, random_state=34, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model1.fit(train_vect1, ytrain.loc[diff1.index])  model2 = LogisticRegression(tol=0.001, C=59.0, random_state=29, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model2.fit(train_vect2, ytrain.loc[diff2.index])  model3 = LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model3.fit(train_vect3, ytrain.loc[diff3.index])  model4 = LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model4.fit(train_vect4, ytrain.loc[diff4.index])
model1 = LogisticRegression(tol=0.001, C=13.0, random_state=34, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model1.fit(train_vect1, ytrain.loc[diff1.index])  model2 = LogisticRegression(tol=0.001, C=59.0, random_state=29, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model2.fit(train_vect2, ytrain.loc[diff2.index])  model3 = LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model3.fit(train_vect3, ytrain.loc[diff3.index])  model4 = LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=100, multi_class='auto', verbose=1, n_jobs=-1) model4.fit(train_vect4, ytrain.loc[diff4.index])	pred1 = model1.predict(test_vect1) pred2 = model2.predict(test_vect2) pred3 = model3.predict(test_vect3) pred4 = model4.predict(test_vect4)
pred1 = model1.predict(test_vect1) pred2 = model2.predict(test_vect2) pred3 = model3.predict(test_vect3) pred4 = model4.predict(test_vect4)	print(accuracy_score(pred1, ytest.loc[diff_test1.index])) print(accuracy_score(pred2, ytest.loc[diff_test2.index])) print(accuracy_score(pred3, ytest.loc[diff_test3.index])) print(accuracy_score(pred4, ytest.loc[diff_test4.index]))
print(accuracy_score(pred1, ytest.loc[diff_test1.index])) print(accuracy_score(pred2, ytest.loc[diff_test2.index])) print(accuracy_score(pred3, ytest.loc[diff_test3.index])) print(accuracy_score(pred4, ytest.loc[diff_test4.index]))	print(f1_score(pred1, ytest.loc[diff_test1.index], average='macro')) # 0.6561271380779565 print(f1_score(pred2, ytest.loc[diff_test2.index], average='macro')) # 0.6593521513806591  print(f1_score(pred3, ytest.loc[diff_test3.index], average='macro')) # 0.4219906210294547 print(f1_score(pred4, ytest.loc[diff_test4.index], average='macro'))
print(f1_score(pred1, ytest.loc[diff_test1.index], average='macro')) # 0.6561271380779565 print(f1_score(pred2, ytest.loc[diff_test2.index], average='macro')) # 0.6593521513806591  print(f1_score(pred3, ytest.loc[diff_test3.index], average='macro')) # 0.4219906210294547 print(f1_score(pred4, ytest.loc[diff_test4.index], average='macro'))	test1 = pd.read_csv('../input/test.csv') test = test1.copy() test.head()
test1 = pd.read_csv('../input/test.csv') test = test1.copy() test.head()	test_diff1 = test[test['difficulty'] == 1] test_diff2 = test[test['difficulty'] == 2] test_diff3 = test[test['difficulty'] == 3] test_diff4 = test[test['difficulty'] == 4]
test_diff1 = test[test['difficulty'] == 1] test_diff2 = test[test['difficulty'] == 2] test_diff3 = test[test['difficulty'] == 3] test_diff4 = test[test['difficulty'] == 4]	test_diff1['ciphertext'] = test_diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) test_diff2['ciphertext'] = test_diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff3['ciphertext'] = test_diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff4['ciphertext'] = test_diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))
test_diff1['ciphertext'] = test_diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) test_diff2['ciphertext'] = test_diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff3['ciphertext'] = test_diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff4['ciphertext'] = test_diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))	start = time.time() test_vect_1 = vect1.transform(test_diff1['ciphertext']) test_vect_2 = vect2.transform(test_diff2['ciphertext']) test_vect_3 = vect3.transform(test_diff3['ciphertext']) test_vect_4 = vect4.transform(test_diff4['ciphertext']) print('Time taken: ' + str(time.time() - start))
start = time.time() test_vect_1 = vect1.transform(test_diff1['ciphertext']) test_vect_2 = vect2.transform(test_diff2['ciphertext']) test_vect_3 = vect3.transform(test_diff3['ciphertext']) test_vect_4 = vect4.transform(test_diff4['ciphertext']) print('Time taken: ' + str(time.time() - start))	test_pred1 = model1.predict(test_vect_1) test_pred2 = model2.predict(test_vect_2) test_pred3 = model3.predict(test_vect_3) test_pred4 = model4.predict(test_vect_4)
test_pred1 = model1.predict(test_vect_1) test_pred2 = model2.predict(test_vect_2) test_pred3 = model3.predict(test_vect_3) test_pred4 = model4.predict(test_vect_4)	test_diff1['pred'] = test_pred1 test_diff2['pred'] = test_pred2 test_diff3['pred'] = test_pred3 test_diff4['pred'] = test_pred4
test_diff1['pred'] = test_pred1 test_diff2['pred'] = test_pred2 test_diff3['pred'] = test_pred3 test_diff4['pred'] = test_pred4	test_diff1.head()
test_diff1.head()	test_diff = pd.concat([test_diff1, test_diff2, test_diff3, test_diff4])
test_diff = pd.concat([test_diff1, test_diff2, test_diff3, test_diff4])	test_diff = test_diff.set_index('Id').loc[test1['Id']]
test_diff = test_diff.set_index('Id').loc[test1['Id']]	test_diff = test_diff.drop(['difficulty', 'ciphertext'], axis=1) test_diff = test_diff.reset_index()
test_diff = test_diff.drop(['difficulty', 'ciphertext'], axis=1) test_diff = test_diff.reset_index()	test_diff.columns = ['Id', 'Predicted']
test_diff.columns = ['Id', 'Predicted']	test_diff.to_csv('submission.csv', index=False)
test_diff.to_csv('submission.csv', index=False)	NB_END
import numpy as np import pandas as pd from sklearn import * from collections import Counter  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') sub = pd.read_csv('../input/sample_submission.csv').rename(columns={'Predicted': 'Predicted2'}) otrain = datasets.fetch_20newsgroups(subset='train') otrain = pd.DataFrame({'Id': otrain['filenames'], 'text': otrain['data'], 'target': otrain['target']}) otrain['Id'] = otrain['Id'].map(lambda x: x.split('/')[-1]) otest = datasets.fetch_20newsgroups(subset='test') otest = pd.DataFrame({'Id': otest['filenames'], 'text': otest['data'], 'target': otest['target']}) otest['Id'] = otest['Id'].map(lambda x: x.split('/')[-1]) train.shape, otrain.shape, test.shape, otest.shape	otrain_ = [] for i in range(len(otrain)):     t = str(otrain['text'][i]).replace('\ ', '\  ')     for j in range(0, len(t), 300):         otrain_.append([otrain['Id'][i] + '_' + str(int(j/300)).zfill(3), t[j: j+300], otrain['target'][i]]) otrain = pd.DataFrame(otrain_, columns=['Id', 'text', 'target']) otest_ = [] for i in range(len(otest)):     t = str(otest['text'][i]).replace('\ ', '\  ')     for j in range(0, len(t), 300):         otest_.append([otest['Id'][i] + '_' + str(int(j/300)).zfill(3), t[j: j+300], otest['target'][i]]) otest = pd.DataFrame(otest_, columns=['Id', 'text', 'target']) otrain = pd.concat((otrain, otest)).reset_index(drop=True) len(otrain), len(train) + len(test)
otrain_ = [] for i in range(len(otrain)):     t = str(otrain['text'][i]).replace('\ ', '\  ')     for j in range(0, len(t), 300):         otrain_.append([otrain['Id'][i] + '_' + str(int(j/300)).zfill(3), t[j: j+300], otrain['target'][i]]) otrain = pd.DataFrame(otrain_, columns=['Id', 'text', 'target']) otest_ = [] for i in range(len(otest)):     t = str(otest['text'][i]).replace('\ ', '\  ')     for j in range(0, len(t), 300):         otest_.append([otest['Id'][i] + '_' + str(int(j/300)).zfill(3), t[j: j+300], otest['target'][i]]) otest = pd.DataFrame(otest_, columns=['Id', 'text', 'target']) otrain = pd.concat((otrain, otest)).reset_index(drop=True) len(otrain), len(train) + len(test)	train_d = {} pattern = {} train_d[0] = Counter(' '.join(otrain['text'].astype(str).values)) pattern[0] = ''.join([c for c, v in train_d[0].most_common(100)]) print(0, repr(pattern[0]))  for i in range(1,5):     train_d[i] = train[train['difficulty']==i].copy()     train_d[i] = Counter(' '.join(train_d[i]['ciphertext'].astype(str).values))     pattern[i] = ''.join([c for c, v in train_d[i].most_common(100)])     print(i, repr(pattern[i]))
train_d = {} pattern = {} train_d[0] = Counter(' '.join(otrain['text'].astype(str).values)) pattern[0] = ''.join([c for c, v in train_d[0].most_common(100)]) print(0, repr(pattern[0]))  for i in range(1,5):     train_d[i] = train[train['difficulty']==i].copy()     train_d[i] = Counter(' '.join(train_d[i]['ciphertext'].astype(str).values))     pattern[i] = ''.join([c for c, v in train_d[i].most_common(100)])     print(i, repr(pattern[i]))	def SubEnc(s, level):     s = str(s)     for i in range(level,0, -1):         s1 = pattern[i]         s2 = pattern[i-1]         SubEnc_ = str.maketrans(s1, s2)         s = s.translate(SubEnc_)     return s
def SubEnc(s, level):     s = str(s)     for i in range(level,0, -1):         s1 = pattern[i]         s2 = pattern[i-1]         SubEnc_ = str.maketrans(s1, s2)         s = s.translate(SubEnc_)     return s	for i in range(1,5):     train['ciphertext'] = train.apply(lambda r: SubEnc(r['ciphertext'], i) if r['difficulty'] == i else r['ciphertext'], axis=1)     test['ciphertext'] = test.apply(lambda r: SubEnc(r['ciphertext'], i) if r['difficulty'] == i else r['ciphertext'], axis=1)
for i in range(1,5):     train['ciphertext'] = train.apply(lambda r: SubEnc(r['ciphertext'], i) if r['difficulty'] == i else r['ciphertext'], axis=1)     test['ciphertext'] = test.apply(lambda r: SubEnc(r['ciphertext'], i) if r['difficulty'] == i else r['ciphertext'], axis=1)	train_d1 = train[train['difficulty']==1].copy() train_d1['len'] = train_d1['ciphertext'].map(len) train_d1[train_d1['len']==224].head()
train_d1 = train[train['difficulty']==1].copy() train_d1['len'] = train_d1['ciphertext'].map(len) train_d1[train_d1['len']==224].head()	otrain['len'] = otrain['text'].map(len) otrain[((otrain['text'].str.contains('not provided a service for the co')) & (otrain['target']==18))].head()
otrain['len'] = otrain['text'].map(len) otrain[((otrain['text'].str.contains('not provided a service for the co')) & (otrain['target']==18))].head()	#Use a word dictionary to fine tune the mappings spelling_dict = Counter(' '.join(otrain['text'].astype(str).values).split(' ')) #[w for w in train_d1[train_d1['len']==224]['ciphertext'].values[0].split(' ') if w not in spelling_dict] #set([w for w in otrain[((otrain['text'].str.contains('not provided a service for the co')) & (otrain['target']==18))]['text'].values[0].split(' ')])
#Use a word dictionary to fine tune the mappings spelling_dict = Counter(' '.join(otrain['text'].astype(str).values).split(' ')) #[w for w in train_d1[train_d1['len']==224]['ciphertext'].values[0].split(' ') if w not in spelling_dict] #set([w for w in otrain[((otrain['text'].str.contains('not provided a service for the co')) & (otrain['target']==18))]['text'].values[0].split(' ')])	results = [] for d in range(1,5):     train_d = train[train['difficulty']==d].reset_index(drop=True)     test_d = test[test['difficulty']==d].reset_index(drop=True)     tfidf = feature_extraction.text.TfidfVectorizer(analyzer = 'char_wb', ngram_range=(1, 7), lowercase=False)      tfidf.fit(pd.concat((train_d['ciphertext'], test_d['ciphertext'])))     trainf = tfidf.transform(train_d['ciphertext'])     testf = tfidf.transform(test_d['ciphertext'])     clf = linear_model.LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=90, multi_class='auto', n_jobs=-1)     clf.fit(trainf, train_d['target'])     print(d, metrics.accuracy_score(train_d['target'], clf.predict(trainf)))     test_d['Predicted'] = clf.predict(testf)     results.append(test_d)
results = [] for d in range(1,5):     train_d = train[train['difficulty']==d].reset_index(drop=True)     test_d = test[test['difficulty']==d].reset_index(drop=True)     tfidf = feature_extraction.text.TfidfVectorizer(analyzer = 'char_wb', ngram_range=(1, 7), lowercase=False)      tfidf.fit(pd.concat((train_d['ciphertext'], test_d['ciphertext'])))     trainf = tfidf.transform(train_d['ciphertext'])     testf = tfidf.transform(test_d['ciphertext'])     clf = linear_model.LogisticRegression(tol=0.001, C=10.0, random_state=0, solver='sag', max_iter=90, multi_class='auto', n_jobs=-1)     clf.fit(trainf, train_d['target'])     print(d, metrics.accuracy_score(train_d['target'], clf.predict(trainf)))     test_d['Predicted'] = clf.predict(testf)     results.append(test_d)	test = pd.concat(results) sub = pd.merge(sub,test, how='left', on=['Id']) sub[['Id','Predicted']].to_csv('submission.csv', index=False)
test = pd.concat(results) sub = pd.merge(sub,test, how='left', on=['Id']) sub[['Id','Predicted']].to_csv('submission.csv', index=False)	for i in range(1,5):     print(i, repr(SubEnc('V8g{9827\\\x0c$A${?^*?}$$v7\x10*.yig$w9.8}', i)))
for i in range(1,5):     print(i, repr(SubEnc('V8g{9827\\\x0c$A${?^*?}$$v7\x10*.yig$w9.8}', i)))	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input/""))  # Any results you write to the current directory are saved as output."	train_data = pd.read_csv('../input/train.csv', delimiter=',') train_data.head()
train_data = pd.read_csv('../input/train.csv', delimiter=',') train_data.head()	X = train_data[['Id', 'ciphertext']] y = train_data['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)  #I use these samples for testing the steps without waiting a long time sample_size = 1000 X_sample = X_train.iloc[0:sample_size,0:2] #rows, columns y_sample = y_train.iloc[0:sample_size] #rows, columns
X = train_data[['Id', 'ciphertext']] y = train_data['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)  #I use these samples for testing the steps without waiting a long time sample_size = 1000 X_sample = X_train.iloc[0:sample_size,0:2] #rows, columns y_sample = y_train.iloc[0:sample_size] #rows, columns	from nltk.tokenize import word_tokenize from nltk.corpus import stopwords  import nltk  def Tokenizer(str_input):     str_input = str_input.lower()     words = word_tokenize(str_input)     #remove stopwords     stop_words = set(stopwords.words('english'))     words = [w for w in words if not w in stop_words]     #stem the words     porter_stemmer=nltk.PorterStemmer()     words = [porter_stemmer.stem(word) for word in words]     return words
from nltk.tokenize import word_tokenize from nltk.corpus import stopwords  import nltk  def Tokenizer(str_input):     str_input = str_input.lower()     words = word_tokenize(str_input)     #remove stopwords     stop_words = set(stopwords.words('english'))     words = [w for w in words if not w in stop_words]     #stem the words     porter_stemmer=nltk.PorterStemmer()     words = [porter_stemmer.stem(word) for word in words]     return words	from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from xgboost import XGBClassifier  text_clf = Pipeline([     ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, max_df=0.3, min_df=0.001, max_features=100000)),     #('svd',   TruncatedSVD(algorithm='randomized', n_components=500)),     ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8, eval_metric='merror')), ])
from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD from xgboost import XGBClassifier  text_clf = Pipeline([     ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, max_df=0.3, min_df=0.001, max_features=100000)),     #('svd',   TruncatedSVD(algorithm='randomized', n_components=500)),     ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8, eval_metric='merror')), ])	"from sklearn.model_selection import GridSearchCV  parameters = {     #\'tfidf__max_df\': (0.25, 0.5, 0.75),     #\'tfidf__min_df\': (0.001, 0.0025, 0.005),     #\'tfidf__max_features\': (50000, 100000, 150000),     #\'tfidf__ngram_range\': ((1, 1), (1, 2)),  # unigrams or bigrams     #\'tfidf__use_idf\': (True, False),     # \'tfidf__norm\': (\'l1\', \'l2\'),     #\'svd__n_components\': (250, 500, 750),     #\'clf__n_estimators\': (250, 500, 750),     \'clf__max_depth\': (4, 6, 8),     \'clf__min_child_weight\': (1, 5, 10),     #\'clf__alpha\': (0.00001, 0.000001),     #\'clf__penalty\': (\'l2\', \'elasticnet\'),     #\'clf__max_iter\': (10, 50, 80), }  #gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1) #gs_clf.fit(X_sample.message, y_sample)  #print(""Best score: %0.3f"" % gs_clf.best_score_) #print(""Best parameters set:"") #best_parameters = gs_clf.best_estimator_.get_params() #for param_name in sorted(parameters.keys()): #    print(""\ %s: %r"" % (param_name, best_parameters[param_name]))"
"from sklearn.model_selection import GridSearchCV  parameters = {     #\'tfidf__max_df\': (0.25, 0.5, 0.75),     #\'tfidf__min_df\': (0.001, 0.0025, 0.005),     #\'tfidf__max_features\': (50000, 100000, 150000),     #\'tfidf__ngram_range\': ((1, 1), (1, 2)),  # unigrams or bigrams     #\'tfidf__use_idf\': (True, False),     # \'tfidf__norm\': (\'l1\', \'l2\'),     #\'svd__n_components\': (250, 500, 750),     #\'clf__n_estimators\': (250, 500, 750),     \'clf__max_depth\': (4, 6, 8),     \'clf__min_child_weight\': (1, 5, 10),     #\'clf__alpha\': (0.00001, 0.000001),     #\'clf__penalty\': (\'l2\', \'elasticnet\'),     #\'clf__max_iter\': (10, 50, 80), }  #gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1) #gs_clf.fit(X_sample.message, y_sample)  #print(""Best score: %0.3f"" % gs_clf.best_score_) #print(""Best parameters set:"") #best_parameters = gs_clf.best_estimator_.get_params() #for param_name in sorted(parameters.keys()): #    print(""\ %s: %r"" % (param_name, best_parameters[param_name]))"	"text_clf.fit(X_train.ciphertext, y_train) predictions = text_clf.predict(X_test.ciphertext) print(""The training predictions are ready"")"
"text_clf.fit(X_train.ciphertext, y_train) predictions = text_clf.predict(X_test.ciphertext) print(""The training predictions are ready"")"	from sklearn.feature_extraction.text import TfidfVectorizer from nltk.tokenize import word_tokenize  vectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3) X_tfidf = vectorizer.fit_transform(X.ciphertext) #vectorizer.get_feature_names() X_tfidf.shape
from sklearn.feature_extraction.text import TfidfVectorizer from nltk.tokenize import word_tokenize  vectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3) X_tfidf = vectorizer.fit_transform(X.ciphertext) #vectorizer.get_feature_names() X_tfidf.shape	from sklearn.decomposition import TruncatedSVD  svd_transformer = TruncatedSVD(algorithm='randomized', n_components=300) X_svd = svd_transformer.fit_transform(X_tfidf) X_svd.shape
from sklearn.decomposition import TruncatedSVD  svd_transformer = TruncatedSVD(algorithm='randomized', n_components=300) X_svd = svd_transformer.fit_transform(X_tfidf) X_svd.shape	"from xgboost import XGBClassifier  xgb_classifier = XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8) xgb_classifier.fit(X_svd, y) print(""The model is ready."")"
"from xgboost import XGBClassifier  xgb_classifier = XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8) xgb_classifier.fit(X_svd, y) print(""The model is ready."")"	X_test = pd.read_csv('../input/classifying-20-newsgroups-test/test.csv', delimiter=',') X_test.head()
X_test = pd.read_csv('../input/classifying-20-newsgroups-test/test.csv', delimiter=',') X_test.head()	X_test_tfidf = vectorizer.transform(X_test.ciphertext) X_test_svd = svd_transformer.transform(X_test_tfidf)  xgb_predictions = xgb_classifier.predict(X_test_svd) predictions = xgb_predictions predictions[0:10]
X_test_tfidf = vectorizer.transform(X_test.ciphertext) X_test_svd = svd_transformer.transform(X_test_tfidf)  xgb_predictions = xgb_classifier.predict(X_test_svd) predictions = xgb_predictions predictions[0:10]	"from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix  print(""Accuracy:"", accuracy_score(y_test, predictions)) print(""Precision:"", precision_score(y_test, predictions, average=\'weighted\')) print(classification_report(y_test, predictions)) print(confusion_matrix(y_test, predictions))"
"from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix  print(""Accuracy:"", accuracy_score(y_test, predictions)) print(""Precision:"", precision_score(y_test, predictions, average=\'weighted\')) print(classification_report(y_test, predictions)) print(confusion_matrix(y_test, predictions))"	"output = X_test.copy() output.insert(2, \'target\', predictions) output.to_csv(\'submission.csv\', sep=\',\', columns=[\'id\', \'topic\'], index=False) print(os.listdir(""../working"")) output.iloc[1000:5010, :]"
"output = X_test.copy() output.insert(2, \'target\', predictions) output.to_csv(\'submission.csv\', sep=\',\', columns=[\'id\', \'topic\'], index=False) print(os.listdir(""../working"")) output.iloc[1000:5010, :]"	results = pd.read_csv('submission.csv', delimiter=',') results.iloc[5000:5010, :]
results = pd.read_csv('submission.csv', delimiter=',') results.iloc[5000:5010, :]	NB_END
"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import os import tqdm import time  from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score, f1_score import sklearn.svm  import warnings warnings.filterwarnings(""ignore"")  print(os.listdir(""../input""))  # source Cipher #1 & Cipher #2 Full Solutions (https://www.kaggle.com/leflal/cipher-1-cipher-2-full-solutions) dic1 = {\'1\': \' \', \'\\x1b\': \'e\', \'t\': \'t\', \'v\': \'s\', \'#\': \'r\', \'0\': \'h\', \'8\': \'l\', \'s\': \'\ \', \'A\': \'d\', \'^\': \'o\', \']\': \'f\', \'O\': \'a\', \'\\x02\': \'n\', \'o\': \'y\', \'c\': \'u\', \'_\': \'c\', \'{\': \'T\', \'\\x03\': \'b\', \'z\': \'v\', \'a\': \'i\', \'W\': \'w\', \'-\': \'m\', \'*\': \'F\', \'G\': \':\', \';\': ""\'"", \'f\': \'k\', \'F\': \'L\', ""\'"": \'p\', \'d\': \'g\', \'>\': \'S\', \'X\': \'j\', \'\\x1a\': \'q\', \'w\': \'K\', \'2\': \'C\', \':\': \'I\', \'9\': \'P\', \'@\': \'U\', \'x\': \'D\', \'+\': \'x\', \'T\': \',\', \'%\': \'O\', \'\\x08\': \'.\', \'q\': \'-\', \'\\x1e\': \'R\', \'h\': \'z\', \'!\': \'X\', \'\\x7f\': \'N\', \'/\': \'A\', \'b\': \'@\', \'}\': \'J\', \'J\': \'B\', \'e\': \'M\', \'""\': \'G\', \'|\': \'(\', \'y\': \')\', \'g\': \'H\', \'u\': \'3\', \'\\x06\': \'7\', \'\ \': \'5\', \',\': \'4\', \'L\': \'1\', \'\\\\\': \'0\', \'n\': \'8\', \'[\': \'>\', \' \': \'<\', \'r\': \'&\', \'l\': \'W\', \'\\x18\': \'V\', \'U\': \'[\', \'i\': \']\', \'3\': \';\', \'~\': \'+\', \'<\': \'9\', \'H\': \'2\', \'5\': \'6\', \'?\': \'/\', \'4\': \'|\', \'Z\': \'=\', \'.\': \'~\', \'m\': \'\\\\\', \'\\x10\': \'Y\', \')\': \'_\', \'S\': \'\\x08\', \'6\': \'""\', \'&\': \'?\', \'\\x1c\': \'*\', \'Q\': \'\ \', \'I\': \'}\', \'`\': \'#\', \'B\': \'$\', \'P\': \'!\', \'k\': \'{\', \'Y\': \'`\', \'V\': \'Q\', \'D\': \'\\x0c\', \'=\': \'\\x10\', \'$\': \'\\x02\', \'K\': \'Z\', \'p\': \'\\x1e\', \'(\': \'^\', \'\\x0c\': \'%\', \'E\': \'E\'} dic2 = {\'8\': \' \', \'z\': \'\ \', \'$\': \'e\', \'{\': \'t\', \'7\': \'h\', \'e\': \'o\', \'d\': \'f\', \'V\': \'a\', \'\\x10\': \'n\', \'H\': \'d\', \'*\': \'r\', \'\\x03\': \'v\', \'^\': \'w\', \'h\': \'i\', \'}\': \'s\', \'v\': \'y\', \'?\': \'l\', \'j\': \'u\', \'1\': \'F\', \'4\': \'m\', \'N\': \':\', \'\\x18\': \'b\', \'f\': \'c\', \'B\': ""\'"", \'m\': \'k\', \'M\': \'L\', \'.\': \'p\', \'k\': \'g\', \'E\': \'S\', \'_\': \'j\', \'9\': \'C\', \'#\': \'q\', \'n\': \'H\', \'[\': \',\', \'&\': \'R\', \'x\': \'-\', \'\\x06\': \'T\', \'~\': \'K\', \'A\': \'I\', \'G\': \'U\', \'\\x7f\': \'D\', \'L\': \'E\', \',\': \'O\', \'o\': \'z\', \'6\': \'A\', ""\'"": \'<\', \'P\': \'}\', \'Q\': \'B\', \'=\': \'""\', \']\': \'Q\', \')\': \'G\', \'\\x19\': \'7\', \'\\x1b\': \'5\', \'\\\\\': \'[\', \'F\': \'/\', \'r\': \'{\', \'/\': \'^\', \'5\': \'~\', \'\ \': \'+\', \'I\': \'$\', \'y\': \'&\', \'!\': \'V\', \'g\': \'#\', \'s\': \'W\', \';\': \'|\', \'i\': \'@\', \'\\x1a\': \'.\', \'\\x08\': \'(\', \'l\': \'M\', \'\\x02\': \')\', \' \': \'Y\', \'(\': \'X\', \'c\': \'0\', \'2\': \'x\', \'W\': \'!\', \'-\': \'?\', \'\ \': \'J\', \'3\': \'4\', \'<\': \'6\', \'\\x0c\': \'N\', \'@\': \'P\', \'S\': \'1\', \'O\': \'2\', \'C\': \'9\', \'u\': \'8\', \'|\': \'3\', \'\\x1e\': \'%\', \'b\': \'>\', \'X\': \'\ \', \'0\': \'_\', \'>\': \'\\x03\', \'J\': \'\\x1a\', \'%\': \'*\', \'Z\': \'\\x08\', \'Y\': \'\\x06\', \'""\': \'\\x1b\', \'t\': \'\\\\\', \'a\': \'=\', \'R\': \'Z\', \'p\': \']\', \':\': \';\', \'T\': \'\\x7f\', \'K\': \'\\x0c\', \'q\': \'\\x18\', \'\\x1c\': \'\\x19\', \'`\': \'`\', \'+\': \'\\x02\'}"	def decrypt(str,dict):     out=''     for i in range(0,len(str)):         transit = str[i]         out += dict[transit]     return out  train = pd.read_csv('../input/train.csv') # add column for clear text train['clear']='' 
def decrypt(str,dict):     out=''     for i in range(0,len(str)):         transit = str[i]         out += dict[transit]     return out  train = pd.read_csv('../input/train.csv') # add column for clear text train['clear']='' 	# Training set Decrypt level 1 for j in range(0,len(train)):     if train.iloc[j,1] == 1:         scrambled = train.iloc[j,2]         clear = decrypt(scrambled,dic1)         train.iloc[j,4] = clear  # Training set Decrypt level 2 for j in range(0,len(train)):     if train.iloc[j,1] == 2:         scrambled = train.iloc[j,2]         clear = decrypt(scrambled,dic2)         train.iloc[j,4] = clear
# Training set Decrypt level 1 for j in range(0,len(train)):     if train.iloc[j,1] == 1:         scrambled = train.iloc[j,2]         clear = decrypt(scrambled,dic1)         train.iloc[j,4] = clear  # Training set Decrypt level 2 for j in range(0,len(train)):     if train.iloc[j,1] == 2:         scrambled = train.iloc[j,2]         clear = decrypt(scrambled,dic2)         train.iloc[j,4] = clear	# update the files... for i in range(0,len(train)):     if train.iloc[i,4] != '':         train.iloc[i,2] = train.iloc[i,4]  train.drop('clear', axis=1, inplace=True)
# update the files... for i in range(0,len(train)):     if train.iloc[i,4] != '':         train.iloc[i,2] = train.iloc[i,4]  train.drop('clear', axis=1, inplace=True)	diff1 = train[train['difficulty'] == 1] diff2 = train[train['difficulty'] == 2] diff3 = train[train['difficulty'] == 3] diff4 = train[train['difficulty'] == 4]
diff1 = train[train['difficulty'] == 1] diff2 = train[train['difficulty'] == 2] diff3 = train[train['difficulty'] == 3] diff4 = train[train['difficulty'] == 4]	diff1 = diff1.drop(['Id','difficulty'], axis = 1) diff2 = diff2.drop(['Id','difficulty'], axis = 1) diff3 = diff3.drop(['Id','difficulty'], axis = 1) diff4 = diff4.drop(['Id','difficulty'], axis = 1) diff1.head()
diff1 = diff1.drop(['Id','difficulty'], axis = 1) diff2 = diff2.drop(['Id','difficulty'], axis = 1) diff3 = diff3.drop(['Id','difficulty'], axis = 1) diff4 = diff4.drop(['Id','difficulty'], axis = 1) diff1.head()	# train = pd.concat([train, pd.get_dummies(train['difficulty'])], axis=1) # train = train.drop(['Id', 'difficulty'], axis = 1)
# train = pd.concat([train, pd.get_dummies(train['difficulty'])], axis=1) # train = train.drop(['Id', 'difficulty'], axis = 1)	# train = train.iloc[:,[0,2,3,4,5,1]]
# train = train.iloc[:,[0,2,3,4,5,1]]	# train.columns = ['ciphertext', '1d', '2d', '3d', '4d', 'target'] # train.head()
# train.columns = ['ciphertext', '1d', '2d', '3d', '4d', 'target'] # train.head()	diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))
diff1['ciphertext'] = diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) diff2['ciphertext'] = diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff3['ciphertext'] = diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) diff4['ciphertext'] = diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))	train_diff = pd.concat([diff1, diff2, diff3, diff4])
train_diff = pd.concat([diff1, diff2, diff3, diff4])	Xtrain, Xtest, ytrain, ytest = train_test_split(train_diff.iloc[:,:1], train_diff['target'], test_size = 0.1, random_state = 0)
Xtrain, Xtest, ytrain, ytest = train_test_split(train_diff.iloc[:,:1], train_diff['target'], test_size = 0.1, random_state = 0)	start = time.time() vect = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect = vect.fit_transform(Xtrain['ciphertext']) test_vect = vect.transform(Xtest['ciphertext']) print('Time: ' + str(time.time() - start) + 's')
start = time.time() vect = TfidfVectorizer(analyzer = 'char_wb', lowercase = False, ngram_range=(1, 6)) train_vect = vect.fit_transform(Xtrain['ciphertext']) test_vect = vect.transform(Xtest['ciphertext']) print('Time: ' + str(time.time() - start) + 's')	start = time.time() model = sklearn.svm.LinearSVC() model.fit(train_vect, ytrain) print('Time: ' + str(time.time() - start) + 's')
start = time.time() model = sklearn.svm.LinearSVC() model.fit(train_vect, ytrain) print('Time: ' + str(time.time() - start) + 's')	pred = model.predict(test_vect)
pred = model.predict(test_vect)	accuracy_score(pred, ytest)
accuracy_score(pred, ytest)	f1_score(pred, ytest, average= 'macro')
f1_score(pred, ytest, average= 'macro')	test1 = pd.read_csv('../input/test.csv') # add column for clear text test1['clear']=''  # treat case of line 8151,38126,38130,42245,43762 test1.iloc[8151,1] = 2 test1.iloc[38130,1] = 2 test1.iloc[42245,1] = 3 test1.iloc[43762,1] = 2  # Test set Decrypt level 1 for j in range(0,len(test1)):     if test1.iloc[j,1] == 1:         scrambled = test1.iloc[j,2]         clear = decrypt(scrambled,dic1)         test1.iloc[j,3] = clear # Test set Decrypt level 2 for j in range(0,len(test1)):     if test1.iloc[j,1] == 2:         scrambled = test1.iloc[j,2]         clear = decrypt(scrambled,dic2)         test1.iloc[j,3] = clear for i in range(0,len(test1)):     if test1.iloc[i,3] != '':         test1.iloc[i,2] = test1.iloc[i,3] test1.drop('clear', axis=1, inplace=True)  test = test1.copy()
test1 = pd.read_csv('../input/test.csv') # add column for clear text test1['clear']=''  # treat case of line 8151,38126,38130,42245,43762 test1.iloc[8151,1] = 2 test1.iloc[38130,1] = 2 test1.iloc[42245,1] = 3 test1.iloc[43762,1] = 2  # Test set Decrypt level 1 for j in range(0,len(test1)):     if test1.iloc[j,1] == 1:         scrambled = test1.iloc[j,2]         clear = decrypt(scrambled,dic1)         test1.iloc[j,3] = clear # Test set Decrypt level 2 for j in range(0,len(test1)):     if test1.iloc[j,1] == 2:         scrambled = test1.iloc[j,2]         clear = decrypt(scrambled,dic2)         test1.iloc[j,3] = clear for i in range(0,len(test1)):     if test1.iloc[i,3] != '':         test1.iloc[i,2] = test1.iloc[i,3] test1.drop('clear', axis=1, inplace=True)  test = test1.copy()	test_diff1 = test[test['difficulty'] == 1] test_diff2 = test[test['difficulty'] == 2] test_diff3 = test[test['difficulty'] == 3] test_diff4 = test[test['difficulty'] == 4]
test_diff1 = test[test['difficulty'] == 1] test_diff2 = test[test['difficulty'] == 2] test_diff3 = test[test['difficulty'] == 3] test_diff4 = test[test['difficulty'] == 4]	test_diff1['ciphertext'] = test_diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) test_diff2['ciphertext'] = test_diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff3['ciphertext'] = test_diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff4['ciphertext'] = test_diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))
test_diff1['ciphertext'] = test_diff1['ciphertext'].apply(lambda x: x.replace('1', ' ')) test_diff2['ciphertext'] = test_diff2['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff3['ciphertext'] = test_diff3['ciphertext'].apply(lambda x: x.replace('8', ' ')) test_diff4['ciphertext'] = test_diff4['ciphertext'].apply(lambda x: x.replace('8', ' '))	test_diff = pd.concat([test_diff1, test_diff2, test_diff3, test_diff4])
test_diff = pd.concat([test_diff1, test_diff2, test_diff3, test_diff4])	test1.head()
test1.head()	test_diff = test_diff.set_index('Id').loc[test1['Id']]
test_diff = test_diff.set_index('Id').loc[test1['Id']]	test_vect = vect.transform(test_diff['ciphertext'])
test_vect = vect.transform(test_diff['ciphertext'])	test_pred = model.predict(test_vect)
test_pred = model.predict(test_vect)	submission = pd.DataFrame([test1['Id'],test_pred]).T
submission = pd.DataFrame([test1['Id'],test_pred]).T	submission.columns = ['Id', 'Predicted']
submission.columns = ['Id', 'Predicted']	submission.to_csv('submission.csv', index=False)
submission.to_csv('submission.csv', index=False)	NB_END
import datetime import gc import numpy as np import os import pandas as pd import random  from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from scipy.stats import skew, kurtosis import lightgbm as lgb  import Levenshtein from sklearn.metrics import f1_score from sklearn.model_selection import KFold  from tqdm import tqdm	id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv')
id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv')	def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))
def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))	print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)
print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)	train.head()
import datetime import gc import numpy as np import os import pandas as pd import random  from sklearn.preprocessing import LabelEncoder from sklearn.feature_extraction.text import TfidfVectorizer from scipy.stats import skew, kurtosis import lightgbm as lgb  import Levenshtein from sklearn.metrics import f1_score from sklearn.model_selection import KFold  from tqdm import tqdm	id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 
id_col = 'Id' target_col = 'target'  train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') 	def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))
def extract_features(df):     df['nunique'] = df['ciphertext'].apply(lambda x: len(np.unique(x)))     df['len'] = df['ciphertext'].apply(lambda x: len(x))      def count_chars(x):         n_l = 0 # count letters         n_n = 0 # count numbers         n_s = 0 # count symbols         n_ul = 0 # count upper letters         n_ll = 0 # count lower letters         for i in range(0, len(x)):             if x[i].isalpha():                 n_l += 1                 if x[i].isupper():                     n_ul += 1                 elif x[i].islower():                     n_ll += 1             elif x[i].isdigit():                 n_n += 1             else:                 n_s += 1          return pd.Series([n_l, n_n, n_s, n_ul, n_ll])      cols = ['n_l', 'n_n', 'n_s', 'n_ul', 'n_ll']     for c in cols:         df[c] = 0     tqdm.pandas(desc='count_chars')     df[cols] = df['ciphertext'].progress_apply(lambda x: count_chars(x))     for c in cols:         df[c] /= df['len']      tqdm.pandas(desc='distances')     df['Levenshtein_distance'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x, x[::-1]))     df['Levenshtein_ratio'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x, x[::-1]))     df['Levenshtein_jaro'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x, x[::-1]))     df['Levenshtein_hamming'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x, x[::-1]))      for m in range(1, 5):         df['Levenshtein_distance_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:-m], x[m:]))         df['Levenshtein_ratio_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:-m], x[m:]))         df['Levenshtein_jaro_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:-m], x[m:]))         df['Levenshtein_hamming_m{}'.format(m)] = df['ciphertext'].progress_apply(lambda x: Levenshtein.hamming(x[:-m], x[m:]))          df['Levenshtein_distance_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.distance(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_ratio_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.ratio(x[:len(x)//2], x[len(x)//2:]))     df['Levenshtein_jaro_h'] = df['ciphertext'].progress_apply(lambda x: Levenshtein.jaro(x[:len(x)//2], x[len(x)//2:]))          # All symbols stats     def strstat(x):         r = np.array([ord(c) for c in x])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_sum', 'str_mean', 'str_std', 'str_min', 'str_max', 'str_skew', 'str_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='strstat')     df[cols] = df['ciphertext'].progress_apply(lambda x: strstat(x))          # Digit stats     def str_digit_stat(x):         r = np.array([ord(c) for c in x if c.isdigit()])         if len(r) == 0:             r = np.array([0])         return pd.Series([             np.sum(r),              np.mean(r),              np.std(r),              np.min(r),              np.max(r),             skew(r),              kurtosis(r),             ])     cols = ['str_digit_sum', 'str_digit_mean', 'str_digit_std', 'str_digit_min',          'str_digit_max', 'str_digit_skew', 'str_digit_kurtosis']     for c in cols:         df[c] = 0     tqdm.pandas(desc='str_digit_stat')     df[cols] = df['ciphertext'].progress_apply(lambda x: str_digit_stat(x))	print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)
print('Extracting features for train:') extract_features(train) print('Extracting features for test:') extract_features(test)	train.head()
train.head()	N_DIFF = 4 subm = None
N_DIFF = 4 subm = None	"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = KFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 10,         \'num_leaves\': 30,         \'learning_rate\': 0.05,         \'feature_fraction\': 0.75,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 7,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 0.5,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),     }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"
"for difficulty in range(1, N_DIFF+1):     cur_train = train[train[\'difficulty\'] == difficulty]     cur_test = test[test[\'difficulty\'] == difficulty]     # TFIDF     for k in range(0, 3):         tfidf = TfidfVectorizer(             max_features=1000,             lowercase=False,             token_pattern=\'\\\\S+\',         )          def char_pairs(x, k=1):             buf = []             for i in range(k, len(x)):                 buf.append(x[i-k:i+1])             return \' \'.join(buf)          cur_train[\'text_temp\'] = cur_train.ciphertext.apply(lambda x: char_pairs(x, k))         cur_test[\'text_temp\'] = cur_test.ciphertext.apply(lambda x: char_pairs(x, k))         train_tfids = tfidf.fit_transform(cur_train[\'text_temp\'].values).todense()         test_tfids = tfidf.transform(cur_test[\'text_temp\'].values).todense()          print(\'k = {}: train_tfids.shape = {}\'.format(k, train_tfids.shape))          for i in range(train_tfids.shape[1]):             cur_train[\'text_{}_tfidf{}\'.format(k, i)] = train_tfids[:, i]             cur_test[\'text_{}_tfidf{}\'.format(k, i)] = test_tfids[:, i]          del train_tfids, test_tfids, tfidf         gc.collect()          print(""Training on difficulty = {}"".format(difficulty))     # Build the model     cnt = 0     p_buf = []     p_valid_buf = []     n_splits = 5     kf = KFold(         n_splits=n_splits,          random_state=0)     err_buf = []        undersampling = 0      lgb_params = {         \'boosting_type\': \'gbdt\',         \'objective\': \'multiclass\',         \'metric\': \'multi_logloss\',         \'max_depth\': 10,         \'num_leaves\': 30,         \'learning_rate\': 0.05,         \'feature_fraction\': 0.75,         \'bagging_fraction\': 0.85,         \'bagging_freq\': 7,         \'verbose\': -1,         \'num_threads\': -1,         \'lambda_l1\': 0.5,         \'lambda_l2\': 1.0,         \'min_gain_to_split\': 0,         \'num_class\': train[target_col].nunique(),     }      cols_to_drop = [         \'difficulty\',         id_col,          \'ciphertext\',         target_col,         \'text_temp\',     ]      X = cur_train.drop(cols_to_drop, axis=1, errors=\'ignore\')     feature_names = list(X.columns)      X = X.values     y = cur_train[target_col].values      X_test = cur_test.drop(cols_to_drop, axis=1, errors=\'ignore\')     id_test = cur_test[id_col].values      print(X.shape, y.shape)     print(X_test.shape)      n_features = X.shape[1]      for train_index, valid_index in kf.split(X, y):         print(\'Fold {}/{}\'.format(cnt + 1, n_splits))         params = lgb_params.copy()           lgb_train = lgb.Dataset(             X[train_index],              y[train_index],              feature_name=feature_names,             )         lgb_train.raw_data = None          lgb_valid = lgb.Dataset(             X[valid_index],              y[valid_index],             )         lgb_valid.raw_data = None          model = lgb.train(             params,             lgb_train,             num_boost_round=10000,             valid_sets=[lgb_train, lgb_valid],             early_stopping_rounds=100,             verbose_eval=100,         )          if cnt == 0:             importance = model.feature_importance()             model_fnames = model.feature_name()             tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]             tuples = [x for x in tuples if x[1] > 0]             print(\'Important features:\')             for i in range(20):                 if i < len(tuples):                     print(tuples[i])                 else:                     break              del importance, model_fnames, tuples          p = model.predict(X[valid_index], num_iteration=model.best_iteration)         err = f1_score(y[valid_index], np.argmax(p, axis=1), average=\'macro\')          print(\'{} F1: {}\'.format(cnt + 1, err))          p = model.predict(X_test, num_iteration=model.best_iteration)         if len(p_buf) == 0:             p_buf = np.array(p, dtype=np.float16)         else:             p_buf += np.array(p, dtype=np.float16)         err_buf.append(err)          cnt += 1          del model, lgb_train, lgb_valid, p         gc.collect          # Train on one fold         if cnt > 0:             break       err_mean = np.mean(err_buf)     err_std = np.std(err_buf)     print(\'F1 = {:.6f} +/- {:.6f}\'.format(err_mean, err_std))      preds = p_buf/cnt          cur_subm = pd.DataFrame()     cur_subm[id_col] = id_test     cur_subm[\'Predicted\'] = np.argmax(preds, axis=1)     if difficulty == 1:         subm = cur_subm     else:         subm = pd.concat([subm,cur_subm])"	"subm = subm.set_index(""Id"") sample = pd.read_csv(""../input/sample_submission.csv"").set_index(""Id"") for idx, row in sample.iterrows():     row[""Predicted""] = subm.loc[idx][""Predicted""]  sample.to_csv(""submission.csv"")"
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input/""))  import collections, itertools import sklearn.feature_extraction.text as sktext from sklearn.pipeline import Pipeline from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import classification_report # Any results you write to the current directory are saved as output."	"train_data = pd.read_csv(""../input/train.csv"") train_data.head()"
"train_data = pd.read_csv(""../input/train.csv"") train_data.head()"	"test_data = pd.read_csv(""../input/test.csv"") test_data.head()"
"test_data = pd.read_csv(""../input/test.csv"") test_data.head()"	from sklearn.datasets import fetch_20newsgroups plaintext_data = fetch_20newsgroups(subset='all', download_if_missing=True) category_names = plaintext_data.target_names
from sklearn.datasets import fetch_20newsgroups plaintext_data = fetch_20newsgroups(subset='all', download_if_missing=True) category_names = plaintext_data.target_names	"plain_counts = pd.Series(collections.Counter(itertools.chain.from_iterable(plaintext_data.data))) \\     .rename(""counts"").to_frame() \\     .sort_values(""counts"", ascending = False) plain_counts = 1000000 * plain_counts / plain_counts.sum() plain_counts = plain_counts.reset_index().rename(columns = {""index"":""plain_char""})  diff_counts = [] for i in range(1,5):     counts = pd.Series(         collections.Counter(itertools.chain.from_iterable(train_data.query(""difficulty == @i"")[""ciphertext""].values)) + \\         collections.Counter(itertools.chain.from_iterable(test_data.query(""difficulty == @i"")[""ciphertext""].values))         ).rename(""counts"").to_frame() \\         .sort_values(""counts"", ascending = False)     counts = 1000000 * counts / counts.sum()     counts = counts.reset_index().rename(columns = {""index"":""diff_{}"".format(i)})     diff_counts.append(counts)  pd.concat([plain_counts] + diff_counts, axis = 1).head(20)"
"plain_counts = pd.Series(collections.Counter(itertools.chain.from_iterable(plaintext_data.data))) \\     .rename(""counts"").to_frame() \\     .sort_values(""counts"", ascending = False) plain_counts = 1000000 * plain_counts / plain_counts.sum() plain_counts = plain_counts.reset_index().rename(columns = {""index"":""plain_char""})  diff_counts = [] for i in range(1,5):     counts = pd.Series(         collections.Counter(itertools.chain.from_iterable(train_data.query(""difficulty == @i"")[""ciphertext""].values)) + \\         collections.Counter(itertools.chain.from_iterable(test_data.query(""difficulty == @i"")[""ciphertext""].values))         ).rename(""counts"").to_frame() \\         .sort_values(""counts"", ascending = False)     counts = 1000000 * counts / counts.sum()     counts = counts.reset_index().rename(columns = {""index"":""diff_{}"".format(i)})     diff_counts.append(counts)  pd.concat([plain_counts] + diff_counts, axis = 1).head(20)"	"##diff2 -> plain, subs key = [[\'8\', \' \'],[\'$\', \'e\'],[\'{\', \'t\'],[\'V\', \'a\'],[\'e\', \'o\'],[\'h\', \'i\'],[\'\\x10\', \'n\'],[\'}\', \'s\'],[\'*\', \'r\'],[\'7\', \'h\'],[\'?\', \'l\'],[\'z\', \'\ \'],[\'H\', \'d\'],[\'f\', \'c\'],[\'j\', \'u\'],[\'4\', \'m\'],[\'\\x1a\', \'.\'],[\'x\', \'-\'],[\'.\', \'p\'],[\'k\', \'g\'],[\'v\', \'y\'],[\'d\', \'f\'],[\'^\', \'w\'],[\'\\x18\', \'b\'],[\'b\', \'>\'],[\'[\', \',\'],[\'\\x03\', \'v\'],[\'6\', \'A\'],[\'A\', \'I\'],[\'m\', \'k\'],[\'B\', ""\'""],[\'N\', \':\'],[\'E\', \'S\'],[\'S\', \'1\'],[\'\\x06\', \'T\'],[\'(\', \'X\'],[\'c\', \'0\'],[\'l\', \'M\'],[\'9\', \'C\'],[\'%\', \'*\'],[\'\\x02\', \')\'],[\'\\x08\', \'(\'],[\'O\', \'2\'],[\'a\', \'=\'],[\'\\x0c\', \'N\'],[\'&\', \'R\'],[\'@\', \'P\'],[\'|\', \'3\'],[\'\\x7f\', \'D\'],[\',\', \'O\'],[\'i\', \'@\'],[\'L\', \'E\'],[\'M\', \'L\'],[\'=\', \'""\'],[\'C\', \'9\'],[\'X\', \'\ \'],[\'\\x1b\', \'5\'],[\'1\', \'F\'],[\'n\', \'H\'],[\'Q\', \'B\'],[\'3\', \'4\'],[\'0\', \'_\'],[\'2\', \'x\'],[\'s\', \'W\'],[\'<\', \'6\'],[\')\', \'G\'],[\'_\', \'j\'],[\'G\', \'U\'],[\'u\', \'8\'],[\'\\x19\', \'?\'],[\'-\', \'?\'],[\'o\', \'z\'],[\'F\', \'/\'],[\';\', \'|\'],[\'\ \', \'J\'],[\'~\', \'K\'],[\'W\', \'!\'],[\'!\', \'V\'],[""\'"", \'<\'],[\' \', \'Y\'],[\'\ \', \'+\'],[\'#\', \'q\'],[\'I\', \'$\'],[\':\', \'#\'],[\']\', \'Q\'],[\'/\', \'^\'],[\'g\', \'#\'],[\'\\x1e\', \'%\'],[\'p\', \']\'],[\'5\', \']\'],[\'\\\\\', \'[\'],[\'`\', \'Z\'],[\'t\', \'&\'],[\'y\', \'&\'],[\'R\', \'Z\'],[\'P\', \'}\'],[\'r\', \'{\'],[\'""\', \'\ \'],[\'T\', \'u\'],[\'Z\', \'\\x02\']] decrypt_map_2 = {i:j for i,j in key}"
"##diff2 -> plain, subs key = [[\'8\', \' \'],[\'$\', \'e\'],[\'{\', \'t\'],[\'V\', \'a\'],[\'e\', \'o\'],[\'h\', \'i\'],[\'\\x10\', \'n\'],[\'}\', \'s\'],[\'*\', \'r\'],[\'7\', \'h\'],[\'?\', \'l\'],[\'z\', \'\ \'],[\'H\', \'d\'],[\'f\', \'c\'],[\'j\', \'u\'],[\'4\', \'m\'],[\'\\x1a\', \'.\'],[\'x\', \'-\'],[\'.\', \'p\'],[\'k\', \'g\'],[\'v\', \'y\'],[\'d\', \'f\'],[\'^\', \'w\'],[\'\\x18\', \'b\'],[\'b\', \'>\'],[\'[\', \',\'],[\'\\x03\', \'v\'],[\'6\', \'A\'],[\'A\', \'I\'],[\'m\', \'k\'],[\'B\', ""\'""],[\'N\', \':\'],[\'E\', \'S\'],[\'S\', \'1\'],[\'\\x06\', \'T\'],[\'(\', \'X\'],[\'c\', \'0\'],[\'l\', \'M\'],[\'9\', \'C\'],[\'%\', \'*\'],[\'\\x02\', \')\'],[\'\\x08\', \'(\'],[\'O\', \'2\'],[\'a\', \'=\'],[\'\\x0c\', \'N\'],[\'&\', \'R\'],[\'@\', \'P\'],[\'|\', \'3\'],[\'\\x7f\', \'D\'],[\',\', \'O\'],[\'i\', \'@\'],[\'L\', \'E\'],[\'M\', \'L\'],[\'=\', \'""\'],[\'C\', \'9\'],[\'X\', \'\ \'],[\'\\x1b\', \'5\'],[\'1\', \'F\'],[\'n\', \'H\'],[\'Q\', \'B\'],[\'3\', \'4\'],[\'0\', \'_\'],[\'2\', \'x\'],[\'s\', \'W\'],[\'<\', \'6\'],[\')\', \'G\'],[\'_\', \'j\'],[\'G\', \'U\'],[\'u\', \'8\'],[\'\\x19\', \'?\'],[\'-\', \'?\'],[\'o\', \'z\'],[\'F\', \'/\'],[\';\', \'|\'],[\'\ \', \'J\'],[\'~\', \'K\'],[\'W\', \'!\'],[\'!\', \'V\'],[""\'"", \'<\'],[\' \', \'Y\'],[\'\ \', \'+\'],[\'#\', \'q\'],[\'I\', \'$\'],[\':\', \'#\'],[\']\', \'Q\'],[\'/\', \'^\'],[\'g\', \'#\'],[\'\\x1e\', \'%\'],[\'p\', \']\'],[\'5\', \']\'],[\'\\\\\', \'[\'],[\'`\', \'Z\'],[\'t\', \'&\'],[\'y\', \'&\'],[\'R\', \'Z\'],[\'P\', \'}\'],[\'r\', \'{\'],[\'""\', \'\ \'],[\'T\', \'u\'],[\'Z\', \'\\x02\']] decrypt_map_2 = {i:j for i,j in key}"	"diff_3_data = train_data.query(""difficulty == 3"").copy() diff_3_data[""trans""] = diff_3_data[""ciphertext""].apply(     lambda x:\'\'.join([decrypt_map_2.get(k,\'?\') for k in x]) )  ##top starting letters  diff_3_data.trans.str[:5].value_counts().head()"
"diff_3_data = train_data.query(""difficulty == 3"").copy() diff_3_data[""trans""] = diff_3_data[""ciphertext""].apply(     lambda x:\'\'.join([decrypt_map_2.get(k,\'?\') for k in x]) )  ##top starting letters  diff_3_data.trans.str[:5].value_counts().head()"	collections.Counter([i[:5] for i in plaintext_data.data]).most_common(5)
collections.Counter([i[:5] for i in plaintext_data.data]).most_common(5)	"diff_3_data.loc[diff_3_data[""trans""].apply(lambda x:x[:5] == \'FrMmZ\')].head(3)"
"diff_3_data.loc[diff_3_data[""trans""].apply(lambda x:x[:5] == \'FrMmZ\')].head(3)"	"diff_3_data.query(""Id == \'ID_fb163c212\'"").trans.iloc[0]"
"diff_3_data.query(""Id == \'ID_fb163c212\'"").trans.iloc[0]"	"target_11_data = [i[:300] for i,j in zip(plaintext_data.data, plaintext_data.target) if j == 11]  [i for i in target_11_data if i.find(\'Russell\') > 0 and i.find(""whatever"") > 0]"
"target_11_data = [i[:300] for i,j in zip(plaintext_data.data, plaintext_data.target) if j == 11]  [i for i in target_11_data if i.find(\'Russell\') > 0 and i.find(""whatever"") > 0]"	pd.options.display.max_columns = 300  pd.options.display.max_rows = 300  plain_1, cipher_1 = [     '''From: trussell@cwis.unomaha.edu (Tim Russell)\ Subject: Re: Once tapped, your code is no good any more.\ Organization: University of Nebraska at Omaha\ Distribution: na\ Lines: 18\ \ geoff@ficus.cs.ucla.edu (Geoffrey Kuenning) writes:\ \ >It always amazes me how quick people are to blame whatever\ >administr''',     '''FrMmZ tr8ssellkWw#s.znWm}h\ .e2H (T#m R]ssell)\  L?bjeut/ Re? Onue tUppeE, @fzr &&?e zs n@ ugo} $n\  m?re.# OrzUnM>?tkWnd $nzversMt8 gf Nebr?s\ \\x02 :t Om9h\\x02g DMstr8b@tkWnd n\ # B#nesu 10k f 0eMizk?#g-s.\ s.Mkl\\x02.eU] (GeoM?re\  K>ennznz) wrWtesZk & #2t }lw\ 0s \\x02m?8es me how q{8uo peMple \\x02re t] bl\\x02me wh$tever& #'''      ]  plain_2, cipher_2 = [     '''From: hollasch@kpc.com (Steve Hollasch)\ Subject: Re: Raytracing Colours?\ Summary: Illumination Equations\ Organization: Kubota Pacific Computer, Inc.\ Lines: 44\ \ asecchia@cs.uct.ac.za (Adrian Secchia) writes:\ | When an incident ray (I) strikes an object at point P ...  The reflected\ | ray (R) and the ''',     '''FrMmZ h]ll\\x02sWhi{p&.\ #m (Eteve {fll'sch)i ZHbje-t? ReZ RU\ tr$okn& C&l#?rs?f ?]mmarfI Sll>mzn\\x02tW@n ?q@Ity&ns> Or{\\x02n#8at]{nZ K>bMt\\x02 PUu80k& C&mpMter, dnu.& 1inesa 44k f :sekuhW\\x02yos.z&t.}i.y\\x02 (AUrHan LeWuh\ 9) wrMtes?f | chen ?n kn&>?ent r9\  (\\x02) strHkes ?n {bjeut :t pMMnt P ...  The rezleute?& | rIu (R) ''' ]  plain_3, cipher_3 = [     '''From: snichols@adobe.com (Sherri Nichols)\ Subject: Re: Braves Pitching UpdateDIR\ Organization: Adobe Systems Incorporated\ Lines: 13\ \ In article <1993Apr15.010745.1@acad.drake.edu> sbp002@acad.drake.edu writes:\ >or second starter.  It seems to me that when quality pitchers take the\ >mound, the other ''',     '''FrMmZ snWuh{lsiIa&be.iMm ($herr# NHchkls)k $>bjektZ Re? ?r$ves P#tghyno $p2\\x02teD\\x02Rc Org?n8W:tz@n? AE{be ?=stems 2n\ #rpMr\\x02teUc L]nesI !3{ # Sn UrtMWle <ZdB3AprL5.o\\x02#?45.ZzaH$E.\\x02r9?e.e1Hz sbp>i90\ k\\x02U.:ra8e.e?H wr\ tes?f k@r seWfnL st?rter.  ?t seems t# me th9t when qH?lHty p]tWhers t\\x02oe the# cm]Hn?, the''' ]  plain_4, cipher_4 = [     '''From: art@cs.UAlberta.CA (Art Mulder)\ Subject: comp.windows.x: Getting more performance out of X.  FAQ\ Summary: This posting contains a list of suggestions about what you can do to get the best performance out of X on your workstation -- without buying more hardware.\ Keywords: FAQ speed X\ Nntp-Posti''',     '''FrMmZ UrtkWs./AlbertI.CA (Art \ fl?er)# $@bjeMt: H{mp.wMn\\x02Mws.xZ GettWnz m{re per0urm?n\ e #?t @0 X.  FAQc S{mm$r\ / Thzs p@stWnz WfntIyns } lust Mi s@?gest]{ns \\x02b??t wh\\x02t #@? oIn a& t# {et the best perifrm'nce k?t @y X Mn \ ]Hr w{rMstIty&n \ o wMthf]t buf8nz m?re h9rEwUre.f Ke@wfrLs' FAQ spee' X> NntpoP'''    ]  plain_5, cipher_5 = [     '''From: joslin@pogo.isp.pitt.edu (David Joslin)\ Subject: Apology to Jim Meritt (Was: Silence is concurance)\ Distribution: usa\ Organization: Intelligent Systems Program\ Lines: 39\ \ m23364@mwunix.mitre.org (James Meritt) writes:\ >}So stop dodging the question.  What is hypocritical about my\ >}criticizing''',     '''FrMmZ j]slMnypf]W.>sp.putt.e2H (D?vHd Jksl8n)f !?bjeut? Ap@l{u= tW J>m Herztt (??su S]lenWe Ms -Mnu8r\\x02nWe)& DkstrybftuMnZ @s'\  OrW$nM#9tM]nZ :ntell#]ent 1]stems Pr#{r\\x02m& 1inesa 3}f { ma3364kmw8nMx.m8tre.fr] (J?mes cerutt) wrztesZ& i}L{ st@p \\x02MEyMn- the q@estkWn.  kh\ t zs h\ pfMrit]W\\x02l :bMHt m#f z2gry''' ]
pd.options.display.max_columns = 300  pd.options.display.max_rows = 300  plain_1, cipher_1 = [     '''From: trussell@cwis.unomaha.edu (Tim Russell)\ Subject: Re: Once tapped, your code is no good any more.\ Organization: University of Nebraska at Omaha\ Distribution: na\ Lines: 18\ \ geoff@ficus.cs.ucla.edu (Geoffrey Kuenning) writes:\ \ >It always amazes me how quick people are to blame whatever\ >administr''',     '''FrMmZ tr8ssellkWw#s.znWm}h\ .e2H (T#m R]ssell)\  L?bjeut/ Re? Onue tUppeE, @fzr &&?e zs n@ ugo} $n\  m?re.# OrzUnM>?tkWnd $nzversMt8 gf Nebr?s\ \\x02 :t Om9h\\x02g DMstr8b@tkWnd n\ # B#nesu 10k f 0eMizk?#g-s.\ s.Mkl\\x02.eU] (GeoM?re\  K>ennznz) wrWtesZk & #2t }lw\ 0s \\x02m?8es me how q{8uo peMple \\x02re t] bl\\x02me wh$tever& #'''      ]  plain_2, cipher_2 = [     '''From: hollasch@kpc.com (Steve Hollasch)\ Subject: Re: Raytracing Colours?\ Summary: Illumination Equations\ Organization: Kubota Pacific Computer, Inc.\ Lines: 44\ \ asecchia@cs.uct.ac.za (Adrian Secchia) writes:\ | When an incident ray (I) strikes an object at point P ...  The reflected\ | ray (R) and the ''',     '''FrMmZ h]ll\\x02sWhi{p&.\ #m (Eteve {fll'sch)i ZHbje-t? ReZ RU\ tr$okn& C&l#?rs?f ?]mmarfI Sll>mzn\\x02tW@n ?q@Ity&ns> Or{\\x02n#8at]{nZ K>bMt\\x02 PUu80k& C&mpMter, dnu.& 1inesa 44k f :sekuhW\\x02yos.z&t.}i.y\\x02 (AUrHan LeWuh\ 9) wrMtes?f | chen ?n kn&>?ent r9\  (\\x02) strHkes ?n {bjeut :t pMMnt P ...  The rezleute?& | rIu (R) ''' ]  plain_3, cipher_3 = [     '''From: snichols@adobe.com (Sherri Nichols)\ Subject: Re: Braves Pitching UpdateDIR\ Organization: Adobe Systems Incorporated\ Lines: 13\ \ In article <1993Apr15.010745.1@acad.drake.edu> sbp002@acad.drake.edu writes:\ >or second starter.  It seems to me that when quality pitchers take the\ >mound, the other ''',     '''FrMmZ snWuh{lsiIa&be.iMm ($herr# NHchkls)k $>bjektZ Re? ?r$ves P#tghyno $p2\\x02teD\\x02Rc Org?n8W:tz@n? AE{be ?=stems 2n\ #rpMr\\x02teUc L]nesI !3{ # Sn UrtMWle <ZdB3AprL5.o\\x02#?45.ZzaH$E.\\x02r9?e.e1Hz sbp>i90\ k\\x02U.:ra8e.e?H wr\ tes?f k@r seWfnL st?rter.  ?t seems t# me th9t when qH?lHty p]tWhers t\\x02oe the# cm]Hn?, the''' ]  plain_4, cipher_4 = [     '''From: art@cs.UAlberta.CA (Art Mulder)\ Subject: comp.windows.x: Getting more performance out of X.  FAQ\ Summary: This posting contains a list of suggestions about what you can do to get the best performance out of X on your workstation -- without buying more hardware.\ Keywords: FAQ speed X\ Nntp-Posti''',     '''FrMmZ UrtkWs./AlbertI.CA (Art \ fl?er)# $@bjeMt: H{mp.wMn\\x02Mws.xZ GettWnz m{re per0urm?n\ e #?t @0 X.  FAQc S{mm$r\ / Thzs p@stWnz WfntIyns } lust Mi s@?gest]{ns \\x02b??t wh\\x02t #@? oIn a& t# {et the best perifrm'nce k?t @y X Mn \ ]Hr w{rMstIty&n \ o wMthf]t buf8nz m?re h9rEwUre.f Ke@wfrLs' FAQ spee' X> NntpoP'''    ]  plain_5, cipher_5 = [     '''From: joslin@pogo.isp.pitt.edu (David Joslin)\ Subject: Apology to Jim Meritt (Was: Silence is concurance)\ Distribution: usa\ Organization: Intelligent Systems Program\ Lines: 39\ \ m23364@mwunix.mitre.org (James Meritt) writes:\ >}So stop dodging the question.  What is hypocritical about my\ >}criticizing''',     '''FrMmZ j]slMnypf]W.>sp.putt.e2H (D?vHd Jksl8n)f !?bjeut? Ap@l{u= tW J>m Herztt (??su S]lenWe Ms -Mnu8r\\x02nWe)& DkstrybftuMnZ @s'\  OrW$nM#9tM]nZ :ntell#]ent 1]stems Pr#{r\\x02m& 1inesa 3}f { ma3364kmw8nMx.m8tre.fr] (J?mes cerutt) wrztesZ& i}L{ st@p \\x02MEyMn- the q@estkWn.  kh\ t zs h\ pfMrit]W\\x02l :bMHt m#f z2gry''' ]	print(pd.DataFrame({'plain':list(plain_1)[:len(cipher_1)], 'cipher':list(cipher_1)}).T)
print(pd.DataFrame({'plain':list(plain_1)[:len(cipher_1)], 'cipher':list(cipher_1)}).T)	plain, cipher = [], []  pairs = [(plain_1, cipher_1), (plain_2, cipher_2), (plain_3, cipher_3), (plain_4, cipher_4), (plain_5, cipher_5)] for p_temp, c_temp in pairs:     i1,i2 = 0,0     while 1:         p,c = p_temp[i1], c_temp[i2]         plain.append(p)         cipher.append(c)         if p == '\ ':             i2+=1         i1 += 1          i2 += 1         if i2 == 300:             break  pd.DataFrame({'plain':list(plain), 'cipher':list(cipher)}).T  possible_maps = collections.defaultdict(list) for i,j in zip(plain, cipher):     possible_maps[i].append(j)  sure_map = {} unsure_map = {} for i,j in possible_maps.items():     if (len(set(j)) == 1) and len(j) !=1:         sure_map[i] = j[0]     else:         unsure_map[i] = set(j) print(len(sure_map), len(unsure_map))
plain, cipher = [], []  pairs = [(plain_1, cipher_1), (plain_2, cipher_2), (plain_3, cipher_3), (plain_4, cipher_4), (plain_5, cipher_5)] for p_temp, c_temp in pairs:     i1,i2 = 0,0     while 1:         p,c = p_temp[i1], c_temp[i2]         plain.append(p)         cipher.append(c)         if p == '\ ':             i2+=1         i1 += 1          i2 += 1         if i2 == 300:             break  pd.DataFrame({'plain':list(plain), 'cipher':list(cipher)}).T  possible_maps = collections.defaultdict(list) for i,j in zip(plain, cipher):     possible_maps[i].append(j)  sure_map = {} unsure_map = {} for i,j in possible_maps.items():     if (len(set(j)) == 1) and len(j) !=1:         sure_map[i] = j[0]     else:         unsure_map[i] = set(j) print(len(sure_map), len(unsure_map))	"d = train_data.query(""difficulty == 3"").copy() d[""trans""] = d[""ciphertext""].apply(     lambda x:\'\'.join([decrypt_map_2.get(k,\'?\') for k in x]) ) d[""trans""] = d[""trans""].apply(     lambda x:\'\'.join([k if k in sure_map else \'?\' for k in x]) )"
"d = train_data.query(""difficulty == 3"").copy() d[""trans""] = d[""ciphertext""].apply(     lambda x:\'\'.join([decrypt_map_2.get(k,\'?\') for k in x]) ) d[""trans""] = d[""trans""].apply(     lambda x:\'\'.join([k if k in sure_map else \'?\' for k in x]) )"	"X_train = [\'\'.join([k if k in sure_map else \'?\' for k in i]) for i in plaintext_data.data] y_train = plaintext_data.target  X_test = d[""trans""].values y_test = d[""target""].values "
"X_train = [\'\'.join([k if k in sure_map else \'?\' for k in i]) for i in plaintext_data.data] y_train = plaintext_data.target  X_test = d[""trans""].values y_test = d[""target""].values "	clf = Pipeline([     ('vectorizer', sktext.CountVectorizer(lowercase=True, ngram_range = (1,2))),     ('tfidf', sktext.TfidfTransformer()),     ('clf', KNeighborsClassifier(n_neighbors = 1)) ])  clf.fit(X_train, y_train)
clf = Pipeline([     ('vectorizer', sktext.CountVectorizer(lowercase=True, ngram_range = (1,2))),     ('tfidf', sktext.TfidfTransformer()),     ('clf', KNeighborsClassifier(n_neighbors = 1)) ])  clf.fit(X_train, y_train)	print(classification_report(y_train, clf.predict(X_train)))
print(classification_report(y_train, clf.predict(X_train)))	print(classification_report(y_test, clf.predict(X_test)))
print(classification_report(y_test, clf.predict(X_test)))	NB_END
import pandas as pd df = pd.read_csv('../input/train.csv') df.head()	"level1 = str.maketrans(     \'*#^-G1>c\\x03X\\x1b_t%dO\\x02ah8?]\\\'\\x08s{0oWfTAvV\\x18\\x7fE:g\\x1e2|y[9we@!&x;\\x10Fz""/Ql\\x1aLn<\ q,H~b\\\\5u J+B\\x06}64UiKP`\\x1cr)3Z(.\\x0cYmIkSp$D=\',     \'From: SubjectOganizl/fp.\ Thywk,dsQVNEIHRC()>PKMUX?D\\\'YLvGA\ Wq1895-42+@063<Bx$7J""|[]Z!#*&_;=^~%`\\\\}{\\x08\\x1e\\x02\\x0c\\x10\' )  df.ciphertext = df.ciphertext.str.translate(level1) df.head()"
"level1 = str.maketrans(     \'*#^-G1>c\\x03X\\x1b_t%dO\\x02ah8?]\\\'\\x08s{0oWfTAvV\\x18\\x7fE:g\\x1e2|y[9we@!&x;\\x10Fz""/Ql\\x1aLn<\ q,H~b\\\\5u J+B\\x06}64UiKP`\\x1cr)3Z(.\\x0cYmIkSp$D=\',     \'From: SubjectOganizl/fp.\ Thywk,dsQVNEIHRC()>PKMUX?D\\\'YLvGA\ Wq1895-42+@063<Bx$7J""|[]Z!#*&_;=^~%`\\\\}{\\x08\\x1e\\x02\\x0c\\x10\' )  df.ciphertext = df.ciphertext.str.translate(level1) df.head()"	df[df.ciphertext.str.startswith('%Z*[U[H-+?5O0I81)@')]
%matplotlib inline import matplotlib.pyplot as plt plt.rcParams['figure.figsize'] = [15, 7.5] import numpy as np import pandas as pd import string from collections import Counter from Crypto.Cipher import * from cryptography import * import hashlib, hmac, secrets, base64 from sympy import primerange import os train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv')	level_one = train[train['difficulty']==1].copy() level_one['ciphertext'] = level_one['ciphertext'].map(lambda x: str(x)[:300]) hist = Counter(' '.join(level_one['ciphertext'].astype(str).values)) cph = pd.DataFrame.from_dict(hist, orient='index').reset_index() cph.columns = ['ciph', 'ciph_freq'] cph = cph.sort_values(by='ciph_freq', ascending=False).reset_index(drop=True) cph.plot(kind='bar')
level_one = train[train['difficulty']==1].copy() level_one['ciphertext'] = level_one['ciphertext'].map(lambda x: str(x)[:300]) hist = Counter(' '.join(level_one['ciphertext'].astype(str).values)) cph = pd.DataFrame.from_dict(hist, orient='index').reset_index() cph.columns = ['ciph', 'ciph_freq'] cph = cph.sort_values(by='ciph_freq', ascending=False).reset_index(drop=True) cph.plot(kind='bar')	# num chars in level 1 cipher cph['ciph_freq'].sum()
# num chars in level 1 cipher cph['ciph_freq'].sum()	from sklearn.datasets import fetch_20newsgroups news = fetch_20newsgroups(subset='train')
from sklearn.datasets import fetch_20newsgroups news = fetch_20newsgroups(subset='train')	news.keys()
news.keys()	news['data'][0:5]
news['data'][0:5]	plain = news['data'][:1420]
plain = news['data'][:1420]	plain_char_sample = Counter(' '.join(plain)) pt = pd.DataFrame.from_dict(plain_char_sample, orient='index').reset_index() pt.columns = ['char', 'freq'] pt = pt.sort_values(by='freq', ascending=False).reset_index(drop=True) pt.plot(kind='bar')
plain_char_sample = Counter(' '.join(plain)) pt = pd.DataFrame.from_dict(plain_char_sample, orient='index').reset_index() pt.columns = ['char', 'freq'] pt = pt.sort_values(by='freq', ascending=False).reset_index(drop=True) pt.plot(kind='bar')	# num plain chars in news sample, very rough now pt['freq'].sum()
# num plain chars in news sample, very rough now pt['freq'].sum()	fq_comp=cph fq_comp['char']=pt['char'] fq_comp['freq']=pt['freq'] fq_comp.head(10)
fq_comp=cph fq_comp['char']=pt['char'] fq_comp['freq']=pt['freq'] fq_comp.head(10)	fq_comp.plot(kind='bar')
fq_comp.plot(kind='bar')	more_plain = news['data'][:10000]
more_plain = news['data'][:10000]	more = Counter(' '.join(more_plain)) mpt = pd.DataFrame.from_dict(more, orient='index').reset_index() mpt.columns = ['char', 'freq'] mpt = mpt.sort_values(by='freq', ascending=False).reset_index(drop=True)
more = Counter(' '.join(more_plain)) mpt = pd.DataFrame.from_dict(more, orient='index').reset_index() mpt.columns = ['char', 'freq'] mpt = mpt.sort_values(by='freq', ascending=False).reset_index(drop=True)	mpt['freq'].sum()
mpt['freq'].sum()	fq_comp2=cph fq_comp2['char']=mpt['char'] fq_comp2['freq']=mpt['freq'] fq_comp2['ratio']=fq_comp2['freq']/fq_comp2['ciph_freq'] fq_comp2.head(10)
fq_comp2=cph fq_comp2['char']=mpt['char'] fq_comp2['freq']=mpt['freq'] fq_comp2['ratio']=fq_comp2['freq']/fq_comp2['ciph_freq'] fq_comp2.head(10)	fq_comp2['ratio'].plot(kind='bar')
fq_comp2['ratio'].plot(kind='bar')	fq_comp2[['ciph', 'char', 'ratio']].sort_values(by='ratio', ascending=False)
fq_comp2[['ciph', 'char', 'ratio']].sort_values(by='ratio', ascending=False)	#chars by frequency from this limited sample fmap=pd.Series(fq_comp2.char.values, fq_comp2.ciph.values).to_dict() fmap
from collections import defaultdict import csv import os import tarfile	"target_dir = {""0"": ""alt.atheism"",              ""1"": ""comp.graphics"",              ""2"": ""comp.os.ms-windows.misc"",              ""3"": ""comp.sys.ibm.pc.hardware"",              ""4"": ""comp.sys.mac.hardware"",              ""5"": ""comp.windows.x"",              ""6"": ""misc.forsale"",              ""7"": ""rec.autos"",              ""8"": ""rec.motorcycles"",              ""9"": ""rec.sport.baseball"",              ""10"": ""rec.sport.hockey"",              ""11"": ""sci.crypt"",              ""12"": ""sci.electronics"",              ""13"": ""sci.med"",              ""14"": ""sci.space"",              ""15"": ""soc.religion.christian"",              ""16"": ""talk.politics.guns"",              ""17"": ""talk.politics.mideast"",              ""18"": ""talk.politics.misc"",              ""19"": ""talk.religion.misc""} dir_target = {v: k for k, v in target_dir.items()}"
"target_dir = {""0"": ""alt.atheism"",              ""1"": ""comp.graphics"",              ""2"": ""comp.os.ms-windows.misc"",              ""3"": ""comp.sys.ibm.pc.hardware"",              ""4"": ""comp.sys.mac.hardware"",              ""5"": ""comp.windows.x"",              ""6"": ""misc.forsale"",              ""7"": ""rec.autos"",              ""8"": ""rec.motorcycles"",              ""9"": ""rec.sport.baseball"",              ""10"": ""rec.sport.hockey"",              ""11"": ""sci.crypt"",              ""12"": ""sci.electronics"",              ""13"": ""sci.med"",              ""14"": ""sci.space"",              ""15"": ""soc.religion.christian"",              ""16"": ""talk.politics.guns"",              ""17"": ""talk.politics.mideast"",              ""18"": ""talk.politics.misc"",              ""19"": ""talk.religion.misc""} dir_target = {v: k for k, v in target_dir.items()}"	def str_to_pattern(s, spacechar):     frm = ''.join([chr(i) for i in range(256)]) + spacechar     to = '.' * 256 + ' '     transtab = str.maketrans(frm, to)     return s.translate(transtab)
def str_to_pattern(s, spacechar):     frm = ''.join([chr(i) for i in range(256)]) + spacechar     to = '.' * 256 + ' '     transtab = str.maketrans(frm, to)     return s.translate(transtab)	def fix_plaindata(data):     data = data.replace('\ \ ', '\ ')     data = data.replace('\ ', '\ ')     data = data.replace('\ ', '\  ')     if data.endswith('\  '):         data = data[:-1]     return data
def fix_plaindata(data):     data = data.replace('\ \ ', '\ ')     data = data.replace('\ ', '\ ')     data = data.replace('\ ', '\  ')     if data.endswith('\  '):         data = data[:-1]     return data	"def get_plain_block_patterns():     tar_fname = ""../input/sklearn-20newsgroup/20news-bydate/20news-bydate.tar.gz""     tar = tarfile.open(tar_fname, ""r:gz"")     plain_block_pattern = defaultdict(list)     for member in tar.getmembers():         if member.isdir():             continue                      # split filename and convert it to a target         head, tail = os.path.split(member.path)         _dir = os.path.split(head)[1]          target = dir_target[_dir]                  # read plaindata         fh=tar.extractfile(member)         plaindata = fh.read()         plaindata = plaindata.decode(\'latin-1\')         plaindata = fix_plaindata(plaindata)                  # Split plaindata into chunks of length 300         # and convert it to block patterns         for pos in range(0, len(plaindata), 300):             block = plaindata[pos:pos+300]             pattern = str_to_pattern(block, \' \')             plain_block_pattern[pattern].append(target)     return plain_block_pattern     "
"def get_plain_block_patterns():     tar_fname = ""../input/sklearn-20newsgroup/20news-bydate/20news-bydate.tar.gz""     tar = tarfile.open(tar_fname, ""r:gz"")     plain_block_pattern = defaultdict(list)     for member in tar.getmembers():         if member.isdir():             continue                      # split filename and convert it to a target         head, tail = os.path.split(member.path)         _dir = os.path.split(head)[1]          target = dir_target[_dir]                  # read plaindata         fh=tar.extractfile(member)         plaindata = fh.read()         plaindata = plaindata.decode(\'latin-1\')         plaindata = fix_plaindata(plaindata)                  # Split plaindata into chunks of length 300         # and convert it to block patterns         for pos in range(0, len(plaindata), 300):             block = plaindata[pos:pos+300]             pattern = str_to_pattern(block, \' \')             plain_block_pattern[pattern].append(target)     return plain_block_pattern     "	"def get_test_rows():     with open(""../input/20-newsgroups-ciphertext-challenge/test.csv"", \'r\') as fh:         reader = csv.reader(fh, delimiter="","")         rows = []         for i, row in enumerate(reader, 1):             rows.append(row)     return rows"
"def get_test_rows():     with open(""../input/20-newsgroups-ciphertext-challenge/test.csv"", \'r\') as fh:         reader = csv.reader(fh, delimiter="","")         rows = []         for i, row in enumerate(reader, 1):             rows.append(row)     return rows"	"def classify_by_pattern():     print(""Starting classification of difficulty 1,2 and 3"")     rows = get_test_rows()      cipher_block_pattern = {}     for _id, difficulty, ciphertext in rows:         if difficulty == ""1"":             s = str_to_pattern(ciphertext, ""1"")             cipher_block_pattern[_id] = s         elif difficulty in ""23"":             s = str_to_pattern(ciphertext, ""8"")             cipher_block_pattern[_id] = s      plain_block_patterns = get_plain_block_patterns()     correct, multiclass, noclass = 0, 0, 0     classified_id_to_target = {}     unclassified_id_to_target = {}     for _id, cipher_pattern in cipher_block_pattern.items():         if cipher_pattern in plain_block_patterns:             targets = plain_block_patterns[cipher_pattern]             target = targets[0]             if all(t == target for t in targets):                 correct += 1                 classified_id_to_target[_id] = target             else:                 multiclass += 1                 targets = set(int(t) for t in targets)                 unclassified_id_to_target[_id] = [str(t) for t in sorted(targets)]         else:             noclass += 1     print(""No class: %d   Correct: %d   Multi: %d"" % (noclass, correct, multiclass))     print(""Success of classification %5.2f pct"" % (100*correct/(correct+multiclass)))      return classified_id_to_target, unclassified_id_to_target"
"def classify_by_pattern():     print(""Starting classification of difficulty 1,2 and 3"")     rows = get_test_rows()      cipher_block_pattern = {}     for _id, difficulty, ciphertext in rows:         if difficulty == ""1"":             s = str_to_pattern(ciphertext, ""1"")             cipher_block_pattern[_id] = s         elif difficulty in ""23"":             s = str_to_pattern(ciphertext, ""8"")             cipher_block_pattern[_id] = s      plain_block_patterns = get_plain_block_patterns()     correct, multiclass, noclass = 0, 0, 0     classified_id_to_target = {}     unclassified_id_to_target = {}     for _id, cipher_pattern in cipher_block_pattern.items():         if cipher_pattern in plain_block_patterns:             targets = plain_block_patterns[cipher_pattern]             target = targets[0]             if all(t == target for t in targets):                 correct += 1                 classified_id_to_target[_id] = target             else:                 multiclass += 1                 targets = set(int(t) for t in targets)                 unclassified_id_to_target[_id] = [str(t) for t in sorted(targets)]         else:             noclass += 1     print(""No class: %d   Correct: %d   Multi: %d"" % (noclass, correct, multiclass))     print(""Success of classification %5.2f pct"" % (100*correct/(correct+multiclass)))      return classified_id_to_target, unclassified_id_to_target"	"classified_id_to_target, unclassified_id_to_target = classify_by_pattern()  print(""Sample of 5 classified ids"") for i, (_id, target) in enumerate(classified_id_to_target.items()):     print(_id, target)     if i > 5:         break print()  print(""Sample of 5 unclassified ids"") for i, (_id, targets) in enumerate(unclassified_id_to_target.items()):     print(_id, targets)     if i > 5:         break"
"classified_id_to_target, unclassified_id_to_target = classify_by_pattern()  print(""Sample of 5 classified ids"") for i, (_id, target) in enumerate(classified_id_to_target.items()):     print(_id, target)     if i > 5:         break print()  print(""Sample of 5 unclassified ids"") for i, (_id, targets) in enumerate(unclassified_id_to_target.items()):     print(_id, targets)     if i > 5:         break"	NB_END
"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from collections import Counter import warnings warnings.simplefilter(action=\'ignore\', category=FutureWarning)  import os print(os.listdir(""../input""))"	data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')
data = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') subm = pd.read_csv('../input/sample_submission.csv')	data.head()
data.head()	data_1 = data.query('difficulty==1')
data_1 = data.query('difficulty==1')	data_1.head()
data_1.head()	alp = pd.Series(Counter(''.join(data_1['ciphertext']))) alp.head(10)
alp = pd.Series(Counter(''.join(data_1['ciphertext']))) alp.head(10)	len(alp)
len(alp)	from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score, classification_report, confusion_matrix from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score, classification_report, confusion_matrix from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import FunctionTransformer from sklearn.model_selection import GridSearchCV	X = data_1.drop('target', axis=1) y = data_1['target']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)
X = data_1.drop('target', axis=1) y = data_1['target']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)	"def tokenize(text):      return text.split(""1"")  vectorizer = CountVectorizer(     analyzer = \'word\',     tokenizer = tokenize,     lowercase = False,     ngram_range=(1, 1))  estimator = LogisticRegression(random_state=0)"
"def tokenize(text):      return text.split(""1"")  vectorizer = CountVectorizer(     analyzer = \'word\',     tokenizer = tokenize,     lowercase = False,     ngram_range=(1, 1))  estimator = LogisticRegression(random_state=0)"	model = Pipeline([('selector', FunctionTransformer(lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])
model = Pipeline([('selector', FunctionTransformer(lambda x: x['ciphertext'], validate=False)),                   ('vectorizer', vectorizer),                    ('tfidf', TfidfTransformer()),                   ('estimator', estimator)])	def generate_tokenizer(separator):     def tokenizer(text):         return text.split(separator)     return tokenizer
def generate_tokenizer(separator):     def tokenizer(text):         return text.split(separator)     return tokenizer	"tokenize_1 = generate_tokenizer(""1"")  model.steps[1][1].set_params(tokenizer=tokenize_1)"
"tokenize_1 = generate_tokenizer(""1"")  model.steps[1][1].set_params(tokenizer=tokenize_1)"	model.fit(X_train, y_train)
model.fit(X_train, y_train)	y_pred = model.predict(X_test)
y_pred = model.predict(X_test)	f1_score(y_test, y_pred, average='macro')
f1_score(y_test, y_pred, average='macro')	def evaluate_delimiters(data):         X = data.drop('target', axis=1)     y = data['target']     X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=.2, stratify=y, random_state=0)          scores = {}          # let's get all the chars that are used:     alp = pd.Series(Counter(''.join(data['ciphertext'])))      for c in alp.keys():         tokenize = generate_tokenizer(c)         model.steps[1][1].set_params(tokenizer=tokenize)         model.fit(X_train, y_train)         y_pred = model.predict(X_test)         score = f1_score(y_test, y_pred, average='macro')         scores[c] = score     return pd.Series(scores).sort_values(ascending=False)
def evaluate_delimiters(data):         X = data.drop('target', axis=1)     y = data['target']     X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=.2, stratify=y, random_state=0)          scores = {}          # let's get all the chars that are used:     alp = pd.Series(Counter(''.join(data['ciphertext'])))      for c in alp.keys():         tokenize = generate_tokenizer(c)         model.steps[1][1].set_params(tokenizer=tokenize)         model.fit(X_train, y_train)         y_pred = model.predict(X_test)         score = f1_score(y_test, y_pred, average='macro')         scores[c] = score     return pd.Series(scores).sort_values(ascending=False)	scores_difficulty_1 = evaluate_delimiters(data.query('difficulty==1'))
scores_difficulty_1 = evaluate_delimiters(data.query('difficulty==1'))	scores_difficulty_1[:10]
scores_difficulty_1[:10]	scores_difficulty_2 = evaluate_delimiters(data.query('difficulty==2'))
scores_difficulty_2 = evaluate_delimiters(data.query('difficulty==2'))	scores_difficulty_2[:10]
scores_difficulty_2[:10]	scores_difficulty_3 = evaluate_delimiters(data.query('difficulty==3'))
scores_difficulty_3 = evaluate_delimiters(data.query('difficulty==3'))	scores_difficulty_3[:10]
scores_difficulty_3[:10]	scores_difficulty_4 = evaluate_delimiters(data.query('difficulty==4'))
scores_difficulty_4 = evaluate_delimiters(data.query('difficulty==4'))	scores_difficulty_4[:10]
scores_difficulty_4[:10]	"book = {\'1\': \' \',  \'\\x1b\': \'e\',  \'t\': \'t\',  \'O\': \'a\',  \'^\': \'o\',  \'a\': \'i\',  \'\\x02\': \'n\',  \'v\': \'s\',  \'#\': \'r\',  \'0\': \'h\',  \'8\': \'l\',  \'s\': \'\ \',  \'A\': \'d\',  \'_\': \'c\',  \'c\': \'u\',  \'-\': \'m\',  \'\\x08\': \'.\',  \'q\': \'-\',  ""\'"": \'p\',  \'d\': \'g\',  \'o\': \'y\',  \']\': \'f\',  \'W\': \'w\',  \'\\x03\': \'b\',  \'T\': \',\',  \'z\': \'v\',  \':\': \'I\',  \'[\': \'>\',  \'f\': \'k\',  \'G\': \':\',  \'L\': \'1\',  \'>\': \'S\',  \'{\': \'T\',  \'/\': \'A\',  \'\\\\\': \'0\',  \'2\': \'C\',  \'y\': \')\',  \'e\': \'M\',  \';\': ""\'"",  \'|\': \'(\',  \'Z\': \'=\',  \'H\': \'2\',  \'\\x1c\': \'*\',  \'\\x1e\': \'R\',  \'x\': \'D\',  \'\\x7f\': \'N\',  \'%\': \'O\',  \'Q\': \'\ \',  \'9\': \'P\',  \'E\': \'E\',  \'F\': \'L\',  \')\': \'E\',  \'u\': \'3\',  \'b\': \'@\',  \'J\': \'B\',  \'6\': \'""\',  \'g\': \'H\',  \'*\': \'F\',  \'<\': \'9\',  \'\ \': \'5\',  \',\': \'4\',  \'+\': \'x\',  \'l\': \'W\',  \'X\': \'j\',  \'5\': \'6\',  \'""\': \'G\',  \'n\': \'8\',  \'@\': \'U\',  \'&\': \'?\',  \'h\': \'z\',  \'?\': \'/\',  \'\\x06\': \'7\',  \'}\': \'J\',  \'4\': \'J\',  \'P\': \'!\',  \'w\': \'K\',  \'\\x18\': \'V\',  \'\\x10\': \'Y\',  \'!\': \'X\',  \'(\': \'Y\',  \' \': \'<\',  \'\\x1a\': \'q\',  \'`\': \'>\',  \'.\': \'#\',  \'B\': \'$\',  \'~\': \'+\',  \'3\': \';\',  \'V\': \'Q\',  \'m\': \'q\',  \'\\x0c\': \'%\',  \'U\': \'[\',  \'i\': \']\',  \'r\': \'&\',  \'K\': \'Z\',  \'Y\': \'~\',  \'I\': \'}\',  \'k\': \'{\',  \'S\': \'\ \',  \'$\': \'\\x08\',  \'p\': \'\\x02\'} "
"book = {\'1\': \' \',  \'\\x1b\': \'e\',  \'t\': \'t\',  \'O\': \'a\',  \'^\': \'o\',  \'a\': \'i\',  \'\\x02\': \'n\',  \'v\': \'s\',  \'#\': \'r\',  \'0\': \'h\',  \'8\': \'l\',  \'s\': \'\ \',  \'A\': \'d\',  \'_\': \'c\',  \'c\': \'u\',  \'-\': \'m\',  \'\\x08\': \'.\',  \'q\': \'-\',  ""\'"": \'p\',  \'d\': \'g\',  \'o\': \'y\',  \']\': \'f\',  \'W\': \'w\',  \'\\x03\': \'b\',  \'T\': \',\',  \'z\': \'v\',  \':\': \'I\',  \'[\': \'>\',  \'f\': \'k\',  \'G\': \':\',  \'L\': \'1\',  \'>\': \'S\',  \'{\': \'T\',  \'/\': \'A\',  \'\\\\\': \'0\',  \'2\': \'C\',  \'y\': \')\',  \'e\': \'M\',  \';\': ""\'"",  \'|\': \'(\',  \'Z\': \'=\',  \'H\': \'2\',  \'\\x1c\': \'*\',  \'\\x1e\': \'R\',  \'x\': \'D\',  \'\\x7f\': \'N\',  \'%\': \'O\',  \'Q\': \'\ \',  \'9\': \'P\',  \'E\': \'E\',  \'F\': \'L\',  \')\': \'E\',  \'u\': \'3\',  \'b\': \'@\',  \'J\': \'B\',  \'6\': \'""\',  \'g\': \'H\',  \'*\': \'F\',  \'<\': \'9\',  \'\ \': \'5\',  \',\': \'4\',  \'+\': \'x\',  \'l\': \'W\',  \'X\': \'j\',  \'5\': \'6\',  \'""\': \'G\',  \'n\': \'8\',  \'@\': \'U\',  \'&\': \'?\',  \'h\': \'z\',  \'?\': \'/\',  \'\\x06\': \'7\',  \'}\': \'J\',  \'4\': \'J\',  \'P\': \'!\',  \'w\': \'K\',  \'\\x18\': \'V\',  \'\\x10\': \'Y\',  \'!\': \'X\',  \'(\': \'Y\',  \' \': \'<\',  \'\\x1a\': \'q\',  \'`\': \'>\',  \'.\': \'#\',  \'B\': \'$\',  \'~\': \'+\',  \'3\': \';\',  \'V\': \'Q\',  \'m\': \'q\',  \'\\x0c\': \'%\',  \'U\': \'[\',  \'i\': \']\',  \'r\': \'&\',  \'K\': \'Z\',  \'Y\': \'~\',  \'I\': \'}\',  \'k\': \'{\',  \'S\': \'\ \',  \'$\': \'\\x08\',  \'p\': \'\\x02\'} "	dec_table = str.maketrans(book)
dec_table = str.maketrans(book)	data_1_clean = data.query('difficulty==1').copy()
data_1_clean = data.query('difficulty==1').copy()	data_1_clean['ciphertext'] = data_1_clean['ciphertext'].map(lambda x: x.translate(dec_table))
data_1_clean['ciphertext'] = data_1_clean['ciphertext'].map(lambda x: x.translate(dec_table))	data_1_clean['ciphertext'][1]
data_1_clean['ciphertext'][1]	X = data_1_clean.drop('target', axis=1) y = data_1_clean['target']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)
X = data_1_clean.drop('target', axis=1) y = data_1_clean['target']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=0)	"tokenize_ws = generate_tokenizer("" "") model.steps[1][1].set_params(tokenizer=tokenize_ws) model.fit(X_train, y_train)"
"tokenize_ws = generate_tokenizer("" "") model.steps[1][1].set_params(tokenizer=tokenize_ws) model.fit(X_train, y_train)"	y_pred = model.predict(X_test)
y_pred = model.predict(X_test)	f1_score(y_test, y_pred, average='macro')
f1_score(y_test, y_pred, average='macro')	NB_END
import numpy as np  import pandas as pd   import os  from IPython.core.display import display from sklearn.datasets import fetch_20newsgroups	competition_path = '20-newsgroups-ciphertext-challenge'
competition_path = '20-newsgroups-ciphertext-challenge'	test = pd.read_csv('../input/' + competition_path + '/test.csv').rename(columns={'ciphertext' : 'text'})
test = pd.read_csv('../input/' + competition_path + '/test.csv').rename(columns={'ciphertext' : 'text'})	train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')
train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')	df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)
df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)	df_p['target'] = df_p['target'].astype(np.int8)
df_p['target'] = df_p['target'].astype(np.int8)	def find_targets(p_indexes_set):     return np.sort(df_p.loc[p_indexes_set]['target'].unique())
def find_targets(p_indexes_set):     return np.sort(df_p.loc[p_indexes_set]['target'].unique())	pickle_1_path = '../input/test-1/'  df_p_indexes_1 = pd.read_pickle(pickle_1_path + 'df_p_indexes-1.pkl') df_p_indexes_1['target'] = df_p_indexes_1['p_indexes'].map(find_targets) display(df_p_indexes_1[df_p_indexes_1['target'].map(len) > 1 ])
pickle_1_path = '../input/test-1/'  df_p_indexes_1 = pd.read_pickle(pickle_1_path + 'df_p_indexes-1.pkl') df_p_indexes_1['target'] = df_p_indexes_1['p_indexes'].map(find_targets) display(df_p_indexes_1[df_p_indexes_1['target'].map(len) > 1 ])	test = test.join(df_p_indexes_1[['target']])
test = test.join(df_p_indexes_1[['target']])	pickle_2_path = '../input/test-2/'  df_p_indexes_2 = pd.read_pickle(pickle_2_path + 'df_p_indexes-2.pkl') df_p_indexes_2['target'] = df_p_indexes_2['p_indexes'].map(find_targets) display(df_p_indexes_2[df_p_indexes_2['target'].map(len) > 1 ])
pickle_2_path = '../input/test-2/'  df_p_indexes_2 = pd.read_pickle(pickle_2_path + 'df_p_indexes-2.pkl') df_p_indexes_2['target'] = df_p_indexes_2['p_indexes'].map(find_targets) display(df_p_indexes_2[df_p_indexes_2['target'].map(len) > 1 ])	test.loc[df_p_indexes_2.index,'target'] = df_p_indexes_2['target']
test.loc[df_p_indexes_2.index,'target'] = df_p_indexes_2['target']	pickle_3_path = '../input/cipher-3-solution/'  df_p_indexes_3 = pd.read_pickle(pickle_3_path + 'test_3.pkl') df_p_indexes_3['target'] = df_p_indexes_3['p_indexes'].map(find_targets) display(df_p_indexes_3[df_p_indexes_3['target'].map(len) > 1 ])
pickle_3_path = '../input/cipher-3-solution/'  df_p_indexes_3 = pd.read_pickle(pickle_3_path + 'test_3.pkl') df_p_indexes_3['target'] = df_p_indexes_3['p_indexes'].map(find_targets) display(df_p_indexes_3[df_p_indexes_3['target'].map(len) > 1 ])	test.loc[df_p_indexes_3.index,'target'] = df_p_indexes_3['target']
test.loc[df_p_indexes_3.index,'target'] = df_p_indexes_3['target']	test.head()
test.head()	test[test['target'].isnull()]['difficulty'].unique()
test[test['target'].isnull()]['difficulty'].unique()	test.to_pickle('test_123.pkl')
test.to_pickle('test_123.pkl')	test.loc[test['difficulty'] < 4,'target'] = test.loc[test['difficulty'] < 4,'target'].map(lambda x: x[0]) #You can implement the target choice you want within the possible targets here
test.loc[test['difficulty'] < 4,'target'] = test.loc[test['difficulty'] < 4,'target'].map(lambda x: x[0]) #You can implement the target choice you want within the possible targets here	test.to_pickle('test_sub.pkl')
test.to_pickle('test_sub.pkl')	NB_END
import numpy as np import pandas as pd  import os import re	from sklearn.datasets import fetch_20newsgroups from collections import Counter
from sklearn.datasets import fetch_20newsgroups from collections import Counter	"print(os.listdir(""../input""))"
"print(os.listdir(""../input""))"	train = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/train.csv') test = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/test.csv')
train = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/train.csv') test = pd.read_csv('../input/20-newsgroups-ciphertext-challenge/test.csv')	train_chars = train['ciphertext'].map(len).sum() test_chars = test['ciphertext'].map(len).sum()
train_chars = train['ciphertext'].map(len).sum() test_chars = test['ciphertext'].map(len).sum()	train_plain = fetch_20newsgroups(subset='train') test_plain = fetch_20newsgroups(subset='test')
train_plain = fetch_20newsgroups(subset='train') test_plain = fetch_20newsgroups(subset='test')	train_plain = pd.DataFrame(data = np.c_[train_plain['data'], train_plain['target']],                     columns= ['plaintext','target'])  test_plain = pd.DataFrame(data = np.c_[test_plain['data'], test_plain['target']],                     columns= ['plaintext','target'])
train_plain = pd.DataFrame(data = np.c_[train_plain['data'], train_plain['target']],                     columns= ['plaintext','target'])  test_plain = pd.DataFrame(data = np.c_[test_plain['data'], test_plain['target']],                     columns= ['plaintext','target'])	train_plain_chars = train_plain['plaintext'].map(len).sum() test_plain_chars = test_plain['plaintext'].map(len).sum()
train_plain_chars = train_plain['plaintext'].map(len).sum() test_plain_chars = test_plain['plaintext'].map(len).sum()	print('# of characters in train ciphertexts: {:,}'.format(train_chars)) print('# of characters in test ciphertexts: {:,}'.format(test_chars)) print('# of characters in train&test ciphertexts: {:,}'.format(train_chars + test_chars))
print('# of characters in train ciphertexts: {:,}'.format(train_chars)) print('# of characters in test ciphertexts: {:,}'.format(test_chars)) print('# of characters in train&test ciphertexts: {:,}'.format(train_chars + test_chars))	print('# of characters in train plaintexts: {:,}'.format(train_plain_chars)) print('# of characters in test plaintexts: {:,}'.format(test_plain_chars)) print('# of characters in train&test plaintexts: {:,}'.format(train_plain_chars + test_plain_chars))
print('# of characters in train plaintexts: {:,}'.format(train_plain_chars)) print('# of characters in test plaintexts: {:,}'.format(test_plain_chars)) print('# of characters in train&test plaintexts: {:,}'.format(train_plain_chars + test_plain_chars))	difficulty_level = 1  df_c = pd.concat([train[train['difficulty']==difficulty_level][['ciphertext']],                   test[test['difficulty']==difficulty_level][['ciphertext']]], axis = 0).rename(columns={'ciphertext':'text'})  df_p = pd.concat([train_plain[['plaintext']],                   test_plain[['plaintext']]], axis = 0).rename(columns={'plaintext':'text'})
difficulty_level = 1  df_c = pd.concat([train[train['difficulty']==difficulty_level][['ciphertext']],                   test[test['difficulty']==difficulty_level][['ciphertext']]], axis = 0).rename(columns={'ciphertext':'text'})  df_p = pd.concat([train_plain[['plaintext']],                   test_plain[['plaintext']]], axis = 0).rename(columns={'plaintext':'text'})	def char_freqs(df):     text = ''.join(df['text'])     freqs = 100 * pd.Series(Counter(text)) / len(text)     freqs.sort_values(ascending= False, inplace=True)     return(freqs)
def char_freqs(df):     text = ''.join(df['text'])     freqs = 100 * pd.Series(Counter(text)) / len(text)     freqs.sort_values(ascending= False, inplace=True)     return(freqs)	c_freqs = char_freqs(df_c) p_freqs = char_freqs(df_p)
c_freqs = char_freqs(df_c) p_freqs = char_freqs(df_p)	c_freqs.head()
c_freqs.head()	p_freqs.head()
p_freqs.head()	c_freqs.describe()
c_freqs.describe()	p_freqs.describe()
p_freqs.describe()	def filter_freqs(freqs, freqs_thresh, freqs_sep_thresh):     freqs = freqs[freqs > freqs_thresh]     freqs_sep = pd.concat([freqs.diff(-1),-freqs.diff(1)],axis=1).apply(np.nanmin, axis=1)     freqs_sep = freqs_sep[freqs_sep > freqs_sep_thresh]     return(freqs[freqs_sep.index])
def filter_freqs(freqs, freqs_thresh, freqs_sep_thresh):     freqs = freqs[freqs > freqs_thresh]     freqs_sep = pd.concat([freqs.diff(-1),-freqs.diff(1)],axis=1).apply(np.nanmin, axis=1)     freqs_sep = freqs_sep[freqs_sep > freqs_sep_thresh]     return(freqs[freqs_sep.index])	c_freqs = filter_freqs(c_freqs, 0.25, 0.10) p_freqs = filter_freqs(p_freqs, 0.25, 0.10)
c_freqs = filter_freqs(c_freqs, 0.25, 0.10) p_freqs = filter_freqs(p_freqs, 0.25, 0.10)	char_dic = pd.concat([pd.DataFrame(c_freqs).reset_index().rename(columns={'index' : 'c',0:'c_freq'}),                       pd.DataFrame(p_freqs).reset_index().rename(columns={'index' : 'p',0:'p_freq'})],                       axis=1).dropna()
char_dic = pd.concat([pd.DataFrame(c_freqs).reset_index().rename(columns={'index' : 'c',0:'c_freq'}),                       pd.DataFrame(p_freqs).reset_index().rename(columns={'index' : 'p',0:'p_freq'})],                       axis=1).dropna()	char_dic
char_dic	ciphertext = '1'.join(df_c['text']) plaintext = ' '.join(df_p['text'])
ciphertext = '1'.join(df_c['text']) plaintext = ' '.join(df_p['text'])	def word_freqs(s, seps):     words = list(filter(None, re.split('[' + ''.join(seps) + ']+',s)))     freqs = pd.Series(words).value_counts()     freqs = freqs.reset_index().rename(columns={'index' : 'word', 0:'count'})     freqs['word_len'] = freqs['word'].map(len)     freqs['abs_freq'] = 100 * freqs['count'] / len(words)     freqs = pd.merge(freqs,                      freqs.groupby('word_len')[['count']].sum().reset_index().rename(columns={'count' : 'word_len_count'}),                      on='word_len')     freqs['rel_freq'] = 100 * freqs['count'] / freqs['word_len_count']     freqs.sort_values(by='abs_freq',ascending=False,inplace=True)     return(freqs)
def word_freqs(s, seps):     words = list(filter(None, re.split('[' + ''.join(seps) + ']+',s)))     freqs = pd.Series(words).value_counts()     freqs = freqs.reset_index().rename(columns={'index' : 'word', 0:'count'})     freqs['word_len'] = freqs['word'].map(len)     freqs['abs_freq'] = 100 * freqs['count'] / len(words)     freqs = pd.merge(freqs,                      freqs.groupby('word_len')[['count']].sum().reset_index().rename(columns={'count' : 'word_len_count'}),                      on='word_len')     freqs['rel_freq'] = 100 * freqs['count'] / freqs['word_len_count']     freqs.sort_values(by='abs_freq',ascending=False,inplace=True)     return(freqs)	c_words = word_freqs(ciphertext,['1','s'])
c_words = word_freqs(ciphertext,['1','s'])	p_words = word_freqs(plaintext,[' ','\ '])
p_words = word_freqs(plaintext,[' ','\ '])	c_words.head(15)
c_words.head(15)	p_words.head(15)
p_words.head(15)	c_words.describe()
c_words.describe()	p_words.describe()
p_words.describe()	c_freqs = filter_freqs(c_words.set_index('word')['abs_freq'], 0.25, 0.05) p_freqs = filter_freqs(p_words.set_index('word')['abs_freq'], 0.25, 0.05)
c_freqs = filter_freqs(c_words.set_index('word')['abs_freq'], 0.25, 0.05) p_freqs = filter_freqs(p_words.set_index('word')['abs_freq'], 0.25, 0.05)	words_dic = pd.concat([pd.DataFrame(c_freqs).reset_index().rename(columns={'index' : 'c',0:'c_freq'}),                       pd.DataFrame(p_freqs).reset_index().rename(columns={'index' : 'p',0:'p_freq'})],                       axis=1).dropna()
words_dic = pd.concat([pd.DataFrame(c_freqs).reset_index().rename(columns={'index' : 'c',0:'c_freq'}),                       pd.DataFrame(p_freqs).reset_index().rename(columns={'index' : 'p',0:'p_freq'})],                       axis=1).dropna()	words_dic
words_dic	words_dic = words_dic.loc[words_dic.iloc[:,0].map(len) == words_dic.iloc[:,2].map(len)]
words_dic = words_dic.loc[words_dic.iloc[:,0].map(len) == words_dic.iloc[:,2].map(len)]	def dico_update(c_list, p_list, dico):     for i, w in enumerate(c_list):         for j, c in enumerate(w):             if c in dico:                 assert(dico[c] == p_list[i][j])             elif p_list[i][j] in dico.values():                 assert(c == p_list[i][j])             else:                 dico[c] =  p_list[i][j]     return(dico)
def dico_update(c_list, p_list, dico):     for i, w in enumerate(c_list):         for j, c in enumerate(w):             if c in dico:                 assert(dico[c] == p_list[i][j])             elif p_list[i][j] in dico.values():                 assert(c == p_list[i][j])             else:                 dico[c] =  p_list[i][j]     return(dico)	cp_dico = dico_update(words_dic.iloc[:,0].values,words_dic.iloc[:,2].values,dict(zip(''.join(char_dic['c']), ''.join(char_dic['p']))))
cp_dico = dico_update(words_dic.iloc[:,0].values,words_dic.iloc[:,2].values,dict(zip(''.join(char_dic['c']), ''.join(char_dic['p']))))	cp_dico
cp_dico	translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	c_words.head()
c_words.head()	word_len = 3
word_len = 3	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	cp_dico['o'] = 'y' cp_dico['c'] = 'u' cp_dico['_'] = 'c' cp_dico['{'] = 'T' cp_dico['\\x03'] = 'b' 
cp_dico['o'] = 'y' cp_dico['c'] = 'u' cp_dico['_'] = 'c' cp_dico['{'] = 'T' cp_dico['\\x03'] = 'b' 	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 4
word_len = 4	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	cp_dico['z'] = 'v' cp_dico['a'] = 'i' cp_dico['W'] = 'w' cp_dico['-'] = 'm'
cp_dico['z'] = 'v' cp_dico['a'] = 'i' cp_dico['W'] = 'w' cp_dico['-'] = 'm'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 5
word_len = 5	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	cp_dico['*'] = 'F' cp_dico['G'] = ':' cp_dico[';'] = '\\'' cp_dico['f'] = 'k'
cp_dico['*'] = 'F' cp_dico['G'] = ':' cp_dico[';'] = '\\'' cp_dico['f'] = 'k'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 6
word_len = 6	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	cp_dico['F'] = 'L' cp_dico['\\''] = 'p' cp_dico['d'] = 'g'
cp_dico['F'] = 'L' cp_dico['\\''] = 'p' cp_dico['d'] = 'g'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 8
word_len = 8	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10)	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10).iloc[1,0][0]
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(10).iloc[1,0][0]	cp_dico['>'] = 'S' cp_dico['X'] = 'j' cp_dico['d'] = 'g' cp_dico['\\x1a'] = 'q' 
cp_dico['>'] = 'S' cp_dico['X'] = 'j' cp_dico['d'] = 'g' cp_dico['\\x1a'] = 'q' 	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 9
word_len = 9	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	cp_dico['w'] = 'K' cp_dico['2'] = 'C' cp_dico[':'] = 'I' cp_dico['9'] = 'P'
cp_dico['w'] = 'K' cp_dico['2'] = 'C' cp_dico[':'] = 'I' cp_dico['9'] = 'P'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 10
word_len = 10	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	cp_dico['@'] = 'U' cp_dico['x'] = 'D' cp_dico['+'] = 'x'
cp_dico['@'] = 'U' cp_dico['x'] = 'D' cp_dico['+'] = 'x'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 11
word_len = 11	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	cp_dico['T'] = ',' cp_dico['%'] = 'O'
cp_dico['T'] = ',' cp_dico['%'] = 'O'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 12
word_len = 12	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[1,0][-1]
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[1,0][-1]	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[3,0][3]
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[3,0][3]	cp_dico['\\x08'] = '.' cp_dico['q'] = '-' cp_dico['\\x1e'] = 'R'  cp_dico['h'] = 'z'
cp_dico['\\x08'] = '.' cp_dico['q'] = '-' cp_dico['\\x1e'] = 'R'  cp_dico['h'] = 'z'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 13
word_len = 13	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[2,0][2]
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15).iloc[2,0][2]	cp_dico['!'] = 'X' cp_dico['\\x7f'] = 'N' cp_dico['/'] = 'A'
cp_dico['!'] = 'X' cp_dico['\\x7f'] = 'N' cp_dico['/'] = 'A'	ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text']) c_words = word_freqs(ciphertext,['1','s']) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 14
word_len = 14	c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
c_words[c_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)
p_words[p_words['word_len'] == word_len].sort_values(by='rel_freq',ascending=False).head(15)	cp_dico['b'] = '@'
cp_dico['b'] = '@'	ciphertext = '1'.join(df_c['text'])  undecoded = ''.join(set(ciphertext).difference(set(cp_dico.keys()))) set_undecoded = set(undecoded)  c_words = word_freqs(ciphertext,['1','s']) c_words['to_decode'] = c_words['word'].map(lambda x: any((c in set(undecoded)) for c in x)) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))
ciphertext = '1'.join(df_c['text'])  undecoded = ''.join(set(ciphertext).difference(set(cp_dico.keys()))) set_undecoded = set(undecoded)  c_words = word_freqs(ciphertext,['1','s']) c_words['to_decode'] = c_words['word'].map(lambda x: any((c in set(undecoded)) for c in x)) translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) c_words['word'] = c_words['word'].map(lambda x: x.translate(translation))	word_len = 6
word_len = 6	c_words[(c_words['word_len'] == word_len) & (c_words['to_decode'] == True)].sort_values(by='rel_freq',ascending=False).head(15)
c_words[(c_words['word_len'] == word_len) & (c_words['to_decode'] == True)].sort_values(by='rel_freq',ascending=False).head(15)	"cp_dico[\'}\'] = \'J\' cp_dico[\'J\'] = \'B\' cp_dico[\'e\'] = \'M\' cp_dico[\'""\'] = \'G\'"
"cp_dico[\'}\'] = \'J\' cp_dico[\'J\'] = \'B\' cp_dico[\'e\'] = \'M\' cp_dico[\'""\'] = \'G\'"	target = 1
target = 1	df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))
df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))	df_c.head()
df_c.head()	df_p = pd.concat([train_plain[train_plain['target'] == str(target)],                   test_plain[test_plain['target'] == str(target)]], axis = 0)
df_p = pd.concat([train_plain[train_plain['target'] == str(target)],                   test_plain[test_plain['target'] == str(target)]], axis = 0)	def complete_dico(c,p):     cp_dico_temp = {}     for i,cc in enumerate(c.replace('\  ','\ ')):         if cc != p[i]:             if cc not in cp_dico_temp:                 cp_dico_temp[cc] = p[i]                 print('cp_dico[\\'{}\\'] = \\'{}\\''.format(cc,p[i]))
def complete_dico(c,p):     cp_dico_temp = {}     for i,cc in enumerate(c.replace('\  ','\ ')):         if cc != p[i]:             if cc not in cp_dico_temp:                 cp_dico_temp[cc] = p[i]                 print('cp_dico[\\'{}\\'] = \\'{}\\''.format(cc,p[i]))	c_index = 398 c = df_c.loc[c_index,'ciphertext'] print(c)
c_index = 398 c = df_c.loc[c_index,'ciphertext'] print(c)	c_sample = 'dfr@usna.navy.mil'
c_sample = 'dfr@usna.navy.mil'	df_p.head()
df_p.head()	df_p[df_p['plaintext'].str.contains(c_sample)]
df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 5620 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 5620 p = df_p.loc[p_index,'plaintext'] print(p)	complete_dico(c,p)
complete_dico(c,p)	c
c	p
p	cp_dico['|'] = '(' cp_dico['y'] = ')' cp_dico['g'] = 'H' cp_dico['u'] = '3' cp_dico['\\x06'] = '7' cp_dico['\ '] = '5' cp_dico[','] = '4' cp_dico['L'] = '1' cp_dico['\\\\'] = '0' cp_dico['n'] = '8' cp_dico['['] = '>' cp_dico[r' '] = '<'
cp_dico['|'] = '(' cp_dico['y'] = ')' cp_dico['g'] = 'H' cp_dico['u'] = '3' cp_dico['\\x06'] = '7' cp_dico['\ '] = '5' cp_dico[','] = '4' cp_dico['L'] = '1' cp_dico['\\\\'] = '0' cp_dico['n'] = '8' cp_dico['['] = '>' cp_dico[r' '] = '<'	df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))
df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))	df_c.head()
df_c.head()	c_index = 146 c = df_c.loc[c_index,'ciphertext'] print(c)
c_index = 146 c = df_c.loc[c_index,'ciphertext'] print(c)	c_sample = 'now back to lurking'
c_sample = 'now back to lurking'	df_p[df_p['plaintext'].str.contains(c_sample)]
df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 1156 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 1156 p = df_p.loc[p_index,'plaintext'] print(p)	p = p[p.find(c_sample):]
p = p[p.find(c_sample):]	complete_dico(c,p)
complete_dico(c,p)	cp_dico['r'] = '&' cp_dico['l'] = 'W' cp_dico['\\x18'] = 'V'
cp_dico['r'] = '&' cp_dico['l'] = 'W' cp_dico['\\x18'] = 'V'	df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))
df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))	df_c.head(15)
df_c.head(15)	c_index = 807 c = df_c.loc[c_index,'ciphertext'] print(c)
c_index = 807 c = df_c.loc[c_index,'ciphertext'] print(c)	c_sample = 'bpirenne@eso.org'
c_sample = 'bpirenne@eso.org'	df_p[df_p['plaintext'].str.contains(c_sample)]
df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 3864 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 3864 p = df_p.loc[p_index,'plaintext'] print(p)	c_sample = 'iga computer.' p = p[p.find(c_sample):] print(p)
c_sample = 'iga computer.' p = p[p.find(c_sample):] print(p)	complete_dico(c,p)
complete_dico(c,p)	cp_dico['U'] = '[' cp_dico['i'] = ']' cp_dico['3'] = ';' cp_dico[' '] = '<' cp_dico['~'] = '+' cp_dico['<'] = '9' cp_dico['H'] = '2' cp_dico['5'] = '6'
cp_dico['U'] = '[' cp_dico['i'] = ']' cp_dico['3'] = ';' cp_dico[' '] = '<' cp_dico['~'] = '+' cp_dico['<'] = '9' cp_dico['H'] = '2' cp_dico['5'] = '6'	df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))
df_c = train[(train['difficulty'] == difficulty_level) & (train['target'] == target)].copy() translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c['ciphertext'] = df_c['ciphertext'].map(lambda x: x.translate(translation))	df_c.tail(15)
df_c.tail(15)	c_index = 38295 c = df_c.loc[c_index,'ciphertext'] print(c)
c_index = 38295 c = df_c.loc[c_index,'ciphertext'] print(c)	c_sample = 'ones I know about are from Maximum Strategy'
c_sample = 'ones I know about are from Maximum Strategy'	df_p[df_p['plaintext'].str.contains(c_sample)]
df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 3474 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 3474 p = df_p.loc[p_index,'plaintext'] print(p)	c_sample = 'y that can do 180MB' p = p[p.find(c_sample):] print(p)
c_sample = 'y that can do 180MB' p = p[p.find(c_sample):] print(p)	complete_dico(c,p)
complete_dico(c,p)	cp_dico['?'] = '/'
cp_dico['?'] = '/'	df_c = train[(train['difficulty'] == difficulty_level)].copy() c_alphabet = pd.Series(Counter(''.join(df_c['ciphertext']))) c_alphabet.shape
df_c = train[(train['difficulty'] == difficulty_level)].copy() c_alphabet = pd.Series(Counter(''.join(df_c['ciphertext']))) c_alphabet.shape	len(cp_dico)
len(cp_dico)	print('We have manually decoded {:.0%} of the cipher #1'.format(len(cp_dico)/c_alphabet.shape[0]))
print('We have manually decoded {:.0%} of the cipher #1'.format(len(cp_dico)/c_alphabet.shape[0]))	df_p = pd.concat([train_plain,test_plain], axis = 0)
df_p = pd.concat([train_plain,test_plain], axis = 0)	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('4')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('4')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_sample = 'Imagination is more important than knowledge' df_c_focus[df_c_focus['ciphertext'].str.contains(c_sample)]
c_sample = 'Imagination is more important than knowledge' df_c_focus[df_c_focus['ciphertext'].str.contains(c_sample)]	c_index = 34448 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 34448 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'Imagination is more important than knowledge'
c_sample = 'Imagination is more important than knowledge'	df_p[df_p['plaintext'].str.contains(c_sample)]
df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 7391 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 7391 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1]
p = df_p.loc[p_index,'plaintext'].iloc[1]	print(p)
print(p)	complete_dico(c,p[-350:]) 
complete_dico(c,p[-350:]) 	cp_dico['4'] = '|' cp_dico['Z'] = '=' cp_dico['.'] = '~' cp_dico['m'] = '\\\\' cp_dico['\\x10'] = 'Y' cp_dico[')'] = '_'
cp_dico['4'] = '|' cp_dico['Z'] = '=' cp_dico['.'] = '~' cp_dico['m'] = '\\\\' cp_dico['\\x10'] = 'Y' cp_dico[')'] = '_'	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('S')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('S')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 15690 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 15690 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'I know there are lots of graphics-board companies out' df_p[df_p['plaintext'].str.contains(c_sample)]
c_sample = 'I know there are lots of graphics-board companies out' df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 9998 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 9998 p = df_p.loc[p_index,'plaintext'] print(p)	complete_dico(c,p) 
complete_dico(c,p) 	p
p	c
c	"cp_dico[\'S\'] = \'\\x08\' cp_dico[\'6\'] = \'""\' cp_dico[\'&\'] = \'?\'"
"cp_dico[\'S\'] = \'\\x08\' cp_dico[\'6\'] = \'""\' cp_dico[\'&\'] = \'?\'"	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('I')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('I')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 1501 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 1501 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'got the same error when I tried to build' df_p[df_p['plaintext'].str.contains(c_sample)]
c_sample = 'got the same error when I tried to build' df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 9368 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 9368 p = df_p.loc[p_index,'plaintext'] print(p)	complete_dico(c,p[862:])  
complete_dico(c,p[862:])  	cp_dico['\\x1c'] = '*' cp_dico['Q'] = '\ ' cp_dico['I'] = '}'
cp_dico['\\x1c'] = '*' cp_dico['Q'] = '\ ' cp_dico['I'] = '}'	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('B')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('B')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 37028 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 37028 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'to think about for the remote machine' df_p[df_p['plaintext'].str.contains(c_sample)]
c_sample = 'to think about for the remote machine' df_p[df_p['plaintext'].str.contains(c_sample)]	p_index = 2593 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 2593 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1] print(p)
p = df_p.loc[p_index,'plaintext'].iloc[1] print(p)	complete_dico(c,p[23543:])   
complete_dico(c,p[23543:])   	cp_dico['`'] = '#' cp_dico['B'] = '$' cp_dico['P'] = '!' cp_dico['k'] = '{' cp_dico['Y'] = '`'
cp_dico['`'] = '#' cp_dico['B'] = '$' cp_dico['P'] = '!' cp_dico['k'] = '{' cp_dico['Y'] = '`'	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('V')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('V')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 3 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 3 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'also hearty proponents of' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'also hearty proponents of' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 928 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 928 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[0] print(p)
p = df_p.loc[p_index,'plaintext'].iloc[0] print(p)	complete_dico(c,p[2617:])   
complete_dico(c,p[2617:])   	cp_dico['V'] = 'Q'
cp_dico['V'] = 'Q'	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('D')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('D')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 4840 c = df_c_focus.loc[c_index,'ciphertext'] c
c_index = 4840 c = df_c_focus.loc[c_index,'ciphertext'] c	c_sample = 'out the environment and their future' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'out the environment and their future' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 11142 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 11142 p = df_p.loc[p_index,'plaintext'] print(p)	p[2642:]
p[2642:]	cp_dico['D'] = '\\x0c'   
cp_dico['D'] = '\\x0c'   	undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)
undecoded = set(c_alphabet.index).difference(cp_dico.keys()) undecoded = list(undecoded) print(undecoded)	df_c_focus = df_c[df_c['ciphertext'].str.contains('=')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('=')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 31228 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 31228 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'Kyle P Hunter' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'Kyle P Hunter' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 5507 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 5507 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1]
p = df_p.loc[p_index,'plaintext'].iloc[1]	print(p)
print(p)	complete_dico(c,p)   
complete_dico(c,p)   	cp_dico['='] = '\\x10'
cp_dico['='] = '\\x10'	df_c_focus = df_c[df_c['ciphertext'].str.contains('\\$')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('\\$')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 11727 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 11727 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'georgel@NeoSoft.com' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'georgel@NeoSoft.com' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 5150 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 5150 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1]
p = df_p.loc[p_index,'plaintext'].iloc[1]	complete_dico(c,p[292:])   
complete_dico(c,p[292:])   	cp_dico['$'] = '\\x02'
cp_dico['$'] = '\\x02'	df_c_focus = df_c[df_c['ciphertext'].str.contains('K')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('K')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 232 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 232 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'From: szh@zcon.com' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'From: szh@zcon.com' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 7361 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 7361 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1]
p = df_p.loc[p_index,'plaintext'].iloc[1]	complete_dico(c,p)   
complete_dico(c,p)   	cp_dico['K'] = 'Z'
cp_dico['K'] = 'Z'	df_c_focus = df_c[df_c['ciphertext'].str.contains('p')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('p')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 9981 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 9981 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'Since it is a Life Time membership, you won' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'Since it is a Life Time membership, you won' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 4938 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 4938 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[0]
p = df_p.loc[p_index,'plaintext'].iloc[0]	print(p)
print(p)	cp_dico['p'] = '\\x1e'  
cp_dico['p'] = '\\x1e'  	df_c_focus = df_c[df_c['ciphertext'].str.contains('\\(')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('\\(')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 38567 c = df_c_focus.loc[c_index,'ciphertext'] print(c)
c_index = 38567 c = df_c_focus.loc[c_index,'ciphertext'] print(c)	c_sample = 'After all the space walking,  they are going to  re-boost the HST' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'After all the space walking,  they are going to  re-boost the HST' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 810 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 810 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[1]
p = df_p.loc[p_index,'plaintext'].iloc[1]	print(p)
print(p)	cp_dico[r'('] = '^'
cp_dico[r'('] = '^'	df_c_focus = df_c[df_c['ciphertext'].str.contains('\\x0c')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('\\x0c')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 38788 c = df_c_focus.loc[c_index,'ciphertext']
c_index = 38788 c = df_c_focus.loc[c_index,'ciphertext']	c
c	c_sample = 'From: dbl@visual.com' df_p[df_p['plaintext'].str.contains(c_sample)] 
c_sample = 'From: dbl@visual.com' df_p[df_p['plaintext'].str.contains(c_sample)] 	p_index = 1459 p = df_p.loc[p_index,'plaintext'] print(p)
p_index = 1459 p = df_p.loc[p_index,'plaintext'] print(p)	p = df_p.loc[p_index,'plaintext'].iloc[0]
p = df_p.loc[p_index,'plaintext'].iloc[0]	p
p	cp_dico['\\x0c'] = r'%'
cp_dico['\\x0c'] = r'%'	df_c_focus = df_c[df_c['ciphertext'].str.contains('E')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus
df_c_focus = df_c[df_c['ciphertext'].str.contains('E')].copy()  translation = str.maketrans(''.join(cp_dico.keys()), ''.join(cp_dico.values())) df_c_focus['ciphertext'] = df_c_focus['ciphertext'].map(lambda x: x.translate(translation))  df_c_focus	c_index = 392 c = df_c_focus.loc[c_index,'ciphertext'] c
c_index = 392 c = df_c_focus.loc[c_index,'ciphertext'] c	cp_dico['E'] = 'E'
cp_dico['E'] = 'E'	print('We have manually decoded {:.2%} of the cipher #1'.format(len(cp_dico)/c_alphabet.shape[0]))
print('We have manually decoded {:.2%} of the cipher #1'.format(len(cp_dico)/c_alphabet.shape[0]))	cipher1_df = pd.DataFrame(list(cp_dico.values()),index=list(cp_dico.keys()),columns=['plain']).reset_index().rename(columns={'index' : 'cipher'})
cipher1_df = pd.DataFrame(list(cp_dico.values()),index=list(cp_dico.keys()),columns=['plain']).reset_index().rename(columns={'index' : 'cipher'})	cipher1_df.head()
cipher1_df.head()	cipher1_df.to_csv('cipher1_map.csv', index=False)
cipher1_df.to_csv('cipher1_map.csv', index=False)	cipher2_df = pd.read_csv('../input/cipher-2-full-solution/cipher2_map.csv')
cipher2_df = pd.read_csv('../input/cipher-2-full-solution/cipher2_map.csv')	cipher2_df.to_csv('cipher2_map.csv', index=False)
import numpy as np  import pandas as pd   import os import re  from collections import Counter from dask import delayed, compute from dask.diagnostics import ProgressBar from fuzzywuzzy import fuzz, process from IPython.core.display import display from itertools import cycle, islice from sklearn.datasets import fetch_20newsgroups	ProgressBar().register()
ProgressBar().register()	chunk_size = 300 pd.options.display.max_columns = chunk_size pd.options.display.max_rows = chunk_size
chunk_size = 300 pd.options.display.max_columns = chunk_size pd.options.display.max_rows = chunk_size	train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')
train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')	df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)
df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)	df_p['target'] = df_p['target'].astype(np.int8)
df_p['target'] = df_p['target'].astype(np.int8)	df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  ')) df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])
df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  ')) df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])	p_text_chunk_list = [] p_text_index_list = []  for p_index, p_row in df_p.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)
p_text_chunk_list = [] p_text_index_list = []  for p_index, p_row in df_p.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)	df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list}) df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')  df_p_chunked_list = [] for i in np.sort(df_p_chunked['target'].unique()):     df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])
df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list}) df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')  df_p_chunked_list = [] for i in np.sort(df_p_chunked['target'].unique()):     df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])	competition_path = '../input/20-newsgroups-ciphertext-challenge/'
competition_path = '../input/20-newsgroups-ciphertext-challenge/'	train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'}) test = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})
train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'}) test = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})	difficulty_level = 3 train = train[train['difficulty'] == difficulty_level] test = test[test['difficulty'] == difficulty_level]
difficulty_level = 3 train = train[train['difficulty'] == difficulty_level] test = test[test['difficulty'] == difficulty_level]	cipher_path = '../input/cipher-1-cipher-2-full-solutions/' cipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv') translation_2 = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain']))
cipher_path = '../input/cipher-1-cipher-2-full-solutions/' cipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv') translation_2 = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain']))	train['t_text'] = train.apply(lambda x: x['text'].translate(translation_2), axis=1)
train['t_text'] = train.apply(lambda x: x['text'].translate(translation_2), axis=1)	df_p_extract = df_p[df_p['text'].str.startswith('From:')].copy()
df_p_extract = df_p[df_p['text'].str.startswith('From:')].copy()	df_p_extract['text'] = df_p_extract['text'].map(lambda x: x[:300])
df_p_extract['text'] = df_p_extract['text'].map(lambda x: x[:300])	df_p_extract['p_len'] = df_p_extract['text'].map(len)
df_p_extract['p_len'] = df_p_extract['text'].map(len)	df_p_list = [] for i in np.sort(df_p_extract['target'].unique()):     df_p_list.append(df_p_extract[df_p_extract['target'] == i])
df_p_list = [] for i in np.sort(df_p_extract['target'].unique()):     df_p_list.append(df_p_extract[df_p_extract['target'] == i])	df_c = train[train['t_text'].str.startswith('FrMmZ')].copy()
df_c = train[train['t_text'].str.startswith('FrMmZ')].copy()	len(df_c)
len(df_c)	def find_match(idx):     target = df_c.loc[idx,'target']     t_text = df_c.loc[idx,'t_text']     t_len = len(t_text)     df_p_match = df_p_list[target][df_p_list[target]['p_len']==t_len]     p_text, fscore, p_index =  process.extractOne(t_text, df_p_match['text'], scorer = fuzz.token_set_ratio)     return(p_text, fscore, p_index)
def find_match(idx):     target = df_c.loc[idx,'target']     t_text = df_c.loc[idx,'t_text']     t_len = len(t_text)     df_p_match = df_p_list[target][df_p_list[target]['p_len']==t_len]     p_text, fscore, p_index =  process.extractOne(t_text, df_p_match['text'], scorer = fuzz.token_set_ratio)     return(p_text, fscore, p_index)	par_compute = [delayed(find_match)(idx) for idx in df_c.index] cp_matches = compute(*par_compute, scheduler='processes')
par_compute = [delayed(find_match)(idx) for idx in df_c.index] cp_matches = compute(*par_compute, scheduler='processes')	cp_matches = pd.DataFrame(list(cp_matches),columns=['p_text','fscore','p_index'])
cp_matches = pd.DataFrame(list(cp_matches),columns=['p_text','fscore','p_index'])	df_c = df_c[['target','text','t_text']].reset_index().rename(columns={'index' : 'c_index', 'text' : 'c_text'}) df_c = pd.concat([df_c,cp_matches],axis=1) df_c.sort_values(by='fscore',ascending=False,inplace=True)
df_c = df_c[['target','text','t_text']].reset_index().rename(columns={'index' : 'c_index', 'text' : 'c_text'}) df_c = pd.concat([df_c,cp_matches],axis=1) df_c.sort_values(by='fscore',ascending=False,inplace=True)	df_c.head()
df_c.head()	df_c_copy = df_c.copy()
df_c_copy = df_c.copy()	def word_freqs(s, seps):     words = list(filter(None, re.split('[' + ''.join(seps) + ']+',s)))     freqs = pd.Series(words).value_counts()     freqs = freqs.reset_index().rename(columns={'index' : 'word', 0:'count'})     freqs['word_len'] = freqs['word'].map(len)     freqs['abs_freq'] = 100 * freqs['count'] / len(words)     freqs = pd.merge(freqs,                      freqs.groupby('word_len')[['count']].sum().reset_index().rename(columns={'count' : 'word_len_count'}),                      on='word_len')     freqs['rel_freq'] = 100 * freqs['count'] / freqs['word_len_count']     freqs.sort_values(by='abs_freq',ascending=False,inplace=True)     return(freqs)
def word_freqs(s, seps):     words = list(filter(None, re.split('[' + ''.join(seps) + ']+',s)))     freqs = pd.Series(words).value_counts()     freqs = freqs.reset_index().rename(columns={'index' : 'word', 0:'count'})     freqs['word_len'] = freqs['word'].map(len)     freqs['abs_freq'] = 100 * freqs['count'] / len(words)     freqs = pd.merge(freqs,                      freqs.groupby('word_len')[['count']].sum().reset_index().rename(columns={'count' : 'word_len_count'}),                      on='word_len')     freqs['rel_freq'] = 100 * freqs['count'] / freqs['word_len_count']     freqs.sort_values(by='abs_freq',ascending=False,inplace=True)     return(freqs)	plaintext = ' '.join(df_p_extract['text']) p_words = word_freqs(plaintext,[' '])
plaintext = ' '.join(df_p_extract['text']) p_words = word_freqs(plaintext,[' '])	p_words.head()
p_words.head()	words = ['Subject:','Organization','Lines:']  t_words = [r'\\s*(..bje.t.)\\s', r'\\s*(Or..n...t..n.)\\s', r'\\s*(..nes.)\\s'] #The above regular expressions have been inferred by manually looking at a few ciphertexts
words = ['Subject:','Organization','Lines:']  t_words = [r'\\s*(..bje.t.)\\s', r'\\s*(Or..n...t..n.)\\s', r'\\s*(..nes.)\\s'] #The above regular expressions have been inferred by manually looking at a few ciphertexts	for i, t in enumerate(t_words):     w = words[i]     df_c[w + '_is'] = df_c['p_text'].map(lambda x: [match.span()[0] for match in re.finditer(w, x) if match is not None])     df_c[w + '_is_t'] = df_c['t_text'].map(lambda x: [match.span(1)[0] for match in re.finditer(t, x,re.DOTALL) if match is not None])
for i, t in enumerate(t_words):     w = words[i]     df_c[w + '_is'] = df_c['p_text'].map(lambda x: [match.span()[0] for match in re.finditer(w, x) if match is not None])     df_c[w + '_is_t'] = df_c['t_text'].map(lambda x: [match.span(1)[0] for match in re.finditer(t, x,re.DOTALL) if match is not None])	df_c.head()
df_c.head()	def frequent_word_match(x):     res = True     for i, t in enumerate(t_words):         w = words[i]         res = res and (x[w + '_is'] == x[w + '_is_t'])     return(res)
def frequent_word_match(x):     res = True     for i, t in enumerate(t_words):         w = words[i]         res = res and (x[w + '_is'] == x[w + '_is_t'])     return(res)	df_c['freq_word_match'] = df_c.apply(lambda x: frequent_word_match(x),axis = 1)
df_c['freq_word_match'] = df_c.apply(lambda x: frequent_word_match(x),axis = 1)	len(df_c[~df_c['freq_word_match']])
len(df_c[~df_c['freq_word_match']])	df_crib = df_c[df_c['freq_word_match']].copy()
df_crib = df_c[df_c['freq_word_match']].copy()	len(df_crib)
len(df_crib)	def word_aligned(x):     t_text = x['t_text']     p_text = x['p_text']     t_list = t_text.split(' ')     p_list = p_text.split(' ')     return [len(s) for s in t_list] == [len(s) for s in p_list]
def word_aligned(x):     t_text = x['t_text']     p_text = x['p_text']     t_list = t_text.split(' ')     p_list = p_text.split(' ')     return [len(s) for s in t_list] == [len(s) for s in p_list]	df_crib['word_aligned'] = df_crib.apply(lambda x: word_aligned(x),axis=1)
df_crib['word_aligned'] = df_crib.apply(lambda x: word_aligned(x),axis=1)	df_crib_misaligned = df_crib[~df_crib['word_aligned']] len(df_crib_misaligned) #We may investigate these misaligned cipher & plaintext pairs later
df_crib_misaligned = df_crib[~df_crib['word_aligned']] len(df_crib_misaligned) #We may investigate these misaligned cipher & plaintext pairs later	df_crib = df_crib[df_crib['word_aligned']] len(df_crib)
df_crib = df_crib[df_crib['word_aligned']] len(df_crib)	df_crib = df_crib[['target','c_index','c_text','p_text','p_index']]
df_crib = df_crib[['target','c_index','c_text','p_text','p_index']]	df_crib.head()
df_crib.head()	translation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption translation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption
translation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption translation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption	# Checking that no characters are missing in cipher #2 map to encrypt the cipher #3 plaintexts from the crib  cipher2_plain_alphabet = set(''.join(cipher2_map['plain'])) df_crib['p_text_ok'] = df_crib['p_text'].map(lambda x: len(set(x).difference(cipher2_plain_alphabet)) == 0) len(df_crib[~df_crib['p_text_ok']])
# Checking that no characters are missing in cipher #2 map to encrypt the cipher #3 plaintexts from the crib  cipher2_plain_alphabet = set(''.join(cipher2_map['plain'])) df_crib['p_text_ok'] = df_crib['p_text'].map(lambda x: len(set(x).difference(cipher2_plain_alphabet)) == 0) len(df_crib[~df_crib['p_text_ok']])	df_crib.drop('p_text_ok',axis=1,inplace=True)
df_crib.drop('p_text_ok',axis=1,inplace=True)	df_crib['pt_text'] = df_crib['p_text'].map(lambda x: x.translate(translation_2_pt)) df_crib['ct_text'] = df_crib['c_text'].map(lambda x: x.translate(translation_2_ct))
df_crib['pt_text'] = df_crib['p_text'].map(lambda x: x.translate(translation_2_pt)) df_crib['ct_text'] = df_crib['c_text'].map(lambda x: x.translate(translation_2_ct))	df_crib.to_pickle('df_crib.pkl')
df_crib.to_pickle('df_crib.pkl')	def compare_ptc(idx):      p_text = df_crib['p_text'].loc[idx]     ct_text = df_crib['ct_text'].loc[idx]          pt_text = df_crib['pt_text'].loc[idx]     c_text = df_crib['c_text'].loc[idx]          c_split = c_text.split('8')     pt_split = pt_text.split('8')     ct_split = ct_text.split(' ')     p_split = p_text.split(' ')      return(pd.DataFrame([p_split,ct_split,pt_split, c_split],index=['p','ct','pt','c']).T)    
def compare_ptc(idx):      p_text = df_crib['p_text'].loc[idx]     ct_text = df_crib['ct_text'].loc[idx]          pt_text = df_crib['pt_text'].loc[idx]     c_text = df_crib['c_text'].loc[idx]          c_split = c_text.split('8')     pt_split = pt_text.split('8')     ct_split = ct_text.split(' ')     p_split = p_text.split(' ')      return(pd.DataFrame([p_split,ct_split,pt_split, c_split],index=['p','ct','pt','c']).T)    	def hide_ok_nok(x,pt = True, hide_ok = True):     pt_w = x['pt']     c_w = x['c']     if pt:         res = pt_w     else:         res = c_w     ok_i = set([i for i,(a,b) in enumerate(zip(pt_w,c_w)) if (ord(a) ^ ord(b) == 0)])     if hide_ok:         return(''.join(['.' if i in ok_i else res[i] for i in range(len(pt_w))]))     else:         return(''.join(['.' if i not in ok_i else res[i] for i in range(len(pt_w))]))
def hide_ok_nok(x,pt = True, hide_ok = True):     pt_w = x['pt']     c_w = x['c']     if pt:         res = pt_w     else:         res = c_w     ok_i = set([i for i,(a,b) in enumerate(zip(pt_w,c_w)) if (ord(a) ^ ord(b) == 0)])     if hide_ok:         return(''.join(['.' if i in ok_i else res[i] for i in range(len(pt_w))]))     else:         return(''.join(['.' if i not in ok_i else res[i] for i in range(len(pt_w))]))	df_crib.head(2)
df_crib.head(2)	df_z = compare_ptc(846) display(df_z.applymap(repr).T)  df_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1) df_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1) df_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1) df_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1) display(df_z.applymap(repr).T)  df_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok']) display(df_zz)  pt_h = ''.join(df_zz.loc['pt_h_hide_ok']) print('Characters to further encipher') print(repr(pt_h)) pt_h_n = ''.join(df_zz.loc['pt_h_hide_nok']) print('Characters of cipher#2 equal to cipher#3') print(repr(pt_h_n))
df_z = compare_ptc(846) display(df_z.applymap(repr).T)  df_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1) df_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1) df_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1) df_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1) display(df_z.applymap(repr).T)  df_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok']) display(df_zz)  pt_h = ''.join(df_zz.loc['pt_h_hide_ok']) print('Characters to further encipher') print(repr(pt_h)) pt_h_n = ''.join(df_zz.loc['pt_h_hide_nok']) print('Characters of cipher#2 equal to cipher#3') print(repr(pt_h_n))	df_z = compare_ptc(549) display(df_z.applymap(repr).T)  df_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1) df_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1) df_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1) df_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1) display(df_z.applymap(repr).T)  df_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok']) display(df_zz)  pt_h = ''.join(df_zz.loc['pt_h_hide_ok']) print('Characters to further encipher') print(repr(pt_h)) pt_h_n = ''.join(df_zz.loc['pt_h_hide_nok']) print('Characters of cipher#2 equal to cipher#3') print(repr(pt_h_n))
df_z = compare_ptc(549) display(df_z.applymap(repr).T)  df_z['pt_h_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True),axis=1) df_z['c_hide_ok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False),axis=1) df_z['pt_h_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=True,hide_ok=False),axis=1) df_z['c_hide_nok'] = df_z.apply(lambda x: hide_ok_nok(x,pt=False,hide_ok=False),axis=1) display(df_z.applymap(repr).T)  df_zz = pd.DataFrame([list(''.join(df_z['pt_h_hide_ok'])),list(''.join(df_z['c_hide_ok'])),list(''.join(df_z['pt_h_hide_nok'])),list(''.join(df_z['c_hide_nok']))],index=['pt_h_hide_ok','c_hide_ok','pt_h_hide_nok','c_hide_nok']) display(df_zz)  pt_h = ''.join(df_zz.loc['pt_h_hide_ok']) print('Characters to further encipher') print(repr(pt_h)) pt_h_n = ''.join(df_zz.loc['pt_h_hide_nok']) print('Characters of cipher#2 equal to cipher#3') print(repr(pt_h_n))	df_z = compare_ptc(846) pt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt'])) c_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c'])) df_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c']) df_3
df_z = compare_ptc(846) pt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt'])) c_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c'])) df_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c']) df_3	df_3_n = df_3.applymap(ord) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0) df_3_n
df_3_n = df_3.applymap(ord) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0) df_3_n	key_ord = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18]
key_ord = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18]	df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0) df_3_n
df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0) df_3_n	(df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()
(df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()	df_z = compare_ptc(549) pt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt'])) c_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c'])) df_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c']) df_3
df_z = compare_ptc(549) pt_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['pt'])) c_t = re.compile(r'[\\W\\d_]+').sub('', ''.join(df_z['c'])) df_3 = pd.DataFrame([list(pt_t),list(c_t)],index=['t','c']) df_3	df_3_n = df_3.applymap(ord) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0) df_3_n
df_3_n = df_3.applymap(ord) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['c'] - df_3_n.loc['t'],columns=['diff']).T],axis=0) df_3_n = pd.concat([df_3_n,pd.DataFrame(df_3_n.loc['diff'].map(lambda x: x + 26 if (x <-1) else x)).rename(columns={'diff' : 'diffMod26'}).T],axis=0) df_3_n	df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0) (df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()
df_3_n = pd.concat([df_3_n,pd.DataFrame(list(islice(cycle(key_ord), len(df_3_n.columns))),columns=['key']).T],axis=0) (df_3_n.loc['diffMod26'] - df_3_n.loc['key']).map(abs).sum()	key_char = [chr(i+ord('a')) if i>=0 else ' ' for i in key_ord] ''.join(key_char)
key_char = [chr(i+ord('a')) if i>=0 else ' ' for i in key_ord] ''.join(key_char)	train.drop('t_text',axis=1,inplace=True)
train.drop('t_text',axis=1,inplace=True)	train.head()
train.head()	def shift_char(c,shift):     if c.islower():         return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))     else:         return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))
def shift_char(c,shift):     if c.islower():         return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))     else:         return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))	def replace_alpha(l,l_alpha_new):     res = []     i_alpha = 0     for i in range(len(l)):         if l[i].isalpha():             res.append(l_alpha_new[i_alpha])             i_alpha += 1         else:             res.append(l[i])     return(res)
def replace_alpha(l,l_alpha_new):     res = []     i_alpha = 0     for i in range(len(l)):         if l[i].isalpha():             res.append(l_alpha_new[i_alpha])             i_alpha += 1         else:             res.append(l[i])     return(res)	def fractional_vigenere(s,key):     l = list(s)     l_alpha = [x for x in l if x.isalpha()]     l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key_ord), len(l_alpha))))]     return(''.join(replace_alpha(l,l_alpha_shifted)))
def fractional_vigenere(s,key):     l = list(s)     l_alpha = [x for x in l if x.isalpha()]     l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key_ord), len(l_alpha))))]     return(''.join(replace_alpha(l,l_alpha_shifted)))	train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))
train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))	target_list = np.sort(df_p_chunked['target'].unique())
target_list = np.sort(df_p_chunked['target'].unique())	p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in train[train['target'] == i].index[:]:         ct_text = train.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)
p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in train[train['target'] == i].index[:]:         ct_text = train.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)	train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})
train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})	print(train.shape[0]) print(train_p_indexes.shape[0])
print(train.shape[0]) print(train_p_indexes.shape[0])	train = train.join(train_p_indexes)
train = train.join(train_p_indexes)	train.to_pickle('train_3.pkl')
train.to_pickle('train_3.pkl')	test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))
test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(x,key_ord).translate(translation_2_ct))	p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in test.index[:]:         t_text = test.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)
p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in test.index[:]:         t_text = test.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)	test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})
test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})	print(test.shape[0]) print(test_p_indexes.shape[0])
print(test.shape[0]) print(test_p_indexes.shape[0])	test = test.join(test_p_indexes)
test = test.join(test_p_indexes)	test.to_pickle('test_3.pkl')
test.to_pickle('test_3.pkl')	NB_END
import numpy as np  import pandas as pd   import itertools import os import re  from collections import Counter from dask import delayed, compute from dask.diagnostics import ProgressBar from fuzzywuzzy import fuzz, process from IPython.core.display import display from itertools import cycle, islice from sklearn.datasets import fetch_20newsgroups  ProgressBar().register()  chunk_size = 300 pd.options.display.max_columns = chunk_size pd.options.display.max_rows = chunk_size	train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')
train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')	df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)
df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)	df_p['target'] = df_p['target'].astype(np.int8)
df_p['target'] = df_p['target'].astype(np.int8)	df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  ')) df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])
df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  ')) df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])	p_text_chunk_list = [] p_text_index_list = []  for p_index, p_row in df_p.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)
p_text_chunk_list = [] p_text_index_list = []  for p_index, p_row in df_p.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)	df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list}) df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')  df_p_chunked_list = [] for i in np.sort(df_p_chunked['target'].unique()):     df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])
df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list}) df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')  df_p_chunked_list = [] for i in np.sort(df_p_chunked['target'].unique()):     df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])	competition_path = '../input/20-newsgroups-ciphertext-challenge/'
competition_path = '../input/20-newsgroups-ciphertext-challenge/'	train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'}) test = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})
train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'}) test = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})	cipher_path = '../input/cipher-1-cipher-2-full-solutions/'  cipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv')  cipher2_map = pd.concat([cipher2_map,pd.DataFrame(data=[['D','\\x10']],columns=['cipher','plain'])],axis=0,ignore_index=True) #Cheating a bit with this update, the cipher character D has to be added to the cipher_2 map because a cipher #4 text, it would have appeared as missing in our decryption matching at the end of this kernel  translation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption translation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption  def shift_char(c,shift):     if c.islower():         return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))     else:         return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))  def replace_alpha(l,l_alpha_new):     res = []     i_alpha = 0     for i in range(len(l)):         if l[i].isalpha():             res.append(l_alpha_new[i_alpha])             i_alpha += 1         else:             res.append(l[i])     return(res)  def fractional_vigenere(s,key):     l = list(s)     l_alpha = [x for x in l if x.isalpha()]     l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key), len(l_alpha))))]     return(''.join(replace_alpha(l,l_alpha_shifted)))  key_ord_3 = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18] # cipher #3 decryption key key_ord_3_n = [-x for x in key_ord_3] # cipher #3 encryption key
cipher_path = '../input/cipher-1-cipher-2-full-solutions/'  cipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv')  cipher2_map = pd.concat([cipher2_map,pd.DataFrame(data=[['D','\\x10']],columns=['cipher','plain'])],axis=0,ignore_index=True) #Cheating a bit with this update, the cipher character D has to be added to the cipher_2 map because a cipher #4 text, it would have appeared as missing in our decryption matching at the end of this kernel  translation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption translation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption  def shift_char(c,shift):     if c.islower():         return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))     else:         return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))  def replace_alpha(l,l_alpha_new):     res = []     i_alpha = 0     for i in range(len(l)):         if l[i].isalpha():             res.append(l_alpha_new[i_alpha])             i_alpha += 1         else:             res.append(l[i])     return(res)  def fractional_vigenere(s,key):     l = list(s)     l_alpha = [x for x in l if x.isalpha()]     l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key), len(l_alpha))))]     return(''.join(replace_alpha(l,l_alpha_shifted)))  key_ord_3 = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18] # cipher #3 decryption key key_ord_3_n = [-x for x in key_ord_3] # cipher #3 encryption key	"p_counts = pd.Series(Counter(\'\'.join(df_p[\'text\']))).rename(""counts"").to_frame().sort_values(""counts"", ascending = False) p_counts = 1000000 * p_counts / p_counts.sum() p_counts = p_counts.reset_index().rename(columns = {""index"":""p_char""})  c_counts = [] for i in range(1,5):     counts = pd.Series(Counter(\'\'.join(pd.concat([train[train[\'difficulty\'] == i][[\'text\']],test[test[\'difficulty\'] == i][[\'text\']]],axis=0)[\'text\']))).rename(\'counts\').to_frame().sort_values(\'counts\', ascending = False)     counts = 1000000 * counts / counts.sum()     counts = counts.reset_index().rename(columns = {\'index\':\'c_{}_char\'.format(i)})     c_counts.append(counts)"
"p_counts = pd.Series(Counter(\'\'.join(df_p[\'text\']))).rename(""counts"").to_frame().sort_values(""counts"", ascending = False) p_counts = 1000000 * p_counts / p_counts.sum() p_counts = p_counts.reset_index().rename(columns = {""index"":""p_char""})  c_counts = [] for i in range(1,5):     counts = pd.Series(Counter(\'\'.join(pd.concat([train[train[\'difficulty\'] == i][[\'text\']],test[test[\'difficulty\'] == i][[\'text\']]],axis=0)[\'text\']))).rename(\'counts\').to_frame().sort_values(\'counts\', ascending = False)     counts = 1000000 * counts / counts.sum()     counts = counts.reset_index().rename(columns = {\'index\':\'c_{}_char\'.format(i)})     c_counts.append(counts)"	pd.concat([p_counts] + c_counts, axis = 1).head(20)
pd.concat([p_counts] + c_counts, axis = 1).head(20)	df_p_chunked['text_len'] = df_p_chunked['text'].map(len)  df_p_chunked['pt_text'] = df_p_chunked['text'].map(lambda x: fractional_vigenere(x.translate(translation_2_pt),key_ord_3_n)) df_p_chunked['pt_counter'] = df_p_chunked['pt_text'].map(lambda x: Counter(x)) df_p_chunked = df_p_chunked.reset_index().rename(columns={'index' : 'p_chunk_index'})
df_p_chunked['text_len'] = df_p_chunked['text'].map(len)  df_p_chunked['pt_text'] = df_p_chunked['text'].map(lambda x: fractional_vigenere(x.translate(translation_2_pt),key_ord_3_n)) df_p_chunked['pt_counter'] = df_p_chunked['pt_text'].map(lambda x: Counter(x)) df_p_chunked = df_p_chunked.reset_index().rename(columns={'index' : 'p_chunk_index'})	difficulty_level = 4  train = train[train['difficulty'] == difficulty_level] train['text_len'] = train['text'].map(len)   test = test[test['difficulty'] == difficulty_level]
difficulty_level = 4  train = train[train['difficulty'] == difficulty_level] train['text_len'] = train['text'].map(len)   test = test[test['difficulty'] == difficulty_level]	res = [] for i in train.index[:]:     c_text = train.loc[i]['text']     c_target = train.loc[i]['target']     c_len = train.loc[i]['text_len']     c_counter = Counter(c_text)     p_chunk_indexes = list(df_p_chunked[(df_p_chunked['text_len'] == c_len) & (df_p_chunked['target'] == c_target) & (df_p_chunked['pt_counter'] == c_counter)]['p_chunk_index'].values)     if len(p_chunk_indexes) == 1:         res.append((i,p_chunk_indexes[0]))
res = [] for i in train.index[:]:     c_text = train.loc[i]['text']     c_target = train.loc[i]['target']     c_len = train.loc[i]['text_len']     c_counter = Counter(c_text)     p_chunk_indexes = list(df_p_chunked[(df_p_chunked['text_len'] == c_len) & (df_p_chunked['target'] == c_target) & (df_p_chunked['pt_counter'] == c_counter)]['p_chunk_index'].values)     if len(p_chunk_indexes) == 1:         res.append((i,p_chunk_indexes[0]))	df_crib2 = pd.DataFrame(res,columns=['train_index','p_chunk_index']) df_crib2['c_text'] = df_crib2['train_index'].map(lambda x: train.loc[x,'text']) df_crib2['pt_text'] = df_crib2['p_chunk_index'].map(lambda x: df_p_chunked.loc[x,'pt_text'])
df_crib2 = pd.DataFrame(res,columns=['train_index','p_chunk_index']) df_crib2['c_text'] = df_crib2['train_index'].map(lambda x: train.loc[x,'text']) df_crib2['pt_text'] = df_crib2['p_chunk_index'].map(lambda x: df_p_chunked.loc[x,'pt_text'])	df_crib2['text_len'] = df_crib2['c_text'].map(len)
df_crib2['text_len'] = df_crib2['c_text'].map(len)	def trans_mapper(x):     pt_text = x['pt_text']     c_text = x['c_text']          pt_series = pd.Series(Counter(pt_text)).rename('pt_counts').to_frame().sort_values('pt_counts', ascending = False)     c_series = pd.Series(Counter(c_text)).rename('c_counts').to_frame().sort_values('c_counts', ascending = False)     ptc_series = pd.merge(pt_series, c_series, left_index=True,right_index=True,how='outer')      if len(ptc_series[ptc_series['pt_counts'] != ptc_series['c_counts']]) > 0:         return(np.nan)      trans_map = ptc_series[ptc_series['pt_counts'] == 1]     trans_map = trans_map.reset_index().rename(columns={'index' : 'char'})     trans_map['p_char_index'] = trans_map['char'].map(lambda x: pt_text.find(x))     trans_map['c_char_index'] = trans_map['char'].map(lambda x: c_text.find(x))      return(dict(zip(trans_map['p_char_index'].values,trans_map['c_char_index'].values)))
def trans_mapper(x):     pt_text = x['pt_text']     c_text = x['c_text']          pt_series = pd.Series(Counter(pt_text)).rename('pt_counts').to_frame().sort_values('pt_counts', ascending = False)     c_series = pd.Series(Counter(c_text)).rename('c_counts').to_frame().sort_values('c_counts', ascending = False)     ptc_series = pd.merge(pt_series, c_series, left_index=True,right_index=True,how='outer')      if len(ptc_series[ptc_series['pt_counts'] != ptc_series['c_counts']]) > 0:         return(np.nan)      trans_map = ptc_series[ptc_series['pt_counts'] == 1]     trans_map = trans_map.reset_index().rename(columns={'index' : 'char'})     trans_map['p_char_index'] = trans_map['char'].map(lambda x: pt_text.find(x))     trans_map['c_char_index'] = trans_map['char'].map(lambda x: c_text.find(x))      return(dict(zip(trans_map['p_char_index'].values,trans_map['c_char_index'].values)))	df_crib2['trans_map'] = df_crib2.apply(lambda x: trans_mapper(x),axis=1)
df_crib2['trans_map'] = df_crib2.apply(lambda x: trans_mapper(x),axis=1)	trans_len = 300 trans_dict = {} for i in range(trans_len):     temp = set()     for j in df_crib2[df_crib2['text_len'] == trans_len].index:         t_map = df_crib2.loc[j,'trans_map']         if i in t_map:             temp = temp.union([t_map[i]])     trans_dict[i] = temp
trans_len = 300 trans_dict = {} for i in range(trans_len):     temp = set()     for j in df_crib2[df_crib2['text_len'] == trans_len].index:         t_map = df_crib2.loc[j,'trans_map']         if i in t_map:             temp = temp.union([t_map[i]])     trans_dict[i] = temp	trans_s = pd.Series(trans_dict)
trans_s = pd.Series(trans_dict)	print(trans_s.map(len).min()) print(trans_s.map(len).max())
print(trans_s.map(len).min()) print(trans_s.map(len).max())	trans_s = trans_s.map(lambda x: list(x)[0] if len(list(x))>0 else np.nan)
trans_s = trans_s.map(lambda x: list(x)[0] if len(list(x))>0 else np.nan)	def rowcol(i,n_cols):     row = i // n_cols     col = i % n_cols     return((row,col))  def draw_square(n,n_cols):     n_rows = rowcol(n,n_cols)[0]     df = pd.DataFrame(data=[[-1]*n_cols]*n_rows,index=range(n_rows),columns=range(n_cols))     for i in range(n):         df.loc[rowcol(i,n_cols)] = i     return(df)
def rowcol(i,n_cols):     row = i // n_cols     col = i % n_cols     return((row,col))  def draw_square(n,n_cols):     n_rows = rowcol(n,n_cols)[0]     df = pd.DataFrame(data=[[-1]*n_cols]*n_rows,index=range(n_rows),columns=range(n_cols))     for i in range(n):         df.loc[rowcol(i,n_cols)] = i     return(df)	"trans_s.rename(\'c_char_index\').to_frame().reset_index().rename(columns={\'index\':\'p_char_index\'}).sort_values(by=\'c_char_index\').T.style.format(""{:.0f}"")"
"trans_s.rename(\'c_char_index\').to_frame().reset_index().rename(columns={\'index\':\'p_char_index\'}).sort_values(by=\'c_char_index\').T.style.format(""{:.0f}"")"	"draw_square(300,24).style.format(""{:.0f}"")"
"draw_square(300,24).style.format(""{:.0f}"")"	def read_col(col,square):     n_rows = len(square)     res = []     for i in range(n_rows):         x_col = square.loc[i,col]         if ~np.isnan(x_col):             res = res + [x_col]     return(res)  def alternate_cols(col_1,col_2,square):     n_rows = len(square)     res = []     for i in range(n_rows):         x_col1 = square.loc[i,col_1]         x_col2 = square.loc[i,col_2]         if ~np.isnan(x_col1):             res = res + [x_col1]         if ~np.isnan(x_col2):             res = res + [x_col2]     return(res)
def read_col(col,square):     n_rows = len(square)     res = []     for i in range(n_rows):         x_col = square.loc[i,col]         if ~np.isnan(x_col):             res = res + [x_col]     return(res)  def alternate_cols(col_1,col_2,square):     n_rows = len(square)     res = []     for i in range(n_rows):         x_col1 = square.loc[i,col_1]         x_col2 = square.loc[i,col_2]         if ~np.isnan(x_col1):             res = res + [x_col1]         if ~np.isnan(x_col2):             res = res + [x_col2]     return(res)	encipher_trans_dict = {} decipher_trans_dict = {} for i in range(1,301):     df = draw_square(i,24)     res = []     res = res + read_col(0,df)     for j in range(1,12):         res = res + alternate_cols(j,24-j,df)     res = res + read_col(12,df)     encipher_trans_dict[i] = res     decipher_trans_dict[i] = np.argsort(res)
encipher_trans_dict = {} decipher_trans_dict = {} for i in range(1,301):     df = draw_square(i,24)     res = []     res = res + read_col(0,df)     for j in range(1,12):         res = res + alternate_cols(j,24-j,df)     res = res + read_col(12,df)     encipher_trans_dict[i] = res     decipher_trans_dict[i] = np.argsort(res)	def encipher4(p_text):     p_len = len(p_text)     res = encipher_trans_dict[p_len]     return(''.join([p_text[int(res[i])] for i in range(p_len)]))  def decipher4(c_text):     c_len = len(c_text)     res = decipher_trans_dict[c_len]     return(''.join([c_text[int(res[i])] for i in range(c_len)]))
def encipher4(p_text):     p_len = len(p_text)     res = encipher_trans_dict[p_len]     return(''.join([p_text[int(res[i])] for i in range(p_len)]))  def decipher4(c_text):     c_len = len(c_text)     res = decipher_trans_dict[c_len]     return(''.join([c_text[int(res[i])] for i in range(c_len)]))	train.head()
train.head()	train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))
train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))	target_list = np.sort(df_p_chunked['target'].unique())
target_list = np.sort(df_p_chunked['target'].unique())	p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in train[train['target'] == i].index[:]:         ct_text = train.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)
p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in train[train['target'] == i].index[:]:         ct_text = train.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)	train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})
train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})	print(train.shape[0]) print(train_p_indexes.shape[0])
print(train.shape[0]) print(train_p_indexes.shape[0])	train = train.join(train_p_indexes)
train = train.join(train_p_indexes)	train.to_pickle('train_4.pkl')
train.to_pickle('train_4.pkl')	test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))
test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))	p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in test.index[:]:         t_text = test.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)
p_indexes_dict = {} for i in target_list[:]:     df = df_p_chunked_list[i]     for j in test.index[:]:         t_text = test.loc[j,'ct_text']         new_p_indexes = set(df[df['text'] == ct_text]['p_index'])         if len(new_p_indexes) > 0:             p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)	test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})
test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})	print(test.shape[0]) print(test_p_indexes.shape[0])
print(test.shape[0]) print(test_p_indexes.shape[0])	test = test.join(test_p_indexes)
test = test.join(test_p_indexes)	test.to_pickle('test_4.pkl')
test.to_pickle('test_4.pkl')	NB_END
import numpy as np  import pandas as pd   import os  from IPython.core.display import display from sklearn.datasets import fetch_20newsgroups	"print(os.listdir(""../input""))"
"print(os.listdir(""../input""))"	competition_path = '20-newsgroups-ciphertext-challenge'
competition_path = '20-newsgroups-ciphertext-challenge'	test = pd.read_csv('../input/' + competition_path + '/test.csv').rename(columns={'ciphertext' : 'text'})
test = pd.read_csv('../input/' + competition_path + '/test.csv').rename(columns={'ciphertext' : 'text'})	train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')
train_p = fetch_20newsgroups(subset='train') test_p = fetch_20newsgroups(subset='test')	df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)
df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],                                    columns= ['text','target']),                       pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],                                    columns= ['text','target'])],                      axis=0).reset_index(drop=True)	df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  '))
df_p['text'] = df_p['text'].map(lambda x: x.replace('\ \ ','\ ').replace('\ ','\ ').replace('\ ','\  '))	df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])
df_p.loc[df_p['text'].str.endswith('\  '),'text'] = df_p.loc[df_p['text'].str.endswith('\  '),'text'].map(lambda x: x[:-1])	df_p['target'] = df_p['target'].astype(np.int8)
df_p['target'] = df_p['target'].astype(np.int8)	cipher_path = 'cipher-1-cipher-2-full-solutions' cipher1_map = pd.read_csv('../input/'+ cipher_path + '/cipher1_map.csv') translation_1 = str.maketrans(''.join(cipher1_map['cipher']), ''.join(cipher1_map['plain']))
cipher_path = 'cipher-1-cipher-2-full-solutions' cipher1_map = pd.read_csv('../input/'+ cipher_path + '/cipher1_map.csv') translation_1 = str.maketrans(''.join(cipher1_map['cipher']), ''.join(cipher1_map['plain']))	test.loc[40]
test.loc[40]	c_text = test.loc[40,'text']
c_text = test.loc[40,'text']	t_text = test.loc[40,'text'].translate(translation_1)
t_text = test.loc[40,'text'].translate(translation_1)	t_text
t_text	df_p.loc[[4473,7227],'text'].str.contains(t_text,regex=False)
df_p.loc[[4473,7227],'text'].str.contains(t_text,regex=False)	df_p.loc[[4473,7227],'target']
df_p.loc[[4473,7227],'target']	df_p_extract = df_p[df_p['text'].str.contains(t_text,regex=False)]
df_p_extract = df_p[df_p['text'].str.contains(t_text,regex=False)]	df_p_extract
df_p_extract	p_text_chunk_list = [] p_text_index_list = []  chunk_size = 300  for p_index, p_row in df_p_extract.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)
p_text_chunk_list = [] p_text_index_list = []  chunk_size = 300  for p_index, p_row in df_p_extract.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)	df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})
df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})	df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')
df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')	df_p_chunked[df_p_chunked['text'].str.contains(t_text,regex=False)]
df_p_chunked[df_p_chunked['text'].str.contains(t_text,regex=False)]	test.loc[31525]
test.loc[31525]	t_text = test.loc[31525,'text'].translate(translation_1)
t_text = test.loc[31525,'text'].translate(translation_1)	t_text
t_text	df_p_extract = df_p.loc[[11001,13188]]
df_p_extract = df_p.loc[[11001,13188]]	p_text_chunk_list = [] p_text_index_list = []  chunk_size = 300  for p_index, p_row in df_p_extract.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)
p_text_chunk_list = [] p_text_index_list = []  chunk_size = 300  for p_index, p_row in df_p_extract.iterrows():     p_text = p_row['text']     p_text_len = len(p_text)     if p_text_len > chunk_size:         for j in range(p_text_len // chunk_size):             p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])             p_text_index_list.append(p_index)         if p_text_len%chunk_size > 0:             p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])             p_text_index_list.append(p_index)     else:         p_text_chunk_list.append(p_text)         p_text_index_list.append(p_index)	df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})
df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})	df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')
df_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')	df_p_chunked[df_p_chunked['p_index'] == 11001]
df_p_chunked[df_p_chunked['p_index'] == 11001]	df_p_chunked[df_p_chunked['p_index'] == 13188]
df_p_chunked[df_p_chunked['p_index'] == 13188]	df_p_chunked[df_p_chunked['p_index'] == 11001].iloc[-1]['text'] == df_p_chunked[df_p_chunked['p_index'] == 13188].iloc[-1]['text']
df_p_chunked[df_p_chunked['p_index'] == 11001].iloc[-1]['text'] == df_p_chunked[df_p_chunked['p_index'] == 13188].iloc[-1]['text']	df_p_chunked[df_p_chunked['p_index'] == 11001].iloc[-1]['text'] == t_text
df_p_chunked[df_p_chunked['p_index'] == 11001].iloc[-1]['text'] == t_text	NB_END
%matplotlib inline import matplotlib.pyplot as plt plt.rcParams['figure.figsize'] = [15, 7.5] import numpy as np import pandas as pd import string from collections import Counter import nltk import os import re	train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv')
train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv')	level_one = train[train['difficulty']==1].copy()
level_one = train[train['difficulty']==1].copy()	level_one.shape
level_one.shape	alp = pd.Series(Counter(''.join(level_one['ciphertext']))) alp.head(10)
alp = pd.Series(Counter(''.join(level_one['ciphertext']))) alp.head(10)	alp.shape
alp.shape	from sklearn.datasets import fetch_20newsgroups news = fetch_20newsgroups(subset='train')
from sklearn.datasets import fetch_20newsgroups news = fetch_20newsgroups(subset='train')	heads = [x[:6] for x in news['data']]
heads = [x[:6] for x in news['data']]	Counter(heads).most_common(10)
Counter(heads).most_common(10)	level_one['ciphertext'].apply(lambda x: x[:6]).value_counts().reset_index().head(10)
level_one['ciphertext'].apply(lambda x: x[:6]).value_counts().reset_index().head(10)	subs = {     'F': '*',     'r': '#',     'o': '^',     'm': '-',     ':': 'G',     ' ': '1' } subs = {v:k for k, v in subs.items()}
subs = {     'F': '*',     'r': '#',     'o': '^',     'm': '-',     ':': 'G',     ' ': '1' } subs = {v:k for k, v in subs.items()}	def decipher(ciphertext):     return ''.join([subs[c] if c in subs.keys() else '?' for c in ciphertext])  def undeciphered(ciphertext):     return ''.join(['?' if c in subs.keys() else c for c in ciphertext])
def decipher(ciphertext):     return ''.join([subs[c] if c in subs.keys() else '?' for c in ciphertext])  def undeciphered(ciphertext):     return ''.join(['?' if c in subs.keys() else c for c in ciphertext])	level_one['ciphertext'].head(10).apply(decipher).reset_index()
level_one['ciphertext'].head(10).apply(decipher).reset_index()	heads = [x[:9] for x in news['data'] if x[:6] != 'From: '] Counter(heads).most_common()
heads = [x[:9] for x in news['data'] if x[:6] != 'From: '] Counter(heads).most_common()	heads = level_one['ciphertext'].apply(lambda x: x[:9]).value_counts().head(20).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads
heads = level_one['ciphertext'].apply(lambda x: x[:9]).value_counts().head(20).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads	subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')})
subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')})	heads = [x[:14] for x in news['data'] if x[:6] != 'From: ' and x[:9] != 'Subject: '] Counter(heads).most_common()
heads = [x[:14] for x in news['data'] if x[:6] != 'From: ' and x[:9] != 'Subject: '] Counter(heads).most_common()	heads = level_one['ciphertext'].apply(lambda x: x[:14]).value_counts().head(20).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads
heads = level_one['ciphertext'].apply(lambda x: x[:14]).value_counts().head(20).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads	subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')})
subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')})	heads = level_one['ciphertext'].apply(lambda x: x[:14]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads
heads = level_one['ciphertext'].apply(lambda x: x[:14]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads	heads = [x[:13] for x in news['data'] if x[:6] != 'From: '] Counter(heads).most_common(10)
heads = [x[:13] for x in news['data'] if x[:6] != 'From: '] Counter(heads).most_common(10)	heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads
heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads	heads['index'][2]
heads['index'][2]	subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')}) subs.update({v: k for k, v in zip('R', '\\x1e')})
subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')}) subs.update({v: k for k, v in zip('R', '\\x1e')})	heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads
heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(10).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads	heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(50).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads[heads['deciphered'].apply(lambda x: x[:4] != 'From')]
heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(50).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads[heads['deciphered'].apply(lambda x: x[:4] != 'From')]	heads = [x[:13] for x in news['data'] if x[:6] != 'From: ' and x[:9] != 'Subject: '] Counter(heads).most_common(10)
heads = [x[:13] for x in news['data'] if x[:6] != 'From: ' and x[:9] != 'Subject: '] Counter(heads).most_common(10)	subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')}) subs.update({v: k for k, v in zip('R', '\\x1e')}) subs.update({v: k for k, v in zip('Ds', 'xv')})
subs = {v: k for k, v in zip('From: ', '*#^-G1')} subs.update({v: k for k, v in zip('Subject', '>c\x03X\x1b_t')}) subs.update({v: k for k, v in zip('Organization', '%#dO\x02ahOta^\x02')}) subs.update({v: k for k, v in zip('R', '\\x1e')}) subs.update({v: k for k, v in zip('Ds', 'xv')})	heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(50).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads[heads['deciphered'].apply(lambda x: x[:4] != 'From')]
heads = level_one['ciphertext'].apply(lambda x: x[:13]).value_counts().head(50).reset_index() heads['deciphered'] = heads['index'].apply(decipher) heads[heads['deciphered'].apply(lambda x: x[:4] != 'From')]	level_one['ciphertext'].apply(decipher).iloc[3]
level_one['ciphertext'].apply(decipher).iloc[3]	np.where([re.search(r'Samue\\w Ross', s) != None for s in news['data']])
np.where([re.search(r'Samue\\w Ross', s) != None for s in news['data']])	news['data'][1646]
news['data'][1646]	level_one['ciphertext'].iloc[3]
level_one['ciphertext'].iloc[3]	level_one['ciphertext'].apply(decipher).iloc[3]
level_one['ciphertext'].apply(decipher).iloc[3]	level_one['ciphertext'].apply(undeciphered).iloc[3]
level_one['ciphertext'].apply(undeciphered).iloc[3]	news['data'][1646][:300]
news['data'][1646][:300]	"subs = {v: k for k, v in zip(\'From: \', \'*#^-G1\')} subs.update({v: k for k, v in zip(\'Subject\', \'>c\x03X\x1b_t\')}) subs.update({v: k for k, v in zip(\'Organization\', \'%#dO\x02ahOta^\x02\')}) subs.update({v: k for k, v in zip(\'R\', \'\\x1e\')}) subs.update({v: k for k, v in zip(\'Ds\', \'xv\')}) subs.update({v: k for k, v in zip(\'s\', \'v\')}) subs.update({v: k for k, v in zip(\'6@vl.d()\ Bkfhp!uywL28\', \'5bz8\\x08A|ysJf]0\\\'P@oWFH,\')}) subs.update({v: k for k, v in zip(\'N-pHMEAyTKIGCJW01\', \'\\x7fq9geE/\\x10{w:""2}l\\\\L\')})"
"subs = {v: k for k, v in zip(\'From: \', \'*#^-G1\')} subs.update({v: k for k, v in zip(\'Subject\', \'>c\x03X\x1b_t\')}) subs.update({v: k for k, v in zip(\'Organization\', \'%#dO\x02ahOta^\x02\')}) subs.update({v: k for k, v in zip(\'R\', \'\\x1e\')}) subs.update({v: k for k, v in zip(\'Ds\', \'xv\')}) subs.update({v: k for k, v in zip(\'s\', \'v\')}) subs.update({v: k for k, v in zip(\'6@vl.d()\ Bkfhp!uywL28\', \'5bz8\\x08A|ysJf]0\\\'P@oWFH,\')}) subs.update({v: k for k, v in zip(\'N-pHMEAyTKIGCJW01\', \'\\x7fq9geE/\\x10{w:""2}l\\\\L\')})"	level_one['ciphertext'].apply(decipher).iloc[3]
level_one['ciphertext'].apply(decipher).iloc[3]	level_one['ciphertext'].apply(undeciphered).iloc[3]
level_one['ciphertext'].apply(undeciphered).iloc[3]	len(subs.keys()) / len(alp)
len(subs.keys()) / len(alp)	for i in range(10):     print(level_one['ciphertext'].apply(decipher).iloc[i])     print('-' * 30)
for i in range(10):     print(level_one['ciphertext'].apply(decipher).iloc[i])     print('-' * 30)	for i in range(10):     print(level_one['ciphertext'].apply(decipher).iloc[-i-1])     print('-' * 30)
"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import numpy as np import pandas as pd import pandas as pd import re import plotly as py import plotly.graph_objs as go %matplotlib inline from plotly import tools import plotly.offline as py py.init_notebook_mode(connected=True) import plotly.graph_objs as go import matplotlib.pyplot as plt import numpy as np from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import TruncatedSVD #import lightgbm as lgb import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import warnings warnings.filterwarnings(\'ignore\') import matplotlib.pyplot as plt import seaborn as sns plt.style.use(\'ggplot\') import tqdm import os %matplotlib inline import collections import spacy #load spacy nlp = spacy.load(""en"", disable=[\'parser\', \'tagger\', \'ner\']) from nltk.corpus import stopwords stops = stopwords.words(""english"") import re from scipy.sparse import hstack, csr_matrix from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc from sklearn.model_selection import cross_val_score  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory import warnings warnings.filterwarnings(\'ignore\')  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	"train_data = pd.read_csv(""../input/train.csv"") test_data = pd.read_csv(""../input/test.csv"") sample_sub = pd.read_csv(""../input/sample_submission.csv"") "
"train_data = pd.read_csv(""../input/train.csv"") test_data = pd.read_csv(""../input/test.csv"") sample_sub = pd.read_csv(""../input/sample_submission.csv"") "	train_data['ciphertext'] = train_data['ciphertext'].str.lower() train_data['ciphertext'] = train_data['ciphertext'].astype(str)  test_data['ciphertext'] = test_data['ciphertext'].str.lower() test_data['ciphertext'] = test_data['ciphertext'].astype(str) 
train_data['ciphertext'] = train_data['ciphertext'].str.lower() train_data['ciphertext'] = train_data['ciphertext'].astype(str)  test_data['ciphertext'] = test_data['ciphertext'].str.lower() test_data['ciphertext'] = test_data['ciphertext'].astype(str) 	X = train_data[['ciphertext']] Y = train_data['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.25)
X = train_data[['ciphertext']] Y = train_data['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.25)	from sklearn.feature_extraction.text import TfidfVectorizer tf = TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')
from sklearn.feature_extraction.text import TfidfVectorizer tf = TfidfVectorizer(token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')	# build TFIDF Vectorizer  tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b'))  word_vectorizer = TfidfVectorizer(     sublinear_tf=True,     #stop_words = 'english',     strip_accents='ascii',     analyzer='word',     token_pattern=tokens,     ngram_range=(1,2),     dtype=np.float32,     max_features=7500 )   # Character Stemmer char_vectorizer = TfidfVectorizer(     sublinear_tf=True,     #stop_words = 'english',     strip_accents='ascii',     analyzer='char',     token_pattern=tokens,     ngram_range=(2, 4),     dtype=np.float32,     max_features=12000 )  word_vectorizer.fit(train_data['ciphertext'])  char_vectorizer.fit(train_data['ciphertext']) 
# build TFIDF Vectorizer  tokens= ((u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b'))  word_vectorizer = TfidfVectorizer(     sublinear_tf=True,     #stop_words = 'english',     strip_accents='ascii',     analyzer='word',     token_pattern=tokens,     ngram_range=(1,2),     dtype=np.float32,     max_features=7500 )   # Character Stemmer char_vectorizer = TfidfVectorizer(     sublinear_tf=True,     #stop_words = 'english',     strip_accents='ascii',     analyzer='char',     token_pattern=tokens,     ngram_range=(2, 4),     dtype=np.float32,     max_features=12000 )  word_vectorizer.fit(train_data['ciphertext'])  char_vectorizer.fit(train_data['ciphertext']) 	 train_word_features = word_vectorizer.transform(train_data['ciphertext']) train_char_features = char_vectorizer.transform(train_data['ciphertext']) 
 train_word_features = word_vectorizer.transform(train_data['ciphertext']) train_char_features = char_vectorizer.transform(train_data['ciphertext']) 	train_features = hstack([     train_char_features,     train_word_features])
train_features = hstack([     train_char_features,     train_word_features])	"Target = train_data[""target""] "
"Target = train_data[""target""] "	"%time print(""Modeling.."") loss = [] X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_features, Target, test_size=0.33)  lr = LogisticRegression(solver=""sag"", max_iter=100,class_weight=\'balanced\',C=2.65,penalty=\'l2\') lr.fit(train_features,Target) lr_pred=lr.predict(X_test_tfidf) "
"%time print(""Modeling.."") loss = [] X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_features, Target, test_size=0.33)  lr = LogisticRegression(solver=""sag"", max_iter=100,class_weight=\'balanced\',C=2.65,penalty=\'l2\') lr.fit(train_features,Target) lr_pred=lr.predict(X_test_tfidf) "	accuracy_tfidf =accuracy_score(y_test_tfidf,lr_pred) 
accuracy_tfidf =accuracy_score(y_test_tfidf,lr_pred) 	"print(accuracy_tfidf) print(""Auc Score: "",np.mean(cross_val_score(lr, train_features, Target, cv=3,))) "
"print(accuracy_tfidf) print(""Auc Score: "",np.mean(cross_val_score(lr, train_features, Target, cv=3,))) "	print(classification_report(y_test_tfidf,lr_pred))
print(classification_report(y_test_tfidf,lr_pred))	"# test test_word_features = word_vectorizer.transform(test_data[\'ciphertext\']) test_char_features = char_vectorizer.transform(test_data[\'ciphertext\'])  test_features = hstack([     test_char_features,     test_word_features])   lr = LogisticRegression(solver=""sag"", max_iter=100,class_weight=\'balanced\',C=2.65,penalty=\'l2\') lr.fit(train_features,Target) Predicted=lr.predict(test_features) "
"# test test_word_features = word_vectorizer.transform(test_data[\'ciphertext\']) test_char_features = char_vectorizer.transform(test_data[\'ciphertext\'])  test_features = hstack([     test_char_features,     test_word_features])   lr = LogisticRegression(solver=""sag"", max_iter=100,class_weight=\'balanced\',C=2.65,penalty=\'l2\') lr.fit(train_features,Target) Predicted=lr.predict(test_features) "	df_submission['Predicted'] = final_prediction df_submission.to_csv('submission.csv', index = False)
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports-capital-goods.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-imports-capital-goods.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports-capital-goods.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-imports-capital-goods.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  
# Histogram of column data  def plotHistogram(df, nHistogramShown, nHistogramPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow      plt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')     for i in range(min(nCol, nHistogramShown)):          plt.subplot(nHistRow, nHistogramPerRow, i+1)          df.iloc[:,i].hist()          plt.ylabel('counts')          plt.xticks(rotation=90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotHistogram(df1, 10, 5)
plotHistogram(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-exports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-exports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotHistogram(df2, 10, 5)
plotHistogram(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotHistogram(df3, 10, 5)
plotHistogram(df3, 10, 5)	NB_END
from mpl_toolkits.mplot3d import Axes3D  from sklearn.preprocessing import StandardScaler  import matplotlib.pyplot as plt # plotting  import numpy as np # linear algebra  import os # accessing directory structure  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  	print(os.listdir('../input'))
print(os.listdir('../input'))	# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  
# Distribution graphs (histogram/bar graph) of column data  def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()      df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values      nRow, nCol = df.shape      columnNames = list(df)      nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow      plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')      for i in range(min(nCol, nGraphShown)):          plt.subplot(nGraphRow, nGraphPerRow, i + 1)          columnDf = df.iloc[:, i]          if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):              valueCounts = columnDf.value_counts()              valueCounts.plot.bar()          else:              columnDf.hist()          plt.ylabel('counts')          plt.xticks(rotation = 90)          plt.title(f'{columnNames[i]} (column {i})')      plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)      plt.show()  	# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  
# Correlation matrix  def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName      df = df.dropna('columns') # drop columns with NaN      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      if df.shape[1] < 2:          print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')          return      corr = df.corr()      plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')      corrMat = plt.matshow(corr, fignum = 1)      plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)      plt.yticks(range(len(corr.columns)), corr.columns)      plt.gca().xaxis.tick_bottom()      plt.colorbar(corrMat)      plt.title(f'Correlation Matrix for {filename}', fontsize=15)      plt.show()  	# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  
# Scatter and density plots  def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns      # Remove rows and columns that would lead to df being singular      df = df.dropna('columns')      df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values      columnNames = list(df)      if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]      ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')      corrs = df.corr().values      for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):          ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)      plt.suptitle('Scatter and Density Plot')      plt.show()  	nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df1 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-balance.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'advance-u.s.-international-trade-in-goods-balance.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')	df1.head(5)
df1.head(5)	plotPerColumnDistribution(df1, 10, 5)
plotPerColumnDistribution(df1, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df2 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'advance-u.s.-international-trade-in-goods-imports.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')	df2.head(5)
df2.head(5)	plotPerColumnDistribution(df2, 10, 5)
plotPerColumnDistribution(df2, 10, 5)	nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports-capital-goods.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports-capital-goods.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')
nRowsRead = 1000 # specify 'None' if want to read whole file df3 = pd.read_csv('../input/advance-u.s.-international-trade-in-goods-imports-capital-goods.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'advance-u.s.-international-trade-in-goods-imports-capital-goods.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')	df3.head(5)
df3.head(5)	plotPerColumnDistribution(df3, 10, 5)
plotPerColumnDistribution(df3, 10, 5)	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	"train_df=pd.read_csv(""../input/train.csv"")"
"train_df=pd.read_csv(""../input/train.csv"")"	train_df.shape
train_df.shape	train_df.head()
train_df.head()	import seaborn as sns  import matplotlib.pyplot as plt
import seaborn as sns  import matplotlib.pyplot as plt	"sns.countplot(train_df[""has_cactus""])"
"sns.countplot(train_df[""has_cactus""])"	import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG16 from keras.optimizers import Adam
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG16 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"""
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"""	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id,0))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr) 
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id,0))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr) 	X_tr=X_tr.reshape(-1,32,32,1)
X_tr=X_tr.reshape(-1,32,32,1)	Y_tr.shape
Y_tr.shape	X_tr.shape
X_tr.shape	"target=train_df[""has_cactus""]"
"target=train_df[""has_cactus""]"	"train_df=train_df.drop(""has_cactus"",axis=1)"
"train_df=train_df.drop(""has_cactus"",axis=1)"	"import os,array import pandas as pd import time import dask as dd  from PIL import Image def pixelconv(file_list,img_height,img_width,pixels):       columnNames = list()      for i in range(pixels):         pixel = \'pixel\'         pixel += str(i)         columnNames.append(pixel)       train_data = pd.DataFrame(columns = columnNames)     start_time = time.time()     for i in tqdm_notebook(file_list):         t = i         img_name = t         img = Image.open(\'../input/train/train/\'+img_name)         rawData = img.load()         #print rawData         data = []         for y in range(img_height):             for x in range(img_width):                 data.append(rawData[x,y][0])         print (i)         k = 0         #print data         train_data.loc[i] = [data[k] for k in range(pixels)]     #print train_data.loc[0]      print (""Done pixel values conversion"")     print  (time.time()-start_time)     print (train_data)     train_data.to_csv(""train_converted_new.csv"",index = False)     print (""Done data frame conversion"")     print  (time.time()-start_time) pixelconv(train_df.id,32,32,1024) # pass pandas dataframe in which path of images only as column                                     # in return csv file will save in working directory "
"import os,array import pandas as pd import time import dask as dd  from PIL import Image def pixelconv(file_list,img_height,img_width,pixels):       columnNames = list()      for i in range(pixels):         pixel = \'pixel\'         pixel += str(i)         columnNames.append(pixel)       train_data = pd.DataFrame(columns = columnNames)     start_time = time.time()     for i in tqdm_notebook(file_list):         t = i         img_name = t         img = Image.open(\'../input/train/train/\'+img_name)         rawData = img.load()         #print rawData         data = []         for y in range(img_height):             for x in range(img_width):                 data.append(rawData[x,y][0])         print (i)         k = 0         #print data         train_data.loc[i] = [data[k] for k in range(pixels)]     #print train_data.loc[0]      print (""Done pixel values conversion"")     print  (time.time()-start_time)     print (train_data)     train_data.to_csv(""train_converted_new.csv"",index = False)     print (""Done data frame conversion"")     print  (time.time()-start_time) pixelconv(train_df.id,32,32,1024) # pass pandas dataframe in which path of images only as column                                     # in return csv file will save in working directory "	"new_data=pd.read_csv(""../working/train_converted_new.csv"")"
"new_data=pd.read_csv(""../working/train_converted_new.csv"")"	new_data.head()
new_data.head()	import numpy as np import pandas as pd  from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA  from keras.models import Sequential from keras.utils import np_utils from keras.layers import Dense, Dropout, GaussianNoise, Conv1D from keras.preprocessing.image import ImageDataGenerator  import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline
import numpy as np import pandas as pd  from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA  from keras.models import Sequential from keras.utils import np_utils from keras.layers import Dense, Dropout, GaussianNoise, Conv1D from keras.preprocessing.image import ImageDataGenerator  import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline	 pca = PCA(n_components=500) pca.fit(new_data)  plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('Number of components') plt.ylabel('Cumulative explained variance')
 pca = PCA(n_components=500) pca.fit(new_data)  plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('Number of components') plt.ylabel('Cumulative explained variance')	NCOMPONENTS = 625  pca = PCA(n_components=NCOMPONENTS) X_pca_train = pca.fit_transform(new_data) pca_std = np.std(X_pca_train) print(X_pca_train.shape)
NCOMPONENTS = 625  pca = PCA(n_components=NCOMPONENTS) X_pca_train = pca.fit_transform(new_data) pca_std = np.std(X_pca_train) print(X_pca_train.shape)	inv_pca = pca.inverse_transform(X_pca_train) #inv_sc = scaler.inverse_transform(inv_pca)
inv_pca = pca.inverse_transform(X_pca_train) #inv_sc = scaler.inverse_transform(inv_pca)	X_pca_train_new=X_pca_train.reshape(X_pca_train.shape[0],25,25,1)
X_pca_train_new=X_pca_train.reshape(X_pca_train.shape[0],25,25,1)	X_pca_train_new.shape
X_pca_train_new.shape	X_pca_train.shape ### this shape will be used in MLP
X_pca_train.shape ### this shape will be used in MLP	import keras model = Sequential() layers = 1 units = 128  model.add(Dense(units, input_dim=NCOMPONENTS, activation='relu')) model.add(GaussianNoise(pca_std)) model.add(Dense(units, activation='relu')) model.add(GaussianNoise(pca_std)) model.add(Dropout(0.1)) model.add(Dense(1, activation='sigmoid'))  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=1e-5), metrics=['acc']) history = model.fit(X_pca_train,target,           batch_size=32,           epochs=10,           verbose=1,           validation_split=0.15)  #model.fit(X_pca_train, Y_train, epochs=100, batch_size=256, validation_split=0.15, verbose=2)
import keras model = Sequential() layers = 1 units = 128  model.add(Dense(units, input_dim=NCOMPONENTS, activation='relu')) model.add(GaussianNoise(pca_std)) model.add(Dense(units, activation='relu')) model.add(GaussianNoise(pca_std)) model.add(Dropout(0.1)) model.add(Dense(1, activation='sigmoid'))  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=1e-5), metrics=['acc']) history = model.fit(X_pca_train,target,           batch_size=32,           epochs=10,           verbose=1,           validation_split=0.15)  #model.fit(X_pca_train, Y_train, epochs=100, batch_size=256, validation_split=0.15, verbose=2)	import matplotlib.pyplot as plt %matplotlib inline accuracy = history.history['acc'] val_accuracy = history.history['val_acc'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(accuracy)) plt.plot(epochs, accuracy, 'bo', label='Training accuracy') plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.legend() plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.legend() plt.show()
import matplotlib.pyplot as plt %matplotlib inline accuracy = history.history['acc'] val_accuracy = history.history['val_acc'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(accuracy)) plt.plot(epochs, accuracy, 'bo', label='Training accuracy') plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy') plt.title('Training and validation accuracy') plt.legend() plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.legend() plt.show()	import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.layers.normalization import BatchNormalization  batch_size = 256 num_classes = 1 epochs = 200  #input image dimensions img_rows, img_cols = 25, 25  model = Sequential() model.add(Conv2D(64, kernel_size=(3, 3),                  activation='relu',                  input_shape=(25,25,1))) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.1)) model.add(Conv2D(128, (3, 3), activation='relu')) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.1)) model.add(Dense(num_classes, activation='sigmoid'))  model.compile(loss=keras.losses.binary_crossentropy,               optimizer=keras.optimizers.Adam(lr=1e-5),               metrics=['accuracy'])
import keras from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.layers.normalization import BatchNormalization  batch_size = 256 num_classes = 1 epochs = 200  #input image dimensions img_rows, img_cols = 25, 25  model = Sequential() model.add(Conv2D(64, kernel_size=(3, 3),                  activation='relu',                  input_shape=(25,25,1))) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.1)) model.add(Conv2D(128, (3, 3), activation='relu')) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dropout(0.1)) model.add(Dense(num_classes, activation='sigmoid'))  model.compile(loss=keras.losses.binary_crossentropy,               optimizer=keras.optimizers.Adam(lr=1e-5),               metrics=['accuracy'])	history1 = model.fit(X_pca_train_new,target,           batch_size=batch_size,           epochs=200,           verbose=1,           validation_split=0.15) 
history1 = model.fit(X_pca_train_new,target,           batch_size=batch_size,           epochs=200,           verbose=1,           validation_split=0.15) 	import matplotlib.pyplot as plt %matplotlib inline accuracy = history1.history['acc'] val_accuracy = history1.history['val_acc'] loss = history1.history['loss'] val_loss = history1.history['val_loss'] epochs = range(len(accuracy)) plt.plot(epochs, accuracy, 'bo', label='Training accuracy') plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy') plt.title('CNN result Training and validation accuracy') plt.legend() plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('cnn Training and validation loss') plt.legend() plt.show()
import matplotlib.pyplot as plt %matplotlib inline accuracy = history1.history['acc'] val_accuracy = history1.history['val_acc'] loss = history1.history['loss'] val_loss = history1.history['val_loss'] epochs = range(len(accuracy)) plt.plot(epochs, accuracy, 'bo', label='Training accuracy') plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy') plt.title('CNN result Training and validation accuracy') plt.legend() plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('cnn Training and validation loss') plt.legend() plt.show()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id,0))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id,0))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	X_tst.shape
X_tst.shape	X_tst=X_tst.reshape(-1,32,32,1)
X_tst=X_tst.reshape(-1,32,32,1)	test_path=[] for i in os.listdir(test_dir):     test_path.append(i)
test_path=[] for i in os.listdir(test_dir):     test_path.append(i)	"test_dataframe=pd.DataFrame(data=test_path,columns=[""id""])"
"test_dataframe=pd.DataFrame(data=test_path,columns=[""id""])"	test_dataframe.head()
test_dataframe.head()	"import os,array import pandas as pd import time import dask as dd  from PIL import Image def pixelconv(file_list,img_height,img_width,pixels):       columnNames = list()      for i in range(pixels):         pixel = \'pixel\'         pixel += str(i)         columnNames.append(pixel)       train_data = pd.DataFrame(columns = columnNames)     start_time = time.time()     for i in file_list:         t = i         img_name = t         img = Image.open(\'../input/test/test/\'+img_name)         rawData = img.load()         #print rawData         data = []         for y in range(img_height):             for x in range(img_width):                 data.append(rawData[x,y][0])         print (i)         k = 0         #print data         train_data.loc[i] = [data[k] for k in range(pixels)]     #print train_data.loc[0]      print (""Done pixel values conversion"")     print  (time.time()-start_time)     print (train_data)     train_data.to_csv(""test_converted_new.csv"",index = False)     print (""Done data frame conversion"")     print  (time.time()-start_time) pixelconv(test_dataframe.id,32,32,1024) # pass pandas dataframe in which path of images only as column                                     # in return csv file will save in working directory "
"import os,array import pandas as pd import time import dask as dd  from PIL import Image def pixelconv(file_list,img_height,img_width,pixels):       columnNames = list()      for i in range(pixels):         pixel = \'pixel\'         pixel += str(i)         columnNames.append(pixel)       train_data = pd.DataFrame(columns = columnNames)     start_time = time.time()     for i in file_list:         t = i         img_name = t         img = Image.open(\'../input/test/test/\'+img_name)         rawData = img.load()         #print rawData         data = []         for y in range(img_height):             for x in range(img_width):                 data.append(rawData[x,y][0])         print (i)         k = 0         #print data         train_data.loc[i] = [data[k] for k in range(pixels)]     #print train_data.loc[0]      print (""Done pixel values conversion"")     print  (time.time()-start_time)     print (train_data)     train_data.to_csv(""test_converted_new.csv"",index = False)     print (""Done data frame conversion"")     print  (time.time()-start_time) pixelconv(test_dataframe.id,32,32,1024) # pass pandas dataframe in which path of images only as column                                     # in return csv file will save in working directory "	"new_test=pd.read_csv(""../working/test_converted_new.csv"")"
"new_test=pd.read_csv(""../working/test_converted_new.csv"")"	X_tst=pca.transform(new_test)
X_tst=pca.transform(new_test)	X_tst.shape
X_tst.shape	X_tst=X_tst.reshape(-1,25,25,1)
X_tst=X_tst.reshape(-1,25,25,1)	X_tst.shape
X_tst.shape	"train_df[""image_location""]=train_dir+train_df[""id""] train_df[""has_cactus""]=target"
"train_df[""image_location""]=train_dir+train_df[""id""] train_df[""has_cactus""]=target"	import tensorflow as tf from keras import backend as K
import tensorflow as tf from keras import backend as K	"def binary_focal_loss(gamma=2., alpha=.25):     """"""     Binary form of focal loss.       FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)       where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.     References:         https://arxiv.org/pdf/1708.02002.pdf     Usage:      model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[""accuracy""], optimizer=adam)     """"""     def binary_focal_loss_fixed(y_true, y_pred):         """"""         :param y_true: A tensor of the same shape as `y_pred`         :param y_pred:  A tensor resulting from a sigmoid         :return: Output tensor.         """"""         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))          epsilon = K.epsilon()         # clip to prevent NaN\'s and Inf\'s         pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)         pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)          return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\                -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))      return binary_focal_loss_fixed"
"def binary_focal_loss(gamma=2., alpha=.25):     """"""     Binary form of focal loss.       FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)       where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.     References:         https://arxiv.org/pdf/1708.02002.pdf     Usage:      model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[""accuracy""], optimizer=adam)     """"""     def binary_focal_loss_fixed(y_true, y_pred):         """"""         :param y_true: A tensor of the same shape as `y_pred`         :param y_pred:  A tensor resulting from a sigmoid         :return: Output tensor.         """"""         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))          epsilon = K.epsilon()         # clip to prevent NaN\'s and Inf\'s         pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)         pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)          return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\                -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))      return binary_focal_loss_fixed"	train_df.head()
train_df.head()	"#from console_progressbar import ProgressBar import shutil import tqdm import os filenames=list(train_df[""image_location""].values) labels=list(train_df[""has_cactus""].values) folders_to_be_created = np.unique(list(train_df[\'has_cactus\'].values)) files=[] path=""../working/trainset/"" for i in folders_to_be_created:     if not os.path.exists(path+str(i)):         os.makedirs(path+str(i))  #pb = ProgressBar(total=100, prefix=\'Save valid data\', suffix=\'\', decimals=3, length=50, fill=\'=\') for f in tqdm_notebook(range(len(filenames))):          current_image=filenames[f]     current_label=labels[f]     src_path=current_image         dst_path =path+str(current_label)           try :         shutil.copy(src_path, dst_path)         #pb.print_progress_bar((f + 1) * 100 / 4000)     except Exception as e :         files.append(src_path)"
"#from console_progressbar import ProgressBar import shutil import tqdm import os filenames=list(train_df[""image_location""].values) labels=list(train_df[""has_cactus""].values) folders_to_be_created = np.unique(list(train_df[\'has_cactus\'].values)) files=[] path=""../working/trainset/"" for i in folders_to_be_created:     if not os.path.exists(path+str(i)):         os.makedirs(path+str(i))  #pb = ProgressBar(total=100, prefix=\'Save valid data\', suffix=\'\', decimals=3, length=50, fill=\'=\') for f in tqdm_notebook(range(len(filenames))):          current_image=filenames[f]     current_label=labels[f]     src_path=current_image         dst_path =path+str(current_label)           try :         shutil.copy(src_path, dst_path)         #pb.print_progress_bar((f + 1) * 100 / 4000)     except Exception as e :         files.append(src_path)"	"import keras from keras_preprocessing.image import ImageDataGenerator from keras.applications.vgg16 import preprocess_input train_datagen = ImageDataGenerator(     rescale = 1./255, #     preprocessing_function= preprocess_input,     #shear_range=0.2,     zoom_range=0.2,     fill_mode = \'reflect\',     #cval = 1,     rotation_range = 30,     width_shift_range=0.2,     height_shift_range=0.2,     horizontal_flip=True,validation_split=.20)  valid_datagen = ImageDataGenerator(rescale=1./255)#,preprocessing_function=preprocess_input)  train_generator = train_datagen.flow_from_directory(     directory=\'../working/trainset/\',     target_size=(32, 32),     batch_size=32,     class_mode=\'binary\',subset=""training"")  validation_generator = train_datagen.flow_from_directory(     directory=\'../working/trainset/\',     target_size=(32,32),     batch_size=32,     class_mode=\'binary\',subset=""validation"")"
"import keras from keras_preprocessing.image import ImageDataGenerator from keras.applications.vgg16 import preprocess_input train_datagen = ImageDataGenerator(     rescale = 1./255, #     preprocessing_function= preprocess_input,     #shear_range=0.2,     zoom_range=0.2,     fill_mode = \'reflect\',     #cval = 1,     rotation_range = 30,     width_shift_range=0.2,     height_shift_range=0.2,     horizontal_flip=True,validation_split=.20)  valid_datagen = ImageDataGenerator(rescale=1./255)#,preprocessing_function=preprocess_input)  train_generator = train_datagen.flow_from_directory(     directory=\'../working/trainset/\',     target_size=(32, 32),     batch_size=32,     class_mode=\'binary\',subset=""training"")  validation_generator = train_datagen.flow_from_directory(     directory=\'../working/trainset/\',     target_size=(32,32),     batch_size=32,     class_mode=\'binary\',subset=""validation"")"	import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG16 from keras.optimizers import Adam 
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG16 from keras.optimizers import Adam 	vgg16_net = VGG16(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))
vgg16_net = VGG16(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))	vgg16_net.trainable = False vgg16_net.summary()
vgg16_net.trainable = False vgg16_net.summary()	model = Sequential() model.add(vgg16_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))
model = Sequential() model.add(vgg16_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))	"model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[""accuracy""], optimizer=Adam(lr=1e-5))"
"model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[""accuracy""], optimizer=Adam(lr=1e-5))"	history=model.fit_generator(train_generator,                     steps_per_epoch = 14001//32,                     epochs=50,                     validation_data = validation_generator,validation_steps=3499//32)
history=model.fit_generator(train_generator,                     steps_per_epoch = 14001//32,                     epochs=50,                     validation_data = validation_generator,validation_steps=3499//32)	"import matplotlib.pyplot as plt import numpy as np #plot of model epochs what has been happened within 100 epochs baseline with vgg16 training 135 million params # freezing first layer fig = plt.figure(figsize=(12,8)) plt.plot(history.history[\'acc\'],\'blue\') plt.plot(history.history[\'val_acc\'],\'orange\') plt.xticks(np.arange(0, 50, 1)) plt.yticks(np.arange(0,1,.1)) plt.rcParams[\'figure.figsize\'] = (10, 10) plt.xlabel(""Num of Epochs"") plt.ylabel(""Accuracy"") plt.title(""Training Accuracy vs Validation Accuracy"") plt.grid(True) plt.gray() plt.legend([\'train\',\'validation\']) plt.show()   plt.figure(1) plt.plot(history.history[\'loss\'],\'blue\') plt.plot(history.history[\'val_loss\'],\'orange\') plt.xticks(np.arange(0, 50, 1)) plt.rcParams[\'figure.figsize\'] = (10, 10) plt.xlabel(""Num of Epochs"") plt.ylabel(""Loss"") plt.title(""Training Loss vs Validation Loss"") plt.grid(True) plt.gray() plt.legend([\'train\',\'validation\']) plt.show()"
"import matplotlib.pyplot as plt import numpy as np #plot of model epochs what has been happened within 100 epochs baseline with vgg16 training 135 million params # freezing first layer fig = plt.figure(figsize=(12,8)) plt.plot(history.history[\'acc\'],\'blue\') plt.plot(history.history[\'val_acc\'],\'orange\') plt.xticks(np.arange(0, 50, 1)) plt.yticks(np.arange(0,1,.1)) plt.rcParams[\'figure.figsize\'] = (10, 10) plt.xlabel(""Num of Epochs"") plt.ylabel(""Accuracy"") plt.title(""Training Accuracy vs Validation Accuracy"") plt.grid(True) plt.gray() plt.legend([\'train\',\'validation\']) plt.show()   plt.figure(1) plt.plot(history.history[\'loss\'],\'blue\') plt.plot(history.history[\'val_loss\'],\'orange\') plt.xticks(np.arange(0, 50, 1)) plt.rcParams[\'figure.figsize\'] = (10, 10) plt.xlabel(""Num of Epochs"") plt.ylabel(""Loss"") plt.title(""Training Loss vs Validation Loss"") plt.grid(True) plt.gray() plt.legend([\'train\',\'validation\']) plt.show()"	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst) 
# Prediction test_predictions = model.predict(X_tst) 	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.5 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.5 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols] 
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols] 	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission_focal_loss_vgg-16.csv',index=False)
sub_df.to_csv('submission_focal_loss_vgg-16.csv',index=False)	from keras.applications import ResNet50 from keras.models import Sequential from keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization from keras.applications.resnet50 import preprocess_input from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img, img_to_array from keras.optimizers import Adam
from keras.applications import ResNet50 from keras.models import Sequential from keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization from keras.applications.resnet50 import preprocess_input from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img, img_to_array from keras.optimizers import Adam	base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))  base_model.summary()
base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(32,32,3))  base_model.summary()	"model = Sequential() model.add(base_model) model.add(Flatten()) # # let\'s add a fully-connected layer model.add(Dense(500, activation=\'relu\')) model.add(Dropout(0.2)) model.add(Dense(500, activation=\'relu\')) model.add(Dropout(0.2)) # # and a logistic layer -- let\'s say we have 200 classes model.add(Dense(1, activation=\'sigmoid\')) model.compile(loss=binary_focal_loss(gamma=2,alpha=0.28), metrics=[""accuracy""], optimizer=Adam(lr=1e-5))"
"model = Sequential() model.add(base_model) model.add(Flatten()) # # let\'s add a fully-connected layer model.add(Dense(500, activation=\'relu\')) model.add(Dropout(0.2)) model.add(Dense(500, activation=\'relu\')) model.add(Dropout(0.2)) # # and a logistic layer -- let\'s say we have 200 classes model.add(Dense(1, activation=\'sigmoid\')) model.compile(loss=binary_focal_loss(gamma=2,alpha=0.28), metrics=[""accuracy""], optimizer=Adam(lr=1e-5))"	history1=model.fit_generator(train_generator,                     steps_per_epoch = 14001//32,                     epochs=85,                     validation_data = validation_generator,validation_steps=3499//32)
history1=model.fit_generator(train_generator,                     steps_per_epoch = 14001//32,                     epochs=85,                     validation_data = validation_generator,validation_steps=3499//32)	NB_END
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalAveragePooling2D from keras.models import Model from keras.applications import VGG16 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	"model = Sequential()  model.add(Conv2D(32, (5, 5), strides = (1, 1), name = \'conv0\', input_shape = (32, 32, 1)))  model.add(BatchNormalization(axis = 3, name = \'bn0\')) model.add(Activation(\'relu\'))  model.add(MaxPooling2D((2, 2), name=\'max_pool\')) model.add(Conv2D(64, (3, 3), strides = (1,1), name=""conv1"")) model.add(Activation(\'relu\')) model.add(AveragePooling2D((3, 3), name=\'avg_pool\'))  model.add(GlobalAveragePooling2D()) model.add(Dense(300, activation=""relu"", name=\'rl\')) model.add(Dropout(0.5)) model.add(Dense(1,activation=\'sigmoid\', name=\'sm\'))"
"model = Sequential()  model.add(Conv2D(32, (5, 5), strides = (1, 1), name = \'conv0\', input_shape = (32, 32, 1)))  model.add(BatchNormalization(axis = 3, name = \'bn0\')) model.add(Activation(\'relu\'))  model.add(MaxPooling2D((2, 2), name=\'max_pool\')) model.add(Conv2D(64, (3, 3), strides = (1,1), name=""conv1"")) model.add(Activation(\'relu\')) model.add(AveragePooling2D((3, 3), name=\'avg_pool\'))  model.add(GlobalAveragePooling2D()) model.add(Dense(300, activation=""relu"", name=\'rl\')) model.add(Dropout(0.5)) model.add(Dense(1,activation=\'sigmoid\', name=\'sm\'))"	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     temp = cv2.imread(train_dir + img_id)     temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)     temp = np.expand_dims(temp, axis=2)     X_tr.append(temp)         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     temp = cv2.imread(train_dir + img_id)     temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)     temp = np.expand_dims(temp, axis=2)     X_tr.append(temp)         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 500
batch_size = 32 nb_epoch = 500	%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.2,               shuffle=True,               verbose=2)
%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.2,               shuffle=True,               verbose=2)	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     temp = cv2.imread(test_dir + img_id)     temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)     temp = np.expand_dims(temp, axis=2)     X_tst.append(temp)           Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     temp = cv2.imread(test_dir + img_id)     temp = cv2.cvtColor(temp, cv2.COLOR_BGR2GRAY)     temp = np.expand_dims(temp, axis=2)     X_tst.append(temp)           Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst)
# Prediction test_predictions = model.predict(X_tst)	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.7 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.7 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG16 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	vgg16_net = VGG16(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))
vgg16_net = VGG16(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))	vgg16_net.trainable = False vgg16_net.summary()
vgg16_net.trainable = False vgg16_net.summary()	model = Sequential() model.add(vgg16_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))
model = Sequential() model.add(vgg16_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 1000
batch_size = 32 nb_epoch = 1000	%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)
%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst)
# Prediction test_predictions = model.predict(X_tst)	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
# libraries import numpy as np import pandas as pd import os import cv2 import matplotlib.pyplot as plt %matplotlib inline  from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import torch from torch.utils.data import TensorDataset, DataLoader,Dataset import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import torch.optim as optim from torch.optim import lr_scheduler import time  from PIL import Image train_on_gpu = True from torch.utils.data.sampler import SubsetRandomSampler from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR from sklearn.metrics import accuracy_score import cv2	!pip install albumentations > /dev/null 2>&1 !pip install pretrainedmodels > /dev/null 2>&1 !pip install kekas > /dev/null 2>&1 !pip install adabound > /dev/null 2>&1
!pip install albumentations > /dev/null 2>&1 !pip install pretrainedmodels > /dev/null 2>&1 !pip install kekas > /dev/null 2>&1 !pip install adabound > /dev/null 2>&1	# more imports import albumentations from albumentations import torch as AT import pretrainedmodels import adabound  from kekas import Keker, DataOwner, DataKek from kekas.transformations import Transformer, to_torch, normalize from kekas.metrics import accuracy from kekas.modules import Flatten, AdaptiveConcatPool2d from kekas.callbacks import Callback, Callbacks, DebuggerCallback from kekas.utils import DotDict
# more imports import albumentations from albumentations import torch as AT import pretrainedmodels import adabound  from kekas import Keker, DataOwner, DataKek from kekas.transformations import Transformer, to_torch, normalize from kekas.metrics import accuracy from kekas.modules import Flatten, AdaptiveConcatPool2d from kekas.callbacks import Callback, Callbacks, DebuggerCallback from kekas.utils import DotDict	"labels = pd.read_csv(\'../input/train.csv\') fig = plt.figure(figsize=(25, 8)) train_imgs = os.listdir(""../input/train/train"") for idx, img in enumerate(np.random.choice(train_imgs, 20)):     ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])     im = Image.open(""../input/train/train/"" + img)     plt.imshow(im)     lab = labels.loc[labels[\'id\'] == img, \'has_cactus\'].values[0]     ax.set_title(f\'Label: {lab}\')"
"labels = pd.read_csv(\'../input/train.csv\') fig = plt.figure(figsize=(25, 8)) train_imgs = os.listdir(""../input/train/train"") for idx, img in enumerate(np.random.choice(train_imgs, 20)):     ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])     im = Image.open(""../input/train/train/"" + img)     plt.imshow(im)     lab = labels.loc[labels[\'id\'] == img, \'has_cactus\'].values[0]     ax.set_title(f\'Label: {lab}\')"	test_img = os.listdir('../input/test/test') test_df = pd.DataFrame(test_img, columns=['id']) test_df['has_cactus'] = -1 test_df['data_type'] = 'test'  labels['has_cactus'] = labels['has_cactus'].astype(int) labels['data_type'] = 'train'  labels.head()
test_img = os.listdir('../input/test/test') test_df = pd.DataFrame(test_img, columns=['id']) test_df['has_cactus'] = -1 test_df['data_type'] = 'test'  labels['has_cactus'] = labels['has_cactus'].astype(int) labels['data_type'] = 'train'  labels.head()	labels.loc[labels['data_type'] == 'train', 'has_cactus'].value_counts()
labels.loc[labels['data_type'] == 'train', 'has_cactus'].value_counts()	# splitting data into train and validation train, valid = train_test_split(labels, stratify=labels.has_cactus, test_size=0.1)
# splitting data into train and validation train, valid = train_test_split(labels, stratify=labels.has_cactus, test_size=0.1)	"def reader_fn(i, row):     image = cv2.imread(f""../input/{row[\'data_type\']}/{row[\'data_type\']}/{row[\'id\']}"")[:,:,::-1] # BGR -> RGB     label = torch.Tensor([row[""has_cactus""]])     return {""image"": image, ""label"": label}"
"def reader_fn(i, row):     image = cv2.imread(f""../input/{row[\'data_type\']}/{row[\'data_type\']}/{row[\'id\']}"")[:,:,::-1] # BGR -> RGB     label = torch.Tensor([row[""has_cactus""]])     return {""image"": image, ""label"": label}"	def augs(p=0.5):     return albumentations.Compose([         albumentations.HorizontalFlip(),         albumentations.Transpose(),         albumentations.Flip(),     ], p=p)
def augs(p=0.5):     return albumentations.Compose([         albumentations.HorizontalFlip(),         albumentations.Transpose(),         albumentations.Flip(),     ], p=p)	"def get_transforms(dataset_key, size, p):      PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[""image""])      NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])          return train_tfms, val_tfms"
"def get_transforms(dataset_key, size, p):      PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[""image""])      NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])          return train_tfms, val_tfms"	"train_tfms, val_tfms = get_transforms(""image"", 32, 0.5)"
"train_tfms, val_tfms = get_transforms(""image"", 32, 0.5)"	train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms) val_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)  batch_size = 32 workers = 0  train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True) val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)
train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms) val_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)  batch_size = 32 workers = 0  train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True) val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)	test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms) test_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)
test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms) test_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)	"class Net(nn.Module):     def __init__(             self,             num_classes: int,             p: float = 0.5,             pooling_size: int = 2,             last_conv_size: int = 2048,             arch: str = ""se_resnext50_32x4d"",             pretrained: str = ""imagenet"") -> None:         """"""A simple model to finetune.                  Args:             num_classes: the number of target classes, the size of the last layer\'s output             p: dropout probability             pooling_size: the size of the result feature map after adaptive pooling layer             last_conv_size: size of the flatten last backbone conv layer             arch: the name of the architecture form pretrainedmodels             pretrained: the mode for pretrained model from pretrainedmodels         """"""         super().__init__()         net = pretrainedmodels.__dict__[arch](pretrained=pretrained)         modules = list(net.children())[:-2]  # delete last layers: pooling and linear                  # add custom head         modules += [nn.Sequential(             # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling              AdaptiveConcatPool2d(size=pooling_size),             Flatten(),             nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),             nn.Dropout(p),             nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)         )]         self.net = nn.Sequential(*modules)      def forward(self, x):         logits = self.net(x)         return logits"
"class Net(nn.Module):     def __init__(             self,             num_classes: int,             p: float = 0.5,             pooling_size: int = 2,             last_conv_size: int = 2048,             arch: str = ""se_resnext50_32x4d"",             pretrained: str = ""imagenet"") -> None:         """"""A simple model to finetune.                  Args:             num_classes: the number of target classes, the size of the last layer\'s output             p: dropout probability             pooling_size: the size of the result feature map after adaptive pooling layer             last_conv_size: size of the flatten last backbone conv layer             arch: the name of the architecture form pretrainedmodels             pretrained: the mode for pretrained model from pretrainedmodels         """"""         super().__init__()         net = pretrainedmodels.__dict__[arch](pretrained=pretrained)         modules = list(net.children())[:-2]  # delete last layers: pooling and linear                  # add custom head         modules += [nn.Sequential(             # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling              AdaptiveConcatPool2d(size=pooling_size),             Flatten(),             nn.BatchNorm1d(2 * pooling_size * pooling_size * last_conv_size),             nn.Dropout(p),             nn.Linear(2 * pooling_size * pooling_size * last_conv_size, num_classes)         )]         self.net = nn.Sequential(*modules)      def forward(self, x):         logits = self.net(x)         return logits"	dataowner = DataOwner(train_dl, val_dl, None) model = Net(num_classes=1) criterion = nn.BCEWithLogitsLoss()
dataowner = DataOwner(train_dl, val_dl, None) model = Net(num_classes=1) criterion = nn.BCEWithLogitsLoss()	"def step_fn(model: torch.nn.Module,             batch: torch.Tensor) -> torch.Tensor:     """"""Determine what your model will do with your data.      Args:         model: the pytorch module to pass input in         batch: the batch of data from the DataLoader      Returns:         The models forward pass results     """"""          inp = batch[""image""]     return model(inp)"
"def step_fn(model: torch.nn.Module,             batch: torch.Tensor) -> torch.Tensor:     """"""Determine what your model will do with your data.      Args:         model: the pytorch module to pass input in         batch: the batch of data from the DataLoader      Returns:         The models forward pass results     """"""          inp = batch[""image""]     return model(inp)"	def bce_accuracy(target: torch.Tensor,                  preds: torch.Tensor,                  thresh: bool = 0.5) -> float:     target = target.cpu().detach().numpy()     preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)     return accuracy_score(target, preds)    def roc_auc(target: torch.Tensor,                  preds: torch.Tensor) -> float:     target = target.cpu().detach().numpy()     preds = torch.sigmoid(preds).cpu().detach().numpy()     return roc_auc_score(target, preds)
def bce_accuracy(target: torch.Tensor,                  preds: torch.Tensor,                  thresh: bool = 0.5) -> float:     target = target.cpu().detach().numpy()     preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)     return accuracy_score(target, preds)    def roc_auc(target: torch.Tensor,                  preds: torch.Tensor) -> float:     target = target.cpu().detach().numpy()     preds = torch.sigmoid(preds).cpu().detach().numpy()     return roc_auc_score(target, preds)	"keker = Keker(model=model,               dataowner=dataowner,               criterion=criterion,               step_fn=step_fn,               target_key=""label"",               metrics={""acc"": bce_accuracy, \'auc\': roc_auc},               opt=torch.optim.SGD,               opt_params={""momentum"": 0.99})"
"keker = Keker(model=model,               dataowner=dataowner,               criterion=criterion,               step_fn=step_fn,               target_key=""label"",               metrics={""acc"": bce_accuracy, \'auc\': roc_auc},               opt=torch.optim.SGD,               opt_params={""momentum"": 0.99})"	"keker.unfreeze(model_attr=""net"")  layer_num = -1 keker.freeze_to(layer_num, model_attr=""net"")"
"keker.unfreeze(model_attr=""net"")  layer_num = -1 keker.freeze_to(layer_num, model_attr=""net"")"	keker.kek_one_cycle(max_lr=1e-2,                  # the maximum learning rate                     cycle_len=5,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs') keker.plot_kek('train_logs')
keker.kek_one_cycle(max_lr=1e-2,                  # the maximum learning rate                     cycle_len=5,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs') keker.plot_kek('train_logs')	 keker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate                     cycle_len=3,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs1') keker.plot_kek('train_logs1')
 keker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate                     cycle_len=3,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs1') keker.plot_kek('train_logs1')	preds = keker.predict_loader(loader=test_dl)
preds = keker.predict_loader(loader=test_dl)	"flip_ = albumentations.HorizontalFlip(always_apply=True) transpose_ = albumentations.Transpose(always_apply=True)  def insert_aug(aug, dataset_key=""image"", size=224):         PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))          AUGS = Transformer(dataset_key, lambda x: aug(image=x)[""image""])          NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     return tfm  flip = insert_aug(flip_) transpose = insert_aug(transpose_)  tta_tfms = {""flip"": flip, ""transpose"": transpose}  # third, run TTA keker.TTA(loader=test_dl,                # loader to predict on            tfms=tta_tfms,                # list or dict of always applying transforms           savedir=""tta_preds1"",  # savedir           prefix=""preds"")               # (optional) name prefix. default is \'preds\'"
"flip_ = albumentations.HorizontalFlip(always_apply=True) transpose_ = albumentations.Transpose(always_apply=True)  def insert_aug(aug, dataset_key=""image"", size=224):         PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))          AUGS = Transformer(dataset_key, lambda x: aug(image=x)[""image""])          NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     return tfm  flip = insert_aug(flip_) transpose = insert_aug(transpose_)  tta_tfms = {""flip"": flip, ""transpose"": transpose}  # third, run TTA keker.TTA(loader=test_dl,                # loader to predict on            tfms=tta_tfms,                # list or dict of always applying transforms           savedir=""tta_preds1"",  # savedir           prefix=""preds"")               # (optional) name prefix. default is \'preds\'"	prediction = np.zeros((test_df.shape[0], 1)) for i in os.listdir('tta_preds1'):     pr = np.load('tta_preds1/' + i)     prediction += pr prediction = prediction / len(os.listdir('tta_preds1'))
prediction = np.zeros((test_df.shape[0], 1)) for i in os.listdir('tta_preds1'):     pr = np.load('tta_preds1/' + i)     prediction += pr prediction = prediction / len(os.listdir('tta_preds1'))	test_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': preds.reshape(-1,)}) test_preds.columns = ['id', 'has_cactus'] test_preds.to_csv('sub.csv', index=False) test_preds.head()
"import gc import glob import os import cv2 import random import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import imageio as im from keras import models from keras.models import Sequential from keras.layers import Activation, Dense, Dropout, Flatten from keras.layers import Conv2D, MaxPooling2D from keras.optimizers import adam from keras.preprocessing import image from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import ModelCheckpoint from keras.utils import np_utils from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import seaborn as sns import matplotlib from matplotlib import pyplot as plt %matplotlib inline  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input/""))  # Any results you write to the current directory are saved as output."	"# load images dataset def loadImagesData(glob_path):     images = []     names = []     for img_path in glob.glob(glob_path):         # load/resize images with cv2         names.append(os.path.basename(img_path))         img = cv2.imread(img_path, cv2.IMREAD_COLOR)         images.append(img) # already 32x32     return (images,names) # map of training label to list of images trainData = {} namesData = {} for label in os.listdir(\'../input/train/\'):     (images,names) = loadImagesData(f""../input/train/{label}/*.jpg"")     trainData[label] = images     namesData[label] = names print(""train labels:"", "","".join(trainData.keys())) print(len(trainData[\'train\'])) # show some data plt.figure(figsize=(4,2)) columns = 4 for i in range(0,8):     plt.subplot(8 / columns + 1, columns, i + 1)     plt.imshow(trainData[\'train\'][i]) plt.show()"
"# load images dataset def loadImagesData(glob_path):     images = []     names = []     for img_path in glob.glob(glob_path):         # load/resize images with cv2         names.append(os.path.basename(img_path))         img = cv2.imread(img_path, cv2.IMREAD_COLOR)         images.append(img) # already 32x32     return (images,names) # map of training label to list of images trainData = {} namesData = {} for label in os.listdir(\'../input/train/\'):     (images,names) = loadImagesData(f""../input/train/{label}/*.jpg"")     trainData[label] = images     namesData[label] = names print(""train labels:"", "","".join(trainData.keys())) print(len(trainData[\'train\'])) # show some data plt.figure(figsize=(4,2)) columns = 4 for i in range(0,8):     plt.subplot(8 / columns + 1, columns, i + 1)     plt.imshow(trainData[\'train\'][i]) plt.show()"	train_meta = pd.read_csv('../input/train.csv') print(train_meta.shape) print(train_meta.has_cactus.value_counts()) # lookup table of name to has_cactus lookupY = {} for i in range(0,len(train_meta)):     row = train_meta.iloc[i,:]     lookupY[row.id] = row.has_cactus train_meta.head()
train_meta = pd.read_csv('../input/train.csv') print(train_meta.shape) print(train_meta.has_cactus.value_counts()) # lookup table of name to has_cactus lookupY = {} for i in range(0,len(train_meta)):     row = train_meta.iloc[i,:]     lookupY[row.id] = row.has_cactus train_meta.head()	# build x/y dataset trainList = [] maxCount = 4364 # number of has_cactus = 0 counts = {'0':0,'1':0} for (i,image) in enumerate(trainData['train']):     label = lookupY[namesData['train'][i]]     counts[str(label)] = 1 + counts[str(label)]     if counts[str(label)] < maxCount:         trainList.append({             'label': label,             'data': image         }) # shuffle dataset random.shuffle(trainList) # dataframe and display train_df = pd.DataFrame(trainList) gc.collect() print(train_df.shape) print(train_df.label.value_counts()) train_df.head()
# build x/y dataset trainList = [] maxCount = 4364 # number of has_cactus = 0 counts = {'0':0,'1':0} for (i,image) in enumerate(trainData['train']):     label = lookupY[namesData['train'][i]]     counts[str(label)] = 1 + counts[str(label)]     if counts[str(label)] < maxCount:         trainList.append({             'label': label,             'data': image         }) # shuffle dataset random.shuffle(trainList) # dataframe and display train_df = pd.DataFrame(trainList) gc.collect() print(train_df.shape) print(train_df.label.value_counts()) train_df.head()	# encode training data data_stack = np.stack(train_df['data'].values) dfloats = data_stack.astype(np.float) all_x = np.multiply(dfloats, 1.0 / 255.0) all_x.shape
# encode training data data_stack = np.stack(train_df['data'].values) dfloats = data_stack.astype(np.float) all_x = np.multiply(dfloats, 1.0 / 255.0) all_x.shape	all_y = np.array(train_df.label).astype(np.float) all_y[0:5]
all_y = np.array(train_df.label).astype(np.float) all_y[0:5]	# split test/training data train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7) print(train_x.shape,test_x.shape)
# split test/training data train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7) print(train_x.shape,test_x.shape)	# x,y and rotation data augmentation datagen = ImageDataGenerator(     featurewise_center=False,  # set input mean to 0 over the dataset     samplewise_center=False,  # set each sample mean to 0     featurewise_std_normalization=False,  # divide inputs by std of the dataset     samplewise_std_normalization=False,  # divide each input by its std     rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)     zoom_range=0.2, # zoom images     horizontal_flip=True,  # randomly flip images     vertical_flip=True)  # randomly flip images datagen.fit(train_x)
# x,y and rotation data augmentation datagen = ImageDataGenerator(     featurewise_center=False,  # set input mean to 0 over the dataset     samplewise_center=False,  # set each sample mean to 0     featurewise_std_normalization=False,  # divide inputs by std of the dataset     samplewise_std_normalization=False,  # divide each input by its std     rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)     zoom_range=0.2, # zoom images     horizontal_flip=True,  # randomly flip images     vertical_flip=True)  # randomly flip images datagen.fit(train_x)	# create the network num_filters = 8 input_shape = train_x.shape[1:] output_shape = 1 # model m = Sequential() def tdsNet(m):     m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))     m.add(Conv2D(16, kernel_size=3, activation='relu'))     m.add(Flatten())     m.add(Dense(units = output_shape, activation='sigmoid')) tdsNet(m) # compile adam with decay and use binary_crossentropy for single category dataset m.compile(optimizer = 'nadam',           loss = 'binary_crossentropy',            metrics = ['accuracy']) # show summary m.summary()
# create the network num_filters = 8 input_shape = train_x.shape[1:] output_shape = 1 # model m = Sequential() def tdsNet(m):     m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))     m.add(Conv2D(16, kernel_size=3, activation='relu'))     m.add(Flatten())     m.add(Dense(units = output_shape, activation='sigmoid')) tdsNet(m) # compile adam with decay and use binary_crossentropy for single category dataset m.compile(optimizer = 'nadam',           loss = 'binary_crossentropy',            metrics = ['accuracy']) # show summary m.summary()	# train model batch_size = 32 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)
# train model batch_size = 32 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)	# create the network num_filters = 8 input_shape = train_x.shape[1:] output_shape = 1 # model m = Sequential() def cnnNet(m):     m.add(Conv2D(30, kernel_size=3, activation='relu', input_shape=input_shape))     m.add(MaxPooling2D(2,2))     m.add(Conv2D(15, kernel_size=3, activation='relu'))     m.add(MaxPooling2D(2,2))     m.add(Dense(7, activation='relu')) # <7 stops working, but higher values do nothing     m.add(Flatten())     m.add(Dense(units = output_shape, activation='sigmoid')) cnnNet(m) # compile adam with decay and use binary_crossentropy for single category dataset m.compile(optimizer = 'nadam',           loss = 'binary_crossentropy',            metrics = ['accuracy']) # show summary m.summary()
# create the network num_filters = 8 input_shape = train_x.shape[1:] output_shape = 1 # model m = Sequential() def cnnNet(m):     m.add(Conv2D(30, kernel_size=3, activation='relu', input_shape=input_shape))     m.add(MaxPooling2D(2,2))     m.add(Conv2D(15, kernel_size=3, activation='relu'))     m.add(MaxPooling2D(2,2))     m.add(Dense(7, activation='relu')) # <7 stops working, but higher values do nothing     m.add(Flatten())     m.add(Dense(units = output_shape, activation='sigmoid')) cnnNet(m) # compile adam with decay and use binary_crossentropy for single category dataset m.compile(optimizer = 'nadam',           loss = 'binary_crossentropy',            metrics = ['accuracy']) # show summary m.summary()	# train model batch_size = 32 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)
# train model batch_size = 32 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)	# build complete x/y dataset trainList = [] for (i,image) in enumerate(trainData['train']):     label = lookupY[namesData['train'][i]]     trainList.append({         'label': label,         'data': image     }) # shuffle dataset random.shuffle(trainList) # dataframe and display train_df = pd.DataFrame(trainList) gc.collect() # encode training data data_stack = np.stack(train_df['data'].values) dfloats = data_stack.astype(np.float) all_x = np.multiply(dfloats, 1.0 / 255.0) all_x.shape all_y = np.array(train_df.label).astype(np.float) # split test/training data train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7) print(train_x.shape,test_x.shape)
# build complete x/y dataset trainList = [] for (i,image) in enumerate(trainData['train']):     label = lookupY[namesData['train'][i]]     trainList.append({         'label': label,         'data': image     }) # shuffle dataset random.shuffle(trainList) # dataframe and display train_df = pd.DataFrame(trainList) gc.collect() # encode training data data_stack = np.stack(train_df['data'].values) dfloats = data_stack.astype(np.float) all_x = np.multiply(dfloats, 1.0 / 255.0) all_x.shape all_y = np.array(train_df.label).astype(np.float) # split test/training data train_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7) print(train_x.shape,test_x.shape)	# continue training model batch_size = 64 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)
# continue training model batch_size = 64 history = m.fit_generator(datagen.flow(train_x, train_y,                           batch_size=batch_size),                           steps_per_epoch= (train_x.shape[0] // batch_size),                           epochs = 4,                           validation_data=(test_x, test_y),                           workers=4)	# check sample submission format pd.read_csv('../input/sample_submission.csv').head()
# check sample submission format pd.read_csv('../input/sample_submission.csv').head()	"# output predicted submission csv (test_images, test_names) = loadImagesData(f""../input/test/test/*.jpg"") data_stack = np.stack(test_images) dfloats = data_stack.astype(np.float32) unknown_x = np.multiply(dfloats, 1.0 / 255.0) # predict predicted = np.ravel(m.predict(unknown_x)) submission_df = pd.DataFrame({\'id\':test_names,\'has_cactus\':predicted}) submission_df.to_csv(\'submission.csv\', index=False) len(submission_df)"
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	import os import cv2 import numpy as np import pandas as pd import tensorflow as tf import tensorflow.contrib.slim as slim  from tqdm import tqdm from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout from keras.models import Sequential from keras.optimizers import Adam
import os import cv2 import numpy as np import pandas as pd import tensorflow as tf import tensorflow.contrib.slim as slim  from tqdm import tqdm from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout from keras.models import Sequential from keras.optimizers import Adam	"class DataLoader:     def __init__(self, npy_file: str = ""npy_data""):         self.npy_file = npy_file         self.csv_name = ""../input/train.csv""         self.df = self.read_csv()         self.n_classes = 2          os.makedirs(self.npy_file, exist_ok=True)      def read_csv(self):         df = pd.read_csv(self.csv_name)          return df      def read_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,                   save: bool = True, categorical: bool = False, n_classes: int = 2):          x_data = []         y_data = []          if load_from_npy:             try:                 x_data = np.load(fr""{self.npy_file}/x_data.npy"")                 y_data = np.load(fr""{self.npy_file}/y_data.npy"")             except FileNotFoundError:                 load_from_npy = False                 print(""NPY files not found!"")                 pass          if not load_from_npy:             x_data = []             y_data = []              for dir_label in tqdm(self.df.values):                 img = cv2.imread(os.path.join(""../input"", ""train/train"", dir_label[0]))                  if make_gray:                     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  img = cv2.resize(img, size2resize)                  x_data.append(img)                 y_data.append(int(dir_label[1]))                  del img              x_data = np.array(x_data)             y_data = np.array(y_data)              if save:                 np.save(fr""{self.npy_file}/x_data.npy"", x_data)                 np.save(fr""{self.npy_file}/y_data.npy"", y_data)          if categorical:             y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)          if not categorical:             y_data = y_data.reshape(-1, 1)          if load_from_npy and make_gray:             try:                 x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]                 x_data = x_data_2             except cv2.error:                 pass          if make_gray:             x_data = np.expand_dims(x_data, axis=-1)          return x_data, y_data      def read_test_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,                   save: bool = True, categorical: bool = False, n_classes: int = 2):          test_df = pd.read_csv(""../input/sample_submission.csv"")          x_data = []         y_data = []          if load_from_npy:             try:                 x_data = np.load(fr""{self.npy_file}/x_data_test.npy"")                 y_data = np.load(fr""{self.npy_file}/y_data_test.npy"")             except FileNotFoundError:                 load_from_npy = False                 print(""NPY files not found!"")                 pass          if not load_from_npy:             x_data = []             y_data = []              for dir_label in tqdm(test_df.values):                 img = cv2.imread(os.path.join(""../input"", ""test/test"", dir_label[0]))                  if make_gray:                     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  img = cv2.resize(img, size2resize)                  x_data.append(img)                 y_data.append(int(dir_label[1]))                  del img              x_data = np.array(x_data)             y_data = np.array(y_data)              if save:                 np.save(fr""{self.npy_file}/x_data_test.npy"", x_data)                 np.save(fr""{self.npy_file}/y_data_test.npy"", y_data)          if categorical:             y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)          if not categorical:             y_data = y_data.reshape(-1, 1)          if load_from_npy and make_gray:             try:                 x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]                 x_data = x_data_2             except cv2.error:                 pass          if make_gray:             x_data = np.expand_dims(x_data, axis=-1)          return x_data, y_data"
"class DataLoader:     def __init__(self, npy_file: str = ""npy_data""):         self.npy_file = npy_file         self.csv_name = ""../input/train.csv""         self.df = self.read_csv()         self.n_classes = 2          os.makedirs(self.npy_file, exist_ok=True)      def read_csv(self):         df = pd.read_csv(self.csv_name)          return df      def read_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,                   save: bool = True, categorical: bool = False, n_classes: int = 2):          x_data = []         y_data = []          if load_from_npy:             try:                 x_data = np.load(fr""{self.npy_file}/x_data.npy"")                 y_data = np.load(fr""{self.npy_file}/y_data.npy"")             except FileNotFoundError:                 load_from_npy = False                 print(""NPY files not found!"")                 pass          if not load_from_npy:             x_data = []             y_data = []              for dir_label in tqdm(self.df.values):                 img = cv2.imread(os.path.join(""../input"", ""train/train"", dir_label[0]))                  if make_gray:                     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  img = cv2.resize(img, size2resize)                  x_data.append(img)                 y_data.append(int(dir_label[1]))                  del img              x_data = np.array(x_data)             y_data = np.array(y_data)              if save:                 np.save(fr""{self.npy_file}/x_data.npy"", x_data)                 np.save(fr""{self.npy_file}/y_data.npy"", y_data)          if categorical:             y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)          if not categorical:             y_data = y_data.reshape(-1, 1)          if load_from_npy and make_gray:             try:                 x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]                 x_data = x_data_2             except cv2.error:                 pass          if make_gray:             x_data = np.expand_dims(x_data, axis=-1)          return x_data, y_data      def read_test_data(self, load_from_npy: bool = True, size2resize: tuple = (75, 75), make_gray: bool = True,                   save: bool = True, categorical: bool = False, n_classes: int = 2):          test_df = pd.read_csv(""../input/sample_submission.csv"")          x_data = []         y_data = []          if load_from_npy:             try:                 x_data = np.load(fr""{self.npy_file}/x_data_test.npy"")                 y_data = np.load(fr""{self.npy_file}/y_data_test.npy"")             except FileNotFoundError:                 load_from_npy = False                 print(""NPY files not found!"")                 pass          if not load_from_npy:             x_data = []             y_data = []              for dir_label in tqdm(test_df.values):                 img = cv2.imread(os.path.join(""../input"", ""test/test"", dir_label[0]))                  if make_gray:                     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  img = cv2.resize(img, size2resize)                  x_data.append(img)                 y_data.append(int(dir_label[1]))                  del img              x_data = np.array(x_data)             y_data = np.array(y_data)              if save:                 np.save(fr""{self.npy_file}/x_data_test.npy"", x_data)                 np.save(fr""{self.npy_file}/y_data_test.npy"", y_data)          if categorical:             y_data = tf.keras.utils.to_categorical(y_data, num_classes=n_classes)          if not categorical:             y_data = y_data.reshape(-1, 1)          if load_from_npy and make_gray:             try:                 x_data_2 = [cv2.cvtColor(n, cv2.COLOR_BGR2GRAY) for n in x_data]                 x_data = x_data_2             except cv2.error:                 pass          if make_gray:             x_data = np.expand_dims(x_data, axis=-1)          return x_data, y_data"	"class TrainWithKeras:     def __init__(self, x_data, y_data, lr: float = 0.001, epochs: int = 10, batch_size: int = 32,                  loss: str = ""categorical_crossentropy"", model_path: str = ""model.h5""):         self.x_data = x_data         self.y_data = y_data         self.model_path = model_path          self.epochs = epochs         self.batch_size = batch_size          self.optimizer = Adam(lr=lr)         self.loss = loss      def make_model(self, summarize: bool = True):         model = Sequential()          model.add(Conv2D(64, (3, 3), strides=1, activation=""relu"",                          input_shape=(self.x_data.shape[1], self.x_data.shape[2], self.x_data.shape[3])))         model.add(MaxPooling2D())         model.add(Conv2D(128, (3, 3), strides=1, activation=""relu""))         model.add(Dropout(0.3))         model.add(BatchNormalization())          model.add(Conv2D(256, (3, 3), strides=1, activation=""relu""))         model.add(MaxPooling2D())         model.add(Conv2D(512, (3, 3), strides=1, activation=""relu""))         model.add(Dropout(0.3))          model.add(Conv2D(1024, (3, 3), strides=1, activation=""relu""))                  model.add(Flatten())          model.add(Dense(1024, activation=""relu""))         model.add(Dropout(0.3))         model.add(Dense(2, activation=""softmax""))          if summarize:             model.summary()          return model      def compile(self, kmodel: Sequential):         kmodel.compile(loss=self.loss, optimizer=self.optimizer, metrics=[""acc""])          return kmodel      def train(self, kmodel: Sequential, save: bool = True):         history = kmodel.fit(self.x_data, self.y_data, batch_size=self.batch_size, epochs=self.epochs,                              validation_split=0.0)          if save:             kmodel.save(self.model_path)          return history, kmodel"
"class TrainWithKeras:     def __init__(self, x_data, y_data, lr: float = 0.001, epochs: int = 10, batch_size: int = 32,                  loss: str = ""categorical_crossentropy"", model_path: str = ""model.h5""):         self.x_data = x_data         self.y_data = y_data         self.model_path = model_path          self.epochs = epochs         self.batch_size = batch_size          self.optimizer = Adam(lr=lr)         self.loss = loss      def make_model(self, summarize: bool = True):         model = Sequential()          model.add(Conv2D(64, (3, 3), strides=1, activation=""relu"",                          input_shape=(self.x_data.shape[1], self.x_data.shape[2], self.x_data.shape[3])))         model.add(MaxPooling2D())         model.add(Conv2D(128, (3, 3), strides=1, activation=""relu""))         model.add(Dropout(0.3))         model.add(BatchNormalization())          model.add(Conv2D(256, (3, 3), strides=1, activation=""relu""))         model.add(MaxPooling2D())         model.add(Conv2D(512, (3, 3), strides=1, activation=""relu""))         model.add(Dropout(0.3))          model.add(Conv2D(1024, (3, 3), strides=1, activation=""relu""))                  model.add(Flatten())          model.add(Dense(1024, activation=""relu""))         model.add(Dropout(0.3))         model.add(Dense(2, activation=""softmax""))          if summarize:             model.summary()          return model      def compile(self, kmodel: Sequential):         kmodel.compile(loss=self.loss, optimizer=self.optimizer, metrics=[""acc""])          return kmodel      def train(self, kmodel: Sequential, save: bool = True):         history = kmodel.fit(self.x_data, self.y_data, batch_size=self.batch_size, epochs=self.epochs,                              validation_split=0.0)          if save:             kmodel.save(self.model_path)          return history, kmodel"	"class MakeSubmission:     def __init__(self, x_test: np.array, model_path: str, csv_path: str):         self.x_test = x_test         self.model_path = model_path         self.csv_path = csv_path          self.model = tf.keras.models.load_model(self.model_path)         self.df = pd.read_csv(self.csv_path)          preds = self.make_predictions()          submission = pd.DataFrame({\'id\': self.df[\'id\'], \'has_cactus\': preds})         submission.to_csv(""sample_submission.csv"", index=False)      def make_predictions(self, make_it_ready: bool = True):         preds = self.model.predict(self.x_test)          if make_it_ready:             preds = [np.argmax(n) for n in preds]          return preds"
"class MakeSubmission:     def __init__(self, x_test: np.array, model_path: str, csv_path: str):         self.x_test = x_test         self.model_path = model_path         self.csv_path = csv_path          self.model = tf.keras.models.load_model(self.model_path)         self.df = pd.read_csv(self.csv_path)          preds = self.make_predictions()          submission = pd.DataFrame({\'id\': self.df[\'id\'], \'has_cactus\': preds})         submission.to_csv(""sample_submission.csv"", index=False)      def make_predictions(self, make_it_ready: bool = True):         preds = self.model.predict(self.x_test)          if make_it_ready:             preds = [np.argmax(n) for n in preds]          return preds"	"os.makedirs(""models"", exist_ok=True)  dl = DataLoader() X_data, Y_data = dl.read_data(True, (32, 32), False, True, True, 2)"
"os.makedirs(""models"", exist_ok=True)  dl = DataLoader() X_data, Y_data = dl.read_data(True, (32, 32), False, True, True, 2)"	"trainer = TrainWithKeras(X_data, Y_data, model_path=""models/model.h5"", epochs=50, batch_size=1024, lr=0.0002) model = trainer.make_model() model = trainer.compile(model)  histroy = trainer.train(model)"
"trainer = TrainWithKeras(X_data, Y_data, model_path=""models/model.h5"", epochs=50, batch_size=1024, lr=0.0002) model = trainer.make_model() model = trainer.compile(model)  histroy = trainer.train(model)"	"X_data_test, Y_data_test = dl.read_test_data(True, (32, 32), False, True, False) ms = MakeSubmission(X_data_test, ""models/model.h5"", ""../input/sample_submission.csv"")"
"import numpy as np import pandas as pd from PIL import Image import os directory_train = ""../input/train/train/"" directory_test = ""../input/test/test/"""	"train_labels = pd.read_csv(""../input/train.csv"") train_labels.head()"
"train_labels = pd.read_csv(""../input/train.csv"") train_labels.head()"	image_names = [] for filename in os.listdir(directory_train):     image_names.append(filename) image_names.sort()
image_names = [] for filename in os.listdir(directory_train):     image_names.append(filename) image_names.sort()	test_image_names = [] for filename in os.listdir(directory_test):     test_image_names.append(filename) test_image_names.sort()
test_image_names = [] for filename in os.listdir(directory_test):     test_image_names.append(filename) test_image_names.sort()	images = [] for filename in image_names:     im = Image.open(directory_train+filename, 'r')     pix_val = list(im.getdata())     pix_val_flat = [x for sets in pix_val for x in sets]     images.append(pix_val_flat)
images = [] for filename in image_names:     im = Image.open(directory_train+filename, 'r')     pix_val = list(im.getdata())     pix_val_flat = [x for sets in pix_val for x in sets]     images.append(pix_val_flat)	test_images = [] for filename in test_image_names:     im = Image.open(directory_test+filename, 'r')     pix_val = list(im.getdata())     pix_val_flat = [x for sets in pix_val for x in sets]     test_images.append(pix_val_flat)
test_images = [] for filename in test_image_names:     im = Image.open(directory_test+filename, 'r')     pix_val = list(im.getdata())     pix_val_flat = [x for sets in pix_val for x in sets]     test_images.append(pix_val_flat)	"col_names = [""pxl_""+str(i) for i in range(len(images[0]))]  df_train=pd.DataFrame(images,columns=col_names) df_train[""has_cactus""] = train_labels.has_cactus   df_test = pd.DataFrame(test_images,columns=col_names)"
"col_names = [""pxl_""+str(i) for i in range(len(images[0]))]  df_train=pd.DataFrame(images,columns=col_names) df_train[""has_cactus""] = train_labels.has_cactus   df_test = pd.DataFrame(test_images,columns=col_names)"	df_train.head()
df_train.head()	df_test.head()
df_test.head()	#train_size = 5000 df = df_train#.iloc[:train_size]
#train_size = 5000 df = df_train#.iloc[:train_size]	""""""" !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm !git clone --recursive https://github.com/Microsoft/LightGBM      !apt-get install -y -qq libboost-all-dev  %%bash cd LightGBM rm -r build mkdir build cd build cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc)  !cd LightGBM/python-package/;python3 setup.py install --precompile  !mkdir -p /etc/OpenCL/vendors && echo ""libnvidia-opencl.so.1"" > /etc/OpenCL/vendors/nvidia.icd !rm -r LightGBM !nvidia-smi """""""
""""""" !rm -r /opt/conda/lib/python3.6/site-packages/lightgbm !git clone --recursive https://github.com/Microsoft/LightGBM      !apt-get install -y -qq libboost-all-dev  %%bash cd LightGBM rm -r build mkdir build cd build cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ .. make -j$(nproc)  !cd LightGBM/python-package/;python3 setup.py install --precompile  !mkdir -p /etc/OpenCL/vendors && echo ""libnvidia-opencl.so.1"" > /etc/OpenCL/vendors/nvidia.icd !rm -r LightGBM !nvidia-smi """""""	params = {'num_leaves': 15,          'min_data_in_leaf': 50,          'objective': 'binary',          'max_depth': 25,          'learning_rate': 0.01,#0.0123,          'boosting': 'goss',          'feature_fraction': 0.7,          'reg_alpha': 1.728,          'reg_lambda': 4.984,          'random_state': 42,          'metric': 'auc',          'verbosity': -1,          'subsample': 0.81,          'min_gain_to_split': 0.01,          'min_child_weight': 19.4,          'num_threads': 4,         # 'device': 'gpu',         #'gpu_platform_id': 0,         #'gpu_device_id': 0          }
params = {'num_leaves': 15,          'min_data_in_leaf': 50,          'objective': 'binary',          'max_depth': 25,          'learning_rate': 0.01,#0.0123,          'boosting': 'goss',          'feature_fraction': 0.7,          'reg_alpha': 1.728,          'reg_lambda': 4.984,          'random_state': 42,          'metric': 'auc',          'verbosity': -1,          'subsample': 0.81,          'min_gain_to_split': 0.01,          'min_child_weight': 19.4,          'num_threads': 4,         # 'device': 'gpu',         #'gpu_platform_id': 0,         #'gpu_device_id': 0          }	"import time from sklearn.model_selection import StratifiedKFold import lightgbm as lgb from sklearn import metrics  for i in range(4):     print(len(df))     t1=time.time()     target = \'has_cactus\'     predictors = df.columns.values.tolist()[:-1]     nfold=5     skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)     oof = np.zeros(len(df))     predictions = np.zeros(len(df_test))      i = 1     for train_index, valid_index in skf.split(df, df[target].values):         print(""\ fold {}"".format(i))         xg_train = lgb.Dataset(df.iloc[train_index][predictors].values,                                label=df.iloc[train_index][target].values,                                feature_name=predictors,                                free_raw_data = False                                )         xg_valid = lgb.Dataset(df.iloc[valid_index][predictors].values,                                label=df.iloc[valid_index][target].values,                                feature_name=predictors,                                free_raw_data = False                                )            clf = lgb.train(params, xg_train, 2000000, valid_sets = [xg_valid], verbose_eval=1000, early_stopping_rounds = 2000)         oof[valid_index] = clf.predict(df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration)           predictions += clf.predict(df_test[predictors], num_iteration=clf.best_iteration) / nfold         i = i + 1     t2=time.time()     print((t2-t1)/60)     print(""\ \ CV AUC: {:<0.4f}"".format(metrics.roc_auc_score(df[target].values, oof)))     df_test[""has_cactus""] = predictions     df = pd.concat([df, df_test[df_test.has_cactus > 0.95]])     df = pd.concat([df, df_test[df_test.has_cactus < 0.05]])     df[""has_cactus""] = df[""has_cactus""].round(0).astype(int)     df_test = df_test.drop(columns=[""has_cactus""])"
"import time from sklearn.model_selection import StratifiedKFold import lightgbm as lgb from sklearn import metrics  for i in range(4):     print(len(df))     t1=time.time()     target = \'has_cactus\'     predictors = df.columns.values.tolist()[:-1]     nfold=5     skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)     oof = np.zeros(len(df))     predictions = np.zeros(len(df_test))      i = 1     for train_index, valid_index in skf.split(df, df[target].values):         print(""\ fold {}"".format(i))         xg_train = lgb.Dataset(df.iloc[train_index][predictors].values,                                label=df.iloc[train_index][target].values,                                feature_name=predictors,                                free_raw_data = False                                )         xg_valid = lgb.Dataset(df.iloc[valid_index][predictors].values,                                label=df.iloc[valid_index][target].values,                                feature_name=predictors,                                free_raw_data = False                                )            clf = lgb.train(params, xg_train, 2000000, valid_sets = [xg_valid], verbose_eval=1000, early_stopping_rounds = 2000)         oof[valid_index] = clf.predict(df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration)           predictions += clf.predict(df_test[predictors], num_iteration=clf.best_iteration) / nfold         i = i + 1     t2=time.time()     print((t2-t1)/60)     print(""\ \ CV AUC: {:<0.4f}"".format(metrics.roc_auc_score(df[target].values, oof)))     df_test[""has_cactus""] = predictions     df = pd.concat([df, df_test[df_test.has_cactus > 0.95]])     df = pd.concat([df, df_test[df_test.has_cactus < 0.05]])     df[""has_cactus""] = df[""has_cactus""].round(0).astype(int)     df_test = df_test.drop(columns=[""has_cactus""])"	""""""" from sklearn.datasets import make_classification from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score from matplotlib import pyplot  auc = roc_auc_score(df[target].values, oof) print(\'AUC: %.3f\' % auc) fpr, tpr, thresholds = roc_curve(df[target].values, oof) pyplot.plot([0, 1], [0, 1], linestyle=\'--\') pyplot.plot(fpr, tpr, marker=\'.\') pyplot.show() """""""
""""""" from sklearn.datasets import make_classification from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score from matplotlib import pyplot  auc = roc_auc_score(df[target].values, oof) print(\'AUC: %.3f\' % auc) fpr, tpr, thresholds = roc_curve(df[target].values, oof) pyplot.plot([0, 1], [0, 1], linestyle=\'--\') pyplot.plot(fpr, tpr, marker=\'.\') pyplot.show() """""""	#predictions.tolist()
#predictions.tolist()	"sub = pd.read_csv(""../input/sample_submission.csv"") sub.has_cactus = predictions sub.to_csv(""submission_lgbm_4runs_5thres.csv"",index=False)"
import numpy as np import pandas as pd import os import fastai from fastai.vision import * from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt import pydicom import torchvision.models as models from tqdm import tqdm	"class RegMetrics(Callback):   ""Stores predictions and targets to perform calculations on epoch end.""   def on_epoch_begin(self, **kwargs):     self.targs, self.preds = Tensor([]), Tensor([])    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):     assert last_output[:, 1].numel() == last_target.numel(), ""Expected same numbers of elements in pred {} & targ {}"".format(last_output.shape, last_target.shape)     self.preds = torch.cat((self.preds, partial(F.softmax, dim=-1)(last_output)[:, 1].cpu()))     self.targs = torch.cat((self.targs, last_target.cpu().float()))  # Define some custom metrics  class AUCROC(RegMetrics):   """""" Compute the area under the receiver operating characteristic curve. """"""   def on_epoch_begin(self, **kwargs):     super().on_epoch_begin()        def on_epoch_end(self, **kwargs):     self.metric = roc_auc_score(self.targs, self.preds)"
"class RegMetrics(Callback):   ""Stores predictions and targets to perform calculations on epoch end.""   def on_epoch_begin(self, **kwargs):     self.targs, self.preds = Tensor([]), Tensor([])    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):     assert last_output[:, 1].numel() == last_target.numel(), ""Expected same numbers of elements in pred {} & targ {}"".format(last_output.shape, last_target.shape)     self.preds = torch.cat((self.preds, partial(F.softmax, dim=-1)(last_output)[:, 1].cpu()))     self.targs = torch.cat((self.targs, last_target.cpu().float()))  # Define some custom metrics  class AUCROC(RegMetrics):   """""" Compute the area under the receiver operating characteristic curve. """"""   def on_epoch_begin(self, **kwargs):     super().on_epoch_begin()        def on_epoch_end(self, **kwargs):     self.metric = roc_auc_score(self.targs, self.preds)"	df_train = pd.read_csv('../input/train.csv') sample_set = df_train.sample(6) print('Train.csv samples:') print(sample_set)  print('\ Number of training samples:{0}'.format(len(os.listdir('../input/train/train')))) print('Number of test samples:{0}'.format(len(os.listdir('../input/test/test'))))
df_train = pd.read_csv('../input/train.csv') sample_set = df_train.sample(6) print('Train.csv samples:') print(sample_set)  print('\ Number of training samples:{0}'.format(len(os.listdir('../input/train/train')))) print('Number of test samples:{0}'.format(len(os.listdir('../input/test/test'))))	"fig, ax = plt.subplots(2, 3) index = 0 for row in ax:     for col in row:         img = open_image(\'../input/train/train/\'+str(sample_set.iloc[index][""id""]))         img.show(col, title=\'Cactus:\'+str(sample_set.iloc[index][""has_cactus""]))         index += 1"
"fig, ax = plt.subplots(2, 3) index = 0 for row in ax:     for col in row:         img = open_image(\'../input/train/train/\'+str(sample_set.iloc[index][""id""]))         img.show(col, title=\'Cactus:\'+str(sample_set.iloc[index][""has_cactus""]))         index += 1"	train_path = '../input/train/train'  number_epochs=5  def train(arch):     tfms = get_transforms(do_flip=True, flip_vert=False, max_rotate=10.,                             max_zoom=1.1, max_lighting=0.2, max_warp=0.2,                              p_affine=1.0, p_lighting=0.0)          #setup data source     data = ImageDataBunch.from_df(path=train_path, df=df_train, label_col=1, bs=16, size=32, ds_tfms=tfms)      #define learner     learn = cnn_learner(data, arch, metrics=[accuracy, AUCROC()], model_dir='../../../models')      #train     learn.fit_one_cycle(number_epochs, 3e-3)          return learn 
train_path = '../input/train/train'  number_epochs=5  def train(arch):     tfms = get_transforms(do_flip=True, flip_vert=False, max_rotate=10.,                             max_zoom=1.1, max_lighting=0.2, max_warp=0.2,                              p_affine=1.0, p_lighting=0.0)          #setup data source     data = ImageDataBunch.from_df(path=train_path, df=df_train, label_col=1, bs=16, size=32, ds_tfms=tfms)      #define learner     learn = cnn_learner(data, arch, metrics=[accuracy, AUCROC()], model_dir='../../../models')      #train     learn.fit_one_cycle(number_epochs, 3e-3)          return learn 	resnet50_learner = train(models.resnet50) densenet121_learner = train(models.densenet121) vgg_learner = train(models.vgg19_bn)
resnet50_learner = train(models.resnet50) densenet121_learner = train(models.densenet121) vgg_learner = train(models.vgg19_bn)	result_csv = 'submission.csv' test_path = '../input/test/test/'  def ensemble_predition(test_img):     img = open_image(test_path + test_img)          resnet50_predicition = resnet50_learner.predict(img)     densenet121_predicition = densenet121_learner.predict(img)     vgg_predicition = vgg_learner.predict(img)          #ensemble average     sum_pred = resnet50_predicition[2] + densenet121_predicition[2] + vgg_predicition[2]     prediction = sum_pred / 3          #prediction results     predicted_label = torch.argmax(prediction).item()          return predicted_label  #to give np array the correct style submission_data = np.array([['dummy', 0]])  #progress bar with tqdm(total=len(os.listdir(test_path))) as pbar:            #test all test images     for img in os.listdir(test_path):         label = ensemble_predition(img)         new_np_array = np.array([[img, label]])         submission_data = np.concatenate((submission_data, new_np_array), axis=0)         pbar.update(1)  #remove dummy submission_data = np.delete(submission_data, 0, 0)  #save final submission result_df = pd.DataFrame(submission_data, columns=['id','has_cactus']) result_df.to_csv(result_csv, index=False)  
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from tensorflow.keras.models import Sequential import tensorflow.keras.backend as K from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense from tensorflow.keras.applications import VGG16 from tensorflow.keras.optimizers import Adam from tensorflow.keras.preprocessing.image import ImageDataGenerator	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	ResNet50_net = VGG16(weights='imagenet',                       include_top=False,                       input_shape=(32, 32, 3))
ResNet50_net = VGG16(weights='imagenet',                       include_top=False,                       input_shape=(32, 32, 3))	ResNet50_net.trainable = False ResNet50_net.summary()
ResNet50_net.trainable = False ResNet50_net.summary()	model = Sequential() model.add(ResNet50_net) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid'))
model = Sequential() model.add(ResNet50_net) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid'))	# def sigmoid(x):     # return 1.0/(1.0 + np.exp(-x))
# def sigmoid(x):     # return 1.0/(1.0 + np.exp(-x))	# def binary_crossentropy(y_true, y_pred):     # return K.mean(K.binary_crossentropy(y_true, K.sigmoid(y_pred)), axis=-1)
# def binary_crossentropy(y_true, y_pred):     # return K.mean(K.binary_crossentropy(y_true, K.sigmoid(y_pred)), axis=-1)	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 1000
batch_size = 32 nb_epoch = 1000	"datagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset\ "",                              samplewise_center=False,  # set each sample mean to 0\ "",                              featurewise_std_normalization=False,  # divide inputs by dataset std\ "",                              samplewise_std_normalization=False,  # divide each input by its std\ "",                              zca_whitening=False,  # apply ZCA whitening\ "",                              zca_epsilon=1e-06,  # epsilon for ZCA whitening\ "",                              rotation_range=40,  # SET TO 0 IF NEEDED # randomly rotate images in 0 to 180 degrees\ "",                              width_shift_range=0.2,  # randomly shift images horizontally\ "",                              height_shift_range=0.2,  # randomly shift images vertically\ "",                              shear_range=0.2,  # set range for random shear\ "",                              zoom_range=0.2,  # set range for random zoom\ "",                              channel_shift_range=0.,  # set range for random channel shifts\ "",                              fill_mode=\'nearest\',                              validation_split=0.1)  # set mode for filling points outside the input  # compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied) datagen.fit(X_tr)  # fits the model on batches with real-time data augmentation: model.fit_generator(datagen.flow(X_tr, Y_tr, batch_size=batch_size, subset=\'training\'), validation_data=datagen.flow(X_tr, Y_tr, batch_size=batch_size, subset=\'validation\'), steps_per_epoch=len(X_tr) / batch_size, epochs=nb_epoch, shuffle=True)"
"datagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset\ "",                              samplewise_center=False,  # set each sample mean to 0\ "",                              featurewise_std_normalization=False,  # divide inputs by dataset std\ "",                              samplewise_std_normalization=False,  # divide each input by its std\ "",                              zca_whitening=False,  # apply ZCA whitening\ "",                              zca_epsilon=1e-06,  # epsilon for ZCA whitening\ "",                              rotation_range=40,  # SET TO 0 IF NEEDED # randomly rotate images in 0 to 180 degrees\ "",                              width_shift_range=0.2,  # randomly shift images horizontally\ "",                              height_shift_range=0.2,  # randomly shift images vertically\ "",                              shear_range=0.2,  # set range for random shear\ "",                              zoom_range=0.2,  # set range for random zoom\ "",                              channel_shift_range=0.,  # set range for random channel shifts\ "",                              fill_mode=\'nearest\',                              validation_split=0.1)  # set mode for filling points outside the input  # compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied) datagen.fit(X_tr)  # fits the model on batches with real-time data augmentation: model.fit_generator(datagen.flow(X_tr, Y_tr, batch_size=batch_size, subset=\'training\'), validation_data=datagen.flow(X_tr, Y_tr, batch_size=batch_size, subset=\'validation\'), steps_per_epoch=len(X_tr) / batch_size, epochs=nb_epoch, shuffle=True)"	"pretrained = True  """""" %%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2) """""""
"pretrained = True  """""" %%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2) """""""	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst) # sigmoid(model.predict(X_tst))
# Prediction test_predictions = model.predict(X_tst) # sigmoid(model.predict(X_tst))	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
import csv import pandas as pd	sub_files = [     '../input/keras-transfer-vgg16/submission.csv',          '../input/cactus-identification-fastai-v1-0-46-baseline/submission.csv',         '../input/simple-fastai-exercise/submission.csv',             ]  sub_weight = [                                 0.962**2,                 0.989**2,                 0.997**2             ]
sub_files = [     '../input/keras-transfer-vgg16/submission.csv',          '../input/cactus-identification-fastai-v1-0-46-baseline/submission.csv',         '../input/simple-fastai-exercise/submission.csv',             ]  sub_weight = [                                 0.962**2,                 0.989**2,                 0.997**2             ]	"Hlabel = \'id\'  Htarget = \'has_cactus\' npt = 1 place_weights = {} for i in range(npt):     place_weights[i] = ( 1 / (i + 1) )      print(place_weights)  lg = len(sub_files) sub = [None]*lg for i, file in enumerate( sub_files ):         print(""Reading {}: w={} - {}"". format(i, sub_weight[i], file))     reader = csv.DictReader(open(file,""r""))     sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))  out = open(""sub_ens.csv"", ""w"", newline=\'\') writer = csv.writer(out) writer.writerow([Hlabel,Htarget])  for p, row in enumerate(sub[0]):     target_weight = {}     for s in range(lg):         row1 = sub[s][p]         for ind, trgt in enumerate(row1[Htarget].split(\' \')):             target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])     tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]     writer.writerow([row1[Hlabel], "" "".join(tops_trgt)]) out.close()"
"Hlabel = \'id\'  Htarget = \'has_cactus\' npt = 1 place_weights = {} for i in range(npt):     place_weights[i] = ( 1 / (i + 1) )      print(place_weights)  lg = len(sub_files) sub = [None]*lg for i, file in enumerate( sub_files ):         print(""Reading {}: w={} - {}"". format(i, sub_weight[i], file))     reader = csv.DictReader(open(file,""r""))     sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))  out = open(""sub_ens.csv"", ""w"", newline=\'\') writer = csv.writer(out) writer.writerow([Hlabel,Htarget])  for p, row in enumerate(sub[0]):     target_weight = {}     for s in range(lg):         row1 = sub[s][p]         for ind, trgt in enumerate(row1[Htarget].split(\' \')):             target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])     tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]     writer.writerow([row1[Hlabel], "" "".join(tops_trgt)]) out.close()"	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import numpy as np import math import tarfile import csv import pandas as pd import pickle as cPickle import matplotlib.pyplot as plt from PIL import Image  from keras.optimizers import Adam, RMSprop, SGD from keras.models import Sequential, Model from keras.layers import Dropout, LeakyReLU, Conv2D, MaxPooling2D, Flatten from keras.layers import Dense, Activation, BatchNormalization, Embedding from keras.layers import CuDNNLSTM from keras.callbacks import EarlyStopping from keras.applications.vgg16 import VGG16  from sklearn.model_selection import train_test_split  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input/aerialcactusidentification"")) print(os.listdir(""../input/aerial-cactus-identification""))  # Any results you write to the current directory are saved as output."	"dataset_directory = ""../input/aerialcactusidentification"" train_label = ""{0}/{1}"".format(dataset_directory, ""train.csv"") #train_image_path = ""{0}/{1}"".format(dataset_directory, ""train"") #test_image_path = ""{0}/{1}"".format(dataset_directory, ""test"") train_path = ""{0}/{1}"".format(dataset_directory, ""train.csv"") test_path = ""{0}/{1}"".format(dataset_directory, ""test.csv"") sample_submission = ""{0}/{1}"".format(""../input/aerial-cactus-identification"", ""sample_submission.csv"") x_train_pickle_path = ""{0}/{1}"".format(dataset_directory, ""x_train_data.npy"") y_train_pickle_path = ""{0}/{1}"".format(dataset_directory, ""y_train_data.npy"") x_test_pickle_path = ""{0}/{1}"".format(dataset_directory, ""x_test_data.npy"")  img_width, img_height, img_space = 32, 32, 3  def _build_header(training = True):     if training == True:         result = [""id"", ""label""]     else:         result = [""id""]      for y in range(0, img_height):         for x in range(0, img_width):             result.append(""{0}_{1}_R"".format(y, x))             result.append(""{0}_{1}_G"".format(y, x))             result.append(""{0}_{1}_B"".format(y, x))      return result  def _load_image( infilename ) :     img = Image.open(infilename)     img.load()     img.thumbnail((img_width, img_height, 3), Image.ANTIALIAS)     data = np.asarray(img, dtype=""float32"")     data /= 255     return data  def _pre_process_training():     header = _build_header()     y_train = pd.read_csv(train_label)     y_train = y_train.values      with open(train_path, ""w"") as write_train_data:         writer = csv.writer(write_train_data)         writer.writerow(header)          for idx, val in enumerate(y_train):             train_file_name = ""{0}/{1}"".format(train_image_path, val[0])             row_data = [val[0], val[1]]              if os.path.isfile(train_file_name) == True:                 img_data = _load_image(train_file_name)                 img_data = img_data.flatten()                                  for _, img_val in enumerate(img_data):                     row_data.append(img_val)                  writer.writerow(row_data)                 write_train_data.flush()                  def _pre_process_testing():     header = _build_header(False)     y_test = pd.read_csv(sample_submission)     y_test = y_test.values      with open(test_path, ""w"") as write_test_data:         writer = csv.writer(write_test_data)         writer.writerow(header)          for idx, val in enumerate(y_test):             test_file_name = ""{0}/{1}"".format(test_image_path, val[0])             row_data = [val[0]]              if os.path.isfile(test_file_name) == True:                 img_data = _load_image(test_file_name)                 img_data = img_data.flatten()                 row_data.append(img_data)                  for _, img_val in enumerate(img_data):                     row_data.append(img_val)                  writer.writerow(row_data)                 write_test_data.flush()                  def load_bin_data():     print (""[*] Read train_x data to {0}"".format(x_train_pickle_path))     train_x = np.load(x_train_pickle_path)     print (""[*] Read train_y data to {0}"".format(y_train_pickle_path))     train_y = np.load(y_train_pickle_path)     print (""[*] Read test_x data to {0}"".format(x_test_pickle_path))     test_x = np.load(x_test_pickle_path)      print(""[+] Load data sucessfully.\ "")     return train_x, train_y, test_x  def load_data(save_pickle=True, preprocess=False):     if preprocess == True:         print (""[*] Pre-process training data"")         _pre_process_training()         print (""[*] Pre-process testing data"")         _pre_process_testing()      print (""[*] Read csv data from {0} for training data"".format(train_path))     train_x = pd.read_csv(train_path)     train_x = train_x.drop([\'id\'], axis=1)     train_x = train_x.drop([\'label\'], axis=1)     train_x = train_x.values      print (""[*] Read csv data from {0} for training label"".format(train_path))     train_y = pd.read_csv(train_path, usecols=[\'label\'])     train_y = train_y.values      print (""[*] Read csv data from {0} from testing data"".format(test_path))     test_x = pd.read_csv(test_path)     test_x = test_x.drop([\'id\'], axis=1)     test_x = test_x.values      if save_pickle == True:         print (""[*] Write train_x data to {0}"".format(x_train_pickle_path))         np.save(x_train_pickle_path, train_x)         print (""[*] Write train_y data to {0}"".format(y_train_pickle_path))         np.save(y_train_pickle_path, train_y)         print (""[*] Write test_x data to {0}"".format(x_test_pickle_path))         np.save(x_test_pickle_path, test_x)               print(""[+] Load data sucessfully.\ "")     return train_x, train_y, test_x  def _Submission(y_pred, submit_filename):     ## submit     read_header = pd.read_csv(sample_submission, usecols=[\'id\'])     result = pd.DataFrame({""id"": read_header.id.values})     result[""has_cactus""] = y_pred     result.to_csv(submit_filename, index=False)     print(""[+] Submission file has created {0}.\ "".format(submit_filename))  def CNN_Model(x_train, y_train, x_val, y_val, x_test, lr=0.01, batch_size=20000, epochs=100, model_filename=""model_cnn.h5"", submit_filename=""sample_submission.csv""):     print (""-------------------------------------------------------------------------------------\ "")     print(""[+] CNN Model.\ "")      x_train = np.reshape(x_train, (x_train.shape[0], img_width, img_height, img_space))     x_val = np.reshape(x_val, (x_val.shape[0], img_width, img_height, img_space))     x_test = np.reshape(x_test, (x_test.shape[0], img_width, img_height, img_space))      model = Sequential()      model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(img_width, img_height, img_space), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(32, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(32, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Flatten())     model.add(Dense(64, activation=\'relu\', kernel_initializer=\'normal\'))     #model.add(Dropout(0.25))      model.add(Dense(1, activation=\'sigmoid\', kernel_initializer=\'normal\'))      model.compile(optimizer=Adam(lr=lr),                   loss=\'binary_crossentropy\',                     metrics=[\'accuracy\'])      print (model.summary())      earlystop = EarlyStopping(monitor=\'val_loss\', mode=\'auto\', patience=10, verbose=1)      train_history = model.fit(x=x_train,                            y=y_train,                           validation_data=(x_val, y_val),                            epochs=epochs,                            batch_size=batch_size,                           verbose=2,                           callbacks=[earlystop])       #model.save(model_filename)     y_pred = model.predict(x_test, batch_size=batch_size, verbose=1)     _Submission(y_pred, submit_filename)      return train_history  def show_train_history(train_history, train, validation):     plt.plot(train_history.history[train])     plt.plot(train_history.history[validation])     plt.title(\'Train History\')     plt.ylabel(train)     plt.xlabel(\'Epoch\')     plt.legend([\'train\', \'validation\'], loc=\'upper left\')     plt.show()  # training set learning_rate = 0.0001 validation_split = 0.2 epochs = 200 batch_size = 2048 random_seed = 12345 "
"dataset_directory = ""../input/aerialcactusidentification"" train_label = ""{0}/{1}"".format(dataset_directory, ""train.csv"") #train_image_path = ""{0}/{1}"".format(dataset_directory, ""train"") #test_image_path = ""{0}/{1}"".format(dataset_directory, ""test"") train_path = ""{0}/{1}"".format(dataset_directory, ""train.csv"") test_path = ""{0}/{1}"".format(dataset_directory, ""test.csv"") sample_submission = ""{0}/{1}"".format(""../input/aerial-cactus-identification"", ""sample_submission.csv"") x_train_pickle_path = ""{0}/{1}"".format(dataset_directory, ""x_train_data.npy"") y_train_pickle_path = ""{0}/{1}"".format(dataset_directory, ""y_train_data.npy"") x_test_pickle_path = ""{0}/{1}"".format(dataset_directory, ""x_test_data.npy"")  img_width, img_height, img_space = 32, 32, 3  def _build_header(training = True):     if training == True:         result = [""id"", ""label""]     else:         result = [""id""]      for y in range(0, img_height):         for x in range(0, img_width):             result.append(""{0}_{1}_R"".format(y, x))             result.append(""{0}_{1}_G"".format(y, x))             result.append(""{0}_{1}_B"".format(y, x))      return result  def _load_image( infilename ) :     img = Image.open(infilename)     img.load()     img.thumbnail((img_width, img_height, 3), Image.ANTIALIAS)     data = np.asarray(img, dtype=""float32"")     data /= 255     return data  def _pre_process_training():     header = _build_header()     y_train = pd.read_csv(train_label)     y_train = y_train.values      with open(train_path, ""w"") as write_train_data:         writer = csv.writer(write_train_data)         writer.writerow(header)          for idx, val in enumerate(y_train):             train_file_name = ""{0}/{1}"".format(train_image_path, val[0])             row_data = [val[0], val[1]]              if os.path.isfile(train_file_name) == True:                 img_data = _load_image(train_file_name)                 img_data = img_data.flatten()                                  for _, img_val in enumerate(img_data):                     row_data.append(img_val)                  writer.writerow(row_data)                 write_train_data.flush()                  def _pre_process_testing():     header = _build_header(False)     y_test = pd.read_csv(sample_submission)     y_test = y_test.values      with open(test_path, ""w"") as write_test_data:         writer = csv.writer(write_test_data)         writer.writerow(header)          for idx, val in enumerate(y_test):             test_file_name = ""{0}/{1}"".format(test_image_path, val[0])             row_data = [val[0]]              if os.path.isfile(test_file_name) == True:                 img_data = _load_image(test_file_name)                 img_data = img_data.flatten()                 row_data.append(img_data)                  for _, img_val in enumerate(img_data):                     row_data.append(img_val)                  writer.writerow(row_data)                 write_test_data.flush()                  def load_bin_data():     print (""[*] Read train_x data to {0}"".format(x_train_pickle_path))     train_x = np.load(x_train_pickle_path)     print (""[*] Read train_y data to {0}"".format(y_train_pickle_path))     train_y = np.load(y_train_pickle_path)     print (""[*] Read test_x data to {0}"".format(x_test_pickle_path))     test_x = np.load(x_test_pickle_path)      print(""[+] Load data sucessfully.\ "")     return train_x, train_y, test_x  def load_data(save_pickle=True, preprocess=False):     if preprocess == True:         print (""[*] Pre-process training data"")         _pre_process_training()         print (""[*] Pre-process testing data"")         _pre_process_testing()      print (""[*] Read csv data from {0} for training data"".format(train_path))     train_x = pd.read_csv(train_path)     train_x = train_x.drop([\'id\'], axis=1)     train_x = train_x.drop([\'label\'], axis=1)     train_x = train_x.values      print (""[*] Read csv data from {0} for training label"".format(train_path))     train_y = pd.read_csv(train_path, usecols=[\'label\'])     train_y = train_y.values      print (""[*] Read csv data from {0} from testing data"".format(test_path))     test_x = pd.read_csv(test_path)     test_x = test_x.drop([\'id\'], axis=1)     test_x = test_x.values      if save_pickle == True:         print (""[*] Write train_x data to {0}"".format(x_train_pickle_path))         np.save(x_train_pickle_path, train_x)         print (""[*] Write train_y data to {0}"".format(y_train_pickle_path))         np.save(y_train_pickle_path, train_y)         print (""[*] Write test_x data to {0}"".format(x_test_pickle_path))         np.save(x_test_pickle_path, test_x)               print(""[+] Load data sucessfully.\ "")     return train_x, train_y, test_x  def _Submission(y_pred, submit_filename):     ## submit     read_header = pd.read_csv(sample_submission, usecols=[\'id\'])     result = pd.DataFrame({""id"": read_header.id.values})     result[""has_cactus""] = y_pred     result.to_csv(submit_filename, index=False)     print(""[+] Submission file has created {0}.\ "".format(submit_filename))  def CNN_Model(x_train, y_train, x_val, y_val, x_test, lr=0.01, batch_size=20000, epochs=100, model_filename=""model_cnn.h5"", submit_filename=""sample_submission.csv""):     print (""-------------------------------------------------------------------------------------\ "")     print(""[+] CNN Model.\ "")      x_train = np.reshape(x_train, (x_train.shape[0], img_width, img_height, img_space))     x_val = np.reshape(x_val, (x_val.shape[0], img_width, img_height, img_space))     x_test = np.reshape(x_test, (x_test.shape[0], img_width, img_height, img_space))      model = Sequential()      model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(img_width, img_height, img_space), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(32, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(32, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(64, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(Conv2D(128, kernel_size=(3, 3), activation=\'relu\', padding=\'same\', kernel_initializer=\'normal\'))     model.add(BatchNormalization())     #model.add(Dropout(0.25))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Flatten())     model.add(Dense(64, activation=\'relu\', kernel_initializer=\'normal\'))     #model.add(Dropout(0.25))      model.add(Dense(1, activation=\'sigmoid\', kernel_initializer=\'normal\'))      model.compile(optimizer=Adam(lr=lr),                   loss=\'binary_crossentropy\',                     metrics=[\'accuracy\'])      print (model.summary())      earlystop = EarlyStopping(monitor=\'val_loss\', mode=\'auto\', patience=10, verbose=1)      train_history = model.fit(x=x_train,                            y=y_train,                           validation_data=(x_val, y_val),                            epochs=epochs,                            batch_size=batch_size,                           verbose=2,                           callbacks=[earlystop])       #model.save(model_filename)     y_pred = model.predict(x_test, batch_size=batch_size, verbose=1)     _Submission(y_pred, submit_filename)      return train_history  def show_train_history(train_history, train, validation):     plt.plot(train_history.history[train])     plt.plot(train_history.history[validation])     plt.title(\'Train History\')     plt.ylabel(train)     plt.xlabel(\'Epoch\')     plt.legend([\'train\', \'validation\'], loc=\'upper left\')     plt.show()  # training set learning_rate = 0.0001 validation_split = 0.2 epochs = 200 batch_size = 2048 random_seed = 12345 "	x_train, y_train, x_test = load_data(False, False) #x_train, y_train, x_test = load_bin_data()  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,                                                   test_size=validation_split,                                                   random_state=random_seed)
x_train, y_train, x_test = load_data(False, False) #x_train, y_train, x_test = load_bin_data()  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,                                                   test_size=validation_split,                                                   random_state=random_seed)	"train_history_cnn = CNN_Model(x_train, y_train, x_val, y_val, x_test,                               lr=learning_rate,                               batch_size=batch_size,                               epochs=epochs,                               submit_filename=""sample_submission.csv""                              )"
"train_history_cnn = CNN_Model(x_train, y_train, x_val, y_val, x_test,                               lr=learning_rate,                               batch_size=batch_size,                               epochs=epochs,                               submit_filename=""sample_submission.csv""                              )"	show_train_history(train_history_cnn, 'acc', 'val_acc') show_train_history(train_history_cnn, 'loss', 'val_loss')
show_train_history(train_history_cnn, 'acc', 'val_acc') show_train_history(train_history_cnn, 'loss', 'val_loss')	NB_END
"#Kernel based on https://www.kaggle.com/kenseitrg/simple-fastai-exercise, for my training purpose  import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  import os print(os.listdir(""../input"")) "	from pathlib import Path from fastai import * from fastai.vision import *
from pathlib import Path from fastai import * from fastai.vision import *	"data_folder = Path(""../input"") data_folder.ls()"
"data_folder = Path(""../input"") data_folder.ls()"	"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/sample_submission.csv"")"
"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/sample_submission.csv"")"	test_img = ImageList.from_df(test_df, path=data_folder/'test', folder='test') train_img = (ImageList.from_df(train_df, path=data_folder/'train', folder='train')         .random_split_by_pct(0.2)         .label_from_df()         .add_test(test_img)         .transform(get_transforms(flip_vert=True), size=128)         .databunch(path='.', bs=64)         .normalize(imagenet_stats)        )
test_img = ImageList.from_df(test_df, path=data_folder/'test', folder='test') train_img = (ImageList.from_df(train_df, path=data_folder/'train', folder='train')         .random_split_by_pct(0.2)         .label_from_df()         .add_test(test_img)         .transform(get_transforms(flip_vert=True), size=128)         .databunch(path='.', bs=64)         .normalize(imagenet_stats)        )	learn = create_cnn(train_img, models.densenet201, metrics=[error_rate, accuracy])
learn = create_cnn(train_img, models.densenet201, metrics=[error_rate, accuracy])	learn.lr_find()
learn.lr_find()	learn.recorder.plot()
learn.recorder.plot()	lr = 0.1 
lr = 0.1 	learn.fit(epochs=3,lr=lr)
learn.fit(epochs=3,lr=lr)	learn.unfreeze() learn.lr_find() learn.recorder.plot()
learn.unfreeze() learn.lr_find() learn.recorder.plot()	learn.fit(epochs=5,lr=2e-6)
learn.fit(epochs=5,lr=2e-6)	interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(7,6))
interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(7,6))	preds,_ = learn.get_preds(ds_type=DatasetType.Test) preds.shape
preds,_ = learn.get_preds(ds_type=DatasetType.Test) preds.shape	classes = preds.argmax(1) classes.shape, classes.min(), classes.max()
classes = preds.argmax(1) classes.shape, classes.min(), classes.max()	test_df.has_cactus = classes test_df.head()
test_df.has_cactus = classes test_df.head()	test_df.to_csv('submission_12_03.csv', index=False)
test_df.to_csv('submission_12_03.csv', index=False)	NB_END
import fastai from fastai.vision import *	# Copy pretrained model weights to the default path !mkdir '/tmp/.torch' !mkdir '/tmp/.torch/models/' !cp '../input/resnet50/resnet50.pth' '/tmp/.torch/models/resnet50-19c8e357.pth'
# Copy pretrained model weights to the default path !mkdir '/tmp/.torch' !mkdir '/tmp/.torch/models/' !cp '../input/resnet50/resnet50.pth' '/tmp/.torch/models/resnet50-19c8e357.pth'	fastai.__version__
fastai.__version__	data_path = Path('../input/aerial-cactus-identification') df = pd.read_csv(data_path/'train.csv') df.head()
data_path = Path('../input/aerial-cactus-identification') df = pd.read_csv(data_path/'train.csv') df.head()	sub_csv = pd.read_csv(data_path/'sample_submission.csv') sub_csv.head()
sub_csv = pd.read_csv(data_path/'sample_submission.csv') sub_csv.head()	test = ImageList.from_df(sub_csv, path=data_path/'test', folder='test') data = (ImageList.from_df(df, path=data_path/'train', folder='train')         .random_split_by_pct(0.2)         .label_from_df()         .add_test(test)         .transform(get_transforms(flip_vert=True), size=128)         .databunch(path='.', bs=64)         .normalize(imagenet_stats)        )
test = ImageList.from_df(sub_csv, path=data_path/'test', folder='test') data = (ImageList.from_df(df, path=data_path/'train', folder='train')         .random_split_by_pct(0.2)         .label_from_df()         .add_test(test)         .transform(get_transforms(flip_vert=True), size=128)         .databunch(path='.', bs=64)         .normalize(imagenet_stats)        )	learn = create_cnn(data, models.resnet50, metrics=[accuracy])
learn = create_cnn(data, models.resnet50, metrics=[accuracy])	# learn.data.show_batch()
# learn.data.show_batch()	learn.lr_find() learn.recorder.plot()
learn.lr_find() learn.recorder.plot()	learn.fit_one_cycle(6, slice(3e-2))
learn.fit_one_cycle(6, slice(3e-2))	learn.save('stage-1')
learn.save('stage-1')	learn.unfreeze()
learn.unfreeze()	learn.fit_one_cycle(3, slice(3e-2/100, 3e-2/10))
learn.fit_one_cycle(3, slice(3e-2/100, 3e-2/10))	learn.save('stage-2')
learn.save('stage-2')	valid_preds = learn.get_preds()
valid_preds = learn.get_preds()	preds = learn.TTA(ds_type=DatasetType.Test)
preds = learn.TTA(ds_type=DatasetType.Test)	sub_csv['has_cactus'] = preds[0].argmax(1)
sub_csv['has_cactus'] = preds[0].argmax(1)	sub_csv.to_csv('submission.csv', index=False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import glob import os import matplotlib.pyplot as plt import seaborn as sns import cv2 from tqdm import tqdm_notebook  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	#read in all our files train_df = pd.read_csv('../input/train.csv') train_images = '../input/train/*' test_images = '../input/test/*'
#read in all our files train_df = pd.read_csv('../input/train.csv') train_images = '../input/train/*' test_images = '../input/test/*'	train_df.head()
train_df.head()	sns.set(style = 'darkgrid') plt.figure(figsize = (12,10)) sns.countplot(train_df['has_cactus'])
sns.set(style = 'darkgrid') plt.figure(figsize = (12,10)) sns.countplot(train_df['has_cactus'])	"#let\'s visualize some cactus images IMAGES = os.path.join(train_images, ""*"") all_images = glob.glob(IMAGES)"
"#let\'s visualize some cactus images IMAGES = os.path.join(train_images, ""*"") all_images = glob.glob(IMAGES)"	#visualize some images  plt.figure(figsize = (12,10)) plt.subplot(1, 3, 1) plt.imshow(plt.imread(all_images[0])) plt.xticks([]) plt.yticks([])  plt.subplot(1, 3, 2) plt.imshow(plt.imread(all_images[10])) plt.xticks([]) plt.yticks([])  plt.subplot(1, 3, 3) plt.imshow(plt.imread(all_images[20])) plt.xticks([]) plt.yticks([])
#visualize some images  plt.figure(figsize = (12,10)) plt.subplot(1, 3, 1) plt.imshow(plt.imread(all_images[0])) plt.xticks([]) plt.yticks([])  plt.subplot(1, 3, 2) plt.imshow(plt.imread(all_images[10])) plt.xticks([]) plt.yticks([])  plt.subplot(1, 3, 3) plt.imshow(plt.imread(all_images[20])) plt.xticks([]) plt.yticks([])	train_path = '../input/train/train/' test_path = '../input/test/test/'
train_path = '../input/train/train/' test_path = '../input/test/test/'	#let's get our image data and image labels toegether #read in all the images images_id = train_df['id'].values X = [] #this list will contain all our images for id_ in images_id:     img = cv2.imread(train_path + id_)     X.append(img)
#let's get our image data and image labels toegether #read in all the images images_id = train_df['id'].values X = [] #this list will contain all our images for id_ in images_id:     img = cv2.imread(train_path + id_)     X.append(img)	#now let's get our labels label_list = [] #will contain all our labels for img_id in images_id:     label_list.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])
#now let's get our labels label_list = [] #will contain all our labels for img_id in images_id:     label_list.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])	#now we can convert our images list and the labels list into numpy array X = np.array(X) y = np.array(label_list)
#now we can convert our images list and the labels list into numpy array X = np.array(X) y = np.array(label_list)	"print(f""THE SIZE OF OUR TRAINING DATA : {X.shape}"") print(f""THE SIZE OF OUR TRAINING LABELS : {y.shape}"")"
"print(f""THE SIZE OF OUR TRAINING DATA : {X.shape}"") print(f""THE SIZE OF OUR TRAINING LABELS : {y.shape}"")"	#let's do some preprocessing such as normalizing our data X = X.astype('float32') / 255
#let's do some preprocessing such as normalizing our data X = X.astype('float32') / 255	#loading in and preprocessing the test data X_test = [] test_images = [] for img_id in tqdm_notebook(os.listdir(test_path)):     X_test.append(cv2.imread(test_path + img_id))          test_images.append(img_id) X_test = np.array(X_test) X_test = X_test.astype('float32') / 255
#loading in and preprocessing the test data X_test = [] test_images = [] for img_id in tqdm_notebook(os.listdir(test_path)):     X_test.append(cv2.imread(test_path + img_id))          test_images.append(img_id) X_test = np.array(X_test) X_test = X_test.astype('float32') / 255	#import the required libraries import keras from keras.layers import Conv2D from keras.layers import Dense from keras.layers import BatchNormalization from keras.layers import Flatten from keras.layers import Activation from keras.layers import Dropout from keras.layers import MaxPooling2D from keras.models import Sequential from keras import optimizers from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau from keras import backend as K
#import the required libraries import keras from keras.layers import Conv2D from keras.layers import Dense from keras.layers import BatchNormalization from keras.layers import Flatten from keras.layers import Activation from keras.layers import Dropout from keras.layers import MaxPooling2D from keras.models import Sequential from keras import optimizers from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau from keras import backend as K	class CNN:     def build(height, width, classes, channels):         model = Sequential()         inputShape = (height, width, channels)         chanDim = -1                  if K.image_data_format() == 'channels_first':             inputShape = (channels, height, width)             chanDim = 1         model.add(Conv2D(32, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(32, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Conv2D(128, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(128, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Conv2D(256, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(256, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Flatten())                  model.add(Dense(128, activation = 'relu'))         model.add(BatchNormalization(axis = chanDim))         model.add(Dropout(0.5))                  model.add(Dense(32, activation = 'relu'))         model.add(BatchNormalization(axis = chanDim))         model.add(Dropout(0.5))                  model.add(Dense(classes, activation = 'sigmoid'))                  return model
class CNN:     def build(height, width, classes, channels):         model = Sequential()         inputShape = (height, width, channels)         chanDim = -1                  if K.image_data_format() == 'channels_first':             inputShape = (channels, height, width)             chanDim = 1         model.add(Conv2D(32, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(32, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Conv2D(128, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(128, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Conv2D(256, (3,3), padding = 'same', input_shape = inputShape))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(Conv2D(256, (3,3), padding = 'same'))         model.add(BatchNormalization(axis = chanDim))         model.add(Activation('relu'))         model.add(MaxPooling2D(2,2))         model.add(Dropout(0.25))                  model.add(Flatten())                  model.add(Dense(128, activation = 'relu'))         model.add(BatchNormalization(axis = chanDim))         model.add(Dropout(0.5))                  model.add(Dense(32, activation = 'relu'))         model.add(BatchNormalization(axis = chanDim))         model.add(Dropout(0.5))                  model.add(Dense(classes, activation = 'sigmoid'))                  return model	input_dim = X.shape[1:] activation = 'relu' classes = 1 height = 32 width = 32 channels = 3  history = dict() #dictionery to store the history of individual models for later visualization prediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset  #here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions for i in np.arange(0, 5):     optim = optimizers.Adam(lr = 0.001)     ensemble_model = CNN.build(height = height, width = width, classes = classes, channels = channels)     ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])     print('TRAINING MODEL NO : {}'.format(i))     H = ensemble_model.fit(X, y,                            batch_size = 32,                            epochs = 200,                            verbose = 1)     history[i] = H          ensemble_model.save('MODEL_{}.model'.format(i))          predictions = ensemble_model.predict(X_test, verbose = 1, batch_size = 32)     prediction_scores[i] = predictions
input_dim = X.shape[1:] activation = 'relu' classes = 1 height = 32 width = 32 channels = 3  history = dict() #dictionery to store the history of individual models for later visualization prediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset  #here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions for i in np.arange(0, 5):     optim = optimizers.Adam(lr = 0.001)     ensemble_model = CNN.build(height = height, width = width, classes = classes, channels = channels)     ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])     print('TRAINING MODEL NO : {}'.format(i))     H = ensemble_model.fit(X, y,                            batch_size = 32,                            epochs = 200,                            verbose = 1)     history[i] = H          ensemble_model.save('MODEL_{}.model'.format(i))          predictions = ensemble_model.predict(X_test, verbose = 1, batch_size = 32)     prediction_scores[i] = predictions	from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import VGG16	vgg16 = VGG16(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False) vgg16.summary()
vgg16 = VGG16(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False) vgg16.summary()	for layer in vgg16.layers:     layer.trainable = False
for layer in vgg16.layers:     layer.trainable = False	vgg_model = Sequential() vgg_model.add(vgg16) vgg_model.add(Flatten()) vgg_model.add(Dense(256, activation = 'relu')) vgg_model.add(BatchNormalization()) vgg_model.add(Dropout(0.5)) vgg_model.add(Dense(128, activation = 'relu')) vgg_model.add(BatchNormalization()) vgg_model.add(Dropout(0.5)) vgg_model.add(Dense(1, activation = 'sigmoid'))  vgg_model.summary()
vgg_model = Sequential() vgg_model.add(vgg16) vgg_model.add(Flatten()) vgg_model.add(Dense(256, activation = 'relu')) vgg_model.add(BatchNormalization()) vgg_model.add(Dropout(0.5)) vgg_model.add(Dense(128, activation = 'relu')) vgg_model.add(BatchNormalization()) vgg_model.add(Dropout(0.5)) vgg_model.add(Dense(1, activation = 'sigmoid'))  vgg_model.summary()	#compile the model vgg_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])
#compile the model vgg_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])	#fit the model on our data vgg_history = vgg_model.fit(X, y,                             batch_size = 64,                             epochs = 500,                             verbose = 1) 
#fit the model on our data vgg_history = vgg_model.fit(X, y,                             batch_size = 64,                             epochs = 500,                             verbose = 1) 	#making predictions on test dat predictions_vgg = vgg_model.predict(X_test)
#making predictions on test dat predictions_vgg = vgg_model.predict(X_test)	predictions_vgg.shape
predictions_vgg.shape	from keras.applications.resnet50 import ResNet50
from keras.applications.resnet50 import ResNet50	resnet = ResNet50(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False) resnet.summary()
resnet = ResNet50(weights = 'imagenet', input_shape = (32, 32, 3), include_top = False) resnet.summary()	for layer in resnet.layers:     layer.trainable = False
for layer in resnet.layers:     layer.trainable = False	resnet_model = Sequential() resnet_model.add(resnet) resnet_model.add(Flatten()) resnet_model.add(Dense(256, activation = 'relu')) resnet_model.add(BatchNormalization()) resnet_model.add(Dropout(0.5)) resnet_model.add(Dense(128, activation = 'relu')) resnet_model.add(BatchNormalization()) resnet_model.add(Dropout(0.5)) resnet_model.add(Dense(1, activation = 'sigmoid'))  resnet_model.summary()
resnet_model = Sequential() resnet_model.add(resnet) resnet_model.add(Flatten()) resnet_model.add(Dense(256, activation = 'relu')) resnet_model.add(BatchNormalization()) resnet_model.add(Dropout(0.5)) resnet_model.add(Dense(128, activation = 'relu')) resnet_model.add(BatchNormalization()) resnet_model.add(Dropout(0.5)) resnet_model.add(Dense(1, activation = 'sigmoid'))  resnet_model.summary()	#compile the model resnet_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])
#compile the model resnet_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])	#fit the model on our data resnet_history = resnet_model.fit(X, y,                                   batch_size = 64,                                    epochs = 500,                                   verbose = 1) 
#fit the model on our data resnet_history = resnet_model.fit(X, y,                                   batch_size = 64,                                    epochs = 500,                                   verbose = 1) 	resnet_predictions = resnet_model.predict(X_test)
resnet_predictions = resnet_model.predict(X_test)	#making predictions prediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models predictions_ensemble = np.mean(prediction, axis = 1) print(predictions_ensemble.shape)
#making predictions prediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models predictions_ensemble = np.mean(prediction, axis = 1) print(predictions_ensemble.shape)	df_ensemble = pd.DataFrame(predictions_ensemble, columns = ['has_cactus']) df_ensemble['has_cactus'] = df_ensemble['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
df_ensemble = pd.DataFrame(predictions_ensemble, columns = ['has_cactus']) df_ensemble['has_cactus'] = df_ensemble['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	df_ensemble['id'] = '' cols = df_ensemble.columns.tolist() cols = cols[-1:] + cols[:-1] df_ensemble = df_ensemble[cols]  for i, img in enumerate(test_images):     df_ensemble.set_value(i,'id',img)  #making submission df_ensemble.to_csv('ensemble_submission.csv',index = False)
df_ensemble['id'] = '' cols = df_ensemble.columns.tolist() cols = cols[-1:] + cols[:-1] df_ensemble = df_ensemble[cols]  for i, img in enumerate(test_images):     df_ensemble.set_value(i,'id',img)  #making submission df_ensemble.to_csv('ensemble_submission.csv',index = False)	df_vgg = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_vgg['has_cactus'] = df_vgg['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
df_vgg = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_vgg['has_cactus'] = df_vgg['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	df_vgg['id'] = '' cols = df_vgg.columns.tolist() cols = cols[-1:] + cols[:-1] df_vgg = df_vgg[cols]  for i, img in enumerate(test_images):     df_vgg.set_value(i,'id',img)  #making submission df_vgg.to_csv('vgg_submission.csv',index = False)
df_vgg['id'] = '' cols = df_vgg.columns.tolist() cols = cols[-1:] + cols[:-1] df_vgg = df_vgg[cols]  for i, img in enumerate(test_images):     df_vgg.set_value(i,'id',img)  #making submission df_vgg.to_csv('vgg_submission.csv',index = False)	df_vgg.head()
df_vgg.head()	df_resnet = pd.DataFrame(resnet_predictions, columns = ['has_cactus']) df_resnet['has_cactus'] = df_resnet['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
df_resnet = pd.DataFrame(resnet_predictions, columns = ['has_cactus']) df_resnet['has_cactus'] = df_resnet['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	df_resnet['id'] = '' cols = df_resnet.columns.tolist() cols = cols[-1:] + cols[:-1] df_resnet = df_resnet[cols]  for i, img in enumerate(test_images):     df_resnet.set_value(i,'id',img)  #making submission df_resnet.to_csv('resnet_submission.csv',index = False)
df_resnet['id'] = '' cols = df_resnet.columns.tolist() cols = cols[-1:] + cols[:-1] df_resnet = df_resnet[cols]  for i, img in enumerate(test_images):     df_resnet.set_value(i,'id',img)  #making submission df_resnet.to_csv('resnet_submission.csv',index = False)	df_vgg1 = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_ensemble1 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])  df_t = 0.5 * df_vgg1['has_cactus'] + 0.5 * df_ensemble1['has_cactus'] df_t = pd.DataFrame(df_t, columns = ['has_cactus']) df_t['has_cactus'] = df_t['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)  df_t['id'] = '' cols = df_t.columns.tolist() cols = cols[-1:] + cols[:-1] df_t = df_t[cols]  for i, img in enumerate(test_images):     df_t.set_value(i,'id',img)  #making submission df_t.to_csv('vgg_ensemble_submission.csv',index = False)
df_vgg1 = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_ensemble1 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus'])  df_t = 0.5 * df_vgg1['has_cactus'] + 0.5 * df_ensemble1['has_cactus'] df_t = pd.DataFrame(df_t, columns = ['has_cactus']) df_t['has_cactus'] = df_t['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)  df_t['id'] = '' cols = df_t.columns.tolist() cols = cols[-1:] + cols[:-1] df_t = df_t[cols]  for i, img in enumerate(test_images):     df_t.set_value(i,'id',img)  #making submission df_t.to_csv('vgg_ensemble_submission.csv',index = False)	df_vgg2 = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_ensemble2 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus']) df_resnet2 = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])  df_t2 = 0.45 * df_vgg2['has_cactus'] + 0.45 * df_ensemble2['has_cactus'] + 0.10 * df_resnet2['has_cactus'] df_t2 = pd.DataFrame(df_t2, columns = ['has_cactus']) df_t2['has_cactus'] = df_t2['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)  df_t2['id'] = '' cols = df_t2.columns.tolist() cols = cols[-1:] + cols[:-1] df_t2 = df_t2[cols]  for i, img in enumerate(test_images):     df_t2.set_value(i,'id',img)  #making submission df_t2.to_csv('vgg_ensemble_resnet_submission.csv',index = False)
df_vgg2 = pd.DataFrame(predictions_vgg, columns = ['has_cactus']) df_ensemble2 = pd.DataFrame(predictions_ensemble, columns = ['has_cactus']) df_resnet2 = pd.DataFrame(resnet_predictions, columns = ['has_cactus'])  df_t2 = 0.45 * df_vgg2['has_cactus'] + 0.45 * df_ensemble2['has_cactus'] + 0.10 * df_resnet2['has_cactus'] df_t2 = pd.DataFrame(df_t2, columns = ['has_cactus']) df_t2['has_cactus'] = df_t2['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)  df_t2['id'] = '' cols = df_t2.columns.tolist() cols = cols[-1:] + cols[:-1] df_t2 = df_t2[cols]  for i, img in enumerate(test_images):     df_t2.set_value(i,'id',img)  #making submission df_t2.to_csv('vgg_ensemble_resnet_submission.csv',index = False)	NB_END
from fastai import * from fastai.vision import * from sklearn.model_selection import KFold	!ls ../input
!ls ../input	PATH = Path('../input')
PATH = Path('../input')	def create_data(valid_idx):     test = ImageList.from_df(sub_csv, path=PATH/'test', folder='test')     data = (ImageList.from_df(df, path=PATH/'train', folder='train')             .split_by_idx(valid_idx)             .label_from_df()             .add_test(test)             .transform(get_transforms(flip_vert=True, max_rotate=25, max_zoom=1.2, max_lighting=0.3))             .databunch(path='.', bs=64)             .normalize(imagenet_stats)            )     return data    
def create_data(valid_idx):     test = ImageList.from_df(sub_csv, path=PATH/'test', folder='test')     data = (ImageList.from_df(df, path=PATH/'train', folder='train')             .split_by_idx(valid_idx)             .label_from_df()             .add_test(test)             .transform(get_transforms(flip_vert=True, max_rotate=25, max_zoom=1.2, max_lighting=0.3))             .databunch(path='.', bs=64)             .normalize(imagenet_stats)            )     return data    	df = pd.read_csv(PATH/'train.csv') df.head()
df = pd.read_csv(PATH/'train.csv') df.head()	sub_csv = pd.read_csv(PATH/'sample_submission.csv') sub_csv.head()
sub_csv = pd.read_csv(PATH/'sample_submission.csv') sub_csv.head()	kf = KFold(n_splits=5, random_state=5) epochs = 6
kf = KFold(n_splits=5, random_state=5) epochs = 6	lr = 1e-2
lr = 1e-2	preds = [] for train_idx, valid_idx in kf.split(df):     data = create_data(valid_idx)     learn = create_cnn(data, models.densenet201, metrics=[accuracy])     learn.fit_one_cycle(epochs, slice(lr))     learn.unfreeze()     learn.fit_one_cycle(epochs, slice(lr/400, lr/4))     learn.fit_one_cycle(epochs, slice(lr/800, lr/8))     preds.append(learn.get_preds(ds_type=DatasetType.Test))
preds = [] for train_idx, valid_idx in kf.split(df):     data = create_data(valid_idx)     learn = create_cnn(data, models.densenet201, metrics=[accuracy])     learn.fit_one_cycle(epochs, slice(lr))     learn.unfreeze()     learn.fit_one_cycle(epochs, slice(lr/400, lr/4))     learn.fit_one_cycle(epochs, slice(lr/800, lr/8))     preds.append(learn.get_preds(ds_type=DatasetType.Test))	ens = torch.cat([preds[i][0][:,1].view(-1, 1) for i in range(5)], dim=1)
ens = torch.cat([preds[i][0][:,1].view(-1, 1) for i in range(5)], dim=1)	ens1  = (ens.mean(1)>=0.5).long(); ens1[:10]
ens1  = (ens.mean(1)>=0.5).long(); ens1[:10]	ens2 = (ens.mean(1)>0.5).long(); ens2[:10]
ens2 = (ens.mean(1)>0.5).long(); ens2[:10]	sub_csv['has_cactus'] = ens1
sub_csv['has_cactus'] = ens1	sub_csv.to_csv('submission.csv', index=False)
sub_csv.to_csv('submission.csv', index=False)	sub_csv['has_cactus'] = ens2
sub_csv['has_cactus'] = ens2	sub_csv.to_csv('submission2.csv', index=False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	from pathlib import Path import fastai from fastai.vision import *
from pathlib import Path import fastai from fastai.vision import *	PATH = Path('../input') np.random.seed(42)
PATH = Path('../input') np.random.seed(42)	batch_size = 64
batch_size = 64	tfms = get_transforms(flip_vert=True, max_zoom=1.5 ,max_lighting=0.5)
tfms = get_transforms(flip_vert=True, max_zoom=1.5 ,max_lighting=0.5)	src = (ImageList.from_csv(csv_name='train.csv', path=PATH, folder='train/train')             .split_by_rand_pct()             .label_from_df(cols='has_cactus'))
src = (ImageList.from_csv(csv_name='train.csv', path=PATH, folder='train/train')             .split_by_rand_pct()             .label_from_df(cols='has_cactus'))	data = (src.transform(tfms,size=128)           .databunch(bs=batch_size)            .normalize(imagenet_stats))
data = (src.transform(tfms,size=128)           .databunch(bs=batch_size)            .normalize(imagenet_stats))	data.show_batch(rows=3, figsize=(9, 9))
data.show_batch(rows=3, figsize=(9, 9))	data.classes, data.c
data.classes, data.c	learn = cnn_learner(data, models.densenet161, metrics=accuracy, path='./')
learn = cnn_learner(data, models.densenet161, metrics=accuracy, path='./')	learn.lr_find() learn.recorder.plot(suggestion=True)
learn.lr_find() learn.recorder.plot(suggestion=True)	lr = 1e-3 learn.fit_one_cycle(5,slice(lr))
lr = 1e-3 learn.fit_one_cycle(5,slice(lr))	learn.save('stage-1-dense161')
learn.save('stage-1-dense161')	learn.unfreeze()
learn.unfreeze()	learn.fit_one_cycle(5, max_lr=slice(1e-5,lr/5))
learn.fit_one_cycle(5, max_lr=slice(1e-5,lr/5))	learn.save('stage-2-dense161')
learn.save('stage-2-dense161')	interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(7,6))
interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(9, figsize=(7,6))	interp.plot_confusion_matrix(figsize=(3, 3))
interp.plot_confusion_matrix(figsize=(3, 3))	data = (src.transform(tfms, size=128)         .add_test_folder('test/test')         .databunch()         .normalize(imagenet_stats))
data = (src.transform(tfms, size=128)         .add_test_folder('test/test')         .databunch()         .normalize(imagenet_stats))	learn.data = data
learn.data = data	probs, _ = learn.get_preds(ds_type=DatasetType.Test)
probs, _ = learn.get_preds(ds_type=DatasetType.Test)	ilst = data.test_ds.x
ilst = data.test_ds.x	fnames = [item.name for item in ilst.items]; fnames[:10]
fnames = [item.name for item in ilst.items]; fnames[:10]	test_df = pd.DataFrame({'id': fnames, 'has_cactus': probs.argmax(1)}); test_df.head()
test_df = pd.DataFrame({'id': fnames, 'has_cactus': probs.argmax(1)}); test_df.head()	test_df.to_csv('submission.csv', index=None)
from PIL import Image import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tqdm import tqdm from keras.preprocessing.image import ImageDataGenerator import keras from keras.models import Sequential from keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D,Activation, BatchNormalization, GlobalAveragePooling2D from sklearn.model_selection import train_test_split from keras.optimizers import Adam, SGD from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau from keras.regularizers import l2 import matplotlib.pyplot as plt	def load_data(dataframe=None, batch_size=16, mode='categorical'):     if dataframe is None:         dataframe = pd.read_csv('../input/train.csv')     dataframe['has_cactus'] = dataframe['has_cactus'].apply(str)     gen = ImageDataGenerator(rescale=1./255., validation_split=0.1, horizontal_flip=True, vertical_flip=True)          trainGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train/',  x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          testGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train/',  x_col='id',y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          return trainGen, testGen
def load_data(dataframe=None, batch_size=16, mode='categorical'):     if dataframe is None:         dataframe = pd.read_csv('../input/train.csv')     dataframe['has_cactus'] = dataframe['has_cactus'].apply(str)     gen = ImageDataGenerator(rescale=1./255., validation_split=0.1, horizontal_flip=True, vertical_flip=True)          trainGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train/',  x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          testGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train/',  x_col='id',y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          return trainGen, testGen	# load data trainGen, valGen = load_data(batch_size=32)
# load data trainGen, valGen = load_data(batch_size=32)	# define model def train_model():     model = Sequential()     model.add(Conv2D(32, (3, 3), padding='same',                      input_shape=(32, 32, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Dropout(0.2))      model.add(Conv2D(64, (3, 3), padding='same'))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Dropout(0.3))          model.add(Conv2D(128, (3, 3), padding='same'))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))      model.add(Flatten()) #     model.add(GlobalAveragePooling2D())     model.add(Dense(16))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Dropout(0.3))     model.add(Dense(2))     model.add(BatchNormalization())     model.add(Activation('softmax'))          return model
# define model def train_model():     model = Sequential()     model.add(Conv2D(32, (3, 3), padding='same',                      input_shape=(32, 32, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Dropout(0.2))      model.add(Conv2D(64, (3, 3), padding='same'))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))     model.add(Dropout(0.3))          model.add(Conv2D(128, (3, 3), padding='same'))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPooling2D(pool_size=(2, 2)))      model.add(Flatten()) #     model.add(GlobalAveragePooling2D())     model.add(Dense(16))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Dropout(0.3))     model.add(Dense(2))     model.add(BatchNormalization())     model.add(Activation('softmax'))          return model	model = train_model() # initiate Adam optimizer opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-5) model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-5, verbose=1)]  model.fit_generator(trainGen, steps_per_epoch=4922, epochs=4, validation_data=valGen,      validation_steps=493, shuffle=True, callbacks=cbs)
model = train_model() # initiate Adam optimizer opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-5) model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-5, verbose=1)]  model.fit_generator(trainGen, steps_per_epoch=4922, epochs=4, validation_data=valGen,      validation_steps=493, shuffle=True, callbacks=cbs)	test_set = pd.read_csv('../input/sample_submission.csv') pred = np.empty((test_set.shape[0],)) for n in tqdm(range(test_set.shape[0])):     data = np.array(Image.open('../input/test/test/'+test_set.id[n]))     data = data.astype(np.float32) / 255.     pred[n] = model.predict(data.reshape((1, 32, 32, 3)))[0][1]  test_set['has_cactus'] = pred test_set.to_csv('sample_submission.csv', index=False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  import fastai from fastai.vision import *  # Any results you write to the current directory are saved as output."	print(fastai.__version__)
print(fastai.__version__)	PATH = Path('.') !ls {PATH}
PATH = Path('.') !ls {PATH}	df = pd.read_csv(PATH/'../input/train.csv'); df.head()
df = pd.read_csv(PATH/'../input/train.csv'); df.head()	df['has_cactus'].hist()
df['has_cactus'].hist()	bs = 128
bs = 128	tfms = get_transforms(do_flip=True, flip_vert=True)
tfms = get_transforms(do_flip=True, flip_vert=True)	data = (ImageList.from_csv(csv_name='train.csv', path=PATH/'../input', folder='train/train')             .split_by_rand_pct()             .label_from_df(cols='has_cactus')             .transform(tfms)             .add_test_folder('test/test')             .databunch(bs=bs)            .normalize(imagenet_stats))
data = (ImageList.from_csv(csv_name='train.csv', path=PATH/'../input', folder='train/train')             .split_by_rand_pct()             .label_from_df(cols='has_cactus')             .transform(tfms)             .add_test_folder('test/test')             .databunch(bs=bs)            .normalize(imagenet_stats))	data.show_batch(rows=3, figsize=(7, 7))
data.show_batch(rows=3, figsize=(7, 7))	data.classes, data.c
data.classes, data.c	learn = cnn_learner(data, models.resnet34, metrics=accuracy, path=PATH)
learn = cnn_learner(data, models.resnet34, metrics=accuracy, path=PATH)	learn.fit_one_cycle(4)
learn.fit_one_cycle(4)	interp = ClassificationInterpretation.from_learner(learn) losses, idxs = interp.top_losses() interp.plot_top_losses(9, figsize=(7, 8))
interp = ClassificationInterpretation.from_learner(learn) losses, idxs = interp.top_losses() interp.plot_top_losses(9, figsize=(7, 8))	interp.plot_confusion_matrix(figsize=(3, 3))
interp.plot_confusion_matrix(figsize=(3, 3))	learn.unfreeze()
learn.unfreeze()	learn.fit_one_cycle(1)
learn.fit_one_cycle(1)	learn.recorder.plot()
learn.recorder.plot()	probs, preds = learn.get_preds(ds_type=DatasetType.Test)
probs, preds = learn.get_preds(ds_type=DatasetType.Test)	preds.shape, probs.shape
preds.shape, probs.shape	!ls {PATH}/../input/test/test | wc -l
!ls {PATH}/../input/test/test | wc -l	ilst = data.test_ds.x
ilst = data.test_ds.x	fnames = [item.name for item in ilst.items]; fnames[:10]
fnames = [item.name for item in ilst.items]; fnames[:10]	test_df = pd.DataFrame({'id': fnames, 'has_cactus': probs[:, 1]}); test_df
test_df = pd.DataFrame({'id': fnames, 'has_cactus': probs[:, 1]}); test_df	test_df.to_csv('submission.csv', index=None)
test_df.to_csv('submission.csv', index=None)	!head submission.csv
%reload_ext autoreload %autoreload 2 %matplotlib inline	from fastai import * from fastai.vision import *
from fastai import * from fastai.vision import *	"PATH = ""../input/"" sz=32 bs=512"
"PATH = ""../input/"" sz=32 bs=512"	"tfms = get_transforms(flip_vert=True, max_rotate=90.) data = ImageDataBunch.from_csv(PATH, ds_tfms=tfms,         folder=""train/train"", csv_labels=\'train.csv\', test=""test/test"",         valid_pct=0.1, fn_col=0, label_col=1).normalize(imagenet_stats)"
"tfms = get_transforms(flip_vert=True, max_rotate=90.) data = ImageDataBunch.from_csv(PATH, ds_tfms=tfms,         folder=""train/train"", csv_labels=\'train.csv\', test=""test/test"",         valid_pct=0.1, fn_col=0, label_col=1).normalize(imagenet_stats)"	print(f'We have {len(data.classes)} different classes\ ') print(f'Classes: \  {data.classes}')
print(f'We have {len(data.classes)} different classes\ ') print(f'Classes: \  {data.classes}')	print (f'We have {len(data.train_ds)+len(data.valid_ds)+len(data.test_ds)} images in the total dataset')
print (f'We have {len(data.train_ds)+len(data.valid_ds)+len(data.test_ds)} images in the total dataset')	data.show_batch(8, figsize=(20,15))
data.show_batch(8, figsize=(20,15))	def get_ex(): return open_image('../input/train/train/000c8a36845c0208e833c79c1bffedd1.jpg')  def plots_f(rows, cols, width, height, **kwargs):     [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(         rows,cols,figsize=(width,height))[1].flatten())]
def get_ex(): return open_image('../input/train/train/000c8a36845c0208e833c79c1bffedd1.jpg')  def plots_f(rows, cols, width, height, **kwargs):     [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(         rows,cols,figsize=(width,height))[1].flatten())]	plots_f(4, 4, 8, 8, size=sz)
plots_f(4, 4, 8, 8, size=sz)	learn = create_cnn(data, models.resnet50, metrics=accuracy, path='../kaggle/working', model_dir='../kaggle/working/model',callback_fns=ShowGraph)
learn = create_cnn(data, models.resnet50, metrics=accuracy, path='../kaggle/working', model_dir='../kaggle/working/model',callback_fns=ShowGraph)	lrf=learn.lr_find() learn.recorder.plot()
lrf=learn.lr_find() learn.recorder.plot()	lr=5e-3
lr=5e-3	learn.fit_one_cycle(1,lr)
learn.fit_one_cycle(1,lr)	learn.save('cactus-stage-1')
learn.save('cactus-stage-1')	learn.unfreeze()
learn.unfreeze()	lrf=learn.lr_find() learn.recorder.plot()
lrf=learn.lr_find() learn.recorder.plot()	learn.fit_one_cycle(3, max_lr=slice(1e-6, 1e-4))
learn.fit_one_cycle(3, max_lr=slice(1e-6, 1e-4))	learn.save('cactus-stage-2')
learn.save('cactus-stage-2')	preds_test,y_test=learn.get_preds(ds_type=DatasetType.Test)
preds_test,y_test=learn.get_preds(ds_type=DatasetType.Test)	preds_test_tta,y_test_tta=learn.TTA(ds_type=DatasetType.Test)
preds_test_tta,y_test_tta=learn.TTA(ds_type=DatasetType.Test)	sub=pd.read_csv(f'{PATH}/sample_submission.csv').set_index('id')
sub=pd.read_csv(f'{PATH}/sample_submission.csv').set_index('id')	"clean_fname=np.vectorize(lambda fname: str(fname).split(\'/\')[-1].split(\'.\')[0]+"".jpg"") fname_cleaned=clean_fname(data.test_ds.items) fname_cleaned=fname_cleaned.astype(str) fname_cleaned"
"clean_fname=np.vectorize(lambda fname: str(fname).split(\'/\')[-1].split(\'.\')[0]+"".jpg"") fname_cleaned=clean_fname(data.test_ds.items) fname_cleaned=fname_cleaned.astype(str) fname_cleaned"	sub.loc[fname_cleaned,'has_cactus']=to_np(preds_test[:,1]) sub.to_csv(f'submission.csv') sub.loc[fname_cleaned,'has_cactus']=to_np(preds_test_tta[:,1]) sub.to_csv(f'submission_tta.csv')
sub.loc[fname_cleaned,'has_cactus']=to_np(preds_test[:,1]) sub.to_csv(f'submission.csv') sub.loc[fname_cleaned,'has_cactus']=to_np(preds_test_tta[:,1]) sub.to_csv(f'submission_tta.csv')	classes = preds_test.argmax(1) classes sub.loc[fname_cleaned,'has_cactus']=to_np(classes) sub.to_csv(f'submission_1_0.csv')
classes = preds_test.argmax(1) classes sub.loc[fname_cleaned,'has_cactus']=to_np(classes) sub.to_csv(f'submission_1_0.csv')	NB_END
"import cv2 from sklearn.model_selection import train_test_split import numpy as np import matplotlib.pyplot as plt import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential import keras from keras.layers import Activation, Dropout, Flatten, Dense,Conv2D,Conv3D,MaxPooling2D,AveragePooling2D,BatchNormalization import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score import seaborn as sns import tensorflow as tf import matplotlib.image as mpimg print(os.listdir(""../input"")) print(os.listdir(""../input/weights/"")) IMAGE_SIZE = 32"	"train_dir = ""../input/aerial-cactus-identification/train/train/"" test_dir = ""../input/aerial-cactus-identification/test/test/"" train_df = pd.read_csv(\'../input/aerial-cactus-identification/train.csv\') train_df.head()"
"train_dir = ""../input/aerial-cactus-identification/train/train/"" test_dir = ""../input/aerial-cactus-identification/test/test/"" train_df = pd.read_csv(\'../input/aerial-cactus-identification/train.csv\') train_df.head()"	"im = cv2.imread(""../input/aerial-cactus-identification/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/aerial-cactus-identification/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     image = np.array(cv2.imread(train_dir + img_id))     X_tr.append(image)     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.flip(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.flipud(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.fliplr(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])                    X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)   
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     image = np.array(cv2.imread(train_dir + img_id))     X_tr.append(image)     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.flip(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.flipud(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])        X_tr.append(np.fliplr(image))     Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])                    X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)   	 X_tr_2 = X_tr Y_tr_2 = Y_tr 
 X_tr_2 = X_tr Y_tr_2 = Y_tr 	 X_tr = X_tr_2 Y_tr = Y_tr_2 
 X_tr = X_tr_2 Y_tr = Y_tr_2 	X_tr.shape,Y_tr.shape
X_tr.shape,Y_tr.shape	test_image_names = [] for filename in os.listdir(test_dir):     test_image_names.append(filename) test_image_names.sort() X_ts = [] #imges = test_df['id'].values for img_id in tqdm_notebook(test_image_names):     X_ts.append(cv2.imread(test_dir + img_id))     X_ts = np.asarray(X_ts) X_ts = X_ts.astype('float32') X_ts /= 255
test_image_names = [] for filename in os.listdir(test_dir):     test_image_names.append(filename) test_image_names.sort() X_ts = [] #imges = test_df['id'].values for img_id in tqdm_notebook(test_image_names):     X_ts.append(cv2.imread(test_dir + img_id))     X_ts = np.asarray(X_ts) X_ts = X_ts.astype('float32') X_ts /= 255	x_train,x_test,y_train,y_test = train_test_split(X_tr, Y_tr, test_size = 0.2 , stratify = Y_tr )
x_train,x_test,y_train,y_test = train_test_split(X_tr, Y_tr, test_size = 0.2 , stratify = Y_tr )	  base=keras.applications.vgg16.VGG16(include_top=False, weights='../input/weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',input_shape=(32,32,3))  
  base=keras.applications.vgg16.VGG16(include_top=False, weights='../input/weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',input_shape=(32,32,3))  	"  print(""Current train size:"",X_tr.shape) model = Sequential() model.add(base)  model.add(Flatten()) model.add(Dense(256, use_bias=True)) model.add(BatchNormalization()) model.add(Activation(""relu"")) model.add(Dropout(0.5)) model.add(Dense(256,activation=\'relu\')) model.add(BatchNormalization()) model.add(Dense(16, activation=\'tanh\')) model.add(Dense(1, activation=\'sigmoid\'))  model.compile(optimizer=\'rmsprop\', loss=\'binary_crossentropy\', metrics=[\'accuracy\']) model.summary() callback=[keras.callbacks.EarlyStopping(monitor=\'val_acc\', patience=20, verbose=1, mode=\'auto\', restore_best_weights=True),          keras.callbacks.ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=10, verbose=1, mode=\'auto\')] model.fit(X_tr,Y_tr,batch_size=64, epochs=80, verbose=1,   validation_split=0.1,callbacks=callback) "
"  print(""Current train size:"",X_tr.shape) model = Sequential() model.add(base)  model.add(Flatten()) model.add(Dense(256, use_bias=True)) model.add(BatchNormalization()) model.add(Activation(""relu"")) model.add(Dropout(0.5)) model.add(Dense(256,activation=\'relu\')) model.add(BatchNormalization()) model.add(Dense(16, activation=\'tanh\')) model.add(Dense(1, activation=\'sigmoid\'))  model.compile(optimizer=\'rmsprop\', loss=\'binary_crossentropy\', metrics=[\'accuracy\']) model.summary() callback=[keras.callbacks.EarlyStopping(monitor=\'val_acc\', patience=20, verbose=1, mode=\'auto\', restore_best_weights=True),          keras.callbacks.ReduceLROnPlateau(monitor=\'val_loss\', factor=0.1, patience=10, verbose=1, mode=\'auto\')] model.fit(X_tr,Y_tr,batch_size=64, epochs=80, verbose=1,   validation_split=0.1,callbacks=callback) "	"   clf=model y_pred_proba = clf.predict_proba(X_tr_2)  y_pred = clf.predict_classes(X_tr_2) conf_mat = confusion_matrix(Y_tr_2, y_pred) fig, ax = plt.subplots(figsize=(10,10))  sns.heatmap(conf_mat, annot=True, fmt=\'d\', xticklabels=[\'0\',\'1\'], yticklabels=[\'0\',\'1\']) plt.ylabel(\'Actual\') plt.xlabel(\'Predicted\') plt.show()  print(classification_report(Y_tr_2, y_pred, target_names=[\'0\',\'1\'])) print(""\ \  AUC: {:<0.4f}"".format(roc_auc_score(Y_tr_2,y_pred_proba)))  "
"   clf=model y_pred_proba = clf.predict_proba(X_tr_2)  y_pred = clf.predict_classes(X_tr_2) conf_mat = confusion_matrix(Y_tr_2, y_pred) fig, ax = plt.subplots(figsize=(10,10))  sns.heatmap(conf_mat, annot=True, fmt=\'d\', xticklabels=[\'0\',\'1\'], yticklabels=[\'0\',\'1\']) plt.ylabel(\'Actual\') plt.xlabel(\'Predicted\') plt.show()  print(classification_report(Y_tr_2, y_pred, target_names=[\'0\',\'1\'])) print(""\ \  AUC: {:<0.4f}"".format(roc_auc_score(Y_tr_2,y_pred_proba)))  "	   test_df = pd.read_csv('../input/aerial-cactus-identification/sample_submission.csv') X_test = [] imges = test_df['id'].values for img_id in tqdm_notebook(imges):     X_test.append(cv2.imread(test_dir + img_id))      X_test = np.asarray(X_test) X_test = X_test.astype('float32') X_test /= 255  y_test_pred  = model.predict_proba(X_test)  test_df['has_cactus'] = y_test_pred test_df.to_csv('tf_learning_vgg16_aug2_80epoch.csv', index=False)
import fastai from fastai.vision import * from sklearn.model_selection import KFold	# Copy pretrained model weights to the default path !mkdir '/tmp/.torch' !mkdir '/tmp/.torch/models/' #!cp '../input/resnet18/resnet18.pth' '/tmp/.torch/models/resnet18-5c106cde.pth' #!cp '../input/densenet121/densenet121.pth' '/tmp/.torch/models/densenet121-a639ec97.pth' !cp '../input/densenet201/densenet201.pth' '/tmp/.torch/models/densenet201-c1103571.pth'
# Copy pretrained model weights to the default path !mkdir '/tmp/.torch' !mkdir '/tmp/.torch/models/' #!cp '../input/resnet18/resnet18.pth' '/tmp/.torch/models/resnet18-5c106cde.pth' #!cp '../input/densenet121/densenet121.pth' '/tmp/.torch/models/densenet121-a639ec97.pth' !cp '../input/densenet201/densenet201.pth' '/tmp/.torch/models/densenet201-c1103571.pth'	fastai.__version__
fastai.__version__	data_path = Path('../input/aerial-cactus-identification') df = pd.read_csv(data_path/'train.csv') df.head()
data_path = Path('../input/aerial-cactus-identification') df = pd.read_csv(data_path/'train.csv') df.head()	sub_csv = pd.read_csv(data_path/'sample_submission.csv') sub_csv.head()
sub_csv = pd.read_csv(data_path/'sample_submission.csv') sub_csv.head()	def create_databunch(valid_idx):     test = ImageList.from_df(sub_csv, path=data_path/'test', folder='test')     data = (ImageList.from_df(df, path=data_path/'train', folder='train')             .split_by_idx(valid_idx)             .label_from_df()             .add_test(test)             .transform(get_transforms(flip_vert=True, max_rotate=20.0), size=128)             .databunch(path='.', bs=64)             .normalize(imagenet_stats)            )     return data
def create_databunch(valid_idx):     test = ImageList.from_df(sub_csv, path=data_path/'test', folder='test')     data = (ImageList.from_df(df, path=data_path/'train', folder='train')             .split_by_idx(valid_idx)             .label_from_df()             .add_test(test)             .transform(get_transforms(flip_vert=True, max_rotate=20.0), size=128)             .databunch(path='.', bs=64)             .normalize(imagenet_stats)            )     return data	kf = KFold(n_splits=5, random_state=379) epochs = 6 lr = 1e-2 preds = [] for train_idx, valid_idx in kf.split(df):     data = create_databunch(valid_idx)     learn = create_cnn(data, models.densenet201, metrics=[accuracy])     learn.fit_one_cycle(epochs, slice(lr))     learn.unfreeze()     learn.fit_one_cycle(epochs, slice(lr/400, lr/4))     learn.fit_one_cycle(epochs, slice(lr/800, lr/8))     preds.append(learn.get_preds(ds_type=DatasetType.Test))
kf = KFold(n_splits=5, random_state=379) epochs = 6 lr = 1e-2 preds = [] for train_idx, valid_idx in kf.split(df):     data = create_databunch(valid_idx)     learn = create_cnn(data, models.densenet201, metrics=[accuracy])     learn.fit_one_cycle(epochs, slice(lr))     learn.unfreeze()     learn.fit_one_cycle(epochs, slice(lr/400, lr/4))     learn.fit_one_cycle(epochs, slice(lr/800, lr/8))     preds.append(learn.get_preds(ds_type=DatasetType.Test))	ens = torch.cat([preds[i][0][:,1].view(-1, 1) for i in range(5)], dim=1) ens  = (ens.mean(1)>0.5).long(); ens[:10]
ens = torch.cat([preds[i][0][:,1].view(-1, 1) for i in range(5)], dim=1) ens  = (ens.mean(1)>0.5).long(); ens[:10]	sub_csv['has_cactus'] = ens
sub_csv['has_cactus'] = ens	sub_csv.to_csv('submission.csv', index=False)
sub_csv.to_csv('submission.csv', index=False)	NB_END
"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.model_selection import train_test_split # from sklearn.model_selection import StratifiedKFold from sklearn.metrics import roc_curve, auc from tqdm import tqdm_notebook from fastai import * from fastai.vision import * from fastai.callbacks import SaveModelCallback from torchvision.models import * import os import cv2  print(os.listdir(""../input""))"	train_path = '../input/train/train/' test_path = '../input/test/test/'
train_path = '../input/train/train/' test_path = '../input/test/test/'	"data = pd.read_csv(""../input/train.csv"")"
"data = pd.read_csv(""../input/train.csv"")"	train_names = data.values train_labels = data['has_cactus'].values
train_names = data.values train_labels = data['has_cactus'].values	# skf = StratifiedKFold(n_splits=5, random_state=321)
# skf = StratifiedKFold(n_splits=5, random_state=321)	# val = [] # for tr_idx, val_idx in skf.split(train_names, train_labels): #     val.append(val_idx)
# val = [] # for tr_idx, val_idx in skf.split(train_names, train_labels): #     val.append(val_idx)	tr_n, val_n, tr_idx, val_idx = train_test_split(train_names, range(len(train_names)), test_size=0.3, stratify=train_labels, random_state=321)
tr_n, val_n, tr_idx, val_idx = train_test_split(train_names, range(len(train_names)), test_size=0.3, stratify=train_labels, random_state=321)	arch = resnet18 batch_size = 256 model_path = str(arch).split()[1]
arch = resnet18 batch_size = 256 model_path = str(arch).split()[1]	test_names = [] for name in os.listdir(test_path):     test_names.append(name)  df_test = pd.DataFrame(np.asarray(test_names), columns= ['id'])
test_names = [] for name in os.listdir(test_path):     test_names.append(name)  df_test = pd.DataFrame(np.asarray(test_names), columns= ['id'])	imgDataBunch = (ImageList.from_df(data, train_path)     .split_by_idx(val_idx)     .label_from_df(cols='has_cactus')     .add_test(ImageList.from_df(df_test, test_path))     .transform(tfms=get_transforms(flip_vert=True), size=32)     .databunch(bs=batch_size)     .normalize(imagenet_stats) )
imgDataBunch = (ImageList.from_df(data, train_path)     .split_by_idx(val_idx)     .label_from_df(cols='has_cactus')     .add_test(ImageList.from_df(df_test, test_path))     .transform(tfms=get_transforms(flip_vert=True), size=32)     .databunch(bs=batch_size)     .normalize(imagenet_stats) )	imgDataBunch.show_batch(rows=2, figsize=(4, 4))
imgDataBunch.show_batch(rows=2, figsize=(4, 4))	def getLearner():     return cnn_learner(imgDataBunch, arch, pretrained=True, path='.', metrics=accuracy, ps=0.5, callback_fns=ShowGraph)  learner = getLearner()
def getLearner():     return cnn_learner(imgDataBunch, arch, pretrained=True, path='.', metrics=accuracy, ps=0.5, callback_fns=ShowGraph)  learner = getLearner()	learner.recorder.plot(suggestion=True)
learner.recorder.plot(suggestion=True)	learner.fit_one_cycle(5, 1e-2)
learner.fit_one_cycle(5, 1e-2)	learner.recorder.plot_losses()
learner.recorder.plot_losses()	interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(title='Confusion matrix')
interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(title='Confusion matrix')	"def cal_auc_and_plot(learner):     preds, y = learner.get_preds()     probs = np.exp(preds[:,1])     fpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)     roc_auc = auc(fpr, tpr)     plt.figure()     plt.plot(fpr, tpr, color=\'darkorange\', label=\'ROC curve (area = %0.2f)\' % roc_auc)     plt.plot([0, 1], [0, 1], color=\'navy\', linestyle=\'--\')     plt.xlim([-0.01, 1.0])     plt.ylim([0.0, 1.01])     plt.xlabel(\'False Positive Rate\')     plt.ylabel(\'True Positive Rate\')     plt.title(\'Receiver operating characteristic\')     plt.legend(loc=""lower right"")     return roc_auc"
"def cal_auc_and_plot(learner):     preds, y = learner.get_preds()     probs = np.exp(preds[:,1])     fpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)     roc_auc = auc(fpr, tpr)     plt.figure()     plt.plot(fpr, tpr, color=\'darkorange\', label=\'ROC curve (area = %0.2f)\' % roc_auc)     plt.plot([0, 1], [0, 1], color=\'navy\', linestyle=\'--\')     plt.xlim([-0.01, 1.0])     plt.ylim([0.0, 1.01])     plt.xlabel(\'False Positive Rate\')     plt.ylabel(\'True Positive Rate\')     plt.title(\'Receiver operating characteristic\')     plt.legend(loc=""lower right"")     return roc_auc"	stage1_auc = cal_auc_and_plot(learner) stage1_auc
stage1_auc = cal_auc_and_plot(learner) stage1_auc	learner.save(model_path + '_stage1')
learner.save(model_path + '_stage1')	learner.load(model_path + '_stage1')
learner.load(model_path + '_stage1')	learner.unfreeze()
learner.unfreeze()	learner.lr_find()
learner.lr_find()	learner.recorder.plot(suggestion=True)
learner.recorder.plot(suggestion=True)	learner.fit_one_cycle(10, max_lr=slice(1e-5, 1e-4), callbacks=[SaveModelCallback(learner, name='stage2')])
learner.fit_one_cycle(10, max_lr=slice(1e-5, 1e-4), callbacks=[SaveModelCallback(learner, name='stage2')])	learner.load('stage2')
learner.load('stage2')	learner.recorder.plot_losses()
learner.recorder.plot_losses()	interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(title='Confusion matrix')
interp = ClassificationInterpretation.from_learner(learner) interp.plot_confusion_matrix(title='Confusion matrix')	stage2_auc = cal_auc_and_plot(learner)
stage2_auc = cal_auc_and_plot(learner)	if stage2_auc < stage1_auc:     learner.load(model_path + '_stage1')
if stage2_auc < stage1_auc:     learner.load(model_path + '_stage1')	preds_test, y_test = learner.TTA(ds_type=DatasetType.Test)
preds_test, y_test = learner.TTA(ds_type=DatasetType.Test)	cactus_preds = preds_test[:, 1] cactus_preds = cactus_preds.tolist()
cactus_preds = preds_test[:, 1] cactus_preds = cactus_preds.tolist()	SAMPLE_SUB = '/kaggle/input/sample_submission.csv' sample_df = pd.read_csv(SAMPLE_SUB) sample_list = list(sample_df.id)
SAMPLE_SUB = '/kaggle/input/sample_submission.csv' sample_df = pd.read_csv(SAMPLE_SUB) sample_list = list(sample_df.id)	pred_list = [p for p in cactus_preds]  # To know the id's, we create a dict of id:pred pred_dic = dict((key, value) for (key, value) in zip(learner.data.test_ds.items, pred_list))
pred_list = [p for p in cactus_preds]  # To know the id's, we create a dict of id:pred pred_dic = dict((key, value) for (key, value) in zip(learner.data.test_ds.items, pred_list))	pred_list_cor = [pred_dic[test_path + idx] for idx in sample_list]
pred_list_cor = [pred_dic[test_path + idx] for idx in sample_list]	# Next, a Pandas dataframe with id and label columns. df_sub = pd.DataFrame({'id':sample_list,'has_cactus':pred_list_cor})  # Export to csv df_sub.to_csv('{0}_submission.csv'.format(model_path), header=True, index=False)
# Next, a Pandas dataframe with id and label columns. df_sub = pd.DataFrame({'id':sample_list,'has_cactus':pred_list_cor})  # Export to csv df_sub.to_csv('{0}_submission.csv'.format(model_path), header=True, index=False)	df_sub.head()
# libraries import numpy as np import pandas as pd import os import cv2 import matplotlib.pyplot as plt %matplotlib inline  from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import torch from torch.utils.data import TensorDataset, DataLoader,Dataset import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import torch.optim as optim from torch.optim import lr_scheduler import time  from PIL import Image train_on_gpu = True from torch.utils.data.sampler import SubsetRandomSampler from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR from sklearn.metrics import accuracy_score import cv2	!pip install albumentations > /dev/null 2>&1 !pip install pretrainedmodels > /dev/null 2>&1 !pip install kekas > /dev/null 2>&1 !pip install adabound > /dev/null 2>&1
!pip install albumentations > /dev/null 2>&1 !pip install pretrainedmodels > /dev/null 2>&1 !pip install kekas > /dev/null 2>&1 !pip install adabound > /dev/null 2>&1	# more imports import albumentations from albumentations import torch as AT import pretrainedmodels import adabound  from kekas import Keker, DataOwner, DataKek from kekas.transformations import Transformer, to_torch, normalize from kekas.metrics import accuracy from kekas.modules import Flatten, AdaptiveConcatPool2d from kekas.callbacks import Callback, Callbacks, DebuggerCallback from kekas.utils import DotDict
# more imports import albumentations from albumentations import torch as AT import pretrainedmodels import adabound  from kekas import Keker, DataOwner, DataKek from kekas.transformations import Transformer, to_torch, normalize from kekas.metrics import accuracy from kekas.modules import Flatten, AdaptiveConcatPool2d from kekas.callbacks import Callback, Callbacks, DebuggerCallback from kekas.utils import DotDict	"labels = pd.read_csv(\'../input/train.csv\') fig = plt.figure(figsize=(25, 8)) train_imgs = os.listdir(""../input/train/train"") for idx, img in enumerate(np.random.choice(train_imgs, 20)):     ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])     im = Image.open(""../input/train/train/"" + img)     plt.imshow(im)     lab = labels.loc[labels[\'id\'] == img, \'has_cactus\'].values[0]     ax.set_title(f\'Label: {lab}\')"
"labels = pd.read_csv(\'../input/train.csv\') fig = plt.figure(figsize=(25, 8)) train_imgs = os.listdir(""../input/train/train"") for idx, img in enumerate(np.random.choice(train_imgs, 20)):     ax = fig.add_subplot(4, 20//4, idx+1, xticks=[], yticks=[])     im = Image.open(""../input/train/train/"" + img)     plt.imshow(im)     lab = labels.loc[labels[\'id\'] == img, \'has_cactus\'].values[0]     ax.set_title(f\'Label: {lab}\')"	test_img = os.listdir('../input/test/test') test_df = pd.DataFrame(test_img, columns=['id']) test_df['has_cactus'] = -1 test_df['data_type'] = 'test'  labels['has_cactus'] = labels['has_cactus'].astype(int) labels['data_type'] = 'train'  labels.head()
test_img = os.listdir('../input/test/test') test_df = pd.DataFrame(test_img, columns=['id']) test_df['has_cactus'] = -1 test_df['data_type'] = 'test'  labels['has_cactus'] = labels['has_cactus'].astype(int) labels['data_type'] = 'train'  labels.head()	labels.loc[labels['data_type'] == 'train', 'has_cactus'].value_counts()
labels.loc[labels['data_type'] == 'train', 'has_cactus'].value_counts()	# splitting data into train and validation train, valid = train_test_split(labels, stratify=labels.has_cactus, test_size=0.2)
# splitting data into train and validation train, valid = train_test_split(labels, stratify=labels.has_cactus, test_size=0.2)	"def reader_fn(i, row):     image = cv2.imread(f""../input/{row[\'data_type\']}/{row[\'data_type\']}/{row[\'id\']}"")[:,:,::-1] # BGR -> RGB     label = torch.Tensor([row[""has_cactus""]])     return {""image"": image, ""label"": label}"
"def reader_fn(i, row):     image = cv2.imread(f""../input/{row[\'data_type\']}/{row[\'data_type\']}/{row[\'id\']}"")[:,:,::-1] # BGR -> RGB     label = torch.Tensor([row[""has_cactus""]])     return {""image"": image, ""label"": label}"	def augs(p=0.5):     return albumentations.Compose([         albumentations.HorizontalFlip(),         albumentations.VerticalFlip(),         albumentations.RandomBrightness(),     ], p=p)
def augs(p=0.5):     return albumentations.Compose([         albumentations.HorizontalFlip(),         albumentations.VerticalFlip(),         albumentations.RandomBrightness(),     ], p=p)	"def get_transforms(dataset_key, size, p):      PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[""image""])      NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])          return train_tfms, val_tfms"
"def get_transforms(dataset_key, size, p):      PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[""image""])      NRM_TFMS = transforms.Compose([         Transformer(dataset_key, to_torch()),         Transformer(dataset_key, normalize())     ])          train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])     val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])          return train_tfms, val_tfms"	"train_tfms, val_tfms = get_transforms(""image"", 32, 0.5)"
"train_tfms, val_tfms = get_transforms(""image"", 32, 0.5)"	train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms) val_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)  batch_size = 64 workers = 0  train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True) val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)
train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms) val_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)  batch_size = 64 workers = 0  train_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True) val_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)	test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms) test_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)
test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms) test_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)	"class Net(nn.Module):     def __init__(             self,             num_classes: int,             p: float = 0.2,             pooling_size: int = 2,             last_conv_size: int = 1664,             arch: str = ""densenet169"",             pretrained: str = ""imagenet"") -> None:         """"""A simple model to finetune.                  Args:             num_classes: the number of target classes, the size of the last layer\'s output             p: dropout probability             pooling_size: the size of the result feature map after adaptive pooling layer             last_conv_size: size of the flatten last backbone conv layer             arch: the name of the architecture form pretrainedmodels             pretrained: the mode for pretrained model from pretrainedmodels         """"""         super().__init__()         net = pretrainedmodels.__dict__[arch](pretrained=pretrained)         modules = list(net.children())[:-1]  # delete last layer         # add custom head         modules += [nn.Sequential(             # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling              # AdaptiveConcatPool2d(size=pooling_size),             Flatten(),             nn.BatchNorm1d(1664),             nn.Dropout(p),             nn.Linear(1664, num_classes)         )]         self.net = nn.Sequential(*modules)      def forward(self, x):         logits = self.net(x)         return logits"
"class Net(nn.Module):     def __init__(             self,             num_classes: int,             p: float = 0.2,             pooling_size: int = 2,             last_conv_size: int = 1664,             arch: str = ""densenet169"",             pretrained: str = ""imagenet"") -> None:         """"""A simple model to finetune.                  Args:             num_classes: the number of target classes, the size of the last layer\'s output             p: dropout probability             pooling_size: the size of the result feature map after adaptive pooling layer             last_conv_size: size of the flatten last backbone conv layer             arch: the name of the architecture form pretrainedmodels             pretrained: the mode for pretrained model from pretrainedmodels         """"""         super().__init__()         net = pretrainedmodels.__dict__[arch](pretrained=pretrained)         modules = list(net.children())[:-1]  # delete last layer         # add custom head         modules += [nn.Sequential(             # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling              # AdaptiveConcatPool2d(size=pooling_size),             Flatten(),             nn.BatchNorm1d(1664),             nn.Dropout(p),             nn.Linear(1664, num_classes)         )]         self.net = nn.Sequential(*modules)      def forward(self, x):         logits = self.net(x)         return logits"	dataowner = DataOwner(train_dl, val_dl, None) model = Net(num_classes=1) criterion = nn.BCEWithLogitsLoss()
dataowner = DataOwner(train_dl, val_dl, None) model = Net(num_classes=1) criterion = nn.BCEWithLogitsLoss()	"def step_fn(model: torch.nn.Module,             batch: torch.Tensor) -> torch.Tensor:     """"""Determine what your model will do with your data.      Args:         model: the pytorch module to pass input in         batch: the batch of data from the DataLoader      Returns:         The models forward pass results     """"""          inp = batch[""image""]     return model(inp)"
"def step_fn(model: torch.nn.Module,             batch: torch.Tensor) -> torch.Tensor:     """"""Determine what your model will do with your data.      Args:         model: the pytorch module to pass input in         batch: the batch of data from the DataLoader      Returns:         The models forward pass results     """"""          inp = batch[""image""]     return model(inp)"	def bce_accuracy(target: torch.Tensor,                  preds: torch.Tensor,                  thresh: bool = 0.5) -> float:     target = target.cpu().detach().numpy()     preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)     return accuracy_score(target, preds)    def roc_auc(target: torch.Tensor,                  preds: torch.Tensor) -> float:     target = target.cpu().detach().numpy()     preds = torch.sigmoid(preds).cpu().detach().numpy()     return roc_auc_score(target, preds)
def bce_accuracy(target: torch.Tensor,                  preds: torch.Tensor,                  thresh: bool = 0.5) -> float:     target = target.cpu().detach().numpy()     preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)     return accuracy_score(target, preds)    def roc_auc(target: torch.Tensor,                  preds: torch.Tensor) -> float:     target = target.cpu().detach().numpy()     preds = torch.sigmoid(preds).cpu().detach().numpy()     return roc_auc_score(target, preds)	"keker = Keker(model=model,               dataowner=dataowner,               criterion=criterion,               step_fn=step_fn,               target_key=""label"",               metrics={""acc"": bce_accuracy, \'auc\': roc_auc},               opt=torch.optim.SGD,               opt_params={""momentum"": 0.99})"
"keker = Keker(model=model,               dataowner=dataowner,               criterion=criterion,               step_fn=step_fn,               target_key=""label"",               metrics={""acc"": bce_accuracy, \'auc\': roc_auc},               opt=torch.optim.SGD,               opt_params={""momentum"": 0.99})"	"keker.unfreeze(model_attr=""net"")  layer_num = -1 keker.freeze_to(layer_num, model_attr=""net"")"
"keker.unfreeze(model_attr=""net"")  layer_num = -1 keker.freeze_to(layer_num, model_attr=""net"")"	keker.kek_one_cycle(max_lr=1e-2,                  # the maximum learning rate                     cycle_len=4,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs') keker.plot_kek('train_logs')
keker.kek_one_cycle(max_lr=1e-2,                  # the maximum learning rate                     cycle_len=4,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.3,        # the part of cycle when learning rate increases                     logdir='train_logs') keker.plot_kek('train_logs')	 keker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate                     cycle_len=4,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.2,        # the part of cycle when learning rate increases                     logdir='train_logs1') keker.plot_kek('train_logs1')
 keker.kek_one_cycle(max_lr=1e-3,                  # the maximum learning rate                     cycle_len=4,                  # number of epochs, actually, but not exactly                     momentum_range=(0.95, 0.85),  # range of momentum changes                     div_factor=25,                # max_lr / min_lr                     increase_fraction=0.2,        # the part of cycle when learning rate increases                     logdir='train_logs1') keker.plot_kek('train_logs1')	preds = keker.predict_loader(loader=test_dl)
preds = keker.predict_loader(loader=test_dl)	"# flip_ = albumentations.HorizontalFlip(always_apply=True) # transpose_ = albumentations.Transpose(always_apply=True)  # def insert_aug(aug, dataset_key=""image"", size=224):     #     PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      #     AUGS = Transformer(dataset_key, lambda x: aug(image=x)[""image""])      #     NRM_TFMS = transforms.Compose([ #         Transformer(dataset_key, to_torch()), #         Transformer(dataset_key, normalize()) #     ])      #     tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS]) #     return tfm  # flip = insert_aug(flip_) # transpose = insert_aug(transpose_)  # tta_tfms = {""flip"": flip, ""transpose"": transpose}  # # third, run TTA # keker.TTA(loader=test_dl,                # loader to predict on  #           tfms=tta_tfms,                # list or dict of always applying transforms #           savedir=""tta_preds1"",  # savedir #           prefix=""preds"")               # (optional) name prefix. default is \'preds\'"
"# flip_ = albumentations.HorizontalFlip(always_apply=True) # transpose_ = albumentations.Transpose(always_apply=True)  # def insert_aug(aug, dataset_key=""image"", size=224):     #     PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))      #     AUGS = Transformer(dataset_key, lambda x: aug(image=x)[""image""])      #     NRM_TFMS = transforms.Compose([ #         Transformer(dataset_key, to_torch()), #         Transformer(dataset_key, normalize()) #     ])      #     tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS]) #     return tfm  # flip = insert_aug(flip_) # transpose = insert_aug(transpose_)  # tta_tfms = {""flip"": flip, ""transpose"": transpose}  # # third, run TTA # keker.TTA(loader=test_dl,                # loader to predict on  #           tfms=tta_tfms,                # list or dict of always applying transforms #           savedir=""tta_preds1"",  # savedir #           prefix=""preds"")               # (optional) name prefix. default is \'preds\'"	# prediction = np.zeros((test_df.shape[0], 1)) # for i in os.listdir('tta_preds1'): #     pr = np.load('tta_preds1/' + i) #     prediction += pr # prediction = prediction / len(os.listdir('tta_preds1'))
# prediction = np.zeros((test_df.shape[0], 1)) # for i in os.listdir('tta_preds1'): #     pr = np.load('tta_preds1/' + i) #     prediction += pr # prediction = prediction / len(os.listdir('tta_preds1'))	test_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': preds.reshape(-1,)}) test_preds.columns = ['id', 'has_cactus'] test_preds.to_csv('sub.csv', index=False) test_preds.head()
test_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': preds.reshape(-1,)}) test_preds.columns = ['id', 'has_cactus'] test_preds.to_csv('sub.csv', index=False) test_preds.head()	NB_END
import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline  from os import listdir from PIL import Image  from keras.models import Sequential, Model, load_model from keras.layers import Conv2D, Dense, Flatten, Input, BatchNormalization, Activation, Dropout from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint from keras.initializers import glorot_uniform from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array from keras.optimizers import Adam  from sklearn.model_selection import train_test_split	from keras.preprocessing.image import load_img, img_to_array from os import listdir import pandas as pd import numpy as np
from keras.preprocessing.image import load_img, img_to_array from os import listdir import pandas as pd import numpy as np	"def loadImages(path):     count = 1     images = []     imageList = sorted(listdir(path))     for i in imageList:         image = load_img(path + i)         image = img_to_array(image)         images.append(image)         if (count % 1000 == 0):             print(""Processing image"", count)         count += 1     print(""Done."")     return np.asarray(images, dtype=\'float\')     "
"def loadImages(path):     count = 1     images = []     imageList = sorted(listdir(path))     for i in imageList:         image = load_img(path + i)         image = img_to_array(image)         images.append(image)         if (count % 1000 == 0):             print(""Processing image"", count)         count += 1     print(""Done."")     return np.asarray(images, dtype=\'float\')     "	"path = ""../input/aerial-cactus-identification/train/train/"" X_train = loadImages(path)"
"path = ""../input/aerial-cactus-identification/train/train/"" X_train = loadImages(path)"	X_train.shape
X_train.shape	"path = ""../input/aerial-cactus-identification/test/test/"" X_test = loadImages(path)"
"path = ""../input/aerial-cactus-identification/test/test/"" X_test = loadImages(path)"	X_test.shape
X_test.shape	"Y_train = pd.read_csv(""../input/aerial-cactus-identification/train.csv"")"
"Y_train = pd.read_csv(""../input/aerial-cactus-identification/train.csv"")"	Y_train.head(5)
Y_train.head(5)	"Y_train = Y_train.sort_values(""id"", ascending=True).has_cactus"
"Y_train = Y_train.sort_values(""id"", ascending=True).has_cactus"	Y_train.value_counts().plot.bar()
Y_train.value_counts().plot.bar()	epochs = 3 batch_size = 32
epochs = 3 batch_size = 32	plt.figure(figsize=(8, 8)) for i in range(0, 15):     plt.subplot(5, 3, i+1)     j = np.random.randint(0, Y_train.shape[0])     plt.imshow(X_train[j][:, :, 0]) plt.tight_layout() plt.show()
plt.figure(figsize=(8, 8)) for i in range(0, 15):     plt.subplot(5, 3, i+1)     j = np.random.randint(0, Y_train.shape[0])     plt.imshow(X_train[j][:, :, 0]) plt.tight_layout() plt.show()	# Note that for errors occur when trying Inception models due to dimension errors.  from keras.applications.xception import Xception from keras.applications.vgg16 import VGG16 from keras.applications.vgg19 import VGG19 from keras.applications.mobilenet import MobileNet from keras.applications.densenet import DenseNet201 from keras.applications.nasnet import NASNetLarge
# Note that for errors occur when trying Inception models due to dimension errors.  from keras.applications.xception import Xception from keras.applications.vgg16 import VGG16 from keras.applications.vgg19 import VGG19 from keras.applications.mobilenet import MobileNet from keras.applications.densenet import DenseNet201 from keras.applications.nasnet import NASNetLarge	learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, \\                                             factor=0.7, min_lr=0.00001)
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, \\                                             factor=0.7, min_lr=0.00001)	datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.2,                               height_shift_range=0.2, zoom_range=0.2,                               horizontal_flip=True, vertical_flip=True,                               validation_split=0.1)
datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.2,                               height_shift_range=0.2, zoom_range=0.2,                               horizontal_flip=True, vertical_flip=True,                               validation_split=0.1)	train_generator = datagen.flow(X_train, Y_train, batch_size=batch_size, subset='training') val_generator = datagen.flow(X_train, Y_train, batch_size=batch_size, subset='validation')
train_generator = datagen.flow(X_train, Y_train, batch_size=batch_size, subset='training') val_generator = datagen.flow(X_train, Y_train, batch_size=batch_size, subset='validation')	def buildModel(base_model, freeze=0.8):          # Freeze 80% layers, if freeze not specified     threshold = int(len(base_model.layers) * freeze)     for i in base_model.layers[:threshold]:         i.trainable = False     for i in base_model.layers[threshold:]:         i.trainable = True      X = base_model.output     X = Flatten()(X)     X = Dense(512, activation='relu', kernel_regularizer='l2')(X)     X = Dense(1, activation='sigmoid')(X)          model = Model(inputs=base_model.input, outputs=X)          return model
def buildModel(base_model, freeze=0.8):          # Freeze 80% layers, if freeze not specified     threshold = int(len(base_model.layers) * freeze)     for i in base_model.layers[:threshold]:         i.trainable = False     for i in base_model.layers[threshold:]:         i.trainable = True      X = base_model.output     X = Flatten()(X)     X = Dense(512, activation='relu', kernel_regularizer='l2')(X)     X = Dense(1, activation='sigmoid')(X)          model = Model(inputs=base_model.input, outputs=X)          return model	def fitModel(model, lr=0.0005, cp=False):      # Compile model     model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])      # Train model     cb = [learning_rate_reduction, checkpoint] if cp else [learning_rate_reduction]     history.append(model.fit_generator(generator=train_generator, epochs=epochs,                                        steps_per_epoch=int(X_train.shape[0] // batch_size * 1.5),                                        validation_data=val_generator,                                         validation_steps=int(X_train.shape[0] // batch_size * 0.4),                                        callbacks=cb, verbose=2))     return model
def fitModel(model, lr=0.0005, cp=False):      # Compile model     model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])      # Train model     cb = [learning_rate_reduction, checkpoint] if cp else [learning_rate_reduction]     history.append(model.fit_generator(generator=train_generator, epochs=epochs,                                        steps_per_epoch=int(X_train.shape[0] // batch_size * 1.5),                                        validation_data=val_generator,                                         validation_steps=int(X_train.shape[0] // batch_size * 0.4),                                        callbacks=cb, verbose=2))     return model	# To store history of training history = []
# To store history of training history = []	"# Build model X_input = Input((32, 32, 3)) base_model = Xception(weights=""../input/pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5"",                       include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"
"# Build model X_input = Input((32, 32, 3)) base_model = Xception(weights=""../input/pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5"",                       include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"	"# Build model X_input = Input((32, 32, 3)) base_model = VGG16(weights=""../input/pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"
"# Build model X_input = Input((32, 32, 3)) base_model = VGG16(weights=""../input/pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"	"# Build model X_input = Input((32, 32, 3)) base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"
"# Build model X_input = Input((32, 32, 3)) base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"	"# # Build model # X_input = Input((32, 32, 3)) # base_model = ResNet50(weights=""../input/pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5"", #                       include_top=False, input_tensor=X_input) # model = buildModel(base_model)  # # Train model # fitModel(model)"
"# # Build model # X_input = Input((32, 32, 3)) # base_model = ResNet50(weights=""../input/pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5"", #                       include_top=False, input_tensor=X_input) # model = buildModel(base_model)  # # Train model # fitModel(model)"	"# Build model X_input = Input((32, 32, 3)) base_model = DenseNet201(weights=""../input/pretrained-models/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5"",                          include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"
"# Build model X_input = Input((32, 32, 3)) base_model = DenseNet201(weights=""../input/pretrained-models/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5"",                          include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"	"# Build model X_input = Input((32, 32, 3)) base_model = NASNetLarge(weights=""../input/pretrained-models/NASNet-large-no-top.h5"",                          include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"
"# Build model X_input = Input((32, 32, 3)) base_model = NASNetLarge(weights=""../input/pretrained-models/NASNet-large-no-top.h5"",                          include_top=False, input_tensor=X_input) model = buildModel(base_model)  # Train model fitModel(model)"	"plt.figure(figsize=(10, 7)) for i, j in enumerate(history):     plt.plot(j.history[\'val_acc\']) plt.title(\'model accuracy\') plt.ylabel(\'accuracy\') plt.xlabel(\'epoch\') plt.legend([""Xception val"", ""VGG16 val"",              ""VGG19 val"", ""DenseNet201 val"",             ""NASNetLarge val""], loc=\'lower right\') plt.show()"
"plt.figure(figsize=(10, 7)) for i, j in enumerate(history):     plt.plot(j.history[\'val_acc\']) plt.title(\'model accuracy\') plt.ylabel(\'accuracy\') plt.xlabel(\'epoch\') plt.legend([""Xception val"", ""VGG16 val"",              ""VGG19 val"", ""DenseNet201 val"",             ""NASNetLarge val""], loc=\'lower right\') plt.show()"	"plt.figure(figsize=(10, 7)) for i, j in enumerate(history):     plt.plot(j.history[\'val_loss\']) plt.title(\'model loss\') plt.ylabel(\'loss\') plt.xlabel(\'epoch\') plt.legend([""Xception val"", ""VGG16 val"",              ""VGG19 val"", ""DenseNet201 val"",             ""NASNetLarge val""], loc=\'lower right\') plt.show()"
"plt.figure(figsize=(10, 7)) for i, j in enumerate(history):     plt.plot(j.history[\'val_loss\']) plt.title(\'model loss\') plt.ylabel(\'loss\') plt.xlabel(\'epoch\') plt.legend([""Xception val"", ""VGG16 val"",              ""VGG19 val"", ""DenseNet201 val"",             ""NASNetLarge val""], loc=\'lower right\') plt.show()"	epochs = 5 history = []
epochs = 5 history = []	# Turn off learning rate decay learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=100, verbose=1, \\                                             factor=0.7, min_lr=0.00001)
# Turn off learning rate decay learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=100, verbose=1, \\                                             factor=0.7, min_lr=0.00001)	"# Build model X_input = Input((32, 32, 3)) lrs = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01] for lr in lrs:     print(""============================="")     print(""Fitting model with learning_rate ="", lr)     base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                        include_top=False, input_tensor=X_input)     model = buildModel(base_model)      # Train model     fitModel(model, lr=lr)"
"# Build model X_input = Input((32, 32, 3)) lrs = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01] for lr in lrs:     print(""============================="")     print(""Fitting model with learning_rate ="", lr)     base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                        include_top=False, input_tensor=X_input)     model = buildModel(base_model)      # Train model     fitModel(model, lr=lr)"	plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(sum(j.history['val_loss']) / 5) plt.plot(lrs, val_loss) plt.title('model loss') plt.ylabel('val loss') plt.xlabel('learning rate') plt.show()
plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(sum(j.history['val_loss']) / 5) plt.plot(lrs, val_loss) plt.title('model loss') plt.ylabel('val loss') plt.xlabel('learning rate') plt.show()	epochs = 5 history = []
epochs = 5 history = []	# Turn on learning rate decay learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, \\                                             factor=0.5, min_lr=1e-5)
# Turn on learning rate decay learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, \\                                             factor=0.5, min_lr=1e-5)	"# Build model X_input = Input((32, 32, 3)) frs = [0, 0.25, 0.5, 0.75, 1] for fr in frs:     print(""============================="")     print(""Fitting model with freeze fraction ="", fr, ""and learning_rate = 1e-4"")     base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                        include_top=False, input_tensor=X_input)     model = buildModel(base_model, freeze=fr)      # Train model     fitModel(model, lr=1e-4)"
"# Build model X_input = Input((32, 32, 3)) frs = [0, 0.25, 0.5, 0.75, 1] for fr in frs:     print(""============================="")     print(""Fitting model with freeze fraction ="", fr, ""and learning_rate = 1e-4"")     base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                        include_top=False, input_tensor=X_input)     model = buildModel(base_model, freeze=fr)      # Train model     fitModel(model, lr=1e-4)"	plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(sum(j.history['val_loss']) / 5) plt.plot(frs, val_loss) plt.title('model loss') plt.ylabel('average val loss') plt.xlabel('freeze fraction') plt.show()
plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(sum(j.history['val_loss']) / 5) plt.plot(frs, val_loss) plt.title('model loss') plt.ylabel('average val loss') plt.xlabel('freeze fraction') plt.show()	plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(min(j.history['val_loss'])) plt.plot(frs, val_loss) plt.title('model loss') plt.ylabel('min val loss') plt.xlabel('freeze fraction') plt.show()
plt.figure(figsize=(10, 7)) val_loss = [] for i, j in enumerate(history):     val_loss.append(min(j.history['val_loss'])) plt.plot(frs, val_loss) plt.title('model loss') plt.ylabel('min val loss') plt.xlabel('freeze fraction') plt.show()	def buildModel(base_model, freeze=0.8):          # Freeze 80% layers, if freeze not specified     threshold = int(len(base_model.layers) * freeze)     for i in base_model.layers[:threshold]:         i.trainable = False     for i in base_model.layers[threshold:]:         i.trainable = True          X = base_model.output     X = Flatten()(X)          X = Dense(512, use_bias=True)(X)     X = BatchNormalization()(X)     X = Activation('relu')(X)     X = Dropout(0.3)(X)          X = Dense(256, use_bias=True)(X)     X = BatchNormalization()(X)     X = Activation('relu')(X)     X = Dropout(0.3)(X)          X = Dense(1, activation='sigmoid')(X)          model = Model(inputs=base_model.input, outputs=X)          return model
def buildModel(base_model, freeze=0.8):          # Freeze 80% layers, if freeze not specified     threshold = int(len(base_model.layers) * freeze)     for i in base_model.layers[:threshold]:         i.trainable = False     for i in base_model.layers[threshold:]:         i.trainable = True          X = base_model.output     X = Flatten()(X)          X = Dense(512, use_bias=True)(X)     X = BatchNormalization()(X)     X = Activation('relu')(X)     X = Dropout(0.3)(X)          X = Dense(256, use_bias=True)(X)     X = BatchNormalization()(X)     X = Activation('relu')(X)     X = Dropout(0.3)(X)          X = Dense(1, activation='sigmoid')(X)          model = Model(inputs=base_model.input, outputs=X)          return model	"checkpoint = ModelCheckpoint(""vgg19.h5"",monitor=""val_acc"",                               verbose=1, save_best_only=True,                              mode=\'max\')"
"checkpoint = ModelCheckpoint(""vgg19.h5"",monitor=""val_acc"",                               verbose=1, save_best_only=True,                              mode=\'max\')"	epochs = 30
epochs = 30	"# Build model X_input = Input((32, 32, 3)) base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model, freeze=0.25) # Train model model = fitModel(model, lr=1e-4, cp=True)"
"# Build model X_input = Input((32, 32, 3)) base_model = VGG19(weights=""../input/pretrained-models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"",                    include_top=False, input_tensor=X_input) model = buildModel(base_model, freeze=0.25) # Train model model = fitModel(model, lr=1e-4, cp=True)"	"submission = test_images = pd.read_csv(""../input/aerial-cactus-identification/sample_submission.csv"") submission = pd.DataFrame(test_images.iloc[:, 0], columns=[""id""])"
"submission = test_images = pd.read_csv(""../input/aerial-cactus-identification/sample_submission.csv"") submission = pd.DataFrame(test_images.iloc[:, 0], columns=[""id""])"	pred = model.predict(X_test).reshape(-1)
pred = model.predict(X_test).reshape(-1)	"submission[""has_cactus""] = pd.Series(pred, index=None)"
"submission[""has_cactus""] = pd.Series(pred, index=None)"	"submission.to_csv(""submission.csv"", index=False)"
"submission.to_csv(""submission.csv"", index=False)"	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	from keras.layers import Flatten, Conv2D, MaxPooling2D, Dropout, MaxPool2D, Activation from keras.layers.normalization import BatchNormalization from keras.layers import Dense from keras.models import Model, Sequential, load_model from keras import applications from keras.utils.np_utils import to_categorical from keras.optimizers import RMSprop, Adam, SGD from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping from keras import backend as K
from keras.layers import Flatten, Conv2D, MaxPooling2D, Dropout, MaxPool2D, Activation from keras.layers.normalization import BatchNormalization from keras.layers import Dense from keras.models import Model, Sequential, load_model from keras import applications from keras.utils.np_utils import to_categorical from keras.optimizers import RMSprop, Adam, SGD from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping from keras import backend as K	train = pd.read_csv('../input/train.csv') train.head()
train = pd.read_csv('../input/train.csv') train.head()	train['has_cactus'].value_counts()
train['has_cactus'].value_counts()	import matplotlib.pyplot as plt import tqdm  img = plt.imread('../input/train/train/'+ train['id'][0]) img.shape
import matplotlib.pyplot as plt import tqdm  img = plt.imread('../input/train/train/'+ train['id'][0]) img.shape	from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_auc_score  x_train, x_test, y_train, y_test = train_test_split(train['id'], train['has_cactus'], test_size = 0.1, random_state = 32)  print(x_train.shape) print(x_test.shape) print(y_train.shape) print(y_test.shape)
from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_auc_score  x_train, x_test, y_train, y_test = train_test_split(train['id'], train['has_cactus'], test_size = 0.1, random_state = 32)  print(x_train.shape) print(x_test.shape) print(y_train.shape) print(y_test.shape)	x_train_arr = [] for images in tqdm.tqdm(x_train):     img = plt.imread('../input/train/train/' + images)     x_train_arr.append(img)  x_train_arr = np.array(x_train_arr) print(x_train_arr.shape)
x_train_arr = [] for images in tqdm.tqdm(x_train):     img = plt.imread('../input/train/train/' + images)     x_train_arr.append(img)  x_train_arr = np.array(x_train_arr) print(x_train_arr.shape)	x_test_arr = [] for images in tqdm.tqdm(x_test):     img = plt.imread('../input/train/train/' + images)     x_test_arr.append(img)  x_test_arr = np.array(x_test_arr) print(x_test_arr.shape)
x_test_arr = [] for images in tqdm.tqdm(x_test):     img = plt.imread('../input/train/train/' + images)     x_test_arr.append(img)  x_test_arr = np.array(x_test_arr) print(x_test_arr.shape)	x_train_arr = x_train_arr.astype('float32') x_test_arr = x_test_arr.astype('float32') x_train_arr = x_train_arr/255 x_test_arr = x_test_arr/255
x_train_arr = x_train_arr.astype('float32') x_test_arr = x_test_arr.astype('float32') x_train_arr = x_train_arr/255 x_test_arr = x_test_arr/255	print(x_train_arr.shape) print(x_test_arr.shape) print(y_train.shape) print(y_test.shape)
print(x_train_arr.shape) print(x_test_arr.shape) print(y_train.shape) print(y_test.shape)	"from keras.applications.densenet import DenseNet201  conv_base = DenseNet201(     weights=\'imagenet\',     include_top=False,     input_shape=(32,32,3) )         model = Sequential() model.add(conv_base) # model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = \'Same\', activation =\'relu\')) # model.add(BatchNormalization()) # model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(256, use_bias=True)) model.add(BatchNormalization()) model.add(Activation(""relu"")) model.add(Dropout(0.5)) # model.add(Dense(128, use_bias=True)) # model.add(BatchNormalization()) # model.add(Activation(""relu"")) # model.add(Dropout(0.3)) model.add(Dense(1, activation = ""sigmoid""))"
"from keras.applications.densenet import DenseNet201  conv_base = DenseNet201(     weights=\'imagenet\',     include_top=False,     input_shape=(32,32,3) )         model = Sequential() model.add(conv_base) # model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = \'Same\', activation =\'relu\')) # model.add(BatchNormalization()) # model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(256, use_bias=True)) model.add(BatchNormalization()) model.add(Activation(""relu"")) model.add(Dropout(0.5)) # model.add(Dense(128, use_bias=True)) # model.add(BatchNormalization()) # model.add(Activation(""relu"")) # model.add(Dropout(0.3)) model.add(Dense(1, activation = ""sigmoid""))"	conv_base.Trainable=True  set_trainable=False for layer in conv_base.layers:     layer.trainable = True
conv_base.Trainable=True  set_trainable=False for layer in conv_base.layers:     layer.trainable = True	model.summary()
model.summary()	"model.compile(\'rmsprop\', loss = ""binary_crossentropy"", metrics=[""accuracy""])"
"model.compile(\'rmsprop\', loss = ""binary_crossentropy"", metrics=[""accuracy""])"	"from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import ModelCheckpoint  batch_size = 128 epochs = 30  filepath=""weights_resnet.hdf5""  checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.3, patience=3, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  train_datagen = ImageDataGenerator(     rotation_range=20,     vertical_flip=True,     horizontal_flip=True)  train_datagen.fit(x_train_arr)   history = model.fit_generator(     train_datagen.flow(x_train_arr, y_train, batch_size=batch_size),     steps_per_epoch=x_train.shape[0] // batch_size,     epochs=epochs,     validation_data=(x_test_arr, y_test),     callbacks=[learning_rate_reduce, checkpoint]  )"
"from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import ModelCheckpoint  batch_size = 128 epochs = 30  filepath=""weights_resnet.hdf5""  checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.3, patience=3, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  train_datagen = ImageDataGenerator(     rotation_range=20,     vertical_flip=True,     horizontal_flip=True)  train_datagen.fit(x_train_arr)   history = model.fit_generator(     train_datagen.flow(x_train_arr, y_train, batch_size=batch_size),     steps_per_epoch=x_train.shape[0] // batch_size,     epochs=epochs,     validation_data=(x_test_arr, y_test),     callbacks=[learning_rate_reduce, checkpoint]  )"	"# filepath=""weights_resnet.hdf5"" # checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') # learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.3, patience=3, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) # early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  # callbacks_list = [learning_rate_reduce, checkpoint, early_stop] # # history = model.fit(x_train_arr, y_train, epochs= 30, batch_size = 128, validation_data = (x_test_arr, y_test), callbacks = callbacks_list)  # history = model.fit(np.concatenate((x_train_arr, x_test_arr)), np.concatenate((y_train, y_test)), epochs= 15, batch_size = 128, validation_data = (x_test_arr, y_test), callbacks = callbacks_list) "
"# filepath=""weights_resnet.hdf5"" # checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') # learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.3, patience=3, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) # early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  # callbacks_list = [learning_rate_reduce, checkpoint, early_stop] # # history = model.fit(x_train_arr, y_train, epochs= 30, batch_size = 128, validation_data = (x_test_arr, y_test), callbacks = callbacks_list)  # history = model.fit(np.concatenate((x_train_arr, x_test_arr)), np.concatenate((y_train, y_test)), epochs= 15, batch_size = 128, validation_data = (x_test_arr, y_test), callbacks = callbacks_list) "	train_pred = model.predict(x_train_arr, verbose= 1) valid_pred = model.predict(x_test_arr, verbose= 1)  train_acc = roc_auc_score(np.round(train_pred), y_train) valid_acc = roc_auc_score(np.round(valid_pred), y_test)
train_pred = model.predict(x_train_arr, verbose= 1) valid_pred = model.predict(x_test_arr, verbose= 1)  train_acc = roc_auc_score(np.round(train_pred), y_train) valid_acc = roc_auc_score(np.round(valid_pred), y_test)	confusion_matrix(np.round(valid_pred), y_test)
confusion_matrix(np.round(valid_pred), y_test)	print('Train score: '+ str(train_acc)) print('Valid score: '+ str(valid_acc))
print('Train score: '+ str(train_acc)) print('Valid score: '+ str(valid_acc))	sample = pd.read_csv('../input/sample_submission.csv') sample.head()
sample = pd.read_csv('../input/sample_submission.csv') sample.head()	test = [] for images in tqdm.tqdm(sample['id']):     img = plt.imread('../input/test/test/' + images)     test.append(img)  test = np.array(test) print(test.shape)
test = [] for images in tqdm.tqdm(sample['id']):     img = plt.imread('../input/test/test/' + images)     test.append(img)  test = np.array(test) print(test.shape)	test = test/255
test = test/255	test_pred = model.predict(test, verbose= 1)
test_pred = model.predict(test, verbose= 1)	sample['has_cactus'] = test_pred sample.head()
sample['has_cactus'] = test_pred sample.head()	sample.to_csv('sub.csv', index= False) ## 11.29 - 
sample.to_csv('sub.csv', index= False) ## 11.29 - 	NB_END
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input"")) # Any results you write to the current directory are saved as output."	from PIL import Image from tqdm import tqdm  from keras.preprocessing.image import ImageDataGenerator  def load_data(dataframe=None, batch_size=16, mode='categorical'):     if dataframe is None:         dataframe = pd.read_csv('../input/train.csv')     dataframe['has_cactus'] = dataframe['has_cactus'].apply(str)     gen = ImageDataGenerator(rescale=1./255., validation_split=0.1, horizontal_flip=True, vertical_flip=True)      trainGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train', x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='training')     testGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train', x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          return trainGen, testGen
from PIL import Image from tqdm import tqdm  from keras.preprocessing.image import ImageDataGenerator  def load_data(dataframe=None, batch_size=16, mode='categorical'):     if dataframe is None:         dataframe = pd.read_csv('../input/train.csv')     dataframe['has_cactus'] = dataframe['has_cactus'].apply(str)     gen = ImageDataGenerator(rescale=1./255., validation_split=0.1, horizontal_flip=True, vertical_flip=True)      trainGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train', x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='training')     testGen = gen.flow_from_dataframe(dataframe, directory='../input/train/train', x_col='id', y_col='has_cactus', has_ext=True, target_size=(32, 32),         class_mode=mode, batch_size=batch_size, shuffle=True, subset='validation')          return trainGen, testGen	from keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Activation, GlobalAveragePooling2D from keras.models import Sequential, Model from keras.regularizers import l2  def baseline_model():     model = Sequential()      model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(GlobalAveragePooling2D())     model.add(Dense(2, activation='softmax'))      return model
from keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Activation, GlobalAveragePooling2D from keras.models import Sequential, Model from keras.regularizers import l2  def baseline_model():     model = Sequential()      model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=l2(1e-4)))     model.add(BatchNormalization())     model.add(Activation('relu'))     model.add(MaxPool2D())      model.add(GlobalAveragePooling2D())     model.add(Dense(2, activation='softmax'))      return model	from keras.optimizers import Adam, SGD from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau   def train_baseline():     batch_size = 32     trainGen, valGen = load_data(batch_size=batch_size)     model = baseline_model()      opt = Adam(1e-3)     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])     cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-5, verbose=1)]      model.fit_generator(trainGen, steps_per_epoch=4922, epochs=3, validation_data=valGen,          validation_steps=493, shuffle=True, callbacks=cbs)              return model      def predict_baseline(model):     testdf = pd.read_csv('../input/sample_submission.csv')     pred = np.empty((testdf.shape[0],))     for n in tqdm(range(testdf.shape[0])):         data = np.array(Image.open('../input/test/test/'+testdf.id[n]))         data = data.astype(np.float32) / 255.         pred[n] = model.predict(data.reshape((1, 32, 32, 3)))[0][1]          testdf['has_cactus'] = pred     testdf.to_csv('sample_submission.csv', index=False)
from keras.optimizers import Adam, SGD from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau   def train_baseline():     batch_size = 32     trainGen, valGen = load_data(batch_size=batch_size)     model = baseline_model()      opt = Adam(1e-3)     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])     cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-5, verbose=1)]      model.fit_generator(trainGen, steps_per_epoch=4922, epochs=3, validation_data=valGen,          validation_steps=493, shuffle=True, callbacks=cbs)              return model      def predict_baseline(model):     testdf = pd.read_csv('../input/sample_submission.csv')     pred = np.empty((testdf.shape[0],))     for n in tqdm(range(testdf.shape[0])):         data = np.array(Image.open('../input/test/test/'+testdf.id[n]))         data = data.astype(np.float32) / 255.         pred[n] = model.predict(data.reshape((1, 32, 32, 3)))[0][1]          testdf['has_cactus'] = pred     testdf.to_csv('sample_submission.csv', index=False)	model = train_baseline() predict_baseline(model)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.image as mpimg import glob import scipy import cv2  import keras
 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.image as mpimg import glob import scipy import cv2  import keras	 import random
 import random	 train_data = pd.read_csv('../input/train.csv')
 train_data = pd.read_csv('../input/train.csv')	 train_data.shape
 train_data.shape	 train_data.head()
 train_data.head()	 train_data.has_cactus.unique()
 train_data.has_cactus.unique()	 train_data.has_cactus.hist()
 train_data.has_cactus.hist()	 train_data.has_cactus.value_counts()
 train_data.has_cactus.value_counts()	 train_data.has_cactus.plot()
 train_data.has_cactus.plot()	 def image_generator(batch_size = 16, all_data=True, shuffle=True, train=True, indexes=None):     while True:         if indexes is None:             if train:                 if all_data:                     indexes = np.arange(train_data.shape[0])                 else:                     indexes = np.arange(train_data[:15000].shape[0])                 if shuffle:                     np.random.shuffle(indexes)             else:                 indexes = np.arange(train_data[15000:].shape[0])                      N = int(len(indexes) / batch_size)                  # Read in each input, perform preprocessing and get labels         for i in range(N):             current_indexes = indexes[i*batch_size: (i+1)*batch_size]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img]                 batch_input += [img[::-1, :, :]]                 batch_input += [img[:, ::-1, :]]                 batch_input += [np.rot90(img)]                                  temp_img = np.zeros_like(img)                 temp_img[:28, :, :] = img[4:, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, :28, :] = img[:, 4:, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[4:, :, :] = img[:28, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, 4:, :] = img[:, :28, :]                 batch_input += [temp_img]                                  batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]                                  for _ in range(11):                     batch_output += [train_data.has_cactus[index]]                              batch_input = np.array( batch_input )             batch_output = np.array( batch_output )                      yield( batch_input, batch_output.reshape(-1, 1) )
model = keras.models.Sequential() model.add(keras.layers.Conv2D(64, (5, 5), input_shape=(32, 32, 3))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(64, (5, 5))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(128, (5, 5))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(128, (5, 5))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(256, (3, 3))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(256, (3, 3))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Conv2D(512, (3, 3))) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Flatten())   model.add(keras.layers.Dense(100)) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.LeakyReLU(alpha=0.3))  model.add(keras.layers.Dense(1, activation='sigmoid'))	 model.summary()
 model.summary()	 opt = keras.optimizers.Adam(0.0001) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
 opt = keras.optimizers.Adam(0.0001) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])	 model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] / 16, epochs=30)
 model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] / 16, epochs=30)	 keras.backend.eval(model.optimizer.lr.assign(0.00001))
 keras.backend.eval(model.optimizer.lr.assign(0.00001))	 model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] / 16, epochs=15)
 indexes = np.arange(train_data.shape[0]) N = int(len(indexes) / 64)    batch_size = 64  wrong_ind = [] for i in range(N):             current_indexes = indexes[i*64: (i+1)*64]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img]                 batch_output.append(train_data.has_cactus[index])                          batch_input = np.array( batch_input ) #             batch_output = np.array( batch_output )              model_pred = model.predict_classes(batch_input)             for j in range(len(batch_output)):                 if model_pred[j] != batch_output[j]:                     wrong_ind.append(i*batch_size+j)	 len(wrong_ind)
 len(wrong_ind)	 indexes = np.arange(train_data.shape[0]) N = int(len(indexes) / 64)    batch_size = 64  wrong_ind = [] for i in range(N):             current_indexes = indexes[i*64: (i+1)*64]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img[::-1, :, :]]                 batch_output.append(train_data.has_cactus[index])                          batch_input = np.array( batch_input )              model_pred = model.predict_classes(batch_input)             for j in range(len(batch_output)):                 if model_pred[j] != batch_output[j]:                     wrong_ind.append(i*batch_size+j)
 indexes = np.arange(train_data.shape[0]) N = int(len(indexes) / 64)    batch_size = 64  wrong_ind = [] for i in range(N):             current_indexes = indexes[i*64: (i+1)*64]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img[::-1, :, :]]                 batch_output.append(train_data.has_cactus[index])                          batch_input = np.array( batch_input )              model_pred = model.predict_classes(batch_input)             for j in range(len(batch_output)):                 if model_pred[j] != batch_output[j]:                     wrong_ind.append(i*batch_size+j)	 wrong_ind
 wrong_ind	 indexes = np.arange(train_data.shape[0]) N = int(len(indexes) / 64)    batch_size = 64  wrong_ind = [] for i in range(N):             current_indexes = indexes[i*64: (i+1)*64]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img[:, ::-1, :]]                 batch_output.append(train_data.has_cactus[index])                          batch_input = np.array( batch_input )              model_pred = model.predict_classes(batch_input)             for j in range(len(batch_output)):                 if model_pred[j] != batch_output[j]:                     wrong_ind.append(i*batch_size+j)
 indexes = np.arange(train_data.shape[0]) N = int(len(indexes) / 64)    batch_size = 64  wrong_ind = [] for i in range(N):             current_indexes = indexes[i*64: (i+1)*64]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img[:, ::-1, :]]                 batch_output.append(train_data.has_cactus[index])                          batch_input = np.array( batch_input )              model_pred = model.predict_classes(batch_input)             for j in range(len(batch_output)):                 if model_pred[j] != batch_output[j]:                     wrong_ind.append(i*batch_size+j)	 wrong_ind
 !ls ../input/test/test/* | wc -l	 test_files = os.listdir('../input/test/test/')
 test_files = os.listdir('../input/test/test/')	 len(test_files)
 len(test_files)	 batch = 40 all_out = [] for i in range(int(4000/batch)):     images = []     for j in range(batch):         img = mpimg.imread('../input/test/test/'+test_files[i*batch + j])         images += [img]     out = model.predict(np.array(images))     all_out += [out]
 all_out = np.array(all_out).reshape((-1, 1)) 	 all_out.shape
 all_out.shape	 sub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})
 sub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})	 sub_file.to_csv('sample_submission.csv', index=False)
" import os print(os.listdir(""../input""))"	 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.image as mpimg import glob import scipy import cv2 import random from sklearn.model_selection import KFold  import keras
 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.image as mpimg import glob import scipy import cv2 import random from sklearn.model_selection import KFold  import keras	 train_data = pd.read_csv('../input/train.csv')
 train_data = pd.read_csv('../input/train.csv')	 positive_examples_indexes = train_data[train_data.has_cactus==1].index negative_examples_indexes = train_data[train_data.has_cactus==0].index
 positive_examples_indexes = train_data[train_data.has_cactus==1].index negative_examples_indexes = train_data[train_data.has_cactus==0].index	 pos_indexes = positive_examples_indexes.tolist()[:] random.shuffle(pos_indexes)
 pos_indexes = positive_examples_indexes.tolist()[:] random.shuffle(pos_indexes)	 k = 5 third = int(len(pos_indexes) / k) folds_indexes = [] for i in range(k):     start = i*third     end = (i+1)*third     if i == k-1:         end = len(pos_indexes)     folds_indexes.append(pos_indexes[start:end])
 k = 5 third = int(len(pos_indexes) / k) folds_indexes = [] for i in range(k):     start = i*third     end = (i+1)*third     if i == k-1:         end = len(pos_indexes)     folds_indexes.append(pos_indexes[start:end])	 def image_generator(indexes=None, batch_size = 16, shuffle=True, train=True):     while True:         if train:             temp_indexes = indexes[:]             temp_indexes.extend(negative_examples_indexes.tolist())             random.shuffle(temp_indexes)             random.shuffle(temp_indexes)         else:             temp_indexes = indexes[:]                      N = int(len(temp_indexes) / batch_size)                  # Read in each input, perform preprocessing and get labels         for i in range(N):             current_indexes = temp_indexes[i*batch_size: (i+1)*batch_size]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img]                 batch_input += [img[::-1, :, :]]                 batch_input += [img[:, ::-1, :]]                 batch_input += [np.rot90(img)]                                  temp_img = np.zeros_like(img)                 temp_img[:28, :, :] = img[4:, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, :28, :] = img[:, 4:, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[4:, :, :] = img[:28, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, 4:, :] = img[:, :28, :]                 batch_input += [temp_img]                                  batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]                                  for _ in range(11):                     batch_output += [train_data.has_cactus[index]]                              batch_input = np.array( batch_input )             batch_output = np.array( batch_output )                      yield( batch_input, batch_output.reshape(-1, 1) )
 def image_generator(indexes=None, batch_size = 16, shuffle=True, train=True):     while True:         if train:             temp_indexes = indexes[:]             temp_indexes.extend(negative_examples_indexes.tolist())             random.shuffle(temp_indexes)             random.shuffle(temp_indexes)         else:             temp_indexes = indexes[:]                      N = int(len(temp_indexes) / batch_size)                  # Read in each input, perform preprocessing and get labels         for i in range(N):             current_indexes = temp_indexes[i*batch_size: (i+1)*batch_size]             batch_input = []             batch_output = []              for index in current_indexes:                 img = mpimg.imread('../input/train/train/' + train_data.id[index])                 batch_input += [img]                 batch_input += [img[::-1, :, :]]                 batch_input += [img[:, ::-1, :]]                 batch_input += [np.rot90(img)]                                  temp_img = np.zeros_like(img)                 temp_img[:28, :, :] = img[4:, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, :28, :] = img[:, 4:, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[4:, :, :] = img[:28, :, :]                 batch_input += [temp_img]                                  temp_img = np.zeros_like(img)                 temp_img[:, 4:, :] = img[:, :28, :]                 batch_input += [temp_img]                                  batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]                                  batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]                                  for _ in range(11):                     batch_output += [train_data.has_cactus[index]]                              batch_input = np.array( batch_input )             batch_output = np.array( batch_output )                      yield( batch_input, batch_output.reshape(-1, 1) )	 def build_model():     model = keras.models.Sequential()     model.add(keras.layers.Conv2D(64, (3, 3), input_shape=(32, 32, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(64, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(128, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(128, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(256, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     # model.add(keras.layers.Conv2D(128, (3, 3)))     # model.add(keras.layers.BatchNormalization())     # # model.add(keras.layers.Activation('relu'))     # model.add(keras.layers.LeakyReLU(alpha=0.3))     # model.add(keras.layers.Conv2D(256, (3, 3)))     # model.add(keras.layers.BatchNormalization())     # # model.add(keras.layers.Activation('relu'))     # model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(256, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(512, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Flatten())     # model.add(keras.layers.Dense(512, activation='relu'))     model.add(keras.layers.Dense(100))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Dense(1, activation='sigmoid'))      opt = keras.optimizers.Adam(0.0001)     model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])          return model
 def build_model():     model = keras.models.Sequential()     model.add(keras.layers.Conv2D(64, (3, 3), input_shape=(32, 32, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(64, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(128, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(128, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(256, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     # model.add(keras.layers.Conv2D(128, (3, 3)))     # model.add(keras.layers.BatchNormalization())     # # model.add(keras.layers.Activation('relu'))     # model.add(keras.layers.LeakyReLU(alpha=0.3))     # model.add(keras.layers.Conv2D(256, (3, 3)))     # model.add(keras.layers.BatchNormalization())     # # model.add(keras.layers.Activation('relu'))     # model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(256, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Conv2D(512, (3, 3)))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Flatten())     # model.add(keras.layers.Dense(512, activation='relu'))     model.add(keras.layers.Dense(100))     model.add(keras.layers.BatchNormalization())     # model.add(keras.layers.Activation('relu'))     model.add(keras.layers.LeakyReLU(alpha=0.3))     model.add(keras.layers.Dense(1, activation='sigmoid'))      opt = keras.optimizers.Adam(0.0001)     model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])          return model	 k
 k	 models = [] for i in range(k):     print('i =', i)     model = build_model()     for j in range(k):         if i == j:             continue         current_pos_indexes = folds_indexes[i]         model.fit_generator(image_generator(current_pos_indexes), steps_per_epoch= len(current_pos_indexes) / 16, epochs=20)     models.append(model)
 models = [] for i in range(k):     print('i =', i)     model = build_model()     for j in range(k):         if i == j:             continue         current_pos_indexes = folds_indexes[i]         model.fit_generator(image_generator(current_pos_indexes), steps_per_epoch= len(current_pos_indexes) / 16, epochs=20)     models.append(model)	 for model in models:     print(model.evaluate_generator(image_generator(indexes=train_data[15000:].index.tolist()), steps=train_data[15000:].shape[0] / 16))
 for model in models:     print(model.evaluate_generator(image_generator(indexes=train_data[15000:].index.tolist()), steps=train_data[15000:].shape[0] / 16))	 for model in models:     for j in range(k):         current_pos_indexes = folds_indexes[j]         print(model.evaluate_generator(image_generator(indexes=current_pos_indexes), steps=len(current_pos_indexes) / 16))
 for model in models:     for j in range(k):         current_pos_indexes = folds_indexes[j]         print(model.evaluate_generator(image_generator(indexes=current_pos_indexes), steps=len(current_pos_indexes) / 16))	 test_files = os.listdir('../input/test/test/')
 test_files = os.listdir('../input/test/test/')	 preds = [] for _ in range(len(models)):     preds.append([]) batch = 40 # all_out = [] for i in range(int(4000/batch)):     images = []     for j in range(batch):         img = mpimg.imread('../input/test/test/'+test_files[i*batch + j])         images += [img]     for k2 in range(len(models)):         model = models[k2]         out = model.predict(np.array(images))         preds[k2] += [out] #     all_out += [out]
 preds = [] for _ in range(len(models)):     preds.append([]) batch = 40 # all_out = [] for i in range(int(4000/batch)):     images = []     for j in range(batch):         img = mpimg.imread('../input/test/test/'+test_files[i*batch + j])         images += [img]     for k2 in range(len(models)):         model = models[k2]         out = model.predict(np.array(images))         preds[k2] += [out] #     all_out += [out]	 all_out = np.array(list(map(lambda x: np.array(x).reshape(-1, 1), preds)))
 all_out = np.array(list(map(lambda x: np.array(x).reshape(-1, 1), preds)))	 all_out.shape
 all_out.shape	 all_out[:, :10, :]
 all_out[:, :10, :]	 all_out = np.mean(all_out, axis=0)
 all_out = np.mean(all_out, axis=0)	 all_out.shape
 all_out.shape	 sub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})
 sub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})	 sub_file.head()
 sub_file.head()	 sub_file.to_csv('sample_submission.csv', index=False)
"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here\'s several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."	from keras.layers import * from keras.models import Model, Sequential, load_model from keras import applications from keras.utils.np_utils import to_categorical from keras.optimizers import RMSprop, Adam, SGD from keras.losses import sparse_categorical_crossentropy, binary_crossentropy from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping from keras import backend as K
from keras.layers import * from keras.models import Model, Sequential, load_model from keras import applications from keras.utils.np_utils import to_categorical from keras.optimizers import RMSprop, Adam, SGD from keras.losses import sparse_categorical_crossentropy, binary_crossentropy from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping from keras import backend as K	train = pd.read_csv('../input/train.csv') train.head()
train = pd.read_csv('../input/train.csv') train.head()	train['has_cactus'].value_counts()
train['has_cactus'].value_counts()	import matplotlib.pyplot as plt import tqdm  img = plt.imread('../input/train/train/'+ train['id'][0]) img.shape
import matplotlib.pyplot as plt import tqdm  img = plt.imread('../input/train/train/'+ train['id'][0]) img.shape	from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_auc_score  x_train, x_test, y_train, y_test = train_test_split(train['id'], train['has_cactus'], test_size = 0.1, random_state = 32)
from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix, roc_auc_score  x_train, x_test, y_train, y_test = train_test_split(train['id'], train['has_cactus'], test_size = 0.1, random_state = 32)	x_train_arr = [] for images in tqdm.tqdm(x_train):     img = plt.imread('../input/train/train/' + images)     x_train_arr.append(img)  x_train_arr = np.array(x_train_arr) print(x_train_arr.shape)
x_train_arr = [] for images in tqdm.tqdm(x_train):     img = plt.imread('../input/train/train/' + images)     x_train_arr.append(img)  x_train_arr = np.array(x_train_arr) print(x_train_arr.shape)	x_test_arr = [] for images in tqdm.tqdm(x_test):     img = plt.imread('../input/train/train/' + images)     x_test_arr.append(img)  x_test_arr = np.array(x_test_arr) print(x_test_arr.shape)
x_test_arr = [] for images in tqdm.tqdm(x_test):     img = plt.imread('../input/train/train/' + images)     x_test_arr.append(img)  x_test_arr = np.array(x_test_arr) print(x_test_arr.shape)	x_train_arr = x_train_arr.astype('float32') x_test_arr = x_test_arr.astype('float32') x_train_arr = x_train_arr/255 x_test_arr = x_test_arr/255
x_train_arr = x_train_arr.astype('float32') x_test_arr = x_test_arr.astype('float32') x_train_arr = x_train_arr/255 x_test_arr = x_test_arr/255	"from keras.applications.densenet import DenseNet201 from keras.layers import *  inputs = Input((32, 32, 3)) base_model = DenseNet201(include_top=False, input_shape=(32, 32, 3))#, weights=None x = base_model(inputs) out1 = GlobalMaxPooling2D()(x) out2 = GlobalAveragePooling2D()(x) out3 = Flatten()(x) out = Concatenate(axis=-1)([out1, out2, out3]) out = Dropout(0.5)(out) out = Dense(256, name=""3_"")(out) out = BatchNormalization()(out) out = Activation(""relu"")(out) out = Dense(1, activation=""sigmoid"", name=""3_2"")(out) model = Model(inputs, out) model.summary()"
"from keras.applications.densenet import DenseNet201 from keras.layers import *  inputs = Input((32, 32, 3)) base_model = DenseNet201(include_top=False, input_shape=(32, 32, 3))#, weights=None x = base_model(inputs) out1 = GlobalMaxPooling2D()(x) out2 = GlobalAveragePooling2D()(x) out3 = Flatten()(x) out = Concatenate(axis=-1)([out1, out2, out3]) out = Dropout(0.5)(out) out = Dense(256, name=""3_"")(out) out = BatchNormalization()(out) out = Activation(""relu"")(out) out = Dense(1, activation=""sigmoid"", name=""3_2"")(out) model = Model(inputs, out) model.summary()"	base_model.Trainable=True  set_trainable=False for layer in base_model.layers:     layer.trainable = True
base_model.Trainable=True  set_trainable=False for layer in base_model.layers:     layer.trainable = True	"model.compile(\'rmsprop\', loss = ""binary_crossentropy"", metrics=[""accuracy""])"
"model.compile(\'rmsprop\', loss = ""binary_crossentropy"", metrics=[""accuracy""])"	"from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import ModelCheckpoint  batch_size = 128 epochs = 36  filepath=""weights_resnet.hdf5""  checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.6, patience=4, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  train_datagen = ImageDataGenerator(     rotation_range=40,     zoom_range=0.1,     vertical_flip=True,     horizontal_flip=True)  train_datagen.fit(x_train_arr) history = model.fit_generator(     train_datagen.flow(x_train_arr, y_train, batch_size=batch_size),     steps_per_epoch=x_train.shape[0] // batch_size,     epochs=epochs,     validation_data=(x_test_arr, y_test),     callbacks=[learning_rate_reduce, checkpoint]  )"
"from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import ModelCheckpoint  batch_size = 128 epochs = 36  filepath=""weights_resnet.hdf5""  checkpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1, save_best_only=True, mode=\'max\') learning_rate_reduce = ReduceLROnPlateau(monitor=\'val_loss\', factor=0.6, patience=4, verbose=1, mode=\'max\', min_delta=0.0, cooldown=0, min_lr=0) early_stop = EarlyStopping(monitor=\'val_acc\', min_delta=0.0001, patience=25, verbose=1, mode=\'max\', baseline=None, restore_best_weights=True)  train_datagen = ImageDataGenerator(     rotation_range=40,     zoom_range=0.1,     vertical_flip=True,     horizontal_flip=True)  train_datagen.fit(x_train_arr) history = model.fit_generator(     train_datagen.flow(x_train_arr, y_train, batch_size=batch_size),     steps_per_epoch=x_train.shape[0] // batch_size,     epochs=epochs,     validation_data=(x_test_arr, y_test),     callbacks=[learning_rate_reduce, checkpoint]  )"	train_pred = model.predict(x_train_arr, verbose= 1) valid_pred = model.predict(x_test_arr, verbose= 1)  train_acc = roc_auc_score(np.round(train_pred), y_train) valid_acc = roc_auc_score(np.round(valid_pred), y_test)
train_pred = model.predict(x_train_arr, verbose= 1) valid_pred = model.predict(x_test_arr, verbose= 1)  train_acc = roc_auc_score(np.round(train_pred), y_train) valid_acc = roc_auc_score(np.round(valid_pred), y_test)	confusion_matrix(np.round(valid_pred), y_test)
confusion_matrix(np.round(valid_pred), y_test)	sample = pd.read_csv('../input/sample_submission.csv')
sample = pd.read_csv('../input/sample_submission.csv')	test = [] for images in tqdm.tqdm(sample['id']):     img = plt.imread('../input/test/test/' + images)     test.append(img)  test = np.array(test)
test = [] for images in tqdm.tqdm(sample['id']):     img = plt.imread('../input/test/test/' + images)     test.append(img)  test = np.array(test)	test = test/255 test_pred = model.predict(test, verbose= 1)
test = test/255 test_pred = model.predict(test, verbose= 1)	sample['has_cactus'] = test_pred sample.head()
sample['has_cactus'] = test_pred sample.head()	sample.to_csv('sub.csv', index= False)
sample.to_csv('sub.csv', index= False)	NB_END
import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)	from pathlib import Path from fastai import * from fastai.vision import * import torch
from pathlib import Path from fastai import * from fastai.vision import * import torch	"data_folder = Path(""../input"") #data_folder.ls()"
"data_folder = Path(""../input"") #data_folder.ls()"	"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/sample_submission.csv"")"
"train_df = pd.read_csv(""../input/train.csv"") test_df = pd.read_csv(""../input/sample_submission.csv"")"	test_img = ImageList.from_df(test_df, path=data_folder/'test', folder='test') trfm = get_transforms(do_flip=True, flip_vert=True, max_rotate=10.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75) train_img = (ImageList.from_df(train_df, path=data_folder/'train', folder='train')         .split_by_rand_pct(0.01)         .label_from_df()         .add_test(test_img)         .transform(trfm, size=128)         .databunch(path='.', bs=64, device= torch.device('cuda:0'))         .normalize(imagenet_stats)        )
test_img = ImageList.from_df(test_df, path=data_folder/'test', folder='test') trfm = get_transforms(do_flip=True, flip_vert=True, max_rotate=10.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75) train_img = (ImageList.from_df(train_df, path=data_folder/'train', folder='train')         .split_by_rand_pct(0.01)         .label_from_df()         .add_test(test_img)         .transform(trfm, size=128)         .databunch(path='.', bs=64, device= torch.device('cuda:0'))         .normalize(imagenet_stats)        )	#train_img.show_batch(rows=3, figsize=(7,6))
#train_img.show_batch(rows=3, figsize=(7,6))	learn = cnn_learner(train_img, models.densenet161, metrics=[error_rate, accuracy])
learn = cnn_learner(train_img, models.densenet161, metrics=[error_rate, accuracy])	#learn.lr_find() #learn.recorder.plot()
#learn.lr_find() #learn.recorder.plot()	lr = 3e-02 learn.fit_one_cycle(5, slice(lr))
lr = 3e-02 learn.fit_one_cycle(5, slice(lr))	#learn.unfreeze() #learn.lr_find() #learn.recorder.plot()
#learn.unfreeze() #learn.lr_find() #learn.recorder.plot()	#learn.fit_one_cycle(1, slice(1e-06))
#learn.fit_one_cycle(1, slice(1e-06))	#interp = ClassificationInterpretation.from_learner(learn) #interp.plot_top_losses(9, figsize=(7,6))
#interp = ClassificationInterpretation.from_learner(learn) #interp.plot_top_losses(9, figsize=(7,6))	preds,_ = learn.get_preds(ds_type=DatasetType.Test)
preds,_ = learn.get_preds(ds_type=DatasetType.Test)	test_df.has_cactus = preds.numpy()[:, 0]
test_df.has_cactus = preds.numpy()[:, 0]	test_df.to_csv('submission.csv', index=False)
"import sys, cv2, glob, os, time import pandas as pd  import numpy as np from keras.datasets import mnist from keras.layers import Input, Dense, Reshape, Flatten,Activation from keras.layers.advanced_activations import LeakyReLU from keras.models import Sequential, Model from keras.optimizers import Adam import matplotlib.pyplot as plt print(os.listdir(""../input"")) %matplotlib inline"	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.tail()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.tail()"	img_rows = 32 img_cols = 32 channels = 3 img_shape = (img_rows, img_cols, channels) z_dim = 100
img_rows = 32 img_cols = 32 channels = 3 img_shape = (img_rows, img_cols, channels) z_dim = 100	def generator(img_shape, z_dim):     model = Sequential()     model.add(Dense(128, input_dim=z_dim))     model.add(LeakyReLU(alpha=0.01))     model.add(Dense(img_rows*img_cols*channels, activation='tanh'))     model.add(Reshape(img_shape))     z = Input(shape=(z_dim,))     img = model(z)     return Model(z, img)  def discriminator(img_shape):     model = Sequential()     model.add(Flatten(input_shape=img_shape))     model.add(Dense(128))     model.add(LeakyReLU(alpha=0.01))     model.add(Dense(1, activation='sigmoid'))     img = Input(shape=img_shape)     prediction = model(img)     return Model(img, prediction)  discriminator = discriminator(img_shape) discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy']) generator = generator(img_shape, z_dim) z = Input(shape=(100,)) img = generator(z) discriminator.trainable = False prediction = discriminator(img) combined = Model(z, prediction) combined.compile(loss='binary_crossentropy', optimizer=Adam())
def generator(img_shape, z_dim):     model = Sequential()     model.add(Dense(128, input_dim=z_dim))     model.add(LeakyReLU(alpha=0.01))     model.add(Dense(img_rows*img_cols*channels, activation='tanh'))     model.add(Reshape(img_shape))     z = Input(shape=(z_dim,))     img = model(z)     return Model(z, img)  def discriminator(img_shape):     model = Sequential()     model.add(Flatten(input_shape=img_shape))     model.add(Dense(128))     model.add(LeakyReLU(alpha=0.01))     model.add(Dense(1, activation='sigmoid'))     img = Input(shape=img_shape)     prediction = model(img)     return Model(img, prediction)  discriminator = discriminator(img_shape) discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy']) generator = generator(img_shape, z_dim) z = Input(shape=(100,)) img = generator(z) discriminator.trainable = False prediction = discriminator(img) combined = Model(z, prediction) combined.compile(loss='binary_crossentropy', optimizer=Adam())	"img_ = cv2.imread(""../input/train/train/00b4dfbb267109b5f0d0dde365fa6161.jpg"",1) #img_ = cv2.cvtColor(img_,cv2.COLOR_BGR2GRAY) plt.imshow(img_)"
"img_ = cv2.imread(""../input/train/train/00b4dfbb267109b5f0d0dde365fa6161.jpg"",1) #img_ = cv2.cvtColor(img_,cv2.COLOR_BGR2GRAY) plt.imshow(img_)"	"def prepareTrainSet(train_df):     train_1 = train_df[train_df.has_cactus == 1]     train_0 = train_df[train_df.has_cactus == 0]     ids_1 = train_1.id.tolist()     ids_0 = train_0.id.tolist()      path = glob.glob(""../input/train/train/*.jpg"")     imgs_0,imgs_1 = [],[]     for img in path:         im = cv2.imread(img) #     uncomment next line while using single channel image         #im = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) #     uncomment next line if your want to scale image         #im = cv2.resize(im,(80,65))         if img.split(""/"")[-1] in ids_1:             imgs_1.append(im)         elif img.split(""/"")[-1] in ids_0:             imgs_0.append(im)                  X_train_0 = np.asarray(imgs_0)     X_train_1 = np.asarray(imgs_1)      X_train_0 = X_train_0 / 127.5 - 1.     X_train_1 = X_train_1 / 127.5 - 1.      #     uncomment next two line while using single channel image  #     X_train_0 = np.expand_dims(X_train_0, axis=3) #     X_train_1 = np.expand_dims(X_train_1, axis=3)      print(X_train_0.shape)     print(X_train_1.shape)          return X_train_0,X_train_1  losses = [] accuracies = [] def train(iterations, batch_size, sample_interval):     gen_images = []     X_train_0,X_train_1 = prepareTrainSet(train_df)          # Assign X_train to X_train_0 for augment non-cactus images     # Assign X_train to X_train_1 for augment cactus images      X_train = X_train_0     real = np.ones((batch_size, 1))     fake = np.zeros((batch_size, 1))      for iteration in range(iterations):                 idx = np.random.randint(0, X_train.shape[0], batch_size)         imgs = X_train[idx]          z = np.random.normal(0, 1, (batch_size, 100))         gen_imgs = generator.predict(z)          d_loss_real = discriminator.train_on_batch(imgs, real)         d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)          z = np.random.normal(0, 1, (batch_size, 100))         gen_imgs = generator.predict(z)         g_loss = combined.train_on_batch(z, real)          if iteration % sample_interval == 0:             print (""%d [D loss: %f, acc.: %.2f%%] [G loss: %f]"" % (iteration, d_loss[0], 100*d_loss[1], g_loss))             losses.append((d_loss[0], g_loss))             accuracies.append(100*d_loss[1])             gen_images.append(sample_images(iteration))     return gen_images"
"def prepareTrainSet(train_df):     train_1 = train_df[train_df.has_cactus == 1]     train_0 = train_df[train_df.has_cactus == 0]     ids_1 = train_1.id.tolist()     ids_0 = train_0.id.tolist()      path = glob.glob(""../input/train/train/*.jpg"")     imgs_0,imgs_1 = [],[]     for img in path:         im = cv2.imread(img) #     uncomment next line while using single channel image         #im = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY) #     uncomment next line if your want to scale image         #im = cv2.resize(im,(80,65))         if img.split(""/"")[-1] in ids_1:             imgs_1.append(im)         elif img.split(""/"")[-1] in ids_0:             imgs_0.append(im)                  X_train_0 = np.asarray(imgs_0)     X_train_1 = np.asarray(imgs_1)      X_train_0 = X_train_0 / 127.5 - 1.     X_train_1 = X_train_1 / 127.5 - 1.      #     uncomment next two line while using single channel image  #     X_train_0 = np.expand_dims(X_train_0, axis=3) #     X_train_1 = np.expand_dims(X_train_1, axis=3)      print(X_train_0.shape)     print(X_train_1.shape)          return X_train_0,X_train_1  losses = [] accuracies = [] def train(iterations, batch_size, sample_interval):     gen_images = []     X_train_0,X_train_1 = prepareTrainSet(train_df)          # Assign X_train to X_train_0 for augment non-cactus images     # Assign X_train to X_train_1 for augment cactus images      X_train = X_train_0     real = np.ones((batch_size, 1))     fake = np.zeros((batch_size, 1))      for iteration in range(iterations):                 idx = np.random.randint(0, X_train.shape[0], batch_size)         imgs = X_train[idx]          z = np.random.normal(0, 1, (batch_size, 100))         gen_imgs = generator.predict(z)          d_loss_real = discriminator.train_on_batch(imgs, real)         d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)          z = np.random.normal(0, 1, (batch_size, 100))         gen_imgs = generator.predict(z)         g_loss = combined.train_on_batch(z, real)          if iteration % sample_interval == 0:             print (""%d [D loss: %f, acc.: %.2f%%] [G loss: %f]"" % (iteration, d_loss[0], 100*d_loss[1], g_loss))             losses.append((d_loss[0], g_loss))             accuracies.append(100*d_loss[1])             gen_images.append(sample_images(iteration))     return gen_images"	def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):      z = np.random.normal(0, 1,                (image_grid_rows * image_grid_columns, z_dim))      gen_imgs = generator.predict(z)     gen_imgs = 0.5 * gen_imgs + 0.5          fig, axs = plt.subplots(image_grid_rows, image_grid_columns,  figsize=(10,10), sharey=True, sharex=True)          cnt = 0     for i in range(image_grid_rows):         for j in range(image_grid_columns):             axs[i,j].imshow(gen_imgs[cnt, :,:,0],)             axs[i,j].axis('off')             cnt += 1                  return gen_imgs
def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):      z = np.random.normal(0, 1,                (image_grid_rows * image_grid_columns, z_dim))      gen_imgs = generator.predict(z)     gen_imgs = 0.5 * gen_imgs + 0.5          fig, axs = plt.subplots(image_grid_rows, image_grid_columns,  figsize=(10,10), sharey=True, sharex=True)          cnt = 0     for i in range(image_grid_rows):         for j in range(image_grid_columns):             axs[i,j].imshow(gen_imgs[cnt, :,:,0],)             axs[i,j].axis('off')             cnt += 1                  return gen_imgs	import warnings; warnings.simplefilter('ignore')
import warnings; warnings.simplefilter('ignore')	# Set iterations at least 10000 for good results iterations = 1000 batch_size = 128 sample_interval = 1000  gen_imgs = train(iterations, batch_size, sample_interval)
# Set iterations at least 10000 for good results iterations = 1000 batch_size = 128 sample_interval = 1000  gen_imgs = train(iterations, batch_size, sample_interval)	#row -1 for lastly generated samples row = -1  #columns -1 for last element of lastly generated samples col = -1  plt.imshow(gen_imgs[row][col])
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import DenseNet121 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	DenseNet121_net = DenseNet121(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))
DenseNet121_net = DenseNet121(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))	DenseNet121_net.trainable = False DenseNet121_net.summary()
DenseNet121_net.trainable = False DenseNet121_net.summary()	model = Sequential() model.add(DenseNet121_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))
model = Sequential() model.add(DenseNet121_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 500
batch_size = 32 nb_epoch = 500	%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)
%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst)
# Prediction test_predictions = model.predict(X_tst)	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import ResNet50 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	ResNet50_net = ResNet50(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))
ResNet50_net = ResNet50(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))	ResNet50_net.trainable = False ResNet50_net.summary()
ResNet50_net.trainable = False ResNet50_net.summary()	model = Sequential() model.add(ResNet50_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))
model = Sequential() model.add(ResNet50_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 1000
batch_size = 32 nb_epoch = 1000	%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)
%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst)
# Prediction test_predictions = model.predict(X_tst)	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
import cv2 import pandas as pd import numpy as np import matplotlib.pyplot as plt import json import os from tqdm import tqdm, tqdm_notebook from keras.models import Sequential from keras.layers import Activation, Dropout, Flatten, Dense from keras.applications import VGG19 from keras.optimizers import Adam	"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"
"train_dir = ""../input/train/train/"" test_dir = ""../input/test/test/"" train_df = pd.read_csv(\'../input/train.csv\') train_df.head()"	"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"
"im = cv2.imread(""../input/train/train/01e30c0ba6e91343a12d2126fcafc0dd.jpg"") plt.imshow(im)"	VGG19_net = VGG19(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))
VGG19_net = VGG19(weights='imagenet',                    include_top=False,                    input_shape=(32, 32, 3))	VGG19_net.trainable = False VGG19_net.summary()
VGG19_net.trainable = False VGG19_net.summary()	model = Sequential() model.add(VGG19_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))
model = Sequential() model.add(VGG19_net) model.add(Flatten()) model.add(Dense(256)) model.add(Activation('relu')) model.add(Dropout(0.5)) model.add(Dense(1)) model.add(Activation('sigmoid'))	model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])
model.compile(loss='binary_crossentropy',               optimizer=Adam(lr=1e-5),                metrics=['accuracy'])	X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)
X_tr = [] Y_tr = [] imges = train_df['id'].values for img_id in tqdm_notebook(imges):     X_tr.append(cv2.imread(train_dir + img_id))         Y_tr.append(train_df[train_df['id'] == img_id]['has_cactus'].values[0])   X_tr = np.asarray(X_tr) X_tr = X_tr.astype('float32') X_tr /= 255 Y_tr = np.asarray(Y_tr)	batch_size = 32 nb_epoch = 1000
batch_size = 32 nb_epoch = 1000	%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)
%%time # Train model history = model.fit(X_tr, Y_tr,               batch_size=batch_size,               epochs=nb_epoch,               validation_split=0.1,               shuffle=True,               verbose=2)	with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()
with open('history.json', 'w') as f:     json.dump(history.history, f)  history_df = pd.DataFrame(history.history) history_df[['loss', 'val_loss']].plot() history_df[['acc', 'val_acc']].plot()	%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255
%%time X_tst = [] Test_imgs = [] for img_id in tqdm_notebook(os.listdir(test_dir)):     X_tst.append(cv2.imread(test_dir + img_id))          Test_imgs.append(img_id) X_tst = np.asarray(X_tst) X_tst = X_tst.astype('float32') X_tst /= 255	# Prediction test_predictions = model.predict(X_tst)
# Prediction test_predictions = model.predict(X_tst)	sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)
sub_df = pd.DataFrame(test_predictions, columns=['has_cactus']) sub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)	sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]
sub_df['id'] = '' cols = sub_df.columns.tolist() cols = cols[-1:] + cols[:-1] sub_df=sub_df[cols]	for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)
for i, img in enumerate(Test_imgs):     sub_df.set_value(i,'id',img)	sub_df.head()
sub_df.head()	sub_df.to_csv('submission.csv',index=False)
sub_df.to_csv('submission.csv',index=False)	NB_END
