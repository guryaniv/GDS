{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport cv2\nimport numpy as np \nimport pandas as pd \nimport time\nimport matplotlib.pyplot as plt\nfrom skimage import measure\nimport matplotlib.patches as patches\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# from sklearn.utils import resample\n# from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.utils import resample\nfrom keras.preprocessing.image import ImageDataGenerator\n#imports\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten,Conv2D,GlobalMaxPooling2D\nfrom keras import optimizers\nfrom keras.preprocessing import image\nfrom keras.metrics import top_k_categorical_accuracy\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, Concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\nfrom keras.applications.resnet50 import ResNet50,preprocess_input\nfrom keras.applications.xception import Xception\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.optimizers import Adam\nfrom keras.models import model_from_json, load_model\nimport time\n\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/humpback-whale-identification\"))\nprint(os.listdir(\"../input/pretrained-model\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dabb6329277ba0b8ee6cd693368a31ffecab0c1"},"cell_type":"code","source":"DATA=\"../input/humpback-whale-identification\"\nTRAIN_IMG=\"../input/humpback-whale-identification/train\"\nTEST_IMG=\"../input/humpback-whale-identification/test\"\nPRETRAIN_MODEL=\"../input/pretrained-model/inceptionresnetv2_model_bb_last.h5\"\nPRETRAIN_WEIGHTS=\"../input/pretrained-model/weights_inceptionresnetv2.hdf5\"\n\nBB_DATA=\"../input/bounding-boxes-using-image-processing\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"407323db267bc659be055d8b9cc8f4bcd5f399e7"},"cell_type":"code","source":"LOAD_MODEL=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fafff923a8a8d0892bb3fb401d1b6832671bb54b"},"cell_type":"code","source":"test_df= pd.DataFrame({\"Image\":  os.listdir(TEST_IMG)})\nprint(\"test images:\"+ str(len(test_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdd9468ae74dca12740901ddc50c05f24bcc546d"},"cell_type":"code","source":"train_lbl = pd.read_csv(os.path.join(DATA, 'train.csv'))\nprint(\"train images:\"+ str(len(train_lbl)))\nprint(\"total unique class:\"+ str(len(np.unique(train_lbl['Id']))))\ntrain_lbl.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed412e6d5306e3bf331345ef531aee6681ec6293"},"cell_type":"markdown","source":"**Balance training set**"},{"metadata":{"trusted":true,"_uuid":"5ca12885666f7d1e932cb072a17ad3a7edbbe53f"},"cell_type":"code","source":"train_lbl['newwhale_lbl']=np.where(train_lbl['Id']=='new_whale',1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8ac451c662bf7898a06119ca2bbbe8bba456e5f"},"cell_type":"code","source":"#take out whales with less than 2 examples\ndf=train_lbl.groupby(['Id']).size().reset_index(\n    name='train_examples')\ndf=df[df['train_examples']>=2]\n\nprint(\"number of classes with more than 2 examples:\"+ str(len(df)))\n# train_lbl_balance=train_lbl[train_lbl['Id'].isin(df['Id'])]\n# print(\"number of train instances :\"+ str(len(train_lbl_balance)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d599c301bc846b66a045ff9a999d954cb13bc55a"},"cell_type":"code","source":"#remove whales with a single train example (2072 examples)\ntrain_lbl=train_lbl[train_lbl['Id'].isin(df['Id'])]\nprint(\"number of train instances after removal:\"+ str(len(train_lbl)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e6cb74c978ce7d625641c7307196abd8aa94364"},"cell_type":"code","source":"#remove problematic image\ntrain_lbl=train_lbl[train_lbl['Image']!='859e1399e.jpg']\nprint(\"number of train instances after removal:\"+ str(len(train_lbl)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0236d0ed9fcacb30271e1c0128a1f548e114a923"},"cell_type":"code","source":"# #remove new_whale\n# train_lbl=train_lbl[train_lbl['newwhale_lbl']==0]\n# print(\"number of train instances after removal:\"+ str(len(train_lbl)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97b7d99d8fe31769011eeced0ed42012d3714bc9"},"cell_type":"markdown","source":"# Bounding boxes"},{"metadata":{"trusted":true,"_uuid":"7f401a7d0bb7e6ceee0707fc0e981ccb7167de3f"},"cell_type":"code","source":"#read BB data fo train and test\ntrain_bb=pd.read_csv(os.path.join(BB_DATA,\"boxs_train.csv\"))\ntest_bb=pd.read_csv(os.path.join(BB_DATA,\"boxs_test.csv\"))\nbb_all=train_bb.append(test_bb, ignore_index=True)\n\nbb = {}\nfor i in range(len(bb_all)):\n    image=bb_all['Image'].iloc[i].split(\"/\")[-1]\n    x0=float(bb_all['x0'].iloc[i])\n    y0=float(bb_all['y0'].iloc[i])\n    x1=float(bb_all['x1'].iloc[i])\n    y1=float(bb_all['y1'].iloc[i])\n    box=(x0,y0,x1,y1)\n    #save to labels file and dir\n#   croped_image= crop_img(image,box)\n    bb[image] = box","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"648780227d8fb1220b8e35445d872de3c1ffc7de"},"cell_type":"code","source":"#define accuracy metric\ndef top_5_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b58fafc40157efa192044eaf7eb96930220c6b13"},"cell_type":"markdown","source":"# data generator "},{"metadata":{"trusted":true,"_uuid":"06da1600eadbd7e8b2a59b3f5f1ce7b7e7b841fd"},"cell_type":"code","source":"# expnd boxes to compensate for bounding box errors\ncrop_margin = 0.1\ndef expand_bb(image,box,margin=crop_margin):\n    size_x,size_y=image.shape[1],image.shape[0]\n    x0, y0, x1, y1 = box[0],box[1],box[2],box[3]\n    dx = x1 - x0\n    dy = y1 - y0\n    x0 = max(0,x0-dx * crop_margin)\n    x1 = min(size_x,x1+ dx * crop_margin + 1)\n    y0 = max(0,y0-dy * crop_margin)\n    y1 = min(size_y, y1+dy * crop_margin + 1)\n    return x0,y0,x1,y1\n\ndef crop_img(image,box):\n    new_box=expand_bb(image,box)\n    if len(image.shape)==3:\n        new_image = image[int(new_box[1]):int(new_box[3]), int(new_box[0]):int(new_box[2]),:]\n    else:\n        new_image = image[int(new_box[1]):int(new_box[3]), int(new_box[0]):int(new_box[2])]\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"124b91c7431ec351bc12a78b9aaa22779716264c"},"cell_type":"code","source":"RESIZE_IMG = 299\nimport tensorflow as tf\nfrom skimage.transform import resize\nfrom keras.utils import np_utils\nimport cv2\nfrom PIL import Image\n\ndef read_crop_resize(image_path,boxes=bb):\n    image = cv2.imread(image_path)\n    if image is not None:\n        if image_path.split(\"/\")[-1] in boxes:\n            box=boxes[image_path.split(\"/\")[-1]]\n            croped_image= crop_img(image,box)\n            image=croped_image\n        try:\n            image = cv2.resize(image, (RESIZE_IMG, RESIZE_IMG)) \n        except cv2.error as e:\n            print(\"error resizing image:\"+image_path )\n            return None\n    return image\n\ndef preprocess_images(image_names, seed, datagen,directory=TRAIN_IMG):\n    np.random.seed(seed)\n    X = np.zeros((len(image_names), RESIZE_IMG, RESIZE_IMG,3))\n    for i, image_name in enumerate(image_names):\n        if os.path.isfile(image_name):\n            image = read_crop_resize(image_name)\n        else:\n            image = read_crop_resize(os.path.join(directory, image_name))\n        if image is not None:\n            if datagen is not None:\n                image = datagen.random_transform(image)\n            else:\n                image=image\n            image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image=np.expand_dims(image, axis=2)\n            X[i]=np.repeat(image, 3, 2)\n#             X[i]=tf.image.grayscale_to_rgb(image)\n    return X\n\ndef image_generator(data_xy,batch_size,directory,augment=True,shuffle=True):\n    datagen_args = dict(rescale=1./255,rotation_range=10,shear_range=0.2,\n                        horizontal_flip=True)\n    if not augment:\n        datagen_args = dict(rescale=1./255)\n    datagen = ImageDataGenerator(**datagen_args)\n    \n    while True:\n        # loop once per epoch\n        num_recs = len(data_xy)\n        if shuffle:\n            indices = np.random.permutation(np.arange(num_recs))\n        else:\n            indices = np.arange(num_recs)\n        num_batches = num_recs // batch_size\n        for bid in range(num_batches):\n            # loop once per batch\n            batch_indices = indices[bid * batch_size : (bid + 1) * batch_size]\n            batch = [data_xy[i] for i in batch_indices]\n            # make sure image data generators generate same transformations\n            seed = np.random.randint(low=0, high=1000, size=1)[0]\n            X = preprocess_images([b[0] for b in batch], seed, \n                                      datagen, directory)\n            \n            \n            Y = np.array([b[1] for b in batch]) # 0 or 1\n            Y= np_utils.to_categorical(Y, num_classes=2932)\n            yield (X.astype(np.uint8), Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b079925419ec54b5ab46529ce5dad41c13c9acd6"},"cell_type":"code","source":"# damages/bad images to exclude. partially taken from https://www.kaggle.com/martinpiotte/whale-recognition-model-with-score-0-78563/data\nimages=['859e1399e.jpg'\n#         ,'0b1e39ff.jpg','0c11fa0c.jpg','1b089ea6.jpg','2a2ecd4b.jpg','2c824757.jpg',\n#         '3e550c8a.jpg','56893b19.jpg','613539b4.jpg','6530809b.jpg','6b753246.jpg',\n#         '6b9f5632.jpg','75c94986.jpg','7f048f21.jpg','7f7702dc.jpg','806cf583.jpg',\n#         '95226283.jpg','a3e9070d.jpg','ade8176b.jpg','b1cfda8a.jpg','b24c8170.jpg',\n#         'b7ea8be4.jpg','b9315c19.jpg','b985ae1e.jpg','baf56258.jpg','c4ad67d8.jpg',\n#         'c5da34e7.jpg','c5e3df74.jpg','ced4a25c.jpg','d14f0126.jpg','e0b00a14.jpg',\n#         'e6ce415f.jpg','e9bd2e9c.jpg','f4063698.jpg','f9ba7040.jpg'\n       ]\nbad_images= set(images)\n# 'a' in a_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b33515fc6b9798bf7a1b4314a532c0b1a0e1ad9"},"cell_type":"code","source":"encoder = LabelEncoder()\ntrain_lbl[\"transformed_label\"] = encoder.fit_transform(train_lbl[\"Id\"])\n# print(trafomed_label)\n\ntraindata=[]\nfor i in range(len(train_lbl)):\n    x=train_lbl[\"Image\"].iloc[i]\n    y=train_lbl[\"transformed_label\"].iloc[i]\n#     if x in bad_images: continue\n    traindata.append((x,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"397915ea73531f515e1ee6bde1ea859eda0c9b4d"},"cell_type":"code","source":"testdata=[]\nfor i in range(len(test_df)):\n    x=test_df[\"Image\"].iloc[i]\n    y=0\n#     if x in bad_images: continue\n    testdata.append((x,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9f417aa3f9577d20459c91429801a7dce626fca"},"cell_type":"code","source":"batch_gen = image_generator(traindata, 32,TRAIN_IMG)\n(X, Y) = batch_gen.__next__()\nprint(\"generator output shapes:\")\nprint(X.shape, Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3e174256df0060c4778aafcc0c59898f24a81be"},"cell_type":"code","source":"plt.imshow(X[15])\n# plt.imshow(X[0][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b062a21cb82c464dbfacc0e6772209ab325ba6bb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,validation =train_test_split(traindata, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7897dcb7634f78e5965f28193741c4ce56775f4a"},"cell_type":"code","source":"train_generator = image_generator(train, 32,directory=TRAIN_IMG)\nvalidation_generator = image_generator(validation, 32,directory=TRAIN_IMG)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"842c7ad654b4256c8047a584f273a5c72f3763e2"},"cell_type":"code","source":"STEP_SIZE_TRAIN=0.8*len(traindata)//32\nSTEP_SIZE_VALID=0.2*len(traindata)//32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"522435ed92b54f122b7d73c1af23aba7bf6e6e32"},"cell_type":"code","source":"CLASSES = len(np.unique(train_lbl['transformed_label'])) \nprint(\"number of unique classes:\"+ str(CLASSES))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2a2d056d351de1d7f9d27e2b5c0dd8c05e02818"},"cell_type":"markdown","source":"**Transfer learning with Inception-resnet-v2**"},{"metadata":{"trusted":true,"_uuid":"c9c92597692801838a1ef2a7c51d79be82f1d80c"},"cell_type":"code","source":"def model_transfer(image_size=RESIZE_IMG):\n    checkpointer = ModelCheckpoint(filepath='transfetlr_mid_bb.hdf5', \n                               verbose=1, save_best_only=True)\n    decayedlr= ReduceLROnPlateau(monitor='val_loss',patience=15,min_lr=1e-9,verbose=1,mode='min')\n    \n    # setup model\n    input_tensor = Input(shape=(image_size,image_size, 3))\n#     base_model = Xception(weights='imagenet', include_top=False,input_shape = (image_size,image_size, 3))\n    base_model = InceptionResNetV2(weights='imagenet', include_top=False,input_shape = (image_size,image_size, 3)) \n\n    bn = BatchNormalization()(input_tensor)\n    x = base_model(bn)\n    x = GlobalAveragePooling2D(name='avg_pool')(x)\n    x = Dropout(0.4)(x)\n    predictions = Dense(CLASSES+1, activation='softmax')(x)\n    model = Model(inputs=input_tensor, outputs=predictions)\n\n    layer_num = len(model.layers)\n    for layer in model.layers[:int(layer_num * 0.9)]:\n        layer.trainable = False\n    \n    for layer in model.layers[int(layer_num * 0.9):]:\n        layer.trainable = True\n\n    model.compile(optimizer='adagrad',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy', top_5_accuracy])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c95feaf60f3c81d3080bb68fd85a14f3eb858e70"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\nfrom keras.utils.vis_utils import plot_model\nfrom PIL import Image as pil_image\ntransfer_model=model_transfer()\nplot_model(transfer_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True,expand_nested=True)\npil_image.open('model_plot.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a389ca75732876b1c07ff38de7836429b796abb"},"cell_type":"code","source":"if not LOAD_MODEL:\n    start = time.time()\n    print(\"start time:\"+str(start))\n\n    transfer_model.save('inceptionresnetv2_model_bb_last.h5')\n    \n    history = transfer_model.fit_generator(generator=train_generator,\n                        steps_per_epoch=STEP_SIZE_TRAIN,\n                        validation_data=validation_generator,\n                        validation_steps=STEP_SIZE_VALID,\n                        epochs=40, callbacks = [checkpointer,decayedlr])\n    transfer_model.save('inceptionresnetv2_model_bb_last.h5')\n\n    end = time.time()\n    print(\"end time:\"+str(end))\n    print(\"Complete time: \" + str(end - start) + \" Secs.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"384f2179e53c71486c60cf93c05444fa7ed7d856"},"cell_type":"markdown","source":"# Performance"},{"metadata":{"trusted":true,"_uuid":"4d22fff184db5770ce91e6056c21597645a325a6"},"cell_type":"code","source":"if not LOAD_MODEL:\n    # if TRAIN:\n        # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    history.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34ca6c987c941fed9c50be04063528409e2caafc"},"cell_type":"code","source":"if not LOAD_MODEL:\n    # if TRAIN:\n        # summarize history for accuracy\n    plt.plot(history.history['top_5_accuracy'])\n    plt.plot(history.history['val_top_5_accuracy'])\n    plt.title('model top 5 accuracy')\n    plt.ylabel('top 5 accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    history.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de24b6d772971bc9f15c78f1d15313f676e23741"},"cell_type":"code","source":"if not LOAD_MODEL:\n        # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a71e8416cadcd8a734fff4c65c474231ceb2de7"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"02439fcd02dcc8422e100eb49bccf96bf5697591"},"cell_type":"code","source":"SUBMIT=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ea6a24825278898b1fe4a6623af3f6f1b49c295"},"cell_type":"code","source":"if SUBMIT:\n    if LOAD_MODEL:\n        if os.path.isfile(PRETRAIN_MODEL):\n            model=load_model(PRETRAIN_MODEL,custom_objects={\"top_5_accuracy\": top_5_accuracy})\n        if os.path.isfile(PRETRAIN_WEIGHTS):\n            model.load_weights(PRETRAIN_WEIGHTS)\n    else:\n       model= transfer_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4877e8763ab48eedc412a8c50068605e4afe7f92"},"cell_type":"code","source":"#submission\nif SUBMIT:\n    test_generator= image_generator(testdata, 1,directory=TEST_IMG,augment=False,shuffle=False)\n    STEP_SIZE_TEST=len(testdata)\n    pred = model.predict_generator(test_generator,verbose = 1,steps=STEP_SIZE_TEST,workers=1)\n    pred_sorted = np.argsort(-pred, axis = 1)[:,:5]\n    pred_sorted_big = np.argsort(-pred, axis = 1)[:,:100]\n    top_pred=np.sort(-pred, axis = 1)[:,:1]\n    top_5_pred=np.sort(-pred, axis = 1)[:,:5]\n    # pred_sorted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcc3156fbddec106d212de49c507872d2433373d"},"cell_type":"code","source":"if SUBMIT:\n    from tqdm import tqdm\n    #create empty list\n    pred_ids = list()\n    test_imgs=list()\n    for i,row in enumerate(tqdm(pred_sorted)):\n        test_imgs.append(testdata[i][0])\n        preds_encoded=row\n        whales=encoder.inverse_transform(preds_encoded)\n        pred_ids.append(whales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0809a3ece8fcc5032c78a122897406e48dd96472"},"cell_type":"code","source":"if SUBMIT:\n    ### for pred big file\n    from tqdm import tqdm\n    #create empty list\n    pred_ids_big = list()\n    for i,row in enumerate(tqdm(pred_sorted_big)):\n        preds_encoded=row\n        whales=encoder.inverse_transform(preds_encoded)\n        pred_ids_big.append(whales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68194d99dc17caca2b5dface467de65902e78bbd"},"cell_type":"code","source":"if SUBMIT:\n    final_preds = []\n    for i,top_5_ids in enumerate(pred_ids):\n        final_preds.append(' '.join(pred_ids[i]))\n        \n    import shutil\n    shutil.rmtree('test_folder', ignore_errors=True)\n    \n    submission = pd.DataFrame({\"Image\": test_imgs, \"Id\": final_preds})\n    submission.to_csv(\"submission.csv\", index = False) #disabled to not override current submission\n    submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c87fd858eca3ad95abb48f488674bb8af8f1f228"},"cell_type":"code","source":"if SUBMIT:\n    ### for pred big file (top100)\n    final_preds = []\n    \n    for i,top_5_ids in enumerate(pred_ids_big):\n        final_preds.append(' '.join(pred_ids_big[i]))\n        \n    import shutil\n    shutil.rmtree('test_folder', ignore_errors=True)\n    \n    top_pred=-1*top_pred\n    top_5_pred=-1*top_5_pred\n    submission = pd.DataFrame({\"Image\": test_imgs,\"top_score\":list(top_pred), \"Id\": final_preds})\n    submission.to_csv(\"submission_top100.csv\", index = False)\n    submission.head(50)\n    \n    submission_scores=  pd.DataFrame({\"Image\": test_imgs,\"1_score\":list(top_5_pred[:,0]),\"2_score\":list(top_5_pred[:,1]),\"3_score\":list(top_5_pred[:,2]),\"4_score\":list(top_5_pred[:,3]),\"5_score\":list(top_5_pred[:,4])})\n    submission.to_csv(\"submission_scores_top5.csv\", index = False)\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}