{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Kaggle Competition: Predict at which speed a pet is adopted"},{"metadata":{"trusted":true,"_uuid":"40a25b03ad9ac2fdef0778c95460ea31e4881269"},"cell_type":"code","source":"# Import Packages\n\n#Dataframe packages\nimport json\nimport glob\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport cv2\nimport numpy as np\nfrom collections import Counter\nfrom functools import partial\nimport scipy as sp\n\n#Plot packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# LightGBM\nimport lightgbm as lgb\nimport scipy as sp\n\n# Load scikit's classifier library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold,RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.metrics import cohen_kappa_score,mean_squared_error, accuracy_score, confusion_matrix, f1_score,classification_report\n\nimport xgboost as xgb\n\n\n#Oversampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8365e4448f25ff3509a9abb9ff97c081f44b163"},"cell_type":"markdown","source":"## Sentiment Data"},{"metadata":{"trusted":true,"_uuid":"7cdff9a926965d5e58feb64a9ed311e841e2ad35"},"cell_type":"code","source":"sentimental_analysis_train = sorted(glob.glob('../input/train_sentiment/*.json'))\nsentimental_analysis_test = sorted(glob.glob('../input/test_sentiment/*.json'))\n\nprint('num of train sentiment files: {}'.format(len(sentimental_analysis_train)))\nprint('num of train sentiment files: {}'.format(len(sentimental_analysis_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e579470994efdaa36e8ddd53ff0d37b2b6e606e0"},"cell_type":"code","source":"# Define Empty lists\nscore=[]\nmagnitude=[]\npetid=[]\n\nfor filename in sentimental_analysis_train:\n             with open(filename, 'r') as f:\n                sentiment_file = json.load(f)\n             file_sentiment = sentiment_file['documentSentiment']\n             file_score =  np.asarray(sentiment_file['documentSentiment']['score'])\n             file_magnitude =np.asarray(sentiment_file['documentSentiment']['magnitude'])\n\n\n             score.append(file_score)\n             magnitude.append(file_magnitude)\n\n             petid.append(filename.replace('.json','').replace('../input/train_sentiment/', ''))\n\n # Output with sentiment data for each pet\nsentimental_analysis_train = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n                                                    pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)\n\nscore=[]\nmagnitude=[]\npetid=[]\n\nfor filename in sentimental_analysis_test:\n             with open(filename, 'r') as f:\n                sentiment_file = json.load(f)\n             file_sentiment = sentiment_file['documentSentiment']\n             file_score =  np.asarray(sentiment_file['documentSentiment']['score'])\n             file_magnitude =np.asarray(sentiment_file['documentSentiment']['magnitude'])\n\n\n             score.append(file_score)\n             magnitude.append(file_magnitude)\n\n             petid.append(filename.replace('.json','').replace('../input/test_sentiment/', ''))\n\n # Output with sentiment data for each pet\nsentimental_analysis_test = pd.concat([ pd.DataFrame(petid, columns =['PetID']) ,pd.DataFrame(score, columns =['sentiment_document_score']),\n                                                    pd.DataFrame(magnitude, columns =['sentiment_document_magnitude'])],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bcee8b84010d33d1911befcbb6acbeea419052f"},"cell_type":"markdown","source":"## Image Metadata"},{"metadata":{"trusted":true,"_uuid":"b42c6c2d455f94f8aa6804d202fdaa48a5475f3e"},"cell_type":"code","source":"image_metadata_train =  sorted(glob.glob('../input/train_metadata/*.json'))\nimage_metadata_test =  sorted(glob.glob('../input/test_metadata/*.json'))\nprint('num of train metadata: {}'.format(len(image_metadata_train)))\nprint('num of train metadata: {}'.format(len(image_metadata_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0db0c9c5970b671ddca7ad5a7bdcfc4bcf9c4d03"},"cell_type":"code","source":"description=[]\ntopicality=[]\nimageid=[]\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_train:\n         with open(filename, 'r') as f:\n            d = json.load(f)\n            file_keys = list(d.keys())\n         if  'labelAnnotations' in file_keys:\n            file_annots = d['labelAnnotations']\n            file_topicality = np.asarray([x['topicality'] for x in file_annots])\n            file_description = [x['description'] for x in file_annots]\n            #Create a list of all descriptions and topicality\n            description.append(file_description)\n            topicality.append(file_topicality)\n            #Create a list with all image id name\n            imageid.append(filename.replace('.json','').replace('../input/train_metadata/',''))\n\n# Prepare the output by renaming all variables\ndescription=pd.DataFrame(description)\ntopicality=pd.DataFrame(topicality)\n\nnew_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\ndescription.rename(columns = dict(new_names), inplace=True)\n\nnew_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\ntopicality.rename(columns = dict(new_names), inplace=True)\n\n# Output with sentiment data for each pet\nimage_labelannot_train = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n\n# create the PetId variable\nimage_labelannot_train['PetID'] = image_labelannot_train['ImageId'].str.split('-').str[0]\n\n\n##############\n# TOPICALITY #\n##############\n\nimage_labelannot_train['metadata_topicality_mean'] = image_labelannot_train.iloc[:,1:10].mean(axis=1)\nimage_labelannot_train['metadata_topicality_mean']  = image_labelannot_train.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n\nimage_labelannot_train['metadata_topicality_max'] = image_labelannot_train.iloc[:,1:10].max(axis=1)\nimage_labelannot_train['metadata_topicality_max'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n\nimage_labelannot_train['metadata_topicality_min'] = image_labelannot_train.iloc[:,1:10].min(axis=1)\nimage_labelannot_train['metadata_topicality_min'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n\n\nimage_labelannot_train['metadata_topicality_0_mean']  = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\nimage_labelannot_train['metadata_topicality_0_max'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform(max)\nimage_labelannot_train['metadata_topicality_0_min'] = image_labelannot_train.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n\n\n###############\n# DESCRIPTION #\n###############\n\n# Create Features from the Images\nimage_labelannot_train['L_metadata_0_cat']=image_labelannot_train['metadata_description_0'].str.contains(\"cat\").astype(int)\nimage_labelannot_train['L_metadata_0_dog'] =image_labelannot_train['metadata_description_0'].str.contains(\"dog\").astype(int)\n\nimage_labelannot_train['L_metadata_any_cat']=image_labelannot_train.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\nimage_labelannot_train['L_metadata_any_dog']=image_labelannot_train.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n\nimage_labelannot_train['L_metadata_0_cat_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_0_cat'].transform('sum')\nimage_labelannot_train['L_metadata_0_dog_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_0_dog'].transform('sum')\n\nimage_labelannot_train['L_metadata_any_cat_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_any_cat'].transform('sum')\nimage_labelannot_train['L_metadata_any_dog_sum'] = image_labelannot_train.groupby(image_labelannot_train['PetID'])['L_metadata_any_dog'].transform('sum')\n\nimage_labelannot_train = image_labelannot_train[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min','metadata_topicality_0_mean','metadata_topicality_0_max',\n                                                 'metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum','L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\nimage_labelannot_train=image_labelannot_train.drop_duplicates('PetID')\n\ndescription=[]\ntopicality=[]\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_test:\n         with open(filename, 'r') as f:\n            d = json.load(f)\n            file_keys = list(d.keys())\n         if  'labelAnnotations' in file_keys:\n            file_annots = d['labelAnnotations']\n            file_topicality = np.asarray([x['topicality'] for x in file_annots])\n            file_description = [x['description'] for x in file_annots]\n            #Create a list of all descriptions and topicality\n            description.append(file_description)\n            topicality.append(file_topicality)\n            #Create a list with all image id name\n            imageid.append(filename.replace('.json','').replace('../input/test_metadata/',''))\n\n# Prepare the output by renaming all variables\ndescription=pd.DataFrame(description)\ntopicality=pd.DataFrame(topicality)\n\nnew_names = [(i,'metadata_description_'+str(i)) for i in description.iloc[:, 0:].columns.values]\ndescription.rename(columns = dict(new_names), inplace=True)\n\nnew_names = [(i,'metadata_topicality_'+str(i)) for i in topicality.iloc[:, 0:].columns.values]\ntopicality.rename(columns = dict(new_names), inplace=True)\n\n# Output with sentiment data for each pet\nimage_labelannot_test = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,topicality,description],axis =1)\n\n# create the PetId variable\nimage_labelannot_test['PetID'] = image_labelannot_test['ImageId'].str.split('-').str[0]\n\n\n##############\n# TOPICALITY #\n##############\n\nimage_labelannot_test['metadata_topicality_mean'] = image_labelannot_test.iloc[:,1:10].mean(axis=1)\nimage_labelannot_test['metadata_topicality_mean']  = image_labelannot_test.groupby(['PetID'])['metadata_topicality_mean'].transform('mean') \n\nimage_labelannot_test['metadata_topicality_max'] = image_labelannot_test.iloc[:,1:10].max(axis=1)\nimage_labelannot_test['metadata_topicality_max'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_max'].transform(max)\n\nimage_labelannot_test['metadata_topicality_min'] = image_labelannot_test.iloc[:,1:10].min(axis=1)\nimage_labelannot_test['metadata_topicality_min'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_min'].transform(min)\n\n\nimage_labelannot_test['metadata_topicality_0_mean']  = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform('mean')\nimage_labelannot_test['metadata_topicality_0_max'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform(max)\nimage_labelannot_test['metadata_topicality_0_min'] = image_labelannot_test.groupby(['PetID'])['metadata_topicality_0'].transform(min)\n\n\n###############\n# DESCRIPTION #\n###############\n\n# Create Features from the Images\nimage_labelannot_test['L_metadata_0_cat']=image_labelannot_test['metadata_description_0'].str.contains(\"cat\").astype(int)\nimage_labelannot_test['L_metadata_0_dog'] =image_labelannot_test['metadata_description_0'].str.contains(\"dog\").astype(int)\n\nimage_labelannot_test['L_metadata_any_cat']=image_labelannot_test.apply(lambda row: row.astype(str).str.contains('cat').any(), axis=1)\nimage_labelannot_test['L_metadata_any_dog']=image_labelannot_test.apply(lambda row: row.astype(str).str.contains('dog').any(), axis=1)\n\nimage_labelannot_test['L_metadata_0_cat_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_0_cat'].transform('sum')\nimage_labelannot_test['L_metadata_0_dog_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_0_dog'].transform('sum')\n\nimage_labelannot_test['L_metadata_any_cat_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_any_cat'].transform('sum')\nimage_labelannot_test['L_metadata_any_dog_sum'] = image_labelannot_test.groupby(image_labelannot_test['PetID'])['L_metadata_any_dog'].transform('sum')\n\nimage_labelannot_test = image_labelannot_test[['PetID','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n                                               'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_cat_sum','L_metadata_0_dog_sum',\n                                               'L_metadata_any_cat_sum','L_metadata_any_dog_sum']]\nimage_labelannot_test=image_labelannot_test.drop_duplicates('PetID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f061436186d9fb202c883aacef0565e2bfe8c438"},"cell_type":"code","source":"color_score_mean=[]\ncolor_score_min=[]\ncolor_score_max=[]\n\ncolor_pixelfrac_mean=[]\ncolor_pixelfrac_min=[]\ncolor_pixelfrac_max=[]\n\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_train:\n         with open(filename, 'r') as f:\n              d = json.load(f)\n              file_keys = list(d.keys())\n              if  'imagePropertiesAnnotation' in file_keys:\n                  file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n               \n                  file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n                  file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n                  file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n                  file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n\n\n                  file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n                  file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n\n\n              #Create a list with all image id name\n              imageid.append(filename.replace('.json','').replace('../input/train_metadata/', ''))\n\n              color_score_mean.append(file_color_score_mean)\n              color_score_min.append(file_color_score_min)\n              color_score_max.append(file_color_score_max)\n\n\n              color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n              color_pixelfrac_min.append(file_color_pixelfrac_min)\n              color_pixelfrac_max.append(file_color_pixelfrac_max)\n\n      \nimage_properties_train = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n\n\n# create the PetId variable\nimage_properties_train['PetID'] = image_properties_train['ImageId'].str.split('-').str[0]\n\n\n##############\n# COLOR INFO #\n##############\nimage_properties_train['metadata_color_pixelfrac_mean']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \nimage_properties_train['metadata_color_pixelfrac_min']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \nimage_properties_train['metadata_color_pixelfrac_max']  = image_properties_train.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n\nimage_properties_train['metadata_color_score_mean']  = image_properties_train.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \nimage_properties_train['metadata_color_score_min']  = image_properties_train.groupby(['PetID'])['metadata_color_score_min'].transform(min) \nimage_properties_train['metadata_color_score_max']  = image_properties_train.groupby(['PetID'])['metadata_color_score_max'].transform(max)\n\nimage_properties_train=image_properties_train.drop_duplicates('PetID')\nimage_properties_train = image_properties_train.drop(['ImageId'], 1)\n\n\ncolor_score_mean=[]\ncolor_score_min=[]\ncolor_score_max=[]\n\ncolor_pixelfrac_mean=[]\ncolor_pixelfrac_min=[]\ncolor_pixelfrac_max=[]\n\nimageid=[]\n\n# Read Zip File and Export a Dataset with the Score and the ID\nfor filename in image_metadata_test:\n         with open(filename, 'r') as f:\n              d = json.load(f)\n              file_keys = list(d.keys())\n              if  'imagePropertiesAnnotation' in file_keys:\n                  file_colors = d['imagePropertiesAnnotation']['dominantColors']['colors']\n               \n                  file_color_score_mean = np.asarray([x['score'] for x in file_colors]).mean()\n                  file_color_pixelfrac_mean = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n                  file_color_score_min = np.asarray([x['score'] for x in file_colors]).min()\n                  file_color_pixelfrac_min = np.asarray([x['pixelFraction'] for x in file_colors]).min()\n\n\n                  file_color_score_max = np.asarray([x['score'] for x in file_colors]).max()\n                  file_color_pixelfrac_max = np.asarray([x['pixelFraction'] for x in file_colors]).max()\n\n\n              #Create a list with all image id name\n              imageid.append(filename.replace('.json','').replace('../input/test_metadata/', ''))\n\n              color_score_mean.append(file_color_score_mean)\n              color_score_min.append(file_color_score_min)\n              color_score_max.append(file_color_score_max)\n\n\n              color_pixelfrac_mean.append(file_color_pixelfrac_mean)\n              color_pixelfrac_min.append(file_color_pixelfrac_min)\n              color_pixelfrac_max.append(file_color_pixelfrac_max)\n\n      \nimage_properties_test = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'metadata_color_pixelfrac_mean':color_pixelfrac_mean}), pd.DataFrame({'metadata_color_pixelfrac_min':color_pixelfrac_min}),pd.DataFrame({'metadata_color_pixelfrac_max':color_pixelfrac_max}),pd.DataFrame({'metadata_color_score_mean':color_score_mean}),pd.DataFrame({'metadata_color_score_min':color_score_min}),pd.DataFrame({'metadata_color_score_max':color_score_max})],axis=1)\n\n\n# create the PetId variable\nimage_properties_test['PetID'] = image_properties_test['ImageId'].str.split('-').str[0]\n\n\n##############\n# COLOR INFO #\n##############\nimage_properties_test['metadata_color_pixelfrac_mean']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_mean'].transform('mean') \nimage_properties_test['metadata_color_pixelfrac_min']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_min'].transform(min) \nimage_properties_test['metadata_color_pixelfrac_max']  = image_properties_test.groupby(['PetID'])['metadata_color_pixelfrac_max'].transform(max) \n\nimage_properties_test['metadata_color_score_mean']  = image_properties_test.groupby(['PetID'])['metadata_color_score_mean'].transform('mean') \nimage_properties_test['metadata_color_score_min']  = image_properties_test.groupby(['PetID'])['metadata_color_score_min'].transform(min) \nimage_properties_test['metadata_color_score_max']  = image_properties_test.groupby(['PetID'])['metadata_color_score_max'].transform(max)\n\nimage_properties_test=image_properties_test.drop_duplicates('PetID')\nimage_properties_test = image_properties_test.drop(['ImageId'], 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2ef53075ab7c5463233c081ee562f3d828ee7e1"},"cell_type":"markdown","source":"## Image Quality "},{"metadata":{"trusted":true,"_uuid":"957c38c495172bba05627e1f86bd8d0df10b6e91"},"cell_type":"code","source":"image_quality_train =sorted(glob.glob('../input/train_images/*.jpg'))\nimage_quality_test =sorted(glob.glob('../input/test_images/*.jpg'))\n\nblur=[]\nimage_pixel=[]\nimageid =[]\n\nfor filename in image_quality_train:\n              #Blur \n              image = cv2.imread(filename)\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n              result = cv2.Laplacian(gray, cv2.CV_64F).var() \n              # Pixels\n              with Image.open(filename) as pixel:\n                  width, height = pixel.size\n              \n              pixel = width*height\n              \n              #image pixel size for each image\n              \n              image_pixel.append(pixel)\n              #blur for each image\n              blur.append(result)\n              #image id\n              imageid.append(filename.replace('.jpg','').replace('../input/train_images/', ''))\n                \n# Join Pixel, Blur and Image ID\nimage_quality_train = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n\n# create the PetId variable\nimage_quality_train['PetID'] = image_quality_train['ImageId'].str.split('-').str[0]\n\n#Mean of the Mean\nimage_quality_train['pixel_mean'] = image_quality_train.groupby(['PetID'])['pixel'].transform('mean')\nimage_quality_train['blur_mean'] = image_quality_train.groupby(['PetID'])['blur'].transform('mean') \n\nimage_quality_train['pixel_min'] = image_quality_train.groupby(['PetID'])['pixel'].transform('min') \nimage_quality_train['blur_min'] = image_quality_train.groupby(['PetID'])['blur'].transform('min')\n\nimage_quality_train['pixel_max'] = image_quality_train.groupby(['PetID'])['pixel'].transform('max') \nimage_quality_train['blur_max'] = image_quality_train.groupby(['PetID'])['blur'].transform('max')\n\nimage_quality_train = image_quality_train.drop(['blur','pixel','ImageId'], 1)\nimage_quality_train=image_quality_train.drop_duplicates('PetID')\n\nblur=[]\nimage_pixel=[]\nimageid =[]\n\nfor filename in image_quality_test:\n              #Blur \n              image = cv2.imread(filename)\n              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n              result = cv2.Laplacian(gray, cv2.CV_64F).var() \n              # Pixels\n              with Image.open(filename) as pixel:\n                  width, height = pixel.size\n              \n              pixel = width*height\n              \n              #image pixel size for each image\n              \n              image_pixel.append(pixel)\n              #blur for each image\n              blur.append(result)\n              #image id\n              imageid.append(filename.replace('.jpg','').replace('../input/test_images/', ''))\n                \n# Join Pixel, Blur and Image ID\nimage_quality_test = pd.concat([ pd.DataFrame(imageid, columns =['ImageId']) ,pd.DataFrame(blur, columns =['blur']),\n                                        pd.DataFrame(image_pixel,columns=['pixel'])],axis =1)\n\n# create the PetId variable\nimage_quality_test['PetID'] = image_quality_test['ImageId'].str.split('-').str[0]\n\n#Mean of the Mean\nimage_quality_test['pixel_mean'] = image_quality_test.groupby(['PetID'])['pixel'].transform('mean')\nimage_quality_test['blur_mean'] = image_quality_test.groupby(['PetID'])['blur'].transform('mean') \n\nimage_quality_test['pixel_min'] = image_quality_test.groupby(['PetID'])['pixel'].transform('min') \nimage_quality_test['blur_min'] = image_quality_test.groupby(['PetID'])['blur'].transform('min')\n\nimage_quality_test['pixel_max'] = image_quality_test.groupby(['PetID'])['pixel'].transform('max') \nimage_quality_test['blur_max'] = image_quality_test.groupby(['PetID'])['blur'].transform('max')\n\nimage_quality_test = image_quality_test.drop(['blur','pixel','ImageId'], 1)\nimage_quality_test=image_quality_test.drop_duplicates('PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f60df714dbbb35eabaa682db0139096de68f6364"},"cell_type":"markdown","source":"## Quality Information: HU Moments"},{"metadata":{"trusted":true,"_uuid":"d1f5f1a31bae56974363bdcaee410578e4247fe0"},"cell_type":"code","source":"from math import copysign, log10\n\nhuMoments0=[]\nhuMoments1=[]\nhuMoments2=[]\nhuMoments3=[]\nhuMoments4=[]\nhuMoments5=[]\nhuMoments6=[]\nimageid =[]\n\nimage_info_train =sorted(glob.glob('../input/train_images/*.jpg'))\nimage_info_test =sorted(glob.glob('../input/test_images/*.jpg'))\n\nfor filename in image_info_train:\n            if filename.endswith(\"-1.jpg\"): # Take only the moments of picture 1\n                image = cv2.imread(filename)\n                im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n                # Calculate Moments\n                moments = cv2.moments(im)\n\n                # Calculate Hu Moments\n                huMoments = cv2.HuMoments(moments)\n                # Log scale hu moments\n                for i in range(0,7):\n                      huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n\n                #image id\n                imageid.append(filename.replace('.jpg','').replace('../input/train_images/', ''))\n                huMoments0.append(huMoments[0])\n\n                huMoments1.append(huMoments[1])\n                huMoments2.append(huMoments[2])\n                huMoments3.append(huMoments[3])\n                huMoments4.append(huMoments[4])\n                huMoments5.append(huMoments[5])\n                huMoments6.append(huMoments[6])\n\nimage_moments_train = pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), \n                                     pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n                                     pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),\n                                     pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),\n                                     pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n                                     pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n            \n\n# create the PetId variable\nimage_moments_train['PetID'] = image_moments_train['ImageId'].str.split('-').str[0]\nimage_moments_train = image_moments_train[image_moments_train['ImageId'].apply(lambda x:x.endswith((\"-1\")))]\n\nhuMoments0=[]\nhuMoments1=[]\nhuMoments2=[]\nhuMoments3=[]\nhuMoments4=[]\nhuMoments5=[]\nhuMoments6=[]\nimageid =[]\nfor filename in image_info_test:\n            if filename.endswith(\"-1.jpg\"): # Take only the moments of picture 1\n                image = cv2.imread(filename)\n                im = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n                # Calculate Moments\n                moments = cv2.moments(im)\n\n                # Calculate Hu Moments\n                huMoments = cv2.HuMoments(moments)\n                # Log scale hu moments\n                for i in range(0,7):\n                      huMoments[i] = round(-1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i])),2)\n\n                #image id\n                imageid.append(filename.replace('.jpg','').replace('../input/test_images/', ''))\n                huMoments0.append(huMoments[0])\n\n                huMoments1.append(huMoments[1])\n                huMoments2.append(huMoments[2])\n                huMoments3.append(huMoments[3])\n                huMoments4.append(huMoments[4])\n                huMoments5.append(huMoments[5])\n                huMoments6.append(huMoments[6])\n\nimage_moments_test= pd.concat([pd.DataFrame({'ImageId':imageid}),pd.DataFrame({'huMoments0':np.concatenate(huMoments0,axis=0)}), pd.DataFrame({'huMoments1':np.concatenate(huMoments1,axis=0)}),\n                                           pd.DataFrame({'huMoments2':np.concatenate(huMoments2,axis=0)}),pd.DataFrame({'huMoments3':np.concatenate(huMoments3,axis=0)}),pd.DataFrame({'huMoments4':np.concatenate(huMoments4,axis=0)}),\n                                           pd.DataFrame({'huMoments5':np.concatenate(huMoments5,axis=0)}),pd.DataFrame({'huMoments6':np.concatenate(huMoments6,axis=0)})],axis=1)\n            \n\n# create the PetId variable\nimage_moments_test['PetID'] = image_moments_test['ImageId'].str.split('-').str[0]\nimage_moments_test = image_moments_test[image_moments_test['ImageId'].apply(lambda x:x.endswith((\"-1\")))]\nimage_moments_test = image_moments_test.drop(['ImageId'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a0dd0e2419fed1f6f65f0cf1d6c3bfc2e3c59e1"},"cell_type":"code","source":"image_moments_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fec79e63cf7187365a6e23a9b93aa8d36015da7"},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\nsample_submission = pd.read_csv('../input/test/sample_submission.csv')\n\nbreed =pd.read_csv('../input/breed_labels.csv',usecols=[\"BreedID\", \"BreedName\"]) #A pet could have multiple breed\ncolor =pd.read_csv('../input/color_labels.csv') #A pet could have multiple colors\nstate =pd.read_csv('../input/state_labels.csv')\n\n# Add information about color, breed, state and sentiment data\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\ntrain = (pd.merge(train, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntrain = (pd.merge(train, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n\ntrain = (pd.merge(train, state,  how='inner', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n\n# Add information about sentimental analysis\ntrain = (pd.merge(train, sentimental_analysis_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about Metadata Images\ntrain = (pd.merge(train, image_properties_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_labelannot_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntrain = (pd.merge(train, image_moments_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about quality Images\ntrain = (pd.merge(train, image_quality_train,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n\n# Add information about color, breed, state and sentiment data\ntest = (pd.merge(test, breed.rename(columns={\"BreedName\": \"BreedName1\"}),  how='left', left_on=['Breed1'], right_on = ['BreedID']).drop('BreedID', axis=1))\ntest = (pd.merge(test, breed.rename(columns={\"BreedName\": \"BreedName2\"}),  how='left', left_on=['Breed2'], right_on = ['BreedID']).drop('BreedID', axis=1))\n\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName1\"}),  how='left', left_on=['Color1'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName2\"}),  how='left', left_on=['Color2'], right_on = ['ColorID']).drop('ColorID', axis=1))\ntest = (pd.merge(test, color.rename(columns={\"ColorName\": \"ColorName3\"}),  how='left', left_on=['Color3'], right_on = ['ColorID']).drop('ColorID', axis=1))\n\ntest = (pd.merge(test, state,  how='inner', left_on=['State'], right_on = ['StateID']).drop('StateID', axis=1))\n\n# Add information about sentimental analysis\ntest = (pd.merge(test, sentimental_analysis_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about Metadata Images\ntest = (pd.merge(test, image_properties_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntest = (pd.merge(test, image_labelannot_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\ntest = (pd.merge(test, image_moments_test,  how='left', left_on=['PetID'], right_on = ['PetID']))\n\n# Add information about quality Images\ntest = (pd.merge(test, image_quality_test,  how='left', left_on=['PetID'], right_on = ['PetID']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67a4ef6e5c84566da939ff24926ad3ad648dec92"},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a017a32f4c94a7cb75234fc7189b5cf332fd1d6e"},"cell_type":"markdown","source":"## Open Source (State Information)"},{"metadata":{"trusted":true,"_uuid":"1d7098d5fe1b0ca427b78d4048770e9abb0173c0"},"cell_type":"code","source":"## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n## Using the Kernel:https://www.kaggle.com/bibek777/stacking-kernels\n\n# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# state population: https://en.wikipedia.org/wiki/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\nstate_area ={\n    41336:19102,\n41325:9500,\n41367:15099,\n41401:243,\n41415:91,\n41324:1664,\n41332:6686,\n41335:36137,\n41330:21035,\n41380:821,\n41327:1048,\n41345:73631,\n41342:124450,\n41326:8104,\n41361:13035}\n\n# https://www.dosm.gov.my/\n# Unemployment Rate in 2017\nstate_unemployment ={\n    41336 : 3.6,\n41325 :2.9,\n41367: 3.8,\n41324: 0.9,\n41332 : 2.7,\n41335: 2.6,\n41330: 3.4,\n41380: 2.9,\n41327: 2.1,\n41345 : 5.4,\n41342 : 3.3,\n41326: 3.2,\n41361: 4.2,\n41415: 7.8,\n41401: 3.3\n}\n# https://www.dosm.gov.my/\n# per 1000 population in 2016\nstate_birth_rate = {\n 41336:16.3,\n41325:17.0,\n41367:21.4,\n41401:14.4,\n41415:18.1,\n41324:16.0,\n41332:16.4,\n41335:17.0,\n41330:14.4,\n41380:17.5,\n41327:12.7,\n41345:13.7,\n41342:13.9,\n41326:16.6,\n41361:23.3,     \n}\n\ntrain[\"state_gdp\"] = train.State.map(state_gdp)\ntrain[\"state_population\"] = train.State.map(state_population)\ntrain[\"state_area\"] = train.State.map(state_area)\ntrain['state_unemployment']=train.State.map(state_unemployment)\ntrain['state_birth_rate']=train.State.map(state_birth_rate)\n\ntest[\"state_gdp\"] =test.State.map(state_gdp)\ntest[\"state_population\"] = test.State.map(state_population)\ntest[\"state_area\"] = test.State.map(state_area)\ntest['state_unemployment']=test.State.map(state_unemployment)\ntest['state_birth_rate']=test.State.map(state_birth_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ace1e577bfb34f2bdf322c61cdc7e72f5b3db76"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d5419699409c7bdac47179adeea5ed3b9057bf1"},"cell_type":"markdown","source":"## Create Features based on Statistical Analysis"},{"metadata":{"trusted":true,"_uuid":"d52dd11185de46b78609f733e1ab56572ef326cb"},"cell_type":"code","source":"# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\ntrain['L_Color1'] = (pd.isnull(train['ColorName3']) & pd.isnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color2'] = (pd.isnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\ntrain['L_Color3'] = (pd.notnull(train['ColorName3']) & pd.notnull(train['ColorName2']) & pd.notnull(train['ColorName1'])).astype(int)\n\n# Breed (create a flag if the pet has 1 breed or 2)\ntrain['L_Breed1'] = (pd.isnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\ntrain['L_Breed2'] = (pd.notnull(train['BreedName2']) & pd.notnull(train['BreedName1'])).astype(int)\n\n#Name (create a flag if the name is missing, with less than two letters)\ntrain['Name_Length']=train['Name'].str.len()\ntrain['L_Name_missing'] =  (pd.isnull(train['Name'])).astype(int)\n\n# Breed create columns\ntrain['L_Breed1_Siamese'] =(train['BreedName1']=='Siamese').astype(int)\ntrain['L_Breed1_Persian']=(train['BreedName1']=='Persian').astype(int)\ntrain['L_Breed1_Labrador_Retriever']=(train['BreedName1']=='Labrador Retriever').astype(int)\ntrain['L_Breed1_Terrier']=(train['BreedName1']=='Terrier').astype(int)\ntrain['L_Breed1_Golden_Retriever ']=(train['BreedName1']=='Golden Retriever').astype(int)\n\n#Description \ntrain['Description_Length']=train['Description'].str.len() \n\n# Fee Amount\ntrain['L_Fee_Free'] =  (train['Fee']==0).astype(int)\n\n#Add the Number of Pets per Rescuer \npets_total = train.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\ntrain= pd.merge(train, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\ntrain.count()\n\n# No photo\ntrain['L_NoPhoto'] =  (train['PhotoAmt']==0).astype(int)\n\n#No Video\ntrain['L_NoVideo'] =  (train['VideoAmt']==0).astype(int)\n\n#Log Age \ntrain['Log_Age']= np.log(train.Age+1) \n\n#Negative Score \ntrain['L_scoreneg'] =  (train['sentiment_document_score']<0).astype(int)\n\n#Quantity Amount >5\ntrain.loc[train['Quantity'] > 5, 'Quantity'] = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fd675e1554e440d9e39c7de64b6ae84efb7764"},"cell_type":"code","source":"# Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\ntest['L_Color1'] = (pd.isnull(test['ColorName3']) & pd.isnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\ntest['L_Color2'] = (pd.isnull(test['ColorName3']) & pd.notnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\ntest['L_Color3'] = (pd.notnull(test['ColorName3']) & pd.notnull(test['ColorName2']) & pd.notnull(test['ColorName1'])).astype(int)\n\n# Breed (create a flag if the pet has 1 breed or 2)\ntest['L_Breed1'] = (pd.isnull(test['BreedName2']) & pd.notnull(test['BreedName1'])).astype(int)\ntest['L_Breed2'] = (pd.notnull(test['BreedName2']) & pd.notnull(test['BreedName1'])).astype(int)\n\n#Name (create a flag if the name is missing, with less than two letters)\ntest['Name_Length']=test['Name'].str.len()\ntest['L_Name_missing'] =  (pd.isnull(test['Name'])).astype(int)\n\n# Breed create columns\ntest['L_Breed1_Siamese'] =(test['BreedName1']=='Siamese').astype(int)\ntest['L_Breed1_Persian']=(test['BreedName1']=='Persian').astype(int)\ntest['L_Breed1_Labrador_Retriever']=(test['BreedName1']=='Labrador Retriever').astype(int)\ntest['L_Breed1_Terrier']=(test['BreedName1']=='Terrier').astype(int)\ntest['L_Breed1_Golden_Retriever ']=(test['BreedName1']=='Golden Retriever').astype(int)\n\n#Description \ntest['Description_Length']=test['Description'].str.len() \n\n# Fee Amount\ntest['L_Fee_Free'] =  (test['Fee']==0).astype(int)\n\n#Add the Number of Pets per Rescuer \npets_total = test.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\ntest= pd.merge(test, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\ntest.count()\n\n# No photo\ntest['L_NoPhoto'] =  (test['PhotoAmt']==0).astype(int)\n\n#No Video\ntest['L_NoVideo'] =  (test['VideoAmt']==0).astype(int)\n\n#Log Age \ntest['Log_Age']= np.log(test.Age+1) \n\n#Negative Score \ntest['L_scoreneg'] =  (test['sentiment_document_score']<0).astype(int)\n\n#Quantity Amount >5\ntest.loc[test['Quantity'] > 5, 'Quantity'] = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd443da961d9f8061d53a28b32176a723de8583"},"cell_type":"markdown","source":"## Features Text Mining"},{"metadata":{"trusted":true,"_uuid":"03b72a2bf64bc2467e46ca7329ef1a521b28c932"},"cell_type":"code","source":"# Normalize the Variable Description\ntrain['Description'] =train['Description'].fillna(\"<MISSING>\")\ntrain['Description'] = train['Description'].str.replace('\\d+', '')\ntrain['Description'] = train['Description'].str.lower()\ntrain[\"Description\"] = train['Description'].str.replace('[^\\w\\s]','')\n\n# Stop Words \nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\npat = r'\\b(?:{})\\b'.format('|'.join(stop))\ntrain['Description'] = train['Description'].str.replace(pat, '')\ntrain['Description'] = train['Description'].str.replace(r'\\s+', ' ')\n\n# Stem Words\ntrain['Description'] = train['Description'].astype(str).str.split()\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nporter_stemmer = PorterStemmer()\ntrain['Description']=train['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n\ntrain['Description']=train['Description'].apply(lambda x : \" \".join(x))\n\ndef get_top_n_words(corpus, n=None):\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    test=pd.DataFrame(words_freq[:n], columns=['words','freq']) \n    \n    sns.barplot(x='words', y='freq', data=test)\n\nget_top_n_words(train['Description'],10)\n\nfrom sklearn.decomposition import TruncatedSVD, NMF\n# Matrix Factorization for dimensionality reduction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nsvd_ = TruncatedSVD(\n    n_components=5, random_state=1337)\nnmf_ = NMF(\n    n_components=5, random_state=1337)\n\ntfidf_col = TfidfVectorizer().fit_transform(train['Description'])\nsvd_col = svd_.fit_transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('SVD_')\n\nnmf_col = nmf_.fit_transform(tfidf_col)\nnmf_col = pd.DataFrame(nmf_col)\nnmf_col = nmf_col.add_prefix('NMF_')\n\n# Concatenate all dataframes\ntrain = pd.concat([train,nmf_col,svd_col],axis=1)\n\n\n# Normalize the Variable Description\ntest['Description'] =test['Description'].fillna(\"<MISSING>\")\ntest['Description'] = test['Description'].str.replace('\\d+', '')\ntest['Description'] = test['Description'].str.lower()\ntest[\"Description\"] = test['Description'].str.replace('[^\\w\\s]','')\n\ntest['Description'] = test['Description'].str.replace(pat, '')\ntest['Description'] = test['Description'].str.replace(r'\\s+', ' ')\n\n# Stem Words\ntest['Description'] = test['Description'].astype(str).str.split()\n\ntest['Description']=test['Description'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n\ntest['Description']=test['Description'].apply(lambda x : \" \".join(x))\n\nsvd_ = TruncatedSVD(\n    n_components=5, random_state=1337)\nnmf_ = NMF(\n    n_components=5, random_state=1337)\n\ntfidf_col = TfidfVectorizer().fit_transform(test['Description'])\nsvd_col = svd_.fit_transform(tfidf_col)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('SVD_')\n\nnmf_col = nmf_.fit_transform(tfidf_col)\nnmf_col = pd.DataFrame(nmf_col)\nnmf_col = nmf_col.add_prefix('NMF_')\n\n# Concatenate all dataframes\ntest = pd.concat([test,nmf_col,svd_col],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f02fb23b73fac981d6223ef9fa88ac196330972a"},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ea55234ef2ba97fc97c276958f293acb3c6ede"},"cell_type":"markdown","source":"## Impute Missing Values"},{"metadata":{"trusted":true,"_uuid":"174d4399a395814aeef2281a2e0b2d6dab134582"},"cell_type":"code","source":"# Cannot be used for this analysis (IDs, Texts...)\ntrain_analysis = train.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n                            'BreedName1','Color1', 'Color2', 'Color3','Age','State','ImageId'],axis=1)\n\nfor col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max',\n           'blur_min','blur_max','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n           'Description_Length','Name_Length','huMoments0','huMoments1','huMoments2','huMoments3','huMoments4','huMoments5','huMoments6']:\n    train_analysis[col].fillna((train_analysis[col].median()), inplace=True)\n    \n# replacing na values with No Color \ntrain_analysis[\"ColorName2\"].fillna(\"No Color\", inplace = True) \n\n# Cannot be used for this analysis (IDs, Texts...)\ntest_analysis = test.drop([\"Name\",\"Description\",\"BreedName2\",\"ColorName3\",'Name','Breed1','Breed2','RescuerID','Description',\n                            'BreedName1','Color1', 'Color2', 'Color3','Age','State'],axis=1)\n\nfor col in ['sentiment_document_score', 'sentiment_document_magnitude','metadata_topicality_max','metadata_topicality_mean','metadata_topicality_min',\n           'metadata_topicality_0_mean','metadata_topicality_0_max','metadata_topicality_0_min','L_metadata_0_dog_sum',\n           'L_metadata_0_cat_sum','L_metadata_any_dog_sum','L_metadata_any_cat_sum','pixel_mean','pixel_min','pixel_max',\n           'blur_min','blur_max','blur_mean','metadata_color_pixelfrac_mean','metadata_color_pixelfrac_min',\n           'metadata_color_pixelfrac_max','metadata_color_score_mean','metadata_color_score_min','metadata_color_score_max',\n           'Description_Length','Name_Length','huMoments0','huMoments1','huMoments2','huMoments3','huMoments4','huMoments5','huMoments6']:\n    test_analysis[col].fillna((test_analysis[col].median()), inplace=True)\n    \n# replacing na values with No Color \ntest_analysis[\"ColorName2\"].fillna(\"No Color\", inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b204491c30af338d84706303f2b32684bb3c60c"},"cell_type":"markdown","source":"## Categorical Encoding"},{"metadata":{"trusted":true,"_uuid":"a2fc9872208a533dce33dd8b8d88abacd9b958c6"},"cell_type":"code","source":"#Label Encoding Breed\n#One Hot Encoding: ColorName1,ColorName2,StateName\ntrain_analysis = pd.concat([train_analysis.drop('StateName', axis=1),pd.get_dummies(train_analysis['StateName'], prefix='State')], axis=1)\n\ncol=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\nfor i in col:\n    train_analysis = pd.concat([train_analysis.drop(i, axis=1),pd.get_dummies(train_analysis[i], prefix=i)], axis=1)\n    #Label Encoding Breed\n#One Hot Encoding: ColorName1,ColorName2,StateName\ntest_analysis = pd.concat([test_analysis.drop('StateName', axis=1),pd.get_dummies(test_analysis['StateName'], prefix='State')], axis=1)\n\ncol=['ColorName1','ColorName2','Health', 'Gender', 'Dewormed','Type','MaturitySize', 'Sterilized','Vaccinated','FurLength']\nfor i in col:\n    test_analysis = pd.concat([test_analysis.drop(i, axis=1),pd.get_dummies(test_analysis[i], prefix=i)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f64cbea3d4993cb5044dd2e26a66fd8224b25ce5"},"cell_type":"code","source":"len(test_analysis)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caa645e61ee069fea838e9e28605d56d43d3bb6b"},"cell_type":"markdown","source":"# Modelisation"},{"metadata":{"trusted":true,"_uuid":"4526d7f70ff12a45f95884a990bafc4df1e7c3b7"},"cell_type":"code","source":"# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    \n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6843fcf15ee3efd184b4980a1f8faf1e413b5f0f"},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.6, 1.7, 2.6, 3.6]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a3098cdca39418905b38c6a6aef4f8a43b1a833"},"cell_type":"markdown","source":"## Performance of the model"},{"metadata":{"trusted":true,"_uuid":"8e1429691d5aa771b99b201b69ca97edce72da76"},"cell_type":"code","source":"def evaluate(y_pred, y_true):\n  \n    cohen_kappa= cohen_kappa_score(y_true, y_pred)\n    accuracy=accuracy_score(y_true,y_pred)\n    f1=f1_score(y_true,y_pred,average='micro')\n    classification=classification_report(y_true,y_pred)\n    \n    #Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(20,6))\n    \n    sns.heatmap(cm, annot=True)\n    plt.title('Confusion matrix')\n    plt.figure(figsize = (5,4))\n    plt.show()\n    #Evaluation Metrics\n    print('Cohen Kappa: {:0.2f}.'.format(cohen_kappa))\n    print('Accuracy Score: {:0.2f}%.'.format(accuracy))\n    print('F1 Score: {:0.2f}%.'.format(f1))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f2d053446132fec24176c9b54a47ccfcd793bf8"},"cell_type":"code","source":"#Extracting Features and Output\nids=train_analysis[['PetID']]\ntrain_analysis=train_analysis.drop(['PetID'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e969c372aec762b4ccd1353f9454448effe1955"},"cell_type":"code","source":"X, y = train_analysis.loc[:, train_analysis.columns != 'AdoptionSpeed'], train_analysis['AdoptionSpeed']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2fc888b5d07611aa12d622c7d5b686d4bda9d7e"},"cell_type":"code","source":"model = lgb.LGBMRegressor()\nmodel.fit(X_train, y_train)\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 17))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\n\nfeatures_selection = SelectFromModel(model, threshold='1.25*median') # The Threshold is the median of features importance*1.25 \nfeatures_selection.fit(X_train, y_train)\n\nfeatures_selection_support = features_selection.get_support()\nfeatures_selection = X_train.loc[:,features_selection_support].columns.tolist()\nfeatures_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"740da49f582d1acd650f84361245f1a9fc9a9610"},"cell_type":"code","source":"X_train =X_train.loc[:,features_selection]\nX_test = X_test.loc[:,features_selection]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b031bf4fdad5933f247bd201cc1e62f9b57f9b5b"},"cell_type":"markdown","source":"## Oversampling"},{"metadata":{"trusted":true,"_uuid":"37c014d06d435cfaa5aebc51b4e4bb474ca5ca3b"},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y_train))\n\nsampling_strategy= {4: 3148, 2: 3028, 3: 2444, 1: 2317, 0: 1500}\nros = RandomOverSampler(sampling_strategy= sampling_strategy, random_state=42)\n\nX_res, y_res = ros.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddbb1f31bafe92781950f129e40e86106db12fd2"},"cell_type":"code","source":"from sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scaled = min_max_scaler.fit_transform(X_train)\nX_res_scaled = min_max_scaler.fit_transform(X_res)\n\n# Instanciate a PCA object for the sake of easy visualisation\npca = PCA(n_components=2)\n# Fit and transform x to visualise inside a 2D feature space\nX_vis = pca.fit_transform(X_scaled)\nX_res_vis = pca.transform(X_res_scaled)\n\nplt.figure()\n\n# sp1\nplt.subplot(121)\nplt.scatter(X_vis[y_train == 0, 0],  X_vis[y_train == 0, 1],  c=\"navy\", alpha=0.5,label=\"Class 0\")\nplt.legend(loc='upper left')\n\nplt.subplot(122)\nplt.scatter(X_res_vis[y_res == 0, 0],  X_res_vis[y_res == 0, 1],  c=\"navy\", alpha=0.5,label=\"Class 0\")\n\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22bf2dcf2101de0b16a144a0d849899fffa3d90"},"cell_type":"markdown","source":"## Cross Validation"},{"metadata":{"trusted":true,"_uuid":"f49c310de752048e19184b0b04eeefade8fc262c"},"cell_type":"code","source":"def cross_val(model,X_train,y_train):\n    X = X_train\n    y = y_train\n    coeff = np.empty((1,4))\n    cv_scores=[]\n    fold=1\n    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    print(skf.get_n_splits(X, y))\n\n    for train_index, val_index in skf.split(X, y):\n        xtrain, xvalid = X[train_index], X[val_index]\n        ytrain, yvalid = y[train_index], y[val_index]\n\n        model.fit(\n            xtrain, ytrain,\n            eval_set=[(xvalid, yvalid)],\n            verbose=100,\n            early_stopping_rounds=100\n        )\n\n        #model.fit(xtrain, ytrain)\n        valid_preds = model.predict(xvalid, num_iteration=model.best_iteration_)\n        yvalid = np.array(yvalid).tolist()\n        optR = OptimizedRounder()\n        optR.fit(valid_preds, yvalid)\n\n        coefficients = optR.coefficients()\n        valid_p = optR.predict(valid_preds, coefficients)\n\n        scr = quadratic_weighted_kappa(yvalid, valid_p)\n        cv_scores.append(scr)\n\n        print(\"QWK = {}. Coef = {}\".format(scr, coefficients))\n        coefficients.reshape((4, 1))\n\n        coeff = np.vstack([coeff, coefficients])\n        fold += 1\n\n\n    coeff = np.delete(coeff, (0), axis=0)\n    global coefficient_mean\n    coefficient_mean = coeff.mean(axis=0)\n    print(\"Coef Mean ={}\".format(coefficient_mean))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9242bc2b37fb274c51dce5fe46ac0f7074da12aa"},"cell_type":"markdown","source":"## LightGBM: Optimize the boundaries"},{"metadata":{"trusted":true,"_uuid":"027cb7b8851221dbbd6109bf5909607be16731ab"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2018\n}\n    \nlgb_model = lgb.LGBMRegressor(**lgb_params)\n\ncross_val(lgb_model,X_res,y_res)\n\n#Prediction\ny_pred=lgb_model.predict(X_train.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e8fead6194aa0ae6502ea931c6098a17db56253"},"cell_type":"code","source":"coefficient_mean = [0.67,1.9, 2.2, 2.7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7306e11fd5832607bf8dfdcf4c8ae9d5f88c66a"},"cell_type":"code","source":"optR=OptimizedRounder()\ny_true = pd.DataFrame(y_train)\ny_true.reset_index(inplace=True,drop=False)\n\npredictions = optR.predict(y_pred, coefficient_mean).astype(int)\npred_lgb = pd.concat([pd.DataFrame(y_pred),y_true,pd.DataFrame(predictions)],axis=1,ignore_index=True)\npred_lgb.columns = ['y_pred','index','y_true','y_pred_class']\n\npred_lgb['y_pred_class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eaa98f27f0a0c7aee19ec06903092aa2847debb"},"cell_type":"code","source":"evaluate(pred_lgb['y_pred_class'].tolist(), pred_lgb['y_true'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41748becf5a715f9d5da884e76c2d7b039184f6a"},"cell_type":"code","source":"y_test = pd.DataFrame(y_test)\ny_test.reset_index(inplace=True,drop=True)\n\ny_pred_test=lgb_model.predict(X_test.values)\npredictions_test = optR.predict(y_pred_test, coefficient_mean).astype(int)\n\npred_lgb = pd.concat([pd.DataFrame(y_pred_test),y_test,pd.DataFrame(predictions_test)],axis=1,ignore_index=True)\npred_lgb.columns = ['y_pred','y_true','y_pred_class']\npred_lgb['y_pred_class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d0c428632582a73b31360daddd0ff2012210653"},"cell_type":"code","source":"evaluate(pred_lgb['y_pred_class'].tolist(), pred_lgb['y_true'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56e3a05ea27496293a64df1495dd382d64300dd6"},"cell_type":"markdown","source":"## LightGBM 2"},{"metadata":{"trusted":true,"_uuid":"9fcbe90b1b8ceaafc3cc882ea00f2e9f93439566"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2001\n}\nlgb_model2 = lgb.LGBMRegressor(**lgb_params)\n\nlgb_model2.fit(X_res, y_res)\n\n#Prediction\ny_pred=lgb_model2.predict(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdde36773889f1dd24cc871d37f76e3b61abcaa5"},"cell_type":"code","source":"predictions2 = optR.predict(y_pred, coefficient_mean).astype(int)\n\npred_lgb2 = pd.concat([pd.DataFrame(y_pred),y_test,pd.DataFrame(predictions2)],axis=1,ignore_index=True)\npred_lgb2.columns = ['y_pred','y_true','y_pred_class']\npred_lgb2['y_pred_class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a66cdcc50b692e79bc8f82862bd7a089df63fb4"},"cell_type":"code","source":"evaluate(pred_lgb2['y_pred_class'].tolist(), pred_lgb2['y_true'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bf19c5875cca703f6409e1e0abbcbd2f2f59a96"},"cell_type":"markdown","source":"## LightGBM 3"},{"metadata":{"trusted":true,"_uuid":"78e6962e5b56e3fbf43f0787505af016a447a665"},"cell_type":"code","source":"lgb_params = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'learning_rate': 0.005,\n'subsample': .8,\n'colsample_bytree': 0.8,\n'min_split_gain': 0.006,\n'min_child_samples': 150,\n'min_child_weight': 0.1,\n'n_estimators': 1000,\n'num_leaves': 80,\n'silent': -1,\n'verbose': -1,\n'max_depth': 11,\n'random_state': 2000\n}\n   \nlgb_model3 = lgb.LGBMRegressor(**lgb_params)\n\nlgb_model3.fit(X_res, y_res)\n\n#Prediction\ny_pred=lgb_model3.predict(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb80018030993da13b531bce199eb7b1aeef1ce"},"cell_type":"code","source":"predictions3 = optR.predict(y_pred, coefficient_mean).astype(int)\n\npred_lgb3 = pd.concat([pd.DataFrame(y_pred),y_test,pd.DataFrame(predictions3)],axis=1,ignore_index=True)\npred_lgb3.columns = ['y_pred','y_true','y_pred_class']\npred_lgb3['y_pred_class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2aa28f48db21fb70762d8df78fafed38740eac7"},"cell_type":"code","source":"evaluate(pred_lgb3['y_pred_class'].tolist(), pred_lgb3['y_true'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76ac2140dc50ab312bb62b18e4227a85e66bba84"},"cell_type":"code","source":"optR=OptimizedRounder()\n# Generate submission:\n#Extracting Features and Output\nids=test[['PetID']]\ntest_features=test.drop(['PetID'],axis=1)\ntest_features =test_features.loc[:,features_selection]\n\npred1 = lgb_model.predict(test_features.values)\npred2 = lgb_model2.predict(test_features.values)\npred3 = lgb_model3.predict(test_features.values)\n\ncombination = pd.concat([pd.DataFrame(pred1),pd.DataFrame(pred2),pd.DataFrame(pred3)],axis=1,ignore_index=True)\ncombination['predn']=combination.iloc[:,0:2].mean(axis=1)\n\npredictions = optR.predict(combination['predn'], coefficient_mean).astype(int)\n#predictions = optR.predict(pred1, coefficient_mean).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b86a877192e7e9a8898573e8a41c15125e48fc3"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': ids['PetID'].values, 'AdoptionSpeed': predictions.astype(np.int32)})\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad1f142d99bba7fcc5ecdb0f8435c91893de048"},"cell_type":"code","source":"len(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dda5a3ddbd40503106c311d75ba5e2e3da95b08f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}