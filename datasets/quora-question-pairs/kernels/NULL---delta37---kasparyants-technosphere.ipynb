{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0d518ac-e53c-27fd-f314-f36422695162"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01923eb1-a0e4-56e7-a7ac-f055365da02c"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('../input/train.csv')\n",
        "df_test = pd.read_csv('../input/test.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2c41719-c2d6-e663-2e06-42f1de89fcf7"
      },
      "outputs": [],
      "source": [
        "#\u0427\u0438\u0441\u0442\u043a\u0430\n",
        "df_train.question1 = df_train.question1.map(lambda x : str(x).lower())\n",
        "df_train.question2 = df_train.question2.map(lambda x : str(x).lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff8da054-3f12-19e6-32de-470b6f5c30a1"
      },
      "outputs": [],
      "source": [
        "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
        "test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c998234d-6246-7023-b74b-a19f309f3bfa"
      },
      "outputs": [],
      "source": [
        "train_qs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ae076383-7ed6-07eb-db4e-1091095e301f"
      },
      "source": [
        "\u041f\u043e\u043d\u044f\u0442\u043d\u043e, \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043e\u0442\u0431\u0438\u0440\u0430\u0442\u044c \u0442\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u043e\u0441\u0432\u0435\u0447\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0434\u0432\u0435 \u0433\u0430\u0443\u0441\u0441\u0438\u0430\u043d\u044b \u0441 \u043d\u0435\u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0449\u0438\u043c\u0438 \u0446\u0435\u043d\u0442\u0440\u0430\u043c\u0438 (\u0438\u043b\u0438 \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u043d\u0430 \u0441 \u0448\u0443\u043c\u043e\u043c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "779f6543-ef3e-feb0-b3c8-6c2caa4eea02"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def WordMatch(row):\n",
        "    q1 = set(str(row['question1']).split()).difference(stops)\n",
        "    q2 = set(str(row['question2']).split()).difference(stops)\n",
        "    \n",
        "    if len(q1) == 0 or len(q2) == 0:\n",
        "        return 0\n",
        "    \n",
        "    inter1 = q1.difference(q2)\n",
        "    inter2 = q2.difference(q1)\n",
        "    return (len(inter1) + len(inter2))/(len(q1) + len(q2) + .0)\n",
        "\n",
        "\n",
        "df_train['WordMatch'] = df_train.apply(WordMatch, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c56b9d49-fd3e-0abf-cc3e-782dccaba350"
      },
      "outputs": [],
      "source": [
        "sns.distplot(df_train[df_train.is_duplicate==0].WordMatch, kde=False)\n",
        "sns.distplot(df_train[df_train.is_duplicate==1].WordMatch, kde=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9315ba19-117a-3140-4676-4ae3b633a240"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def Jakkar(row):\n",
        "    q1 = set(str(row['question1']).split()).difference(stops)\n",
        "    q2 = set(str(row['question2']).split()).difference(stops)\n",
        "    \n",
        "    if len(q1) == 0 or len(q2) == 0:\n",
        "        return 0\n",
        "    \n",
        "    inter = q1.intersection(q2)\n",
        "    un = q1.union(q2)\n",
        "    return 1 - (len(inter))/(len(un) + .0)\n",
        "\n",
        "\n",
        "df_train['Jakkar'] = df_train.apply(Jakkar, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e402166-9aeb-ce87-0b2a-2184afe00096"
      },
      "outputs": [],
      "source": [
        "sns.distplot(df_train[df_train.is_duplicate==0].Jakkar, kde=False)\n",
        "sns.distplot(df_train[df_train.is_duplicate==1].Jakkar, kde=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7f0274cf-7a51-3f2f-8f1a-821cf13cd450"
      },
      "source": [
        "Jakkar \u0434\u0430\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0447\u043d\u044b\u0439 \u0441\u0440\u0435\u0437, \u0433\u0434\u0435 \u043d\u0430\u0447\u0438\u043d\u0430\u044e\u0442\u0441\u044f \u043d\u0435\u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u0430 WordMatch \u0434\u0430\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u043e\u0435 \u043c\u0435\u0441\u0442\u043e \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0440\u043e\u0441\u0430 \u043d\u0435\u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c27c91ac-6fde-3fc4-6ee3-3c43556117af"
      },
      "source": [
        "\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u043f\u0435\u0440\u0432\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 sklearn - TfIdfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a01fea6-3d8e-183c-0a21-de80c29b43fd"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "tfidf_txt = pd.Series(test_qs.tolist() + train_qs.tolist())\n",
        "tfidf.fit_transform(tfidf_txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c72511b-0e83-9629-ed85-75806ac62a4f"
      },
      "source": [
        "\u041f\u0440\u043e\u0441\u0442\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0444\u0438\u0447\u0438, \u043a\u0430\u043a \u0431\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0444\u0440\u0430\u0437\u0443 \u043a\u0430\u043a \u0441\u0443\u043c\u043c\u0443 \u0432\u0435\u0441\u043e\u0432 \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0432 \u043d\u0435\u0435 \u0441\u043b\u043e\u0432"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bac10232-82b0-c6da-d03c-c8cfc40b9969"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_weight(count, eps=10000, min_count=2):\n",
        "    if count < min_count:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return 1.0 / (count + eps)\n",
        "\n",
        "\n",
        "words = (\" \".join(train_qs)).lower().split()\n",
        "counts = Counter(words)\n",
        "weights = {word: get_weight(count) for word, count in counts.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "119530d9-0b47-f917-23f1-ef9ad01e2beb"
      },
      "outputs": [],
      "source": [
        "def WeightMatch(row):\n",
        "    q1 = set(str(row['question1']).split()).difference(stops)\n",
        "    q2 = set(str(row['question2']).split()).difference(stops)\n",
        "    \n",
        "    if len(q1) == 0 or len(q2) == 0:\n",
        "        return 0\n",
        "    \n",
        "    shared_weights = [weights.get(w, 0) for w in q1.intersection(q2)]\n",
        "    total_weights = [weights.get(w, 0) for w in q1.union(q2)]\n",
        "    \n",
        "    R = np.sum(shared_weights) / (np.sum(total_weights) + .0)\n",
        "    return R\n",
        "\n",
        "df_train['WeightMatch'] = df_train.apply(WeightMatch, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d4c899f-dd8e-7b69-0f7a-c86438890180"
      },
      "outputs": [],
      "source": [
        "sns.distplot(df_train[df_train.is_duplicate==0].WeightMatch, kde=False)\n",
        "sns.distplot(df_train[df_train.is_duplicate==1].WeightMatch, kde=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "29a10e7c-27fe-6293-0e71-9f6e56987f0a"
      },
      "outputs": [],
      "source": [
        "x_test = pd.DataFrame()\n",
        "\n",
        "x_test['WordMatch'] = df_test.apply(WordMatch, axis=1, raw=True)\n",
        "x_test['Jakkar'] = df_test.apply(Jakkar, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "88f9ff52-739f-e0d1-5563-65508215fc08"
      },
      "outputs": [],
      "source": [
        "x_test['WeightMatch'] = df_test.apply(WeightMatch, axis=1, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae8323cf-6a82-e714-ecb4-fb407cd72d43"
      },
      "outputs": [],
      "source": [
        "x_train = df_train.drop(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], axis=1)\n",
        "y_train = df_train['is_duplicate'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b8d44da-a86c-92f2-fabf-56c86b69b5fd"
      },
      "outputs": [],
      "source": [
        "pos_train = x_train[y_train == 1]\n",
        "neg_train = x_train[y_train == 0]\n",
        "\n",
        "# Now we oversample the negative class\n",
        "# There is likely a much more elegant way to do this...\n",
        "p = 0.165\n",
        "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_train = pd.concat([neg_train, neg_train])\n",
        "    scale -=1\n",
        "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "print(len(pos_train) / (len(pos_train) + len(neg_train)))\n",
        "\n",
        "x_train = pd.concat([pos_train, neg_train])\n",
        "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
        "del pos_train, neg_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36bb9cf4-e389-839c-cb35-59f637ad5c9c"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Set our parameters for xgboost\n",
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "params['eta'] = 0.02\n",
        "params['max_depth'] = 4\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "\n",
        "watchlist = [(d_train, 'train')]\n",
        "\n",
        "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71369237-4803-28aa-494e-a636d2a112e6"
      },
      "outputs": [],
      "source": [
        "d_test = xgb.DMatrix(x_test)\n",
        "p_test = bst.predict(d_test)\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['test_id'] = df_test['test_id']\n",
        "sub['is_duplicate'] = p_test\n",
        "sub.to_csv('simple.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f550016-8192-2ed6-b5aa-33e55733c8bb"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}