{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7448604d-56cc-5eb6-bbd1-8905bf716a60"
      },
      "source": [
        "##Predict Duplicate using basic ML + NLP techniques##\n",
        "\n",
        "I am trying to predict the duplicate sentences using vector similarity calculations and NLP technique in this module and its other forked versions.\n",
        "\n",
        "BOW + Cosine/Euclidean/Manhattan/Jaccard/Minskowiski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7d4aa51-afc3-ef96-11e6-f944b9bdda73"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "542c44bf-ec1d-ba34-7ab0-dae0bfc5e25f"
      },
      "source": [
        "##Reading train data, Cleaning##\n",
        "\n",
        "*Reading Training Data ,\n",
        "Removing duplicates , \n",
        "Removing NULL values*\n",
        "\n",
        "Using pandas read_csv command to read data from train files.  And doing some basic cleanup on the data by removing any duplicates or null values that may be present in the data. \n",
        "\n",
        "Note: Reducing the size of the data set so that the Kernel memory does not run out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "acafea10-9564-f25a-6849-d976ee90a51a"
      },
      "outputs": [],
      "source": [
        "'''from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_data():\n",
        "    df = pd.read_csv(\"../input/train.csv\", nrows=20000)\n",
        "    print (\"Shape of base training File = \", df.shape)\n",
        "    # Remove missing values and duplicates from training data\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"Shape of base training data after cleaning = \", df.shape)\n",
        "    return df\n",
        "\n",
        "df = read_data()\n",
        "df_train, df_test = train_test_split(df, test_size = 0.02)\n",
        "print (df_train.head(2))\n",
        "print (df_test.shape)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b3e20ba9-2e98-37dd-3d95-a3ebf9d9c1fb"
      },
      "source": [
        "As we can see from the above output the file contains six columns\n",
        "\n",
        "- **qid1** & **qid2**  - which contains the unique id assigned to the question\n",
        "- **question1** & **question2** - which contains the actual questions\n",
        "- **is_duplicate** - which contains information if the question1 and 2 are duplicate or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ff92a6d-8a72-cde1-24bb-093fb60fb452"
      },
      "source": [
        "## EDA ##\n",
        "\n",
        "Some EDA on the data to get a look and feel about the data. Here we are trying to see the distribution of output data. Duplicate questions available etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad969c65-aac3-f40c-4650-b50ccc143fa6"
      },
      "outputs": [],
      "source": [
        "''''''from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "\n",
        "def eda(df):\n",
        "    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n",
        "           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n",
        "    \n",
        "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
        "    \n",
        "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
        "    \n",
        "    question_ids_counter = Counter(question_ids_combined)\n",
        "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
        "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
        "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n",
        "    \n",
        "    \n",
        "eda(df_train)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "607dec70-9919-5f49-3a56-0cd23c9e9670"
      },
      "source": [
        "## Train Dictionary ##\n",
        "\n",
        "First we will tokenize the sentences to extract words from the question. Lets also apply porter stemmer to break down words into their basic form. This should help us increase the accuracy of the system.\n",
        "\n",
        "Then we use gensims to train a dictionary of words available in the corpus. We are training the dictionary based on the Bag Of Words concept. Gensims dictionary will assign a id to each word which we can use later to convert documents into vectors. \n",
        "\n",
        "Also, filter extremes to remove words appearing less than 5times in the corpus or in more than 80% of the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9"
      },
      "outputs": [],
      "source": [
        "'''import re\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "words = re.compile(r\"\\w+\",re.I)\n",
        "stopword = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize_questions(df):\n",
        "    question_1_tokenized = []\n",
        "    question_2_tokenized = []\n",
        "\n",
        "    for q in df.question1.tolist():\n",
        "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    for q in df.question2.tolist():\n",
        "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
        "\n",
        "    df[\"Question_1_tok\"] = question_1_tokenized\n",
        "    df[\"Question_2_tok\"] = question_2_tokenized\n",
        "    \n",
        "    return df\n",
        "\n",
        "def train_dictionary(df):\n",
        "    \n",
        "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
        "    \n",
        "    dictionary = corpora.Dictionary(questions_tokenized)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
        "    dictionary.compactify()\n",
        "    \n",
        "    return dictionary\n",
        "    \n",
        "df_train = tokenize_questions(df_train)\n",
        "dictionary = train_dictionary(df_train)\n",
        "print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
        "\n",
        "df_test = tokenize_questions(df_test)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "42fe02fe-bf22-fadb-b3b9-812d82e7b60a"
      },
      "source": [
        "As we can see that the number of unique words in the dictionary after filtering are 4831. \n",
        "This would be the size of each of the vector in the question set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56bed0e8-b345-d2cd-3816-1748b47c4483"
      },
      "source": [
        "##Create Vector##\n",
        "\n",
        "Here we are using the simple method of Bag Of Words Technique to convert sentences into vectors. There are two vector matrices thus created where each of the matrix is a sparse matrix to save  memory in the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6c05814-8314-8af4-cabc-b39c51a58293"
      },
      "outputs": [],
      "source": [
        "'''def get_vectors(df, dictionary):\n",
        "    \n",
        "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
        "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
        "    \n",
        "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
        "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
        "    \n",
        "    return question1_csc.transpose(),question2_csc.transpose()\n",
        "\n",
        "\n",
        "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
        "\n",
        "print (q1_csc.shape)\n",
        "print (q2_csc.shape)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0d40f181-547d-8f31-5633-847bff79dde1"
      },
      "source": [
        "As we can see each of the matrix is of size \n",
        "404288 X 30114   = > (size of the training data) X (no of words in the dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "113ed18c-6634-8ba2-f693-2521cfba2ac7"
      },
      "source": [
        "##Define Similarity Calculation Fucntions##\n",
        "\n",
        "Here we have defined various Distance calculation functions for \n",
        "\n",
        " - Cosine Distance\n",
        " - Euclidean Distance\n",
        " - Manhattan Distance\n",
        " - Jaccard Distance\n",
        " - Minkowski Distance\n",
        "\n",
        "As Eucledian, Manhattan and Minkowski Distance may go beyond 1 we must scale them down between0 - 1 , for that we are using MinMaxScaler and training them on training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b90d4fd-5c98-9f58-3e6e-1ed968b6f869"
      },
      "outputs": [],
      "source": [
        "'''from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "from sklearn.metrics.pairwise import manhattan_distances as md\n",
        "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
        "from sklearn.metrics import jaccard_similarity_score as jsc\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
        "mms_scale_man = MinMaxScaler()\n",
        "mms_scale_euc = MinMaxScaler()\n",
        "mms_scale_mink = MinMaxScaler()\n",
        "\n",
        "def get_similarity_values(q1_csc, q2_csc):\n",
        "    cosine_sim = []\n",
        "    manhattan_dis = []\n",
        "    eucledian_dis = []\n",
        "    jaccard_dis = []\n",
        "    minkowsk_dis = []\n",
        "    \n",
        "    for i,j in zip(q1_csc, q2_csc):\n",
        "        sim = cs(i,j)\n",
        "        cosine_sim.append(sim[0][0])\n",
        "        sim = md(i,j)\n",
        "        manhattan_dis.append(sim[0][0])\n",
        "        sim = ed(i,j)\n",
        "        eucledian_dis.append(sim[0][0])\n",
        "        i_ = i.toarray()\n",
        "        j_ = j.toarray()\n",
        "        try:\n",
        "            sim = jsc(i_,j_)\n",
        "            jaccard_dis.append(sim)\n",
        "        except:\n",
        "            jaccard_dis.append(0)\n",
        "            \n",
        "        sim = minkowski_dis.pairwise(i_,j_)\n",
        "        minkowsk_dis.append(sim[0][0])\n",
        "    \n",
        "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
        "\n",
        "\n",
        "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
        "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
        "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
        "print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
        "print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
        "print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
        "print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
        "\n",
        "eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
        "manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
        "minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
        "    \n",
        "manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
        "eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
        "minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
        "\n",
        "eucledian_dis = eucledian_dis_array.flatten()\n",
        "manhattan_dis = manhattan_dis_array.flatten()\n",
        "minkowsk_dis = minkowsk_dis_array.flatten()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ef529f38-e1c5-4c01-f250-65b37e8efc62"
      },
      "source": [
        "##Calculate Log Loss##\n",
        "\n",
        "Here we will use log loss formula to set a base criteria as to what accuracy our algorithm is able to achieve in terms of log loss which is the competition calucation score.\n",
        "\n",
        "We will also use Eucledian, Manhattan , Minkowski and Jaccard to calculate the similarity and then have a look at the log loss from each one of them. These are the five most widely used similarity classes used in Data Science so Lets use each one of them to see which performs best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c4e9eb2-f52a-950e-8ddb-14110f64ba7b"
      },
      "outputs": [],
      "source": [
        "''''''from sklearn.metrics import log_loss\n",
        "\n",
        "def calculate_logloss(y_true, y_pred):\n",
        "    loss_cal = log_loss(y_true, y_pred)\n",
        "    return loss_cal\n",
        "\n",
        "q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
        "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
        "y_true = df_test.is_duplicate.tolist()\n",
        "\n",
        "y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
        "y_pred_man = y_pred_man_array.flatten()\n",
        "\n",
        "y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
        "y_pred_euc = y_pred_euc_array.flatten()\n",
        "\n",
        "y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
        "y_pred_mink = y_pred_mink_array.flatten()\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_cos)\n",
        "print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_man)\n",
        "print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_euc)\n",
        "print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_jac)\n",
        "print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
        "\n",
        "logloss = calculate_logloss(y_true, y_pred_mink)\n",
        "print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6715aaba-6649-6f89-f284-63765995f61f"
      },
      "source": [
        "Although this test is run on a small set it indicates that cosine similarity is working as the best parameter for finding duplicate among sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "82d19c90-b56f-06ff-2d82-68a28dc333b5"
      },
      "source": [
        "## Adding Machine Learning Models to improve logloss accuracy ##\n",
        "\n",
        "Now in order to improve on the accuracy let us feed the results from these similarity coefficients to a Random Forest Regressor and Support Vector Regressor and check if we can improve on the log loss values.\n",
        "\n",
        "Not concentrating on the hyper parameters of RF and SVM we are just allowing the algorithms to run as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8022d9e6-5b04-9473-821c-7df290040fe4"
      },
      "outputs": [],
      "source": [
        "'''from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
        "y_train = df_train.is_duplicate\n",
        "\n",
        "X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
        "y_test = y_true\n",
        "\n",
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train,y_train)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c1b70db8-92e5-f0f9-0374-49bcd4bc8a15"
      },
      "source": [
        "Now that we have trained the model . Lets predict duplicate from models and calcualte logloss from them to check if their is any improvement in the logloss values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77cc19b8-a2a0-a3c3-2f63-df0496fdba52"
      },
      "outputs": [],
      "source": [
        "''''y_rfr_predicted = rfr.predict(X_test)\n",
        "y_svr_predicted = svr.predict(X_test)\n",
        "\n",
        "logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n",
        "logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n",
        "\n",
        "print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n",
        "print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "573a925a-ea42-132c-0b75-e51918d6828f"
      },
      "source": [
        "As we can see from the above results that we are able to bring down the logloss values to nearly **half** of what was predicted earlier using base similarity techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b5756cac-f00c-f9c8-ece2-141d9c292378"
      },
      "source": [
        "## Tuning SVR model and producing output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a34e8af-e645-cf11-0260-b726c9dfee01"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Predict Duplicate using basic ML + NLP techniques\n",
        "I am trying to predict the duplicate sentences using vector \n",
        "similarity blnCreateSubmitFile and NLP technique in this module and its other forked versions.\n",
        "'''\n",
        "from scipy.optimize import differential_evolution\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "from sklearn.metrics.pairwise import manhattan_distances as md\n",
        "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
        "from sklearn.metrics import jaccard_similarity_score as jsc\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "from sklearn.metrics import log_loss\n",
        "import re\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import SnowballStemmer\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "coding:'utf-8'\n",
        "FILE_PATH='C:\\\\Kaggle\\\\Quora\\\\QuoraComp\\\\'\n",
        "FILE_PATH=\"../input/\"\n",
        "global df_train \n",
        "global X_train \n",
        "global y_train\n",
        "#global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n",
        "global y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\n",
        "global y_true\n",
        "\n",
        "global blnCreateSubmitFile\n",
        "\n",
        "\n",
        "def calculate_logloss(y_true, y_pred):\n",
        "    loss_cal = log_loss(y_true, y_pred)\n",
        "    return loss_cal\n",
        "\n",
        "def mainPredict():\n",
        "\n",
        "    global df_train\n",
        "    global y_true\n",
        "    #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n",
        "    global y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\n",
        "    global X_train, y_train\n",
        "\n",
        "    def read_data():\n",
        "        global blnCreateSubmitFile\n",
        "        lstrFile='train.csv'\n",
        "\n",
        "        if blnCreateSubmitFile:\n",
        "            lstrFile='test.csv'\n",
        "\n",
        "        df = pd.read_csv(FILE_PATH+lstrFile) #, nrows=250000)\n",
        "        df.reindex(np.random.permutation(df.index))\n",
        "        df=df[0:70000]\n",
        "        print (\"Shape of base training File = \", df.shape)\n",
        "\n",
        "        if not blnCreateSubmitFile:\n",
        "            # Remove missing values and duplicates from training data\n",
        "            df.drop_duplicates(inplace=True)\n",
        "            df.dropna(inplace=True)\n",
        "\n",
        "        print(\"Shape of base training data after cleaning = \", df.shape)\n",
        "        return df\n",
        "\n",
        "    df = read_data()\n",
        "\n",
        "    if blnCreateSubmitFile:\n",
        "        #create submission file, use df_train variable but it holds test data file.\n",
        "        df_train =df\n",
        "        df_test =df[0:6]\n",
        "        del df\n",
        "    else:\n",
        "        df_train, df_test = train_test_split(df, test_size = 0.14)\n",
        "\n",
        "        print (df_train.head(2))\n",
        "        print (df_test.shape)\n",
        "        print (str(len(df_train))+' rows in df_train')\n",
        "\n",
        "\n",
        "    '''Train Dictionary\u00b6\n",
        "    First we will tokenize the sentences to extract words from the question. \n",
        "    Lets also apply porter stemmer to break down words into their basic form. \n",
        "    This should help us increase the accuracy of the system.\n",
        "    Then we use gensims to train a dictionary of words available in the corpus.\n",
        "    We are training the dictionary based on the Bag Of Words concept. \n",
        "    Gensims dictionary will assign a id to each word which we can use \n",
        "    later to convert documents into vectors.\n",
        "    Also, filter extremes to remove words appearing less than 5 \n",
        "    times in the corpus or in more than 80% of the questions.\n",
        "    '''\n",
        "\n",
        "\n",
        "    words = re.compile(r\"\\w+\",re.I)\n",
        "    stopword = stopwords.words('english')\n",
        "    #stemmer = PorterStemmer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    def tokenize_questions(df):\n",
        "        question_1_tokenized = []\n",
        "        question_2_tokenized = []\n",
        "\n",
        "        try:\n",
        "            for q in df.question1.tolist():\n",
        "                question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword ])\n",
        "\n",
        "            for q in df.question2.tolist():\n",
        "                question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword ])\n",
        "\n",
        "            df[\"Question_1_tok\"] = question_1_tokenized\n",
        "            df[\"Question_2_tok\"] = question_2_tokenized\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(len(question_1_tokenized))\n",
        "            print ('hit exception')\n",
        "            #print (df[78217:78218]['question1'])\n",
        "        return df\n",
        "\n",
        "    def train_dictionary(df):\n",
        "    \n",
        "        questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
        "    \n",
        "        dictionary = corpora.Dictionary(questions_tokenized)\n",
        "        dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
        "        dictionary.compactify()\n",
        "    \n",
        "        return dictionary\n",
        "    \n",
        "    df_train = tokenize_questions(df_train)\n",
        "    dictionary = train_dictionary(df_train)\n",
        "    print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
        "\n",
        "    df_test = tokenize_questions(df_test)\n",
        "\n",
        "\n",
        "    '''\n",
        "    Create Vector\n",
        "    Here we are using the simple method of Bag Of Words Technique\n",
        "    to convert sentences into vectors.\n",
        "    There are two vector matrices thus created where each of\n",
        "    the matrix is a sparse matrix to save memory in the system.\n",
        "    '''\n",
        "    def get_vectors(df, dictionary):\n",
        "    \n",
        "        question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
        "        question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
        "    \n",
        "        question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
        "        question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
        "    \n",
        "        return question1_csc.transpose(),question2_csc.transpose()\n",
        "\n",
        "\n",
        "    q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
        "\n",
        "    print (q1_csc.shape)\n",
        "    print (q2_csc.shape)\n",
        "\n",
        "    '''\n",
        "    Define Similarity Calculation Functions:\n",
        "    Here we have defined various Distance calculation functions for\n",
        "    Cosine Distance\n",
        "    Euclidean Distance\n",
        "    Manhattan Distance\n",
        "    Jaccard Distance\n",
        "    Minkowski Distance\n",
        "    As Eucledian, Manhattan and Minkowski Distance \n",
        "    may go beyond 1 we must scale them down between0 - 1 ,\n",
        "    for that we are using MinMaxScaler and training them on training data.\n",
        "    '''\n",
        "    minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
        "    mms_scale_man = MinMaxScaler()\n",
        "    mms_scale_euc = MinMaxScaler()\n",
        "    mms_scale_mink = MinMaxScaler()\n",
        "\n",
        "    def get_similarity_values(q1_csc, q2_csc):\n",
        "        #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n",
        "        cosine_sim = []\n",
        "        manhattan_dis = []\n",
        "        eucledian_dis = []\n",
        "        jaccard_dis = []\n",
        "        minkowsk_dis = []\n",
        "    \n",
        "        for i,j in zip(q1_csc, q2_csc):\n",
        "            sim = cs(i,j)\n",
        "            cosine_sim.append(sim[0][0])\n",
        "            sim = md(i,j)\n",
        "            manhattan_dis.append(sim[0][0])\n",
        "            sim = ed(i,j)\n",
        "            eucledian_dis.append(sim[0][0])\n",
        "            i_ = i.toarray()\n",
        "            j_ = j.toarray()\n",
        "            try:\n",
        "                sim = jsc(i_,j_)\n",
        "                jaccard_dis.append(sim)\n",
        "            except:\n",
        "                jaccard_dis.append(0)\n",
        "            \n",
        "            sim = minkowski_dis.pairwise(i_,j_)\n",
        "            minkowsk_dis.append(sim[0][0])\n",
        "    \n",
        "        return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
        "\n",
        "\n",
        "    # cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
        "    cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
        "    print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
        "    print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
        "    print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
        "    print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
        "    print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
        "\n",
        "    eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
        "    manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
        "    minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
        "    \n",
        "    manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
        "    eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
        "    minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
        "\n",
        "    eucledian_dis = eucledian_dis_array.flatten()\n",
        "    manhattan_dis = manhattan_dis_array.flatten()\n",
        "    minkowsk_dis = minkowsk_dis_array.flatten()\n",
        "\n",
        "    '''\n",
        "    Calculate Log Loss\u00b6\n",
        "    Here we will use log loss formula to set a base criteria as\n",
        "    to what accuracy our algorithm is able to achieve in terms \n",
        "    of log loss which is the competition calucation score.\n",
        "    We will also use Eucledian, Manhattan ,\n",
        "    Minkowski and Jaccard to calculate the similarity and then\n",
        "    have a look at the log loss from each one of them. These are\n",
        "    the five most widely used similarity classes used in Data Science\n",
        "    so Lets use each one of them to see which performs best.\n",
        "    '''\n",
        "\n",
        "    q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
        "    y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
        "    \n",
        "    if not blnCreateSubmitFile:\n",
        "        y_true = df_test.is_duplicate.tolist()\n",
        "\n",
        "    y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
        "    y_pred_man = y_pred_man_array.flatten()\n",
        "\n",
        "    y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
        "    y_pred_euc = y_pred_euc_array.flatten()\n",
        "\n",
        "    y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
        "    y_pred_mink = y_pred_mink_array.flatten()\n",
        "\n",
        "    if not blnCreateSubmitFile:\n",
        "        logloss = calculate_logloss(y_true, y_pred_cos)\n",
        "        print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
        "\n",
        "        logloss = calculate_logloss(y_true, y_pred_man)\n",
        "        print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
        "\n",
        "        logloss = calculate_logloss(y_true, y_pred_euc)\n",
        "        print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
        "\n",
        "        logloss = calculate_logloss(y_true, y_pred_jac)\n",
        "        print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
        "\n",
        "        logloss = calculate_logloss(y_true, y_pred_mink)\n",
        "        print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)\n",
        "\n",
        "    X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
        "    \n",
        "    if not blnCreateSubmitFile:\n",
        "        y_train = df_train.is_duplicate\n",
        "\n",
        "def fnRunSVR(*pArgs):\n",
        "    '''\n",
        "    Adding Machine Learning Models to improve logloss accuracy\n",
        "    Now in order to improve on the accuracy let us feed the\n",
        "    results from these similarity coefficients to a Random Forest Regressor\n",
        "    and Support Vector Regressor and check if we can improve on the log loss values.\n",
        "    Not concentrating on the hyper parameters of RF and SVM we \n",
        "    are just allowing the algorithms to run as it is.\n",
        "    '''\n",
        "    print (pArgs)\n",
        "    pC=int(pArgs[0][0])\n",
        "    pGamma=pArgs[0][1]\n",
        "    global blnCreateSubmitFile\n",
        "    global df_train\n",
        "    #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n",
        "    global y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\n",
        "    global y_true\n",
        "    global X_train \n",
        "    global y_train\n",
        "    \n",
        "    X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
        "    \n",
        "    if not blnCreateSubmitFile:\n",
        "        y_test = y_true\n",
        "\n",
        "    #rfr = RandomForestRegressor()\n",
        "    #rfr.fit(X_train, y_train)\n",
        "\n",
        "    if not blnCreateSubmitFile:\n",
        "        svr = SVR(C=pC, gamma=pGamma)\n",
        "        print ('training SVR model')\n",
        "        svr.fit(X_train,y_train)\n",
        "        joblib.dump(svr, 'Quora_SVR.pkl') \n",
        "        print ('saved SVR to pickle')\n",
        "    else:\n",
        "        #load from pickle saved file\n",
        "        svr = joblib.load('Quora_SVR.pkl') \n",
        "\n",
        "    '''\n",
        "    Now that we have trained the model . \n",
        "    Lets predict duplicate from models and calculate logloss\n",
        "    from them to check if their is any improvement in the logloss values.\n",
        "    '''\n",
        "    #y_rfr_predicted = rfr.predict(X_test)\n",
        "    if not blnCreateSubmitFile:\n",
        "        y_svr_predicted = svr.predict(X_test)\n",
        "\n",
        "        #logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n",
        "        logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n",
        "        print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)\n",
        "    else:\n",
        "        y_svr_predicted =svr.predict(X_train)\n",
        "    #print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n",
        "    \n",
        "    \n",
        "    if blnCreateSubmitFile:\n",
        "        df_train['is_duplicate']=y_svr_predicted\n",
        "        df_train['is_duplicate'] =np.maximum(0,df_train['is_duplicate'] )\n",
        "        #submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':y_svr_predicted})\n",
        "        df_train[['test_id','is_duplicate']].to_csv('QuoraSubmission.csv', index=False)\n",
        "\n",
        "    return logloss_svr\n",
        "\n",
        "if __name__=='__main__':\n",
        "    blnCreateSubmitFile=False\n",
        "    mainPredict()\n",
        "    lBounds=[(1,100),(.000000001,.25)]\n",
        "    fnRunSVR([  5.42448063e+01,   2.37859253e-02])\n",
        "    #result=differential_evolution(func=fnRunSVR,bounds=lBounds,disp=1)\n",
        "    #print(result)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}