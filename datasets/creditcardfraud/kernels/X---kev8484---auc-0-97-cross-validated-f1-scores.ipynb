{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "38541fd0-854b-b5ae-dd1d-d5a181160c1c"
      },
      "source": [
        "# Import Basic Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5fb6410-9e53-24bf-8f8e-0fd19e110c62"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e628c172-6edc-c818-1532-e787d782d612"
      },
      "source": [
        "# 1. Data Input, Preparation, & Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "478defd6-4f2f-31bf-8001-ad4e8fd11798"
      },
      "source": [
        "## 1.1 Read in Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "256f3663-58ec-121d-a1ae-f345bbfb9c6d"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"../input/creditcard.csv\")\n",
        "print(\"No. of Rows: \\t\\t\", data.shape[0])\n",
        "print(\"No. of Columns: \\t\", data.shape[1])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "302cfc01-d0c3-ddf7-a897-73d3cca6dc73"
      },
      "source": [
        "## 1.2 Class Distribution\n",
        "\n",
        "As described in the Credit Card Fraud summary, this is a highly unbalanced dataset.  Check out the histogram and class counts below (Class 0 for normal, Class 1 for fraudulent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7d5ce17-5636-39fd-e257-787010f848f1"
      },
      "outputs": [],
      "source": [
        "norm_count = data[data['Class'] == 0].shape[0] # Normal transactions\n",
        "fraud_count = data[data['Class'] == 1].shape[0] # Fraudulent transcations\n",
        "total_count = data.shape[0]\n",
        "print(\"No. of normal transactions: \\t\\t\", norm_count)\n",
        "print(\"No. of fraudulent transactions: \\t\", fraud_count)\n",
        "print(\"% normal transactions: \\t\\t\", norm_count/total_count * 100)\n",
        "print(\"% fraudulent transcations: \\t\", fraud_count/total_count * 100)\n",
        "pd.value_counts(data['Class'], sort = True).sort_index().plot(kind='bar')\n",
        "plt.title(\"Class Histogram\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "842e8fc6-56e2-1457-4edc-ab92e4973f5f"
      },
      "source": [
        "## 1.3 Standardize Input Data\n",
        "\n",
        "Before we tackle the class imbalance, let's bring the transaction dollar amount onto a standard scale (i.e. mean = 0, std = 1.0).  This is a common preprocessing step.  The other feature variables have been outputted from Principal Component Analysis so we can leave them alone.  Additionally, we'll drop the unscaled dollar amount column and the time column.  A time series analysis would be interesting, but we don't have information for which account is making a particular transaction so the time column won't be very helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f6f35ec1-b928-ac06-da74-4227c584a6f8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data['Amount_scl'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
        "data = data.drop(['Time','Amount'],axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd464857-7a67-7b12-e682-12911cfe145e"
      },
      "outputs": [],
      "source": [
        "X = data.ix[:, data.columns != 'Class'] # features\n",
        "y = data['Class'] # labels\n",
        "print(\"X.shape: \", X.shape)\n",
        "print(\"y.shape: \", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2779886c-d420-e1fa-be0b-bed2aa09a2d0"
      },
      "source": [
        "## 1.4 Split Entire Dataset into Train-Test Sets\n",
        "\n",
        "Next, we will break up the data into training and test sets.  The test set will be untouched until the final evaluation of each model so we have an unbiased estimate of model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78ffa6bc-ab94-7c24-9d76-bd0e03d8bfc6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 70% training data, 30% testing data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fb839282-f238-85e1-d33d-25f4aa9569f1"
      },
      "source": [
        "## 1.5 Undersample Training Data to Create Balanced Class Distributions\n",
        "\n",
        "Now we will handle the skewed class distribution using a technique known as undersampling:\n",
        "\n",
        " 1.  Get the number of fraudulent transactions and the indices of the corresponding rows in our training data\n",
        " 2. Get the indices of normal transactions in our training data\n",
        " 3. Take a random sample of normal transactions with a sample size equal to the number of fraudulent transactions\n",
        " 4. Combine the fraudulent transactions and the random sample of normal transactions to get balanced training data set (50% fraud, 50% normal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "22fea321-d877-f4e1-8497-aa8447ac0d43"
      },
      "outputs": [],
      "source": [
        "# Get the indices of the fraudulent and normal classes:\n",
        "fraud_idx = np.array(y_train[y_train == 1].index)\n",
        "num_fraud = len(fraud_idx)\n",
        "normal_idx = y_train[y_train == 0].index\n",
        "\n",
        "# From the normal indices, sample a random subset (subset size = # of frauds):\n",
        "normal_idx_sample = np.random.choice(normal_idx, num_fraud, replace=False)\n",
        "normal_idx_sample = np.array(normal_idx_sample)\n",
        "\n",
        "# Group together our normal and fraud indices:\n",
        "# (we'll have a balanced class distribution, 50% normal, 50% fraud)\n",
        "undersample_idx = np.concatenate([fraud_idx,normal_idx_sample])\n",
        "\n",
        "# Grab the records at the indices:\n",
        "undersample_data = data.iloc[undersample_idx,:]\n",
        "\n",
        "# Split into features and labels:\n",
        "X_undersample = undersample_data.ix[:, undersample_data.columns != 'Class']\n",
        "y_undersample = undersample_data['Class']\n",
        "\n",
        "norm_count = undersample_data[undersample_data['Class'] == 0].shape[0]\n",
        "fraud_count = undersample_data[undersample_data['Class'] == 1].shape[0]\n",
        "\n",
        "print(\"---Undersampled Data Set---\")\n",
        "print(\"No. of normal transactions: \\t\", norm_count)\n",
        "print(\"No. of fraudulent transactions: \\t\\t\", fraud_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0fe1f55-5163-5229-eafd-d96215f417df"
      },
      "source": [
        "# 2. Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "912bd6a7-b37c-b22a-723a-02ce696b9876"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import auc,roc_auc_score,roc_curve,recall_score,f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d42b86c-ced4-6952-002b-56c041e9c5fc"
      },
      "source": [
        "## 2.1 Tune Hyperparameters with Cross Validation\n",
        "\n",
        "Here we'll search the hyperparameter space and use 5-fold cross validation on the undersampled training data to output accuracy, recall, and F1 metrics.  The goal is to find the best combination of hyperparameters that gives the highest F1-score.  The cross validation step here further subdivides the training data into two smaller components: (1) training subset and (2) validation subset.  The training subset is used to train the model on with a pair of hyperparameters, and the validation subset is used to score the trained model.  5-fold CV means we repeat this process of splitting up the data 5 different times to get a more accurate score.  \n",
        "\n",
        "Once we identify the pair of hyperparameters with the highest score, we will use them to train the model on the full training data (i.e. training subset + validation subset).  Keep in mind, we still haven't touched the test dataset from Section 1.4.  We will save that for final model evaluation to get an unbiased result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb202318-c46c-d889-4693-196003360b78"
      },
      "outputs": [],
      "source": [
        "def get_best_hypers_lr(X, y):\n",
        "    \"\"\" Search parameter space for the optimal values.\n",
        "    \n",
        "             -Perform Logistic Regression using a range of C parameter values and two different\n",
        "             penalty terms (L1 & L2)\n",
        "             -Compute mean recall,accuracy and f1-scores using kfold cross validation \n",
        "             for each run\n",
        "             -Output the C parameter and penalty term with the best f1 score\n",
        "    \"\"\"\n",
        "    c_range = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "    f1_max = 0\n",
        "    best_c = 0\n",
        "    penalty = ''\n",
        "    \n",
        "    for c_param in c_range:\n",
        "        print('='*25)\n",
        "        print('C parameter: ', c_param)\n",
        "        print('='*25)\n",
        "        print('')\n",
        "    \n",
        "        print('-'*25)\n",
        "        print('L1-penalty')\n",
        "        print('-'*25)\n",
        "        print('')\n",
        "        \n",
        "        lr_l1 = LogisticRegression(C=c_param, penalty='l1')\n",
        "        acc_score = cross_val_score(lr_l1, X, y, cv=5)\n",
        "        recall_score = cross_val_score(lr_l1, X, y, cv=5, scoring='recall')\n",
        "        f1_score = cross_val_score(lr_l1, X,y,cv=5, scoring='f1')\n",
        "        l1_f1=np.mean(f1_score)\n",
        "        \n",
        "        print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "        print(\"Mean Recall: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "        print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
        "        print('')\n",
        "        \n",
        "        print('-'*25)\n",
        "        print('L2-penalty')\n",
        "        print('-'*25)\n",
        "        print('')\n",
        "        \n",
        "        lr_l2 = LogisticRegression(C=c_param, penalty='l2')\n",
        "        score = cross_val_score(lr_l2, X, y, cv=5)\n",
        "        recall_score = cross_val_score(lr_l2, X, y, cv=5, scoring='recall')\n",
        "        f1_score = cross_val_score(lr_l2, X, y, cv=5, scoring='f1')\n",
        "        l2_f1 = np.mean(f1_score)\n",
        "        \n",
        "        print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "        print(\"Mean Recall: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "        print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
        "        print('')\n",
        "        \n",
        "        # compare l1_f1 & l2_f1:\n",
        "        if l2_f1 > l1_f1:\n",
        "            # compare to max:\n",
        "            if l2_f1 > f1_max:\n",
        "                f1_max = l2_f1\n",
        "                best_c = c_param\n",
        "                penalty='l2'\n",
        "        else:\n",
        "            # compare to max:\n",
        "            if l1_f1 > f1_max:\n",
        "                f1_max = l1_f1\n",
        "                best_c = c_param\n",
        "                penalty='l1'\n",
        "            \n",
        "\n",
        "    print('*'*25)\n",
        "    print('Optimal C parameter = ', best_c)\n",
        "    print('Optimal penalty = ', penalty)\n",
        "    print('Optimal F1 = ', f1_max)\n",
        "    print('*'*25)\n",
        "    \n",
        "    return best_c, penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e6ca420-fa59-8a81-3919-10fbb984ec2f"
      },
      "outputs": [],
      "source": [
        "best_c_lr, penalty_lr = get_best_hypers_lr(X_undersample,y_undersample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aab23f26-2d3b-8ffb-038e-fee04ccdc38e"
      },
      "source": [
        "## 2.2 Train Model On Undersampled Training Data & Evaluate on Full Test Data\n",
        "\n",
        "With our optimal hyperparameters found, let's set them on a fresh Linear Regression model and train it on the \"full\" training data (this is still our 50/50 balanced training set).\n",
        "\n",
        "The final step is to use the unseen test data from Section 1.4 to evaluate the model performance.  It's important to remember that the test set has a similar class imbalance as the original transaction data we started with in Section 1.2.  For evaluation, we're using Receiver Operation Characteristic (ROC) curves and Area Under the Curve (AUC) to strike a balance between model sensitivity and specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a482730-429a-4cfc-fe2f-2787cf9b26a8"
      },
      "outputs": [],
      "source": [
        "# Use best hyperparameters:\n",
        "lr = LogisticRegression(C=best_c_lr, penalty=penalty_lr)\n",
        "# Train on full undersample data set:\n",
        "lr.fit(X_undersample, y_undersample)\n",
        "# Test on unseen test data set:\n",
        "y_pred_score = lr.decision_function(X_test.values)\n",
        "# Compute ROC metrics:\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values, y_pred_score)\n",
        "# Get AUC:\n",
        "roc_auc = auc(fpr,tpr)\n",
        "\n",
        "# Plot ROC:\n",
        "plt.title('ROC Curve - Linear Regression')\n",
        "plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39f3ba8c-df8f-4f26-0d11-54abdb5c060d"
      },
      "source": [
        "# 3. Support Vector Machine Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac1cbe69-9309-54ab-9f50-40c175d4e4f6"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9f1ab31d-3884-05ea-a62c-134b668cb185"
      },
      "source": [
        "## 3.1 Tune Hyperparameters with Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41d4f774-cd1c-dd2a-7478-5c8d86599de2"
      },
      "outputs": [],
      "source": [
        "def get_best_hypers_svc(X, y):\n",
        "    \"\"\" Search parameter space for the optimal values.\n",
        "    \n",
        "             -Perform Support Vector Classifier using a range of C parameter values and two different\n",
        "             penalty terms (L1 & L2)\n",
        "             -Compute mean recall, accuracy, and f1-scores using kfold cross validation\n",
        "             for each run\n",
        "             -Output the C parameter and penalty term with the best f1-score\n",
        "    \"\"\"\n",
        "    c_range = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "    f1_max = 0\n",
        "    best_c = 0\n",
        "    penalty = ''\n",
        "    \n",
        "    for c_param in c_range:\n",
        "        print('='*25)\n",
        "        print('C parameter: ', c_param)\n",
        "        print('='*25)\n",
        "        print('')\n",
        "    \n",
        "        print('-'*25)\n",
        "        print('L1-penalty')\n",
        "        print('-'*25)\n",
        "        print('')\n",
        "        \n",
        "        svc_l1 = LinearSVC(C=c_param, penalty='l1', dual=False)\n",
        "        acc_score = cross_val_score(svc_l1, X, y, cv=5)\n",
        "        recall_score = cross_val_score(svc_l1, X, y, cv=5, scoring='recall')\n",
        "        f1_score = cross_val_score(svc_l1, X, y, cv=5, scoring='f1')\n",
        "        l1_f1 = np.mean(f1_score)\n",
        "        \n",
        "        print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "        print(\"Mean Recall: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "        print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
        "        print('')\n",
        "        \n",
        "        print('-'*25)\n",
        "        print('L2-penalty')\n",
        "        print('-'*25)\n",
        "        print('')\n",
        "        \n",
        "        svc_l2 = LinearSVC(C=c_param, penalty='l2')\n",
        "        score = cross_val_score(svc_l2, X, y, cv=5)\n",
        "        recall_score = cross_val_score(svc_l2, X, y, cv=5, scoring='recall')\n",
        "        f1_score = cross_val_score(svc_l2, X, y, cv=5, scoring='f1')\n",
        "        l2_f1 = np.mean(f1_score)\n",
        "        \n",
        "        print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "        print(\"Mean Recall: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "        print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
        "        print('')\n",
        "        \n",
        "        # compare l1_recall & l2_recall:\n",
        "        if l2_f1 > l1_f1:\n",
        "            # compare to max:\n",
        "            if l2_f1 > f1_max:\n",
        "                f1_max = l2_f1\n",
        "                best_c = c_param\n",
        "                penalty='l2'\n",
        "        else:\n",
        "            # compare to max:\n",
        "            if l1_f1 > f1_max:\n",
        "                f1_max = l1_f1\n",
        "                best_c = c_param\n",
        "                penalty='l1'\n",
        "            \n",
        "\n",
        "    print('*'*25)\n",
        "    print('Optimal C parameter = ', best_c)\n",
        "    print('Optimal penalty = ', penalty)\n",
        "    print('Optimal F1 = ', f1_max)\n",
        "    print('*'*25)\n",
        "    \n",
        "    return best_c, penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e29cbd1-f516-6059-e75c-04a863b372f3"
      },
      "outputs": [],
      "source": [
        "best_c_svc, penalty_svc = get_best_hypers_svc(X_undersample, y_undersample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "87823185-ea69-23aa-1f1d-46dd166f9d46"
      },
      "source": [
        "## 3.2 Train Model on Undersampled Training Data & Evaluate on Full Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f98f24bc-f70c-14fc-d13d-b8a8facf5e10"
      },
      "outputs": [],
      "source": [
        "# Use best hyperparameters:\n",
        "dual_svc = (penalty_svc == 'l2') # 'dual' option must be set to false if penalty is 'l1'\n",
        "svc = LinearSVC(C=best_c_svc, penalty=penalty_svc, dual=dual_svc)\n",
        "# Train on full undersample data set:\n",
        "svc.fit(X_undersample, y_undersample)\n",
        "# Test on unseen test data set:\n",
        "y_pred_score = svc.decision_function(X_test.values)\n",
        "# Compute ROC metrics:\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values, y_pred_score)\n",
        "# Get AUC:\n",
        "roc_auc = auc(fpr,tpr)\n",
        "\n",
        "# Plot ROC:\n",
        "plt.title('ROC Curve - SVC')\n",
        "plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e269cd92-dc59-a611-6ef5-0277884a4e7f"
      },
      "source": [
        "# 4. Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bef3d4f4-4138-b410-e41e-1babe77eaf12"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a8e9ff8-7a68-ded6-3178-baba32a98ca7"
      },
      "source": [
        "## 4.1 Omit Hyperparameter Tuning, Use Cross Validation to Get Mean F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8342274d-68ea-5479-a2fa-4b4bca3c08ee"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "acc_score = cross_val_score(dt, X_undersample, y_undersample, cv=5)\n",
        "recall_score = cross_val_score(dt, X_undersample, y_undersample, cv=5, scoring='recall')\n",
        "f1_score = cross_val_score(dt, X_undersample, y_undersample, cv=5, scoring='f1')\n",
        "print(\"Accuracy Score: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "print(\"Recall Score: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1cb4dc32-5f6f-686e-b3de-bbc9d02b3f60"
      },
      "source": [
        "## 4.2 Train Model on Undersampled Data & Evaluate on Full Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf67a52d-685c-c331-3048-4cb8e280a851"
      },
      "outputs": [],
      "source": [
        "# Train on full undersample data set:\n",
        "dt.fit(X_train, y_train)\n",
        "# Test on unseen test data set:\n",
        "y_pred_score = dt.predict_proba(X_test.values)[:,1]\n",
        "# Compute ROC metrics:\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values,y_pred_score)\n",
        "# Get AUC:\n",
        "roc_auc = auc(fpr, tpr)\n",
        "                         \n",
        "                                            \n",
        "# Plot ROC:\n",
        "plt.title('ROC Curve - DecisionTree')\n",
        "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0ad95cd4-8c1e-2f06-2fba-149f4be541bd"
      },
      "source": [
        "# 5. Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bf91508-e637-7923-aa9c-6290aaac1e09"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d871ac79-4584-90b3-6ebe-b7e40a7bf760"
      },
      "source": [
        "## 5.1 Omit Hyperparameter Tuning, Use Cross Validation to Get Mean F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6f3348d-b58f-8f95-67be-6529176db193"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(random_state=0)\n",
        "acc_score = cross_val_score(rfc, X_undersample, y_undersample, cv=5)\n",
        "recall_score = cross_val_score(rfc, X_undersample, y_undersample, cv=5, scoring='recall')\n",
        "f1_score = cross_val_score(rfc, X_undersample, y_undersample, cv=5, scoring='f1')\n",
        "print(\"Accuracy Score: %0.3f (+/- %0.3f)\" % (np.mean(acc_score), np.std(acc_score)) )\n",
        "print(\"Recall Score: %0.3f (+/- %0.3f)\" % (np.mean(recall_score), np.std(recall_score)) )\n",
        "print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fdc4efd8-40b2-1415-e240-09ddc56c2818"
      },
      "source": [
        "## 5.2 Train Model on Undersampled Data & Evaluate on Full Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3523c4b3-4554-3568-fd0f-558cf5ed4e3b"
      },
      "outputs": [],
      "source": [
        "# Train on full undersample data set:\n",
        "rfc.fit(X_train, y_train)\n",
        "# Test on unseen test data set:\n",
        "y_pred_score = rfc.predict_proba(X_test.values)[:,1]\n",
        "# Compute ROC metrics:\n",
        "fpr, tpr, thresholds = roc_curve(y_test.values,y_pred_score)\n",
        "# Get AUC:\n",
        "roc_auc = auc(fpr, tpr)\n",
        "                         \n",
        "                                            \n",
        "# Plot ROC:\n",
        "plt.title('ROC Curve - RandomForest')\n",
        "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}