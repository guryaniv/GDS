{"cells":[{"metadata":{"_uuid":"0c2908faf5ef69a30a16789950bd20f3b66320f3"},"cell_type":"markdown","source":"# was this competition ill conceived ?\n\nDoing some aftermath a year after this competition, i think the competition was ill conceived.  The actual problem is that cars are tested during 100seconds on their emissions, and this test is evidently function of the properties of the cars.\nThe cars come during the production in a whole mix, and this variability induces another variability. The question to minimize the emission, could rather have been: what are the key parameters influencing the test, or what are the key causes of the delays and variability ?\n\nWhat one is sure, that certain cars, and certain properties end up with a minimal 'test time'. The lowest test-time is probabily the realistic solution if the test would be executed robotically.\nWhat is also sure, is applying the robust analysis, elmininates the influence of those outliers... and this method predicts the essential realistic testing time and not the measured time..\nHence since the competition benchmarked the testing time, you could try to forecast the testing time, what we did, but was fundamentally flawed. We should have tried to forecast the 'realistic time'\n"},{"metadata":{"_uuid":"4ebe49577edb2714de370db662855a656e5e7b09"},"cell_type":"markdown","source":"# import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score\n\n\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(train.shape,test.shape)\nytrain=train['y']\n\nX0=pd.DataFrame(train.groupby('X0').min()['y'])\ntrain2=pd.merge(train,X0,how='outer',left_on='X0',right_index=True,sort=False)\ntrain2['diff']=train2['y_x']-train2['y_y']\ndifftrain=train2['diff'].sort_index()\n\n\ntrain['trte']=0\ntest['trte']=1\ntrain=train.append(test)\n\nprint(train.head(),train.shape)\n\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n   \n\n#Xtrain=Xtrain.sort_values(['y','ID'])\n\n\nXtrain=train.drop(['y','ID'],axis=1)\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b17e8fa70c9521c9d58c65eefdae1343ffc0a924"},"cell_type":"markdown","source":"# the difference between Min of each X0 group\n\nwhat explains the difference ?"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6203bc30c33b0b1d0e904fe89227c23be216ca0"},"cell_type":"code","source":"def cohen_effect_size(X, y):\n    \"\"\"Calculates the Cohen effect size of each feature.\n    \n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n        Returns\n        -------\n        cohen_effect_size : array, shape = [n_features,]\n            The set of Cohen effect values.\n        Notes\n        -----\n        Based on https://github.com/AllenDowney/CompStats/blob/master/effect_size.ipynb\n    \"\"\"\n    print(X.shape,y.shape,y.mean())\n    medi=y.mean()\n    group1, group2 = X[y<medi], X[y>=medi]\n    diff = group1.mean() - group2.mean()\n    var1, var2 = group1.var(), group2.var()\n    n1, n2 = group1.shape[0], group2.shape[0]\n    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n    d = diff / np.sqrt(pooled_var)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09b9506a879e89dd69344e2310bfd42cbbb656bf"},"cell_type":"markdown","source":"# Most siginificant features explaining the test time y\n\nhigly correlated X314,X127,X261"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"2762b7894bba1999a0071d60b6b95c7313808fd3"},"cell_type":"code","source":"\n\nexcluded_feats = [] #['SK_ID_CURR']\n\nfeatures = [f_ for f_ in Xtrain.columns if f_ not in excluded_feats]\nprint('Number of features %d' % len(features),Xtrain.shape,ytrain.shape)\n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(Xtrain[:len(ytrain)],ytrain)\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features), significant_features))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d13d674578c073f84036b9e6ef32ddec6f65d62b"},"cell_type":"markdown","source":"# Most siginificant features explaining the test time difference\n\nexplaining with relative low correlation 0.2\nOne could conclude there is no very good explanation for that variability X47, X267 are probably the best explanation"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n\nexcluded_feats = [] #['SK_ID_CURR']\n\nfeatures = [f_ for f_ in Xtrain.columns if f_ not in excluded_feats]\nprint('Number of features %d' % len(features),Xtrain.shape,ytrain.shape)\n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(Xtrain[:len(ytrain)],pd.DataFrame(difftrain).reset_index().set_index('index')['diff'])\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(50).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features2 = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.1]\nprint('Significant features %d: %s' % (len(significant_features2), significant_features2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"272895f77f57e35c352f8c2d867664a7464de9fe"},"cell_type":"markdown","source":"# Plotting most relevant features 3D\n\nplotting time spend on test, verss X0 , versus most relevant features"},{"metadata":{"trusted":true,"_uuid":"1dc019bb8d2d687b9b7bece665165b47298a095a"},"cell_type":"code","source":"from time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nfrom sklearn.utils import check_random_state\n\n# Next line to silence pyflakes.\nAxes3D\n\ncolors = np.round(np.log(train2['y_x']))#np.round(train['y'])\n\n\nx, y, z =  train['y'],train['X314'],Xtrain['X0']\n# Plot our dataset.\nfig = plt.figure(figsize=(45, 22))\nax = fig.add_subplot(251, projection='3d')\nax.scatter(x, y, z, c=colors, cmap=plt.cm.rainbow)\nax.view_init(25, -25)\n\nx, y, z = train['y'],train['X261'],Xtrain['X0']\n# Plot our dataset.\nfig = plt.figure(figsize=(45, 22))\nax = fig.add_subplot(252, projection='3d')\nax.scatter(x, y, z, c=colors, cmap=plt.cm.rainbow)\nax.view_init(25, -25)\n\nx, y, z = train['y'],train['X127'],Xtrain['X0']\n# Plot our dataset.\nfig = plt.figure(figsize=(45, 22))\nax = fig.add_subplot(253, projection='3d')\nax.scatter(x, y, z, c=colors, cmap=plt.cm.rainbow)\nax.view_init(25, -25)\n\nx, y, z = train['y'],train['X225'],Xtrain['X150']\n# Plot our dataset.\nfig = plt.figure(figsize=(45, 22))\nax = fig.add_subplot(254, projection='3d')\nax.scatter(x, y, z, c=colors, cmap=plt.cm.rainbow)\nax.view_init(25, -25)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18554adf159414dff8f4a689e66447010693a8d2"},"cell_type":"markdown","source":"# splitting data\nin a group effect A_ matrix, is forecasting the basic matrix without any variability.\n\nand an error matrix e_ is the leftover part.\n\n# first find important features"},{"metadata":{"trusted":true,"_uuid":"ac7abb19a74099bfd66ef9302f349fdf4a48dffc"},"cell_type":"code","source":"import pandas_profiling\nprofile = pandas_profiling.ProfileReport(Xtrain[significant_features])\nrejected_variables = profile.get_rejected_variables(threshold=0.95)\nselected_features = list(set(significant_features) - set(rejected_variables))\nprint(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96278a47c33eb0da4f56b49ce03d86b810c90b05"},"cell_type":"code","source":"import numpy as np\nfrom numpy.linalg import norm, svd\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import sparse_random_matrix\n#svd = TruncatedSVD(n_components=int(disc.shape[1]-1), n_iter=7, random_state=42)\n\n\ndef robustSVD(X,n_comp,lmbda=.01, tol=1e-3, maxiter=100, verbose=True):\n    svd = TruncatedSVD(n_components=n_comp, n_iter=10, random_state=42)\n    \"\"\"\n    Inexact Augmented Lagrange Multiplier\n    \"\"\"\n    Y = X\n    norm_two = norm(Y.ravel(), 2)\n    norm_inf = norm(Y.ravel(), np.inf) / lmbda\n    dual_norm = np.max([norm_two, norm_inf])\n    Y = Y / dual_norm\n    A = np.zeros(Y.shape)\n    E = np.zeros(Y.shape)\n    dnorm = norm(X, 'fro')\n    mu = 1.25 / norm_two\n    rho = 1.5\n    sv = 10.\n    n = Y.shape[0]\n    itr = 0\n    while True:\n        Eraw = X - A + (1 / mu) * Y\n        Eupdate = np.maximum(Eraw - lmbda / mu, 0) + np.minimum(Eraw + lmbda / mu, 0)\n        #U, S, V = svd(X - Eupdate + (1 / mu) * Y, full_matrices=False)\n        X_n=svd.fit_transform(X - Eupdate + (1 / mu) * Y)\n        S=svd.singular_values_\n        if itr ==0:\n            pd.DataFrame(svd.explained_variance_ratio_*100).plot()\n        svp = (S > 1 / mu).shape[0]\n        print(S.sum())\n        if svp < sv:\n            sv = np.min([svp + 1, n])\n        else:\n            sv = np.min([svp + round(.05 * n), n])\n        #Aupdate = np.dot(np.dot(U[:, :svp], np.diag(S[:svp] - 1 / mu)), V[:svp, :])\n        Aupdate = svd.inverse_transform(X_n)\n        A = Aupdate\n        E = Eupdate\n        Z = X - A - E\n        Y = Y + mu * Z\n        mu = np.min([mu * rho, mu * 1e7])\n        itr += 1\n        \n        if ((norm(Z, 'fro') / dnorm) < tol) or (itr >= maxiter):\n            u=svd.components_\n            pd.DataFrame(svd.explained_variance_ratio_*100).plot()\n            pd.DataFrame(A[:33,:20]).plot()\n            pd.DataFrame(E[:33,:20]).plot()\n            break\n    if verbose:\n        print(\"Finished at iteration %d\" % (itr))  \n    return A, E, u, S\n\nA_,e_,u_,s_=robustSVD(Xtrain[selected_features].values,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"568854cc2767fa1c384f0351f4a2fd36494f00bc"},"cell_type":"markdown","source":"# finding significant features again from the A_ matrix\nyou can see there aren't anymore highly correlated features, are they all hoover around +-0.4"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"24e1111dab3dfee1fe729af08a43e312540f1afb"},"cell_type":"code","source":"excluded_feats = [] #['SK_ID_CURR']\n\nfeatures = [f_ for f_ in Xtrain[selected_features].columns if f_ not in excluded_feats]\nprint('Number of features %d' % len(features),Xtrain.shape,ytrain.shape)\n#effect_sizes = cohen_effect_size(Xtrain[:len(ytrain)], ytrain)\neffect_sizes = cohen_effect_size(pd.DataFrame(A_,columns=Xtrain[selected_features].columns)[:len(ytrain)],ytrain)\neffect_sizes.reindex(effect_sizes.abs().sort_values(ascending=False).nlargest(30).index)[::-1].plot.barh(figsize=(6, 10));\nprint('Features with the 30 largest effect sizes')\nsignificant_features2 = [f for f in features if np.abs(effect_sizes.loc[f]) > 0.05]\nprint('Significant features %d: %s' % (len(significant_features2), significant_features2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aef6b3c849e593990e6f20dc282059755b141fa9"},"cell_type":"markdown","source":"# selecting the essential features"},{"metadata":{"trusted":true,"_uuid":"9ee2a4dcd0361e3ceaabdda6909dd8fdbcd287b5"},"cell_type":"code","source":"import pandas_profiling\nprofile = pandas_profiling.ProfileReport(pd.DataFrame(A_,columns=Xtrain[selected_features].columns))\nrejected_variables = profile.get_rejected_variables(threshold=0.95)\nselected_features3 = list(set(significant_features2) - set(rejected_variables))\nprint(selected_features3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f08e271f93fef792728beae9184a2fd975d23b00"},"cell_type":"markdown","source":"# doing gradient boost with selected features\nas you can see the influence of the outlier disappeared"},{"metadata":{"trusted":true,"_uuid":"1b885e1922ddcdf854c62d2d48f8dfc49b0aec0c"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nstart=len(train)-len(test)\nt_=pd.DataFrame(A_,columns=Xtrain[selected_features].columns)\n#X = similarities[:start]\n#Y=train[:start]['y'].round()\nprint(t_.shape)\nx_train, x_valid, y_train, y_valid = train_test_split((t_[:start])[selected_features3], ytrain[:start], test_size=0.15, random_state=4242)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix((t_[start:])[selected_features3] )\n\nparams = {}\nparams['objective'] = 'reg:linear'\nparams['eta'] = 0.17\nparams['gamma'] = 0.125\nparams['max_depth'] = 50\nparams['min_child_weight'] = 1 \nparams['num_round'] = 7\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#feval=xgb_r2_score, maximize=True,\n\nclf = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=10,  verbose_eval=10)\n\npre = pd.DataFrame()\npre['y']= y_train\npre['pre']=clf.predict(d_train)\npre.plot.scatter('y','pre')\nsub = pd.DataFrame()\nsub['ID'] = test['ID']\nsub['y'] = clf.predict(d_test)\nsub.to_csv('xgb_a.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92b463de3292eb3bf254e0c3ff67107e5d8a2010"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nstart=len(train)-len(test)\nt_=pd.DataFrame(e_,columns=Xtrain[selected_features].columns)\n#X = similarities[:start]\n#Y=train[:start]['y'].round()\nprint(t_.shape)\nx_train, x_valid, y_train, y_valid = train_test_split((t_[:start])[selected_features3], ytrain[:start], test_size=0.15, random_state=4242)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix((t_[start:])[selected_features3] )\n\nparams = {}\nparams['objective'] = 'reg:linear'\nparams['eta'] = 0.17\nparams['gamma'] = 0.125\nparams['max_depth'] = 50\nparams['min_child_weight'] = 1 \nparams['num_round'] = 7\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#feval=xgb_r2_score, maximize=True,\n\nclf = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=10,  verbose_eval=10)\n\npre = pd.DataFrame()\npre['y']= y_train\npre['pre']=clf.predict(d_train)\npre.plot.scatter('y','pre')\nsub = pd.DataFrame()\nsub['ID'] = test['ID']\nsub['y'] = clf.predict(d_test)\nsub.to_csv('xgb_e.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}