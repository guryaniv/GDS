{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "53938549-dbb6-4078-bd7f-2f48585b6252"
      },
      "source": [
        "**Part 0: Importation and preprocessing**\n",
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c3a7095a-e559-87db-a928-c35327d699e4"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "pd.options.mode.chained_assignment = None\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "#Getting data\n",
        "tweets = pd.read_csv('../input/demonetization-tweets.csv', encoding = \"ISO-8859-1\")\n",
        "display(tweets.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0a8def2-5f6f-d07d-d004-e3f99f1ba230"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#Preprocessing del RT @blablabla:\n",
        "tweets['text_new'] = ''\n",
        "tweets['tweetos'] = '' \n",
        "\n",
        "#add tweetos first part\n",
        "for i in range(len(tweets['text'])):\n",
        "    try:\n",
        "        tweets['tweetos'][i] = tweets['text'].str.split(':')[i][0]\n",
        "    except AttributeError:    \n",
        "        tweets['tweetos'][i] = 'other'\n",
        "\n",
        "#Preprocessing tweetos. select tweetos contains 'RT @'\n",
        "for i in range(len(tweets['text'])):\n",
        "    if tweets['tweetos'].str.contains('RT @')[i]  == False:\n",
        "        tweets['tweetos'][i] = 'other'\n",
        "\n",
        "#'text_new' is the feature 'text' without the tweetos    \n",
        "for i in range(len(tweets['text'])):\n",
        "    m = re.search('(?<=:)(.*)', tweets['text'][i])\n",
        "    if tweets['text'].str.contains('RT @')[i]  == True:\n",
        "        try:\n",
        "            tweets['text_new'][i]=m.group(0)\n",
        "        except AttributeError:\n",
        "            tweets['text_new'][i]=tweets['text'][i] \n",
        "    else:       \n",
        "        tweets['text_new'][i] =  tweets['text'][i]       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0dbd8cd7-9781-c56b-d035-34b48ac42cb9"
      },
      "outputs": [],
      "source": [
        "#tweets['text_new_bis'] = tweets['text_new'].str.contains(r'^https?:\\/\\/.*[\\r\\n]*')\n",
        "#for i in range(len(tweets['text'])):\n",
        "#    m =  re.split('https', tweets['text_new'][i])\n",
        "#    #tweets['text_new_bis'][i]\n",
        "#    try:\n",
        "#        print(m[1])\n",
        "#    except IndexError:  \n",
        "#        print('')\n",
        "#print(tweets['text_new_bis'][0])\n",
        "#print(tweets['text_new_bis'])\n",
        "#print(tweets['text_new'][7999])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e2a01ac8-e5f0-1e00-b789-a52ff0067f7a"
      },
      "source": [
        "**Part I: WordCloud**\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1926c9f8-1916-c59b-bb95-ac96355991af"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def wordcloud_by_province(tweets):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    stopwords.add(\"https\")\n",
        "    stopwords.add(\"00A0\")\n",
        "    stopwords.add(\"00BD\")\n",
        "    stopwords.add(\"00B8\")\n",
        "    stopwords.add(\"ed\")\n",
        "    stopwords.add(\"demonetization\")\n",
        "    stopwords.add(\"Demonetization co\")\n",
        "    #Narendra Modi is the Prime minister of India\n",
        "    stopwords.add(\"lakh\")\n",
        "    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets['text_new'].str.upper()]))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Demonetization\")\n",
        "\n",
        "wordcloud_by_province(tweets)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a6ae675c-c21f-ac86-437a-757996e1e059"
      },
      "source": [
        "**\"Since terrorists\"? \"Narenda Modi\"? Ok We must continue to investigate. I think that we must investigate separetly the tweets with the words \"terrorists\" and \"narendramodi\"** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "07934fde-6086-76c7-cb64-5a8a0515a324"
      },
      "outputs": [],
      "source": [
        "def wordcloud_by_province(tweets):\n",
        "    a = pd.DataFrame(tweets['text'].str.contains(\"terrorists\").astype(int))\n",
        "    b = list(a[a['text']==1].index.values)\n",
        "    stopwords = set(STOPWORDS)\n",
        "    stopwords.add(\"https\")\n",
        "    stopwords.add(\"terrorists\")\n",
        "    stopwords.add(\"00A0\")\n",
        "    stopwords.add(\"00BD\")\n",
        "    stopwords.add(\"00B8\")\n",
        "    stopwords.add(\"ed\")\n",
        "    stopwords.add(\"demonetization\")\n",
        "    stopwords.add(\"Demonetization co\")\n",
        "    stopwords.add(\"lakh\")\n",
        "    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets.ix[b,:]['text_new'].str.upper()]))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Tweets with word 'terrorists'\")\n",
        "\n",
        "wordcloud_by_province(tweets)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "78b0695e-22e4-aa3d-8a2d-8d6357939adf"
      },
      "source": [
        "**It is possible that The Demonitazation have a link with Kishtwar and the terrorist**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03f87c69-94d5-24b0-9d40-23e8a744a46f"
      },
      "outputs": [],
      "source": [
        "def wordcloud_by_province(tweets):\n",
        "    a = pd.DataFrame(tweets['text'].str.contains(\"narendramodi\").astype(int))\n",
        "    b = list(a[a['text']==1].index.values)\n",
        "    stopwords = set(STOPWORDS)\n",
        "    stopwords.add(\"narendramodi\")\n",
        "    stopwords.add(\"https\")\n",
        "    stopwords.add(\"00A0\")\n",
        "    stopwords.add(\"00BD\")\n",
        "    stopwords.add(\"00B8\")\n",
        "    stopwords.add(\"ed\")\n",
        "    stopwords.add(\"demonetization\")\n",
        "    stopwords.add(\"Demonetization co\")\n",
        "    stopwords.add(\"lakh\")\n",
        "    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets.ix[b,:]['text_new'].str.upper()]))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Tweets with word 'narendramodi'\")\n",
        "\n",
        "wordcloud_by_province(tweets)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d5be4bf6-74fe-ed76-4f45-c83a159047de"
      },
      "source": [
        "**I think that \"PM\" represents the word \"Prime Minister\".  The different tweets with narendramodi are supports ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f1fae3e-d902-e569-8e82-3c6bb8e7c0ce"
      },
      "source": [
        "**Part II: Timeseries plotting**\n",
        "--------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3273f889-217a-e789-7ca7-bc6ecfbfc57c"
      },
      "outputs": [],
      "source": [
        "print(tweets['retweetCount'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd69f23a-555e-1493-29fd-32f3f92eeede"
      },
      "outputs": [],
      "source": [
        "tweets['nb_words'] = 0\n",
        "for i in range(len(tweets['text'])):\n",
        "    tweets['nb_words'][i] = len(tweets['text'][i].split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b71ed0cf-555f-32af-979b-14620286dc52"
      },
      "outputs": [],
      "source": [
        "tweets['hour'] = pd.DatetimeIndex(tweets['created']).hour\n",
        "tweets['date'] = pd.DatetimeIndex(tweets['created']).date\n",
        "tweets['minute'] = pd.DatetimeIndex(tweets['created']).minute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8e13028-95d7-29bd-12a9-a07321db73ef"
      },
      "outputs": [],
      "source": [
        "tweets_hour = tweets.groupby(['hour'])['retweetCount'].sum()\n",
        "tweets_minute = tweets.groupby(['minute'])['retweetCount'].sum()\n",
        "tweets['text_len'] = tweets['text'].str.len()\n",
        "tweets_avgtxt_hour = tweets.groupby(['hour'])['text_len'].mean()\n",
        "tweets_avgwrd_hour = tweets.groupby(['hour'])['nb_words'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2679ff08-774c-d5f5-2a0b-751c680f48f3"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "tweets_hour.transpose().plot(kind='line',figsize=(6.5, 4))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('The number of retweet by hour', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fdf84c1b-0ef3-8655-7ba4-b26c75027d36"
      },
      "outputs": [],
      "source": [
        "tweets_minute.transpose().plot(kind='line',figsize=(6.5, 4))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('The number of retweet by minute', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "335fc7d3-112e-f65e-a1c3-c7b9fc88e9b1"
      },
      "outputs": [],
      "source": [
        "tweets_avgtxt_hour.transpose().plot(kind='line',figsize=(6.5, 4))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('The Average of lenght by hour', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "baea8e62-2977-8121-f1bf-451c122d449e"
      },
      "outputs": [],
      "source": [
        "tweets_avgwrd_hour.transpose().plot(kind='line',figsize=(6.5, 4))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('The Average number of words by hour', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2599dd0-353c-0890-dfbc-7863c665273c"
      },
      "outputs": [],
      "source": [
        "#print(get_corpus(tweets['text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "688b1920-4641-ca90-a581-1f3ba72ec270"
      },
      "outputs": [],
      "source": [
        "def get_stop_words(s, n):\n",
        "\t'''\n",
        "\t:s : pd.Series; each element as a list of words from tokenization\n",
        "\t:n : int; n most frequent words are judged as stop words \n",
        "\n",
        "\t:return : list; a list of stop words\n",
        "\t'''\n",
        "\tfrom collections import Counter\n",
        "\tl = get_corpus(s)\n",
        "\tl = [x for x in Counter(l).most_common(n)]\n",
        "\treturn l\n",
        "\n",
        "def get_corpus(s):\n",
        "\t'''\n",
        "\t:s : pd.Series; each element as a list of words from tokenization\n",
        "\n",
        "\t:return : list; corpus from s\n",
        "\t'''\n",
        "\tl = []\n",
        "\ts.map(lambda x: l.extend(x))\n",
        "\treturn l\n",
        "\n",
        "#freqwords = get_stop_words(tweets['text'],n=60)\n",
        "\n",
        "#freq = [s[1] for s in freqwords]\n",
        "\n",
        "#plt.title('frequency of top 60 most frequent words', bbox={'facecolor':'0.8', 'pad':0})\n",
        "#plt.plot(freq)\n",
        "#plt.xlim([-1,60])\n",
        "#plt.ylim([0,1.1*max(freq)])\n",
        "#plt.ylabel('frequency')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da0d5e37-ad0f-94c6-5724-be9d773fe95b"
      },
      "source": [
        "**Part III: Source of tweets**\n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f00c1c9-98a0-7173-8840-0408dd4ff33a"
      },
      "outputs": [],
      "source": [
        "tweets['statusSource_new'] = ''\n",
        "\n",
        "for i in range(len(tweets['statusSource'])):\n",
        "    m = re.search('(?<=>)(.*)', tweets['statusSource'][i])\n",
        "    try:\n",
        "        tweets['statusSource_new'][i]=m.group(0)\n",
        "    except AttributeError:\n",
        "        tweets['statusSource_new'][i]=tweets['statusSource'][i]\n",
        "        \n",
        "#print(tweets['statusSource_new'].head())   \n",
        "\n",
        "tweets['statusSource_new'] = tweets['statusSource_new'].str.replace('</a>', ' ', case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4fdb3e18-7cef-fbad-2172-be234ca69490"
      },
      "outputs": [],
      "source": [
        "tweets['statusSource_new'] = tweets['statusSource_new'].str.replace('</a>', ' ', case=False)\n",
        "#print(tweets[['statusSource_new','retweetCount']])\n",
        "\n",
        "tweets_by_type= tweets.groupby(['statusSource_new'])['retweetCount'].sum()\n",
        "#print(tweets_by_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f320bbcd-a3e6-79cf-5df7-ff1118a79069"
      },
      "outputs": [],
      "source": [
        "tweets_by_type.transpose().plot(kind='bar',figsize=(10, 5))\n",
        "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('Number of retweetcount by Source', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a1ed0e56-1f2f-4392-7d7b-d84025711ece"
      },
      "source": [
        "**Top 3 of Source: 1 - Twitter For Android 2 - Twitter Web Client and finally 3 - Twitter for Iphone !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98670679-5f13-0a2d-7f7d-be00d7f0a3a0"
      },
      "outputs": [],
      "source": [
        "tweets['statusSource_new2'] = ''\n",
        "\n",
        "for i in range(len(tweets['statusSource_new'])):\n",
        "    if tweets['statusSource_new'][i] not in ['Twitter for Android ','Twitter Web Client ','Twitter for iPhone ']:\n",
        "        tweets['statusSource_new2'][i] = 'Others'\n",
        "    else:\n",
        "        tweets['statusSource_new2'][i] = tweets['statusSource_new'][i] \n",
        "#print(tweets['statusSource_new2'])       \n",
        "\n",
        "tweets_by_type2 = tweets.groupby(['statusSource_new2'])['retweetCount'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "913b1d3a-c4e7-4a60-04a4-da9dd4abe630"
      },
      "outputs": [],
      "source": [
        "tweets_by_type2.rename(\"\",inplace=True)\n",
        "explode = (0, 0, 0, 1.0)\n",
        "tweets_by_type2.transpose().plot(kind='pie',figsize=(6.5, 4),autopct='%1.1f%%',shadow=True,explode=explode)\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc=6, borderaxespad=0.)\n",
        "plt.title('Number of retweetcount by Source bis', bbox={'facecolor':'0.8', 'pad':5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b3fbc407-16f3-b936-4ac1-9691075b5448"
      },
      "source": [
        "**Part IV: Clustering with Kmeans**\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0650978-d249-3b91-2440-307e93106353"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "####\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#tweets['text_sep'] = [''.join(z).strip() for z in tweets['text_new']]\n",
        "tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in tweets['text_new']]       \n",
        "####\n",
        "vectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=10,stop_words='english',use_idf=True)\n",
        "X = vectorizer.fit_transform(tweets['text_lem'].str.upper())\n",
        "print(X.shape)\n",
        "#print(tweets['text_sep'])\n",
        "#print(tweets['text_new'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0d2fca04-933f-bc4c-0bb7-e0a3d04e4410"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3fc2e97-68f7-12e4-8b55-d07f5cd8db1c"
      },
      "outputs": [],
      "source": [
        "km.fit(X)\n",
        "terms = vectorizer.get_feature_names()\n",
        "order_centroids = km.cluster_centers_.argsort()[:,::-1]\n",
        "for i in range(5):\n",
        "    print(\"cluster %d:\" %i, end='')\n",
        "    for ind in order_centroids[i,:10]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "    print()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93bc2e3b-3d78-e1e6-4c47-8bab20860e8e"
      },
      "source": [
        "**It is possible to to improve that. We must delete the words \"https\"092d\" \"00a0\" etc...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfa4f2f9-0554-6ae7-d388-c3f60cad486c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dist = 1 - cosine_similarity(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87304daf-87a2-5e90-da0b-15d5e49d8882"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pos = pca.fit_transform(dist)\n",
        "xs, ys = pos[:,0], pos[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f295571a-b537-3a41-f625-e0462050aa29"
      },
      "outputs": [],
      "source": [
        "#set up colors per clusters using a dict\n",
        "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}#, 5: '#8A2BE2', 6: '#E9967A'}\n",
        "#8A2BE2\n",
        "##E9967A\n",
        "#set up cluster names using a dict\n",
        "cluster_names = {0: 'cluster 1', \n",
        "                 1: 'cluster 2', \n",
        "                 2: 'cluster 3', \n",
        "                 3: 'cluster 4', \n",
        "                 4: 'cluster 5'}\n",
        "                 #5: 'cluster 6',\n",
        "                 #6: 'cluster 7'}\n",
        "clusters = km.labels_.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d4d4a17b-7ce3-e0ad-47a4-e90ce70a2f63"
      },
      "outputs": [],
      "source": [
        "#some ipython magic to show the matplotlib plots inline\n",
        "%matplotlib inline \n",
        "\n",
        "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
        "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title='')) \n",
        "\n",
        "#group by cluster\n",
        "groups = df.groupby('label')\n",
        "\n",
        "\n",
        "# set up plot\n",
        "fig, ax = plt.subplots(figsize=(10, 4)) # set size\n",
        "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
        "\n",
        "#iterate through groups to layer the plot\n",
        "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
        "for name, group in groups:\n",
        "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=10, \n",
        "            label=cluster_names[name], color=cluster_colors[name],\n",
        "            mec='none')\n",
        "    ax.set_aspect('auto')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelbottom='off')\n",
        "    ax.tick_params(\\\n",
        "        axis= 'y',         # changes apply to the y-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        left='off',      # ticks along the bottom edge are off\n",
        "        top='off',         # ticks along the top edge are off\n",
        "        labelleft='off')\n",
        "    \n",
        "ax.legend(numpoints=1)  #show legend with only 1 point\n",
        "plt.title('Cluster plotting with ACP', bbox={'facecolor':'0.8', 'pad':0})\n",
        "#add label in x,y position with the label as the film title\n",
        "for i in range(len(df)):\n",
        "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=5)  \n",
        "\n",
        "    \n",
        "    \n",
        "plt.show() #show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f07f995b-ff07-27c0-aede-a819f807a744"
      },
      "source": [
        "**It is possible to display a title for each points. But in order to have something visible in this case it is not realized.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd48f605-1149-34a6-006f-ed6a9f896de3"
      },
      "source": [
        "**Part V: Correlation between numerical features**\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4541f0ef-b870-7e68-7d95-15503e850b28"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "tweets['favorited'] = le.fit_transform(tweets['favorited'])\n",
        "tweets['replyToSN'] = tweets['replyToSN'].fillna(-999)\n",
        "tweets['truncated'] = le.fit_transform(tweets['truncated'])\n",
        "tweets['replyToSID'] = tweets['replyToSID'].fillna(-999)\n",
        "tweets['id'] = le.fit_transform(tweets['id'])\n",
        "tweets['replyToUID'] = tweets['replyToUID'].fillna(-999)\n",
        "tweets['statusSource_new'] = le.fit_transform(tweets['statusSource_new'])\n",
        "tweets['isRetweet'] = le.fit_transform(tweets['isRetweet'])\n",
        "tweets['retweeted'] = le.fit_transform(tweets['retweeted'])\n",
        "tweets['screenName'] = le.fit_transform(tweets['screenName'])\n",
        "tweets['tweetos'] = le.fit_transform(tweets['tweetos'])\n",
        "\n",
        "tweets_num = tweets[tweets.select_dtypes(exclude=['object']).columns.values]\n",
        "tweets_num.drop('Unnamed: 0',inplace=True,axis=1)\n",
        "tweets_num.drop('retweeted',inplace=True,axis=1)\n",
        "tweets_num.drop('favorited',inplace=True,axis=1)\n",
        "print(tweets.select_dtypes(exclude=['object']).columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d630f42-70cc-3578-0a77-c7aaad127761"
      },
      "outputs": [],
      "source": [
        "#from string import letters\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"white\")\n",
        "# Compute the correlation matrix\n",
        "corr = tweets_num.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(920, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.1,\n",
        "            square=True, xticklabels=True, yticklabels=True,\n",
        "            linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
        "plt.title('Correlation between numerical features', bbox={'facecolor':'0.8', 'pad':0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56629406-c97b-f13d-def1-dd1efe5b5316"
      },
      "source": [
        "**Low correlation between numerical features (between 0.30 and -O.30)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0ee5ac8e-11bf-3278-8fc5-34784343d92c"
      },
      "source": [
        "**Part VI: Sentiment Analysis**\n",
        "-------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39aa4adb-e54e-1fbf-fac2-dd66ca97dc7e"
      },
      "source": [
        "**This part is inspired by Priya Ananthram's scripts. You can look her work here:** https://www.kaggle.com/priyaananthram/d/arathee2/demonetization-in-india-twitter-data/sentiment-analysis-of-tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ca9093b-b64f-2452-64e9-ae138551dd5e"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "\n",
        "from nltk import tokenize\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "tweets['sentiment_compound_polarity']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
        "tweets['sentiment_neutral']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
        "tweets['sentiment_negative']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
        "tweets['sentiment_pos']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
        "tweets['sentiment_type']=''\n",
        "tweets.loc[tweets.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\n",
        "tweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n",
        "tweets.loc[tweets.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "334df886-8bf5-d2a2-bdfc-a8e1aea7fb0c"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "tweets_sentiment = tweets.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
        "tweets_sentiment.rename(\"\",inplace=True)\n",
        "explode = (0, 0, 1.0)\n",
        "plt.subplot(221)\n",
        "tweets_sentiment.transpose().plot(kind='barh',figsize=(10, 6))\n",
        "plt.title('Sentiment Analysis 1', bbox={'facecolor':'0.8', 'pad':0})\n",
        "plt.subplot(222)\n",
        "tweets_sentiment.plot(kind='pie',figsize=(10, 6),autopct='%1.1f%%',shadow=True,explode=explode)\n",
        "plt.legend(bbox_to_anchor=(1, 1), loc=3, borderaxespad=0.)\n",
        "plt.title('Sentiment Analysis 2', bbox={'facecolor':'0.8', 'pad':0})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7231847e-17be-5d68-6e49-c04d37696418"
      },
      "outputs": [],
      "source": [
        "tweets['count'] = 1\n",
        "tweets_filtered = tweets[['hour', 'sentiment_type', 'count']]\n",
        "pivot_tweets = tweets_filtered.pivot_table(tweets_filtered, index=[\"sentiment_type\", \"hour\"], aggfunc=np.sum)\n",
        "print(pivot_tweets.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f6c362b-0bfd-3e49-6eb1-6635fdd36fa9"
      },
      "outputs": [],
      "source": [
        "sentiment_type = pivot_tweets.index.get_level_values(0).unique()\n",
        "#f, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
        "plt.setp(ax, xticks=list(range(0,24)))\n",
        "\n",
        "for sentiment_type in sentiment_type:\n",
        "    split = pivot_tweets.xs(sentiment_type)\n",
        "    split[\"count\"].plot( legend=True, label='' + str(sentiment_type))\n",
        "plt.title('Evolution of sentiments by hour', bbox={'facecolor':'0.8', 'pad':0})    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aeef5c85-4c83-715d-b540-4b506e9d3834"
      },
      "source": [
        "**Part Bonus: Modelisation of retweetCount just with numerical features?**\n",
        "---------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68b727a8-755f-f40a-1c0d-ffed2bf90298"
      },
      "outputs": [],
      "source": [
        "from  sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import plot_importance\n",
        "\n",
        "tweets_num_mod = tweets[tweets.select_dtypes(exclude=['object']).columns.values]\n",
        "target = tweets_num_mod['retweetCount']\n",
        "tweets_num_mod.drop('retweetCount',inplace=True,axis=1)\n",
        "tweets_num_mod.drop('Unnamed: 0',inplace=True,axis=1)\n",
        "\n",
        "#Just simple  and single model\n",
        "model_xg = XGBRegressor()\n",
        "model_rf = RandomForestRegressor()\n",
        "model_et = ExtraTreesRegressor()\n",
        "model_gb = GradientBoostingRegressor()\n",
        "model_dt = DecisionTreeRegressor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "07db6b21-c3ec-29ad-655a-6c5a6ccba767"
      },
      "outputs": [],
      "source": [
        "scores_xg = cross_val_score(model_xg, tweets_num_mod, target, cv=5,scoring='r2')\n",
        "scores_rf = cross_val_score(model_rf, tweets_num_mod, target, cv=5,scoring='r2')\n",
        "scores_dt = cross_val_score(model_dt, tweets_num_mod, target, cv=5,scoring='r2')\n",
        "scores_et = cross_val_score(model_et, tweets_num_mod, target, cv=5,scoring='r2')\n",
        "scores_gb = cross_val_score(model_gb, tweets_num_mod, target, cv=5,scoring='r2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "365759a1-ca0d-1abd-fd6d-878db3607dd3"
      },
      "outputs": [],
      "source": [
        "print(\"Mean of scores for XG:\", sum(scores_xg) / float(len(scores_xg)))\n",
        "print(\"Mean of scores for RF:\", sum(scores_rf) / float(len(scores_rf)))\n",
        "print(\"Mean of scores for DT:\", sum(scores_dt) / float(len(scores_dt)))\n",
        "print(\"Mean of scores for ET:\", sum(scores_et) / float(len(scores_et)))\n",
        "print(\"Mean of scores for gb:\", sum(scores_gb) / float(len(scores_et)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "edc08d34-7fb6-b5e4-c8d0-49a5329a069f"
      },
      "outputs": [],
      "source": [
        "model_xg.fit(tweets_num_mod,target)\n",
        "# plot feature importance for xgboost\n",
        "plot_importance(model_xg)\n",
        "plt.title('Feature importance', bbox={'facecolor':'0.8', 'pad':0})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4f648926-7f7f-b344-5ed9-f9b1e228b355"
      },
      "source": [
        "**We can see that with ExtraTreesRegressor() we have 0.8XX with the metric R2. Very good score. It's possible to improve that with more work on the feature text...**"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}