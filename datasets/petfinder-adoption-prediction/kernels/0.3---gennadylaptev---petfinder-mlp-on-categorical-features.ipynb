{"cells":[{"metadata":{"_uuid":"b7cb69580f59223531e0020f126b681db09ad243"},"cell_type":"markdown","source":"# PetFinder: MLP on categorical (and numerical) features\n\nIn this kernel I demostrate a simple NN baseline implemented in PyTorch. I use only categorical and numerical features form train/test.csv. To encode categorical features I use embedding layers as described in https://arxiv.org/abs/1604.06737 (also this work is mentioned in a lesson 4 of the fast.ai DL course https://course.fast.ai/videos/?lesson=4 as well as in this discussion https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76100, however, I stumbled upon this thread after I created this kernel). <br><BR>\n\nI borrowed a lot of code from this blog post https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/ as well as from this tutorial on PyTorch https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html. Also I used some code from this public kernel https://www.kaggle.com/peterhurford/pets-lightgbm-baseline-with-all-the-data and took a small snippet of code from https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch/data <br><br>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import json\nimport time\nimport copy\n\nimport scipy as sp\nimport pandas as pd\nimport numpy as np\n\nimport random\nimport os\n\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b428347d65e8f58abc6a27f9cca0b48c68eb1c55"},"cell_type":"code","source":"# https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch/data\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc49ce567fd41bf67451f6e69eeb6999439cc4ce"},"cell_type":"code","source":"print('Train')\ntrain = pd.read_csv(\"../input/train/train.csv\")\nprint(train.shape)\n\nprint('Test')\ntest = pd.read_csv(\"../input/test/test.csv\")\nprint(test.shape)\n\nprint('Breeds')\nbreeds = pd.read_csv(\"../input/breed_labels.csv\")\nprint(breeds.shape)\n\nprint('Colors')\ncolors = pd.read_csv(\"../input/color_labels.csv\")\nprint(colors.shape)\n\nprint('States')\nstates = pd.read_csv(\"../input/state_labels.csv\")\nprint(states.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06f3e354607a8172db78728918b757f300ca6e48"},"cell_type":"code","source":"target = train['AdoptionSpeed']\ntrain_id = train['PetID']\ntest_id = test['PetID']\ntrain.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\ntest.drop(['PetID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"605865bcd48bf57fa2cfb466126f5e31ea37a63d"},"cell_type":"markdown","source":"#### Drop useless columns"},{"metadata":{"trusted":true,"_uuid":"f508f56d4cab23c936752a1033028117e5a4b08c"},"cell_type":"code","source":"train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\ntest.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11c6643d3d97546bfe2904702fb6061c268c212f"},"cell_type":"code","source":"train.Age = train.Age.apply(lambda x: x if x < 12 else (x // 12)*12)\ntest.Age = test.Age.apply(lambda x: x  if x < 12 else (x // 12)*12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b468e7c139c7b7e8ef730d20c84bc50221199e5f"},"cell_type":"code","source":"train.Quantity = train.Quantity.apply(lambda x: x if x < 10 else 10)\ntest.Quantity = test.Quantity.apply(lambda x: x if x < 10 else 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"234b9a87e450f97561d1595b4a81c43c3b5b25f7"},"cell_type":"markdown","source":"### Define columns types"},{"metadata":{"trusted":true,"_uuid":"253e65605d45da4db28f84363edeb98fb54431fb"},"cell_type":"code","source":"numeric_cols = [\n                'Type',\n                'Age', \n                'Fee', \n                'VideoAmt', \n                'PhotoAmt', \n               ] \n\ncat_cols = list(set(train.columns) - set(numeric_cols))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a84e9ff30fd684d28d888595160ba2410fde3b7a"},"cell_type":"markdown","source":"#### Standart scale numerical columns"},{"metadata":{"trusted":true,"_uuid":"ba935266224ee79c71862bd98e9b7af768225168"},"cell_type":"code","source":"for col in ['Age', 'Fee', 'VideoAmt', 'PhotoAmt']:\n    train[col] = train[col].transform(lambda x: (x - x.mean()) / x.std())\n    test[col] =  test[col].transform(lambda x: (x - x.mean()) / x.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df621564e2d17a59afe90851f33a1695f76dd5af"},"cell_type":"code","source":"# transform some cat columns to 0..N-1 range\nfor col in cat_cols:\n    if train[col].min() == 1:\n        train[col] -= 1\n        test[col] -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8338fd4514e72c236a670123031d7735da1b874"},"cell_type":"code","source":"# label encode States columns\ntrain['State'], States_indexer = pd.factorize(train.State)\n\ntest['State'] = States_indexer.get_indexer(test.State)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f73d43e965f8f28e97d855e1afaa28d4948184"},"cell_type":"code","source":"cat_dims = {\n    #'Type': 2,\n    'Breed1': 308,\n    'Breed2': 308,\n    'Gender': 3,\n    'Color1': 8,\n    'Color2': 8,\n    'Color3': 8,\n    'MaturitySize': 4, # some values are not present\n    'FurLength': 3,  # some values are not present\n    'Vaccinated': 3,\n    'Dewormed': 3,\n    'Sterilized': 3,\n    'Health': 3,\n    'Quantity': 10,      # check num of categories\n    'State': 14\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fc7305d223c440e2ed0442e0cfcc714bd57878e"},"cell_type":"code","source":"emb_dims = {\n    #'Type': 2,\n    'Breed1': 200,\n    'Breed2': 200,\n    'Gender': 2,\n    'Color1': 4,\n    'Color2': 4,\n    'Color3': 4,\n    'MaturitySize': 2, # some values are not present\n    'FurLength': 2,  # some values are not present\n    'Vaccinated': 2,\n    'Dewormed': 2,\n    'Sterilized': 2,\n    'Health': 2,\n    'Quantity': 5,      # check num of categories, default 10\n    'State': 7\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aee512defae1de908ac3dbe5787a080fb7bf1776"},"cell_type":"code","source":"emb_dims_tuples = [(cat_dims[c], emb_dims[c]) for c in cat_cols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d17c08d15fa1104d22b4e04e8a6bc795bcd098a"},"cell_type":"markdown","source":"#### Define Tabular Dataset"},{"metadata":{"trusted":true,"_uuid":"4c8bb852193dd2b8021ab6d143676a6727f9b145"},"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, data, num_cols=None, cat_cols=None, target=None):\n\n        self.n = data.shape[0]\n\n        if isinstance(target, pd.Series):\n            self.y = target.values\n        else:\n            self.y = target\n\n        self.cat_cols = cat_cols\n        self.cont_cols = num_cols\n\n        if self.cont_cols:\n            self.cont_X = data[self.cont_cols].values.astype(np.float32)\n        else:\n            self.cont_X = np.zeros((self.n, 1))\n\n        if self.cat_cols:\n            self.cat_X = data[cat_cols].values.astype(np.long)\n        else:\n            self.cat_X =  np.zeros((self.n, 1))\n\n    def __len__(self):\n        \"\"\"\n        Denotes the total number of samples.\n        \"\"\"\n        return self.n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generates one sample of data.\n        \"\"\"\n        if self.y is None:\n            return [self.cont_X[idx], self.cat_X[idx]] \n        else:\n            return [self.cont_X[idx], self.cat_X[idx], self.y[idx]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db7ea3dacf5f431f304d3a2dea7092517156de01"},"cell_type":"markdown","source":"### Define MLP"},{"metadata":{"trusted":true,"_uuid":"db5ee3da19cc75f68de09448a5af49a9928599ba"},"cell_type":"code","source":"class FeedForwardNN(nn.Module):\n\n    def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n               output_size, emb_dropout, lin_layer_dropouts):\n        \"\"\"\n        Parameters\n        ----------\n\n        emb_dims: List of two element tuples\n          This list will contain a two element tuple for each\n          categorical feature. The first element of a tuple will\n          denote the number of unique values of the categorical\n          feature. The second element will denote the embedding\n          dimension to be used for that feature.\n\n        no_of_cont: Integer\n          The number of continuous features in the data.\n\n        lin_layer_sizes: List of integers.\n          The size of each linear layer. The length will be equal\n          to the total number\n          of linear layers in the network.\n\n        output_size: Integer\n          The size of the final output.\n\n        emb_dropout: Float\n          The dropout to be used after the embedding layers.\n\n        lin_layer_dropouts: List of floats\n          The dropouts to be used after each linear layer.\n        \"\"\"\n\n        super().__init__()\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n\n        no_of_embs = sum([y for x, y in emb_dims])\n\n        self.no_of_embs = no_of_embs\n        self.no_of_cont = no_of_cont\n\n        # Linear Layers\n        first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont, lin_layer_sizes[0])\n        self.lin_layers = nn.ModuleList([first_lin_layer] +\\\n              [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1]) for i in range(len(lin_layer_sizes) - 1)])\n\n        # initialize weigths in linear layers\n        for lin_layer in self.lin_layers:\n            nn.init.kaiming_normal_(lin_layer.weight.data)\n\n        # Output Layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n\n        # Batch Norm Layers\n        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size) for size in lin_layer_sizes])\n\n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.droput_layers = nn.ModuleList([nn.Dropout(size) for size in lin_layer_dropouts])\n\n    def forward(self, cont_data, cat_data):\n\n        if self.no_of_embs != 0:\n            x = [emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)]\n            x = torch.cat(x, 1)\n            x = self.emb_dropout_layer(x)\n\n        if self.no_of_cont != 0:\n            normalized_cont_data = self.first_bn_layer(cont_data)\n\n            if self.no_of_embs != 0:\n                x = torch.cat([x, normalized_cont_data], 1) \n            else:\n                x = normalized_cont_data\n\n        for lin_layer, dropout_layer, bn_layer in zip(self.lin_layers, self.droput_layers, self.bn_layers):\n            \n            # NB activation func\n            x = F.tanh(lin_layer(x))\n            \n            x = bn_layer(x)\n            x = dropout_layer(x)\n\n        x = self.output_layer(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dedefe52f261ff8043d2a9743c3c6918c708ed5d"},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5939613de3ea426ffcd993f18659b751f80676bd"},"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n    seed_everything()\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for cont_data, cat_data, labels in dataloaders[phase]:\n                cont_data, cat_data = cont_data.to(device), cat_data.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    outputs = model(cont_data, cat_data)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * cont_data.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s; Best val Acc: {:4f}'.format(time_elapsed // 60, time_elapsed % 60, best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905af12cbbe40d26147b603ac346ffa6861a617b"},"cell_type":"code","source":"batch_size = 64\nN_SPLITS = 10\nN_EPOCHS = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eeb43c6f49d1071dcb3205d6dd1c20e3ec3e619"},"cell_type":"code","source":"test_ds = TabularDataset(test, num_cols=numeric_cols, cat_cols=cat_cols, target=None)\ntest_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2481348e056aad7916edb79e72abfaef95d80509"},"cell_type":"code","source":"def predict(model, dataloader, output_size=5):\n    preds = np.zeros((len(dataloader.dataset), output_size))\n    model = model.to(device)\n    model.eval()\n    with torch.no_grad():\n        for i, (cont_data, cat_data, *other) in enumerate(dataloader):\n            cont_data, cat_data = cont_data.to(device), cat_data.to(device)\n            preds[i*batch_size: (i+1)*batch_size] = model(cont_data, cat_data).cpu().numpy()\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"75b236cd936f1907593858cf67f43f642991c413"},"cell_type":"code","source":"fold_splits = StratifiedKFold(n_splits=N_SPLITS, random_state=42).split(train, target)\n\ntest_preds = np.zeros((test.shape[0], 5))\ntrain_preds = np.zeros((train.shape[0], 5))\nfor i, (train_index, val_index) in enumerate(fold_splits):\n    print(f'Fold {i+1}/{N_SPLITS}')\n        \n    if isinstance(train, pd.DataFrame):\n        train_X, val_X = train.iloc[train_index], train.iloc[val_index]\n        train_y, val_y = target[train_index], target[val_index]\n    else:\n        train_X, val_X = train[train_index], train[val_index]\n        train_y, val_y = target[train_index], target[val_index]\n    \n    train_ds = TabularDataset(train_X, num_cols=numeric_cols, cat_cols=cat_cols, target=train_y)\n    valid_ds = TabularDataset(val_X, num_cols=numeric_cols, cat_cols=cat_cols, target=val_y)\n    \n    dataloaders = {\n        'train': DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        'val': DataLoader(valid_ds, batch_size=batch_size, shuffle=False),\n    }\n    \n    model = FeedForwardNN(emb_dims_tuples, len(numeric_cols), lin_layer_sizes=[384, 256, 128],\n                          output_size=5, emb_dropout=0.01,\n                          lin_layer_dropouts=[0.1,0.1,0.001]).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer_ft = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    \n    # train\n    model, val_history = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=N_EPOCHS)\n    \n    # predict\n    test_preds += predict(model, test_dl, output_size=5)\n    train_preds[val_index] = predict(model, dataloaders['val'], output_size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae5c8b6e57c402abd8ab5bcab8dd4197f05f1574"},"cell_type":"code","source":"# just pick the index of the max value \n# w/o dividing by the num of folds and applying sigmoid function\ntest_preds_class = np.argmax(test_preds, axis=1)\ntrain_preds_class = np.argmax(train_preds, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35aed6307e55ea15836ff521a0c5850aea342d01"},"cell_type":"code","source":"quadratic_weighted_kappa(target, train_preds_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f6278282b47aeae87af9da543edccac737c419"},"cell_type":"code","source":"pd.DataFrame(sk_cmatrix(target, train_preds_class), index=list(range(5)), columns=list(range(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36ee65ded47bdbde1d8033c30224e65abdc17f43"},"cell_type":"code","source":"submission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_preds_class})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2059154b328091cb23629cd28726e187f06f824"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}