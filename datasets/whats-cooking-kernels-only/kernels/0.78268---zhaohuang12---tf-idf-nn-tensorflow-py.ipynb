{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport json\nimport tensorflow as tf\n\nprint (\"Read Dataset ... \")\ntrain = json.load(open(\"../input/train.json\"))\ntest = json.load(open(\"../input/test.json\"))\nprint (\"Prepare text data of Train and Test ... \")\ntrain_text = [\" \".join(doc[\"ingredients\"]).lower() for doc in train]\ntest_text = [\" \".join(doc[\"ingredients\"]).lower() for doc in test]\ntarget = [doc[\"cuisine\"] for doc in train]\n\nprint (\"{} different cuisines\".format(len(set(target))))\n\n# Feature Engineering \nprint (\"TF-IDF on text data ... \")\ntfidf = TfidfVectorizer(binary=True)\nX = tfidf.fit_transform(train_text)\nX_test = tfidf.transform(test_text)\n\nX = X.toarray()\nX_test = X_test.toarray()\n\n# Label Encoding - Target \nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)\n\n\ntrain_X,test_X, train_y, test_y = train_test_split(X, y, test_size = 0.1, random_state = 0)\nprint (\"{} training samples and {} validation samples\".format(train_X.shape[0], test_X.shape[0]))\n\nclass NN():\n  def __init__(self, feature_size, num_classes, learning_rate=0.001):\n    self.feature_size = feature_size\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    with tf.name_scope(\"placeholder\"):\n      self.input = tf.placeholder(tf.float32, [None, feature_size], name='input')\n      self.target = tf.placeholder(tf.int32, [None], name='target')\n      self.dropout = tf.placeholder(tf.float32, name = 'dropout')\n    with tf.name_scope(\"inference\"):\n      hidden_units = 512\n      W0 = tf.Variable(tf.truncated_normal([feature_size, hidden_units]))\n      b0 = tf.Variable(tf.zeros(hidden_units))\n      h = tf.matmul(self.input, W0)+ b0\n      h = tf.nn.relu(h)\n      h = tf.nn.dropout(h, keep_prob = self.dropout)\n      W = tf.Variable(tf.truncated_normal([hidden_units, num_classes]))\n      b = tf.Variable(tf.zeros(num_classes))\n\n      self.logits = tf.matmul(h, W)+b\n      \n    with tf.name_scope(\"loss\"):\n      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels = self.target)\n      self.loss =tf.reduce_sum(losses)\n\n    with tf.name_scope('optimize'):\n      self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n    with tf.name_scope('prediction'):\n      self.prob = tf.nn.softmax(self.logits)\n      self.prediction = tf.argmax(self.prob, axis = 1)\n      corrects = tf.equal(tf.cast(self.target, tf.int64), self.prediction)\n      self.accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n\nif __name__ == '__main__':\n  print (\"Train the model......\")\n  model = NN(feature_size = train_X.shape[1], num_classes = len(set(target)))\n  num_epoch  = 80\n  batch_size = 128\n  with tf.Session() as sess:\n    init_op = tf.global_variables_initializer()\n    sess.run(init_op)\n    for epoch in range(num_epoch):\n\n\n      num_batches = int((train_X.shape[0]-1)/batch_size)+1\n      for batch in range(num_batches):\n        start_index = batch_size*batch\n        end_index = min(train_X.shape[0], (batch+1)*batch_size)\n        feed_train = {model.input: train_X[start_index:end_index],\n                      model.target: train_y[start_index:end_index],\n                      model.dropout: 0.5}\n        _, loss, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict = feed_train)\n        #print(\"\\tepoch {}, \\tbatch {}, \\tloss {:g}, \\tacc {:g}\".format(epoch, batch, loss, accuracy))\n\n      feed_val = {\n                 model.input: test_X,\n                 model.target: test_y,\n                 model.dropout: 1.0\n      }\n      loss_val, accuracy_val = sess.run([model.loss, model.accuracy], feed_dict = feed_val)\n      print (\"\\tepoch {}, \\tloss {}, \\tacc {:g}\".format(epoch, loss_val, accuracy_val))\n\n    print (\"predict on test data......\")\n    feed_test = {model.input: X_test,\n                 model.dropout: 1.0}\n    predictions = sess.run(model.prediction, feed_dict=feed_test)\n    prediction_labels = lb.inverse_transform(predictions)\n    test_ids = [doc['id'] for doc in test]\n    submission = pd.DataFrame({'id':test_ids, 'cuisine':prediction_labels}, columns = ['id', 'cuisine'])\n    submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}