{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Activation,Dropout, Conv1D, MaxPooling1D, Flatten\nfrom keras.preprocessing import sequence\nimport keras as keras\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport os\nprint(os.listdir(\"../input\"))\nTRAIN = os.path.join(\"../input\", \"train.json\")\nSAMPLE_SUBMISSION = os.path.join(\"../input\", \"sample_submission.csv\")\n# SUBMISSION = os.path.join(\"submission.csv\")\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01bf625e7db3eb8dc0b916485311da4640f332db","collapsed":true},"cell_type":"code","source":"# !cat ../input/train.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"789f1bc119cb4d89c84194a933a25b5fb3199831","collapsed":true},"cell_type":"code","source":"\ndf = pd.read_json(TRAIN)\nprint(df['cuisine'].describe())\nprint(df.groupby('cuisine').count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9130047ef6d9aa31e6025fbc75fd032c500b9fae","collapsed":true},"cell_type":"code","source":"import re\ndef flat_ingredients(x): \n    x = ' '.join([i.replace(' ', '_').replace('-', '_').lower() for i in x])\n    # x = re.sub(re.compile('[^a-zA-Z0-9]+'), \" \", x)\n    # print(x)\n    return x\ndf['flat'] = df['ingredients'].apply(lambda x: flat_ingredients(x))\n\"\"\"print(df.head(15))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad850684715225b36c080b7e35735242e37cb313","collapsed":true},"cell_type":"code","source":"max_len = 0\nfor i in df['flat'].tolist():\n    l = len(str(i).split())\n    if str(i) != str(i).lower(): print(i)\n    if max_len < l:\n        max_len = l\nprint(max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56e4b6f1c719b7ab65ae9cd9a85f1b28e14863cd"},"cell_type":"code","source":"cuisines = list(set([str(i) for i in df['cuisine'].tolist()]))\ningredients = list(set([j  for i in df['flat'].tolist() for j in str(i).split()]))\ningredients.append(\"blank\")\ncuisines_to_idx = {w: i for i, w in enumerate(cuisines)}\ningredients_to_idx = {w:i for i, w in enumerate(ingredients)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a676b31499bf27e1e19d1df84962246c9650d4e8","collapsed":true},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom random import shuffle\n\nX = []\nfor i in df['flat'].tolist():\n    temp = []\n    for j in str(i).split():\n        temp.append(ingredients_to_idx[j])\n    X.append(temp)    \ny = [cuisines_to_idx[str(i)] for i in df['cuisine'].tolist()]\nprint(str(len(X)) + \" original length! \")\nfor i in range(0, len(X)):\n    for j in range(0, 3):\n        temp = X[i]\n        shuffle(temp)\n        X.append(temp)\n        y.append(y[i])\n\"\"\"print(X[0])\nprint(shuffle(X[0]))\nprint(X[0])\n\nprint(shuffle(X[0]))\nprint(X[0])\"\"\"\n\"\"\"print(X[:10], y[:10])\nprint(len(X))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5bd7df9e9021e7f1a3e59b87a56b2a49d9b073c","collapsed":true},"cell_type":"code","source":"# print(type(X[0]))\n# print(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a91ed61fc36b006f24ea2103af0a3e21f9ba60b5","collapsed":true},"cell_type":"code","source":"\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=ingredients_to_idx[\"blank\"])\ny = to_categorical(y, num_classes=len(cuisines))\n\"\"\"from sklearn.model_selection import train_test_split\nX_tr, X_vd, y_tr, y_vd = train_test_split(X, y, test_size=0.3, random_state=2)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e389360230336edd6d72719aaca4f3e5856017","collapsed":true},"cell_type":"code","source":"\"\"\"\ni = 0\n# print(len(X_tr))\nprint(len(y_tr))\n# print(type(X_tr[0]))\n# print(X_tr.shape)\n# a = np.random.permutation(X_tr[i])\n# print(a.shape)\n# print(len(np.insert(X_tr, len(X_tr), a, axis=0)))\ny_tr = np.insert(y_tr, len(y_tr), y_tr[i], axis=0)\n\n# print(X_tr.shape)\nprint(len(y_tr))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0614a0cfba0bae2e1cae05ed484cf4c843e186ed"},"cell_type":"code","source":"from keras.engine.topology import Layer\nimport numpy as np\nfrom keras import backend as K, activations\nclass AttentiveConv(Layer):\n    def __init__(self, kernel_activation='tanh', filters=3, **kwargs):\n        super(AttentiveConv, self).__init__(**kwargs)\n        self.kernel_activation = activations.get(kernel_activation)\n        if filters%2 == 0:\n            self.filters = filters - 1\n        else:\n            self.filters = filters\n        self.filters = filters\n        K.set_floatx('float32')\n\n    def build(self, input_shape):\n        self.num_words = input_shape[0][1]\n        self.em_dim = input_shape[0][2]\n        self.W2 = self.add_weight(shape=(self.em_dim, self.filters*self.em_dim), dtype=K.floatx(), name='att_cont_weight', trainable=True, initializer='glorot_normal')\n        self.We = self.add_weight(shape=(self.em_dim, self.em_dim), dtype=K.floatx(), name='window_weight', trainable=True, initializer='glorot_normal')\n        super(AttentiveConv, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        #the input is a list of two tensors. As this layers computes a score for every element of the first input I just\n        #return the shape of this tensor.\n        return input_shape[0]\n\n    def get_config(self):\n        config = {'kernel_activation': activations.serialize(self.kernel_activation),\n                  'filters': self.filters}\n        base_config = super(AttentiveConv, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def call(self, x, mask=None):\n        #x is a list of two tensors\n        #casting makes no sense so I deleted it\n        text = x[0]\n        context = x[1]\n\n        #applies bilinear energy funtion (text * We * context)\n        #and weights the computed feature map like in equation 6 (W2 * ci)\n        \n        #shape of text/context is (batch_size, num_words, em_dim), num_words for text is 200 and em_dim is also 200. \n        #I want to do the computation for every sample of the batch. I found batch_matmul but thats not available in \n        #tensorflow 1.5\n        #shape of weighted_attentive_context should be the same shape as text.\n        weighted_attentive_context = self._compute_attentive_context(text, context)\n        return weighted_attentive_context\n\n    def _compute_attentive_context(self, text, context):\n        #computes the context-score for every vector like equation 2\n        temp = K.dot(text, self.We)\n        scores = K.batch_dot(temp, K.permute_dimensions(context, (0,2,1)))\n\n        #softmax along every vector-element\n        #scores = text\n        scores_softmax = activations.softmax(scores, axis=1)\n\n        #computes the context featur_map like equation 4\n        res = tf.matmul(scores_softmax, context)\n\n        #weights the output like equation 6\n        res = K.permute_dimensions(K.dot(self.W2,K.permute_dimensions(res, (0,2,1))),(1,2,0))\n        #res = scores\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e764ac1544da3a307d9fb8f8bac555ae842876c2","collapsed":true},"cell_type":"code","source":"print(str(len(X)) + ' exploded length!')\nfrom sklearn.model_selection import train_test_split\nX_tr, X_vd, y_tr, y_vd = train_test_split(X, y, test_size=0.0001, random_state=2)\n\n\"\"\"for i in range(0, len(X_tr)):\n    for j in range(3):\n        X_tr = np.insert(X_tr, len(X_tr), np.random.permutation(X_tr[i]), axis=0)\n        y_tr = np.insert(y_tr, len(y_tr), y_tr[i], axis=0)\"\"\"\n\ndef get_model():\n    model_conv = Sequential()\n    model_conv.add(Embedding(len(ingredients), 100, input_length=max_len))\n    model_conv.add(Dropout(0.2))\n    model_conv.add(AttentiveConv(filters=64, kernel_activation='relu'))\n    model_conv.add(MaxPooling1D(pool_size=4))\n    model_conv.add(Flatten())\n    # model_conv.add(LSTM(100))\n    model_conv.add(Dense(64))\n    model_conv.add(Activation('relu'))\n    model_conv.add(Dropout(0.5))\n    model_conv.add(Dense(len(cuisines), activation='softmax'))\n    model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model_conv.summary())\n    return model_conv\n\n\"\"\"def get_model2():\n    model = Sequential()\n    model.add(Dense(2048, input_shape=(max_len,)))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(len(cuisines)))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    print(model.summary())\n    return model\"\"\"\nmodel = get_model()\nmodel.fit(X_tr, np.array(y_tr), batch_size=128, epochs=11, verbose=1, validation_data=(X_vd, np.array(y_vd)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c3f5dd9fdc1d3503d1c00eba8a458e12d2958fb","collapsed":true},"cell_type":"code","source":"model.evaluate(X_vd, np.array(y_vd), batch_size=32, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166d0540e9f96ffbaf2328120ef139b6e7be1979","collapsed":true},"cell_type":"code","source":"model.predict_classes(X_vd[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7696efc21747af3482dbb0623066f5b55539e38d","collapsed":true},"cell_type":"code","source":"targets = model.predict_classes(X_vd[:20])\nprint(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96f4ddf5f959d8fffd850b3ee87fd7c01b971918","collapsed":true},"cell_type":"code","source":"for i in range(0, len(targets)):\n    if 1 != int(y_vd[i][targets[i]]):\n        print(\"sentence : \" + str([ingredients[k] for k in X_vd[i]]))\n        print(\"correct: \" + str(y_vd[i]))\n        print(\"incorrect: \" + str(cuisines[targets[i]] ))\n        print(\"incorrect: \" + str(targets[i] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3a32566d7f2f90c3044b6fcd68906dba510fc1e","collapsed":true},"cell_type":"code","source":"test_df = pd.read_json('../input/test.json')\ntest_df['flat'] = test_df['ingredients'].apply(lambda x: flat_ingredients(x))\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3d8c868b2dc26dd2f2b406ff4ae601598884953","collapsed":true},"cell_type":"code","source":"test_X = []\nfor i in test_df['flat'].tolist():\n    temp = []\n    for j in str(i).split():\n        try:\n            temp.append(ingredients_to_idx[j])\n        except:\n            temp.append(ingredients_to_idx[\"blank\"])\n    test_X.append(temp)\ntest_X = pad_sequences(maxlen=max_len, sequences=test_X, padding=\"post\", value=ingredients_to_idx[\"blank\"])\n\ntargets = model.predict_classes(test_X)\ndef insert_column(x):\n    return(int (x.index[0]))\n    return cuisines[targets[x.index]]\n# print(test_df.apply(lambda x: insert_column(x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"518713ea9ccaec2a89cc3edf70ddee4b2380b1aa","collapsed":true},"cell_type":"code","source":"test_df['cuisine'] = pd.Series([cuisines[i] for i in targets])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b16d540cb9bf5cb14740736bae6a3864c30aee5a","collapsed":true},"cell_type":"code","source":"test_df = test_df[['id', 'cuisine']]\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb9ef724936491bab96ad0d442642d060ca4b3e2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}