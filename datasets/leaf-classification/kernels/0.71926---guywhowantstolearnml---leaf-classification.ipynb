{"cells": [{"source": ["Importing Libraries"], "cell_type": "markdown", "metadata": {"_cell_guid": "2f5d84cb-b20d-4061-9d14-2097a6823191", "_uuid": "41a8d3db97bfe8bd504bd3f5d75060574b890684"}}, {"source": ["import numpy as np\n", "import pandas as pd\n", "from scipy import ndimage\n", "import matplotlib.pyplot as plt\n", "from os import listdir\n", "%matplotlib inline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.grid_search import GridSearchCV\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import StandardScaler\n", "from PIL import Image"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "bc28ee7a-e446-42c1-9510-3bda05cdcd2f", "_uuid": "e3ebaaeefdc46d831442db0f89842d65a6e927b8"}, "execution_count": 1}, {"source": ["Setting Path Variables"], "cell_type": "markdown", "metadata": {"_cell_guid": "9843bc2b-8045-4ec0-b36b-d7f07b47636e", "_uuid": "d3be088b71b7352b543b87e580d02ffc2095eb81"}}, {"source": ["path = '../input/images/'\n", "train = pd.read_csv('../input/train.csv')\n", "image_paths = [path + f for f in listdir(path)]"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8f27f213-7832-4b3f-a1e2-153c20cbd93c", "collapsed": true, "_uuid": "a5c0d1302adb1d2c3c8b912568a4fe1a15e05da3"}, "execution_count": 2}, {"source": ["Defining Image Plotting and imagetraversal functions"], "cell_type": "markdown", "metadata": {"_cell_guid": "52546858-f3b6-4ad5-b0f9-ad26b5cc2daf", "_uuid": "78ec4730340f8ad69f6f0591fcfc8584be6a98b6"}}, {"source": ["def plotGalery(classes, n_col=10, scale_x = 1.5, scale_y = 1.7):\n", "    \n", "    def pathsBySpecies(classes):\n", "        paths = {}\n", "        for row in train.values:\n", "            if row[1] in classes:\n", "                if row[1] in paths:\n", "                    paths[row[1]].append('../input/images/' + str(row[0]) + '.jpg')\n", "                else:\n", "                    paths[row[1]] = ['../input/images/' + str(row[0]) + '.jpg']\n", "        return paths\n", "    \n", "    dic = pathsBySpecies(classes)\n", "    \n", "    n_row = len(dic.keys())\n", "    plt.figure(figsize=(scale_x * n_col, scale_y * n_row))\n", "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n", "    for i in range(n_row * n_col):\n", "        key = list(dic.keys())[i // n_col]\n", "        path = dic[key][i % n_col]\n", "        image = Image.open(path)\n", "        image = np.array(image)\n", "        plt.subplot(n_row, n_col, i + 1)\n", "        plt.imshow(image, cmap=plt.cm.gray, interpolation='none')\n", "        plt.xticks(())\n", "        plt.yticks(())\n", "        plt.tight_layout\n", "        \n", "def imageById(id_):\n", "    img = Image.open('../input/images/'+str(id_)+'.jpg')\n", "    #img = img.resize((50, 50), Image.ANTIALIAS)\n", "    return np.array(img)\n", "'''\n", "def imageByIdPCA(id_):\n", "    img = Image.open('../input/images/'+str(id_)+'.jpg')\n", "    img = img.resize((50, 50), Image.ANTIALIAS)\n", "    from sklearn.decomposition import PCA\n", "    # Make an instance of the Model\n", "    pca = PCA(35)\n", "    pcm = pca.fit_transform(img)\n", "    return pcm\n", "'''"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6d4bf64a-adf0-4008-80b9-a23e0273245e", "_uuid": "aafd3f1be60dcf40a5668302140bcde74db08236"}, "execution_count": 3}, {"source": ["classes = train.species.value_counts().keys()\n", "plotGalery(classes)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0ed21c3b-cc85-46c9-aeb4-38e200de692c", "collapsed": true, "_uuid": "1181d37b4a0a94f849990e18774acea525d1147f"}, "execution_count": null}, {"source": ["import cv2\n", "\n", "def imageFeatures(source, filepath):\n", "    df = pd.read_csv(source)\n", "    ids = df.values[:,0].astype(np.int)\n", "    images = [imageById(id_) for id_ in ids]\n", "    height = [image.shape[0] for image in images]\n", "    width = [image.shape[1] for image in images]\n", "    orientation = [int(h > w) for h, w in zip(height, width)]\n", "    perimeters = [cv2.Canny(im,100,200).sum() / 255.0 for im in images]\n", "    square = [image.sum() / 255.0/ image.size for image in images]\n", "    square_r = [im.sum() / sq for im, sq in zip(images, square)]\n", "    sums = [im.sum() for im in images]\n", "    pd.DataFrame({\n", "            'height': height,\n", "            'width': width,\n", "            'orientation': orientation,\n", "            'square': square,\n", "            'square_r': square_r,\n", "            'sum': sums\n", "        }).to_csv(filepath, index=False)\n"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "872b436f-f910-4392-b78c-43b0869592a1", "collapsed": true, "_uuid": "8eddfad0c48b65d50ec8e8334e428a03ca44be27"}, "execution_count": 4}, {"source": ["PCA"], "cell_type": "markdown", "metadata": {"_cell_guid": "89f36ed8-e33a-4c0d-a0b6-49f779131000", "_uuid": "821833902a99f67cd7401225f2c23cbea5803325"}}, {"source": ["#Principal Component Analysis\n", "import matplotlib\n", "df = pd.read_csv('../input/train.csv')\n", "df2 = pd.read_csv('../input/test.csv')\n", "ids = df.values[:,0].astype(np.int)\n", "testids = df2.values[:,0].astype(np.int)\n", "pca_train = []\n", "pca_test = []\n", "for id_ in ids:\n", "    img=Image.open('../input/images/'+str(id_)+'.jpg')\n", "    img = img.resize((50, 50), Image.ANTIALIAS)\n", "    img= np.array(img)\n", "    pca_train.append(img)\n", "for id_ in testids:\n", "    img=Image.open('../input/images/'+str(id_)+'.jpg')\n", "    img = img.resize((50, 50), Image.ANTIALIAS)\n", "    img= np.array(img)\n", "    pca_test.append(img)\n", "pca_train=np.array(pca_train)\n", "pca_test=np.array(pca_test)\n", "pca_train = pca_train.reshape(990,2500)\n", "pca_test = pca_test.reshape(594,2500)\n", "from sklearn.decomposition import PCA\n", "# Make an instance of the Model\n", "pca = PCA(35)\n", "pca.fit(pca_train)\n", "pca_train = pca.transform(pca_train)\n", "pca_test = pca.transform(pca_test)\n"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b96c1467-4311-4d3c-a1dc-ded9037a0a9f", "collapsed": true, "_uuid": "8391d24cc0c349516d5a9464df702c7bf80759c1"}, "execution_count": 5}, {"source": ["Saving PCs as DataFrames"], "cell_type": "markdown", "metadata": {"_cell_guid": "80fc2bf8-894f-40dc-8ad1-59bc66355b6e", "_uuid": "31ab10fbb573b3333feb714852d0cc019145f0ba"}}, {"source": ["pca_train=pd.DataFrame(pca_train)\n", "pca_test = pd.DataFrame(pca_test)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5e6b4a90-6ba8-4238-b290-e844b8004139", "collapsed": true, "_uuid": "9752654409fae0cc30fe7ade4249a5ba6dc4c2bc"}, "execution_count": 6}, {"source": ["imageFeatures('../input/train.csv', 'train_f1.csv')\n", "imageFeatures('../input/test.csv', 'test_f1.csv')\n", "#pcaImage('../input/train.csv','train_pca.csv')\n", "#pcaImage('../input/test.csv','test_pca.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b7edfa41-c0dd-4bdf-8f1c-c0a1f323c195", "collapsed": true, "_uuid": "6cbdc7c4e655852f393e81d6a528427394e1f11c"}, "execution_count": 7}, {"source": ["I Moments Functions"], "cell_type": "markdown", "metadata": {"_cell_guid": "a81e084a-2f78-400a-b12f-d47285af039e", "_uuid": "ea61c5bd3316135b554eb96364dc21a02320d327"}}, {"source": ["def M(im):\n", "    ret,thresh = cv2.threshold(im,127,255,0)\n", "    contours,hierarchy, _ = cv2.findContours(thresh, 1, 2)\n", "    cnt = contours[0]\n", "    x = np.fromiter(iter(cv2.moments(cnt).values()), dtype=float)\n", "    return x\n", "\n", "def imageMoments(source, filepath):\n", "    df = pd.read_csv(source)\n", "    ids = df.values[:,0].astype(np.int)\n", "    images = [imageById(id_) for id_ in ids]\n", "        \n", "    moments = [1.0*M(im)/im.size for im in images]\n", "    pd.DataFrame(moments).to_csv(filepath, index=False)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0431821c-cd9c-44fc-b090-dc7203c69206", "collapsed": true, "_uuid": "515185ba2ce4ca8b792128f0bb1d5570e2400ee2"}, "execution_count": 8}, {"source": ["imageMoments('../input/train.csv', 'train_M1.csv')\n", "imageMoments('../input/test.csv', 'test_M1.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "666350ad-f16e-4a49-b570-61e5d7ded2fd", "collapsed": true, "_uuid": "fdeb2c447dfa299159d96f3e654723a52a3c6591"}, "execution_count": 9}, {"source": ["Performing initial Logistic Regression to figure out confusion classes."], "cell_type": "markdown", "metadata": {"_cell_guid": "44ddf226-215c-446c-9db4-e3c213989cb9", "_uuid": "b63afe3239e965b6a31bd0c26f317af0661591d1"}}, {"source": ["train_f1 = pd.read_csv('train_f1.csv')\n", "train_M1 = pd.read_csv('train_M1.csv')\n", "test_f1 = pd.read_csv('test_f1.csv')\n", "test_M1 = pd.read_csv('test_M1.csv')\n", "\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = pd.read_csv(\"../input/train.csv\")\n", "\n", "# train\n", "x_train = train.drop(['id', 'species'], axis=1)\n", "y_train = train['species']\n", "le = LabelEncoder()\n", "y_train = le.fit_transform(y_train)\n", "test_ids = test.pop('id')\n", "x_test = test\n", "x_train = pd.concat([train_f1,x_train,pca_train,train_M1], axis=1)\n", "x_test = pd.concat([test_f1,x_test,pca_test,test_M1], axis =1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "175da6a5-e0d3-4a7c-9ff6-ed7c5849eb49", "collapsed": true, "_uuid": "ddf975c33fcfaca2b90576fc61f878e24b01bfa9"}, "execution_count": 10}, {"source": ["scaler = StandardScaler().fit(x_train)\n", "x_train = scaler.transform(x_train)\n", "x_test = scaler.transform(x_test)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8c88b296-5a18-4a96-ab59-6684aacef4a0", "collapsed": true, "_uuid": "7d30ca458c4501da0443b06742ee2f419d25ad14"}, "execution_count": 11}, {"source": ["params = {'C':[1000], 'tol': [0.0008, 0.0007]}\n", "log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n", "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='False', cv=3)\n", "clf.fit(x_train, y_train)\n", "\n", "y_pred = clf.predict_proba(x_test)\n", "\n", "submission = pd.DataFrame(y_pred, index=test_ids, columns=le.classes_)\n", "#submission.to_csv('test.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "c55489ce-4f63-49f7-9317-ffa7f598faaa", "_uuid": "2d8b0731bee2c8d7715135a033604c7610a4c803"}, "execution_count": 12}, {"source": ["Finding Classes which fail to perform well using LR"], "cell_type": "markdown", "metadata": {"_cell_guid": "02dad6f3-7e05-46ce-8db0-ce620a1828e4", "_uuid": "a608ac030a42282e74ffd5e799dd0dba2f4affef"}}, {"source": ["colsub=[]\n", "for i in submission.columns:\n", "    colsub.append(i)\n", "finalcol=[]\n", "from sklearn.feature_selection import VarianceThreshold\n", "selector = VarianceThreshold()\n", "selector.fit_transform(submission)     \n", "j=0\n", "for i in selector.variances_:\n", "    if(i>0.009999):\n", "        finalcol.append(colsub[j])\n", "    j=j+1\n", "#finalcol is the nparray of columns that don't perform well using LR"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fe8f6be5-3719-4dad-8889-80ee4919570e", "collapsed": true, "_uuid": "4ea67a768fd551d1b7cfdd1b60dc07a67057777d"}, "execution_count": 13}, {"source": ["**Building Final Model**"], "cell_type": "markdown", "metadata": {"_cell_guid": "31b231f8-e373-4d7c-956c-9a0237641757", "_uuid": "2ea3253882eca1a71957135d8870d4ba690f6379"}}, {"source": ["train_f1 = pd.read_csv('train_f1.csv')\n", "train_M1 = pd.read_csv('train_M1.csv')\n", "test_f1 = pd.read_csv('test_f1.csv')\n", "test_M1 = pd.read_csv('test_M1.csv')\n", "\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = pd.read_csv(\"../input/train.csv\")\n", "#Training df\n", "x_train = train\n", "x_rf_train = train\n", "\n", "le = LabelEncoder()\n", "y_train = train['species']\n", "y_rf_train =train['species']\n", "#labels\n", "test_ids = test.pop('id')\n", "x_test = test\n", "#merging extracted features with given features\n", "x_train = pd.concat([train_f1,x_train,pca_train,train_M1], axis=1)\n", "x_rf_train = pd.concat([train_f1,x_rf_train,pca_train,train_M1], axis=1)\n", "for index, row in x_train.iterrows():\n", "    if(row['species'] in finalcol):\n", "        x_train = x_train.drop(index)\n", "        y_train = y_train.drop(index)\n", "        print(index)\n", "    else:\n", "        x_rf_train = x_rf_train.drop(index)\n", "        y_rf_train = y_rf_train.drop(index)\n", "\n", "y_train = le.fit_transform(y_train)\n", "x_train = x_train.drop(['id', 'species'], axis=1)\n", "x_rf_train =x_rf_train.drop(['id', 'species'], axis=1)\n", "x_test = pd.concat([test_f1,x_test,pca_test,test_M1], axis =1)\n", "lerf=LabelEncoder()\n", "y_rf_train =lerf.fit_transform(y_rf_train)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1868b200-a44a-4d9d-a1c5-16a140712643", "_uuid": "e928e83b9d3c8b876c66dac7729240e1257dcb91"}, "execution_count": 14}, {"source": ["#Scaling for Logistic Regression\n", "scaler = StandardScaler().fit(x_rf_train)\n", "x_rf_train = scaler.transform(x_rf_train)\n", "x_rf_test = scaler.transform(x_test)\n", "#scaling for Random Forest\n", "scaler = StandardScaler().fit(x_train)\n", "x_train = scaler.transform(x_train)\n", "x_test = scaler.transform(x_test)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "94a99067-dcbe-4c72-94bf-4e39daf9d741", "collapsed": true, "_uuid": "d41380ebc30bdcd621563defb43190d39ab7e20d"}, "execution_count": 15}, {"source": ["Using RandomForest and storing partial results in a dataframe"], "cell_type": "markdown", "metadata": {"_cell_guid": "91fae610-c228-40e2-94ae-a416a9cd1863", "_uuid": "d00d4a5ccc1c64ea6b3c347fa84e8c0c85af285c"}}, {"source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.datasets import make_classification\n", "\n", "clf = RandomForestClassifier()\n", "clf.fit(x_rf_train, y_rf_train)\n", "y_predrf = clf.predict_proba(x_rf_test)\n", "submissionrf = pd.DataFrame(y_predrf, index=test_ids, columns=lerf.classes_)\n"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "fd43380a-7b9a-4bfd-9093-46b71bbb35ad", "collapsed": true, "_uuid": "a3b5c59ed845f9398ede69754375ba4422fb0b3c"}, "execution_count": 16}, {"source": ["Using Logistic Regression and storing the rest of the results in another dataframe"], "cell_type": "markdown", "metadata": {"_cell_guid": "454de4aa-f3e5-48a8-afac-c299f8773533", "_uuid": "17d524dc4d9b91f24db08e1fb8de0d05d8bc30f5"}}, {"source": ["params = {'C':[1000], 'tol': [0.0008, 0.0007]}\n", "log_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n", "clf = GridSearchCV(log_reg, params, scoring='log_loss', refit='False', cv=3)\n", "clf.fit(x_train, y_train)\n", "\n", "y_pred = clf.predict_proba(x_test)\n", "\n", "submission = pd.DataFrame(y_pred, index=test_ids, columns=le.classes_)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "1730314a-e007-4c94-8aad-5df9210ac4bf", "_uuid": "01251588ccc2fdf3c7ed30e429732255ca7c0864"}, "execution_count": 17}, {"source": ["Concatening both databases and creating CSV for submission"], "cell_type": "markdown", "metadata": {"_cell_guid": "486e5695-e9db-4950-904b-39ecfb8ce63e", "_uuid": "3f84b32ecf50293c66375de73f73d9fdac8870ec"}}, {"source": ["submission = pd.concat([submission,submissionrf], axis =1)\n", "submission.to_csv('final.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "cb24e3ad-1c61-42ae-a6d7-e9617b7ef2a6", "collapsed": true, "_uuid": "779c52b7f9d19b924218fc2f792dab5f09b53c13"}, "execution_count": null}, {"source": [], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4efebb09-1f05-4725-bedc-0b09cd8b1de7", "collapsed": true, "_uuid": "4bb73a46bca95b68b5d30014fab1efcf48fc3789"}, "execution_count": null}, {"source": ["> **Rough Work.... Alternate approach.**"], "cell_type": "markdown", "metadata": {"_cell_guid": "a462fbc8-8285-4ed7-adf2-396265eed23b", "_uuid": "8ff9390cb66424d285bd33692b956e2f727125b3"}}, {"source": ["for i in result.index:\n", "    if(result[prediction[0][i]][i]>0.1):\n", "        print(prediction[0][i])\n", "\n"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "76095ed9-cd5c-437c-ad8a-8f2ba5018c06", "collapsed": true, "_uuid": "de93cd481e2cd09afcb12c2f143b7a1e16ad95aa"}, "execution_count": null}, {"source": ["train_f1 = pd.read_csv('train_f1.csv')\n", "#train_M1 = pd.read_csv('train_M1.csv')\n", "test_f1 = pd.read_csv('test_f1.csv')\n", "#test_M1 = pd.read_csv('test_M1.csv')\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = pd.read_csv(\"../input/train.csv\")\n", "train=pd.concat([train_f1,train,pca_train], axis=1)\n", "test = pd.concat([test_f1,test,pca_test], axis =1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "148e9918-81a8-4f0f-afef-4af8eeebba6e", "collapsed": true, "_uuid": "f6261983fd23bf1c1e456a91c089d3ec9255705d"}, "execution_count": null}, {"source": ["from sklearn.preprocessing import LabelEncoder\n", "from sklearn.cross_validation import StratifiedShuffleSplit"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d9d03cec-d1d8-450e-8ab0-bdbe92a3be2f", "collapsed": true, "_uuid": "73856a1f9bf4ca368a70b2b4b0ecc99ba30e2e11"}, "execution_count": null}, {"source": ["def encode(train, test):\n", "    le = LabelEncoder().fit(train.species) \n", "    labels = le.transform(train.species)           # encode species strings\n", "    classes = list(le.classes_)                    # save column names for submission\n", "    test_ids = test.id                             # save test ids for submission\n", "    \n", "    train = train.drop(['species', 'id'], axis=1)  \n", "    test = test.drop(['id'], axis=1)\n", "    \n", "    return train, labels, test, test_ids, classes\n", "\n", "train, labels, test, test_ids, classes = encode(train, test)\n", "train.head(1)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a880e5bc-be17-4566-a5a1-c7b5a6b9d770", "collapsed": true, "_uuid": "d3f5c991ba447f1b3c23440b0ff74e3b440af84f"}, "execution_count": null}, {"source": ["sss = StratifiedShuffleSplit(labels, 10, test_size=0.2, random_state=23)\n", "\n", "for train_index, test_index in sss:\n", "    X_train, X_test = train.values[train_index], train.values[test_index]\n", "    y_train, y_test = labels[train_index], labels[test_index]"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5d01da76-3dc0-438a-912c-1c2657a8497d", "collapsed": true, "_uuid": "e5a5e121c232f35db63007e29c8bfe2d8d9e8039"}, "execution_count": null}, {"source": ["y_train.shape"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "7f2a01c3-2f65-401b-b662-266ea83fc876", "collapsed": true, "_uuid": "f7393deece7c95a3d692704055c4e84836c81fa3"}, "execution_count": null}, {"source": ["from sklearn.metrics import accuracy_score, log_loss\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.svm import SVC, LinearSVC, NuSVC\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n", "\n", "classifiers = [\n", "    KNeighborsClassifier(3),\n", "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n", "    NuSVC(probability=True),\n", "    DecisionTreeClassifier(),\n", "    RandomForestClassifier(),\n", "    AdaBoostClassifier(),\n", "    GradientBoostingClassifier(),\n", "    GaussianNB(),\n", "    LinearDiscriminantAnalysis(),\n", "    QuadraticDiscriminantAnalysis()]\n", "\n", "# Logging for Visual Comparison\n", "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n", "log = pd.DataFrame(columns=log_cols)\n", "\n", "for clf in classifiers:\n", "    clf.fit(X_train, y_train)\n", "    name = clf.__class__.__name__\n", "    \n", "    print(\"=\"*30)\n", "    print(name)\n", "    \n", "    print('****Results****')\n", "    train_predictions = clf.predict(X_test)\n", "    acc = accuracy_score(y_test, train_predictions)\n", "    print(\"Accuracy: {:.4%}\".format(acc))\n", "    \n", "    train_predictions = clf.predict_proba(X_test)\n", "    ll = log_loss(y_test, train_predictions)\n", "    print(\"Log Loss: {}\".format(ll))\n", "    \n", "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n", "    log = log.append(log_entry)\n", "    \n", "print(\"=\"*30)"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "7b24b42e-5aad-4029-b535-ee247a428813", "collapsed": true, "_uuid": "b1ebed879ccbc0ea6a8e4011338791108a1eae9e"}, "execution_count": null}, {"source": ["# Predict Test Set\n", "favorite_clf =  LinearDiscriminantAnalysis(solver='lbfgs', multi_class='multinomial')\n", "favorite_clf.fit(X_train, y_train)\n", "y_pred = favorite_clf.predict_proba(test)\n", "\n", "# Format DataFrame\n", "#submission = pd.DataFrame(test_predictions, columns=classes)\n", "#submission.insert(0, 'id', test_ids)\n", "#ubmission.reset_index()\n", "\n", "# Export Submission\n", "#submission.to_csv('submissiondiff.csv', index = False)\n", "#submission.tail()"], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "84d4c8df-6ddf-4847-be68-f1fdb57f0e22", "collapsed": true, "_uuid": "6035a474523f501f49990f566cdd36ef33c7739e"}, "execution_count": null}, {"source": [], "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "762c7ba0-1ea8-4b75-809f-8667eb291966", "collapsed": true, "_uuid": "4db007737eb720c976495ba0bf531a0437abc2b2"}, "execution_count": null}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "name": "python", "version": "3.6.4", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1}