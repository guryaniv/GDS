{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56f3c3499260fe04c9d0c6535f59ea3320b8c987"},"cell_type":"markdown","source":"> Please go through Giba's post and kernel  to underrstand what this leak is all about\n> https://www.kaggle.com/titericz/the-property-by-giba (kernel)\n> https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61329 (post)\n> \n> Also, go through this Jiazhen's kernel which finds more columns to exploit leak\n> https://www.kaggle.com/johnfarrell/giba-s-property-extended-result\n> \n> I just exploit data property in brute force way and then fill in remaining by row non zero means! This should bring everyone on level-playing field.\n> \n> **Let the competition begin! :D**\n\n> ### Just some small modifications from [original baseline](https://www.kaggle.com/tezdhar/breaking-lb-fresh-start)~\n>- The leak rows are calculated separately on train/test set\n>- Calculated the leaky values, correctness, for each lag\n>- Hope this can help to do some *lag_selection*\n\n>### Update leak process codes to Dmitry Frumkin's *fast* [version](https://www.kaggle.com/dfrumkin/a-simple-way-to-use-giba-s-features-v2)\n>- The result of Dmitry's original function and result of Hasan's function seem slightly different\n>- Modified to make the output consistent with Hasan's function (Seems better score)"},{"metadata":{"_uuid":"010ef7e2ec4bfab584be28d0e2b4476fd9b0d344"},"cell_type":"markdown","source":"Updated this Kernel by Jiazhen Xi with some of the new patterns I found. Using 6 of the 40 those patterns. Hope this helps everyone and provide them with some sort of help in this dying stage of the competition."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import mode, skew, kurtosis, entropy\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc37a766646b5993cef0bc87ad6882728dd20cb2"},"cell_type":"code","source":"train = pd.read_csv(\"../input/santander-value-prediction-challenge/train.csv\")\ntest = pd.read_csv(\"../input/santander-value-prediction-challenge/test.csv\")\n\ntransact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\ny = np.log1p(train[\"target\"]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9951f375af0d5a753031352e04a85a5a12a93e18"},"cell_type":"code","source":"test[\"target\"] = train[\"target\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dcfc4df1340c38bfeac43fd4d19ba2763b3b916"},"cell_type":"markdown","source":"We take time series columns from [here](https://www.kaggle.com/johnfarrell/giba-s-property-extended-result)"},{"metadata":{"trusted":true,"_uuid":"95a5c734a5f982ab13ec79a74c0c304996b8eec9"},"cell_type":"code","source":"all_df = pd.concat([train, test]).reset_index(drop=True)\nall_df.columns = all_df.columns.astype(str)\nprint(all_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n        '6619d81fc', '1db387535', \n        'fc99f9426', '91f701ba2', '0572565c2', '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n       ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f297b5fca50b079bf777c88c015ade10d6aebf1"},"cell_type":"code","source":"pattern_1964666 = pd.read_csv('../input/pattern-found/pattern_1964666.66.csv')\npattern_1166666 = pd.read_csv('../input/pattern-found/pattern_1166666.66.csv')\npattern_812666 = pd.read_csv('../input/pattern-found/pattern_812666.66.csv')\npattern_2002166 = pd.read_csv('../input/pattern-found/pattern_2002166.66.csv')\npattern_3160000 = pd.read_csv('../input/pattern-found/pattern_3160000.csv')\npattern_3255483 = pd.read_csv('../input/pattern-found/pattern_3255483.88.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1528fc328e7d6e4c44c0e1a300a0dde3a1924b8"},"cell_type":"code","source":"pattern_1964666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_1166666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_812666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_2002166.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_3160000.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_3255483.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e0eed16eada664d156a7ca0b6c95728c4a38b14"},"cell_type":"code","source":"pattern_1166666.rename(columns={'8.50E+43': '850027e38'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10732e8b2160cc845c9bf74a6ec16fd771f91bc9"},"cell_type":"code","source":"l=[]\nl.append(pattern_1964666.columns.values.tolist())\nl.append(pattern_1166666.columns.values.tolist())\nl.append(pattern_812666.columns.values.tolist())\nl.append(pattern_2002166.columns.values.tolist())\nl.append(pattern_3160000.columns.values.tolist())\nl.append(pattern_3255483.columns.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"894fdefbe479e983b85100a9f0dc0d06af279249"},"cell_type":"markdown","source":"Updating this function on the basis of the hint provided by Paradox [here](http://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61472#363394)."},{"metadata":{"trusted":true,"_uuid":"2664b88d70c76fde0df57ca6f22f75997f72cb00"},"cell_type":"code","source":"def _get_leak(df, cols,extra_feats, lag=0):\n    f1 = cols[:((lag+2) * -1)]\n    f2 = cols[(lag+2):]\n    for ef in extra_feats:\n        f1 += ef[:((lag+2) * -1)]\n        f2 += ef[(lag+2):]\n    \n    d1 = df[f1].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d1.to_csv('extra_d1.csv')\n    d2 = df[f2].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'}) \n\n    d2['pred'] = df[cols[lag]]\n#     d2.to_csv('extra_d2.csv')\n    #d2 = d2[d2.pred != 0] ### to make output consistent with Hasan's function\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    d4 = d1[~d1.duplicated(['key'], keep=False)]\n    d5 = d4.merge(d3, how='inner', on='key')\n    \n    d6 = d1.merge(d5, how='left', on='key')\n    d6.to_csv('extra_d6.csv')\n    \n    return d1.merge(d5, how='left', on='key').pred.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d61c75092518f50a879e9e3d5883ab752f73912b"},"cell_type":"code","source":"def compiled_leak_result():\n    \n    max_nlags = len(cols)-2\n    train_leak = train[[\"ID\", \"target\"] + cols]\n    train_leak[\"compiled_leak\"] = 0\n    train_leak[\"nonzero_mean\"] = train[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    scores = []\n    leaky_value_counts = []\n    leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        print('Processing lag', i)\n        train_leak[c] = _get_leak(train, cols,l, i)\n        \n        leaky_cols.append(c)\n        train_leak = train.join(\n            train_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n        zeroleak = train_leak[\"compiled_leak\"]==0\n        train_leak.loc[zeroleak, \"compiled_leak\"] = train_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(train_leak[\"compiled_leak\"] > 0))\n        _correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        leaky_value_corrects.append(_correct_counts*1.0/leaky_value_counts[-1])\n        print(\"Leak values found in train\", leaky_value_counts[-1])\n        print(\n            \"% of correct leaks values in train \", \n            leaky_value_corrects[-1]\n        )\n        tmp = train_leak.copy()\n        tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        print('Na count',tmp.compiled_leak.isna().sum())\n        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        print(\n            'Score (filled with nonzero mean)', \n            scores[-1]\n        )\n    result = dict(\n        score=scores, \n        leaky_count=leaky_value_counts,\n        leaky_correct=leaky_value_corrects,\n    )\n    return train_leak, result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"68f3fde6e5e9d274a4de6ef0975bbb4b682d5e85","_kg_hide-output":true},"cell_type":"code","source":"train_leak, result = compiled_leak_result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d904afe6c581aa250592432402977b1ab2b3ede"},"cell_type":"code","source":"result = pd.DataFrame.from_dict(result, orient='columns')\nresult.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"629ec0c6ee122fd82bf40f09a55a3f6f8f6c7fdf"},"cell_type":"code","source":"result.to_csv('train_leaky_stat.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42bfb7c9b41f687c189229f83cc6c8c7fd100536"},"cell_type":"code","source":"train_leak.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15e49a1e6f3d95150a4114edca7d3ae910bc6ee3"},"cell_type":"code","source":"best_score = np.min(result['score'])\nbest_lag = np.argmin(result['score'])\nprint('best_score', best_score, '\\nbest_lag', best_lag)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}