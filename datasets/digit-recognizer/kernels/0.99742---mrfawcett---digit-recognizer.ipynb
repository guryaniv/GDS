{"cells":[{"metadata":{"_uuid":"7ac94b4120c35484dc50051ca42c61ebc213f1e4"},"cell_type":"markdown","source":"## Digit Recognizer Walk Through \n#### M. Fawcett\n#### Started October 28, 2018\n\nThis is a step by step walk through of numeric digit recognition using deep learning based on a Kaggle Kernel by Yassine Ghouzam, PhD. \"Introduction to CNN Keras - Acc 0.997 (top 8%)\".  https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6  Many thanks to him for providing such a good starting point.\n\nAdditional annotations are made to Mr. Ghouzam's code that reflect my learning from Andrew Ng's five course Deep Learning specialization on Coursera.  My notes are indicated by a '#+' comment sign.\n\nStarting with his basic model I conducted a series of experiments trying out different layer sizes, drop-out, optimizations, etc..  The results of the experiments are near the end of the report.  Code lines with comments like \"Exp 1\", \"Exp 2\" identify the modifications made for the different experiments.  The scores I got from the Kaggle public leaderboard are listed for each experiment.  \n\nUsing the unmodified model demonstrated by Mr. Ghouzam, the best score I was able to achieve was 0.99471.  But after making several changes I was able to improve my results considerably.\n\nIn the end, I used an ensemble technique where the results of three models are combined using the majority vote method.  This turned out to have the best result, and put me into position 162 out of 2,714 on 11/29/2018.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load Libraries and data provided by Kaggle.\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nimport random\n# import BatchNormalization\nfrom keras.layers.normalization import BatchNormalization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nsns.set(style='white', context='notebook', palette='deep')\n\n\n\n# Kaggle note:\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bdcba1725dc8577beaf6be8dd9e1fb04f6cc7c2"},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\n#+ The current working directory\nprint(\"Kaggle working directory is\", os.getcwd())\nprint(\"Output generated by this notebook will go here\")\nprint(\"Current contents:\",os.listdir(os.getcwd()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d17587e32fda930f72408436f4cbda9e85894c1"},"cell_type":"code","source":"#+ Set random seed for reproducibility\nrandom.seed(10)\nprint(\"Setting randomization seed for results reproducibility...\\n\", \"You should see a '74' here -> \", random.randint(1,101))\n#+ the number \"74\" should display everytime this kernel is run","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the data.  Data files were automatically placed when the competiion was joined and this kernel was started.\ntrain = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest = pd.read_csv(\"../input/digit-recognizer/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdee2ded37383db58884740934ca4e228e9972f0"},"cell_type":"code","source":"# Get dimensions of the datasets\nprint(\"Shape of train =\", train.shape)\nprint(\"Shape of test =\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9c0c4f12d6da6795c714188bf0a8704faa4e559"},"cell_type":"code","source":"# Peek at a tiny part of both datasets\nprint(\"Training set:\\n\", train.iloc[31000, :10])\nprint(\"Testing set:\\n\", test.iloc[26000, :10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2207a7aab342c72f49769c133d30ae6e5941dd6"},"cell_type":"code","source":"# Build a -list- of just the labels in the training set\nY_train = train[\"label\"]\n\n# Drop 'label' column from the training set\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\n# del train \n\n# Show a colorful plot comparing the number of examples of each digit 0-9 in the training set\ng = sns.countplot(Y_train)\n\n# Show the actual counts of each digit in the training set\nY_train.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0005e42efe21c8b415de9793f2f09c5cc403235"},"cell_type":"code","source":"# Check the training data\nprint(X_train.isnull().any().describe())\n\n# Are there any nulls\n# a if condition else b\n'There are nulls present in the training set' if (X_train.isnull().any().any()) else 'There are NO nulls present in the training set'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f450cc7fa48b5ea67b02e717c3955bb2703e61dc"},"cell_type":"code","source":"# Check the test data\nprint(test.isnull().any().describe())\n\n# Are there any nulls\n'There are nulls present in the test set' if print(test.isnull().any().any()) else 'There are NO nulls present in the test set'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8caa3bb51fbbc202baf0d9c8fb165442886549e"},"cell_type":"code","source":"# Perform a grayscale normalization to reduce the effect of illumination's differences.\n# The CNN converg faster on [0..1] data than on [0..255].\nX_train = X_train / 255.0\ntest = test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f012185728614351ee65604fcc94ca074385099","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n# Basically all we are doing here is adding the channel dimension 1.  Greyscale uses 1 channel.  RGB uses 3\n# Its needed by the NN algorithms\nX_train = X_train.values.reshape(-1,28,28,1)  ## the -1 is a placeholder for the number of examples. \n#+ -1 is sort of a wildcard in this sense.\ntest = test.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b54aac059a2e6f4a39bc1ad57cd8449eb2439fe2"},"cell_type":"code","source":"#+ Show an example image from the training set\ng = plt.imshow(X_train[1000][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c6cab0b9a76e7d45ebc54c316f0c5c82df50f2e"},"cell_type":"code","source":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda638239a016429cb436d46d18467191fc8d73a"},"cell_type":"markdown","source":"#### Split the training set into separate training and validation partitions"},{"metadata":{"trusted":true,"_uuid":"cc8cd53bf7fedc0ee58c9ee14b1efc4d4520e9c5"},"cell_type":"code","source":"# Choose a seed value for reproducible results\nrandom_seed = 2\n\n# Do the split the train and the validation set \nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\n\n# -train_test_split- (from sklearn.model_selection, scikit-learn package) is a quick utility that wraps input \n# validation # and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting \n# (and optionally subsampling) data in a oneliner.\n\n# Using a 90/10 split here. 90% train/10% validation.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fff41c6d60ed19df5fd42a5de651c9a744f068a"},"cell_type":"code","source":"# Some examples\ng = plt.imshow(X_train[0][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c07dbf49ee4c8c5b61e53c3c91f9af607f0d201"},"cell_type":"markdown","source":"Start building the Convolutional Neural Network layers  \nThe example CNN architechture is Input -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out"},{"metadata":{"trusted":true,"_uuid":"eeaefd9634acbb22c0efda05adce0d725b013590"},"cell_type":"code","source":"### A \"Sequential\" model in Keras model is a linear stack of layers.\nmodel = Sequential()\n\n### According to Andrew Ng, a common pattern you see in convolutional neural networks is:\n### Conv -> Conv -> Pool -> Conv -> Pool -> FC -> FC -> FC -> Softmax\n\n### The first convolutional layer\nmodel.add(Conv2D(filters = 32, \n                 kernel_size = (5,5),\n                 padding = 'Same', \n                 activation ='relu', \n                 input_shape = (28,28,1)))  # need to implicitly specify the dimensions of the example image because, don't forget, the image\n# has been reshaped into a one-dimensional vector for input into the NN.\n\n#+ Convolutional operations are the application of filters (small matrices) to the input values.  Their purpose is to detect -features- within the\n#+ image.  They can also have the effect of reducing the size of the image and thereby shrinking the number of parameters that need to be updated during gradient\n#+ descent calculations. This can be important with large images having multiple channels (eg. red-blue-green). The images here are relatively small so parameter \n#+ reduction is not particularly important.  That's why in this case the \"padding\" parameter is set to \"Same\" causing the output to be the same size as the \n#+ input; no size reduction.  \n\n#+ Each cell within the filter has a value, or \"weight\", that is used to smooth and summarize the input in order to find features.  The filters in the first \n#+ layer are 5 by 5 in size.  In a Keras implementation the weights are automatically randomly initialized and over time will gradually adjust into values \n#+ that can identify features, like edges, within the image.\n\n#+ Every layer in a NN must be associated with a non-linear \"activation\" function. This function is applied to the product of the current layer's weight and the previous \n#+ layer's activation value to generate the next activation value which will in turn be passed to the next layer in the NN.  The first convolutional layer uses\n#+ -relu- as its non-linear function. Note - the first set of activation values are by default simply the pixel values of the original image.\n\n#+ Using non-linear activation functions are what make NNs work.  It can be demonstrated (not here) that if a -linear- activation function is used, it won't \n#+ matter how complex the NN is in terms of the number of layers, the prediction result will be the same as if there were only one layer.  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9dbca9391a5fcc105c8489bd6a028bbce3716f8"},"cell_type":"code","source":"### Add another convolutional layer.\nmodel.add(Conv2D(filters = 32, \n                 kernel_size = (5,5),\n                 padding = 'Same', \n                 activation ='relu'))\n#+ This further refines the low level feature detection.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7947172a90d6bb96a2de78698bef600575b34502"},"cell_type":"code","source":"### Add a pooling level.\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\n#+ Pooling is a way of reducing the size of the image and speeding up the processing of computation.  It makes the recognition of features more robust.\n#+ This layer uses -max pooling- size 2 by 2, which means for every 2 by 2 region in the previous layer the maximum value is selected and passed on as the new\n#+ value.  There is no good theoretical underpinning for pooling other than it provides good results.  What it is doing is finding the most distinctive features.\n#+ Since no \"stride\" parameter is specified, the pooling window will be moved in increments of 2 pixels across and then down, then across, etc until the \n#+ entire image is scanned.  The result will be a 7 by 7 image (28 by 28 image divided into 2 by 2 regions condenses to 7 by 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de8e13a5bd32596d0c7438b027a8b1a166038931"},"cell_type":"code","source":"### Add a \"drop out\" layer.\nmodel.add(Dropout(0.25))\n\n#+ \"Drop out\" ignores a percentage of the activations coming from the previous layer. In this case 1 out of 4 will be ignored.  This is a form of regularization\n#+ used to prevent overfitting.  It helps in ignoring noise in the image and forces the remaining activations \"to work harder\".","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8ecab258d088c947cb28f980911a1203423e49"},"cell_type":"code","source":"### Add another convolutional layer\nmodel.add(Conv2D(filters = 64, \n                 kernel_size = (3,3),\n                 padding = 'Same', \n                 activation ='relu'))\n#+ The filter size is 3 by 3 and there are 64 filters.  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02b9408612ac91a4079600c38643ed577c6b9a61"},"cell_type":"code","source":"### Another convolutional layer\n#+ For Experiment 7 do not use this second 64 filter layer\n#+ model.add(Conv2D(filters = 64, \n#+                  kernel_size = (3,3),\n#+                  padding = 'Same', \n#+                  activation ='relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af09c4434134573a980f99bf21d435ddc9d9ec68"},"cell_type":"code","source":"### Add a pooling layer\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16abfab0f226180300871bbf3c1bdf2673d105ad"},"cell_type":"code","source":"### Another drop out layer\nmodel.add(Dropout(0.25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"166c17c1255de337e5d35cc516073af135d5aa21"},"cell_type":"code","source":"### Reshape the 2d data to a one dimensional vector\nmodel.add(Flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e01a18bdf3be64edacd434b509246b32621e113"},"cell_type":"code","source":"### Add a regular neural network layer with 256 nodes\nmodel.add(Dense(256, activation = \"relu\"))\n#+ This layer will be \"fully connected\" to the previous layer, meaning every activation value in the previous\n#+ layer will be passed to every node in the \"Dense\" layer.\n\n#+ For Experiment 2-7, add a batch normalization layer.\nmodel.add(BatchNormalization())\n#+ Batch normalization standardizes the mean and variance of Z values prior to an activation layer.\n#+ It works with gradient descent, RMSProp, Adam, momentum and mini batch.\n#+ The normalization parameters Beta and Gamma are learned during training.  These determine the mean and\n#+ variance that the Z values will be constrained by.  Used as a defense against \"covariant shifting\".","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b437c75339b4ea4fed2840fdc95c101ec452e8fb"},"cell_type":"code","source":"### Add another regular neural network layer with 128 nodes\n#+ For Experiment 6 use this additional fully connected layer. For Basic model and all other experiments comment this out.\n#+ model.add(Dense(128, activation = \"relu\"))\n#+ This layer will be \"fully connected\" to the previous layer, meaning every activation value in the previous\n#+ layer will be passed to every node in the \"Dense\" layer.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4d28dca6c4e945713b6d5d4dd5c86c7a387f82"},"cell_type":"code","source":"### Add a drop out layer that will randomly select half the nodes in the Dense layer to be ignored.\nmodel.add(Dropout(0.5))  #+ Basic model and Experiment 5, 6 uses this drop out layer\n#+ Exp 1, Exp 2, Exp 3, Exp 4: eliminate the final drop out layer by commenting out the previous line.\n#+ My thinking about taking out the drop out layer is that it is usually used as a regularization technique to \n#+ prevent over fitting.  There didn't seem to be over fitting going on so I took it out. Accuracy seemed to\n#+ improve without it.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"497a9a4ba9ff315d5f5033ae8363d6027fc61402"},"cell_type":"code","source":"### Add a final fully connected Dense layer where the final classification step will take place\nmodel.add(Dense(10, activation = \"softmax\"))\n#+ Activation is \"Softmax\" which offers a probability of each digit value being the input image.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5076a33ce9941673d33c673cb8ecbcceb623c1f6"},"cell_type":"code","source":"### Define the optimizer using RMSProp\n#+ For Basic Model and Experiment 1 - 3 and 6 use RMSProp.\nmy_optimizer = RMSprop(lr=0.001, \n                    rho=0.9, \n                    epsilon=1e-08, \n                    decay=0.0)\n#+ The optimizer function has influence over the effectiveness of the process of arriving at the final solution. \n#+ The \"learning rate\" (LR) is a multiplier factor that determines the size of the change in the weight parameters\n#+ during each backpropagation step.  A larger LR speeds up learning but too large can lead to wild occilations in the \n#+ gradient descent process and failure to find an optimal solution. Too small a learning rate \n#+ can cause the learning process to proceed too slowly and maybe never reaching an optimal solution.\n#+ 'rho' is a constant typically set to 0.9 or 0.99 is a dampening factor for how gradient descent occilates during learning.\n#+ It has the effect of speeding the path taken to finding the optimal solution.\n#+ 'epsilon' is an arbitrary tiny, near zero, value that is added to prevent division by zero in the calculations of the gradient descent delta.\n#+ 'decay' is a factor that decreases the learning rate periodically.  It is set to zero here.  A learning rate decay factor is applied\n#+ in a later step in a more effective way.\n\n### Define the optimizer using Adam (Adaptive Movement Estimation)\n#+ For Exp 4 and 5 use Adam.\n#+ my_optimizer = Adam(lr=0.001, \n#+                     beta_1 = 0.09, \n#+                     beta_2 = 0.999, \n#+                     epsilon = 1e-08, \n#+                     decay = 0.0, \n#+                     amsgrad = False)\n#+ lr: float >= 0. Learning rate.\n#+ beta_1: float, 0 < beta < 1. Generally close to 1.\n#+ beta_2: float, 0 < beta < 1. Generally close to 1.\n#+ epsilon: float >= 0. Fuzz factor. If None, defaults to K.epsilon().\n#+ decay: float >= 0. Learning rate decay over each update.\n#+ amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm from the paper \"On the Convergence of Adam and Beyond\".\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"394dc0a6a0003efa8f145f201670e77dd448e1b0"},"cell_type":"code","source":"### Compile the model with the optimizer defined above.  \nmodel.compile(optimizer = my_optimizer, \n              loss = \"categorical_crossentropy\", \n              metrics=[\"accuracy\"])\n# Using a cost function called \"categorical cross entropy\" that is appropriate for multi-class classification.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96690eaf26595d24428937e9b6f46395d8f37ce8"},"cell_type":"code","source":"### Set the number of epochs for training.  \n#+ 1 epoch used here just for a reality check that the model can be trained successfully.\n#+ Will switch to using more epochs for the real model.\nmy_epochs = 1 \nmy_batch_size = 86\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69085901f8b8e78090b357cf3c9f72c3c789a077"},"cell_type":"code","source":"import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25fdf626c945f1b48cc0c6e8f72da341cf05e9ca"},"cell_type":"code","source":"# Train the model with the training data.\n# In Keras, training returns a \"history\" object which contains information about the accuracy of the fitting process.\nprint(datetime.datetime.now())\nhistory = model.fit(X_train, \n                    Y_train, \n                    batch_size = my_batch_size, \n                    epochs = my_epochs, \n                    validation_data = (X_val, Y_val), \n                    verbose = 2)\nprint(datetime.datetime.now())\n#+ One epoch takes three minutes without GPU. 10 seconds with GPU.\n#+ Good demo of the value of faster computer hardware.\n#+ 30 epochs took 3 minutes with the GPU.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce613d0161b2685254b43aadf936928123f24ffc"},"cell_type":"code","source":"# Try adding more data using data augmentation.  \n#+ Training with more data is one way to reduce overfitting.\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n#+ The ImageDataGenerator is a terrific feature in Keras as it simplifies into one step the generation of\n#+ additional image data to increase the number of training examples. It comes with a large number of parameters \n#+ for distorting images, so #+ examine them carefully in the Keras documentation to find the ones you \n#+ will explicitly set and which you will leave at their default values.\n\n# Fits the data generator to the example data\ndatagen.fit(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"900c0c3b333a10e4ef4b519fbb0efc0b001121cc"},"cell_type":"markdown","source":"For the data augmentation:\n\n* Randomly rotate some training images by 10 degrees\n* Randomly Zoom by 10% some training images\n* Randomly shift images horizontally by 10% of the width\n* Randomly shift images vertically by 10% of the height\n* Do not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n"},{"metadata":{"_uuid":"93c36285d8ddb019c9b5f7352961b135e3359263"},"cell_type":"markdown","source":"#### Learning Rate:\n\nIn order to make the optimizer converge faster and closest to the global minimum of the loss function, use an annealing method of the learning rate (LR).\n\nThe LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n\nTo keep the advantage of the fast computation time with a high LR, decrease the LR dynamically every X epochs when accuracy is not improving.\n\nIn this example, using the ReduceLROnPlateau function from Keras.callbacks, reduce the LR by half if the accuracy is not improved after 3 epochs."},{"metadata":{"trusted":true,"_uuid":"89e54007d65aa554ad90265d95944431c66ce2e4"},"cell_type":"code","source":"# Set a learning rate annealer.\n#+ Use the change in the validation accuracy to control the speed of LR reduction.\n#+ don't let the learning rate drop below 0.00001.\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n#+ The term \"annealing\" has multiple definitions.  It's commonly used to describe a process of hardening metal \n#+ or glass by controlling the speed at which it cools from its molten state.  There are also meanings within \n#+ biology and mathematics.  The deep learning meaning has to do with slowing the speed with which the learning \n#+ rate changes.  Extra Note - \"Simulated annealing\" is related to stochastic gradient descent.  It is used \n#+ to find a satisfactory approximate solution in solution spaces that are too large to find an exact solution.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dab6dca498931e79cb22bb843779b83ab2ef5be"},"cell_type":"code","source":"#+ Increase the number of epochs to 30.  Use GPU if available.\n#+ Experiment 3 - 7. 100 epochs\n#+ my_epochs = 100\nmy_epochs = 1  ### The Kaggle Kernel concept is great but could use some embelishments such as the ability to cache results or the ability to disable cells temporarily.\n#+ The epoch count is set to \"1\" here in order to allow the program to more quickly commit and produce the final ensemble result using previously generated results.\n\n# Fit the model using the augmented data generator.  \n#+ Make predictions for the validation set.\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size = my_batch_size),\n                              epochs = my_epochs, \n                              validation_data = (X_val,Y_val),\n                              verbose = 2, \n                              steps_per_epoch = X_train.shape[0] // my_batch_size, \n                              callbacks=[learning_rate_reduction])\n#+ the .flow method takes data & label arrays and generates batches of augmented data while learning takes \n#+ place\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c00824de2f1f703a71290153d810fb7043d2b1ca"},"cell_type":"markdown","source":"After 30 epochs the basic model reached 99.50%+ validation accuracy.\n\nI'm noticing that in my results, validation accuracy tends to be higher than training accuracy.  A fairly common explanation for this is that using drop out during training can tend to underfit on the training data, causing higher error rate in training.\n"},{"metadata":{"trusted":true,"_uuid":"e36fd7ddfb59faa82a78639ebd009b2a048e9f0e"},"cell_type":"code","source":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c8ad3eae738ecad9895b2cb353fe74263a5931b"},"cell_type":"markdown","source":"#### Assessing the validation set results using a confusion matrix\n"},{"metadata":{"trusted":true,"_uuid":"7217c38462bb07f220a2f138555a8e264a1b900f"},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n### End of function definition\n    \n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n#+ Y_pred contains a list of probabilities for each image. There are 10 probabilities for each image corresponding to\n#+ the probability that the image represents a 0, 1, 2... 9.  The largest probability in the list is the number \n#+ that will be declared the predicted value.\n#+ Example for the first image: Y_pred[1] = [1.2758733e-13 2.1202715e-10 2.4014818e-07 4.6213042e-10 5.0552443e-02\n#+ 9.5689595e-11 5.5256303e-14 8.5465127e-04 3.7065365e-06 9.4858891e-01] \n#+ 9.4858891e-01 is the largest value, so the predicted value for the image is \"9\".\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"615f82db245a43a79dcb6ae12e5d52b72f9e34b9"},"cell_type":"markdown","source":"The most frequent error occurred when trying to distinguish a true \"4\" that looked like a \"9\".  True \"3\"s and \"5\"s were the next most frequent error.\n\nTake a look at Google LeNet or VGG16/19 network , they are very deep networks but very well built to better extract features from images.\n\n#### Error Analysis\n"},{"metadata":{"trusted":true,"_uuid":"5f6bbd21f0fd74bda9c391be4cc3c53463d42837"},"cell_type":"code","source":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\n#+ Y_pred_classes are the digits that were predicted for each image.  Eg: [6 9 5 ... 2 2 6]\n#+ Y_true are the actual digit (class) for each image.  Eg: [6 9 5 ... 2 2 6]\nerrors = (Y_pred_classes - Y_true != 0)\n#+ errors is a list of True & False values, one for each image in the validation set.\n#+ False means there was no error and the predicton was correct. True means the prediction was wrong. \n#+ When the difference between Y_pred_classes - Y_true is equal to zero, the expression is False and will be \n#+ ones we got right. \n#+ Instances where Y_pred_classes - Y_true is not zero, the expression is True and will be the ones we got wrong.\n\nY_pred_classes_errors = Y_pred_classes[errors]\n#+ This pulls out just the predictions that we got wrong (the \"True\" values in errors).  \n#+ It is a list of all the mistakes.\n#+ Example: [8 9 9 6 8 6 8 8 7 9 3 6 7 8 4 4 9 6 0]\n\nY_pred_errors = Y_pred[errors]\n#+ Y_pred[errors] gets a list of all the probabilities that were made for the errors.  There is a probability \n#+ for each class of possible outcomes.\n\nY_true_errors = Y_true[errors]\n#+ The true labels of the ones we got wrong.\n\nX_val_errors = X_val[errors]\n#+ X_val_errors contains pixel values for the errors. So they can be displayed graphically to see what the \n#+ hard to predict digits look like.\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images that had prediction errors with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows, ncols, sharex = True, sharey = True)\n    fig.subplots_adjust(hspace = .5)  #+ adds space between rows of subplots so captions are all visible.\n    #+ Citation: https://stackoverflow.com/questions/5159065/need-to-add-space-between-subplots-for-x-axis-label-maybe-remove-labelling-of-a \n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label: {}\\nTrue label: {}\".format(pred_errors[error],obs_errors[error]))\n            n += 1  #+ increment counter n\n\n# Probabilities of the wrong predicted numbers.\n#+ In the final Softmax layer, every class of outcome (0, 1,...9) has a probability calculated, for each input image.\n#+ Y_pred_errors contains a 2 dimensional matrix of all the prediction probabilities for the validation images the model got wrong. \n#+ Each row has the calculated probability for each possible class outcome (0...9)\n#+ Y_pred_errors_prob contains the probabilties that the model calculated for each incorrectly predicted class.  It contains the maximum\n#+ probability among the 10 classes \nY_pred_errors_prob = np.max(Y_pred_errors, axis = 1)\n\n# Predicted probabilities of the true values in the error set.\n#+ true_prob_errors contains the probabilty the model calculated for the correct class.\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n#+ Calculate the difference between the probability that was assigned to the incorrectly predicted label and the \n#+ probability that was assigned to the true label.  A larger difference means the model got it \"more wrong\".\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n#+ Since we are only dealing with the errors instances here, by definition Y_pred_errors_prob is greater \n#+ than true_prob_errors for a given image.\n\n# Sorted list of the delta prob errors\n#+ Argsort sorts in ascending order\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \n#+ Use -6 to select the bottommost 6 items in the sorted list.  These will be the 6 with the greatest error.\nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\n#+ Using the display_errors() function defined above.\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f89cf48063810e9b141fe9a5a3cad550c9215eeb"},"cell_type":"markdown","source":"### Summary Of Results for Basic Model (30 epochs)\n(First 3 and last 3 of 30 epochs)  \nEpoch 1/30  - 14s - loss: 0.1713 - acc: 0.9491 - val_loss: 0.0514 - val_acc: 0.9855  \nEpoch 2/30  - 13s - loss: 0.1000 - acc: 0.9701 - val_loss: 0.0398 - val_acc: 0.9900  \nEpoch 3/30  - 13s - loss: 0.0840 - acc: 0.9752 - val_loss: 0.0351 - val_acc: 0.9902  \n...  \nEpoch 28/30  - 14s - loss: 0.0261 - acc: 0.9926 - val_loss: 0.0157 - val_acc: 0.9950  \nEpoch 29/30  - 13s - loss: 0.0255 - acc: 0.9922 - val_loss: 0.0158 - val_acc: 0.9945  \nEpoch 30/30  - 13s - loss: 0.0273 - acc: 0.9921 - val_loss: 0.0162 - val_acc: 0.9943  \nSubmitted 11/25/2015:  Scored 0.99471 placed #567.\n\n### Summary Of Results for Experiment 1 (Eliminate final drop out layer. 30 epochs.)\nEpoch 1/30 - 13s - loss: 0.1338 - acc: 0.9579 - val_loss: 0.0388 - val_acc: 0.9893  \nEpoch 2/30 - 13s - loss: 0.0734 - acc: 0.9770 - val_loss: 0.0378 - val_acc: 0.9902  \nEpoch 3/30 - 12s - loss: 0.0618 - acc: 0.9812 - val_loss: 0.0310 - val_acc: 0.9914  \n ...  \nEpoch 28/30 - 13s - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0182 - val_acc: 0.9948  \nEpoch 29/30 - 13s - loss: 0.0156 - acc: 0.9955 - val_loss: 0.0177 - val_acc: 0.9952  \nEpoch 30/30 - 13s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0179 - val_acc: 0.9948    \nSubmitted 11/26/2015:  Scored 0.99614 placed #338.\n\n### Summary Of Results for Experiment 2 (Eliminate final drop out layer AND add a batch normalization layer before final Softmax. 30 epochs.)  \nEpoch 1/30 - 14s - loss: 0.1137 - acc: 0.9646 - val_loss: 0.0459 - val_acc: 0.9876  \nEpoch 2/30 - 13s - loss: 0.0715 - acc: 0.9779 - val_loss: 0.0316 - val_acc: 0.9893  \nEpoch 3/30 - 13s - loss: 0.0571 - acc: 0.9818 - val_loss: 0.0433 - val_acc: 0.9862  \n ...  \nEpoch 28/30 - 14s - loss: 0.0133 - acc: 0.9960 - val_loss: 0.0136 - val_acc: 0.9955  \nEpoch 29/30 - 14s - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0131 - val_acc: 0.9952  \nEpoch 30/30 - 14s - loss: 0.0128 - acc: 0.9960 - val_loss: 0.0129 - val_acc: 0.9952  \nSubmitted 11/26/2015:  Scored 0.99628 placed #311\n\n### Summary Of Results for Experiment 3 (Eliminate final drop out layer AND add a batch normalization layer before final Softmax. 100 epochs.)  \nEpoch 1/100 - 14s - loss: 0.1161 - acc: 0.9644 - val_loss: 0.1906 - val_acc: 0.9533  \nEpoch 2/100 - 14s - loss: 0.0725 - acc: 0.9779 - val_loss: 0.0476 - val_acc: 0.9855  \nEpoch 3/100 - 13s - loss: 0.0574 - acc: 0.9823 - val_loss: 0.0259 - val_acc: 0.9910  \n ...  \n Epoch 98/100 - 14s - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0145 - val_acc: 0.9964  \nEpoch 99/100 - 14s - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0147 - val_acc: 0.9967  \nEpoch 100/100 - 14s - loss: 0.0113 - acc: 0.9966 - val_loss: 0.0147 - val_acc: 0.9967  \nSubmitted 11/26/2015:  Scored 0.99714 placed #185\n\n### Summary Of Results for Experiment 4 (Eliminate final drop out layer AND add a batch normalization layer before final Softmax AND use Adam optimization. 100 epochs.)  \nStopped after 70 epochs because validation accuracy was stuck at 0.9955\nValidation error was twice as high as training error on each epoch.  Next experiment puts back the final drop out layer.\n\n### Summary Of Results for Experiment 5 (50% final drop out layer AND batch normalization layer before final Softmax AND use Adam optimization. 100 epochs.)\nEpoch 1/100 - 14s - loss: 0.1507 - acc: 0.9522 - val_loss: 0.0471 - val_acc: 0.9855  \nEpoch 2/100 - 14s - loss: 0.0888 - acc: 0.9730 - val_loss: 0.0380 - val_acc: 0.9871  \nEpoch 3/100 - 14s - loss: 0.0742 - acc: 0.9779 - val_loss: 0.0341 - val_acc: 0.9898  \n...  \nEpoch 98/100 - 14s - loss: 0.0140 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 0.9955  \nEpoch 99/100 - 14s - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0151 - val_acc: 0.9957  \nEpoch 100/100 - 15s - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0150 - val_acc: 0.9955  \nResults not improved. Not submitted. So far experiment 3 has had the best results.\n\n### Summary Of Results for Experiment 6 (Additional 128 node fully connected layer after the 256 node layer. Then 50% final drop out layer AND batch normalization layer before final Softmax AND use RMSProp optimization. 100 epochs.)\nEpoch 1/100 - 14s - loss: 0.1748 - acc: 0.9491 - val_loss: 0.0813 - val_acc: 0.9738  \nEpoch 2/100 - 14s - loss: 0.1001 - acc: 0.9710 - val_loss: 0.0378 - val_acc: 0.9883  \nEpoch 3/100 - 14s - loss: 0.0863 - acc: 0.9751 - val_loss: 0.0346 - val_acc: 0.9902  \n...  \nEpoch 98/100 - 15s - loss: 0.0130 - acc: 0.9964 - val_loss: 0.0182 - val_acc: 0.9957 \nEpoch 99/100 - 15s - loss: 0.0126 - acc: 0.9960 - val_loss: 0.0177 - val_acc: 0.9955  \nEpoch 100/100 - 15s - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0182 - val_acc: 0.9957  \nSubmitted 11/27/2015:  Scored 0.99657 .  Not an improvement over best result.\n\n### Summary Of Results for Experiment 7 (Just like Experiment 3 except 2nd 64 filter convolutional layer eliminated. 100 epochs.)  \nEpoch 1/100 - 14s - loss: 0.1709 - acc: 0.9468 - val_loss: 0.0508 - val_acc: 0.9843  \nEpoch 2/100 - 14s - loss: 0.1048 - acc: 0.9687 - val_loss: 0.0305 - val_acc: 0.9907  \nEpoch 3/100 - 14s - loss: 0.0880 - acc: 0.9736 - val_loss: 0.0486 - val_acc: 0.9850  \n...  \nEpoch 98/100 - 14s - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0162 - val_acc: 0.9952  \nEpoch 99/100 - 14s - loss: 0.0277 - acc: 0.9916 - val_loss: 0.0163 - val_acc: 0.9950  \nEpoch 100/100 - 14s - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0163 - val_acc: 0.9952  \n\n"},{"metadata":{"trusted":true,"_uuid":"88060cb31763e47d9f1c1f44be96dc3d57454811","_kg_hide-output":false},"cell_type":"code","source":"# predict results\nresults = model.predict(test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"_uuid":"fa8da0b6aa9056347221c0478d3c80762e4b3f00"},"cell_type":"code","source":"#+ These are all commented out when subm,itting ensemble. See below.\n#+ submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n#+ Put a copy of the submission in the Kaggle workoing directory\n#+ submission.to_csv(path_or_buf = \"cnn_mnist_datagen.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7791ce3b8b19f9c008499468057b0c5a29c2f208"},"cell_type":"markdown","source":"### Ensemble The Results of Three Models\nThis assumes three models have been committed and run and the resulting submission file downloaded to a local folder.\nThe three files are called: cnn_mnist_datagen_Experiment_07.csv, cnn_mnist_datagen_Experiment_03.csv and  cnn_mnist_datagen_Experiment_06.csv\n\nThe next code block compares the three predictions made for each image and chooses the one that occurs the most. If all three are different, use the value that's from \ncnn_mnist_datagen_Experiment_03.csv.  It had the best accuracy on the hidden test images.\n"},{"metadata":{"trusted":true,"_uuid":"fa27c41d774e0b127b5f07d6f57d8f3fbef09a60"},"cell_type":"code","source":"#+ Load the three prediction files needed for the ensemble.  It assumes these have been previously uploaded to the ../input folder in Kaggle.\nExp03 = pd.read_csv(\"../input/ensemble-input/cnn_mnist_datagen_Experiment_03.csv\")\nExp06 = pd.read_csv(\"../input/ensemble-input/cnn_mnist_datagen_Experiment_06.csv\")\nExp07 = pd.read_csv(\"../input/ensemble-input/cnn_mnist_datagen_Experiment_07.csv\")\n\n#+ Confirm the shapes. They should all be (28000, 2)\nassert Exp03.shape == (28000, 2)\nassert Exp06.shape == (28000, 2)\nassert Exp07.shape == (28000, 2)\n\n#+ Change column names\nExp03.columns = ['ImageId', 'Exp03']\nExp06.columns = ['ImageId', 'Exp06']\nExp07.columns = ['ImageId', 'Exp07']\n\n#+ Concatenate the three dataframes lable columns.\nEnsembleDF = pd.concat([Exp03, Exp06[['Exp06']], Exp07[['Exp07']]], axis=1)\n\n#+ Ensemble shape should be (28000, 4)\nassert EnsembleDF.shape == (28000, 4)\n\n#+ Ensemble column names should be ['ImageId', 'Exp03', 'Exp06', 'Exp07']\nassert list(EnsembleDF) == ['ImageId', 'Exp03', 'Exp06', 'Exp07']\n\n#+ Exp03 will be the baseline value. If the modal value (most frequent value) for a row is different from Exp03, use the modal value instead.\nmodevalues = pd.DataFrame(EnsembleDF.mode(axis=1, numeric_only=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b9fc17a9de4eee4f82bc84ca574d31b9f955865"},"cell_type":"code","source":"ModeValuesDF = modevalues\nModeValuesDF.columns = ['ModalValue', '1', '2', '3']\nlist(ModeValuesDF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0de36c988f33bb196ca9abd97939ae57406d79cc"},"cell_type":"code","source":"#+ Concatenate the three dataframes lable columns.\n\nEnsembleSubmission = pd.concat([Exp03['ImageId'], ModeValuesDF['ModalValue'].astype(int)], axis = 1)\nEnsembleSubmission.columns = ['ImageId', 'Label']\nEnsembleSubmission.to_csv(path_or_buf = \"cnn_mnist_datagen_ensemble.csv\",index=False)\nprint(EnsembleSubmission.shape)\nprint(EnsembleSubmission.head(100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770a367aec7161c9f8a6b72a883682b1fc93763d"},"cell_type":"markdown","source":"### Results of Ensembling\n\nUsing a three model ensemble I improved my score to 0.99742 which pushed me up to 162nd place (out of 2,714).  It was a 22 poistion improvement over the best single model score I had previously achieved."},{"metadata":{"trusted":true,"_uuid":"b54d6a2ce23570e551ed8f597ef748ebae0122ab"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}