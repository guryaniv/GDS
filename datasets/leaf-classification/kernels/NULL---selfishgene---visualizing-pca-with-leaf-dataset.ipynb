{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["***Visualizing PCA with Leaf Dataset***\n", "================================\n", "\n", "In this script we will apply PCA on leaf images and try to get a feel for the distribution of leaf images using visualizations that (hopefully) clarify different aspects about how to interpret PCA results.\n", "\n", "We will then continue to see if the PCA features are informative in terms of classifying leafs and determine how many of those we need.\n", "\n", "I've just updated another script, similar in nature to this one, just focused about k-means.\n", "if you enjoyed this one, be sure to also check out the [k-means script][1] as well\n", "\n", "  [1]: https://www.kaggle.com/selfishgene/leaf-classification/visualizing-k-means-with-leaf-dataset/notebook"], "metadata": {"_uuid": "e57fd7c2e8181b3224e7bcdd4c26bb0f824b529d", "_cell_guid": "3f7fe7df-1326-d944-2ec5-22c5c9155822"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib\n", "import matplotlib.image as mpimg\n", "import matplotlib.pyplot as plt\n", "import matplotlib.gridspec as gridspec\n", "\n", "from sklearn import model_selection\n", "from sklearn import decomposition\n", "from sklearn import linear_model\n", "from sklearn import ensemble\n", "from sklearn import neighbors\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.neighbors import KernelDensity\n", "from sklearn.manifold import TSNE\n", "from sklearn.metrics import accuracy_score\n", "\n", "from skimage.transform import rescale\n", "from scipy import ndimage as ndi\n", "\n", "matplotlib.style.use('fivethirtyeight')"], "metadata": {"collapsed": true, "_uuid": "2f4660dbd15090cbfdd88b93dc7c0c7bb57045ed", "_cell_guid": "f0c7a02b-f9b6-f3e8-7b9a-67d34671ef7e"}}, {"cell_type": "markdown", "source": ["## Load the data and Pre-process it\n", "\n", "For the sake of the script not being too cluttered, I suppressed the output of all intermediate plots during the data loading and preparation phases. anyone who is interested is welcome to **fork and unhide output** to see what is going on.\n", "\n", "(the main assumption of this pre-processing stage is that the absolute sizes of the leafs matter, and not just their shape. i.e. leafs with different sizes are most definitely different types of leafs. not sure if it's actually important, but just in case)"], "metadata": {"_uuid": "22276c5c5d157fffa47c5b0cecbde409201557cb", "_cell_guid": "347f2648-d405-9620-505c-c7d91d1f9b70"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["#%% load the data\n", "dataDir = '../input/'\n", "trainData = pd.read_csv(dataDir + 'train.csv')\n", "classEncoder = LabelEncoder()\n", "trainLabels = classEncoder.fit_transform(trainData.loc[:,'species'])\n", "trainIDs = np.array(trainData.loc[:,'id'])\n", "\n", "# show some random images\n", "plt.figure(figsize=(12,12))\n", "for k in range(28):\n", "    randTrainInd = np.random.randint(len(trainIDs))\n", "    randomID = trainIDs[randTrainInd]\n", "    imageFilename = dataDir + 'images/' + str(randomID) + '.jpg'\n", "    plt.subplot(4,7,k+1); plt.imshow(mpimg.imread(imageFilename), cmap='gray')\n", "    plt.title(classEncoder.classes_[trainLabels[randTrainInd]], fontsize=8); plt.axis('off')\n", "\n", "#%% preprocess images\n", "\n", "# go over training images and store them in a list\n", "numImages = 1584\n", "\n", "shapesMatrix = np.zeros((2,numImages))\n", "listOfImages = []\n", "for k in range(numImages):\n", "    imageFilename = dataDir + 'images/' + str(k+1) + '.jpg'\n", "    currImage = mpimg.imread(imageFilename)\n", "    shapesMatrix[:,k] = np.shape(currImage)\n", "    listOfImages.append(currImage)\n", "    \n", "# create a large 3d array with all images\n", "maxShapeSize = shapesMatrix.max(axis=1)\n", "for k in range(len(maxShapeSize)):\n", "    if maxShapeSize[k] % 2 == 0:\n", "        maxShapeSize[k] += 311\n", "    else:\n", "        maxShapeSize[k] += 310\n", "    \n", "fullImageMatrix3D = np.zeros(np.hstack((maxShapeSize,\n", "                                        np.shape(shapesMatrix[1]))).astype(int),dtype=np.dtype('u1'))\n", "destXc = (maxShapeSize[1]+1)/2; destYc = (maxShapeSize[0]+1)/2\n", "for k, currImage in enumerate(listOfImages):\n", "    Yc, Xc = ndi.center_of_mass(currImage)\n", "    Xd = destXc - Xc; Yd = destYc - Yc\n", "    rowIndLims = (int(round(Yd)),int(round(Yd)+np.shape(currImage)[0]))\n", "    colIndLims = (int(round(Xd)),int(round(Xd)+np.shape(currImage)[1]))\n", "    fullImageMatrix3D[rowIndLims[0]:rowIndLims[1],colIndLims[0]:colIndLims[1],k] = currImage\n", "\n", "# make sure nothing was ruined in the process\n", "plt.figure(figsize=(12,12))\n", "plt.suptitle('large reference frame images', fontsize=10)\n", "for k in range(28):\n", "    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])\n", "    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')\n", "\n", "# remove redundent rows and columns\n", "xValid = fullImageMatrix3D.mean(axis=2).sum(axis=0) > 0\n", "yValid = fullImageMatrix3D.mean(axis=2).sum(axis=1) > 0\n", "xLims = (np.nonzero(xValid)[0][0],np.nonzero(xValid)[0][-1])\n", "yLims = (np.nonzero(yValid)[0][0],np.nonzero(yValid)[0][-1])\n", "fullImageMatrix3D = fullImageMatrix3D[yLims[0]:yLims[1],xLims[0]:xLims[1],:]\n", "\n", "# make sure nothing was ruined in the process\n", "plt.figure(figsize=(12,12))\n", "plt.suptitle('final reference frame images', fontsize=10)\n", "for k in range(28):\n", "    randInd = np.random.randint(np.shape(fullImageMatrix3D)[2])\n", "    plt.subplot(4,7,k+1); plt.imshow(fullImageMatrix3D[:,:,randInd], cmap='gray'); plt.axis('off')\n", "\n", "# scale down all images\n", "rescaleFactor = 0.15\n", "\n", "scaledDownImage = rescale(fullImageMatrix3D[:,:,0],rescaleFactor)\n", "scaledDownImages = np.zeros(np.hstack((np.shape(scaledDownImage),\n", "                                       np.shape(fullImageMatrix3D)[2])),dtype=np.dtype('f4'))\n", "for imInd in range(np.shape(fullImageMatrix3D)[2]):\n", "    scaledDownImages[:,:,imInd] = rescale(fullImageMatrix3D[:,:,imInd],rescaleFactor)\n", "    \n", "del fullImageMatrix3D"], "metadata": {"_kg_hide-output": true, "_uuid": "44192704e8667c4bc802c472b5c638e958191491", "_cell_guid": "a77bab6e-468f-ed20-10fb-3305ae95ee9b"}}, {"cell_type": "markdown", "source": ["## Look at the final processing stage and view several random leaf images:"], "metadata": {"_uuid": "5c07a06b187fbb71cf54427c1fb9065f06639063", "_cell_guid": "7a132a3c-d492-66b5-fc74-884c58dfd0f7"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["np.random.seed(1) # use a nice looking random seed\n", "\n", "plt.figure(figsize=(12,10));\n", "for k in range(25):\n", "    randInd = np.random.randint(np.shape(scaledDownImages)[2])\n", "    plt.subplot(5,5,k+1); \n", "    plt.imshow(scaledDownImages[:,:,randInd], cmap='gray'); \n", "    plt.axis('off'); plt.title('imageID = ' + str(randInd), fontsize=12)\n", "plt.tight_layout()"], "metadata": {"_uuid": "6968fb0185d84a10743d335b2fe429ee92cc65ab", "_cell_guid": "8e6ffe82-b590-d33a-9311-ad7de33d3942"}}, {"cell_type": "markdown", "source": ["## Define a 'GaussianModel' class that will help us visualize things:\n", "\n", "This is long, so I've hidden the code, but if you are intereseted in delving deeper and looking at the implementation then please unhide or better yet fork the script and try playing around by editing the code."], "metadata": {"_uuid": "f98c08921d95ec273b556aea4a18d1271de60576", "_cell_guid": "e4890d77-fc68-530a-951d-fc097d4c2bf2"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["#%% define GaussianModel class\n", "\n", "class GaussianModel:\n", "    def __init__(self, X, numBasisFunctions=10, objectPixels=None):\n", "        '''\n", "        inputs: \n", "            X                    - numSamples x numDimentions matrix\n", "            numBasisFunctions       - number of basis function to use\n", "            objectPixels (optional) - an binnary mask image used for presentation\n", "                                      will be used as Im[objectPixels] = dataSample\n", "                                      must satisfy objectPixels.ravel().sum() = X.shape[1]\n", "        '''\n", "        \n", "        self.numBasisFunctions = numBasisFunctions        \n", "        if objectPixels is None:\n", "            self.objectPixels = np.ones((1,X.shape[1]),dtype=np.bool)\n", "        else:\n", "            self.objectPixels = objectPixels\n", "        assert(self.objectPixels.ravel().sum() == X.shape[1])\n", "\n", "        PCAModel = decomposition.PCA(n_components=numBasisFunctions, whiten=True)\n", "        self.dataRepresentation = PCAModel.fit_transform(X)\n", "        self.PCAModel = PCAModel\n", "\n", "    def RepresentUsingModel(self, X):\n", "        return self.PCAModel.transform(X)\n", "\n", "    def ReconstructUsingModel(self, X_transformed):\n", "        return self.PCAModel.inverse_transform(X_transformed)\n", "\n", "    def InterpretUsingModel(self, X):\n", "        return self.PCAModel.inverse_transform(self.PCAModel.transform(X))\n", "\n", "    # shows the eigenvectors of the gaussian covariance matrix\n", "    def ShowVarianceDirections(self, numDirectionsToShow=16):\n", "        numDirectionsToShow = min(numDirectionsToShow, self.numBasisFunctions)\n", "        \n", "        numFigRows = 4; numFigCols = 4;\n", "        numDirectionsPerFigure = numFigRows*numFigCols\n", "        numFigures = int(np.ceil(float(numDirectionsToShow)/numDirectionsPerFigure))\n", "        \n", "        for figureInd in range(numFigures):\n", "            plt.figure()\n", "            for plotInd in range(numDirectionsPerFigure):\n", "                eigVecInd = numDirectionsPerFigure*figureInd + plotInd\n", "                if eigVecInd >= self.numBasisFunctions:\n", "                    break\n", "                deltaImage = np.zeros(np.shape(self.objectPixels))\n", "                deltaImage[self.objectPixels] = self.PCAModel.components_[eigVecInd,:].ravel()\n", "\n", "                plt.subplot(numFigRows,numFigCols,plotInd+1)\n", "                if np.shape(self.objectPixels)[0] == 1:\n", "                    plt.plot(deltaImage)\n", "                else:\n", "                    plt.imshow(deltaImage,cmap='jet'); plt.axis('off')\n", "                titleStr = str(100*self.PCAModel.explained_variance_ratio_[eigVecInd])[0:5]\n", "                plt.title(titleStr + '% explained');\n", "            plt.tight_layout()\n", "            \n", "    # shows several random model reconstructions\n", "    def ShowReconstructions(self, X, numReconstructions=5):\n", "        assert(np.shape(X)[1] == self.objectPixels.ravel().sum())\n", "        numSamples = np.shape(X)[0]\n", "        numReconstructions = min(numReconstructions, numSamples)\n", "        \n", "        originalImage      = np.zeros(np.shape(self.objectPixels))\n", "        reconstructedImage = np.zeros(np.shape(self.objectPixels))\n", "        \n", "        numReconstructionsPerFigure = min(5, numReconstructions)\n", "        numFigures = int(np.ceil(float(numReconstructions)/numReconstructionsPerFigure))\n", "        \n", "        for figureInd in range(numFigures):\n", "            plt.figure()\n", "            for plotCol in range(numReconstructionsPerFigure):\n", "                dataSampleInd = np.random.randint(numSamples)\n", "                originalImage[self.objectPixels] = X[dataSampleInd,:].ravel()\n", "                reconstructedImage[self.objectPixels] = \\\n", "                    self.InterpretUsingModel(np.reshape(X[dataSampleInd,:],[1,-1])).ravel()\n", "                diffImage = abs(originalImage - reconstructedImage)\n", "                \n", "                # original image\n", "                plt.subplot(3,numReconstructionsPerFigure,0*numReconstructionsPerFigure+plotCol+1)\n", "                if np.shape(self.objectPixels)[0] == 1:\n", "                    plt.plot(originalImage); plt.title('original signal')\n", "                else:\n", "                    plt.imshow(originalImage, cmap='gray'); \n", "                    plt.title('original image'); plt.axis('off')\n", "                    \n", "                # reconstred image\n", "                plt.subplot(3,numReconstructionsPerFigure,1*numReconstructionsPerFigure+plotCol+1)\n", "                if np.shape(self.objectPixels)[0] == 1:\n", "                    plt.plot(reconstructedImage); plt.title('reconstructed signal')\n", "                else:\n", "                    plt.imshow(reconstructedImage, cmap='gray'); \n", "                    plt.title('reconstructed image'); plt.axis('off')\n", "\n", "                # diff image\n", "                plt.subplot(3,numReconstructionsPerFigure,2*numReconstructionsPerFigure+plotCol+1)\n", "                if np.shape(self.objectPixels)[0] == 1:\n", "                    plt.plot(diffImage); plt.title('abs difference signal')\n", "                else:\n", "                    plt.imshow(diffImage, cmap='gray'); \n", "                    plt.title('abs difference image'); plt.axis('off')\n", "            plt.tight_layout()\n", "\n", "    # shows distrbution along the variance directions and several images along that variance direction\n", "    def ShowModelVariations(self, numVariations=5):\n", "\n", "        showAsTraces = (np.shape(self.objectPixels)[0] == 1)\n", "        numVariations = min(numVariations, self.numBasisFunctions)\n", "                \n", "        numVarsPerFigure = min(5,numVariations)\n", "        numFigures = int(np.ceil(float(numVariations)/numVarsPerFigure))\n", "        \n", "        lowRepVec     = np.percentile(self.dataRepresentation, 2, axis=0)\n", "        medianRepVec  = np.percentile(self.dataRepresentation, 50, axis=0)\n", "        highRepVec    = np.percentile(self.dataRepresentation, 98, axis=0)\n", "\n", "        for figureInd in range(numFigures):\n", "            plt.figure()\n", "            for plotCol in range(numVarsPerFigure):\n", "                eigVecInd = numVarsPerFigure*figureInd+plotCol\n", "                if eigVecInd >= self.numBasisFunctions:\n", "                    break\n", "\n", "                # create the low and high precentile representation activation vectors\n", "                currLowPrecentileRepVec             = medianRepVec.copy()\n", "                currLowPrecentileRepVec[eigVecInd]  = lowRepVec[eigVecInd]\n", "                currHighPrecentileRepVec            = medianRepVec.copy()\n", "                currHighPrecentileRepVec[eigVecInd] = highRepVec[eigVecInd]\n", "\n", "                # create blank images\n", "                deltaImage          = np.zeros(np.shape(self.objectPixels))\n", "                medianImage         = np.zeros(np.shape(self.objectPixels))\n", "                lowPrecentileImage  = np.zeros(np.shape(self.objectPixels))\n", "                highPrecentileImage = np.zeros(np.shape(self.objectPixels))\n", "\n", "                # fill the object pixels with the relevant data\n", "                deltaImage[self.objectPixels]          = \\\n", "                        self.PCAModel.components_[eigVecInd,:].ravel()\n", "                lowPrecentileImage[self.objectPixels]  = \\\n", "                        self.ReconstructUsingModel(currLowPrecentileRepVec).ravel()\n", "                medianImage[self.objectPixels]         = \\\n", "                        self.ReconstructUsingModel(medianRepVec).ravel()\n", "                highPrecentileImage[self.objectPixels] = \\\n", "                        self.ReconstructUsingModel(currHighPrecentileRepVec).ravel()\n", "\n", "                # calculate the Gaussian smoothed distribution of values along the eignevector direction\n", "                sigmaOfKDE = 0.12\n", "                pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE\n", "                pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE\n", "                xAxis = np.linspace(pdfStart,pdfStop,200)\n", "                PDF_Model = KernelDensity(kernel='gaussian', \n", "                                  bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))\n", "                logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))\n", "\n", "                # show distribution of current component \n", "                plt.subplot(5,numVarsPerFigure,0*numVarsPerFigure+plotCol+1)\n", "                plt.fill(xAxis, np.exp(logPDF), fc='b');\n", "                percentExplainedString = str(100*self.PCAModel.explained_variance_ratio_[eigVecInd])[0:5]\n", "                plt.title(percentExplainedString + '% explained'); \n", "                \n", "                # show variance direction (eigenvector)\n", "                plt.subplot(5,numVarsPerFigure,1*numVarsPerFigure+plotCol+1);\n", "                if showAsTraces:\n", "                    plt.plot(deltaImage); plt.title('eigenvector ' + str(eigVecInd))\n", "                else:\n", "                    plt.imshow(deltaImage, cmap='jet'); \n", "                    plt.title('eigenvector ' + str(eigVecInd)); plt.axis('off')\n", "\n", "                # show 2nd precentile image\n", "                plt.subplot(5,numVarsPerFigure,2*numVarsPerFigure+plotCol+1)\n", "                if showAsTraces:\n", "                    plt.plot(lowPrecentileImage); plt.title('2nd precentile')\n", "                else:\n", "                    plt.imshow(lowPrecentileImage, cmap='gray'); \n", "                    plt.title('2nd precentile image'); plt.axis('off')\n", "\n", "                # show median image\n", "                plt.subplot(5,numVarsPerFigure,3*numVarsPerFigure+plotCol+1)\n", "                if showAsTraces:\n", "                    plt.plot(medianImage); plt.title('median signal')\n", "                else:\n", "                    plt.imshow(medianImage, cmap='gray'); \n", "                    plt.title('median Image'); plt.axis('off')\n", "\n", "                # show 98th precentile image\n", "                plt.subplot(5,numVarsPerFigure,4*numVarsPerFigure+plotCol+1)\n", "                if showAsTraces:\n", "                    plt.plot(highPrecentileImage); plt.title('98th precentile')\n", "                else:\n", "                    plt.imshow(highPrecentileImage, cmap='gray'); \n", "                    plt.title('98th precentile image'); plt.axis('off')\n", "            plt.tight_layout()\n", "        \n", "    # shows distrbution along the variance directions and several images along that variance direction\n", "    def ShowSingleComponentVariation(self, X, listOfComponents=[0,1]):\n", "\n", "        showAsTraces = (np.shape(self.objectPixels)[0] == 1)\n", "        assert(all([(x in range(self.numBasisFunctions)) for x in listOfComponents]))\n", "                \n", "        X_rep = self.RepresentUsingModel(X)\n", "        \n", "        percentilesToShow = [1,20,40,60,80,99]\n", "        numReadDataSamplePerPercentile = 4\n", "        representationPercentiles = []\n", "        for percentile in percentilesToShow:\n", "            representationPercentiles.append(np.percentile(self.dataRepresentation, percentile, axis=0))\n", "        medianRepVec =  np.percentile(self.dataRepresentation, 50, axis=0)\n", "\n", "        for eigVecInd in listOfComponents:\n", "            plt.figure(); gs = gridspec.GridSpec(numReadDataSamplePerPercentile+2,\n", "                                                 len(percentilesToShow))\n", "\n", "            # calculate the Gaussian smoothed distribution of values along the eignevector direction\n", "            sigmaOfKDE = 0.12\n", "            pdfStart   = min(self.dataRepresentation[:,eigVecInd]) - 3*sigmaOfKDE\n", "            pdfStop    = max(self.dataRepresentation[:,eigVecInd]) + 3*sigmaOfKDE\n", "            xAxis = np.linspace(pdfStart,pdfStop,200)\n", "            PDF_Model = KernelDensity(kernel='gaussian', \n", "                              bandwidth=sigmaOfKDE).fit(self.dataRepresentation[:,eigVecInd].reshape(-1,1))\n", "            logPDF = PDF_Model.score_samples(xAxis.reshape(-1,1))\n", "            percentileValuesToShow = \\\n", "                    [representationPercentiles[x][eigVecInd] for x in range(len(representationPercentiles))]\n", "            percentilesToShowLogPDF = \\\n", "                    PDF_Model.score_samples(np.array(percentileValuesToShow).reshape(-1,1))\n", "\n", "            # show distribution of current component and red dots at the list of precentiles to show \n", "            plt.subplot(gs[0,:])\n", "            plt.fill(xAxis, np.exp(logPDF), fc='b');\n", "            plt.scatter(percentileValuesToShow, np.exp(percentilesToShowLogPDF), c='r',s=40);\n", "            plt.title(str(100*self.PCAModel.explained_variance_ratio_[eigVecInd]) + '% explained');\n", "            \n", "            for plotCol, currPrecentile in enumerate(percentilesToShow):                \n", "                currPrecentileRepVec             = medianRepVec.copy()\n", "                currPrecentileRepVec[eigVecInd]  = representationPercentiles[plotCol][eigVecInd]\n", "                \n", "                currPrecentileImage = np.zeros(np.shape(self.objectPixels))\n", "                currPrecentileImage[self.objectPixels] = \\\n", "                        self.ReconstructUsingModel(currPrecentileRepVec).ravel()\n", "                \n", "                # show the median image with current precentile as activation of the curr image\n", "                plt.subplot(gs[1,plotCol]);\n", "                if showAsTraces:\n", "                    plt.plot(currPrecentileImage); \n", "                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%')\n", "                else:\n", "                    plt.imshow(currPrecentileImage, cmap='gray'); \n", "                    plt.title('precentile: ' + str(percentilesToShow[plotCol]) + '%'); \n", "                    plt.axis('off')\n", "\n", "                # find the most suitible candidates in X for current precentile\n", "                distFromPercentile = abs(X_rep[:,eigVecInd] - \n", "                                         representationPercentiles[plotCol][eigVecInd])\n", "                X_inds = np.argpartition(distFromPercentile, \n", "                                         numReadDataSamplePerPercentile)[:numReadDataSamplePerPercentile]\n", "                for k, X_ind in enumerate(X_inds):\n", "                    currNearestPrecentileImage = np.zeros(np.shape(self.objectPixels))\n", "                    currNearestPrecentileImage[self.objectPixels]  = X[X_ind,:].ravel()\n", "                    \n", "                    plt.subplot(gs[2+k,plotCol]);\n", "                    if showAsTraces:\n", "                        plt.plot(currNearestPrecentileImage); \n", "                        plt.title('NN with closest percentile');\n", "                    else:\n", "                        plt.imshow(currNearestPrecentileImage, cmap='gray'); \n", "                        plt.title('NN with closest percentile'); plt.axis('off')\n", "            plt.tight_layout()\n", "            \n", "    def ShowDataScatterPlotsWithTSNE(self, X=None, y=None, tSNE_perplexity=30.0, colorMap='Paired'):\n", "        \n", "        if X is None:\n", "            X_rep = self.dataRepresentation\n", "        else:\n", "            X_rep = self.RepresentUsingModel(X)\n", "            \n", "        if y is None:\n", "            y = np.ones(X_rep.shape[0])\n", "            \n", "        tSNE_PCAModel = TSNE(n_components=2, perplexity=tSNE_perplexity, random_state=0)\n", "        X_rep_tSNE = tSNE_PCAModel.fit_transform(X_rep) \n", "        (tSNE_xmin, tSNE_xmax) = (np.percentile(X_rep_tSNE[:,0], 0.3), np.percentile(X_rep_tSNE[:,0], 99.7))\n", "        (tSNE_ymin, tSNE_ymax) = (np.percentile(X_rep_tSNE[:,1], 0.3), np.percentile(X_rep_tSNE[:,1], 99.7))\n", "\n", "        plt.figure()\n", "        plt.subplot(1,2,1); \n", "        plt.scatter(X_rep[:,0],X_rep[:,1],c=y,cmap=colorMap,s=10,alpha=0.9)\n", "        plt.title('PCA representation'); plt.xlabel('PC1 coeff'); plt.ylabel('PC2 coeff')\n", "        plt.subplot(1,2,2); \n", "        plt.scatter(X_rep_tSNE[:,0],X_rep_tSNE[:,1],c=y,cmap=colorMap,s=10,alpha=0.9)\n", "        plt.xlim(tSNE_xmin, tSNE_xmax); plt.ylim(tSNE_ymin, tSNE_ymax);\n", "        plt.title('t-SNE representation'); plt.xlabel('t-SNE axis1'); plt.ylabel('t-SNE axis2')"], "metadata": {"_kg_hide-output": true, "collapsed": true, "_uuid": "1d48738f23b203d0acdaf81df67f9100ceac2c62", "_cell_guid": "c80e25cd-829b-742e-f6a8-482aea76757a", "_kg_hide-input": true}}, {"cell_type": "markdown", "source": ["## Train The Gaussian Model (also known as PCA)"], "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["# train the Gaussian Model \n", "sampleDim = np.shape(scaledDownImages)[0]*np.shape(scaledDownImages)[1]\n", "X = scaledDownImages.reshape(sampleDim,-1).T\n", "\n", "objectPixelsMask = np.ones((np.shape(scaledDownImages)[0],np.shape(scaledDownImages)[1]))==1\n", "leaf_PCAModel = GaussianModel(X, numBasisFunctions=100, objectPixels=objectPixelsMask)"], "metadata": {"_kg_hide-output": true, "_uuid": "98028c6407ce3dfedd16dd7227a15749dc0cb4ea", "_cell_guid": "c2a00d98-472a-e719-e4bc-37ae275a44a1"}}, {"cell_type": "markdown", "source": ["## Now lets look at the main variance directions of the PCA\n", "\n", "These are also known as Principal Components.  \n", "Each image can be though of as a different \"direction\" in the high dimensional image space"], "metadata": {"_uuid": "1b7d4bde992ae2eacbf389c9e796b74a58e8e666", "_cell_guid": "34227248-456f-b1f6-4164-e462246af904"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 10\n", "matplotlib.rcParams['figure.figsize'] = (12,9)\n", "leaf_PCAModel.ShowVarianceDirections(numDirectionsToShow=16)"], "metadata": {"_uuid": "332bf3b8ccf962af5d191dcbad297f97ef249ce6", "_cell_guid": "018bcc67-d694-2094-e413-12b66b25a7ff"}}, {"cell_type": "markdown", "source": ["We can see some interesting shapes arising from the data, especially the first and second row look nice, and we will soon understand exactly what these images mean. \n", "\n", "But first, let's look at **some original images** and how the low dimensional PCA model can **reconstruct** them."], "metadata": {"_uuid": "574dc2f6f90de41a57c5b3465259a129bcba6712", "_cell_guid": "7b5f8094-d104-0aba-f6c1-a0b6a18bd703"}}, {"cell_type": "markdown", "source": ["## Show some image and their model Reconstructions\n"], "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 9\n", "matplotlib.rcParams['figure.figsize'] = (12,5)\n", "leaf_PCAModel.ShowReconstructions(X, numReconstructions=10)"], "metadata": {"_uuid": "0a4ef1f0a89e56ddcf6aa0cbf0aebcfdf82cfd52", "_cell_guid": "384eb592-686a-8613-29d2-6f0d0c5fec6e"}}, {"cell_type": "markdown", "source": ["From the absolute difference images on the bottom row of both plots, we can see that the main regions that cannot be reconstructed are the edges of the leafs, but the general leaf structure can be reconstructed using the 100 basis functions.\n", "\n", "**Now, let's take a closer look at how the leaf images vary around the mean image:**"], "metadata": {"_uuid": "9ad2126477bd2d2046bb8f4fed49e82aaf8b14a5", "_cell_guid": "04c88ee9-b11d-779c-0188-0a6d1cfcb3f4"}}, {"cell_type": "markdown", "source": ["## Show Model Variations around the Mean Image\n"], "metadata": {}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 9\n", "matplotlib.rcParams['figure.figsize'] = (12,9)\n", "leaf_PCAModel.ShowModelVariations(numVariations=5)"], "metadata": {"_uuid": "ad951d20270f7fc46cb62b5f526c3925ce1e4bec", "_cell_guid": "731c0170-8911-7489-b28c-e7607226dad3"}}, {"cell_type": "markdown", "source": ["**For those of us unfamiliar with this kind of a plot and since this is quite a busy plot, let me explain what we see:**\n", "------------------------------------------------------------------------\n", "  \n", "\n", "\n", "----------\n", "\n", "\n", " - The upper most row contains the data distributions of each eigenvector (i.e. the histogram along that \"direction\")\n", " - The second row contains what we already saw in a previous plot, what we called the variance directions.\n", " - The forth row contains the median image of leafs. notice that this row is identical for all eigenvectors\n", " - The third row holds the 2nd percentile images of each eigenvector. it's easier to think of this as the median image minus the eigenvector image multiplied by some constant. i.e the image we see is the forth row image, minus the second row image, when the second row image is multiplied by a constant. The constant is chosen to show the varying degree of influence of this specific eigenvector on the \"average\" image, so we can visualize what type of variation this particular eigenvector tends to capture. 2nd percentile will subtract a relatively large absolute value from the median image, showing us what images look like when this coefficient is highly negative. 98th percentile would be just the opposite, showing us what images look like when this coefficient is at the upper end of the range. 50th percentile would give us a \"middle of the road\" effect of this coefficient.\n", "\n", "\n", "----------\n", "\n", "\n", "This plot helps us visualize what a direction in this high dimensional image space means. For example:\n", "\n", " - **The first eigenvector** (leftmost column), we can see\n", "   that it **controls the difference between large radius leafs and small radius\n", "   leafs**. i.e we can say that some of the variance along the change of leaf radius is explained by this component.\n", " - The **second eigenvector** (second column from the left) controls the difference between an\n", "   **upright vetrically oriented leaf** and a **horizontally oriented leaf**.\n", "\n", "----------\n", "\n", "We can now deep deeper into some interesting looking eigenvectors\n", "\n", "**Eigenvector 1:**\n", "------------------"], "metadata": {"_uuid": "b299cafa47119dec2fc33b0d7ef95a64f8e7ab0d", "_cell_guid": "67673f0d-c46f-1550-7655-99d0fe568efc"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 6\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[0])"], "metadata": {"_uuid": "a4e280ce33458b67d810c1102c187d3c609b2b1e", "_cell_guid": "7e15f8c5-44fb-d5f4-3b75-deef69e770de"}}, {"cell_type": "markdown", "source": ["Lets explain what we see:\n", "\n", " - the first row shows the data distribution of the coefficients along this main variance direction. the red dots correspond to 1st, 20th, 40th, 60th, 80th and 99th percentiles of this distribution.\n", " - the second row is like the columns were in the previous plot. for example, we can see here in this particular case a gradual increase in leaf size from left to right.\n", " - the bottom 4 rows at each column hold real leaf images that have the first PCA coefficient be at the value of the corresponding percentile  of that column. for example, the left most 4 bottom pictures are leafs with a PC1 coefficient to be approximately -1.6 and the right most 4 bottom pictures are leafs with a PC1 coefficient to be approximately 2.7\n", "\n", "By examining the the leafs that have different coefficients **we can see what this component coefficient represents**.  from the point of view of this particular component, the leaf images in the same column are very similar. we can therefore see what this particular feature \"thinks\" about similar leafs.\n", "\n", "In this particular case we can see that it's about **leaf size** since we see **a gradual increase in leaf size from left to right**.\n", "\n", "\n", "----------\n", "\n", "\n", "**Eigenvector 2:**\n", "------------------\n", "\n"], "metadata": {"_uuid": "090b81bd356f313c1f9f8ae49ada0c4ff2b3d0ea", "_cell_guid": "071253fe-e5a4-c5f0-e721-f19326e205c0"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 6\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[1])"], "metadata": {"_uuid": "a105cfdbb56bbea98dbe10fcd6cdb9554df47d96", "_cell_guid": "a3e69802-5266-beeb-aa75-04dc3915a4ab"}}, {"cell_type": "markdown", "source": ["Here we can see that the second principal component is about explaining the difference between vertical and horizontal leafs"], "metadata": {"_uuid": "de4bc084ebb9f0a3520f9856bd49c263e8c5b8dc", "_cell_guid": "6216ea32-cf9d-f918-3a08-1e0c81037d4b"}}, {"cell_type": "markdown", "source": ["**Eigenvector 4:**\n", "------------------"], "metadata": {"_uuid": "b4152cecf9cfbf0c3551597366361cddaa9f177c", "_cell_guid": "cab18f4e-00b8-5d9c-39cb-62e75118c902"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 6\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[3])"], "metadata": {"_uuid": "bcbf5eba827622c066186710e4135d85df53c7f4", "_cell_guid": "85b2caca-1cfd-b449-d749-2f82b077dc8e"}}, {"cell_type": "markdown", "source": ["This is another eigenvector that is about vertical vs horizontal, but we can see on the left most column that in addition to being horizontal, these leaf images also have a pointy tip at the top (well, except from the first image)\n", "\n", "\n", "**EigenVector 8:**\n", "------------------"], "metadata": {"_uuid": "44acbcd9e499918a21419392dfd6a0aa59d873ca", "_cell_guid": "ecdbfdba-bbe6-30aa-b467-7733379c31ba"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["matplotlib.rcParams['font.size'] = 6\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "leaf_PCAModel.ShowSingleComponentVariation(X, listOfComponents=[7])"], "metadata": {"_uuid": "399eb24a1c1a11b7a92578a2887df62632453f6a", "_cell_guid": "cf8e4022-e4c1-f6ef-8ad1-bff41a70adb6"}}, {"cell_type": "markdown", "source": ["This one is about beeing a star like leaf"], "metadata": {"_uuid": "5789346efe4fcb55298b7d77c49d28e0baae398f", "_cell_guid": "58a367b9-09d4-550a-e8bf-05e56fa36643"}}, {"cell_type": "markdown", "source": ["## Show Scatter plot of Leaf images as points in high dimentional space\n", "Ok, now that we have some grasp about these distributions, let's also visualize the scatter of the subspace that is spanned by the PCs. we will do this in two ways:\n", "\n", " - Plot the scatter plot of the **first two principal component coeffients**\n", " - Plot a **2D approximation** of the \"high dimensional scatter plot\" of the entire space using **t-SNE** "], "metadata": {"_uuid": "98e9f37aff242c8d2daa8f312e51d5ffbcb1fc3a", "_cell_guid": "cff0bb8b-3613-0f04-5519-48b0e5207e2f"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["#%% plot scatter of 2 PCs and t-SNE of all PCs (with labels as colors)\n", "matplotlib.rcParams['font.size'] = 12\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "\n", "X_train = X[trainIDs-1,:]\n", "y_train = trainLabels\n", "\n", "leaf_PCAModel.ShowDataScatterPlotsWithTSNE(X_train, y_train, tSNE_perplexity=10.0)"], "metadata": {"_uuid": "5ef4faf6a13b186c915a2d5f70719fd4ced81fcc", "_cell_guid": "3e6549c4-a554-44f6-0c1e-1fba63c2bc4e"}}, {"cell_type": "markdown", "source": ["We can see that nearby points usually have similar color and this means they have similar leaf label. This makes us confident that we can achieve at least some classification accuracy from these PCA features.\n", "\n", "\n", "----------\n", "## Show Model Accuracy as function of num PCA components\n", "\n", "Now, let's see what is the **classification accuracy** using this PCA representation if we use **different amount of PCA coefficients** for **several different types of classifiers**."], "metadata": {"_uuid": "b2141113b8eaaa05f25cffae7464efaf4bc6dedc", "_cell_guid": "1c1029eb-7f72-6e45-d936-e6c14e031360"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": ["#%% plot CV classification accuracy as function of num components used for 3 very different type of classifiers\n", "matplotlib.rcParams['font.size'] = 12\n", "matplotlib.rcParams['figure.figsize'] = (12,8)\n", "\n", "X_PCA = leaf_PCAModel.RepresentUsingModel(X)\n", "\n", "X_PCA_train = X_PCA[trainIDs-1,:]\n", "y_train = trainLabels\n", "\n", "numPCsToUse = [1,2,4,8,16,32,64]\n", "\n", "logReg = linear_model.LogisticRegression(C=10.0)\n", "kNN = neighbors.KNeighborsClassifier(n_neighbors=7)\n", "RF = ensemble.RandomForestClassifier(n_estimators=100)\n", "\n", "logRegMeanAccuracy = []; kNN_MeanAccuracy = []; RF_MeanAccuracy = []\n", "logRegAccuracyStd  = []; kNN_AccuracyStd  = []; RF_AccuracyStd  = []\n", "\n", "for numPCs in numPCsToUse:\n", "    stratifiedCV = model_selection.StratifiedKFold(n_splits=5, random_state=1)\n", "    logRegAccuracy = []; kNN_Accuracy = []; RF_Accuracy = []\n", "    for trainInds, validInds in stratifiedCV.split(X_PCA_train, y_train):\n", "        X_train_cv = X_PCA_train[trainInds,:numPCs]\n", "        X_valid_cv = X_PCA_train[validInds,:numPCs]\n", "\n", "        y_train_cv = y_train[trainInds]\n", "        y_valid_cv = y_train[validInds]\n", "\n", "        logReg.fit(X_train_cv, y_train_cv)\n", "        kNN.fit(X_train_cv, y_train_cv)\n", "        RF.fit(X_train_cv, y_train_cv)\n", "    \n", "        logRegAccuracy.append(accuracy_score(y_valid_cv, logReg.predict(X_valid_cv)))\n", "        kNN_Accuracy.append(accuracy_score(y_valid_cv, kNN.predict(X_valid_cv)))\n", "        RF_Accuracy.append(accuracy_score(y_valid_cv, RF.predict(X_valid_cv)))\n", "\n", "    logRegMeanAccuracy.append(np.array(logRegAccuracy).mean())\n", "    logRegAccuracyStd.append(np.array(logRegAccuracy).std())\n", "\n", "    kNN_MeanAccuracy.append(np.array(kNN_Accuracy).mean())\n", "    kNN_AccuracyStd.append(np.array(kNN_Accuracy).std())\n", "\n", "    RF_MeanAccuracy.append(np.array(RF_Accuracy).mean()) \n", "    RF_AccuracyStd.append(np.array(RF_Accuracy).std())\n", "        \n", "plt.figure()\n", "plt.errorbar(x=numPCsToUse, y=logRegMeanAccuracy, yerr=logRegAccuracyStd)\n", "plt.errorbar(x=numPCsToUse, y=kNN_MeanAccuracy  , yerr=kNN_AccuracyStd)\n", "plt.errorbar(x=numPCsToUse, y=RF_MeanAccuracy   , yerr=RF_AccuracyStd)\n", "plt.xlim(min(numPCsToUse)-1,max(numPCsToUse)+1); \n", "plt.legend(['Logistic Regression','k Nearest Neighbor','Random Forest'],loc=2)\n", "plt.xlabel('num PCA Components'); \n", "plt.ylabel('Validation Accuracy'); \n", "plt.title('Accuracy as function of num PCs')"], "metadata": {"_uuid": "28de7984bd1ecd96dfc10e050e5f002a61dc2f7f", "_cell_guid": "3ccd8f59-2478-1dce-9513-958ff604edb1"}}, {"cell_type": "markdown", "source": ["Overall, it's evident that all classifiers achieve approximately similar performance.\n", "\n", "But it's interesting to note the somewhat different behavior of these different classifiers as a function of number of components used. \n", "\n", "For example, the nearest neighbor classifier flattens out early and does not benefit from additional components beyond 8, whereas the logistic regression classifier continues to increase it's performance up to around 32 components.\n", "The Random Forest classifier is consistently the best performing but it also flattens out at around 32 components."], "metadata": {"_uuid": "61fed6a317ac5341461787793bf6845e75734714", "_cell_guid": "2fd63a23-eb3e-1ac0-1e99-cb63f631e783"}}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python"}, "_change_revision": 0, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "_is_fork": false}}