{"cells":[{"metadata":{"_uuid":"853cf299effe8df27af023e0d5ca632e597721aa"},"cell_type":"markdown","source":"# Outline of this notebook "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 300)\n%matplotlib inline\n\nsns.set(style='white', context='notebook', palette='deep')\nmycols = [\"#66c2ff\", \"#5cd6d6\", \"#00cc99\", \"#85e085\", \"#ffd966\", \"#ffb366\", \"#ffb3b3\", \"#dab3ff\", \"#c2c2d6\"]\nsns.set_palette(palette = mycols, n_colors = 4)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom contextlib import contextmanager\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nimport warnings\nimport gc\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30b0c20891ae049566767c057a8fb6080fc18e34"},"cell_type":"markdown","source":"# 1. Read the Dataset and Select the Target Variable"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c2bbbc2c794201e79e589f0275a518204c166ad"},"cell_type":"code","source":"def load_data():\n    train_set = pd.read_csv('../input/costa-rican-household-poverty-prediction/train.csv')\n    test_set = pd.read_csv('../input/costa-rican-household-poverty-prediction/test.csv')\n    print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n    print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')\n    #Let's take a look at target\n    target = train_set['Target']\n    target.value_counts(normalize=True)\n    return (train_set,test_set,target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2e4882f6fc7e23559f6180c66975a6e04eaa869"},"cell_type":"markdown","source":"# 2. Handle the missing value."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74a46bbfa5604615ad23ea45736a0b6776a68784"},"cell_type":"code","source":"def handle_missing_value(train_set,test_set):\n    '''\n    Handle the missing value of train and test set and return the \n    '''\n    # Train set Missing value\n    data_na = train_set.isnull().sum().values / train_set.shape[0] *100\n    df_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\n    df_na = df_na.sort_values(by=['Count'], ascending=False)\n    missing_value_count = df_na[df_na['Count']>0].shape[0]\n    print(f'We got {missing_value_count} rows which have missing value in train set ')\n    df_na.head(6)\n    # Test set Missing value\n#     data_na1 = test_set.isnull().sum().values / test_set.shape[0] *100\n#     df_na1 = pd.DataFrame(data_na1, index=test_set.columns, columns=['Count'])\n#     df_na1 = df_na1.sort_values(by=['Count'], ascending=False)\n#     missing_value_count1 = df_na1[df_na1['Count']>0].shape[0]\n#     print(f'We got {missing_value_count} rows which have missing value in test set ')\n#     df_na1.head(6)\n    data_na = train_set.isnull().sum().values / train_set.shape[0] *100\n    df_na = pd.DataFrame(data_na, index=train_set.columns, columns=['Count'])\n    df_na = df_na.sort_values(by=['Count'], ascending=False)\n\n    missing_value_count = df_na[df_na['Count']>0].shape[0]\n\n    print(f'We got {missing_value_count} rows which have missing value in test set ')\n    df_na.head(6)\n\n    #Fill na\n    def repalce_v18q1(x):\n        if x['v18q'] == 0:\n            return x['v18q']\n        else:\n            return x['v18q1']\n\n    train_set['v18q1'] = train_set.apply(lambda x : repalce_v18q1(x),axis=1)\n    test_set['v18q1'] = test_set.apply(lambda x : repalce_v18q1(x),axis=1)\n\n    train_set['v2a1'] = train_set['v2a1'].fillna(value=train_set['tipovivi3'])\n    test_set['v2a1'] = test_set['v2a1'].fillna(value=test_set['tipovivi3'])\n    \n    return (train_set,test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d2fbbd21746844436fe530079b16d9334f9375c7"},"cell_type":"markdown","source":"# 3.Feature Engineering"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da2a67557e9ec261b11cc444a2407b1d31888879"},"cell_type":"code","source":"def feature_engineering(train_set,test_set):\n    '''\n    Feature Engineering and Feature generation\n    '''\n    cols = ['edjefe', 'edjefa']\n    train_set[cols] = train_set[cols].replace({'no': 0, 'yes':1}).astype(float)\n    test_set[cols] = test_set[cols].replace({'no': 0, 'yes':1}).astype(float)\n    \n    train_set['roof_waste_material'] = np.nan\n    test_set['roof_waste_material'] = np.nan\n    train_set['electricity_other'] = np.nan\n    test_set['electricity_other'] = np.nan\n\n    def fill_roof_exception(x):\n        if (x['techozinc'] == 0) and (x['techoentrepiso'] == 0) and (x['techocane'] == 0) and (x['techootro'] == 0):\n            return 1\n        else:\n            return 0\n\n    def fill_no_electricity(x):\n        if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n            return 1\n        else:\n            return 0\n\n    train_set['roof_waste_material'] = train_set.apply(lambda x : fill_roof_exception(x),axis=1)\n    test_set['roof_waste_material'] = test_set.apply(lambda x : fill_roof_exception(x),axis=1)\n    train_set['electricity_other'] = train_set.apply(lambda x : fill_no_electricity(x),axis=1)\n    test_set['electricity_other'] = test_set.apply(lambda x : fill_no_electricity(x),axis=1)\n    \n    train_set['adult'] = train_set['hogar_adul'] - train_set['hogar_mayor']\n    train_set['dependency_count'] = train_set['hogar_nin'] + train_set['hogar_mayor']\n    train_set['dependency'] = train_set['dependency_count'] / train_set['adult']\n    train_set['child_percent'] = train_set['hogar_nin']/train_set['hogar_total']\n    train_set['elder_percent'] = train_set['hogar_mayor']/train_set['hogar_total']\n    train_set['adult_percent'] = train_set['hogar_adul']/train_set['hogar_total']\n    test_set['adult'] = test_set['hogar_adul'] - test_set['hogar_mayor']\n    test_set['dependency_count'] = test_set['hogar_nin'] + test_set['hogar_mayor']\n    test_set['dependency'] = test_set['dependency_count'] / test_set['adult']\n    test_set['child_percent'] = test_set['hogar_nin']/test_set['hogar_total']\n    test_set['elder_percent'] = test_set['hogar_mayor']/test_set['hogar_total']\n    test_set['adult_percent'] = test_set['hogar_adul']/test_set['hogar_total']\n\n    train_set['rent_per_adult'] = train_set['v2a1']/train_set['hogar_adul']\n    train_set['rent_per_person'] = train_set['v2a1']/train_set['hhsize']\n    test_set['rent_per_adult'] = test_set['v2a1']/test_set['hogar_adul']\n    test_set['rent_per_person'] = test_set['v2a1']/test_set['hhsize']\n\n    train_set['overcrowding_room_and_bedroom'] = (train_set['hacdor'] + train_set['hacapo'])/2\n    test_set['overcrowding_room_and_bedroom'] = (test_set['hacdor'] + test_set['hacapo'])/2\n\n    train_set['no_appliances'] = train_set['refrig'] + train_set['computer'] + train_set['television']\n    test_set['no_appliances'] = test_set['refrig'] + test_set['computer'] + test_set['television']\n\n    train_set['r4h1_percent_in_male'] = train_set['r4h1'] / train_set['r4h3']\n    train_set['r4m1_percent_in_female'] = train_set['r4m1'] / train_set['r4m3']\n    train_set['r4h1_percent_in_total'] = train_set['r4h1'] / train_set['hhsize']\n    train_set['r4m1_percent_in_total'] = train_set['r4m1'] / train_set['hhsize']\n    train_set['r4t1_percent_in_total'] = train_set['r4t1'] / train_set['hhsize']\n    test_set['r4h1_percent_in_male'] = test_set['r4h1'] / test_set['r4h3']\n    test_set['r4m1_percent_in_female'] = test_set['r4m1'] / test_set['r4m3']\n    test_set['r4h1_percent_in_total'] = test_set['r4h1'] / test_set['hhsize']\n    test_set['r4m1_percent_in_total'] = test_set['r4m1'] / test_set['hhsize']\n    test_set['r4t1_percent_in_total'] = test_set['r4t1'] / test_set['hhsize']\n\n    train_set['rent_per_room'] = train_set['v2a1']/train_set['rooms']\n    train_set['bedroom_per_room'] = train_set['bedrooms']/train_set['rooms']\n    train_set['elder_per_room'] = train_set['hogar_mayor']/train_set['rooms']\n    train_set['adults_per_room'] = train_set['adult']/train_set['rooms']\n    train_set['child_per_room'] = train_set['hogar_nin']/train_set['rooms']\n    train_set['male_per_room'] = train_set['r4h3']/train_set['rooms']\n    train_set['female_per_room'] = train_set['r4m3']/train_set['rooms']\n    train_set['room_per_person_household'] = train_set['hhsize']/train_set['rooms']\n\n    test_set['rent_per_room'] = test_set['v2a1']/test_set['rooms']\n    test_set['bedroom_per_room'] = test_set['bedrooms']/test_set['rooms']\n    test_set['elder_per_room'] = test_set['hogar_mayor']/test_set['rooms']\n    test_set['adults_per_room'] = test_set['adult']/test_set['rooms']\n    test_set['child_per_room'] = test_set['hogar_nin']/test_set['rooms']\n    test_set['male_per_room'] = test_set['r4h3']/test_set['rooms']\n    test_set['female_per_room'] = test_set['r4m3']/test_set['rooms']\n    test_set['room_per_person_household'] = test_set['hhsize']/test_set['rooms']\n\n    train_set['rent_per_bedroom'] = train_set['v2a1']/train_set['bedrooms']\n    train_set['edler_per_bedroom'] = train_set['hogar_mayor']/train_set['bedrooms']\n    train_set['adults_per_bedroom'] = train_set['adult']/train_set['bedrooms']\n    train_set['child_per_bedroom'] = train_set['hogar_nin']/train_set['bedrooms']\n    train_set['male_per_bedroom'] = train_set['r4h3']/train_set['bedrooms']\n    train_set['female_per_bedroom'] = train_set['r4m3']/train_set['bedrooms']\n    train_set['bedrooms_per_person_household'] = train_set['hhsize']/train_set['bedrooms']\n\n    test_set['rent_per_bedroom'] = test_set['v2a1']/test_set['bedrooms']\n    test_set['edler_per_bedroom'] = test_set['hogar_mayor']/test_set['bedrooms']\n    test_set['adults_per_bedroom'] = test_set['adult']/test_set['bedrooms']\n    test_set['child_per_bedroom'] = test_set['hogar_nin']/test_set['bedrooms']\n    test_set['male_per_bedroom'] = test_set['r4h3']/test_set['bedrooms']\n    test_set['female_per_bedroom'] = test_set['r4m3']/test_set['bedrooms']\n    test_set['bedrooms_per_person_household'] = test_set['hhsize']/test_set['bedrooms']\n\n    train_set['tablet_per_person_household'] = train_set['v18q1']/train_set['hhsize']\n    train_set['phone_per_person_household'] = train_set['qmobilephone']/train_set['hhsize']\n    test_set['tablet_per_person_household'] = test_set['v18q1']/test_set['hhsize']\n    test_set['phone_per_person_household'] = test_set['qmobilephone']/test_set['hhsize']\n\n    train_set['age_12_19'] = train_set['hogar_nin'] - train_set['r4t1']\n    test_set['age_12_19'] = test_set['hogar_nin'] - test_set['r4t1']    \n\n    train_set['escolari_age'] = train_set['escolari']/train_set['age']\n    test_set['escolari_age'] = test_set['escolari']/test_set['age']\n\n    train_set['rez_esc_escolari'] = train_set['rez_esc']/train_set['escolari']\n    train_set['rez_esc_r4t1'] = train_set['rez_esc']/train_set['r4t1']\n    train_set['rez_esc_r4t2'] = train_set['rez_esc']/train_set['r4t2']\n    train_set['rez_esc_r4t3'] = train_set['rez_esc']/train_set['r4t3']\n    train_set['rez_esc_age'] = train_set['rez_esc']/train_set['age']\n    test_set['rez_esc_escolari'] = test_set['rez_esc']/test_set['escolari']\n    test_set['rez_esc_r4t1'] = test_set['rez_esc']/test_set['r4t1']\n    test_set['rez_esc_r4t2'] = test_set['rez_esc']/test_set['r4t2']\n    test_set['rez_esc_r4t3'] = test_set['rez_esc']/test_set['r4t3']\n    test_set['rez_esc_age'] = test_set['rez_esc']/test_set['age']\n    \n    train_set['dependency'] = train_set['dependency'].replace({np.inf: 0})\n    test_set['dependency'] = test_set['dependency'].replace({np.inf: 0})\n\n    print(f'train set has {train_set.shape[0]} rows, and {train_set.shape[1]} features')\n    print(f'test set has {test_set.shape[0]} rows, and {test_set.shape[1]} features')\n    \n    df_train = pd.DataFrame()\n    df_test = pd.DataFrame()\n\n    aggr_mean_list = ['rez_esc', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco2',\n                 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12',\n                 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',]\n\n    other_list = ['escolari', 'age', 'escolari_age']\n\n    for item in aggr_mean_list:\n        group_train_mean = train_set[item].groupby(train_set['idhogar']).mean()\n        group_test_mean = test_set[item].groupby(test_set['idhogar']).mean()\n        new_col = item + '_aggr_mean'\n        df_train[new_col] = group_train_mean\n        df_test[new_col] = group_test_mean\n\n    for item in other_list:\n        for function in ['mean','std','min','max','sum']:\n            group_train = train_set[item].groupby(train_set['idhogar']).agg(function)\n            group_test = test_set[item].groupby(test_set['idhogar']).agg(function)\n            new_col = item + '_' + function\n            df_train[new_col] = group_train\n            df_test[new_col] = group_test\n\n    print(f'new aggregate train set has {df_train.shape[0]} rows, and {df_train.shape[1]} features')\n    print(f'new aggregate test set has {df_test.shape[0]} rows, and {df_test.shape[1]} features')\n    \n    df_test = df_test.reset_index()\n    df_train = df_train.reset_index()\n\n    train_agg = pd.merge(train_set, df_train, on='idhogar')\n    test = pd.merge(test_set, df_test, on='idhogar')\n\n    #fill all na as 0\n    train_agg.fillna(value=0, inplace=True)\n    test.fillna(value=0, inplace=True)\n    print(f'new train set has {train_agg.shape[0]} rows, and {train_agg.shape[1]} features')\n    print(f'new test set has {test.shape[0]} rows, and {test.shape[1]} features')\n    \n    train = train_agg.query('parentesco1==1')\n    \n    submission = test[['Id']]\n\n    #Remove useless feature to reduce dimension\n    train.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n    test.drop(columns=['idhogar','Id', 'tamhog', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n\n    correlation = train.corr()\n    correlation = correlation['Target'].sort_values(ascending=False)\n    print(f'The most 20 positive feature: \\n{correlation.head(20)}')\n    print('*'*50)\n\n    print(f'The most 20 negative feature: \\n{correlation.tail(20)}')\n    \n    return (train,test,train_set,test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"10ecb56a1ef56fd2f454b74ca02815ebf43893b9"},"cell_type":"markdown","source":"# 4.Model Training"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"799b7fbd63cbb4b448fa797338e933cbe1fd4304"},"cell_type":"code","source":"def model_training(train,test):\n    #parameter value is copied from \n    y = train['Target']\n    train.drop(columns=['Target'], inplace=True)\n    clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                                 random_state=None, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5500, class_weight='balanced',\n                                 colsample_bytree =  0.89, min_child_samples = 90, num_leaves = 56, subsample = 0.96)\n    \n    kfold = 7\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\n    predicts_result = []\n    for train_index, test_index in kf.split(train, y):\n        print(\"###\")\n        X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train, eval_set=[(X_val, y_val)],early_stopping_rounds=100)\n        predicts_result.append(clf.predict(test))\n    \n    return (predicts_result,clf)\n       \n\ndef feature_plot(predicts_result,clf,train):\n    indices = np.argsort(clf.feature_importances_)[::-1]\n    indices = indices[:75]\n    # Visualise these with a barplot\n    plt.subplots(figsize=(20, 15))\n    g = sns.barplot(y=train.columns[indices], x = clf.feature_importances_[indices], orient='h', palette = mycols)\n    g.set_xlabel(\"Relative importance\",fontsize=12)\n    g.set_ylabel(\"Features\",fontsize=12)\n    g.tick_params(labelsize=9)\n    g.set_title(\"LightGBM feature importance\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae007998e2788c0dcdf5b652649d17cd034ecaf3"},"cell_type":"markdown","source":"# 5. Load Main Process Function at Once"},{"metadata":{"trusted":true,"_uuid":"8b1a0482d37ca8b72d68bbe487f488dd539517ae","collapsed":true},"cell_type":"code","source":"def main(debug = False):\n    with timer(\"Dataset is Reading... \"):\n        train_set,test_set,target = load_data()\n        #outlier in test set which rez_esc is 99.0\n        test_set.loc[test_set['rez_esc'] == 99.0 , 'rez_esc'] = 5\n        gc.collect()\n    with timer(\"Handle Missing Value...\"):\n        train_set,test_set = handle_missing_value(train_set,test_set)\n        gc.collect()\n    with timer(\"Feature Engineering...\"):\n        train,test,train_set,test_set = feature_engineering(train_set,test_set)\n        gc.collect()\n    with timer(\"Automatic Model Tuning...\"):\n        predicts_result,clf = model_training(train,test)\n        feature_plot(predicts_result,clf,train)\n        gc.collect()\n    with timer(\"Final Submission\"):\n        submission = pd.read_csv(\"../input/costa-rican-household-poverty-prediction/sample_submission.csv\")\n        submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\n        submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"125f531f361f0011550939bd7cb6d3278ce857f5","scrolled":true,"collapsed":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    with timer(\"Full model run\"):\n        main(debug= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c604795934ce59fc3978cb4cf4d3ba87b45fa91"},"cell_type":"code","source":"sub = pd.read_csv(\"../input/modeltune/Ashish4321.csv\")\nsub.to_csv(\"submission1.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2f7a9c2b32ce70e936accf099b9a160f4994b3f7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}