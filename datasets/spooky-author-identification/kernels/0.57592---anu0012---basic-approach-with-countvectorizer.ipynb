{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py"}}, "cells": [{"outputs": [], "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np\n", "import os\n", "import pandas as pd\n", "import sys\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.svm import LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.linear_model import LogisticRegression,SGDClassifier\n", "from nltk.corpus import wordnet as wn\n", "from nltk.corpus import stopwords\n", "from nltk.stem.snowball import SnowballStemmer\n", "from nltk.stem import PorterStemmer\n", "import nltk\n", "from nltk import word_tokenize, ngrams\n", "from nltk.classify import SklearnClassifier\n", "from wordcloud import WordCloud,STOPWORDS\n", "import xgboost as xgb\n", "np.random.seed(25)\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_uuid": "f57eae421649a1a54e3d77dcdcbd8451bc3bf05f", "_cell_guid": "5c58e60b-2f86-4972-b5fa-1470c975bbab"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train.head()"], "metadata": {"_uuid": "264dee8d28a7a7aa48bccb48e00c8e1bcdfc434a", "_cell_guid": "40d8f19c-ffb9-4a83-b26d-85a0b99f1470"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# Target Mapping\n", "mapping_target = {'EAP':0, 'HPL':1, 'MWS':2}\n", "train = train.replace({'author':mapping_target})"], "metadata": {"_uuid": "018fdcaf3f40a28610d7d6f5b86ea1039ad758fd", "collapsed": true, "_cell_guid": "5bc97bf7-83c2-48d9-afc2-233f67c1b381"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["train.head()"], "metadata": {"_uuid": "45cb41466f9fdb88e0955e0ee8270c28a7ddf694", "_cell_guid": "a17bf366-ac47-49af-bef9-ef814c3fd87f"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["test_id = test['id']\n", "target = train['author']"], "metadata": {"_uuid": "2233c58f025a9d8bc241da4896dd9429db03db7a", "collapsed": true, "_cell_guid": "fad2ebb8-4c7b-4c4e-97c6-ca5c3ae68885"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# function to clean data\n", "import string\n", "import itertools \n", "import re\n", "from nltk.stem import WordNetLemmatizer\n", "from string import punctuation\n", "\n", "stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n", "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n", "              'Is','If','While','This']\n", "# punct = list(string.punctuation)\n", "# punct.append(\"''\")\n", "# punct.append(\":\")\n", "# punct.append(\"...\")\n", "# punct.append(\"@\")\n", "# punct.append('\"\"')\n", "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n", "    \n", "    txt = str(text)\n", "    \n", "    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n", "    \n", "     \n", "    if lowercase:\n", "        txt = \" \".join([w.lower() for w in txt.split()])\n", "        \n", "    if remove_stops:\n", "        txt = \" \".join([w for w in txt.split() if w not in stops])\n", "    if stemming:\n", "        st = PorterStemmer()\n", "        txt = \" \".join([st.stem(w) for w in txt.split()])\n", "    \n", "    if lemmatization:\n", "        wordnet_lemmatizer = WordNetLemmatizer()\n", "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n", "\n", "    return txt"], "metadata": {"_uuid": "c7710ed8a476f2e705bc8ebf3da20edc61bc419a", "collapsed": true, "_cell_guid": "c099a9ef-555e-4eec-8e0c-a1b2efe9766f"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# def fraction_noun(row):\n", "#     \"\"\"function to give us fraction of noun over total words \"\"\"\n", "#     text = row['text']\n", "#     text_splited = text.split(' ')\n", "#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n", "#     text_splited = [s for s in text_splited if s]\n", "#     word_count = text_splited.__len__()\n", "#     pos_list = nltk.pos_tag(text_splited)\n", "#     noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n", "#     return (noun_count/word_count)\n", "\n", "# def fraction_adj(row):\n", "#     \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n", "#     text = row['text']\n", "#     text_splited = text.split(' ')\n", "#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n", "#     text_splited = [s for s in text_splited if s]\n", "#     word_count = text_splited.__len__()\n", "#     pos_list = nltk.pos_tag(text_splited)\n", "#     adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n", "#     return (adj_count/word_count)\n", "\n", "# def fraction_verbs(row):\n", "#     \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n", "#     text = row['text']\n", "#     text_splited = text.split(' ')\n", "#     text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n", "#     text_splited = [s for s in text_splited if s]\n", "#     word_count = text_splited.__len__()\n", "#     pos_list = nltk.pos_tag(text_splited)\n", "#     verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n", "#     return (verbs_count/word_count)"], "metadata": {"_uuid": "c9c0fa3e866493a19683f73182e844a4b46ad5f6", "collapsed": true, "_cell_guid": "ad2ca37d-d052-4679-b938-c312bb06c5ca"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# train['fraction_noun'] = train.apply(lambda row: fraction_noun(row), axis =1)\n", "# train['fraction_adj'] = train.apply(lambda row: fraction_adj(row), axis =1)\n", "# train['fraction_verbs'] = train.apply(lambda row: fraction_verbs(row), axis =1)\n", "\n", "# test['fraction_noun'] = test.apply(lambda row: fraction_noun(row), axis =1)\n", "# test['fraction_adj'] = test.apply(lambda row: fraction_adj(row), axis =1)\n", "# test['fraction_verbs'] = test.apply(lambda row: fraction_verbs(row), axis =1)"], "metadata": {"_uuid": "180aaac8707e139d2d4e4a353c2bdb57fff94f1b", "collapsed": true, "_cell_guid": "0c5176fc-42b9-4c5a-9e86-241c492a6343"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["## Number of words in the text ##\n", "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n", "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n", "\n", "## Number of unique words in the text ##\n", "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "\n", "## Number of characters in the text ##\n", "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n", "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n", "\n", "## Number of stopwords in the text ##\n", "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n", "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n", "\n", "## Number of punctuations in the text ##\n", "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "\n", "## Number of title case words in the text ##\n", "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "\n", "## Number of title case words in the text ##\n", "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "\n", "## Average length of the words in the text ##\n", "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"], "metadata": {"_uuid": "2b2c032cc12f119ec9bee54272027dd5a76b88ab", "collapsed": true, "_cell_guid": "b2942fed-8a79-4995-9899-0e1f44eb3ebd"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# clean text\n", "train['text'] = train['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = False))\n", "test['text'] = test['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = False))"], "metadata": {"_uuid": "58685087a0322fcda8706880104b09d577ea7ab2", "collapsed": true, "_cell_guid": "1eb905b0-d6af-49a6-aa51-532e4d73ae9a"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["test['author'] = np.nan\n", "alldata = pd.concat([train, test]).reset_index(drop=True)"], "metadata": {"_uuid": "4b94a2f380b03a6f9d085783ad32a36436b8be40", "collapsed": true, "_cell_guid": "f5640a15-ee88-435f-b7af-17bdac551a27"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["#tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1))\n", "tfidfvec = CountVectorizer(analyzer='word', ngram_range = (1,1),min_df = 1, max_features= 5000)\n", "tfidfdata = tfidfvec.fit_transform(alldata['text'])"], "metadata": {"_uuid": "9579e4cba026c878d0f8c3ebf105c351969c6568", "collapsed": true, "_cell_guid": "f3f35dfc-90ec-42cf-abf7-46bf720cf3c1"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["tfidfdata.shape"], "metadata": {"_uuid": "7e5d6d40fa269f6c22bf8fa7acfa55975a71d034", "_cell_guid": "ca2d71fc-0d10-433d-8785-ee13b7967d32"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# create dataframe for features\n", "tfidf_df = pd.DataFrame(tfidfdata.todense())"], "metadata": {"_uuid": "1fb0fcc606ddef4c81235e2df4792f7a6b920aec", "collapsed": true, "_cell_guid": "ce32b9b5-00fd-4900-b11e-3590f3a1d323"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]"], "metadata": {"_uuid": "a64faf9836953130b5f864b72442343337406f47", "collapsed": true, "_cell_guid": "8db3c9d0-65e2-48e8-a470-ffe544d58997"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["tfid_df_train = tfidf_df[:len(train)]\n", "tfid_df_test = tfidf_df[len(train):]"], "metadata": {"_uuid": "ab28dedf854ceda85667fab196e56b16e15030be", "collapsed": true, "_cell_guid": "3c5ffc8d-cd76-4c9d-85be-a434d433dc62"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# merge into a new data frame with features\n", "train_feats2 = pd.concat([tfid_df_train], axis=1)\n", "test_feats2 = pd.concat([tfid_df_test], axis=1)"], "metadata": {"_uuid": "41c8520f729039377d3c57a8482a17032d81c916", "collapsed": true, "_cell_guid": "eeedc511-2859-40df-a60f-2e7846d04c65"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# lgb\n", "import lightgbm as lgb\n", "# default parameters\n", "params = {'objective':'multi:softprob',\n", "          'gamma':1,\n", "           'eval_metric':'mlogloss',\n", "          'max_depth': 13,\n", "          'seed':2017,\n", "          'num_class':3,\n", "          'subsample':0.5,\n", "          'eta':0.5\n", "         }"], "metadata": {"_uuid": "ab7cd2637abe745be619c0ae13ae00d824923bdf", "collapsed": true, "_cell_guid": "2ec79f6f-295a-4243-9d4c-cb447267f68f"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["X_train, X_valid, y_train, y_valid = train_test_split(train_feats2, target, train_size = 0.7, stratify = target, random_state = 2017)"], "metadata": {"_uuid": "dc182dc3921155ea857c5afddb0ac096622b99cb", "_cell_guid": "9153a316-78e6-4518-8e9b-5ff9f9dbf1c3"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["dtrain = xgb.DMatrix(data=X_train, label=y_train)\n", "dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n", "dtest = xgb.DMatrix(data=test_feats2)\n", "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]"], "metadata": {"_uuid": "17c6a99adc42d11c51961b33793b1640ec6915ee", "collapsed": true, "_cell_guid": "32644949-ea6f-4123-837c-58fb07908d64"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["model = xgb.train(params, dtrain, 1000, watchlist, maximize=False, verbose_eval=20, early_stopping_rounds=40)"], "metadata": {"_uuid": "030e37f6c9e96d12e07666dd2b5c3b8cc6405ec4", "_cell_guid": "1bcea2e0-d3b3-4d54-9ffe-24c197534522"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# from sklearn.linear_model import LogisticRegression,SGDClassifier\n", "# from sklearn.ensemble import VotingClassifier\n", "# from sklearn.naive_bayes import MultinomialNB,BernoulliNB, GaussianNB\n", "# from sklearn.svm import SVC\n", "# from sklearn.tree import DecisionTreeClassifier\n", "# from sklearn.ensemble import ExtraTreesClassifier\n", "\n", "# # clf1 = LogisticRegression(penalty='l1', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=1)\n", "# # #clf2 = LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=2)\n", "# # #clf3 = LogisticRegression(penalty='l2', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=0.2, class_weight=None, random_state=25)\n", "# # #clf1 = BernoulliNB()\n", "# # #clf2 =  GaussianNB()\n", "# # clf3 = MultinomialNB()\n", "# # model = VotingClassifier(estimators=[('lr', clf1), ('svc', clf3)],weights=[3,3], voting='soft')\n", "# model = LogisticRegression(penalty='l1', dual=False, tol=0.0005, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None, random_state=2)\n", "\n"], "metadata": {"_uuid": "118ceb938e4272111344a149e5a98dceb78d2323", "collapsed": true, "_cell_guid": "cffe35aa-5d79-4a53-8743-1a48ade0a4d5"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# # MultinomialNB - term counts is giving higher CV score\n", "# from sklearn.model_selection import cross_val_score\n", "# from sklearn.metrics import accuracy_score, make_scorer, log_loss\n", "# print(cross_val_score(model, train_feats2, target, cv=5, scoring=make_scorer(accuracy_score)))"], "metadata": {"_uuid": "22dd12d1b822cdcb1b567ddffa4ddf1d33d0014c", "collapsed": true, "_cell_guid": "7c420103-dfe9-4e5e-9f19-1c0eed2fb4d9"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["# model.fit(train_feats2, target)"], "metadata": {"_uuid": "8dba7e3c23c09db6db41ec8d222f80ed1c2458c0", "collapsed": true, "_cell_guid": "cc8a44fc-a829-4292-b66e-3047e01eb6a7"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["preds = model.predict(dtest)"], "metadata": {"_uuid": "846a8f9ad054a0cbb0ccfca485b8473da84efbaf", "collapsed": true, "_cell_guid": "2ebd2663-5e20-4089-8f69-11fa81e87811"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["result = pd.DataFrame()\n", "result['id'] = test_id\n", "result['EAP'] = [x[0] for x in preds]\n", "result['HPL'] = [x[1] for x in preds]\n", "result['MWS'] = [x[2] for x in preds]\n", "\n", "result.to_csv(\"result.csv\", index=False)"], "metadata": {"_uuid": "78970d433c0a03813b93eb74dcdd121ccc9a555a", "collapsed": true, "_cell_guid": "892831b5-a329-41bd-bd8a-1a0a085b1d62"}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": [], "metadata": {"_uuid": "ffb50ad0046407131d58f9cb706da2f30c7e95b5", "collapsed": true, "_cell_guid": "47422afe-ba78-4268-a98b-54920c6c484c"}, "cell_type": "code"}], "nbformat": 4}