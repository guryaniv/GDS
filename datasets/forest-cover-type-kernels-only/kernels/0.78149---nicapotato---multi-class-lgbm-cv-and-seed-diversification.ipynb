{"cells":[{"metadata":{"_uuid":"9cf11d0446f8b35daeafc0a085566083929419a8"},"cell_type":"markdown","source":"## Multi-Class LGBM CV and Seed Diversification\n_By Nick Brooks, June 2018_\n\n**Contains:**\n- Data Load\n- Feature Engineering\n- Exploratory Data Analysis\n- Light GBM Cross Validation\n- Seed Diversification\n- Feature Importance\n\n**Load:** <br>"},{"metadata":{"_kg_hide-input":false,"_uuid":"f3ae27d2f6c11d8100dd600dbb9ffaabd55d8c25","scrolled":false,"trusted":true},"cell_type":"code","source":"import time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport random\nrandom.seed(2018)\nimport re\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\n\n# Gradient Boosting\nimport lightgbm as lgb\n\n# Text Models\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\n\n# TSNE\nfrom yellowbrick.text import TSNEVisualizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Specify index/ target name\nid_col = \"Id\"\ntarget_var = \"Cover_Type\"\n\n# House Keeping Parameters\nDebug = False\nHome = False\nBuild_Results_csv = False # if running for first time at Home\n\nresults = pd.DataFrame(columns = [\"Rounds\",\"Score\",\"STDV\", \"LB\", \"Parameters\"])\nif Build_Results_csv is True & Home is True: results.to_csv(\"results.csv\")\nif Home is True:\n    import os\n    path = r\"D:\\My Computer\\DATA\\Forest Cover\"\n    os.chdir(path)\n    \n    print(\"Data Load Stage\")\n    training = pd.read_csv('train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(500)\n    traindex = training.index\n    testing = pd.read_csv('test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(500)\n    testdex = testing.index\nelse:\n    print(\"Data Load Stage\")\n    training = pd.read_csv('../input/train.csv', index_col = id_col)\n    if Debug is True : training = training.sample(100)\n    traindex = training.index\n    testing = pd.read_csv('../input/test.csv', index_col = id_col)\n    if Debug is True : testing = testing.sample(100)\n    testdex = testing.index\n\ny = training[target_var]\ntraining.drop(target_var,axis=1, inplace=True)\nprint('Train shape: {} Rows, {} Columns'.format(*training.shape))\nprint('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n\nprint(\"Combine Train and Test\")\ndf = pd.concat([training,testing],axis=0)\ndel training, testing\ngc.collect()\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4051f7f94bd7f15abdb2f4c905e6f2ea3a069902"},"cell_type":"markdown","source":"**Class Distribution:** <br>\nThis is crucial for all classification problems. Since unbalanced classes must be delt with the appropriate metrics, re-sampling, and stratification methods."},{"metadata":{"_uuid":"01c189e5ac228bd5a6d9ede708bfe521e991a2be","trusted":true},"cell_type":"code","source":"y_trees = y.map({1: \"Spruce/Fir\", 2: \"Lodgepole Pine\", 3: \"Ponderosa Pine\",\n                 4: \"Cottonwood/Willow\",5:\"Aspen\",6:\"Douglas-fir\",7:\"Krummholz\"})\nprint(\"Percent Class Distribution:\")\nprint(y_trees.value_counts(normalize=True)*100)\ndisplay(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff79ea0540e8bfc864e91dfc10023a97de581f69"},"cell_type":"markdown","source":"These classes are balanced! We can proceed."},{"metadata":{"_uuid":"c1f1876bf271b6fbcc5d26e370d592653c47b25b"},"cell_type":"markdown","source":"***\n## Feature Engineering\n\n- Time Difference Features\n- Product of Elevation and Slope\n- Bag-Of-Words on Soil Description\n- Lathwal Distance Features"},{"metadata":{"_kg_hide-input":true,"_uuid":"5717df7c8fb9c2cff3ad4d5160037071bce775ef","trusted":true},"cell_type":"code","source":"print(\"Engineering Text Features:\")\n# Shade Difference\ndf[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\ndf[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\ndf[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n# Mountain Trees\ndf[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"] # Only some trees can grow on steep montain tops\n\n# Text Features from Description\nsoil_descriptions = {\n    1: \"Cathedral family - Rock outcrop complex, extremely stony.\",\n    2: \"Vanet - Ratake families complex, very stony.\",\n    3: \"Haploborolis - Rock outcrop complex, rubbly.\",\n    4: \"Ratake family - Rock outcrop complex, rubbly.\",\n    5: \"Vanet family - Rock outcrop complex complex, rubbly.\",\n    6: \"Vanet - Wetmore families - Rock outcrop complex, stony.\",\n    7: \"Gothic family.\",\n    8: \"Supervisor - Limber families complex.\",\n    9: \"Troutville family, very stony.\",\n    10: \"Bullwark - Catamount families - Rock outcrop complex, rubbly.\",\n    11: \"Bullwark - Catamount families - Rock land complex, rubbly.\",\n    12: \"Legault family - Rock land complex, stony.\",\n    13: \"Catamount family - Rock land - Bullwark family complex, rubbly.\",\n    14: \"Pachic Argiborolis - Aquolis complex.\",\n    15: \"unspecified in the USFS Soil and ELU Survey.\",\n    16: \"Cryaquolis - Cryoborolis complex.\",\n    17: \"Gateview family - Cryaquolis complex.\",\n    18: \"Rogert family, very stony.\",\n    19: \"Typic Cryaquolis - Borohemists complex.\",\n    20: \"Typic Cryaquepts - Typic Cryaquolls complex.\",\n    21: \"Typic Cryaquolls - Leighcan family, till substratum complex.\",\n    22: \"Leighcan family, till substratum, extremely bouldery.\",\n    23: \"Leighcan family, till substratum - Typic Cryaquolls complex.\",\n    24: \"Leighcan family, extremely stony.\",\n    25: \"Leighcan family, warm, extremely stony.\",\n    26: \"Granile - Catamount families complex, very stony.\",\n    27: \"Leighcan family, warm - Rock outcrop complex, extremely stony.\",\n    28: \"Leighcan family - Rock outcrop complex, extremely stony.\",\n    29: \"Como - Legault families complex, extremely stony.\",\n    30: \"Como family - Rock land - Legault family complex, extremely stony.\",\n    31: \"Leighcan - Catamount families complex, extremely stony.\",\n    32: \"Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\",\n    33: \"Leighcan - Catamount families - Rock outcrop complex, extremely stony.\",\n    34: \"Cryorthents - Rock land complex, extremely stony.\",\n    35: \"Cryumbrepts - Rock outcrop - Cryaquepts complex.\",\n    36: \"Bross family - Rock land - Cryumbrepts complex, extremely stony.\",\n    37: \"Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\",\n    38: \"Leighcan - Moran families - Cryaquolls complex, extremely stony.\",\n    39: \"Moran family - Cryorthents - Leighcan family complex, extremely stony.\",\n    40: \"Moran family - Cryorthents - Rock land complex, extremely stony..\"\n    }\n# First Isolate Soil Columns\nsoil_columns = [col for col in df.columns if col.startswith('Soil_Type')]\n# Then get number\nget_soil_number = [int(re.findall('\\d+', x )[0]) for x in soil_columns]\n\n# Since these are binary variables, map 1 to the description, and 0 to nothing\nfor soil_num in get_soil_number:\n    df[\"mapped_Soil_Type\" + str(soil_num)] = df[\"Soil_Type\"+str(soil_num)].map({0: \" \",1: soil_descriptions[soil_num]})\n\n# Isolate my mappedcolumns within Df\nmapped_columns = [col for col in df.columns if col.startswith('mapped_Soil_Type')]\n# Join mapped columns so that I can apply text model\ndf[\"description\"] = df[mapped_columns].apply(lambda x: ' '.join(x), axis=1)\n# Drop mapped columns since they have been concentrted into 'description'\ndf.drop(mapped_columns,axis=1,inplace=True)\n\n# Term Frequency - Inverse Document Frequency\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 3),\n    dtype = np.float32,\n    norm='l2',\n    min_df=0,\n    smooth_idf=False)\n\n# Fit and Transform\nword_features = word_vectorizer.fit_transform(df[\"description\"])\ndf1 = pd.DataFrame(word_features.toarray(), columns=word_vectorizer.get_feature_names())\ndisplay(df1.sample(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e5c1638c339e395a438ecf5ce4b5f5c4da47fd8"},"cell_type":"markdown","source":"## Word Cloud for Text Feature"},{"metadata":{"_kg_hide-input":true,"_uuid":"8dcf39c6e66b10f6182dfd213125722c5fd43c69","trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nsize = (6,3)\n\ndef cloud(text, title, stopwords=stopwords, size=size):\n    \"\"\"\n    Function to plot WordCloud\n    Includes: \n    \"\"\"\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)\n    mpl.rcParams['font.size']=12\n    mpl.rcParams['savefig.dpi']=100\n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    # Redundant when combined with my Preprocessing function\n    wordcloud = WordCloud(width=800, height=400,\n                          background_color='black',\n                          stopwords=stopwords,\n                         ).generate(str(text))\n    \n   # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=50,color='y')\n    plt.tight_layout(pad=0)\n    plt.show()\nprint(\"Word Cloud Function Done\")\n\n# Data Set for Word Clouds\ncloud_df = pd.concat([df.loc[traindex,'description'], y_trees],axis=1)\ncloud(df[\"description\"].values, title=\"All Cover Types\", size=[8,5])\n\nprint(\"Description by Cover Type\")\ncloud_df = pd.concat([df.loc[traindex,'description'], y_trees],axis=1)\nfor cover in cloud_df.Cover_Type.unique():\n    cloud(cloud_df.loc[cloud_df.Cover_Type == cover, \"description\"].values, title=\"{}\".format(cover), size=[8,5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d73fba85def0dd73e7dd16875237bd16c9261494"},"cell_type":"markdown","source":"**Lathwal Features:** <br>"},{"metadata":{"_uuid":"c5dc6ce3184cbabd0a9f4947e4f3a68ed4f722a6","trusted":true,"collapsed":true},"cell_type":"code","source":"# Features By Lathwal - Source: https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model\ndf['HF1'] = df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points']\ndf['HF2'] = abs(df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\ndf['HR1'] = abs(df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\ndf['HR2'] = abs(df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\ndf['FR1'] = abs(df['Horizontal_Distance_To_Fire_Points']+df['Horizontal_Distance_To_Roadways'])\ndf['FR2'] = abs(df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae5195c7c69bdc3e4a7b15d9b6c3779ee9128b8"},"cell_type":"markdown","source":"## Train and Test Data Split"},{"metadata":{"_uuid":"4fd3cd7ef2784e1373bbf9f9b2a280fe4cbedf99","trusted":true},"cell_type":"code","source":"# Clean up and add TFIDF features\ndf.drop(\"description\", axis = 1, inplace=True)\nnon_text_cols = df.columns\ndf = pd.concat([df,df1], axis= 1)\ndel df1\n\n# Modeling Datasets\ntrain_df = df.loc[traindex,:]\ntest_df = df.loc[testdex,:]\nvocab = df.columns\n\n# Has to start at zero for some reason\ny = y - 1\n\n# LGBM Dataset\nlgtrain = lgb.Dataset(df.loc[traindex,vocab],y, categorical_feature= \"auto\")\nprint(\"Starting LightGBM.\\nTrain shape: {}\\nTest shape: {}\".format(train_df.shape,test_df.shape))\nprint(\"Feature Num: \",len(vocab))\nprint(\"\\nDtypes in Model:\\n\",df.dtypes.value_counts())\ndel df; gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e65ee9c0cd59d607a79992de68154e48eeb42297"},"cell_type":"markdown","source":"## TSNE"},{"metadata":{"trusted":true,"_uuid":"9edd2ff3ab1e9d8e8106a44512f10b44b3e05567"},"cell_type":"code","source":"# Create the visualizer and draw the vectors\nplt.figure(figsize = [15,9])\ntsne = TSNEVisualizer()\ntsne.fit(train_df, y_trees)\ntsne.poof()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e67ca6dd863cbd7339a25fb2539b45dbe0e3c430"},"cell_type":"markdown","source":"## [EDA] Continuous Variable Distribution by Cover Type\n\nOut of these distributions, elevation is definitely the most desisive "},{"metadata":{"trusted":true,"_uuid":"4033db8b08e035b1f74a3cc17995ec6947790f27"},"cell_type":"code","source":"# Seperating Variables by Number of Unique Values\ndf_nnunique = train_df.nunique().reset_index().rename(columns = {\"index\": \"cols\",0:\"unique_num\"})\nbinary = list(df_nnunique.loc[df_nnunique.unique_num <= 2, \"cols\"])\ncontinuous = list(df_nnunique.loc[df_nnunique.unique_num > 10, \"cols\"])\nfew_categories = list(df_nnunique.loc[(df_nnunique.unique_num >= 3)\n                                      & (df_nnunique.unique_num <= 10) , \"cols\"])\n\nprint(\"Number of Binary Variables: \", len(binary)-1)\nprint(\"Number of Continous Variables: \", len(continuous)-1)\nprint(\"Number of Non-Binary, Categorical Variables: \", len(few_categories))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c4c80d4662e98a3f2fd7c0b95be0d417eba3f35","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Melt\nmelt_df = pd.melt(pd.concat([train_df.loc[:,continuous],y_trees],axis=1), id_vars=target_var)\ngrid = sns.FacetGrid(melt_df,col=\"variable\", hue=target_var, col_wrap=4 , size=4.0, aspect=1.3, sharex=False, sharey=False)\ngrid.map(sns.kdeplot, \"value\")\ngrid.set_titles(size=20)\ngrid.add_legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00722de9d63b17f864832a67f9a02efd9c945729"},"cell_type":"markdown","source":"**Correlations:** <br>"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"154b2ab5c3533abd63d8107bbb26dbfb8eb18e31"},"cell_type":"code","source":"def rank_correlations(df, figsize=(12,20), n_charts = 18, polyorder = 2, asc = False):\n    # Rank Correlations\n    continuous_rankedcorr = (df\n                             .corr()\n                             .unstack()\n                             .sort_values(ascending=asc)\n                             .drop_duplicates().reset_index())\n    continuous_rankedcorr.columns = [\"f1\",\"f2\",\"Absoluate Correlation Coefficient\"]   \n\n    # Plot Top Correlations\n    top_corr = [(x,y) for x,y in list(continuous_rankedcorr.iloc[:, 0:2].values) if x != y]\n    f, axes = plt.subplots(int(n_charts/3),3, figsize=figsize, sharex=False, sharey=False)\n    row = 0\n    col = 0\n    for (x,y) in top_corr[:n_charts]:\n        if col == 3:\n            col = 0\n            row += 1\n        g = sns.regplot(x=x, y=y, data=df, order=polyorder, ax = axes[row,col])\n        axes[row,col].set_title('Correlation for\\n{} and\\n{}'.format(x, y))\n        col += 1\n    plt.tight_layout(pad=0)\n    plt.show()\nprint(\"rank_correlations Plot Function Ready..\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"167e11cbad85c1f1c9dcef1007f6d1323ee985d4"},"cell_type":"code","source":"rank_correlations(df = train_df.loc[traindex,continuous])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2176e4ac7ec95b88e559b33d8db45324fcf0a6c9"},"cell_type":"markdown","source":"## [EDA] Binary Variable Distribution by Cover Type"},{"metadata":{"_uuid":"6a2195bbdaed0529490309420f34793e563321ea","trusted":true},"cell_type":"code","source":"# Melt\nmelt_df = pd.melt(pd.concat([train_df.loc[:,binary].astype(\"category\"),y_trees],axis=1), id_vars=target_var)\nbinary_data = pd.pivot_table(melt_df, values=\"value\", index=\"variable\",columns=[\"Cover_Type\"], aggfunc = np.sum)\n\nf, ax = plt.subplots(figsize=[8,6])\nsns.heatmap(binary_data, annot=False, fmt=\".2f\",cbar_kws={'label': 'Occurence'},cmap=\"magma\",ax=ax)\nax.set_title(\"Binary Variable Positive Occurence Count by Cover Type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a7c25462528493a447219728b03bcfb2cdf41bd"},"cell_type":"markdown","source":"## Modeling Stage\n\nIf you would like to learn more about TF-IDF and Decision Trees, check out my earlier works:\n- [ABC's of TF-IDF & Boosting](https://www.kaggle.com/nicapotato/abc-s-of-tf-idf-boosting-0-798)\n- [My Titanic Overview of Model Paradigms](https://www.kaggle.com/nicapotato/titanic-voting-pipeline-stack-and-guide)"},{"metadata":{"_uuid":"5a281f230dd6d1f639c771d2b348937d619b422d"},"cell_type":"markdown","source":"### Cross-Validation\n**Hand Tuning:**"},{"metadata":{"_uuid":"132ecdf9c74c19234b14fd1ad6a2b7473f7524c2","trusted":true},"cell_type":"code","source":"print(\"Light Gradient Boosting Classifier: \")\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': ['multi_error'],\n    \"learning_rate\": 0.05,\n     \"num_leaves\": 60,\n     \"max_depth\": 9,\n     \"feature_fraction\": 0.45,\n     \"bagging_fraction\": 0.3,\n     \"reg_alpha\": 0.15,\n     \"reg_lambda\": 0.15,\n#      \"min_split_gain\": 0,\n      \"min_child_weight\": 0\n                }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e7310ab293e89b282b0e776d75bdbc3b3d1764c","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"modelstart= time.time()\n# Find Optimal Parameters / Boosting Rounds\nlgb_cv = lgb.cv(\n    params = lgbm_params,\n    train_set = lgtrain,\n    num_boost_round=2000,\n    stratified=True,\n    nfold = 5,\n    verbose_eval=50,\n    seed = 23,\n    early_stopping_rounds=75)\n\nloss = lgbm_params[\"metric\"][0]\noptimal_rounds = np.argmin(lgb_cv[str(loss) + '-mean'])\nbest_cv_score = min(lgb_cv[str(loss) + '-mean'])\n\nprint(\"\\nOptimal Round: {}\\nOptimal Score: {} + {}\".format(\n    optimal_rounds,best_cv_score,lgb_cv[str(loss) + '-stdv'][optimal_rounds]))\n\nresults = results.append({\"Rounds\": optimal_rounds,\n                          \"Score\": best_cv_score,\n                          \"STDV\": lgb_cv[str(loss) + '-stdv'][optimal_rounds],\n                          \"LB\": None,\n                          \"Parameters\": lgbm_params}, ignore_index=True)\nif Home is True:\n    with open('results.csv', 'a') as f:\n        results.to_csv(f, header=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61428128e80ea2e69ab8e1892fac611de314ed80","scrolled":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 800)\nif Home is False:\n    display(results.sort_values(by=\"Score\",ascending = True))\nelse:\n    display(pd.read_csv(\"results.csv\").sort_values(by=\"Score\",ascending = True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bc4f5e58b77f0c5b04a498f20ee566ff813622a"},"cell_type":"markdown","source":"**Iterative Tuning:** <br>\nMy current learning rate is 0.05. Say I what to see how well it's neigbors perform.."},{"metadata":{"_uuid":"f6b13283e701dd3297c49942d75f164e555e160e","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"tune_parameter = [0.08,0.03]\ntune = 'learning_rate'\nfor param in tune_parameter:\n    print(\"{} Parameter: {}\".format(tune, param))\n    modelstart= time.time()\n    lgbm_params[tune] = param\n    # Find Optimal Parameters / Boosting Rounds\n    lgb_cv = lgb.cv(\n        params = lgbm_params,\n        train_set = lgtrain,\n        num_boost_round=10000,\n        stratified=True,\n        nfold = 5,\n        verbose_eval=50,\n        seed = 23,\n        early_stopping_rounds=75)\n\n    optimal_rounds = np.argmin(lgb_cv[str(loss) + '-mean'])\n    best_cv_score = min(lgb_cv[str(loss) + '-mean'])\n\n    print(\"Optimal Round: {}\\nOptimal Score: {} + {}\".format(\n        optimal_rounds,best_cv_score,lgb_cv[str(loss) + '-stdv'][optimal_rounds]))\n    print(\"###########################################################################################\")\n\n    results = results.append({\"Rounds\": optimal_rounds,\n                              \"Score\": best_cv_score,\n                              \"STDV\": lgb_cv[str(loss) + '-stdv'][optimal_rounds],\n                              \"LB\": None,\n                              \"Parameters\": lgbm_params}, ignore_index=True)\n    if Home is True:\n        with open('results.csv', 'a') as f:\n            results.to_csv(f, header=False)\n        # results = pd.read_csv(\"results.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0adfba411a52c4c95d1f0067466375e27367937a","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.set_option('max_colwidth', 800)\nif Home is False:\n    display(results.sort_values(by=\"Score\",ascending = True))\nelse:\n    display(pd.read_csv(\"results.csv\").sort_values(by=\"Score\",ascending = True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb5bb129aac2718351d80548420f6c352397515"},"cell_type":"markdown","source":"## Final Model and Seed Diversification\nOut-of-Fold Methods are very poplular in public kernels at the moment. This is a alternative that operates in a similar way. Instead of explicitly dividing the data, the randomized seed changes the row and column subsampling choice which also forces the model to do without some data to achieve a more diverse, robust prediction."},{"metadata":{"_uuid":"efbb975ccab79d7a6ee7b58522561e1cffea4cdd","trusted":true,"collapsed":true},"cell_type":"code","source":"# Best Parameters\nfinal_model_params = results.iloc[results[\"Score\"].idxmin(),:][\"Parameters\"]\noptimal_rounds = results.iloc[results[\"Score\"].idxmin(),:][\"Rounds\"]\nprint(\"Parameters for Final Models:\\n\",final_model_params)\nprint(\"Score: {} +/- {}\".format(results.iloc[results[\"Score\"].idxmin(),:][\"Score\"],results.iloc[results[\"Score\"].idxmin(),:][\"STDV\"]))\nprint(\"Rounds: \", optimal_rounds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f3516fd021a6bfe71967b70acc7da5fe1c6c2af","trusted":true,"collapsed":true},"cell_type":"code","source":"allmodelstart= time.time()\n# Run Model with different Seeds\nmulti_seed_pred = dict()\nall_feature_importance_df  = pd.DataFrame()\n\n# To submit each seed model seperately aswell\ndef seed_submit(model,seed):\n    # Output position with highest probability\n    class_prediction = (pd.DataFrame(model.predict(test_df)).idxmax(axis=1) + 1).rename(target_var)\n    class_prediction.index = testdex\n\n    # Submit\n    class_prediction.to_csv('mean_seed{}_sub_ep{}_sc{}.csv'.format(seed,optimal_rounds,round(best_cv_score,5))\n                ,index = True, header=True)\n\nall_seeds = [27,22,300,401]\nfor seeds_x in all_seeds:\n    modelstart= time.time()\n    print(\"Seed: \", seeds_x,)\n    # Go Go Go\n    final_model_params[\"seed\"] = seeds_x\n    lgb_final = lgb.train(\n        final_model_params,\n        lgtrain,\n        num_boost_round = optimal_rounds + 1,\n        verbose_eval=200)\n\n    # Feature Importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = vocab\n    fold_importance_df[\"importance\"] = lgb_final.feature_importance()\n    all_feature_importance_df = pd.concat([all_feature_importance_df, fold_importance_df], axis=0)\n\n    multi_seed_pred[seeds_x] =  pd.DataFrame(lgb_final.predict(test_df))\n    # Submit Model Individually\n    seed_submit(model= lgb_final, seed= seeds_x)\n    print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n    print(\"###########################################################################################\")\n    del lgb_final\n\ncols = all_feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\nbest_features = all_feature_importance_df.loc[all_feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(8,10))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgb_finalm_importances.png')\nprint(\"All Model Runtime: %0.2f Minutes\"%((time.time() - allmodelstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a74f0fe4899b4810fc748f9e85fcfe0aaac3ea1c","trusted":true,"collapsed":true},"cell_type":"code","source":"# Collapse Seed DataFrames\npanel = pd.Panel(multi_seed_pred)\nprint(\"Seed Effect Breakdown: Classwise Statistics\")\nfor i,(std,mean) in enumerate(zip(panel.std(axis=0).mean(axis=0),panel.mean(axis=0).mean(axis=0))):\n    print(\"Class {}:\".format(i+1))\n    print(\"Mean {0:.3f} (+/-) {1:.5f}\\n\".format(mean,std))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81296e84510115e420846523a4ac4f9b9649cf1e","trusted":true,"collapsed":true},"cell_type":"code","source":"# Take Mean over Seed prediction\nmean_prob = panel.mean(axis=0)\n# Output position with highest probability\nclass_prediction = mean_prob.idxmax(axis=1) + 1\nclass_prediction.rename(target_var,inplace=True)\nclass_prediction.index = testdex\n\n# Submit\nclass_prediction.to_csv('mean_sub_ep{}_sc{}.csv'.format(optimal_rounds,round(best_cv_score,5))\n            ,index = True, header=True)\n# Save Results..\nresults.to_csv(\"model_tuning_results.csv\")\nclass_prediction.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f44e40d8483c5ff0b8bc9ac37f9cc9b07b7e84da","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78477c3483e91e8033cebd1436ccfaabf71b3d1f","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}