{"cells":[{"metadata":{"_uuid":"e44e5ca4e44464ffdcce0e840c543577bd6569bf"},"cell_type":"markdown","source":"# How to understand feature importance of categorical features reported by LightGBM?\nLightGBM allows one to specify directly categorical features and handles those internally in a smart way, that might out-perform OHE. Originally, *I was puzzled about feature importance reported for such categorical features*. After  iterating in comments, and learning more about feature importance reported, it seems that:\n\n- **the default implementation is not very useful**, as there are several types of importance and importance values do not behave according to intuitive expectation. See [this blog post](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27) for clear motivation and introduction into SHAP;\n- it is beneficial to use [SHAP package in python](https://github.com/slundberg/shap) to produce stable feature-importance evaluation.\n\nIt all started with abnormally high importance reported for `ORGANIZATION_TYPE` in [an earlier version of my modified fork](https://www.kaggle.com/mlisovyi/modular-good-fun-with-ligthgbm?scriptVersionId=3888846) of  [olivier's](https://www.kaggle.com/ogrellier) very popular [Good_fun_with_LigthGBM kernel](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm). After some investigation I realized that the problem was due to missing OHE of categorical features (beacuse categorical feature were stores as `categories` instead of `objects`). I fixed that, `ORGANIZATION_TYPE` got OHE-transformed and disappeared from tops of  important features. \n\nThen I started to looking into how to use internal handling of categorical features in LightGBM. It turns out that the **sklearn API of LightGBM actually has those enabled by default**, in a sense that by default it tries to guess which features are categorical, if you provided a `pd.DataFrame` as input (because it has `feature_name='auto', categorical_feature='auto'` as the defaults in the `lgb.LGBMModel.fit()` method). And it makes that guess assuming that all features of type `category` have to be treated with the internal categorical treatment (i.e. following [this procedure from the docs](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support)). It turns out that in such case LightGBM reports unexpectedly high importance  in some cases.\n\nBelow is a minimalistic example to reproduce this behaviour and an illustrastion of SHAP usage."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport gc\ngc.enable()\n\nPATH = '../input/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed4d74b3e586cc5b1be41bf67756e370b86186a"},"cell_type":"markdown","source":"## Read in the basic 'application' data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"application_train = pd.read_csv(PATH+'application_train.csv')\n\ny = application_train['TARGET']\nX = application_train.drop(['TARGET', 'SK_ID_CURR'], axis=1)\n\ndel application_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b16b0521be60b8908cc03855b8f83e2a2721a706"},"cell_type":"markdown","source":"Transform categorical features into the appropriate type that is expected by LightGBM."},{"metadata":{"trusted":true,"_uuid":"a8b70981911b2dd1f674ebcdc4c1464efb284113","collapsed":true},"cell_type":"code","source":"for c in X.columns:\n    col_type = X[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[c] = X[c].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9e31b9753f8a4a042e44c71dbcb0ac90a78714b"},"cell_type":"markdown","source":"Printout types of features in the dataset"},{"metadata":{"trusted":true,"_uuid":"b8439adb3926082be37d327a838a17e1055870bc","collapsed":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"077f571d694f0b446b0c2b84991bf91071d84ce0"},"cell_type":"markdown","source":"# Model fitting\nWe will use LightGBM classifier (i.e. sklearn API)\n### Split the full sample into train/test (80/20)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c9053167195838284544e5d717c1c68b27b46fb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8af9e2ff9ca6f6bb59b7380bc65c99a5063f4c7"},"cell_type":"markdown","source":"### Use test subset for early stopping criterion \nThis allows us to avoid overtraining and we do not need to optimise the number of trees"},{"metadata":{"trusted":true,"_uuid":"f565eef3d14a6d8ade0602823dcd316ad2829117","collapsed":true},"cell_type":"code","source":"fit_params={\"early_stopping_rounds\":10, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'feature_name': 'auto', # that's actually the default\n            'categorical_feature': 'auto' # that's actually the default\n           }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b127e1f2d1e119632bec1b0c4c18d64ee5de9aff"},"cell_type":"markdown","source":"### Create a model. \nParameters are rough guesstimates. they are not supposed to be the best optimal choice.[](http://)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5bb7998f3db77da6fc23737bbf8d80a260391f24"},"cell_type":"code","source":"import lightgbm as lgb\n#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 1000 define only the absolute maximum\nclf = lgb.LGBMClassifier(num_leaves= 15, max_depth=-1, \n                         random_state=314, \n                         silent=True, \n                         metric='None', \n                         n_jobs=4, \n                         n_estimators=1000,\n                         colsample_bytree=0.9,\n                         subsample=0.9,\n                         learning_rate=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af7d792fa3b44bbf11bec995032c95bfacf12ee2"},"cell_type":"markdown","source":"## Train the model\nWe do training with the 0.8 subset of the dataset and 0.2 subset for early stopping. "},{"metadata":{"trusted":true,"_uuid":"53bb37030e953d3e5d332e5886997de9925109e0","scrolled":true,"collapsed":true},"cell_type":"code","source":"#force larger number of max trees and smaller learning rate\nclf.fit(X_train, y_train, **fit_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d7b27535ec0dfdfe5ae599d3120d714487de4ec"},"cell_type":"markdown","source":"### Plot feature importance"},{"metadata":{"trusted":true,"_uuid":"f92343b2289eeac9ff3006bb8efcf8591ad89f2b","collapsed":true},"cell_type":"code","source":"feat_imp = pd.Series(clf.feature_importances_, index=X.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cb372267b91470fb7579a5fe38f77e6cd8e48a0c"},"cell_type":"markdown","source":"Upsss. `ORGANIZATION_TYPE` pops up as the most *important*. But do not celerbate- if you train the same model on the same data with OHE for categorical features you will get the same ROC AUC (and a similar importance for the `EXT_SOURCE_x` features as on this plot), i.e. most likely just importance of `ORGANIZATION_TYPE` is reported wrong, unless i misunderstand something. Any feedback will be helpful for me to make the next step in LightGBM usage."},{"metadata":{"trusted":true,"_uuid":"50e48408622d1bc45d4cacd0f8f119469f2103b4","collapsed":true},"cell_type":"code","source":"class LGBMClassifier_GainFE(lgb.LGBMClassifier):\n    @property\n    def feature_importances_(self):\n        if self._n_features is None:\n            raise LGBMNotFittedError('No feature_importances found. Need to call fit beforehand.')\n        return self.booster_.feature_importance(importance_type='gain')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"319339af1e09fff5a4b5e69f5a8c848edf04770a"},"cell_type":"code","source":"clf2 = LGBMClassifier_GainFE(num_leaves= 15, max_depth=-1, \n                         random_state=314, \n                         silent=True, \n                         metric='None', \n                         n_jobs=4, \n                         n_estimators=1000,\n                         colsample_bytree=0.9,\n                         subsample=0.9,\n                         learning_rate=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7db7ce35928abfc9aacf0e57396721d330580ed1","collapsed":true},"cell_type":"code","source":"clf2.fit(X_train, y_train, **fit_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de9fd64cc4a42f493cbad02dd08c0f9f2e858211","collapsed":true},"cell_type":"code","source":"feat_imp = pd.Series(clf2.feature_importances_, index=X.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c50fadea9dcdc60db5903034674509791f029b6e"},"cell_type":"markdown","source":"## Let's try SHAP\nfollowing comment by [@cast42](https://www.kaggle.com/cast42) and his kernel [here]()"},{"metadata":{"trusted":true,"_uuid":"e07999507bcc2fbe9643fa4904b03ef8229d3b68","collapsed":true},"cell_type":"code","source":"import shap\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c147eebcbca082a9f35d014f07eb249a785084e"},"cell_type":"code","source":"shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8929a4365f231a09f9e83080b955d178b5588cdd","collapsed":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"094a315481480f5a62d98b2ce8e02728106fc450"},"cell_type":"markdown","source":"This agrees more with our intuitive expectation of which features show show up on top in importance ranking. Note, that categoric features do not show colour highlighting, as higher/lower value is not defined."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5634b51a8829d060da619429d1177e69d3985af1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}