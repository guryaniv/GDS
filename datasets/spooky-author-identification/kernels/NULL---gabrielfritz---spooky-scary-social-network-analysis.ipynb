{"cells": [{"cell_type": "markdown", "metadata": {"_uuid": "e55947712ed0d46313a4087afd481cc0509cb39c", "_cell_guid": "63e21bb2-336f-475b-9a81-75a8f79ecbda"}, "source": ["<img src='https://78.media.tumblr.com/f19cda02e2b30628efe6b3fc526b4186/tumblr_otc6gnYF9J1w1ai4so1_500.gif'>"]}, {"cell_type": "markdown", "metadata": {"_uuid": "eba46b01c78882f266d992c383f181dd74f31683", "_cell_guid": "1448b1a1-0d37-4061-b84a-c59c0c21986e"}, "source": ["# Introduction\n", "\n", "In this kernel, I will show how basic **NLP** and **Social Network Analysis** can work together to generate a similarity network between the texts. Social Networks are structures that represents entities and relations between each other. We will visualize the social networks using **graphs** and draw some results using network metrics.\n", "\n", "__note:__ This kernel got some inspiration on the kernel of [Anisotropic](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial). Please, check his kernel! It's very interesting!\n", "\n", "## Notebook summary<br><br>\n", "\n", "**1. Data contextualization**\n", "\n", "**2. Basic NLP: Constructing Document x Document Matrix**\n", "\n", "**3. Social Network as a Graph**\n", "\n", "**4. Network Centralities: Eigenvector and Betweenness**<br><br><br>\n", "\n", "\u201cInvisible things are the only realities.\u201d \n", "> Egdar Allan Poe, Loss of Breath "]}, {"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "from matplotlib import pyplot as plt\n", "import networkx as nx\n", "import networkx.drawing.layout as nxlayout\n", "from math import sqrt\n", "%matplotlib inline\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "metadata": {"_uuid": "df9f8c84bed9f7bc919edadffa2a88f5152cfef1", "_cell_guid": "ebef48ff-e699-44f7-bbc7-3c9e1bad45c9"}, "execution_count": 1}, {"cell_type": "markdown", "metadata": {"_uuid": "ae2f961ce4f4fe22ef524e3e8069a9d614cce3d3", "_cell_guid": "88fe424c-c193-407e-b3d9-f03516cf9238"}, "source": ["## 1. Data Contextualization: The Authors\n", "\n", "* [Edgar Allan Poe - EAP](https://en.wikipedia.org/wiki/Edgar_Allan_Poe): \"Edgar Allan Poe was an American writer, editor, and literary critic. Poe is best known for his **poetry and short stories**, particularly his tales of **mystery and the macabre**. He is widely regarded as a central figure of **Romanticism** in the United States and American literature as a whole, and he was one of the country's earliest practitioners of the short story. Poe is generally considered the **inventor of the detective fiction genre** and is further credited with **contributing to the emerging genre of science fiction**. He was the first well-known American writer to try to earn a living through writing alone, resulting in a **financially difficult life and career**.\"\n", "* [H. P. Lovecraft - HPL](https://en.wikipedia.org/wiki/H._P._Lovecraft):  \"Howard Phillips Lovecraft was an American writer who achieved posthumous fame through his influential works of **horror fiction**. Regarded as one of the most significant 20th-century authors in his genre.  Among his most celebrated tales are **\"The Rats in the Walls\", \"The Call of Cthulhu\", \"At the Mountains of Madness\" and \"The Shadow Out of Time\"**, all canonical to the Cthulhu Mythos.\"\n", "* [Mary Shelley - MWS](https://en.wikipedia.org/wiki/Mary_Shelley): \"Mary Wollstonecraft Shelley was an English novelist, short story writer, dramatist, essayist, biographer, and travel writer, best known for her Gothic novel **Frankenstein: or, The Modern Prometheus (1818)**.\"\n", "> Wikipedia (adapted)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b58570109bd16f689b9c685df6d4eb369fa5c54f", "_cell_guid": "170f2e3b-838e-494f-b2b7-dfb9c8d88aa3"}, "source": ["## 2. Basic NLP: Constructing Document x Document Matrix"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2c51c17045ab48771f029d599fe1b9eb25e22b9d", "_cell_guid": "7a1975e1-7760-4987-8338-35bf253a1181"}, "source": ["Let's build our [Document x Term matrix (DxT)](https://en.wikipedia.org/wiki/Document-term_matrix). This matrix has, as rows, documents and, as columns, terms. Each value DxT[a,b] in the matrix will be the [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) of the term[b] in the document[a]\n", "\n", "I will let the **sklearn.feature_extraction.text.TfidfVectorizer** deal with removing stopwords, tokenization and normalization of the matrix. :D"]}, {"outputs": [], "source": ["df = pd.read_csv('../input/train.csv')\n", "tfidf = TfidfVectorizer(stop_words='english',norm='l2')\n", "DxT = tfidf.fit_transform(df['text'])\n", "pd.DataFrame(data=DxT.toarray(),index=df['id'],columns=tfidf.get_feature_names()).head()"], "cell_type": "code", "metadata": {"scrolled": true, "_kg_hide-input": true, "_uuid": "c7861c790a59c68d3b78ee7d17b127480e51f8e0", "_cell_guid": "3a3e8e58-000b-4956-8a73-1bcb155821b0"}, "execution_count": 2}, {"cell_type": "markdown", "metadata": {"_uuid": "9e121ea83c65ab81587d473cc43d51ea2d69c96a", "_cell_guid": "1753dae6-6106-437f-bbeb-6f40b4424697"}, "source": ["As you can see, there's a bunch of zeroes on our DxT matrix. This is because DxT matrices are, normally, [sparse](https://en.wikipedia.org/wiki/Sparse_matrix)."]}, {"cell_type": "markdown", "metadata": {"_uuid": "ece0678d0b38067536e75bf8c4541c8f3dce569a", "collapsed": true, "_cell_guid": "42f277f5-7155-4c50-a61a-e232b0a2572e"}, "source": ["You can think of each row of the DxT matrix as  **vector representations** of the documents. In the next step, We will calculate the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the vectors representing documents. This will produce a Document x Document (DxD) matrix where each value DxD[a,b] is the **cosine of the angle between the Document[a] and Document[b]**. The formula to calculate the cosine of the angle between two vectors is:\n", "\n", "<img src='http://www.sciweavers.org/upload/Tex2Img_1510312920/render.png'>\n", "\n", "But, in our case, we have already **normalized** the document vectors! Remember the **TfidfVectorizer**? The \"norm\" argument allows us to normalize the vectors by some normalization method. Using the method **\"l2\"**, also called **Euclidean Norm**, TfidfVectorizer take every Document vectors A calculated and divide it by |A|. This means that the vector modulus of A becomes 1! In this case, our cosine similarity formula becomes:\n", "<img src='http://www.sciweavers.org/upload/Tex2Img_1510313323/render.png'>\n", "\n", "Applying the dot product between the matrix DxT and the matrix TxD (DxT transposed) results in the matrix DxD!"]}, {"outputs": [], "source": ["DxD = np.dot(DxT,DxT.T)"], "cell_type": "code", "metadata": {"_uuid": "beb1195012ddc10d77284be0e871851dabf0bc2f", "collapsed": true, "_cell_guid": "5be5b665-540a-4a60-ab3a-68b3e09c0b10"}, "execution_count": 3}, {"cell_type": "markdown", "metadata": {"_uuid": "7ef74eb9788d1fd759df35ec0ab0c09f71cfdefa", "_cell_guid": "ca81b5f3-2593-4010-a82b-95daaf797c49"}, "source": ["Now that NLP is over, we can start with Social Network Analysis! :D"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4055c89dcc19f1089c628c543d3fcb7b167b952b", "_cell_guid": "3bc12c8d-86b8-4fe4-b378-4d1ed3f683fb"}, "source": ["## 3. Social Network as a Graph"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b2dc2ffbf2bcc75212bc4491f6e4d434c172714e", "_cell_guid": "ce0af709-5262-4c44-902f-9c9a0883e600"}, "source": ["In [Social Network Analysis](https://en.wikipedia.org/wiki/Social_network_analysis), there're two basic elements that caracterizes network structures: **Nodes** (representing entities) and **Edges** (establishing relations). In our case,\n", "* The Nodes will be each row in the dataset, identified by its \"id\" and having \"text\" and \"author\" as attributes;\n", "* The establishment of an Edge between Nodes will be defined as follow:\n", "\n", "        Be \"a\" and \"b\" Nodes,\n", "\n", "        if DxD[a,b] >= cutoff, establish Edge[a,b,w] with w = DxD[a,b];\n", " **Graph Representation:**<br>\n", " Now that our basic structures are chosen, we can worry about visual aspects of our graph representation. The nodes will be represented as **circles** and the edges will be represented as **lines** between nodes. In this case, the network is **undirected**, this means that the relations between nodes are in **both ways**. When the network is directed, the relations have direction associated with them, so we represent it using **arrows**. The **thickness** of the edges will be proportional to the similarity between the pair of nodes and the **color** of the nodes will represent the **author**.\n", " \n", " "]}, {"outputs": [], "source": ["G = nx.Graph()\n", "for i in range(len(df)):\n", "    idx = df.at[i,'id']\n", "    text = df.at[i,'text']\n", "    author = df.at[i,'author']\n", "    G.add_node(idx,text=text,author=author)\n", "\n", "dense_DxD = DxD.toarray()\n", "len_dense = len(dense_DxD)\n", "cutoff=0.5\n", "for i in range(len_dense):\n", "    for j in range(i+1,len_dense):\n", "        if dense_DxD[i,j]>=cutoff:\n", "            weight=dense_DxD[i,j]\n", "            G.add_edge(df.at[i,'id'],df.at[j,'id'],weight=weight)\n", "\n", "for node,degree in list(G.degree().items()):\n", "    if degree == 0:\n", "        G.remove_node(node)\n", "\n", "pos = nxlayout.fruchterman_reingold_layout(G,k=1.5/sqrt(len(G.nodes())))\n", "\n", "edge_data = []\n", "colors = {'EAP':'1','HPL':'2','MWS':'3'}\n", "for u,v,w in G.edges(data=True):\n", "    x0,y0 = pos[u]\n", "    x1,y1 = pos[v]\n", "    w = w['weight']\n", "    edge_data.append(go.Scatter(x=[x0,x1,None],\n", "                            y=[y0,y1,None],\n", "                            line=go.Line(width=3.0*w,color='#888'),\n", "                            hoverinfo='none',\n", "                            mode='lines'))\n", "\n", "\n", "node_data = go.Scatter(\n", "        x=[],\n", "        y=[],\n", "        text=[],\n", "        mode='markers',\n", "        hoverinfo='text',\n", "        marker=go.Marker(\n", "            showscale=True,\n", "            colorscale='Viridis',\n", "            reversescale=True,\n", "            color=[],\n", "            size=5.0,\n", "            colorbar=dict(\n", "                thickness=15,\n", "                xanchor='left',\n", "                tickmode='array',\n", "                tickvals=[1,2,3],\n", "                ticktext=['EAP','HPL','MWS'],\n", "                ticks = 'outside'\n", "            ),\n", "            line=dict(width=0.5)))\n", "\n", "for u,w in G.nodes(data=True):\n", "    x,y = pos[u]\n", "    color = colors[w['author']]\n", "    text = w['text']\n", "    node_data['x'].append(x)\n", "    node_data['y'].append(y)\n", "    node_data['text'].append(text)\n", "    node_data['marker']['color'].append(color)"], "cell_type": "code", "metadata": {"_kg_hide-input": true, "collapsed": true, "_kg_hide-output": true, "_uuid": "a601066d775423f264aa9eef51bcb788024a19f0", "scrolled": true, "_cell_guid": "6880e82e-5796-4903-b0fd-28eb9a9be839"}, "execution_count": 4}, {"outputs": [], "source": ["py.iplot(go.Figure(data=edge_data+[node_data],\n", "                layout=go.Layout(\n", "                width=800,\n", "                height=600,\n", "                title='<br>Spooky Similarity Network',\n", "                titlefont=dict(size=16),\n", "                showlegend=False,\n", "                hovermode='closest',\n", "                margin=dict(b=20,l=5,r=5,t=40),\n", "                annotations=[ dict(\n", "                    text=\"Kaggle\",\n", "                    showarrow=False,\n", "                    xref=\"paper\", yref=\"paper\",\n", "                    x=0.005, y=-0.002 ) ],\n", "                xaxis=go.XAxis(showgrid=False, zeroline=False, showticklabels=False),\n", "                yaxis=go.YAxis(showgrid=False, zeroline=False, showticklabels=False))))"], "cell_type": "code", "metadata": {"_kg_hide-input": true, "_uuid": "b22b1fc33a595f2ea2160bf78687d4c04f761671", "_cell_guid": "e915ee30-b1ab-4f27-ac5e-8168497bf414"}, "execution_count": 5}, {"cell_type": "markdown", "metadata": {"_uuid": "be92326dac28f5e78ce589206d1b0879a6650b91", "_cell_guid": "07f13b4e-f87f-4cfc-a611-4ab05097c8c6"}, "source": ["**Pretty cool, right?** :D This kind of visualization is very good to navigate and have fun!\n", "\n", "Nooow! Let's calculate some metrics for this network!"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0aa67bc3f6a0eb52f48e293769d8dd43012df7c9", "_cell_guid": "7598a405-84db-4e59-9aff-c37264593f67"}, "source": ["## 4. Network Centralities: Betweenness and Eigenvector\n", "<br><br>\n", "To analyse this network, we will use two centrality measurements: **Betweenness centrality** and **Eigenvector centrality**.<br>\n", "[**Betweenness centrality**](https://en.wikipedia.org/wiki/Centrality#Betweenness_centrality) measures the number of times a node acts as a **bridge** along the shortest path between two other nodes. It measures control of a node over the communication of groups in the network. In our case, allows us to identify which nodes work as a \"semantic bridge\" (this is not a thing, I made up because I did not find a better way of explaining) between semantic groups.<br><br>\n", "[**Eigenvector centrality**](https://en.wikipedia.org/wiki/Centrality#Eigenvector_centrality) is a measure of the **influence** of a node in a network. It assigns relative scores to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. Google's PageRank and the Katz centrality are variants of the eigenvector centrality.<br><br>\n", "In an hypothetical practical situation, if I had to generally understand a set of texts but can't read everything, I would calculate this metrics, take the Top N nodes with the bigger centralities values and read them! (I, usually, do it a lot by the way). In this specific case, this doesn't work much because the text sizes are small.<br><br>\n", "\n", "Ploting the Eigenvector and the Betweenness in a scatter plot is a good way to visualize! :D"]}, {"outputs": [], "source": ["from math import log\n", "betweenness = nx.betweenness_centrality(G)\n", "max_betweenness = sorted(betweenness.items(),key=lambda x:x[1],reverse=True)[0][1]\n", "betweenness = [(a,(log(1+float(b)/(max_betweenness)))) for a,b in betweenness.items()]\n", "eigen = nx.eigenvector_centrality(G)\n", "max_eigen = sorted(eigen.items(),key=lambda x:x[1],reverse=True)[0][1]\n", "eigen = [(a,(log(1+float(b)/(max_eigen)))) for a,b in eigen.items()]\n", "eigen = dict(eigen)\n", "betweenness = dict(betweenness)\n", "scatter_data = go.Scatter(\n", "        x=[],\n", "        y=[],\n", "        text=[],\n", "        mode='markers',\n", "        hoverinfo='text',\n", "        marker=go.Marker(\n", "            showscale=True,\n", "            colorscale='Viridis',\n", "            reversescale=True,\n", "            color=[],\n", "            size=5.0,\n", "            colorbar=dict(\n", "                thickness=15,\n", "                xanchor='left',\n", "                tickmode='array',\n", "                tickvals=[1,2,3],\n", "                ticktext=['EAP','HPL','MWS'],\n", "                ticks = 'outside'\n", "            ),\n", "            line=dict(width=0.5)))\n", "for u,w in G.nodes(data=True):\n", "    x,y = pos[u]\n", "    color = colors[w['author']]\n", "    text = w['text']\n", "    scatter_data['x'].append(eigen[u])\n", "    scatter_data['y'].append(betweenness[u])\n", "    scatter_data['text'].append(text)\n", "    scatter_data['marker']['color'].append(color)\n", "py.iplot(go.Figure(data=[scatter_data],\n", "                layout=go.Layout(\n", "                width=800,\n", "                height=600,\n", "                title='<br>Log of Eigenvector Centrality X Log of Betweenness Centrality',\n", "                titlefont=dict(size=16),\n", "                showlegend=False,\n", "                hovermode='closest',\n", "                margin=dict(b=50,l=50,r=100,t=100),\n", "                annotations=[ dict(\n", "                    text=\"Kaggle\",\n", "                    showarrow=False,\n", "                    xref=\"paper\", yref=\"paper\",\n", "                    x=0, y=-0.005 ) ],\n", "                xaxis=go.XAxis(title='Eigenvector',showgrid=True, zeroline=True, showticklabels=True),\n", "                yaxis=go.YAxis(title='Betweenness',showgrid=True, zeroline=True, showticklabels=True))))"], "cell_type": "code", "metadata": {"_kg_hide-input": true, "_uuid": "083eca57daffada737d03fd2eb5ba91f667813b0", "_cell_guid": "1c74665d-ac2a-4efa-932f-c81338b2285f"}, "execution_count": 6}, {"cell_type": "markdown", "metadata": {"_uuid": "fc82d2aab728d748d92fb68121cb698a516251d0", "_cell_guid": "7cc60596-58df-4109-ba4c-026627ec8054"}, "source": ["That's it for now! :D"]}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}