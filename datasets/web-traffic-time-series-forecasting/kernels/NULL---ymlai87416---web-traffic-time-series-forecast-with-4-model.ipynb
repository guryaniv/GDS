{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.1", "pygments_lexer": "ipython3", "file_extension": ".py"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "b9e54ae2e59bbe01fae9ae8538f9051a7cc5523f", "_cell_guid": "a2b0c508-7688-4ed9-9acf-b95b4a9f2dd3", "collapsed": true}, "source": ["# Web Traffic Time Series Forecasting (Experimenting with different method)\n", "\n", "By Lai Yiu Ming, Tom\n", "\n", "1. Introduction\n", "    1. Competition details\n", "    2. Load libraries and data files, file structure and content\n", "    3. Missing values\n", "    4. Data visualization\n", "    5. Extreme data\n", "2. Data transformation and helper functions\n", "    1. Article names and metadata\n", "    2. Split into train and validation dataset\n", "3. Forecast methods\n", "    1. SMAPE, the measurement\n", "    2. Simple median model\n", "    3. Median model - weekday, weekend and holiday\n", "    4. ARIMA model\n", "    5. Facebook prophet model\n", "    6. Sample series analysis (For script reconciliation)\n", "4. Selected model performance (validation score) over train dataset\n", "    1. Simple median model\n", "    2. Median model - weekday, weekend, holiday\n", "    3. ARIMA model\n", "    4. Facebook model\n", "    5. mixed model"]}, {"cell_type": "markdown", "metadata": {"_uuid": "466341ff90ee4d39548c2f90174d6d5933c2c62d", "_cell_guid": "80d89790-42b4-4117-82c0-4452210f13f7"}, "source": ["# 1. Introduction \n", "\n", "## A. Competition details\n", "\n", "### First stage\n", "* Training data from 2015-07-01 to 2016-12-29\n", "* Testing data from 2017-01-01 to 2017-03-01\n", "* Length of training vs length of testing = 547 days vs 59 days\n", "* Predict interval is ~10.7% of the training interval\n", "\n", "### Second stage\n", "* Training data from 2015-07-01 to 2017-09-01\n", "* Testing data from 2017-09-10 to 2017-11-10\n", "* Length of training vs length of testing = 793 days vs 61 days\n", "* Predict interval is ~7.7% of the training interval"]}, {"cell_type": "markdown", "metadata": {"_uuid": "10592e734dd2568886858d69960a1e2c8f3f5ed9", "_cell_guid": "323c4a34-6421-4288-ac99-2916900dd11e"}, "source": ["## B. Load libraries and data files, file structure and content"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "28610d78b73a0eb79868e208dd46f91045c7bb75", "_cell_guid": "102debc2-87d8-41c4-bfac-82508f003abb", "collapsed": true}, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from fbprophet import Prophet\n", "import matplotlib.pyplot as plt\n", "import math as math\n", "\n", "%matplotlib inline"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "9d9e2a33b5f7e70d1e3f080c01ef8b4f28a14bc6", "_cell_guid": "a7bd9791-f58e-4b4b-ad28-dab934014da8", "collapsed": true}, "source": ["# Load the data\n", "train = pd.read_csv(\"../input/web-traffic-time-series-forecasting/train_1.csv\")\n", "keys = pd.read_csv(\"../input/web-traffic-time-series-forecasting/key_1.csv\")\n", "ss = pd.read_csv(\"../input/web-traffic-time-series-forecasting/sample_submission_1.csv\")"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f340420a80c636df3d8c6e45f07a418bd0d57583", "_cell_guid": "7cf073a9-c545-4199-865e-8ffd911cb53b", "collapsed": true}, "source": ["train.head()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9ffedd5f603ceafcd6e6b192f813f64b22fdc9e9", "_cell_guid": "9c72cdb3-e2ce-4ad5-8fbd-4cf501441145"}, "source": ["## C. Missing values"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c1be90c632f06a95e31a19a6305d4b204fe0c3dd", "_cell_guid": "b897047e-daa0-49e7-8f60-2bb46487b9a6", "collapsed": true}, "source": ["# Check the data\n", "print(\"Check the number of records\")\n", "print(\"Number of records: \", train.shape[0], \"\\n\")\n", "\n", "print(\"Null analysis\")\n", "empty_sample = train[train.isnull().any(axis=1)]\n", "print(\"Number of records contain 1+ null: \", empty_sample.shape[0], \"\\n\")"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7ff43c83dacae8bbdea49114e98c73808628dba5", "scrolled": false, "_cell_guid": "e47c83b5-c349-45c0-be65-a333871f14fc", "collapsed": true}, "source": ["empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5375f461500b9994ec73ee54bbb50f9dfa566708", "_cell_guid": "d6370b56-3177-46af-a1f3-58c1d2c8eb6a"}, "source": ["## D. Data visualization"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "73b03e85ebe22e512ed840f44f9c87fbed17a2b7", "_cell_guid": "d5a55e88-28ab-4d0d-82a4-d912295cf3a3", "collapsed": true}, "source": ["# plot 3 the time series\n", "def plot_time_series(df, row_num, start_col =1, ax=None):\n", "    if ax is None:\n", "            fig = plt.figure(facecolor='w', figsize=(10, 6))\n", "            ax = fig.add_subplot(111)\n", "    else:\n", "        fig = ax.get_figure()\n", "        \n", "    series_title = df.iloc[row_num, 0]\n", "    sample_series = df.iloc[row_num, start_col:]\n", "    sample_series.plot(style=\".\", ax=ax)\n", "    ax.set_title(\"Series: %s\" % series_title)\n", "\n", "fig, axs  = plt.subplots(4,1,figsize=(12,12))\n", "plot_time_series(empty_sample, 1, ax=axs[0])\n", "plot_time_series(empty_sample, 10, ax=axs[1])\n", "plot_time_series(empty_sample, 100, ax=axs[2])\n", "plot_time_series(empty_sample, 1005, ax=axs[3])\n", "\n", "plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "b05fff7f45fcd175a98d9fbfc51c1129f92ab752", "_cell_guid": "731719c5-3a82-46cb-8e95-a72f342e97cb"}, "source": ["## E. Extreme data"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "53dc5fd82f094abb5775b8eab7ac320a502b5a8d", "_cell_guid": "9c2b896d-d160-4097-9ff3-0202896cd15f", "collapsed": true}, "source": ["# series with all NaN\n", "empty_sample.iloc[1000:1010]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "e8471f75073027f67fdf8bd9d2427d19451af98b", "_cell_guid": "6feda8d9-9b89-4c3a-ac5a-4c15ea2b1f8a"}, "source": ["# 2. Data transformation and helper functions\n", "## A. Article names and metadata"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "89747bd76d2f750f36e7335f2012262f5926edd0", "_cell_guid": "8682bf5e-f60f-45d3-8df3-88a0996f4031", "collapsed": true}, "source": ["import re\n", "\n", "def breakdown_topic(str):\n", "    m = re.search('(.*)\\_(.*).wikipedia.org\\_(.*)\\_(.*)', str)\n", "    if m is not None:\n", "        return m.group(1), m.group(2), m.group(3), m.group(4)\n", "    else:\n", "        return \"\", \"\", \"\", \"\"\n", "\n", "print(breakdown_topic(\"\u0420\u0443\u0434\u043e\u0432\u0430,_\u041d\u0430\u0442\u0430\u043b\u044c\u044f_\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u043e\u0432\u043d\u0430_ru.wikipedia.org_all-access_spider\"))\n", "print(breakdown_topic(\"\u53f0\u7063\u707d\u96e3\u5217\u8868_zh.wikipedia.org_all-access_spider\"))\n", "print(breakdown_topic(\"File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents\"))"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8558f96774bc5dcb78a31e936862c3933d254336", "_cell_guid": "da27ba91-e49b-4a0f-bdb5-3cb649b69f80", "collapsed": true}, "source": ["page_details = train.Page.str.extract(r'(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)')\n", "\n", "page_details[0:10]"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "540494ecf1658f7649e0a9043b64c6ba274643e5", "_cell_guid": "949a7a7c-010c-4792-af07-66016a4a6f86", "collapsed": true}, "source": ["unique_topic = page_details[\"topic\"].unique()\n", "print(unique_topic)\n", "print(\"Number of distinct topics: \", unique_topic.shape[0])"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "04fb2f9495a2b53e554af13c9b5cc30dd5d731e0", "_cell_guid": "93e6ec07-6165-410f-a753-a83780c8a5c5", "collapsed": true}, "source": ["fig, axs  = plt.subplots(3,1,figsize=(12,12))\n", "\n", "page_details[\"lang\"].value_counts().sort_index().plot.bar(ax=axs[0])\n", "axs[0].set_title('Language - distribution')\n", "\n", "page_details[\"access\"].value_counts().sort_index().plot.bar(ax=axs[1])\n", "axs[1].set_title('Access - distribution')\n", "\n", "page_details[\"type\"].value_counts().sort_index().plot.bar(ax=axs[2])\n", "axs[2].set_title('Type - distribution')\n", "\n", "plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "35d2c27cfe8db4b0881b403d57773156eccc69c2", "_cell_guid": "06e8a5ac-b792-4abc-978d-fef1c9c6d8f4"}, "source": ["## B. Split into train and validation dataset"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "aceb98a79ee58d03db63929da691a08fe8ccfe5e", "_cell_guid": "8b2a5205-893c-4a57-87d9-f7117915d46c", "collapsed": true}, "source": ["# Generate train and validate dataset\n", "train_df = pd.concat([page_details, train], axis=1)\n", "\n", "def get_train_validate_set(train_df, test_percent):\n", "    train_end = math.floor((train_df.shape[1]-5) * (1-test_percent))\n", "    train_ds = train_df.iloc[:, np.r_[0,1,2,3,4,5:train_end]]\n", "    test_ds = train_df.iloc[:, np.r_[0,1,2,3,4,train_end:train_df.shape[1]]]\n", "    \n", "    return train_ds, test_ds\n", "\n", "X_train, y_train = get_train_validate_set(train_df, 0.1)\n", "\n", "print(\"The training set sample:\")\n", "print(X_train[0:10])\n", "print(\"The validation set sample:\")\n", "print(y_train[0:10])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f11bf9c1d853839b7ee1f1d1403e70a57904ad65", "_cell_guid": "d1756260-6638-41c6-b6b8-09d7574d948a"}, "source": ["# 3 Forecast methods\n", "\n", "In this section, I will show some popular methods in predicting time series. (Will work on XGBoost if I finish my study)."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "005f5e7d99ebfc8fe6ebc11921d031974b4f03b7", "_cell_guid": "1dd91743-c36a-454e-9d48-520653ade0eb", "collapsed": true}, "source": ["def extract_series(df, row_num, start_idx):\n", "    y = df.iloc[row_num, start_idx:]\n", "    df = pd.DataFrame({ 'ds': y.index, 'y': y.values})\n", "    return df"]}, {"cell_type": "markdown", "metadata": {"_uuid": "e02a22b21b2ed36585d4c5c84ffde3b21e7e3fcd", "_cell_guid": "4287b753-9bfd-4a16-aa70-272e4bf2050e"}, "source": ["## A. SMAPE, the measurement\n", "\n", "SMAPE is harsh when the series is near zero.\n", "A notebook https://www.kaggle.com/cpmpml/smape-weirdness give a very good visualization of the SMAPE function.\n", "\n", "After you find that there is no way to further improve quality of the result, you may consider doing a little bit hacking on SMAPE to give you better score."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "1b9849d8bdc4e7d2877acfbc3e84d512a6f573ee", "_cell_guid": "39c51bf7-2449-4bbc-a787-5404a144dbe2", "collapsed": true}, "source": ["def smape(predict, actual, debug=False):\n", "    '''\n", "    predict and actual is a panda series.\n", "    In this implementation I will skip all the datapoint with actual is null\n", "    '''\n", "    actual = actual.fillna(0)\n", "    data = pd.concat([predict, actual], axis=1, keys=['predict', 'actual'])\n", "    data = data[data.actual.notnull()]\n", "    if debug:\n", "        print('debug', data)\n", "    \n", "    evals = abs(data.predict - data.actual) * 1.0 / (abs(data.predict) + abs(data.actual)) * 2\n", "    evals[evals.isnull()] = 0\n", "    #print(np.sum(evals), len(data), np.sum(evals) * 1.0 / len(data))\n", "    \n", "    result = np.sum(evals) / len(data)\n", "    \n", "    return result\n", "\n", "# create testing series\n", "testing_series_1 = X_train.iloc[0, 5:494]\n", "testing_series_2 = X_train.iloc[0, 5:494].shift(-1)\n", "testing_series_3 = X_train.iloc[1, 5:494]\n", "testing_series_4 = pd.Series([0,0,0,0])"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "26889456ad3d2f4f2566a9b5318565546974db77", "scrolled": true, "_cell_guid": "fc669ec6-3568-4c10-b17f-16c159663852", "collapsed": true}, "source": ["random_series_1 = pd.Series(np.repeat(3, 500))\n", "random_series_2 = pd.Series(np.random.normal(3, 1, 500))\n", "random_series_3 = pd.Series(np.random.normal(500, 20, 500))\n", "random_series_4 = pd.Series(np.repeat(500, 500))\n", "\n", "# testing 1 same series\n", "print(\"\\nSMAPE score to predict a constant array of 3\")\n", "print(\"Score (same series): %.3f\" % smape(random_series_1, random_series_1))\n", "print(\"Score (same series - 1) %.3f\" % smape(random_series_1, random_series_1-1))\n", "print(\"Score (same series + 1) %.3f\" % smape(random_series_1, random_series_1+1))\n", "\n", "# testing 2 same series shift by one\n", "print(\"\\nSMAPE score to predict a array of normal distribution around 3\")\n", "print(\"Score (random vs mean) %.3f\" % smape(random_series_2, random_series_1))\n", "print(\"Score (random vs mean-1) %.3f\" % smape(random_series_2, random_series_2-1))\n", "print(\"Score (random vs mean+1) %.3f\" % smape(random_series_2, random_series_2+1))\n", "print(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_2, random_series_2*0.9))\n", "print(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_2, random_series_2*1.1))\n", "\n", "# testing 3 totally different series\n", "print(\"\\nSMAPE score to predict a array of normal distribution around 500\")\n", "print(\"Score (random vs mean) %.3f\" % smape(random_series_3, random_series_4))\n", "print(\"Score (random vs mean-20) %.3f\" % smape(random_series_3, random_series_3-20))\n", "print(\"Score (random vs mean+20) %.3f\" % smape(random_series_3, random_series_3+20))\n", "print(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_3, random_series_3*0.9))\n", "print(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_3, random_series_3*1.1))\n"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "07b01ca5365bd2537a9cc83827e5b1b974c5fefb", "_cell_guid": "74171c24-cc33-4229-bbe7-9af4fa486a1e", "collapsed": true}, "source": ["y_true_1 = pd.Series(np.random.normal(1, 1, 500))\n", "y_true_2 = pd.Series(np.random.normal(2, 1, 500))\n", "y_true_3 = pd.Series(np.random.normal(3, 1, 500))\n", "y_pred = pd.Series(np.ones(500))\n", "x = np.linspace(0,10,1000)\n", "res_1 = list([smape(y_true_1, i * y_pred) for i in x])\n", "res_2 = list([smape(y_true_2, i * y_pred) for i in x])\n", "res_3 = list([smape(y_true_3, i * y_pred) for i in x])\n", "plt.plot(x, res_1, color='b')\n", "plt.plot(x, res_2, color='r')\n", "plt.plot(x, res_3, color='g')\n", "plt.axvline(x=1, color='k')\n", "plt.axvline(x=2, color='k')\n", "plt.axvline(x=3, color='k')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2e92732c900a79072586caeb9a275319f879dc77", "_cell_guid": "d803e04a-a1f1-486c-a437-7ef813fa95c4"}, "source": ["## B. Simple median model"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f9df34726d19532211e00897c5a342902a5013e1", "_cell_guid": "c345d9f2-5bb1-40ca-b3f8-d97c57ad1ba2", "collapsed": true}, "source": ["def plot_prediction_and_actual_2(train, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n", "    fig, ax  = plt.subplots(1,1,figsize=figSize)\n", "    ax.plot(pd.to_datetime(train.index), train.values, 'k.')\n", "    ax.plot(pd.to_datetime(actual.index), actual.values, 'r.')\n", "    ax.plot(pd.to_datetime(forecast.index), forecast.values, 'b-')\n", "    ax.set_title(title)\n", "    plt.show()"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8b8f71ccfaa3830f44e3f99d0c1e526ad4b65e3c", "_cell_guid": "f4d0da8a-7064-4401-86aa-bdaa20160eb1", "collapsed": true}, "source": ["def median_model(df_train, df_actual, p, review=False, figSize=(12, 4)):\n", "    \n", "    def nanmedian_zero(a):\n", "        return np.nan_to_num(np.nanmedian(a))\n", "    \n", "    df_train['y'] = df_train['y'].convert_objects(convert_numeric=True)\n", "    df_actual['y'] = df_actual['y'].convert_objects(convert_numeric=True)\n", "    visits = nanmedian_zero(df_train['y'].values[-p:])\n", "    train_series = df_train['y']\n", "    train_series.index = df_train.ds\n", "    \n", "    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n", "    b = [row[row>=0] for row in df_train.y.values[idx]]\n", "    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n", "    pre_forecast.index = df_train.ds\n", "    \n", "    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n", "    forecast_series.index = df_actual.ds\n", "    \n", "    forecast_series = pre_forecast.append(forecast_series)\n", "    \n", "    actual_series = df_actual.y\n", "    actual_series.index = df_actual.ds\n", "    \n", "    if(review):\n", "        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model')\n", "    \n", "    return smape(forecast_series, actual_series)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "38904abe75b4ccce7f9a481841bc7ef67703aea3", "_cell_guid": "e0b6ed0f-c108-48b6-9a91-c1dcec6e5ec5", "collapsed": true}, "source": ["# This is to demo the median model\n", "print(train.iloc[[2]])\n", "\n", "df_train = extract_series(X_train, 2, 5)\n", "df_actual = extract_series(y_train, 2, 5)\n", "lang = X_train.iloc[2, 1]\n", "score = median_model(df_train.copy(), df_actual.copy(), 15, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7894159ebfcaced527c078a8e209530625f4df40", "_cell_guid": "0f97bda6-75b5-41c2-a21e-acd7eaa7abe7"}, "source": ["## C. Median model - weekday, weekend and holiday"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7b9cb56a8fee5548340ce23db7f7446f3e83c3ed", "_cell_guid": "d8710782-db8d-46bc-9442-796cec9550d7", "collapsed": true}, "source": ["# holiday variable\n", "#holiday_en = ['2015-01-01', '2015-01-19', '2015-04-03', '2015-05-04', '2015-05-25', '2015-07-01', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-01-18', '2016-03-25', '2016-05-02', '2016-05-30', '2016-07-01', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-16', '2017-04-14', '2017-05-01', '2017-05-29', '2017-07-01', '2017-07-03', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25', '2017-12-26']\n", "\n", "holiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25']\n", "holiday_en_uk = ['2015-01-01', '2015-04-03', '2015-05-04', '2015-05-25', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-03-25', '2016-05-02', '2016-05-30', '2016-12-26', '2016-12-27', '2017-01-01', '2017-04-14', '2017-05-01', '2017-05-29', '2017-12-25', '2017-12-26']\n", "holiday_en_canada = ['2015-01-01', '2015-07-01', '2015-09-07', '2015-12-25', '2016-01-01', '2016-07-01', '2016-09-05', '2016-12-25', '2017-01-01', '2017-07-01', '2017-07-03', '2017-09-04', '2017-12-25']\n", "\n", "holiday_ru_russia = ['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08', '2015-01-09', '2015-02-23', '2015-03-09', '2015-05-01', '2015-05-04', '2015-05-09', '2015-05-11', '2015-06-12', '2015-11-04', '2016-01-01', '2016-01-04', '2016-01-05', '2016-01-06', '2016-01-07', '2016-02-22', '2016-02-23', '2016-03-08', '2016-05-01', '2016-05-09', '2016-06-12', '2016-06-13', '2016-11-04', '2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-02-23', '2017-02-24', '2017-03-08', '2017-05-01', '2017-05-08', '2017-05-09', '2017-06-12', '2017-11-04', '2017-11-06']\n", "#holiday_es = ['2015-01-01', '2015-01-06', '2015-01-12', '2015-02-02', '2015-03-16', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-09-16', '2015-10-12', '2015-11-01', '2015-11-02', '2015-11-16', '2015-12-06', '2015-12-08', '2015-12-12', '2015-12-25', '2016-01-01', '2016-01-06', '2016-01-11', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-09-16', '2016-10-12', '2016-10-17', '2016-11-01', '2016-11-02', '2016-11-07', '2016-11-14', '2016-11-21', '2016-12-06', '2016-12-08', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-06', '2017-01-09', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-09-16', '2017-10-12', '2017-10-16', '2017-11-01', '2017-11-02', '2017-11-06', '2017-11-13', '2017-11-20', '2017-12-06', '2017-12-08', '2017-12-12', '2017-12-25']\n", "\n", "holiday_es_mexico = ['2015-01-01', '2015-02-02', '2015-03-16', '2015-04-02', '2015-04-03', '2015-05-01', '2015-09-16', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-12', '2015-12-25', '2016-01-01', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-09-16', '2016-10-12', '2016-11-02', '2016-11-21', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-09-16', '2017-10-12', '2017-11-02', '2017-11-20', '2017-12-12', '2017-12-25']\n", "holiday_es_spain = ['2017-01-01', '2017-01-06', '2017-04-14', '2017-05-01', '2017-08-15', '2017-10-12', '2017-11-01', '2017-12-06', '2017-12-08', '2017-12-25', '2016-01-01', '2016-01-06', '2016-03-25', '2016-05-01', '2016-08-15', '2016-10-12', '2016-11-01', '2016-12-06', '2016-12-08', '2016-12-25', '2015-01-01', '2015-01-06', '2015-04-03', '2015-05-01', '2015-10-12', '2015-11-01', '2015-12-06', '2015-12-08', '2015-12-25']\n", "holiday_es_colombia = ['2015-01-01', '2015-01-12', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-08', '2015-12-25', '2016-01-01', '2016-01-11', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-10-17', '2016-11-07', '2016-11-14', '2016-12-08', '2016-12-25', '2017-01-01', '2017-01-09', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-10-16', '2017-11-06', '2017-11-13', '2017-12-08', '2017-12-25']\n", "\n", "holiday_fr_france = ['2015-01-01', '2015-04-06', '2015-05-01', '2015-05-08', '2015-05-14', '2015-05-25', '2015-07-14', '2015-08-15', '2015-11-01', '2015-11-11', '2015-12-25', '2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-08', '2016-05-16', '2016-07-14', '2016-08-15', '2016-11-01', '2016-11-11', '2016-12-25', '2017-01-01', '2017-04-17', '2017-05-01', '2017-05-08', '2017-05-25', '2017-06-05', '2017-07-14', '2017-08-15', '2017-11-01', '2017-11-11', '2017-12-25']\n", "holiday_jp_japan = ['2015-01-01', '2015-01-12', '2015-02-11', '2015-03-21', '2015-04-29', '2015-05-03', '2015-05-04', '2015-05-05', '2015-05-06', '2015-07-20', '2015-09-21', '2015-09-22', '2015-09-23', '2015-10-12', '2015-11-03', '2015-11-23', '2015-12-23', '2016-01-01', '2016-01-11', '2016-02-11', '2016-03-21', '2016-04-29', '2016-05-03', '2016-05-04', '2016-05-05', '2016-07-18', '2016-08-11', '2016-09-19', '2016-09-22', '2016-10-10', '2016-11-03', '2016-11-23', '2016-12-23', '2017-01-01', '2017-01-09', '2017-02-11', '2017-03-20', '2017-04-29', '2017-05-03', '2017-05-04', '2017-05-05', '2017-07-17', '2017-08-11', '2017-09-18', '2017-09-22', '2017-10-09', '2017-11-03', '2017-11-23', '2017-12-23']\n", "\n", "#holiday_de = ['2015-01-01', '2015-01-06', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-01', '2015-08-15', '2015-10-03', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-01', '2016-08-15', '2016-10-03', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-01', '2017-08-15', '2017-10-03', '2017-10-26', '2017-10-31', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n", "\n", "holiday_de_germany = ['2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-14', '2015-05-25', '2015-10-03', '2015-12-25', '2015-12-26', '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-10-03', '2016-12-25', '2016-12-26', '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26']\n", "holiday_de_austria = ['2015-01-01', '2015-01-06', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-15', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-15', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-15', '2017-10-26', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n", "holiday_de_switzerland = ['2015-01-01', '2015-04-03', '2015-05-14', '2015-08-01', '2015-12-25', '2016-01-01', '2016-03-25', '2016-05-05', '2016-08-01', '2016-12-25', '2017-01-01', '2017-04-14', '2017-05-25', '2017-08-01', '2017-12-25']\n", "\n", "#holiday_zh = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-19', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-09', '2015-10-10', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-04-05', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-06-10', '2016-07-01', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-29', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-04', '2017-10-05', '2017-10-09', '2017-10-10', '2017-10-28', '2017-12-25', '2017-12-26']\n", "\n", "holiday_zh_hongkong = ['2015-01-01', '2015-02-19', '2015-02-20', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-08', '2016-02-09', '2016-02-10', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-07-01', '2016-09-16', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-28', '2017-01-30', '2017-01-31', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-05', '2017-10-28', '2017-12-25', '2017-12-26']\n", "holiday_zh_taiwan = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-05', '2015-04-06', '2015-06-19', '2015-06-20', '2015-09-28', '2015-10-09', '2015-10-10', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-04-04', '2016-04-05', '2016-06-09', '2016-06-10', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-10', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-05-01', '2017-05-29', '2017-05-30', '2017-10-04', '2017-10-09', '2017-10-10']\n", "\n", "holidays_en_us = pd.DataFrame({\n", "  'holiday': 'US public holiday',\n", "  'ds': pd.to_datetime(holiday_en_us),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_en_uk = pd.DataFrame({\n", "  'holiday': 'UK public holiday',\n", "  'ds': pd.to_datetime(holiday_en_uk),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_en_canada = pd.DataFrame({\n", "  'holiday': 'Canada public holiday',\n", "  'ds': pd.to_datetime(holiday_en_canada),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_en = pd.concat((holidays_en_us, holidays_en_uk, holidays_en_canada))\n", "\n", "holidays_ru_russia = pd.DataFrame({\n", "  'holiday': 'Russia public holiday',\n", "  'ds': pd.to_datetime(holiday_ru_russia),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_ru = holidays_ru_russia\n", "\n", "holidays_es_mexico = pd.DataFrame({\n", "  'holiday': 'Mexico public holiday',\n", "  'ds': pd.to_datetime(holiday_es_mexico),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_es_spain = pd.DataFrame({\n", "  'holiday': 'Spain public holiday',\n", "  'ds': pd.to_datetime(holiday_es_spain),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_es_colombia = pd.DataFrame({\n", "  'holiday': 'Colombia public holiday',\n", "  'ds': pd.to_datetime(holiday_es_colombia),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_es = pd.concat((holidays_es_mexico, holidays_es_spain, holidays_es_colombia))\n", "\n", "holidays_fr_france = pd.DataFrame({\n", "  'holiday': 'France public holiday',\n", "  'ds': pd.to_datetime(holiday_fr_france),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_fr = holidays_fr_france\n", "\n", "holidays_jp_japan = pd.DataFrame({\n", "  'holiday': 'Japan public holiday',\n", "  'ds': pd.to_datetime(holiday_jp_japan),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_jp = holidays_jp_japan\n", "\n", "holidays_de_germany = pd.DataFrame({\n", "  'holiday': 'Germany public holiday',\n", "  'ds': pd.to_datetime(holiday_de_germany),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_de_austria = pd.DataFrame({\n", "  'holiday': 'Austria public holiday',\n", "  'ds': pd.to_datetime(holiday_de_austria),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_de_switzerland = pd.DataFrame({\n", "  'holiday': 'Switzerland public holiday',\n", "  'ds': pd.to_datetime(holiday_de_switzerland),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_de = pd.concat((holidays_de_germany, holidays_de_austria, holidays_de_switzerland))\n", "\n", "holidays_zh_hongkong = pd.DataFrame({\n", "  'holiday': 'HK public holiday',\n", "  'ds': pd.to_datetime(holiday_zh_hongkong),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_zh_taiwan = pd.DataFrame({\n", "  'holiday': 'Taiwan public holiday',\n", "  'ds': pd.to_datetime(holiday_zh_taiwan),\n", "  'lower_window': 0,\n", "  'upper_window': 0,\n", "})\n", "\n", "holidays_zh = pd.concat((holidays_zh_hongkong, holidays_zh_taiwan))\n", "\n", "holidays_dict = {\"en\": holidays_en, \n", "                 \"ru\": holidays_ru, \n", "                 \"es\": holidays_es, \n", "                 \"fr\": holidays_fr, \n", "                 \"ja\": holidays_jp,\n", "                 \"de\": holidays_de,\n", "                 \"zh\": holidays_zh}"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ccb89f2b0cc92eedcd3b6d714c97ae4212bf5393", "_cell_guid": "08c4b047-d6ae-4a08-98a6-ce55e85c7f39", "collapsed": true}, "source": ["def median_holiday_model(df_train, df_actual, p, lang, review=False, figSize=(12, 4)):\n", "    # Split the train and actual set\n", "    \n", "        \n", "    df_train['ds'] = pd.to_datetime(df_train['ds'])\n", "    df_actual['ds'] = pd.to_datetime(df_actual['ds'])\n", "    train_series = df_train['y']\n", "    train_series.index = df_train.ds\n", "    \n", "    if(isinstance(lang, float) and math.isnan(lang)):\n", "        df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n", "        df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\n", "    else:\n", "        df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n", "        df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)\n", "     \n", "    # Combine the train and actual set\n", "    predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\n", "    predict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)\n", "\n", "    forecast_series = predict_non_holiday.combine_first(predict_holiday)\n", "    \n", "    actual_series = df_actual.y\n", "    actual_series.index = df_actual.ds\n", "    \n", "    if(review):\n", "        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model with holiday')\n", "    \n", "    return smape(forecast_series, actual_series)\n", "\n", "\n", "def median_holiday_helper(df_train, df_actual, p, holiday):\n", "    def nanmedian_zero(a):\n", "        return np.nan_to_num(np.nanmedian(a))\n", "    \n", "    df_train['y'] = pd.to_numeric(df_train['y'])\n", "    df_actual['y'] = pd.to_numeric(df_actual['y'])\n", "    \n", "    sample = df_train[-p:]\n", "    if(holiday):\n", "        sample = sample[sample['holiday']]\n", "    else:\n", "        sample = sample[~sample['holiday']]\n", "\n", "    visits = nanmedian_zero(sample['y'])\n", "    \n", "    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n", "    b = [row[row>=0] for row in df_train.y.values[idx]]\n", "    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n", "    pre_forecast.index = df_train.ds\n", "    \n", "    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n", "    forecast_series.index = df_actual.ds\n", "    \n", "    forecast_series = pre_forecast.append(forecast_series)\n", "    \n", "    return forecast_series"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "79170ffdb313705aca01d50f5ab4b150809aeecf", "_cell_guid": "51348547-d50e-4bb9-9b80-d437280a324a", "collapsed": true}, "source": ["# This is to demo the median model - weekday, weekend and \n", "print(train.iloc[[2]])\n", "\n", "df_train = extract_series(X_train, 2, 5)\n", "df_actual = extract_series(y_train, 2, 5)\n", "lang = X_train.iloc[2, 1]\n", "score = median_holiday_model(df_train.copy(), df_actual.copy(), 15, lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "fffe6fb76a7606a0bdc4c7a6f38917dab2bb9cc4", "_cell_guid": "8d3efd7d-ef3d-4e7e-9b93-e4da2cbc1c78"}, "source": ["## D. ARIMA model\n", "\n", "The below use the ARIMA from a Python library statsmodels. Please refer to http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html for details.\n", "\n", "The model is slow and may throw exception if the model cannot find a solution. (Make the model difficult to build for all series).\n", "\n", "We will further investigate it performance in the later section."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f03c747e4520763f8e4d7eaf3cfca801ba3b1afc", "_cell_guid": "ca14c193-4782-45bc-b438-d9d515cef043", "collapsed": true}, "source": ["from statsmodels.tsa.arima_model import ARIMA   \n", "import warnings\n", "\n", "def arima_model(df_train, df_actual, p, d, q, figSize=(12, 4), review=False):\n", "    df_train = df_train.fillna(0)\n", "    train_series = df_train.y\n", "    train_series.index = df_train.ds\n", "\n", "    result = None\n", "    with warnings.catch_warnings():\n", "        warnings.filterwarnings('ignore')\n", "        try:\n", "            arima = ARIMA(train_series ,[p, d, q])\n", "            result = arima.fit(disp=False)\n", "        except Exception as e:\n", "            print('\\tARIMA failed', e)\n", "                \n", "    #print(result.params)\n", "    start_idx = df_train.ds[d]\n", "    end_idx = df_actual.ds.max()\n", "    forecast_series = result.predict(start_idx, end_idx,typ='levels')\n", "    \n", "    actual_series = df_actual.y\n", "    actual_series.index = pd.to_datetime(df_actual.ds)\n", "\n", "    if(review):\n", "        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='ARIMA model')\n", "    \n", "    return smape(forecast_series, actual_series)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e16d0523074cfa0c3b87447f24ec9e238226b9b8", "_cell_guid": "a42595cf-46d3-477e-9bb7-f0c44492dd21", "collapsed": true}, "source": ["# This is to demo the ARIMA model\n", "print(train.iloc[[2]])\n", "\n", "df_train = extract_series(X_train, 2, 5)\n", "df_actual = extract_series(y_train, 2, 5)\n", "lang = X_train.iloc[2, 1]\n", "score = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3905533849e82db4352abbef4d247d98df3ae258", "_cell_guid": "eb67304c-ea85-49f6-9845-c5173ecd9766"}, "source": ["## E. Facebook prophet library\n", "\n", "Facebook prophet library is created by facebook and aims to create a human-friendly time series forecasting libary. For details, please refer to https://facebookincubator.github.io/prophet/\n", "\n", "There are several favor, but I will focus on holiday, yearly and log model"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "9f43ea61b480446da7cc51d73bf8afafacb54938", "_cell_guid": "f16f77ec-5cd6-46aa-934d-31295451bf9f", "collapsed": true}, "source": ["def plot_prediction_and_actual(model, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n", "    fig, ax  = plt.subplots(1,1,figsize=figSize)\n", "    ax.set_ylim(ylim)\n", "    ax.plot(pd.to_datetime(actual.ds), actual.y, 'r.')\n", "    model.plot(forecast, ax=ax);\n", "    ax.set_title(title)\n", "    plt.show()"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "5a5590c9f5bfb769a5609ccd2c4149af8f617e08", "_cell_guid": "36860209-bf20-46fd-9464-0839b80d2cb9", "collapsed": true}, "source": ["# simple linear model\n", "def normal_model(df_train, df_actual, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "    \n", "    m = Prophet()\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "        \n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n", "        #\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model')\n", "    \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = forecast[mask].yhat\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "\n", "    return smape(forecast_series, actual_series)\n", "\n", "def holiday_model(df_train, df_actual, lang, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "\n", "    if(isinstance(lang, float) and math.isnan(lang)):\n", "        holidays = None\n", "    else:\n", "        holidays = holidays_dict[lang]\n", "\n", "    m = Prophet(holidays=holidays)\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "        \n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model')\n", "    \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = forecast[mask].yhat\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "\n", "    return smape(forecast_series, actual_series)\n", "\n", "def yearly_model(df_train, df_actual, lang, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "\n", "    if(isinstance(lang, float) and math.isnan(lang)):\n", "        holidays = None\n", "    else:\n", "        holidays = holidays_dict[lang]\n", "\n", "    m = Prophet(holidays=holidays, yearly_seasonality=True)\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "        \n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yealry model')\n", "    \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = forecast[mask].yhat\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "\n", "    return smape(forecast_series, actual_series)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c9124dba2e2e0a5f31ba0beda2e8a95c73ecedc1", "_cell_guid": "ac907ab4-6951-47b2-a881-9ad0bb641c90", "collapsed": true}, "source": ["# log model\n", "def normal_model_log(df_train, df_actual, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    df_train.y = np.log1p(df_train.y)\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "    df_actual.y = np.log1p(df_actual.y)\n", "    \n", "    m = Prophet()\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "    \n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model in log')\n", "        \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = np.expm1(forecast[mask].yhat)\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "\n", "    return smape(forecast_series, actual_series)\n", "\n", "def holiday_model_log(df_train, df_actual, lang, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    df_train.y = np.log1p(df_train.y)\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "    df_actual.y = np.log1p(df_actual.y)\n", "\n", "    if(isinstance(lang, float) and math.isnan(lang)):\n", "        holidays = None\n", "    else:\n", "        holidays = holidays_dict[lang]\n", "    m = Prophet(holidays=holidays)\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "    \n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model in log')\n", "        \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = np.expm1(forecast[mask].yhat)\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "    \n", "    return smape(forecast_series, actual_series)\n", "\n", "def yearly_model_log(df_train, df_actual, lang, review=False):\n", "    start_date = df_actual.ds.min()\n", "    end_date = df_actual.ds.max()\n", "    \n", "    actual_series = df_actual.y.copy()\n", "    actual_series.index = df_actual.ds\n", "\n", "    df_train['y'] = df_train['y'].astype('float')\n", "    df_train.y = np.log1p(df_train.y)\n", "    \n", "    df_actual['y'] = df_actual['y'].astype('float')\n", "    df_actual.y = np.log1p(df_actual.y)\n", "\n", "    if(isinstance(lang, float) and math.isnan(lang)):\n", "        holidays = None\n", "    else:\n", "        holidays = holidays_dict[lang]\n", "        \n", "    m = Prophet(holidays=holidays, yearly_seasonality=True)\n", "    m.fit(df_train)\n", "    future = m.make_future_dataframe(periods=60)\n", "    forecast = m.predict(future)\n", "\n", "    if(review):\n", "        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n", "        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n", "        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yearly model in log')\n", "        \n", "    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n", "    forecast_series = np.expm1(forecast[mask].yhat)\n", "    forecast_series.index = forecast[mask].ds\n", "    forecast_series[forecast_series < 0] = 0\n", "    \n", "    return smape(forecast_series, actual_series)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "2be1e7acedf5b938ac62af9bd0cc3445deaedecd", "_cell_guid": "5ea8c040-f6a1-4aec-92c9-4c659908961a", "collapsed": true}, "source": ["# This is to demo the facebook prophet model\n", "print(train.iloc[[2]])\n", "\n", "df_train = extract_series(X_train, 2, 5)\n", "df_actual = extract_series(y_train, 2, 5)\n", "lang = X_train.iloc[2, 1]\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4f42b04b568d3faab544815b2960881f491bb839", "_cell_guid": "1039edbc-fc66-4b3a-9d5c-8f58b88066e4"}, "source": ["## F. Sample series analysis (For script reconciliation)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "26fd25bd241293c5a70078dc8a49101ceeddf177", "_cell_guid": "d24966ad-e24b-4f15-8289-6c31acf7303f"}, "source": ["### Case 1: SMAPE evaluation near zero and SMAPE score is too big"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "24d413fecd75c366a1afb56362fbd4f61fd27743", "_cell_guid": "93edcdf8-f679-44fd-a7ca-f30a4e06e6af", "collapsed": true}, "source": ["import warnings\n", "warnings.filterwarnings('ignore')"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8950b58c273e8d1f304d3874c19c8d970eae24e1", "_cell_guid": "266500b3-cee0-4c6d-add1-d856e3d65cc2", "collapsed": true}, "source": ["print(train.iloc[[2]])\n", "\n", "df_train = extract_series(X_train, 2, 5)\n", "df_actual = extract_series(y_train, 2, 5)\n", "lang = X_train.iloc[2, 1]\n", "score = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d5c1ef4155f38326acd3f3c91110c39d7d623c8a", "_cell_guid": "9a9f2476-5884-469a-8edf-8587f258b61b"}, "source": ["### Case 2: Yearly model is the best model"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "20d94726c954632bdcc15cfa16a5334085174112", "scrolled": false, "_cell_guid": "bace57c0-6635-4d3a-8c08-37905ded472c", "collapsed": true}, "source": ["print(train.iloc[[4464]])\n", "\n", "df_train = extract_series(X_train, 4464, 5)\n", "df_actual = extract_series(y_train, 4464, 5)\n", "lang = X_train.iloc[4464, 1]\n", "\n", "score = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3c53ee45f04e8353b58773cc9daad8568d9e72e9", "_cell_guid": "7bcd1ddf-179f-4f02-a5dc-e2508f2f0931"}, "source": ["### Case 3: Non-yearly model is better"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ed258f644bc4369b67134298857ed37eee7c3ff5", "_cell_guid": "96cfa1a0-82dd-4540-959e-1bf8cee97399", "collapsed": true}, "source": ["train.iloc[[6245]]\n", "\n", "df_train = extract_series(X_train, 6245, 5)\n", "df_actual = extract_series(y_train, 6245, 5)\n", "lang = X_train.iloc[6245, 1]\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "eed2138dc51c80efc7d0ccc4484ad5c4d6016177", "_cell_guid": "9014a21d-bf34-45df-ab60-b702dff43d1d"}, "source": ["### Case 4: SMAPE score is too high for all proposed models"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "992e8486db836f77989723cf3204181a0703253e", "_cell_guid": "6bf9e927-a141-408d-8ab7-927d74fe9d19", "collapsed": true}, "source": ["train.iloc[[80002]]\n", "\n", "df_train = extract_series(X_train, 80002, 5)\n", "df_actual = extract_series(y_train, 80002, 5)\n", "lang = X_train.iloc[80002, 1]\n", "title = X_train.iloc[80002, 4]\n", "print(title)\n", "\n", "score = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "# Please use this case to check your implementation of SMAPE\n", "score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "45f88848f6c62854cc7a249644f42ff5ab4eced9", "_cell_guid": "bcedcdbc-dc97-4d4e-a46c-678aef805e77"}, "source": ["### Case 5: SMAPE score is too high for all proposed models"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b401c383500e117689177a81c7190e00fd5d4f70", "_cell_guid": "740c9c66-db23-477e-a859-32b9f13895f5", "collapsed": true}, "source": ["train.iloc[[80009]]\n", "\n", "df_train = extract_series(X_train, 80009, 5)\n", "df_actual = extract_series(y_train, 80009, 5)\n", "lang = X_train.iloc[80009, 1]\n", "title = X_train.iloc[80009, 4]\n", "print(title)\n", "\n", "score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang=lang)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "22f16608fb3bb04d2677f1c5fe10d22e8de2d0f1", "_cell_guid": "b79fae38-900b-4568-a5c1-c4e09c8e656e"}, "source": ["### Case 6: SMAPE score is too high for all proposed models"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d2a2835f3739b3cd3f0b2d642b53d4f98503d558", "scrolled": false, "_cell_guid": "e926c977-a13f-44a7-b947-02cbd0ce45a7", "collapsed": true}, "source": ["train.iloc[[14211]]\n", "\n", "df_train = extract_series(X_train, 14211, 5)\n", "df_actual = extract_series(y_train, 14211, 5)\n", "lang = X_train.iloc[14211, 1]\n", "title = X_train.iloc[14211, 4]\n", "print(title)\n", "score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "# if there is too many zero, just use normal is OK.\n", "score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)\n", "\n", "score = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\n", "print(\"The SMAPE score is : %.5f\" % score)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "bae92d3c1d4e641f160181fa697fc0db611d7a86", "_cell_guid": "6ef8d842-6b5c-4fc7-8a49-a6b6b72b58c0"}, "source": ["### Case ?: Adhoc study"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "cb1a78f470e4135f3e6e2093cfd4dee3a3043edb", "scrolled": false, "_cell_guid": "e7244a09-d28b-47bd-94f8-05c5091bb8d3", "collapsed": true}, "source": ["series_num = 145033\n", "series_num = 145057\n", "\n", "print(train.iloc[[series_num]])\n", "\n", "df_train = extract_series(X_train, series_num, 5)\n", "df_actual = extract_series(y_train, series_num, 5)\n", "\n", "lang = X_train.iloc[series_num, 1]\n", "title = X_train.iloc[series_num, 4]\n", "print(title)\n", "\n", "try:\n", "    score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n", "    print(\"The SMAPE score is : %.5f\" % score)\n", "except Exception as e:\n", "    print(\"Error in calculating median model\", e)\n", "\n", "try:\n", "    score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\n", "    print(\"The SMAPE score is : %.5f\" % score)\n", "except Exception as e:\n", "    print(\"Error in calculating holiday model\", e)\n", "    \n", "try:\n", "    score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "    print(\"The SMAPE score is : %.5f\" % score)\n", "except Exception as e:\n", "    print(\"Error in calculating holiday model in log\", e)\n", "    \n", "try:\n", "    score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n", "    print(\"The SMAPE score is : %.5f\" % score)\n", "except Exception as e:\n", "    print(\"Error in calculating yearly model in log\", e)\n", "\n", "try:\n", "    score = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\n", "    print(\"The SMAPE score is : %.5f\" % score)\n", "except Exception as e:\n", "    print(\"Error in calculating arima model\", e)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "686deb71c24c0fe70ce8a5c85ea33fa0fc2fc8c3", "_cell_guid": "1fb4f35b-706e-4065-b0b9-292195f1cc1f", "collapsed": true}, "source": ["warnings.resetwarnings()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "6f5596a8ad3d79e261df1d80174b238f07c8dfc8", "_cell_guid": "eddfe1ff-8074-4beb-940b-c7d048d515a6"}, "source": ["## 4. Selected model performance (validation score) over train dataset\n", "\n", "In this session, we wil train the model and do prediction over 145000+ series in dataset. \n", "To find out the validation score for comparison"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f1f20720611d8b754be84626695aa47840aaf5a1", "_cell_guid": "0f8be8a7-6d92-4817-bc3e-85f6dfd21c8f", "collapsed": true}, "source": ["import glob\n", "\n", "def read_from_folder(path):\n", "    filenames = glob.glob(path + \"/*.csv\")\n", "\n", "    dfs = []\n", "    for filename in filenames:\n", "        dfs.append(pd.read_csv(filename, index_col=0))\n", "    \n", "    frame = pd.concat(dfs)\n", "    return frame.sort_index()"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b9171b0b831ddb461f8d50f9d3179180b3171a81", "_cell_guid": "f7d00919-d6cf-403f-bbf9-c983c18b9743", "collapsed": true}, "source": ["# TODO: overall validation score in one number.\n", "def validation_score(score_series):\n", "    return score_series.mean()"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "6df92f7616e7e7936e95beb680f283b743b8479f", "_cell_guid": "0805700f-ba13-46b8-993e-aad1cc23ad65", "collapsed": true}, "source": ["valid_fn = r\"../input/wiktraffictimeseriesforecast/validation_score.csv\"\n", "valid_score_data = pd.read_csv(valid_fn, index_col=0)\n", "\n", "print(valid_score_data[0:10])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a9dff8bf2917d7cef03964e1a1c47563db002257", "_cell_guid": "947ddf57-d10a-44f8-b9ab-e7470707166d"}, "source": ["### A. Simple median model\n", "\n", "We will train up the median model using popular choice 7 to 49, with step 7, and compare the overall score."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "27a1a57f047143412b2000a239ee9f56e1f8c8e4", "_cell_guid": "5be881c9-e59e-42eb-89a4-82c0777c82b0", "collapsed": true}, "source": ["# Check which model is the best\n", "print(\"Validation score for median model (7 days) is: %.6f\" % validation_score(valid_score_data['median7']))\n", "print(\"Validation score for median model (14 days) is: %.6f\" % validation_score(valid_score_data['median14']))\n", "print(\"Validation score for median model (21 days) is: %.6f\" % validation_score(valid_score_data['median21']))\n", "print(\"Validation score for median model (28 days) is: %.6f\" % validation_score(valid_score_data['median28']))\n", "print(\"Validation score for median model (35 days) is: %.6f\" % validation_score(valid_score_data['median35']))\n", "print(\"Validation score for median model (42 days) is: %.6f\" % validation_score(valid_score_data['median42']))\n", "print(\"Validation score for median model (49 days) is: %.6f\" % validation_score(valid_score_data['median49']))\n", "\n", "fig, axs  = plt.subplots(4,2,figsize=(12,12))\n", "valid_score_data['median7'].plot.hist(bins=40, ax=axs[0][0])\n", "valid_score_data['median14'].plot.hist(bins=40, ax=axs[0][1])\n", "valid_score_data['median21'].plot.hist(bins=40, ax=axs[1][0])\n", "valid_score_data['median28'].plot.hist(bins=40, ax=axs[1][1])\n", "valid_score_data['median35'].plot.hist(bins=40, ax=axs[2][0])\n", "valid_score_data['median42'].plot.hist(bins=40, ax=axs[2][1])\n", "valid_score_data['median49'].plot.hist(bins=40, ax=axs[3][0])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "81a4eb1a4c237aacf2c06b8d5c19646ee2a773cc", "_cell_guid": "a03e2e3f-8ddb-4eae-af05-fc6b011c1594"}, "source": ["### B. Median model - weekday, weekend, holiday"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c1c934998daa244f426f747f9a5362440b06d113", "_cell_guid": "8d9ea6cb-332f-43dc-b9df-e0b5081ed0ed", "collapsed": true}, "source": ["print(\"Validation score for median model w/holiday (7 days) is: %.6f\" % validation_score(valid_score_data['median7_h']))\n", "print(\"Validation score for median model w/holiday (14 days) is: %.6f\" % validation_score(valid_score_data['median14_h']))\n", "print(\"Validation score for median model w/holiday (21 days) is: %.6f\" % validation_score(valid_score_data['median21_h']))\n", "print(\"Validation score for median model w/holiday (28 days) is: %.6f\" % validation_score(valid_score_data['median28_h']))\n", "print(\"Validation score for median model w/holiday (35 days) is: %.6f\" % validation_score(valid_score_data['median35_h']))\n", "print(\"Validation score for median model w/holiday (42 days) is: %.6f\" % validation_score(valid_score_data['median42_h']))\n", "print(\"Validation score for median model w/holiday (49 days) is: %.6f\" % validation_score(valid_score_data['median49_h']))\n", "\n", "fig, axs  = plt.subplots(4,2,figsize=(12,12))\n", "valid_score_data['median7_h'].plot.hist(bins=40, ax=axs[0][0])\n", "valid_score_data['median14_h'].plot.hist(bins=40, ax=axs[0][1])\n", "valid_score_data['median21_h'].plot.hist(bins=40, ax=axs[1][0])\n", "valid_score_data['median28_h'].plot.hist(bins=40, ax=axs[1][1])\n", "valid_score_data['median35_h'].plot.hist(bins=40, ax=axs[2][0])\n", "valid_score_data['median42_h'].plot.hist(bins=40, ax=axs[2][1])\n", "valid_score_data['median49_h'].plot.hist(bins=40, ax=axs[3][0])"]}, {"cell_type": "markdown", "metadata": {"_uuid": "bf844730db0123db1c1cbb09ad6b4bcb85eb404e", "_cell_guid": "746a9c9c-6bdc-4577-91cf-61dfca3afa93"}, "source": ["### C. ARIMA model\n", "\n", "Currently, it is no promising, and median seems a better base line, so I give up this section."]}, {"cell_type": "markdown", "metadata": {"_uuid": "cf15ab6eb24b181889e283d907edf67bfd6e3174", "_cell_guid": "87e6ef50-1392-43f5-989a-9e35b9ee675d"}, "source": ["### D. Facebook model\n", "We will train up the model using model with yearly and non-yearly model to see the difference"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "5ac78581fb57079909ab4f1ad35599216e52ddf7", "_cell_guid": "b6ff168f-c927-4f6e-889e-16ea52d1654b", "collapsed": true}, "source": ["print(\"Validation score for holiday model is: %.6f\" % validation_score(valid_score_data['holiday']))\n", "print(\"Validation score for holiday model w/log is: %.6f\" % validation_score(valid_score_data['holiday_log']))\n", "print(\"Validation score for yearly model w/log is: %.6f\" % validation_score(valid_score_data['yearly_log']))\n", "\n", "fig, axs  = plt.subplots(3,1,figsize=(12,12))\n", "valid_score_data['holiday'].plot.hist(bins=40, ax=axs[0])\n", "axs[0].set_title(\"Holiday model\")\n", "valid_score_data['holiday_log'].plot.hist(bins=40, ax=axs[1])\n", "axs[1].set_title(\"Holiday model w/log\")\n", "valid_score_data['yearly_log'].plot.hist(bins=40, ax=axs[2])\n", "axs[2].set_title(\"Yearly model w/log\")"]}, {"cell_type": "markdown", "metadata": {"_uuid": "33b68b34bcf8f5bb3116f26deeab8c36ef6a1327", "_cell_guid": "06234020-ef09-43d6-a158-2001674f664e"}, "source": ["### E. mixed model\n", "\n", "In this section, we will try to mix the model together to give a better prediction model"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "65cc25e7b304aca87b199c480330a08da2d1f69c", "_cell_guid": "cfa216a4-e638-4b6f-b9d9-1a39c8b1e658", "collapsed": true}, "source": ["def model_to_use( median, holiday_log, yearly_log):\n", "    result = median\n", "    if(median * 1 > yearly_log):\n", "        result = yearly_log\n", "    elif(median * 1 > holiday_log):\n", "        result = holiday_log\n", "        \n", "    return result\n", "\n", "model_score = valid_score_data.apply(lambda x: model_to_use( x['median14'], x['holiday_log'], x['yearly_log']), axis=1)\n", "\n", "print(\"Validation score for a proposed model is: %.6f\" % validation_score(model_score))\n", "model_score.plot.hist(bins=40)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "df64f2422afc0b8dd04d8cd5fa9d6325b72ef009", "_cell_guid": "aeb57480-2340-4288-b7fb-48c97c2c34f2", "collapsed": true}, "source": ["model_score_2 = valid_score_data.min(axis=1)\n", "print(\"Best possible Validation score for a mixed model is: %.6f\" % validation_score(model_score_2))\n", "\n", "model_score_2.plot.hist(bins=40)"]}], "nbformat_minor": 2}