{"cells":[{"metadata":{"_uuid":"a72e260cd5811a24aa93cf0ac27047c563dd5473"},"cell_type":"markdown","source":"# Two Sigma: Rental Interest Competition"},{"metadata":{"trusted":true,"_uuid":"eb3f9c721cca63b1eb95c63691e486e1cf2f50fa"},"cell_type":"code","source":"\"\"\"/data.py\n\nTools for loading data.\n\"\"\"\n\nimport errno\nimport itertools\nimport json\nimport os\nimport pandas as pd\n\n\ndef load_json_from_path(path):\n    try:\n        with open(path, \"r\") as f:\n            return json.load(f)\n    except OSError as error:\n        if error.errno != errno.ENOENT:\n            raise error\n\n    return None\n\n\ndef json_to_pandas_dataframe(dictionary):\n    columns = list(dictionary.keys())\n    rows = sorted(list(set(itertools.chain.from_iterable([\n        list(dictionary[k].keys())\n        for k in columns\n    ]))), key=lambda x: int(x))\n\n    # map(list, zip(*data)) is a quick trick to transpose\n    # a list of lists\n    data = list(map(list, zip(*([rows] + [\n        [\n            dictionary[column][r] if r in dictionary[column] else None\n            for r in rows\n        ]\n        for column in columns\n    ]))))\n    df = pd.DataFrame(data, columns=['id'] + columns)\n    df.set_index('id')\n\n    return df\n\n\ndef load_training_test_data(training_data_path, test_data_path):\n    return (\n        json_to_pandas_dataframe(load_json_from_path(training_data_path)),\n        json_to_pandas_dataframe(load_json_from_path(test_data_path))\n    )\n\n\"\"\"/utils/dataframe.py\n\nUtilities to clean out the data\nin the dataframe.\n\"\"\"\n\nimport datetime\nimport functools\nimport itertools\nimport json\nimport numpy as np\nimport operator\nimport pandas as pd\nimport pprint\nimport re\nimport spacy\n\nfrom collections import Counter, deque\n\n\ndef string_to_category_name(string):\n    return string.lower().replace(\" \", \"_\")\n\n\ndef categories_from_column(data_frame, column):\n    return list(set(list(itertools.chain.from_iterable(\n        data_frame[column].tolist()\n    ))))\n\n\ndef normalize_whitespace(string):\n    return re.sub(r\"\\s+\", \" \", string)\n\n\ndef normalize_category(category):\n    return normalize_whitespace(re.sub(r\"[\\*\\-\\!\\&]\", \" \", category.lower())).strip()\n\n\ndef normalize_categories(categories):\n    return [\n        normalize_category(c) for c in categories\n    ]\n\n\ndef sliding_window(sequence, n):\n    \"\"\"Returns a sliding window of width n over data from sequence.\"\"\"\n    it = iter(sequence)\n    window = deque((next(it, None) for _ in range(n)), maxlen=n)\n\n    yield list(window)\n\n    for element in it:\n        window.append(element)\n        yield list(window)\n\n\ndef create_ngrams(content, n):\n    for ngram in sliding_window(content.split(), n):\n        yield \" \".join(ngram)\n\n\ndef create_ngrams_up_to_n(content, n):\n    for i in range(n):\n        yield from create_ngrams(content, i)\n\n\ndef count_ngrams_up_to_n(content, n):\n    return Counter(list(create_ngrams_up_to_n(content, n)))\n\n\ndef remove_small_or_stopwords_from_ranking(ranking, nlp, min_len):\n    for word, rank in ranking:\n        if nlp.vocab[word].is_stop or len(word) < min_len:\n            continue\n\n        yield word, rank\n\n\ndef column_list_to_category_flags(data_frame, column, grams):\n    categories = [\n        \"{}_{}\".format(column, string_to_category_name(n))\n        for n in grams\n    ]\n    row_cleaned_categories = [\n        normalize_category(\" \".join(r))\n        for r in data_frame[column].tolist()\n    ]\n    category_flags = pd.DataFrame.from_records([\n        [1 if gram in r else 0 for gram in grams]\n        for r in row_cleaned_categories\n    ], columns=categories)\n\n    return pd.concat((data_frame, category_flags), axis=1)\n\n\ndef remap_column(data_frame, column, new_column, mapping):\n    data_frame[new_column] = data_frame[column].transform(mapping)\n    return data_frame\n\n\ndef remap_date_column_to_days_before(data_frame,\n                                     column,\n                                     new_column,\n                                     reference_date):\n    data_frame[new_column] = data_frame[column].transform(\n        lambda x: (reference_date - datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")).days\n    )\n    return data_frame\n\n\ndef map_categorical_column_to_category_ids(train_data_frame,\n                                           test_data_frame,\n                                           column,\n                                           new_column,\n                                           min_freq=1):\n    category_counts = Counter(train_data_frame[column]) + Counter(test_data_frame[column])\n    category_to_unknown_mapping = {\n        category: category if count >= min_freq else \"Unknown\"\n        for category, count in category_counts.items()\n    }\n    category_to_id_map = {\n        category: i\n        for i, category in enumerate(sorted([\n            category_to_unknown_mapping[c] for c in\n            (set(train_data_frame[column]) | set(test_data_frame[column]))\n        ]))\n    }\n    id_to_category_map = {\n        i: category\n        for category, i in category_to_id_map.items()\n    }\n\n    return (\n        category_to_unknown_mapping,\n        category_to_id_map,\n        id_to_category_map,\n        remap_column(train_data_frame,\n                     column,\n                     new_column,\n                     lambda x: category_to_id_map[category_to_unknown_mapping[x]]),\n        remap_column(test_data_frame,\n                     column,\n                     new_column,\n                     lambda x: category_to_id_map[category_to_unknown_mapping[x]])\n    )\n\n\ndef remap_columns_with_transform(train_data_frame,\n                                 test_data_frame,\n                                 column,\n                                 new_column,\n                                 transform):\n    \"\"\"Remove some columns with a transform.\"\"\"\n    return (\n        remap_column(train_data_frame,\n                     column,\n                     new_column,\n                     transform),\n        remap_column(test_data_frame,\n                     column,\n                     new_column,\n                     transform)\n    )\n\n\ndef normalize_description(description):\n    \"\"\"Normalize the description field.\"\"\"\n    description = description.lower()\n    description = re.sub(r\"<[^<]+?(>|$)\", \" \", description)\n    description = re.sub(r\"[0-9\\-]+\", \" \", description)\n    description = re.sub(r\"[a-z0-9]@[a-z0-9]\\.[a-z]\", \" \", description)\n    description = re.sub(r\"[\\!]+\", \"! \", description)\n    description = re.sub(r\"[\\-\\:]\", \" \", description)\n    description = re.sub(\"\\*\", \" \", description)\n    return re.sub(r\"\\s+\", \" \", description)\n\n\ndef add_epsilon(array):\n    return np.array([a + 10e-10 if a == 0 else a for a in array])\n\n\ndef numerical_feature_engineering_on_dataframe(dataframe,\n                                               numerical_columns):\n    \"\"\"Do per-dataframe feature engineering.\"\"\"\n    for lhs_column, rhs_column in itertools.combinations(numerical_columns, 2):\n        dataframe['{}_add_{}'.format(lhs_column, rhs_column)] = dataframe[lhs_column] + dataframe[rhs_column]\n        dataframe['{}_sub_{}'.format(lhs_column, rhs_column)] = dataframe[lhs_column] - dataframe[rhs_column]\n        dataframe['{}_mul_{}'.format(lhs_column, rhs_column)] = dataframe[lhs_column] * dataframe[rhs_column]\n        dataframe['{}_div_{}'.format(lhs_column, rhs_column)] = dataframe[lhs_column] / add_epsilon(dataframe[rhs_column])\n\n    return dataframe\n\n\ndef numerical_feature_engineering(train_data_frame,\n                                  test_data_frame,\n                                  numerical_columns):\n    \"\"\"Add, subtract, divide, multiply, exponentiate and take log.\"\"\"\n    return (\n        numerical_feature_engineering_on_dataframe(train_data_frame,\n                                                   numerical_columns),\n        numerical_feature_engineering_on_dataframe(test_data_frame,\n                                                   numerical_columns),\n    )\n\n\ndef normalize_eastwest(eastwest):\n    eastwest = eastwest.lower().strip()\n\n    if not eastwest:\n        return \"\"\n\n    if eastwest[0] == \"e\":\n        return \"e\"\n    elif eastwest[0] == \"w\":\n        return \"w\"\n    else:\n        return \"\"\n\n\nSTREET_MAPPING = {\n    \"st\": \"street\",\n    \"ave\": \"avenue\",\n    \"pl\": \"place\",\n    \"rd\": \"road\"\n}\n\n\ndef normalize_name(name):\n    m = re.match(r\"(?P<address>[\\w\\s]+)(?P<st>st|street|ave|avenue|place|pl|road|rd).*\",\n                 name.lower().strip())\n\n    if not m:\n        return name.lower().strip()\n\n    return \"{address} {street}\".format(\n        address=m.groupdict()[\"address\"].strip(),\n        street=STREET_MAPPING.get(m.groupdict()[\"st\"], m.groupdict()[\"st\"])\n    )\n\n\ndef normalize_address(address_dict):\n    return \"{eastwest} {name}\".format(\n        eastwest=normalize_eastwest(address_dict[\"eastwest\"] or \"\"),\n        name=normalize_name(address_dict[\"name\"] or \"\")\n    )\n\n\ndef parse_address_components_from_address(address):\n    m = re.match(r\"(?P<number>[0-9]*\\s+)?\\s*(?P<eastwest>East|West|E\\s|W\\s)?\\s*(?P<name>[A-Za-z0-9\\.\\-\\s]*)\",\n                 normalize_whitespace(address),\n                 flags=re.IGNORECASE)\n    return {\n        \"normalized\": normalize_address(m.groupdict()) if m is not None else address\n    }\n\n\ndef parse_address_components_for_column(dataframe, column):\n    return pd.concat((dataframe, pd.DataFrame.from_records([\n        {\n            \"{}_{}\".format(column, key): value for key, value in\n            parse_address_components_from_address(cell).items()\n        }\n        for cell in dataframe[column]\n    ])), axis=1)\n\n\ndef parse_address_components(train_data_frame,\n                             test_data_frame,\n                             columns):\n    return (\n        functools.reduce(lambda df, c: parse_address_components_for_column(df,\n                                                                           c),\n                         columns,\n                         train_data_frame),\n        functools.reduce(lambda df, c: parse_address_components_for_column(df,\n                                                                           c),\n                         columns,\n                         test_data_frame)\n    )\n\n\ndef count_json_column(dataframe, column):\n    return pd.DataFrame([\n        len(c) for c in dataframe[column]\n    ])\n\n\ndef count_json(train_data_frame,\n               test_data_frame,\n               column):\n    train_data_frame[\"{}_count\".format(column)] = count_json_column(train_data_frame,\n                                                                    column)\n    test_data_frame[\"{}_count\".format(column)] = count_json_column(test_data_frame,\n                                                                   column)\n\n    return train_data_frame, test_data_frame\n\n\"\"\"/feedback2vec.py\n\nGiven some raw string of feedback and a label (good/bad), build\na model capable of predicting whether the feedback was good or bad.\n\nTo do this we have a character encoder which encodes the\ncharacters in the dataset as one-hot encoded letters. We then pass each\ncharacter in the stream through an embedding layer, then through a forward\nand backward LSTM. The output is then passed to a fully connected\nlayer which predicts if the feedback was good or bad.\n\nThe theory is that we learn representations in the embedding layer which\nput the feedback into an appropriate vector space.\n\"\"\"\n\nimport argparse\nimport torch\nimport math\nimport numpy as np\nimport pandas as pd\nimport json\n\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nfrom sklearn.utils import shuffle\n\n\ndef maybe_cuda(tensor):\n    \"\"\"CUDAifies a tensor if possible.\"\"\"\n    if torch.cuda.is_available():\n        return tensor.cuda()\n\n    return tensor.cpu()\n\n\nclass Doc2Vec(nn.Module):\n    \"\"\"Doc2Vec model, based on Tweet2Vec.\"\"\"\n\n    def __init__(self,\n                 embedding_size,\n                 hidden_layer_size,\n                 vocab_size,\n                 output_size,\n                 batch_size):\n        super().__init__()\n\n        self.hidden_dim = hidden_layer_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n\n        # One hidden layers for each direction\n        self.batch_size = batch_size\n        self.embedding_size = embedding_size\n        self.hidden = (maybe_cuda(torch.randn(2, batch_size, self.hidden_dim)),\n                       maybe_cuda(torch.randn(2, batch_size, self.hidden_dim)))\n        self.lstm = nn.LSTM(embedding_size,\n                            self.hidden_dim,\n                            num_layers=1,\n                            bidirectional=True)\n        self.linear = nn.Linear(self.hidden_dim * 2, output_size)\n\n    def sentence_embedding(self, sentence):\n        self.hidden = (maybe_cuda(torch.randn(2, self.batch_size, self.hidden_dim)),\n                       maybe_cuda(torch.randn(2, self.batch_size, self.hidden_dim)))\n        embeddings = self.embedding(sentence)\n        out, self.hidden = self.lstm(embeddings.view(-1, self.batch_size, self.embedding_size),\n                                     self.hidden)\n        added = self.hidden[0] + self.hidden[1]\n        return added / torch.norm(added)\n\n    def forward(self, sentence):\n        embedding = self.sentence_embedding(sentence)\n        lin = self.linear(embedding.view(-1, self.hidden_dim * 2))\n        return F.softmax(lin, dim=1)\n\n\ndef train_model(model, optimizer, epochs, sentence_tensors, label_tensors):\n    for epoch in range(epochs):\n        total_loss = 0\n        loss_criterion = nn.CrossEntropyLoss()\n\n        shuffled_sentence_tensors, shuffled_label_tensors = shuffle(\n            sentence_tensors, label_tensors\n        )\n\n        progressable_tensors = tqdm(\n            zip(shuffled_sentence_tensors, shuffled_label_tensors),\n            total=len(shuffled_label_tensors),\n            desc=\"Processing sentence vectors\"\n        )\n\n        for sentence_tensor, label_tensor in progressable_tensors:\n            optimizer.zero_grad()\n\n            preds = model(sentence_tensor)\n            loss = loss_criterion(preds,\n                                  label_tensor)\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            progressable_tensors.set_postfix(loss=loss.item())\n\n        print('Epoch', epoch, 'total loss', total_loss)\n\n        with torch.no_grad():\n            print(\"Model Accuracy:\",\n                  compute_model_accuracy(model,\n                                         shuffled_sentence_tensors[0],\n                                         shuffled_label_tensors[0]))\n\n    return model\n\n\ndef characters_to_one_hot_lookups(all_characters):\n    set_characters = set(all_characters)\n    character_to_one_hot = {\n        c: i for i, c in enumerate(sorted(set_characters))\n    }\n    one_hot_to_character = {\n        c: i for i, c in enumerate(sorted(set_characters))\n    }\n\n    return character_to_one_hot, one_hot_to_character\n\n\ndef character_sequence_to_matrix(sentence, character_to_one_hot):\n    return np.array([character_to_one_hot[c] for c in sentence])\n\n\ndef compute_model_accuracy(model, sentence_tensors, label_tensors):\n    \"\"\"A floating point value of how accurate the model was at predicting each label.\"\"\"\n    predictions = np.argmax(model(maybe_cuda(sentence_tensors)).detach().cpu().numpy(), axis=1).flatten()\n    labels = label_tensors.detach().cpu().numpy().flatten()\n    return len([p for p in (predictions == labels) if p == True]) / len(predictions)\n\n\ndef pad_sentence(sentence, padding):\n    truncated = sentence[:padding]\n    return truncated + (\" \" * (padding - len(truncated)))\n\n\ndef to_batches(sentences, batch_size, pad_value):\n    for i in range(math.ceil(len(sentences) / batch_size)):\n        batch = [\n            sentences[i * batch_size + j]\n            for j in range(min((batch_size, len(sentences[i * batch_size:]))))\n        ]\n        padding = [\n            pad_value\n            for k in range(max(0, batch_size - len(sentences[i * batch_size:])))\n        ]\n        yield torch.stack(batch + padding, dim=0)\n\n\ndef documents_to_vectors_model(train_documents,\n                               test_documents,\n                               labels,\n                               epochs,\n                               parameters,\n                               learning_rate,\n                               load=None,\n                               save=None,\n                               sentence_length=1000,\n                               batch_size=200):\n    \"\"\"Convert some documents to vectors based on labels.\"\"\"\n    character_to_one_hot, one_hot_to_character = characters_to_one_hot_lookups(\n        \"\".join(train_documents) + \"\".join(test_documents)\n    )\n    train_sentence_tensors = list(to_batches([\n        maybe_cuda(torch.tensor(character_sequence_to_matrix(pad_sentence(sentence, sentence_length),\n                                                             character_to_one_hot), dtype=torch.long))\n        for sentence in train_documents\n    ], batch_size, maybe_cuda(torch.tensor([character_to_one_hot[\" \"] for i in range(sentence_length)]))))\n\n    label_tensors = list(to_batches([maybe_cuda(torch.tensor(i)) for i in labels],\n                                    batch_size,\n                                    maybe_cuda(torch.tensor(0))))\n\n    model = maybe_cuda(Doc2Vec(parameters,\n                               parameters * 2,\n                               len(character_to_one_hot.keys()), max(labels) + 1,\n                               batch_size))\n\n    if not load:\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        try:\n            train_model(model,\n                        optimizer,\n                        epochs,\n                        train_sentence_tensors,\n                        label_tensors)\n        except KeyboardInterrupt:\n            print(\"Interrupted, saving current state now\")\n    else:\n        model.load_state_dict(torch.load(load))\n\n    if save:\n        torch.save(model.state_dict(), save)\n\n    return character_to_one_hot, one_hot_to_character, model\n\n\ndef generate_document_vector_embeddings_from_model(model,\n                                                   character_to_one_hot,\n                                                   sentences,\n                                                   sentence_length,\n                                                   batch_size):\n    # Generate the embeddings for all of our documents now\n    sentence_tensors = list(to_batches([\n        maybe_cuda(torch.tensor(character_sequence_to_matrix(pad_sentence(sentence, sentence_length),\n                                                             character_to_one_hot), dtype=torch.long))\n        for sentence in sentences\n    ], batch_size, maybe_cuda(torch.tensor([character_to_one_hot[\" \"] for i in range(sentence_length)]))))\n    with torch.no_grad():\n        return np.row_stack([\n            model.sentence_embedding(sentence_tensor).detach().cpu().numpy()\n            for sentence_tensor in sentence_tensors\n        ]).reshape(-1, model.hidden_dim * 2)[:len(sentences)]\n\n\ndef column_to_doc_vectors(train_data_frame,\n                          test_data_frame,\n                          description_column,\n                          target_column,\n                          document_vector_column,\n                          epochs=100,\n                          parameters=40,\n                          learning_rate=0.01,\n                          load=None,\n                          save=None,\n                          sentence_length=1000,\n                          batch_size=200):\n    \"\"\"Convert some description columns to document vector columns.\"\"\"\n    train_descriptions = list(train_data_frame[description_column])\n    test_descriptions = list(test_data_frame[description_column])\n    labels = list(train_data_frame[target_column])\n\n    character_to_one_hot, one_hot_to_character, model = documents_to_vectors_model(\n        train_descriptions,\n        test_descriptions,\n        labels,\n        epochs,\n        parameters,\n        learning_rate,\n        load=load,\n        save=save,\n        sentence_length=sentence_length,\n        batch_size=batch_size\n    )\n\n    train_description_vectors = pd.DataFrame(\n        generate_document_vector_embeddings_from_model(\n            model,\n            character_to_one_hot,\n            train_descriptions,\n            sentence_length,\n            batch_size\n        )\n    )\n\n    test_description_vectors = pd.DataFrame(\n        generate_document_vector_embeddings_from_model(\n            model,\n            character_to_one_hot,\n            test_descriptions,\n            sentence_length,\n            batch_size\n        )\n    )\n\n    return (\n        pd.concat((train_data_frame, train_description_vectors), axis=1),\n        pd.concat((test_data_frame, test_description_vectors), axis=1)\n    )\n\n\n\"\"\"/utils/model.py\n\nModels to use with the data.\n\nThis module creates pipelines, which depending on the underlying\nmodel, will one-hot encode categorical data or just leave it as is,\nconverting it to a number. All the returned models satisfy the\nsklearn estimator API, so we can use them with grid search/evolutionary\nalgorithms for hyperparameter search if we want to.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\n\nfrom category_encoders import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef sklearn_pipeline_steps(categorical_columns, verbose=False):\n    return (\n        ('one_hot',\n         OneHotEncoder(cols=categorical_columns, verbose=verbose)),\n        ('scaling', StandardScaler())\n    )\n\n\ndef basic_logistic_regression_pipeline(categorical_columns,\n                                       verbose=False):\n    return Pipeline((\n        *sklearn_pipeline_steps(categorical_columns, verbose=verbose),\n        ('xgb', xgb.XGBClassifier(\n            n_estimators=1000,\n            seed=42,\n            objective='multi:softprob',\n            subsample=0.8,\n            colsample_bytree=0.8,\n        ))\n    ))\n\n\ndef calculate_statistics(statistics, test_labels, predictions):\n    return {\n        k: s(test_labels, predictions)\n        for k, s in statistics.items()\n    }\n\n\ndef format_statistics(calculated_statistics):\n    return \", \".join([\n        \"{0}: {1:.2f}\".format(k, s)\n        for k, s in calculated_statistics.items()\n    ])\n\n\ndef prediction_accuracy(labels, predictions):\n    return (\n        len([a for a, b in zip(labels, predictions) if a == b]) / len(predictions)\n    )\n\n\ndef fit_one_split(model, features, labels, statistics, train_index, test_index):\n    train_data, train_labels = features.iloc[train_index], labels[train_index]\n    test_data, test_labels = features.iloc[test_index], labels[test_index]\n\n    model.fit(train_data, train_labels)\n    predictions = model.predict(test_data)\n    return (\n        test_labels,\n        predictions,\n        calculate_statistics(statistics, test_labels, predictions)\n    )\n\n\ndef test_model_with_k_fold_cross_validation(model,\n                                            features,\n                                            labels,\n                                            statistics,\n                                            n_splits=5,\n                                            random_state=42):\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    test_labels, predictions = [], []\n\n    for i, (train_index, test_index) in enumerate(kf.split(features, labels)):\n        fold_test_labels, fold_predictions, calculated_statistics = fit_one_split(\n            model,\n            features,\n            labels,\n            statistics,\n            train_index,\n            test_index\n        )\n        print('Fold', i, format_statistics(calculated_statistics))\n        test_labels.extend(fold_test_labels)\n        predictions.extend(fold_predictions)\n\n    return (\n        calculate_statistics(statistics, test_labels, predictions),\n        test_labels,\n        predictions,\n        model\n    )\n\n\ndef get_prediction_probabilities_with_columns(model,\n                                              test_dataframe,\n                                              keep_columns):\n    return pd.concat((test_dataframe[keep_columns],\n                      pd.DataFrame(model.predict_proba(test_dataframe.drop(keep_columns, axis=1)))),\n                     axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"634f49f2609c9f4036068ebd102cbfc756a43ce7"},"cell_type":"code","source":"import datetime\nimport itertools\nimport json\nimport operator\nimport os\nimport pandas as pd\nimport pprint\nimport numpy as np\nimport re\nimport spacy\nimport torch\n\nfrom collections import Counter, deque\nfrom sklearn.metrics import mean_squared_error\n\nnlp = spacy.load(\"en\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d29bcb949361728151cf1f33a6c0e60733c29b8c"},"cell_type":"markdown","source":"Check GPU support"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0696e8b84801fa991e0834a865758c7eedf025c0"},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3fbd78922e5b7a258b4de91209e7fd5c706b116"},"cell_type":"markdown","source":"## 1) Load data"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4c5b76c94f201a9f72c877a0ed762d16c9334d52"},"cell_type":"code","source":"(TRAIN_DATAFRAME, TEST_DATAFRAME) = \\\n  load_training_test_data(os.path.join('..', 'input', 'train.json'),\n                          os.path.join('..', 'input', 'test.json'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e70a2fdae142664fae596a5842825b6ecb96f2ff"},"cell_type":"markdown","source":"## 2) Data Cleaning"},{"metadata":{"_uuid":"d43d837340ccaacdaa300af84ece161fa530cf8c"},"cell_type":"markdown","source":"Let's see what this table looks like. We'll display the head of the table which shows its features"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e955323d8249c1059e0b493a0f69e927fa9299e7"},"cell_type":"code","source":"TRAIN_DATAFRAME.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2befa8984359f6b00b838cb6706dafb6058f5072"},"cell_type":"markdown","source":"### 2.1) Cleaning up categories"},{"metadata":{"_uuid":"658e000f951bc7a046394ee702e41c7c0cdc8406"},"cell_type":"markdown","source":"Let's clean up the categories and put them into a sensible vector. Unfortunately the categories are a bit of a mess - since the user can specify what categories they want there isn't much in the way of consistency between categories.\n\nSome of the patterns that we frequently see in the categories are:\n - Separating category names with \"**\"\n - Mix of caps/nocaps\n - Some common themes, such as:\n   - \"pets\"\n   - \"office\"\n   - \"living room\"\n   - \"garden\"\n   - \"common area\"\n   - \"storage\"\n   - \"no pets\"\n   - \"parking\"\n   - \"bicycle\"\n   - \"doorman\"\n   - etc\n\nTo deal with this, lets pull out all of the categories and normalize them\nby removing excess punctuation, normalizing for whitespace, lowercasing, and counting for certain n-grams."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"bf038caf842c6c9297057df5b093c7d38b90352a"},"cell_type":"code","source":"normalized_categories = sorted(normalize_categories(categories_from_column(TRAIN_DATAFRAME, 'features')))\nnormalized_categories[:50]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6023e67a6f17ebfb8a5ea9395078a961b5a4ac8c"},"cell_type":"markdown","source":"Now that we have our slightly tidied up categories, we can create some n-grams and count their frequency"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"5222d3597833a577caed445ffe5afee8a5fcd6db"},"cell_type":"code","source":"most_common_ngrams = sorted(count_ngrams_up_to_n(\" \".join(normalized_categories), 3).most_common(),\n                            key=lambda x: (-x[1], x[0]))\nmost_common_ngrams[:50]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"794324d1c3c89457db9c7a403e713414e9e769ab"},"cell_type":"markdown","source":"There's quite a few words here that don't add much value. We can remove them by consulting a list of stopwords"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"3a657f8d0fa7a43e7cb76e9dbc4162f459a3ec67"},"cell_type":"code","source":"most_common_ngrams = sorted(list(remove_small_or_stopwords_from_ranking(most_common_ngrams, nlp, 3)),\n                            key=lambda x: (-x[1], x[0]))\nmost_common_ngrams[:50]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c53ab5b870baeaf84923122f0f6de9335983580c"},"cell_type":"markdown","source":"Now that we have these, we can probably take 100 most common and arrange\nthem into category flags for our table"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"4478fb8ad8e3ed798de3bae9b370447eaf681418"},"cell_type":"code","source":"TRAIN_DATAFRAME = column_list_to_category_flags(TRAIN_DATAFRAME, 'features', list(map(operator.itemgetter(0), most_common_ngrams[:100])))\nTEST_DATAFRAME = column_list_to_category_flags(TEST_DATAFRAME, 'features', list(map(operator.itemgetter(0), most_common_ngrams[:100])))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d23834851332d5d9bccd43960f3dc707614e9853"},"cell_type":"code","source":"TRAIN_DATAFRAME.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cc89327d7431dab33306d0121fc13af1fd21ce8"},"cell_type":"markdown","source":"### 2.2) Cleaning up listing_date"},{"metadata":{"_uuid":"670ea3789ea4b4760cd2a01a3aafa49cdb99ff85"},"cell_type":"markdown","source":"We can also do something useful with the listing date - it may be better to say how many days ago the property was listed - older properties are probably going to get a lot less interest than newer properties."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"b72d1ea3a0132b68ccd2d10d258a7a38308a7c9d"},"cell_type":"code","source":"TRAIN_DATAFRAME = remap_date_column_to_days_before(TRAIN_DATAFRAME, \"created\", \"created_days_ago\", datetime.datetime(2017, 1, 1))\nTEST_DATAFRAME = remap_date_column_to_days_before(TEST_DATAFRAME, \"created\", \"created_days_ago\", datetime.datetime(2017, 1, 1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"c504d058714329fefd1985e64f47d31944bc9c37"},"cell_type":"code","source":"TRAIN_DATAFRAME[\"created_days_ago\"].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad2148d0a04121da120d3f4d759f8f0049a60b1"},"cell_type":"markdown","source":"### 2.3) Cleaning up interest_level"},{"metadata":{"_uuid":"9f75fac29d46c93b44618cbc1f099760650fa863"},"cell_type":"markdown","source":"Right now the interest level is encoded on a scale of \"Low, Medium, High\". The competition\nwants us to classify the entries in to each, so we assign a label"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"8093e618ca8140850502e586fc7475ba883b9664"},"cell_type":"code","source":"TRAIN_DATAFRAME = remap_column(TRAIN_DATAFRAME, \"interest_level\", \"label_interest_level\", lambda x: {\n    \"high\": 0,\n    \"medium\": 1,\n    \"low\": 2\n}[x])\n# The TEST_DATAFRAME does not have an interest_level column, so we\n# instead add it and replace it with all zeros\nTEST_DATAFRAME[\"label_interest_level\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7a1cd0ccc400649ec3e99a1d8b5d23362e06cd85"},"cell_type":"code","source":"TRAIN_DATAFRAME[\"label_interest_level\"].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51d2e43d1dfeebb09c141066ddcd6b6896c6e347"},"cell_type":"markdown","source":"### 2.4) Cleaning up building_id, manager_id"},{"metadata":{"_uuid":"1ff537eaff201e715abd875fdb9452aea558d1e2"},"cell_type":"markdown","source":"`building_id` and `manager_id` look a bit useless to us on the outside, but according to https://www.kaggle.com/den3b81/some-insights-on-building-id they are actually quite predictive of interest since 20% of the manager make up 80% of the rentals (we can also see this in their writing style as well).\n\nSince there aren't too many managers or buildings in total, we can convert these into category ID's where we'll pass them through an embedding later on.\n\nNote that we need to do this over both dataframes - since there could\nbe some managers that are in the test dataframe which are not in the training dataframe and vice versa.\n\nNote that we want to lump all the \"misc\" buildings and managers together\ninto a single building or manager since listings by \"non-property managers\" or \"non-frequently-rented-buildings\" are different from ones run by property managers."},{"metadata":{"trusted":true,"_uuid":"1846416b1333d4259ce769b72afffb9327c9e20a"},"cell_type":"code","source":"(BUILDING_ID_UNKNOWN_REMAPPING,\n BUILDING_ID_TO_BUILDING_CATEGORY,\n BUILDING_CATEGORY_TO_BUILDING_ID,\n TRAIN_DATAFRAME,\n TEST_DATAFRAME) = map_categorical_column_to_category_ids(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    'building_id',\n    'building_id_category',\n    min_freq=40\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f43579f13f502c804d550ff4c7775d4eac71096"},"cell_type":"code","source":"(MANAGER_ID_UNKNOWN_REMAPPING,\n MANAGER_ID_TO_MANAGER_CATEGORY,\n MANAGER_CATEGORY_TO_MANAGER_ID,\n TRAIN_DATAFRAME,\n TEST_DATAFRAME) = map_categorical_column_to_category_ids(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    'manager_id',\n    'manager_id_category',\n    min_freq=40\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"457f262a766a53498e9038b8e3b7bd8e7e02f6fc"},"cell_type":"markdown","source":"### 2.4AA) Parsing and Separating Out Address Components\nSome properties might be in the same neighbourhood, the same street or\npart of the same building. If we separate out the address components then\nwe might be able to get some more meaningful feature groupings.\n\nWe first parse all the components into their own columns and then map them into categories (dropping them later on)."},{"metadata":{"trusted":true,"_uuid":"e0feefe4d42a932e329cf5ea96aca39cbfb511d9"},"cell_type":"code","source":"import imp\n\nTRAIN_DATAFRAME, TEST_DATAFRAME = parse_address_components(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    [\n        \"display_address\",\n        \"street_address\"\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75478efe592e8dfddfb57a2696cbf7627ee26909"},"cell_type":"code","source":"(DISP_ADDR_ID_UNKNOWN_REMAPPING,\n DISP_ADDR_TO_DISP_ADDR_CATEGORY,\n DISP_ADDR_CATEGORY_TO_DISP_ADDR_ID,\n TRAIN_DATAFRAME,\n TEST_DATAFRAME) = map_categorical_column_to_category_ids(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    'display_address_normalized',\n    'display_address_category',\n    min_freq=10\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea8ed684c4da688ce999d77d44b757ce98e80b6a"},"cell_type":"markdown","source":"### 2.4AB) Counting Number of Photos\nThe number of photos a place has might be predictive of its interest as well, so lets at least count the number of photos."},{"metadata":{"trusted":true,"_uuid":"6b3bcdd59d25983728826b7a36e7d560950193e0"},"cell_type":"code","source":"TRAIN_DATAFRAME, TEST_DATAFRAME = count_json(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    \"photos\"\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fef7f38befc67c8f6a1831075028f7ae626d051"},"cell_type":"markdown","source":"### 2.4AC) Feature Engineering on Numerical Columns\nSome models can't do simple math, but ratios or additions/subtractions\nbetween things might be important. Lets do that now for all of our\nnumerical data"},{"metadata":{"trusted":true,"_uuid":"6a0ae31ab8c6dee371b44b2b9b47ec0df15e4313"},"cell_type":"code","source":"NUMERICAL_COLUMNS = [\n    'bathrooms',\n    'bedrooms',\n    'price',\n    'latitude',\n    'longitude',\n    'photos_count'\n]\n\nTRAIN_DATAFRAME, TEST_DATAFRAME = numerical_feature_engineering(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    NUMERICAL_COLUMNS\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60a7067018bbb72c1f4cdad743b624db8a6174c1"},"cell_type":"markdown","source":"### 2.4A) Generating document vectors for description\nThe text in the descriptions are pretty messy. We can generate some document vectors to embed the entire thing into vector space.\n\nTo do that we use an LSTM to train some embeddings, similar to the Tweet2Vec paper."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"8af3d329256099480be26e764108ddfbf1e26d5e"},"cell_type":"code","source":"TRAIN_DATAFRAME, TEST_DATAFRAME = remap_columns_with_transform(\n    TRAIN_DATAFRAME,\n    TEST_DATAFRAME,\n    'description',\n    'clean_description',\n    normalize_description\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"820fc6b61c83baa11704472d7464710c1b9e0827"},"cell_type":"code","source":"#TRAIN_DATAFRAME, TEST_DATAFRAME = column_to_doc_vectors(\n#    TRAIN_DATAFRAME,\n#    TEST_DATAFRAME,\n#    'clean_description',\n#    'label_interest_level',\n#    'description_vector',\n#    epochs=1000,\n#    parameters=200,\n#    learning_rate=0.01,\n#    save='word_embedding.npy',\n#    batch_size=100\n#)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9c35190b5cdab17b5ff9c1338faf20109689727"},"cell_type":"markdown","source":"### 2.5) Drop unnecessary columns"},{"metadata":{"_uuid":"041c1b7c6ea91c8e5f0b53f06a6d087a39d9b6e0"},"cell_type":"markdown","source":"Now that we have made our data nicer to work with, we can drop all the inconvenient to work with columns."},{"metadata":{"trusted":true,"_uuid":"348ab471f23e8dd9aaf6606d7b9cf46bf509f433"},"cell_type":"code","source":"DROP_COLUMNS = [\n    'id',\n    'created',\n    'building_id',\n    'clean_description',\n    'description',\n    'features',\n    'display_address',\n    'display_address_normalized',\n    # We keep listing_id in the dataframe\n    # since we'll need it later\n    # 'listing_id',\n    'manager_id',\n    'photos',\n    'street_address',\n    'street_address_normalized',\n    'interest_level',\n]\nTRAIN_DATAFRAME = TRAIN_DATAFRAME.drop(DROP_COLUMNS, axis=1)\n# TEST_DATAFRAME doesn't have interest_level, so we remove it\n# from the DROP_COLUMNS\nTEST_DATAFRAME = TEST_DATAFRAME.drop(DROP_COLUMNS[:-1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae74ed452c3e5ee9c3a7923d26e960b237bb4496"},"cell_type":"code","source":"TRAIN_DATAFRAME.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f19560234eaa55cb4b134dea998eab43bfd37abd"},"cell_type":"markdown","source":"## 3) Fitting models"},{"metadata":{"_uuid":"915d5e2c60536419cc1d8fa88b3b7c92a8049e2b"},"cell_type":"markdown","source":"Now we can try out a few models and see what works well for the data that\nwe have so far."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d762dafeb8c0ef967b7f7123c11ec1304cb35cfe"},"cell_type":"code","source":"[(i, x) for i, x in enumerate(np.all(np.isfinite(TRAIN_DATAFRAME.drop(['listing_id', 'label_interest_level'], axis=1)), axis=0)) if not x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"312181f8416ab8e963070f33581ff5227b95e1fe"},"cell_type":"code","source":"TRAIN_DATAFRAME.drop(['listing_id', 'label_interest_level'], axis=1).columns[119]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d8f775e0cc7d09b1c20e2093281536142c0c7c"},"cell_type":"code","source":"TEST_DATAFRAME.columns[109]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47c8c45943788e9f48a4eafb0b9563ba458d9f1a"},"cell_type":"code","source":"np.argwhere(~np.isfinite(TEST_DATAFRAME.drop(['listing_id', 'label_interest_level'], axis=1).as_matrix()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c7eedf293758126d273dde80c1d8c30d81e82e3"},"cell_type":"code","source":"CATEGORICAL_COLUMNS = ('building_id_category', 'manager_id_category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70042045a485de2eb787bf12e925aa497bca5dde"},"cell_type":"code","source":"(LINEAR_MODEL_STATISTICS,\n LINEAR_MODEL_LABELS,\n LINEAR_MODEL_PREDICTIONS,\n LINEAR_MODEL) = test_model_with_k_fold_cross_validation(\n    basic_logistic_regression_pipeline(CATEGORICAL_COLUMNS),\n    TRAIN_DATAFRAME.drop(['listing_id', 'label_interest_level'], axis=1).astype(float),\n    TRAIN_DATAFRAME['label_interest_level'],\n    {\n        \"mse_loss\": mean_squared_error,\n        \"accuracy\": prediction_accuracy\n    },\n    n_splits=2\n)\n\nprint('Linear Model', format_statistics(LINEAR_MODEL_STATISTICS))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"be89f81f26df80458d37ce9094f2eee6a84dc5b2"},"cell_type":"code","source":"pd.set_option(\"display.max_columns\",200)\npd.set_option(\"display.max_rows\",500)\nTRAIN_DATAFRAME[TRAIN_DATAFRAME.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94434f6f49d2eadb180f1a67a56074ac6a1b3196"},"cell_type":"markdown","source":"## 4) Generate Submission"},{"metadata":{"trusted":true,"_uuid":"6ef34a1e731763e8f9102bf9c6e412d2c3f967f2"},"cell_type":"code","source":"table = get_prediction_probabilities_with_columns(LINEAR_MODEL,\n                                                  TEST_DATAFRAME.drop('label_interest_level', axis=1),\n                                                  ['listing_id'])\ntable.columns = ['listing_id', 'high', 'medium', 'low']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a515f437d2ac59ecbfe4463c7493fb7970fb0843"},"cell_type":"code","source":"table.to_csv('submission.csv', columns=['listing_id', 'high', 'medium', 'low'], index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}