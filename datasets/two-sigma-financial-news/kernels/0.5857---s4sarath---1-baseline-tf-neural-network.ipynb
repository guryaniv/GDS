{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom kaggle.competitions import twosigmanews\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffe02b9c1e8d781ff340ae81e6b972cb712b865d"},"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n# distribution of confidence as a sanity check: they should be distributed as above\nimport time\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"env = twosigmanews.make_env()\n(mt_df, nt_df) = env.get_training_data()\nprint(\"Market data {}\".format(mt_df.shape))\nprint(\"News data {}\".format(nt_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efc5a02d05d0e3878e019ffe85bbdc5e498b5785"},"cell_type":"code","source":"############ For memory free up \ndel nt_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cba1701a1aa1d13c067a8da2e6cd4e81bac35165"},"cell_type":"code","source":"##### Analyze the null values in the dataset ( from some other kernel)\ntotal = mt_df.isnull().sum().sort_values(ascending = False)\npercent = round(mt_df.isnull().sum().sort_values(ascending = False)/len(mt_df)*100, 2)\nmt_df_null = pd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])\nmt_df_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b40168310619af3756f3b01a3dc7c8113c3eba73"},"cell_type":"code","source":"# Disable asset codes  for the time being\n# np.random.seed(0)\n# asset_embedding_dimension = 5\n# asset_code_dict = {k: v for v, k in enumerate(mt_df['assetCode'].unique())}\n# asset_code_dict['UNK_asset'] = len(asset_code_dict) # for unknown asset in prediction time\n# # asset_code_embedding = np.random.uniform(-1,1,(len(asset_code_dict), asset_embedding_dimension)).astype(np.float32)\n# asset_code_embedding = np.random.normal(loc=0.0, scale=1.0, size=(len(asset_code_dict), asset_embedding_dimension))\n# print(asset_code_embedding.shape)\n# asset_code_embedding = dict(zip(list(asset_code_dict.keys()), asset_code_embedding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd801633db0c8b9283073967961b2b912973733c"},"cell_type":"code","source":"####### Features for the model \ncategorical_features = ['assetCode' , 'universe' , 'time']\nnumerical_features = ['volume', \n                      'close' ,\n                      'open' ,\n                      'returnsClosePrevRaw1',\n                      'returnsOpenPrevRaw1',\n                      'returnsClosePrevMktres1',\n                      'returnsOpenPrevMktres1',\n                      'returnsClosePrevRaw10',\n                      'returnsOpenPrevRaw10',\n                      'returnsClosePrevMktres10',\n                      'returnsOpenPrevMktres10']\nprediction_column = ['returnsOpenNextMktres10']\nprediction_column_scaled = ['returnsOpenNextMktres10_scaled']\n###### Select only relevant columns \n\nmt_df = mt_df[categorical_features+numerical_features+prediction_column]\nprint(mt_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18d626b17987d56c9ef7a562683b2f8e5e45b737","scrolled":true},"cell_type":"code","source":"############## Preprocessing flow\n\ndef impute_missing(df, columns):\n    X = df[columns].fillna(0.0)\n    return X.values\n\ndef scaler_fn(df, columns, scaler_to_use = 'minmax'):\n    \n    if scaler_to_use is 'minmax':\n        from sklearn.preprocessing import MinMaxScaler\n        scaler = MinMaxScaler()\n        scaler.fit(df[columns].values)\n        scaled_matrix = scaler.transform(df[columns].values)\n        for index, num_col in enumerate(numerical_features):\n            print(\"Mean of {} is {:.2f}\".format(num_col,scaled_matrix[:,index].mean()))\n            print(\"Std deviation of {} is {:.2f}\".format(num_col,scaled_matrix[:,index].std()))\n        return scaler, scaled_matrix\n    if scaler_to_use is 'standard_scaler':\n        from sklearn.preprocessing import StandardScaler\n        scaler = StandardScaler()\n        scaler.fit(df[columns].values)\n        scaled_matrix = scaler.transform(df[columns].values)\n        return scaler, scaled_matrix\n\n##### Let it be here for future\ndef sigmoid(x):\n  return 1 / (1 + np.exp(-x))\n\n####### Naive programming , i know :-(\ndef binary_scale(value):\n    if value >= 0.0:\n        return 1.0\n    else:\n        return 0.0\n    \n\nX_impute = impute_missing(mt_df, numerical_features)\nmt_df[numerical_features] = X_impute\nscaler, scaled_matrix = scaler_fn(mt_df, numerical_features , scaler_to_use='standard_scaler')\nprint(\"Max {} and Min is {} after normalization\".format(scaled_matrix.max() , scaled_matrix.min()))\nmt_df[prediction_column_scaled[0]] = mt_df[prediction_column[0]].apply(binary_scale)\nmt_df[numerical_features] = scaled_matrix\n\ndel scaled_matrix # For memory\n\n######## For test data :-)\ndef test_preprocess(test_df):\n    test_df_matrix = impute_missing(test_df, numerical_features)\n    test_df[numerical_features] = test_df_matrix\n    del test_df_matrix\n    X_test = scaler.transform(test_df[numerical_features].values)\n    return X_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f94f613a778536523e2260eb42d2a6adc0d9d563"},"cell_type":"code","source":"######### Label counts \nprint(mt_df[prediction_column_scaled[0]].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ae5c3f305cac1da9894604798b82acc3a88140"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cd2b768d12a2b9ba3f210dd562e3d5fba43feb4"},"cell_type":"code","source":"########### Batch generator\n\ndef batch_generator(X, Y=None, batch_size = 64, max_seq_len = 20, is_shuffle = False, test=False):\n    \n    if is_shuffle:\n        if not test:\n            p = np.random.permutation(len(X))\n            X = X[p]\n            Y = Y[p]\n    combined_len = batch_size*max_seq_len\n    if len(X) % (combined_len) == 0:\n        iterations = int(len(X) / (combined_len))\n    else:\n        iterations = int(len(X) / (combined_len)) + 1\n        \n    for iter_ in range(iterations):\n        start = iter_*combined_len\n        end   = (iter_+1)*combined_len\n        X_batch = X[start:end]\n        if not test:\n            Y_batch = Y[start:end]\n            yield X_batch , Y_batch\n        else:\n            yield X_batch\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f3d31e50f3108c2d5fe7efbafaa2adb58e6b219d"},"cell_type":"code","source":"########## Train test split ################\n\n\n# all_asset_codes_unique = list(pd.unique(mt_df['assetCode']))\n# mt_df_grouped = mt_df.groupby(\"assetCode\")\n# validation_df = pd.DataFrame()\n# train_df      = pd.DataFrame()\n\n# sample_test = 110 # sampling all asset_codes is super expensive time based\n# asset_codes_for_test = all_asset_codes_unique[:sample_test]\n    \n# for count_ , asset_code_ in enumerate(asset_codes_for_test):\n#     sample_df = mt_df_grouped.get_group(asset_code_)\n#     split_index = int(0.10 * len(sample_df))\n#     train_sample    = sample_df[:(len(sample_df)-split_index)]\n#     test_sample = sample_df[-1*split_index:]\n#     validation_df = pd.concat([validation_df, test_sample])\n#     train_df      = pd.concat([train_df, train_sample])\n#     print(\"{} , train {} , validation {}\".format(asset_code_,\n#                                                  train_sample.shape,\n#                                                  test_sample.shape))\n# train_df = pd.concat([train_df, mt_df[~mt_df['assetCode'].isin(asset_codes_for_test)]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(mt_df.index.values,test_size=0.25, random_state=23)\ntrain_df = mt_df.loc[train_indices]\nvalidation_df = mt_df.loc[val_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56787bc2bb724f6262541141c4928dfbe54f63cb"},"cell_type":"code","source":"print(train_df.shape, validation_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1085b9cbfdfbfe1d12248fc99bc6bd2459777b02"},"cell_type":"code","source":"# For memory\n# del mt_df\n# del sample_df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b11c7a81424ef61161db384619da757255defb1b"},"cell_type":"code","source":"BATCH_SIZE  = 256\nNUM_EPOCHS  = 11\nNUM_FEATURES   = 11\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74b28b96af3ddafb388fb53299e376bf6975ac5e"},"cell_type":"code","source":"import tensorflow as tf\ntf.reset_default_graph()\n\ninput_ph = tf.placeholder(tf.float32, [None, NUM_FEATURES], 'input_ph')\nlabel_ph = tf.placeholder(tf.float32, [None], 'labels_ph')\n\nW1 = tf.get_variable(\n                        name='W1',\n                        shape=(NUM_FEATURES,64),\n                        initializer=tf.truncated_normal_initializer(-1,1),\n                        trainable=True)\n\nb1 = tf.Variable(tf.zeros(shape=[64]),\n                                 name='b1', dtype=tf.float32)\n    \nW2 = tf.get_variable(\n                        name='W2',\n                        shape=(64,128),\n                        initializer=tf.truncated_normal_initializer(-1,1),\n                        trainable=True)\n\nb2 = tf.Variable(tf.zeros(shape=[128]),\n                                 name='b2', dtype=tf.float32)\n\nW_out = tf.get_variable(\n                        name='W_out',\n                        shape=(128,1),\n                        initializer=tf.truncated_normal_initializer(-1,1),\n                        trainable=True)\n\nb_out = tf.Variable(tf.zeros(shape=[1]),\n                                 name='b_out', dtype=tf.float32)\n\nh1 = tf.nn.sigmoid(tf.matmul(input_ph, W1) + b1)\nh2 = tf.nn.tanh(tf.matmul(h1, W2) + b2)\nlogits = tf.matmul(h2, W_out) + b_out\nlogits = tf.reshape(logits, [-1])\npredictions = tf.nn.sigmoid(logits)\nloss   = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=label_ph, logits=logits))\n\ncorrect_pred = tf.equal(tf.round(predictions), label_ph)\ntf_accuracy_score = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\noptimizer = tf.train.AdamOptimizer()\ngradients, variables = zip(*optimizer.compute_gradients(loss))\ngradients, _ = tf.clip_by_global_norm(gradients, 5.0)\ntrain_op = optimizer.apply_gradients(zip(gradients, variables))\n\nprint(tf.trainable_variables())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceb11a2d109d3293e9bebe948f7cd8f5abd10ed1"},"cell_type":"code","source":"try:\n    if sess:\n        tf.InteractiveSession.close(sess)\nexcept:\n    pass\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e409acefe88576ddd2c252bca2fcb258cd5a6139"},"cell_type":"code","source":"def check_validation(df, batch_size = 1500, check_loss = True):\n    if len(df) < batch_size:\n        batch_size = len(df)\n    val_epoch_cost = 0.0\n    val_prediction = []\n    validate_batch = batch_generator(df[numerical_features].values, df[prediction_column_scaled].values,\n                                        batch_size = batch_size, test=False)\n    epoch_time = 0.0\n    val_logits_final = []\n    for batch_count, (X_batch , Y_batch) in enumerate(validate_batch):\n        start_time = time.time()\n        feed_dict = {\n                     input_ph: X_batch ,\n                     label_ph: Y_batch.flatten()\n        }\n        val_logits , val_acc  = sess.run((predictions,tf_accuracy_score), feed_dict=feed_dict)\n        val_prediction.append(val_acc)\n        val_logits_final.extend(val_logits)\n        end_time = time.time()\n        batch_time = end_time-start_time\n        epoch_time += batch_time\n        if batch_count % 500 == 0.0:\n            print(\"Validation , done batch {} , time {}\".format(batch_count,batch_time ))\n    return val_logits_final, np.mean(val_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"570dca4858c0ad62353c8d5e8f00e831b94c8a34","scrolled":false},"cell_type":"code","source":"training_loss = []\ntraining_accuracy = []\nvalidation_accuracy = []\ncheck_validation_step = 1.0\nNUM_EPOCHS = 11\nfor epoch in range(NUM_EPOCHS):\n    epoch_cost = 0.0\n    epoch_time = 0.0\n    train_batch = batch_generator(train_df[numerical_features].values, \n                                  train_df[prediction_column_scaled].values, \n                                    batch_size = BATCH_SIZE,is_shuffle=True)\n    for batch_count, (X_batch ,Y_batch) in enumerate(train_batch):\n        start_time = time.time()\n        Y_batch = Y_batch.flatten() #### For tensorflow placholder\n        feed_dict = {\n                     input_ph: X_batch ,\n                     label_ph : Y_batch\n        }\n        \n        batch_cost , _ = sess.run((loss, train_op) , feed_dict=feed_dict)\n        epoch_cost += batch_cost\n        end_time = time.time()\n        batch_time = end_time-start_time\n        epoch_time += batch_time\n#         print(\"Epoch {} , batch {} , loss {} , time {}\".format(epoch,batch_count,\n#                                                                batch_cost,batch_time ))\n        if batch_count % 500 == 0.0:\n            print(\"Epoch {} , Done {} , time {}\".format(epoch, batch_count, epoch_time))\n    print(\"Epoch {} ,epoch loss {} , time {}\".format(epoch,epoch_cost/batch_count,\n                                                               epoch_time ))\n    training_loss.append(epoch_cost/batch_count)\n    \n    val_probs , val_acc = check_validation(validation_df)\n    train_probs , train_acc = check_validation(train_df)\n    validation_accuracy.append(val_acc)\n    training_accuracy.append(train_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c09f8fa1e691c9f638dc6772eac6abd3f5982718"},"cell_type":"code","source":"\nplt.plot(training_loss)\nplt.title('training loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch') \nplt.show()\n\n\nplt.plot(training_accuracy)\nplt.title('training accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch') \nplt.show()\n\nplt.plot(validation_accuracy)\nplt.title('validation accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcfdd4ed0d2b6c35518bcddc2c551bc6f06cc50"},"cell_type":"code","source":"# del mt_df\n# del X_batch \n# del Y_batch\n# del seq_len_batch\n# del train_df\n# del X_impute","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50dbc68df2515cb9d9358ae12f59d4ea5abcc2a2"},"cell_type":"code","source":"# distribution of confidence that will be used as submission\nconfidence_valid = np.array(val_probs)*2 -1\nprint(accuracy_score(confidence_valid>0,validation_df[prediction_column_scaled].values))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d995311f38f62fcdcebb06d3a78d97244e1edbb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a70f95fdbc1e55935cac50bb458bc25b65ed1af1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d0a29c499f3e8427a2f51829714cd1a90ace8cc"},"cell_type":"code","source":"# calculation of actual metric that is used to calculate final score\nr_valid = validation_df[prediction_column_scaled].values\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nu_valid = validation_df['universe'].values\nd_valid = validation_df['time'].dt.date\nx_t_i = confidence_valid * r_valid.flatten() * u_valid.flatten()\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35dbf8477af7c7f7a53e251a4cef01e23800b565","scrolled":true},"cell_type":"code","source":"days = env.get_prediction_days()\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = []\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    del news_obs_df\n    #########################\n    X_test = test_preprocess(market_obs_df)\n    test_batch_gen = batch_generator(X_test\n                                     ,batch_size = 128,test=True)\n    market_prediction = []\n    for X_test_batch  in test_batch_gen:\n        feed_dict_test = {\n             input_ph: X_test_batch ,\n            }\n        test_batch_prediction = sess.run(predictions,\n                                 feed_dict=feed_dict_test)\n        market_prediction.extend(test_batch_prediction)\n    market_prediction = np.array(market_prediction)\n    market_prediction_scaled = (2*market_prediction)-1.0                                       \n    n_days +=1\n    print(n_days,end=' ')\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction_scaled))\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction_scaled})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8e786ca17b6e247cbd0227550407d6f077b5666"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46f1cf5e9fd1ba5e0325a0f772f383f90e6d7719","scrolled":true},"cell_type":"code","source":"\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"347520b790fdecf02f81fc757abc6b7bd05183c1"},"cell_type":"code","source":"predicted_confidences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dad61ccfba0bc74f94eeda9a48e12e4385f08f9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}