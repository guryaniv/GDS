{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone, is_classifier\nfrom sklearn.model_selection._split import check_cv\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inlineimport gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class UniqueTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, axis=1, accept_sparse=False):\n        if axis == 0:\n            raise NotImplementedError('axis is 0! Not implemented!')\n        if accept_sparse:\n            raise NotImplementedError('accept_sparse is True! Not implemented!')\n        self.axis = axis\n        self.accept_sparse = accept_sparse\n        \n    def fit(self, X, y=None):\n        _, self.unique_indices_ = np.unique(X, axis=self.axis, return_index=True)\n        return self\n    \n    def transform(self, X, y=None):\n        return X[:, self.unique_indices_]\n\n\nclass StatsTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, stat_funs=None, verbose=0, n_jobs=-1, pre_dispatch='2*n_jobs'):\n        self.stat_funs = stat_funs\n        self.verbose = verbose\n        self.n_jobs = n_jobs\n        self.pre_dispatch = pre_dispatch\n    \n    def _get_stats(self, row):\n        stats = []\n        for fun in self.stat_funs:\n            stats.append(fun(row))\n        return stats\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        parallel = Parallel(\n            n_jobs=self.n_jobs,\n            pre_dispatch=self.pre_dispatch,\n            verbose=self.verbose\n        )\n        stats_list = parallel(delayed(self._get_stats)(X[i_smpl, :]) for i_smpl in range(len(X)))\n        return np.array(stats_list)\n    \nclass LnTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return np.sign(X)*np.log1p(np.abs(X))   \n\n\nclass ClassifierTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator=None, n_classes=2, cv=3):\n        self.estimator = estimator\n        self.n_classes = n_classes\n        self.cv = cv\n    \n    def _get_labels(self, y):\n        y_labels = np.zeros(len(y))\n        y_us = np.sort(np.unique(y))\n        step = int(len(y_us) / self.n_classes)\n        \n        for i_class in range(self.n_classes):\n            if i_class + 1 == self.n_classes:\n                y_labels[y >= y_us[i_class * step]] = i_class\n            else:\n                y_labels[\n                    np.logical_and(\n                        y >= y_us[i_class * step],\n                        y < y_us[(i_class + 1) * step]\n                    )\n                ] = i_class\n        return y_labels\n        \n    def fit(self, X, y):\n        y_labels = self._get_labels(y)\n        cv = check_cv(self.cv, y_labels, classifier=is_classifier(self.estimator))\n        self.estimators_ = []\n        \n        for train, _ in cv.split(X, y_labels):\n            self.estimators_.append(\n                clone(self.estimator).fit(X[train], y_labels[train])\n            )\n        return self\n    \n    def transform(self, X, y=None):\n        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n        \n        X_prob = np.zeros((X.shape[0], self.n_classes))\n        X_pred = np.zeros(X.shape[0])\n        \n        for estimator, (_, test) in zip(self.estimators_, cv.split(X)):\n            X_prob[test] = estimator.predict_proba(X[test])\n            X_pred[test] = estimator.predict(X[test])\n        return np.hstack([X_prob, np.array([X_pred]).T])\n\n\nclass _StatFunAdaptor:\n    \n    def __init__(self, stat_fun, *funs, **stat_fun_kwargs):\n        self.stat_fun = stat_fun\n        self.funs = funs\n        self.stat_fun_kwargs = stat_fun_kwargs\n\n    def __call__(self, x):\n        x = x[x != 0]\n        for fun in self.funs:\n            x = fun(x)\n        if x.size == 0:\n            return -99999\n        return self.stat_fun(x, **self.stat_fun_kwargs)\n\n\ndef diff2(x):\n    return np.diff(x, n=2)\n\n\ndef get_stat_funs():\n    \"\"\"\n    Previous version uses lambdas.\n    \"\"\"\n    stat_funs = []\n    \n    stats = [len, np.min, np.max, np.median, np.std, skew, kurtosis] + 19 * [np.percentile]\n    stats_kwargs = [{} for i in range(7)] + [{'q': i} for i in np.linspace(0.05, 0.95, 19)]\n\n    for stat, stat_kwargs in zip(stats, stats_kwargs):\n        stat_funs.append(_StatFunAdaptor(stat,**stat_kwargs))\n        stat_funs.append(_StatFunAdaptor(stat, np.diff, **stat_kwargs))\n        stat_funs.append(_StatFunAdaptor(stat, diff2, **stat_kwargs))\n        stat_funs.append(_StatFunAdaptor(stat, np.unique, **stat_kwargs))\n        stat_funs.append(_StatFunAdaptor(stat, np.unique, np.diff, **stat_kwargs))\n        stat_funs.append(_StatFunAdaptor(stat, np.unique, diff2, **stat_kwargs))\n    return stat_funs\n\n\ndef get_rfc():\n    return RandomForestClassifier(\n        n_estimators=100,\n        max_features=0.5,\n        max_depth=None,\n        max_leaf_nodes=270,\n        min_impurity_decrease=0.0001,\n        random_state=123,\n        n_jobs=-1\n    )\n\ndef get_input():\n    train = pd.read_csv('../input/train.csv')\n    test = pd.read_csv('../input/test.csv')\n    y_train_log = np.log1p(train['target'])\n    id_test = test['ID']\n    del test['ID']\n    del train['ID']\n    del train['target']\n    return train.values, y_train_log.values, test.values, id_test.values\n\n\npipe = Pipeline(\n    [\n        ('vt', VarianceThreshold(threshold=0.0)),\n        ('ut', UniqueTransformer()),\n        ('fu', FeatureUnion(\n                [\n                    ('pca', PCA(n_components=100)),\n                    ('ct-2', ClassifierTransformer(get_rfc(), n_classes=2, cv=5)),\n                    ('ct-3', ClassifierTransformer(get_rfc(), n_classes=3, cv=5)),\n                    ('ct-4', ClassifierTransformer(get_rfc(), n_classes=4, cv=5)),\n                    ('ct-5', ClassifierTransformer(get_rfc(), n_classes=5, cv=5)),\n                    ('st', StatsTransformer(stat_funs=get_stat_funs(), verbose=2))\n                ]\n            )\n        ),\n        ('ln', LnTransformer())\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4092849b233bbeafc12d5dd915e0c7fe6a60783","collapsed":true},"cell_type":"code","source":"X_train, y_train_log, X_test, id_test = get_input()\npipe.fit(X_train, y_train_log)\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"987d1073b34dff697ba769d02920adc26a9a48eb"},"cell_type":"code","source":"ss = StandardScaler()\nss.fit(np.vstack([X_train,X_test]))\nX_train = np.round(ss.transform(X_train).astype(float),6)\nX_test = np.round(ss.transform(X_test).astype(float),6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7f5085e9aae6236891b04dc9cf5fd36ecfbd94bb"},"cell_type":"code","source":"traindf = pd.DataFrame(data=X_train,columns=['s' + str(s) for s in range(X_train.shape[1])])\ntestdf = pd.DataFrame(data=X_test,columns=['s' + str(s) for s in range(X_test.shape[1])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c0a04926f6ff04f7e44228be4dfc6994c18eea66"},"cell_type":"code","source":"def GPClusterX(data):\n    v = pd.DataFrame()\n    v[\"s0\"] = np.tanh(np.where(data[\"s107\"]>0, (((data[\"s110\"]) + (np.minimum(((-1.0)), ((2.0)))))/2.0), ((((5.0)) > (data[\"s139\"]))*1.) )) \n    v[\"s1\"] = np.tanh(np.minimum((((8.61502456665039062))), ((np.tanh((np.minimum((((3.65845537185668945))), ((((data[\"s100\"]) * ((((((np.tanh((-1.0))) + (0.0))) + (0.0))/2.0)))))))))))) \n    v[\"s2\"] = np.tanh(((((((((data[\"s105\"]) + (((((((-1.0*((data[\"s112\"])))) + ((-1.0*((((data[\"s112\"]) / 2.0))))))/2.0)) * 2.0)))/2.0)) / 2.0)) + (((data[\"s139\"]) + (data[\"s136\"]))))/2.0)) \n    v[\"s3\"] = np.tanh(((((((data[\"s139\"]) + ((((data[\"s105\"]) + (((data[\"s139\"]) / 2.0)))/2.0)))/2.0)) + (np.minimum((((((data[\"s136\"]) > ((((data[\"s112\"]) + (2.0))/2.0)))*1.))), ((data[\"s136\"])))))/2.0)) \n    v[\"s4\"] = np.tanh(((((((((data[\"s136\"]) - ((((data[\"s136\"]) > (((data[\"s136\"]) / 2.0)))*1.)))) - (data[\"s112\"]))) - ((((data[\"s112\"]) + (-1.0))/2.0)))) + (data[\"s112\"]))) \n    v[\"s5\"] = np.tanh((((-2.0) + (data[\"s117\"]))/2.0)) \n    v[\"s6\"] = np.tanh(((data[\"s110\"]) - (-1.0))) \n    v[\"s7\"] = np.tanh(np.where(((((data[\"s110\"]) / 2.0)) / 2.0)>0, ((data[\"s136\"]) * (((((data[\"s136\"]) * 2.0)) / 2.0))), ((data[\"s107\"]) * (((data[\"s136\"]) * (data[\"s112\"])))) )) \n    v[\"s8\"] = np.tanh(((np.maximum(((data[\"s139\"])), ((data[\"s112\"])))) * (((np.maximum(((data[\"s112\"])), ((((data[\"s110\"]) * (data[\"s139\"])))))) * (((data[\"s139\"]) / 2.0)))))) \n    v[\"s9\"] = np.tanh((((3.0) < (-2.0))*1.)) \n    v[\"s10\"] = np.tanh(((((data[\"s105\"]) * (data[\"s136\"]))) - (data[\"s116\"]))) \n    v[\"s11\"] = np.tanh(((data[\"s100\"]) - ((((data[\"s103\"]) > (data[\"s107\"]))*1.)))) \n    v[\"s12\"] = np.tanh((((((-1.0*((data[\"s112\"])))) * (((((data[\"s136\"]) * (((data[\"s136\"]) / 2.0)))) * (data[\"s136\"]))))) * (data[\"s136\"]))) \n    v[\"s13\"] = np.tanh(np.tanh((((np.maximum(((np.where(data[\"s101\"]>0, data[\"s116\"], (6.0) ))), ((((-1.0) * 2.0))))) * 2.0)))) \n    v[\"s14\"] = np.tanh((((((-2.0) / 2.0)) + (np.maximum(((data[\"s105\"])), (((((-1.0) + (2.0))/2.0))))))/2.0)) \n    v[\"s15\"] = np.tanh((((data[\"s103\"]) + (np.where(data[\"s104\"]>0, (((data[\"s104\"]) < (data[\"s103\"]))*1.), (((-1.0) > ((((-1.0) + (np.minimum(((1.0)), ((data[\"s116\"])))))/2.0)))*1.) )))/2.0)) \n    v[\"s16\"] = np.tanh(((np.maximum(((((np.minimum(((-2.0)), ((data[\"s110\"])))) + (data[\"s107\"])))), (((((((((data[\"s116\"]) + (-1.0))/2.0)) / 2.0)) / 2.0))))) * (data[\"s110\"]))) \n    v[\"s17\"] = np.tanh((((((-3.0) > (data[\"s109\"]))*1.)) + ((((-3.0) > (((((1.0) / 2.0)) / 2.0)))*1.)))) \n    v[\"s18\"] = np.tanh((((((np.minimum(((data[\"s38\"])), ((data[\"s38\"])))) * (data[\"s38\"]))) + (((((-1.0*((data[\"s112\"])))) + (np.minimum(((data[\"s38\"])), (((-1.0*(((-1.0*((data[\"s112\"])))))))))))/2.0)))/2.0)) \n    v[\"s19\"] = np.tanh((((((data[\"s116\"]) + (data[\"s40\"]))/2.0)) + (np.minimum(((np.minimum((((((-2.0) < (data[\"s40\"]))*1.))), ((((data[\"s0\"]) + (data[\"s103\"]))))))), ((data[\"s40\"])))))) \n    v[\"s20\"] = np.tanh(((np.minimum((((4.0))), ((data[\"s112\"])))) * 2.0)) \n    v[\"s21\"] = np.tanh((((data[\"s122\"]) + (((np.maximum(((data[\"s119\"])), ((data[\"s116\"])))) + (data[\"s122\"]))))/2.0)) \n    v[\"s22\"] = np.tanh(((data[\"s41\"]) + (np.maximum(((data[\"s76\"])), ((np.maximum(((((data[\"s41\"]) + (data[\"s76\"])))), ((((((data[\"s69\"]) + (data[\"s76\"]))) + (data[\"s69\"]))))))))))) \n    v[\"s23\"] = np.tanh(((np.minimum(((np.minimum(((np.tanh((np.tanh(((((((data[\"s136\"]) * 2.0)) + (data[\"s119\"]))/2.0))))))), (((((1.34242451190948486)) * 2.0)))))), ((data[\"s112\"])))) * 2.0)) \n    v[\"s24\"] = np.tanh(((((((np.where(data[\"s118\"]>0, data[\"s118\"], ((data[\"s157\"]) / 2.0) )) * 2.0)) - (data[\"s77\"]))) / 2.0)) \n    v[\"s25\"] = np.tanh(np.where(data[\"s69\"]>0, ((np.tanh(((((3.0) + (data[\"s258\"]))/2.0)))) / 2.0), 0.0 )) \n    v[\"s26\"] = np.tanh(((3.0) + (data[\"s122\"]))) \n    v[\"s27\"] = np.tanh(np.maximum(((np.where(np.maximum(((((data[\"s64\"]) + (data[\"s14\"])))), ((data[\"s64\"])))>0, ((data[\"s64\"]) * 2.0), data[\"s19\"] ))), (((((-1.0*((data[\"s79\"])))) - (data[\"s19\"])))))) \n    v[\"s28\"] = np.tanh(np.where(((data[\"s119\"]) + (((np.tanh((data[\"s119\"]))) + (data[\"s121\"]))))>0, (((((data[\"s122\"]) < ((((data[\"s122\"]) < (data[\"s119\"]))*1.)))*1.)) * 2.0), data[\"s122\"] )) \n    v[\"s29\"] = np.tanh(((((np.minimum(((np.where(data[\"s131\"]>0, (((data[\"s11\"]) > (data[\"s136\"]))*1.), np.maximum(((data[\"s131\"])), ((data[\"s49\"]))) ))), (((((data[\"s49\"]) > (data[\"s136\"]))*1.))))) * 2.0)) * 2.0)) \n    v[\"s30\"] = np.tanh(np.minimum(((3.0)), ((((data[\"s155\"]) * (np.tanh((3.0)))))))) \n    v[\"s31\"] = np.tanh(((((np.minimum(((((data[\"s70\"]) - (data[\"s136\"])))), ((data[\"s138\"])))) - (data[\"s32\"]))) + (np.where(data[\"s136\"]>0, ((data[\"s70\"]) - (data[\"s14\"])), data[\"s138\"] )))) \n    v[\"s32\"] = np.tanh((-1.0*((((data[\"s157\"]) - (np.maximum(((np.where(data[\"s157\"]>0, data[\"s148\"], np.maximum(((data[\"s148\"])), ((((data[\"s148\"]) * 2.0)))) ))), ((((((-1.0) * 2.0)) * 2.0)))))))))) \n    v[\"s33\"] = np.tanh(((((-1.0*((data[\"s252\"])))) < (np.minimum(((((data[\"s246\"]) - (1.0)))), ((data[\"s246\"])))))*1.)) \n    v[\"s34\"] = np.tanh(((((((data[\"s3\"]) - (data[\"s136\"]))) - (((data[\"s115\"]) + (((data[\"s136\"]) * (data[\"s115\"]))))))) + ((((data[\"s29\"]) + (data[\"s137\"]))/2.0)))) \n    v[\"s35\"] = np.tanh(np.minimum((((((((((data[\"s152\"]) + (data[\"s151\"]))/2.0)) - (data[\"s157\"]))) + ((((data[\"s151\"]) + (data[\"s151\"]))/2.0))))), (((((data[\"s122\"]) < (((data[\"s151\"]) / 2.0)))*1.))))) \n    v[\"s36\"] = np.tanh(np.maximum(((((np.maximum(((data[\"s246\"])), ((data[\"s108\"])))) * (((data[\"s222\"]) - (data[\"s108\"])))))), ((((((((data[\"s108\"]) + (data[\"s246\"]))/2.0)) + (data[\"s258\"]))/2.0))))) \n    v[\"s37\"] = np.tanh((((np.maximum(((np.maximum(((data[\"s122\"])), ((((((data[\"s152\"]) * (data[\"s118\"]))) * (data[\"s148\"]))))))), ((data[\"s152\"])))) + (data[\"s158\"]))/2.0)) \n    v[\"s38\"] = np.tanh(((data[\"s88\"]) * ((((((1.0) - (((data[\"s13\"]) / 2.0)))) > (data[\"s93\"]))*1.)))) \n    v[\"s39\"] = np.tanh((((((((np.tanh((((((-1.0) * (np.tanh(((2.0)))))) + (np.minimum(((data[\"s8\"])), ((data[\"s107\"])))))))) / 2.0)) / 2.0)) + (data[\"s21\"]))/2.0)) \n    v[\"s40\"] = np.tanh((((data[\"s93\"]) < (((np.minimum(((np.tanh((data[\"s27\"])))), ((np.where(data[\"s93\"]>0, data[\"s121\"], 2.0 ))))) * 2.0)))*1.)) \n    v[\"s41\"] = np.tanh((((((data[\"s22\"]) * (1.0))) + ((((0.0) < ((-1.0*((data[\"s13\"])))))*1.)))/2.0)) \n    v[\"s42\"] = np.tanh(np.maximum(((data[\"s121\"])), ((((((data[\"s155\"]) * 2.0)) - (((data[\"s157\"]) * 2.0))))))) \n    v[\"s43\"] = np.tanh(np.where(data[\"s78\"]>0, ((((data[\"s84\"]) - (data[\"s95\"]))) * 2.0), np.maximum(((((((data[\"s95\"]) * (data[\"s84\"]))) - (data[\"s79\"])))), ((data[\"s24\"]))) )) \n    v[\"s44\"] = np.tanh(((((np.maximum(((np.where((((((data[\"s101\"]) > (data[\"s121\"]))*1.)) + (data[\"s101\"]))>0, (3.0), data[\"s77\"] ))), ((data[\"s77\"])))) + (data[\"s151\"]))) * 2.0)) \n    v[\"s45\"] = np.tanh(np.maximum((((((((data[\"s31\"]) + (data[\"s18\"]))/2.0)) * (data[\"s19\"])))), ((np.minimum(((((((data[\"s18\"]) - (data[\"s22\"]))) + (data[\"s64\"])))), ((data[\"s102\"]))))))) \n    v[\"s46\"] = np.tanh(((((data[\"s121\"]) * 2.0)) - (np.tanh((-1.0))))) \n    v[\"s47\"] = np.tanh(((((((data[\"s117\"]) > (data[\"s145\"]))*1.)) < (((data[\"s151\"]) - (data[\"s139\"]))))*1.)) \n    v[\"s48\"] = np.tanh(np.tanh((((np.minimum(((((((1.0) * 2.0)) * (data[\"s250\"])))), ((((-1.0) / 2.0))))) * 2.0)))) \n    v[\"s49\"] = np.tanh(((((data[\"s170\"]) + (data[\"s188\"]))) + (np.minimum(((((np.minimum(((((((-3.0) * 2.0)) * (-1.0)))), ((data[\"s200\"])))) + (data[\"s170\"])))), (((4.0))))))) \n    v[\"s50\"] = np.tanh(((((((data[\"s104\"]) * (data[\"s0\"]))) * 2.0)) * 2.0)) \n    v[\"s51\"] = np.tanh((((-1.0*((((np.maximum(((data[\"s104\"])), ((data[\"s182\"])))) * (data[\"s104\"])))))) + (np.where(data[\"s212\"]>0, data[\"s128\"], data[\"s104\"] )))) \n    v[\"s52\"] = np.tanh((((data[\"s13\"]) + (((data[\"s222\"]) * (np.minimum(((data[\"s13\"])), (((((((((data[\"s139\"]) * (data[\"s222\"]))) + (data[\"s139\"]))) + (3.0))/2.0))))))))/2.0)) \n    v[\"s53\"] = np.tanh(((data[\"s116\"]) * (np.minimum(((((data[\"s116\"]) * ((((np.minimum(((data[\"s116\"])), ((-1.0)))) + (data[\"s139\"]))/2.0))))), ((((data[\"s118\"]) + (data[\"s116\"])))))))) \n    v[\"s54\"] = np.tanh(((((3.0) - (data[\"s216\"]))) * 2.0)) \n    v[\"s55\"] = np.tanh((((((((data[\"s132\"]) * ((((((data[\"s150\"]) + (data[\"s132\"]))/2.0)) * (data[\"s132\"]))))) + (data[\"s150\"]))/2.0)) * (((data[\"s140\"]) - (1.0))))) \n    v[\"s56\"] = np.tanh((((((((((((data[\"s160\"]) + (data[\"s160\"]))/2.0)) * (data[\"s175\"]))) * 2.0)) * ((((data[\"s124\"]) + (data[\"s160\"]))/2.0)))) * (((2.0) - (data[\"s163\"]))))) \n    v[\"s57\"] = np.tanh(((np.minimum(((data[\"s188\"])), ((((((-2.0) * (data[\"s182\"]))) * 2.0))))) + (((data[\"s128\"]) - (np.tanh((data[\"s176\"]))))))) \n    v[\"s58\"] = np.tanh((((((-1.0) < (data[\"s111\"]))*1.)) - (np.tanh((np.tanh((data[\"s111\"]))))))) \n    v[\"s59\"] = np.tanh(((((np.minimum(((((1.0) + (data[\"s113\"])))), ((((((((((data[\"s107\"]) - (data[\"s40\"]))) - (data[\"s139\"]))) * 2.0)) * 2.0))))) * 2.0)) - (data[\"s139\"]))) \n    v[\"s60\"] = np.tanh((((-1.0*((data[\"s31\"])))) * (((np.tanh((data[\"s98\"]))) - (1.0))))) \n    v[\"s61\"] = np.tanh(((((data[\"s151\"]) * (((((((((((data[\"s139\"]) + (data[\"s139\"]))) + (data[\"s90\"]))) + (data[\"s90\"]))) * 2.0)) + (data[\"s90\"]))))) - (data[\"s15\"]))) \n    v[\"s62\"] = np.tanh((((2.0) < (np.maximum(((data[\"s253\"])), (((((((-3.0) > (np.maximum(((1.0)), ((data[\"s253\"])))))*1.)) - (((data[\"s259\"]) / 2.0))))))))*1.)) \n    v[\"s63\"] = np.tanh(((((data[\"s145\"]) * 2.0)) + (((((data[\"s107\"]) + (((np.maximum(((data[\"s107\"])), ((((data[\"s145\"]) + (data[\"s145\"])))))) + (data[\"s145\"]))))) * 2.0)))) \n    return v.sum(axis=1).values*.1\n\ndef GPClusterY(data):\n    v = pd.DataFrame()\n    v[\"s0\"] = np.tanh((((((((((data[\"s110\"]) + (data[\"s101\"]))) + (((data[\"s136\"]) - (data[\"s107\"]))))/2.0)) + (data[\"s136\"]))) + (np.minimum(((data[\"s136\"])), (((-1.0*((data[\"s112\"]))))))))) \n    v[\"s1\"] = np.tanh((((data[\"s268\"]) + ((((((data[\"s105\"]) + (np.tanh((data[\"s107\"]))))/2.0)) + (((data[\"s116\"]) - (data[\"s107\"]))))))/2.0)) \n    v[\"s2\"] = np.tanh(np.maximum(((((((((np.tanh((np.tanh((data[\"s139\"]))))) > (1.0))*1.)) + (data[\"s105\"]))/2.0))), ((data[\"s136\"])))) \n    v[\"s3\"] = np.tanh((((data[\"s139\"]) + (np.tanh((data[\"s139\"]))))/2.0)) \n    v[\"s4\"] = np.tanh(((data[\"s116\"]) + (data[\"s136\"]))) \n    v[\"s5\"] = np.tanh(np.minimum((((((data[\"s268\"]) > (((data[\"s107\"]) + ((((data[\"s136\"]) > (data[\"s107\"]))*1.)))))*1.))), ((data[\"s136\"])))) \n    v[\"s6\"] = np.tanh(((((((data[\"s136\"]) / 2.0)) - (data[\"s103\"]))) * (((np.tanh((((((data[\"s136\"]) / 2.0)) / 2.0)))) - ((((-1.0*((data[\"s136\"])))) * (data[\"s136\"]))))))) \n    v[\"s7\"] = np.tanh((((np.where(((((data[\"s136\"]) + (data[\"s136\"]))) + ((0.26983028650283813)))>0, data[\"s110\"], (((-1.0*((data[\"s112\"])))) * 2.0) )) + ((-1.0*((data[\"s136\"])))))/2.0)) \n    v[\"s8\"] = np.tanh((-1.0*(((((data[\"s139\"]) < (data[\"s107\"]))*1.))))) \n    v[\"s9\"] = np.tanh(((np.where(np.maximum(((np.tanh((data[\"s271\"])))), ((np.maximum(((0.0)), ((data[\"s136\"]))))))>0, data[\"s271\"], ((0.0) + ((-1.0*((data[\"s103\"]))))) )) / 2.0)) \n    v[\"s10\"] = np.tanh(np.minimum(((((((2.0) - (data[\"s107\"]))) + (data[\"s136\"])))), ((np.maximum(((((((((data[\"s107\"]) + (data[\"s105\"]))/2.0)) < (data[\"s107\"]))*1.))), ((data[\"s136\"]))))))) \n    v[\"s11\"] = np.tanh((((data[\"s139\"]) > (np.tanh((2.0))))*1.)) \n    v[\"s12\"] = np.tanh((((((12.41106319427490234)) + (((-3.0) / 2.0)))) * ((((((data[\"s107\"]) * (((data[\"s112\"]) + (3.0))))) + ((-1.0*((data[\"s110\"])))))/2.0)))) \n    v[\"s13\"] = np.tanh(((((((((((2.0) - (data[\"s112\"]))) + (2.0))/2.0)) + (((data[\"s112\"]) * (data[\"s101\"]))))/2.0)) * (data[\"s112\"]))) \n    v[\"s14\"] = np.tanh((((((data[\"s110\"]) < (((((((((data[\"s110\"]) / 2.0)) < (data[\"s105\"]))*1.)) < (data[\"s103\"]))*1.)))*1.)) - (data[\"s103\"]))) \n    v[\"s15\"] = np.tanh(((((data[\"s103\"]) + (((data[\"s259\"]) * (data[\"s105\"]))))) * (np.where(data[\"s116\"]>0, data[\"s116\"], ((((((data[\"s105\"]) + (1.0))/2.0)) + (data[\"s116\"]))/2.0) )))) \n    v[\"s16\"] = np.tanh((-1.0*(((((-1.0*((data[\"s107\"])))) - (np.maximum((((7.0))), ((-2.0))))))))) \n    v[\"s17\"] = np.tanh(((((((data[\"s115\"]) / 2.0)) * ((((((data[\"s107\"]) < (((((data[\"s115\"]) / 2.0)) / 2.0)))*1.)) + ((((data[\"s115\"]) + (((data[\"s107\"]) * 2.0)))/2.0)))))) / 2.0)) \n    v[\"s18\"] = np.tanh(np.maximum(((((-1.0) * 2.0))), ((data[\"s38\"])))) \n    v[\"s19\"] = np.tanh(np.minimum(((((3.0) - (data[\"s112\"])))), ((((data[\"s0\"]) - ((((((np.minimum(((data[\"s0\"])), ((((data[\"s40\"]) * 2.0))))) * 2.0)) + (data[\"s40\"]))/2.0))))))) \n    v[\"s20\"] = np.tanh(np.minimum((((((np.maximum(((data[\"s40\"])), ((data[\"s112\"])))) > (1.0))*1.))), (((((((((2.0) - (data[\"s112\"]))) * 2.0)) + (2.0))/2.0))))) \n    v[\"s21\"] = np.tanh(((((((((data[\"s158\"]) + (((data[\"s116\"]) + ((7.56830501556396484)))))) * ((-1.0*((data[\"s119\"])))))) + ((7.56830501556396484)))) * ((-1.0*((data[\"s119\"])))))) \n    v[\"s22\"] = np.tanh(((((np.where(data[\"s85\"]>0, data[\"s85\"], ((data[\"s76\"]) * (data[\"s112\"])) )) - (data[\"s69\"]))) - (data[\"s14\"]))) \n    v[\"s23\"] = np.tanh(((((((((data[\"s107\"]) * (data[\"s136\"]))) * (data[\"s136\"]))) + (data[\"s136\"]))) + ((((((((data[\"s136\"]) * 2.0)) + (data[\"s119\"]))) + (-1.0))/2.0)))) \n    v[\"s24\"] = np.tanh(np.tanh(((-1.0*((np.maximum(((data[\"s119\"])), (((10.0)))))))))) \n    v[\"s25\"] = np.tanh(np.minimum(((data[\"s131\"])), ((np.where(((1.0) + (data[\"s131\"]))>0, (((((((0.57987225055694580)) + (data[\"s131\"]))/2.0)) + (((((data[\"s69\"]) / 2.0)) / 2.0)))/2.0), data[\"s130\"] ))))) \n    v[\"s26\"] = np.tanh(((np.maximum(((((np.maximum(((np.minimum(((data[\"s157\"])), ((data[\"s118\"]))))), ((data[\"s118\"])))) * 2.0))), ((data[\"s158\"])))) + (data[\"s158\"]))) \n    v[\"s27\"] = np.tanh(np.minimum(((3.0)), ((data[\"s79\"])))) \n    v[\"s28\"] = np.tanh(np.tanh((((((np.where(data[\"s119\"]>0, data[\"s157\"], data[\"s121\"] )) + (np.tanh((2.0))))) * 2.0)))) \n    v[\"s29\"] = np.tanh((-1.0*((((-2.0) * ((-1.0*((data[\"s97\"]))))))))) \n    v[\"s30\"] = np.tanh(np.maximum(((np.maximum(((data[\"s157\"])), ((data[\"s121\"]))))), ((np.where(((data[\"s121\"]) + (np.maximum(((data[\"s157\"])), ((data[\"s121\"])))))>0, data[\"s157\"], ((data[\"s152\"]) / 2.0) ))))) \n    v[\"s31\"] = np.tanh(((((data[\"s70\"]) * (data[\"s138\"]))) + (data[\"s93\"]))) \n    v[\"s32\"] = np.tanh((-1.0*((((2.0) - (-2.0)))))) \n    v[\"s33\"] = np.tanh(((np.minimum(((data[\"s228\"])), ((((((((data[\"s240\"]) > (np.tanh((data[\"s222\"]))))*1.)) > (data[\"s222\"]))*1.))))) + ((((data[\"s240\"]) > (data[\"s240\"]))*1.)))) \n    v[\"s34\"] = np.tanh(((((((((-1.0) - (((data[\"s137\"]) + (np.tanh((data[\"s97\"]))))))) + (np.tanh((((data[\"s137\"]) - (data[\"s97\"]))))))) - (data[\"s97\"]))) * 2.0)) \n    v[\"s35\"] = np.tanh(((((data[\"s152\"]) * ((-1.0*((0.0)))))) - (np.maximum(((data[\"s151\"])), ((0.0)))))) \n    v[\"s36\"] = np.tanh(np.maximum(((np.maximum(((data[\"s258\"])), ((data[\"s246\"]))))), ((data[\"s264\"])))) \n    v[\"s37\"] = np.tanh(((np.minimum((((((((data[\"s118\"]) > (((data[\"s152\"]) + (np.maximum(((data[\"s152\"])), ((data[\"s122\"])))))))*1.)) / 2.0))), ((np.maximum(((data[\"s152\"])), ((data[\"s122\"]))))))) * 2.0)) \n    v[\"s38\"] = np.tanh(((((np.where(((((data[\"s73\"]) * (data[\"s16\"]))) + (data[\"s88\"]))>0, data[\"s67\"], data[\"s73\"] )) + (data[\"s88\"]))) - (data[\"s73\"]))) \n    v[\"s39\"] = np.tanh(np.where(data[\"s107\"]>0, ((((data[\"s107\"]) + (data[\"s21\"]))) + (-1.0)), (((-1.0) > (data[\"s27\"]))*1.) )) \n    v[\"s40\"] = np.tanh((((((np.where(data[\"s118\"]>0, (-1.0*((data[\"s27\"]))), np.where((-1.0*((data[\"s27\"])))>0, (-1.0*((data[\"s151\"]))), 2.0 ) )) + (data[\"s8\"]))) + (data[\"s93\"]))/2.0)) \n    v[\"s41\"] = np.tanh(((np.maximum(((((-2.0) - (data[\"s13\"])))), ((((data[\"s104\"]) + ((((data[\"s13\"]) < (((data[\"s104\"]) * (np.tanh((data[\"s104\"]))))))*1.))))))) * ((10.0)))) \n    v[\"s42\"] = np.tanh(((np.where(data[\"s118\"]>0, ((data[\"s157\"]) * (data[\"s157\"])), ((data[\"s44\"]) + (data[\"s148\"])) )) + ((((data[\"s44\"]) + ((((data[\"s121\"]) + (data[\"s44\"]))/2.0)))/2.0)))) \n    v[\"s43\"] = np.tanh(np.maximum(((data[\"s24\"])), ((((data[\"s84\"]) * 2.0))))) \n    v[\"s44\"] = np.tanh((-1.0*(((((np.maximum(((data[\"s121\"])), ((np.minimum(((data[\"s148\"])), ((0.0))))))) > (np.minimum(((((np.minimum(((data[\"s101\"])), ((data[\"s151\"])))) + ((0.0))))), ((data[\"s118\"])))))*1.))))) \n    v[\"s45\"] = np.tanh(((3.0) + (data[\"s13\"]))) \n    v[\"s46\"] = np.tanh(((np.minimum(((np.minimum(((data[\"s255\"])), ((((data[\"s249\"]) * (data[\"s267\"]))))))), ((((((data[\"s273\"]) + (-2.0))) * (data[\"s255\"])))))) * (((data[\"s249\"]) * 2.0)))) \n    v[\"s47\"] = np.tanh(((data[\"s136\"]) * (((((data[\"s139\"]) * (data[\"s139\"]))) - (np.maximum(((data[\"s107\"])), ((np.maximum(((data[\"s145\"])), ((data[\"s136\"]))))))))))) \n    v[\"s48\"] = np.tanh(((((((data[\"s256\"]) * (data[\"s250\"]))) + ((-1.0*((np.where(data[\"s136\"]>0, data[\"s256\"], 1.0 ))))))) - (data[\"s136\"]))) \n    v[\"s49\"] = np.tanh((((10.86935424804687500)) * (np.where(data[\"s170\"]>0, np.where(((data[\"s170\"]) * 2.0)>0, (((3.0) + (((-2.0) * (data[\"s188\"]))))/2.0), data[\"s170\"] ), data[\"s128\"] )))) \n    v[\"s50\"] = np.tanh(((data[\"s104\"]) + (((data[\"s139\"]) * (((((((data[\"s139\"]) * (data[\"s139\"]))) * (((data[\"s111\"]) - (data[\"s104\"]))))) - ((2.0)))))))) \n    v[\"s51\"] = np.tanh(((np.minimum(((data[\"s182\"])), ((data[\"s200\"])))) + (data[\"s164\"]))) \n    v[\"s52\"] = np.tanh(np.maximum((((((data[\"s13\"]) > (((data[\"s13\"]) / 2.0)))*1.))), ((data[\"s139\"])))) \n    v[\"s53\"] = np.tanh(((((((data[\"s116\"]) + (((data[\"s116\"]) / 2.0)))/2.0)) + (((0.0) / 2.0)))/2.0)) \n    v[\"s54\"] = np.tanh(np.minimum(((((data[\"s138\"]) + (np.maximum(((data[\"s111\"])), ((((data[\"s168\"]) * 2.0)))))))), ((np.where(data[\"s111\"]>0, (-1.0*((data[\"s168\"]))), ((data[\"s138\"]) - (data[\"s210\"])) ))))) \n    v[\"s55\"] = np.tanh(((((((((2.0) + ((((((((-3.0) + (data[\"s266\"]))/2.0)) / 2.0)) * 2.0)))/2.0)) + (((data[\"s144\"]) / 2.0)))/2.0)) + (data[\"s272\"]))) \n    v[\"s56\"] = np.tanh(((data[\"s160\"]) - ((((-1.0*(((((((data[\"s175\"]) * ((-1.0*((data[\"s124\"])))))) + (data[\"s169\"]))/2.0))))) / 2.0)))) \n    v[\"s57\"] = np.tanh(np.where(np.minimum(((data[\"s194\"])), ((data[\"s194\"])))>0, (((1.0) < (((data[\"s170\"]) * (3.0))))*1.), data[\"s176\"] )) \n    v[\"s58\"] = np.tanh(((np.where((((data[\"s111\"]) + (data[\"s83\"]))/2.0)>0, ((data[\"s115\"]) * ((((data[\"s111\"]) + (data[\"s115\"]))/2.0))), ((((data[\"s111\"]) + (data[\"s83\"]))) * 2.0) )) * 2.0)) \n    v[\"s59\"] = np.tanh(((3.0) * (-3.0))) \n    v[\"s60\"] = np.tanh(((2.0) * (np.minimum(((((data[\"s106\"]) * (data[\"s111\"])))), ((np.where((((data[\"s111\"]) > (data[\"s38\"]))*1.)>0, 2.0, (-1.0*((data[\"s98\"]))) ))))))) \n    v[\"s61\"] = np.tanh((((data[\"s15\"]) > (((((((-1.0*(((((((np.tanh(((1.54698407649993896)))) + (2.0))) < (-2.0))*1.))))) < (data[\"s68\"]))*1.)) / 2.0)))*1.)) \n    v[\"s62\"] = np.tanh(((data[\"s235\"]) + ((((data[\"s271\"]) > (-1.0))*1.)))) \n    v[\"s63\"] = np.tanh(((((((data[\"s116\"]) - (data[\"s116\"]))) - (data[\"s116\"]))) - (((data[\"s139\"]) * (data[\"s145\"]))))) \n    return v.sum(axis=1).values*.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6adfc6a901ecc7d5505a0cd107f01bbf69b5e403"},"cell_type":"code","source":"x1 = GPClusterX(traindf)\nx2 = GPClusterY(traindf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"849e870147d0aa26797de912a3b97695007db15e","collapsed":true},"cell_type":"code","source":"gpdata = np.hstack([x1.reshape(-1,1),x2.reshape(-1,1),y_train_log.reshape(-1,1)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2de6598978b4e4cdd4cd5349e4dff9b789fd78a5"},"cell_type":"code","source":"cm = plt.cm.get_cmap('RdYlBu')\nfig, axes = plt.subplots(1, 1, figsize=(15, 15))\nsc = axes.scatter(gpdata[:,0], gpdata[:,1], alpha=.5, c=(gpdata[:,2]), cmap=cm, s=30)\ncbar = fig.colorbar(sc, ax=axes)\ncbar.set_label('Log1p(target)')\n_ = axes.set_title(\"Clustering colored by target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49e560d163af9c76feffe51bddf692b922c19322","collapsed":true},"cell_type":"code","source":"folds = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(gpdata)):\n    trn_x, trn_y = gpdata[trn_idx,0:2], gpdata[trn_idx,-1]\n    val_x, val_y = gpdata[val_idx,0:2], gpdata[val_idx,-1]\n    \n       \n    clf = KNeighborsRegressor(n_neighbors=100)\n    clf.fit(trn_x,trn_y)\n    score = np.sqrt(mean_squared_error(val_y,clf.predict(val_x)))\n    print('Fold:', n_fold,score)\n    scores.append(score)\n    del clf, trn_x, trn_y, val_x, val_y\n    gc.collect()\nprint('Mean Score:',np.mean(scores))\nprint('Std Score:',np.std(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d7a7b40eb9da085be3af6e9926f9b3713695e121"},"cell_type":"code","source":"x1 = GPClusterX(traindf)\nx2 = GPClusterY(traindf)\ngptraindata = np.hstack([x1.reshape(-1,1),x2.reshape(-1,1)])\nx1 = GPClusterX(testdf)\nx2 = GPClusterY(testdf)\ngptestdata = np.hstack([x1.reshape(-1,1),x2.reshape(-1,1)])\n\ntest_preds = np.zeros(testdf.shape[0])\ntraindata = np.hstack([traindf.values,gptraindata])\ntestdata = np.hstack([testdf.values,gptestdata])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70b95ebb56c36a4924a6c8921d2d3fe2349db785"},"cell_type":"code","source":"xgb_params = {\n        'n_estimators': 360,\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.02,\n        'max_depth': 5,\n        'min_child_weight': 100,#57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456\n    }\n\nfit_params = {\n    'eval_metric': 'rmse',\n    'verbose': False\n}\n\nclf = xgb.XGBRegressor(**xgb_params).fit(\n                traindata, y_train_log,\n                **fit_params\n            )\n       \nclf.fit(traindata,y_train_log)\nscore = np.sqrt(mean_squared_error(y_train_log,clf.predict(traindata)))\nprint(score)\ntest_preds = clf.predict(testdata)\n\nsubmission = pd.DataFrame()\nsubmission['ID'] = id_test\nsubmission['target'] = np.expm1(test_preds)\nsubmission.to_csv('sergeipluscluster.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}