{"cells":[{"metadata":{"_uuid":"9027e4b0f8ddc737e1b9560eba4a4d97ef9ffcc6"},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Credit Card Fraud Detection Predictive Models</font></center></h1>\n\n\n<img src=\"https://kaggle2.blob.core.windows.net/datasets-images/310/684/3503c6c827ca269cc00ffa66f2a9c207/dataset-card.jpg\" width=\"400\"></img>\n\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Load packages</a>  \n- <a href='#3'>Read the data</a>  \n- <a href='#4'>Check the data</a>  \n    - <a href='#41'>Glimpse the data</a>  \n    - <a href='#42'>Check missing data</a>\n    - <a href='#43'>Check data unbalance</a>\n- <a href='#5'>Data exploration</a>\n- <a href='#6'>Predictive models</a>  \n    - <a href='#61'>RandomForrestClassifier</a> \n    - <a href='#62'>AdaBoostClassifier</a>     \n    - <a href='#63'>CatBoostClassifier</a> \n    - <a href='#64'>XGBoost</a> \n    - <a href='#65'>LightGBM</a> \n- <a href='#7'>Conclusions</a>\n- <a href='#8'>References</a>\n"},{"metadata":{"_uuid":"0b64f006df02ccee229320fef8063dc5a2933c1a"},"cell_type":"markdown","source":"# <a id=\"1\">Introduction</a>  \n\nThe datasets contains transactions made by credit cards in **September 2013** by european cardholders. This dataset presents transactions that occurred in two days, where we have **492 frauds** out of **284,807 transactions**. The dataset is **highly unbalanced**, the **positive class (frauds)** account for **0.172%** of all transactions.  \n\nIt contains only numerical input variables which are the result of a **PCA transformation**.   \n\nDue to confidentiality issues, there are not provided the original features and more background information about the data.  \n\n* Features **V1**, **V2**, ... **V28** are the **principal components** obtained with **PCA**;  \n* The only features which have not been transformed with PCA are **Time** and **Amount**. Feature **Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature **Amount** is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.   \n* Feature **Class** is the response variable and it takes value **1** in case of fraud and **0** otherwise.  \n\n"},{"metadata":{"_uuid":"e94b258fe0c91fd5bb76107c509fc896db021f2b"},"cell_type":"markdown","source":"# <a id=\"2\">Load packages</a>"},{"metadata":{"trusted":true,"_uuid":"0f34e1e378b762166698b0136c57048a45e7858c"},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN/VALIDATION/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\n\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"../input/credit-card-fraud-detection\"\nelse:\n    PATH=\"../input\"\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eb142be459775c38046265e3b27ccbfd63ed0f6"},"cell_type":"markdown","source":"# <a id=\"3\">Read the data</a>"},{"metadata":{"trusted":true,"_uuid":"42e4a9f9b53f6d46cabd00b64a0e9f52b7abcd82"},"cell_type":"code","source":"data_df = pd.read_csv(PATH+\"/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0148d5c242773fbe8a10ae54873b6e59b083398e"},"cell_type":"markdown","source":"# <a id=\"4\">Check the data</a>"},{"metadata":{"trusted":true,"_uuid":"ed93a290bcf10bcf817dc9489a308d489c0cfd40"},"cell_type":"code","source":"print(\"Credit Card Fraud Detection data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91c1d7ca1b2e1a15d2dd83c3478df667d6ba6ace"},"cell_type":"markdown","source":"## <a id=\"41\">Glimpse the data</a>\n\nWe start by looking to the data features (first 5 rows)."},{"metadata":{"trusted":true,"_uuid":"c426639fdeb77bea60513724c6c438a5e8a02f8a"},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df10bc43389fb9e8159150a566bd52abd0dd2839"},"cell_type":"markdown","source":"Let's look into more details to the data."},{"metadata":{"trusted":true,"_uuid":"8705f721907820bfc8c720f2f5ac5e0f9f28fa42"},"cell_type":"code","source":"data_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf88e4180834a1bf7e76a583328094926fade824"},"cell_type":"markdown","source":"Looking to the **Time** feature, we can confirm that the data contains **284,807** transactions, during 2 consecutive days (or **172792** seconds)."},{"metadata":{"_uuid":"39299ca7dc0324113fcd854530cbe7a945cc94bb"},"cell_type":"markdown","source":"## <a id=\"42\">Check missing data</a>  \n\nLet's check if there is any missing data."},{"metadata":{"trusted":true,"_uuid":"51940c5cfb59bb4720d70157c310a339ce483eb6"},"cell_type":"code","source":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa67814319f2b87367a7a675d956f1aa8f806363"},"cell_type":"markdown","source":"There is no missing data in the entire dataset."},{"metadata":{"_uuid":"5c17d77aaa693057767dc2a0a23efbcae8f9f34e"},"cell_type":"markdown","source":"## <a id=\"43\">Data unbalance</a>"},{"metadata":{"_uuid":"dc1ecb4caa03723aa3e3153fe1311ed532464b54"},"cell_type":"markdown","source":"Let's check data unbalance with respect with *target* value, i.e. **Class**."},{"metadata":{"trusted":true,"_uuid":"c07847fe84817c98455db7ad7e49c5d991ac58a9"},"cell_type":"code","source":"temp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='class')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77eddeb05f44c47ee4a1be98bb22bb3cb0345030"},"cell_type":"markdown","source":"Only **492** (or **0.172%**) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable **Class**."},{"metadata":{"_uuid":"88fc9ad3d2dde75457f3b8c5417e07cbd9564a30"},"cell_type":"markdown","source":"# <a id=\"5\">Data exploration</a>"},{"metadata":{"_uuid":"b3c42863cec8d34e87dc098b85b3c709ec2739c0"},"cell_type":"markdown","source":"## Transactions in time"},{"metadata":{"trusted":true,"_uuid":"b9521e8cc9f179887c5dc454ed84d0ba20e55946"},"cell_type":"code","source":"class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\nclass_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n#plt.figure(figsize = (14,4))\n#plt.title('Credit Card Transactions Time Density Plot')\n#sns.set_color_codes(\"pastel\")\n#sns.distplot(class_0,kde=True,bins=480)\n#sns.distplot(class_1,kde=True,bins=480)\n#plt.show()\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64cd713602c33ce4491492088279f26adc209674"},"cell_type":"markdown","source":"Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone."},{"metadata":{"_uuid":"c4393be5698f192c6839e8d372b453ed247d40d5"},"cell_type":"markdown","source":"## Transactions amount"},{"metadata":{"trusted":true,"_uuid":"639b6fec6066d52f91c6766ef372df39a39c2fe1"},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a797b1b01b66a3171197cbfc1a1a37bf36b0d57"},"cell_type":"code","source":"tmp = data_df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90705320eb4f147ec29f13620e010501217b981d"},"cell_type":"code","source":"class_1.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6aedd49dc678f1a022ddc98901b0a53758bc7f6"},"cell_type":"markdown","source":"The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n\nLet's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days)."},{"metadata":{"trusted":true,"_uuid":"29575f5187154024c911642bd329289199253a90"},"cell_type":"code","source":"fraud = data_df.loc[data_df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],\n    name=\"Amount\",\n     marker=dict(\n                color='rgb(238,23,11)',\n                line=dict(\n                    color='red',\n                    width=1),\n                opacity=0.5,\n            ),\n    text= fraud['Amount'],\n    mode = \"markers\"\n)\ndata = [trace]\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='fraud-amount')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f144ed18c23667fde41f190c060a8bd4990b7be"},"cell_type":"markdown","source":"## Features correlation"},{"metadata":{"trusted":true,"_uuid":"2a35594b8bf2ec37febec7570d4d3a25872b648b"},"cell_type":"code","source":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8db97792d1f815582f037455e9ead600f8c9433c"},"cell_type":"markdown","source":"As expected, there is no notable correlation between features **V1**-**V28**. There are certain correlations between some of these features and **Time** (inverse correlation with **V3**) and **Amount** (direct correlation with **V7** and **V20**, inverse correlation with **V1** and **V5**).\n\n\nLet's plot the correlated and inverse correlated values on the same graph.\n\nLet's start with the direct correlated values: {V20;Amount} and {V7;Amount}."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fd8f8bd03486fe3f2d7c89d00a5c0575a153d969"},"cell_type":"code","source":"s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7430c8631e2a7b34b7d4b0dabf2fb006f88d6f8a"},"cell_type":"markdown","source":"We can confirm that the two couples of features are correlated (the regression lines for **Class = 0** have a positive slope, whilst the regression line for **Class = 1** have a smaller positive slope).\n\nLet's plot now the inverse correlated values."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4ae4211f20ba8db85b7f8f61ade2d24788be9aa4"},"cell_type":"code","source":"s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5e0440e0cd0c950190fffc539ddeb63de13b7fb"},"cell_type":"markdown","source":"We can confirm that the two couples of features are inverse correlated (the regression lines for **Class = 0** have a negative slope while the regression lines for **Class = 1** have a very small negative slope).\n"},{"metadata":{"_uuid":"5bdb7deecd32fd919f5431e8b5b02b6392ccd5bc"},"cell_type":"markdown","source":"## Features density plot"},{"metadata":{"trusted":true,"_uuid":"b041d9877768c687ab4d2b314dbef5f4f3738176"},"cell_type":"code","source":"var = data_df.columns.values\n\ni = 0\nt0 = data_df.loc[data_df['Class'] == 0]\nt1 = data_df.loc[data_df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae5a78aaff7b694c209af05a0d12cbfd131000b7"},"cell_type":"markdown","source":"For some of the features we can observe a good selectivity in terms of distribution for the two values of **Class**: **V4**, **V11** have clearly separated distributions for **Class** values 0 and 1, **V12**, **V14**, **V18** are partially separated, **V1**, **V2**, **V3**, **V10** have a quite distinct profile, whilst **V25**, **V26**, **V28** have similar profiles for the two values of **Class**.  \n\nIn general, with just few exceptions (**Time** and **Amount**), the features distribution for legitimate transactions (values of **Class = 0**)  is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of **Class = 1**) have a skewed (asymmetric) distribution."},{"metadata":{"_uuid":"86636e53408b574ac5f7c84d73a5d8cff5c6e23f"},"cell_type":"markdown","source":"# <a id=\"6\">Predictive models</a>  \n\n"},{"metadata":{"_uuid":"3de9724f89804b1e82c2b30c9767bbc0be6ac0dc"},"cell_type":"markdown","source":"### Define predictors and target values\n\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature."},{"metadata":{"trusted":true,"_uuid":"05466e1862ba4f872dc05faa4b4dc84430660639"},"cell_type":"code","source":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"574f8a166087bc8e95f27a9b152ffc4c1f83c4ce"},"cell_type":"markdown","source":"### Split data in train, test and validation set\n\nLet's define train, validation and test sets."},{"metadata":{"trusted":true,"_uuid":"c27f7166f9a0d96a0a7135df30ae5201d5a2bb3a"},"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"477275d990dc322046f8d16a0c0d6b942acaac4b"},"cell_type":"markdown","source":"Let's start with a RandomForrestClassifier <a href='#8'>[3]</a>   model."},{"metadata":{"_uuid":"046490acb18502c24c8197a4f2d1833f71ce7366"},"cell_type":"markdown","source":"## <a id=\"61\">RandomForestClassifier</a>\n\n\n### Define model parameters\n\nLet's set the parameters for the model."},{"metadata":{"_uuid":"13b7a22599e96474dbb275bc6658bcf87a92b774"},"cell_type":"markdown","source":"Let's run a model using the training set for training. Then, we will use the validation set for validation. \n\nWe will use as validation criterion **GINI**, which formula is **GINI = 2 * (AUC) - 1**, where **AUC** is the **Receiver Operating Characteristic - Area Under Curve (ROC-AUC)** <a href='#8'>[4]</a>.  Number of estimators is set to **100** and number of parallel jobs is set to **4**.\n\nWe start by initializing the RandomForestClassifier."},{"metadata":{"trusted":true,"_uuid":"88cfc485083044018748f977b8e9cb13bf91a4a8"},"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfcf4074dfe96da0ac2489303ab3d7d8899f4175"},"cell_type":"markdown","source":"Let's train the **RandonForestClassifier** using the **train_df** data and **fit** function."},{"metadata":{"trusted":true,"_uuid":"5042efd869d672392cb81437a03719d07ae684f1"},"cell_type":"code","source":"clf.fit(train_df[predictors], train_df[target].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa2d70ec9136bdf46339fb3ad408f1179015a214"},"cell_type":"markdown","source":"Let's now predict the **target** values for the **valid_df** data, using **predict** function."},{"metadata":{"trusted":true,"_uuid":"976e7e8d9adec49369b81c8a14e4e676e15709c9"},"cell_type":"code","source":"preds = clf.predict(valid_df[predictors])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2e26b3c0dc5498196784292bb7c71337291fd15"},"cell_type":"markdown","source":"Let's also visualize the features importance.\n\n### Features importance"},{"metadata":{"trusted":true,"_uuid":"149ae586de1a7318f4063f2497d16b6457c5f478"},"cell_type":"code","source":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"588e3a996a7108a4f1e66cda53d225e6e236bfd6"},"cell_type":"markdown","source":"The most important features are **V17**, **V12**, **V14**, **V10**, **V11**, **V16**.\n\n\n### Confusion matrix\n\nLet's show a confusion matrix for the results we obtained. "},{"metadata":{"trusted":true,"_uuid":"acdc37e14e96287ea2f96c4530bfde388d8af2f2"},"cell_type":"code","source":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5795d41b1b0922db8121601d3b5e5702043efd50"},"cell_type":"markdown","source":"### Type I error and Type II error\n\nWe need to clarify that confussion matrix are not a very good tool to represent the results in the case of largely unbalanced data, because we will actually need a different metrics that accounts in the same time for the **selectivity** and **specificity** of the method we are using, so that we minimize in the same time both **Type I errors** and **Type II errors**.\n\n\n**Null Hypothesis** (**H0**) - The transaction is not a fraud.  \n**Alternative Hypothesis** (**H1**) - The transaction is a fraud.  \n\n**Type I error** - You reject the null hypothesis when the null hypothesis is actually true.  \n**Type II error** - You fail to reject the null hypothesis when the the alternative hypothesis is true.  \n\n**Cost of Type I error** - You erroneously presume that the the transaction is a fraud, and a true transaction is rejected.  \n**Cost of Type II error** - You erroneously presume that the transaction is not a fraud and a ffraudulent transaction is accepted.  \n\nThe following image explains what **Type I error** and **Type II error** are:    \n\n\n<img src=\"https://i.stack.imgur.com/x1GQ1.png\" width=\"600\"/>\n\nAnd this alternative image explains even better:  \n\n<img src=\"https://i2.wp.com/flowingdata.com/wp-content/uploads/2014/05/Type-I-and-II-errors1.jpg\" width=\"600\"/>\n\n\n\nLet's calculate the ROC-AUC score <a href='#8'>[4]</a>.\n\n### Area under curve"},{"metadata":{"trusted":true,"_uuid":"b67d0d3a695ccd69fba3b9ea576e7019d6091257"},"cell_type":"code","source":"roc_auc_score(valid_df[target].values, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b6d28b9ac4316545572d91b351f2c15bed671ba"},"cell_type":"markdown","source":"The **ROC-AUC** score obtained with **RandomForrestClassifier** is **0.85**.\n\n\n\n"},{"metadata":{"_uuid":"8e1fb8633dc75828f659b6d4d6b3e61d87379cb7"},"cell_type":"markdown","source":"## <a id=\"62\">AdaBoostClassifier</a>\n\n\nAdaBoostClassifier stands for Adaptive Boosting Classifier <a href='#8'>[5]</a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model."},{"metadata":{"trusted":true,"_uuid":"17c5211f4f1f5f4bb406ab17d45abdc5b46c087f"},"cell_type":"code","source":"clf = AdaBoostClassifier(random_state=RANDOM_STATE,\n                         algorithm='SAMME.R',\n                         learning_rate=0.8,\n                             n_estimators=NUM_ESTIMATORS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24263fd86b09447c3c413bf438cb262823daf338"},"cell_type":"markdown","source":"### Fit the model\n\nLet's fit the model."},{"metadata":{"trusted":true,"_uuid":"1bdbe7616f0c2ab48932705e0a143c6a0a9b8049"},"cell_type":"code","source":"clf.fit(train_df[predictors], train_df[target].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0678f70c1bce4a0aca24de10b2c7130a702a3823"},"cell_type":"markdown","source":"### Predict the target values\n\nLet's now predict the **target** values for the **valid_df** data, using predict function."},{"metadata":{"trusted":true,"_uuid":"77ddd7b77838e7b83d6a24fa6d6449ae0567b2bf"},"cell_type":"code","source":"preds = clf.predict(valid_df[predictors])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4eea5368cc69c6b499aa5c527d33f13ede9e31c0"},"cell_type":"markdown","source":"### Features importance\n\nLet's see also the features importance."},{"metadata":{"trusted":true,"_uuid":"2efbe2f5f9554fa96788ad1a619fe1943ee1e6df"},"cell_type":"code","source":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e4a72f967d3e0d1e7051668178eb497555a16ca"},"cell_type":"markdown","source":"### Confusion matrix\n\nLet's visualize the confusion matrix."},{"metadata":{"trusted":true,"_uuid":"0adcb109549fc61381f8dcf6fd894b83f3a69809"},"cell_type":"code","source":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29a1e7c5d5b056fc59ef0c08f8a1091ee2f6fdff"},"cell_type":"markdown","source":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve"},{"metadata":{"trusted":true,"_uuid":"f95a9ced1ff6ed2257f1e21c604a2ffcf769feed"},"cell_type":"code","source":"roc_auc_score(valid_df[target].values, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"525a893a4f511b3fd8909f3655c9a5c20afe585c"},"cell_type":"markdown","source":"The ROC-AUC score obtained with AdaBoostClassifier is **0.83**."},{"metadata":{"_uuid":"719e7673307eb938cbd22422c11165e6433da036"},"cell_type":"markdown","source":"## <a id=\"63\">CatBoostClassifier</a>\n\n\nCatBoostClassifier is a gradient boosting for decision trees algorithm with support for handling categorical data <a href='#8'>[6]</a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model."},{"metadata":{"trusted":true,"_uuid":"5a663cefd0b35e44bec74dda1e3ece37f430bc69"},"cell_type":"code","source":"clf = CatBoostClassifier(iterations=500,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='AUC',\n                             random_seed = RANDOM_STATE,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = VERBOSE_EVAL,\n                             od_wait=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f571a8ac45728504d805e1b4b12e3e672a01d381"},"cell_type":"code","source":"clf.fit(train_df[predictors], train_df[target].values,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fb5dfcb26fbdffb6e4526cf24bbd9a20ea2b0cf"},"cell_type":"markdown","source":"### Predict the target values\n\nLet's now predict the **target** values for the **val_df** data, using predict function."},{"metadata":{"trusted":true,"_uuid":"5b1c1fbaf25f4d16c6fc3ddcf3a25a16181fea3b"},"cell_type":"code","source":"preds = clf.predict(valid_df[predictors])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4d5e197719c2caa46451c804d616e8fd6a0d6fb"},"cell_type":"markdown","source":"### Features importance\n\nLet's see also the features importance."},{"metadata":{"trusted":true,"_uuid":"afeee803fd3273651754c68619a00250e1cd524d"},"cell_type":"code","source":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7500dac8e4d829b2651b027dac78ea886ec95ca6"},"cell_type":"markdown","source":"### Confusion matrix\n\nLet's visualize the confusion matrix."},{"metadata":{"trusted":true,"_uuid":"39ac2f75b7317e00a688d4d6872a1671e9d2da21"},"cell_type":"code","source":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f9e2cec59953df1c4a5fc83333443484929dac"},"cell_type":"markdown","source":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve"},{"metadata":{"trusted":true,"_uuid":"701ff6a7cd11cd6556e50c9c073cbddf80555306"},"cell_type":"code","source":"roc_auc_score(valid_df[target].values, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18a05c1761c673b0838dd4d4a71b2315616737e2"},"cell_type":"markdown","source":"The ROC-AUC score obtained with CatBoostClassifier is **0.86**."},{"metadata":{"_uuid":"39c96e378c957d251f4d7e62a4def9e71aabfd81"},"cell_type":"markdown","source":"## <a id=\"63\">XGBoost</a>"},{"metadata":{"_uuid":"eaad6d74ea593d3a71bedf4252ba0bb5c1617159"},"cell_type":"markdown","source":"XGBoost is a gradient boosting algorithm <a href='#8'>[7]</a>.\n\nLet's prepare the model."},{"metadata":{"_uuid":"fa57161424d5de2cb482848c6104b8d51901fa02"},"cell_type":"markdown","source":"### Prepare the model\n\nWe initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning."},{"metadata":{"trusted":true,"_uuid":"eeb48986db87e26413d3a5efa286d18e82c0efc6"},"cell_type":"code","source":"# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\ndtest = xgb.DMatrix(test_df[predictors], test_df[target].values)\n\n#What to monitor (in this case, **train** and **valid**)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a6981b4ce504d3e836e19dd262c64b4b03e591a"},"cell_type":"markdown","source":"### Train the model\n\nLet's train the model. "},{"metadata":{"trusted":true,"_uuid":"5d066b1791e0d2463897b8a0c797cfbc38afa846"},"cell_type":"code","source":"model = xgb.train(params, \n                dtrain, \n                MAX_ROUNDS, \n                watchlist, \n                early_stopping_rounds=EARLY_STOP, \n                maximize=True, \n                verbose_eval=VERBOSE_EVAL)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb42148758c28251be27ce20828864b1e4df1b59"},"cell_type":"markdown","source":"The best validation score (ROC-AUC) was **0.984**, for round **241**."},{"metadata":{"_uuid":"ca7375264c18646c11c399fdb1aa5dfaece44bd5"},"cell_type":"markdown","source":"### Plot variable importance"},{"metadata":{"trusted":true,"_uuid":"2592eeaa312ba24ead2fea4cbd7b71d4537f4a75"},"cell_type":"code","source":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb583b48bd0ffd38cddf4ea1eb41df20b3f5d566"},"cell_type":"markdown","source":"### Predict test set\n\n\nWe used the train and validation sets for training and validation. We will use the trained model now to predict the target value for the test set."},{"metadata":{"trusted":true,"_uuid":"99b130c043241ecc0ea5c64df1a92940d6c458a8"},"cell_type":"code","source":"preds = model.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eca8c0e71d679a8e2743ce20cd66934f101a0ac"},"cell_type":"markdown","source":"### Area under curve\n\nLet's calculate ROC-AUC."},{"metadata":{"trusted":true,"_uuid":"bb7ddd8e0f7fc14556495aaab62065e9cad7a2d3"},"cell_type":"code","source":"roc_auc_score(test_df[target].values, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61683181c29747a17162ad3d0ef871949c22a4c5"},"cell_type":"markdown","source":"The AUC score for the prediction of fresh data (test set) is **0.974**."},{"metadata":{"_uuid":"a49d1d15412ebde99ce816eef35dbcde02549388"},"cell_type":"markdown","source":"## <a id=\"64\">LightGBM</a>\n\n\nLet's continue with another gradient boosting algorithm, LightGBM <a href='#8'>[8]</a> <a href='#8'>[9]</a>.\n\n\n### Define model parameters\n\nLet's set the parameters for the model. We will use these parameters only for the first lgb model."},{"metadata":{"trusted":true,"_uuid":"17cb571126b6a176a016e7e962441a1fbefc4e1c"},"cell_type":"code","source":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':150, # because training data is extremely unbalanced \n         }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f28ad6e71f428ab8686ef5e12fcf18875cc34ba"},"cell_type":"markdown","source":"### Prepare the model\n\nLet's prepare the model, creating the **Dataset**s data structures from the train and validation sets."},{"metadata":{"trusted":true,"_uuid":"182aad358dc1359111ff8704d25b7449c0fa5901"},"cell_type":"code","source":"dtrain = lgb.Dataset(train_df[predictors].values, \n                     label=train_df[target].values,\n                     feature_name=predictors)\n\ndvalid = lgb.Dataset(valid_df[predictors].values,\n                     label=valid_df[target].values,\n                     feature_name=predictors)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"589c9f7f08f158fc2fbb77ef0a5839a90fda82de"},"cell_type":"markdown","source":"### Run the model\n\nLet's run the model, using the **train** function."},{"metadata":{"trusted":true,"_uuid":"e0590673dd712c4070133451ef8b9f8f4bf07712"},"cell_type":"code","source":"evals_results = {}\n\nmodel = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=2*EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f90fd70986e7e55da136386d249c8dd9f7bcfba9"},"cell_type":"markdown","source":"Best validation score  was obtained for round **85**, for which **AUC ~= 0.974**.\n\nLet's plot variable importance."},{"metadata":{"trusted":true,"_uuid":"cc63244fa4d424f2680d60722c6ce3343f63f162"},"cell_type":"code","source":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nlgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61f871e6e32a14c21f0722df0be376e6726222e2"},"cell_type":"markdown","source":"Let's predict now the target for the test data.\n\n### Predict test data"},{"metadata":{"trusted":true,"_uuid":"76814af125071b56398f76511b1a88b7830300f2"},"cell_type":"code","source":"preds = model.predict(test_df[predictors])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1b3afefa6f55801c55535e4a0e84d83be7d158d"},"cell_type":"markdown","source":"### Area under curve\n\nLet's calculate the ROC-AUC score for the prediction."},{"metadata":{"trusted":true,"_uuid":"c95c74cecb7a9facd24e11d763728e488c364035"},"cell_type":"code","source":"roc_auc_score(test_df[target].values, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e4664a3177f437bd72f58149ad3e0b970d870d2"},"cell_type":"markdown","source":"The ROC-AUC score obtained for the test set is **0.946**."},{"metadata":{"_uuid":"46283d37fd7eb33b7a93447a9809f384b2fd7d13"},"cell_type":"markdown","source":"### Training and validation using cross-validation\n\nLet's use now cross-validation. We will use cross-validation (KFolds) with 5 folds. Data is divided in 5 folds and, by rotation, we are training using 4 folds (n-1) and validate using the 5th (nth) fold.\n\nTest set is calculated as an average of the predictions "},{"metadata":{"trusted":true,"_uuid":"9b277ba40e30206134a6796eeaf979fc2f57274e"},"cell_type":"code","source":"kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\ntest_preds = np.zeros(test_df.shape[0])\nfeature_importance_df = pd.DataFrame()\nn_fold = 0\nfor train_idx, valid_idx in kf.split(train_df):\n    train_x, train_y = train_df[predictors].iloc[train_idx],train_df[target].iloc[train_idx]\n    valid_x, valid_y = train_df[predictors].iloc[valid_idx],train_df[target].iloc[valid_idx]\n    \n    evals_results = {}\n    model =  LGBMClassifier(\n                  nthread=-1,\n                  n_estimators=2000,\n                  learning_rate=0.01,\n                  num_leaves=80,\n                  colsample_bytree=0.98,\n                  subsample=0.78,\n                  reg_alpha=0.04,\n                  reg_lambda=0.073,\n                  subsample_for_bin=50,\n                  boosting_type='gbdt',\n                  is_unbalance=False,\n                  min_split_gain=0.025,\n                  min_child_weight=40,\n                  min_child_samples=510,\n                  objective='binary',\n                  metric='auc',\n                  silent=-1,\n                  verbose=-1,\n                  feval=None)\n    model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                eval_metric= 'auc', verbose= VERBOSE_EVAL, early_stopping_rounds= EARLY_STOP)\n    \n    oof_preds[valid_idx] = model.predict_proba(valid_x, num_iteration=model.best_iteration_)[:, 1]\n    test_preds += model.predict_proba(test_df[predictors], num_iteration=model.best_iteration_)[:, 1] / kf.n_splits\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = predictors\n    fold_importance_df[\"importance\"] = clf.feature_importances_\n    fold_importance_df[\"fold\"] = n_fold + 1\n    \n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n    del model, train_x, train_y, valid_x, valid_y\n    gc.collect()\n    n_fold = n_fold + 1\ntrain_auc_score = roc_auc_score(train_df[target], oof_preds)\nprint('Full AUC score %.6f' % train_auc_score)                                    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5c6d3d5c436367802d4a5f32def281767b439ae"},"cell_type":"markdown","source":"The AUC score for the prediction from the test data was 0.93.\n\nWe prepare the test prediction, from the averaged predictions for test over the 5 folds."},{"metadata":{"_uuid":"e2a1e8c82317fd667ba9c159456602a42599e784","trusted":true},"cell_type":"code","source":"pred = test_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c281bff3a66a72421dd66976494080839fdead29"},"cell_type":"markdown","source":"# <a id=\"7\">Conclusions</a>"},{"metadata":{"_uuid":"ac803cb42c6211a6b96b1f30075b82092a50e5d2"},"cell_type":"markdown","source":"We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. \nWe then investigated two predictive models. The data was split in 3 parts, a train set, a validation set and a test set. For the first three models, we only used the train and test set.  \n\nWe started with **RandomForrestClassifier**, for which we obtained an AUC scode of **0.85** when predicting the target for the test set.  \n\nWe followed with an **AdaBoostClassifier** model, with lower AUC score (**0.83**) for prediction of the test set target values.    \n\nWe then followed with an **CatBoostClassifier**, with the AUC score after training 500 iterations **0.86**.    \n\nWe then experimented with a **XGBoost** model. In this case, se used the validation set for validation of the training model.  The best validation score obtained was   **0.984**. Then we used the model with the best training step, to predict target value from the test data; the AUC score obtained was **0.974**.\n\nWe then presented the data to a **LightGBM** model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With the first method we obtained values of AUC for the validation set around **0.974**. For the test set, the score obtained was **0.946**.   \nWith the cross-validation, we obtained an AUC score for the test prediction of  **0.93**."},{"metadata":{"_uuid":"04ae7944c90114df8e14e6f53f5884d72dc7a304"},"cell_type":"markdown","source":"# <a id=\"8\">References</a>\n\n[1] Credit Card Fraud Detection Database, Anonymized credit card transactions labeled as fraudulent or genuine, https://www.kaggle.com/mlg-ulb/creditcardfraud  \n[2] Principal Component Analysis, Wikipedia Page, https://en.wikipedia.org/wiki/Principal_component_analysis  \n[3] RandomForrestClassifier, http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html  \n[4] ROC-AUC characteristic, https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve   \n[5] AdaBoostClassifier, http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html  \n[6] CatBoostClassifier, https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboostclassifier-docpage/  \n[7] XGBoost Python API Reference, http://xgboost.readthedocs.io/en/latest/python/python_api.html  \n[8] LightGBM Python implementation, https://github.com/Microsoft/LightGBM/tree/master/python-package  \n[9] LightGBM algorithm, https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/lightgbm.pdf   \n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}