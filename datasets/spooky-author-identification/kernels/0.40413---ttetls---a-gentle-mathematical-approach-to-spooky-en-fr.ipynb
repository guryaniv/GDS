{"cells": [{"execution_count": null, "outputs": [], "source": ["import numpy as np # linear algebra \n", "from math import log, sqrt # neperian logarithm, square root\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "import nltk\n", "\n", "\n", "## Read the train\n", "train_df = pd.read_csv(\"../input/train.csv\")\n", "\n", "authors = {'EAP':0,'HPL':1,'MWS':2}\n", "authors_rev = ['EAP','HPL','MWS']"], "cell_type": "code", "metadata": {"_uuid": "d5f3f98e4f2d3a6051fc3c7fe90b9f9da2d0b854", "_kg_hide-input": false, "_cell_guid": "f340aa47-08c8-4085-a2f7-44afb2dc46ec", "collapsed": true, "_kg_hide-output": false, "scrolled": true}}, {"metadata": {"_uuid": "ec65c3f51f4f402245bea8877d5e3c258fcf1503", "_cell_guid": "7e307dfd-0549-429c-8f0d-74a78627f92b"}, "source": ["# A step-by-step mathematical approach to Spooky | [English]/[French]\n", "\n", "It is often regretted that computer scientists are not always very rigorous.\n", "They manipulate mathematics ... but in their own way and sometimes make strange mishmash with!\n", "\n", "We aim to show that one can harmoniously combine mathematics &amp; computer science; that is to say, at the same time to make the computer science properly and the mathematics usefull!\n", "\n", "Here is the course of this tutorial:\n", "- we will begin with a short introduction about this area straddling the linguistics and the satistics which aims to identify the authors of texts (and so which is at the heart of our competition Soopky) : * ** the stylometry ** *\n", "- we will then think about Spooky's notation system: the \"log loss\" (or * cross-entropy *) notation: \n", "$-\\frac{1}{N}\\sum_{i = 1}^N\\sum_{j = 1}^3y_{ij}\\log(p_ {ij}).$\n", "Assuming that we already have the program that transforms a sentence into a vector of $ \\mathbb R ^ d $, we will see what are the quantities $p_{ij}^{opt}$ to return to minimize the expectation of the \"log loss\"\n", "- after, we will present a very simple algorithm: the k-nearest-neighbors (kNN) algorithm. This algorithm has the mathematical property of asymptotically returning (when $ N \\to \\infty $) the optimal probabilities $ p_ {ij} ^ {opt} $ that we are looking for.\n", "- finally, we'll show some examples of programs (with the code python!) to transform our sentences into vectors. At first, some common examples (length of sentences, length of words, presence of some typical words, etc ...) Then a more elaborate example which calculates the log-likelihood of a sentence for each of the authors from their vocabulary. Finally, we will propose an example of an even more elaborate kernel using the library \"nltk\": it allows to transform a sentence into a tree from its grammatical structure.\n", "- we will devote the last part to the visualization of the data. We will present two algorithms: that of the Principal Component Analysis (PCA) which consists of an orthogonal transformation and that of the \"t-SNE\" which decreases the dimension of the space $ \\mathbb R ^ d $ while seeking to report the distances of the original space in the new space.\n", "\n", "\n", "<ul>\n", "  <li> <a href=\"#aintroduction\"> Introduction to stylometry </a></li>\n", "  <li> <a href=\"#alogloss\"> What to look for to minimize log loss? </a></li>\n", "  <li> <a href=\"#akppv\"> The k-nearest-neighbors algorithm (which asymptotically minimizes log loss) </a></li>\n", "  <li> <a href=\"#aexemples\"> Some examples of features </a>\n", "      <ul style = \"list-style-type: circle\">\n", "      <li> <a href=\"#aexemples_courants\"> Classic examples of stylometry </a></li>\n", "      <li> <a href=\"#avoca_auteurs\"> Calculating log-likelihood for each author by vocabulary </a></li>\n", "      <li> <a href=\"#akernel\"> Kernel on the grammatical structure of sentences </a></li>\n", "      </ul>\n", "  </li>\n", "  <li> <a href=\"#avisualisation\"> Data Visualization </a>\n", "        <ul style = \"list-style-type: circle\">\n", "      <li> <a href=\"#aACP\"> Principal Component Analysis (PCA) </a> </li>\n", "      <li> <a href=\"#atSNE\"> t-SNE </a> </li>\n", "      </ul>\n", "  </li>\n", "   <li> <a href=\"#aresultats\"> Submission and results (score : 0.40)</a></li>\n", "</ul>\n", "\n", "**NB**: we ask the reader to kindly excuse the many faults of English disseminated throughout this tutorial; English is not my mother tongue.\n", "Moreover, the French speaking reader can find a French version of this tutorial at the end: <a href=\"#francais\">french version</a>."], "cell_type": "markdown"}, {"metadata": {"_uuid": "9566eaa53a1b65fd5cfa0ef14490e64092dca506", "_cell_guid": "6598308d-d3de-49b3-86ca-cb909c66bfff"}, "source": ["<div id=\"aintroduction\" />\n", "\n", "# Introduction to stylometry\n", "Kaggle offers us, with the help of a training corpus, to find the authors (who are three in number: Edgar A. Poe, Howard P. Lovecraft and Mary Shelley) of about twenty thousand sentences.\n", "\n", "What Kaggle asks us to do is named: *** stylometry ***.\n", "\n", "Stylometry is this *art* (taken in its Greek sense of *\u03c4\u03ad\u03c7\u03bd\u03b7*) at the crossroads of statistics and linguistics whose purpose is to produce from text an *information* (which can be treated statistically ) reporting on the *style* of these; that is to say, which would characterize - ideally, this means ... - at the same time its author, but also its kind, its time, etc ...\n", "More practically, these statistics generally relate to the vocabulary used (which can be refined by identifying the meaning in which the author uses such a word), the grammatical categories (nouns / pronouns, adjectives, adverbs, verbs ... at what time / mode?) as well as the grammatical structure of the sentences, punctuation, etc ...\n", "\n", "It must be understood that to make good stylometry, it is necessary to play in both ways:\n", "- that of linguistics by proposing criteria (features) that are most relevant to isolate the style of an author,\n", "- that of statistics by using adapted mathematical and computer tools.\n", "\n", "Throughout this tutorial, we'll try to take up these two aspects of stylometry by always being as rigorous as possible and justifying (as far as possible) everything we do.\n", "\n", "But first, let's look at Spooky's evaluation system:"], "cell_type": "markdown"}, {"metadata": {"_uuid": "89d908e75c2deddbd6e91a688e55ea723de3f971", "_cell_guid": "d3aacf3e-a51a-4cf6-9e5d-bdb3393d4a9f"}, "source": ["<div id=\"alogloss\" />\n", "# What to look for to minimize log loss?\n", "\n", "First we will see why the evaluation system \"logloss\" has been chosen and what are the mathematical values to look for to minimize it in expectation.\n", "From these values to look for, we will deduce a suitable algorithm (that is to say that we justify from a mathematical point of view) for the computation of the probabilities $ p_ {ij} $ to return. This is the agorithm of the k-nearest-neighbors.\n", "\n", "The notation system that has been chosen for Spooky is the \"log loss\" (or *cross entropy*):\n", "        \n", "$$ log loss = - \\frac{1}{N} \\sum_{\\substack {i \\in \\{1, ..., N \\} \\\\ j \\in \\{EAP, HPL, MWS \\}}} y_{ij} \\log (p_ {ij}) $$\n", "where $ p_ {ij} $ is the estimate: the probability according to our program that the text $ i $ is of the author $ j $, $ y_{ij} $ is the solution: it equals $ 1 $ when the text $ i $ is of the author $ j $, $ 0 $ otherwise.\n", "\n", "The goal is to minimize this log loss.\n", "\n", "We will now formalize a little bit the problem from a mathematical point of view in order to compute the expected log loss and to find the optimal values $ p_{ij}^{opt} $ to return to achieve the minimal expectation.\n", "\n", "Suppose we already have the first part of the program: the one that transforms a sentence into a computer-processable \"information\". In this case we will assume that this information is a vector of $ \\mathbb R ^ d $ and that they are all independently fetched according to the same law $ X $ admitting a compact $ f $ density. It is also assumed that this law $ \\mathcal L (X) $ can be broken down into two stages:\n", "\n", "- first, we choose the author of the sentence according to a law of \"Bernoulli with three issues\": 'EAP' for Edgar A. Poe, 'HPL' for Howard P. Lovecraft and 'MWS' for Mary Shelley. We note $ \\mathcal B (\\lambda_1, \\lambda_2, \\lambda_3) $ this law (with $ \\sum \\lambda_j = 1 $); $ \\lambda_j $ being the proportion of $ j $ author's sentences.\n", "- According to the author $ j $ which is chosen in the first step, our sentence is drawn according to a law of density $ f_j $ on $ \\mathbb R ^ d $.\n", "\n", "![shema_bernouilli](http://bourg-la-reine-echecs.fr/telechargements/shema_bernoulli_auteurs_corrige.png)\n", "\n", "So we have that: $ f = \\sum \\lambda_j f_j $.\n", "\n", "On the training corpus, we get:"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["nb_texts = len(train_df)\n", "lambdas = (train_df.author.value_counts()/nb_texts).to_dict()\n", "print(\"Lambdas :\",lambdas)"], "cell_type": "code", "metadata": {"_uuid": "6546b4287cf458361e78d8c453f72ecc50977f61", "collapsed": true, "_cell_guid": "08dc435d-f04b-4d61-be58-cb206b01fa7e"}}, {"metadata": {"_uuid": "ee5c5818e04ec7f7651301aef530e8c5cb015c04", "_cell_guid": "a240bb1c-bcf8-4346-b40f-be2400d2e94f"}, "source": ["- $ \\lambda_1 = 0.403493538995863 $\n", "- $ \\lambda_2 = 0.287808366106543 $\n", "- $ \\lambda_3 = 0.308698094897594 $\n", "\n", "The attentive reader may have noticed that the sample file \"sample\\_submission\" for each sentence precisely returns the probabilities $ (\\lambda_1, \\lambda_2, \\lambda_3) $ ... it's no coincidence !! It is the return that minimizes the \"log loss\" with the worst program: the one that does not distinguish anything and that sends all the sentences on the null vector ($ \\mathcal L (X) = \\delta_0 $ where $ \\delta_0 $ is the measure of \"dirac\" at the point $ 0 $).\n", "This follows from a classical result of information theory: if $ (\\lambda_j)_j $ and $ (p_j)_j $ are two discrete probability measures, then cross entropy:\n", "$$ H_\\lambda (p) = \\mathbb E [log loss] = - \\sum_j \\lambda_j \\log (p_j) $$\n", "is minimized when $ p_j = \\lambda_j $ for all $ j $.\n", "\n", "We will now generalize this result!\n", "\n", "Given a sentence transformed by our program into a $ x $ vector (according to the $ X $ law), the best solution (in the sense that it is the one that minimizes the expectation of the final \"log loss\" score) of Probabilities to return is the triplet:\n", "\n", "$$ p_1 ^ {opt} (x) = \\frac{\\lambda_1 f_1 (x)} {\\sum_j \\lambda_j f_j (x)}, \\ \\ \\ \\\n", "p_2 ^ {opt} (x) = \\frac{\\lambda_2 f_2 (x)}{\\sum_j \\lambda_j f_j (x)}, \\ \\ \\ \\\n", "p_3 ^ {opt} (x) = \\frac{\\lambda_3 f_3 (x)} {\\sum_j \\lambda_j f_j (x)}. $$\n", "\n", "Good! now that it has been said, there is more to roll up the sleeves!\n", "\n", "The expectation of the \"log loss\" rating is given in this case by the following formula:\n", "\n", "$$ \\mathbb E [log loss] = - \\sum_j {\\lambda_j \\int_ {\\mathbb R ^ d} \\log (p_j (x)) f_j (x) \\mathrm d x} $$\n", "where $ p_j (x) $ is the probability that we return for the author $ j $ when we receive the vector $ x $; it is our estimation that the sentence that has been transformed into the vector $ x $ is of the author $ j $ (we want to show that $ p_j ^ {opt} (x) = \\frac {\\lambda_j f_j (x )} {\\sum_k \\lambda_k f_k (x)} $ is indeed the optimal solution).\n", "\n", "In general, the program will return a probability that can be written in the form:\n", "$$ q_j (x) = \\frac {\\lambda_j g_j (x)} {\\sum_k \\lambda_k g_k (x)} $$\n", "where $ g_j $ is a positive continuous function (which is assumed to be at least as large as $ f_j $ because if there exists $ x $ such that $ q_j (x) = 0 $ and $ f_j ( x)> 0 $, then $ \\mathbb P (logloss = \\infty)> 0 $ ...).\n", "So that the $ g_j $ will \"lose\" their weight outside the support of $ f $ (that is, if $ f (x) = 0 $, $ x $ has no chance to appear, so we do not care about the value of $ g_j $ in these points), we can also assume that $ g_j $ are density functions (that is, $ \\int _{\\mathbb R ^ d} g_j (x) dx = 1 $).\n", "\n", "We can take, for example, $ g_j = \\frac {1} {\\lambda_j} f_j $ to disregard the difference in the number of sentences between authors (here, $g_j$ are not density functions since their integral does not equal $ 1 $, but one could come back to it as said just before).\n", "If we want \"extreme\" probabilities (close to $ 0 $ or $ 1 $), we can ask: $ g_j = f_j ^ 2 \\times \\frac {1} {\\int f_j ^ 2} $.\n", "On the contrary, if one wishes to \"qualify\" these probabilities, one will ask: $ g_j = \\sqrt {f_j} \\times \\frac {1} {\\int \\sqrt f_j} $.\n", "\n", "Now let us show that the best choice for minimizing log loss is: $ p_j ^ {opt} = \\frac {\\lambda_j f_j (x)} {\\sum_k \\lambda_k f_k (x)} $. Let us note $ log loss ^ {opt} $ the score obtained with this last choice and $ log loss ^ {q_j} $ the score obtained with the probabilities $ q_j $ defined using the functions $ g_j $.\n", "We then have:\n", "\n", "\\begin{align}\n", "\\mathbb E[log loss^{opt}] - \\mathbb E[log loss^{q_j}] & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j^{opt})f_j(x) \\mathrm d x} - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(q_j)f_j(x) \\mathrm d x}\\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R^d}(\\log(p_j^{opt})-\\log(q_j))f_j(x) \\mathrm d x} \\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R^d}\\log(\\frac{\\lambda_j f_j(x)}{\\lambda_j g_j(x)}\\frac{\\sum_k \\lambda_k g_k(x)}{\\sum_k \\lambda_k f_k(x)})f_j(x) \\mathrm d x} \\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{ f_j(x)}{ g_j(x)})f_j(x) \\mathrm d x} + {\\int_{\\mathbb R ^d}\\log(\\frac{\\sum_k \\lambda_k f_k(x)}{\\sum_k \\lambda_k g_k(x)})(\\sum_j \\lambda_j f_j(x))\\mathrm d x} \\\\\n", " & = - \\sum_j \\lambda_j D_{KL}(f_j || g_j) + D_{KL}(\\sum_k \\lambda_k f_k || \\sum_k \\lambda_k g_k)\\\\\n", " & \\text{(where $ D_ {KL} $ denotes the Kullback-Leibler divergence ... we'll get back to it!)} \\\\\n", " & \\le 0 \\text{ by the convexity property of $ D_ {KL} $ in the function pairs;}\\\\\n", " & \\text{here, there is $ 3 $: $ (f_1, g_1) $, $ (f_2, g_2) $ and $ (f_3, g_3) $. We conclude that $ p_j ^ {opt} $ is optimal:}\\\\\n", " \\mathbb E[log loss^{opt}] & \\le \\mathbb E[log loss^{q_j}]\n", "\\end{align}\n", "\n", "The Kullback-Leibler divergence between two densities $ p $ and $ q $ on $ \\mathbb R ^ d $ is given by the following formula:\n", "$ D_ {KL} (p || q) = \\int_{\\mathbb R ^ d} \\log (\\frac{p (x)}{q (x)}) p (x) \\mathrm d x $. To see it quickly, in information theory, it is the average amount of additional bits that will be needed to code from the source $ q $ an optimal code of the source $p$.\n", "We will talk again about the divergence of Kullback-Leibler in the part devoted to the \"t-SNE\" (algorithm that works precisely on a minimization of this same divergence during the change of dimension of the data space).\n", "\n", "\n", "So we took the first step boldly (our point being to show that we can do IT cleanly and rigorously!): We now know that once we have our program that turns sentences into vectors, the amount to look for (and return to Kaggle) is:\n", "$$ p_j ^ {opt} (x) = \\frac {\\lambda_j f_j (x)}{\\sum_k \\lambda_k f_k (x)}. $$\n", "\n", ".\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "3275483ab9b100c1ab4d6be6e2da5d090dafc6d5", "_cell_guid": "1c440496-dfd5-48ae-a747-36a345e58b56"}, "source": ["\n", "<div id=\"akppv\" />\n", "\n", "# K-nearest-neighbors (to find the optimal $ p_ {ij} ^ {opt} $ probabilities)\n", "\n", "We will show a way to recover these probabilities. Oh ! it is certainly not an extraordinary algorithm since it is the nearest-neighbors algorithm.\n", "Nevertheless, it has the advantage of asymptotically giving (when $ N $ the number of texts tends to infinity) the best possible result. Other algorithms often used are, for example, the hypothesis of a Gaussian distribution of data (this is the case of classifiers naive Bayes) which give good results on small samples (since they are based on the computation of expectations and covariances of the variables, which requires much less data than to estimate a density) but these algorithms cease to give good results as soon as the learning base is sufficient.\n", "\n", "Here is the algorithm of K-nearest-neighbors:\n", "\n", "- We have a learning base $ \\{(x_1, y_1), \\ ... \\ , \\ (x_N, y_N) \\} $ containing the $ N $ vectors $ x_1, ..., x_N \\in \\mathbb R ^ d $ formed from $ N $ sentences with their respective author $ y_1, ..., y_N \\in \\{1,2,3 \\} $. We also choose a distance $ d $ on $ \\mathbb R ^ d $ (usually the Euclidean distance, but this is not obligatory, the asymptotic results will remain true for any distance: we say that the operator of the kNN is *universally* consistent).\n", "- We choose an integer $ k $ depending on $ N $ which must be small in front of $ N $ (we want: $ k / N \\underset {N \\to \\infty} {\\longrightarrow} 0 $) but still big enough (it must be that: $ k \\underset {N \\to \\infty} {\\longrightarrow} \\infty $).\n", "- Now, each time we receive a new vector $ x $, we consider the set $kNN$ of the $k$ indices  of the points the closer to $x$:\n", "\n", "$$kNN = \\{i_1 <..<i_k \\ | \\ \\forall \\ j \\notin \\{i_1, ..., i_N \\}, \\forall \\ i \\in \\{i_1, ..., i_N \\},  \\ d (x, x_i) \\le d (x , x_j) \\}$$\n", "\n", "\n", "- Finally, we return probabilities $ \\hat p_j (x) $ proportional to the number of more-near-neighbors who are of the author $ j $. So :\n", "$$ \\text {For $ j \\in \\{1,2,3 \\} $, } \\hat p_j (x) = \\frac {1} {k} \\# \\{i \\in kNN \\ | \\ y_i = j \\} $$\n", "\n", "\n", "We have therefore presented the very simple $ k $-NN algorithm which returns $ \\hat p_j (x) $ estimates of $ p_j ^ {opt} (x) $. It checks the following \"weak consistency\" property:\n", "$$ \\text{If $ k \\underset {N \\to \\infty} {\\longrightarrow} \\infty $ and $ k / N \\underset {N \\to \\infty} {\\longrightarrow} 0 $, then:} \\ \\forall \\ \\epsilon> 0, \\ \\mathbb P (| \\hat p_j - p_j ^ {opt} |> \\epsilon) \\underset {N \\to \\infty} {\\longrightarrow} 0 $$\n", "\n", "Asymptotically, we thus find empirically the value of $ p_j ^ {opt} $. This does not tell us anything about the speed of convergence. There are refinements of this algorithm (for example by using proximity graphs) which make it possible to improve this speed of convergence.\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "b0adf5a0ec08ea5757ba9f9ea178bb273bc005c5", "_cell_guid": "8b9b371e-2d5f-417d-b19d-be3c966f50fb"}, "source": ["<div id=\"aexemples\" />\n", "# Examples of features"], "cell_type": "markdown"}, {"metadata": {"_uuid": "87ef1f36fb96ba1d726ad8cfe1fa850e9d308db5", "_cell_guid": "e39239c7-4357-441c-bb95-78a7f8408902"}, "source": ["<div id=\"aexemples_courants\" />\n", "## - Current Examples\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "5c9ef814be18b362e6281b31b4d4ab120fef2596", "_cell_guid": "a6e24674-d0d0-463c-8972-bde6a48a963e"}, "source": ["<div id=\"avoca_auteurs\" />\n", "## - Example of vocabulary usage\n", "\n", "The examples we saw in the previous section are common examples of stylometry. They are also very general examples in that they do not require the presence of a training corpus: they could equally well be used in the case of *unsupervised classification* (and not only *supervised* as this is the case here).\n", "\n", "We'll explain now an example of a function that uses the vocabulary used in the training set. Given a sentence, this function returns a $ 3 $ size vector containing the log-likelihood of using that vocabulary for each of the $ 3 $ authors.\n", "\n", "Let's split our training corpus in two parts: \n", "- a training set to build the dictionary ($95\\%$ of the data)\n", "- a test set that will allow us to visualize the results ($5\\%$ of the data)."], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["N_train = len(train_df)\n", "\n", "# cutting of the train set\n", "cut = round(N_train * 0.95)\n", "\n", "train = pd.DataFrame(train_df, index=range(cut))\n", "test = pd.DataFrame(train_df, index=range(cut,N_train))"], "cell_type": "code", "metadata": {"_uuid": "27c4410830361e185629d38509fed3aa6c9bff47", "collapsed": true, "_cell_guid": "0b8fb185-e561-454e-a145-2d0bdf1922c8"}}, {"metadata": {"_uuid": "efd1d7e2f303404d571cf3b27a8e784a6e100e15", "_cell_guid": "13fc4597-8fa4-496a-8675-98dfcf2eb356"}, "source": ["\n", "Let's start by building a dictionary containing all the words appearing in the training corpus. To each entry (a word) in this dictionary we'll associate its occurrences for each of the authors.\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["# built from a training corpus a dictionary of used words. For each word, we fill a dictionary that each author associates the number of times this word appeared in the author\n", "def voca_Authors(train):\n", "    voca = {}\n", "    for i,line in train.iterrows():\n", "        words = nltk.word_tokenize(line['text'])\n", "        for word in words:\n", "            word = word.lower()\n", "            if not word in voca:\n", "                voca[word] = {auth:0 for auth in authors}\n", "                voca[word][line['author']] = 1\n", "            else:\n", "                voca[word][line['author']] += 1\n", "    return voca\n", "\n", "print('Computation of vocabulary dictionary.')\n", "voca_authors = voca_Authors(train)\n", "print('Dictionary size:',len(voca_authors),'\\n20 examples from this dictionary :\\n')\n", "for i,word in enumerate(voca_authors):\n", "    if i < 20:\n", "        print(word,voca_authors[word])"], "cell_type": "code", "metadata": {"_uuid": "93bd3539fd0c343ad7aec167667e7371693fa1bd", "collapsed": true, "_cell_guid": "65aeeb94-675f-45be-8ec9-163e0ab4d7ec"}}, {"metadata": {"_uuid": "b15547c149b0c07fc8f352dd41524b4e8ebb0c82", "_cell_guid": "b02a44f6-8e79-49a2-a965-b49302c6889d"}, "source": ["All we need know is to understand what has been computed and what is computable.\n", "\n", "We will make the hypothesis quite strong (that is to say rather false) that:\n", "- the length of the sentences follows a law independent of the author (one can look at the histograms of the lengths of sentence according to the author with the following address: link and to note that indeed these 3 histograms are quite close) ,\n", "- all the words of a sentence are drawn independently and identically according to a law that depends only on the author (this hypothesis is very strong and very false).\n", "\n", "Given those independence hypotheses, the computation of the likelihood for an author who has written the sentence $ s = [w_1, ..., w_k] $ is quite straightforward :\n", "\n", "$$ L(w_1, \\ ... \\ , \\ w_k \\ | \\ author = j) = \\prod_ {i = 1} ^ k \\mathbb P (\\text {author of $ w_i $ = $ j $} \\ | \\ \\text {word = $ w_i $}) $$\n", "\n", "Now, let's take the word \"of\" as an example. We get the following statistics:"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["voca_authors['of']"], "cell_type": "code", "metadata": {"_uuid": "57d8a5010356966d78ff2ced4c00858de0a752ff", "collapsed": true, "_cell_guid": "b0421299-6a7c-4613-b5d6-635a3561b4c8"}}, {"metadata": {"_uuid": "20807310fb11005414ad34b95cdcd3ee942c9f6d", "_cell_guid": "41e86e7f-7d28-41b3-91e6-d1d2cbf507ef"}, "source": ["There are $ 8539 + 5568 + 5850 = 19957 $ occurrences of the word \"of\" in our training corpus. Edgar A. Poe is the author of 8539 of them, that is, a proportion $ x_1=8539/19957=0.43$. \n", "He is also the author of a proportion $\\lambda_1=0.40<x_1$ of all the sentences of the corpus. Since we've been hypothesizing that the law on the length of sentences was author independent, this means that he therefore writes more the word 'of' than his overrepresentativeness in the corpus would allow; that is, $ p_1 : = \\mathbb P (\\text {author of $ 'of' = 1 $})> 1/3 $.\n", "\n", "If we write $ p_j = \\mathbb P (\\text {author of 'of' = j }) $ and $ x_j $ the proportion of appearances in the training corpus, we get the following relation:\n", "$$ x_j = \\frac {\\lambda_j p_j} {\\sum_k \\lambda_k p_k}. $$\n", "\n", "We thus know the $ x_j $ and the $ \\lambda_j $ and we want to know the $ p_j $ to get our log-likelihoods!\n", "\n", "Note that one can very easily compute $ \\sum_k \\lambda_k p_k $ using :$$ \\sum_j \\frac {x_j} {\\lambda_j} = \\sum_j \\frac{p_j}{\\sum_k  \\lambda_k p_k } = \\frac{1}{\\sum_k \\lambda_k p_k }$$\n", "... and thus find the $ p_j $ we're looking for:\n", "$$ p_j = \\frac {x_j} {\\lambda_j}\\left( \\sum_k \\frac {x_k} {\\lambda_k}\\right)^{-1} $$\n", "\n", "This gets us the log-likelihood for each author, Yay !\n", "$$ \\log L (w_1, ..., w_k \\ | \\ author = j) = \\sum_ {i = 1} ^ k \\log (p_j (w_i)) $$"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["def vect_Voca(sentence, dict = voca_authors, zero = 0.01):\n", "    words = nltk.word_tokenize(sentence)\n", "    ret = [0]*len(authors)\n", "    for word in words:\n", "        word = word.lower()\n", "        if word in dict:\n", "            vect = [0]*len(authors)\n", "            nb_appar = 0\n", "            for auth in dict[word]:\n", "                nb_appar += dict[word][auth]\n", "            for auth in dict[word]:\n", "                vect[authors[auth]] = dict[word][auth]/nb_appar/lambdas[auth] # vect[j] = x_j/lambda_j = p_j / (Sum lambda_j p_j)\n", "            s = 1/sum(vect) # s = (Sum lambda_j p_j) \n", "            vect = [p_j_div_sum * s for p_j_div_sum in vect] # vect[j] = p_j\n", "\n", "            for j,p_j in enumerate(vect):\n", "                if p_j == 0:\n", "                    ret[j] += log(zero)\n", "                else:\n", "                    ret[j] += log(p_j)\n", "    return ret"], "cell_type": "code", "metadata": {"_uuid": "005874f260caf95194cd656beb71eb991b45c51e", "collapsed": true, "_cell_guid": "7597b1c3-ef4d-497a-85ac-70d246e1f8de"}}, {"metadata": {"_uuid": "37d206b84a8a89fbe290ecbfd71f828b6bc8dc1d", "_cell_guid": "0143fefe-fedb-4eff-989d-03094fc862c9"}, "source": ["... and visualize the comet's tail-like results:\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["# the matrix of sentences of the corpus of text transformed using vect_Voca\n", "X = []\n", "col = [] # the color according to the author: Poe => red, Lovecraft => green, Shelley => blue\n", "# colours = {'EAP':'rgb(150, 5, 5)','HPL':'rgb(5, 150, 5)','MWS':'rgb(5, 5, 150)'}\n", "colours = {'EAP':'r','HPL':'g','MWS':'b'}\n", "# computation of X, the size matrix N_tests * 3\n", "for i,line in test.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    X.append(vect)\n", "    col.append(colours[line['author']])\n", "X = np.array(X)\n", "\n", "## The display\n", "\n", "from mpl_toolkits.mplot3d import Axes3D\n", "import matplotlib.pyplot as plt\n", "\n", "\n", "fig = plt.figure(figsize=(15,15))\n", "ax = fig.add_subplot(111, projection='3d')\n", "\n", "ax.scatter(X[:,0], X[:,1], X[:,2], c=col, marker='+')\n", "\n"], "cell_type": "code", "metadata": {"_uuid": "650cc0d6dd30df822e1f94267711b9611bb70d57", "collapsed": true, "_cell_guid": "79c3035d-5dff-45a8-9ca8-ec55630c6901"}}, {"metadata": {"_uuid": "36a74374c1505ff8a10a22dd54b091d2e4db9592", "_cell_guid": "dda3ef8a-60e4-4e94-aaf8-920fc2835f7e"}, "source": ["<div id=\"akernel\" />\n", "## - Example of kernel on the tree structures of sentences\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "3cae76a78e01e8e73907aacc7b97202d31a80e27", "_cell_guid": "a8b1c84b-84fb-4a64-ba34-f714489b74e2"}, "source": ["<div id=\"avisualisation\" />\n", "# Data visualization\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "341093d1f0ebc6584338935c484aa2d27348367e", "_cell_guid": "5891cf0d-7252-477f-a15e-988a9e751270"}, "source": ["<div id=\"aACP\" />\n", "## - The Principal Components Analysis (PCA)\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4896d46176ebb825c69e4e56cc96de26393d6edc", "_cell_guid": "fe214d31-c3d6-4c46-835a-e03c85918e3d"}, "source": ["<div id=\"atSNE\" />\n", "## - The t-SNE\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "e2f133975ddfbb68a725687646660364c8b1c87d", "_cell_guid": "e7e226d8-b4ca-4a0e-adf3-e4e15f5c9284"}, "source": ["<div id=\"aresultats\" />\n", "# Submission (score : 0.40)\n", "\n", "A simple submission with kNN just on 3 size vectors of vocabulary log-likelihood :"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["from sklearn.neighbors import NearestNeighbors\n", "\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "\n", "voca_authors = voca_Authors(train)\n", "# la matrice des phrases du corpus de texte transform\u00e9es \u00e0 l'aide de vect_Voca\n", "M = [] # the train matrix\n", "Sol = [] # the solutions of the train matrix\n", "X = [] # the test matrix\n", "\n", "\n", "for i,line in train.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    M.append(vect)\n", "    Sol.append(line['author'])\n", "M = np.array(M)\n", "\n", "for i,line in test.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    X.append(vect)\n", "X = np.array(X)\n", "\n", "\n", "# The kNN algorithm\n", "adjunction = 0.025\n", "nb_ppv = 200 # number of nearest neighbors\n", "M_ppv = NearestNeighbors(n_neighbors=nb_ppv)\n", "M_ppv.fit(M)\n", "tab_kneighbors = M_ppv.kneighbors(X, return_distance=False)\n", "\n", "Probas = [] # the return\n", "\n", "for i,line in test.iterrows():\n", "    p = [0]*3\n", "    kneighbors = tab_kneighbors[i]\n", "    for kneighbor in kneighbors:\n", "        p[authors[Sol[kneighbor]]] += 1\n", "    s = sum(p)\n", "    p = [x/s for x in p]\n", "    Probas.append(p)\n", "\n", "# harmonisation of the probabilities\n", "for v in Probas:\n", "    for i,prob in enumerate(v):\n", "        v[i] = (prob + adjunction)/(1+3*adjunction)\n", "\n", "\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,['EAP', 'HPL', 'MWS']] = Probas\n", "submission.to_csv(\"Log_likelihoof_on_vocabulary.csv\", index=False)\n", "submission.head()"], "cell_type": "code", "metadata": {"_uuid": "4b937734be45f9401993270c917fd46f6a9e468e", "collapsed": true, "_cell_guid": "c3dfd3f7-d95f-433d-ba7b-a1b8824ac8f7"}}, {"metadata": {"_uuid": "5d86bafb60813143952daec65dd354c73c533ad4", "_cell_guid": "40278fac-626a-464e-8f5e-3efa0a7b85f3"}, "source": ["<div id=\"francais\" />\n", "# La version fran\u00e7aise / the french version :\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "dab2ce215b3500fe6a30d532be9f3fb103a2422e", "_cell_guid": "ccdce257-67ee-4203-8468-b1424e945a3d"}, "source": ["# Une approche (pas \u00e0 pas) math\u00e9matique de la stylom\u00e9trie\n", "\n", "On d\u00e9plore souvent que les informaticiens ne soient pas toujours tr\u00e8s rigoureux.\n", "Ils manipulent les math\u00e9matiques... mais \u00e0 leur sauce et font parfois de dr\u00f4les de tambouilles/une dr\u00f4le de cuisine avec !?\n", "\n", "Le but de ce tutoriel est de tenter de montrer qu'une telle situation n'est pas une fatalit\u00e9 : qu'on peut allier harmonieusement math\u00e9matiques & informatique ; c'est-\u00e0-dire \u00e0 la fois faire de l'informatique proprement et des math\u00e9matiques qui servent \u00e0 quelque chose !!\n", "\n", "Voici donc le d\u00e9roul\u00e9 de ce tutoriel :\n", "- on commencera par une courte introduction \u00e0 propos de ce domaine \u00e0 cheval sur la linguistique et la satistiqu consistant \u00e0 identifier les auteurs et qui est au c\u0153ur de notre comp\u00e9tition Soopky... j'ai nomm\u00e9 : la **\"stylom\u00e9trie\"**\n", "- on r\u00e9fl\u00e9chira ensuite au syst\u00e8me de notation de Spooky : la note \"log loss\" (ou *entropie crois\u00e9e*) : $ -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^3 y_{ij}\\log(p_{ij})$. En supposant que l'on poss\u00e8de d\u00e9j\u00e0 le programme qui transforme une phrase en vecteur de $\\mathbb R ^d$, on verra quelles sont les quantit\u00e9s $p_{ij}^{opt}$ \u00e0 renvoyer pour minimiser l'esp\u00e9rance de la \"log loss\"\n", "- apr\u00e8s, on pr\u00e9sentera un algorithme tr\u00e8s simple : celui des k-plus-proches-voisins. Cet algorithme poss\u00e8de la propri\u00e9t\u00e9 math\u00e9matique de renvoyer asymptotiquement (quand $N \\to \\infty$) les probabilit\u00e9s optimales $p_{ij}^{opt}$ que l'on recherche.\n", "- enfin, on pr\u00e9sentera quelques exemples de programmes (avec le code python !) pour transformer nos phrases en vecteurs. Dans un premier temps, quelques exemples courants (longueur de phrases, longueur des mots, pr\u00e9sence de certains mots typiques, etc...) Puis un exemple un peu plus \u00e9labor\u00e9 qui calcule la log-vraisemblance d'une phrase pour chacun des auteurs \u00e0 partir de leur vocabulaire. Pour finir, on proposera un exemple de kernel encore plus \u00e9labor\u00e9 utilisant la biblioth\u00e8que \"nltk\" : elle permet de transformer une phrase en un arbre \u00e0 partir de sa structure grammaticale. \n", "- on consacrera la derni\u00e8re partie \u00e0 la visualisation des donn\u00e9es. On pr\u00e9sentera deux algorithmes : celui de l'Analyse en Composantes Principales qui consiste en un changement de base orthnorm\u00e9e et celui de la \"t-SNE\" qui diminue la dimension de l'espace $\\mathbb R^d$ tout en cherchant \u00e0 rendre compte le mieux possible des distances de l'espace original dans le nouvel espace.\n", "\n", "\n", "<ul>\n", "  <li><a href=\"#introduction\" >Introduction \u00e0 la stylom\u00e9trie</a></li>\n", "  <li><a href=\"#logloss\" >Que rechercher pour minimiser la \"log loss\" ?</a></li>\n", "  <li><a href=\"#kppv\">L'algorithme des k-plus-proches-voisins (qui minimise asymptotiquement la \"log loss\")</a></li>\n", "  <li><a href=\"#exemples\">Quelques exemples de features</a>\n", "      <ul style=\"list-style-type:circle\">\n", "      <li><a href=\"#exemples_courants\">Exemples classiques de stylom\u00e9trie</a></li>\n", "      <li><a href=\"#voca_auteurs\">Calcul de la log-vraisemblance pour chacun des auteurs en fonction du vocabulaire</a></li>\n", "      <li><a href=\"#kernel\">Kernel sur la structure grammaticale des phrases</a></li>\n", "      </ul>\n", "  </li>\n", "  <li><a href=\"#visualisation\">Kernel sur la structure grammaticale des phrases</a>\n", "        <ul style=\"list-style-type:circle\">\n", "      <li><a href=\"#ACP\">l'Analyse en Composantes Principale (ACP)</a></li>\n", "      <li><a href=\"#tSNE\">la t-SNE</a></li>\n", "      </ul>\n", "  </li>\n", "   <li> <a href=\"#resultats\"> R\u00e9sultats (score : 0,40)</a></li>\n", "</ul>\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "9715e491e6cebe988a6dabc0347c07b8a7f7f87c", "_cell_guid": "27587b05-4e85-401b-bcda-0563d0e85a7c"}, "source": ["<div id=\"introduction\" />\n", "# Introduction \u00e0 la stylom\u00e9trie\n", "Kaggle nous propose, avec l'aide d'un corpus d'entra\u00eenement, de retrouver les auteurs (qui sont au nombre de trois : Edgar A. Poe,  Howard P. Lovecraft et Mary Shelley) d'une petite dizaine de mille de phrases.\n", "\n", "Ce que nous demande de faire Kaggle porte un nom : la* **stylom\u00e9trie***.\n", "\n", "La *stylom\u00e9trie* est cet *art* (pris dans son sens grec de *\u03c4\u03ad\u03c7\u03bd\u03b7*) \u00e0 la crois\u00e9e de la statistique et de la linguistique dont le but est de produire \u00e0 partir de textes une*information* (qui puisse \u00eatre trait\u00e9e statistiquement) rendant compte du *style* de ces derniers ; c'est-\u00e0-dire qui caract\u00e9riserait -- id\u00e9alement, cela s'entend... -- \u00e0 la fois son auteur, mais aussi son genre, son \u00e9poque, etc...\n", "Plus pratiquement, ces statistiques portent g\u00e9n\u00e9ralement sur le vocabulaire utilis\u00e9 (qui peut \u00eatre affin\u00e9 en rep\u00e9rant le sens en lequel l'auteur emploie tel mot), les cat\u00e9gories grammaticales (noms/pronoms, adjectifs, adverbes, verbes... \u00e0 quel temps/mode ?) ainsi que la structure grammaiticale des phrases, la ponctuation, etc... \n", "\n", "Il faut donc bien comprendre que pour faire de la bonne stylom\u00e9trie, il faut \u00e0 la fois jouer sur les deux tableaux :\n", "- celui de la linguistitique en proposant des crit\u00e8res qui soient les plus pertinents pour isoler le style d'un auteur,\n", "- celui de la statistique en employant des outils math\u00e9matiques et informatiques adapt\u00e9s.\n", "\n", "Au cours de ce tutoriel, nous essayerons d'aborder ces deux aspects de la stylom\u00e9trie en restant toujours le plus possible rigoureux et en justifiant (autant que faire se peut) tout ce que nous ferons.\n", "\n", "Mais d'abord, int\u00e9ressons-nous au syt\u00e8me d'\u00e9valuation de Spooky :"], "cell_type": "markdown"}, {"metadata": {"_uuid": "1d44bd3e5f0023e50776cb803d5b9f667063e969", "_cell_guid": "cd46fa10-961e-4f20-9857-b9f227fa9bcb"}, "source": ["<div id=\"logloss\" />\n", "# Que rechercher pour minimiser la \"log loss\" ?\n", "\n", "Tout d'abord nous allons voir pourquoi le syst\u00e8me d'\u00e9valuation \"logloss\" a \u00e9t\u00e9 choisi  et quelles sont les valeurs math\u00e9matiques \u00e0 rechercher pour le minimiser en esp\u00e9rance.\n", "De ces valeurs \u00e0 rechercher, nous en d\u00e9duirons un algorithme adapt\u00e9 (c'est-\u00e0-dire que nous justifierons d'un point de vue math\u00e9matique) pour le calcul des probabilit\u00e9s $p_{ij}$ \u00e0 retourner. C'est l'agorithme des k-plus proches-voisins.\n", "\n", "Le syst\u00e8me de notation qui a \u00e9t\u00e9 choisi pour Spooky est celui de \"log loss\" (ou *entropie crois\u00e9e*) :\n", "        \n", "$$ log loss = - \\frac{1}{N}\\sum_{\\substack{i \\in \\{1,...,N\\} \\\\ j \\in \\{ EAP, HPL, MWS \\}}}y_{ij} \\log(p_{ij})$$\n", "o\u00f9 $p_{ij}$ est l'estimation que l'on doit rendre ; la probabilit\u00e9 selon notre programme que le texte $i$ soit de l'auteur $j$.\n", "$y_{ij}$ est la solution : il vaut $1$ lorsque le texte $i$ est de l'auteur $j$, $0$ sinon.\n", "\n", "Le but est de minimiser cette \"log loss\".\n", "\n", "Nous allons maintenant un tout petit peu formaliser le probl\u00e8me d'un point de vue math\u00e9matique afin de pouvoir calculer l'esp\u00e9rance de la \"log loss\" et de trouver les valeurs optimales $p_{ij}^{opt}$ \u00e0 retourner afin de minimiser cette esp\u00e9rance.\n", "\n", "Supposons que nous disposions d\u00e9j\u00e0 de la premi\u00e8re partie du programme : celle qui transforme une phrase en une \"information\" traitable par l'informatique. En l'occurrence nous supposerons que ces informations sont des vecteurs de $\\mathbb R ^d$ et qu'ils sont tous tir\u00e9s de fa\u00e7on ind\u00e9pendante selon une m\u00eame loi $X$ admettant une densit\u00e9 $f$ \u00e0 support compact. On suppose aussi qu'on peut d\u00e9composer cette loi $\\mathcal L(X)$ selon deux \u00e9tapes :\n", "\n", "-  tout d'abord, on choisit l'auteur de la phrase selon une loi de \"Bernoulli \u00e0 trois issues\" : 'EAP' pour Edgar A. Poe, 'HPL' pour Howard P. Lovecraft et 'MWS' pour Mary Shelley. On notera $\\mathcal B(\\lambda_1, \\lambda_2, \\lambda_3)$ cette loi (avec $\\sum \\lambda_j = 1$) ; $\\lambda_j$ \u00e9tant la proportion de phrases de l'auteur $j$.\n", "-  En fonction de l'auteur $j$ qui est choisi \u00e0 la premi\u00e8re \u00e9tape, notre phrase est tir\u00e9e selon une loi de densit\u00e9 $f_j$ sur $\\mathbb R ^d$.\n", "\n", "![shema_bernouilli_fr](http://bourg-la-reine-echecs.fr/telechargements/shema_bernoulli_auteurs_corrige.png)\n", "\n", "On a donc que : $f = \\sum \\lambda_j f_j$.\n", "\n", "Avec le corpus d'entra\u00eenement, on obtient que : \n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["nb_texts = len(train_df)\n", "lambdas = (train_df.author.value_counts()/nb_texts).to_dict()\n", "print(\"Lambdas :\",lambdas)"], "cell_type": "code", "metadata": {"_uuid": "f5b3f6602bb7be0d95d15ce87e4fcab249feb7d3", "collapsed": true, "_cell_guid": "bd32b467-bbf1-4a70-a786-31e3199aafde"}}, {"metadata": {"_uuid": "ae5e7ddeaf1023ab69ebd3c84689af41dbf11153", "_cell_guid": "7d702003-ce64-4ce3-82c2-e552e847e919"}, "source": ["- $\\lambda_1 = 0.403493538995863$ \n", "- $\\lambda_2 = 0.287808366106543$\n", "- $\\lambda_3 = 0.308698094897594$\n", "\n", "Le lecteur attentif aura peut-\u00eatre remarqu\u00e9 que le fichier d'exemple \"sample\\_submission\" renvoie justement pour chaque phrase les probabilit\u00e9s $(\\lambda_1, \\lambda_2, \\lambda_3)$... ce n'est pas un hasard !! C'est le retour qui minimise la \"log loss\" avec le plus mauvais programme qui soit : celui qui ne distingue rien et qui envoie toutes les phrases sur le vecteur nul ($\\mathcal L (X) = \\delta_0$ o\u00f9 $\\delta_0$ est la mesure de \"dirac\" au point $0$).\n", "Cela d\u00e9coule d'un r\u00e9sultat classique de th\u00e9orie de l'information : si $(\\lambda_j)_j$ et $(p_j)_j$ sont deux mesures de probabilit\u00e9 discr\u00e8tes, alors l'entropie crois\u00e9e :\n", "$$H_\\lambda(p) = \\mathbb E[log loss] = -\\sum_j \\lambda_j \\log(p_j)$$\n", "est minimis\u00e9e quand $p_j = \\lambda_j$ pour tout $j$.\n", "\n", "Nous allons maintenant g\u00e9n\u00e9raliser ce r\u00e9sultat !\n", "\n", "\u00c9tant donn\u00e9 une phrase transform\u00e9e par notre programme en un vecteur $x$ (selon donc la loi $X$), la meilleure solution (au sens que c'est celle qui minimise l'esp\u00e9rance de la note finale \"log loss\") de probabilit\u00e9s  \u00e0 renvoyer est le triplet :\n", "\n", "$$ p_1^{opt}(x) = \\frac{\\lambda_1 f_1(x)}{\\sum_j \\lambda_j f_j(x)}, \\ \\ \\ \\ \n", "p_2^{opt}(x) = \\frac{\\lambda_2 f_2(x)}{\\sum_j \\lambda_j f_j(x)}, \\ \\ \\ \\ \n", "p_3^{opt}(x) = \\frac{\\lambda_3 f_3(x)}{\\sum_j \\lambda_j f_j(x)}.$$\n", "\n", "Bon ! maintenant que cela a \u00e9t\u00e9 dit, il n'y a plus qu'\u00e0 se retrousser les manches !\n", "\n", "L'esp\u00e9rance de la note \"log loss\" est donn\u00e9e dans ce cas par la formule suivante :\n", "\n", "$$ \\mathbb E [log loss] = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j(x))f_j(x) \\mathrm d x}$$\n", "o\u00f9 $p_j(x)$ est la probabilit\u00e9 que l'on renvoie pour l'auteur $j$ quand on re\u00e7oit le vecteur $x$  ; c'est notre estimation que la phrase qui a \u00e9t\u00e9 transform\u00e9 en le vecteur $x$ soit de l'auteur $j$ (on veut donc montrer que $p_j^{opt}(x)= \\frac{\\lambda_j f_j(x)}{\\sum_k \\lambda_k f_k(x)}$ est effectivement la solution optimale).\n", "\n", "En toute g\u00e9n\u00e9ralit\u00e9, le programme  renverra une probabilit\u00e9 qui peut s'\u00e9crire sous la forme :\n", "$$ q_j(x)= \\frac{\\lambda_j g_j(x)}{\\sum_k \\lambda_k g_k(x)}$$\n", "o\u00f9 $g_j$ est une fonction continue positive (qu'on supposera quand m\u00eame de support au moins aussi grand que celui de $f_j$ car s'il existe $x$ tel que $q_j(x) = 0$ et $f_j(x) > 0$, alors $\\mathbb P(logloss = \\infty) >0$...). \n", "Quitte \u00e0 ce que les $g_j$ aillent \"perdre\" de leur poids en dehors du support de $f$ (c'est-\u00e0-dire que si $f(x) = 0$, $x$ n'a aucune chance d'appara\u00eetre ; peu nous importe donc la valeur des $g_j$ en ces points), on peut aussi supposer que les $g_j$ sont des fonctions de densit\u00e9 (c'est-\u00e0-dire que : $\\int_{\\mathbb R^d} g_j(x) dx = 1$).\n", "\n", "On peut prendre, par exemple, $g_j = \\frac{1}{\\lambda_j}f_j$ pour ne plus tenir compte de la diff\u00e9rence du nombre de phrases entre les auteurs (ici, les $g_j$ ne sont pas des fonctions de densit\u00e9 puisque leur int\u00e9grale ne vaut pas $1$, mais l'on pourrait s'y ramener comme dit juste avant).\n", "Si l'on veut des probabilit\u00e9s \"extr\u00eames\" (proches de $0$ ou de $1$), on pourra poser : $g_j = f_j^2 \\times \\frac{1}{\\int f_j^2}$.\n", "Au contraire, si l'on souhaite \"nuancer\" ces probabilit\u00e9s, on posera : $ g_j = \\sqrt{f_j} \\times \\frac{1}{\\int \\sqrt f_j}$. \n", "\n", "Montrons \u00e0 pr\u00e9sent que le meilleur choix pour minimiser la note \"log loss\" est bel et bien : $p_j^{opt} = \\frac{\\lambda_j f_j(x)}{\\sum_k \\lambda_k f_k(x)}$. Notons $log loss^{opt}$ la note obtenue avec ce dernier choix et $log loss^{q_j}$ la note obtenue avec les probabilit\u00e9s $q_j$ d\u00e9finies \u00e0 l'aide des fonctions $g_j$. \n", "On a alors :\n", "\n", "\\begin{align}\n", "\\mathbb E[log loss^{opt}] - \\mathbb E[log loss^{q_j}] & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j^{opt})f_j(x) \\mathrm d x} - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(q_j)f_j(x) \\mathrm d x}\\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}(\\log(p_j^{opt})-\\log(q_j))f_j(x) \\mathrm d x} \\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{\\lambda_j f_j(x)}{\\lambda_j g_j(x)}\\frac{\\sum_k \\lambda_k g_k(x)}{\\sum_k \\lambda_k f_k(x)})f_j(x) \\mathrm d x} \\\\\n", " & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{ f_j(x)}{ g_j(x)})f_j(x) \\mathrm d x} + {\\int_{\\mathbb R ^d}\\log(\\frac{\\sum_k \\lambda_k f_k(x)}{\\sum_k \\lambda_k g_k(x)})(\\sum_j \\lambda_j f_j(x))\\mathrm d x} \\\\\n", " & = - \\sum_j \\lambda_j D_{KL}(f_j || g_j) + D_{KL}(\\sum_k \\lambda_k f_k || \\sum_k \\lambda_k g_k)\\\\\n", " & \\text{(o\u00f9 $D_{KL}$ d\u00e9signe la divergence de Kullback-Leibler... nous y reviendrons !)} \\\\\n", " & \\le 0 \\text{ par la propri\u00e9t\u00e9 de convexit\u00e9 de $D_{KL}$ en les paires de fonctions ;}\\\\\n", " & \\text{ici, il y en a $3$ : $(f_1,g_1)$, $(f_2, g_2)$ et $(f_3,g_3)$. On conclut que $p_j^{opt}$ est bien optimal :}\\\\\n", " \\mathbb E[log loss^{opt}] & \\le \\mathbb E[log loss^{q_j}]\n", "\\end{align}\n", "\n", "La divergence de Kullback-Leibler entre deux densit\u00e9s $p$ et $q$ sur $\\mathbb R^d$ est donn\u00e9e par la formule suivante :\n", "$D_{KL}(p||q) = \\int_{\\mathbb R^d}\\log(\\frac{p(x)}{q(x)})p(x)\\mathrm d x$. Pour le voir rapidement, en th\u00e9orie de l'information, c'est la quantit\u00e9 moyenne de bits suppl\u00e9mentaires qu'il va falloir pour coder \u00e0 partir de la source $q$ un code optimal de la source $p$. \n", "Nous reparlerons de la divergence de Kullback-Leibler dans la partie consacr\u00e9e \u00e0 la \"t-SNE\" (algorithme qui fonctionne justement sur une minimisation de cette m\u00eame divergence lors du changement de dimension de l'espace des donn\u00e9es).\n", "\n", "\n", "Nous avons donc franchi vaillamment la premi\u00e8re \u00e9tape (notre propos \u00e9tant de montrer qu'on peut faire de l'informatique de fa\u00e7on propre et rigoureuse !) : on sait maintenant que, une fois que l'on poss\u00e9dera notre programme qui transforme les phrases en vecteurs, la quantit\u00e9 \u00e0 rechercher (et \u00e0 renvoyer \u00e0 Kaggle) est :\n", "$$ p_j^{opt}(x) = \\frac{\\lambda_jf_j(x)}{\\sum_k \\lambda_k f_k(x)}.$$\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "9c31cf5999d1b500a43ea5ca086945d071eda8d6", "_cell_guid": "58541e73-4def-4a4b-9efe-39319e8000f5"}, "source": ["<div id=\"kppv\" />\n", "# k-plus-proches-voisins (pour retrouver les probabilit\u00e9s $p_{ij}^{opt}$ optimales)\n", "\n", "Nous allons montrer une mani\u00e8re de r\u00e9cup\u00e9rer ces probabilit\u00e9s. Oh ! ce n'est certes pas un algorithme extraordinaire puisque c'est celui des plus-proches-voisins.\n", "N\u00e9anmoins, il a cet avantage de donner asymptotiquement (lorsque $N$ le nombre de textes tend vers l'infini) le meilleur r\u00e9sultat possible. D'autres algorithmes souvent employ\u00e9s font par exemple l'hypoth\u00e8se d'une distribution gaussienne des donn\u00e9es (c'est le cas des classificateurs na\u00effs de Bayes) qui donnent de bons r\u00e9sultats sur de petits \u00e9chantillons (puisqu'il se basent sur le calcul des esp\u00e9rances et des covariances des variables ; ce qui n\u00e9cessite beaucoup moins de donn\u00e9es que pour estimer une densit\u00e9) mais ces algorithmes cessent de donner de bons r\u00e9sultats sit\u00f4t que la base d'apprentissage est suffisante.\n", "\n", "Voici donc l'algorithme des k-plus-proches-voisins :\n", "\n", "- On poss\u00e8de une base d'apprentissage $\\{(x_1,y_1), \\ ... \\ , \\ (x_N,y_N)\\}$ contenant les $N$ vecteurs $x_1, ..., x_N \\in \\mathbb R ^d$ form\u00e9s \u00e0 partir des $N$ phrases avec leur auteur respectif $y_1,...,y_N \\in \\{1,2,3\\}$. On choisit aussi une distance $d$ sur $\\mathbb R^d$ (g\u00e9n\u00e9ralement la distance euclidienne, mais cela n'est pas obligatoire ; les r\u00e9sultats asymptotiques resteront vrais pour n'importe quelle distance : on dit que l'op\u00e9rateur des k-plus-proches voisins est \\textit{universellement} consistant).\n", "- On choisit un entier $k$ d\u00e9pendant de $N$ qui doit \u00eatre petit devant $N$ (on veut : $k/N \\underset{N \\to \\infty}{\\longrightarrow} 0$) mais assez grand tout de m\u00eame (il faut que : $k \\underset{N\\to \\infty}{\\longrightarrow} \\infty$).\n", "- Maintenant, \u00e0 chaque fois que l'on re\u00e7oit une nouvelle phrase (en fait un nouveau vecteur $x$), on consid\u00e8re l'ensemble $kNN$ des $k$ indices $i_1 <... < i_k$ des points les plus proches de $x$ :\n", "$$ kNN = \\{i_1 < ... < i_k \\ | \\ \\forall \\ j \\notin \\{i_1,...,i_N\\}, \\forall \\ i \\in \\{i_1,...,i_N\\}, \\ \\ d(x,x_i) \\le d(x,x_j)\\} $$\n", "- Enfin, on retourne des probabilit\u00e9s $\\hat p_j(x)$ proportionnelles au nombre de plus-proches-voisins qui sont de l'auteur $j$. Ainsi :\n", "$$\\text{Pour $j \\in \\{1,2,3\\}$, } \\hat p_j(x) = \\frac{1}{k} \\# \\{ i \\in kNN \\ | \\ y_i = j\\}$$\n", "\n", "\n", "Nous avons donc pr\u00e9sent\u00e9 l'algorithme tr\u00e8s simple des $k$-plus-proches-voisins qui renvoie des estimations $\\hat p_j(x)$ de $p_j^{opt}(x)$. Il v\u00e9rifie la propri\u00e9t\u00e9 de \"consistance faible\" suivante :\n", "$$\\text{Si $k \\underset{N\\to \\infty}{\\longrightarrow} \\infty$ et que $k/N \\underset{N \\to \\infty}{\\longrightarrow} 0$, alors : } \\forall \\ \\epsilon >0,\\ \\mathbb P (|\\hat p_j - p_j^{opt}| > \\epsilon ) \\underset{N\\to \\infty}{\\longrightarrow} 0$$\n", "\n", "Asymptotiquement, on retrouve donc bien empiriquement la valeur de $p_j^{opt}$. Cela ne nous dit rien pour autant sur la vitesse de convergence. Il existe des raffinements de cet algorithme (par exemple en ayant recours \u00e0 des graphes de proximit\u00e9) qui permettent d'am\u00e9liorer cette vitesse de convergence.\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "d01a638acf2526829fc659941c5982f07d4a2a75", "_cell_guid": "99a66cf8-5e2a-4a93-9e0e-fa00fe3017db"}, "source": ["<div id=\"exemples\" />\n", "# Exemples"], "cell_type": "markdown"}, {"metadata": {"_uuid": "64f533d93f75d3b563bb47636a716e2ea51f3660", "_cell_guid": "28294278-765c-4b3a-b3ac-077aae087dfd"}, "source": ["\n", "<div id=\"exemples_courants\" />\n", "## Exemples courants\n", "TODO\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4316b025c6aa0e383360cfdee4ddf20fdaab1ede", "_cell_guid": "3997f0b6-9d59-4fa2-ad13-66f5be4facbb"}, "source": ["\n", "\n", "<div id=\"voca_auteurs\" />\n", "## Exemple d'utilisation du vocabulaire\n", "\n", "Les exemples que nous avons vus dans la section pr\u00e9c\u00e9dente sont des exemples courants de stylom\u00e9trie. Ce sont aussi des exemples tr\u00e8s g\u00e9n\u00e9raux en ce sens qu'ils ne n\u00e9cessitent pas la pr\u00e9sence d'un corpus d'entra\u00eenement : ils pourraient tout aussi bien servir dans le cas de classification *non-supervis\u00e9e* (et pas seulement *supervis\u00e9e* comme c'est le cas ici).\n", "\n", "Nous allons voir maintenant un exemple de fonction qui recourt au vocabulaire utilis\u00e9 dans le corpus d'entra\u00eenement. \u00c9tant donn\u00e9 une phrase, cette fonction retourne un vecteur de taille $3$ contenant la log-vraisemblance de l'utilisation de ce vocabulaire pour chacun des $3$ auteurs.\n", "\n", "D\u00e9coupons notre corpus d'apprentisage en deux : \n", "- un nouveau corpus d'apprentissage pour construire le dictionnaire (95% des donn\u00e9es)\n", "- un corpus de test qui nous permettra de visualiser les r\u00e9sultats (5% des donn\u00e9es)\n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["N_train = len(train_df)\n", "\n", "# d\u00e9coupe du train\n", "cut = round(N_train * 0.95)\n", "\n", "train = pd.DataFrame(train_df, index=range(cut))\n", "test = pd.DataFrame(train_df, index=range(cut,N_train))"], "cell_type": "code", "metadata": {"_kg_hide-input": false, "_uuid": "74e0d147b6062c14f29b225555b80efa52bbeb4c", "collapsed": true, "_kg_hide-output": false, "_cell_guid": "1f92cf5c-5aac-4af0-9d99-c78b01b8d835"}}, {"metadata": {"_uuid": "3ece3bdcf0b34e38ac1eef9083af905b564b438c", "_cell_guid": "bccac24e-5e6b-48be-a761-fcd84970f97d"}, "source": ["\n", "On commence par construire un dictionnaire contenant tous les mots apparaissant dans le corpus d'entra\u00eenement. Chaque entr\u00e9e de ce dictionnaire est associ\u00e9e \u00e0 un tableau donnant le nombre d'apparitions du mot pour chacun des diff\u00e9rents auteurs.\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["# construit \u00e0 partir d'un corpus d'entra\u00eenement un dictionnaire des mots utilis\u00e9s. Pour chaque mot, on remplit un dictionnaire qui \u00e0 chaque auteur associe le nombre de fois que ce mot est apparu chez l'auteur\n", "def voca_Authors(train):\n", "    voca = {}\n", "    for i,line in train.iterrows():\n", "        words = nltk.word_tokenize(line['text'])\n", "        for word in words:\n", "            word = word.lower()\n", "            if not word in voca:\n", "                voca[word] = {auth:0 for auth in authors}\n", "                voca[word][line['author']] = 1\n", "            else:\n", "                voca[word][line['author']] += 1\n", "    return voca\n", "\n", "print('Calcul du dictionnaire du vocabulaire.')\n", "voca_authors = voca_Authors(train)\n", "print('Taille du dictionnaire :',len(voca_authors),'\\n20 exemples tir\u00e9s de ce dictionnaire :\\n')\n", "for i,word in enumerate(voca_authors):\n", "    if i < 20:\n", "        print(word,voca_authors[word])"], "cell_type": "code", "metadata": {"_uuid": "4bdc96bd995f1c820f96f473d32418e2d0aef837", "collapsed": true, "_cell_guid": "45af3900-a473-42ca-85e5-30e60d229f69"}}, {"metadata": {"_uuid": "b241c0c0a9914bba5c54bbacdd9f5b73314f9976", "_cell_guid": "3e4ad7ae-1541-45d7-8ba6-abc46ed486f2"}, "source": ["Le tout maintenant est de bien voir ce que l'on calcule.\n", "On fera l'hypoth\u00e8se assez forte (c'est-\u00e0-dire assez fausse) que :\n", "- la longueur d'une phrase suit une loi ind\u00e9pendante de l'auteur (on peut regarder les histogrammes des longueurs de phrase en fonction de l'auteur \u00e0 l'adresse suivante : lien et constater qu'effectivement ces 3 histogrammes sont assez proches),\n", "- tous les mots d'une phrase sont tir\u00e9s ind\u00e9pendamment et identiquement selon une loi ne d\u00e9pendant que de l'auteur (c'est cette hypoth\u00e8se qui est tr\u00e8s forte et tr\u00e8s fausse).\n", "\n", "Gr\u00e2ce aux hypoth\u00e8ses d'ind\u00e9pendances, on peut calculer la vraisemblance que l'auteur $j$ ait \u00e9crit la phrase $s = [w_1,...,w_k]$ :\n", "$$ L (w_1, \\ ... \\ , \\ w_k \\ | \\ author = j) = \\prod_{i=1}^k \\mathbb P(\\text{author of $w_i$ = $j$} \\ | \\ \\text{word = $w_i$})$$\n", "\u00c0 pr\u00e9sent, prenons comme exemple le mot \"of\" : on a les statistiques suivantes : \n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["voca_authors['of']"], "cell_type": "code", "metadata": {"_uuid": "9ba8d66874b1b17b4b178f422cdf8d18d0ebffc7", "collapsed": true, "_cell_guid": "a2e64cb2-9ea5-47a0-8609-e4df5e52fc7e"}}, {"metadata": {"_uuid": "b3aada10014ab914f5da0e8468ddbf5c0693fbc5", "_cell_guid": "df3b5f6c-1602-418d-bdf6-907d1c0c49cb"}, "source": ["Il y a $8539+5568+5850 = 19957$ occurrences du mot \"of\" dans notre corpus d'entra\u00eenement. Edgar Poe est l'auteur de 8539 d'entre elles, c'est-\u00e0-dire d'une proportion $x_1 = 8539/19957 = 0.428$. Il est aussi l'auteur d'une proportion $\\lambda_1 = 0.40 < x_1$ de l'ensemble des phrases du corpus. Comme on a fait l'hypoth\u00e8se que la loi sur la longueur des phrases \u00e9tait ind\u00e9pendante de l'auteur, cela signifie qu'il \u00e9crit donc davantage le mot 'of' que sa sur-repr\u00e9sentativit\u00e9 dans le corpus lui permettrait ; c'est-\u00e0-dire que : $ p_1 := \\mathbb P(\\text{author of $'of' = 1$}) > 1/3$.\n", "Si l'on note $ p_j = \\mathbb P(\\text{author of $'of'  = j$})$ et $x_j$ la proportion d'apparitions dans le corpus d'entra\u00eenement, on a la relation suivante :\n", "$$ x_j = \\frac{\\lambda_j p_j}{\\sum_k \\lambda_k p_k}.$$\n", "\n", "On a donc les $x_j$ et les $\\lambda_j$ et l'on veut retrouver les $p_j$ pour calculer nos log-vraisemblances !\n", "\n", "\u00c0 noter que l'on peut calculer tr\u00e8s facilement $\\sum_k \\lambda_k p_k$ : $$ \\sum_j \\frac{x_j}{\\lambda_j} = \\sum_j \\sum_k \\lambda_k p_k p_j = \\sum_k \\lambda_k p_k$$\n", "... et ainsi retrouver la valeur $p_j$ que l'on recherche :\n", "$$ p_j = \\frac{x_j}{\\lambda_j} \\times \\sum_k\\frac{x_k}{\\lambda_k}$$\n", "\n", "On peut alors calculer la log-vraisemblance pour chacun des auteurs :\n", "$$ \\log L(w_1,...,w_k \\ | \\ author = j) = \\sum_{i=1}^k \\log(p_j(w_i)) $$\n", "\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["def vect_Voca(sentence, dict = voca_authors, zero = 0.01):\n", "    words = nltk.word_tokenize(sentence)\n", "    ret = [0]*len(authors)\n", "    for word in words:\n", "        word = word.lower()\n", "        if word in dict:\n", "            vect = [0]*len(authors)\n", "            nb_appar = 0\n", "            for auth in dict[word]:\n", "                nb_appar += dict[word][auth]\n", "            for auth in dict[word]:\n", "                vect[authors[auth]] = dict[word][auth]/nb_appar/lambdas[auth] # vect[j] = x_j/lambda_j = p_j / (Sum lambda_j p_j)\n", "            s = 1/sum(vect) # s = (Sum lambda_j p_j) \n", "            vect = [p_j_div_sum * s for p_j_div_sum in vect] # vect[j] = p_j\n", "\n", "            for j,p_j in enumerate(vect):\n", "                if p_j == 0:\n", "                    ret[j] += log(zero)\n", "                else:\n", "                    ret[j] += log(p_j)\n", "    return ret"], "cell_type": "code", "metadata": {"_uuid": "74cd03a6781236026d98fc574a863f5fe076fac0", "collapsed": true, "_cell_guid": "0c032078-7be7-4638-9d3d-b1379f7fefda"}}, {"metadata": {"_uuid": "edd3b6d45fff503ad256d634ff9e6bbc284720c3", "_cell_guid": "b6cd9df7-2333-44b1-94db-e849b5fe943d"}, "source": ["\n", "... et visualiser les r\u00e9sultats en queue de com\u00e8te :\n"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["# la matrice des phrases du corpus de texte transform\u00e9es \u00e0 l'aide de vect_Voca\n", "X = []\n", "col = [] # la couleur en fonction de l'auteur : Poe=>rouge, Lovecraft=>vert, Shelley=>bleu\n", "\n", "# calcul de X la matrice de taille N_tests * 3\n", "for i,line in test.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    X.append(vect)\n", "    col.append(colours[line['author']])\n", "X = np.array(X)\n", "\n", "## L'affichage\n", "\n", "#import plotly\n", "#from plotly.graph_objs import Scatter, Layout, Scatter3d\n", "fig = plt.figure(figsize=(15,15))\n", "ax = fig.add_subplot(111, projection='3d')\n", "\n", "ax.scatter(X[:,0], X[:,1], X[:,2], c=col, marker='+')\n", "#plotly.iplot({\n", " #   \"data\": [Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', \n", " #   marker=dict(size=4,\n", " #       color=col, \n", "  #      opacity=0.7\n", "  #  ))],\n", "  #  \"layout\": Layout(title=\"vect_Voca\", scene=dict(camera= dict(\n", " #   up=dict(x=0, y=0, z=1),\n", "  #  center=dict(x=0, y=0, z=0),\n", "  #  eye=dict(x=1.5, y=0.75, z=0.475)\n", "#)))\n", "#})"], "cell_type": "code", "metadata": {"_uuid": "c78f9435a933373a067f9a9ce44cc103cd60d85c", "collapsed": true, "_cell_guid": "2e8496fc-cf9e-484d-a7ef-5944f142212d"}}, {"metadata": {"_uuid": "04b239cbe0bfcf311b13584dcbc515da8a70061d", "_cell_guid": "eb312e20-22fa-4684-ae60-a2822007d71f"}, "source": ["<div id=\"kernel\" />\n", "## Exemple de kernel sur les structures arborescentes des phrases\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "6009daab7b109dcb74032980f569c07727a55ae2", "collapsed": true, "_cell_guid": "2ed5f3b6-e5bb-4569-831d-11d86872699b"}, "source": ["<div id=\"visualisation\" />\n", "# Visualisation des donn\u00e9es\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "03138e5a4f2d83b2fd88ef0db455a142e54e6a2e", "_cell_guid": "1a6a445b-2e08-438c-a714-fe050fd457a8"}, "source": ["<div id=\"ACP\" />\n", "## L'Analyse en Composantes Principales (ACP)\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "98b519f65441168ece4cccee7c3ca1c873133fce", "_cell_guid": "5b186af8-6962-4627-aa3a-38c0d2bcc280"}, "source": ["<div id=\"tSNE\" />\n", "## La t-SNE\n", "TODO"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4249749011e75bd4e46f4a8fdbbd058379312bcc", "_cell_guid": "c0d96ae4-eb47-4eac-ada1-5e7b267286c3"}, "source": ["<div id=\"resultats\" />\n", "# R\u00e9sultats (score : 0,40)\n", "\n", "Score obtenu simplement avec la log-vraisemblance de l'utilisation du vocabulaire :"], "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["from sklearn.neighbors import NearestNeighbors\n", "\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "\n", "voca_authors = voca_Authors(train)\n", "# la matrice des phrases du corpus de texte transform\u00e9es \u00e0 l'aide de vect_Voca\n", "M = [] # the train matrix\n", "Sol = [] # the solutions of the train matrix\n", "X = [] # the test matrix\n", "\n", "\n", "for i,line in train.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    M.append(vect)\n", "    Sol.append(line['author'])\n", "M = np.array(M)\n", "\n", "for i,line in test.iterrows():\n", "    vect = vect_Voca(line['text'])\n", "    X.append(vect)\n", "X = np.array(X)\n", "\n", "\n", "# The kNN algorithm\n", "adjunction = 0.025\n", "nb_ppv = 200 # number of nearest neighbors\n", "M_ppv = NearestNeighbors(n_neighbors=nb_ppv)\n", "M_ppv.fit(M)\n", "tab_kneighbors = M_ppv.kneighbors(X, return_distance=False)\n", "\n", "Probas = [] # the return\n", "\n", "for i,line in test.iterrows():\n", "    p = [0]*3\n", "    kneighbors = tab_kneighbors[i]\n", "    for kneighbor in kneighbors:\n", "        p[authors[Sol[kneighbor]]] += 1\n", "    s = sum(p)\n", "    p = [x/s for x in p]\n", "    Probas.append(p)\n", "\n", "# harmonisation of the probabilities\n", "for v in Probas:\n", "    for i,prob in enumerate(v):\n", "        v[i] = (prob + adjunction)/(1+3*adjunction)\n", "\n", "\n", "submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.loc[:,['EAP', 'HPL', 'MWS']] = Probas\n", "submission.to_csv(\"Log_likelihoof_on_vocabulary.csv\", index=False)\n", "submission.head()"], "cell_type": "code", "metadata": {"_uuid": "24e292a576b6715eeed15f0caf35d630a74e53d2", "collapsed": true, "_cell_guid": "831fcc88-c6c6-40e3-9949-eb91654cf539"}}], "metadata": {"language_info": {"nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1, "nbformat": 4}