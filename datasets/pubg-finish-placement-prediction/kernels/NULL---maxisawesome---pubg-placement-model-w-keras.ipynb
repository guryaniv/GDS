{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport os, gc, sys\ngc.enable()\n\n\nprint(os.listdir(\"../input\"))\ntrain_data = pd.read_csv(\"../input/train_V2.csv\")\ntest_data = pd.read_csv(\"../input/test_V2.csv\")\n\n# there's like one nan value so get rid of it\ntrain_data = train_data.dropna()\ntest_data = test_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78610a913f8a7d7384c2af074d2a27c2dd5d7c2c"},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\ntrain_data = reduce_mem_usage(train_data)\ntest_data = reduce_mem_usage(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7949e5947b0a7f70e45652de133e20dcce81c01a"},"cell_type":"code","source":"def feature_engineering(test, train, is_train=True):\n    if is_train: \n        print(\"processing train.csv\")\n        df = train\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        df = test\n    \n    # df = reduce_mem_usage(df)\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    \n    # df = df[:100]\n    \n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    \n    features.remove(\"matchType\")\n    \n    # matchType = pd.get_dummies(df['matchType'])\n    # df = df.join(matchType)    \n    \n    y = None\n    \n    print(\"get target\")\n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    else: df_out = df[['matchId','groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # print(\"get group sum feature\")\n    # agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    # agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    # df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    # df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # print(\"get group sum feature\")\n    # agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    # agg_rank = agg.groupby('matchId')[features].agg('sum')\n    # df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    # df_out = df_out.merge(agg_rank.reset_index(), suffixes=[\"_sum\", \"_sum_pct\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # print(\"get match type feature\")\n    # agg = df.groupby(['matchId'])[matchType.columns].agg('mean').reset_index()\n    # df_out = df_out.merge(agg, suffixes=[\"\", \"_match_type\"], how='left', on=['matchId'])\n    \n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n\n    X = np.array(df_out, dtype=np.float64)\n    \n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y, feature_names ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a4aab796911b4d00e7e45c5a56cf88604281885"},"cell_type":"code","source":"INPUT_DIR = \"../input/\"\nx_train, y_train, names = feature_engineering(test_data, train_data, True)\nx_test, _, _ = feature_engineering(test_data, train_data, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1338fdfae2ab3dc81eb0d0c18f80f9bea9c425b3"},"cell_type":"code","source":"scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=False).fit(x_train)\n#scaler = preprocessing.QuantileTransformer().fit(x)\n#scaler.transform(x)\nprint(\"x_train before scaling\", x_train.shape, x_train.max(), x_train.min())\nscaler.transform(x_train)\nprint(\"x_train after scaling\", x_train.shape, x_train.max(), x_train.min())\n\n# x_train_split, y_train split is our test set\n# x_test_split, y_test_split is our val set\n# This is because we've split the training set\nx_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1234)\nsplit = train_test_split(x_train, y_train, test_size = 0.2, random_state = 1234)\n\n#x_train = scaler.transform(x_train)\n#x_test = scaler.transform(x_test)\n#Y = scaler.transform(Y)\n\nprint(\"x_train_split after split\", x_train_split.shape, x_train_split.min(), x_train_split.max())\nprint(\"x_test_split after split\", x_test_split.shape, x_test_split.min(), x_test_split.max())\n#print(\"Y\", Y.shape, Y.min(), Y.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc9db648c7d7b42609f5713468aab6e158b003be"},"cell_type":"code","source":"from keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"059b7a23982dbc2accb2b75eb87baba7967ef4de"},"cell_type":"code","source":"start_vec_size = x_train_split.shape[1]\nmodel = Sequential()\nmodel.add(Dense(start_vec_size*2, input_shape=(start_vec_size,)))\nmodel.add(LeakyReLU(.1))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.2))\n\nmodel.add(Dense(256))\nmodel.add(LeakyReLU(.1))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.2))\n\nmodel.add(Dense(256))\nmodel.add(LeakyReLU(.1))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.2))\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(.1))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(.2))\n\nmodel.add(Dense(64))\nmodel.add(LeakyReLU(.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db4c402dd7c47dd2eb512cbc76a58b93762092e8"},"cell_type":"code","source":"optimizer = optimizers.Adam(lr=0.01, epsilon=1e-8, decay=1e-4, amsgrad=False, clipnorm=1.)\nearly_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode = 'min', patience=4, verbose=1, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"156a06947501065bd65d0fcd1c922f2e8392894b","scrolled":true},"cell_type":"code","source":"model.compile(optimizer=optimizer,\n              loss=\"mse\",\n              metrics=[\"mae\"])\nmodel.fit(x_train_split, y_train_split,\n          validation_data=(x_test_split, y_test_split),\n          epochs=8, batch_size=1024,\n          callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9505415c03852722b5223b309253eff85a7c57f"},"cell_type":"code","source":"# .0486 --> dropout at .3, no first star_vec_size*2 layer\n# .0319 w/ 256 layers\n# uhhh less with all 64 layers\n# .033 with x2 256 256 128 64 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b3906ffd3069591b953210b725b7aba3d7b9ac2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}