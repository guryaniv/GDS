{"cells":[{"metadata":{"_uuid":"c98ee7f740eb8c1f355540221e30b0f9bda809f5"},"cell_type":"markdown","source":"## Introduction\n\nHello Kaggle~! I'm Jon and happy to be a part of this competition. I'm excited by data science and am currently finishing up a bootcamp. Hopefully my contributions will help someone out while I further my learning. \n\nI started by looking at Bojan Tunguz's kernel available here: https://www.kaggle.com/tunguz/yaeda-yet-another-eda. I'm going to be making a lot of revisions as I progress towards better predictions.\n\nWelcome to the Santander Value Prediction Challenge. Santander Bank is looking to personalize service for their broad customer base. To do this they are looking to predict the value of transactions for each potential customer, and that's where we come in. \n\nWe are given both testing and training data sets which will we look into and analyze. It is important to note that because this is sensitive banking data it has been anonymized meaning we will need to rely on statistical results as opposed to domain knowledge for feature engineering. Unfortunately unless Santander releases more data, the data we have will not be expanded, but that doesn't stop feature engineering. I cleaned the data by checking for empty values, of which there were none, and removing any columns comprised entirely of zeros."},{"metadata":{"_uuid":"56f15633cc4bec0a57b9bd0803993f7d11b5d6b2"},"cell_type":"markdown","source":"#### Imports"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"346dccedd97f9dcdb81d807ee46d319fa5fe630c"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be6d269289b50a26ae037a6c39ff29adba2ffd2d"},"cell_type":"markdown","source":"#### Data Entry"},{"metadata":{"trusted":true,"_uuid":"1c95053c8c27f976e25c97c02f158d1b968109b8"},"cell_type":"code","source":"# Change the working directory\n# ATTN: You will need to change this locally.\n# os.chdir('C:/Users/jonda/OneDrive/Documents/Springboard/Santander_Capstone')\n\n# Read in the csv's for test, and train data sets.\ntest_df  = pd.read_csv('../input/test.csv')\ntrain_df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a55e8b13a9af0a8ab6515e61aa8f088ecaf53e47"},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"_uuid":"c0351dc6085277de6c98657dc8ebc4f8809ed7cb"},"cell_type":"markdown","source":"#### Heads"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df856d996c320d4e160a542dd4f4b2bd0f125825"},"cell_type":"code","source":"# Take a first look at the training set\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"50bb091336ac1d53300b6d2e2a2767d5822b374f"},"cell_type":"code","source":"# Take a first look at the testing set\ntest_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccb1f931d8b565cf48e46d5d13e4c2c9875858d5"},"cell_type":"markdown","source":"Santander did remark that this data is sparse, we will need to look into just how spare to determine which of the 4992/4993 columns contribute non-zero values and drop any irrelevant columns. We will also want to:\n- Check for missing values\n- Determine if all of the columns in test exist in train, and vice versa sans target \n- Do some initial plotting"},{"metadata":{"_uuid":"060e6f26ca37405c0788f8b1c69be5a58c6f1b80"},"cell_type":"markdown","source":"#### Missing Values"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fd06b450db3d8d464528e213389677b388c56b2e"},"cell_type":"code","source":"test_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6131bc9754aa7ebfc0c0e49cc1fb54cf73e40aa1"},"cell_type":"code","source":"train_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7637cf94312e0a05f159486a44ffd2dbc5bad139"},"cell_type":"markdown","source":"Thankfully there are no missing values, so we will be able to skip imputation for now."},{"metadata":{"_uuid":"ad5d393ff8f4303cdf7a873265de9e276b75fca8"},"cell_type":"markdown","source":"#### Column name verification"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e8c6dca56db0e24bd6388f47dec4105b9eaf1626"},"cell_type":"code","source":"train_columns = list(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bab31fc1fbaa6d25018a13db6ac5ced100bd1429"},"cell_type":"code","source":"test_columns  = list(test_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0580ad129df7544b2a5f78d75e32a5107fbfc6a0"},"cell_type":"code","source":"set(train_columns) - set(test_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b6a639c1b3cca0d3f6bb5c6e6ad56412d6a7b7a"},"cell_type":"markdown","source":"All the columns from the training set, sans target, are accounted for in the test set. "},{"metadata":{"_uuid":"9ae6ebfff55d4ea77087d004f37a807cdcfd5543"},"cell_type":"markdown","source":"#### Shapes"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"49f7d75eb5ed0c12ce918507aa0fa80222e77b22"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0071716e3b59dd7bb2b1b5595458f31c3351e9db"},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d5f4888995fadfdb2ffdba3a16c03d84a020ea7"},"cell_type":"markdown","source":"It's certainly odd that the test_df is 11 times larger than the training set. There was some concern that perhaps the sets were swapped but Santander has verified that the data sets are correct. The larger training set force the model to actually be predictive. If it can predict a much larger audience of users then it is indicative that the model keyed into actual signal instead of 'getting lucky' with noise,."},{"metadata":{"_uuid":"ce888dd8d4ad85a70133d38fa43c5cc349e24d9e"},"cell_type":"markdown","source":"#### Target plot"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"693dddf23a546efbfbf1b8e8dff17be547fc4b1d"},"cell_type":"code","source":"# A couple style settings\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"da63c3615e7183cc921ba8398f10b49d4e6539ab"},"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nplt.hist(train_df['target'])\nplt.title('Histogram of target values in the training set')\nplt.xlabel('Count')\nplt.ylabel('Target value')\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc4824969c6696a878891e98136ec85593f120ab"},"cell_type":"markdown","source":"Once you realize that the x-axis of this plot is scalled by 1e7 it makes perfect sense. Most customers at a bank have fewer than 1,000,000 dollar valuations, while some certainly do have higher valuations and one aspect of this project is to learn which customers will yield those high valuations thanks to the other anonymized properties, which for conversations sake I will assume are asset types such as stocks, registered collectibles, or bonds. Looking at a cumulative plot should help shed light on this this."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"66ac98d8790846d68af1762e2a837d18dd2d21cb"},"cell_type":"code","source":"x = train_df['target']\n\nfig, ax = plt.subplots(figsize=(12, 6))\nn_bins = 50\n\n# plot the cumulative histogram\nn, bins, patches = ax.hist(x, n_bins, normed=1, histtype='step',\n                           cumulative=True, label='Empirical')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5d456dc32d610d18eb31b8fd96f9d09368fb305"},"cell_type":"markdown","source":"#### Target Statistics"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"acb6efdb02488105b1642e576373bff143e41711"},"cell_type":"code","source":"train_df['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a94970e5cd4ba58b06cf11d1cff7b66c43bdb09b"},"cell_type":"markdown","source":"#### How sparse is the feature data?\nWe have nearly 5000 feature columns, but from the .head() they seem to be quite sparse. But just how sparse are they? If any columns are entirely zero then they should affect all predictions equally and should be removed to save complexity. Let's remove those columns first."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"90b1cc004b2e863efc48bc1435d441c21012c038"},"cell_type":"code","source":"train_df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"86c237448b16f41bb028bf1931465b4bce36be9e"},"cell_type":"code","source":"# Drop all columns that consist of only zeros.\ndf = train_df.loc[:, (train_df != 0).any(axis=0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c66d20f66964d8d100196e4b0763f02cf6bda41"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"61e42fba8177254d3eaa41b1cd9ab22703a1d3a2"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2193e299dd1729a3692ec8181ec943d53061957d"},"cell_type":"code","source":"nz = list(df.columns.values) \nnz.remove('ID')\nnz.remove('target')\ntype(nz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1a62cf1c5084dd7a3ed5f4b85f9a400d1ad96268"},"cell_type":"code","source":"# This next bit was inspired by Bojan Tunguz's idea to determine just how sparse each column is\n# https://www.kaggle.com/tunguz/yaeda-yet-another-eda\ntrain_nz = pd.DataFrame({'Percentile':((df[nz].values)==0).mean(axis=0),\n                           'Column' : nz})\ntrain_nz.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e08ff6b0a1218365c4c701d7086cfb2d98353eee"},"cell_type":"code","source":"plt.figure(figsize = (12,5))\nplt.hist(train_nz['Percentile'], bins = 100)\nplt.title('Percentge of column that has value 0')\nplt.xlabel('Percentage zero')\nplt.ylabel('Number of columns')\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3cf174903ed05926770be0409a1dd3f27a55f0f9"},"cell_type":"code","source":"train_nz['Percentile'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6833317c161d35bde5691463c4cf44ed852964f3"},"cell_type":"markdown","source":"Notice how the 50th percentile is far above the mean, this is due to the low minimum value and the upper bound on the highest values. More interseting is that standard deviation is 4% while our values are incredibly tightly packed. This tells us that vast majority of features are very underused by the public. Perhaps this is a wide varity of assets and stocks for individual companies. As there are an massive amount of companies to invest in, many are left with few investors. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8aa0aece7018f0ed30a9c0ee6bdebe3b3f35e85b"},"cell_type":"code","source":"sns.set_style('ticks')\nfig, ax = plt.subplots()\n\nfig.set_size_inches(11, 1)\nsns.boxplot(x=\"Percentile\",\n            data=train_nz, palette=\"Set3\", ax = ax)\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bb68da6142507edfcde39621928373fa20e97dd"},"cell_type":"markdown","source":"The cluster on the far left of the plot is particularly interesting. Those columns have the most nonzero data points."},{"metadata":{"_uuid":"96f5528b03eda893672add5d3d29dd47bb8f2e77"},"cell_type":"markdown","source":"### Machine Learning"},{"metadata":{"_uuid":"03e786bc98eaee523c2d1eb92eacaef3f5a75f39"},"cell_type":"markdown","source":"#### Which methods to try?\nBecause we are attempting to predict a value we will look at regression methods. Because of the medium size of our sample we will begin RandomForestRegressors, and then move towards ensemble methods like boosted trees such as AdaBoost, LightGBM, and XGBoost.\n\nRandom forest will be very usefull in identifying variable importance. \n\nCertain high 0 assest could be indicative of high value accounts. Example: Very few people own high rises hotels. But those who do probably have high valuation accounts. \n\nTo give ourself a baseline our first submission will simply use all the data and no hyperparameter tuning."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6e634fe0d1d20a10ad70035f503047185f7a5385"},"cell_type":"code","source":"# The approach for machine learning was also inspired by Bojan Tunguz's work\n\ny_train = train_df.target.values\nprint('y shape: ', y_train.shape)\nX_train = train_df[nz]\nprint('\\n')\nprint('X_train 5 line head below: ')\nX_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fd454efff98b773511b96d1c0ad49b2fce3fa16e"},"cell_type":"code","source":"clf = RandomForestRegressor()\nclf.fit(X_train, y_train)\n\npreds = clf.predict(test_df[nz])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"951715566f2dfc9cc5c81ab91d5d180291f781b1"},"cell_type":"code","source":"sample_submission = pd.read_csv(\"C:/Users/jonda/OneDrive/Documents/Springboard/Santander_Capstone/sample_submission.csv\")\nsample_submission.target = preds\nsample_submission.to_csv('simple_rfr_all_default.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c72ed1d93dc15e2a4c3bc655f6dc4226056be2a"},"cell_type":"markdown","source":"This default RandomForestRegressor scored 1.73. There is much work to be done! Thankfully we are planning to do more than just an all default RandomForestRegressor! Our goal is to reach the top 10%, this means we need to reach a score of 1.38. Now it's important to not get entirely bogged down in score. The learning process is the most important aspect of this exercise, personally."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"59e6c65ee92e683d11b2e0aa2232b28a3fec3999"},"cell_type":"code","source":"train_df_no_target = train_df.loc[:, train_df.columns != 'target']\ntype(train_df_no_target)\ntrain_df_target = train_df.loc[:, train_df.columns == 'target']\ntype(train_df_target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9033cc35bdec9f456004194db8332d5b4b170baa"},"cell_type":"markdown","source":"Let's try hyperparameter tuning on the all features data\n\nThis first section is setting up the grid and importing the necessary modules"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4164026312464629a65bc47533aede6e71d5f503"},"cell_type":"code","source":"############### Start: Randomized Search CV ##################################\n\n# Look at parameters used by our current forest\n# from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\n\nfrom pprint import pprint\n\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n# Method of selecting samples for training each tree\n# bootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\npprint(random_grid)\n\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78ca5862a29a87bbc51436f1b85b015de5ed153c"},"cell_type":"markdown","source":"Warning: This next section can take a long time to run as it was set to run off of one core for stability."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bb4f0d9b188395ff4d328393dc0fa478a084d5e1"},"cell_type":"code","source":"# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 1)\n\n# Fit the random search model\n# In order to test these models I will need to do a train test split with the training data-set. \nX_train, X_test, y_train, y_test = train_test_split(train_df[nz], train_df.target.values, test_size=0.2)\n\n\n# rf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13f8bb4f6073d703d4bdeb26dee617be1a04ffdd"},"cell_type":"markdown","source":"Set up the evaluate function"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"15fb9b19c232278a6a844337cb6fff903647659a"},"cell_type":"code","source":"# Evaluation of Random Search\ndef evaluate(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    errors = np.sqrt(mean_squared_error(y_test, predictions))\n    print('Model Performance')\n    print('MSE of: ', errors)\n    \n    return errors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbaac80e71293889d12510d7ecd90b0810be22d8"},"cell_type":"markdown","source":"This section can only be ran after running the rf_random.fit() block"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f1261ed189fab4a66cc28a86bc2e0be467d4c4a7"},"cell_type":"code","source":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)\n\n\nbest_random = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\nbest_random.fit(X_train , y_train)\n\nrandom_accuracy = evaluate(best_random, X_test, y_test)\n\nprint('\\n')\nprint('Base Accuracy: ', base_accuracy)\nprint('\\n')\nprint('Random Accuracy: ', random_accuracy)\nprint('Improvement of {:0.2f}%.'.format((random_accuracy - base_accuracy) / base_accuracy))\n\nprint('\\n')\nprint('RF_Randomized_Search_CV')\nprint('\\n')\n\n\n# =============================================================================\n# Best param set for random forest regression on Registered Users\n# =============================================================================\n# RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n#           max_features='sqrt', max_leaf_nodes=None,\n#           min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=2, min_samples_split=5,\n#           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n#           oob_score=False, random_state=None, verbose=0, warm_start=False)\n# =============================================================================\n\n################# End: Randomized Search CV ##################################","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13fe73e86d819df1d15bccabcf250ca420e60666"},"cell_type":"markdown","source":"RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)"},{"metadata":{"_uuid":"1e2b08db859a9c1c3477b4ee70cf4cf09dd2a058"},"cell_type":"markdown","source":"#### All features searched forest submission"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c213c7b7cd7e5f3940888b69f40cfe3c9676875"},"cell_type":"code","source":"y_train = train_df.target.values\nX_train = train_df[nz]\n\n\nclf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(test_df[nz])\n\nsample_submission = pd.read_csv(\"C:/Users/jonda/OneDrive/Documents/Springboard/Santander_Capstone/sample_submission.csv\")\nsample_submission.target = preds\nsample_submission.to_csv('simple_rfr_searchCV_all_features.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e71529b490afdee8ba5c89f5eaf924a77d70a7b"},"cell_type":"markdown","source":"This did not improve performance significantly so we are going to pursue more refine feature selection and consider only the cluster of values with fewer than 70% zeros."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3f1abc276c8747f0272b647804ecdb3bdf81612f"},"cell_type":"code","source":"train_nz['Percentile'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"da31252035578f3696f4c9e1424503f2c3244f17"},"cell_type":"code","source":"sub_seventy = pd.DataFrame(train_nz.loc[train_nz['Percentile'] < 0.7])\nsub_seventy_col_series = sub_seventy['Column']\nsub_seventy_col = list(sub_seventy_col_series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b82967fb6012d87be9943f147a1c578c399074a2"},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.boxplot(sub_seventy['Percentile'], patch_artist = True, vert = False)\nplt.title('Boxplot for percentage zero of columns sub seventy')\nplt.xlabel('Percentage zero')\nplt.show()\nplt.clf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"af38eff53b6991402384fa6f52090ea447b0d3e8"},"cell_type":"code","source":"len(sub_seventy_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d2b71efea62089e63bef098dda6402b94978c52f"},"cell_type":"code","source":"sub_seventy_df = train_df[sub_seventy_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4a10eeb20649240c1c1092ae4354ff5df35cf900"},"cell_type":"code","source":"sub_seventy_df['target'] = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9a90669a259fe0ed12c0d717cca5f705551e4bea"},"cell_type":"code","source":"sub_seventy_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a6a2cf376bae7e132c2c9b5a3d5940d1d040d6c6"},"cell_type":"code","source":"sub_seventy_y = sub_seventy_df['target']\nsub_seventy_X = sub_seventy_df.loc[: , sub_seventy_df.columns != 'target']\n\ntrain = train_df[sub_seventy_col]\n# train['target'] = train_df['target']\n\ntest = test_df[sub_seventy_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0d56b9a2a95232f3e5545179714200d208a3d107"},"cell_type":"code","source":"# Functions from Haim Feldman's kernal: https://www.kaggle.com/haimfeld87/randomforest-with-50-features\nfrom sklearn import model_selection\n\nY = train_df['target']\nY = np.log(Y+1)\n\ntest_id = test_df.ID\n\ndef rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\n\nkf = model_selection.KFold(n_splits=10, shuffle=True)\ndef runRF(x_train, y_train,x_test, y_test,test):\n    #model=RandomForestRegressor(bootstrap=True, max_features=0.75, min_samples_leaf=11, min_samples_split=13, n_estimators=100)\n    model = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\n    model.fit(x_train, y_train)\n    y_pred_train=model.predict(x_test)\n    mse=rmsle(np.exp(y_pred_train)-1,np.exp(y_test)-1)\n    y_pred_test=model.predict(test)\n    return y_pred_train,mse,y_pred_test\n\npred_full_test_RF = 0    \nrmsle_RF_list=[]\n\nfor dev_index, val_index in kf.split(train):\n    dev_X, val_X = train.loc[dev_index], train.loc[val_index]\n    dev_y, val_y = Y.loc[dev_index], Y.loc[val_index]\n    ypred_valid_RF,rmsle_RF,ytest_RF=runRF(dev_X, dev_y, val_X, val_y,test)\n    print(\"fold_ RF _ok \"+str(rmsle_RF))\n    rmsle_RF_list.append(rmsle_RF)\n    pred_full_test_RF = pred_full_test_RF + ytest_RF\n    \nrmsle_RF_mean=np.mean(rmsle_RF_list)\nprint(\"Mean cv score : \", np.mean(rmsle_RF_mean))\nytest_RF=pred_full_test_RF/10\n\n\nytest_RF = np.exp(ytest_RF)-1\nout_df = pd.DataFrame(ytest_RF)\nout_df.columns = ['target']\nout_df.insert(0, 'ID', test_id)\nout_df.to_csv(\"RF_\" + str(rmsle_RF_mean) + \"_.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14155d40dba814a371c2bbd51794fbfc1e3d78de"},"cell_type":"markdown","source":"Much better. Now we are up to 1.48 on the public submissions. To continue the project I would like to look at putting quartiles of the data together, such as the fourth and the first quartile. Then trying not only the searched parameter random forest, but expanding the to other algorithms and trying my hand at feature engineering. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"63b12373c35522f2fa1ec858f0f1d9362d6fa5f0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}