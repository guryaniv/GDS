{"cells":[{"metadata":{"_cell_guid":"442d2a2f-cbe8-4aa6-9941-5ab71da4fa73","_uuid":"6460a0841f81a92761299622855417aa4df8b806"},"cell_type":"markdown","source":"# Inspired by  \nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\n## Contribution from  \nHiep Nguyen  \nAssociated Model RNN + Ridge  \nhttps://www.kaggle.com/nvhbk16k53/associated-model-rnn-ridge","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"08c1fea4-bcc0-4c3e-89f4-e5030c57f23e","_uuid":"0776269023a99d55d3a88fc875752571bc995107","collapsed":true,"trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, Bidirectional\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\nfrom keras import backend as K\n\ndef rmsle(Y, Y_pred):\n    # Y and Y_red have already been in log scale.\n    assert Y.shape == Y_pred.shape\n    return np.sqrt(np.mean(np.square(Y_pred - Y )))\n\ntrain_df = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv')\ntest_df = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv')\n# print(train_df.shape, test_df.shape)\n\ndef split_cat(text):\n    try:\n        return text.split(\"/\")\n    except:\n        return (\"No Label\", \"No Label\", \"No Label\")\n\n\ndef fill_missing_values(df):\n    df.category_name.fillna(value=\"Other\", inplace=True)\n    df.general_cat.fillna(value=\"Other\", inplace=True)\n    df.subcat_1.fillna(value=\"Other\", inplace=True)\n    df.subcat_2.fillna(value=\"Other\", inplace=True)\n    df.brand_name.fillna(value=\"missing\", inplace=True)\n    df.item_description.fillna(value=\"None\", inplace=True)\n    return df\n\ntrain_df['general_cat'], train_df['subcat_1'], train_df['subcat_2'] = \\\n    zip(*train_df['category_name'].apply(lambda x: split_cat(x)))\n\ntest_df['general_cat'], test_df['subcat_1'], test_df['subcat_2'] = \\\n    zip(*test_df['category_name'].apply(lambda x: split_cat(x)))\n\n# train_df.drop('category_name', axis=1, inplace=True)\n# test_df = test_df.drop('category_name', axis=1, inplace=True)\n\ntrain_df = fill_missing_values(train_df)\ntest_df = fill_missing_values(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1ffa417-07b1-4fe3-a07b-77fee962eff4","_uuid":"ee279f608e047abe63ee49c1cb180a6c6a0cd349","collapsed":true,"trusted":true},"cell_type":"code","source":"submission: pd.DataFrame = test_df[['test_id']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba2480be-7657-44eb-8556-94278ef75e92","_uuid":"7517d96d0f0cf473f39da8ec9a4e1e579e2441f6","collapsed":true,"trusted":true},"cell_type":"code","source":"test_df = test_df.drop(\"test_id\",axis=1)\n\n# Scale target variable to log.\ntrain_df[\"target\"] = np.log1p(train_df.price)\n\n# Split training examples into train/dev examples.\ntrain_df, dev_df = train_test_split(train_df, random_state=347, train_size=0.99)\n\nY_train = train_df.target.values.reshape(-11, 1)\nY_dev = dev_df.target.values.reshape(-1, 1)\n\n# Calculate number of train/dev/test examples.\nn_trains = train_df.shape[0]\nn_devs = dev_df.shape[0]\nn_tests = test_df.shape[0]\n# print(\"Training on\", n_trains, \"examples\")\n# print(\"Validating on\", n_devs, \"examples\")\n# print(\"Testing on\", n_tests, \"examples\")\n\nfull_df = pd.concat([train_df, dev_df, test_df])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54cf8449-47be-4090-ace1-bc737e1dd3a5","_uuid":"818e85ca99bcf6198f710b303887ad4cc4ca84e4","collapsed":true,"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad6f14e6-4a39-4ca9-852f-a5ad63bc4d75","_uuid":"dc70b4ee240dc033974ceba4cd1a372585d145bb"},"cell_type":"markdown","source":"## Preprocess text\n### less unknown words for GloVe dataset","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a598ad87-7f0e-4246-9896-d183d33515a4","_uuid":"918bd59ee50cfb87dbcd8f34d8919d559ce98455","collapsed":true,"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    \n    text = text.lower()\n    \n    bad_char_re = r\"[^a-z1-9',.!;:/|$%&+-=()]\"\n    text = re.sub(bad_char_re,\" \",text)\n    \n    text = re.sub( r'([1-9])([a-z])', r'\\1 \\2', text)\n    text = re.sub( r'([a-z])([1-9])', r'\\1 \\2', text)\n    text = re.sub( r'([1-9])\\'([1-9])', r' ', text)\n    text = text.replace(\"' \", \" \").replace(\"’ \", \" \")\\\n        .replace(\"'re\", \" are\").replace(\"’re\", \" are\")\\\n        .replace(\"'ve\", \" have\").replace(\"’ve\", \" have\")\\\n        .replace(\"n't\",\" not\").replace(\"n’t\",\" not\")\\\n        .replace(\"'ll\",\" will\").replace(\"’ll\",\" will\")\\\n        .replace(\"it's\",\"it is\").replace(\"it’s\",\"it is\")\\\n        .replace(\"'d'\",\" had\").replace(\"’d\",\" had\")\\\n        .replace(\"'s\",\" 's\").replace(\"’s\",\" 's\")\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34af81e5-0e9f-44b4-b2e7-ae3ef33f8fea","_uuid":"0a5705ed38b5c5b663d3737025a46d0cb28d96b5","collapsed":true,"trusted":true},"cell_type":"code","source":"full_df.item_description = [preprocess(a) for a in full_df.item_description.values]\nfull_df.general_cat = [preprocess(a) for a in full_df.general_cat.values]\nfull_df.subcat_1 = [preprocess(a) for a in full_df.subcat_1.values]\nfull_df.subcat_2 = [preprocess(a) for a in full_df.subcat_2.values]\nfull_df.name = [preprocess(a) for a in full_df.name.values]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fc77af2-76bc-4c9e-8930-227ddf90e52c","_uuid":"1f26a0d9c599c4fc1d2040de3a90a886f6774dcc","collapsed":true,"trusted":true},"cell_type":"code","source":"full_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce65705c-e636-443e-a931-d6f0749c9cae","_uuid":"a18c60bfb99f154796b8b7e494ee9812b8bd678c","collapsed":true,"trusted":true},"cell_type":"code","source":"embeddings_index_42b = dict()\nf_42B = open('../input/glove-stanford/glove.42B.300d.txt')\nfor line in f_42B:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index_42b[word] = coefs\nf_42B.close()\nprint('Loaded %s word vectors.' % len(embeddings_index_42b))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59880c4e-498d-4965-baff-6344f43aaca2","_uuid":"e96553f3ed18417c79980bc37c2d874f0a28d5ea","collapsed":true,"trusted":true},"cell_type":"code","source":"%%time\nt = Tokenizer()\nt.fit_on_texts(full_df.item_description)\nfull_df['seq_item_description'] = t.texts_to_sequences(full_df.item_description)\nitem_description_vocab_size = len(t.word_index) + 1\nprint(\"description vocab_size: \",item_description_vocab_size)\nitem_description_embedding_matrix = np.zeros((item_description_vocab_size, 300))\nitem_description_undefined_count = 0\nitem_description_defined_count = 0\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index_42b.get(word)\n\tif embedding_vector is not None:\n\t\titem_description_embedding_matrix[i] = embedding_vector\n\t\titem_description_defined_count += 1\n\telse:\n\t\titem_description_embedding_matrix[i] = np.random.rand(1,300)*6\n\t\titem_description_undefined_count += 1\nprint(\"Defined words:\",item_description_defined_count,\"Undefined words:\",item_description_undefined_count)\n\nt.fit_on_texts(full_df.name)\nfull_df['seq_name'] = t.texts_to_sequences(full_df.name)\nname_vocab_size = len(t.word_index) + 1\nprint(\"name vocab_size: \",name_vocab_size)\nname_embedding_matrix = np.zeros((name_vocab_size, 300))\nname_undefined_count = 0\nname_defined_count = 0\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index_42b.get(word)\n\tif embedding_vector is not None:\n\t\tname_embedding_matrix[i] = embedding_vector\n\t\tname_defined_count += 1\n\telse:\n\t\tname_embedding_matrix[i] = np.random.rand(1,300)*6\n\t\tname_undefined_count += 1\nprint(\"Defined words:\",name_defined_count,\"Undefined words:\",name_undefined_count)\n\nt.fit_on_texts(full_df.general_cat)\nfull_df['seq_general_cat'] = t.texts_to_sequences(full_df.general_cat)\ngeneral_cat_vocab_size = len(t.word_index) + 1\nprint(\"general_cat vocab_size: \",general_cat_vocab_size)\ngeneral_cat_embedding_matrix = np.zeros((general_cat_vocab_size, 300))\ngeneral_cat_undefined_count = 0\ngeneral_cat_defined_count = 0\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index_42b.get(word)\n\tif embedding_vector is not None:\n\t\tgeneral_cat_embedding_matrix[i] = embedding_vector\n\t\tgeneral_cat_defined_count += 1\n\telse:\n\t\tgeneral_cat_embedding_matrix[i] = np.random.rand(1,300)*6\n\t\tgeneral_cat_undefined_count += 1\nprint(\"Defined words:\",general_cat_defined_count,\"Undefined words:\",general_cat_undefined_count)        \n\nt.fit_on_texts(full_df.subcat_1)\nfull_df['seq_subcat_1'] = t.texts_to_sequences(full_df.subcat_1)\nsubcat_1_vocab_size = len(t.word_index) + 1\nprint(\"subcat_1 vocab_size: \",subcat_1_vocab_size)\nsubcat_1_embedding_matrix = np.zeros((subcat_1_vocab_size, 300))\nsubcat_1_undefined_count = 0\nsubcat_1_defined_count = 0\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index_42b.get(word)\n\tif embedding_vector is not None:\n\t\tsubcat_1_embedding_matrix[i] = embedding_vector\n\t\tsubcat_1_defined_count += 1\n\telse:\n\t\tsubcat_1_embedding_matrix[i] = np.random.rand(1,300)*6\n\t\tsubcat_1_undefined_count += 1\nprint(\"Defined words:\",subcat_1_defined_count,\"Undefined words:\",subcat_1_undefined_count)    \n\nt.fit_on_texts(full_df.subcat_2)\nfull_df['seq_subcat_2'] = t.texts_to_sequences(full_df.subcat_2)\nsubcat_2_vocab_size = len(t.word_index) + 1\nprint(\"subcat_2 vocab_size: \",subcat_2_vocab_size)\nsubcat_2_embedding_matrix = np.zeros((subcat_2_vocab_size, 300))\nsubcat_2_undefined_count = 0\nsubcat_2_defined_count = 0\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index_42b.get(word)\n\tif embedding_vector is not None:\n\t\tsubcat_2_embedding_matrix[i] = embedding_vector\n\t\tsubcat_2_defined_count += 1\n\telse:\n\t\tsubcat_2_embedding_matrix[i] = np.random.rand(1,300)*6\n\t\tsubcat_2_undefined_count += 1\nprint(\"Defined words:\",subcat_2_defined_count,\"Undefined words:\",subcat_2_undefined_count)    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d283d2c1-ec81-4d22-a81e-ad65f2cbd69f","_uuid":"144ce74c56d3a2576feb4685bd6505941be0da2a","collapsed":true,"trusted":true},"cell_type":"code","source":"max_seq_item_description_len = max([len(a) for a in full_df.seq_item_description])\nmax_seq_item_description_len = min(30, max_seq_item_description_len) #save memory\nfull_df.seq_item_description = [a[:max_seq_item_description_len] for a in full_df.seq_item_description]\n\nmax_seq_name_len = max([len(a) for a in full_df.seq_name])\nmax_seq_name_len = min(6, max_seq_name_len) #save memory\nfull_df.seq_name = [a[:max_seq_name_len] for a in full_df.seq_name]\n\nmax_seq_general_cat_len = max([len(a) for a in full_df.seq_general_cat])\nmax_seq_general_cat_len = min(6, max_seq_general_cat_len) #save memory\nfull_df.seq_general_cat = [a[:max_seq_general_cat_len] for a in full_df.seq_general_cat]\n\nmax_seq_subcat_1_len = max([len(a) for a in full_df.seq_subcat_1])\nmax_seq_subcat_1_len = min(5, max_seq_subcat_1_len) #save memory\nfull_df.seq_subcat_1 = [a[:max_seq_subcat_1_len] for a in full_df.seq_subcat_1]\n\nmax_seq_subcat_2_len = max([len(a) for a in full_df.seq_subcat_2])\nmax_seq_subcat_2_len = min(4, max_seq_subcat_2_len) #save memory\nfull_df.seq_subcat_2 = [a[:max_seq_subcat_2_len] for a in full_df.seq_subcat_2]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ed5aa19-2424-4988-a24b-b6397f7446f4","_uuid":"8610eb869f05df976e7933466719f12d4138a506","collapsed":true,"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(full_df.brand_name)\nfull_df.brand_name = le.transform(full_df.brand_name)\ndel le\nMAX_BRAND = np.max(full_df.brand_name.max()) + 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83037b08-1f09-422f-96ab-6bf50c102247","_uuid":"048269b5ef42dc4e30c012ec68c36ada5bfda064","collapsed":true,"trusted":true},"cell_type":"code","source":"import gc\ndel embeddings_index_42b\ndel f_42B\ndel train_df\ndel dev_df\ndel test_df\ngc.collect()\nfull_df = full_df.drop(\"item_description\",axis=1)\nfull_df = full_df.drop(\"general_cat\",axis=1)\nfull_df = full_df.drop(\"subcat_1\",axis=1)\nfull_df = full_df.drop(\"subcat_2\",axis=1)\nfull_df = full_df.drop(\"name\",axis=1)\nfull_df = full_df.drop(\"category_name\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7354a62-9ffa-42f1-ad25-439b74ae229c","_uuid":"45dac8089a80a75fc04af43ae5f374eabde0a700","collapsed":true,"trusted":true},"cell_type":"code","source":"full_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ada8e670-2bf9-4160-8e53-030cb89959a2","_uuid":"c7f782926cb99d1ad988145bf27251a8d6ed9c88","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_keras_data(df):\n    X = {\n        'name': pad_sequences(df.seq_name, maxlen=max_seq_name_len,padding='post', truncating='post'),\n        'item_desc': pad_sequences(df.seq_item_description, maxlen=max_seq_item_description_len,padding='post', truncating='post'),\n        'brand_name': np.array(df.brand_name),\n        # 'category_name': np.array(df.category_name),\n        'general_cat': pad_sequences(df.seq_general_cat, maxlen=max_seq_general_cat_len,padding='post', truncating='post'),\n        'subcat_1': pad_sequences(df.seq_subcat_1, maxlen=max_seq_subcat_1_len,padding='post', truncating='post'),\n        'subcat_2': pad_sequences(df.seq_subcat_2, maxlen=max_seq_subcat_2_len,padding='post', truncating='post'),\n        'item_condition': np.array(df.item_condition_id),\n        'num_vars': np.array(df[[\"shipping\"]]),\n    }\n    return X","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ede83c6-cd34-415f-aed9-e25217d8c057","_uuid":"65f25ac2ecfafae50a87a7a84357afe954483847","collapsed":true,"trusted":true},"cell_type":"code","source":"train = full_df[:n_trains]\ndev = full_df[n_trains:n_trains+n_devs]\ntest = full_df[n_trains+n_devs:]\n\nX_train = get_keras_data(train)\nX_dev = get_keras_data(dev)\nX_test = get_keras_data(test)\ndel full_df\ndel train\ndel dev\ndel t\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3014e009-be76-4c1c-8a8d-08b4885d1591","_uuid":"7ad3281870533fcecf27fa39ed48de6de62e47c6","collapsed":true,"trusted":true},"cell_type":"code","source":"def new_rnn_model(lr=0.01, decay=1e-6):    \n    # Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    general_cat = Input(shape=[X_train[\"general_cat\"].shape[1]], name=\"general_cat\")\n    subcat_1 = Input(shape=[X_train[\"subcat_1\"].shape[1]], name=\"subcat_1\")\n    subcat_2 = Input(shape=[X_train[\"subcat_2\"].shape[1]], name=\"subcat_2\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n\n    # Embeddings layers\n    emb_name = Embedding(name_embedding_matrix.shape[0], \n                              name_embedding_matrix.shape[1],\n                              weights=[name_embedding_matrix], \n                              input_length=max_seq_name_len, \n                              trainable=False)(name)\n    emb_item_desc = Embedding(item_description_embedding_matrix.shape[0], \n                              item_description_embedding_matrix.shape[1],\n                              weights=[item_description_embedding_matrix], \n                              input_length=max_seq_item_description_len, \n                              trainable=False)(item_desc)\n    emb_general_cat = Embedding(general_cat_embedding_matrix.shape[0], \n                              general_cat_embedding_matrix.shape[1],\n                              weights=[general_cat_embedding_matrix], \n                              input_length=max_seq_general_cat_len, \n                              trainable=False)(general_cat)\n    emb_subcat_1 = Embedding(subcat_1_embedding_matrix.shape[0], \n                              subcat_1_embedding_matrix.shape[1],\n                              weights=[subcat_1_embedding_matrix], \n                              input_length=max_seq_subcat_1_len, \n                              trainable=False)(subcat_1)\n    emb_subcat_2 = Embedding(subcat_2_embedding_matrix.shape[0], \n                              subcat_2_embedding_matrix.shape[1],\n                              weights=[subcat_2_embedding_matrix], \n                              input_length=max_seq_subcat_2_len, \n                              trainable=False)(subcat_2)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    \n\n    # rnn layers\n    rnn_layer1 = Bidirectional(GRU(8, return_sequences=True)) (emb_item_desc)\n    rnn_layer1 = GRU(8) (rnn_layer1)\n    \n    # main layers\n    main_l = concatenate([\n        Flatten() (emb_brand_name),\n        Flatten() (emb_name),\n        Flatten() (emb_general_cat),\n        Flatten() (emb_subcat_1),\n        Flatten() (emb_subcat_2),\n        item_condition,\n        rnn_layer1,\n        num_vars\n    ])\n    \n    main_l = Dense(64)(main_l)\n    main_l = Activation('elu')(main_l)\n    main_l = Dense(64)(main_l)\n    main_l = Activation('elu')(main_l)\n\n    # the output layer.\n    output = Dense(1, activation=\"elu\") (main_l) \n\n    model = Model([name, item_desc, brand_name , general_cat, subcat_1, subcat_2, item_condition, num_vars], output)\n\n    # SGD momentum leads to wrong direction and increasing loss\n    # Use tanh activation instead of \"elu\" or \"relu\" to avoid exploding loss to NaN\n    # optimizer = SGD(lr=lr,momentum=0.9,decay=decay,nesterov=True)  \n    \n    optimizer = Adam(lr=lr,decay=decay)\n    model.compile(loss=\"mse\", optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2415701b-c2a1-4c87-a3e6-c980cfe4bb2d","_uuid":"9a48bf90f71d55c8ade8c93d8e378c20bfd164ad","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set hyper parameters for the model.\nbatch_sizes = [1024,512]\nepochs = 1\n# Calculate learning rate decay.\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = 0\nfor batch_size in batch_sizes:\n    steps += int(n_trains / batch_size) * epochs\nlr_init, lr_fin = 0.01, 0.0005\nlr_decay = exp_decay(lr_init, lr_fin, steps)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f01e75fb-2d6c-429b-854a-e6e66972a652","_uuid":"e8c7c4b3a002383f219ac90f361733f5ba1887f0"},"cell_type":"markdown","source":"### Code for loss debug","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"56472b95-6f50-443b-80e1-21c813840a84","_uuid":"f9b8d9d0ec4cc2a260d0bceb38b32207ca5d5c66","collapsed":true,"trusted":true},"cell_type":"code","source":"# X_train_small = {}\n# for i in X_train:\n#     X_train_small[i] = X_train[i][:1000]\n# Y_train_small = Y_train[:1000]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0005e585-24ca-49a0-9771-8dcb4b033a54","_uuid":"684b1341f7cd84065c0d396823c018d82eeecf8d","collapsed":true,"trusted":true},"cell_type":"code","source":"# rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n# rnn_model.fit(\n#         X_train_small, Y_train_small, epochs=1, batch_size=256,\n# #         validation_data=(X_dev, Y_dev), \n#     verbose=1\n# )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9acbf5a-c970-4db0-ae59-de8922626c3c","_uuid":"4c84aa9df5af021f210b091aa71f2c12ce9c2b11","collapsed":true,"trusted":true},"cell_type":"code","source":"# rnn_preds = rnn_model.predict(X_test_small, batch_size=1024, verbose=1)\n# rnn_preds = np.expm1(rnn_preds)\n# rnn_preds\n# del rnn_model\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a506e470-5bed-492d-b5ac-2e8385d75eec","_uuid":"0abe4801bac8e54cca85170ececbde191544945b"},"cell_type":"markdown","source":"## Predict and submit","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"874322f0-c9f2-44fe-b2b9-6ec44d39dc8d","_uuid":"99c55eda30a524ab9b330f4fdd27bc33433f9b7d","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\nfor batch_size in batch_sizes:\n    rnn_model.fit(\n            X_train, Y_train, epochs=epochs, batch_size=batch_size,\n            validation_data=(X_dev, Y_dev), verbose=1,\n    )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7055621b-7877-4592-8153-2a9a9294441c","_uuid":"7d561067e426e1025f87ea23e19738a864682427","collapsed":true,"trusted":true},"cell_type":"code","source":"rnn_preds = rnn_model.predict(X_test, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"585fa3f9-27eb-46db-8a80-120f1d40670d","_uuid":"f0736e8107d4436358587766f0799db114290112","collapsed":true,"trusted":true},"cell_type":"code","source":"rnn_preds = np.expm1(rnn_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4a1c0b5-e60b-487a-8b4f-8c7ea565cdc2","_uuid":"5eb7000a43749198e84d69d8ef309500c5dfb6dd","collapsed":true,"trusted":true},"cell_type":"code","source":"submission.loc[:, 'price'] = rnn_preds\nsubmission.loc[submission['price'] < 1.0, 'price'] = 1.0","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4176e4ac-366c-4d6f-ba44-a728e9391d46","_uuid":"7dd964069b482e17d4329a54de11a950c7a0ecf1","collapsed":true,"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48140340-9555-491a-a11f-4c3d8e01c985","_uuid":"87eab79e68683ca55e426fadb4c1882bd1c3c6a0","collapsed":true,"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission_glove_transfer.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}