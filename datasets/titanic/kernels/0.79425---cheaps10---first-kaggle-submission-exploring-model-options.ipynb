{"cells":[{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"0c3e2e79bf7998d5a5eb81b1ab2e6f72d1499676"},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster\nChad Heaps\nMarch 2018\n\nExploring different features and models in my first Kaggle project. My top submission score is 0.79425.  I think my biggest room for improvement is in data pre-processing and feature engineering/selection.\n\nThanks to the Kaggle forums and public notebooks for a lot of help.  Also, a lot of help and some of the code came from Hands-on Machine Learning by Aurélien Géron http://shop.oreilly.com/product/0636920052289.do and the associated Notebooks at https://github.com/ageron/handson-ml\n\n\n## Overview of data  from Kaggle\n\nThe data has been split into two groups:  \ntraining set (train.csv)  \ntest set (test.csv)  \nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.  \n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.  \n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.  \n\nData Dictionary  \nVariable\tDefinition\tKey  \nsurvival\tSurvival\t0 = No, 1 = Yes  \npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd  \nsex\tSex\t  \nAge\tAge in years\t  \nsibsp\t# of siblings / spouses aboard the Titanic\t  \nparch\t# of parents / children aboard the Titanic\t  \nticket\tTicket number\t  \nfare\tPassenger fare\t  \ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton  \nVariable Notes  \npclass: A proxy for socio-economic status (SES)  \n1st = Upper  \n2nd = Middle  \n3rd = Lower  \n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5  \n\nsibsp: The dataset defines family relations in this way...  \nSibling = brother, sister, stepbrother, stepsister  \nSpouse = husband, wife (mistresses and fiancés were ignored)  \n\nparch: The dataset defines family relations in this way...  \nParent = mother, father  \nChild = daughter, son, stepdaughter, stepson  \nSome children travelled only with a nanny, therefore parch=0 for them.  "},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.057869Z","start_time":"2018-03-29T04:09:14.974887Z"},"_cell_guid":"a9be289f-068a-4383-9c94-c1d301a6c5a0","_uuid":"315bc512deca3d2159cf95d912daff4dfda8d097","collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"import copy\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom scipy import sparse\nfrom scipy.stats import randint, uniform\n\n\nfrom pandas.plotting import scatter_matrix\n\n#Matplotlib related imports\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.colors as colors\nimport pylab\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nimport seaborn as sns\n\n#Various Scikit-learn imports\n#Pre-processing and such\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer, Imputer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\n\n\n#Fitting algorithms\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Scoring utilities, etc.\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, log_loss, auc, confusion_matrix\nfrom sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, roc_curve\nfrom sklearn.utils.validation import check_is_fitted\nplt.rcParams['patch.force_edgecolor'] = True #Add lines to bars in histograms\n\nrng = 42\nnp.random.seed(rng)","execution_count":1,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ffc2e8345f424fca2f3a64722b9e24f97aa9021"},"cell_type":"code","source":"ls ../input","execution_count":4,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.071084Z","start_time":"2018-03-29T04:09:15.061427Z"},"_cell_guid":"9693dc21-89f2-4af0-83d9-60fe3f8f43ed","_uuid":"7be5fa097f547ea98943a436c105d9a61d290b90","collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv', index_col=0)","execution_count":5,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.083695Z","start_time":"2018-03-29T04:09:15.073463Z"},"collapsed":true,"trusted":false,"_uuid":"c14b2da2db4469a5813e7193821e44879f6f5f70"},"cell_type":"code","source":"df.info()","execution_count":164,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.123936Z","start_time":"2018-03-29T04:09:15.086792Z"},"trusted":false,"_uuid":"d14a368652c813ea9e1686cfc935c191ed0060d4"},"cell_type":"code","source":"df.describe()","execution_count":165,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.134283Z","start_time":"2018-03-29T04:09:15.127123Z"},"collapsed":true,"trusted":false,"_uuid":"34e8a59477eca32e52ae0d64d26dba7fe4d617f3"},"cell_type":"code","source":"live_total = df['Survived'].sum()\ndead_total = df.shape[0] - live_total\nprint('Out of {} passengers (in the train set), {} survived'.format(df.shape[0], \n                                                 live_total))\nprint('This is {: 6.2f}% in total'.format(100.*(live_total / df.shape[0])))","execution_count":166,"outputs":[]},{"metadata":{"_uuid":"9d8cf6f9f20f1a8918059687029e6954e8df54c8"},"cell_type":"markdown","source":"### A little EDA"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.145818Z","start_time":"2018-03-29T04:09:15.137162Z"},"collapsed":true,"trusted":false,"_uuid":"c2ab2ee49dd79cd0d7c1075c6803278110ee5593"},"cell_type":"code","source":"dead_df = df.loc[df['Survived'] == 0, :].copy()\nlive_df = df.loc[df['Survived'] == 1, :].copy()","execution_count":167,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.157252Z","start_time":"2018-03-29T04:09:15.148631Z"},"trusted":false,"_uuid":"ec25c3c1a877f4d4a2a43868ac170350c60a2fc1"},"cell_type":"code","source":"df.corr()['Survived']","execution_count":168,"outputs":[]},{"metadata":{"_uuid":"0bef9ac0911afab75cc0320cbe3e0a5bb06ad9dd"},"cell_type":"markdown","source":"Unsurprisingly, Passenger Class and Fare are strongly correlated with survival.  In the next cell, we map Sex to a numerical value and see it is also strongly correlated."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.169709Z","start_time":"2018-03-29T04:09:15.159937Z"},"trusted":false,"_uuid":"4b8c56dc5971a184cbd60d719f428e44312b99d5"},"cell_type":"code","source":"pd.concat([pd.get_dummies(df['Sex']), df.Survived], axis=1).corr()['Survived']","execution_count":169,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.317751Z","start_time":"2018-03-29T04:09:15.173081Z"},"trusted":false,"_uuid":"cd01bc587db3e63b99ee900116443938bddad6c2"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(6,4))\nsns.kdeplot(dead_df['Age'].dropna().values, shade=True, label='Dead', ax=ax)\nsns.kdeplot(live_df['Age'].dropna().values, shade=True, label='Alive', ax=ax)\nax.set_xlabel('Age')\nax.set_label('Kernel Density Estimate')\nplt.legend()\nplt.show()","execution_count":170,"outputs":[]},{"metadata":{"_uuid":"d966f36087a4320ba8b41fb0aa699ea13f754a4f"},"cell_type":"markdown","source":"Do things change appreciably if we fill the NAs with the mean?  Not really."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.501387Z","start_time":"2018-03-29T04:09:15.320019Z"},"trusted":false,"_uuid":"ec54fb3f555483b822135cf14965ebcfae599725"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(6,4))\nsns.kdeplot(dead_df['Age'].fillna(dead_df.Age.mean()).values, shade=True, label='Dead', ax=ax)\nsns.kdeplot(live_df['Age'].fillna(live_df.Age.mean()).values, shade=True, label='Alive', ax=ax)\nax.set_xlabel('Age')\nax.set_label('Kernel Density Estimate')\nplt.legend()\nplt.show()","execution_count":171,"outputs":[]},{"metadata":{"_uuid":"41ab4308e554468c5d34bddc9ec7c16a25059d88"},"cell_type":"markdown","source":"Much of the distribution is similar, although being very young evidently helped survival.  Still, less compelling than I might expect"},{"metadata":{"_uuid":"8c281e1b1051c147c48e1fc14bfa46154dcff9fc"},"cell_type":"markdown","source":"#### Family size\n\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.658828Z","start_time":"2018-03-29T04:09:15.504119Z"},"trusted":false,"_uuid":"696c5bde27cee99bbb2250f0f7897035b44d4c2f"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(6,4))\nsns.kdeplot((dead_df.SibSp.fillna(dead_df.SibSp.mean()).values \n             + dead_df.Parch.fillna(dead_df.Parch.mean()).values), \n            shade=True, label='Dead', ax=ax)\nsns.kdeplot((live_df.SibSp.fillna(live_df.SibSp.mean()).values \n             + live_df.Parch.fillna(live_df.Parch.mean()).values), \n            shade=True, label='Live', ax=ax)\nax.set_title('Survival as a function of family size')\nax.set_xlabel('Family count')\n\nplt.legend()\nplt.show()","execution_count":172,"outputs":[]},{"metadata":{"_uuid":"76b3783e249958d9cccf0d99b1f170c6d7d12998"},"cell_type":"markdown","source":"Having some family appears to help somewhat.  Right now, I have the options of using a Boolean feature as to whether Family size is > 0 or to use it as a continuous variable (although only integer values are present)"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.791663Z","start_time":"2018-03-29T04:09:15.661287Z"},"scrolled":true,"trusted":false,"_uuid":"9709e9c54afe2c947be271f3b46edf8b7bd06edc"},"cell_type":"code","source":"ax = sns.boxplot(x='Survived', y='Parch', data=df)\nplt.show()","execution_count":173,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:15.898211Z","start_time":"2018-03-29T04:09:15.794835Z"},"trusted":false,"_uuid":"b60e5d04eff1e97f34bf988f2b7cb403a3c61e05"},"cell_type":"code","source":"ax = sns.boxplot(x='Survived', y='Fare', data=df)\nplt.show()","execution_count":174,"outputs":[]},{"metadata":{"_uuid":"382ee13f5447f00eb3243fdd56526baec8125eff"},"cell_type":"markdown","source":"## Train/Test split and functions for data processing"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:24:30.963946Z","start_time":"2018-03-29T04:24:30.942849Z"},"collapsed":true,"trusted":false,"_uuid":"e3ac8462f7726c4e88cfc78dd41289a560ad079a"},"cell_type":"code","source":"label = ['Survived']\n\nX = df.drop(label, axis=1)\nY = df.drop([col for col in df.columns if col not in label], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, \n                                                    stratify = Y.values, \n                                                    random_state = rng)\ny_train = y_train.values.reshape(-1)\ny_test = y_test.values.reshape(-1)\ny_tot = Y.values.reshape(-1)","execution_count":211,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"674af4e47d2c40b5daa83a45ae9a5b9b5e70e0e3"},"cell_type":"markdown","source":"#### Various functions and classes written to make my custom pre-processing compatible with sklearn pipeline"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:16.600112Z","start_time":"2018-03-29T04:09:15.921877Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"hide_input":false,"trusted":false,"_uuid":"6b340d3bdd91a7872d4938f886dd72ff9848e9ca"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This comes from Hands-on Machine Learning by Aurélien Géron\n    http://shop.oreilly.com/product/0636920052289.do\n    Create a class to select numerical or categorical columns \n    Haven't looked into the sklearn-pandas module yet\n    Should be at beginning of a given pipeline\n    \"\"\"\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].copy()\n\nclass DataFrameExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Extract the numeric data from the transformed DataFrame\n    This should be the last pipeline step before sklearn preprocessing\n    Will also store the final set of column names as a parameter for \n    easy access when evaluating feature contributions to model\n    \"\"\"\n    def __init__(self, colParams = None):\n        self.colParams = colParams\n        return\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        self.colParams = X.columns.tolist()\n        return X.values.copy().astype(np.float)\n\nclass Prep_category(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Pipeline compatible class to convert DataFrame of \n    character data into encoded data suitable for learning algorithms\n    Uses pandas get_dummies for encoding\n    \n    There are alternatives to pandas get_dummies.\n    \n    With get_dummies, I have set drop_first = True, which removes one of the\n    OneHot category columns.  This was after checking the pearson-r values and \n    seeing the redundancy.  The ill-conditioning is a problem for models\n    https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn\n    \n    1) CategoricalEncoder: This requires code from \n    https://github.com/scikit-learn/scikit-learn/pull/9151\n    and is used in the Hands-on ML github notebooks.\n\n    ce = CategoricalEncoder(encoding='onehot-dense')\n    ce.fit_transform(df['Pclass'].values.reshape(-1,1))\n\n    2) Using native sklearn functionality.\n    le = LabelEncoder()\n    le.fit(df['Pclass'])\n    class_ints = le.transform(df['Pclass'])\n    \n    To achieve one-hot encoding like get_dummies, add this\n    \n    enc = OneHotEncoder(sparse = False)\n    enc.fit(class_ints.reshape(-1,1))\n    enc.transform(class_ints.reshape(-1,1))\n    \n    I am still learning about different methods to encode\n    categorical data.\n    \n    method = 'OneHot' uses get_dummies\n    method = 'LabelEncoder' \n    \n    OneHot creates a column for each possible value with a value of 0 or 1.\n    As an example, the data for the original Sex and the transformed data is\n    array([['male', 1, 0],\n           ['female', 0, 1],\n           ['female', 0, 1],\n           ['female', 0, 1],\n           ['male', 1, 0]])\n           \n    The LabelEncoder gives a single column with 0-(n_classes - 1) specifying the\n    category.  Using the column Embarked, which has three possibilities, looks like this\n    array([['S', 2],\n           ['C', 0],\n           ['S', 2],\n           ['S', 2],\n           ['S', 2],\n           ['Q', 1],\n           ['S', 2],\n           ['S', 2],\n           ['S', 2],\n           ['C', 0]],)\n    \"\"\"\n    \n    def __init__(self, drop_first = True, method = 'OneHot', \n                 cat_modes = None):\n        self.method = method\n        self.cat_modes = cat_modes\n        self.drop_first = drop_first\n        return\n    def fit(self, X, y=None):\n        temp_df = X.copy()\n        self.cat_modes = {}\n        for i_col in temp_df.columns.tolist():\n            self.cat_modes[i_col] =  temp_df[i_col].value_counts().idxmax()\n        return self\n    def transform(self, X):\n        temp_df = X.copy()\n        for i_col in temp_df.columns.tolist():\n            if temp_df[i_col].isnull().sum():\n                temp_df.loc[temp_df[i_col].isnull(),i_col] = (self.cat_modes\n                                                              [i_col])\n                \n            temp_df[i_col] = temp_df[i_col].astype('category')\n            if self.method == 'OneHot':\n                i_df = pd.get_dummies(temp_df[i_col], \n                                      prefix=i_col,\n                                      drop_first = self.drop_first)\n                temp_df = pd.concat([temp_df, i_df], axis=1)\n            elif self.method == 'LabelEncoder':\n                le = LabelEncoder()\n                arr = le.fit_transform(temp_df[i_col])\n                series1 = pd.Series(data=arr, \n                                    index=temp_df.index, dtype='category', \n                                    name = i_col + 'Encode')\n                temp_df = pd.concat([temp_df, series1], axis=1)\n                \n            temp_df.drop(i_col, axis = 1, inplace = True)\n        return temp_df\n  \n    \nclass Deck_add(BaseEstimator, TransformerMixin):\n    \"\"\"\n    I have split up fit and transform so that I can find the proper\n    fillna values from the train set that will be used on the test set.\n    The point is to run fit_transform on the train to determine fill_vals,\n    then when transform is called on the test data, it fills in the most \n    frequent deck from the test set.\n    The code is a little redundant but should get the job done.\n    \n    It does assume that every passenger has a Pclass.\n    \"\"\"\n    def __init__(self, fill_vals = None):\n        self.fill_vals = fill_vals\n        self.allowed_decks = ['A','B','C','D','E','F']\n    def fit(self, X, y=None):\n        temp_df = X.copy()\n        \n        cabin_df = temp_df.loc[temp_df['Cabin'].notnull(), :]\n        deck_series =pd.Series(index=temp_df.index, dtype=str)\n        for i,j in cabin_df.iterrows():\n            deck_series.loc[i] = j['Cabin'].split()[0][0]\n            if deck_series.loc[i] not in self.allowed_decks:\n                deck_series.loc[i] = np.nan\n\n        temp_df = temp_df.assign(Deck = deck_series.values)\n        self.fill_vals = (temp_df.groupby(\n                                        ['Deck', 'Pclass']\n                                        )\n                                        .size()\n                                        .unstack(fill_value=0)\n                                        .idxmax()\n                         )\n        return self\n    def transform(self, X):\n        temp_df = X.copy()\n        cabin_df = temp_df.loc[temp_df['Cabin'].notnull(), :]\n        deck_series = pd.Series(index=temp_df.index, dtype=str)\n        for i,j in cabin_df.iterrows():\n            deck_series.loc[i] = j['Cabin'].split()[0][0]\n            if deck_series.loc[i] not in self.allowed_decks:\n                deck_series.loc[i] = np.nan\n\n        temp_df = temp_df.assign(Deck = deck_series.values)\n        null_inds = temp_df.loc[temp_df['Cabin'].isnull()].index\n        for i in null_inds:\n            temp_df.loc[i, 'Deck'] = self.fill_vals.loc[temp_df.loc[i, 'Pclass']]\n        return temp_df\n\ndef split_ticket(df, inplace = False):\n    \"\"\"\n    A function to parse the ticket numbers from Titanic passengers.\n    I do not know how to interpret the numbers, but I have split them\n    into the preceding character strings (if present) and the\n    integer number at the end (if present).\n    \n    The ticket numbers are highly unstructured in that they have a variety of\n    lengths, letters, and what appear to be typos in some of the preceding\n    characters such as 'stono' vs. 'sotono'.  \n    \n    I do not think I will try to fit the ticket features for now.\n    \n    If inplace == True, return df updated \n    else if inplace == False, return a modified copy of df\n    \n    \"\"\"\n    find_int = re.compile('[0-9]+')\n    find_char = re.compile('([a-zA-Z].?\\/?[0-9]*\\.?)+\\D')\n    if inplace == False:\n        temp_df = df.copy()\n    else:\n        temp_df = df\n    \n    if 'Ticket_char' in temp_df:\n        temp_df.drop('Ticket_char', axis = 1, inplace = True)\n    if 'Ticket_number' in temp_df:\n        temp_df.drop('Ticket_number', axis = 1, inplace = True)\n    temp_df['Ticket_char'] = ''\n    temp_df['Ticket_number'] = 0\n        \n    for i,j in temp_df['Ticket'].iteritems():\n        csearch = find_char.search(j)\n        isearch = find_int.findall(j)\n        if csearch:\n            temp_df.loc[i, 'Ticket_char'] = csearch.group()\n        if isearch:\n            temp_df.loc[i, 'Ticket_number'] = int(isearch[-1])\n    #Lowercase ticket characters and remove non-alphanumeric characters\n    temp_df['Ticket_char'] = temp_df['Ticket_char'].str.lower()\n    temp_df['Ticket_char'] = (temp_df['Ticket_char'].\n                              str.replace('[^a-zA-Z0-9]', ''))\n    if inplace == True:\n        return\n    elif inplace == False:\n        return temp_df","execution_count":176,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"3465cac4a99797990e3c92f80e9b254760f7f4ce"},"cell_type":"markdown","source":"Some functions for scoring, plotting, etc.\n\nSome of these are adopted from Hands-on ML, too"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:16.693847Z","start_time":"2018-03-29T04:09:16.602628Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"2b9c17a01a2882947d731118fb7ff6b497b2c4a4"},"cell_type":"code","source":"def print_classifier_scores(model, x_train, y_train, x_test, y_test):\n    \"\"\"\n    Prints the scores and standard deviations for the train \n    and test sets for a given fit.\n    Assumes 'fit' has been called on the model and the model \n    has a score function.\n    \"\"\"\n    train_score = model.score(x_train, y_train)   \n    test_score  = model.score(x_test, y_test)\n    print('Training set score = {: 6.4f}'.format(train_score))\n    print('Testing set score  = {: 6.4f}'.format(test_score))\n    \n    logloss_train = log_loss(y_train, model.predict(x_train))\n    logloss_test = log_loss(y_test, model.predict(x_test))\n\n    print('Training set log loss = {: 6.4f}'.format(logloss_train))\n    print('Testing set log loss  = {: 6.4f}'.format(logloss_test))\n    return   \n\ndef print_cv_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n    return\n\ndef print_grid_search_results(grid_search, X_attribs, \n                       x_train, y_train,\n                       x_test, y_test, verbose=0):\n\n    print('Best score using scoring metric '\n          'from CV {: 8.5f}'.format(grid_search.best_score_))\n    print('Score in train data from best parameters {: 8.5f}'\n          .format(grid_search.best_estimator_.score(x_train, y_train)))\n    print('Score on test data from best parameters {: 8.5f}'\n          .format(grid_search.best_estimator_.score(x_test, y_test)))\n    print('Parameters for best score in CV {}\\n'\n          .format(grid_search.best_params_))\n    if verbose & hasattr(grid_search.best_estimator_, 'feature_importances_'):\n        feature_importances = grid_search.best_estimator_.feature_importances_\n        list_feature = sorted(zip(feature_importances, X_attribs))\n        print('Feature importance for best scoring parameters')\n        for i in list_feature:\n            print('Feature/Importance: {:s} {: 8.5f}'.format(i[1], i[0]))\n    if verbose:\n        cvres = grid_search.cv_results_\n        print('\\nMean score and parameters')\n        for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n            print(mean_score, params)\n            \ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)","execution_count":177,"outputs":[]},{"metadata":{"_uuid":"53e7da34c511fa1cebf7055213fa930e09fa958c"},"cell_type":"markdown","source":"#### Functions wrapping some common model evaluation tools\n\nSome of these are modified from the Hands-on ML book"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:16.714697Z","start_time":"2018-03-29T04:09:16.696714Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"8c41d638539dedf0c8f68c322135bfbc039c6e03"},"cell_type":"code","source":"def model_eval(model, X_train, y_train, X_test, y_test):\n    \"\"\"\n    A simple wrapper function to quickly evaluate a model's performance\n    on the train and test sets\n    \"\"\"\n    print('Evaluating ' + str(model.__class__).strip('>')\n                                              .strip('\\'')\n                                              .split('.')[-1])\n    \n    clf = copy.deepcopy(model)\n    #clf.fit(X_train, y_train)\n    train_score = clf.score(X_train, y_train)\n    test_score = clf.score(X_test, y_test)\n    print('Train score: {: 8.5f} '  \n          ' Test score: {: 8.5}  '\n          'Difference {: 8.5}'.format(train_score, \n                                      test_score, \n                                      train_score - test_score))\n    \n    scores = cross_val_score(clf, X_train, y_train, cv=10)\n    print('Mean score on train set from 10-fold CV: {: 8.5f} '\n          '  Std. Dev.: {: 8.5f}'.format(scores.mean(), scores.std()))\n    return    ","execution_count":178,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:16.753251Z","start_time":"2018-03-29T04:09:16.717573Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"e9c90f4beee5d0b7781d7a506c7d6a755ed1be11"},"cell_type":"code","source":"#Boolean of whether passenger has cabin assignment\nCabin_bool = FunctionTransformer(lambda x : \n                                 x.assign(Cabin_bool = x['Cabin'].notnull()),\n                                 validate = False)                                    \n#Boolean of whether passenger has Parch or SibSp\nFamily_bool = FunctionTransformer(lambda x : \n                                  x.assign(Family_bool = \n                                           (x['Parch'].fillna(0) > 0) \n                                           | (x['SibSp'].fillna(0) > 0)),\n                                 validate = False)  \n#Just Siblings/Spouses\nSibSp_bool = FunctionTransformer(lambda x : \n                                  x.assign(\n                                      SibSp_bool = (\n                                                  x['SibSp'].fillna(0) > 0)\n                                                    ),\n                                 validate = False)  \n#Just parents/children\nParch_bool = FunctionTransformer(lambda x : \n                                  x.assign(\n                                      Parch_bool = (\n                                                  x['Parch'].fillna(0) > 0)\n                                                    ),\n                                 validate = False) \n#Count family members as Parch + SibSp\nFamily_total = FunctionTransformer(lambda x : \n                                   x.assign(Family_total = \n                                            (x['Parch'].fillna(0) \n                                             + x['SibSp'].fillna(0))),\n                                 validate = False) \ndef sharedticket(xx):\n    temp1 = split_ticket(xx)\n    ticket_counts = pd.value_counts(temp1['Ticket_number'])\n    temp_series = temp1['Ticket_number'].apply(lambda xt : 1                                                           \n                                                          if ticket_counts[xt] > 1 \n                                                          else 0)\n    return xx.assign(Shared_ticket = temp_series)\nShared_ticket = FunctionTransformer(sharedticket, validate=False)   \n\n#Name Lengh\nName_len = FunctionTransformer(lambda x : x.assign(Name_len = \n                                                   df['Name'].str.len()), \n                               validate=False)\n","execution_count":179,"outputs":[]},{"metadata":{"_uuid":"a7b2c7c644e07067458726f18056bbb17670abbf"},"cell_type":"markdown","source":"These functions should take care of most of my pipeline fitting needs.  They allow easy transformation of data sets to test different features and pre-processing"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:17.080857Z","start_time":"2018-03-29T04:09:16.756289Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"4c5235ace94fff04542763290b56ddfb1bd518ea"},"cell_type":"code","source":"def construct_pipeline(cat_attrs = [None], num_attrs = [None], \n                  cat_encode = 'OneHot', imputer_strat = 'median',\n                  num_scaling = 'std_scaler', drop_cat_first = True, **kwargs):\n    \"\"\"\n    Given a list of numerical and categorial attributes, construct a pipeline\n    to process data.\n    \n    This will make testing different parameters easier\n    \n    Input:\n    cat_attributes: \n                    List of categorical attributes to use in ML model.\n                    These may be columns in the original DataFrame or\n                    engineered features if you have constructed an appropriate\n                    FunctionTransformer class and added it as\n                    if 'Feature' in cat_attributes:\n                    at the beginning of the pipeline creation\n    num_attributes:\n                    List of numerical attributes.  Same general idea as\n                    categorical_attributes although an sklearn Imputer is \n                    used to fill NA values and either StdScaler or \n                    MinMaxScaler is used to scale data.\n                \n    cat_encode:\n                Default = 'OneHot'.\n                This option defines encoding of categorial attributes.\n                'OneHot': \n                        Construct a new Boolean column for each category\n                        of each feature.  All values are binary (0 or 1)\n                'LabelEncoder':\n                        Retains a single column for each categorial\n                        attribute.  Each label is encoded with a value\n                        between 0 and n_categories-1.\n                        Warning: When n_categories > 2, the encoded data\n                        will take on values  other than (0, 1).  ML algorithms\n                        may treat adjacent encodings as being more similar, \n                        which is not necessarily the case.  \n                        When 'LabelEncoder' is selected, the features are\n                        scaled using MinMaxScaler to maintain (0,1) interval\n    impute_strat:\n                Default = 'median'\n                The Imputer strategy for sklearn.preprocessing.Imputer\n                Median is often fine, but refer to the sklearn documentation\n                for more options\n    \n    Output:\n            full_pipeline: An instance of sklearn.pipeline.FeatureUnion\n                An sklearn Transformer that concatenates the categorical\n                and numerical features.  \n                Methods (from sklearn documentation):\n                    fit(X[,y])\n                    fit_transform(X[, y])\n                    get_feature_names()\n                    get_params([deep])\n                    set_params(**kwargs)\n                    transform(X)\n                Use fit_transform on train data.\n                Use transform (not fit_transform!) on test data\n                to \n                See sklearn documentation for additional details\n                \n    \"\"\"\n    #Fill in options specified in kwargs.  Doing this added some flexibility\n    #from just the optional arguments\n    for arg, val in kwargs:\n        if arg == 'cat_attrs':\n            cat_attrs = val\n        if arg == 'num_attrs':\n            num_attrs = val \n        if arg == 'cat_encode':\n            cat_encode = val\n        if arg == 'imputer_strat':\n            imputer_strat = val\n        if arg == 'num_scaling':\n            num_scaling = val\n        if arg == 'drop_cat_first':\n            drop_cat_first == val\n    \n    cat_pipe = []\n    #Add classes for engineered features first\n    if 'Deck' in cat_attrs:\n        cat_pipe.append(('Deck', Deck_add()))\n    if 'Cabin_bool' in cat_attrs:\n        cat_pipe.append(('cabin_bool', Cabin_bool))\n    if 'Family_bool' in cat_attrs:\n        cat_pipe.append(('family_bool', Family_bool))\n    if 'Shared_ticket' in cat_attrs:\n        cat_pipe.append(('shared_ticket', Shared_ticket))\n    if 'Family_total' in cat_attrs:\n        cat_pipe.append(('Family_total', Family_total))  \n    #Extract those features from the full pipeline\n    cat_pipe.append(('selector', DataFrameSelector(cat_attrs)))\n    #Transform selected features to ML-friendly form, either\n    #Encoded as OneHot features or done with LabelEncoder which implies\n    #ordering in the categorial variables.\n    cat_pipe.append(('encode', Prep_category(drop_cat_first, \n                                             method=cat_encode)))\n    cat_pipe.append(('extractor', DataFrameExtractor()))\n    #Renormalize LabelEncoder features so that hey are between 0 and 1\n    if cat_encode == 'LabelEncoder':\n        cat_pipe.append(('minmax_scaler', MinMaxScaler()))\n    cat_pipeline = Pipeline(cat_pipe)\n    \n    num_pipe = []\n    #Add engineered numerical features\n    if 'Family_total' in num_attrs:\n        num_pipe.append(('Family_total', Family_total))\n    if 'Name_len' in num_attrs:\n        num_pipe.append(('name_len', Name_len))   \n    num_pipe += [\n                 ('selector', DataFrameSelector(num_attrs)), \n                 ('extractor', DataFrameExtractor()),\n                 ('imputer', Imputer(strategy=imputer_strat)),\n                ]\n    if num_scaling == 'std_scaler':\n        num_pipe += [('std_scaler', StandardScaler())]\n    elif num_scaling == 'minmax_scaler':\n        num_pipe += [('minmax_scaler', MinMaxScaler())]\n        \n    num_pipeline = Pipeline(num_pipe)\n    #Join the two pipelines for full data preparation\n    full_pipeline = FeatureUnion(\n                            transformer_list=[(\"num_pipeline\", num_pipeline),\n                                              (\"cat_pipeline\", cat_pipeline),\n                                              ])\n\n    return full_pipeline\n\ndef fit_pipelines(model_dict, X_train, X_test, df, inplace=True, verbose=True):\n    \"\"\"\n    A function takesa  dictionary of features and options for the pipeline,\n    constructs the pipeline and fits.  \n    \n    Returns the updated dictionary by default but can return copy with\n    inplace=False\n    \n    \"\"\"\n    \n    if inplace == True:\n        models = model_dict\n    else:\n        models = copy.deepcopy(models)\n\n    for model, params in models.items():\n        print('Fitting model ', model)\n        models[model]['pipeline'] = construct_pipeline(**params)\n        #Apply fit_transform to training data.  Saves hyperparameters for\n        #pipeline construction\n        models[model]['X_train'] = models[model]['pipeline'].fit_transform(\n                                                            X_train)\n        #Call transform to use training set hyperparameters in test set prep\n        models[model]['X_test'] = models[model]['pipeline'].transform(\n                                                            X_test)\n        #For convenience and analysis, get the feature names and make a \n        #DataFrame out of the test set\n        columns = ((models[model]['pipeline'].get_params()\n                   ['num_pipeline__extractor__colParams'])\n                + (models[model]['pipeline'].get_params()\n                  ['cat_pipeline__extractor__colParams']))\n        \n        models[model]['df'] = pd.DataFrame(data = models[model]['X_train'], \n                                columns = columns,\n                                index=X_train.index)\n        models[model]['n_features'] = models[model]['df'].shape[1]\n        models[model]['X_attribs'] = columns\n        #Since the dataframe will only be used for analysis, I will attach the \n        #Survived column back on it\n        models[model]['df'] = models[model]['df'].assign(Survived=df.loc[\n                                                    models[model]['df'].index, \n                                                    'Survived'])\n        if verbose:\n            print('Correlation coefficients of features with survival')\n            print(models[model]['df'].corr()['Survived'])\n            print('')\n    if inplace == True:\n        return\n    else:\n        return models\n","execution_count":180,"outputs":[]},{"metadata":{"_uuid":"8816cfead19ba7d8d6cc0052530991568ec60031"},"cell_type":"markdown","source":"## Model construction\n\nTo expedite evaluation of features, I will use a dictionary of different feature sets and hyperparameters for the pipeline.  It should allow easy comparison of features.\n\n\n### Selecting categorial encoding and Numerical rescaling\nIn this first test, I just want an idea of how the different encodings and rescaling options work, so the same features will be used in all cases.\n\nThere is the possibility that different categorial features may be better suited for different encodings, but for now I use a single encoding each time."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:17.122698Z","start_time":"2018-03-29T04:09:17.083462Z"},"collapsed":true,"trusted":false,"_uuid":"b69abbc20b9c669f8e6498f4141505b4d6b0950c"},"cell_type":"code","source":"models = {}\nmodels['set1'] = {}\n#A solid set of features from previous experience\nmodels['set1']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nmodels['set1']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nmodels['set1']['cat_encode'] = 'OneHot'\nmodels['set1']['num_scaling'] = 'std_scaler'\n\n#Switch to LabelEncoder\nmodels['set2'] = {}\nmodels['set2']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nmodels['set2']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nmodels['set2']['cat_encode'] = 'LabelEncoder'\nmodels['set2']['num_scaling'] = 'std_scaler'\n\n#Switch to minmax_scaler\nmodels['set3'] = {}\nmodels['set3']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nmodels['set3']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nmodels['set3']['cat_encode'] = 'OneHot'\nmodels['set3']['num_scaling'] = 'minmax_scaler'\n\n#Switch to both\nmodels['set4'] = {}\nmodels['set4']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nmodels['set4']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nmodels['set4']['cat_encode'] = 'LabelEncoder'\nmodels['set4']['num_scaling'] = 'minmax_scaler'\n","execution_count":181,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:19.325565Z","start_time":"2018-03-29T04:09:17.125067Z"},"collapsed":true,"trusted":false,"_uuid":"bab1cc355d9d2e4a32846f66f218278b1b620f8d"},"cell_type":"code","source":"fit_pipelines(models, X_train, X_test, df, verbose=0)","execution_count":182,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:44.421295Z","start_time":"2018-03-29T04:09:19.327614Z"},"collapsed":true,"trusted":false,"_uuid":"4dc6a0a89e5f36b8db1eed6b88702215213903f3"},"cell_type":"code","source":"for model, params in models.items():\n    print('GridSearch for model: ', model)\n    param_grid = [\n                {'C':np.linspace(0.1, 6., 200)},\n                 ]\n    grid_search = GridSearchCV(LogisticRegression(random_state = rng),\n                               param_grid, \n                               cv= 10, \n                               n_jobs = 1,\n                               scoring= 'neg_log_loss')\n    grid_search.fit(params['X_train'], y_train)\n    print_grid_search_results(grid_search, params['X_attribs'],\n                              params['X_train'], y_train,\n                              params['X_test'], y_test, verbose=0)\n","execution_count":183,"outputs":[]},{"metadata":{"_uuid":"6cf5687f94ea98cde20c8e88faf5342099881fb2"},"cell_type":"markdown","source":"While they are all very similar, my first choice would have been OneHot and std_scaler and this offers no reason to depart from that choice.  Additionally, the regularization parameter is larger and I will happily keep that smaller to keep the decision function smoother."},{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"63b548b63ca6d765125dc45975a7ac258e4272cd"},"cell_type":"markdown","source":"# Feature selection with cross-validation\n\n1.  Model selection:  Out of curiosity, I will try some of the common classification algorithms\n    - LogisticRegression\n    - SVM classifier\n    - Random Forest\n    - K-nearest Neighbors\n2.  Metrics\n    - I will primarily use accuracy, but may look into the confusion matrix, precision/recall score, or ROC-curve\n3.  Parameter optimization\n    - Use 10-fold cross validation with GridSearch to find best parameters\n\n\n### Possible features\n1. Categorial\n    - Deck\n    - Cabin_bool\n    - Family_bool\n    - Shared_ticket (if ticket is not unique)\n    - Sex\n    - Pclass\n    - Embarked (not used in any of my models)\n2.  Numerical\n    - Fare\n    - Age\n    - Name_len\n3. Could be either (family counts)\n    - SibSp\n    - Parch\n    - Family_total (SiSp + Parch)"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:09:46.667131Z","start_time":"2018-03-29T04:09:44.428258Z"},"collapsed":true,"trusted":false,"_uuid":"9616555ac121d26b0043fce16d1a8013b0ae8dce"},"cell_type":"code","source":"models = {}\nmodels['set1'] = {}\n#All features\nmodels['set1']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool', 'Family_bool',\n                              'Shared_ticket']\nmodels['set1']['num_attrs'] = ['Fare', 'Age', 'Family_total', 'Name_len']\nmodels['set1']['cat_encode'] = 'OneHot'\nmodels['set1']['num_scaling'] = 'std_scaler'\n\n#Minimal set of effective features\nmodels['set2'] = {}\nmodels['set2']['cat_attrs'] = ['Pclass', 'Sex']\nmodels['set2']['num_attrs'] = ['Fare', 'Age']\nmodels['set2']['cat_encode'] = 'OneHot'\nmodels['set2']['num_scaling'] = 'std_scaler'\n\n#A moderate collection, the ones used in the preliminary check\nmodels['set3'] = {}\nmodels['set3']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nmodels['set3']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nmodels['set3']['cat_encode'] = 'OneHot'\nmodels['set3']['num_scaling'] = 'std_scaler'\n\n#One last variation\nmodels['set4'] = {}\nmodels['set4']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', \n                               'Family_bool', 'Cabin_bool']\nmodels['set4']['num_attrs'] = ['Fare', 'Age']\nmodels['set4']['cat_encode'] = 'OneHot'\nmodels['set4']['num_scaling'] = 'std_scaler'\n\n#Fit all models\nfit_pipelines(models, X_train, X_test, df, verbose=0)","execution_count":184,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"8738e846611fdf52bc712742bc16e6c86d0485eb"},"cell_type":"markdown","source":"## Logistic Regression\nThe principal parameter to optimize is C, the regularization parameter:   \nSmall C = Strong regularization   \nMay also select L1 or L2 norm  \n\nIn some other preliminary work, the L1 estimator was not appreciably better and the scores are noisier as a function of C, so I will restrict my choices to L2"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:10:14.788476Z","start_time":"2018-03-29T04:09:46.669851Z"},"trusted":false,"_uuid":"c0de9c632014fce6b05e08df806059328df2d252"},"cell_type":"code","source":"for model, params in models.items():\n    print('GridSearch for model: ', model)\n    param_grid = [\n                {'C':np.linspace(0.1, 6., 200)},\n                 ]\n    grid_search = GridSearchCV(LogisticRegression(random_state = rng),\n                               param_grid, \n                               cv= 10, \n                               n_jobs = 1,\n                               scoring= 'neg_log_loss')\n    grid_search.fit(params['X_train'], y_train)\n    print_grid_search_results(grid_search, params['X_attribs'],\n                              params['X_train'], y_train,\n                              params['X_test'], y_test, verbose=0)\n\n    #Save useful information to each dataset dictionary\n    params['logreg_grid_search'] = grid_search    \n    params['clf_logreg'] = grid_search.best_estimator_   \n    cvres = grid_search.cv_results_\n    scores = np.array([ [j['C'], i] \n                       for i,j in list(zip(cvres['mean_test_score'], \n                                           cvres['params']))])\n    plt.plot(scores[:,0], scores[:,1], 'bo-', markersize=3)\n    plt.xlabel('C parameter for logreg')\n    plt.ylabel('Score')\n    plt.show()","execution_count":185,"outputs":[]},{"metadata":{"_uuid":"b2e7769d92445a20b8557529ec3592d2231e8edf"},"cell_type":"markdown","source":"#### Score summary from logistic regression"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:10:14.910999Z","start_time":"2018-03-29T04:10:14.792014Z"},"collapsed":true,"trusted":false,"_uuid":"42a798c4adc37a196c434d6d158e6232fc1fef54"},"cell_type":"code","source":"for model, params in models.items():\n    print('Model :', model)\n    model_eval(params['clf_logreg'], \n               params['X_train'], \n               y_train, \n               params['X_test'], \n               y_test)","execution_count":186,"outputs":[]},{"metadata":{"_uuid":"b62c3ec011a569bb686df672a468fafc95cd900d"},"cell_type":"markdown","source":"### SVC grid search\n\nThe optimal hyperparameters for set1 are much different than the other sets, so I will run that one separately"},{"metadata":{"_uuid":"c35bf91a8849a537cb233720428ae5baf07a3fe1"},"cell_type":"markdown","source":"#### Comparing scoring metrics"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:11:10.496118Z","start_time":"2018-03-29T04:10:14.914139Z"},"trusted":false,"_uuid":"92d8a1e54923690d43aca00d888485639ef8150f"},"cell_type":"code","source":"params = models['set1']\nfor scoring in ['accuracy', 'roc_auc', 'average_precision']:\n    param_grid = [\n                 {'C':np.geomspace(1e-2, 30., 20), \n                  'gamma':np.geomspace(1e-2, 1e-1, 10),\n                 }\n              ]\n\n    grid_search = GridSearchCV(SVC(random_state = rng),\n                               param_grid, \n                               cv= 10, \n                               n_jobs = 3,\n                               scoring= scoring,\n                               verbose=1)\n    grid_search.fit(params['X_train'], y_train)\n    print('Scoring metric: ', scoring)\n    print_grid_search_results(grid_search, params['X_attribs'],\n                              params['X_train'], y_train,\n                              params['X_test'], y_test, verbose=0)\n    \n    fig, ax = plt.subplots(1,1, figsize=(8,6))\n\n    cvres = grid_search.cv_results_\n    plot_params = np.array([[i['C'], i['gamma'],j] \n                            for i, j in zip(list(cvres['params']), \n                                            cvres['mean_test_score'])])\n\n    nC = np.unique(plot_params[:,0]).size\n    nG = np.unique(plot_params[:,1]).size\n    CC = plot_params[:,0].reshape(nC, nG)\n    GG = plot_params[:,1].reshape(nC, nG)\n    train_scores = plot_params[:,2].reshape(nC, nG)\n    lims = [CC.min(), CC.max(), GG.min(), GG.max()]\n\n    pclr = ax.pcolormesh(CC,GG,train_scores)\n    # pclr = ax.imshow(train_scores.T,\n    #                  origin='lower',\n    #                  extent=lims,\n    #                  aspect=np.ptp(CC)/np.ptp(GG))\n\n    fig.colorbar(pclr, shrink=1.0, ax=ax).set_label(label='Score',\n                                                    size=15,weight='semibold')\n    ax.set_xlabel('C')\n    ax.set_ylabel('Gamma')\n    ax.set_title('Test scores using ' + scoring)\n\n    plt.show()","execution_count":187,"outputs":[]},{"metadata":{"_uuid":"4aeeb8a9db52de8d1e96fb8b899076e3102dddae"},"cell_type":"markdown","source":"While all the scoring methods select hyperparameters that score comparably well, the heatmap for 'accuracy' is poorly defined.  The shallow hyperparameter surface is hard to find an optimal set.  Of the other two, the 'roc_auc' performs a bit better, so I will go with that."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:12:25.137114Z","start_time":"2018-03-29T04:11:10.499155Z"},"collapsed":true,"trusted":false,"_uuid":"30f638a24a6f1019675ce94360c94e1a77b7a001"},"cell_type":"code","source":"for model, params in models.items():\n    print('GridSearch for model: ', model)\n    param_grid = [\n                 {'C':np.geomspace(1e-1, 15., 15), \n                  'gamma':np.linspace(1e-2, 0.5, 15),\n#                   'kernel':['rbf', 'poly']\n                 }\n              ]\n\n    grid_search = GridSearchCV(SVC(random_state = rng),\n                               param_grid, \n                               cv= 10, \n                               n_jobs = 3,\n                               scoring= 'roc_auc',\n                               verbose=1)\n    grid_search.fit(params['X_train'], y_train)\n    params['svc_grid_search'] = grid_search\n    params['clf_svc'] = grid_search.best_estimator_\n    print_grid_search_results(grid_search, params['X_attribs'],\n                              params['X_train'], y_train,\n                              params['X_test'], y_test, verbose=0)","execution_count":188,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:12:25.390720Z","start_time":"2018-03-29T04:12:25.140664Z"},"trusted":false,"_uuid":"dd6df2bb79ea4087f65178e8ed9c957a7022397b"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(8,6))\n\ncvres = models['set1']['svc_grid_search'].cv_results_\nplot_params = np.array([[i['C'], i['gamma'],j] \n                        for i, j in zip(list(cvres['params']), \n                                        cvres['mean_test_score'])])\n\nnC = np.unique(plot_params[:,0]).size\nnG = np.unique(plot_params[:,1]).size\nCC = plot_params[:,0].reshape(nC, nG)\nGG = plot_params[:,1].reshape(nC, nG)\ntrain_scores = plot_params[:,2].reshape(nC, nG)\nlims = [CC.min(), CC.max(), GG.min(), GG.max()]\n\npclr = ax.pcolormesh(CC,GG,train_scores)\n# pclr = ax.imshow(train_scores.T,\n#                  origin='lower',\n#                  extent=lims,\n#                  aspect=np.ptp(CC)/np.ptp(GG))\n\nfig.colorbar(pclr, shrink=1.0, ax=ax).set_label(label='Score',\n                                                size=15,weight='semibold')\nax.set_xlabel('C')\nax.set_ylabel('Gamma')\nax.set_title('Test scores from CV')\n\nplt.show()","execution_count":189,"outputs":[]},{"metadata":{"_uuid":"8b1d0b220320f161f83e091253d89800fa7d1ead"},"cell_type":"markdown","source":"#### Scoring summary for SVC"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:12:25.905428Z","start_time":"2018-03-29T04:12:25.393945Z"},"collapsed":true,"trusted":false,"_uuid":"262a3456e692e8958a4f861cb1f9bba2317c4b43"},"cell_type":"code","source":"for model, params in models.items():\n    print('Model :', model)\n    model_eval(params['clf_svc'], \n               params['X_train'], \n               y_train, \n               params['X_test'], \n               y_test)","execution_count":190,"outputs":[]},{"metadata":{"_uuid":"f1f5f379e0420c3eb2d6659c736aa737f6787315"},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"_uuid":"2a71b008de3106dde90f61a5639792880810e3f4"},"cell_type":"markdown","source":"I have not been able to find a set of parameters that does not dramatically overfit or underfit the data.  "},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:29:25.783785Z","start_time":"2018-03-29T04:27:34.873790Z"},"collapsed":true,"trusted":false,"_uuid":"f1985ca0a7e0380957c0108e9fdb0302ef12725e"},"cell_type":"code","source":"\nfor model, params in models.items():\n    param_grid = [\n                {'n_estimators': np.arange(50, 250, 50), \n#                  'criterion':['gini','entropy'],\n                 'max_features': np.arange(4, len(params['X_attribs'])+1, 2),\n                 'min_impurity_decrease':[1e-5, 1e-3],\n                     'min_samples_leaf': [11],\n                     'max_depth': [6],}\n#                   'oob_score': [False, True]},\n              ]\n\n    grid_search = GridSearchCV(RandomForestClassifier(random_state = rng),\n                           param_grid, \n                           cv= 10, \n                           n_jobs = 3,\n                           scoring= 'neg_log_loss',\n                           verbose=1)\n    grid_search.fit(params['X_train'], y_train)\n    # print_classifier_scores(grid_search.best_estimator_,\n    #                     params['X_train'], y_train,\n    #                     params['X_test'], y_test)\n\n    print_grid_search_results(grid_search, params['X_attribs'],\n                               params['X_train'], y_train,\n                               params['X_test'], y_test, verbose=0)\n    params['forest_grid_search'] = grid_search\n    params['clf_forest'] = grid_search.best_estimator_\n","execution_count":215,"outputs":[]},{"metadata":{"_uuid":"d0233bc594078ee9a85cd877e4b1c8ddaab95e81"},"cell_type":"markdown","source":"### K-Nearest Neighbors classifier"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T03:34:40.927608Z","start_time":"2018-03-29T03:34:33.988214Z"},"collapsed":true,"trusted":false,"_uuid":"31ec49304d96c24f0ea6607dec6fd456846dbee9"},"cell_type":"code","source":"for model, params in models.items():\n    param_grid = [\n                    {'n_neighbors':np.arange(2, 20), \n                     'metric':['minkowski', 'manhattan']\n                    },\n                  ]\n\n    grid_search = GridSearchCV(KNeighborsClassifier(),\n                           param_grid, \n                           cv= 10, \n                           n_jobs = 3,\n                           scoring= 'average_precision',\n                           verbose=1)\n    grid_search.fit(params['X_train'], y_train)\n    # print_classifier_scores(grid_search.best_estimator_,\n    #                     params['X_train'], y_train,\n    #                     params['X_test'], y_test)\n\n    print_grid_search_results(grid_search, params['X_attribs'],\n                               params['X_train'], y_train,\n                               params['X_test'], y_test, verbose=0)\n    params['knn_grid_search'] = grid_search\n    params['clf_knn'] = grid_search.best_estimator_\n\n","execution_count":102,"outputs":[]},{"metadata":{"_uuid":"aa5af667cf697b1b70ac81e815a32de0cafe12e2"},"cell_type":"markdown","source":"# Load the test data, train the full training set using parameters from gridsearchCV, transform the test set, predict survival"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T03:59:25.602648Z","start_time":"2018-03-29T03:59:25.576855Z"},"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"b60a648610fc0891aa3d7869ae01ad08b7e080f6"},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv', index_col=0)\ntest_data.head()","execution_count":6,"outputs":[]},{"metadata":{"hideCode":false,"hidePrompt":false,"_uuid":"e74772914582970bb66d9f287baaec281541e0f0"},"cell_type":"markdown","source":"Use all of the available train data to train final model using the parameters determined from the split test/train set"},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:08:18.783847Z","start_time":"2018-03-29T04:08:15.581155Z"},"collapsed":true,"trusted":false,"_uuid":"110bb03b4a2d7069edfc6eae4d072c80054c35fb"},"cell_type":"code","source":"full_models = {}\nfull_models['set1'] = {}\n#All features\nfull_models['set1']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool', 'Family_bool',\n                              'Shared_ticket']\nfull_models['set1']['num_attrs'] = ['Fare', 'Age', 'Family_total', 'Name_len']\nfull_models['set1']['cat_encode'] = 'OneHot'\nfull_models['set1']['num_scaling'] = 'std_scaler'\n\n#Minimal set of effective features\nfull_models['set2'] = {}\nfull_models['set2']['cat_attrs'] = ['Pclass', 'Sex']\nfull_models['set2']['num_attrs'] = ['Fare', 'Age']\nfull_models['set2']['cat_encode'] = 'OneHot'\nfull_models['set2']['num_scaling'] = 'std_scaler'\n\n#A moderate collection, the ones used in the preliminary check\nfull_models['set3'] = {}\nfull_models['set3']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', 'Cabin_bool']\nfull_models['set3']['num_attrs'] = ['Fare', 'Age', 'Family_total']\nfull_models['set3']['cat_encode'] = 'OneHot'\nfull_models['set3']['num_scaling'] = 'std_scaler'\n\n#One last variation\nfull_models['set4'] = {}\nfull_models['set4']['cat_attrs'] = ['Pclass', 'Sex', 'Deck', \n                               'Family_bool', 'Cabin_bool']\nfull_models['set4']['num_attrs'] = ['Fare', 'Age']\nfull_models['set4']['cat_encode'] = 'OneHot'\nfull_models['set4']['num_scaling'] = 'std_scaler'\n\n#Fit all full_models\nfit_pipelines(full_models, X, test_data, df, verbose=0)","execution_count":161,"outputs":[]},{"metadata":{"_uuid":"1734c5221a9c024abb5e0fcc409ba8dfc9f5f5d6"},"cell_type":"markdown","source":"## Prediction on test data\n\nAfter trying a few different ML models and feature sets, I obtained a new high score for myself of 0.79425!\n\nThis was accomplished using the RandomForestClassifier."},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:31:02.367274Z","start_time":"2018-03-29T04:31:01.742732Z"},"collapsed":true,"trusted":false,"_uuid":"c759893705cf04464560a3b158ffb8f0865ad828"},"cell_type":"code","source":"for model, params in full_models.items():\n    params['clf_forest'] = RandomForestClassifier(**models[model]['forest_grid_search'].best_params_,  \n                   random_state=rng)\n    params['clf_forest'].fit(params['X_train'], y_tot)","execution_count":216,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:31:26.386888Z","start_time":"2018-03-29T04:31:23.316206Z"},"collapsed":true,"trusted":false,"_uuid":"c479c2d580a9300e119168b03618385d8849f658"},"cell_type":"code","source":"model_eval(full_models['set1']['clf_forest'], full_models['set1']['X_train'],\n          y_tot, full_models['set1']['X_train'], y_tot)","execution_count":217,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-03-29T04:31:41.765251Z","start_time":"2018-03-29T04:31:41.733562Z"},"collapsed":true,"hideCode":false,"hidePrompt":false,"trusted":false,"_uuid":"9fadbb675a5a6471ff02a45e20efe27f164fc079"},"cell_type":"code","source":"test_y = full_models['set1']['clf_forest'].predict(full_models['set1']['X_test'])\nresult = pd.DataFrame(data=test_y, index=test_data.index, columns=['Survived'])\nresult.to_csv('submission5.csv')","execution_count":219,"outputs":[]}],"metadata":{"celltoolbar":"Hide code","hide_code_all_hidden":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}