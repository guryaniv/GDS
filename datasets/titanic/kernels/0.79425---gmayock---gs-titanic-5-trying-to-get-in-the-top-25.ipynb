{"cells":[{"metadata":{"_uuid":"82e399c8474cc38a6f111c625b173f7aaf997373"},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\n['train.csv', 'gender_submission.csv', 'test.csv']\n# Step 1 is to import both data sets\ntraining_data = pd.read_csv(\"../input/train.csv\")\ntesting_data = pd.read_csv(\"../input/test.csv\")\n\n# Step two is to create columns which I will add to the respective datasets, in order to know which row came from which dataset when I combine the datasets\ntraining_column = pd.Series([1] * len(training_data))\ntesting_column = pd.Series([0] * len(testing_data))\n\n# Now we append them by creating new columns in the original data. We use the same column name\ntraining_data['is_training_data'] = training_column\ntesting_data['is_training_data'] = testing_column","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134ebeb91f0cf5b14ac16223a56e6e2caec8abed"},"cell_type":"markdown","source":"# Combine and process"},{"metadata":{"trusted":true,"_uuid":"7a57f3d3aea5941d8bddcec3ff51dec15bfb1b0c"},"cell_type":"code","source":"# Now we can merge the datasets while retaining the key to split them later\ncombined_data = training_data.append(testing_data, ignore_index=True, sort=False)\n\n# Encode gender (if == female, True)\ncombined_data['female'] = combined_data.Sex == 'female'\n\n# Split out Title\ntitle = []\nfor i in combined_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ncombined_data['title'] = title\n\n# Replace the title values with an aliased dictionary\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    \"Capt\" : \"Rare\",\n    \"Col\" : \"Rare\",\n    \"Major\" : \"Rare\",\n    \"Jonkheer\" : \"Rare\",\n    \"Don\" : \"Rare\",\n    \"Sir\" : \"Rare\",\n    \"Dr\" : \"Rare\",\n    \"Rev\" : \"Rare\",\n    \"the Countess\" : \"Rare\",\n    \"Dona\" : \"Rare\",\n    \"Mme\" : \"Mrs\",\n    \"Mlle\" : \"Miss\",\n    \"Ms\" : \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Rare\"\n}\ncleaned_title = title_arr.map(title_dict)\ncombined_data['cleaned_title'] = cleaned_title\n\n# Fill NaN of Age - first create groups to find better medians than just the overall median and fill NaN with the grouped medians\ngrouped = combined_data.groupby(['female','Pclass', 'cleaned_title']) \ncombined_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n#add an age bin\nage_bin_conditions = [\n    combined_data['Age'] == 0,\n    (combined_data['Age'] > 0) & (combined_data['Age'] <= 16),\n    (combined_data['Age'] > 16) & (combined_data['Age'] <= 34),\n    (combined_data['Age'] > 34) & (combined_data['Age'] <= 49),\n    (combined_data['Age'] > 49) & (combined_data['Age'] <= 64),\n    combined_data['Age'] > 64\n]\nage_bin_outputs = [0, 1, 2, 3, 4, 5]\ncombined_data['age_bin'] = np.select(age_bin_conditions, age_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Embarked\ncombined_data['Embarked'] = combined_data['Embarked'].fillna(\"S\") \n\n# Fill NaN of Fare, adding flag for boarded free, binning other fares\ncombined_data['Fare'] = combined_data['Fare'].fillna(combined_data['Fare'].mode()[0]) \ncombined_data['boarded_free'] = combined_data['Fare'] == 0 \nfare_bin_conditions = [\n    combined_data['Fare'] == 0,\n    (combined_data['Fare'] > 0) & (combined_data['Fare'] <= 7.9),\n    (combined_data['Fare'] > 7.9) & (combined_data['Fare'] <= 14.4),\n    (combined_data['Fare'] > 14.4) & (combined_data['Fare'] <= 31),\n    combined_data['Fare'] > 31\n]\nfare_bin_outputs = [0, 1, 2, 3, 4]\ncombined_data['fare_bin'] = np.select(fare_bin_conditions, fare_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.\ncombined_data['Cabin'] = combined_data['Cabin'].fillna(\"U\") \n\n# Counting how many people are riding on a ticket\nfrom collections import Counter\ntickets_count = pd.DataFrame([Counter(combined_data['Ticket']).keys(), Counter(combined_data['Ticket']).values()]).T\ntickets_count.rename(columns={0:'Ticket', 1:'ticket_riders'}, inplace=True)\ntickets_count['ticket_riders'] = tickets_count['ticket_riders'].astype(int)\ncombined_data = combined_data.merge(tickets_count, on='Ticket')\n\n# Finding cabin group\ncabin_group = []\nfor i in combined_data['Cabin']:\n    cabin_group.append(i[0])\ncombined_data['cabin_group'] = cabin_group\n\n# Adding a family_size feature as it may have an inverse relationship to either of its parts\ncombined_data['family_size'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Mapping ports to passenger pickup order\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ncombined_data['pickup_order'] = combined_data['Embarked'].map(port)\n\n# Encode childhood\ncombined_data['child'] = combined_data.Age < 16\n\n# One-Hot Encoding the titles\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cleaned_title'], prefix=\"C_T\")], axis = 1)\n\n# One-Hot Encoding the Pclass\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Pclass'], prefix=\"PClass\")], axis = 1)\n\n# One-Hot Encoding the  cabin group\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cabin_group'], prefix=\"C_G\")], axis = 1)\n\n# One-Hot Encoding the ports\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Embarked'], prefix=\"Embarked\")], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38745e1e38eabbb933efc3f627e4fc1181dd366a"},"cell_type":"code","source":"new_train_data=combined_data.loc[combined_data['is_training_data']==1]\nnew_test_data=combined_data.loc[combined_data['is_training_data']==0]\n# here is the expanded model set and metric tools\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\nk_fold = KFold(n_splits = 10, shuffle=True, random_state=0) \nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n# Here are the features\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Rare', 'C_G_A', 'C_G_B', 'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', \n            'C_G_T', 'C_G_U', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3', 'ticket_riders']\ntarget = 'Survived'\ncvs_train_data = new_train_data[features]\ncvs_test_data = new_test_data[features]\ncvs_target = new_train_data['Survived']\ncvs_train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb2f704d4129136fac366820d179be8ae36a5d1"},"cell_type":"code","source":"# Define the classifiers I will use\nclassifiers = [\n    RandomForestClassifier(n_estimators=10, random_state=0),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=9, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    DecisionTreeClassifier(random_state=0),\n    LogisticRegression(solver='liblinear'),\n    KNeighborsClassifier(n_neighbors=15),\n    SVC(gamma='auto'),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(),\n    GaussianNB(),\n    LinearSVC()]\n\n# Fit and use cross_val_score and k_fold to score accuracy\nclf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(cvs_train_data, cvs_target)\n    prediction = clf.predict(cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1\n    \nprint(clf_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa585e1c1f2b7f373197824d28243ad8e7a29d21"},"cell_type":"code","source":"clf_scores2 = clf_scores.copy()\ndf_scores = pd.DataFrame(clf_scores2, columns=clf_scores2.pop(0))\ndf_scores.sort_values('Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86ae3a62e4ebe719682bafad3217eb1b1369ef71"},"cell_type":"markdown","source":"# Well I am stuck with feature engineering. Let's see if I can get rid of some noise by checking feature selection"},{"metadata":{"trusted":true,"_uuid":"9b76eb20aa2e4cfd0af1c8f4058896cc98012385"},"cell_type":"code","source":"from sklearn.feature_selection import chi2, SelectKBest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89a1762388ecf1cacd57c6bc0f9c1c221f3e4d68"},"cell_type":"code","source":"fit_list = [SelectKBest(score_func=chi2, k=i) for i in range (0,32)]\n\nfit_test = SelectKBest(score_func=chi2, k='all')\nfit_test.fit(cvs_train_data, cvs_target)\nnp.set_printoptions(precision=3)\n\nfeatures3 = fit_test.transform(cvs_train_data)\nfit_test_scores = pd.DataFrame(fit_test.scores_)\nfit_test_scores['feature_name'] = pd.DataFrame(cvs_train_data.columns)\nfit_test_scores.columns = ['chi_squared', 'feature_name']\nfit_test_scores.sort_values('chi_squared', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94d085ce902e44d4021f1477fe622f496c3fccd3"},"cell_type":"code","source":"# So we're just going to take the features with a chi_squared over 1. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c55c63c0f9186e9145cac4ccef369d8284e1eb6"},"cell_type":"code","source":"new_features = ['Fare','female','C_T_Mr','C_T_Mrs','C_T_Miss','PClass_1','PClass_3','C_G_B','C_G_U','Embarked_C','Age','C_G_D','C_G_E','child','C_G_C','Parch','C_T_Master','PClass_2','ticket_riders','Embarked_S','pickup_order','C_G_F','SibSp']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc5ecedbbfd28bca99305845d26c157a36282890"},"cell_type":"code","source":"len(new_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b2b99c4fa23d0185baa6b9857d6cc8917d4b01c"},"cell_type":"code","source":"# rf_ stands for reduced features\nrf_cvs_train_data = new_train_data[new_features]\nrf_cvs_test_data = new_test_data[new_features]\nrf_cvs_target = new_train_data['Survived']\nrf_cvs_train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a66995abd3501927383c06184ca30ebf5b700f9"},"cell_type":"code","source":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, rf_cvs_train_data, rf_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(rf_cvs_train_data, rf_cvs_target)\n    prediction = clf.predict(rf_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_2.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"527054afaa5094f220caf5ca2550106c9855d867"},"cell_type":"markdown","source":"# try again with bins"},{"metadata":{"trusted":true,"_uuid":"3e36230679369ee2b666699b3587550c98df5d06"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a7ce30872a0a7daf8d63d81a01c16737d4e818f"},"cell_type":"code","source":"new_features_2 = ['age_bin', 'SibSp', 'Parch', 'fare_bin', 'female', 'child', 'Embarked_S', 'Embarked_C', \n            'Embarked_Q', 'pickup_order', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Rare', 'family_size', 'PClass_1', 'PClass_2', 'PClass_3', 'ticket_riders']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b63ee4a0c069af664ba5c01911f4c92d1fb42b"},"cell_type":"code","source":"#nf stands for new features aka with bins\nnf_cvs_train_data = new_train_data[new_features_2]\nnf_cvs_test_data = new_test_data[new_features_2]\nnf_cvs_target = new_train_data['Survived']\nnf_cvs_train_data.shape, nf_cvs_test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a26eafebe30c76faf23a1a65ad1de7a44c2d95e"},"cell_type":"code","source":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, nf_cvs_train_data, nf_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(nf_cvs_train_data, nf_cvs_target)\n    prediction = clf.predict(nf_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_3.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ccf9f671b3f28d5fcceff8016cb3a8b8eeee55"},"cell_type":"markdown","source":"# I need to find a better way to determine which features to use. "},{"metadata":{"trusted":true,"_uuid":"b3a1dfac78e83f5a9b3f120b4933dec7f9b648c6"},"cell_type":"code","source":"combined_data.dtypes.index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8395c6a4637645c6f074212830ed5e0f825cf1b5"},"cell_type":"markdown","source":"## New chi squared"},{"metadata":{"trusted":true,"_uuid":"7075d8aff27b6f28495d9ffb12ec0cd0d2c0c606"},"cell_type":"code","source":"new_features_3 = ['PassengerId', 'Pclass', 'Age', 'SibSp',\n       'Parch', 'Fare', 'female', 'age_bin', 'boarded_free',\n       'fare_bin', 'ticket_riders', 'family_size',\n       'pickup_order', 'child', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n       'C_T_Rare', 'PClass_1', 'PClass_2', 'PClass_3', 'C_G_A', 'C_G_B',\n       'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', 'C_G_T', 'C_G_U',\n       'Embarked_C', 'Embarked_Q', 'Embarked_S']\n#nf stands for new features aka with bins\nnf3_cvs_train_data = new_train_data[new_features_3]\nnf3_cvs_test_data = new_test_data[new_features_3]\nnf3_cvs_target = new_train_data['Survived'].astype(int)\nnf3_cvs_train_data.shape, nf3_cvs_test_data.shape, nf3_cvs_target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b350a9661a4b6150a6d05a55c519e711ab4df02"},"cell_type":"code","source":"fit_list = [SelectKBest(score_func=chi2, k=i) for i in range (0,32)]\n\nfit_test = SelectKBest(score_func=chi2, k='all')\nfit_test.fit(nf3_cvs_train_data, nf3_cvs_target)\nnp.set_printoptions(precision=3)\n\nfeatures3 = fit_test.transform(nf3_cvs_train_data)\nfit_test_scores = pd.DataFrame(fit_test.scores_)\nfit_test_scores['feature_name'] = nf3_cvs_train_data.columns\nfit_test_scores.columns = ['chi_squared', 'feature_name']\nfit_test_scores.sort_values('chi_squared', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf948315bb9acc531327de1ef71de84b63e87840"},"cell_type":"code","source":"from sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccc7af16d91c2dd2118ac8db1347d9132e0e2451"},"cell_type":"code","source":"classifiers2 = [\n    RandomForestClassifier(n_estimators=10, random_state=0),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=9, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    DecisionTreeClassifier(random_state=0),\n    LogisticRegression(solver='liblinear'),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(),\n    LinearSVC()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"100fcc83385aeccfc99ae713419599907d825d3c"},"cell_type":"code","source":"# d = [['clf_name', 'feature_name', 'n_features', 'support', 'ranking']]\n# for clf in classifiers2:\n#     for i in range (1,3):\n#         name = clf.__class__.__name__\n#         print(name, i)\n#         rfe = RFE(clf, i)\n#         fitted_clf = rfe.fit(nf3_cvs_train_data, nf3_cvs_target)\n#         n_feat = fitted_clf.n_features_\n#         n_supp = fitted_clf.support_\n#         rank = fitted_clf.ranking_\n#         d_new = [name, new_features_3, n_feat, n_supp, rank]\n#         d.append(d_new)\n        \n# d_f = pd.DataFrame(d)\n# d_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9de1da27cb06e45597408f2e58762169e5da8f9d"},"cell_type":"code","source":"# from yellowbrick.features import RFECV\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Create RFECV visualizer with linear SVM classifier\n# for clf in classifiers2:\n#     viz = RFECV(clf)\n#     viz.fit(nf3_cvs_train_data, nf3_cvs_target);\n#     viz.poof();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1b746632f81cb61ebb84f75262f573e908b5f1"},"cell_type":"code","source":"for clf in classifiers2:\n    for i in (1, 3, 8, 15, 20, 21, 24, 27, 28, 31, 33):\n        name = clf.__class__.__name__\n        print(name, i)\n        rfe = RFE(clf, i)\n        fitted_clf = rfe.fit(nf3_cvs_train_data, nf3_cvs_target)\n        n_feat = fitted_clf.n_features_\n        n_supp = fitted_clf.support_\n        rank = fitted_clf.ranking_\n        d_new = [name, new_features_3, n_feat, n_supp, rank]\n        d.append(d_new)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b261b575f3be20077a99c37ef05c9fed594cb55"},"cell_type":"code","source":"d_f = pd.DataFrame(d)\nd_f.to_csv('csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63c9374c07fd504e61f2d59b48fe6b09fcea2f3a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98c93fd7b553d314ec504ceb4454139bc8720265"},"cell_type":"code","source":"# Fit and use cross_val_score and k_fold to score accuracy\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, nf3_cvs_train_data, nf3_cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(nf3_cvs_train_data, nf3_cvs_target)\n    prediction = clf.predict(nf3_cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 's_{}_{}_4.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc04b53a150f89a8f42e3a5c8ed25e205efb7af0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}