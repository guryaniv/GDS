{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["Feature engineering and LightGBM framework.\n", "Datasets:\n", "\n", " - New York City Taxi Trip Duration (https://www.kaggle.com/c/nyc-taxi-trip-duration)\n", " - New York City Taxi with OSRM (https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm)\n", " - Weather data in New York City - 2016 (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\n", " - New York City Taxi Trip - Hourly Weather Data (https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data)"], "metadata": {"_execution_state": "idle", "_uuid": "1eae1a3c34c175a43f7bb3af33741102d07a63cc", "_cell_guid": "b976893c-35bd-452c-be1e-e6f7212648ad"}}, {"metadata": {"_execution_state": "idle", "_uuid": "76715269bd776d2661cc2bcd1d3edf8dc14c4c8f", "_cell_guid": "612bb5d3-1401-4cac-8fde-367c9b77132e", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "# Plots\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "# Preprocessing\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.decomposition import PCA\n", "from sklearn.cluster import MiniBatchKMeans\n", "import gc \n", "# LightGBM framework\n", "import lightgbm as lgb"], "execution_count": 1}, {"cell_type": "markdown", "source": ["## Data"], "metadata": {"_execution_state": "idle", "_uuid": "1939bb61957ae2d666fc81f24a6abe1f2e229c3f", "_cell_guid": "e2c5ecf6-d988-43b9-a599-154c0da2807d"}}, {"metadata": {"_execution_state": "idle", "_uuid": "abf982994a366fa6b9d6ef972546fd3993db1fba", "_cell_guid": "6399403e-0535-477a-8c47-f72aee532313"}, "cell_type": "code", "outputs": [], "source": ["### Get the data\n", "# Main dataset\n", "train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\n", "test = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\n", "# New York City Taxi with OSRM \n", "# (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\n", "train_fastest_1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv')\n", "train_fastest_2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv')\n", "test_fastest = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv')\n", "# Weather data in New York City - 2016\n", "# (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\n", "weather = pd.read_csv('../input/weather-data-in-new-york-city-2016/weather_data_nyc_centralpark_2016.csv')\n", "# New York City Taxi Trip - Hourly Weather Data\n", "# (https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data)\n", "weather_hour = pd.read_csv('../input/new-york-city-taxi-trip-hourly-weather-data/Weather.csv')\n", "# Train-validation split\n", "train, valid, _, _ = train_test_split(train, train.trip_duration, \n", "                                      test_size=0.2, random_state=2017)\n", "# Add set marker\n", "train['eval_set'] = 0; valid['eval_set'] = 1; test['eval_set'] = 2\n", "test['trip_duration'] = np.nan; test['dropoff_datetime'] = np.nan\n", "# Glue tables\n", "frame = pd.concat([train, valid, test], axis=0)\n", "frame_fastest = pd.concat([train_fastest_1, train_fastest_2, test_fastest], axis = 0)\n", "\n", "### Memory optimization\n", "# Main dataframe\n", "frame.eval_set = frame.eval_set.astype(np.uint8)\n", "frame.passenger_count = frame.passenger_count.astype(np.int8)\n", "frame.store_and_fwd_flag = pd.get_dummies(frame['store_and_fwd_flag'], \n", "                                          prefix='store_and_fwd_flag', drop_first=True)\n", "frame.vendor_id = frame.vendor_id.astype(np.int8)\n", "# Weather dataframe\n", "weather.replace('T', 0.001, inplace=True)\n", "weather['date'] = pd.to_datetime(weather['date'], dayfirst=True).dt.date\n", "weather['average temperature'] = weather['average temperature'].astype(np.int64)\n", "weather['precipitation'] = weather['precipitation'].astype(np.float64)\n", "weather['snow fall'] = weather['snow fall'].astype(np.float64)\n", "weather['snow depth'] = weather['snow depth'].astype(np.float64)\n", "# Weather hourly dataframe\n", "weather_hour['Datetime'] = pd.to_datetime(weather_hour['pickup_datetime'], dayfirst=True)\n", "weather_hour['date'] = weather_hour.Datetime.dt.date\n", "weather_hour['hour'] = weather_hour.Datetime.dt.hour\n", "weather_hour['hour'] = weather_hour.hour.astype(np.int8)\n", "weather_hour['fog'] = weather_hour.fog.astype(np.int8)\n", "weather_hour = weather_hour[['date', 'hour', 'tempm', 'dewptm', 'hum', 'wspdm', \n", "                             'wdird', 'vism', 'pressurei', 'fog']]\n", "del train, valid, test, train_fastest_1, train_fastest_2, test_fastest\n", "gc.collect();"], "execution_count": 2}, {"cell_type": "markdown", "source": ["## Adding features"], "metadata": {"_execution_state": "idle", "_uuid": "c0ea4940e6140fccdf7c415f2b2767dc8f50707d", "_cell_guid": "06d32536-3ae2-4443-89a9-8cd198729df0"}}, {"metadata": {"_execution_state": "idle", "_uuid": "e16fb1997acbb9a9e7e617adff5604ebc7b28cbe", "_cell_guid": "599321d4-68a6-438f-a9ed-ae3cdcf2c6c7", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["# Define clusters\n", "def clusters(df, pic=False):\n", "    coords = np.vstack((df[['pickup_longitude', 'pickup_latitude']].values,\n", "                        df[['dropoff_longitude', 'dropoff_latitude']].values))\n", "    sample_ind = np.random.permutation(len(coords))[:500000]\n", "    kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n", "    cl_pickup = kmeans.predict(df[['pickup_longitude', 'pickup_latitude']])\n", "    cl_dropoff = kmeans.predict(df[['dropoff_longitude', 'dropoff_latitude']])\n", "    # If pic = True, show pictures\n", "    if pic:\n", "        plt.figure(figsize=(10,4))\n", "        plt.subplot(1,2,1)\n", "        plt.scatter(df.pickup_longitude.values, \n", "                    df.pickup_latitude.values, \n", "                    s=0.2, lw=0, c=cl_pickup, cmap='tab20', alpha=0.5)\n", "        plt.xlim(-74.03, -73.77); plt.ylim(40.63, 40.85)\n", "        plt.xlabel('pickup_longitude'); plt.ylabel('pickup_latitude')\n", "        plt.subplot(1,2,2)\n", "        plt.scatter(df.dropoff_longitude.values, \n", "                    df.dropoff_latitude.values, \n", "                    s=0.2, lw=0, c=cl_dropoff, cmap='tab20', alpha=0.5)\n", "        plt.xlim(-74.03, -73.77); plt.ylim(40.63, 40.85)\n", "        plt.xlabel('dropoff_longitude'); plt.ylabel('dropoff_latitude')\n", "        plt.show()\n", "    return cl_pickup, cl_dropoff\n", "\n", "# Rotate the map\n", "def rotate_coords(df, col1, col2, pic=False):\n", "    alpha = 0.610865 # angle = 35 degrees\n", "    #alpha = 0.506145 # angle = 29 degrees\n", "    # Center of rotation\n", "    x_c = df[col1].mean()\n", "    y_c = df[col2].mean()\n", "    # Coordinates\n", "    C = df[[col1, col2]] - np.array([x_c, y_c])\n", "    # Rotation matrix\n", "    R = np.array([[np.cos(alpha), -np.sin(alpha)],\n", "                  [np.sin(alpha),  np.cos(alpha)]])\n", "    C_rot = np.matmul(R, C.transpose().values).transpose() + np.array([x_c, y_c])    \n", "    if pic:\n", "        plt.figure(figsize=(10,4))\n", "        plt.subplot(1,2,1)\n", "        plt.scatter(df[col1], df[col2], s=0.2, alpha=0.5)\n", "        plt.xlabel(col1); plt.ylabel(col2); \n", "        plt.ylim(40.6, 40.9); plt.xlim(-74.1, -73.7);\n", "        plt.subplot(1,2,2)\n", "        plt.scatter(C_rot[:, 0], C_rot[:, 1], s=0.2, alpha=0.5)\n", "        plt.xlabel(col1+'_rot'); plt.ylabel(col2+'_rot'); \n", "        plt.ylim(40.6, 40.9); plt.xlim(-74.1, -73.7);\n", "        plt.show()\n", "    return C_rot\n", "\n", "# Manhattan distances\n", "def my_manhattan_distances(x1, x2, y1, y2):\n", "    return np.abs(x1 - x2) + np.abs(y1 - y2)\n", "# Euclidean distances\n", "def my_euclidean_distances(x1, x2, y1, y2):\n", "    return np.square(x1 - x2) + np.square(y1 - y2)\n", "my_manhattan_distances = np.vectorize(my_manhattan_distances)\n", "my_euclidean_distances = np.vectorize(my_euclidean_distances)\n", "\n", "# Adding features\n", "def add_features(df, predict=False):\n", "    # If predict = True, this function will prepare (add new features) \n", "    # train set (all train data) and test (all test data); else, \n", "    # if predict = False, the function will prepare train and validation datasets\n", "    if predict: \n", "        train_inds = df[(df.eval_set != 2)].index\n", "    else:\n", "        df = df[(df.eval_set != 2)].copy()\n", "        train_inds = df[df.eval_set != 1].index\n", "    \n", "    ### Log trip\n", "    print('Log trip duration')\n", "    df['trip_duration'] = df.trip_duration.apply(np.log)\n", "    \n", "    ### PCA transformation\n", "    print('Add PCA geo-coordinates')\n", "    coords = np.vstack((df[['pickup_latitude', 'pickup_longitude']], \n", "                        df[['dropoff_latitude', 'dropoff_longitude']]))\n", "    pca = PCA().fit(coords) # define 2 main axis\n", "    df['pickup_pca0'] = pca.transform(df[['pickup_longitude', 'pickup_latitude']])[:,0]\n", "    df['pickup_pca1'] = pca.transform(df[['pickup_longitude', 'pickup_latitude']])[:,1]\n", "    df['dropoff_pca0'] = pca.transform(df[['dropoff_longitude', 'dropoff_latitude']])[:,0]\n", "    df['dropoff_pca1'] = pca.transform(df[['dropoff_longitude', 'dropoff_latitude']])[:,1]\n", "    df['distance_pca0'] = np.abs(df.pickup_pca0-df.dropoff_pca0)\n", "    df['distance_pca1'] = np.abs(df.pickup_pca1-df.dropoff_pca1)\n", "    \n", "    print('Rorate geo-coordinates')\n", "    C_rot_pickup = rotate_coords(df, 'pickup_longitude', 'pickup_latitude', not predict)\n", "    C_rot_dropoff = rotate_coords(df, 'dropoff_longitude', 'dropoff_latitude', not predict)\n", "    df['pickup_longitude_rot'] = C_rot_pickup[:, 0]\n", "    df['pickup_latitude_rot'] = C_rot_pickup[:, 1]\n", "    df['dropoff_longitude_rot'] = C_rot_dropoff[:, 0]\n", "    df['dropoff_latitude_rot'] = C_rot_dropoff[:, 1]\n", "    \n", "    ### Add clusters\n", "    print('Add clusters')\n", "    cl_pu, cl_do = clusters(df, not predict)\n", "    df['pickup_clusters'] = cl_pu\n", "    df['dropoff_clusters'] = cl_do\n", "       \n", "    ### to DateTime\n", "    print('Convert to datetime format')\n", "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n", "    # Add weather info\n", "    df['date'] = df['pickup_datetime'].dt.date # adding date column\n", "    df['hour'] = df['pickup_datetime'].dt.hour # adding hour column\n", "    df = pd.merge(left=df, right=weather, on='date', how='left')\n", "    # Add weather hourly\n", "    df = pd.merge(left=df, right=weather_hour.drop_duplicates(subset=['date', 'hour']), \n", "                  on=['date', 'hour'], how='left')\n", "    df.drop(['date'], axis=1, inplace=True)\n", "    # Weather added\n", "    df['month'] = df['pickup_datetime'].dt.month\n", "    df['week_of_year'] = df['pickup_datetime'].dt.week\n", "    df['day'] = df['pickup_datetime'].dt.day\n", "    df['month_day'] = df['month'] + df['day']\n", "    df['day_of_year'] = df['pickup_datetime'].dt.dayofyear\n", "    #df['hour'] = df['pickup_datetime'].dt.hour\n", "    df['day_of_year_hour'] = 24*df['day_of_year'] + df['hour']\n", "    df['hour_minute'] = 60*df['hour'] + df['pickup_datetime'].dt.minute\n", "    df['day_week'] = df['pickup_datetime'].dt.weekday\n", "    df['month_day_hour'] = 31*24*df['month'] + 24*df['day'] + df['hour']\n", "    \n", "    ### Some usfull averages ###\n", "    print('Add averages')\n", "    train_info = df.iloc[train_inds].copy() # get only train data\n", "    # Month average \n", "    month_avg = train_info.groupby('month').trip_duration.mean()\n", "    month_avg = month_avg.reset_index(); month_avg.columns = ['month', 'month_avg']\n", "    df = pd.merge(left=df, right=month_avg, on='month', how='left')\n", "    \n", "    \n", "    # Week of year average\n", "    week_year_avg = train_info.groupby('week_of_year').trip_duration.mean()\n", "    week_year_avg = week_year_avg.reset_index()\n", "    week_year_avg.columns = ['week_of_year', 'week_of_year_avg']\n", "    df = pd.merge(left=df, right=week_year_avg, on='week_of_year', how='left')\n", "        \n", "    # Day of month average\n", "    day_month_avg = train_info.groupby('day').trip_duration.mean()\n", "    day_month_avg = day_month_avg.reset_index()\n", "    day_month_avg.columns = ['day', 'day_of_month_avg']\n", "    df = pd.merge(left=df, right=day_month_avg, on='day', how='left')\n", "        \n", "    # Day of year average\n", "    day_year_avg = train_info.groupby('day_of_year').trip_duration.mean()\n", "    day_year_avg = day_year_avg.reset_index()\n", "    day_year_avg.columns = ['day_of_year', 'day_of_year_avg']\n", "    df = pd.merge(left=df, right=day_year_avg, on='day_of_year', how='left')\n", "        \n", "    # Hour average\n", "    hour_avg = train_info.groupby('hour').trip_duration.mean()\n", "    hour_avg = hour_avg.reset_index(); hour_avg.columns = ['hour', 'hour_avg']\n", "    df = pd.merge(left=df, right=hour_avg, on='hour', how='left')\n", "        \n", "    # Day week average\n", "    day_week_avg = train_info.groupby('day_week').trip_duration.mean()\n", "    day_week_avg = day_week_avg.reset_index()\n", "    day_week_avg.columns = ['day_week', 'day_week_avg']\n", "    df = pd.merge(left=df, right=day_week_avg, on='day_week', how='left')  \n", "    \n", "    # Clusters\n", "    print('Pickup clusters')\n", "    cl_pu_avg = train_info.groupby('pickup_clusters').trip_duration.mean()\n", "    cl_pu_avg = cl_pu_avg.reset_index()\n", "    cl_pu_avg.columns = ['pickup_clusters', 'pickup_clusters_avg']\n", "    df = pd.merge(left=df, right=cl_pu_avg, on='pickup_clusters', how='left')\n", "    \n", "    print('Dropoff clusters')\n", "    cl_do_avg = train_info.groupby('dropoff_clusters').trip_duration.mean()\n", "    cl_do_avg = cl_do_avg.reset_index()\n", "    cl_do_avg.columns = ['dropoff_clusters', 'dropoff_clusters_avg']\n", "    df = pd.merge(left=df, right=cl_do_avg, on='dropoff_clusters', how='left')\n", "    \n", "    ### Distances ###\n", "    print('Add distances')\n", "    # Manhattan rot\n", "    df['distance_manhattan_rot'] = my_manhattan_distances(df.pickup_longitude_rot, \n", "                                                          df.dropoff_longitude_rot,\n", "                                                          df.pickup_latitude_rot, \n", "                                                          df.dropoff_latitude_rot)\n", "    # Manhattan pca\n", "    df['distance_manhattan_pca'] = df['distance_pca0'] + df['distance_pca1']\n", "    # Euclidean\n", "    df['distance_euclidean'] = my_euclidean_distances(df.pickup_latitude, \n", "                                                      df.dropoff_latitude,\n", "                                                      df.pickup_longitude, \n", "                                                      df.dropoff_longitude)\n", "    \n", "    # Fastest route\n", "    df = pd.merge(left=df,\n", "                  right=frame_fastest[['id', 'total_distance', \n", "                                       'total_travel_time', 'number_of_steps']],\n", "                  on='id', how='left')\n", "    \n", "    # Same destination\n", "    print('Add same destination marker')\n", "    df['same_destination'] = (df.distance_euclidean == 0).astype(np.uint)\n", "    \n", "    # Remove outliers\n", "    mask = (df.eval_set != 2) & (df.trip_duration > np.log(20*24*60*60))\n", "    print('Delete outliers:', mask.astype(np.uint).sum())\n", "    df = df[~mask]\n", "    return df"], "execution_count": 3}, {"cell_type": "markdown", "source": ["## Train-validation split"], "metadata": {"_execution_state": "idle", "_uuid": "6bdfd7b4a68dc9711cc4f1c6a6382c83eedbe302", "_cell_guid": "49813acd-d4d8-4a8e-ac7a-0884dbb1e7a5"}}, {"metadata": {"_execution_state": "idle", "_uuid": "bc93aef2e1eabc42b062d8dbe4256231fe0e5ecb", "_cell_guid": "338cc15e-8d31-4424-a44b-9fbb83553c18"}, "cell_type": "code", "outputs": [], "source": ["### Add features\n", "frame_augm = add_features(frame, predict=False)\n", "\n", "drop_cols = ['dropoff_datetime', 'eval_set', 'pickup_datetime'] \\\n", "            + ['dropoff_pca1', 'month_avg', 'dropoff_longitude', \\\n", "               'dropoff_longitude_rot', 'pickup_longitude', \\\n", "               'pickup_longitude_rot', 'pickup_pca0', \\\n", "               'pickup_pca1', 'dropoff_pca0']\n", "# Train\n", "X_train = frame_augm[(frame_augm.eval_set==0)].copy()\n", "train_id = X_train.pop('id')\n", "y_train = X_train.pop('trip_duration')\n", "X_train.drop(drop_cols, axis=1, inplace=True)\n", "# Validation\n", "X_valid = frame_augm[frame_augm.eval_set==1].copy()\n", "valid_id = X_valid.pop('id')\n", "y_valid = X_valid.pop('trip_duration')\n", "X_valid.drop(drop_cols, axis=1, inplace=True)\n", "print('Train shape:', X_train.shape, '\\nTest shape:', X_valid.shape)"], "execution_count": 4}, {"cell_type": "markdown", "source": ["## LightDBM model\n", "\n", "### Important:\n", "\n", "Since kernels should work **under 1200 seconds** in the next cell some parameters of the model were changed. For better CV try, for example, **learning_rate = 0.3**. Some other parameters can be optimized too."], "metadata": {"_execution_state": "idle", "_uuid": "88be8bf2033834e2deadfe30c73dd8586b584ebd", "_cell_guid": "a8a5570f-513c-4f69-9bdf-a38369eed2b8"}}, {"metadata": {"_execution_state": "idle", "_uuid": "9bbfce28d7e16111e2fd3204bbdea89fa5a99f4a", "_cell_guid": "d4f471de-6910-4427-b82d-0fbe64c07f6b"}, "cell_type": "code", "outputs": [], "source": ["def lgb_rmsle_score(preds, dtrain):\n", "    labels = np.exp(dtrain.get_label())\n", "    preds = np.exp(preds.clip(min=0))\n", "    return 'rmsle', np.sqrt(np.mean(np.square(np.log1p(preds)-np.log1p(labels)))), False\n", "\n", "d_train = lgb.Dataset(X_train, y_train)\n", "\n", "lgb_params = {\n", "    'learning_rate': 1.0, # try 0.2\n", "    'max_depth': 8,\n", "    'num_leaves': 55, \n", "    'objective': 'regression',\n", "    #'metric': {'rmse'},\n", "    'feature_fraction': 0.9,\n", "    'bagging_fraction': 0.5,\n", "    #'bagging_freq': 5,\n", "    'max_bin': 200}       # 1000\n", "cv_result_lgb = lgb.cv(lgb_params,\n", "                       d_train, \n", "                       num_boost_round=5000, \n", "                       nfold=3, \n", "                       feval=lgb_rmsle_score,\n", "                       early_stopping_rounds=50, \n", "                       verbose_eval=100, \n", "                       show_stdv=True)\n", "n_rounds = len(cv_result_lgb['rmsle-mean'])\n", "print('num_boost_rounds_lgb=' + str(n_rounds))"], "execution_count": null}, {"metadata": {"_execution_state": "idle", "_uuid": "03951ddcaff665474a7f0b091b444a8810c1be87", "_cell_guid": "7f9b8087-0fba-4553-b2fb-ffd51c2bbf4c", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["def dummy_rmsle_score(preds, y):\n", "    return np.sqrt(np.mean(np.square(np.log1p(np.exp(preds))-np.log1p(np.exp(y)))))\n", "\n", "# Train a model\n", "model_lgb = lgb.train(lgb_params, \n", "                      d_train, \n", "                      feval=lgb_rmsle_score, \n", "                      num_boost_round=n_rounds)\n", "# Predict on train\n", "y_train_pred = model_lgb.predict(X_train)\n", "print('RMSLE on train = {}'.format(dummy_rmsle_score(y_train_pred, y_train)))\n", "# Predict on validation\n", "y_valid_pred = model_lgb.predict(X_valid)\n", "print('RMSLE on valid = {}'.format(dummy_rmsle_score(y_valid_pred, y_valid)))"], "execution_count": null}, {"metadata": {"_execution_state": "idle", "_uuid": "6bee32e55e175baa239c086fba9fcce842e597e1", "_cell_guid": "916275bf-071e-44c7-8dc6-1cbf162f2205", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["plt.figure(figsize=(10,5))\n", "# CV scores\n", "plt.subplot(1,2,1)\n", "train_scores = np.array(cv_result_lgb['rmsle-mean'])\n", "train_stds = np.array(cv_result_lgb['rmsle-stdv'])\n", "plt.plot(train_scores, color='green')\n", "plt.fill_between(range(len(cv_result_lgb['rmsle-mean'])), \n", "                 train_scores - train_stds, train_scores + train_stds, \n", "                 alpha=0.1, color='green')\n", "plt.title('LightGMB CV-results')\n", "#plt.ylim(0.34,0.40)\n", "plt.subplot(1,2,2)\n", "plt.scatter(y_valid, y_valid_pred, s=0.2, alpha=0.7)\n", "plt.plot([0,12], [0,12], color='g', alpha=0.3)\n", "plt.xlabel('True (log) validation set'); plt.xlim(0,12)\n", "plt.ylabel('Pred. (log) validation set'); plt.ylim(0,12)\n", "plt.show()"], "execution_count": null}, {"cell_type": "markdown", "source": ["### Feature importance"], "metadata": {"_execution_state": "idle", "_uuid": "305060a5de75eaa4e2e0b8aa9c335773a64529d2", "_cell_guid": "4d3a17df-38fb-4baf-a2bb-2dd6358cb516"}}, {"metadata": {"_execution_state": "idle", "_uuid": "e5f3f3d81b6b0953ee87b4b33bae38ccbff80b4b", "_cell_guid": "3353ec2d-a9b4-48f2-bbe0-ce8d884a0e46", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["feature_imp = pd.Series(dict(zip(X_train.columns, model_lgb.feature_importance()))) \\\n", "                    .sort_values(ascending=False)\n", "feature_imp"], "execution_count": null}, {"cell_type": "markdown", "source": ["### Predict"], "metadata": {"_execution_state": "idle", "_uuid": "2c73dbcba0a128e1f0ebff4fb46fde78fb52ef1c", "_cell_guid": "2bb20226-4c9f-45c4-b658-b23b07f149f1"}}, {"metadata": {"_execution_state": "idle", "_uuid": "530ca9115538f33c5d140d136d4acbd55611f7ec", "_cell_guid": "470d0ba1-84c9-422f-a970-04ddd0248cbb", "collapsed": true}, "cell_type": "code", "outputs": [], "source": ["# Prediction on the test dataset\n", "# Clean training\n", "del frame_augm; gc.collect()\n", "\n", "def test_predict():\n", "    ### Add features\n", "    frame_augm = add_features(frame, True)\n", "    drop_cols = ['dropoff_datetime', 'eval_set', 'pickup_datetime'] \\\n", "                + ['dropoff_pca1', 'month_avg', 'dropoff_longitude', \\\n", "                   'dropoff_longitude_rot', 'pickup_longitude', \\\n", "                   'pickup_longitude_rot', 'pickup_pca0', \\\n", "                   'pickup_pca1', 'dropoff_pca0']\n", "    # Train\n", "    X_train = frame_augm[(frame_augm.eval_set!=2)].copy()\n", "    train_id = X_train.pop('id')\n", "    y_train = X_train.pop('trip_duration')\n", "    X_train.drop(drop_cols, axis=1, inplace=True)\n", "    # Test\n", "    X_test = frame_augm[frame_augm.eval_set==2].copy()\n", "    test_id = X_test.pop('id')\n", "    X_test.drop(drop_cols + ['trip_duration'], axis=1, inplace=True)\n", "    print('Train shape:', X_train.shape, 'Test shape', X_test.shape)\n", "\n", "    # Train a model\n", "    d_train = lgb.Dataset(X_train, y_train)\n", "    model_lgb = lgb.train(lgb_params, \n", "                          d_train, \n", "                          feval=lgb_rmsle_score, \n", "                          num_boost_round=n_rounds)\n", "    # Predict on validation\n", "    y_test_pred = model_lgb.predict(X_test)\n", "    return y_test_pred\n", "\n", "### Predict on TEST\n", "#y_test_pred = test_predict()\n", "### Submission\n", "#subm = pd.DataFrame()\n", "#subm['id'] = test_id\n", "#subm['trip_duration'] = np.exp(y_test_pred)\n", "#subm.to_csv('submission.csv', index=False)"], "execution_count": null}], "nbformat": 4, "nbformat_minor": 1}